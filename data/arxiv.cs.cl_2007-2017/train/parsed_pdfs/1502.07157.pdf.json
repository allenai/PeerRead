{
  "name" : "1502.07157.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exploiting a comparability mapping to improves bi-lingual data categorization: a three-mode data analysis perspective",
    "authors" : [ "Pierre-Francois Marteau" ],
    "emails" : [ "pierre-francois.marteau@univ-ubs.fr", "yannick.crystal@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n07 15\n7v 2\n[ cs\n.I R\n] 2\n6 Fe\nb 20"
    }, {
      "heading" : "1. Introduction",
      "text" : "Parallel corpora are sets of tuples of aligned documents that are formed with texts placed alongside with their translation(s). If such resources are of great utility in particular in the field of assisted translation or multilingual information retrieval, they are expensive to develop and often difficult to transpose from a specialty domain to another. The notion of comparable corpora has emerged in the nineties to palliate this lack of versatility and expensiveness and to offer avenues to a wider scope of applications such as multilingual terminology extraction, multilingual information retrieval or knowledge engineering (Baker, 1996), (EAGLES, 1996). However, the notion of comparability between documents expressed in different languages is not easy to introduce: it is widely admitted that two documents in different languages are comparable when they share analogous criteria of composition, genre and topics. The term of comparable corpora was introduced by (Fung & Yee, 1998), (Munteanu, Fraser, & Marcu, 2004) and remains quite subjective. (Déjean & Gaussier, 2002) proposed a quantitative definition of the concept of comparability according to which \"Two corpora in two languages L1 and L2 are called comparable if there is a significant\nsub-part of the vocabulary of the L1 language corpus, respectively L2 language corpus, whose translation is in the corpus of language L2, respectively L1.\" (Li & Gaussier, 2010) have then derived a quantitative measure that is based on a bilingual translation dictionary. This measure consists primarily in counting the presence of the translations of dictionary entries that occur in the paired documents. It depends in a non-explicit way upon jointly the coverage of the bilingual translation dictionary and the studied corpora themselves.\nThis comparability measure defined for bilingual corpora indeed applies when dealing with monolingual documents that partition in two distinct linguistic spaces, as far as a bilingual dictionary connecting the two spaces is available. At a document level we thus face a situation where monolingual similarity measures exist in each linguistic space that are potentially linked by a comparability measure. In the scope of the construction of thematic comparable corpora, this leads to address the co-classification or co-clustering of bilingual data since we are targeting the mapping of highly comparable clusters of documents that are furthermore thematically coherent in each linguistic space, i.e. characterized by a high intra-similarity. We confront such situation when harvesting multilingual data from the web for instance. With the need for comparable resources getting pressing, approaches that exploit consistently similarity and comparability measures are becoming particularly useful.\nThere is apparently no existing direct method available to map comparable clusters of documents that lay in two different linguistic spaces. Nevertheless, there exist some work which is somehow related to this problem, like biclustering, co-clustering, or two-mode clustering introduced by (Mirkin, 1996) and (Van Mechelen I, 2004). However, these works are mainly relevant to the clustering of the rows and columns (instances and features axes) of a given matrix and does not fit with the sort of three-mode categorization or clustering we are facing.\nRecently, (Jagarlamudi, Daumé, & Udupa, 2011a), (Jagarlamudi, Udupa, Daumé, & Bhole, 2011b) have developed quite successfully a supervised method that learns interlingual representations from aligned training documents. They exploit word association measures and bilingual dictionary to remove noisy pairs of aligned documents. In (Amini & Goutte, 2010) the authors proposed to learn a co-classification from multi-lingual corpora, based on a co-regularization of the categories in order to maintain a consistency of the categorization process across languages. (Li, Gaussier, & Aizawa, 2011) have proposed a solution for clustering bilingual corpora by using the comparability measure only.\nHowever, if our approach also seeks the joint clustering or classification of data that lay in two distinct linguistic spaces, it aims at exploiting, in conjunction with a comparability mapping existing between the two spaces, native similarity measures (a native similarity has to be understood as any quantitative intra-language similarity measure, such as a cosine similarity measure) existing within these two linguistic spaces. More precisely, the proposed approach is lying between the work reported in (Amini & Goutte, 2010) and (Li et al., 2011). It exploits directly, i.e. without any learning phase, the comparability measure that maps the two linguistic spaces to provide new similarity measures that combine native similarity measures with a similarity measure that is induced by the comparability mapping.\nThus the approach that we develop in the following sections only rely on a bilingual dictionary and does not assume that any aligned data preexist as learning data. Indeed, this approach could be enriched using a feature-extraction technique, such as the one proposed\nin (Vu, Aw, & Zhang, 2009) for instance, to align bilingual documents that have a similar content.\nAfter introducing our main motivations, we develop a straightforward mixing model to combine similarity and comparability measures in an efficient way that allows for the development of consistent co-clustering and co-classification of comparable data and assess it on purely synthetic data. We then address the concept of comparability for mapping bilingual textual data, and define, from the original measure proposed by (Li et al., 2011), two alternative variant measures to overcome some limitation of the original measure. To assess the proposed approach on real textual data, we then detail an experimentation based on a subset of comparable documents collected from some Wikipedia categories. Basically, we evaluate jointly the three tested comparability measures and the proposed similaritycomparability mixing model in the scope of co-classification and co-clustering of bilingual data. Finally we discuss our results and draw some perspectives."
    }, {
      "heading" : "2. Motivations: similarity spaces connected by a comparability mapping",
      "text" : "When confronting with complex data one may encounter situations where two distinct spaces S and S ′, in which preexist native similarity measures SS and SS′, are interconnected by a mapping CSS′ . Figure 1 gives an example of such situation. This is the case when considering comparable corpora that are composed with texts written in at least two distinct languages. For such data, a bilingual dictionary allows for the construction of a comparability measure (Li & Gaussier, 2010) yielding to the definition of a comparability mapping (Marteau & Ménier, 2013) that links the two sets of comparable documents. More generally speaking, such case arises in situations where heterogeneous but analogous data is available, through different sources, in different formats, or characterized using different sets of descriptors, or comply to different semantic models such as heterogeneous ontologies for instance, etc. The principle of mapping heterogeneous but comparable data that we address is quite general since it takes the form of any bipartite weighted undirected graph. We call it a comparability mapping. Hence, a comparability mapping establishes a bi-directional connection between the elements of the two similarity spaces that could be used to challenge native similarity measures (or distances) defined in the two spaces. By doing so, we introduce a kind of three mode data analysis scheme: the two first modes are associated to the two native similarity spaces, while the third mode is related to the comparability mapping itself that links these two spaces.\nAs an example, in figure 1, two discrete sets of elements S and S ′ are presented. We suppose that the notion of native similarity between elements of these sets is defined, we call them respectively SS(., .): S × S → R and SS′(., .): S\n′ × S ′ → R. Furthermore, the two sets are point-wisely connected by a mapping defined by a comparability measure CSS′(., .): S × S\n′ → R. This mapping that takes the form of a bipartite graph is a comparability mapping. The edges of this graph are bidirectional and weighted by a real value that can be bounded into [−1, 1].\nThe main idea that we develop in this article is that of similarity induced by a comparability mapping: in other words, if two elements in the set S are mapped to a same subset of elements in the set S ′, then their similarity should be important (and vice versa). a contrario, if two elements in the set S ′ are mapped to disjoint subsets of elements in the set\nS, then their similarity should be small (and vice versa). Thus, in figure 1, from the point of view of the similarity derived from the comparability mapping alone, element e3 should move away from element e2 to get closer to elements e4 and e5. Similarly, element e ′ 6 should move away from elements e′3, e′4 and e ′ 5. The expected utility of such a similarity induced by a comparability mapping is a kind of noise filtering capability. When exploited in conjunction with native similarity measures in S and S ′ a fusion of complementary sources of knowledge is achieved that could help building more robust similarity functions into S and S ′ spaces. The noise in question could have many sources, in particular it could be inherent to the representational models of the element themselves due to a lack of knowledge, e.g. lack of structural variability, data heterogeneity, semantic ambiguities, etc."
    }, {
      "heading" : "3. Combining similarity and comparability: a three-mode analysis scheme",
      "text" : "3.1 Similarity measure induced by a comparability mapping\nIn this line of work, (Marteau & Ménier, 2013) proposed an algorithm, Hit-ComSim, to iteratively construct the concept of similarity induced by a comparability bipartite graph. Unfortunately, this algorithm does not scale well due to its high algorithmic complexity (in O(N4)). We propose here a much more straightforward approach that consists in exploiting directly the comparability matrix constructed from the two bilingual finite collections of documents.\nLet us consider S and S ′ two collections of documents belonging to two distinct linguistic spaces (L and L′ respectively) in which two native similarity measures SS and SS′ are defined. Let C(., .) : SS × SS′ → R be the comparability function that maps the two finite collections or equivalently that defines a weighted bipartite graph between the two linguistic spaces. The two similarity functions SS , SS′ and the comparability measure C allows for the definition of the following three-mode analysis scheme.\nWe define the similarity measures induced by the comparability mapping C as the following two normalized (in [−1, 1]) measures respectively noted SS1,C and SS′,C :\n∀(di, dj) ∈ S 2 and ∀(d′i, d ′ j) ∈ S ′2\nSS,C(di, dj) = CCT (i, j) √\nCCT (i, i)CCT (j, j)\nSS′,C(d ′ i, d ′ j) =\nCTC(i, j) √\nCTC(i, i)CTC(j, j)\n(1)\nThe interpretation of the similarity measures that are induced by a comparability mapping C is straightforward. First, considering each row i of the C matrix as a feature vector that characterizes document di ∈ S, for any (di, dj) ∈ S, CC\nT (i, j) can be interpreted as an inner product between the two feature vectors representing di and dj respectively. Then, SS,C(di, dj) is nothing but a cosine similarity between documents di and dj based on the comparability mapping only.\nSimilarly, considering each column i of the C matrix as a feature vector that characterizes document d′i ∈ S ′, SS′,C(d ′ i, d ′ j) is nothing but a cosine similarity between documents d ′ i and d′j ∈ S ′ based on the comparability mapping only.\n3.2 Mixing native similarity and induced similarity\nThe comparability/similarity mixing model that we propose is a simple linear combination of the native and induced similarity measures defined in each linguistic space. Basically we use a single parameter α ∈ [0, 1] to combine linearly the two measures as follows\nS′S(di, dj) = αSS,C(di, dj) + (1− α)SS(di, dj) S′S′(d ′ i, d ′ j) = αSS′,C(d ′ i, d ′ j) + (1− α)SS′(d ′ i, d ′ j)\n(2)\nSince the induced similarity measures are normalized into the interval [−1, 1], we advocate using a cosine similarity as native similarity measures in the two connected linguistic spaces such that the mixed similarity measures defined by equation 2 are consistent.\nFinally, as this model mixes two sources of native similarity with the induced similarity measures that are directly derived from the comparability mapping, it implements the socalled three-mode data analysis scheme that we were referring to in the motivation section."
    }, {
      "heading" : "4. Experimenting on synthetic data",
      "text" : "To evaluate the effectiveness of the proposed similarity-comparability mixing model, we generated 20 distinct tests by randomly defining:\n• two similarity spaces S and S ′,\n• a categorization of the elements within each of these spaces,\n• the comparability mapping between them.\nThe algorithm 1 describes the way each of these 20 tests is generated. The variance parameters Vs and Vc are set up such that the categories are significantly overlapping making the classification problems difficult enough. To put more discriminative weight on the native similarity measures, the variance Vc associated to the comparability mapping matrix that is used to provide the induced similarity measures is three times the variance Vs used for producing the native similarity measures.\nAlgorithm 1 Random generation of two native similarity spaces connected by a comparability mapping. The algorithm provides two random similarity matrices SS and SS′, the random comparability mapping matrix CS,S′, two sets of comparable clusters associated to a cluster map, mapC, also randomly defined on spaces S and S ′.\n// Randn(n,m) returns an n-by-m matrix containing pseudo-random values drawn // from a normal distribution with mean zero and standard deviation one. // Randn() returns a single value from the previous distribution. 0) Vs = 1.0; Vc = 3.0; 1) Randomly select the number of clusters ncS (resp. n c S′) in S (resp. S\n′) from the set {3, ..., 18}; 2) For each cluster ck in S (resp. c ′ l in S\n′) randomly select the number of elements in ck, |ck| (resp.c ′ l, |c ′ l|) from the set {20, .., 40}; 3) For each pair of elements (ei, ej) in S 2 (resp. S ′2) if ei and ej belong to the same cluster then SS(ei, ej) = 0.5 + VS ∗Randn(); resp. SS′(ei, ej) = 0.5 + VS ∗Randn(); else SS(ei, ej) = −0.5 + VS ∗Rand(); resp. SS′(ei, ej) = −0.5 + VS ∗Rand(); end if 4) mapC = Randn(ncS , n c S′); J = 0 for k = 1 : ncS do\nI = 0; for l = 1 : ncS′ do for i = 1 : |c′l| do\nfor j = 1 : |ck| do CS,S′(I + i, J + j) = randn() ∗ V c+mapC(l, k);\nend for end for I = I + |c′l|;\nend for J = J + |ck|;\nend for\nTable 1 gives for each similarity spaces S and S ′ the number of elements and the number of clusters for each of the 20 tests."
    }, {
      "heading" : "15 491 7 189 15 411 11 314",
      "text" : ""
    }, {
      "heading" : "12 355 5 148 15 524 12 343",
      "text" : ""
    }, {
      "heading" : "16 485 10 309 6 169 17 488",
      "text" : ""
    }, {
      "heading" : "16 516 8 243 8 223 14 420",
      "text" : "To evaluate the effectiveness of a 1-NN classification as the mixing parameter α varies, we use the classification error rate measure. The mean and the variance for this measure are estimated on the basis of the 20 tests and the 10-fold cross validation.\nFigure 2 gives the mean and variance of the error rates for the 20 tests obtained when a 10 fold cross validation is performed using a 1-NN classifier. As shown in this figure the 1-NN classification using the induced similarity measures alone performs the worse, which was expected since the variance on the comparability mapping is three times the one used to generate the native similarity measures. The error rate is thus 46%in S and in S ′ when the induced similarity measures alone are used and 35% in S and 39% in S ′ when the native similarity measures alone are used.\nThe effect of mixing native and induced similarity measures is strongly effective on these synthetic data sets since for both spaces the error rates drop below 25% and reach a minimum when α = 0.75. Note that the variance of the mean error decrease slightly when the mixing parameter α is around this optimal value 0.75. This experiment shows that even when the native and induced similarity measures are significantly noisy, the combination of the two sources of information allows for a significant reduction of the noise.\nThis is precisely this effect that we would like to show on real bilingual comparable data, when the comparability mapping is elaborated from a bilingual lexicon."
    }, {
      "heading" : "5. Variations around a quantitative comparability measure for bilingual",
      "text" : "texts\n5.1 Comparability measure by Li and Gaussier (CLG)\nThe quantitative comparability measure proposed by (Li & Gaussier, 2010) is based on the simple counting of word translation connections that exist between two corpora in different languages according to a translation lexicon. Formally, let S1 and S2 be two corpora ex-\npressed respectively in language L1 and L2. This comparability measure is formally defined as:\nCLG(S1,S2) =\n∑\nw1∈WS1∩WD1\nσ(w1) + ∑\nw2∈WS2∩WD2\nσ(w2)\n|WS1 ∩WD1|+ |WS2 ∩WD2| (3)\nwhere: WSi, i ∈ {1, 2} is the lexicon in language Li associated with the corpus Si; WDi is the set of entries for language Li into the bilingual dictionary that occur in WSi; σ(wi) is an indicator function that takes the value 1 if at least one potential translation of the term wi ∈ WSi in language Li exists in the vocabulary associated with the corpus of the other language, 0 otherwise.\nThis measure was originally designed for a bilingual lexicon extraction purposes, and not for the clustering or categorization of textual data. Hence, the authors did not incorporate any term weighting since it is a priori irrelevant for a lexicon extraction task. However, if their definition is in line with a general definition of comparability such as the one given in introduction, the lack of term weighting is questionable when addressing a clustering/categorization task. The two variants that we propose hereinafter introduce a term weighting based on the number of term occurrences to specifically adapt the measure defined by (Li & Gaussier, 2010) to clustering or categorizing tasks.\n5.2 Enrichment of the CLG measure\nThe CLG measure proposed by Li and Gaussier (eq.3) takes account of neither the number of occurrences of the lexical entries in the documents nor their number of translations into the paired documents. The binary presence or absence of joint translation entries that is modeled by the indicator function σ(wi) is a strong feature that may affect the average comparability between pairs of documents. This could be the case when addressing corpora for which frequency of lexical entries helps discriminating between genres and topics. We propose the following two similar variants of the CLG measure that explicitly propose to go beyond the presence or absence of joint translations, conjecturing that this improvement will produce a positive effect in certain situations and tasks."
    }, {
      "heading" : "5.2.1 First variant : CV A1",
      "text" : "The first variant symmetrically exploits (from the stand point of L1 and L2 languages) the following three elements: the number of occurrences of entries w taken into the vocabulary of the first language corpus, the number of their translations in the bilingual dictionary and the presence of at least one of their translations in the vocabulary of the second language corpus.\nLet A1|2, A1, A2|1, A2 be defined as follows:\nA1|2 = ∑\nw1∈WS1∩WD1\n(\ntf(w1,S1)\nτ(w1,WD1) · σ(w1)\n)\nA1 = ∑\nw1∈WS1∩WD1\n(\ntf(w1,S1)\nτ(w1,WD1)\n)\nA2|1 = ∑\nw2∈WS2∩WD2\n(\ntf(w2,S2)\nτ(w2,WD2) · σ(w2)\n)\nA2 = ∑\nw2∈WS2∩WD2\n(\ntf(w2,S2)\nτ(w2,WD2)\n)\nwhere tf(wi,Si) is the number of occurrences of entry wi in the corpus Si expressed in language Li, i ∈ {1, 2}; τ(wi,WDi) is the number of translations of entry wi of the corpus Si in the dictionary WDi; σ(wi) is defined as above.\nCV A1 = 1\n2 ·\n(\nA1|2\nA1 +\nA2|1\nA2\n)\n(4)"
    }, {
      "heading" : "5.2.2 Second variant : CV A2",
      "text" : "This second variant is very similar to the previous one. It distinguishes mainly on the way the measure is symmetrized. Basically the first variant relates to a geometric mean while the second variant relates to an arithmetic mean.\nCV A2 = A1|2 +A2|1\nA1 +A2 (5)"
    }, {
      "heading" : "6. Experimenting on textual bilingual data",
      "text" : "We have collected the assessment corpora from 21 Wikipedia categories, from English (EN) and French (FR) languages. It originally consists of 154828 documents in total with 87793 English documents and 67035 French documents categorized in 21 categories, taken from existing Wikipedia categories. Since such corpus is thematically very large, corresponding similarity and comparability matrices are basically very sparse. To avoid the algorithmic complexity behind the calculation of the induces similarity matrices (O(N3)), we proceeded as follows which drastically reduces the sparsity of our matrices:\n1. For each class and each language, we evaluate firstly the intra-language similarity matrices, using a cosine similarity based on a tf − idf weighting,\n2. secondly, we prune these intra-language similarity matrices using a threshold (typically 0.5) and order the documents according to their number of remaining neighbors (with whom they share a similarity above the threshold).\n3. by keeping for each language the best hundred documents, we get a refined comparable bilingual corpus.\n4. Finally, to complexify the experiment, we enrich this corpus by adding, for each language, and for each class, 50% of the initial number of documents. These added documents are randomly drawn from the initial 21 Wikipedia categories.\nEach Wikipedia article is then represented by its plain textual content. tags and hyperlink have thus been removed. This Wikipedia corpus1 contains 5822 documents in total, and is composed with 2745 French documents and 3077 English documents distributed into the 21 categories as listed in Table 2.\nThis corpus has been lemmatized using the TreeTagger (Schmid, 1994) (Schmid, 2009). Stoplists for French and English languages have been used and the term frequencies (tf ) for each vocabulary entry/document pair have been evaluated, as well as the inverse document frequencies idf (Spärck Jones, 1972) that were estimated on the corpus. Each Wikipedia article is finally represented by a tf-idf weighted vector according to the classical vector space model (Salton, Wong, & Yang, 1975)."
    }, {
      "heading" : "6.1 Bilingual dictionary",
      "text" : "To estimate the quantitative comparability between a pair of Enlish/French documents we have used the bilingual dictionary available at ELRA under reference ELRA-M0033. This dictionary contains 243,580 pairs of lexical entries in French and in English, which decompose into 110,541 lexical entries in English and 109,196 lexical entries in French.\nThe influence of the dictionary coverage rate has been partially studied in (Li & Gaussier, 2010) and (Ke, Marteau, & Ménier, 2014). It is shown that, for all three comparability measures CLG, CV A1 and CV A2 , the correlation of these measures with a gold standard comparability measure reference degrades when the dictionary coverage rate relatively to the corpus lexicon decreases. We do not address this issue in this paper, keeping in mind\n1. The Wikipedia corpus is available at http://people.irisa.fr/Pierre-Francois.Marteau/Corpora/Wikipedia_21classes.zip\nthat an enrichment of the bilingual dictionary by including in particular domain dependent bilingual terminology entries would likely greatly improve our results."
    }, {
      "heading" : "6.2 Evaluation measures",
      "text" : "The performance of the 1-NN classifier is evaluated using the classification error rate estimate using a 10-fold cross validation. The performance of the tested clustering algorithms are also evaluated by comparing the predicted label for each document with its true label. The accuracy (AC) and normalized mutual information (NMI) measures are used to evaluate the clustering performance (Wei Xu & Gong, 2003). As an internal evaluation scheme for estimating the quality of the clustering obtained in each linguistic space, we also use the Davies–Bouldin index (DB) (Davies & Bouldin, 1979) which roughly measures the quotient of intra and inter cluster average similarity measures.\nThe accuracy (AC) measure is defined as follows: it measures the fraction of documents that are correctly labeled, assuming a one-to-one correspondence between true categories and assigned clusters. Let p denote any possible permutation of index set of clusters and true categories. The Accuracy is thus defined as\nAC = 1\nN MAXp\n∑\ni=1···K\nni,p(i) (6)\nwhere ni,p(i) denotes the number of documents shared by class i and cluster p(i), K is the number of categories and clusters, and N is the total number of documents.\nThe NMI measure between the true clustering C and the predicted one C̃ is defined as follows:\nNMI(C̃, C) = I(C̃, C)\n(H(C̃) +H(C))/2 (7)\nwith\nI(C̃, C) = ∑\nk\n∑\nj\nP (c̃k ∩ cj) log P (c̃k ∩ cj)\nP (c̃k)P (cj)\nand\nH(C̃) = − ∑\nk\nP (c̃k) log P (c̃k)\nH(C) = − ∑\nk\nP (ck) log P (ck)\nThe Davies-Boulding index DB is a data intrinsic evaluation measure, which is defined as follows\nDB = 1\nK\nn ∑\ni=1\nmax i 6=j\n(\nσi + σj d(ci, cj)\n)\n(8)\nwhere K is the number of clusters, Ck is the centroid of cluster, σk is the average distance of all elements in cluster k to centroid ck, and d(ci, cj) is the distance between centroids i and j. The lower is this DB index value, the better is the clustering since this corresponds to low intra-cluster distances (high intra-cluster similarity) and high inter-cluster distances (low inter-cluster similarity)."
    }, {
      "heading" : "7. Experiments",
      "text" : "On the basis of the categorized comparable corpora collected from Wikipedia, we assess the benefit of mixing native similarity measures with comparability on a 1-NN classification task and on a k-medoid clustering (Kaufman & Rousseeuw, 1987) (Kaufman & Rousseeuw, 1990) task."
    }, {
      "heading" : "7.1 1-NN classification task",
      "text" : "We first study the effect of mixing similarity and comparability on the 1-NN classification task while varying the parameter α ∈ [0, 1].\nFigures 3 and 4 show that the similarity/comparability mixing has a significant impact for the two variants CV A1 and CV A2 since it allows reducing by 3% the error rate of the classification for the English language documents and 1.5% for the French language documents. However, comparatively, the CLG measure improves slightly for both languages the classification accuracy, and is less stable when α varies."
    }, {
      "heading" : "7.2 k-medoids clustering task",
      "text" : "We study here the effect of mixing comparability and similarity measures on a k-medoids clustering task for all three comparability measures. We used the previously defined AC, NMI and DB measures for the assessment of this clustering task.\nFigures 5 and 6 show that both AC and NMI measures can be improved up to 15% in the scope of the clustering of French language documents and up to 3% in the scope of the clustering of English language documents for both CV A1 and CV A2 measures. However, once again, the CLG brings comparatively less improvement for both languages.\nFigure 7 depicts the DB measure as a function of parameter α, for all three comparability measures. It is shown that, for CV A1 and CV A2 , this ratio decreases for some good α values, especially for the French language, whereas for the measure CLG, this value increases in general. A good mixing of the comparability and similarity measures has thus a positive impact when using CV A1 and CV A2 measures and a rather negative impact when using the CLG measure."
    }, {
      "heading" : "8. Analysis and conclusions",
      "text" : "In this paper, we have proposed a new approach for the co-clustering and co-categorization of bi-lingual data when a comparability mapping exists. This approach, that could be characterized as a kind of three-mode clustering or categorization, is based on the concept of similarity induced by a comparability bipartite graph. The three-mode data analysis scheme is implemented as a mixing model used to merge native and induced similarity measures inside each of the two linguistic space. The assessment of this mixing model on purely synthetic random data is quite informative and demonstrates the noise reduction capability of the method.\nOn real bilingual textual data, the approach involves a quantitative comparability measure that is based on the exploitation of a bilingual dictionary. To this end, two variants of the comparability measure proposed by (Li & Gaussier, 2010) have been proposed to adapt this measure to clustering and categorization tasks. The implementation of our model on semi-manually constructed comparable corpora collected from the Wikipedia resource shows to be quite effective. Our detailed experimentation shows that the mixing of native similarity measures with a quantitative comparability measure has a clear impact on the classification and clustering accuracies. It is noticeable that the improvement is more important in the French linguistic space comparatively to the English linguistic space. Furthermore, our approach works specifically well for the CV A1 and CV A2 comparability variant measures with stable and robust classification or clustering result improvements.\nIt nevertheless has a small positive impact when the CLG measure is used, leading to conclude that taking into account of the frequency of occurrence of lexical entries and frequencies of their translations into the comparability measure design is of crucial importance for thematic classification or clustering of bilingual English/French documents. One potential explanation is that these frequencies of occurrence pair well with the tf-idf heuristic that takes place in native cosine similarity. Moreover, according to our results, the choice of the value of the mixing parameter α is quite important. A relatively high α value (between 0.5 and 0.8), that slightly favors the induced similarity measures comparatively to the native similarity, will be a good choice in general. Finally our experimentation shows that the CV A2 , whose symmetrization is homogeneous to an arithmetic mean, is more robust than CV A1 , a result that needs to be consolidated on other independent experiments.\nIn terms of perspective, ensuring the scalability and generalizing the approach and experimentation are major prospects to help constructing thematic comparable corpora on demand.\nThe bilingual dictionary is a particularly important resource in our approach, since the quality of the comparability mapping linking the two linguistic spaces directly relies on it. The impact of the coverage of the dictionary relatively to the corpus has been partly studied in (Li & Gaussier, 2010) and (Ke et al., 2014). In the context of comparable thematic data processing, it is likely that the enrichment of a general bilingual resource by introducing domain specific terminology entries would bring some benefit.\nFinally, another perspective is to expand it to various pairing of languages for which bilingual resources are available, in particular bilingual dictionaries."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work has been partially funded by the French National Research Agency (ANRMETRICC)."
    } ],
    "references" : [ {
      "title" : "A co-classification approach to learning from multilingual corpora",
      "author" : [ "Amini", "M.-R", "C. Goutte" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Amini et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Amini et al\\.",
      "year" : 2010
    }, {
      "title" : "Corpus-based translation studies: The challenges that lie ahead",
      "author" : [ "M. Baker" ],
      "venue" : null,
      "citeRegEx" : "Baker,? \\Q1996\\E",
      "shortCiteRegEx" : "Baker",
      "year" : 1996
    }, {
      "title" : "A cluster separation measure",
      "author" : [ "D.L. Davies", "D.W. Bouldin" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions,",
      "citeRegEx" : "Davies and Bouldin,? \\Q1979\\E",
      "shortCiteRegEx" : "Davies and Bouldin",
      "year" : 1979
    }, {
      "title" : "Une nouvelle approche a l’extraction de lexiques bilingues à partir de corpus comparables. Lexicometrica, Numéro spécial, corpus alignés",
      "author" : [ "H. Déjean", "E. Gaussier" ],
      "venue" : null,
      "citeRegEx" : "Déjean and Gaussier,? \\Q2002\\E",
      "shortCiteRegEx" : "Déjean and Gaussier",
      "year" : 2002
    }, {
      "title" : "An ir approach for translating new words from nonparallel, comparable texts",
      "author" : [ "P. Fung", "L.Y. Yee" ],
      "venue" : "In Proc. of the 36th ACL meeting,",
      "citeRegEx" : "Fung and Yee,? \\Q1998\\E",
      "shortCiteRegEx" : "Fung and Yee",
      "year" : 1998
    }, {
      "title" : "From bilingual dictionaries to interlingual document representations",
      "author" : [ "J. Jagarlamudi", "III Daumé", "R. Udupa" ],
      "venue" : "In Proc. ACL-HLT - Vol. 2,",
      "citeRegEx" : "Jagarlamudi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jagarlamudi et al\\.",
      "year" : 2011
    }, {
      "title" : "Improving bilingual projections via sparse covariance matrices",
      "author" : [ "J. Jagarlamudi", "R. Udupa", "III Daumé", "A. Bhole" ],
      "venue" : "In Proc.s of the Conf. on EMNLP,",
      "citeRegEx" : "Jagarlamudi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jagarlamudi et al\\.",
      "year" : 2011
    }, {
      "title" : "Clustering by means of Medoids, in Statistical Data Analysis Based on the L1–Norm and Related Methods",
      "author" : [ "L. Kaufman", "P.J. Rousseeuw" ],
      "venue" : null,
      "citeRegEx" : "Kaufman and Rousseeuw,? \\Q1987\\E",
      "shortCiteRegEx" : "Kaufman and Rousseeuw",
      "year" : 1987
    }, {
      "title" : "Variations on quantitative comparability measures and their evaluations on synthetic French-English comparable corpora",
      "author" : [ "G. Ke", "Marteau", "P.-F", "G. Ménier" ],
      "venue" : "In LREC 2014, the 9th edition of the Language Resources and Evaluation Conference,",
      "citeRegEx" : "Ke et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ke et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving corpus comparability for bilingual lexicon extraction from comparable corpora",
      "author" : [ "B. Li", "E. Gaussier" ],
      "venue" : "In COLING,",
      "citeRegEx" : "Li and Gaussier,? \\Q2010\\E",
      "shortCiteRegEx" : "Li and Gaussier",
      "year" : 2010
    }, {
      "title" : "Clustering comparable corpora for bilingual lexicon extraction",
      "author" : [ "B. Li", "E. Gaussier", "A. Aizawa" ],
      "venue" : "In Proc. of the 49th ACL-HLT-",
      "citeRegEx" : "Li et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "Similarités induites par mesure de comparabilité : signification et utilité pour le clustering et l’alignement de textes comparables",
      "author" : [ "Marteau", "P.-F", "G. Ménier" ],
      "venue" : "In TALN,",
      "citeRegEx" : "Marteau et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Marteau et al\\.",
      "year" : 2013
    }, {
      "title" : "Mathematical Classification and Clustering",
      "author" : [ "B. Mirkin" ],
      "venue" : null,
      "citeRegEx" : "Mirkin,? \\Q1996\\E",
      "shortCiteRegEx" : "Mirkin",
      "year" : 1996
    }, {
      "title" : "Improved machine translation performance via parallel sentence extraction from comparable corpora",
      "author" : [ "D.S. Munteanu", "A. Fraser", "D. Marcu" ],
      "venue" : "In HLT-NAACL,",
      "citeRegEx" : "Munteanu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Munteanu et al\\.",
      "year" : 2004
    }, {
      "title" : "A vector space model for automatic indexing",
      "author" : [ "G. Salton", "A. Wong", "C.S. Yang" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Salton et al\\.,? \\Q1975\\E",
      "shortCiteRegEx" : "Salton et al\\.",
      "year" : 1975
    }, {
      "title" : "Probabilistic Part-of-Speech Tagging Using Decision Trees",
      "author" : [ "H. Schmid" ],
      "venue" : "In Proceedings of the Int. Conf.e on New Methods in Language Processing,",
      "citeRegEx" : "Schmid,? \\Q1994\\E",
      "shortCiteRegEx" : "Schmid",
      "year" : 1994
    }, {
      "title" : "TreeTagger, www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/",
      "author" : [ "H. Schmid" ],
      "venue" : null,
      "citeRegEx" : "Schmid,? \\Q2009\\E",
      "shortCiteRegEx" : "Schmid",
      "year" : 2009
    }, {
      "title" : "A statistical interpretation of term specificity and its application in retrieval",
      "author" : [ "K. Spärck Jones" ],
      "venue" : "Journal of Documentation,",
      "citeRegEx" : "Jones,? \\Q1972\\E",
      "shortCiteRegEx" : "Jones",
      "year" : 1972
    }, {
      "title" : "Two-mode clustering methods:a structured overview",
      "author" : [ "I Van Mechelen", "HH D.B.P. Bock" ],
      "venue" : "Statistical Methods in Medical Research,",
      "citeRegEx" : "Mechelen and Bock,? \\Q2004\\E",
      "shortCiteRegEx" : "Mechelen and Bock",
      "year" : 2004
    }, {
      "title" : "Feature-based method for document alignment in comparable news corpora",
      "author" : [ "T. Vu", "A.T. Aw", "M. Zhang" ],
      "venue" : "In Proceedings of the 12th EACL Conf.,",
      "citeRegEx" : "Vu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2009
    }, {
      "title" : "Document clustering based on non-negative matrix factorization",
      "author" : [ "X.L. Wei Xu", "Y. Gong" ],
      "venue" : "In SIGIR’03,",
      "citeRegEx" : "Xu and Gong,? \\Q2003\\E",
      "shortCiteRegEx" : "Xu and Gong",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "The notion of comparable corpora has emerged in the nineties to palliate this lack of versatility and expensiveness and to offer avenues to a wider scope of applications such as multilingual terminology extraction, multilingual information retrieval or knowledge engineering (Baker, 1996), (EAGLES, 1996).",
      "startOffset" : 275,
      "endOffset" : 288
    }, {
      "referenceID" : 12,
      "context" : "Nevertheless, there exist some work which is somehow related to this problem, like biclustering, co-clustering, or two-mode clustering introduced by (Mirkin, 1996) and (Van Mechelen I, 2004).",
      "startOffset" : 149,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "More precisely, the proposed approach is lying between the work reported in (Amini & Goutte, 2010) and (Li et al., 2011).",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "We then address the concept of comparability for mapping bilingual textual data, and define, from the original measure proposed by (Li et al., 2011), two alternative variant measures to overcome some limitation of the original measure.",
      "startOffset" : 131,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "This corpus has been lemmatized using the TreeTagger (Schmid, 1994) (Schmid, 2009).",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "This corpus has been lemmatized using the TreeTagger (Schmid, 1994) (Schmid, 2009).",
      "startOffset" : 68,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "The impact of the coverage of the dictionary relatively to the corpus has been partly studied in (Li & Gaussier, 2010) and (Ke et al., 2014).",
      "startOffset" : 123,
      "endOffset" : 140
    } ],
    "year" : 2013,
    "abstractText" : "We address in this paper the co-clustering and co-classification of bilingual data laying in two linguistic similarity spaces when a comparability measure defining a mapping between these two spaces is available. A new approach that we can characterized as a three-mode data analysis scheme, is proposed to mix the comparability measure with the two similarity measures. Our aim is to improve jointly the accuracy of classification and clustering tasks performed in each of the two linguistic spaces, as well as the quality of the final alignment of comparable clusters that can be obtained. We used first some purely synthetic random data sets to assess our formal similarity-comparability mixing model. We then propose two variants of the comparability measure that has been defined by (Li & Gaussier, 2010) in the context of bilingual lexicon extraction to adapt it to clustering or categorizing tasks. These two variant measures are subsequently used to evaluate our similarity-comparability mixing model in the context of the co-classification and co-clustering of comparable textual data sets collected from Wikipedia categories for the English and French languages. Our experiments show clear improvements in clustering and classification accuracies when mixing comparability with similarity measures, with, as expected, a higher robustness obtained when the two comparability variant measures that we propose are used. We believe that this approach is particularly well suited for the construction of thematic comparable corpora of controllable quality.",
    "creator" : "gnuplot 4.4 patchlevel 3"
  }
}