{
  "name" : "1405.3546.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "alviano@mat.unical.it)", "dodaro@mat.unical.it)", "ricca@mat.unical.it)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n35 46\nv3 [\ncs .A\nI] 1\nTo appear in Theory and Practice of Logic Programming (TPLP).\nKEYWORDS: answer set programming, query answering, cautious reasoning, anytime algorithms"
    }, {
      "heading" : "1 Introduction",
      "text" : "Answer Set Programming (ASP) is a declarative language for knowledge representation and reasoning (Niemelä 1999; Marek and Truszczyński 1999; Lifschitz 2002; Baral 2003; Gelfond and Kahl 2014). In ASP, knowledge concerning an application domain is encoded by a logic program whose semantics is given by a set of stable models (Gelfond and Lifschitz 1991), also referred to as answer sets. As for other nonmonotonic formalisms, the resulting knowledge base can be queried according to two possible modes of reasoning, usually referred to as brave (or credulous) and cautious (or skeptical). Brave reasoning provides answers to the input query that are witnessed by some stable model of the knowledge base. For cautious reasoning, instead, answers have to be witnessed by all stable models. Cautious reasoning over ASP knowledge bases has relevant applications in various fields ranging from Databases to Artificial Intelligence. Among them are consistent query answering (Arenas et al. 2003), data integration (Eiter 2005), and ontologybased reasoning (Eiter et al. 2008).\nA common practice in ASP is to reduce query answering to the computation of a subset of the cautious consequences of a logic program (Leone et al. 2006), where cautious consequences are atoms belonging to all stable models. As an example, in the context of Consistent Query Answering (CQA), consider an inconsistent database D where in relation R= {〈1,1,1〉 ,〈1,2,1〉 , 〈2,2,2〉 ,〈2,2,3〉 ,〈3,2,2〉 ,〈3,3,3〉} the second argument is required to functionally depend on the first. Given a query q over D, CQA amounts to computing answers of q that are true in all repairs of the original database. Roughly, a repair is a revision of the original database that is maximal and satisfies its integrity constraints. In the example, repairs can be modeled by the\nfollowing ASP rules:\nRout(X ,Y1,Z1) ← R(X ,Y1,Z1), R(X ,Y2,Z2), Y1 6= Y2, ∼Rout(X ,Y2,Z2) (1)\nRin(X ,Y,Z) ← R(X ,Y,Z), ∼Rout(X ,Y,Z) (2)\nwhere (1) detects inconsistent pairs of tuples and guesses tuples to remove in order to restore consistency, while (2) defines the repaired relation as the set of tuples that have not been removed. The first and third arguments of R can thus be retrieved by means of the following query rule:\nQ(X ,Z)← Rin(X ,Y,Z) (3)\nwhose consistent answers are tuples of the form 〈x,z〉 such that Q(x,z) belongs to all stable models. In this case the answer is {〈1,1〉 ,〈2,2〉 ,〈2,3〉}.\nCautious reasoning has been implemented by two ASP solvers, namely DLV (Leone et al. 2006) and clasp (Gebser et al. 2012a), as a variant of their stable model search algorithms. In a nutshell, DLV and clasp compute stable models of a given program by means of a two-phase process. The first phase is actually implemented by a possibly external instantiator producing a ground version of the input program. The ground program is then processed by the second phase, which actually searches for stable models. Cautious reasoning can be obtained by reiterating the stable model search step according to a specific solving strategy. The procedure implemented by DLV searches for stable models and computes their intersection, which eventually results in the set of cautious consequences of the input program. At each step of the computation, the intersection of the identified stable models represents an overestimate of the solution, which however is not provided as output by DLV. The procedure implemented by clasp is similar, but the overestimate is outputted and used to further constrain the computation. In fact, the overestimate is considered a constraint of the logic program, so that the next computed stable model is guaranteed to improve the current overestimate.\nIt is important to note that cautious reasoning is a resource demanding task, which is often not affordable to complete in reasonable time. As a matter of fact, in these cases current ASP solvers do not produce any sound cautious consequence, as also the overestimate produced by clasp can only guarantee that some atoms do not belong to the solution. It is interesting to observe that query answering is addressed differently in other logic programming languages. For example, Prolog queries having infinitely many answers are common due to the presence of uninterpreted function symbols. Prolog systems are thus designed to produce underestimates of the complete, possibly infinite solution, which actually represent sound answers to the input query. In fact, underestimates are useful in practice, especially in the cases in which waiting for termination is not affordable, and this may be the case even if termination is guaranteed. It is thus natural to ask whether underestimates can be computed also in the context of ASP.\nThe paper provides insights in this respect, showing that underestimates can actually be obtained by improving algorithms employed by ASP systems, or adapting to ASP the iterative consistency testing algorithm for computing backbones of propositional theories (Marques-Silva et al. 2010). The paper also introduces a modified version of this last algorithm that takes advantage of restarts and heuristic values for faster improvement of underestimates. An interesting aspect of the algorithms analyzed in this paper is that underestimates are produced during the computation of the complete solution. The computation can thus be stopped either when a sufficient number of cautious consequences have been produced, or when no new answer is produced after a specified amount of time. Such algorithms are referred to as anytime in the literature. The empirical comparison of these algorithms highlights that they could be combined in a parallel implementation\nto converge faster to the complete solution. Actually, a proof-of-concept implementation of the parallel approach is also presented in the paper to confirm this conjecture."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Syntax and semantics of propositional ASP programs are briefly introduced in this section. A quick overview of the main steps of stable model search is also reported in order to provide the reader with essential knowledge on a process that is used but not substantially modified by the algorithms analyzed in this paper. (For complementary introductory material on ASP see Gelfond and Lifschitz 1991; Baral 2003; Gelfond and Kahl 2014.)\nSyntax. A normal logic program consists of a set of rules of the following form:\na0 ← a1, . . . ,am,∼am+1, . . . ,∼an (4)\nwhere each ai (i= 0, . . . ,n) is a propositional atom in a fixed, countable set A , ∼ denotes negation as failure, and n ≥ m ≥ 0. For a rule r of the form (4), atom a0 is called head of r, denoted H(r); conjunction a1, . . . ,am,∼am+1, . . . ,∼an is named body of r; sets {a1, . . . ,am} and {am+1, . . . ,an} are denoted B+(r) and B−(r), respectively. A constraint is a rule of the form (4) such that a0 =⊥, where ⊥ is a fixed atom in A .\nSemantics. An interpretation I is a subset of A \\ {⊥}. I is a model of a rule r, denoted I |= r, if H(r)∈ I whenever B+(r)⊆ I and B−(r)∩ I = /0. It is a model of a program P, denoted I |= P, if it is a model of all rules in P. The definition of stable model is based on a notion of program reduct (Gelfond and Lifschitz 1991): Let P be a normal logic program, and I an interpretation. The reduct of P w.r.t. I, denoted PI , is obtained from P by deleting each rule r such that B−(r)∩ I 6= /0, and removing negated atoms in the remaining rules. An interpretation I is a stable model of P if I |= PI and there is no J ⊂ I such that J |= PI . Let SM(P) denote the set of stable models of P. If SM(P) 6= /0 then P is coherent. An atom a ∈ A is a cautious consequence of a program P if a belongs to all stable models of P. The set of cautious consequences of P is denoted CC(P).\nExample 1 The following is a (ground) program equivalent to the one reported in Section 1:\nRout(1,1,1)← ∼Rout(1,2,1); Rout(1,2,1)← ∼Rout(1,1,1); Rout(3,2,2)← ∼Rout(3,3,3); Rout(3,3,3)← ∼Rout(3,2,2); Rin(1,1,1)← ∼Rout(1,1,1); Rin(1,2,1)← ∼Rout(1,2,1); Rin(3,2,2)← ∼Rout(3,2,2); Rin(3,3,3)← ∼Rout(3,3,3); Q(1,1)← Rin(1,1,1); Q(1,1)← Rin(1,2,1); Q(3,2)← Rin(3,2,2); Q(3,3)← Rin(3,3,3); Rin(2,2,2)←; Rin(2,2,3)←; Q(2,2)←; Q(2,3)←.\nThe program above has four stable models:\n1. I ∪{Rout(1,1,1), Rout(3,2,2), Rin(1,2,1), Rin(3,3,3), Q(3,3)}; 2. I ∪{Rout(1,2,1), Rout(3,2,2), Rin(1,1,1), Rin(3,3,3), Q(3,3)}; 3. I ∪{Rout(1,1,1), Rout(3,3,3), Rin(1,2,1), Rin(3,2,2), Q(3,2)}; 4. I ∪{Rout(1,2,1), Rout(3,3,3), Rin(1,1,1), Rin(3,2,2), Q(3,2)};\nwhere I = {Rin(2,2,2), Rin(2,2,3), Q(1,1),Q(2,2), Q(2,3)} is the set of cautious consequences.\nStable model search. Given a normal ASP program P, its stable models can be computed by means of an algorithm similar to the DPLL backtracking search algorithm (Davis et al. 1962) adopted by SAT solvers. In this algorithm, atoms are associated with a truth value among true,\nfalse and undefined. Moreover, atoms are associated with a nonnegative integer called level. Actually, the backtracking search is usually preceded by simplification techniques (Eén and Biere 2005). Simplifications include polynomial time algorithms that (i) identify atoms whose truth value is deterministically implied by the input program, (ii) strengthen and remove rules, and (iii) eliminate atoms by means of rule rewriting. Then, the nondeterministic search starts choosing branching atoms according to some heuristic, propagating consequences of these choices until either a stable model is found, or a conflict is detected. The level associated with an atom a is the depth of the search tree in which a has been either chosen or determined, where atoms assigned by (i) have level 0. The propagation step is polynomial, and corresponds to unit propagation in SAT solvers. When a conflict is found, previous choices and their consequences are unrolled until consistency is restored (backjumping; Gaschnig 1979). Modern solvers analyze conflicts in order to learn constraints that are implicit in the original program and inhibit future explorations of the same (conflictual) branch of the search tree. This learning step corresponds to clause learning in SAT (Zhang et al. 2001), and is usually complemented with heuristic techniques that control the number of learned constraints, and possibly restart the computation in order to explore different branches of the search tree. Restart policies are based on specific sequences of thresholds that guarantee termination of the algorithm (Gomes et al. 1998; Luby et al. 1993)."
    }, {
      "heading" : "3 Computation of Cautious Consequences",
      "text" : "Several strategies for computing cautious consequences of a given program are reported in this section. Some of these strategies aim at solving the problem producing overestimates of the solution, which are improved and eventually result in the set of cautious consequences of the input program. Among them are the algorithms implemented by the ASP solvers DLV (Alviano et al. 2011) and clasp (Gebser et al. 2012a), respectively called enumeration of models and overestimate reduction in the following. Other strategies can in addition produce sound answers during the computation of the complete solution, thus providing underestimates also when termination is not affordable in reasonable time. One of these strategies is iterative coherence testing, an adaptation of an algorithm computing backbones of propositional formulas (Marques-Silva et al. 2010). To the best of our knowledge, no previous attempt to bring such an algorithm in ASP is reported in the literature. A variant of this algorithm, namely iterative partial coherence testing, is also introduced here. Finally, a strategy for obtaining underestimates from enumeration of models and overestimate reduction is presented, which can also be used to improve the other algorithms. More in detail, the algorithms considered here have a common skeleton, reported as Algorithm 1. They receive as input a program P and a set of atoms Q representing answer candidates of a query, and produce as output either the largest subset of Q that only contains cautious consequences of P, in case P is coherent, or ⊥ when P is incoherent. Initially, the underestimate U and the overestimate O are set to /0 and Q, respectively (line 1). A coherence test of P is then performed (lines 2–4) by calling function ComputeStableModel, which actually implements stable model search as described in Section 2. (To simplify the presentation, branching atoms are assumed to be assigned the false truth value.) The first argument of the function is a program P. The second argument is a set of learned constraints, which is initially empty. The third argument is a set C of atoms used to restrict branching atoms of level 1. The function returns either I in case a stable model I of P is found, or ⊥ otherwise. Note that ⊥ is returned not only when P is incoherent, but in general when each stable model M of P is such that C ⊆ M. Similarly, when I is returned, stable model I satisfies C 6⊆ I. When C = A , the\nAlgorithm 1: CautiousReasoning Input : a program P and a set of atoms Q Output: atoms in Q that are cautious consequences of P, or ⊥\n1 U := /0; O := Q; L := /0; 2 I := ComputeStableModel(P, L, A ); 3 if I =⊥ then 4 return ⊥;\n5 O := O∩ I; 6 while U 6= O do\n// EnumerationOfModels or other procedure\n7 return U ;\nProcedure EnumerationOfModels (A1) 1 P := P∪ Constraint(I); 2 I := ComputeStableModel(P, L, A ); 3 if I =⊥ then 4 U := O; 5 else\n6 O := O∩ I;\nProcedure OverestimateReduction (A2) 1 P := P∪ Constraint(O); 2 I := ComputeStableModel(P, L, A ); 3 if I =⊥ then 4 U := O; 5 else 6 P := P\\ Constraint(O); 7 O := O∩ I;\nProcedure IterativeCoherenceTesting (A3)\n1 a := OneOf(O\\U); 2 I := ComputeStableModel(P, L, {a}); 3 if I =⊥ then 4 U :=U ∪{a}; 5 else 6 O := O∩ I;\nProcedure IterativePartialCoherTest (A4)\n1 a := OneOf(O\\U); 2 I := ComputeUpToNextRestart(P, L, {a}); 3 if I =⊥ then 4 U :=U ∪{a}; 5 else if I 6= RESTART then 6 O := O∩ I;\nFunction ComputeStableModel∗(P: program, L: learned constraints, C: set of atoms) Global variables: the underestimate U\n1 repeat 2 U :=U ∪{a ∈ Q | L contains ⊥← ∼a}; 3 I := ComputeUpToNextRestart(P, L, OneOf(C)); 4 until I 6= RESTART ; 5 return I;\ncondition C 6⊆ I is trivially satisfied because I ⊆ A \\ {⊥} by definition of interpretation. When C = {a} for some atom a, instead, this function results in an incremental stable model search in which a is forced to be false (Eén and Sörensson 2003b).\nThe first stable model found improves the overestimate (line 5). At this point, estimates are improved according to different strategies until they are equal (line 6). EnumerationOfModels adds to P a constraint that eliminates the last stable model found (line 1). In fact, function Constraint({a1, . . . ,an}) returns a singleton of the form {⊥ ← a1, . . . ,an}. The algorithm then searches for a new stable model (line 2) to improve the overestimate (line 6). If no new stable model exists, the underestimate is set equal to the overestimate (lines 3–4), thus terminating the computation. OverestimateReduction is similar, but the constraint added is obtained from the\ncurrent overestimate (line 1). In this way, when a new stable model is found, an improvement of the overestimate is guaranteed, and the constraint can be reduced accordingly (lines 6 and 1).\nThe strategy implemented by IterativeCoherenceTesting can also improve the underestimate many times during its computation. In fact, one cautious consequence candidate is selected by calling function OneOf (line 1). This candidate is then constrained to be false and a stable model is searched (line 2). If none is found then the underestimate can be increased (lines 3–4). Otherwise, the overestimate can be improved (lines 5–6). IterativePartialCoherenceTesting is similar, but forces falsity of a candidate only up to the next restart (lines 1–2). In fact, ComputeStableModel is replaced by ComputeUpToNextRestart, a function that searches for a stable model but also terminates when a restart occurs, in which case it returns the value RESTART . In this way, the algorithm can select the most promising candidate after each restart.\nVariants of these four algorithms can be obtained by replacing function ComputeStableModel with function ComputeStableModel∗, which actually implements stable model search, but also improves the current underestimate after each restart (line 2).\nTheorem 1 Let P be a program and Q ⊆ A a set of atoms. CautiousReasoning(P,Q) terminates after finitely many steps and returns Q ∩CC(P) if P is coherent; otherwise, it returns ⊥. Moreover, U ⊆ Q∩CC(P)⊆O holds at each step of computation. The claim holds for all variants of Algorithm 1."
    }, {
      "heading" : "4 Implementation and Experiment",
      "text" : "We implemented the algorithms introduced in the previous section in order to analyze their performances. Details on the implementation, on the tested benchmarks, and on the obtained results are reported in this section."
    }, {
      "heading" : "4.1 Implementation",
      "text" : "Algorithms described in Section 3 are implemented in an experimental branch of the ASP solver WASP (Alviano et al. 2013), distributed under the Apache 2.0 license. Source codes can be downloaded from the branch queries of the public GIT repository https://github.com/alviano/wasp.git.\nWASP implements ASP solving with backjumping (Gaschnig 1979), learning (Zhang et al. 2001) and restarts (Gomes et al. 1998). More in detail, the branch of WASP used in this experiment implements support propagation via program completion (Lierler and Maratea 2004), branching heuristics and deletion strategy inspired by MiniSAT (Eén and Sörensson 2003a), and simplifications via subsumption and atom elimination techniques as described by Eén and Biere (2005). (Actually, atom elimination, called variable elimination in SAT, is not applied on atoms involved in queries.) In the following, A2, A3, A4 will denote WASP running Algorithm 1 with procedures OverestimateReduction, IterativeCoherenceTesting, and IterativePartialCoherenceTesting, respectively. A2∗, A3∗, A4∗ will instead denote the variants using procedure ComputeStableModel∗. Procedure EnumerationOfModels is not considered in the analysis since it is significantly outperformed by the other strategies in general.\nWe also implemented a proof-of-concept prototype of a parallel system, in the following referred to as multi. It consists of a master controller implemented in Python that coordinates the execution and the exchange of information of two instances of WASP. In particular, estimates and learned constraints of size at most two are exchanged. In our experiment, multi runs A2∗ and\nA4∗, but A4∗ in this case does not perform the first coherence check (lines 2–4 of Algorithm 1) in order to avoid a redundant computation. Other combinations of algorithms are possible, but not considered in our analysis. Results for multi average real time over three runs."
    }, {
      "heading" : "4.2 Benchmark settings",
      "text" : "We compared the implemented algorithms on three benchmarks, corresponding to different applications of cautious reasoning, briefly described below.\nMulti-Context Systems Querying (MCS). Multi-context systems (Brewka and Eiter 2007) are a formalism for interlinking heterogeneous knowledge bases, called contexts, using bridge rules that model the flow of information among contexts. Testcases in this benchmark are roughly those of the third ASP competition (Calimeri et al. 2014), where each context is modeled by a normal logic program under the stable model semantics. We actually made the testcases harder by requiring the computation of all pairs of the form 〈c,a〉 such that atom a is true in context c, while in the original testcases a single pair of that form was involved in the query. The benchmark contains 53 of the 73 instances submitted to the third ASP competition. We in fact excluded instances corresponding to incoherent theories, which are solved by the first coherence test in around 6 seconds on the average, and always in less than 14 seconds.\nConsistent Query Answering (CQA) is a well-known application of ASP (Arenas et al. 2003; Manna et al. 2013) described in Section 1. We considered the benchmark proposed by Kolaitis et al. (2013), and in particular query Q3 encoded according to the rewritings by Manna et al.. The benchmark contains 13 randomly-generated databases of increasing size ranging from 1000 to 7000 tuples per relation. Each relation contains around 30% of primary key violations.\nSAT Backbones (SBB). The backbone of a propositional formula ϕ is the set of literals that are true in all models of ϕ . When ϕ is a set of clauses over variables v1, . . . ,vn (n ≥ 1), satisfiability of ϕ can be modeled in ASP by rules ti ← ∼ fi and fi ← ∼ti (i = 1, . . . ,n), and introducing a constraint for each clause in ϕ . Backbone computation thus corresponds to the computation of cautious consequences of an ASP program. The benchmark contains 20 industrial instances used in the SAT Challenge 2012 (Järvisalo et al. 2012).\nThe experiment was run on a Mac Pro equipped with two 3 GHz Intel Xeon X5365 (quad core) processors, with 4 MB of L2 cache and 16 GB of RAM, running Debian Linux 7.3 (kernel ver. 3.2.0-4-amd64). Binaries were generated with the GNU C++ compiler 4.7.3-4 shipped by Debian. The parallel controller was interpreted by Python 3.3.2. Time and memory limits were set to 600 seconds and 8 GB, respectively. Performance was measured using the tool RunLim (http://fmv.jku.at/runlim/). All instances were grounded by gringo 3.0.5 (Gebser et al. 2011), whose execution time is not included in our analysis because our focus is on propositional programs. We however report that the grounding time was often less than 1 second, with a peak of around 5 seconds for the largest 10 instances of MCS."
    }, {
      "heading" : "4.3 Discussion of the results",
      "text" : "The performance of the algorithms for computing cautious consequences introduced in Section 3 can be studied from several perspectives. On the one hand, we want to know which solution performs better and in which cases. On the other hand, we are interested in analyzing the rate at which each algorithm produces sound answers.\nOverall performance. Table 1 summarizes the number of solved instances and average running times. In particular, the first two columns report the total number of instances (#) and the number of instances that are solved by all solvers (#all), respectively; the remaining columns report the number of solved instances within the time-out (sol.), the average running times on solved instances (t) and on instances solved by all algorithms (tall). Comparing the single-process approaches, we note that A4∗ solves more instances and is also the fastest on average in the instances solved by all algorithms, even if the performance of A3∗ is comparable. A4∗ and A3∗ outperform A2∗ in MCS, and are faster also in CQA. On the other hand, A2∗ performs well in SBB, solving one instance more than A3∗ and A4∗, and being faster on the average. Note that, as expected, if one considers both the number of solved instances and running time, A2, A3, and A4 perform as A2∗, A3∗, and A4∗, respectively. Concerning multi, it provides in general the best performance.\nDetailed analysis. An important feature of the algorithms analyzed in this paper is the ability to produce both sound answers and overestimates during the computation. Figure 1 reports, for each benchmark, the average percentage of (a) sound answers produced and (b) candidates reduction within the initial steps of the computation. In particular, we plot the effects of simplifications and of the first coherence test. The improvement of the overestimate reported in Figure 1(b) is significant. The first steps of the computation are able to reduce the number of candidates of at least 51% (in MCS) up to around 75% (in CQA). Simplifications are already very effective in CQA, where candidates are reduced of around 45%. It is important to note that the reduction of candidates at this stage applies to all algorithms, while sound answers are produced only by anytime algorithms. This is effective in practice, as shown in Figure 1(a). Indeed, anytime algorithms print from 40% (in SBB) to 90% (in CQA) of sound answers already after simplifications, which requires few seconds on the average. The first coherence test further improves the underestimate, which ranges from 52% (in MCS) to around 91% (in CQA). However, we observe that the first coherence test may require some time (25s on the average for SBB instances, with a peak of 193s), which motivated the starred variants. In fact, starred variants can produce underestimates at each restart, not only when a coherence test is completed. Actually, A2∗, A3∗ and A4∗ improve progressively the underestimate up to around an additional 30% before the first stable model is found, which is desirable on hard instances.\nIn order to further confirm the above observations, we analyze in detail the behavior of the algorithms after simplifications. In particular, Figure 2 plots both the number of sound answers (line below) and the number of candidate answers (line above) over time. In particular, Figure 2(b) is devoted to the starred algorithms on an instance of MCS, whereas Figure 2(a) plots the behavior of the basic algorithms on the same instance. First, we note that A3 and A4 perform similarly and outperform A2, which timed out. Notably, A2∗ can produce the underestimate (see the bottom line in Figure 2(b)) whereas A2 can only print the overestimate (there is no underestimate line for A2 in Figure 2(a)). In general, A3 and A4 are able to improve their estimates better than A2. Note that there is a point in the plots for each improvement of estimates, and lines are very dense on MCS instances. This confirms that MCS instances have a huge number of stable models that can be rapidly computed. We observed an analogous behavior for CQA. Plots for SBB instances on Figure 2(c) and Figure 2(d) have, instead, sparse lines, confirming that stable model search is harder for this benchmark. Nevertheless, the starred algorithms can rapidly pro-\nduce most of the sound answers. A deeper look at Figure 2(c) suggests that A2 is much faster than both A3 and A4 in solving this instance. In fact, it improves the overestimate faster than any other algorithm. Figure 2(e) and Figure 2(f) focus on multi and its components. In Figure 2(f), multi is faster and improves estimates better than A2∗ and A4∗. In contrast, multi is slower than A2∗ in Figure 2(e). A possible cause is the information exchange among processes that modifies the program handled by A2∗ with constraints produced by A3∗, which in this case results in an harder instance also for the A2∗ process. Note that multi has a non-deterministic behavior due to its parallel nature. Indeed, information exchanged may be different in different runs.\nMore insights on the general behavior of the algorithms in the non-deterministic part of the computation can be obtained by looking at Figure 3. In particular, Figure 3(a) reports the average percentage of sound answers produced after the simplification step, while candidates reduction is shown on Figure 3(b). A3 is not shown in the figure because it performs similarly to A3∗ in this perspective. The same holds for A4 and A4∗. We point out that all bars refer to sound answers and candidates remaining after simplifications, also for A2. As a general observation, A2 prints sound answers only at the end of the computation, while other algorithms are anytime. Consequently, A2 does not provide sound answers as soon as the other algorithms, as shown on Figure 3(a). Basically, A2 can print something in the first 10s only for easy instances, while A2∗ improves a lot in this respect. For example, A2∗ outputs around 14% of sound answers already in the first 10s of computation in MCS, while A2 produces no output. Nonetheless, A3∗ and A4∗ perform generally better than A2∗. The difference between A3∗ and A4∗ emerges only in SBB, where finding stable models is harder. In particular, by looking at Figure 3(a), A4∗ produces more sound answers than A3∗, whereas A3∗ is more effective in reducing the number of candidates on Figure 3(b). Note that A4∗ may change the candidate to test at each restart, and in our implementation it selects the one with the largest value of activity (which very roughly means the one that was involved more often in conflicts). On the other hand, A3∗ insists on the same candidate until the end of a stable model search. As a consequence, A4∗ has more chances to find inconsistent branches and, therefore, to improve the underestimate. On the contrary, A3∗ has more chances to find a stable model and, thus, to improve the overestimate. This suggests that A4∗ should be preferred, since it outputs sound answers more frequently, and also because, as discussed above, A4∗ is faster than A3∗ on the average (see Table 1). Concerning multi, we observe that it outperforms\nthe alternatives on CQA and SBB, while it is less effective in printing sound answers on the average for SBB instances. This is due to the presence of an outlier in this benchmark that spoils the average. That outlier can be explained by the already discussed non-deterministic behavior of multi. Analogous considerations can be done for the reduction of candidates on Figure 3(b).\nAnother perspective on the behavior of the various algorithms can be obtained by looking at Figure 4. Here are reported, for each benchmark, two variants of the classical cactus plot. Recall that, in a cactus plot, the x-axis reports the number of instances that are solved within the time reported on the y-axis. Here we consider variants where on the x-axis is the number of instances for which an algorithm printed 25% (resp. 100%) of sound answers within the time reported on the y-axis. We point out that anytime solvers can print 100% of sound answers before the timeout, even if termination is not reached within the allotted time. Figure 4 confirms that A2 is slower than anytime algorithms in printing sound answers. It is interesting to note that the anytime A2∗ improves sensibly A2 in all benchmarks, especially at the beginning of the computation. We also observe that A4∗ is slightly preferable to A3∗, and multi is the fastest solution. Note that the differences are more evident in the plots on the left that focus on the first 25% of sound answers. Finally, we confirm that A2∗ is the fastest single-process implementation in SBB. Nonetheless, A3∗ and A4∗ print more sound answers also in non-terminating instances.\nTo sum up, anytime algorithms are convenient in practice because they determine several sound answers already in the first steps of computation. In particular, the starred variants are preferable (A4∗ leading the group) because they provide sound answers as soon as possible and are thus effective also in non-terminating instances. Finally, the parallel system combining A4∗ with A2∗ is the best variant overall."
    }, {
      "heading" : "5 Related Work",
      "text" : "The computation of cautious consequences in ASP is a feature available in two solvers, namely DLV (Maratea et al. 2008) and clasp (Gebser et al. 2012a). The algorithm implemented by DLV is enumeration of models, while clasp implements overestimate reduction. Our implementation differs from these solvers especially with respect to the output produced during the computation of cautious consequences. In fact, DLV does not print any form of estimation during the computation, and clasp only prints overestimates. Our implementation, instead, is anytime and thus prints both underestimates and overestimates during the computation. Underestimates provide sound answers also when termination is not affordable in reasonable time, and are thus of practi-\ncal importance for hard problems. It is interesting to observe that among the strategies supported by our implementation there is A2∗, an anytime variant of the algorithm used by clasp that performed very well on two of our three benchmarks. We also note that DLV and clasp feature brave reasoning, which is not currently supported by our implementation.\nClasp, being a parallel ASP solver (Gebser et al. 2012b), also supports the parallel computation of cautious consequences by means of the overestimate reduction algorithm. Our proposal is different, as it is based on the combination of two different algorithms, namely iterative partial coherence testing and overestimate reduction, for reducing both estimates at the same time. However, we observe that our parallel implementation is a proof-of-concept prototype obtained by combining two instances of WASP (properly modified to share short learned constraints and answer estimates) controlled by a Python script. It is devised to show the benefits of combining two different algorithms, while a more efficient implementation is subject of future work.\nThe computation of cautious consequences of a ground program is related to the problem of backbone computations of propositional formulas (Marques-Silva et al. 2010; Slaney and Walsh 2001). In fact, the backbone of a propositional formula ϕ is the set of literals that are true in all models of ϕ . Several algorithms for computing backbones of propositional formulas are based on variants of the iterative consistency testing algorithm (Marques-Silva et al. 2010; Janota et al. 2014), which essentially corresponds to the iterative coherence testing algorithm analyzed in this paper. Backbone search algorithms usually feature additional techniques for removing candidates to be tested, such as implicant reduction and core-based chunking (Ravi and Somenzi 2004). Most of the implicant reduction techniques are not applicable to normal ASP programs because of the intrinsic minimality of stable models. For example, backbone search algorithms can reduce their overestimate by removing all unassigned variables when a (partial) model is found; in our setting, ASP solvers always terminate with a complete assignment. Core-based chunking, instead, requires a portfolio of algorithms (Janota et al. 2014) in order to be effective, which is beyond the scope of this paper.\nNote that all considered algorithms work on ground programs. Combinations with query optimization techniques such as magic sets (Greco 2003; Alviano and Faber 2011) are possible but not the focus of the paper."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Several algorithms for computing cautious consequences of ASP programs were analyzed in this paper. At the time of this writing, ASP solvers do not implement anytime algorithms, which means that computation must terminate in order to obtain some cautious consequences. On the other hand, the computation of cautious consequences is similar to the computation of backbones of propositional theories, for which anytime algorithms do exist. We adapted one of these algorithms to cautious reasoning, showing that underestimates can be effectively obtained in reasonable time also for hard instances. Moreover, we introduced a general strategy to obtain anytime variants of existing algorithms such as those implemented by DLV and clasp. All algorithms as well as a proof-of-concept parallel implementation were implemented in the solver WASP. Our empirical evaluation highlights that sound answers are computable within the first seconds of computation in many cases. Moreover, the performance of the parallel system is encouraging and leaves space for future work on this subject. Acknowledgements. This research has been partly supported by Regione Calabria under the EU Social Fund and project PIA KnowRex POR FESR 2007- 2013, by the Italian Ministry of\nUniversity and Research under PON project “Ba2Know S.I.-LAB” n. PON03PE 0001, and by National Group for Scientific Computation (GNCS-INDAM)."
    }, {
      "heading" : "LEONE, N., PFEIFER, G., FABER, W., EITER, T., GOTTLOB, G., PERRI, S., AND SCARCELLO, F. 2006.",
      "text" : "The DLV system for knowledge representation and reasoning. ACM Transactions on Computational Logic 7, 3 (July), 499–562. LIERLER, Y. AND MARATEA, M. 2004. Cmodels-2: SAT-based answer set solver enhanced to nontight programs. In Proceedings of the 7th International Conference on Logic Programming and NonMonotonic Reasoning (LPNMR-7), V. Lifschitz and I. Niemelä, Eds. LNAI, vol. 2923. Springer, 346– 350. LIFSCHITZ, V. 2002. Answer set programming and plan generation. Artificial Intelligence 138, 39–54. LIFSCHITZ, V. AND TURNER, H. 1994. Splitting a logic program. In Proceedings of the 11th International Conference on Logic Programming (ICLP’94), P. Van Hentenryck, Ed. MIT Press, Santa Margherita Ligure, Italy, 23–37. LUBY, M., SINCLAIR, A., AND ZUCKERMAN, D. 1993. Optimal speedup of Las Vegas algorithms. Inf. Process. Lett. 47, 173–180. MANNA, M., RICCA, F., AND TERRACINA, G. 2013. Consistent query answering via ASP from different perspectives: Theory and practice. Theory and Practice of Logic Programming 13, 2, 227–252. MARATEA, M., RICCA, F., FABER, W., AND LEONE, N. 2008. Look-back techniques and heuristics in dlv: Implementation, evaluation, and comparison to qbf solvers. J. Algorithms 63, 1-3, 70–89. MAREK, V. W. AND TRUSZCZYŃSKI, M. 1999. Stable models and an alternative logic programming paradigm. In The Logic Programming Paradigm – A 25-Year Perspective, K. R. Apt, V. W. Marek, M. Truszczyński, and D. S. Warren, Eds. Springer Verlag, 375–398. MARQUES-SILVA, J., JANOTA, M., AND LYNCE, I. 2010. On computing backbones of propositional theories. In ECAI, H. Coelho, R. Studer, and M. Wooldridge, Eds. Frontiers in Artificial Intelligence and Applications, vol. 215. IOS Press, 15–20. NIEMELÄ, I. 1999. Logic programming with stable model semantics as constraint programming paradigm. Annals of Mathematics and Artificial Intelligence 25, 3–4, 241–273. RAVI, K. AND SOMENZI, F. 2004. Minimal assignments for bounded model checking. In TACAS, K. Jensen and A. Podelski, Eds. LNCS, vol. 2988. Springer, 31–45. SLANEY, J. K. AND WALSH, T. 2001. Backbones in optimization and approximation. In IJCAI, B. Nebel, Ed. Morgan Kaufmann, 254–259. ZHANG, L., MADIGAN, C. F., MOSKEWICZ, M. W., AND MALIK, S. 2001. Efficient conflict driven learning in boolean satisfiability solver. In Proceedings of the International Conference on ComputerAided Design (ICCAD 2001). 279–285."
    }, {
      "heading" : "Appendix A Proof of Theorem 1",
      "text" : "The proof is split into several lemmas using Pi,Li,Ui,Oi, Ii to denote the content of variables P,L,U,O, I at step i of computation (i≥ 0). More in detail, in Lemma 1 we will first show that underestimates form an increasing sequence and, on the contrary, overestimates form a decreasing sequence. Then, in Lemma 2 we will prove properties of stable models of programs Pi∪Li (i≥ 0). Correctness of estimates will be shown in Lemmas 3–4, and termination of the algorithms in Lemma 5. Finally, in Lemma 6 we will extend the proof to variants using ComputeStableModel∗.\nLemma 1 Ui ⊆Ui+1 and Oi+1 ⊆ Oi ⊆ Q for each i ≥ 0."
    }, {
      "heading" : "Proof",
      "text" : "VariableU is initially empty. EnumerationOfModels and OverestimateReduction reassign U only once. IterativeCoherenceTesting and UnderestimateReduction always enlarge the set stored in U by means of set union (line 4). Concerning variable O, it is initially equal to Q and restricted at each reassignment by means of set intersection (line 7 for OverestimateReduction; line 6 for the other procedures).\nLemma 2 SM(Pi+1 ∪Li+1) ⊆ SM(Pi ∪Li) for each i ≥ 0. For IterativeCoherenceTesting and IterativePartialCoherenceTesting we also have SM(Pi+1 ∪Li+1) = SM(Pi ∪Li) for each i ≥ 0."
    }, {
      "heading" : "Proof",
      "text" : "Variable P is reassigned only by EnumerationOfModels and OverestimateReduction, where constraints are added to the previous program. Constraints can only remove stable models (as a consequence of the Splitting Set Theorem by Lifschitz and Turner 1994). On the other hand, learned constraints stored in variable L are implicit in the program stored by variable P, and thus cannot change its semantics.\nLemma 3"
    }, {
      "heading" : "Oi ⊇ Q∩CC(P) for each i ≥ 0.",
      "text" : ""
    }, {
      "heading" : "Proof",
      "text" : "The base case is true because O0 = Q. Assume the claim is true for some i ≥ 0 and consider Oi+1 = Oi ∩ Ii+1, where Ii+1 ∈ SM(Pi ∪ Li). By i applications of Lemma 2, we obtain Ii+1 ∈ SM(P0 ∪L0), i.e., Ii+1 ∈ SM(P). We can thus conclude a ∈ Oi \\Oi+1 implies a /∈CC(P), and we are done.\nLemma 4"
    }, {
      "heading" : "Ui ⊆ Q∩CC(P) for each i ≥ 0.",
      "text" : ""
    }, {
      "heading" : "Proof",
      "text" : "The base case is true because U0 = /0. Assume the claim is true for some i ≥ 0 and consider Ui+1. If Ui+1 =Ui then the claim is true. Otherwise, we distinguish two cases.\nFor IterativeCoherenceTesting and IterativePartialCoherenceTesting,Ui+1 =Ui∪{a} for some a ∈ Oi \\Ui. Moreover, there is no M ∈ SM(Pi ∪ Li) such that a /∈ M because Ii+1 = ⊥. From Lemma 2, we can conclude that there is no M ∈ SM(P) such that a /∈ M, i.e., a ∈ CC(P). Since a ∈ Oi \\Ui, we have a ∈ Oi and thus a ∈ Q by Lemma 1. Therefore, a ∈ Q∩CC(P) and we are done.\nFor EnumerationOfModels and OverestimateReduction, Ui+1 = Oi and the algorithm terminates. Exactly i + 1 constraints were added to P, one for each stable model of P found, i.e., I1, . . . , Ii. Moreover, Ii+1 =⊥ holds. Assume by contradiction that there is a ∈Oi \\CC(P). Hence, there is M ∈ SM(P) such that a /∈ M. Moreover, a ∈ I j ( j = 1, . . . , i) and thus M is a model of all constraints added at line 1. Consequently, M is a stable model of Pi ∪Li, which contradicts Ii+1 =⊥.\nLemma 5 Algorithm 1 terminates after finitely many steps."
    }, {
      "heading" : "Proof",
      "text" : "When EnumerationOfModels is used, termination is guaranteed because P has a finite number of stable models. OverestimateReduction either sets U equal to O, or reduces O, which initially is equal to Q, a finite set. IterativeCoherenceTesting either increases U , or reduces O, and thus terminates because O is finite and Ui ⊆ Oi holds for each i ≥ 0 by Lemmas 3 and 4. Termination of IterativePartialCoherenceTesting is guaranteed if restarts are properly delayed during the computation, as it must be done already for guaranteeing termination of stable model search.\nLemma 6 Underestimates produced by ComputeStableModel∗ are sound."
    }, {
      "heading" : "Proof",
      "text" : "Follows by the fact that L contains constraints that are implicit in the program stored by variable P."
    } ],
    "references" : [ {
      "title" : "WASP: A native ASP solver based on constraint learning",
      "author" : [ "M. ALVIANO", "C. DODARO", "W. FABER", "N. LEONE", "F. RICCA" ],
      "venue" : "LPNMR, P. Cabalar and T. C. Son, Eds. LNCS, vol. 8148. Springer, 54–66.",
      "citeRegEx" : "ALVIANO et al\\.,? 2013",
      "shortCiteRegEx" : "ALVIANO et al\\.",
      "year" : 2013
    }, {
      "title" : "Dynamic magic sets and super-coherent answer set programs",
      "author" : [ "M. ALVIANO", "W. FABER" ],
      "venue" : "AI Communications. IOS Press 24, 2, 125–145.",
      "citeRegEx" : "ALVIANO and FABER,? 2011",
      "shortCiteRegEx" : "ALVIANO and FABER",
      "year" : 2011
    }, {
      "title" : "The disjunctive datalog system DLV",
      "author" : [ "M. ALVIANO", "W. FABER", "N. LEONE", "S. PERRI", "G. PFEIFER", "G. TERRACINA" ],
      "venue" : "Datalog 2.0, G. Gottlob, Ed. Vol. 6702. Springer Berlin/Heidelberg, 282–301.",
      "citeRegEx" : "ALVIANO et al\\.,? 2011",
      "shortCiteRegEx" : "ALVIANO et al\\.",
      "year" : 2011
    }, {
      "title" : "Answer sets for consistent query answering in inconsistent databases",
      "author" : [ "M. ARENAS", "L.E. BERTOSSI", "J. CHOMICKI" ],
      "venue" : "Theory and Practice of Logic Programming 3, 4-5, 393–424.",
      "citeRegEx" : "ARENAS et al\\.,? 2003",
      "shortCiteRegEx" : "ARENAS et al\\.",
      "year" : 2003
    }, {
      "title" : "Knowledge Representation, Reasoning and Declarative Problem Solving",
      "author" : [ "C. BARAL" ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "BARAL,? 2003",
      "shortCiteRegEx" : "BARAL",
      "year" : 2003
    }, {
      "title" : "Equilibria in heterogeneous nonmonotonic multi-context systems",
      "author" : [ "G. BREWKA", "T. EITER" ],
      "venue" : "AAAI. AAAI Press, 385–390.",
      "citeRegEx" : "BREWKA and EITER,? 2007",
      "shortCiteRegEx" : "BREWKA and EITER",
      "year" : 2007
    }, {
      "title" : "The third open answer set programming competition",
      "author" : [ "F. CALIMERI", "G. IANNI", "F. RICCA" ],
      "venue" : "Theory and Practice of Logic Programming 14, 1, 117–135.",
      "citeRegEx" : "CALIMERI et al\\.,? 2014",
      "shortCiteRegEx" : "CALIMERI et al\\.",
      "year" : 2014
    }, {
      "title" : "A machine program for theorem proving",
      "author" : [ "M. DAVIS", "G. LOGEMANN", "D. LOVELAND" ],
      "venue" : "Commun. ACM 5, 394–397.",
      "citeRegEx" : "DAVIS et al\\.,? 1962",
      "shortCiteRegEx" : "DAVIS et al\\.",
      "year" : 1962
    }, {
      "title" : "Effective preprocessing in SAT through variable and clause elimination",
      "author" : [ "N. EÉN", "A. BIERE" ],
      "venue" : "SAT. LNCS, vol. 3569. Springer, 61–75.",
      "citeRegEx" : "EÉN and BIERE,? 2005",
      "shortCiteRegEx" : "EÉN and BIERE",
      "year" : 2005
    }, {
      "title" : "An extensible SAT-solver",
      "author" : [ "N. EÉN", "N. SÖRENSSON" ],
      "venue" : "SAT, E. Giunchiglia and A. Tacchella, Eds. LNCS, vol. 2919. Springer, 502–518.",
      "citeRegEx" : "EÉN and SÖRENSSON,? 2003a",
      "shortCiteRegEx" : "EÉN and SÖRENSSON",
      "year" : 2003
    }, {
      "title" : "Temporal induction by incremental sat solving",
      "author" : [ "N. EÉN", "N. SÖRENSSON" ],
      "venue" : "Electr. Notes Theor. Comput. Sci. 89, 4, 543–560.",
      "citeRegEx" : "EÉN and SÖRENSSON,? 2003b",
      "shortCiteRegEx" : "EÉN and SÖRENSSON",
      "year" : 2003
    }, {
      "title" : "Data integration and answer set programming",
      "author" : [ "T. EITER" ],
      "venue" : "LPNMR, C. Baral, G. Greco, N. Leone, and G. Terracina, Eds. LNCS, vol. 3662. Springer, 13–25.",
      "citeRegEx" : "EITER,? 2005",
      "shortCiteRegEx" : "EITER",
      "year" : 2005
    }, {
      "title" : "Combining answer set programming with description logics for the semantic web",
      "author" : [ "T. EITER", "G. IANNI", "T. LUKASIEWICZ", "R. SCHINDLAUER", "H. TOMPITS" ],
      "venue" : "Artif. Intell. 172, 12-13, 1495– 1539.",
      "citeRegEx" : "EITER et al\\.,? 2008",
      "shortCiteRegEx" : "EITER et al\\.",
      "year" : 2008
    }, {
      "title" : "Performance measurement and analysis of certain search algorithms",
      "author" : [ "J. GASCHNIG" ],
      "venue" : "Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA, USA. Technical Report CMU-CS-79-124.",
      "citeRegEx" : "GASCHNIG,? 1979",
      "shortCiteRegEx" : "GASCHNIG",
      "year" : 1979
    }, {
      "title" : "Advances in gringo series 3",
      "author" : [ "M. GEBSER", "R. KAMINSKI", "A. KÖNIG", "T. SCHAUB" ],
      "venue" : "LPNMR, J. P. Delgrande and W. Faber, Eds. LNCS, vol. 6645. Springer, 345–351.",
      "citeRegEx" : "GEBSER et al\\.,? 2011",
      "shortCiteRegEx" : "GEBSER et al\\.",
      "year" : 2011
    }, {
      "title" : "Conflict-driven answer set solving: From theory to practice",
      "author" : [ "M. GEBSER", "B. KAUFMANN", "T. SCHAUB" ],
      "venue" : "Artif. Intell. 187, 52–89.",
      "citeRegEx" : "GEBSER et al\\.,? 2012a",
      "shortCiteRegEx" : "GEBSER et al\\.",
      "year" : 2012
    }, {
      "title" : "Multi-threaded ASP solving with clasp",
      "author" : [ "M. GEBSER", "B. KAUFMANN", "T. SCHAUB" ],
      "venue" : "Theory and Practice of Logic Programming 12, 4-5, 525–545.",
      "citeRegEx" : "GEBSER et al\\.,? 2012b",
      "shortCiteRegEx" : "GEBSER et al\\.",
      "year" : 2012
    }, {
      "title" : "Knowledge Representation, Reasoning, and the Design of Intelligent Agents: The Answer-Set Programming Approach",
      "author" : [ "M. GELFOND", "Y. KAHL" ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "GELFOND and KAHL,? 2014",
      "shortCiteRegEx" : "GELFOND and KAHL",
      "year" : 2014
    }, {
      "title" : "Classical negation in logic programs and disjunctive databases",
      "author" : [ "M. GELFOND", "V. LIFSCHITZ" ],
      "venue" : "New Generation Computing 9, 365–385.",
      "citeRegEx" : "GELFOND and LIFSCHITZ,? 1991",
      "shortCiteRegEx" : "GELFOND and LIFSCHITZ",
      "year" : 1991
    }, {
      "title" : "Boosting combinatorial search through randomization",
      "author" : [ "C.P. GOMES", "B. SELMAN", "H.A. KAUTZ" ],
      "venue" : "Proceedings of AAAI/IAAI 1998. AAAI Press, 431–437.",
      "citeRegEx" : "GOMES et al\\.,? 1998",
      "shortCiteRegEx" : "GOMES et al\\.",
      "year" : 1998
    }, {
      "title" : "Binding propagation techniques for the optimization of bound disjunctive queries",
      "author" : [ "S. GRECO" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering 15, 2 (March/April), 368–385.",
      "citeRegEx" : "GRECO,? 2003",
      "shortCiteRegEx" : "GRECO",
      "year" : 2003
    }, {
      "title" : "Algorithms for computing backbones of propositional formulae",
      "author" : [ "M. JANOTA", "I. LYNCE", "J. MARQUES-SILVA" ],
      "venue" : "AI Commun.. To appear.",
      "citeRegEx" : "JANOTA et al\\.,? 2014",
      "shortCiteRegEx" : "JANOTA et al\\.",
      "year" : 2014
    }, {
      "title" : "The international SAT solver competitions",
      "author" : [ "M. JÄRVISALO", "D.L. BERRE", "O. ROUSSEL", "L. SIMON" ],
      "venue" : "AI Magazine 33, 1.",
      "citeRegEx" : "JÄRVISALO et al\\.,? 2012",
      "shortCiteRegEx" : "JÄRVISALO et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient querying of inconsistent databases with binary integer programming",
      "author" : [ "P.G. KOLAITIS", "E. PEMA", "TAN", "W.-C." ],
      "venue" : "PVLDB 6, 6, 397–408.",
      "citeRegEx" : "KOLAITIS et al\\.,? 2013",
      "shortCiteRegEx" : "KOLAITIS et al\\.",
      "year" : 2013
    }, {
      "title" : "The DLV system for knowledge representation and reasoning",
      "author" : [ "N. LEONE", "G. PFEIFER", "W. FABER", "T. EITER", "G. GOTTLOB", "S. PERRI", "F. SCARCELLO" ],
      "venue" : "ACM Transactions on Computational Logic 7, 3 (July), 499–562.",
      "citeRegEx" : "LEONE et al\\.,? 2006",
      "shortCiteRegEx" : "LEONE et al\\.",
      "year" : 2006
    }, {
      "title" : "Cmodels-2: SAT-based answer set solver enhanced to nontight programs",
      "author" : [ "Y. LIERLER", "M. MARATEA" ],
      "venue" : "Proceedings of the 7th International Conference on Logic Programming and NonMonotonic Reasoning (LPNMR-7), V. Lifschitz and I. Niemelä, Eds. LNAI, vol. 2923. Springer, 346– 350.",
      "citeRegEx" : "LIERLER and MARATEA,? 2004",
      "shortCiteRegEx" : "LIERLER and MARATEA",
      "year" : 2004
    }, {
      "title" : "Answer set programming and plan generation",
      "author" : [ "V. LIFSCHITZ" ],
      "venue" : "Artificial Intelligence 138, 39–54.",
      "citeRegEx" : "LIFSCHITZ,? 2002",
      "shortCiteRegEx" : "LIFSCHITZ",
      "year" : 2002
    }, {
      "title" : "Splitting a logic program",
      "author" : [ "V. LIFSCHITZ", "H. TURNER" ],
      "venue" : "Proceedings of the 11th International Conference on Logic Programming (ICLP’94), P. Van Hentenryck, Ed. MIT Press, Santa Margherita Ligure, Italy, 23–37.",
      "citeRegEx" : "LIFSCHITZ and TURNER,? 1994",
      "shortCiteRegEx" : "LIFSCHITZ and TURNER",
      "year" : 1994
    }, {
      "title" : "Optimal speedup of Las Vegas algorithms",
      "author" : [ "M. LUBY", "A. SINCLAIR", "D. ZUCKERMAN" ],
      "venue" : "Inf. Process. Lett. 47, 173–180.",
      "citeRegEx" : "LUBY et al\\.,? 1993",
      "shortCiteRegEx" : "LUBY et al\\.",
      "year" : 1993
    }, {
      "title" : "Consistent query answering via ASP from different perspectives: Theory and practice",
      "author" : [ "M. MANNA", "F. RICCA", "G. TERRACINA" ],
      "venue" : "Theory and Practice of Logic Programming 13, 2, 227–252.",
      "citeRegEx" : "MANNA et al\\.,? 2013",
      "shortCiteRegEx" : "MANNA et al\\.",
      "year" : 2013
    }, {
      "title" : "Look-back techniques and heuristics in dlv: Implementation, evaluation, and comparison to qbf solvers",
      "author" : [ "M. MARATEA", "F. RICCA", "W. FABER", "N. LEONE" ],
      "venue" : "J. Algorithms 63, 1-3, 70–89.",
      "citeRegEx" : "MARATEA et al\\.,? 2008",
      "shortCiteRegEx" : "MARATEA et al\\.",
      "year" : 2008
    }, {
      "title" : "Stable models and an alternative logic programming paradigm",
      "author" : [ "V.W. MAREK", "M. TRUSZCZYŃSKI" ],
      "venue" : "The Logic Programming Paradigm – A 25-Year Perspective, K. R. Apt, V. W. Marek, M. Truszczyński, and D. S. Warren, Eds. Springer Verlag, 375–398.",
      "citeRegEx" : "MAREK and TRUSZCZYŃSKI,? 1999",
      "shortCiteRegEx" : "MAREK and TRUSZCZYŃSKI",
      "year" : 1999
    }, {
      "title" : "On computing backbones of propositional theories",
      "author" : [ "J. MARQUES-SILVA", "M. JANOTA", "I. LYNCE" ],
      "venue" : "ECAI, H. Coelho, R. Studer, and M. Wooldridge, Eds. Frontiers in Artificial Intelligence and Applications, vol. 215. IOS Press, 15–20.",
      "citeRegEx" : "MARQUES.SILVA et al\\.,? 2010",
      "shortCiteRegEx" : "MARQUES.SILVA et al\\.",
      "year" : 2010
    }, {
      "title" : "Logic programming with stable model semantics as constraint programming paradigm",
      "author" : [ "I. NIEMELÄ" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence 25, 3–4, 241–273.",
      "citeRegEx" : "NIEMELÄ,? 1999",
      "shortCiteRegEx" : "NIEMELÄ",
      "year" : 1999
    }, {
      "title" : "Minimal assignments for bounded model checking",
      "author" : [ "K. RAVI", "F. SOMENZI" ],
      "venue" : "TACAS, K. Jensen and A. Podelski, Eds. LNCS, vol. 2988. Springer, 31–45.",
      "citeRegEx" : "RAVI and SOMENZI,? 2004",
      "shortCiteRegEx" : "RAVI and SOMENZI",
      "year" : 2004
    }, {
      "title" : "Backbones in optimization and approximation",
      "author" : [ "J.K. SLANEY", "T. WALSH" ],
      "venue" : "IJCAI, B. Nebel, Ed. Morgan Kaufmann, 254–259.",
      "citeRegEx" : "SLANEY and WALSH,? 2001",
      "shortCiteRegEx" : "SLANEY and WALSH",
      "year" : 2001
    }, {
      "title" : "Efficient conflict driven learning in boolean satisfiability solver",
      "author" : [ "L. ZHANG", "C.F. MADIGAN", "M.W. MOSKEWICZ", "S. MALIK" ],
      "venue" : "Proceedings of the International Conference on ComputerAided Design (ICCAD 2001). 279–285.",
      "citeRegEx" : "ZHANG et al\\.,? 2001",
      "shortCiteRegEx" : "ZHANG et al\\.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Answer Set Programming (ASP) is a declarative language for knowledge representation and reasoning (Niemelä 1999; Marek and Truszczyński 1999; Lifschitz 2002; Baral 2003; Gelfond and Kahl 2014).",
      "startOffset" : 98,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "Answer Set Programming (ASP) is a declarative language for knowledge representation and reasoning (Niemelä 1999; Marek and Truszczyński 1999; Lifschitz 2002; Baral 2003; Gelfond and Kahl 2014).",
      "startOffset" : 98,
      "endOffset" : 192
    }, {
      "referenceID" : 17,
      "context" : "Answer Set Programming (ASP) is a declarative language for knowledge representation and reasoning (Niemelä 1999; Marek and Truszczyński 1999; Lifschitz 2002; Baral 2003; Gelfond and Kahl 2014).",
      "startOffset" : 98,
      "endOffset" : 192
    }, {
      "referenceID" : 18,
      "context" : "In ASP, knowledge concerning an application domain is encoded by a logic program whose semantics is given by a set of stable models (Gelfond and Lifschitz 1991), also referred to as answer sets.",
      "startOffset" : 132,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "Among them are consistent query answering (Arenas et al. 2003), data integration (Eiter 2005), and ontologybased reasoning (Eiter et al.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "2003), data integration (Eiter 2005), and ontologybased reasoning (Eiter et al.",
      "startOffset" : 24,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "2003), data integration (Eiter 2005), and ontologybased reasoning (Eiter et al. 2008).",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "A common practice in ASP is to reduce query answering to the computation of a subset of the cautious consequences of a logic program (Leone et al. 2006), where cautious consequences are atoms belonging to all stable models.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 24,
      "context" : "Cautious reasoning has been implemented by two ASP solvers, namely DLV (Leone et al. 2006) and clasp (Gebser et al.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "2006) and clasp (Gebser et al. 2012a), as a variant of their stable model search algorithms.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 32,
      "context" : "The paper provides insights in this respect, showing that underestimates can actually be obtained by improving algorithms employed by ASP systems, or adapting to ASP the iterative consistency testing algorithm for computing backbones of propositional theories (Marques-Silva et al. 2010).",
      "startOffset" : 260,
      "endOffset" : 287
    }, {
      "referenceID" : 18,
      "context" : "The definition of stable model is based on a notion of program reduct (Gelfond and Lifschitz 1991): Let P be a normal logic program, and I an interpretation.",
      "startOffset" : 70,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "Given a normal ASP program P, its stable models can be computed by means of an algorithm similar to the DPLL backtracking search algorithm (Davis et al. 1962) adopted by SAT solvers.",
      "startOffset" : 139,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "When a conflict is found, previous choices and their consequences are unrolled until consistency is restored (backjumping; Gaschnig 1979).",
      "startOffset" : 109,
      "endOffset" : 137
    }, {
      "referenceID" : 36,
      "context" : "This learning step corresponds to clause learning in SAT (Zhang et al. 2001), and is usually complemented with heuristic techniques that control the number of learned constraints, and possibly restart the computation in order to explore different branches of the search tree.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : "Restart policies are based on specific sequences of thresholds that guarantee termination of the algorithm (Gomes et al. 1998; Luby et al. 1993).",
      "startOffset" : 107,
      "endOffset" : 144
    }, {
      "referenceID" : 28,
      "context" : "Restart policies are based on specific sequences of thresholds that guarantee termination of the algorithm (Gomes et al. 1998; Luby et al. 1993).",
      "startOffset" : 107,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "Among them are the algorithms implemented by the ASP solvers DLV (Alviano et al. 2011) and clasp (Gebser et al.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "2011) and clasp (Gebser et al. 2012a), respectively called enumeration of models and overestimate reduction in the following.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 32,
      "context" : "One of these strategies is iterative coherence testing, an adaptation of an algorithm computing backbones of propositional formulas (Marques-Silva et al. 2010).",
      "startOffset" : 132,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "Algorithms described in Section 3 are implemented in an experimental branch of the ASP solver WASP (Alviano et al. 2013), distributed under the Apache 2.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "WASP implements ASP solving with backjumping (Gaschnig 1979), learning (Zhang et al.",
      "startOffset" : 45,
      "endOffset" : 60
    }, {
      "referenceID" : 36,
      "context" : "WASP implements ASP solving with backjumping (Gaschnig 1979), learning (Zhang et al. 2001) and restarts (Gomes et al.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "2001) and restarts (Gomes et al. 1998).",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "More in detail, the branch of WASP used in this experiment implements support propagation via program completion (Lierler and Maratea 2004), branching heuristics and deletion strategy inspired by MiniSAT (Eén and Sörensson 2003a), and simplifications via subsumption and atom elimination techniques as described by Eén and Biere (2005).",
      "startOffset" : 113,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "Algorithms described in Section 3 are implemented in an experimental branch of the ASP solver WASP (Alviano et al. 2013), distributed under the Apache 2.0 license. Source codes can be downloaded from the branch queries of the public GIT repository https://github.com/alviano/wasp.git. WASP implements ASP solving with backjumping (Gaschnig 1979), learning (Zhang et al. 2001) and restarts (Gomes et al. 1998). More in detail, the branch of WASP used in this experiment implements support propagation via program completion (Lierler and Maratea 2004), branching heuristics and deletion strategy inspired by MiniSAT (Eén and Sörensson 2003a), and simplifications via subsumption and atom elimination techniques as described by Eén and Biere (2005). (Actually, atom elimination, called variable elimination in SAT, is not applied on atoms involved in queries.",
      "startOffset" : 100,
      "endOffset" : 746
    }, {
      "referenceID" : 5,
      "context" : "Multi-context systems (Brewka and Eiter 2007) are a formalism for interlinking heterogeneous knowledge bases, called contexts, using bridge rules that model the flow of information among contexts.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "Testcases in this benchmark are roughly those of the third ASP competition (Calimeri et al. 2014), where each context is modeled by a normal logic program under the stable model semantics.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Consistent Query Answering (CQA) is a well-known application of ASP (Arenas et al. 2003; Manna et al. 2013) described in Section 1.",
      "startOffset" : 68,
      "endOffset" : 107
    }, {
      "referenceID" : 29,
      "context" : "Consistent Query Answering (CQA) is a well-known application of ASP (Arenas et al. 2003; Manna et al. 2013) described in Section 1.",
      "startOffset" : 68,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "5 (Gebser et al. 2011), whose execution time is not included in our analysis because our focus is on propositional programs.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Consistent Query Answering (CQA) is a well-known application of ASP (Arenas et al. 2003; Manna et al. 2013) described in Section 1. We considered the benchmark proposed by Kolaitis et al. (2013), and in particular query Q3 encoded according to the rewritings by Manna et al.",
      "startOffset" : 69,
      "endOffset" : 195
    }, {
      "referenceID" : 30,
      "context" : "The computation of cautious consequences in ASP is a feature available in two solvers, namely DLV (Maratea et al. 2008) and clasp (Gebser et al.",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "2008) and clasp (Gebser et al. 2012a).",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "Clasp, being a parallel ASP solver (Gebser et al. 2012b), also supports the parallel computation of cautious consequences by means of the overestimate reduction algorithm.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 32,
      "context" : "The computation of cautious consequences of a ground program is related to the problem of backbone computations of propositional formulas (Marques-Silva et al. 2010; Slaney and Walsh 2001).",
      "startOffset" : 138,
      "endOffset" : 188
    }, {
      "referenceID" : 35,
      "context" : "The computation of cautious consequences of a ground program is related to the problem of backbone computations of propositional formulas (Marques-Silva et al. 2010; Slaney and Walsh 2001).",
      "startOffset" : 138,
      "endOffset" : 188
    }, {
      "referenceID" : 32,
      "context" : "Several algorithms for computing backbones of propositional formulas are based on variants of the iterative consistency testing algorithm (Marques-Silva et al. 2010; Janota et al. 2014), which essentially corresponds to the iterative coherence testing algorithm analyzed in this paper.",
      "startOffset" : 138,
      "endOffset" : 185
    }, {
      "referenceID" : 21,
      "context" : "Several algorithms for computing backbones of propositional formulas are based on variants of the iterative consistency testing algorithm (Marques-Silva et al. 2010; Janota et al. 2014), which essentially corresponds to the iterative coherence testing algorithm analyzed in this paper.",
      "startOffset" : 138,
      "endOffset" : 185
    }, {
      "referenceID" : 34,
      "context" : "Backbone search algorithms usually feature additional techniques for removing candidates to be tested, such as implicant reduction and core-based chunking (Ravi and Somenzi 2004).",
      "startOffset" : 155,
      "endOffset" : 178
    }, {
      "referenceID" : 21,
      "context" : "Core-based chunking, instead, requires a portfolio of algorithms (Janota et al. 2014) in order to be effective, which is beyond the scope of this paper.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "Combinations with query optimization techniques such as magic sets (Greco 2003; Alviano and Faber 2011) are possible but not the focus of the paper.",
      "startOffset" : 67,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "Combinations with query optimization techniques such as magic sets (Greco 2003; Alviano and Faber 2011) are possible but not the focus of the paper.",
      "startOffset" : 67,
      "endOffset" : 103
    } ],
    "year" : 2014,
    "abstractText" : "Query answering in Answer Set Programming (ASP) is usually solved by computing (a subset of) the cautious consequences of a logic program. This task is computationally very hard, and there are programs for which computing cautious consequences is not viable in reasonable time. However, current ASP solvers produce the (whole) set of cautious consequences only at the end of their computation. This paper reports on strategies for computing cautious consequences, also introducing anytime algorithms able to produce sound answers during the computation. To appear in Theory and Practice of Logic Programming (TPLP).",
    "creator" : "gnuplot 4.6 patchlevel 4"
  }
}