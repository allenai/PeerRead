{
  "name" : "1705.05249.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CLBlast: A Tuned OpenCL BLAS Library",
    "authors" : [ "Cedric Nugteren" ],
    "emails" : [ "mail@cedricnugteren.nl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nEfficient and fast software has become more important than ever as transistor scaling benefits are diminishing [2], affecting all types of platforms: from embedded devices to desktops and supercomputers. Most of such high performance software is built up around basic building blocks, of which the ‘Basic Linear Algebra Subroutines’ (BLAS) library is one of the most widely used. BLAS is the main dense linear algebra library, providing amongst others the GEMV (generalized matrix-vector multiplication) and GEMM routines (generalized matrix-multiplication). These routines are nowadays even more important due to their widespread use in deep learning: the most common and compute intensive layers in neural networks are the convolution layers (can be expressed as the GEMM routine) and the fully-connected layers (either GEMM or GEMV) [3], [12], [13]. Apart from its new use for deep learning, BLAS remains a pillar for many HPC application areas such as quantum chemistry and fluid dynamics, and for other domains such as machine learning in general, computer vision, and data analytics.\nMore than 30 years after the introduction of the Netlib BLAS API many highly optimized implementations are available for all kinds of purposes and platforms: ATLAS, BLIS, GotoBLAS, OpenBLAS, MKL and so on. However, for graphics processing units (GPUs) and other parallel processors there are fewer alternatives. The most well-known GPU BLAS implementation is NVIDIA’s cuBLAS. However, since it is written in CUDA, cuBLAS will not work on any non-NVIDIA hardware. Furthermore, it is closed-source. The main alternative is the open-source clBLAS library, written in OpenCL and thus supporting many platforms. However, it is originally designed for AMD GPUs and doesn’t perform well or sometimes doesn’t work at all on other devices which support OpenCL,\nsuch as NVIDIA GPUs, Intel GPUs, embedded devices (e.g. Mali, Adreno) and CPUs. Moreover, features relevant for deep learning such as half-precision support and batched operations are missing from clBLAS.\nThis paper presents CLBlast, a BLAS library written in OpenCL targeting a wide variety of devices including GPUs. It is open-source, it is written in C++11 and OpenCL, it is well tested on different platforms, and it implements the full set of BLAS routines. This paper first introduces CLBlast and subsequently discusses and evaluates its four main advantages:\n1) All kernels in CLBlast are highly parameterized and are not device-specific. That way, they can be auto-tuned for a given OpenCL device through integration of the CLTune auto-tuner [10]. This results in performance portability, which is demonstrated in this paper by showing matching or superior performance compared to clBLAS on NVIDIA, AMD, Intel and ARM hardware. 2) Thanks to integration of the CLTune auto-tuner, users can also tune CLBlast for specific problem-sizes. For example in deep learning matrices will have a particular shape depending on the configuration of a neural network layer. Using the auto-tuner, performance of CLBlast can be maximized for a specific problem. We demonstrate the benefits of problem-specific tuning experimentally. 3) In contrast to existing OpenCL BLAS libraries, CLBlast also implements half-precision routines using the 16-bit floating-point format (FP16). This reduces storage and bandwidth requirements by a factor two, but also allows for much faster and more energy efficient computations (e.g. 1.8x faster GEMM on a Skylake GT2 GPU). 4) CLBlast provides a special interface for batching BLAS routines. This can yield up to an order of magnitude better performance especially when processing many small vectors or matrices. This is of special interest for deep learning, as multiple smaller operations are typically batched."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "From a technical perspective, AMD’s clBLAS is the most closely related work: it is also a full OpenCL BLAS open-source library. However, it does not have CLBlast’s performance-portability, problem-specific tuning, FP16 support and batched routines. Furthermore, from a technical perspective it lacks proper testing on less-common devices, it has no C++ interface, it requires a newer version of OpenCL (1.2 instead of 1.1), and its OpenCL kernels are partly generated as strings and thus not easily readable nor editable. NVIDIA’s cuBLAS is also related, but it is CUDA-only and thus doesn’t run on non-NVIDIA hardware. Those two libraries (cuBLAS\nar X\niv :1\n70 5.\n05 24\n9v 1\n[ cs\n.M S]\n1 2\nM ay\n2 01\n7\nand clBLAS) are also combined together with additional autotuned kernels in the ISAAC project1. However, ISAAC is not a full BLAS library yet, supporting only a few routines so far. However, it does support input-size based tuning based on a learned model.\nFrom a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9]. In fact, the GEMM kernel in CLBlast is based on and evolved from the work by Matsumoto et al. [9]. There are also several recent publications on batched GEMM operations and more in general on optimizing GEMM for small matrices [1], [4], [7]. Related to the batched operations in CLBlast is also a comparison article for possible standard interfaces and approaches [11]."
    }, {
      "heading" : "III. THE CLBLAST LIBRARY",
      "text" : "CLBlast is an APACHE 2.0 licensed open-source2 OpenCL implementation of the BLAS API. The host code is written in C++11 and the kernel code in OpenCL C, compatible with any device supporting the OpenCL 1.1 or newer standards. There are automated build tests on Windows, macOS and Linux systems and there is continuous integration through automated correctness tests on six different devices from three different vendors. CLBlast has an active community: there are 3rd party Java bindings, 8 contributors, 28 forks, 65 resolved issues, it is being integrated into ArrayFire, and it is being used in experimental OpenCL versions of Caffe3 and Tensorflow4."
    }, {
      "heading" : "A. Library design",
      "text" : "Implementing the exact Netlib BLAS API would require internal host-to-device and device-to-host OpenCL transfers in the library. This could be detrimental for performance, especially for O(n) and O(n2) BLAS routines. That is why clBLAS and cuBLAS take pointers to device memory in the API, leaving full control over data transfers to the user. For the same reasons, CLBlast also provides this as the main interface. There is a C, C++ and Java (external JOCLBlast project) interface available. On top of this, there is also a fully compatible Netlib BLAS interface available. However this is not recommended for performance, since OpenCL datatransfers are done internally, leaving no control to the user.\nBLAS routines are divided into three levels. CLBlast implements all of these routines plus a few extra, see table I: 10 level-1 scalar, vector, and vector-vector routines, 23 level-2 matrix-vector routines, 9 level-3 matrix-matrix routines, and 6 extra BLAS-like routines. For each of these routines, CLBlast has (if possible) an implementation in 5 different precisions: half-precision FP16 (e.g. HGEMM), single-precision FP32 (e.g. SGEMM), double-precision FP64 (e.g. DGEMM), complex single-precision 2xFP32 (e.g. CGEMM), and complex double-precision 2xFP64 (e.g. ZGEMM).\nAlthough there are a total of 48 routines per precision in CLBlast, there are not that many OpenCL kernels implemented. First of all, the kernels are precision-agnostic.\n1ISAAC project: http://github.com/ptillet/isaac 2CLBlast repository: http://github.com/CNugteren/CLBlast 3Caffe with CLBlast: http://github.com/dividiti/ck-caffe 4TensorFlow with CLBlast: http://github.com/hughperkins/tensorflow-cl\nAlthough C++ templates aren’t supported in OpenCL C 1.1, we can still use a type alias and at kernel-compile-time define the type as either half, single, double precision or one of the complex data-types. Second, there are a few families of kernels which can be re-used for other routines. The kernels axpy, dot, gemv, ger and gemm span already most of the routines given some support kernels such as copying or padding vectors and matrices. For example, to implement the GBMV routine, CLBlast uses the gemv OpenCL kernel but uses a pre-processor macro to change the loading of the input from a general matrix into a banded matrix. The remainder of the kernel - with all performance-critical optimizations - can be re-used, thus avoiding code duplication. This is done for almost all routines, similarly to what is described in [9] for level-3 routines."
    }, {
      "heading" : "B. Parameterized kernels",
      "text" : "All kernel implementations in CLBlast are written highly parameterized to be tunable across devices: they are not written for a specific device or OpenCL implementation. This is achieved by creating a few pre-processor constants which can be changed without affecting the correctness of the program. The simplified example of the axpy kernel in figure 1 illustrates this: the local work-group size is tunable (WGS), the amount of work-per-thread can vary (WPT), and the vector width is configurable (VW). In contrast to clBLAS, we rely on the target compiler to perform the low-level optimizations such as loop-unrolling and pointer-arithmetic, increasing the readability, portability, and maintainability of the kernels.\nAlthough it is unfeasible to discuss all of CLBlast’s kernels and their parameters here, we believe it is important to briefly illustrate CLBlast’s generality with a larger parameterized kernel as well: the gemm kernel. Similar to [8], the CLBlast main kernel makes many assumptions on the input arguments which are handled by pre-processing and post-processing kernels (matrix sizes are a multiple of the work-group sizes, offsets are zero, matrix B is transposed, etc.). This is a good solution for larger problem sizes since O(n2) data movement is typically cheaper than O(n3) computation, but the hidden constant starts to play a role for smaller n. Therefore, there is also a singlekernel ‘direct’ version available for those cases, but it shares most of the design and parameters as discussed below.\nThe gemm kernel has 14 different parameters, of which 6 are illustrated in figure 2. The parameters define among others the work-group sizes in 2 dimensions (Mwg, Nwg), the 2D register tiling configuration (Mwi, Nwi), the vector widths of both input matrices, loop unroll factors (Kwi), and whether or not and how to use the local memory. For more details we refer to the CLTune paper which discusses an earlier version [10] and to the work of Matsumoto et al. which served as inspiration for the design of the kernel [8]."
    }, {
      "heading" : "C. Performance tuning",
      "text" : "The parameterized kernels in CLBlast can be tuned for a specific device or problem size using the CLTune library5, which is an open-source CUDA and OpenCL auto-tuner written in C++. For details on the CLTune library, we refer to [10]. CLBlast provides binaries to interface with CLTune for each kernel. By default, these tuners will find optimal kernel parameters for each kernel in CLBlast for each precision but for a fixed problem size. Tuning results for previously unseen devices are collected in a central tuning database6 from which CLBlast takes its optimized parameters. The database contains timings of each kernel execution while tuning, not just the optimal. Thus, the database can be used for other purpose as well, such as research on performance modeling or optimal parameter prediction.\nTo illustrate how the tuners in CLBlast work, consider the gemm kernel discussed earlier. Although each of the parameters has at most only 4 or 5 reasonable values to try, the total search-space explodes quickly and has more than 100.000\n5CLTune repository: http://github.com/CNugteren/CLTune 6CLBlast tuning database: http://github.com/CNugteren/CLBlast-database\npossible combinations to explore for the 14 parameters. This is even after filtering out non-valid parameter combinations due to device or software dependent restrictions (e.g. maximum work-group size, local memory size). Depending on the usecase the amount of combinations might be too much to explore. Therefore, CLBlast defines two sets of tuning parameters for such a kernel: one set with the most likely combinations (e.g. 500) and one set with all combinations. The first set is explored exhaustively, while the other set is additionally explored by random sampling in the search-space. The tuner will thus always explore the basic kernel parameter combinations and on top of that an extra user-configurable amount.\nAll kernels in CLBlast have already been tuned for 38 different devices thanks to the community, and they can be tuned for any new device. Nevertheless, the library can also perform decently on previously unseen devices: default parameters per device vendor/type are computed by taking the average best performing parameters for similar devices. For example, kernel parameters for a new unseen AMD GPU will be set to the parameters corresponding to the average best performing case across all existing AMD GPU devices in the database. Of course, there is no guarantee that performance on the new device will be good or that the parameters are legal at all. However, in case the device architecture is significantly different, the user can always still run the tuners on his/her device to make sure performance is optimized.\nThe default tuning results are only for a specific problem size to limit the total run-time of the tuners. Nevertheless, users can still tune for a specific problem size, e.g. a small rectangular matrix of size 279 by 32. The CLBlast API furthermore provides an interface to change the library’s default parameters at run-time with others. Thus, the user can also programmatically provide optimized parameters for (multiple) custom problem sizes."
    }, {
      "heading" : "IV. EXPERIMENTAL SET-UP",
      "text" : "The following sections will contain some experimental results: this section explains the set-up used in this paper. All results and graphs shown are also available on-line7, as well as other experimental results for the same and other devices. In this paper we report results of single-precision and halfprecision only. We test on the OpenCL devices listed in table II.\nAll experimental results presented in the following sections are obtained through the included CLBlast ‘clients’: binaries which compare run-time of CLBlast against other libraries. The clients perform a warm-up run first, followed by 10 repeated timed runs. The reported results are based on the average time over those runs. We report GB/s for routines which are\n7On-line appendix of this paper at: http://cnugteren.github.io/clblast/\ntypically bandwidth-bound (level-1 and level-2) and GFLOPS for routines which are typically compute-bound (level-3). We test with CLBlast release 0.11.0 (latest version as of May 2017) and compare against:\n1) AMD’s clBLAS 2.12 (latest version as of May 2017). After installing clBLAS, we run the included ‘clBLAS-tune’ to fine-tune performance. However, for some devices the tuner did not complete successfully. 2) NVIDIA’s cuBLAS, included as part of the CUDA installation (see table II for the version)."
    }, {
      "heading" : "V. PERFORMANCE ACROSS DEVICES",
      "text" : "CLBlast is performance-portable due to its tuning capabilities and the generic OpenCL kernels. However, the level of performance achieved is of course still limited by the design and flexibility of the implemented kernels. The design of CLBlast has mainly focused on the gemm kernel which is used for almost all level-3 BLAS operations.\nIn a first set of experiments in figure 3 we demonstrate performance-portability across devices. We test on 6 different devices (see table II) for 3 different types of routines: AXPY, GEMV, and GEMM. These routines are chosen for being the most representative for each BLAS level and actually include kernels covering almost all routines. We show results for both multiples of a power-of-2 and for a variety of irregular vector and matrix sizes: multiples of some odd number. For the full results including more complete experiments we refer to the on-line appendix8. We draw the following conclusions from figure 3:\n• The AXPY results are for almost all GPU cases onpar with clBLAS. This is to be expected, as it is a simple bandwidth-bound operation. CLBlast tunes the work-group size, the amount of work-per-thread, and the vector width, but the first two don’t matter too much for most devices as long as they don’t take extreme values. For the CPU experiment, CLBlast is clearly much closer to the peak memory bandwidth. • The GEMV results also show on-par or slightly better performance compared to clBLAS for most devices. However, from the more elaborate tests in the on-line appendix with other argument settings it becomes clear that CLBlast’s GEMV routine is still sub-optimal: clBLAS achieves better performance for certain cases and devices. Nevertheless, in certain cases CLBlast is much faster, such as the CPU experiment in the bottom right in figure 3. • The GEMM results for CLBlast are much more stable across input sizes compared to clBLAS. This is due to the ‘indirect’ kernel design with the additional kernels to transform data in the expected format. The main benefit of this is manifested for irregular sizes. • The GEMM results show significantly better overall results for CLBlast compared to clBLAS. This is especially the case for irregular sizes and for certain devices: Skylake ULT GT2 and the Radeon M370X. However, we note that the clBLAS library on both of these devices couldn’t be tuned due to repeated crashes. Nevertheless, other devices for which clBLAS worked as expected such\n8On-line appendix of this paper at: http://cnugteren.github.io/clblast/\nas the GeForce GTX750Ti also show results in favor of CLBlast. • Some devices seem to saturate performance towards the end of the tested vector or matrix sizes. However, the Titan X does not saturate yet: it is a much bigger GPU compared to the other tested devices. • The GEMM results for the two NVIDIA devices show sudden changes around m = n = k = 1280. This is a manually set switching point between the indirect and direct kernels for NVIDIA GPUs. Apparently it is set too high for the GeForce GTX750Ti but too low for the Titan X. Further work is required to include this non-kernel parameter in the whole tuning process."
    }, {
      "heading" : "VI. PROBLEM-SPECIFIC TUNING",
      "text" : "Thanks to the included CLTune-based tuners and the user community, CLBlast is already tuned for a wide variety of devices. However, tuning is currently only done for a default set of input arguments. For example, the GEMM routine is perdefault tuned for squared matrices of dimensions m = 1024, n = 1024, k = 1024. To get the maximum performance out of CLBlast, it is possible to tune for specific routine arguments. It is even possible to tune for multiple cases: CLBlast provides an API to change the tuning parameters at run-time. Problemspecific tuning can be beneficial for example for deep learning applications, in which matrices have a specific size based on the neural network’s layout.\nTo illustrate the benefits of problem-specific tuning we tuned the gemm kernel for 9 different matrix sizes. We then benchmarked the SGEMM routine for each of the 9 different sets of tuning parameters on the same 9 problems. The results are shown as heat-maps in figure 4. Shown are the relative performances compared to the diagonal, i.e. the case for which the parameters were tuned. There are a few cases with performance slightly higher than the diagonal (i.e. higher than 100%), which can happen because the tuner explores a random sub-set of the search space and might have been more ‘lucky’ in a particular case. Overall we see quite some potential benefit for problem-specific tuning, even up to a factor 2 for the Radeon M370X GPU. For the Skylake ULT GT2 GPU we see less benefit: tuning for the larger matrix dimensions seems to generalize towards smaller matrices (top left corner has high values), whereas the opposite is not true (bottom right corner has low values). In conclusion, benefits from problem-specific tuning vary per use-case and per device. In general, it seems worth exploring the potential to get optimal performance."
    }, {
      "heading" : "VII. HALF-PRECISION FLOATING-POINT",
      "text" : "We also demonstrate the benefit of half-precision floatingpoint (FP16) in CLBlast. This mode trades-off precision for potential memory savings, faster computation, and less energy consumption. Traditionally mostly used in computer graphics and image processing applications, FP16 has seen renewed interest with the recent successes of deep-learning. Devices with native FP16 at 2x FP32 speed can be found in the embedded and low-power domain (e.g. Intel Skylake GPUs and ARM Mali GPUs) and the very high-end (NVIDIA Tesla P100). Recent AMD GPUs (Polaris and Vega) also support FP16, but for memory and energy savings only: they run FP16 computations at FP32 speed.\nCLBlast supports all routines in half-precision mode, whereas other libraries are lagging behind the recent hardware advances and software requirements. For example, clBLAS has no FP16 support at all. Intel does have a single special-purpose OpenCL FP16 kernel integrated in the ISAAC library9, but there is no BLAS library or interface available. NVIDIA’s cuBLAS only supports the GEMM routine in half-precision.\n9See Intel FP16 GEMM at https://github.com/ptillet/isaac/pull/20\nWe have tested CLBlast’s half-precision mode on the two devices with FP16 support from table II. Since there is no FP16 support in clBLAS, we test against FP32 versions of CLBlast and clBLAS to show the advantage of FP16. The results are shown in figure 5. We draw the following conclusions from the figure:\n• On the Skylake low-power GPU we achieve up to 180 GFLOPS in half-precision GEMM. All across the board\nwe see significant speed-ups of around 1.8x over FP32 mode. • On the Mali GPU we see only limited benefits of around 30% when using FP16 mode. This is caused by the fact that the gemm kernel runs sub-optimally anyway: with 8 GFLOPS in FP32 we achieve only 22% of the peak. This is because the ARM compiler does not promote CLBlast’s small register arrays (e.g. float reg[WPT]) to actual registers by unrolling all relevant loops. This in turn causes many unnecessary memory accesses and thus explains the limited benefits from FP16: the performance bottleneck is elsewhere. A compiler performance bug is reported to ARM."
    }, {
      "heading" : "VIII. BATCHED BLAS ROUTINES",
      "text" : "This section discusses the benefits of batched routines: grouping multiple similar traditional BLAS calls into one routine for better efficiency. Although the overhead of CLBlast’s routines is minimal, performing multiple small operations comes at a cost: the OpenCL device can become underutilized when running too few threads and work-groups. For example on an NVIDIA GTX 750 Ti with 5 compute units, a 32 by 32 matrix-multiplication with a work-group size of 512 results in 2 work-groups, leaving 3 units unoccupied. Even if the work-group size would be smaller, there would be insufficient threads to hide the GPU’s memory latency. Batched BLAS routines can alleviate this issue by running unrelated but similar computations simultaneously.\nCLBlast implements two batched routines: batched AXPY and batched GEMM. Their interfaces are similar to the nonbatched counterpart, with the following exceptions:\n1) There is an additional integer parameter specifying the number of batches. 2) All offset arguments have become arrays, specifying the starting points of the individual vectors or matrices with respect to an OpenCL memory object for each of the computations within the batch. 3) The scalar arguments (alpha and beta) are now arrays such that they can be set differently per item within the batch.\nThese batched routines are specifically beneficial for machine learning applications, in which data is often processed in batches for training (stochastic gradient descent) and for inference (bulk-processing). In fact, deep-learning-specific libraries such as cuBLAS/cuDNN and GreenTea libDNN implement batched GEMM routines as well.\nFigure 6 presents results of running batched AXPY and batched GEMM on two different devices. Additional results for batched routines can be found in the on-line appendix10. We evaluate the benefit of batched routines compared to a non-batched clBLAS reference for a fixed batch-size but with varying data-size (left and middle) and for a fixed data-size but with a varying batch-size (right). From the results, we conclude that the batched routines perform significantly better overall compared to their non-batched counter-parts (see nonbatched CLBlast from figure 3). In some cases this can even be an order of magnitude better: batched routines can bring the performance of operations on small vectors and matrices to the level of much larger operations. There are exceptions however: for a small batch size there is an overhead due to loading of the offsets and scalars from global memory. Of course, the advantage of batching diminishes as input sizes grow bigger."
    }, {
      "heading" : "IX. COMPARISON WITH OTHER LIBRARIES",
      "text" : "So far we compared CLBlast only with its direct competitor clBLAS. In figure 7 we also compare performance against NVIDIA’s cuBLAS on the two NVIDIA GPUs in our test suite. It is clear that cuBLAS is still superior over both OpenCL libraries. This is especially the case for smaller sized problems: cuBLAS reached higher performance levels much quicker. The reason is two-fold. First, cuBLAS might be tuned at assembly/PTX level for specific hardware, whereas CLBlast relies on the compiler performing low-level optimizations. Second, specific instructions such as __ldg for L1 caching are available from CUDA, but not from OpenCL. In-depth analysis of CUDA vs OpenCL for GEMM can be found on-line11."
    }, {
      "heading" : "X. FUTURE WORK",
      "text" : "The current version of CLBlast is already mature and performs well on a variety of platforms. Nevertheless, we do identify some topics of future work to further improve the library:\n• The current out-of-the-box tuning parameters are optimized for specific routine arguments (e.g. matrix size). To get optimal performance for a particular use-case, the user is currently required to manually tune. However, if we would run the tuners per-default for a mixed set of arguments (e.g. both small and large matrices), we could estimate tuning parameters for every use-case. Selecting the argument mix and performing the estimation is not trivial and might require elaborate (machine-learned?) models of the kernels and the hardware. • The above idea can be applied in another dimension: predicting tuning parameters for unseen devices instead of for unseen argument combinations. Again, this might require sophisticated models of the kernels and hardware, potentially machine-learned.\n10On-line appendix of this paper at: http://cnugteren.github.io/clblast/ 11OpenCL GEMM tutorial: http://www.cedricnugteren.nl/tutorial/\n• CLBlast has two GEMM kernels: an ‘indirect’ version requiring extra pre/post-processing kernels and a ‘direct’ version. The latter is currently used for problems with small matrices, but a detailed trade-off analysis is not yet performed. Deciding between these trade-offs can become part of the per-device tuning process. • The two implemented batched routines demonstrate significant benefits over their non-batched counterparts. Plans are in place to implement batched interfaces for other routines as well."
    }, {
      "heading" : "XI. CONCLUSIONS",
      "text" : "This paper discussed CLBlast, an OpenCL BLAS library written in C++11. Thanks to its integrated auto-tuning support and the generic OpenCL kernels it performs well on a wide range of OpenCL devices, offering a viable alternative to the de-facto standard clBLAS. Users of the library can further improve performance by fine-tuning for their specific hardware or even for their specific use-case (e.g. matrix size). Furthermore, this paper demonstrated that CLBlast is equipped with features which go beyond BLAS: half-precision floating-point (FP16) and batched routines. Both are highly beneficial for deep-learning in which high precision is not always required and in which batched operations are common. With the right hardware, CLBlast’s FP16 mode can give you a factor of 2 performance gain and memory savings. Batching can improve performance up to an order of magnitude, depending on the use-case and hardware.\nIn conclusion, CLBlast is already a mature and highlyperformant library and can be used today to accelerate programs on OpenCL hardware. In the future CLBlast will continue to support the needs of the high-performance computing and deep learning communities to make the library even more tuned to the needs of the users. Readers are invited to contribute by sharing ideas for future work, tuning results or patches on http://github.com/CNugteren/CLBlast, the main project website of CLBlast."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank all the contributors on the CLBlast project. We thank dividiti for providing access to ARM Mali hardware."
    } ],
    "references" : [ {
      "title" : "Performance, Design, and Autotuning of Batched GEMM for GPUs",
      "author" : [ "A. Abdelfattah", "A. Haidar", "S. Tomov", "J. Dongarra" ],
      "venue" : "In 31st International Conference on High Performance Computing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "The Future of Microprocessors",
      "author" : [ "S. Borkar", "A.A. Chien" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "cuDNN: Efficient Primitives for Deep Learning",
      "author" : [ "S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "MAGMA Batched: A Batched BLAS Approach for Small Matrix Factorizations and Applications on GPUs",
      "author" : [ "T. Dong", "A. Haidar", "P. Luszczek", "S. Tomov", "A. Abdelfattah", "J. Dongarra" ],
      "venue" : "Technical report,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Performance Upper Bound Analysis and Optimization of SGEMM on Fermi and Kepler GPUs",
      "author" : [ "J. Lai", "A. Seznec" ],
      "venue" : "In CGO ’13: Code Generation and Optimization",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "A Note on Auto-tuning GEMM for GPUs",
      "author" : [ "Y. Li", "J. Dongarra", "S. Tomov" ],
      "venue" : "In ICCS: Int. Conf. on Computational Science. Springer,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "High-Performance Matrix-Matrix Multiplications of Very Small Matrices",
      "author" : [ "I. Masliah", "A. Abdelfattah", "A. Haidar", "S. Tomov", "M. Baboulin", "J. Falcou", "J. Dongarra" ],
      "venue" : "In Euro-Par 2016: International Conference on Parallel and Distributed Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Performance Tuning of Matrix Multiplication in OpenCL on Different GPUs and CPUs",
      "author" : [ "K. Matsumoto", "N. Nakasato", "S. Sedukhin" ],
      "venue" : "In SC Companion",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Implementing Level-3 BLAS Routines in OpenCL on Different Processing Units",
      "author" : [ "K. Matsumoto", "N. Nakasato", "S. Sedukhin" ],
      "venue" : "Technical Report TR 2014-001,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "CLTune: A Generic Auto-Tuner for OpenCL Kernels",
      "author" : [ "C. Nugteren", "V. Codreanu" ],
      "venue" : "In MCSOC ’15: International Symposium on Embedded Multicore/Many-core Systems-on-Chip,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Zounon. A Comparison of Potential Interfaces for Batched BLAS Computations",
      "author" : [ "S.D. Relton", "P. Valero-Lara" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Parallel Multi Channel Convolution using General Matrix Multiplication",
      "author" : [ "A. Vasudevan", "A. Anderson", "D. Gregg" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Why GEMM is at the heart of deep learning",
      "author" : [ "P. Warden" ],
      "venue" : "https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-ofdeep-learning/,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "INTRODUCTION Efficient and fast software has become more important than ever as transistor scaling benefits are diminishing [2], affecting all types of platforms: from embedded devices to desktops and supercomputers.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "These routines are nowadays even more important due to their widespread use in deep learning: the most common and compute intensive layers in neural networks are the convolution layers (can be expressed as the GEMM routine) and the fully-connected layers (either GEMM or GEMV) [3], [12], [13].",
      "startOffset" : 277,
      "endOffset" : 280
    }, {
      "referenceID" : 11,
      "context" : "These routines are nowadays even more important due to their widespread use in deep learning: the most common and compute intensive layers in neural networks are the convolution layers (can be expressed as the GEMM routine) and the fully-connected layers (either GEMM or GEMV) [3], [12], [13].",
      "startOffset" : 282,
      "endOffset" : 286
    }, {
      "referenceID" : 12,
      "context" : "These routines are nowadays even more important due to their widespread use in deep learning: the most common and compute intensive layers in neural networks are the convolution layers (can be expressed as the GEMM routine) and the fully-connected layers (either GEMM or GEMV) [3], [12], [13].",
      "startOffset" : 288,
      "endOffset" : 292
    }, {
      "referenceID" : 9,
      "context" : "That way, they can be auto-tuned for a given OpenCL device through integration of the CLTune auto-tuner [10].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "From a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 5,
      "context" : "From a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "From a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 8,
      "context" : "From a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : "[9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "There are also several recent publications on batched GEMM operations and more in general on optimizing GEMM for small matrices [1], [4], [7].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "There are also several recent publications on batched GEMM operations and more in general on optimizing GEMM for small matrices [1], [4], [7].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "There are also several recent publications on batched GEMM operations and more in general on optimizing GEMM for small matrices [1], [4], [7].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : "Related to the batched operations in CLBlast is also a comparison article for possible standard interfaces and approaches [11].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "This is done for almost all routines, similarly to what is described in [9] for level-3 routines.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "Similar to [8], the CLBlast main kernel makes many assumptions on the input arguments which are handled by pre-processing and post-processing kernels (matrix sizes are a multiple of the work-group sizes, offsets are zero, matrix B is transposed, etc.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "For more details we refer to the CLTune paper which discusses an earlier version [10] and to the work of Matsumoto et al.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "which served as inspiration for the design of the kernel [8].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "For details on the CLTune library, we refer to [10].",
      "startOffset" : 47,
      "endOffset" : 51
    } ],
    "year" : 2017,
    "abstractText" : "This work demonstrates how to accelerate dense linear algebra computations using CLBlast, an open-source OpenCL BLAS library providing optimized routines for a wide variety of devices. It is targeted at machine learning and HPC applications and thus provides a fast matrix-multiplication routine (GEMM) to accelerate the core of many applications (e.g. deep learning, iterative solvers, astrophysics, computational fluid dynamics, quantum chemistry). CLBlast has four main advantages over other BLAS libraries: 1) it is optimized for and tested on a large variety of OpenCL devices including less commonly used devices such as embedded and low-power GPUs, 2) it can be explicitly tuned for specific problem-sizes on specific hardware platforms, 3) it can perform operations in halfprecision floating-point FP16 saving precious bandwidth, time and energy, 4) and it can combine multiple operations in a single batched routine, accelerating smaller problems significantly. This paper describes the library and demonstrates the advantages of CLBlast experimentally for different use-cases on a wide variety of OpenCL hardware.",
    "creator" : "LaTeX with hyperref package"
  }
}