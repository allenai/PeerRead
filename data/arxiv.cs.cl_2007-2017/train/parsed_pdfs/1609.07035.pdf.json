{
  "name" : "1609.07035.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Abstractive Meeting Summarization Using Dependency Graph Fusion",
    "authors" : [ "Siddhartha Banerjee", "Prasenjit Mitra", "Kazunari Sugiyama" ],
    "emails" : [ "sub253@ist.psu.edu", "pmitra@qf.org.qa", "sugiyama@comp.nus.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Categories and Subject Descriptors I.2.7 [Artificial Intelligence]: Natural Language Processing—Language generation\nKeywords Abstractive meeting summarization; Integer linear programming"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Meeting summarization helps both participants and non-participants by providing a short and concise snapshot of the most important content discussed in the meetings. A recent study revealed that people generally prefer abstractive summaries [4]. Table 1 shows the human-written abstractive summaries along with the humangenerated extractive summaries from a meeting transcript. As can be seen, the utterances are highly noisy and contain unnecessary information. Even if an extractive summarizer can accurately classify these utterances as “important” and present them to a reader, it is hard to read and synthesize information from such utterances. In contrast, human written summaries are compact and readable.\nWe propose an automatic way of generating short and concise abstractive summaries of meetings. Any meeting conversation includes dialogues on several topics. For example, in Table 1, the participants converse on two topics: design features and selling prices. Given the most important sentences within a topic segment, our goal is to generate a one-sentence summary from each segment and appending them to form a comprehensive summary of the meeting. Moreover, we also aim to generate summaries that resemble human-written summaries in terms of writing style.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). WWW 2015 Companion, May 18–22, 2015, Florence, Italy. ACM 978-1-4503-3473-0/15/05. http://dx.doi.org/10.1145/2740908.2742751.\nTo aggregate the information from multiple utterances, we adapt an existing integer linear programming (ILP) based fusion technique [1]. The fusion technique is based on the idea of merging dependency parse trees of the utterances. The trees are merged on the common nodes that are represented by the word and parts-ofspeech (POS) combination. Each edge of the merged structure is represented as a variable in the ILP objective function, and the solution will decide whether the edge has to be preserved or discarded. We modify the technique by introducing an anaphora resolution step and also an ambiguity resolver that takes the context of words into account. Further, to solve the ILP, we introduce several constraints, such as desired length of the output, etc.\nTo the best of our knowledge, our work is the first to address the problems of readability, grammaticality and content selection jointly for meeting summary generation without employing a templatebased approach. We conduct experiments on the AMI corpus1 that consists of meeting transcripts and show that our best method outperforms extractive model significantly on ROUGE-2 scores (0.048 vs 0.026)."
    }, {
      "heading" : "2. PROPOSED APPROACH",
      "text" : "Dependency fusion on meeting data requires an algorithm that is robust for noisy data as utterances often have disfluencies. Our work applies fusion to all the important utterances within the topic segment to generate the best sub-tree that satisfies the constraints and maximizes the objective function of the optimization problem. 1 http://groups.inf.ed.ac.uk/ami/corpus/\nar X\niv :1\n60 9.\n07 03\n5v 1\n[ cs\n.C L\n] 2\n2 Se\np 20\n16\nAnaphora resolution step replaces pronouns with the original nouns in the previous utterance that they refer to in order to increase the chances of merging. Consider the following utterances:\n“so we’re designing a new remote control and um” “Um, as you can see it’s supposed to be original”\nWithout pronoun resolution, these two utterances cannot be merged. Once we apply anaphora resolution, it in the second utterance is modified to a new remote control and then both the utterances are fused into a common structure. The utterances are parsed using the Stanford dependency parser. Every individual utterance has an explicit ROOT node. We add two dummy nodes in the graph – the start node and the end node to ensure defined start and end points of the merged structure. The words from the utterances are iteratively added onto the graph. The words that have the same word form and POS tag are assigned to the same nodes. Ambiguity resolver. Suppose that a new word wi that has k ambiguous nodes where it can be mapped to. The k ambiguous nodes are referred to as mappable nodes. For every ambiguous mapping candidate, we first find the words to the left and right of the mappable node of the sentences, and then compute the number of words in both the directions that are common to the words in either direction of the word wi. Finally, wi is mapped to the node that has the highest directed context. ILP formulation. Figure 1 shows the sub-graph (marked using blue bold arrows) that we wish to retain from the merged graph structure to generate a one-sentence summary from several merged utterances. All the sentences generated from each meeting transcript are concatenated to produce the final abstractive summary. We need to maximize the information content of the generated sentence, keeping it grammatical. We model the problem as an integer linear programming (ILP) formulation, similar to the dependency graph fusion as proposed by Fillipova and Strube [1]. The directed edges in the graph (binary variables) are represented as xg,d,l, where g, d and l denote the governor node, dependent node and the label of an edge, respectively. We maximize the following objective function:∑\nx xg,d,l · p(l | g) · I(d) ·\npx N (1)\nAs shown in Equation (1), we introduce three different terms: p(l | g), I(d) and px\nN . Each relation in a dependency graph consists\nof the governing node, the dependent node and the relation type. The term p(l | g) denotes the probabilities of the labels given a governor node, g. For every node (word and POS) in the entire corpus, the probabilities are represented as the ratio of the sum of the frequency of a particular label and the sum of the frequencies of all the labels emerging from a node. In this work, we calculate\nthese values using Reuters corpora [5] to obtain dominant relations from non-conversational style of text. For example, Table 2 shows the probabilities of outgoing edges from a node, (produced/VBN). This term assigns the importance of grammatical relations to a node and only the relations that are more dominant from a node will be preferred. The term I(d) denotes the informativeness of a node calculated using Hori and Furui’s formula [2]. The last term in Equation (1) is based on the idea of lexical cohesion. Towards the end of any segment, generally, more important discussions might happen that will conclude a particular topic and then start another. In order to take this fact into account, we introduce the term px\nN ,\nwhere N and px denote the total number of extracted utterances in a segment and the position of the utterance (the edge x belongs to) in the set of N utterances, respectively.\nIn order to solve the above ILP problem, we impose a number of constraints. Some of the constraints have been directly adapted from the original ILP formulation [1]. For example, we use the same constraints for restricting one incoming edge per node, as well as we impose the connectivity constraint to ensure a connected graph structure. Further, we restrict the subtree to have just one start edge and one end edge. This helps in preserving one ROOT node, as well as it limits to one end node for the generated subtree. We also limit the generated subtree to have a maximum of 15 nodes that controls the length of the summary sentence. We also add few linguistic constraints that ensure the coherence of the output such as every node can have maximum of one determinant, etc. We also impose constraints to prevent cycles in the graph structure, otherwise finding the best path from start and end nodes might be difficult. The final graph is linearized to obtain a coherent sentence. In the linearization process, we order the nodes based on their original ordering in the utterance."
    }, {
      "heading" : "3. EXPERIMENTAL RESULTS",
      "text" : "The AMI Meeting corpus contains 20 meeting transcripts in the test set along with their corresponding abstractive (human-written) summaries as well as the annotations of topic segments. ROUGE is used to compare content selection of several approaches. We compared the content selection of our approach to an extractive summarizer [3], which works as a baseline. We also compared our model without using anaphora resolution to see the impact of resolving pronouns. All the summaries were compared against the human-written summaries as reference. The results in Table 3 show that our method outperforms the other techniques on both ROUGE-2 (R2) and ROUGE-SU4 (R-SU4) recall scores. Moreover, we computed a coarse estimate of grammaticality using the log-likelihood score (LL) from the parser. Our technique significantly outperforms the extractive method. In future work, we plan to design an end-to-end framework for summary generation from meetings."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This material is based upon work supported by the National Science Foundation under Grant No. 0845487."
    }, {
      "heading" : "4. REFERENCES",
      "text" : "[1] K. Filippova and M. Strube. Sentence Fusion via Dependency\nGraph Compression. In Proc. of EMNLP, pages 177–185, 2008.\n[2] C. Hori and S. Furui. A New Approach to Automatic Speech Summarization. IEEE Transactions on Multimedia, 5(3):368–378, 2003.\n[3] G. Murray and G. Carenini. Summarizing Spoken and Written Conversations. In Proc. of EMNLP, pages 773–782, 2008.\n[4] G. Murray, G. Carenini, and R. Ng. Generating and Validating Abstracts of Meeting Conversations: a User Study. In Proc. of INLG, pages 105–113, 2010.\n[5] T. Rose, M. Stevenson, and M. Whitehead. The Reuters Corpus Volume 1-from Yesterday’s News to Tomorrow’s Language Resources. In Proc. of LREC, pages 827–832, 2002."
    } ],
    "references" : [ {
      "title" : "Sentence Fusion via Dependency Graph Compression",
      "author" : [ "K. Filippova", "M. Strube" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "A New Approach to Automatic Speech Summarization",
      "author" : [ "C. Hori", "S. Furui" ],
      "venue" : "IEEE Transactions on Multimedia,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Summarizing Spoken and Written Conversations",
      "author" : [ "G. Murray", "G. Carenini" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Generating and Validating Abstracts of Meeting Conversations: a User Study",
      "author" : [ "G. Murray", "G. Carenini", "R. Ng" ],
      "venue" : "In Proc. of INLG,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "The Reuters Corpus Volume 1-from Yesterday’s News to Tomorrow’s Language Resources",
      "author" : [ "T. Rose", "M. Stevenson", "M. Whitehead" ],
      "venue" : "In Proc. of LREC,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "A recent study revealed that people generally prefer abstractive summaries [4].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "To aggregate the information from multiple utterances, we adapt an existing integer linear programming (ILP) based fusion technique [1].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "We model the problem as an integer linear programming (ILP) formulation, similar to the dependency graph fusion as proposed by Fillipova and Strube [1].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "these values using Reuters corpora [5] to obtain dominant relations from non-conversational style of text.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "The term I(d) denotes the informativeness of a node calculated using Hori and Furui’s formula [2].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "Some of the constraints have been directly adapted from the original ILP formulation [1].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 2,
      "context" : "We compared the content selection of our approach to an extractive summarizer [3], which works as a baseline.",
      "startOffset" : 78,
      "endOffset" : 81
    } ],
    "year" : 2016,
    "abstractText" : "ive Meeting Summarization Using Dependency Graph Fusion Siddhartha Banerjee The Pennsylvania State University University Park PA, USA 16802 sub253@ist.psu.edu Prasenjit Mitra Qatar Computing Research Institute Tornado Tower, 18th floor Doha, Qatar pmitra@qf.org.qa Kazunari Sugiyama National University of Singapore 13 Computing Drive Singapore 117417 sugiyama@comp.nus.edu.sg",
    "creator" : "LaTeX with hyperref package"
  }
}