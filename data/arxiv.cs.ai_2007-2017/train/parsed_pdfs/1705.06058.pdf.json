{
  "name" : "1705.06058.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Pitfalls and Best Practices in Algorithm Configuration",
    "authors" : [ "Katharina Eggensperger", "Frank Hutter" ],
    "emails" : [ "eggenspk@cs.uni-freiburg.de", "lindauer@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "To obtain peak performance of an algorithm, it is often necessary to tune its parameters. The AI community has recently developed automated methods for the resulting algorithm configuration (AC) problem (Hutter et al., 2009) to replace tedious, irreproducible and errorprone manual parameter tuning. Some example applications, for which automated AC procedures led to new state-of-the-art performance, include satisfiability solving (Hutter et al., 2007, 2017), maximum satisfiability (Ansótegui et al., 2016), scheduling (Chiarandini et al., 2008), mixed integer programming (Hutter et al., 2010a), evolutionary algorithms (Bezerra et al., 2016), answer set solving (Gebser et al., 2011), AI planning (Vallati et al., 2013) and machine learning (Thornton et al., 2013; Feurer et al., 2015).\nAlthough the usability of AC systems improved over the years (e.g., SpySMAC (Falkner et al., 2015)), we still often observe fundamental issues in the design and execution of experiments with algorithm configuration methods by both experts and new users. The goals of this work are therefore to:\n• highlight the many pitfalls we have encountered in AC experiments (run by ourselves and others);\n• present best practices to avoid most of these pitfalls; and\nar X\niv :1\n70 5.\n06 05\n8v 1\n[ cs\n.A I]\n1 7\nM ay\n• propose a unified interface between an AC system and the algorithm it optimizes (the so-called target algorithm) that directly implements best practices related to properly measuring the target algorithm’s performance with different parameter settings.\nSpecifically, after a short overview of algorithm configuration (Section 2.1), we present an extensively tested package to provide an interface between AC systems and target algorithms that aims to improve the reliability, reproducibility and robustness of AC experiments (Section 2.2). Afterwards, we describe common pitfalls in using AC systems and recommendations on how to avoid them. We first discuss pitfalls concerning the interface between AC systems and target algorithms (Section 3), followed by pitfalls regarding overtuning (Section 4). We note that, while we observed the described pitfalls in the context of AC, most of them also apply to the experimental evaluation of algorithms in general. Throughout, we illustrate pitfalls by AC experiments on propositional satisfiability solvers (Biere et al., 2009) as a prototypical AC example, but insights directly transfer to other AC problems. We end this note by providing further general recommendations for effective configuration in Section 5."
    }, {
      "heading" : "2 Background: Algorithm Configuration",
      "text" : "The algorithm configuration problem can briefly be described as follows: given an algorithm A to be optimized (the so-called target algorithm) with parameter configuration space Θ, a set of instances Π, and a cost metric c : Θ × Π → R, find a configuration θ ∈ Θ that minimizes the cost metric c across the instances in Π:\nθ∗ ∈ arg min θ∈Θ ∑ π∈Π c(θ, π). (1)\nA concrete example for this algorithm configuration problem would be to find a parameter setting θ ∈ Θ of a solver A for the propositional satisfiability problem (SAT) (such as glucose (Audemard & Simon, 2009) or lingeling (Biere, 2013)) on a set of CNF instances Π (e.g., hardware or software verification instances) that minimizes A’s average running time c. Another example would be to find a hyperparameter setting for a machine learning algorithm that minimizes its error c on a given dataset (Snoek et al., 2012; Feurer et al., 2015); in this latter example, error could either be measured via k-fold inner crossvalidation (giving rise to k instances for algorithm configuration), or a single validation set (in which case there is just a single instance for algorithm configuration).\nThe general workflow of a sequential algorithm configuration procedure (short: configurator) is shown in Figure 1. In each step, the configurator picks a configuration θ ∈ Θ and an instance π ∈ Π, triggers a run of algorithm A with configuration θ on instance π with a maximal running time cutoff κ, and measures the resulting cost c(θ, π). (As detailed in Section 2.2, this step is usually mediated by a target-algorithm specific wrapper.) The configurator uses this collected data about the target algorithm’s performance to find a well-performing configuration, typically operating as an anytime procedure until its configuration budget is exhausted (e.g., a maximal number of target algorithm calls or a time budget); when terminated, it returns its best found configuration so far."
    }, {
      "heading" : "2.1 Approaches for solving the AC problem",
      "text" : "There are several quite different configurators for solving this AC problem. ParamILS (Hutter et al., 2009) uses local search in the configuration space, employing a racing strategy to decide which of two configurations performs better without running both of them on all instances. irace (López-Ibáñez et al., 2016) uses iterative races via F-race (Birattari et al., 2002) on a set of picked configurations to determine the best one. SMAC (Hutter et al., 2011) and its distributed version dSMAC (Hutter et al., 2012)) use probabilistic models of algorithm performance, so-called empirical performance models (Hutter et al., 2014), to guide the search for good configurations by means of an extension of Bayesian Optimization (Brochu et al., 2010). GGA (Ansótegui et al., 2009) represents parameters as genes and uses a genetic algorithm with a competitive and a non-competitive gender; its newest version GGA++ (Ansótegui et al., 2015) also uses an empirical performance model for guidance. For a more detailed description of these algorithms, we refer the interested reader to the original papers or to the report of the Configurable SAT Solver Challenge (Hutter et al., 2017).\nIf the cost metric c is running time, several configurators use an adaptive capping strategy (Hutter et al., 2009) to terminate slow algorithm runs prematurely to save time. For example, if the maximal cutoff time used at test time is κmax = 5000 seconds and the best configuration known so far solves each instance in 10 seconds, we can save dramatically by cutting off slow algorithm runs after κ > 10 seconds instead of running all the way to κmax. Since κ is adapted dynamically, each target algorithm run can be issued with a different one."
    }, {
      "heading" : "2.2 The Role of the Target Algorithm Wrapper",
      "text" : "As depicted in Figure 1, configurators execute the target algorithm with configurations θ ∈ Θ on instances π ∈ Π and measure the resulting cost c(θ, π). To be generally applicable, configurators specify an interface through which they evaluate the cost c(θ, π) of arbitrary algorithms to be optimized.\nFor a new algorithm A, users need to implement this interface to actually execute A with the desired configuration θ on the desired instance π and measure the desired cost metric c(θ, π) (e.g., running time or solution cost). In order to avoid having to change\nthe algorithm to be optimized, this interface is usually implemented by a wrapper.1 In the simplest case, the input to the wrapper is just a parameter configuration, but it can also include an instance, a random seed and computational resource limits, such as a running time cutoff κ. Given these inputs, the wrapper returns the cost function to be optimized (e.g., the running time an algorithm required to solve a problem instance).\nFrom an implementation point of view, the wrapper has to provide a communication layer between the configurator and the target algorithm. It needs to\n1. parse the input arguments provided by the configurator and call the target algorithm with them;\n2. limit the run’s computational resources (i.e., running time and memory consumption); and\n3. parse the target algorithm’s output and compute the cost function being optimized.\nIt turns out that there are many possible pitfalls when writing such a wrapper, and we will demonstrate several of these in Section 3. To simplify the task of writing a robust and reliable wrapper that automatically avoids all of the pitfalls described in that section, we provide a Python package called GenericWrapper4AC2. This wrapper supports the input format of the most popular configurators ParamILS , GGA, irace and SMAC and automatically limits the computational resources of the target algorithm run with the runsolver tool developed by Roussel (2011). The user only needs to implement two target algorithm-specific functions:\n1. translate the provided input arguments to a call of the target algorithm;\n2. parse the target algorithm’s output to extract its cost.\nWe provide example implementations of these functions for SAT solvers, other combinatorial problem solvers, machine learning algorithms, and arbitrary black box functions as example configuration scenarios that ship with the newest SMAC release. Appendix A provides additional details about our generic wrapper, and an example wrapper for a SAT solver."
    }, {
      "heading" : "3 Pitfalls and Best Practices Concerning Algorithm Execu-",
      "text" : "tion\nIn this and the next section, we describe common pitfalls in algorithm configuration and illustrate their consequences on existing benchmarks from the algorithm configuration library AClib (Hutter et al., 2014)3. Based on the insights we acquired in thousands of algorithm configuration experiments over the years, we propose best practices to avoid these pitfalls.\nThroughout, we will use the state-of-the-art configurator SMAC (Hutter et al., 2011) as an example, typically optimizing penalized average running time (PAR-10), which counts\n1An alternative to a general wrapper would be a programming language-specific reliable interface for the communication between configurator and target algorithm (Hoos, 2012), which would make it easier for users to apply algorithm configuration to new target algorithms. However, the design of such an interface would also need to consider the pitfalls identified in this paper.\n2https://github.com/mlindauer/GenericWrapper4AC 3www.aclib.net\ntimeouts at κmax as 10 · κmax. Where not specified otherwise, we ran all experiments using our GenericWrapper4AC wrapper on the University of Freiburg’s META cluster, each of whose nodes shares 64 GB of RAM among two Intel Xeon E5-2650v2 8-core CPUs with 20 MB L3 cache and runs Ubuntu 14.04 LTS 64 bit.4\nPitfall 1: Trusting Your Target Algorithm\nMany state-of-the-art algorithms have been exhaustively benchmarked and tested with their default parameter configuration. However, since the configuration space of many algorithms is very large, we frequently observed hidden bugs triggered only by rarely-used combinations of parameter values. For example, Hutter et al. (2010a) reported finding bugs in mixed integer programming solvers and Manthey and Lindauer (2016) bugs in SAT solvers. Due to the size of the associated configuration spaces (e.g., 214 parameters and a discretized space of 1086 configurations in the state-of-the-art SAT solver Riss (Manthey, 2014)), exhaustive checks are infeasible in practice.\nOver the years, the types of bugs we have experienced even in commercial solvers (that are the result of dozens of person-years of development time) include:\n• Segmentation faults, Null pointer exceptions, and other unsuccessful algorithm terminations;\n• Wrong results (e.g., claiming a satisfiable SAT instance to be unsatisfiable); • Not respecting a specified running time cutoff that is passed as an input; • Not respecting a specified memory limit that is passed as an input; • Rounding down running time cutoffs to the next integer (even if that integer is zero);\nand\n• Returning faulty running time measurements (even negative ones!)\nEffects The various issues above have a multitude of negative effects, from obvious to subtle. If the algorithm run does not respect its resource limits this can lead to congested compute nodes (see Pitfall 3) and to configurator runs that are stuck waiting for an endless algorithm run to finish. Wrongly reported running times (e.g., close to negative infinity in one example) can lead to endless configuration runs when trusted. Rounding down cutoff times can let configurators miss the best configuration (e.g., when they use adaptive capping to cap running times at the best observed running time for an instance – if that running time is below one second then each new configuration will fail on the instance due to using a cutoff of zero seconds).\nAlgorithm crashes can be fairly benign when they are noticed and counted with the highest possible cost, but they can be catastrophic when not recognized as crashes: e.g., when blindly minimizing an algorithm’s running time the configurator will typically simply find a configuration that crashes quickly. While this can be exploited to quickly find bugs (Hutter et al., 2010a; Manthey & Lindauer, 2016), obtaining faulty configurations is typically the worst possible result of using algorithm configuration in practice. Bugs that\n4Data and scripts for the experiments in this paper are available at http://www.ml4aad.org/ best-practices-in-algorithm-configuration/.\nlead to wrong results tend to be discovered by configurators when optimizing for running time, since (at least for NP-hard problems) we found that such bugs often allow algorithms to find shortcuts and thus shorten running times. Therefore, blindly minimizing running time without solution checking often yields faulty configurations.\nDetailed Example In 2012, we used algorithm configuration to minimize the running time of the state-of-the-art solver glucose (Audemard & Simon, 2009). We quickly found a parameter configuration that appeared to yield new state-of-the-art performance on the industrial instances of the SAT Challenge 20125; however, checking this configuration with the authors of Glucose revealed that it led to a bug which made Glucose falsely report some satisfiable instances as unsatisfiable.6 Figure 2 plots the performance of the best Glucose v2.1 configurations that SMAC found over time when trusting Glucose’s correctness at configuration time; we contrast the low running time we thought we obtained (when trusting Glucose’s outputs) to the dramatically worse true result actually obtained (when using solution checking). This reveals that the parameter configurations we believed to perform well triggered bugs in Glucose.\n5http://baldur.iti.kit.edu/SAT-Challenge-2012/ 6The bug in Glucose version 2.1 was fixed after we reported it to the developers, and we are not aware\nof any bugs in the newest Glucose version 4.1.\nBest Practice Most of the issues above can be avoided by wrapping target algorithm runs with a reliable piece of code that limits their resources and checks whether they yield correct results. Cast differently, the job of this wrapper is to actually measure the cost function c(θ, π) of interest, which should intuitively heavily penalize any sort of crashes or bugs that lead to wrong results. Our generic wrapper is one readily-available tool for this purpose.\nIf enough computational time is available, we recommend to first run systems such as SpyBug (Manthey & Lindauer, 2016) to find bugs in the configuration space, and to either fix them or to exclude the faulty part of the configuration space from consideration. Regardless of whether this is done or not, since it is infeasible to perfectly check the entire configuration space, we always recommend to check the returned solution of the target algorithms during the configuration process. For example, for SAT instances our example instantiations of the generic wrapper exploit the standard SAT checker tool routinely used in the SAT competitions to verify the correctness of runs. For solvers that output unsatisfiability proofs, there are also effective tools for checking these proofs (Heule et al., 2014).\nPitfall 2: Not Terminating Target Algorithm Runs Properly\nGiven the undecidability of the halting problem, target algorithm runs need to be limited by some kind of running time cutoff κmax to prevent poor configurations from running forever. In many AI communities, it is a common practice to set a running time cutoff as part of the cost metric and measure the number of timeouts with that cutoff (e.g., κmax = 5000 seconds in the SAT race series). In algorithm configuration, the ability to prematurely cut off unsuccessful runs also enables adaptive capping (see Section 2). Therefore, it is essential that target algorithm runs respect their cutoff.\nEffects Consequences of target algorithm runs not respecting their cutoffs can include:\n1. If the target algorithm always uses the maximal cutoff κmax and ignores an adapted cutoff κ < κmax, the configuration process is slowed down since the benefits of adaptive capping are given up;\n2. If the target algorithm completely ignores the cutoff, the configuration process may stall since the configurator waits for a slow target algorithm to terminate (which, in the worst case, may never happen);\n3. If a wrapper is used that fails to terminate the actual algorithm run but nevertheless returns the control flow to the configurator after the cutoff time κ then the slow runs executed by the configurator will continue to run in parallel and overload the machine, messing up the cost computation (e.g., wallclock time).\nExample The latter (quite subtle) issue actually happened in a recent publication that compared GGA++ and SMAC , in which a wrapper bug caused SMAC to perform poorly (Ansótegui et al., 2015). The authors wrote a wrapper for SMAC that tried to terminate its target algorithm runs (here: Glucose or Lingeling) after the specified cutoff time κ\nby sending a KILL signal, but since it ran the target algorithm through a shell (using subprocess.Popen(cmd, shell=True) in Python) the KILL signal only terminated the shell process but not the actual target algorithm (which continued uninterrupted until successful, sometimes for days). When attempting to reproduce the paper’s experiments with the original wrapper kindly provided by the authors, over time more and more target algorithms were spawned without being terminated, causing our 16-core machine to slow down and eventually become unreachable. This issue demonstrates that SMAC heavily relies on a robust wrapper that automatically terminates its target algorithm runs properly.7 This downside of SMAC is alleviated by our new tool GenericWrapper4AC.\nTo illustrate this issue in isolation, we compared SMAC using the standard version of GenericWrapper4AC and a broken version that returns the control flow to the configurator when the running time cutoff is reached, without terminating the target algorithm run process. Figure 3 shows the performance achieved when SMAC is run with either wrapper to configure Cryptominisat (Soos, 2014) for penalized average running time (PAR-10) to solve Circuit Fuzz instances (Brummayer et al., 2012) as used in the CSSC 2014 (Hutter et al., 2017). We executed 80 SMAC runs for each wrapper, with 16 independent parallel runs each on five 16-core machines. Both SMAC versions performed equally well until too many target algorithm processes remained on the machines and prevented SMAC from progressing further. Only on one of the five machines that ran SMAC with the broken wrapper the runs terminated after the specified wallclock-limit of 2 days; after an additional\n7In contrast to SMAC , GGA++ does not require a wrapper; in the experiments by Ansótegui et al. (2015), GGA++ directly sent its KILL signal to the target algorithm and therefore did not suffer from the same problem SMAC suffered from, which confounded the paper’s comparison between GGA++ and SMAC . Additionally, there was also a simple typo in the authors’ wrapper for SMAC in parsing the target algorithm’s output (here: Glucose) that caused it to count all successful runs on unsatisfiable instances as timeouts. Receiving wrong results for all unsatisfiable instances (about half the instance set) severely affected SMAC ’s trajectory; this issue was only present in the wrapper for SMAC (and therefore did not affect GGA++), confounding the comparison between GGA++ and SMAC further.\nday, three of the remaining machines were still frozen caused by overload and the fourth could not be reached at all.\nBest Practice To avoid this pitfall, we recommend to use the GenericWrapper4AC or some other well-tested piece of code to reliably control and terminate target algorithm runs.\nPitfall 3: Slow File System\nRelated to Pitfall 2, another way to ruin running time measurements by slowing down a machine is to overload the used file system. Each target algorithm run typically has to read the given problem instance and writes some log files, and thus executing many algorithm configuration runs in parallel can stress the file system.\nExample 1 Over the years, we have experienced file system issues on a variety of clusters with shared file systems when target algorithm runs were allowed to write to the shared network file system. When executing hundreds (or on one cluster, even thousands) of algorithm configuration runs in parallel, this stressed the file system to the point where the system became very slow for all users and we measured 100-fold overheads in individual target algorithm evaluations. Writing target algorithm outputs to the local file system fixed these issues.\nExample 2 Distributing configuration runs across multiple nodes in a compute cluster (e.g., in GGA, irace, or dSMAC can be error-prone if the configurators communicate over the file system. In particular, we experienced issues with several shared network file systems with asynchronous I/O; e.g., on one compute node a file was written, but that file was not immediately accessible (or still empty) on other compute nodes. Often a second read access resolved the problem, but this solution can be brittle; a change of parallelization strategy may in that case yield more robust results.\nExample 3 Even when writing target algorithm output to the local file system, we once experienced 200-fold overheads in target algorithm runs (invocations of sub-second target algorithm runs hanging for minutes) due to a subtle combination of issues when performing hundreds of algorithm configuration experiments in parallel. On the Orcinus cluster (part of Compute Canada’s Westgrid cluster), which uses a Lustre file system, we had made our algorithm configuration benchmarks read-only to prevent accidental corruption. While that first seemed like a good idea, it disallowed our Python wrapper to create .pyc bytecode files and forced it to recompile at every invocation, which in turn triggered a stats call (similar to ls on the Linux command line) for each run. Stats calls are known to be slow on the Lustre file system, and executing them for each sub-second target algorithm run on hundreds of compute nodes lead to extreme file system slowdowns. After testing many other possible reasons for the slowdowns, removing the read-only condition immediately fixed all issues.\nBest Practice Issues with shared file systems on compute clusters can have subtle reasons and sometimes require close investigation (as in our Example 3). Nevertheless, most issues can be avoided by using the faster local file system (typically /tmp/, or even better, a\ntemporary job-specific subdirectory thereof8) for all temporary files, and by measuring CPU time instead of wallclock time (at least for sequential algorithms). Both best practices are implemented in the GenericWrapper4AC package.\nPitfall 4: Comparing Configurators using Different Wrappers\nThe required functionalities of the target algorithm wrapper differ slightly for different configurators. For example, SMAC and ParamILS trust the wrapper to terminate target algorithms, but GGA sends a KILL signal on its own. Therefore, sometimes configurators are compared by using different wrappers. However, if this is not done properly, it can lead to a biased comparison between configurators.\nEffect Using different wrappers for different configurators can lead to different behaviors of the target algorithm and hence, to different returned performance values for the same input. If the configurators receive different performance measurements, they will optimize different objective functions and their runs become incomparable.\nExample During the early development of SMAC (before any publication), we used the same wrappers for ParamILS and SMAC but an absolute path to the problem instance for one and a relative path for the other. Even this tiny difference lead to reproducible differences of running time measurements of up to 20% when optimizing an algorithm implemented in UBCSAT 1.1.0 (Tompkins & Hoos, 2005). The reason was that that version of UBCSAT stored its callstring in its heap space such that the number of characters in the instance name affected data locality and therefore the number of cache misses and the running time (whereas the number of search steps stayed the same).9 This subtle issue demonstrates the importance of using exactly the same wrapper for all configurators being compared.\nBest Practice We recommend to use a single wrapper (e.g., based on GenericWrapper4AC) when comparing configurators against each other, in order to guarantee that all configurators optimize the same objective. Maintaining and testing a single wrapper also requires less effort than doing so for multiple wrappers. For studies comparing configurators, it is also paramount to use tried-and-tested publicly available benchmark scenarios (lowering the riks of typos, etc); our algorithm configuration benchmark library AClib (Hutter et al., 2014) provides a very broad collection of such benchmarks."
    }, {
      "heading" : "4 Pitfalls and Best Practices Concerning Over-Tuning",
      "text" : "A common issue in applying algorithm configuration is the over-tuning effect (Birattari, 2004; Hutter et al., 2007). Over-tuning is very related to the concept of over-fitting in machine learning and denotes the phenomenon of finding parameter configurations that yield\n8We note that on some modern Linux distributions, /tmp/ can be a RAM disk and therefore may use resources allotted to the algorithm runs; in general, we recommend to make the choice about a fast temporary directory specific to the cluster used.\n9This issue is fixed in later versions of UBCSAT.\nstrong performance for the training task but do not generalize to test tasks. To safeguard against over-tuning effects, we recommend to always evaluate generalization performance (typically, using a set of benchmark instances disjoint from the benchmarks used for training). In the following, we discuss three pitfalls related to over-tuning."
    }, {
      "heading" : "4.1 Pitfall 5: Over-tuning to Random Seeds",
      "text" : "Many algorithms are randomized (e.g., SAT solvers or AI planners). However, in many communities, the random seeds of these algorithms are fixed to simulate a deterministic behavior and to ensure reproducibility of benchmark results.\nEffect Ignoring the stochasticity of an algorithm in algorithm configuration by fixing the random seed can lead to over-tuning effects to this seed, i.e., finding a configuration that yields good performance with this fixed random seed (or set of seeds) but poor performance when used with other random seeds. The extreme case is not to only fix the random seed, but to tune the random seed, which can lead to an even stronger over-tuning effect.\nExample To illustrate over-tuning to a random seed in its purest form, independent of a difference between training and test instances, we optimized the parameters of the local-search SAT solver Saps (Hutter et al., 2002) on a single instance, the only difference between training and test being the set of random seeds used. We used different settings of SMAC to handle random seeds: SMAC (1) used only one run with a fixed seed to evaluate a configuration; SMAC (10) and SMAC (100) evaluated each configuration with a fixed set of 10 or 100 random seeds, respectively; and standard SMAC handled the random seeds itself (using a larger number of seeds to evaluate the best configurations).\nAs a cost metric, we minimized the number of local search steps (the solver’s so-called runlength) since this is perfectly reproducible. For the parameter configurations recommended at each step of each SMAC run, we measured SMAC ’s training cost (as the mean\nacross the respective sets of seeds discussed above) as well as its test cost (the mean runlength across 1000 fixed random seeds that were disjoint from the sets of seeds used for configuration).\nFigure 4 shows median costs across 10 SMAC runs, contrasting training cost (left) and test cost (right). On training, SMAC (1) quickly improved and achieved the best training cost on its one random seed, but its performance does not generalize to the test seeds. SMAC (10) and SMAC (100) were slower but generalized better, and standard SMAC was both fast and generalized best by adaptively handling the number of seeds to run for each configuration.\nBest Practice For randomized algorithms, we recommend to tune parameter configurations across different random seeds—most configurators will take care of the required number of random seeds if the corresponding options are used. If a configuration’s performance does not even generalize well to new random seeds, we expect it to also not generalize well to new instances. Furthermore, the number of available instances is often restricted, but there are infinitely many random seeds which can be easily sampled. Likewise, when there are only few test instances, at validation time we recommend to perform multiple runs with different random seeds for each test instance."
    }, {
      "heading" : "4.2 Pitfall 6: Over-tuning to Training Instances",
      "text" : "The most common over-tuning effect is over-tuning to the set of training instances, i.e., finding configurations that perform well on training instances but not on new unseen instances. This can happen if the training instances are not representative for the test instances; in particular this is often an issue if the training instance set is too small or the instances are not homogeneous (Hutter et al., 2010b; Schneider & Hoos, 2012), i.e., if there exists no single configuration with strong performance for all instances.\nEffect In practice, over-tuned configurations are of little value, because users are not interested in configurations that only perform well on a small finite set of instances but that also perform well on new instances with similar characteristics. Phrasing this more generally, research insights should also generalize to experiments with similar characteristics.\nExample To illustrate this problem, we studied training and test performance of various configurations for three exemplary benchmarks (see Figure 5):\nClasp on N-Rooks We studied running time of the solver Clasp (Gebser et al., 2012) on N-Rooks instances (Manthey & Steinke, 2014), a benchmark from the Configurable SAT Solver Challenge (CSSC 2014; Hutter et al. (2017)). In this case, the running times on the training and test set were almost perfectly linearly correlated, with a Spearman correlation coefficient of 0.99, i.e., the ranking of the configurations on both sets is nearly identical; this is also visualized in Figure 5a. This is a very good case for applying algorithm configuration, and, correspondingly, in the CSSC 2014 algorithm configuration yielded large improvements for this benchmark.\nLingeling on mixed SAT We reconstructed a benchmark from Ansótegui et al. (2015) in which they optimized Lingeling (Biere, 2014) on a mixed set of industrial SAT instances. Instead of randomly splitting the data into train and test instances, they first created a training set by removing hard instances (i.e., not solved within the cutoff time by reference solvers) and used these remaining hard instances as test instances. Figure 5b shows that SMAC improved the running time of Lingeling on the training set but that these improvements did not generalize to the test instances. In fact, the optimized configurations (blue dots) are weakly correlated with a Spearman correlation coefficient of 0.15. The benchmark’s heterogeneity and the corresponding mismatch between training and test set make this benchmark poorly suited for applying algorithm configuration and call for portfolio approaches (Xu et al., 2008; Malitsky et al., 2012; Lindauer et al., 2015) that can handle heterogeneous instances.\nClasp on LABS Figure 5c shows another benchmark from the CSSC: configuration of Clasp on SAT-encoded low autocorrelation binary sequence (LABS) benchmarks (Mugrauer & Balint, 2013). This illustrates a rare worst case for algorithm configuration, in which performance even degrades on the training set, which is possible due to SMAC ’s (and any other configurator’s) racing approach: the configurator already changes the incumbent before all training instances have been evaluated, and if a subset is not representative of the full set this may lead to performance degradation on the full set. Although visually not apparent from Figure 5c, for this benchmark, there was also no clear correlation between the running time on training and test instances for good configurations (with a correlation coefficient of 0.42 on the 20% best-performing randomly sampled configurations).\nWhile we have occasionally observed such strong heterogeneity on instances with very heterogeneous sources, it was very surprising to observe this in a case where all instances stemmed from the same instance family. We therefore analyzed this benchmark further (Hutter et al., 2017), showing that twice as many SMAC runs with a fivefold larger configuration budget managed to improve training performance slightly. However, that improvement on the training set did not generalize to the test set due to the benchmark’s heterogeneity. Again, for such heterogeneous benchmarks we recommend the usage of portfolio approaches.\nBest Practice Over-tuning is often not easy to avoid before running algorithm configuration experiments, since the effect can only be observed afterwards (for example by scatter plots such as in Figure 5). Nevertheless, the following strategies minimize the chance of over-tuning (see also Section 5):\n1. The training instances should be representative of the test instances;\n2. The training set should be relatively large (typically hundreds to thousands of instances) to increase the chance of being representative;\n3. The instance sets should stem from a similar application, use context, etc., increasing the likelihood that they have similar structures which can be exploited with similar solution strategies."
    }, {
      "heading" : "4.3 Pitfall 7: Over-tuning to a Particular Machine Type",
      "text" : "In the age of cloud computing and large compute clusters, an obvious idea is to use these remotely-accessible compute resources to benchmark algorithms and configure them. However in the end, these remote machines are not always the production systems the algorithms are used on in the end. Geschwender et al. (2014) showed in a preliminary study that it is possible in principle to configure algorithms in the cloud and for the found configurations to perform well on another machine. Unfortunately, this does not hold for all kind of algorithms – for example, the performance of solvers for SAT (Aigner et al., 2013) and mixed integer programming (Lodi & Tramontani, 2014) can depend strongly on the used machine type (including hardware, operating system and installed software libraries)\nEffect If different parameter configurations of an algorithm perform well on the machine used for configuration than on the production machine, we might choose configurations that perform poorly on our production system.\nExample An example for such machine-dependent algorithms are SAT solvers that are often highly optimized against cache misses (Aigner et al., 2013). To study the effect of different machines, we optimized three SAT solvers from the configurable SAT solver challenge (Hutter et al., 2017), namely Minisat-HACK-999ED (Oh, 2014), Clasp (Gebser et al., 2012) and Lingeling (Biere, 2014) on Circuit Fuzz instances (Brummayer et al., 2012). As different machine types, we used AWS m4.4xlarge instances with 2.4-GHz Intel Xeon E5-2676 v3 CPUs with 30MB level-3 cache and the META-cluster at the University of Freiburg with 2.6GHz Intel Xeon E5-2650 v2 CPUs with 20MB level-3 cache. On both systems, we ran Ubuntu 14.04 64bit and allowed for a memory limit of 3GB for each solver run. The binaries were statically compiled such that they are not linked against different libraries on the different systems. We configured the solvers with 12 independent SMAC\nruns and validated the cost on test instances on the same system used for the configuration process.\nTable 1 lists the ranking and the PAR10 scores of the solvers on each machine (showing the test cost of the configuration performing best on training); we note that the PAR10 scores are only comparable on the same system. The ranking between Clasp and Lingeling is always the same, with Lingeling performing worst on both systems. However, Minisat-HACK-999ED is ranked differently on both machines, i.e., best on AWS, but second on the Meta-cluster. If the AWS cloud would be our benchmark environment we would decide for Minisat-HACK-999ED based on the AWS results, but this would not be the best choice on the Meta cluster. We verified this by also validating the configurations from AWS on the Meta-cluster; in that setting the configured Minisat-HACK-999ED performed even worse than Lingeling and Clasp.\nBest Practice We note that this pitfall exists only for some machine-sensitive algorithms. Therefore, we recommend to investigate whether an algorithm at hand has machinedependent performance, for example, by validating the performance of various configurations on both the system used for configuration and the production system."
    }, {
      "heading" : "5 Further Recommendations for Effective Configuration",
      "text" : "In the following, we describe recommendations for users of algorithm configuration systems to obtain parameter configurations that will perform better in production. Some of these recommendations are rules of thumb, since the involved factors for a successful configuration can be very complex and can change across configuration scenarios. For general empirical algorithmics, Hoos (2017) recommends further best practices, including design, reports and analysis of computational experiments.\nTraining and Test Sets As discussed before, we strongly recommend to split the available instances into a training and a test set to obtain an unbiased estimate of generalization performance from the test set. To obtain trivial parallelization of randomized configuration procedures, we recommend to run n independent configuration runs and use the training set to select the best of the n resulting configurations (Hutter et al., 2012). Only that single chosen configuration should be evaluated on the test set; we explicitly note that we cannot select the configuration that performs best on the test set, because that would amount to peeking at our test data and render performance estimates on the test set biased.\nRepresentative Instances and Running Time Cutoff Intuitively, instances for which every parameter configuration times out do not help the configurator to make progress. One strategy can be to remove these from the training set. However, this comes with the risk to bias the training set towards easy instances and should be used with caution. Generally, we therefore recommend to use training instances for the configuration process that are representative of the ones to be solved later. Using training instances from a range of hardness can also often help yield configurations that generalize (Hoos et al., 2013). If feasible, we recommend to select instances and running time cutoffs such that roughly 75% or more of the training instances used during configuration can be solved by the initial parameter configuration within the cutoff. We emphasize that – while the configuration protocol may in principle choose to subsample the training instances in arbitrary ways – the test set should never be touched and not pre-evaluated to ensure an unbiased cost estimate of the optimized configurations in the end (see Pitfall 6). To select a good training instance set, Bayless et al. (2014) proposed a way to quantify whether an instance set is a good proxy for another instance set. Furthermore, Styles and Hoos (2015) proposed a splitting strategy of the instances for better scaling to hard instances. They split the instances into a training, validation and test set to use easy instances during configuration for fast progress and select a configuration on the harder validation set such that the configuration will perform well on the hard test set.\nHomogeneous vs Heterogenous Instance Sets Sometimes configurators are used to obtain well-performing and robust configurations on a heterogeneous instance set. However, we know from algorithm selection (Rice, 1976; Kotthoff, 2014) that often no single configuration exists that performs well for all instances in a heterogeneous set, but a portfolio of configurations is required to obtain good performance (Xu et al., 2011; Kadioglu et al., 2010). Furthermore, the task of algorithm configuration becomes a lot harder if all instances can be solved best with very different configurations. Therefore, we recommend to use algorithm configuration mainly on homogeneous instance sets (Schneider & Hoos, 2012), i.e., instances with similar characteristics where we can expect to find one configuration that performs well on all instances. Furthermore, the size of the used instance set should be adjusted accordingly to the homogeneity of the instance set: on homogeneous instance sets, 50 instances might suffice for good generalization performance to new instances, but on fairly heterogeneous instance sets, we recommend to use at least 300 or, if possible, more than 1000 instances to obtain a robust parameter configuration.\nAppropriate Configuration Settings To use configurators, the user has to set the available configuration budget for the configurator. If the configuration budget is too small, the configurator might make little or no progress within in. In contrast, if the configuration budget is too large, we waste a lot of time and computational resources because the configurator might converge long before the budget is used up. A good rule of thumb in our experience is to use a budget that equals at least the expected running time of the default configuration on 200 to 1000 instances. In practice, an effective configuration budget strongly depends on several factors, including heterogeneity of the instance set (more heterogeneous instance sets require a larger configuration budget) or size of the configura-\ntion space (larger configuration spaces require more time to search effectively (Hutter et al., 2017)).\nEfficient Use of Parallel Resources Some configurators (such as GGA, irace and dSMAC ) can make use of parallel resources, while others (such as ParamILS and SMAC ) benefit from executing several independent parallel runs10 (and using the result from the one with the best training set performance; see, e.g., Hutter et al. (2012)). In the special case of GGA, using more parallel resources can actually improve the adaptive capping mechanism. Given k cores, we therefore recommend to execute one GGA run with k cores, but k independent ParamILS or SMAC runs with one core each. While this protocol was not used in early works11, it has been used in more recent evaluations (Ansótegui et al., 2015; Hutter et al., 2017).\nReasonable Configuration Space Another challenge in using algorithm configuration systems is to find the best configuration space. The user has to decide which parameters to optimize and which ranges to allow. The optimal set of parameters is often not clear and in case of doubt, we recommend to add more parameters to the configuration space and to use generous value ranges. With SMAC , we have successfully optimized configuration spaces with hundreds of parameters (Thornton et al., 2013). However, we note that unreasonably large configuration spaces are hard to configure and require substantially larger configuration budgets. For example, the state-of-the-art SAT solver Lingeling (Biere, 2013) has more than 300 parameters and most of them have a value range between 0 and 32bit maxint, but most of these parameters are either not really relevant for optimizing Lingeling ’s running time or the relevant value ranges are much smaller. Even though Lingeling can already substantially benefit from configuration we expect that with a more carefully designed configuration space even better results can be obtained. Therefore, we recommend to avoid including such parameters and to use smaller value ranges if corresponding expert knowledge is available.\nWhich Parameters to Tune Parameters should never be part of the configuration space if they change the semantics of the problem to be solved; e.g., do not tune the allowed memory or parameters that control whether a run is counted as successful (such as the allowed optimality gap in an optimization setting). Furthermore, to obtain an unbiased estimate of a configuration’s performance across seeds one should not include the seed (or parameters with a similar effect) as a tunable parameter.\nRunning Time Metrics A common cost metric in algorithm configuration is running time. However, obtaining clean running time measurements can be tricky because benchmark machines can be influenced by other running processes or I/O load on a shared file\n10In order to perform k independent runs with ParamILS or SMAC , one should use a different seed (equivalent to the numRun parameter) for each run.\n11Hutter et al. (2011) only used a single core per run of GGA, but still followed the protocol by Ansótegui et al. (2009) to race groups of 8 runs in parallel per core; therefore, GGA’s adaptive capping mechanism was the same in that work as in Ansótegui et al. (2009).\nsystem (see Pitfall 3). Therefore, we recommend to measure CPU time instead of wallclock time. However, CPU time can also sometimes be brittle; e.g., its resolution can be insufficient for very short target algorithm runs, such as milliseconds. One solution for this case is to measure elementary operations, such as search steps of a local search algorithm or MEMS (number of memory accesses (Knuth, 2011)); however, it has to be ensured that such proxy metrics correlate well with running time.\nComparing Configurators on Existing, Open-Source Benchmarks Although algorithm configuration has been well established for over a decade, nearly every new paper on this topic uses a new set of benchmarks to compare different configurators. This makes it harder to assess progress in the field, and every new benchmark could again suffer from one of the pitfalls described above. Therefore, we recommend to use existing and open-source algorithm configuration benchmarks that are already well tested and can be freely used by the community. The only existing library of such benchmarks we are aware of is the algorithm configuration library AClib (Hutter et al., 2014), which comprises 326 benchmarks in Version 1.2 and allows users to pick benchmarks from different domains (e.g., mixed integer programming, AI Planning, SAT, machine learning) and with different characteristics (e.g., small or large configuration spaces)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Manuel López-Ibáñez and Kevin Tierney for adapting the interfaces of irace and GGA to work together with GenericWrapper4AC, Yuri Malitsky and Horst Samulowitz for providing the wrappers and benchmarks of Ansótegui et al. (2015), and Kevin Tierney, Manuel López-Ibáñez and Lars Kotthoff for very helpful feedback on the first draft of the paper that led to the inclusion of some further possible issues. Some of the recommendations in Section 5 were inspired by a discussion at a Dagstuhl seminar (see Lindauer and Hutter (2017) for more details) and we are thankful for the valuable contributions of the attendees of that discussion: Aymeric Blot, Wanru Gao, Holger Hoos, Laetitia Jourdan, Lars Kotthoff, Manuel López-Ibáñez, Nysret Musliu, Günter Rudolph, Marc Schoenauer, Thomas Stützle and Joaquin Vanschoren. The authors acknowledge funding by the DFG (German Research Foundation) under Emmy Noether grant HU 1900/2-1. K. Eggensperger additionally acknowledges funding by the State Graduate Funding Program of Baden-Württemberg."
    } ],
    "references" : [ {
      "title" : "Analysis of portfolio-style parallel SAT solving on current multi-core architectures",
      "author" : [ "M. Aigner", "A. Biere", "C. Kirsch", "A. Niemetz", "M. Preiner" ],
      "venue" : "In Proceeding of the Fourth International Workshop on Pragmatics of SAT (POS’13)",
      "citeRegEx" : "Aigner et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Aigner et al\\.",
      "year" : 2013
    }, {
      "title" : "Maxsat by improved instance-specific algorithm configuration",
      "author" : [ "C. Ansótegui", "J. Gabàs", "Y. Malitsky", "M. Sellmann" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Ansótegui et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ansótegui et al\\.",
      "year" : 2016
    }, {
      "title" : "Model-based genetic algorithms for algorithm configuration",
      "author" : [ "C. Ansótegui", "Y. Malitsky", "M. Sellmann", "K. Tierney" ],
      "venue" : "Proceedings of the 25th International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Ansótegui et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ansótegui et al\\.",
      "year" : 2015
    }, {
      "title" : "A gender-based genetic algorithm for the automatic configuration of algorithms",
      "author" : [ "C. Ansótegui", "M. Sellmann", "K. Tierney" ],
      "venue" : "Proceedings of the Fifteenth International Conference",
      "citeRegEx" : "Ansótegui et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ansótegui et al\\.",
      "year" : 2009
    }, {
      "title" : "Predicting learnt clauses quality in modern SAT solvers",
      "author" : [ "G. Audemard", "L. Simon" ],
      "venue" : "Proceedings of the 22th International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Audemard and Simon,? \\Q2009\\E",
      "shortCiteRegEx" : "Audemard and Simon",
      "year" : 2009
    }, {
      "title" : "Evaluating instance generators by configuration",
      "author" : [ "S. Bayless", "D. Tompkins", "H. Hoos" ],
      "venue" : "Proceedings of the Eighth International Conference on Learning and Intelligent Optimization (LION’14),",
      "citeRegEx" : "Bayless et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bayless et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatic component-wise design of multiobjective evolutionary algorithms",
      "author" : [ "L. Bezerra", "M. López-Ibáñez", "T. Stützle" ],
      "venue" : "IEEE Trans. Evolutionary Computation,",
      "citeRegEx" : "Bezerra et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bezerra et al\\.",
      "year" : 2016
    }, {
      "title" : "Lingeling, Plingeling and Treengeling entering the SAT competition 2013",
      "author" : [ "A. Biere" ],
      "venue" : "Balint, A., Belov, A., Heule, M., & Järvisalo, M. (Eds.), Proceedings of SAT Competition 2013: Solver and Benchmark Descriptions, Vol. B-2013-1 of Department of Computer Science Series of Publications B, pp. 51–52. University of Helsinki.",
      "citeRegEx" : "Biere,? 2013",
      "shortCiteRegEx" : "Biere",
      "year" : 2013
    }, {
      "title" : "Yet another local search solver and Lingeling and friends entering the SAT competition 2014",
      "author" : [ "A. Biere" ],
      "venue" : "Belov, A., Diepold, D., Heule, M., & Järvisalo, M. (Eds.), Proceedings of SAT Competition 2014: Solver and Benchmark Descriptions, Vol. B-2014-2 of Department of Computer Science Series of Publications B, pp. 39–40. University of Helsinki.",
      "citeRegEx" : "Biere,? 2014",
      "shortCiteRegEx" : "Biere",
      "year" : 2014
    }, {
      "title" : "A racing algorithm for configuring metaheuristics",
      "author" : [ "M. Birattari", "T. Stützle", "L. Paquete", "K. Varrentrapp" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference",
      "citeRegEx" : "Birattari et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Birattari et al\\.",
      "year" : 2002
    }, {
      "title" : "The problem of tuning metaheuristics as seen from a machine learning perspective",
      "author" : [ "M. Birattari" ],
      "venue" : "Ph.D. thesis, Université Libre de Bruxelles.",
      "citeRegEx" : "Birattari,? 2004",
      "shortCiteRegEx" : "Birattari",
      "year" : 2004
    }, {
      "title" : "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
      "author" : [ "E. Brochu", "V. Cora", "N. de Freitas" ],
      "venue" : null,
      "citeRegEx" : "Brochu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Brochu et al\\.",
      "year" : 2010
    }, {
      "title" : "Automated testing and debugging of SAT and QBF solvers",
      "author" : [ "R. Brummayer", "F. Lonsing", "A. Biere" ],
      "venue" : "Proceedings of the Fifteenth International Conference on Theory and Applications of Satisfiability Testing (SAT’12),",
      "citeRegEx" : "Brummayer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Brummayer et al\\.",
      "year" : 2012
    }, {
      "title" : "A modular multiphase heuristic solver for post enrolment course timetabling",
      "author" : [ "M. Chiarandini", "C. Fawcett", "H. Hoos" ],
      "venue" : "Proceedings of the Seventh International Conference on the Practice and Theory of Automated Timetabling",
      "citeRegEx" : "Chiarandini et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chiarandini et al\\.",
      "year" : 2008
    }, {
      "title" : "An extensible SAT-solver",
      "author" : [ "N. Eén", "N. Sörensson" ],
      "venue" : "Proceedings of the conference on Theory and Applications of Satisfiability Testing (SAT),",
      "citeRegEx" : "Eén and Sörensson,? \\Q2004\\E",
      "shortCiteRegEx" : "Eén and Sörensson",
      "year" : 2004
    }, {
      "title" : "SpySMAC: Automated configuration and performance analysis of SAT solvers",
      "author" : [ "S. Falkner", "M. Lindauer", "F. Hutter" ],
      "venue" : "Proceedings of the Eighteenth International Conference on Theory and Applications of Satisfiability Testing (SAT’15),",
      "citeRegEx" : "Falkner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Falkner et al\\.",
      "year" : 2015
    }, {
      "title" : "Initializing Bayesian hyperparameter optimization via meta-learning",
      "author" : [ "M. Feurer", "T. Springenberg", "F. Hutter" ],
      "venue" : "Proceedings of the Twenty-nineth National Conference on Artificial Intelligence",
      "citeRegEx" : "Feurer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feurer et al\\.",
      "year" : 2015
    }, {
      "title" : "A portfolio solver for answer set programming: Preliminary report",
      "author" : [ "M. Gebser", "R. Kaminski", "B. Kaufmann", "T. Schaub", "M. Schneider", "S. Ziller" ],
      "venue" : "Proceedings of the Eleventh International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR’11),",
      "citeRegEx" : "Gebser et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gebser et al\\.",
      "year" : 2011
    }, {
      "title" : "Conflict-driven answer set solving: From theory to practice",
      "author" : [ "M. Gebser", "B. Kaufmann", "T. Schaub" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Gebser et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gebser et al\\.",
      "year" : 2012
    }, {
      "title" : "Algorithm configuration in the cloud: A feasibility study",
      "author" : [ "D. Geschwender", "F. Hutter", "L. Kotthoff", "Y. Malitsky", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "Proceedings of the Eighth International Conference on Learning and Intelligent Optimization (LION’14),",
      "citeRegEx" : "Geschwender et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Geschwender et al\\.",
      "year" : 2014
    }, {
      "title" : "Bridging the gap between easy generation and efficient verification of unsatisfiability proofs",
      "author" : [ "M. Heule", "W. Hunt", "N. Wetzler" ],
      "venue" : "Software Testing Verification and Reliability,",
      "citeRegEx" : "Heule et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Heule et al\\.",
      "year" : 2014
    }, {
      "title" : "Programming by optimization",
      "author" : [ "H. Hoos" ],
      "venue" : "Communications of the ACM, 55 (2), 70–80.",
      "citeRegEx" : "Hoos,? 2012",
      "shortCiteRegEx" : "Hoos",
      "year" : 2012
    }, {
      "title" : "Robust benchmark set selection for boolean constraint solvers",
      "author" : [ "H. Hoos", "B. Kaufmann", "T. Schaub", "M. Schneider" ],
      "venue" : "Proceedings of the Seventh International Conference on Learning and Intelligent Optimization (LION’13),",
      "citeRegEx" : "Hoos et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hoos et al\\.",
      "year" : 2013
    }, {
      "title" : "Empirical Algorithmics",
      "author" : [ "H. Hoos" ],
      "venue" : "Cambridge University Press. To appear.",
      "citeRegEx" : "Hoos,? 2017",
      "shortCiteRegEx" : "Hoos",
      "year" : 2017
    }, {
      "title" : "Boosting verification by automatic tuning of decision procedures",
      "author" : [ "F. Hutter", "D. Babić", "H. Hoos", "A. Hu" ],
      "venue" : "Formal Methods in Computer Aided Design (FMCAD’07),",
      "citeRegEx" : "Hutter et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2007
    }, {
      "title" : "Automated configuration of mixed integer programming solvers",
      "author" : [ "F. Hutter", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "Proceedings of the Seventh International Conference on Integration of AI and OR Techniques in Constraint Programming (CPAIOR’10),",
      "citeRegEx" : "Hutter et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2010
    }, {
      "title" : "Tradeoffs in the empirical evaluation of competing algorithm designs",
      "author" : [ "F. Hutter", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "Annals of Mathematics and Artificial Intelligenc (AMAI), Special Issue on Learning and Intelligent Optimization,",
      "citeRegEx" : "Hutter et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2010
    }, {
      "title" : "Sequential model-based optimization for general algorithm configuration",
      "author" : [ "F. Hutter", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION’11),",
      "citeRegEx" : "Hutter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2011
    }, {
      "title" : "Parallel algorithm configuration",
      "author" : [ "F. Hutter", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "Proceedings of the Sixth International Conference on Learning and Intelligent Optimization (LION’12),",
      "citeRegEx" : "Hutter et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2012
    }, {
      "title" : "ParamILS: An automatic algorithm configuration framework",
      "author" : [ "F. Hutter", "H. Hoos", "K. Leyton-Brown", "T. Stützle" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Hutter et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2009
    }, {
      "title" : "Automatic algorithm configuration based on local search",
      "author" : [ "F. Hutter", "H. Hoos", "T. Stützle" ],
      "venue" : "Proceedings of the Twenty-second National Conference on Artificial Intelligence",
      "citeRegEx" : "Hutter et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2007
    }, {
      "title" : "The configurable SAT solver challenge (CSSC)",
      "author" : [ "F. Hutter", "M. Lindauer", "A. Balint", "S. Bayless", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Hutter et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2017
    }, {
      "title" : "AClib: a benchmark library for algorithm configuration",
      "author" : [ "F. Hutter", "M. López-Ibánez", "C. Fawcett", "M. Lindauer", "H. Hoos", "K. Leyton-Brown", "T. Stützle" ],
      "venue" : "Proceedings of the Eighth International Conference on Learning and Intelligent Optimization (LION’14),",
      "citeRegEx" : "Hutter et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2014
    }, {
      "title" : "Scaling and probabilistic smoothing: Efficient dynamic local search for SAT",
      "author" : [ "F. Hutter", "D. Tompkins", "H. Hoos" ],
      "venue" : "Proceedings of the international conference on Principles and Practice of Constraint Programming,",
      "citeRegEx" : "Hutter et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2002
    }, {
      "title" : "Algorithm runtime prediction: Methods and evaluation",
      "author" : [ "F. Hutter", "L. Xu", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Hutter et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2014
    }, {
      "title" : "ISAC - instance-specific algorithm configuration",
      "author" : [ "S. Kadioglu", "Y. Malitsky", "M. Sellmann", "K. Tierney" ],
      "venue" : "Proceedings of the Nineteenth European Conference on Artificial Intelligence",
      "citeRegEx" : "Kadioglu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kadioglu et al\\.",
      "year" : 2010
    }, {
      "title" : "The Art of Computer Programming, Volume IV",
      "author" : [ "D. Knuth" ],
      "venue" : "Addison-Wesley.",
      "citeRegEx" : "Knuth,? 2011",
      "shortCiteRegEx" : "Knuth",
      "year" : 2011
    }, {
      "title" : "Algorithm selection for combinatorial search problems: A survey",
      "author" : [ "L. Kotthoff" ],
      "venue" : "AI Magazine, 48–60.",
      "citeRegEx" : "Kotthoff,? 2014",
      "shortCiteRegEx" : "Kotthoff",
      "year" : 2014
    }, {
      "title" : "Autofolio: An automatically configured algorithm selector",
      "author" : [ "M. Lindauer", "H. Hoos", "F. Hutter", "T. Schaub" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Lindauer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lindauer et al\\.",
      "year" : 2015
    }, {
      "title" : "Pitfalls and best practices for algorithm configuration (breakout session report)",
      "author" : [ "M. Lindauer", "F. Hutter" ],
      "venue" : "Dagstuhl Reports,",
      "citeRegEx" : "Lindauer and Hutter,? \\Q2017\\E",
      "shortCiteRegEx" : "Lindauer and Hutter",
      "year" : 2017
    }, {
      "title" : "Performance variability in mixed-integer programming",
      "author" : [ "A. Lodi", "A. Tramontani" ],
      "venue" : "Theory Driven by Influential Applications,",
      "citeRegEx" : "Lodi and Tramontani,? \\Q2014\\E",
      "shortCiteRegEx" : "Lodi and Tramontani",
      "year" : 2014
    }, {
      "title" : "The irace package: Iterated racing for automatic algorithm configuration",
      "author" : [ "M. López-Ibáñez", "J. Dubois-Lacoste", "L.P. Caceres", "M. Birattari", "T. Stützle" ],
      "venue" : "Operations Research Perspectives,",
      "citeRegEx" : "López.Ibáñez et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "López.Ibáñez et al\\.",
      "year" : 2016
    }, {
      "title" : "Parallel SAT solver selection and scheduling",
      "author" : [ "Y. Malitsky", "A. Sabharwal", "H. Samulowitz", "M. Sellmann" ],
      "venue" : "Proceedings of the Eighteenth International Conference on Principles and Practice of Constraint Programming (CP’12),",
      "citeRegEx" : "Malitsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Malitsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Spybug: Automated bug detection in the configuration space of sat solvers",
      "author" : [ "N. Manthey", "M. Lindauer" ],
      "venue" : "Proceedings of the Nineteenth International Conference on Theory and Applications of Satisfiability Testing (SAT’16),",
      "citeRegEx" : "Manthey and Lindauer,? \\Q2016\\E",
      "shortCiteRegEx" : "Manthey and Lindauer",
      "year" : 2016
    }, {
      "title" : "Too many rooks",
      "author" : [ "N. Manthey", "P. Steinke" ],
      "venue" : "Proceedings of SAT Competition 2014: Solver and Benchmark Descriptions, Vol. B-2014-2 of Department of Computer Science Series of Publications B,",
      "citeRegEx" : "Manthey and Steinke,? \\Q2014\\E",
      "shortCiteRegEx" : "Manthey and Steinke",
      "year" : 2014
    }, {
      "title" : "SAT encoded low autocorrelation binary sequence (LABS) benchmark description",
      "author" : [ "F. Mugrauer", "A. Balint" ],
      "venue" : "Proceedings of SAT Competition 2013: Solver and Benchmark Descriptions,",
      "citeRegEx" : "Mugrauer and Balint,? \\Q2013\\E",
      "shortCiteRegEx" : "Mugrauer and Balint",
      "year" : 2013
    }, {
      "title" : "MiniSat HACK 999ED, MiniSat HACK 1430ED and SWDiA5BY",
      "author" : [ "C. Oh" ],
      "venue" : "Belov, A., Diepold, D., Heule, M., & Järvisalo, M. (Eds.), Proceedings of SAT Competition 2014: Solver and Benchmark Descriptions, Vol. B-2014-2 of Department of Computer Science Series of Publications B, p. 46. University of Helsinki.",
      "citeRegEx" : "Oh,? 2014",
      "shortCiteRegEx" : "Oh",
      "year" : 2014
    }, {
      "title" : "The algorithm selection problem",
      "author" : [ "J. Rice" ],
      "venue" : "Advances in Computers, 15, 65–118.",
      "citeRegEx" : "Rice,? 1976",
      "shortCiteRegEx" : "Rice",
      "year" : 1976
    }, {
      "title" : "Controlling a solver execution with the runsolver tool",
      "author" : [ "O. Roussel" ],
      "venue" : "Journal on Satisfiability, Boolean Modeling and Computation, 7 (4), 139–144.",
      "citeRegEx" : "Roussel,? 2011",
      "shortCiteRegEx" : "Roussel",
      "year" : 2011
    }, {
      "title" : "Quantifying homogeneity of instance sets for algorithm configuration",
      "author" : [ "M. Schneider", "H. Hoos" ],
      "venue" : "Proceedings of the Sixth International Conference on Learning and Intelligent Optimization (LION’12),",
      "citeRegEx" : "Schneider and Hoos,? \\Q2012\\E",
      "shortCiteRegEx" : "Schneider and Hoos",
      "year" : 2012
    }, {
      "title" : "Practical Bayesian optimization of machine learning algorithms",
      "author" : [ "J. Snoek", "H. Larochelle", "R.P. Adams" ],
      "venue" : "Proceedings of the 26th International Conference on Advances in Neural Information Processing Systems",
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    }, {
      "title" : "CryptoMiniSat v4",
      "author" : [ "M. Soos" ],
      "venue" : "Belov, A., Diepold, D., Heule, M., & Järvisalo, M. (Eds.), Proceedings of SAT Competition 2014: Solver and Benchmark Descriptions, Vol. B-2014-2 of Department of Computer Science Series of Publications B, p. 23. University of Helsinki.",
      "citeRegEx" : "Soos,? 2014",
      "shortCiteRegEx" : "Soos",
      "year" : 2014
    }, {
      "title" : "Ordered racing protocols for automatically configuring algorithms for scaling performance",
      "author" : [ "J. Styles", "H. Hoos" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference",
      "citeRegEx" : "Styles and Hoos,? \\Q2015\\E",
      "shortCiteRegEx" : "Styles and Hoos",
      "year" : 2015
    }, {
      "title" : "Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms",
      "author" : [ "C. Thornton", "F. Hutter", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’13),",
      "citeRegEx" : "Thornton et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Thornton et al\\.",
      "year" : 2013
    }, {
      "title" : "UBCSAT: An implementation and experimentation environment for SLS algorithms for SAT and MAX-SAT",
      "author" : [ "D. Tompkins", "H. Hoos" ],
      "venue" : "In Proceedings of the Seventh International Conference",
      "citeRegEx" : "Tompkins and Hoos,? \\Q2005\\E",
      "shortCiteRegEx" : "Tompkins and Hoos",
      "year" : 2005
    }, {
      "title" : "Automatic generation of efficient",
      "author" : [ "M. Springer-Verlag. Vallati", "C. Fawcett", "A. Gerevini", "H. Hoos", "A. Saetti" ],
      "venue" : null,
      "citeRegEx" : "Vallati et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Vallati et al\\.",
      "year" : 2013
    }, {
      "title" : "Hydra-MIP: Automated algorithm configuration",
      "author" : [ "F. Hutter", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "SAT. Journal of Artificial Intelligence Research,",
      "citeRegEx" : "L. et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "L. et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "The AI community has recently developed automated methods for the resulting algorithm configuration (AC) problem (Hutter et al., 2009) to replace tedious, irreproducible and errorprone manual parameter tuning.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : ", 2007, 2017), maximum satisfiability (Ansótegui et al., 2016), scheduling (Chiarandini et al.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : ", 2016), scheduling (Chiarandini et al., 2008), mixed integer programming (Hutter et al.",
      "startOffset" : 20,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : ", 2010a), evolutionary algorithms (Bezerra et al., 2016), answer set solving (Gebser et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : ", 2016), answer set solving (Gebser et al., 2011), AI planning (Vallati et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 55,
      "context" : ", 2011), AI planning (Vallati et al., 2013) and machine learning (Thornton et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 53,
      "context" : ", 2013) and machine learning (Thornton et al., 2013; Feurer et al., 2015).",
      "startOffset" : 29,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : ", 2013) and machine learning (Thornton et al., 2013; Feurer et al., 2015).",
      "startOffset" : 29,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : ", SpySMAC (Falkner et al., 2015)), we still often observe fundamental issues in the design and execution of experiments with algorithm configuration methods by both experts and new users.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "A concrete example for this algorithm configuration problem would be to find a parameter setting θ ∈ Θ of a solver A for the propositional satisfiability problem (SAT) (such as glucose (Audemard & Simon, 2009) or lingeling (Biere, 2013)) on a set of CNF instances Π (e.",
      "startOffset" : 223,
      "endOffset" : 236
    }, {
      "referenceID" : 50,
      "context" : "Another example would be to find a hyperparameter setting for a machine learning algorithm that minimizes its error c on a given dataset (Snoek et al., 2012; Feurer et al., 2015); in this latter example, error could either be measured via k-fold inner crossvalidation (giving rise to k instances for algorithm configuration), or a single validation set (in which case there is just a single instance for algorithm configuration).",
      "startOffset" : 137,
      "endOffset" : 178
    }, {
      "referenceID" : 16,
      "context" : "Another example would be to find a hyperparameter setting for a machine learning algorithm that minimizes its error c on a given dataset (Snoek et al., 2012; Feurer et al., 2015); in this latter example, error could either be measured via k-fold inner crossvalidation (giving rise to k instances for algorithm configuration), or a single validation set (in which case there is just a single instance for algorithm configuration).",
      "startOffset" : 137,
      "endOffset" : 178
    }, {
      "referenceID" : 29,
      "context" : "ParamILS (Hutter et al., 2009) uses local search in the configuration space, employing a racing strategy to decide which of two configurations performs better without running both of them on all instances.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 41,
      "context" : "irace (López-Ibáñez et al., 2016) uses iterative races via F-race (Birattari et al.",
      "startOffset" : 6,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : ", 2016) uses iterative races via F-race (Birattari et al., 2002) on a set of picked configurations to determine the best one.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 27,
      "context" : "SMAC (Hutter et al., 2011) and its distributed version dSMAC (Hutter et al.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 28,
      "context" : ", 2011) and its distributed version dSMAC (Hutter et al., 2012)) use probabilistic models of algorithm performance, so-called empirical performance models (Hutter et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 32,
      "context" : ", 2012)) use probabilistic models of algorithm performance, so-called empirical performance models (Hutter et al., 2014), to guide the search for good configurations by means of an extension of Bayesian Optimization (Brochu et al.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : ", 2014), to guide the search for good configurations by means of an extension of Bayesian Optimization (Brochu et al., 2010).",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "GGA (Ansótegui et al., 2009) represents parameters as genes and uses a genetic algorithm with a competitive and a non-competitive gender; its newest version GGA++ (Ansótegui et al.",
      "startOffset" : 4,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : ", 2009) represents parameters as genes and uses a genetic algorithm with a competitive and a non-competitive gender; its newest version GGA++ (Ansótegui et al., 2015) also uses an empirical performance model for guidance.",
      "startOffset" : 142,
      "endOffset" : 166
    }, {
      "referenceID" : 31,
      "context" : "For a more detailed description of these algorithms, we refer the interested reader to the original papers or to the report of the Configurable SAT Solver Challenge (Hutter et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 186
    }, {
      "referenceID" : 29,
      "context" : "If the cost metric c is running time, several configurators use an adaptive capping strategy (Hutter et al., 2009) to terminate slow algorithm runs prematurely to save time.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 48,
      "context" : "This wrapper supports the input format of the most popular configurators ParamILS , GGA, irace and SMAC and automatically limits the computational resources of the target algorithm run with the runsolver tool developed by Roussel (2011). The user only needs to implement two target algorithm-specific functions:",
      "startOffset" : 222,
      "endOffset" : 237
    }, {
      "referenceID" : 32,
      "context" : "In this and the next section, we describe common pitfalls in algorithm configuration and illustrate their consequences on existing benchmarks from the algorithm configuration library AClib (Hutter et al., 2014)3.",
      "startOffset" : 189,
      "endOffset" : 210
    }, {
      "referenceID" : 27,
      "context" : "Throughout, we will use the state-of-the-art configurator SMAC (Hutter et al., 2011) as an example, typically optimizing penalized average running time (PAR-10), which counts",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 21,
      "context" : "An alternative to a general wrapper would be a programming language-specific reliable interface for the communication between configurator and target algorithm (Hoos, 2012), which would make it easier for users to apply algorithm configuration to new target algorithms.",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "For example, Hutter et al. (2010a) reported finding bugs in mixed integer programming solvers and Manthey and Lindauer (2016) bugs in SAT solvers.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 24,
      "context" : "For example, Hutter et al. (2010a) reported finding bugs in mixed integer programming solvers and Manthey and Lindauer (2016) bugs in SAT solvers.",
      "startOffset" : 13,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : "For solvers that output unsatisfiability proofs, there are also effective tools for checking these proofs (Heule et al., 2014).",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Example The latter (quite subtle) issue actually happened in a recent publication that compared GGA++ and SMAC , in which a wrapper bug caused SMAC to perform poorly (Ansótegui et al., 2015).",
      "startOffset" : 166,
      "endOffset" : 190
    }, {
      "referenceID" : 51,
      "context" : "Figure 3 shows the performance achieved when SMAC is run with either wrapper to configure Cryptominisat (Soos, 2014) for penalized average running time (PAR-10) to solve Circuit Fuzz instances (Brummayer et al.",
      "startOffset" : 104,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "Figure 3 shows the performance achieved when SMAC is run with either wrapper to configure Cryptominisat (Soos, 2014) for penalized average running time (PAR-10) to solve Circuit Fuzz instances (Brummayer et al., 2012) as used in the CSSC 2014 (Hutter et al.",
      "startOffset" : 193,
      "endOffset" : 217
    }, {
      "referenceID" : 31,
      "context" : ", 2012) as used in the CSSC 2014 (Hutter et al., 2017).",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "In contrast to SMAC , GGA++ does not require a wrapper; in the experiments by Ansótegui et al. (2015), GGA++ directly sent its KILL signal to the target algorithm and therefore did not suffer from the same problem SMAC suffered from, which confounded the paper’s comparison between GGA++ and SMAC .",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 32,
      "context" : "For studies comparing configurators, it is also paramount to use tried-and-tested publicly available benchmark scenarios (lowering the riks of typos, etc); our algorithm configuration benchmark library AClib (Hutter et al., 2014) provides a very broad collection of such benchmarks.",
      "startOffset" : 208,
      "endOffset" : 229
    }, {
      "referenceID" : 10,
      "context" : "A common issue in applying algorithm configuration is the over-tuning effect (Birattari, 2004; Hutter et al., 2007).",
      "startOffset" : 77,
      "endOffset" : 115
    }, {
      "referenceID" : 24,
      "context" : "A common issue in applying algorithm configuration is the over-tuning effect (Birattari, 2004; Hutter et al., 2007).",
      "startOffset" : 77,
      "endOffset" : 115
    }, {
      "referenceID" : 33,
      "context" : "Example To illustrate over-tuning to a random seed in its purest form, independent of a difference between training and test instances, we optimized the parameters of the local-search SAT solver Saps (Hutter et al., 2002) on a single instance, the only difference between training and test being the set of random seeds used.",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 18,
      "context" : "Clasp on N-Rooks We studied running time of the solver Clasp (Gebser et al., 2012) on N-Rooks instances (Manthey & Steinke, 2014), a benchmark from the Configurable SAT Solver Challenge (CSSC 2014; Hutter et al.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "Clasp on N-Rooks We studied running time of the solver Clasp (Gebser et al., 2012) on N-Rooks instances (Manthey & Steinke, 2014), a benchmark from the Configurable SAT Solver Challenge (CSSC 2014; Hutter et al. (2017)).",
      "startOffset" : 62,
      "endOffset" : 219
    }, {
      "referenceID" : 8,
      "context" : "(2015) in which they optimized Lingeling (Biere, 2014) on a mixed set of industrial SAT instances.",
      "startOffset" : 41,
      "endOffset" : 54
    }, {
      "referenceID" : 42,
      "context" : "The benchmark’s heterogeneity and the corresponding mismatch between training and test set make this benchmark poorly suited for applying algorithm configuration and call for portfolio approaches (Xu et al., 2008; Malitsky et al., 2012; Lindauer et al., 2015) that can handle heterogeneous instances.",
      "startOffset" : 196,
      "endOffset" : 259
    }, {
      "referenceID" : 38,
      "context" : "The benchmark’s heterogeneity and the corresponding mismatch between training and test set make this benchmark poorly suited for applying algorithm configuration and call for portfolio approaches (Xu et al., 2008; Malitsky et al., 2012; Lindauer et al., 2015) that can handle heterogeneous instances.",
      "startOffset" : 196,
      "endOffset" : 259
    }, {
      "referenceID" : 1,
      "context" : "Lingeling on mixed SAT We reconstructed a benchmark from Ansótegui et al. (2015) in which they optimized Lingeling (Biere, 2014) on a mixed set of industrial SAT instances.",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 31,
      "context" : "We therefore analyzed this benchmark further (Hutter et al., 2017), showing that twice as many SMAC runs with a fivefold larger configuration budget managed to improve training performance slightly.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "Unfortunately, this does not hold for all kind of algorithms – for example, the performance of solvers for SAT (Aigner et al., 2013) and mixed integer programming (Lodi & Tramontani, 2014) can depend strongly on the used machine type (including hardware, operating system and installed software libraries)",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : "Geschwender et al. (2014) showed in a preliminary study that it is possible in principle to configure algorithms in the cloud and for the found configurations to perform well on another machine.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "Example An example for such machine-dependent algorithms are SAT solvers that are often highly optimized against cache misses (Aigner et al., 2013).",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 31,
      "context" : "To study the effect of different machines, we optimized three SAT solvers from the configurable SAT solver challenge (Hutter et al., 2017), namely Minisat-HACK-999ED (Oh, 2014), Clasp (Gebser et al.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 46,
      "context" : ", 2017), namely Minisat-HACK-999ED (Oh, 2014), Clasp (Gebser et al.",
      "startOffset" : 35,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : ", 2017), namely Minisat-HACK-999ED (Oh, 2014), Clasp (Gebser et al., 2012) and Lingeling (Biere, 2014) on Circuit Fuzz instances (Brummayer et al.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : ", 2012) and Lingeling (Biere, 2014) on Circuit Fuzz instances (Brummayer et al.",
      "startOffset" : 22,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : ", 2012) and Lingeling (Biere, 2014) on Circuit Fuzz instances (Brummayer et al., 2012).",
      "startOffset" : 62,
      "endOffset" : 86
    }, {
      "referenceID" : 21,
      "context" : "For general empirical algorithmics, Hoos (2017) recommends further best practices, including design, reports and analysis of computational experiments.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "To obtain trivial parallelization of randomized configuration procedures, we recommend to run n independent configuration runs and use the training set to select the best of the n resulting configurations (Hutter et al., 2012).",
      "startOffset" : 205,
      "endOffset" : 226
    }, {
      "referenceID" : 22,
      "context" : "Using training instances from a range of hardness can also often help yield configurations that generalize (Hoos et al., 2013).",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "To select a good training instance set, Bayless et al. (2014) proposed a way to quantify whether an instance set is a good proxy for another instance set.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "To select a good training instance set, Bayless et al. (2014) proposed a way to quantify whether an instance set is a good proxy for another instance set. Furthermore, Styles and Hoos (2015) proposed a splitting strategy of the instances for better scaling to hard instances.",
      "startOffset" : 40,
      "endOffset" : 191
    }, {
      "referenceID" : 47,
      "context" : "However, we know from algorithm selection (Rice, 1976; Kotthoff, 2014) that often no single configuration exists that performs well for all instances in a heterogeneous set, but a portfolio of configurations is required to obtain good performance (Xu et al.",
      "startOffset" : 42,
      "endOffset" : 70
    }, {
      "referenceID" : 37,
      "context" : "However, we know from algorithm selection (Rice, 1976; Kotthoff, 2014) that often no single configuration exists that performs well for all instances in a heterogeneous set, but a portfolio of configurations is required to obtain good performance (Xu et al.",
      "startOffset" : 42,
      "endOffset" : 70
    }, {
      "referenceID" : 35,
      "context" : "However, we know from algorithm selection (Rice, 1976; Kotthoff, 2014) that often no single configuration exists that performs well for all instances in a heterogeneous set, but a portfolio of configurations is required to obtain good performance (Xu et al., 2011; Kadioglu et al., 2010).",
      "startOffset" : 247,
      "endOffset" : 287
    }, {
      "referenceID" : 31,
      "context" : "tion space (larger configuration spaces require more time to search effectively (Hutter et al., 2017)).",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "While this protocol was not used in early works11, it has been used in more recent evaluations (Ansótegui et al., 2015; Hutter et al., 2017).",
      "startOffset" : 95,
      "endOffset" : 140
    }, {
      "referenceID" : 31,
      "context" : "While this protocol was not used in early works11, it has been used in more recent evaluations (Ansótegui et al., 2015; Hutter et al., 2017).",
      "startOffset" : 95,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : ", Hutter et al. (2012)).",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 53,
      "context" : "With SMAC , we have successfully optimized configuration spaces with hundreds of parameters (Thornton et al., 2013).",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "For example, the state-of-the-art SAT solver Lingeling (Biere, 2013) has more than 300 parameters and most of them have a value range between 0 and 32bit maxint, but most of these parameters are either not really relevant for optimizing Lingeling ’s running time or the relevant value ranges are much smaller.",
      "startOffset" : 55,
      "endOffset" : 68
    }, {
      "referenceID" : 21,
      "context" : "Hutter et al. (2011) only used a single core per run of GGA, but still followed the protocol by Ansótegui et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "(2011) only used a single core per run of GGA, but still followed the protocol by Ansótegui et al. (2009) to race groups of 8 runs in parallel per core; therefore, GGA’s adaptive capping mechanism was the same in that work as in Ansótegui et al.",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "(2011) only used a single core per run of GGA, but still followed the protocol by Ansótegui et al. (2009) to race groups of 8 runs in parallel per core; therefore, GGA’s adaptive capping mechanism was the same in that work as in Ansótegui et al. (2009).",
      "startOffset" : 82,
      "endOffset" : 253
    }, {
      "referenceID" : 36,
      "context" : "One solution for this case is to measure elementary operations, such as search steps of a local search algorithm or MEMS (number of memory accesses (Knuth, 2011)); however, it has to be ensured that such proxy metrics correlate well with running time.",
      "startOffset" : 148,
      "endOffset" : 161
    }, {
      "referenceID" : 32,
      "context" : "The only existing library of such benchmarks we are aware of is the algorithm configuration library AClib (Hutter et al., 2014), which comprises 326 benchmarks in Version 1.",
      "startOffset" : 106,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "We thank Manuel López-Ibáñez and Kevin Tierney for adapting the interfaces of irace and GGA to work together with GenericWrapper4AC, Yuri Malitsky and Horst Samulowitz for providing the wrappers and benchmarks of Ansótegui et al. (2015), and Kevin Tierney, Manuel López-Ibáñez and Lars Kotthoff for very helpful feedback on the first draft of the paper that led to the inclusion of some further possible issues.",
      "startOffset" : 213,
      "endOffset" : 237
    }, {
      "referenceID" : 1,
      "context" : "We thank Manuel López-Ibáñez and Kevin Tierney for adapting the interfaces of irace and GGA to work together with GenericWrapper4AC, Yuri Malitsky and Horst Samulowitz for providing the wrappers and benchmarks of Ansótegui et al. (2015), and Kevin Tierney, Manuel López-Ibáñez and Lars Kotthoff for very helpful feedback on the first draft of the paper that led to the inclusion of some further possible issues. Some of the recommendations in Section 5 were inspired by a discussion at a Dagstuhl seminar (see Lindauer and Hutter (2017) for more details) and we are thankful for the valuable contributions of the attendees of that discussion: Aymeric Blot, Wanru Gao, Holger Hoos, Laetitia Jourdan, Lars Kotthoff, Manuel López-Ibáñez, Nysret Musliu, Günter Rudolph, Marc Schoenauer, Thomas Stützle and Joaquin Vanschoren.",
      "startOffset" : 213,
      "endOffset" : 537
    } ],
    "year" : 2017,
    "abstractText" : "Good parameter settings are crucial to achieve high performance in many areas of artificial intelligence (AI), such as satisfiability solving, AI planning, scheduling, and machine learning (in particular deep learning). Automated algorithm configuration methods have recently received much attention in the AI community since they replace tedious, irreproducible and error-prone manual parameter tuning and can lead to new state-of-the-art performance. However, practical applications of algorithm configuration are prone to several (often subtle) pitfalls in the experimental design that can render the procedure ineffective. We identify several common issues and propose best practices for avoiding them, including a tool called GenericWrapper4AC for preventing the many possible problems in measuring the performance of the algorithm being optimized by executing it in a standardized, controlled manner.",
    "creator" : "TeX"
  }
}