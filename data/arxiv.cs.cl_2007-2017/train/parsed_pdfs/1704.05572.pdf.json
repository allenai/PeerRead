{
  "name" : "1704.05572.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Answering Complex Questions Using Open Information Extraction",
    "authors" : [ "Tushar Khot", "Ashish Sabharwal", "Peter Clark" ],
    "emails" : [ "tushark@allenai.org", "ashishs@allenai.org", "peterc@allenai.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Effective question answering (QA) systems have been a long-standing quest of AI research. Structured curated KBs have been used successfully for this task (Berant et al., 2013; Berant and Liang, 2014). However, these KBs are expensive to build and typically domain-specific. Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices (Fader et al., 2014; Yin et al., 2015).\nOur goal in this work is to develop a QA system that can perform reasoning with Open IE (Banko\net al., 2007) tuples for complex multiple-choice questions that require tuples from multiple sentences. Such a system can answer complex questions in resource-poor domains where curated knowledge is unavailable. Elementary-level science exams is one such domain, requiring complex reasoning (Clark, 2015). Due to the lack of a large-scale structured KB, state-of-the-art systems for this task either rely on shallow reasoning with large text corpora (Clark et al., 2016; Cheng et al., 2016) or deeper, structured reasoning with a small amount of automatically acquired (Khot et al., 2015) or manually curated (Khashabi et al., 2016) knowledge.\nConsider the following question from an Alaska state 4th grade science test:\nWhich object in our solar system reflects light and is a satellite that orbits around one planet? (A) Earth (B) Mercury (C) the Sun (D) the Moon\nThis question is challenging for QA systems because of its complex structure and the need for multi-fact reasoning. A natural way to answer it is by combining facts such as (Moon; is; in the solar system), (Moon; reflects; light), (Moon; is; satellite), and (Moon; orbits; around one planet).\nA candidate system for such reasoning, and which we draw inspiration from, is the TABLEILP system of Khashabi et al. (2016). TABLEILP treats QA as a search for an optimal subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP). We similarly want to search for an optimal subgraph. However, a large, automatically extracted tuple KB makes the reasoning context different on three fronts: (a) unlike reasoning with tables, chaining tuples is less important and reliable as join rules aren’t\nar X\niv :1\n70 4.\n05 57\n2v 1\n[ cs\n.A I]\n1 9\nA pr\n2 01\n7\navailable; (b) conjunctive evidence becomes paramount, as, unlike a long table row, a single tuple is less likely to cover the entire question; and (c) again, unlike table rows, tuples are noisy, making combining redundant evidence essential. Consequently, a table-knowledge centered inference model isn’t the best fit for noisy tuples.\nTo address this challenge, we present a new ILP-based model of inference with tuples, implemented in a reasoner called TUPLEINF. We demonstrate that TUPLEINF significantly outperforms TABLEILP by 11.8% on a broad set of over 1,300 science questions, without requiring manually curated tables, using a substantially simpler ILP formulation, and generalizing well to higher grade levels. The gains persist even when both solvers are provided identical knowledge. This demonstrates for the first time how Open IE based QA can be extended from simple lookup questions to an effective system for complex questions."
    }, {
      "heading" : "2 Related Work",
      "text" : "We discuss two classes of related work: retrievalbased web question-answering (simple reasoning with large scale KB) and science questionanswering (complex reasoning with small KB).\nWeb QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002). While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data. QA systems using semistructured Open IE tuples (Fader et al., 2013, 2014; Yin et al., 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query.\nScience QA: Elementary-level science QA tasks require reasoning to handle complex questions. Markov Logic Networks (Richardson and Domingos, 2006) have been used to perform probabilistic reasoning over a small set of logical rules (Khot et al., 2015). Simple IR techniques have also been proposed for science tests (Clark et al., 2016) and Gaokao tests (equivalent to the SAT exam in China) (Cheng et al., 2016).\nThe work most related to TUPLEINF is the aforementioned TABLEILP solver. This approach focuses on building inference chains using man-\nually defined join rules for a small set of curated tables. While it can also use open vocabulary tuples (as we assess in our experiments), its efficacy is limited by the difficulty of defining reliable join rules for such tuples. Further, each row in some complex curated tables covers all relevant contextual information (e.g., each row of the adaptation table contains (animal, adaptation, challenge, explanation)), whereas recovering such information requires combining multiple Open IE tuples."
    }, {
      "heading" : "3 Tuple Inference Solver",
      "text" : "We first describe the tuples used by our solver. We define a tuple as (subject; predicate; objects) with zero or more objects. We refer to the subject, predicate, and objects as the fields of the tuple."
    }, {
      "heading" : "3.1 Tuple KB",
      "text" : "We use the text corpora (S) from Clark et al. (2016) to build our tuple KB. For each test set, we use the corresponding training questions Qtr to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question (q, A) ∈ Qtr and each choice a ∈ A, we use all non-stopword tokens in q and a as an ElasticSearch1 query against S. We take the top 200 hits, run Open IE v4,2 and aggregate the resulting tuples over all a ∈ A and over all questions in Qtr to create the tuple KB (T).3"
    }, {
      "heading" : "3.2 Tuple Selection",
      "text" : "Given a multiple-choice question qa with question text q and answer choices A={ai}, we select the most relevant tuples from T and S as follows.\nSelecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens tok(qa).4. We also filter out any tuples that overlap only with tok(q) as they do not support any answer. We compute the normalized TF-IDF score treating the question, q as a query and each tuple, t as a document:\ntf(x, q) = 1 if x ∈ q; idf(x) = log(1 +N/nx) tf-idf(t, q) = ∑ x∈t∩q idf(x)\n1https://www.elastic.co/products/elasticsearch 2http://knowitall.github.io/openie 3Available at http://anonymized 4All tokens are stemmed and stop-word filtered\nwhere N is the number of tuples in the KB and nx are the number of tuples containing x. We normalize the tf-idf score by the number of tokens in t and q. We finally take the 50 top-scoring tuples Tqa\n5. On-the-fly tuples from text: To handle questions from new domains not covered by the training set, we extract additional tuples on the fly from S (similar to Sharma et al. (2015)). We perform the same ElasticSearch query described earlier for building T. We ignore sentences that cover none or all answer choices as they are not discriminative. We also ignore long sentences (>300 characters) and sentences with negation6 as they tend to lead to noisy inference. We then run Open IE on these sentences and re-score the resulting tuples using the Jaccard score7 due to the lossy nature of Open IE, and finally take the 50 top-scoring tuples T ′qa."
    }, {
      "heading" : "3.3 Support Graph Search",
      "text" : "Similar to TABLEILP, we view the QA task as searching for a graph that best connects the terms in the question (qterms) with an answer choice via the knowledge; see Figure 1 for a simple illustrative example. Unlike standard alignment models used for tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2010), however, we must score alignments between a set Tqa ∪ T ′qa of structured tuples and a (potentially multi-sentence) multiple-choice question qa.\nThe qterms, answer choices, and tuples fields form the set of possible vertices, V , of the support graph. Edges connecting qterms to tuple fields and tuple fields to answer choices form the set of possible edges, E . The support graph, G(V,E), is a subgraph of G(V, E) where V and E denote “active” nodes and edges, resp. We define the desired behavior of an optimal support graph via an ILP\n5Available at http://allenai.org/data.html 6containing not, ’nt, or except 7| tok(t) ∩ tok(qa) | / | tok(t) ∪ tok(qa) |\nmodel as follows8.\nObjective Function Similar to TABLEILP, we score the support graph based on the weight of the active nodes and edges. Each edge e(t, h) is weighted based on a wordoverlap score.9 While TABLEILP used WordNet (Miller, 1995) paths to compute the weight, this measure results in unreliable scores when faced with longer phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that match irrelevant parts of the questions. To mitigate this issue, we improve the scoring of qterms in our ILP objective to focus on important terms. Since the later terms in a question tend to provide the most critical information, we scale qterm coefficients based on their position. Also, qterms that appear in almost all of the selected tuples tend not to be discriminative as any tuple would support such a qterm. Hence we scale the coefficients by the inverse frequency of the tokens in the selected tuples.\nConstraints Since Open IE tuples do not come with schema and join rules, we can define a substantially simpler model compared to TABLEILP. This reduces the reasoning capability but also eliminates the reliance on hand-authored join rules and regular expressions used in TABLEILP. We discovered (see empirical evaluation) that this simple model can achieve the same score as TABLEILP on the Regents test (target test set used by TABLEILP) and generalizes better to different grade levels.\nWe define active vertices and edges using ILP constraints: an active edge must connect two active vertices and an active vertex must have at least one active edge. To avoid positive edge coeffi-\n8c.f. Appendix A for more details 9w(t, h) = |tok(t) ∩ tok(h)|/|tok(h)|\ncients in the objective function resulting in spurious edges in the support graph, we limit the number of active edges from an active tuple, question choice, tuple fields, and qterms (first group of constraints in Table 1). Our model is also capable of using multiple tuples to support different parts of the question as illustrated in Figure 1. To avoid spurious tuples that only connect with the question (or choice) or ignore the relation being expressed in the tuple, we add constraints that require each tuple to connect a qterm with an answer choice (second group of constraints in Table 1).\nWe also define new constraints based on the Open IE tuple structure. Since an Open IE tuple expresses a fact about the tuple’s subject, we require the subject to be active in the support graph. To avoid issues such as (Planet; orbit; Sun) matching the sample question in the introduction (“Which object. . .orbits around a planet”), we also add an ordering constraint (third group in Table 1).\nIts worth mentioning that TUPLEINF only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
    }, {
      "heading" : "4 Experiments",
      "text" : "Comparing our method with two state-of-the-art systems for 4th and 8th grade science exams, we demonstrate that (a) TUPLEINF with only automatically extracted tuples significantly outperforms TABLEILP with its original curated knowledge as well as with additional tuples, and (b) TUPLEINF’s complementary approach to IR leads to an improved ensemble. Numbers in bold indicate statistical significance based on the Binomial exact test (Howell, 2012) at p = 0.05.\nWe consider two question sets. (1) 4th Grade set (1220 train, 1304 test) is a 10x larger superset of the NY Regents questions (Clark et al., 2016), and includes professionally written licensed questions. (2) 8th Grade set (293 train, 282 test) contains 8th grade questions from various states.10\nWe consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted 80K sentences and 280 GB of plain text extracted from\n10http://allenai.org/data/science-exam-questions.html\nweb pages used by Clark et al. (2016). This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples T ′qa. Additionally, TABLEILP uses∼70 Curated tables (C) designed for 4th grade NY Regents exams.\nWe compare TUPLEINF with two state-of-theart baselines. IR is a simple yet powerful information-retrieval baseline (Clark et al., 2016) that selects the answer option with the best matching sentence in a corpus. TABLEILP is the stateof-the-art structured inference baseline (Khashabi et al., 2016) developed for science questions."
    }, {
      "heading" : "4.1 Results",
      "text" : "Table 2 shows that TUPLEINF, with no curated knowledge, outperforms TABLEILP on both question sets by more than 11%. The lower half of the table shows that even when both solvers are given the same knowledge (C+T),11 the improved selection and simplified model of TUPLEINF12 results in a statistically significant improvement. Our simple model, TUPLEINF(C + T), also achieves scores comparable to TABLEILP on the latter’s target Regents questions (61.4% vs TABLEILP’s reported 61.5%) without any specialized rules.\nTable 3 shows that while TUPLEINF achieves similar scores as the IR solver, the approaches are complementary (structured lossy knowledge reasoning vs. lossless sentence retrieval). The two solvers, in fact, differ on 47.3% of the training questions. To exploit this complementarity,\n11See Appendix B for how tables (and tuples) are used by TUPLEINF (and TABLEILP).\n12On average, TABLEILP (TUPLEINF) has 3,403 (1,628, resp.) constraints and 982 (588, resp.) variables. TUPLEINF’s ILP can be solved in half the time taken by TABLEILP, resulting in 68.6% reduction in overall question answering time.\nwe train an ensemble system (Clark et al., 2016) which, as shown in the table, provides a substantial boost over the individual solvers. Further, IR + TUPLEINF is consistently better than IR + TABLEILP. Finally, in combination with IR and the statistical association based PMI solver (that scores 54.1% by itself) of Clark et al. (2016), TUPLEINF achieves a score of 58.2% as compared to TABLEILP’s ensemble score of 56.7% on the 4th grade set, again attesting to TUPLEINF’s strength."
    }, {
      "heading" : "5 Error Analysis",
      "text" : "We describe four classes of failures that we observed, and the future work they suggest.\nMissing Important Words: Which material will spread out to completely fill a larger container? (A)air (B)ice (C)sand (D)water In this question, we have tuples that support water will spread out and fill a larger container but miss the critical word “completely”. An approach capable of detecting salient question words could help avoid that.\nLossy IE: Which action is the best method to separate a mixture of salt and water? ... The IR solver correctly answers this question by using the sentence: Separate the salt and water mixture by evaporating the water. However, TUPLEINF is not able to answer this question as Open IE is unable to extract tuples from this imperative sentence. While the additional structure from Open IE is useful for more robust matching, converting sentences to Open IE tuples may lose important bits of information.\nBad Alignment: Which of the following gases is necessary for humans to breathe in order to live?(A) Oxygen(B) Carbon dioxide(C) Helium(D) Water vapor TUPLEINF returns “Carbon dioxide” as the answer because of the tuple (humans; breathe out; carbon dioxide). The chunk “to breathe” in the question has a high alignment score to the “breathe out” relation in the tuple even though they have completely different meanings. Improving the phrase alignment can mitigate this issue.\nOut of scope: Deer live in forest for shelter. If the forest was cut down, which situation would most likely happen?... Such questions that require modeling a state presented in the question and reasoning over the state are out of scope of our solver."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a new QA system, TUPLEINF, that can reason over a large, potentially noisy tuple KB to answer complex questions. Our results show that TUPLEINF is a new state-of-the-art structured solver for elementary-level science that does not rely on curated knowledge and generalizes to higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context and distributional measures."
    }, {
      "heading" : "A Appendix: ILP Model Details",
      "text" : "To build the ILP model, we first need to get the questions terms (qterm) from the question by chunking the question using an in-house chunker based on the postagger from FACTORIE. 13\nVariables The ILP model has an active vertex variable for each qterm (xq), tuple (xt), tuple field (xf ) and question choice (xa). Table 4 describes the coefficients of these active variables. For example, the coefficient of each qterm is a constant value (0.8) scaled by three boosts. The idf boost, idfB for a qterm, x is calculated as log(1 + (|Tqa| + |T ′qa|)/nx) where nx is the number of tuples in Tqa ∪ T ′qa containing x. The science term boost, scienceB boosts coefficients of qterms that are valid science terms based on a list of 9K terms. The location boost, locB of a qterm at index i in the question is given by i/tok(q) (where i=1 for the first term).\nSimilarly each edge, e has an associated active edge variable with the word overlap score as its coefficient, ce. For efficiency, we only create qterm→field edge and field→choice edge if the coefficient is greater than a certain threshold (0.1 and 0.2, respectively). Finally the objective function of our ILP model can be written as:∑\nq∈qterms cqxq + ∑ t∈tuples ctxt + ∑ e∈edges cexe\nDescription Var. Coefficient (c)\nQterm xq 0.8·idfB·scienceB·locB Tuple xt -1 + jaccardScore(t, qa) Tuple Field xf 0 Choice xa 0\nTable 4: Coefficients for active variables.\nConstraints Next we describe the constraints in our model. We have basic definitional constraints over the active variables.\nActive variable must have an active edge Active edge must have an active source node Active edge must have an active target node Exactly one answer choice must be active Active field implies tuple must be active\n13http://factorie.cs.umass.edu/\nApart from the constraints described in Table 1, we also use the which-term boosting constraints defined by TABLEILP (Eqns. 44 and 45 in Table 13 (Khashabi et al., 2016)). As described in Section B, we create a tuple from table rows by setting pairs of cells as the subject and object of a tuple. For these tuples, apart from requiring the subject to be active, we also require the object of the tuple. This would be equivalent to requiring at least two cells of a table row to be active."
    }, {
      "heading" : "B Experiment Details",
      "text" : "We use the SCIP ILP optimization engine (Achterberg, 2009) to optimize our ILP model. To get the score for each answer choice ai, we force the active variable for that choice xai to be one and use the objective function value of the ILP model as the score. For evaluations, we use a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM. To evaluate TABLEILP and TUPLEINF on curated tables and tuples, we converted them into the expected format of each solver as follows.\nB.1 Using curated tables with TUPLEINF For each question, we select the 7 best matching tables using the tf-idf score of the table w.r.t. the question tokens and top 20 rows from each table using the Jaccard similarity of the row with the question. (same as Khashabi et al. (2016)). We then convert the table rows into the tuple structure using the relations defined by TABLEILP. For every pair of cells connected by a relation, we create a tuple with the two cells as the subject and primary object with the relation as the predicate. The other cells of the table are used as additional objects to provide context to the solver. We pick top-scoring 50 tuples using the Jaccard score.\nB.2 Using Open IE tuples with TABLEILP We create an additional table in TABLEILP with all the tuples in T . Since TABLEILP uses fixed-length (subject; predicate; object) triples, we need to map tuples with multiple objects to this format. For each object, Oi in the input Open IE tuple (S;P ;O1;O2 . . .), we add a triple (S;P ;Oi) to this table."
    } ],
    "references" : [ {
      "title" : "SCIP: solving constraint integer programs",
      "author" : [ "Tobias Achterberg." ],
      "venue" : "Math. Prog. Computation 1(1):1–",
      "citeRegEx" : "Achterberg.,? 2009",
      "shortCiteRegEx" : "Achterberg.",
      "year" : 2009
    }, {
      "title" : "Open information extraction from the web",
      "author" : [ "Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Banko et al\\.,? 2007",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2007
    }, {
      "title" : "Semantic parsing on Freebase from question-answer pairs",
      "author" : [ "J. Berant", "A. Chou", "R. Frostig", "P. Liang." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Semantic parsing via paraphrasing",
      "author" : [ "Jonathan Berant", "Percy Liang." ],
      "venue" : "ACL.",
      "citeRegEx" : "Berant and Liang.,? 2014",
      "shortCiteRegEx" : "Berant and Liang.",
      "year" : 2014
    }, {
      "title" : "An analysis of the AskMSR question-answering system",
      "author" : [ "Eric Brill", "Susan Dumais", "Michele Banko." ],
      "venue" : "Proceedings of EMNLP. pages 257–264.",
      "citeRegEx" : "Brill et al\\.,? 2002",
      "shortCiteRegEx" : "Brill et al\\.",
      "year" : 2002
    }, {
      "title" : "Taking up the gaokao challenge: An information retrieval approach",
      "author" : [ "Gong Cheng", "Weixi Zhu", "Ziwei Wang", "Jianghui Chen", "Yuzhong Qu." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Elementary school science and math tests as a driver for AI: take the Aristo challenge! In 29th AAAI/IAAI",
      "author" : [ "Peter Clark." ],
      "venue" : "Austin, TX, pages 4019–4021.",
      "citeRegEx" : "Clark.,? 2015",
      "shortCiteRegEx" : "Clark.",
      "year" : 2015
    }, {
      "title" : "Combining retrieval, statistics, and inference to answer elementary science questions",
      "author" : [ "Peter Clark", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Turney", "Daniel Khashabi." ],
      "venue" : "30th AAAI.",
      "citeRegEx" : "Clark et al\\.,? 2016",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2016
    }, {
      "title" : "Recognizing textual entailment: Rational, evaluation and approaches–erratum",
      "author" : [ "Ido Dagan", "Bill Dolan", "Bernardo Magnini", "Dan Roth." ],
      "venue" : "Natural Language Engineering 16(01):105–105.",
      "citeRegEx" : "Dagan et al\\.,? 2010",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2010
    }, {
      "title" : "Paraphrase-driven learning for open question answering",
      "author" : [ "Anthony Fader", "Luke S. Zettlemoyer", "Oren Etzioni." ],
      "venue" : "ACL.",
      "citeRegEx" : "Fader et al\\.,? 2013",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2013
    }, {
      "title" : "Open question answering over curated and extracted knowledge bases",
      "author" : [ "Anthony Fader", "Luke S. Zettlemoyer", "Oren Etzioni." ],
      "venue" : "KDD.",
      "citeRegEx" : "Fader et al\\.,? 2014",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2014
    }, {
      "title" : "Building Watson: An overview of the DeepQA project. AI Magazine 31(3):59–79",
      "author" : [ "David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager" ],
      "venue" : null,
      "citeRegEx" : "Ferrucci et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ferrucci et al\\.",
      "year" : 2010
    }, {
      "title" : "Statistical methods for psychology",
      "author" : [ "David Howell." ],
      "venue" : "Cengage Learning.",
      "citeRegEx" : "Howell.,? 2012",
      "shortCiteRegEx" : "Howell.",
      "year" : 2012
    }, {
      "title" : "Question answering via integer programming over semistructured knowledge",
      "author" : [ "Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni", "Dan Roth." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Khashabi et al\\.,? 2016",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploring Markov logic networks for question answering",
      "author" : [ "Tushar Khot", "Niranjan Balasubramanian", "Eric Gribkoff", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Khot et al\\.,? 2015",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2015
    }, {
      "title" : "Scaling semantic parsers with on-the-fly ontology matching",
      "author" : [ "Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kwiatkowski et al\\.,? 2013",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George Miller." ],
      "venue" : "Communications of the ACM 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Compositional semantic parsing on semi-structured tables",
      "author" : [ "Panupong Pasupat", "Percy Liang." ],
      "venue" : "ACL.",
      "citeRegEx" : "Pasupat and Liang.,? 2015",
      "shortCiteRegEx" : "Pasupat and Liang.",
      "year" : 2015
    }, {
      "title" : "Markov logic networks",
      "author" : [ "Matthew Richardson", "Pedro Domingos." ],
      "venue" : "Machine learning 62(1– 2):107–136.",
      "citeRegEx" : "Richardson and Domingos.,? 2006",
      "shortCiteRegEx" : "Richardson and Domingos.",
      "year" : 2006
    }, {
      "title" : "Towards addressing the winograd schema challenge - building and using a semantic parser and a knowledge hunting module",
      "author" : [ "Arpit Sharma", "Nguyen Ha Vo", "Somak Aditya", "Chitta Baral." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Sharma et al\\.,? 2015",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2015
    }, {
      "title" : "Table cell search for question answering",
      "author" : [ "Huan Sun", "Hao Ma", "Xiaodong He", "Wen tau Yih", "Yu Su", "Xifeng Yan." ],
      "venue" : "WWW.",
      "citeRegEx" : "Sun et al\\.,? 2016",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2016
    }, {
      "title" : "Answering questions with complex semantic constraints on open knowledge bases",
      "author" : [ "Pengcheng Yin", "Nan Duan", "Ben Kao", "Jun-Wei Bao", "Ming Zhou." ],
      "venue" : "CIKM.",
      "citeRegEx" : "Yin et al\\.,? 2015",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Structured curated KBs have been used successfully for this task (Berant et al., 2013; Berant and Liang, 2014).",
      "startOffset" : 65,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Structured curated KBs have been used successfully for this task (Berant et al., 2013; Berant and Liang, 2014).",
      "startOffset" : 65,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices (Fader et al., 2014; Yin et al., 2015).",
      "startOffset" : 182,
      "endOffset" : 220
    }, {
      "referenceID" : 21,
      "context" : "Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices (Fader et al., 2014; Yin et al., 2015).",
      "startOffset" : 182,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : "Our goal in this work is to develop a QA system that can perform reasoning with Open IE (Banko et al., 2007) tuples for complex multiple-choice questions that require tuples from multiple sentences.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "Elementary-level science exams is one such domain, requiring complex reasoning (Clark, 2015).",
      "startOffset" : 79,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "Due to the lack of a large-scale structured KB, state-of-the-art systems for this task either rely on shallow reasoning with large text corpora (Clark et al., 2016; Cheng et al., 2016) or deeper, structured reasoning with a small amount of automatically acquired (Khot et al.",
      "startOffset" : 144,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "Due to the lack of a large-scale structured KB, state-of-the-art systems for this task either rely on shallow reasoning with large text corpora (Clark et al., 2016; Cheng et al., 2016) or deeper, structured reasoning with a small amount of automatically acquired (Khot et al.",
      "startOffset" : 144,
      "endOffset" : 184
    }, {
      "referenceID" : 14,
      "context" : ", 2016) or deeper, structured reasoning with a small amount of automatically acquired (Khot et al., 2015) or manually curated (Khashabi et al.",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : ", 2015) or manually curated (Khashabi et al., 2016) knowledge.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "A candidate system for such reasoning, and which we draw inspiration from, is the TABLEILP system of Khashabi et al. (2016). TABLEILP treats QA as a search for an optimal subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP).",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002).",
      "startOffset" : 72,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "Web QA: There exist several systems for retrieval-based Web QA problems (Ferrucci et al., 2010; Brill et al., 2002).",
      "startOffset" : 72,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data.",
      "startOffset" : 61,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data.",
      "startOffset" : 61,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "While structured KBs such as Freebase have been used in many (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013), such approaches are limited by the coverage of the data.",
      "startOffset" : 61,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "QA systems using semistructured Open IE tuples (Fader et al., 2013, 2014; Yin et al., 2015) or automatically extracted web tables (Sun et al.",
      "startOffset" : 47,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : ", 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query.",
      "startOffset" : 46,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : ", 2015) or automatically extracted web tables (Sun et al., 2016; Pasupat and Liang, 2015) have broader coverage but are limited to simple questions with a single query.",
      "startOffset" : 46,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "Markov Logic Networks (Richardson and Domingos, 2006) have been used to perform probabilistic reasoning over a small set of logical rules (Khot et al.",
      "startOffset" : 22,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "Markov Logic Networks (Richardson and Domingos, 2006) have been used to perform probabilistic reasoning over a small set of logical rules (Khot et al., 2015).",
      "startOffset" : 138,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "Simple IR techniques have also been proposed for science tests (Clark et al., 2016) and Gaokao tests (equivalent to the SAT exam in China) (Cheng et al.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : ", 2016) and Gaokao tests (equivalent to the SAT exam in China) (Cheng et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "We use the text corpora (S) from Clark et al. (2016) to build our tuple KB.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 19,
      "context" : "On-the-fly tuples from text: To handle questions from new domains not covered by the training set, we extract additional tuples on the fly from S (similar to Sharma et al. (2015)).",
      "startOffset" : 158,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "Unlike standard alignment models used for tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2010), however, we must score alignments between a set Tqa ∪ T ′ qa of structured tuples and a (potentially multi-sentence) multiple-choice question qa.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "9 While TABLEILP used WordNet (Miller, 1995) paths to compute the weight, this measure results in unreliable scores when faced with longer phrases found in Open IE tuples.",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "Numbers in bold indicate statistical significance based on the Binomial exact test (Howell, 2012) at p = 0.",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "(1) 4th Grade set (1220 train, 1304 test) is a 10x larger superset of the NY Regents questions (Clark et al., 2016), and includes professionally written licensed questions.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "IR is a simple yet powerful information-retrieval baseline (Clark et al., 2016) that selects the answer option with the best matching sentence in a corpus.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "TABLEILP is the stateof-the-art structured inference baseline (Khashabi et al., 2016) developed for science questions.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "web pages used by Clark et al. (2016). This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples T ′ qa.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "we train an ensemble system (Clark et al., 2016) which, as shown in the table, provides a substantial boost over the individual solvers.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "we train an ensemble system (Clark et al., 2016) which, as shown in the table, provides a substantial boost over the individual solvers. Further, IR + TUPLEINF is consistently better than IR + TABLEILP. Finally, in combination with IR and the statistical association based PMI solver (that scores 54.1% by itself) of Clark et al. (2016), TUPLEINF achieves a score of 58.",
      "startOffset" : 29,
      "endOffset" : 337
    } ],
    "year" : 2017,
    "abstractText" : "While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrievalbased methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.",
    "creator" : "LaTeX with hyperref package"
  }
}