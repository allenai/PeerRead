{
  "name" : "1702.04577.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Discrepancy Between Kleinberg’s Clustering Axioms and k-Means Clustering Algorithm Behavior",
    "authors" : [ "Robert K LOPOTEK" ],
    "emails" : [ "robert@klopotek.com.pl", "klopotek@ipipan.waw.pl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— clusterability, learnability, Kleinberg axioms, k-means\nar X\niv :1\n70 2.\n04 57\n7v 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "One of important application areas of machine learning is the so-called cluster analysis or clustering, referred to also as unsupervised learning or learning without a teacher. It seeks to split a set of items into subsets (usually disjoint, though not necessarily, possibly with the subsets forming a hierarchy) called clusters or groups that should be ”similar” within the clusters and ”dissimilar” between them. Additional criteria like group balancing, group size limits from below and above etc. may be also taken into account. Subsequently let us restrict somehow the meaning of these terms. By partition we will understand the output of the process of cluster analysis. So the partition would be an object - a set of objects called clusters that are sets of original items (called elements).\nAs the diversity of clustering methods grows, there exists a strong pressure for finding some formal framework to get a systematic overview of the expected properties of the partitions obtained.\nA number of axiomatic frameworks have been devised for methods of clustering, the most cited probably the Kleinberg’s system [20]1. Kleinberg defines [20] clustering function as\nDefinition 1. Clustering function is ”a function f that takes a distance function d on [set] S [of size n ≥ 2] and returns a partition Γ of S. The sets in Γ will be called its clusters.” We are interested only in such partitions Γ of S that ∪C∈ΓC = S, Ci 6= ∅ and for any two distinct Ci, Cj ∈ Γ Ci ∩ Cj = ∅.\nAdditionally, he defines the distance as\nDefinition 2. ”with the set S = {1, 2, . . . , n} [...] we define a distance function to be any function d : S ×S → R such that for distinct i, j ∈ S we have di, j) > 0, d(i, j) = d(j, i) and d(i, i) = 0. One can optionally restrict attention to distance functions that are metrics by imposing the triangle inequality: d(i, k) ≤ d(i, j)+d(j, k), for all i, j, k ∈ S. We will not require the triangle inequality [...], but the results to follow both negative and positive still hold if one does require”\nJon Kleinberg [20] claims that a good partition may only be a result of a reasonable method of clustering and he formulated axioms, for distance-based cluster analysis, that need to be met by the clustering method itself. He postulated that some quite ”natural” axioms need to be met, when we manipulate the distances between objects. As, however, the axioms proved to be not applicable to all clustering algorithms, we will rather speak about properties that Kleinberg expects of clustering functions, following e.g. Ackerman et al. [4]. These are:\nProperty 1. The method should allow to obtain any partition of the objects (so-called richness property)2,\n1Google Scholar lists about 400 citations. 2 ”let Range(f) denote the set of all partitions Γ such that f(d) = Γ for some distance\nfunction d. Range(f) is equal to the set of all partitions of S.” [20]\nProperty 2. The method should deliver partitions invariant with respect to distance scale (so-called scale-invariance property)3,\nProperty 3. The method should deliver the same partition if we move elements within a cluster closer to one another and elements from different clusters further away (so-called consistency property)4.\nNote that invariance and consistency properties assume a transformation on the clusters. With respect to this transformations we will speak about invariance transform(ation) and consistency transform(ation).\nSubsequently, while referring to Kleinberg’s axiomatic systems, we will use the term ”axioms”, but keeping in mind, that researchers treat them rather as properties that some algorithms have and other don’t.\nKleinberg demonstrated that the above three ”axioms” (properties) cannot be met all at once. So Kleinberg’s work points at an important issue that we shall first of all revise our expectations towards the obtained partition, as the seemingly obvious axiom set is apparently not sound. In particular he stated the Impossibility Theorem.\nTheorem 1. [20, Theorem 2.1] For each n ≥ 2, there is no clustering function f that satisfies Scale-Invariance, Richness, and Consistency.\nKleinberg himself proved this theorem in the above-mentioned paper. Another proof can be found in a paper by Ambroszkiewicz and Koronacki [7], along with some discussion of the Kleinberg’s concepts. Ackerman et al. [4] prove a bit more general impossibility theorem (engaging so called inner-consistency and outer-consistency).\nBeside providing a proof that his axioms are contradictory, Kleinberg showed that the axioms can be met pairwise. He uses for purpose of this demonstration versions of the well-known statistical single-linkage procedure. The versions differ by the stopping condition:\n• k-cluster stopping condition (which stops adding edges when the subgraph first consists of k connected components) - not ”rich”,\n• distance-r stopping condition (which adds edges of weight at most r only) - not scale-invariant,\n• scale-stopping condition (which adds edges of weight being at most some percentage of the largest distance between nodes) - not consistent5.\n3 ”For any distance function d and any α > 0, we have f(d) = f(α · d).” [20] 4 ”Let Γ be a partition of S, and d and d′ two distance functions on S. We say that d′ is a Γ-transformation of d if (a) for all i, j ∈ S belonging to the same cluster of Γ, we have d′(i, j) ≤ d(i, j) and (b) for all i, j ∈ S belonging to different clusters of Γ, we have d′(i, j) ≥ d(i, j). Let d and d′ be two distance functions. If f(d) = Γ, and d′ is a Γtransformation of d, then f(d′) = Γ” [20]. This should reflect the property of reducing distance within a cluster and enlarging that between the clusters.\n5 Notice that, as demonstrated by Kleinberg in his paper, also k-median and k-means clustering do not have the consistency property.\nNote, however, that Ben-David and Ackerman [10] drew attention by an illustrative example (their Figure 2), that consistency is a problematic property by itself as it may give rise to new clusters at micro or macro-level.\nLet us draw attention to the fact that by introduction of his definition of clustering function, Kleinberg introduces implicitly two additional axioms onto the clustering function:\nProperty 4. A clustering function always returns a clustering ( nonrefutability).\nProperty 5. A clustering function works even if the distances cannot be embedded in Euclidean space ( permission of non-embeddability).\nThe well-known k-means clustering algorithm seeks to minimize the function6\nQ(Γ) = m∑ i=1 k∑ j=1 uij‖xi − µj‖2 = k∑ j=1 1 nj ∑ xi,xl∈Cj ‖xi − xl‖2 (1)\nfor a dataset X under some partition Γ into the predefined number k of clusters, where uij is an indicator of the membership of data point xi in the cluster Cj having the center at µj .\nWe will call ”k-means-ideal” such an algorithm that finds a Γopt that attains the minimum of function Q(Γ). It is known that it is a hard task. 7 Hence in practice an algorithm is used with the following structure:\n1. Initialize k cluster centers µ1, . . . ,µk.\n2. Assign each data element xi to the cluster Cj identified by the closest µj .\n3. Update µj of each cluster Cj as the gravity center of the data elements in Cj .\n4. Repeat steps 2 and 3 until reaching a stop criterion (usually no change of cluster membership).\nIf step 1 is performed as random uniform sampling (without replacement), then we will speak about k-means-random algorithm. If step 1 is performed according to k-means++ heuristics proposed by Arthur and Vassilvitskii [8], then we will speak about k-means++ algorithm. Note that both attain a local minimum at worst. We will also touch the ”incremental” k-means discussed by\n6 The considerations would apply also to kernel k-means algorithm using the quality function\nQ(Γ) = m∑ i=1 k∑ j=1 uij‖Φ(xi − µΦj ‖2\nwhere Φ is a non-linear mapping from the original space to the so-called feature space. 7There exists a whole stream of research papers that attempt to approximate k-meansideal within a reasonable error bound via cleverly initiated k-means type algorithms, e.g. k-means++, like [24], but it has to be stated that at the current point these algorithms are rather of theoretical value.\nAckerman and Dasgupta [6]. This k-means version does not guarantee to reach a local minimum and has purely theoretical virtues.\nThe verification of Kleinberg’s axioms for k-means is a bit difficult because even for k-means-ideal we cannot guarantee that there exists a single (global) minimum of the Q function. But if we talk instead of the set of all possible minimizing Γs, then it is easily seen that it is scale-invariant, but one sees immediately that it is not rich (only partitions with k clusters are considered). It has also been demonstrated by Kleinberg that it is not consistent.\nWith k-means-random and k-means++ it is even worse, as Q hits usually a local minimum there. So we can talk about a random variable assuming particular Γ with some probability. Under this assumption, again it is easily seen that both are scale-invariant, but one sees immediately that none is rich (only partitions with k clusters are considered). As k-means-ideal is not consistent, so neither of the realistic variants is so.\nHence the widely used algorithm violates in practice two of three Kleinberg’s axioms. so that it cannot be considered to be a ”clustering function”. We perceive this to be at least counterintuitive. Ben-David and Ackerman in [10] in section 4.2., raised also similar concern from the perspective of what an axiomatic system should accomplish. They state that one would expect, for the axiomatised set of objects, a kind of soundness and completeness. By soundness they mean that most useful clustering algorithms would fit the axioms. The completeness expresses that apparent non-clustering algorithms would fail on at least one axiom. While Kleinberg’s axioms explicitly address the distance-based clustering algorithms (and not e.g. density based ones), they fall apparently short of reaching this goal. In this paper we demonstrate that even for a narrower set of algorithms, ones over data embedded in Euclidean space, the axioms fail.\nThere exist a number of open questions on why it is so. Recall that in [25] it has been observed by van Laarhoven and Marchiori that Kleinberg’s proof of Impossibility Theorem stops to be valid in case of graph clustering. This raises immediately the question of its validity in Rm Euclidean space. Note that Kleinberg did not bother about embedding the distance in such a space. So one may ask whether or not k-means does not fit Kleinberg’s axioms because this is a peculiar property of k-means or because any algorithm embedded in Euclidean space would fail to fit.\nPaying a special attention to k-means algorithm does not constitute a too restrictive limitation. k-means is applied in many domains, not only in its natural domains of data embedded in Rm, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22]. It has been demonstrated by Dhillon et al. [15] that kmeans is equivalent in some sense to normalized cut method of graph clustering, which in turn can be viewed as equivalent to balanced Newman’s modularity, used in community detection, as shown by Bolla [12]. Crisp and fuzzy versions are used.\nAmong others, the equivalence results on k-means and graph clustering in\n[15] and the impossibility theorem challenge for graphs in [25] encourage to investigate Kleinberg’s axioms in the context of Euclidean space.\nTherefore we made an effort to identify and overcome at least some reasons for the difficulties connected with axiomatic understanding of research area of cluster analysis and hope that this may be a guidance for further generalizations to encompass if not all then at least a considerable part of the real-life algorithms. This paper investigates why the k-means algorithm violates the Kleinberg’s axioms for clustering functions. We claim that the reason is a mismatch between informal intuitions and formal formulations of these axioms. We claim also that there is a way to reconcile k-means with Kleinberg’s consistency requirement via introduction of centric consistency and motion consistency which are neither a subset nor superset of Kleinberg’s consistency, but rather a kmeans clustering model specific adaptation of the general idea of shrinking the cluster or moving cluster away.\nOur contribution is as follows:\n• To substantiate our claim that there is a mismatch between informal intuitions and formal formulations of Kleinberg’s axioms, we present a series of carefully constructed examples.\n• We show in Section 3 that by the implicit non-embeddability axiom alone Kleinberg precludes consideration of k-means as a clustering algorithm.\n• We demonstrate in Section 2.1 that richness and scaling-invariance alone may lead to a contradiction for a special case. This denies Kleinberg’s claims that his axioms can be fulfilled pair-wise.\n• In Section 4 we show that known relationships between Kleinberg’s axioms and k-means apply also for Euclidean space, that is k-richness is granted, richness or near richness is not achievable, consistency is violated. We show also that the refinement consistency is violated too.\n• We show in Section 5 that in Rm scaling invariance transformations, by interference, annihilate effects of consistency transformation that is clusters being further away may get closer to one another.\n• Furthermore we show in Section 6 that consistency alone leads to contradictions. We demonstrate that in practical settings of application of many algorithms. In a metric m-dimensional space where m is the number of features, it is impossible to contract a single cluster without moving the other ones and as a consequence running at risk of moving some clusters closer together. Also we show that k-means version where we allow for k to range over a set, will change the optimal clustering k when Kleinberg’s Γ operation (consistency transform) is applied.\n• We demonstrate in Section 7 that also the richness axiom denies common sense by itself, as it is unrealistic to be achieved by k-means-ideal, kmeans-random and k-means++.\n• We propose a reformulation of the Kleinberg’s axioms in accordance with the intuitions and demonstrate that under this reformulation the axioms stop to be contradictory (Section 8). In particular we introduce the notion of centric consistency which is an adaptation of the general idea of shrinking the cluster. It relies simply on moving cluster elements towards its center. We provide an example of a clustering function that fits the axioms of near-richness, scale-invariance and possesses the property of centric consistency, so that it is clear that they are not contradictory.\n• We show that k-means is centric-consistent (Section 9). This implies that even a real-world algorithm like k-means conforms to the above-mentioned augmented axiomatic system (section 9).\n• As the centric consistency imitates only the consistency inside a cluster, we introduce also the notion of motion consistency, to approximate the consistency property outside a cluster and show that k-means, in order to be motion-consistent (Section 10), must impose the requirement of a gap between clusters. The introduction of gap requirement, on the other hand, violates Kleinberg’s non-refutability axiom (Property 4).\n• We investigate the issue of gaps between clusters and show appropriately designed gaps induce local minima (section 11) for k-means and formulate conditions under which the gap leads to a global minimum for k-means (section 12).\n• Based on the above, we propose an alternative approach to reconcile Kleinberg’s axioms with k-means. We demonstrate that under assumption of appropriate gaps we can either relax centric consistency to inner cluster consistency or go over from k-richness to an approximation of richness (sections 11 and 12)\nWe start this paper with a review of the previous work on development of an axiomatic system (Section 2) and round the paper up with a discussion of some open problems (Section 13)."
    }, {
      "heading" : "2 Previous work",
      "text" : "Axiomatic systems may be traced back to as early as 1973, when Wright [29] proposed axioms of clustering functions creating unsharp partitions, similar to fuzzy systems. In his framework every domain object was attached a positive real-valued weight, that could be distributed among multiple clusters.\nIn general, as exposed by van Laarhoven and Marchiori [25] and Ben-David and Ackerman [10] the clustering axiomatic frameworks address either:\n• required properties of clustering functions, or\n• required properties of the values of a clustering quality function, or\n• required properties of the relation between qualities of different partitions (ordering of partitions for a particular set of objects and distance or similarity or dissimilarity relations).\nOne of prominent axiomatic sets, that were later fiercely discussed, was that of Kleinberg, as already stated. From the point of view of the above classification, it imposes restrictions on the clustering function itself.\nWe have already discussed the Impossibility Theorem of Kleinberg that demonstrates the contradiction between the axioms of the set. However, there are further problems with this set, not covered by that Theorem. So Ben-David and Ackerman [10], as mentioned, pointed at the problems with consistency as such. They showed in an example in their Fig.2 that when moving clusters away the clusters themselves can create new groups. In this paper we repeat their findings for fix-dimensional Euclidean space, but we go beyond that. We draw attention to the fact that in Rm moving clusters away may be completely impossible without going into other dimension. Furthermore we show that also shrinking of a single cluster in a consistent way is also impossible in Rm. We demonstrate that interaction of consistency transformation and scaling-invariance transformation actually does something contrary to intuition behind consistency, that is it pulls cluster closer instead of pushing them away.\nA number of relaxations of axioms related to clustering functions have been proposed in order to overcome the Kleinberg’s impossibility result. We recall several of them here, based on an overview by Ackerman [1] and tutorial by Ben-David [9].\nSo it was proposed to weaken Kleinberg’s richness (by Kleinberg himself) to so-called k-richness as follows:\nProperty 6 (Zadeh and Ben-David [30]). For any partition Γ of the set X consisting of exactly k clusters there exists such a distance function d that the clustering function f(d) returns this partition Γ.\nThis relaxation8 allows for some algorithms splitting the data into a fixed number of clusters, like k-means, not to be immediately discarded as ”clustering algorithms”, given that no cluster is allowed to be empty.9\nHowever, this weakening of Kleinberg’s axioms does not suffice to make kmeans a ”clustering function” as it still violates consistency axiom.\nAckerman et al. [4] propose the concept of outer-consistency\nProperty 7. The method is said to be outer-consistent if it delivers the same partition if one increases only distances between elements from different clusters and lets the distances within clusters unchanged.\n8Still another relaxation of richness was proposed by Hopcroft and Kannan [19]: Richness II: For any set K of k distinct points in the given Euclidean space, there is an n and a set of S of n points such that the algorithm on input S produces k clusters, whose centers are the respective points in K. Here the weakness lies in the fact that the k points may be subject to clustering themselves in reasonable algorithms.\n9Even k-richness is still a problematic issue because as demonstrated by Ackerman et al. [5], a useful property of stability of clusters under malicious addition of data points holds only for balanced clusters.\nk-means algorithm is said to be in this sense outer-consistent.10 They propose also so-called inner consistency\nProperty 8. The method is said to be inner-consistent if it delivers the same partition when one decreases only distances between elements from same cluster and lets the distances between elements of different clusters unchanged.\nk-means algorithm is in this sense not inner-consistent. Later we will discuss the representation problem for this type of consistency with k-means. Let us mention here that they prove that (1) no general clustering function can simultaneously satisfy outer-consistency, scale- invariance, and richness, and (2) no general clustering function can simultaneously satisfy inner-consistency, scaleinvariance, and richness. They claim also that k -means-ideal has the properties of outer-consistency and locality11. None of these properties is claimed to be satisfied by k-means-random nor by a k-means with furthest element initialization. Furthermore, k-richness (in probabilistic sense) is not matched by k-means-random algorithm. In this paper we point at the fact that in Euclidean space even inner-consistency alone (see our Theorem 8) / outer-consistency alone (see our Theorem 10) are self-contradictory. Also the consistency alone poses problem (see our Theorem 7) So they do so for k-means-ideal. But on the other hand we show k-richness (in probabilistic sense) is matched by k-means-random algorithm (see our Theorem 5).\nStill another relaxation of the Kleinberg’s consistency is called Refinement Consistency. It is a modification of the consistency axiom by replacing the requirement that f(d) = f(d′) with the requirement that one of f(d), f(d′) is a refinement of the other. A partition Γ′ is a refinement of a partition Γ if for each cluster c′ ∈ Γ′ there exists a cluster c ∈ Γ such that c′ ⊆ c. Obviously the replacement of the consistency requirement with refinement consistency breaks the impossibility proof of Kleinberg’s axiom system. But there is a practical concern: In general, refinement consistency means that by the Γ transformation and scaling you may transform any clustering in any other. The usefulness of such an axiom is hence questionable. In this paper (Section 12) we show that under some circumstances unidirectional refinement consistency may be achieved, which makes much more sense.\nZadeh Ben-David [30] propose instead the order-consistency so that some versions of single-linkage algorithm can be classified as ”clustering algorithm”. For any two distance functions d and d′, if the orderings of edge lengths are the same then f(d) = f(d′). k-means is not order-consistent.\nOne could also relax Scale-Invariance instead to e.g. Robustness, that is, ”Small changes in distance function d should result in small changes of partition f(d)”. The basic problem here is that partitions are discrete and the term ”small” is hard to define reasonably. Small changes in distances may result in major changes of partitions obtained from k-means algorithm.\n10We show, however, that this is not true. 11 A clustering function clustering into k clusters has the locality property, if whenever a set S for a given k is clustered by it into the partition Γ, and we take a subset Γ′ ⊂ Γ with |Γ′| = k′ < k, then clustering of ∪C∈Γ′ into k′ clusters will yield exactly Γ′.\nLet us also mention here some works like that by Dunn [17] or Ackerman and Dasgupta [6] that seemingly have nothing to do with Kleinberg’s axioms, but this is only a superficial impression. Papers discussing the issue of ”well separated clusters” or ”nicely separated”, or ”perfectly separated ” point in fact at the weakness of the non-refutability axiom, because it is apparent that we do not want to get any partition but rather one that is meaningful.\nAckerman and Dasgupta [6] handle incremental clustering algorithms. They introduce an incremental version of k-means algorithm. The clusters are ”nicely separated”, as defined by [6], if a distance between an element and any other element of the same cluster is lower than the distance from this element to an element outside of the cluster. The authors demonstrate that no incremental algorithm of space complexity linear in k can (routinely) discover the clusters that are nicely separated. This is contrary to single-link algorithm which can identify a set of 2k−1 candidate elements among which k are from different clusters, if a nice clustering is unique. A nice clustering can be only detected in this sense (a set of candidates) by an incremental algorithm with memory linear in 2k−1. But it cannot be detected by the incremental k-means even with such a large memory. However, when looking at the issue with randomly generated sequence of data, a memory linear in k suffices for incremental kmeans with some probability. Then they introduce the ”perfect clustering” with the property that the smallest distance between elements of distinct clusters is larger than the distance between any two elements of the same cluster. They demonstrate that there exists an incremental algorithm discovering the ”perfect clustering” that is linear in k with respect to space. But the incremental kmeans fails to do so. We will discuss this issue in section 11.\nAckerman and Ben-David [10] propose another direction of resolving the problem of Kleinberg’s axiomatisation impossibility. Instead of axiomatising the clustering function, one should rather create axioms for cluster quality function.\nA number of further characterizations of clustering functions has been proposed to overcome Kleinberg axiom problems, e.g. [2] for linkage algorithms, [14] for hierarchical algorithms, [13] for multiscale clustering.\nNote that beside Kleinberg’s axioms there exist other ”impossible” characterizations of clustering functions. Meila [23] demonstrates that one can’t compare partitions in a manner that agrees with the lattice of partitions, is convexly additive and bounded.\nGeneral tendency of researchers wanting to overcome Kleinberg’s contradiction was to weaken one or more axioms of Kleinberg. While in this way the contradiction was removed, the removal relied on weakening the reasoning capabilities so that no strong conclusions can be reached. In this research we try the different way - one of strengthening the assumptions so that for example a proof of k-means consistency becomes possible.\nBut before we present a consistent set of algorithm properties and show its validity for k-means algorithm, we will investigate counter-intuitiveness of Kleinberg’s formalization of his axioms.\nLet us still mention briefly, that other characteristics of k-means algorithms were studied in the past, see e.g. papers by Ackerman et al. [3, 5]. [5] deals\nwith the susceptivity of among others the k-means algorithm to hostile addition of new points to the data set. It turns out that k-means is stable under such disturbances given that the clusters are well balanced (cluster sizes do not differ very much) and there are sufficient gaps between the clusters. [3] demonstrates that one can put any two data points into different clusters if one applies weighting functions to data points. Both of these papers, though not explicitly addressing the k-richness, demonstrate problems resulting from this axiom. [5] implies that too small clusters may be disintegrated by hostile new points so that for practical purposes one shall be only interested in larger clusters. [3] allows to conclude that poor estimates of densities for sparse clusters may lead to erroneous drawing of cluster boundaries."
    }, {
      "heading" : "2.1 Counter-intuitiveness of Scale-invariance and Consistency Axioms",
      "text" : "Kleinberg in his paper proved so-called anti-chain theorem that implies that by scaling and contraction (Γ-transform) one can transform any clustering into any other.12 This fact combined with the richness axiom leads directly to contradiction in the three axioms.\nFirst of all let us state that\nTheorem 2. For n = 2, for data in Rm, there is no clustering function f that satisfies Scale-Invariance and Richness.13\nProof. Any set S = {e1, e2} consisting of only two elements has potentially two partitions: Γ1 = {{e1}, {e2}} (”singleton partition”) and Γ2 = {e1, e2} (”nosplit-partition”). Let f(d1, S) = Γ1 and f(d2, S) = Γ2. Then f(\nd2(e1,e2) d1(e1,e2) d1, S) =\nΓ1 according to scale-invariance but by definition f( d2(e1,e2) d1(e1,e2) d1 = d2, S) = Γ2, so we have an obvious contradiction.\nNote that this theorem strengthens the result of Kleinberg stated in Theorem 1 - two kleinberg’s properties/axioms already (and not three) lead to a contradition.\nAs we will demonstrate later, a function matching richness axiom of Kleinberg does not necessarily exhibit richness, if distances will be confined to Rm. But in case of the above theorem it does not matter because we talk about 2 data points only and hence automatically the validity as distance in Rm is granted.\nIt is further easy to show (also based on Kleinberg’s anti-chain theorem) that14\n12 It is why Kleinberg proposed in his paper the ”refinement consistency”. 13Contrary to Kleinberg’s intuitions, scale-invariance plus richness alone lead to a contradiction. 14This shows that richness is not needed at all to get a contradiction from consistency and scale-invariance\nTheorem 3. For any n > 2, for data in Rm, no function f can produce no-split partition Γ1 under some distance function d1 and any other partition Γ2 under some distance function d2 if it satisfies both consistency and scale-invariance properties.\nBy the way this variant of Kleinberg’s anti-chain theorem is a reason why he proposed to weaken richness requirement to ”near-richness”, omitting the ”all-in-one” partition.15\nProof. To show this let mind = mine1,e2∈S d1 and maxd = maxe1,e2∈S d2. It is easy to see that d2 is a Γ-transform of maxd mind d1 for partition Γ1. Therefore as f(d1, S) = Γ1, because of scale-invariance f( maxd mind d1, S) = f(d1, S) = Γ1, hence by consistency f(d2, S) = f( maxd mind d1, S) = Γ1. This contradicts the assumption that f(d2, S) = Γ2.\nAs mentioned, [25] pointed at the fact that such a construction would not be possible in the realm of graph clustering. We shall ask then: what about Rm? We provided the above proof to show that forcing data points into the Euclidean space does not invalidate the construction because the scaling operation keeps the points in the original Euclidean space.\nSo there is surely a need to redefine the richness property into a ”nearrichness”.16\nBut ”near-richness” is again not enough to resolve all contradictions (as by the way is visible from the Kleinberg’s anti-chain theorem [20]).\nTheorem 4. For any n > 1, for data in Rm (m > 2), no function f can produce a partition Γ1 consisting of two sets of elements Γ1 = {{1, 2, . . . , n}, {n+ 1, n+ 2}} under some distance function d1 and any other partition Γ2 consisting of three sets of elements Γ2 = {{1, 2, . . . , n}, {n+1}, {n+2}} under some distance function d2 if it satisfies both consistency and scale-invariance properties.\nProof. For n ≥ 1 take a set of n + 2 elements. The richness property implies that under two distinct distance functions d1, d2 the clustering function f may form two partitions: Γ1,Γ2, resp., as defined in the theorem. By invariance property, we can derive from d2 the distance function d4 such that no distance between the elements under d4 is lower than the biggest distance under d1. By invariance property, we can derive from d1 the distance function d3 such that the distance between elements n + 1, n + 2 is bigger than under d4. We have then f({1, . . . , n+2}; d4) = Γ2, f({1, . . . , n+2}; d3) = Γ1. Now let us apply the consistency axiom. From d4 we derive the distance function d6 such that for elements 1, . . . , n d1 and d6 are identical, the distance between n+1, n+2 is same as in d4 and the distances between any element of 1, . . . , n and any of n+1, n+2 is some l that is bigger than any distances between any elements under d1, . . . , d4. From d3 we derive the distance function d5 such that for elements 1, . . . , n d1\n15In fact the Kleinberg’s anti-chain theorem implies that also a partition putting each element into a separate cluster should be excluded from ”near-richness”\n16A similar reasoning is possible for singleton partition, but we choose this way.\nand d5 are identical, the distance between n+ 1, n+ 2 is same as in d4 and the distances between any element of 1, . . . , n and any of n + 1, n + 2 is same l as above. We have then f({1, . . . , n+ 2}; d6) = Γ2, f({1, . . . , n+ 2}; d5) = Γ1. But then we have a contradiction because by construction d5 and d6 are identical.\nIn this proof, however, assumptions are made that may possibly be not correct if we require the distances to be distances in Euclidean space. So not for any configuration of n points a n+ 1-st point may be found to be equidistant to all the other ones. And even if it is so, it is not guaranteed that a second distinct n+ 2-nd point exists with the same property. Hence in the above construction of the proof, an initial step is needed, matching using consistency property, that will pose the points 1, . . . , n onto a sphere both for Γ1 and Γ2, and points n+1, n+2 on a line orthogonal to the subspace containing 1, . . . , n and passing through the origin of the sphere.\nIn the end, of course, the contradiction is still valid in Euclidean space, but this exercise shows that proofs of Kleinberg need to be rewritten if we deal with Euclidean spaces. But note that if we restrict ourselves to R2, posing the points onto a sphere does not work anymore. Points n+ 1 ad n+ 2 will become identical.\nSo, there is still an open question, whether or not we can have a clustering function matching Kleinberg’s axioms, that is still not contradictory. We will at this issue below. in Rm."
    }, {
      "heading" : "3 To embed or not to embed",
      "text" : "Kleinberg’s permission of non-embeddability axiom (Property 5) assumes that distances can be any non-negative symmetric functions over the set of pairs of objects.\nk-means normally operates in an Euclidean space, but by using so-called kernel-trick17 one can operate on the objects as if they were embedded in a (highly dimensional) space without actually finding the embedding (just working on a kernel matrix derived from distances). And one can get a clustering in that space optimizing the Q function.\nIt is well known that if there exists an embedding of a set of n points in an Euclidean space, then we do not need to consider more than n− 1 dimensions.\nBut it is well known that not for each distance function in the sense of Kleinberg’s definition there exists an embedding. Just consider the points in the table 1.\n17 We will not dive deeper in this paper into the discussion of properties of kernel k-means. Let us only make the remark that kernel k-means, given that there exists an embedding in Rm), is in fact k-means in the feature space. So all the findings related to k-means would apply also in the feature space. The weighted version of kernel k-means may be considered a bit tricky, but it can be ”approximated” by multiplying the unweighted points, under the restriction that all multiplied points will go into the same cluster, but this doss not seem to invalidate any findings. A separate question of course is whether or not we can invert the kernel function (if it is given explicitly) in order to find points transformed by e.g. centric consistency transform in the feature space."
    }, {
      "heading" : "A 0 10 2.236 20 22.361 20.125",
      "text" : "It is visible at the first glance that not even the triangle inequality holds in this data set (just look at points A,B,C alone). So no embedding in Euclidean space is possible.\nBut what if we still apply the kernel trick? One can easily find an embedding in a three-dimensional space if one allows for ”imaginary” coordinates (allowing for square-rooting negative eigenvalues). See table 2. The distances are kept if we rigidly use the distance formula\nd(P, T ) = √ (xP,1 − xT,1)2 + (xP,2 − xT,3)2 + (xP,2 − xT,3)2\nA quick look into the table 1 would suggest that points A,B,C form one cluster, and D,E, F form another.\nHowever, if we take the centers of the respective clusters µ1 = (0.667 + 0.000i, 10.000 + 0.000i, 0.000 + 0.333i) and µ2 = (0.667 + 0.000i,−10.000 + 0.000i, 0.000 + 0.333i), then the Q function for such 2-means amounts to 100. But if we take the points S1 = (0 + 0.00i, 0 + 0.00i, 0 − 10.18i), S2 = (0 + 0.000i, 0 + 0.000i, 0 + 9.198i) as cluster centers, then clusters {A,B,D,E} and {C,F, } are formed around them with Q function value equal 6 · 10−6.\nSo the Kleinberg’s non-embeddability axiom is not suitable for clustering algorithms for which position of other points in space needs to be anticipated. Under the assumption of Euclidean embedding this problem is clearly solved.\nFrom now on we will always assume that, if not stated otherwise, we constrain the Kleinberg’s consistency transform to the cases embeddable into a fixed dimensional Euclidean space.\nNote that with this result also the Kleinberg’s non-refutability axiom is\nindirectly questioned.18\n4 Kleinberg’s axioms and k-means – Conformance and Violations\nLet us briefly discuss here the relationship of k-means algorithm to the already mentioned axiomatic systems, keeping in mind that we apply it in Rm Euclidean space.\nScale-invariance is fulfilled because k-means qualifies objects into clusters based on relative distances to cluster centers and not their absolute values as may be easily seen from equation (1).19\nOn the other hand richness, a property denial of which has nothing to do with distances, hence with embedding in an Euclidean space, as already known from mentioned publications, e.g. [30], is obviously violated because k-means returns only partitions into k clusters.\nBut what about its relaxation that is k-richness. Let us briefly show here that\nTheorem 5. k-means algorithm is k-rich\nProof. We proceed by constructing a data set for each required partition. Let us consider n data points arranged on a straight line and we want to split them into k clusters fitting a concrete partition Γ0. For this purpose arrange the clusters on the line (left to right) in non-increasing order of their cardinality. Each cluster shall occupy (uniformly) a unit length. The space between the clusters (distance between closest elements of ith and (i+ 1)st cluster) should be set as follows: For i = 1, . . . , k − 1 let dce(j, i) denote the distance between the most extreme data points of clusters j and i, cardc(j, i) shall denote the combined cardinality of clusters j, j + 1, . . . , i. The distance between closest elements of clusters i and i+ 1 shall be then set to 2 ∗ dce(1, i) cardc(1,i)+cardc(i+1,i+1)cardc(i+1,i+1) . In this case application of k-means algorithm (k-means-ideal, k-meansrandom, k-means++) will lead the desired partition. The reasons are as follows: In case of k-means-ideal, let A be the most right cluster of a partition Γ different from Γ0, containing the ”space between clusters”. The definition of this distance is chosen in such a way that if we split A into two parts along this ”space between clusters” and attach the left and the right part to the neighboring clusters, and splitting any cluster if the number of clusters falls below k in this way, then the resulting new partition will be more optimal. Hence the optimal k-means-ideal clustering will not contain any ”spaces between\n18It does not mean that there do not exist versions of k-means for distances other than Euclidean distance. What we wanted to demonstrate here is that the notion of embedding is needed if we want to look at k-means from Kleinberg’s axioms perspective.\n19However, this quality function fails on the axiom of Function Scale Invariance, proposed in [10].\nclusters” within the clusters and be identical with the intended Γ0. This implies we can construct any partition in this way.\nIn case of k-means-random, if each of the clusters of Γ0 is seeded, the spaces between clusters of Γ0 are so large, that the clustering resulting from such a seeding is identical with Γ0 and upon subsequent steps the partition will not change any more. So consider now the case that after the random initialization (or at any later step) we get a partition Γ with cluster centers µ1, . . . µk such that there be a cluster C of Γ0 that has not been seeded (does not contain a µi in its range). No cluster of Γ with center to the right of C would nonetheless contain any data element from C. Consider therefore only clusters of Γ0 to the left of C and let µr be the cluster center most to the right in this set. The cluster Cr formed from elements closest to µr will contain C. Therefore during the cluster center update step of k-means-random µr will move to the right to a position, from which only C will be the set of points closest to µr. Therefore after 3 steps µr will become the center of C. Later on the same process will happen with other not seeded clusters of Γ0 to the left of it till we get the partition Γ0.\nAs k-means++ behaves similarly to k-means-random after initial seeding, the same effect will be reached. 20\n20 Let us formulate the argument more precisely. As mentioned earlier, following [4], for probabilistic algorithms we will talk about probabilistic k-richness, that is one obtainable with some probability, independent of the actual clustering that is intended to be obtained. The probability can further be increased if one wishes to.\nIn the above scheme we see that whenever during the initialization at each place more seeds are there than clusters, then they will spread to the right, given there are clusters without seeds there, ensuring that each cluster gets its cluster center. Note that if there are clusters lacking seeds to the left, there is no way to move seeds there. Hence, as the cluster s are sorted in decreasing size order from the left to right, the probability, that we have a seeding upon which by moving cluster centers to the right we can assign each cluster a cluster center amounts to at least k!/kk. This is computed as follows: The favorable seeding occurs, if the first seed is in the first cluster, and the ith seed in a cluster 1 or 2 or ... or i from the left. As the clusters are sorted non-increasingly, the probability of hitting the first cluster is at least 1 k , that of first or second 2 k , that of first, or second, or,. . . ,or ith is i k\n. This results in the aforementioned estimation.\nNote that the estimated probability is independent of the sample size and the actual distribution of sizes of clusters. It depends on k only.\nFurthermore, the targeted clustering is the absolute minimum of the k-means-ideal, hence we can run k-means-random multiple time in order to achieve the desired probability of krichness. E.g. if we need 95% certainty, we need to rerun k-means-random r times with r such that 1− (1− k!/kk)r ≥ 95%.\nThe issue with k-means++ is a bit more complex due to the way how probabilities of seeding are computed. In fact, we do not rely on the k-means iterating process, but have rather to ensure that each cluster gets a seed during the seeding phase.\nWhen the first seed is distributed, like in k-means, we have the assurance that an unhit cluster will be hit. The probability that a cluster is hit during the seeding step after the first one is proportional to the sum of squared distances of cluster elements to the closest seed assigned earlier. Consider the ith cluster (from the left) that was not hit so far. The closest hit cluster to the left can lie at least a distance\nα ∗ dce(1, i− 1) cardc(1, i− 1) + cardc(i, i)\ncardc(i, i)\nLet us stress here that there exist attempts to upgrade k-means algorithm to choose the proper k. The portion of variance explained by the clustering is used as quality criterion21. It is well known that increase of k increases the value of this criterion. The optimal k is deemed to be one when this increase\nand that to the right at\nα ∗ dce(1, i) cardc(1, i) + cardc(i+ 1, i+ 1)\ncardc(i+ 1, i+ 1)\n(note that the first cluster has no left neighbor, and the kth - no right neighbor). α = 2. So the contribution of the ith cluster to the sum of squares estimation for hitting probability in a current state amounts to at least the smaller number of the following two:\ncardc(i, i) ( α ∗ dce(1, i− 1)\ncardc(1, i− 1) + cardc(i, i) cardc(i, i) )2 cardc(i, i) ( α ∗ dce(1, i) cardc(1, i) + cardc(i+ 1, i+ 1)\ncardc(i+ 1, i+ 1)\n)2 ≥\n≥ cardc(i, i) ( α ∗ dce(1, i) cardc(1, i) + cardc(i+ 1, i+ 1)\ncardc(i, i) )2 Obviously the first expression is the smaller one so we will consider it only.\ncardc(i, i) ( α ∗ dce(1, i− 1)\ncardc(1, i− 1) + cardc(i, i) cardc(i, i)\n)2 = α2 ∗ dce(1, i− 1)2 cardc(1, i)2\ncardc(i, i)\nNote that due to the non-increasing order of cluster sizes, cardc(1, i) ≥ i · cardc(i, i), and cardc(1, i) ≥ in\nk . Therefore\nα2 ∗ dce(1, i− 1)2 cardc(1, i)2\ncardc(i, i) ≥ α2 ∗ dce(1, i− 1)2i\nn\nk\nFurthermore, dce(1, 1) = 1, and dce(1, i) = dce(1, i − 1) + 1 + α ∗ dce(1, i − 1)\ncardc(1,i−1)+cardc(i,i) cardc(i,i)\n≥ dce(1, i− 1) + 1 +α ∗ dce(1, i− 1) · i = 1 + dce(1, i− 1) · (1 + iα) ≥ dce(1, i− 1) · (1 + iα) Hence\ndce(1, i) ≥ (1 + 2α)i−1\nSo α2 ∗ dce(1, i− 1)2i n\nk ≥ α2(1 + 2α)2(i−1)i\nn\nk After s seeds were distributed the sum of squared distances to the closest seed for hit clusters amounts to at most the combined cardinality of the clusters with seeds times 1 so this does not exceed n.\nTherefore the probability of hitting an unhit cluster after s seeds were already distributed and hit different clusters is not bigger than\nα2(1 + 2α)2 n k\n+ ∑k−s\ni=2 α 2(1 + 2α)2(i−1)in k\nn+ α2(1 + 2α)2 n k\n+ ∑k−s\ni=2 α 2(1 + 2α)2(i−1)in k\n= α2(1 + 2α)2 +\n∑k−s i=2 α 2(1 + 2α)2(i−1)i\nk + α2(1 + 2α)2 + ∑k−s\ni=2 α 2(1 + 2α)2(i−1)i\nSo the probability that during the seeding all clusters are hit by a seed amounts to at least.\nk−1∏ s=1\nα2(1 + 2α)2 + ∑k−s\ni=2 α 2(1 + 2α)2(i−1)i k + α2(1 + 2α)2 + ∑k−s\ni=2 α 2(1 + 2α)2(i−1)i\nIn order to increase the success probability we can now repeat the seed independently sufficiently many times, or we can increase the distances by letting α be (much) greater than 2.\n21Such a quality function would satisfy axiom of Function Scale Invariance, proposed in [10]\nstops to be ”significant”. The above construction could be extended to cover a range of k values to choose from. However, the full richness is not achievable because a split into two clusters will be better than keeping a single cluster, and the maximum is attained for this criterion if k = n. So either the clustering will be trivial or quite a large number of partitions will be excluded. However, even k-richness offers a large number of partitions to choose from.\nKleinberg himself proved via a bit artificial example (with unbalanced samples and an awkward distance function) that k-means algorithm with k=2 is not consistent. Kleinberg’s counter-example would require an embedding in a very high dimensional space, non-typical for k-means applications. Also kmeans tends to produce rather balanced clusters, so Kleinberg’s example could be deemed to be eccentric.\nLet us illustrate by a more realistic example (balanced, in Euclidean space) that this is a real problem. Let A,B,C,D,E, F be points in three-dimensional space with coordinates: A(1, 0, 0), B(33, 32, 0), C(33,−32, 0), D(−1, 0, 0), E(−33, 0,−32), F (−33, 0, 32). Let SAB , SAC , SDE , SDF be sets of say 1000 points randomly uniformly distributed over line segments (except for endpoints) AB,AC,DE,EF resp. Let X = SAB ∪ SAC ∪ SDE ∪ SEF . k-means with k = 2 applied to X yields a partition {SAB ∪SAC , SDE ∪SDF }. But let us perform a Γ transformation consisting in rotating line segments AB,BC around the point A in the plane spread by the first two coordinates towards the first coordinate axis so that the angle between this axis and AB′ and AC ′ is say one degree. Now the k-means with k = 2 yields a different partition, splitting line segments AB′ and AC ′.22\nWith this example not only consistency violation is shown, but also refinement-consistency violation."
    }, {
      "heading" : "5 Problems with Consistency in Euclidean",
      "text" : "Space\nHow does it happen that seemingly intuitive axioms lead to such a contradiction. We need to look more carefully at the consistency axiom in conjunction with scale-invariance. Γ-transform does not do what Kleinberg claimed it should that is describing a situation when moving elements from distinct clusters apart and elements within a cluster closer to one another.23\nWe shall now demonstrate that application of scaling invariance axiom leads to violation of the consistency axiom of Kleinberg. More precisely:\n22In a test run with 100 restarts, in the first case we got clusters of equal sizes, with cluster centers at (17,0,0) and (-17,0,0), (between SS / total SS = 40 %) whereas after rotation we got clusters of sizes 1800, 2200 with centers at (26,0,0), (-15,0,0) (between SS / total SS = 59 %)\n23 Recall that the intuition behind clustering is to partition the data points in such a way that members of the same cluster are ”close” to one another, that is their distance is low, and members of two different clusters are ”distant” from one another, that is their distance is high. So it is intuitively obvious that moving elements from distinct clusters apart and elements within a cluster closer to one another should make a partition ”look better”.\nTheorem 6. For a clustering algorithm f , conforming to consistency and scaling invariance axioms, if distance d2 is derived from the distance d1 by consistency transformation, and d3 is obtained from d2 via scaling, then the existence of a d3 cannot always be obtained from d1 via consistency axiom transformation.\nProof. We prove the Theorem by finding a suitable example. let S consist of four elements e1, e2, e3, e4 and let a clustering function partition it into {e1}, {e2, e3}, {e4} under some distance function d1. One can easily construct a distance function d2 being a Γ-transform of d1 such that d2(e2, e3) = d1(e2, e3) and d2(e1, e2)+d2(e2, e3) = d2(e1, e3) and d2(e2, e3)+d2(e3, e4) = d2(e2, e4) and d2(e1, e2) + d2(e2, e3) + d2(e3, e4) = d2(e1, e4) which implies that these points under d2 can be embedded in the space R that is the straight line. Without restricting the generality (the qualitative illustration) assume that the coordinates of these points in this space are located at points 0, 0.4, 0.6, 1 resp. Now assume we want to perform Γ-transformation of Kleinberg (obtaining the distance function d3) in such a manner that the data points remain in R and move elements of the second set i.e. {e2, e3} (d2(e2, e3) = 0.2) closer to one another so that e2 = (0.5), e3 = (0.6) (d3(e2, e3) = 0.1). e1 may then stay where it is but e4 has to be shifted at least to (1.1) (under d3 the clustering function shall yield same clustering). Now apply rescaling into the original interval that is multiply the coordinates (and hence the distances, yielding d4) by 1/1.1. e1 stays at (0), e2 = ( 5 11 ), e3 = 6 11 , e4 = (1). e3 is now closer to e1 than before. We could have made the things still more drastic by transforming d2 to d ′ 3 in such a way that instead of e4 going to (1.1), as under d3, we set it at (2). In this case the rescaling would result in e1 = (0), e2 = (0.25), e3 = (0.3), e4 = (1) (with the respective distances d′4) which means a drastic relocation of the second cluster towards the first - the distance between clusters decreases instead of increasing as claimed by Kleinberg. This is a big surprise. The Γ transform should have moved elements of a cluster closer together and further apart those from distinct clusters and rescaling should not disturb the proportions. It turned out to be the other way. This contradicts the consistency assumption.\nSo something is wrong either with the idea of scaling or of Γ-transformation. We shall be reluctant to blame the scaling, except for the practical case when scaling down leads to indiscernibility between points with respect to measurement errors.\nNote that we do not observe such a clash between invariance and richness. If a set of distance functions demonstrates the richness of a clustering function conforming to richness and scaling, then after scaling all these distance functions demonstrate the richness of the same clustering function again. Scaling does not impair the richness."
    }, {
      "heading" : "6 Counter-intuitiveness of Consistency Axiom",
      "text" : "Alone\nSo we will consider counter-intuitiveness of consistency axiom. To illustrate it, recall first the fact that a large portion of known clustering algorithms uses data points embedded in anm dimensional feature space, usually Rm and the distance is the Euclidean distance therein. Now imagine that we want to perform a Γtransform on a single cluster of a partition that is the Γ-transform shall provide distances compatible with the situation that only elements of a single cluster change position in the embedding space.\nTheorem 7. Under the above-mentioned circumstances it is impossible to perform Γ-transform reducing distances within a single cluster.\nProof. Assume the cluster is an ”internal” one that is for a point e in this cluster any hyperplane containing it has points from some other clusters on each side. Furthermore assume that other clusters contain together more than m data points, which should not be an untypical case. Here the problem starts. The position of e is determined by the distances from the elements of the other clusters in such a way that the increase of distance from one of them would necessarily decrease the distance to some other (except for strange configurations), contrary to consistency requirement. Hence the claim\nSo the Γ-transform enforces either adding a new dimension and moving the affected single cluster along it (which does not seem to be quite natural) or to change positions of elements in at least two clusters within the embedding space. Therefore vast majority of such algorithms does not meet not only the consistency but also inner consistency requirement.\nTheorem 8. No algorithm operating in a fix-dimensional space under Euclidean distance can conform to inner-consistency axiom.24\nWhy not moving a second cluster is so problematic? Let us illustrate the difficulties with the original Kleinberg’s consistency by looking at an application of the known k-means algorithm, with k being allowed to cover a range, not just a single value, to the two-dimensional data set visible in Figure 125. This example is a mixture of data points sampled from 5 normal distributions. The k-means algorithm with k = 5, as expected, separates quite well the points from various distributions. As visible from the second column of Table 3, in fact k = 5 does the best job in reducing the unexplained variance. Figure 2 illustrates a result of a Γ-transform on the results of the former clustering. Visually we would tell that now we have two clusters. A look into the third column of the Table 3 convinces that really k = 2 is the best choice for clustering these data with k-means algorithm. This of course contradicts Kleinberg’s consistency axiom. And demonstrates the weakness of outer-consistency concept as well.\n24This impossibility does not mean that there is an inner contradiction when executing the inner-consistency transform. Rather it means that considering inner-consistency is pointless\nOriginal data\nKleinberg's Gamma Transformation\nTheorem 9. k-means with k allowed to range over a set of values (approximating richness) with limited variance increase criterion for choice of k operating in a fix-dimensional space under Euclidean distance cannot conform to outerconsistency axiom.\nAnd finally have a look at Figure 1 once again. If we ignore the most right cluster, it turns out that each cluster has points being ”surrounded” by points in other clusters. Therefore, it is not possible to move a cluster by infinitely small distance without decreasing distances of some different clusters. Therefore\nTheorem 10. No algorithm operating in a fix-dimensional space under Euclidean distance can conform continuously to outer-consistency axiom.\nThis theorem contradicts apparently [4] claim that k-means possesses the property of outer-consistency. The key word in this theorem is however ”continuously” in strict conjunction with ”Euclidean distance”. It is the embedding into the Euclidean space that causes the problem."
    }, {
      "heading" : "7 Problems of Richness Axiom",
      "text" : "As already mentioned, richness or near-richness forces the introduction of ”refinement-consistency” which is a too weak concept. But even if we allow for such a resolution of the contradiction in Kleinberg’s framework, it still does not make it suitable for practical purposes. The most serious drawback of Kleinberg’s axioms is the richness requirement.\nBut we may ask whether or not it is possible to have richness, that is for any partition there exists always a distance function that the clustering function will return this partition, and yet if we restrict ourselves to Rm, the very same clustering function is not rich any more, or even it is not anti-chain.\nConsider the following clustering function f(). If it takes a distance function d() that takes on only two distinct values d1 and d2 such that d1 < 0.5d2 and for any three data points a, b, c if d(a, b) = d1, d(b, c) = d1 then d(a, c) = d1, it creates clusters of points in such a way that a, b belong to the same cluster if and only if d(a, b) = d1, and otherwise they belong to distinct clusters. If on the other hand f() takes a distance function not exhibiting this property, it works like k-means. Obviously, function f() is rich, but at the same time, if confined to Rm, if n > m + 1 and k n, then it is not rich – it is in fact k-rich, and hence not anti-chain.\nCan we get around the problems of all three Kleinberg’s axioms in a similar way in Rm? Regrettably,\nTheorem 11. If Γ is a partition of n > 2 elements returned by a clustering function f under some distance function d, and f satisfies Consistency, then there exists a distance function dE embedded in Rm for the same set of elements such that Γ is the partition of this set under dE.\nbecause inner-consistency transform is in general impossible. 25Already Ben-David [10] indicated problems in this direction.\nThe consequence of this theorem is of course that the constructs of contradiction of Kleinberg axioms are simply transposed from the domain of any distance functions to distance functions in Rm.\nProof. To show the validity of the theorem, we will construct the appropriate distance function dE by embedding in the Rm. Let dmax be the maximum distance between the considered elements under d. Let C1, . . . , Ck be all the clusters contained in Γ. For each cluster Ci we construct a ball Bi with radius ri equal to ri = 1 2 minx,y∈Ci,x 6=y d(x, y). The ball B1 will be located in the origin of the coordinate system. B1,...,i be the ball of containing all the balls B1, . . . , Bi. Its center be at c1,...,i and radius r1,...,i. The ball Bi will be located on the surface of the ball with center at c1,...,i−1 and radius r1...,i−1 +dmax+ri. For each i = 1, . . . , k select distinct locations for elements of Ci within the ball Bi. The distance function dE define as the Euclidean distances within Rm in these constructed locations.\nApparently, dE is a Γ-transform of d, as distances between elements of Ci are smaller than or equal to 2ri = minx,y∈Ci,x 6=y d(x, y), and the distances between elements of different balls exceed dmax.\nBut richness is not only a problem in conjunction with scale-invariance and consistency, but rather it is a problem by itself.\nIt has to be stated first that richness is easy to achieve. Imagine the following ’clustering function”. You order nodes by average distance to other nodes, on tights on squared distance and so on, and if no sorting can be achieved, the unsortable points are set into one cluster. Then we create an enumeration of all clusters and map it onto unit line segment. Then we take the quotient of the lowest distance to the largest distance and state that this quotient mapped to that line segment identifies the optimal clustering of the points. Though the algorithm is simple in principle (and useless also), and meets axioms of richness and scale -invariance, we have a practical problem: As no other limitations\nare imposed, one has to check up to ∑n k=2 1 k! ∑k j=1(−1)k−j ( k j ) jn possible partitions (Bell number) in order to verify which one of them is the best for a given distance function because there must exist at least one distance function suitable for each of them. This is prohibitive and cannot be done in reasonable\ntime even if each check is polynomial (even linear) in the dimensions of the task (n).\nFurthermore, most algorithms of cluster analysis are constructed in an incremental way. But this can be useless if the clustering quality function is designed in a very unfriendly way. For example as an XOR function over logical functions of class member distances and non-class member distances (e.g. being true if the distance rounded to an integer is odd between class members and divisible by a prime number for distances between class members and non-class members, or the same with respect to class center or medoid).\nJust have a look at sample data from Table 4. A cluster quality function was invented along the above line and exact quality value was computed for partitioning first n points from this data set as illustrated in Table 5. It turns out that the best partition for n points does not give any hint for the best partition for n + 1 points therefore each possible partition needs to be investigated in order to find the best one.26\nSummarizing these examples, the learnability theory points at two basic weaknesses of the richness or even near-richness axioms. On the one hand the hypothesis space is too big for learning a clustering from a sample (it grows too quickly with the sample size). On the other hand an exhaustive search in this space is prohibitive sop that some theoretical clustering functions do not make practical sense.\nThere is one more problem. If the clustering function can fit any data, we are practically unable to learn any structure of data space from data [21]. And this learning capability is necessary at least in the cases: either when the data may be only representatives of a larger population or the distances are measured with some measurement error (either systematic or random) or both. Note that we speak here about a much broader aspect than so-called cluster stability or cluster validity, pointed at by Luxburg [27, 26].\n26 Strict separation [11] mentioned earlier is another kind of a weird cluster quality function, requiring visits to all the partitions"
    }, {
      "heading" : "8 Correcting Formalization of Kleinberg Ax-",
      "text" : "It is obvious that richness axiom of Kleinberg needs to be replaced with a requirement of the space of hypotheses to be ”large enough”. For k-means algorithm it has been shown via Theorem 5 that k-richness is satisfied (and the space is still large, a Bell number of partitions to choose from). k-means satisfies the scale-invariance axiom, so that only the consistency axiom needs to be adjusted to be more realistic.\nTherefore a meaningful redefinition of Kleinberg’s Γ-transform is urgently needed. It must not be annihilated by scaling and it must be executable.\nLet us create for R a working definition of the Γ∗ transform as follows: Distances in only one cluster X are changed by moving a point along the axis connecting it to cluster X center reducing them within the cluster X by the same factor, the distances between any elements outside the cluster X are kept [as well as to the gravity center of the cluster X]27\nConsider the following one-dimensional clustering function: For a set of n ≥ 2 points two elements belong to the same cluster if their distance is strictly lower than 1n+1 of the largest distance between the elements. When a, b belong to the same cluster and b, c belong to the same cluster, then a, c belong to the same cluster. As a consequence, the minimum distance between elements of distinct clusters is 1n+1 of the largest distance between the elements of S. It is easily seen that the weakened richness is fulfilled. The scale-invariance is granted by the relativity of inter-cluster distance. And the consistency under redefined Γ\n27Obviously, for any element outside the cluster X the distance to the closest element of X before the transform will not be smaller than its distance to the closest element of X after the transform. Note the shift of attention. We do not insist any longer that the distance to each element of other cluster is increased, rather only the distance to the cluster as a ”whole” shall increase. This is by the way a stronger version of inner-consistency which would be insufficient for our purposes.\ntransform holds also. In this way all three axioms hold. A generalization to an Euclidean space of higher dimensionality seems to be quite obvious if there are no ties on distances (the exist one pair of points the distance between which is unique and largest among distances28). We embed the points in the space, and then say that two points belong to the same cluster if the distance along each of the dimensions is lower than 1n+1 of the largest distance between the elements along the respective dimension. The distance is then understood as the maximum of distances along all dimensions.\nDefinition 3. Let Γ be a partition embedded in Rm. Let C ∈ Γ and let µc be the center of the cluster C. We say that we execute the Γ∗ transform (or a centric consistency transformation) if for some 0 < λ ≤ 1 we create a set C ′ with cardinality identical with C such that for each element x ∈ C there exists x’ ∈ C ′ such that x’ = µc + λ(x− µc), and then substitute C in Γ with C ′.\nProperty 9. A method matches the condition of centric consistency if after a Γ∗ transform it returns the same partition.\nHence\nTheorem 12. For each n ≥ 2, there exists clustering function f that satisfies Scale-Invariance, near-Richness, and Centric-Consistency29.\nProof. The above-mentioned clustering function is the proof of validity of this theorem.\nThis way of resolving Kleinberg’s contradictions differs from earlier approaches in that a realistic embedding into an Rm is considered and the distances are metric.\nWe created herewith the possibility of shrinking a single cluster without having to ”move” the other ones. As pointed out, this was impossible under Kleinberg’s Γ transform, that is under increase of all distances between objects from distinct clusters. In fact intuitively we do not want the objects to be more distant but rather the clusters. We proposed to keep the cluster centroid unchanged while decreasing distances between cluster elements proportionally, insisting that no distance of other elements to the closest element of the shrunk cluster should decrease. This approach is pretty rigid. It assumes that we are capable to embed the objects into some Euclidean space so that the centroid has a meaning.\n9 k-means fitting centric-consistency axiom\nOur proposal of centric-consistency has a practical background. Kleinberg proved that k-means does not fit his consistency axiom. As shown experimen-\n28otherwise some tie breaking measures have to be taken that would break the any symmetry and allow to choose a unique direction\n29 Any algorithm being consistent is also refinement-consistent. Any algorithm being innerconsistent is also consistent. Any algorithm being outer-consistent is also consistent. But there are no such subsumptions for the centric-consistency.\nCentralised Gamma Transformation\ntally in table 3, k-means algorithm behaves properly under Γ∗ transformation. Figure 3 illustrates a two-fold application of the Γ∗ transform (same clusters affected as by Γ-transform in the preceding figure). As recognizable visually and by inspecting the forth column of Table 3, here k = 5 is the best choice for k-means algorithm, so the centric-consistency axiom is followed.\nLet us now demonstrate theoretically, that k-means algorithm really fits ”in the limit” the centric-consistency axiom.\nTheorem 13. k-means algorithm satisfies centric consistency in the following way: if the partition Γ is a local minimum of k-means, and the partition Γ has been subject to centric consistency yielding Γ′, then Γ′ is also a local minimum of k-means.\nProof. The k-means algorithm minimizes the sum30 Q from equation (1). V (Cj) be the sum of squares of distances of all objects of the cluster Cj from its\ngravity center. Hence Q(Γ) = ∑k j=1 1 nj V (Cj). Consider moving a data point x ∗ from the cluster Cj0 to cluster Cjl As demonstrated by [16], V (Cj0 − {x∗}) = V (Cj0)−\nnj0 nj0−1 ‖x∗ − µj0‖ 2 and V (Cjl ∪ {x∗}) = V (Cjl) + nlnl+1‖x ∗ − µjl‖ 2 So\nit pays off to move a point from one cluster to another if nj0 nj0−1 ‖x∗ − µj0‖ 2 >\nnjl njl+1 ‖x∗−µjl‖ 2. If we assume local optimality of Γ, this obviously did not pay\n30We use here the symbol Q for the cluster quality function instead of J from section 2 because Q does not fit axiomatic system for J - it is not scale-invariant and in case of consistency it changes in opposite direction, and with respect of richness we can only apply k-richness.\noff. Now transform this data set to X′ in that we transform elements of cluster Cj0 in such a way that it has now elements x ′ i = xi + λ(xi − µj0) for some 0 < λ < 1, see figure 4. Consider a partition Γ′ of X′. All clusters are the same as in Γ except for the transformed elements that form now a cluster C ′j0 . The question is: does it pay off to move a data point x’∗ ∈ C ′j0 between the clusters? Consider the plane containing x∗,µj0 ,µjl . Project orthogonally the point x ∗ onto the line µj0 ,µjl , giving a point p. Either p lies between µj0 ,µjl or µj0 lies between p,µjl . Properties of k-means exclude other possibilities. Denote distances y = ‖x∗ − p‖, x = ‖µj0 − p‖, d = ‖µj0 − µjl‖ In the second case the condition that moving the point does not pay off means:\nnj0 nj0 − 1 (x2 + y2) ≤ njl njl + 1 ((d+ x)2 + y2)\nIf we multiply both sides with λ2, we have:\nλ2 nj0\nnj0 − 1 (x2 + y2) = nj0 nj0 − 1 ((λx)2 + (λy)2)\n≤λ2 njl njl + 1 ((d+ x)2 + y2) = njl\nnjl + 1 (λ2d2 + λ22dx+ λ2x2 + λ2y2)\n≤ njl njl + 1 (d2 + 2dλx+ λ2x2 + λ2y2) = njl\nnjl + 1 ((d+ λx)2 + (λy)2) (2)\nwhich means that it does not payoff to move the point x’∗ between clusters either. Consider now the first case and assume that it pays off to move x’∗. So we would have\nnj0 nj0 − 1 (x2 + y2) ≤ njl njl + 1 ((d− x)2 + y2)\nand at the same time\nnj0 nj0 − 1 λ2(x2 + y2) > njl njl + 1 ((d− λx)2 + λ2y2)\nSubtract now both sides:\nnj0 nj0 − 1 (x2 + y2)− nj0 nj0 − 1 λ2(x2 + y2)\n< njl njl + 1 ((d− x)2 + y2)− njl njl + 1 ((d− λx)2 + λ2y2)\nThis implies\nnj0 nj0 − 1 (1− λ2)(x2 + y2) < njl njl + 1 ((1− λ2)(x2 + y2)− 2dλx)\nIt is a contradiction because\nnj0 nj0 − 1 (1−λ2)(x2+y2) > njl njl + 1 (1−λ2)(x2+y2) > njl njl + 1 ((1−λ2)(x2+y2)−2dλx)\nSo it does not pay off to move x’∗, hence the partition Γ′ remains locally optimal for the transformed data set.\nIf the data have one stable optimum only like in case of ”well separated” normally distributed k real clusters, then both turn to global optima.\nHowever, it is possible to demonstrate that the newly defined transform preserves also the global optimum of k-means.\nTheorem 14. k-means algorithm satisfies centric consistency in the following way: if the partition Γ is a global minimum of k-means, and the partition Γ has been subject to centric consistency yielding Γ′, then Γ′ is also a global minimum of k-means.\nProof. Let us consider first the simple case of two clusters only (2-means). Let the optimal clustering for a given set of objects X consist of two clusters: T and Z. The subset T shall have its gravity center at the origin of the coordinate system. The quality of this partition Q({T,Z}) = nTV ar(T ) + nZV ar(Z) where nT , nZ denote the cardinalities of T,Z and V ar(T ), V ar(Z) their variances (averaged squared distances to gravity center). We will prove by contradiction that by applying our Γ transform we get partition that will be still optimal for the transformed data points. We shall assume the contrary that is that we can transform the set T by some 1 > λ > 0 to T ′ in such a way that optimum of 2-means clustering is not the partition {T ′, Z} but another one, say {A′ ∪D,B′ ∪ C} where Z = C ∪D, A′ and B′ are transforms of sets A,B for which in turn A ∪B = T . It may be easily verified that\nQ({A ∪B,C ∪D}) = nAV ar(A) + nAv2A + nBV ar(B) + nBv2B\n+nCV ar(C) + nDV ar(D) + nCnD nC + nD (vC − vD)2\nwhile\nQ({A ∪ C,B ∪D}) = nAV ar(A) + nDV ar(D) + + nAnD nA + nD (vA − vD)2\n+nBV ar(B) + nCV ar(C) + + nBnC nB + nC (vB − vC)2\nand\nQ({A′ ∪B′, C ∪D}) = nAλ2V ar(A) + nAλ2v2A + nBλ2V ar(B) + nBλ2v2B\n+nCV ar(C) + nDV ar(D) + nCnD nC + nD (vC − vD)2\nwhile\nQ({A′ ∪ C,B′ ∪D}) = nAλ2V ar(A) + nDV ar(D) + + nAnD nA + nD (λvA − vD)2\n+nBλ 2V ar(B) + nCV ar(C) + + nBnC nB + nC (λvB − vC)2\nThe following must hold:\nQ({A′ ∪B′, C ∪D}) > Q({A′ ∪D,B′ ∪ C}) (3)\nand Q({A ∪B,C ∪D}) < Q({A ∪D,B ∪ C}) (4)\nAdditionally also\nQ({A ∪B,C ∪D}) < Q({A ∪B ∪ C,D}) (5)\nand Q({A ∪B,C ∪D}) < Q({A ∪B ∪D,C}) (6)\nThese two latter inequalities imply:\nnCnD nC + nD (vC − vD)2 < (nA + nB)nC (nA + nB) + nC v2C\nand nCnD nC + nD (vC − vD)2 < (nA + nB)nD (nA + nB) + nD v2D\nConsider now an extreme contraction (λ = 0) yielding sets A”, B” out of A,B. Then we have\nQ({A” ∪B”, C ∪D})−Q({A” ∪ C,B” ∪D})\n= nCnD nC + nD (vC − vD)2 − nAnD nA + nD v2D − nBnC nB + nC v2C\n= nCnD nC + nD (vC − vD)2\n− nAnD nA + nD (nA + nB) + nD (nA + nB)nD (nA + nB)nD (nA + nB) + nD v2D\n− nBnC nB + nC (nA + nB) + nC (nA + nB)nC (nA + nB)nC (nA + nB) + nC v2C\n= nCnD nC + nD (vC − vD)2\n− nA nA + nD (nA + nB) + nD (nA + nB) (nA + nB)nD (nA + nB) + nD v2D\n− nB nB + nC (nA + nB) + nC (nA + nB) (nA + nB)nC (nA + nB) + nC v2C\n= nCnD nC + nD (vC − vD)2\n− nA nA + nB (1 + nB nA + nD ) (nA + nB)nD (nA + nB) + nD v2D\n− nB nA + nB (1 + nA nB + nC ) (nA + nB)nC (nA + nB) + nC v2C\n< nCnD nC + nD (vC − vD)2\n− nA nA + nB (nA + nB)nD (nA + nB) + nD v2D\n− nB nA + nB (nA + nB)nC (nA + nB) + nC v2C < 0\nbecause the linear combination of two numbers that are bigger than a third yields another number bigger than this. Let us define a function\nh(x) = +nAx 2v2A + nBx 2v2B + nCnD nC + nD (vC − vD)2\n− nAnD nA + nD (xvA − vD)2 − nBnC nB + nC (xvB − vC)2\nIt can be easily verified that h(x) is a quadratic polynomial with a positive coefficient at x2. Furthermore h(1) = Q({A∪B,C∪D})−Q({A∪C,B∪D}) < 0, h(λ) = Q({A′∪B′, C ∪D})−Q({A′∪C,B′∪D}) > 0, h(0) = Q({A”∪B”, C ∪ D})−Q({A” ∪ C,B” ∪D}) < 0. But no quadratic polynomial with a positive coefficient at x2 can be negative at the ends of an interval and positive in the middle. So we have the contradiction. This proves the thesis that the (globally) optimal 2-means clustering remains (globally) optimal after transformation.\nLet us turn to the general case of k-means. Let the optimal clustering for a given set of objects X consist of k clusters: T and Z1, . . . , Zk−1. The subset T shall have its gravity center at the origin of the coordinate system. The quality of this partition Q({T,Z1, . . . , Zk−1}) = nTV ar(T ) + ∑k−1 i=1 nZiV ar(Zi), where nZi is the cardinality of the cluster Zi. We will prove by contradiction that\nby applying our Γ transform we get partition that will be still optimal for the transformed data points. We shall assume the contrary that is that we can transform the set T by some 1 > λ > 0 to T ′ in such a way that optimum of k-means clustering is not the partition {T ′, Z1, . . . , Zk−1} but another one, say {T ′1∪Z1,1∪· · ·∪Zk−1,1, T ′2∪Z1,2∪· · ·∪Zk−1,2 . . . , T ′k∪Z1,k∪· · ·∪Zk−1,k} where Zi = ∪kj=1Zi,j (where Zi,j are pairwise disjoint), T ′1, . . . , T ′k are transforms of disjoint sets T1, . . . , Tk for which in turn ∪kj=1Tj = T . It may be easily verified that\nQ({T,Z1, . . . , Zk−1}) = k∑ j=1 nTjV ar(Tj) + k∑ j=1 nTjv 2 Tj + k−1∑ i=1 nZiV ar(Zi)\nwhile (denoting Z∗,j = ∪i=1k − 1Z∗,j)\nQ({T1 ∪ Z∗,1, . . . , Tk ∪ Z∗,k}) =\n= k∑ j=1 ( nTjV ar(Tj) + nZ∗,jV ar(Z∗,j) + + nTjnZ∗,j nTj + nZ∗,j (vTj − vZ∗,j )2 )\nwhereas\nQ({T ′, Z1, . . . , Zk−1}) = k∑ j=1 nTjλ 2V ar(Tj) + k∑ j=1 nTjλ 2v2Tj\n+ k−1∑ i=1 nZiV ar(Zi)\nwhile Q({T ′1 ∪ Z∗,1, . . . , T ′k ∪ Z∗,k}) =\n= k∑ j=1 ( nTjλ 2V ar(Tj) + nZ∗,jV ar(Z∗,j) + + nTjnZ∗,j nTj + nZ∗,j (λvTj − vZ∗,j )2 )\nThe following must hold:\nQ({T ′, Z1, . . . , Zk−1}) > Q({T ′1 ∪ Z∗,1, . . . , T ′k ∪ Z∗,k}) (7)\nand Q({T,Z1, . . . , Zk−1}) < Q({{T1 ∪ Z∗,1, . . . , Tk ∪ Z∗,k}) (8)\nAdditionally also\nQ({T,Z1, . . . , Zk−1}) < Q({{T ∪ Z∗,1, Z∗,2, . . . , Z∗,k) (9)\nand Q({T,Z1, . . . , Zk−1}) < Q({T ∪ Z∗,2, Z∗,1, Z∗,3, . . . , Z∗,k}) (10)\nand . . . and\nQ({T,Z1, . . . , Zk−1}) < Q({T ∪ Z∗,k, Z∗,1, . . . , Z∗,k−1}) (11)\nThese latter k inequalities imply that for l = 1, . . . , k:\nQ({T,Z1, . . . , Zk−1}) = nTV ar(T ) + k∑ j=1 nTjV ar(Tj) + k∑ j=1 nTjv 2 Tj\n+ k−1∑ i=1 nZiV ar(Zi) <\nQ({T ∪ Z∗,l, Z∗,1, . . . , Z∗,l−1, Z∗,l+1 . . . , Z∗,k}) =\n= nTV ar(T ) + k∑ j=1 nZ∗,jV ar(Z∗,j) + nTnZ∗,l nT + nZ∗,l (vT − vZ∗,l)2\n+ k−1∑ i=1 nZiV ar(Zi) <\nk∑ j=1 nZ∗,jV ar(Z∗,j) + nTnZ∗,l nT + nZ∗,l (vT − vZ∗,l)2\n+ k−1∑ i=1 nZiV ar(Zi)− k∑ j=1 nZ∗,jV ar(Z∗,j) <\nnTnZ∗,l nT + nZ∗,l (vZ∗,l) 2\nConsider now an extreme contraction (λ = 0) yielding sets Tj” out of Tj . Then we have\nQ({T”, Z1, . . . , Zk−1})−Q({T”1 ∪ Z∗,1, . . . , T”k ∪ Z∗,k})\n= k−1∑ i=1 nZiV ar(Zi)− k∑ j=1 ( nZ∗,jV ar(Z∗,j) + nTjnZ∗,j nTj + nZ∗,j (vZ∗,j ) 2 )\n= k−1∑ i=1 nZiV ar(Zi)− k∑ j=1 nZ∗,jV ar(Z∗,j)\n− k∑ j=1 nTjnZ∗,j nTj + nZ∗,j nT + nZ∗,j nTnZ∗,j nTnZ∗,j nT + nZ∗,j (vZ∗,j ) 2\n= k−1∑ i=1 nZiV ar(Zi)− k∑ j=1 nZ∗,jV ar(Z∗,j)\n− k∑ j=1 nTj nTj + nZ∗,j nT + nZ∗,j nT nTnZ∗,j nT + nZ∗,j (vZ∗,j ) 2\n≤ k−1∑ i=1 nZiV ar(Zi)− k∑ j=1 nZ∗,jV ar(Z∗,j)− k∑ j=1 nTj nT nTnZ∗,j nT + nZ∗,j (vZ∗,j ) 2 < 0\nbecause the linear combination of numbers that are bigger than a third yields another number bigger than this. Let us define a function\ng(x) = k∑ j=1 nTjx 2v2Tj + k−1∑ i=1 nZiV ar(Zi)\n− k∑ j=1 ( nZ∗,jV ar(Z∗,j) + + nTjnZ∗,j nTj + nZ∗,j (xvTj − vZ∗,j )2 )\nIt can be easily verified that g(x) is a quadratic polynomial with a positive coefficient at x2. Furthermore g(1) = Q({T,Z1, . . . , zk−1})−Q({T1 ∪Z∗,1, . . . , Tk ∪ Z∗,k}) < 0, g(λ) = Q({T ′, Z1, . . . , Zk−1}) − Q({T ′1 ∪ Z∗,1, . . . , T ′k ∪ Z∗,k}) > 0, g(0) = Q({T”, Z1, . . . , Zk−1}) − Q({T”1 ∪ Z∗,1, . . . , T”k ∪ Z∗,k}) < 0. But no quadratic polynomial with a positive coefficient at x2 can be negative at the ends of an interval and positive in the middle. So we have the contradiction. This proves the thesis that the (globally) optimal k-means clustering remains (globally) optimal after transformation.\nSo summarizing the new Γ transformation preserves local and global optima of k-means for a fixed k. Therefore k-means algorithm is consistent under this transformation.\nHence\nTheorem 15. k-means algorithm satisfies Scale-Invariance, k-Richness, and centric Consistency.\nNote that (Γ∗ based) centric Consistency is not a specialization of Kleinberg’s consistency as the requirement of increased distance between all elements of different clusters is not required in Γ∗ based Consistency. Note also that the decrease of distance does not need to be equal for all elements as long as the gravity center does not relocate. Also a limited rotation of the cluster may be allowed for."
    }, {
      "heading" : "10 Moving clusters - motion consistency",
      "text" : "As we have stated already, in the Rn it is actually impossible to move clusters in such a way as to increase distances to all the other elements of all the other clusters (see Theorem 10). However, we shall ask ourselves if we may possibly move away clusters as whole, via increasing the distance between cluster centers and not overlapping cluster regions, which, in case of k-means, represent Voronoi-regions.\nProperty 10. A clustering method conforms to motion consistency, if it returns the same clustering when the distances of cluster centers are increased by moving each point of a cluster by the same vector without leading to overlapping of the convex regions of clusters.\nLet us concentrate on the k-means case and let us look at two neighboring clusters. The Voronoi regions, associated with k-means clusters, are in fact polyhedrons, such that the ”outer” polyhedrons (at least one of them) can be moved away from the rest without overlapping any other region.\nSo is such an operation on regions permissible without changing the cluster structure? A closer look at the issue tells us that it is not. As k-means terminates, the neighboring clusters’ polyhedrons touch each other via a hyperplane such that the straight line connecting centers of the clusters is orthogonal to this hyperplane. This causes that points on the one side of this hyperplane lie more closely to the one center, and on the other to the other one. But if we move the clusters in such a way that both touch each other along the same hyperplane, then it happens that some points within the first cluster will become closer to the center of the other cluster and vice versa.31 So moving the clusters generally will change their structure (points switch clusters) unless the points lie actually not within the polyhedrons but rather within ”paraboloids” with appropriate equations. Then moving along the border hyperplane will not change cluster membership (locally). But the intrinsic cluster borders are now ”paraboloids”. What would happen if we relocate the clusters allowing for touching along the ”paraboloids”? The problem will occur again.\nHence the question can be raised: What shape should have the k-means clusters in order to be (locally) immune to movement of whole clusters?\nLet us consider the problem of susceptibility to class membership change within a 2D plane containing the two cluster centers. Let the one cluster center be located at a point (0,0) in this plane and the other at (2x0, 2y0). Let further the border of the first cluster be characterized by a (symmetric) function f(x) and le the shape of the border of the other one g(x) be the same, but properly rotated: g(x) = 2y0 − f(x− 2x0) so that the cluster center is in the same. Let both have a touching point (we excluded already a straight line and want to have convex smooth borders). From the symmetry conditions one easily sees that the touching point must be (x0, y0). As this point lies on the surface of f(), y0 = f(x0) must hold. For any point (x, f(x) of the border of the first cluster with center (0, 0) the following must hold:\n(x− 2x0)2 + (f(x)− 2f(x0))2 − x2 − f2(x) ≥ 0 (12)\nThat is −2x0(2x− 2x0)− 2f(x0) (2f(x)− 2f(x0)) ≥ 0\n−f(x0) (f(x)− f(x0)) ≥ x0(x− x0) 31This is by the way the nice trick behind the claim in [6] that incremental k-means does not identify perfectly separated clusters. Clusters in k-means are not the points, they are polyhedrons, contrary to the assumptions in [6].\nLet us consider only positions of the center of the second cluster below the X axis. In this case f(x0) < 0. Further let us concentrate on x lower than x0. We get\n−f(x)− f(x0) x− x0 ≥ x0 −f(x0)\nIn the limit, when x approaches x0.\n−f ′(x0) ≥ x0\n−f(x0)\nNow turn to x greater than x0. We get\n−f(x)− f(x0) x− x0 ≤ x0 −f(x0)\nIn the limit, when x approaches x0.\n−f ′(x0) ≤ x0\n−f(x0)\nThis implies\n− f ′(x0) = −1 f(x0) x0\n(13)\nNote that f(x0)x0 is the directional tangent of the straight line connecting both cluster centers. As well as it is the directional tangent of the line connecting the center of the first cluster to its surface. f ′(x0) is the tangential of the borderline of the first cluster at the touching point of both clusters. The equation above means both are orthogonal. But this property implies that f(x) must be definition (of a part) a circle centered at (0, 0). As the same reasoning applies at any touching point of the clusters, a k-means cluster would have to be (hyper)ball-shaped in order to allow the movement of the clusters without elements switching cluster membership.\nThe tendency of k-means to recognize best ball-shaped clusters has been known long ago, but we are not aware of presenting such an argument for this tendency.\nIt has to be stated however that clusters, even if enclosed in a ball-shaped region, need to be separated sufficiently to be properly recognized. Let us consider, under which circumstances a cluster C1 of radius r1 containing n1 elements would take over n21 elements (subcluster C21) of a cluster C2 of radius r2 of cardinality n2. Let n22 = n2−n21 be the number of the remaining elements (subcluster C22 of the second cluster. Let the enclosing balls of both clusters be separated by the distance (gap) g. Let us consider the worst case that is that the center of the C21 subcluster lies on a straight line segment connecting both cluster centers. The center of the remaining C22 subcluster would lie on the same line but on the other side of the second cluster center. Let r21, r22 be distances of centers of n21 and n22 from the center of the second cluster. The relations\nn21 · r21 = n22 · r22, r21 ≤ r2, r22 ≤ r2\nmust hold. Let denote with SSC(C) the sum of squared distances of elements of the set C to the center of this set.\nSo in order for the clusters to be stable\nSSC(C1) + SSC(C2) ≤ SSC(C1 ∪ C21) + SSC(C22)\nmust hold. But\nSSC(C2) = SSC(C21) + SSC(C22) + n21 · r221 + n22 · r222\nSSC(C1 ∪ C21) = SSC(C1) + SSC(C21) + n1n21 n1 + n21 (r1 + r2 + g − r21)2\nHence\nSSC(C1) + SSC(C21) + SSC(C22) + n21 · r221 + n22 · r222\n≤ SSC(C1) + SSC(C21) + n1n21 n1 + n21 (r1 + r2 + g − r21)2 + SSC(C22)\nn21 · r221 + n22 · r222 ≤ n1n21 n1 + n21 (r1 + r2 + g − r21)2\nn21 · r221 + n22 · r222 n1n21 n1+n21\n≤ (r1 + r2 + g − r21)2√ n21 · r221 + n22 · r222\nn1n21 n1+n21\n≤ r1 + r2 + g − r21\n√ n21 · r221 + n22 · r222\nn1n21 n1+n21\n− r1 − r2 + r21 ≤ g\n√ n21 · r221 + n21 · r21 · r22\n1 1/n1+1/n21\n− r1 − r2 + r21 ≤ g\n√ (n21 · r221 + n21 · r21 · r22)(1/n1 + 1/n21)− r1 − r2 + r21 ≤ g√\n(r221 + r21 · r22)(n21/n1 + 1)− r1 − r2 + r21 ≤ g\nAs r22 = r21n21 n2−n21√ (r221 + r21 ·\nr21n21 n2 − n21 )(n21/n1 + 1)− r1 − r2 + r21 ≤ g\nr21\n√ (1 +\nn21 n2 − n21 )(n21/n1 + 1)− r1 − r2 + r21 ≤ g\nr21\n√ n2\nn2 − n21 n1 + n21 n1 − r1 − r2 + r21 ≤ g\nr21 √ n2 n1 n1 + n21 n2 − n21 − r1 − r2 + r21 ≤ g\nLet us consider the worst case when the elements to be taken over are at the ”edge” of the cluster region (r21 = r2). Then\nr2 √ n2 n1 n1 + n21 n2 − n21 − r1 ≤ g\nThe lower limit on g will grow with n21, but n21 ≤ 0.5n2, because otherwise r22 would exceed r2. Hence in the worst case\nr2 √ n2 n1 n1 + n2/2 n2/2 − r1 ≤ g\nr2 √\n2(1 + 0.5n2/n1)− r1 ≤ g (14) In case of clusters with equal sizes and equal radius this amounts to\ng ≥ r1( √ 3− 1) ≈ 0.7r1\nSo we can conclude\nTheorem 16. k-means algorithm conforms (locally) to Motion Consistency axiom.\nNote that the motion consistency axiom is a substitute for outer-consistency which is impossible continuously in Euclidean space. It is to be underlined that we speak here about local optimum of k-means. With the abovementioned gap size the global k-means minimum may lie elsewhere, in a clustering possibly without gaps. Also the motion consistency transformation preserves as local minimum the partition it is applied to. Other local minima and global minimum can change.\nNote that compared to inner-consistency the centric consistency is quite rigid. And so is motion consistency compared to outer-consistency.\nIn two subsequent sections we will investigate if the rigidity of these transformations can be weakened under appropriate width of the gaps, and if we can grant these properties under global minimum. In particular, we shall study, how well the clusters need to be separated so that it is enough to find the global optimum."
    }, {
      "heading" : "11 Cluster separation versus Kleinberg’s axioms",
      "text" : "of k-richness, consistency, scaling invariance"
    }, {
      "heading" : "11.1 Perfect ball clusterings",
      "text" : "The problem with k-means (-random and ++) is the discrepancy between the theoretically optimized function ()k-means-ideal) and the actual approximation of this value. It appears to be problematic even for ”well-separated” clusters.\nFirst let us point to the fact that ”well-separatedness” may keep the algorithm in a local minimum.\nIt is commonly assumed that a good initialization of a k-means clustering is one where the seeds hit different clusters. It is well known, that under some circumstances the k-means does not recover from poor initialization and as a consequence a natural cluster may be split even for ”well-separated” data.\nBut hitting each cluster may be not sufficient as neighboring clusters may be able to shift the cluster center away from its cluster.\nHence let us investigate what kind of well-separability would be sufficient to ensure that once clusters are hit by one seed each, would never loose the cluster center.\nLet us investigate the working hypothesis that two clusters are well separated if we can draw a ball of some radius ρ around true cluster center of each of them and there is a gap between these balls. We claim that\nTheorem 17. If the distance between any the cluster centers A,B is at least 4ρAB, where ρAB is the radius of a ball centered at A and enclosing its cluster (that is cluster lies in the interior of the ball) and it also is the radius of a ball centered at B and enclosing its cluster, then once each cluster is seeded the clusters cannot loose their cluster elements for each other during k-meansrandom and k-means++ iterations.\nBefore starting the proof, let us introduce related definitions.\nDefinition 4. We shall say that clusters centered at A and B and enclosed in balls centered at A,B and with radius ρAB each are nicely ball-separated, if the distance between A,B is at least 4ρAB. If all pairs of clusters are nicely ball separated with the same ball radius, then we shall say that they are perfectly ball-separated.\nProof. For the illustration of the proof see Figure 5. Consider the two points A,B being the two ball centers and two points, X,Y , one being in each ball (presumably the cluster centers at some stage of the k-means algorithm). To represent their distances faithfully, we need at most a 3D space.\nLet us consider the plane established by the line AB and parallel to the line XY . Let X ′ and Y ′ be projections of X,Y onto this plane. Now let us establish that the hyperplane π orthogonal to X,Y , and passing through the middle of the line segment XY , that is the hyperplane containing the boundary between clusters centered at X and Y does not cut any of the balls centered at A and B. This hyperplane will be orthogonal to the plane of the Figure 5 and so it will manifest itself as an intersecting line l that should not cross circles around A and B, being projections of the respective balls. Let us draw two solid lines k,m between circles O(A, ρ) and O(B, ρ) tangential to each of them. Line l should lie between these lines, in which case the cluster center will not jump to the other ball.\nLet the line X ′Y ′ intersect with the circles O(A, ρ) and O(B, ρ) at points C,D,E, F as in the figure.\nseparation\nIt is obvious that the line l would get closer to circle A, if the points X’, Y’ would lie closer to C and E, or closer to circle B if they would be closer to D and F .\nTherefore, to show that the line l does not cut the circle O(A, ρ) it is sufficient to consider X ′ = C and Y ′ = E. (The case with ball Ball(B, ρ) is symmetrical).\nLet O be the center of the line segment AB. Let us draw through this point a line parallel to CE that cuts the circles at points C ′, D′, E′ and F ′. Now notice that centric symmetry through point O transforms the circles O(A, ρ),O(B, ρ) into one another, and point C ′ into F ′ and D′ into E′. Let E∗ and F ∗ be images of points E and F under this symmetry.\nIn order for the line l to lie between m and k, the middle point of the line segment CE shall lie between these lines.\nLet us introduce a planar coordinate system centered at O with X axis parallel to lines m, k, such that A has both coordinates non-negative, and B non-positive. Let us denote with α the angle between the lines AB and k. As we assume that the distance between A and B equals 4ρ, then the distance between lines k and m amounts to 2ρ(2 sin(α)− 1). Hence the Y coordinate of line k equals ρ(2 sin(α)− 1).\nSo the Y coordinate of the center of line segment CE shall be not higher than this. Let us express this in vector calculus:\n4(yOC + yOE)/2 ≤ ρ(2 sin(α)− 1)\nNote, however that\nyOC +yOE = yOA+yAC +yOB+yBE = yAC +yBE = yAC−yAE∗ = yAC +yE∗A\nSo let us examine the circle with center at A. Note that the lines CD and E∗F ∗ are at the same distance from the line C’D’. Note also that the absolute values of direction coefficients of tangentials of circle A at C’ and D’ are identical. The more distant these lines are, as line CD gets closer to A, the yAC gets bigger, and yE∗A becomes smaller. But from the properties of the circle we see that yAC increases at a decreasing rate, while yE∗A decreases at an increasing rate. So the sum yAC + yE∗A has the biggest value when C is identical with C\n′ and we need hence to prove only that\n(yAC′ + yD′A)/2 = yAC′ ≤ ρ(2 sin(α)− 1)\nLet M denote the middle point of the line segment C ′D′. As point A has the coordinates (2ρ cos(α), 2ρ sin(α)), the point M is at distance of 2ρ cos(α) from A. But C ′M2 = ρ2 − (2ρ cos(α))2.\nSo we need to show that\nρ2 − (2ρ cos(α))2 ≤ (ρ(2 sin(α)− 1))2\nIn fact we get from the above\nρ2 − 4ρ2 cos(α)2 ≤ ρ2(2 sin(α)− 1)2\nDividing by ρ2\n1− 4 cos(α)2 ≤ (2 sin(α)− 1)2\n1− 4 cos(α)2 ≤ 4 sin(α)2 − 4 sin(α) + 1\nAdding 4 cos(α)2 to both sides and subtracting 1 we get\n0 ≤ 4− 4 sin(α)\nDividing by 4 0 ≤ 1− sin(α)\nwhich is a known trigonometric relation. This means in practice that whatever point from the one and the other cluster is picked randomly as cluster center, then the Voronoi tessellation of the space will contain only points from a single cluster.\nLet us discuss at this point a bit the notions of ”perfect separation” as introduced in [6]. In their Theorem 4.4. Ackerman and Dasgupta [6] show that the incremental k-means algorithm, as introduced in their Algorithm 2.2 , is not able to cluster correctly data that is ”perfectly clusterable” (their Definition 4.1). However, it is obvious that under the ”perfect-ball-separation” as introduced here their incremental k-means algorithm32 will discover the structure of the clusters. The reason is as follows. Perfect ball separation ensures that there exists an r of the enclosing ball such that the distance between any two points within the same ball is lower than 2r and between them is bigger than 2r. So whenever Ackerman’s incremental k-mean merges two points, they are the points of the same ball. And upon merging the resulting point lies again within the ball. So we can conclude\nTheorem 18. The incremental k-means algorithm will discover the structure of perfect-ball-clustering.\nLet us note at this point, however, that the incremental k-means algorithm would return only a set of cluster centers without stating whether or not we got a perfect ball clustering. But it is important to know if this is the case because otherwise the resulting set of cluster centers may be arbitrary and under unfavorable conditions it may not correspond to a local minimum of k-means ideal at all. However, if we are allowed to inspect the data for the second time,\n32 Algorithm 2.2. (Sequential k-means) should be slightly modified: Set T = (t1, . . . , tk) to the first k data points Initialize the counts n1, n2, . . . , nk to 1 Repeat:\nAcquire the next example, tk+1. Set nk+1 = 1 If ti is the closest center to tj , j 6= i,\nReplace ti = (tini + tjnj)/(ni + nj), thereafter ni = ni + nj If j 6= k + 1 then replace tj = tk+1, nj = nk+1.\nsuch an information can be provided.33 A second pass for other algorithms from their section 2 would not yield such a decision.\nThe difference between our and their definition of well separatedness lies essentially in their understanding of clustering as a partition of data points, while in fact the user is interested in partition of the sample space (in terms of learnability theory of Valiant). Hence also a further correction of Kleinberg’s axiomatic framework should take this flaw into account.\nLet us further turn to their concept of ”nice clustering” (their Def. 3.1.). As they show in their Theorem 3.8., nice clustering cannot be discovered by an incremental algorithm with memory linear in k. In Theorem 5.3 they show that their incremental algorithm 5.2. with up to 2k−1 cluster centers can detect points from each of nice clusters. Again it is not the incremental k-means that may achieve it (see their theorem 5.7.) even under ”nice convex” conditions. Surely our concept of nice-ball-clustering is even more restrictive than their ”nice-convex” clustering. But if we upgrade their CANDIDATES(S) algorithm so that it behaves like k-means that is if we replace the step ”Moving bottom-up, assign each internal node the data point in one of its children” with the assignment to the internal node the properly weighted (with respective cardinalities of leaves) average, then the algorithm 5.2. upgraded to incremental k-means version will in fact return the ”refinement” of the clustering.34 What is more, if we are allowed to have a second pass through the data, then we can pick out the real cluster centers using an upgrade of the CANDIDATES(S) algorithm. The other algorithms considered in their section 5 will fail to do this on the second pass through the data (because of deviations from true cluster center).35\n33 One shall proceed as follows on the second pass: Let T = (t1, . . . , tk) be the resulting set of cluster centers from the first pass. Initialize the furthest neighbors f1, f2, . . . , fk with t1, t2 . . . , tk respectively. Repeat:\nAcquire the next example, x. If ti is the closest center to x,\nif x is further away from ti than fi then replace fi with X. Compute distances between corresponding ti and fi, pick the highest one, compute distances between each pair ti, tj and pick the lowest one. If the latter is 4 times or more higher than the former one, we got a perfect ball clustering.\n34The modified algorithm would look like: CANDIDATES(S) Run single linkage on S to get a tree (distances between t are used) Assign each leaf node the corresponding data point Moving bottom-up, assign each internal node the n = nL + nR, t = (tLnL + tRnR)/n, L,R indicating left and right child. Return all points at distance < k from the root\n35The needed algorithm would look like: Take the tree from the first pass with t values assigned in the first pass. Assign each node an f value identical to t value. Repeat:\nAcquire the next example, x. Find the leaf with t closest to x. Update its f value with x if it is further away from t than f . Pass x to all direct and indirect ancestors (internal) nodes of this leaf\nwhere in each of these nodes update its f value with x\nLet us discuss Kleinberg axioms for perfectly ball-separated clusters. It is clear that if k-means random or k-means++ gets initiated in such a way that each initial cluster center hits a different cluster, then upon subsequent steps the cluster centers will not leave the clusters. One gets stuck in a minimum, not necessarily the global one. Let us understand the Kleinberg’s phrase ”the function returns the clustering” as one of possible (local) minima of the clustering functions. k-richness is trivially granted if we restrict ourselves to perfectlyball-separated clusters. If one performs the scaling on perfectly ball separated clusters, they will remain perfectly ball separated (scale invariance). If one applies moving-consistency transformation (keeping inner distances and relative positions to the cluster fixed coordinate systems, not bothering about distances between elements in distinct clusters) then the clusters will remain perfectly ball separated. Also a centric-consistency transformation will keep the partition in the realm of perfect-ball-clusterings. Hence\nTheorem 19. k-means, if restricted to perfectly ball separated clusterings, conforms (locally) to k-richness, scale-invariance, motion consistency and centric consistency.\nBut we gain still something more.\nProperty 11. A clustering method conforms to inner cluster consistency, if it returns the same clustering when the positions distances of cluster centers are kept, while the distances within each cluster are decreased\nNote that inner cluster consistency, as compared to inner-consistency, is less restrictive as one does not need to care about distances between elements in different clusters.\nIf one performs an inner cluster consistency transformation, the clusters will remain perfectly ball separated (a kind of inner-consistency). So we get\nTheorem 20. k-means, if restricted to perfectly ball separated clusterings, conforms (locally) to k-richness, scale-invariance, motion consistency and inner cluster consistency.\nAs with perfect clustering (see [6]), also if there exists a perfect ball clustering into k clusters, then there exists only one such clustering. Regrettably, via an inner cluster consistency transformation for a data set with perfect ball kclustering one can obtain a data set for which perfect ball k + l clustering is possible for an l > 0 even if it was impossible prior to transformation. Albeit only nested clusters will emerge. If one would choose to have the largest number of clusters with cluster cardinality ≥ 2, then one can speak about ”refinement inner cluster consistency”, with the direction of the refinement towards smaller clusters.\nif it is further away from t than f . For each cut of the tree engaging exactly k nodes check if the nice ball clustering condition is fulfilled for balls rooted at t with radii ‖f − t‖. If for any such a cut the condition holds, the nice ball clustering is found, otherwise it is not.\nSimilarly, if we move away cluster centers (motion consistency transformation), we can obtain a new perfect ball k − l clustering even if it did not exist prior to the transformation. Again, cluster nesting occurs. So if one would choose to have the lowest number of clusters k ≥ 2, then one can speak about ”refinement motion consistency”, with the direction of the refinement towards larger clusters.\nThe very same statements can be made about Kleinberg’s axioms for nice ball clustering and k-means. Except that for a given k the clustering, if exists, does not need to be unique.\nLast not least let us make the remark that even if the perfect-ball-clustering exists, it does not need to be the global optimum of k-means ideal, because of possible different cardinalities of these clusters. So in fact the global optimum may be one that is imperfect, even if the perfect clustering exists.\nBut let us state one more thing. Assume that we allow for a broader range of k values with k-means. Note that with centric consistency, contrary to inner cluster consistency transform, no new perfect ball structures will emerge. Therefore:\nTheorem 21. k-means, with k ranging over a set of values, if we assume that it returns the perfectly/nicely ball separated clusterings for the largest possible k (excluding too small clusters, we call it max-k-means algorithm), then it conforms (locally) to richness, scale-invariance, motion consistency and centric consistency."
    }, {
      "heading" : "11.2 Core-based clusterings",
      "text" : "But as we have seen in the previous section, for various purposes the distance between the balls enclosing clusters may be smaller. So let us discuss what happens if the distances (gaps) between clusters are smaller.\nWe claim that\nTheorem 22. Let A,B be cluster centers. Let ρAB be the radius of a ball centered at A and enclosing its cluster and it also is the radius of a ball centered at B and enclosing its cluster. If the distance between the cluster centers A,B amounts to 2ρAB + g, g > 0 (g being the ”gap” between clusters), if we pick any two points, X from the cluster of A and Y from the cluster of B, then the new clusters will preserve the balls centered at A and B of radius g/2 (called subsequently ”cores”) each (X the core of A, Y the core of B).\nDefinition 5. If the gap between each pair of clusters fulfills the condition of the above theorem, then we say that we have core-clustering.\nProof. For the illustration of the proof see Figure 6. The proof does not differ too much from the previous one and in fact the previous theorem is a special case when g = 2ρ. Consider the two points A,B being the two centers of double balls. The inner call represents the core of radius g/2, the outer ball of radius ρ (ρ = ρAB).\ngap\nConsider two points, X,Y , one being in each outer ball (presumably the cluster centers at some stage of the k-means algorithm). To represent their distances faithfully, we need at most a 3D space.\nLet us consider the plane established by the line AB and parallel to the line XY . Let X ′ and Y ′ be projections of X,Y onto this plane. Now let us establish that the hyperplane π orthogonal to X,Y , and passing through the middle of the line segment XY , that is the hyperplane containing the boundary between clusters centered at X and Y does not cut any of the balls centered at A and B. This hyperplane will be orthogonal to the plane of the Figure 6 and so it will manifest itself as an intersecting line l that should not cross inner circles around A and B, being projections of the respective balls. Let us draw two solid lines k,m between circles O(A, g/2) and O(B, g/2) tangential to each of them. Line l should lie between these lines, in which case the cluster center will not jump to the other ball.\nLet the line X ′Y ′ intersect with the circles O(A, ρ) and O(B, ρ) at points C,D,E, F as in the figure.\nIt is obvious that the line l would get closer to circle A, if the points X ′, Y ′ would lie closer to C and E, or closer to circle B if they would be closer to D and F .\nTherefore, to show that it does not cut the circle O(A, g/2) it is sufficient to consider X ′ = C and Y ′ = E. (The case with ball Ball(B, g/2) is symmetrical).\nLet O be the center of the line segment AB. Let us draw through this point a line parallel to CE that cuts the circles at points C ′, D′, E′ and F ′. Now notice that centric symmetry through point O transforms the circles O(A, ρ),O(B, ρ) into one another, and point C ′inF ′ and D′inE′. Let E∗ and F ∗ be images of points E and F under this symmetry.\nIn order for the line l to lie between m and k, the middle point of the line segment CE shall lie between these lines.\nLet us introduce a planar coordinate system centered at O with X axis parallel to lines m, k, such that A has both coordinates non-negative, and B non-positive. Let us denote with α the angle between the lines AB and k. As we assume that the distance between A and B equals 2ρ+ g, then the distance between lines k and m amounts to 2((ρ + g/2) sin(α) − g/2). Hence the Y coordinate of line k equals ((ρ+ g/2) sin(α)− g/2).\nSo the Y coordinate of the center of line segment CE shall be not higher than this. Let us express this in vector calculus:\n4(yOC + yOE)/2 ≤ ((ρ+ g/2) sin(α)− g/2)\nNote, however that\nyOC +yOE = yOA+yAC +yOB+yBE = yAC +yBE = yAC−yAE∗ = yAC +yE∗A\nSo let us examine the circle with center at A. Note that the lines CD and E∗F ∗ are at the same distance from the line C’D’. Note also that the absolute values of direction coefficients of tangentials of circle A at C’ and D’ are identical. The more distant these lines are, as line CD gets closer to A, the yAC gets bigger,\nand yE∗A becomes smaller. But from the properties of the circle we see that yAC increases at a decreasing rate, while yE∗A decreases at an increasing rate. So the sum yAC + yE∗A has the biggest value when C is identical with C\n′ and we need hence to prove only that\n(yAC′ + yD′A)/2 = yAC′ ≤ ((ρ+ g/2) sin(α)− g/2)\nLet M denote the middle point of the line segment C ′D′. As point A has the coordinates ((ρ + g/2) cos(α), (ρ + g/2) sin(α)), the point M is at distance of (ρ+ g/2) cos(α) from A. But C ′M2 = ρ2 − ((ρ+ g/2) cos(α))2.\nSo we need to show that\nρ2 − ((ρ+ g/2) cos(α))2 ≤ ((ρ+ g/2) sin(α)− g/2)2\nIn fact we get from the above\nρ2 − ((ρ+ g/2) cos(α))2 ≤ ((ρ+ g/2) sin(α))2 + (g/2)2 − 2(ρ+ g/2)(g/2) sin(α)\nρ2 ≤ (ρ+ g/2)2 + (g/2)2 − 2(ρ+ g/2)(g/2) sin(α) 0 ≤ 2(ρ+ g/2)(g/2)− 2(ρ+ g/2)(g/2) sin(α)\n0 ≤ 2(ρ+ g/2)(g/2)(1− sin(α)) which is obviously true, as sin never exceeds 1.\nBut we have still to ask what is the gain of having an untouched core. Consider a cluster C of a clustering C and let it have the share p of its mass at its core of radius (g/2) and the remaining 1 − p in the ball of radius ρ (all identical for each cluster from the clustering) and that the gaps between clusters amount to at least g. Let X be a randomly picked point from this cluster to be used as an initial cluster center for k-means. If it happens that each initial cluster center lies in the appropriate core, then in the first iteration of k-means all clusters are properly formed.\nIf however cluster centers lie off core then you have a chance that in the first iteration some clusters possess stranger cluster elements, but these strangers come not from the cores of other clusters. Hence we would be interested in getting the cluster centers into the cores in the next iteration. In the worst case a cluster C may lose all its off-core elements to other clusters and obtain all the other off-core elements.\nThe question is now: what portion (1− p) shall be allowed to lie off-core to ensure the convergence of iteration step. The answer is:\n(g/2/ρ) ∗ nc/((g/2/ρ) ∗ nc − (g/2/ρ) ∗ (n− nc) + n) where n is the total number of elements, nc is the cardinality of the cluster.\nClearly, with this core separation incremental k-means will fail usually to recover the clustering. But if either of the well-separatedness criterion of core-clustering, perfect-ball-clustering or nice-ball-clustering applies, k-meansrandom and k-means++ will find the appropriate clusters, if it is seeded with one representative of each cluster. The theorems 20 and 21 when substituting ”perfect” with ”core” clustering, apply.\n11.3 k-richness and the problems with realistic k-means algorithms\nBut what is the probability of such a seeding of the k-means that each cluster has a seed? Let us consider the k-means-random. If the share of elements in each cluster amounts to p1, . . . , pk, pi ≥ p respectively, then the probability of appropriate seeding in a single run amounts to at least q = ∏k−1 j=1 (1− (k− j)p). After say m runs, we can increase the probability of appropriate seeding to 1− (1− q)m, and reach the required success probability of e.g. 96%.\nUnder k-means++, in case of at least 4ρ distances between clusters (perfect ball clustering) these probabilities amount to\nq = k−1∏ j=1 (3ρ)2(k − j)p (3ρ)2(k − j)p+ (2ρ)2(1− (k − j)p)\nNow it becomes obvious why the k-richness axiom does not make much sense. Even if the clusters should turn out to be well separated (perfect ball clustering existent), the probability of hitting a cluster with 1 element out of n with growing sample size n is prohibitively small. Under k-means random for l such small clusters it is lower than 1\nnl . So the number of required restarts\nof k-means will grow approximately linearly with nk−1, which is better than the exhaustive search with at least kn−k possibilities, but it is still prohibitive. This would render k-means useless. Respective retrial counts look significantly better for k-means++ but are still unacceptable.\n11.4 k-means++ with dispersion off-core elements\nAlternatively we can consider the off-core elements as noise that does not need to be bounded by any ball. The cores then are parts of the cluster such that they are enclosed into balls centered at cluster center where the distance to the other ball centers is four times the own radius of the core. In this case we can apply k-means++ with the provision of rejecting p · n most distant elements upon initialization. p must be surely lower than the core of the smallest cluster. By rejecting p share of elements we run at risk of removing parts of most distant cluster. So to keep it to be likely included in seeding we must keep bounded the ration of noise contribution and cluster contribution. Noise would be at distance 4ρ while the cluster at 2.5ρ in unfavorable case. So to balance the contribution the noise to cluster minus noise ratio should be 2.52/42 = 1/2.56 So that the noise to smallest cluster ration should be 1:3.56.\nThis speaks again against the k-richness. Again theorems analogous to 20 and 21 apply, but now limited to the cores and not entire clusters. The noise allowed should not push cluster centers off core if other clusters are seeded in cores.\n12 k-richness versus global minimum of k-means\nLast not least let us discuss the issue whether or not we can tell that the wellseparated clusters constitute the global minimum of k-means (recall that perfect ball clustering did not).\nWe will investigate below under what circumstances it is possible to tell, without exhaustive check that the well separated clusters are the global minimum of k-means. We will see that the ratio between the largest and the smallest cluster cardinality plays here an important role. Therefore k-richness is in fact not welcome.\nIn particular, let us consider the set of k clusters C = {C1, . . . , Ck} of cardinalities n1, . . . , nk and with radii of balls enclosing the clusters (with centers located at cluster centers) r1, . . . , rk.\nWe are interested in a gap g between clusters such that it does not make sense to split each cluster Ci into subclusters Ci1, . . . , Cik and to combine them into a set of new clusters S = {S1, . . . , Sk} such that Sj = ∪ki=1Cij .\nWe seek a g such that the highest possible central sum of squares combined over the clusters Ci would be lower than the lowest conceivable combined sums of squares around respective centers of clusters Sj . Let V ar(C) be the variance of the cluster C (average squared distance to cluster gravity center). Let rij be the distance of the center of subcluster Cij to the center of cluster Ci. Let vilj be the distance of the center of subcluster Cij to the center of subcluster Clj . So the total k-means function for the set of clusters (C1, . . . , Ck) will amount to:\nQ(C) = k∑ i=1 k∑ j=1 (nijV ar(Cij) + nijr 2 ij) (15)\nAnd the total k-means function for the set of clusters (S1, . . . , Sk) will amount to: Q(S) = k∑ j=1 (( k∑ i=1 nijV ar(Cij) ) + ( k∑ i=1 nij) ( k−1∑ i=1 k∑ l=i+1 nij∑k i=1 nij nlj∑k i=1 nij v2ilj )) (16)\nShould (C1, . . . , Ck) constitute the absolute minimum of the k-means target function, then Q(S) ≥ Q(C) should hold, that is:\nk∑ j=1 (( k∑ i=1 nijV ar(Cij) ) + ( k∑ i=1 nij) ( k−1∑ i=1 k∑ l=i+1 nij∑k i=1 nij nlj∑k i=1 nij v2ilj ))\n≥ k∑ i=1 k∑ j=1 (nijV ar(Cij) + nijr 2 ij)\nThis implies:\nk∑ j=1 ( k−1∑ i=1 k∑ l=i+1 nijnlj∑k i=1 nij v2ilj ) ≥ k∑ i=1 k∑ j=1 nijr 2 ij (17)\nTo maximize ∑k j=1 nijr 2 ij for a single cluster Ci of enclosing ball radius ri, note that you should set rij to ri. Let mj = arg maxj∈{1,...,k} nij . If we set rij = ri for all j except mj , then the maximal rimj is delimited by the relation∑k j=1;j 6=mj nijrij ≥ nimjrimj . So\nk∑ j=1 nijr 2 ij ≤ ( k∑ j=1;j 6=mj nij)r 2 i min(2, (1 +\n∑k j=1;j 6=mj nij\nnimj )) (18)\n≤2( k∑\nj=1;j 6=mj\nnij)r 2 i\nSo if we can guarantee that the gap between cluster balls (of clusters from C) amounts to g then surely\nk∑ j=1 ( k−1∑ i=1 k∑ l=i+1 nijnlj∑k i=1 nij v2ilj ) ≥ g2 k∑ j=1 ( k−1∑ i=1 k∑ l=i+1 nijnlj∑k i=1 nij ) (19)\nbecause in such case g ≤ vilj for all i, l, j. By combining inequalities (17), (18) and (19) we see that the global minimum is granted if the following holds:\ng2 k∑ j=1 ( k−1∑ i=1 k∑ l=i+1 nijnlj∑k i=1 nij ) ≥ 2 k∑ i=1 ( k∑ j=1;j 6=mj nij)r 2 i (20)\nOne can distinguish two cases: either (1) there exists a cluster St containing two subclusters Cpt, Cqt such that t = arg maxj |Cpj | and t = arg maxj |Cqj | (maximum cardinality subclasses of their respective original clusters Cp, Cq or (2) not.\nConsider the first case. Let Cp, Cq be the two clusters where Cpt and Cqt be two subclusters of highest cardinality within Cp, Cq resp. This implies that npt ≥ 1knp, nqt ≥ 1 knq. Also this implies that for i 6= p, i 6= q nit ≤ ni/2.\nk∑ j=1 k−1∑ i=1 k∑ l=i+1 nijnlj∑k i=1 nij\n≥ k−1∑ i=1 k∑ l=i+1 nitnlt∑k i=1 nit\n≥ nptnqt∑k i=1 nit\n≥ nptnqt np/2 + nq/2 + ∑k i=1 ni/2 = nptnqt np/2 + nq/2 + n/2\n≥ 1 k2 npnq np/2 + nq/2 + n/2\nNote that\n2 k∑ i=1 ( k∑ j=1;j 6=mj nij)r 2 i ≤ 2 k∑ i=1 nir 2 i\nSo, in order to fulfill inequality (20), it is sufficient to require that\ng ≥ √√√√ 2∑ki=1 nir2i 1 k2 npnq np/2+nq/2+n/2 = k √ np/2 + nq/2 + n/2 √ 2 ∑k i=1 nir 2 i npnq = k √ np + nq + n √∑k i=1 nir 2 i npnq (21) This of course maximized over all combinations of p, q.\nLet us proceed to the second case. Here each cluster Sj contains a subcluster of maximum cardinality of a different cluster Ci. As the relation between Sj and Ci is unique, we can reindex Sj in such a way that actually Cj contains its maximum cardinality subcluster Cjj . Let us rewrite the inequality (20).\ng2 k∑ j=1 ( k−1∑ i=1 k∑ l=i+1 nijnlj∑k i=1 nij ) − 2 k∑ i=1 ( k∑ j=1;j 6=mj nij)r 2 i ≥ 0\nThis is met if\ng2 k∑ j=1 j−1∑ i=1 nijnjj∑k i=1 nij + k∑ l=j+1 njjnlj∑k i=1 nij − 2 k∑ i=1 (ni − nii)r2i ≥ 0\nThis is the same as:\ng2 k∑ j=1  ∑ i=1,...,j−1,j+1,...,k nijnjj∑k i=1 nij − 2 k∑ i=1 (ni − nii)r2i ≥ 0\nThis is fulfilled if:\ng2 k∑ j=1  ∑ i=1,...,j−1,j+1,...,k\nnijnj/k nj/2 + ∑k i=1 ni/2 − 2 k∑ i=1 (ni − nii)r2i ≥ 0\nLet M be the maximum over n1, . . . , nk. The above holds if\ng2 k∑ j=1  ∑ i=1,...,j−1,j+1,...,k nijnj/k M/2 + n/2 − 2 k∑ i=1 (ni − nii)r2i ≥ 0\nLet m be the minimum over n1, . . . , nk. The above holds if\ng2 k∑ j=1  ∑ i=1,...,j−1,j+1,...,k nijm/k M/2 + n/2 − 2 k∑ i=1 (ni − nii)r2i ≥ 0\nThis is the same as\ng2 m/k\nM/2 + n/2  k∑ j=1 ∑ i=1,...,j−1,j+1,...,k nij − 2 k∑ i=1 (ni − nii)r2i ≥ 0\ng2 m/k\nM/2 + n/2  k∑ j=1 (( k∑ i=1 nij ) − njj ) − 2 k∑ i=1 (ni − nii)r2i  ≥ 0\ng2 m/k\nM/2 + n/2  k∑ j=1 k∑ i=1 nij − ( k∑ j=1 njj) − 2( k∑ i=1 (ni − nii)r2i ) ≥ 0\ng2 m/k\nM/2 + n/2 ( k∑ i=1 ni ) − ( k∑ j=1 njj) − 2 k∑ i=1 (ni − nii)r2i ≥ 0\ng2 m/k\nM/2 + n/2 ( k∑ i=1 (ni − nii) ) − 2 k∑ i=1 (ni − nii)r2i ≥ 0\nk∑ i=1 (ni − nii) ( g2\nm/k\nM/2 + n/2 − 2r2i\n) ≥ 0\nThe above will hold, if for every i = 1, . . . , k g ≥ ri √ 2 m/k\nM/2+n/2\ng ≥ ri √ k M + n\nm (22)\nSo the inequality (20) is fulfilled, if both inequality (21) and inequality (22) are held by an appropriately chosen g.\nSo we may call the above-mentioned ”well-separatedness” as ”absolute clustering”. One sees immediately that inner cluster consistency is kept, this time in terms of global optimum, under the restraint to k clusters.\nTheorem 23. k-means, if restricted to absolute clusterings, conforms globally to k-richness, scale-invariance, motion consistency and inner cluster consistency.\nRegrettably, a structure may emerge upon such consistency and therefore the maximal number of possible absolute clusters is not kept. However, if we apply centric consistency, the max-k-means[absolute] keeps the richness/invariance/motion consistency axioms.\nTheorem 24. k-means, with k ranging over a set of values, if we assume that it returns the absolutely separated clusterings for the largest possible k (excluding cluster sizes considered as too small), then it conforms globally to richness, scale-invariance, motion consistency and centric consistency.\nIn the end let us make a remark on Theorem 23. If one applies Kleinberg’s consistency transformation in Euclidean space, not continuously of course, because it is not possible, as already shown, but in a discrete manner, with ”jumping” clusters, then this transform can be represented as (again in discrete manner) a superposition of motion consistency transform and inner cluster consistency transform. The reason is as follows: Consider a cluster C and a point x from another cluster C2. Let us compute the distance between x and the cluster center µC of the cluster C prior and after Kleinberg’s consistency transformation to see that it increases. Consider the distance ‖x−µC‖2. It may be expressed as a multiple (factor |C|+1|C| ) of the distance between x and the center\nµ of the data set C ∪ {x}. And ‖x− µ‖2 = 1|C|+1 ∑\ny∈C ‖x− y‖2. Hence it is obvious that increasing distance between x and elements of C, we increase also the distance of x to the cluster center of C.\nSo one can generalize that also the distances between clusters C,C2 increase under Kleinberg’s consistency transformation. Hence in fact any Kleinberg’s consistency transformation can be represented as a superposition of the mentioned transforms.\nThis means that\nTheorem 25. k-means, if restricted to absolute clusterings, conforms globally to k-richness, invariance and consistency axioms.\nFurthermore, let us relax a bit the centric consistency.\nProperty 12. A method matches the condition of inner cluster proportional consistency if after decreasing distances within a cluster by the same factor, specific to each cluster, while keeping the position of cluster center in space, it returns the same partition.\nTheorem 26. k-means, with k ranging over a set of values, if we assume that it returns the absolutely separated clusterings for the largest possible k (excluding cluster sizes considered as too small), then it conforms globally to richness, scaleinvariance, motion consistency and inner cluster proportional consistency.\nNote that motion consistency and inner cluster proportional consistency include as a special case the outer-consistency. So in this way we denied the theorem from [4] that ”no general clustering function can simultaneously satisfy outer-consistency, scale- invariance, and richness”.\nLet us make at this point a remark why we insist on inner cluster proportional consistency. A reasonable assumption for consistency transformation would be that no possible partition of a given cluster being subject to consistency transformation would take advantage of the consistency transformation, so that no new substructures would occur in the cluster. In the context of k-means this\nwould mean the following. Consider a cluster C of a partition Γ of a whole set, say S, a distance d prior to a consistency transformation and a distance dΓ after the consistency transformation. Consider alternative partitions Γ1 and Γ2 of C. Let Q(Γ, d) denote the quality function Q(Γ) under the distance d. So we would expect that Q(Γ1,dΓ)Q(Γ2,d) = Q(Γ2,dΓ) Q(Γ2,d) unless we have a trivial partition such that each element is in separate cluster. This should hold for any pair of partitions of C, including the following ones: Γ1 puts all points into separate clusters except for x,y which go into a single cluster, Γ2 puts all points into separate clusters except for x, z. Let λ1, λ2 ∈ (0, 1] be coefficients by which distances ‖x−y‖, ‖x−z‖ are shortened respectively under consistency transformation. So we will have the requirement ‖λ1(x−y)‖ 2/2\n‖x−y‖2/2 = λ 2 1 = ‖λ2(x−z)‖2/2 ‖x−y‖2/2 = λ 2 2 which means that λ1 = λ2.\nBy induction over the whole set C we see that consistency transformation would need to shorten all distances within C by the same factor. This result justifies inner cluster proportional consistency concept, with a special case of centric consistency.\nWith respect to Kleinberg’s consistency, we can say\nTheorem 27. k-means, with k ranging over a set of values, if we assume that it returns the absolutely separated clusterings for the largest possible k (excluding cluster sizes considered as too small), then it conforms globally to richness, scale-invariance and unidirectional refinement consistency.\nLet us inspect the effect of k-richness in both described cases. From inequality (22) we see that a large discrepancy between the maximum and minimum cluster size implies that the gap g between clusters needs to grow to get absolute clustering. From inequality (21) we see something similar, but this time the relation between the smallest cluster and the overall number of elements in the sample play the dominant role. Additionally, the gap size is impacted by the number of clusters.\nSo once again it is visible that k-richness is unfavorable for clustering process."
    }, {
      "heading" : "13 Conclusions and future work",
      "text" : "In this paper, contrary to results of former researchers, we reached the conclusion, that k-means algorithm can comply simultaneously to Kleinberg’s krichness, scale-invariance and consistency axioms. A variant of k-means can comply simultaneously to Kleinberg’s richness, scale-invariance and refinement consistency axioms. The same variant of k-means can even comply to richness, scale-invariance and motion plus inner proportional consistency axioms. where the last two axioms pretty well approximate Kleinberg’s consistency without creating a risk of emergence of new structures within a cluster.\nThese new results emerged from the insight that our understanding of clustering process is to separate clusters with gaps.\nAs has been pointed at in earlier work of other researchers. k-means, like many other algorithms, is appropriately described neither by the richness-axiom nor by the consistency axiom of Kleinberg.\nAs richness is concerned, already Ackerman [4] showed that properties like stability against malicious attacks requires balanced clusters, hence k-richness is counterproductive when seeking stable clusterings.\nIn this paper we pointed at a number of further problems with the richness or near-richness axiom by itself. The major ones are: (a) the huge space to search through under ”hostile” clustering criterion, (b) problems with ensuring learnability of the concept of a clustering for the population, (c) richness and scaling-invariance alone may lead to a contradiction for a special case.\nBut we showed also that resorting to k-richness, which was deemed as a remedy to Kleinberg’s Impossibility Theorem, does not resolve all problems:\n• The initial seeding of cluster centers becomes extremely difficult both for k-means-random and k-means++ given that the cluster sizes differ extremely.\n• Even if we restrict ourselves to perfect ball clusterings realm, large differences in cluster sizes are prohibitive for a successful seeding.\n• For perfect ball clusterings with noise, even the smallest clusters require a high cluster size to noise size ratio.\n• In the realm of absolute clusterings, a high ratio between the lowest and the largest cluster result in high required gaps between clusters.\nWe showed also that the consistency axiom constitutes a problem: neither consistency, nor inner-consistency nor outer-consistency can be executed continuously in Euclidean space of limited dimension. Therefore, as a substitute of the inner-consistency, we proposed centric consistency and showed that k-means has the property of centric consistency.\nWhen investigating a substitute for outer-consistency, the motion consistency, we showed that (a) a gap between clusters is necessary for them to have a motion consistency with k-means, (b) the shape of the cluster counts - it has to be enclosed in a ball for k-means.\nTherefore we investigated further the impact of the gap on the behavior of the k-means in the light of Kleinberg’s axioms. We showed that perfect ball clustering is a local minimum for k-means function so that for perfect ball clusterings axioms of invariance, k-richness, inner cluster consistency and motion consistency hold (the last pair as a fair substitute of the consistency). If we consider a variant of k-means with varying k over a broad spectrum of k, and take as the final clustering the perfect ball clustering into the largest number of clusters possible, and instead of inner cluster consistency the centric consistency is used then an approximation to near-richness can be achieved.36\n36We would exclude clusters with several cluster members on the grounds of the fact that statistically speaking we want to be sure that the probability of an element occurring in the gap should be smaller than in a cluster, say p times. So if we have n elements in a cluster\nand none in the gap. then we should have (\np p+1\n)n ≤ 0.05 for example. With p=10, we need\na minimal cluster size of at least n=32.\nAgain for k-means-random and k-means++ the k-richness (big variation of cluster sizes) constitutes a problem for appropriate seeding. Seeding becomes more important with gaps because gaps may prohibit recovery from inappropriate seeding.\nWe investigated absolute clustering realm that is space where perfect ball clusterings turn to global minimum for k-means. k-richness requirement widens the gaps between clusters that are necessary. Axiomatic behavior does not differ much from that of perfect ball clusterings except for the fact that after the transformations we remain in the real of absolute clusterings.\nThe introduction of gaps draws our attention to one important issue: the broader the gaps the more are the clustering properties close to Kleinberg’s axioms. But this happens at a price of violating some Kleinberg’s implicit assumptions: that the clustering function always returns a clustering. Let us illustrate the point with the incremental clustering algorithms of Ackerman and Dasgupta. They prove theorems of the form: ”If a perfect clustering exists, then the algorithm returns it”. But the question is not raised: what does the algorithm return if the clustering is not perfect? Their algorithms return ”something”. We do not agree with such an approach. If the clustering type the algorithm is good looking at may not exist, the algorithm should state: ”I found the clustering of this type / I did not find the clustering of this type / The clustering of the given type does not exist”. This would be a response from an ideal algorithm. A worse one, but still usable, would give one of the first two answers. In this investigation we show that a post-processing for kmeans would be capable to answer the question, whether the found clustering is a nice-ball-clustering, perfect-ball-clustering, or absolute-clustering, or none of them.\nBetter types of algorithms should provide with more diagnostics, concerning violations of shape, gap sizes, risks resulting from unbalanced cluster sizes and/or radii.\nSo the first conclusion is that the clustering algorithm should respond that either a clustering of required type was found or not found (along with the clustering).\nThe second question is what is the type of clustering we are looking for? It is a bad habit to run k-means over and over again and stop when the lowest value of the quality function was reached. But this clustering may be worse than ones generated in-between, e.g. if a perfect-ball-clustering exists, it may become a victim of the unbalanced cluster sizes.\nBut what we are looking for may become also a victim of the transformations Kleinberg is proposing. As the natural clusters returned by k-means are preferably ball-shaped, centric consistency transformation and the motion consistency transformation and Kleinberg consistency transform preserve them, when the gaps conform to perfect-ball or absolute separation. If Voronoi diagrams are to be shapes, then motion consistency transformation and Kleinberg’s consistency transformations are destructive. If, however, any connected, well separated area would be deemed a good cluster, then even centric consistency transformation may turn out to be disastrous. Kleinberg’s consistency transformation is dis-\nastrous by itself (especially under richness expectation), as it can create new cluster like structures not present in the original data.\nA similar statement may be made about the richness or any related axioms. The requirement of a too rich space of hypotheses imposes a too heavy burden on the clustering algorithms. One shall instead envision hypotheses spaces that are just rich enough and are still learnable, and where the decision is possible if we are still in the hypotheses space with our solution.\nSo, we disagree to some extent with the opinion expressed in [25, 10] that axiomatic systems deal with either clustering functions, or clustering quality function, or relations of quality of partitions. The particular formulations of axiomatic systems state rather the equivalence relations between the clusterings themselves. Hence we must first have an imagination what kind of clusters we are looking for and only then formulate the axioms with transformations that are reasonable within the target class of clusterings, do not lead outside of this class and equivalence or other relations between clusterings makes sense within this class and does not need to be defined outside.\nHence there is still much space for research on clustering axiomatization, especially for clarification, what types of clusters are of real interest and whether or not all of them can be axiomatised in the same way. Kleinberg pointed at the problem and is was a good starting point."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors wish to thank to the Institute of Computer Science of Polish Academy of Sciences for promoting and financing this research. Research done by Robert A. K lopotek was financed by research fellowship within Project ’Information technologies: research and their interdisciplinary applications’, agreement number UDA-POKL.04.01.01-00-051/10-00."
    } ],
    "references" : [ {
      "title" : "Towards theoretical foundations of clustering",
      "author" : [ "M. Ackerman" ],
      "venue" : "University of Waterloo, PhD Thesis",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Characterization of linkagebased clustering",
      "author" : [ "M. Ackerman", "S. Ben-David", "D. Loker" ],
      "venue" : "COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 270–281,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Weighted clustering",
      "author" : [ "Margareta Ackerman", "Shai Ben-David", "Simina Brânzei", "David Loker" ],
      "venue" : "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-26,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Towards property-based classification of clustering paradigms",
      "author" : [ "Margareta Ackerman", "Shai Ben-David", "David Loker" ],
      "venue" : "In J.D. Lafferty,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Clustering oligarchies",
      "author" : [ "Margareta Ackerman", "Shai Ben-David", "David Loker", "Sivan Sabato" ],
      "venue" : "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Incremental clustering: The case for extra clusters",
      "author" : [ "Margareta Ackerman", "Sanjoy Dasgupta" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "O pewqnym wyniku orzekajcym niemonoc przeprowadzenia analizy skupie",
      "author" : [ "S. Ambroszkiewicz", "J. Koronacki" ],
      "venue" : "krtki dowd z komentarzem. decision support systems conferernce, zakopane, poland, 2010, lecture,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "k-means++: the advantages of careful seeding",
      "author" : [ "D. Arthur", "S. Vassilvitskii" ],
      "venue" : "N. Bansal, K. Pruhs, and C. Stein, editors, Proc. of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, pages 1027–1035, New Orleans, Louisiana, USA, 7-9 Jan.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Attempts to axiomatize clustering, 2005",
      "author" : [ "S. Ben-David" ],
      "venue" : "NIPS Workshop, December",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Measures of clustering quality: A working set of axioms for clustering",
      "author" : [ "S. Ben-David", "M. Ackerman" ],
      "venue" : "D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 121–128. Curran Associates, Inc.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Thoughts on clustering",
      "author" : [ "A. Blum" ],
      "venue" : "essay for the 2009 nips workshop ”clustering: Science or art?”,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Penalized versions of the newman-girvan modularity and their relation to normalized cuts and k-means clustering",
      "author" : [ "Marianna Bolla" ],
      "venue" : "Phys Rev E Stat Nonlin Soft Matter Phys",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Persistent clustering and a theorem of j",
      "author" : [ "G. Carlsson", "F. Mémoli" ],
      "venue" : "kleinberg. arXiv preprint arXiv:0808.2241,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Characterization, stability and convergence of hierarchical clustering methods",
      "author" : [ "G. Carlsson", "F. Mémoli" ],
      "venue" : "J. Mach. Learn. Res., 11:1425–1470, August",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Kernel k-means, spectral clustering and normalized cuts",
      "author" : [ "Inderjit S. Dhillon", "Yuqiang Guan", "Brian Kulis" ],
      "venue" : "In KDD’04, August 22–25,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Pattern Classification",
      "author" : [ "R.O. Duda", "P.E. Hart", "G. Stork" ],
      "venue" : "J. Wiley & Sons, New York, 2nd edition,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A fuzzy relative of the isodata process and its use in detecting compact well-separated clusters",
      "author" : [ "J.C. Dunn" ],
      "venue" : "J. Cyber., 3(3):32–57,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Intelligent kernel k-means for clustering gene expression",
      "author" : [ "Teny Handhayania", "Lely Hiryantob" ],
      "venue" : "In International Conference on Computer Science and Computational Intelligence (ICCSCI",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Computer science theory for the information age, 2012. chapter 8.13.2",
      "author" : [ "J. Hopcroft", "R. Kannan" ],
      "venue" : "A Satisfiable Set of Axioms. page 272ff",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "An impossibility theorem for clustering",
      "author" : [ "J. Kleinberg" ],
      "venue" : "Proc. NIPS 2002, pages 446–453,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "On the phenomenon of flattening ’flexible prediction",
      "author" : [ "M.A. K  lopotek" ],
      "venue" : "Fundamentals of Artificial Intelligence Research. International Workshop,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1991
    }, {
      "title" : "Comparative analysis for k-means algorithms in network community detection",
      "author" : [ "Jian Liu" ],
      "venue" : "Advances in Computation and Intelligence: 5th International Symposium,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Comparing clusterings: An axiomatic view",
      "author" : [ "Marina Meilǎ" ],
      "venue" : "In Proceedings of the 22Nd International Conference on Machine Learning,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Fast algorithms for constant approximation k-means clustering",
      "author" : [ "Mingjun Song", "Sanguthevar Rajasekaran" ],
      "venue" : "Trans. MLDM,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Axioms for graph clustering quality functions",
      "author" : [ "T. van Laarhoven", "E. Marchiori" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Clustering stability: An overview",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "Clustering: Science or art? initially, opinion paper for the NIPS Workshop ”Clustering: Science or Art",
      "author" : [ "U. von Luxburg", "R. C Williamson", "I. Guyon" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "K-means clustering with manifold",
      "author" : [ "Lai Wei", "Weiming Zeng", "Hong Wang" ],
      "venue" : "In Seventh International Conference on Fuzzy Systems and Knowledge Discovery,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "A formalization of cluster analysis",
      "author" : [ "W.E. Wright" ],
      "venue" : "Pattern Recognition, 5(3):273–282,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "A uniqueness theorem for clustering",
      "author" : [ "R.B. Zadeh", "S. Ben-David" ],
      "venue" : "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 639–646, Arlington, Virginia, United States,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "A number of axiomatic frameworks have been devised for methods of clustering, the most cited probably the Kleinberg’s system [20].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "Kleinberg defines [20] clustering function as Definition 1.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "], but the results to follow both negative and positive still hold if one does require” Jon Kleinberg [20] claims that a good partition may only be a result of a reasonable method of clustering and he formulated axioms, for distance-based cluster analysis, that need to be met by the clustering method itself.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "[4].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "” [20]",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "Another proof can be found in a paper by Ambroszkiewicz and Koronacki [7], along with some discussion of the Kleinberg’s concepts.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "[4] prove a bit more general impossibility theorem (engaging so called inner-consistency and outer-consistency).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "” [20] 4 ”Let Γ be a partition of S, and d and d′ two distance functions on S.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : "If f(d) = Γ, and d′ is a Γtransformation of d, then f(d′) = Γ” [20].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "Note, however, that Ben-David and Ackerman [10] drew attention by an illustrative example (their Figure 2), that consistency is a problematic property by itself as it may give rise to new clusters at micro or macro-level.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "If step 1 is performed according to k-means++ heuristics proposed by Arthur and Vassilvitskii [8], then we will speak about k-means++ algorithm.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 23,
      "context" : "k-means++, like [24], but it has to be stated that at the current point these algorithms are rather of theoretical value.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "Ackerman and Dasgupta [6].",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "Ben-David and Ackerman in [10] in section 4.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : "Recall that in [25] it has been observed by van Laarhoven and Marchiori that Kleinberg’s proof of Impossibility Theorem stops to be valid in case of graph clustering.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 17,
      "context" : "k-means is applied in many domains, not only in its natural domains of data embedded in R, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 27,
      "context" : "k-means is applied in many domains, not only in its natural domains of data embedded in R, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22].",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 14,
      "context" : "k-means is applied in many domains, not only in its natural domains of data embedded in R, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22].",
      "startOffset" : 259,
      "endOffset" : 263
    }, {
      "referenceID" : 21,
      "context" : "k-means is applied in many domains, not only in its natural domains of data embedded in R, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22].",
      "startOffset" : 310,
      "endOffset" : 314
    }, {
      "referenceID" : 14,
      "context" : "[15] that kmeans is equivalent in some sense to normalized cut method of graph clustering, which in turn can be viewed as equivalent to balanced Newman’s modularity, used in community detection, as shown by Bolla [12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[15] that kmeans is equivalent in some sense to normalized cut method of graph clustering, which in turn can be viewed as equivalent to balanced Newman’s modularity, used in community detection, as shown by Bolla [12].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 14,
      "context" : "[15] and the impossibility theorem challenge for graphs in [25] encourage to investigate Kleinberg’s axioms in the context of Euclidean space.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[15] and the impossibility theorem challenge for graphs in [25] encourage to investigate Kleinberg’s axioms in the context of Euclidean space.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 28,
      "context" : "2 Previous work Axiomatic systems may be traced back to as early as 1973, when Wright [29] proposed axioms of clustering functions creating unsharp partitions, similar to fuzzy systems.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 24,
      "context" : "In general, as exposed by van Laarhoven and Marchiori [25] and Ben-David and Ackerman [10] the clustering axiomatic frameworks address either: • required properties of clustering functions, or • required properties of the values of a clustering quality function, or",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "In general, as exposed by van Laarhoven and Marchiori [25] and Ben-David and Ackerman [10] the clustering axiomatic frameworks address either: • required properties of clustering functions, or • required properties of the values of a clustering quality function, or",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "So Ben-David and Ackerman [10], as mentioned, pointed at the problems with consistency as such.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "We recall several of them here, based on an overview by Ackerman [1] and tutorial by Ben-David [9].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "We recall several of them here, based on an overview by Ackerman [1] and tutorial by Ben-David [9].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : "So it was proposed to weaken Kleinberg’s richness (by Kleinberg himself) to so-called k-richness as follows: Property 6 (Zadeh and Ben-David [30]).",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "[4] propose the concept of outer-consistency Property 7.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "8Still another relaxation of richness was proposed by Hopcroft and Kannan [19]: Richness II: For any set K of k distinct points in the given Euclidean space, there is an n and a set of S of n points such that the algorithm on input S produces k clusters, whose centers are the respective points in K.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "[5], a useful property of stability of clusters under malicious addition of data points holds only for balanced clusters.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 29,
      "context" : "Zadeh Ben-David [30] propose instead the order-consistency so that some versions of single-linkage algorithm can be classified as ”clustering algorithm”.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 16,
      "context" : "Let us also mention here some works like that by Dunn [17] or Ackerman and Dasgupta [6] that seemingly have nothing to do with Kleinberg’s axioms, but this is only a superficial impression.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Let us also mention here some works like that by Dunn [17] or Ackerman and Dasgupta [6] that seemingly have nothing to do with Kleinberg’s axioms, but this is only a superficial impression.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "Ackerman and Dasgupta [6] handle incremental clustering algorithms.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "The clusters are ”nicely separated”, as defined by [6], if a distance between an element and any other element of the same cluster is lower than the distance from this element to an element outside of the cluster.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "Ackerman and Ben-David [10] propose another direction of resolving the problem of Kleinberg’s axiomatisation impossibility.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "[2] for linkage algorithms, [14] for hierarchical algorithms, [13] for multiscale clustering.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "[2] for linkage algorithms, [14] for hierarchical algorithms, [13] for multiscale clustering.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "[2] for linkage algorithms, [14] for hierarchical algorithms, [13] for multiscale clustering.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "Meila [23] demonstrates that one can’t compare partitions in a manner that agrees with the lattice of partitions, is convexly additive and bounded.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : "[3, 5].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "[3, 5].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "[5] deals",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] demonstrates that one can put any two data points into different clusters if one applies weighting functions to data points.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] implies that too small clusters may be disintegrated by hostile new points so that for practical purposes one shall be only interested in larger clusters.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] allows to conclude that poor estimates of densities for sparse clusters may lead to erroneous drawing of cluster boundaries.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 24,
      "context" : "As mentioned, [25] pointed at the fact that such a construction would not be possible in the realm of graph clustering.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "But ”near-richness” is again not enough to resolve all contradictions (as by the way is visible from the Kleinberg’s anti-chain theorem [20]).",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 29,
      "context" : "[30], is obviously violated because k-means returns only partitions into k clusters.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "19However, this quality function fails on the axiom of Function Scale Invariance, proposed in [10].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "As mentioned earlier, following [4], for probabilistic algorithms we will talk about probabilistic k-richness, that is one obtainable with some probability, independent of the actual clustering that is intended to be obtained.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "21Such a quality function would satisfy axiom of Function Scale Invariance, proposed in [10]",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "This theorem contradicts apparently [4] claim that k-means possesses the property of outer-consistency.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "25Already Ben-David [10] indicated problems in this direction.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : "If the clustering function can fit any data, we are practically unable to learn any structure of data space from data [21].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 26,
      "context" : "Note that we speak here about a much broader aspect than so-called cluster stability or cluster validity, pointed at by Luxburg [27, 26].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 25,
      "context" : "Note that we speak here about a much broader aspect than so-called cluster stability or cluster validity, pointed at by Luxburg [27, 26].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "26 Strict separation [11] mentioned earlier is another kind of a weird cluster quality function, requiring visits to all the partitions",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "Consider moving a data point x ∗ from the cluster Cj0 to cluster Cjl As demonstrated by [16], V (Cj0 − {x∗}) = V (Cj0)− nj0 nj0−1 ‖x∗ − μj0‖ 2 and V (Cjl ∪ {x∗}) = V (Cjl) + nl nl+1‖x ∗ − μjl‖ 2 So it pays off to move a point from one cluster to another if nj0 nj0−1 ‖x∗ − μj0‖ 2 > njl njl+1 ‖x−μjl‖ .",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "For any point (x, f(x) of the border of the first cluster with center (0, 0) the following must hold: (x− 2x0) + (f(x)− 2f(x0)) − x − f(x) ≥ 0 (12) That is −2x0(2x− 2x0)− 2f(x0) (2f(x)− 2f(x0)) ≥ 0 −f(x0) (f(x)− f(x0)) ≥ x0(x− x0) 31This is by the way the nice trick behind the claim in [6] that incremental k-means does not identify perfectly separated clusters.",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 5,
      "context" : "Clusters in k-means are not the points, they are polyhedrons, contrary to the assumptions in [6].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "Let us discuss at this point a bit the notions of ”perfect separation” as introduced in [6].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "Ackerman and Dasgupta [6] show that the incremental k-means algorithm, as introduced in their Algorithm 2.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "As with perfect clustering (see [6]), also if there exists a perfect ball clustering into k clusters, then there exists only one such clustering.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "So in this way we denied the theorem from [4] that ”no general clustering function can simultaneously satisfy outer-consistency, scale- invariance, and richness”.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "As richness is concerned, already Ackerman [4] showed that properties like stability against malicious attacks requires balanced clusters, hence k-richness is counterproductive when seeking stable clusterings.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "So, we disagree to some extent with the opinion expressed in [25, 10] that axiomatic systems deal with either clustering functions, or clustering quality function, or relations of quality of partitions.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "So, we disagree to some extent with the opinion expressed in [25, 10] that axiomatic systems deal with either clustering functions, or clustering quality function, or relations of quality of partitions.",
      "startOffset" : 61,
      "endOffset" : 69
    } ],
    "year" : 2017,
    "abstractText" : "This paper investigates the validity of Kleinberg’s axioms for clustering functions with respect to the quite popular clustering algorithm called k-means.We suggest that the reason why this algorithm does not fit Kleinberg’s axiomatic system stems from missing match between informal intuitions and formal formulations of the axioms. While Kleinberg’s axioms have been discussed heavily in the past, we concentrate here on the case predominantly relevant for k-means algorithm, that is behavior embedded in Euclidean space. We point at some contradictions and counter intuitiveness aspects of this axiomatic set within R that were evidently not discussed so far. Our results suggest that apparently without defining clearly what kind of clusters we expect we will not be able to construct a valid axiomatic system. In particular we look at the shape and the gaps between the clusters. Finally we demonstrate that there exist several ways to reconcile the formulation of the axioms with their intended meaning and that under this reformulation the axioms stop to be contradictory and the real-world k-means algorithm conforms to this axiomatic system.",
    "creator" : "LaTeX with hyperref package"
  }
}