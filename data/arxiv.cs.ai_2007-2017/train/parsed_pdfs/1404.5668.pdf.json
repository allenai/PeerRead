{
  "name" : "1404.5668.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An Adversarial Interpretation of Information-Theoretic Bounded Rationality",
    "authors" : [ "Pedro A. Ortega", "Daniel D. Lee" ],
    "emails" : [ "ope@seas.upenn.edu", "ddlee@seas.upenn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: bounded rationality, free energy, game theory, Legendre-Fenchel transform."
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, there has been a renewed interest in bounded rationality, originally proposed by Herbert A. Simon as an alternative account to the model of perfect rationality in the face of complex decisions (Simon 1972). In artificial intelligence (AI), this development has been largely motivated by the intractability of exact planning in complex and uncertain environments (Papadimitriou and Tsitsiklis 1987) and the difficulty of finding domain-specific simplifications or approximations that would render planning tractable despite the latest theoretical advancements in understanding the AI problem (Hutter 2004). Newly proposed techniques based on Monte Carlo simulations such as Monte Carlo Tree Search (Coulom 2006; Browne et al. 2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability\nCopyright c© 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nand their unprecedented success in difficult problems such as computer Go (Gelly et al. 2006), universal planning (Veness et al. 2011), human-level video game playing (Mnih et al. 2013) and robot learning (Theodorou, Buchli, and Schaal 2010). Roughly, these efforts can be broadly classified into two groups: (a) randomized approximations to the perfect rationality model and (b) exact statistical-mechanical or information-theoretic (IT) approaches. The latter group is of special theoretical interest, as it rests on a model of bounded rationality1 that yields randomized optimal policies as a consequence of resource-boundedness (Ortega and Braun 2013).\nThe difference between the perfectly rational and the IT bounded rational model lies in the choice of the objective function: the IT approach adds a regularization term in the form of a Kullback-Leibler divergence (KL-divergence) to the expected utility of the perfectly rational model. The resulting objective function goes under various names in the literature, such as KL-control cost and free energy, and has been motivated in numerous ways. The initial inspiration comes from the maximum entropy principle of statistical mechanics as a way to model stochastic control problems that are linearly-solvable (Fleming 1977; Kappen 2005a; Todorov 2006). The methods from robust control were also adopted by economists to characterize model misspecification (Hansen and Sargent 2008). Later, it was shown that the free energy can be derived from axioms that treat utilities and information as commensurable quantities (Ortega and Braun 2013). Then in robot reinforcement learning, researchers proposed the free energy as a way to control the information-loss resulting from policy updates (Peters, Mülling, and Altün 2010). Finally, the free energy has also been motivated from arguments that parallel rate-distortion theory and information geometry by showing that the optimal policy minimizes the decision complexity subject to a constraint on the expected utility (Tishby and Polani 2011). It is also worth mentioning that similar approaches arise in the context of neuroscience (Friston 2009) and in game the-\n1There are several approaches to bounded rationality in the literature, most notably those coming from the field of behavioral economics (Rubinstein 1998), which put emphasis on the procedural elements of decision making. Here we have settled on the qualifier “information-theoretic” as a way to distinguish from these approaches.\nar X\niv :1\n40 4.\n56 68\nv1 [\ncs .A\nI] 2\n2 A\npr 2\n01 4\nory (Wolpert 2004). An important yet intriguing property of the free energy is that it is a universal aggregator of value. More precisely, it can implement the minimum, expectation, maximization, and all the “soft” aggregations in between by varying a single parameter. This has at least two important consequences. First, the free energy corresponds, in economic jargon, to the certainty-equivalent, i.e. the value a risk-sensitive decision-maker is thought to assign to an uncertain choice (van den Broek, Wiegerinck, and Kappen 2010). Second, it has been pointed out that decision trees are nested certaintyequivalent operations (Ortega and Braun 2012). This is important, as decision making in the face of an adversarial, an indifferent, and a cooperative environment, which have previously been treated as unrelated modeling assumptions, are actually instantiations of a single decision rule. Previous work explores this property in multi-agent settings, showing that solutions change from being risk-dominant to payoffdominant (Kappen, Gómez, and Opper 2012)."
    }, {
      "heading" : "Aim of this Work",
      "text" : "Despite the identification of the free energy with the certainty-equivalent, the connection to adversarial environments is as yet not fully understood. Our work makes an important step into understanding this connection. By applying a Legendre-Fenchel transformation to the regularization term of the free energy, it is revealed that a single-agent free energy optimization can be thought of as representing a game between the agent and an imaginary adversary. Furthermore, this result explains stochastic policies as a strategy that guards the agent from adversarial reactions of the environment."
    }, {
      "heading" : "Structure of this Article",
      "text" : "This article is organized into four sections. The aim of the next section (Section 2) is to familiarize the reader with the mathematical foundations underlying the planning problem in AI, and to give a basic introduction into IT bounded rationality necessary in order to contextualize the results of our work. Section 3 contains our central contribution. It first briefly reviews Legendre-Fenchel transformations and then applies them to the free energy functional in order to unveil the adversarial assumptions implicit in the objective function. Finally, Section 4 discusses the results and concludes."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We review the abstract foundations of planning under expected utility theory and IT bounded rationality."
    }, {
      "heading" : "Variational Principles",
      "text" : "The behavior of an agent is typically characterized in one of two ways: (a) by directly describing its policy, or (b) by specifying a variational problem that has the policy as its solution. While the former is a direct specification of the agent’s actions under any contingency, the latter has the advantage that it provides an explicit (typically convex) objective function that the policy has to optimize. Thus, a variational principle has additional explanatory power: not only\ndoes it single out an optimal policy, but it also encodes a preference relation over the set of feasible policies. Crucially, the qualifier “optimal” only holds relative to the objective function and is by no means absolute: because given any policy, one can always engineer a variational principle that is extremized by it. In AI, virtually all planning algorithms are framed (either explicitly or implicitly) as maximum expected utility problems. This encompasses popular problem classes such as multi-armed bandits, Markov decision processes and partially observable Markov decision processes (Legg 2008, Chapter 3)."
    }, {
      "heading" : "Sequential versus Single-Step Decisions",
      "text" : "We briefly recall a basic theoretical result that will simplify our analysis. In planning, sequential decision problems can be rephrased as single-step decision problems: instead of letting the agent choose an action in each turn, one can equivalently let the agent choose a single policy in the beginning that it then must follow during its interactions with the environment2. This observation was first made in game theory, where it was proven that every extensive form game can always be re-expressed as a normal form game (Von Neumann and Morgenstern 1944). Consequently, we abstract away from the sequential nature of the general problem by limiting our discussion to single-step decisions involving a single action and observation. The price we pay is to hide the dynamical structure and to increase the complexity of the policy, but the mathematical results can stated more concisely.\n(Subjective) Expected Utility\nExpected utility (Von Neumann and Morgenstern 1944; Savage 1954) is the de facto standard variational principle in artificial intelligence (Russell and Norvig 2010). It is based on a utility function, that is, a real-valued mapping of the outcomes encoding the desirability of each possible realization of the interactions between the agent and the environment. The qualifiers “subjective” and “expected” in its name derive from the fact that the desirability of a stochastic realization is calculated as the expectation over the utilities of the particular realizations measured with respect to the subjective beliefs of the agent.\nFormally, let X and Y be two finite sets, the former corresponding to the set of actions and the latter to the set of observations. A realization is a pair (x, y) ∈ X × Y . Furthermore, let U : X × Y → R be a utility function, such that U(x, y) represents the desirability of the realization (x, y) ∈ X × Y; and let q(·|·) be a conditional probability distribution characterizing the environmental response where q(y|x) represents the probability of the observation y ∈ Y given the action x ∈ X . Then, the agent’s optimal\n2Note that this reduction encompasses policies that appear to change during its execution as a function of the history: the metapolicy controlling the changes is a compressed, yet sufficient description of the changing policy.\npolicy p ∈ ∆(X ) is chosen so as to maximize the functional E[U ] = ∑ x p(x)E[U |x]\n= ∑ x p(x) ∑ y q(y|x)U(x, y). (1)\nThis is illustrated in Figure 1. An optimal policy is any distribution p∗ with no support over suboptimal actions, that is, p∗(x) = 0 whenever E[U |x] ≤ maxz E[U |z]. In particular, because the expected utility is linear in the policy probabilities, one can always choose a solution that is a vertex of the probability simplex ∆(X ):\np∗(x) = δxx∗ = { 1 if x = x∗ := arg maxx E[U |x] 0 otherwise, (2)\nwhere δ is the Kronecker delta. Hence, there always exists a deterministic optimal policy.\nOnce the optimal policy has been chosen, the utility U becomes a well-defined random variable with probability distribution\nP(U = u) = P { (x, y) : U(x, y) = u }\n= ∑\nU−1(u)\np∗(x)q(y|x). (3)\nEven though the optimal policy might be deterministic, the utility of the ensuing realization is in general stochastic.\nLet us now consider the case when the environment is another agent. Formally, this means that the agent lacks the conditional probability distribution q(·|·) over observations that it requires in order the evaluate the expected utilities. Instead, the agent possesses a second utility function V : X × Y → R characterizing the desires of the environment. Game theory then invokes a solution concept, most notably the Nash equilibrium, in order to obtain the missing distribution q(·|·) from U and V that renders the original description as a well-defined decision problem. Thus, conceptually, game theory starts from slightly weaker assumptions than expected utility theory. For simplicity, we restrict ourselves to two particular cases: (a) the fully adversarial case U(x, y) = −V (x, y); and (b) the fully cooperative case U(x, y) = V (x, y). The Nash equilibrium then yields the\ndecision rules (Osborne and Rubinstein 1999):\np∗ = arg max p ∑ x p(x) { min q q(y|x)U(x, y) } , (4)\np∗ = arg max p ∑ x p(x) { max q q(y|x)U(x, y) } (5)\nfor the two cases respectively. Comparing these to (1) we immediately see that, for planning purposes, it is irrelevant whether the decision is made by the agent or not—what matters is the degree to which an outcome (be it an action or an observation) contributes to the agent’s objective function (as encoded by one of the three possible aggregation operators). Thus, equations (1), (4) and (5) can be regarded as consisting of not one, but two nested “decision steps” of the form\nmax x {U(x)}, expp[U(X)] or min x {U(x)}.\nWe end this brief review by summarizing the main properties of expected utility:\n1. Linearity: The objective function is linear in the policy. 2. Existence of Deterministic Solution: Although there are\noptimal stochastic policies, there always exists an equivalent deterministic optimal policy.\n3. Indifference to Higher-Order Moments: Expected utility places constraints on the first, but not on the highermoments of the probability distribution of the utility.\n4. Exhaustive Search: Finding the optimal deterministic policy requires, in the worst-case, an exhaustive evaluation of all the expected utilities.\nIt is important to note that these properties only apply to expected utility, but not to game theory."
    }, {
      "heading" : "Free Energy",
      "text" : "How does an agent make decisions when it cannot exhaustively evaluate all the alternatives? IT bounded rationality addresses this question by defining a new objective function that trades off utilities versus information costs. Planning is conceptualized as a process that transforms a prior policy into a posterior policy that incurs a cost measured in utiles. Importantly, it is assumed that the agent is not allowed itself to reason about the costs of this transformation3; rather, it tries to optimize the original expected utility but then runs out of resources. From the point of view of an external observer, this “interrupted” planning process will appear as if it were explicitly optimizing the free energy.\nFormally, let X be a finite set corresponding to the set of realizations and U : X → R is the utility function. Furthermore, let p0 ∈ ∆(X ) be a prior policy and β ∈ R be a boundedness parameter. Then, the agent’s optimal policy p ∈ ∆(X ) is chosen so as to extremize the free energy functional\nFβ [p] := ∑ x\np(x)U(x)︸ ︷︷ ︸ Expected Utility\n− 1 β ∑ x p(x) log p(x)\np0(x)︸ ︷︷ ︸ Information Costs\n, (6)\n3The inability to reason about resource costs renders IT bounded rationality fundamentally different from the metareasoning approach of Stuart J. Russell (Russell 1995).\nwhere it is seen that β controls the conversion from units of information to utiles (Figure 2). The optimal policy is given by the “softmax”-like distribution\np∗(x) = p0(x)e βU(x)∑ x p0(x)e βU(x) (7)\nwhich is known as the equilibrium distribution. Notice that β controls how much the agent is in control of the choice: a value β ≈ 0 falls back to the prior policy and represents lack of influence; and values far away from zero yield either adversarial or cooperative choices depending on the sign of β. An important property of the equilibrium distribution is that it can be simulated exactly without evaluating all the utilities using Monte Carlo methods (Ortega, Braun, and Tishby 2014).\nAs we have mentioned before, the free energy (6) corresponds to the certainty-equivalent. This is seen as follows: the extremum of (6) given by\n1 β logZβ , where Zβ := ∑ x p0(x)e βU(x), (8)\nbut now seen as a function of β, can be thought of as an interpolation of the maximum, expectation and minimum operator, since\n1 β logZβ = maxx {U(x)} β → +∞ 1 β logZβ = Ep0 [U ] β → 0 1 β logZβ = minx {U(x)} β → −∞.\nWe end this brief review by summarizing the main properties of IT bounded rationality:\n1. Nonlinearity: The objective function is nonlinear in the policy.\n2. Stochastic Solutions: Optimal policies are inherently stochastic.\n3. Sensitive to Higher-Order Moments: Because the free energy is nonlinear in the policy, it places constraints on the higher-order moments of the policy too. In particular, a Taylor expansion of the KL-divergence term reveals that the free energy is sensitive to all the moments of the policy.\n4. Randomized Search: Obtaining a decision from the optimal policy does not require the exhaustive evaluation of the utilities. Rather, it can be sampled using Monte Carlo techniques."
    }, {
      "heading" : "3 Adversarial Interpretation",
      "text" : "Before presenting our main results, we give a brief definition of Legendre-Fenchel transforms."
    }, {
      "heading" : "Legendre-Fenchel Transforms",
      "text" : "The Legendre-Fenchel transformation is a transformation that yields an alternative encoding of a given functional relationship. More precisely, the information contained in the function is expressed using the derivative as the independent variable. Because of this, Legendre-Fenchel transformations play an important role in thermodynamics and optimization. For a function f(x) : Rn → R, it is defined by the variational formula\nf?(s) = sup x∈X {〈s, x〉 − f(x)}\nwhere f?(s) : Rn → R is the convex conjugate and 〈s, x〉 is the inner product of two vectors s, x in Rn. The following are common examples:\n1. Affine function: f(x) = ax− b f?(s) = { b if s = a, +∞ if s 6= a.\n2. Power function:\nf(x) = 1 α |x|α f?(s) = 1 α′ |s|α ′\nwhere 1α + 1 α′ = 1.\n3. Exponential function:\nf(x) = ex f?(s) =  s log s− s if s > 0, 0 if s = 0, +∞ if s < 0.\nWe refer the reader to Touchette’s article (Touchette 2005) for a brief introduction or Boyd and Vanderberghe’s book (Boyd and Vandenberghe 2004) for a thorough treatment."
    }, {
      "heading" : "Unveiling the Adversarial Environment",
      "text" : "Using a Legendre-Fenchel transformation, one can show that the regularization term of the free energy can be rewritten as a variational problem.\nLemma 1.\n− 1 β ∑ x p(x) log p(x) p0(x)\n= min C ∑ x −p(x)C(x) + p0(x)eβC(x) − 1 β (log β + 1)\n(9)\nProof. Since n = |X | is finite, p(x), C(x) are vectors in Rn. Essentially, the lemma says that the l.h.s. is the convex conjugate of the function\nf(C) = − ∑ x p0(x)e βC(x) + 1 β (log β + 1),\nbecause the r.h.s. can be re-expressed as\nmin C ∑ x −p(x)C(x) + f(C) = max C ∑ x p(x)C(x)− f(C).\nSetting the derivative w.r.t. C to zero yields the optimality condition\np(x) = βp0(x)e βC(x). Isolating C(x) and substituting into ∑ x p(x)C(x) − f(C) proves the lemma.\nConsequently, the regularization term of the free energy encapsulates a second variational problem over an auxiliary vector C. In particular, the logarithmic form encodes an objective function that is exponential in C. Equipped with this lemma, we can state our main result.\nTheorem 2. The maximization of the free energy (6) is equivalent to the maximization of\nmin C ∑ x\np(x)[U(x)− C(x)]︸ ︷︷ ︸ Expected Net Utility\n+ ∑ x p0(x)e βC(x)\n︸ ︷︷ ︸ Penalty of Adversary\n(10)\nw.r.t. the policy p ∈ ∆(X ).\nProof. The proof is an immediate consequence of Lemma 1:\nargmax p {∑ x p(x)U(x)− 1 β ∑ x p(x) log p(x) p0(x) } = argmax\np {∑ x p(x)U(x) + min C {∑ x −p(x)C(x)\n+ p0(x)e βC(x) − 1\nβ (log β + 1) }} = argmax\np min C {∑ x p(x)[U(x)− C(x)] + ∑ x p0(x)e βC(x) } .\nThis result can be interpreted as follows. When a single agent maximizes the free energy, it is implicitly assuming a situation where it is playing against an imaginary adversary. In this situation, the agent first chooses its policy p ∈ ∆(X ), and then the adversary attempts to decrease the agent’s net utilities by subtracting costs C(x). However, the adversary cannot choose these costs arbitrarily; instead, it must pay exponential penalties. In fact, its optimal strategy can be directly calculated from Lemma 1. Corollary 3. The imaginary adversary’s optimal strategy is given by\nC∗(x) = 1\nβ log\np(x)\nβp0(x) . (11)\nInspecting (11), we see that the optimal costs scale relatively with the agent’s deviation from its prior probabilities. This leads to an interesting interaction between the agent’s and the imaginary adversary’s choices."
    }, {
      "heading" : "Indifference",
      "text" : "Our second main result characterizes the solution to this adversarial setup. It turns out that the adversary’s best strategy is to choose costs such that the agent’s net payoffs are uniform. Theorem 4. The solution to\nmax p min C ∑ x p(x)[U(x)− C(x)] + ∑ x p0(x)e βC(x)\nhas the property that for all x ∈ X ,\nU(x)− C(x) = constant.\nProof. We first note that we can exchange the maximum and minimum operations,\nmax p min C ∑ x p(x)[U(x)− C(x)] + ∑ x p0(x)e βC(x)\n= min C max p ∑ x p(x)[U(x)− C(x)] + ∑ x p0(x)e βC(x)\nbecause there is no duality gap due to the concavity of the exponential function. We can thus maximize first w.r.t. the policy p. Define X ∗ ⊂ X as the subset of elements maximizing the penalized expected utility, that is, for all x∗ ∈ X and x ∈ X ,\nU(x∗)− C(x∗) ≥ U(x)− C(x). (12)\nMaximizing w.r.t. to p yields optimal probabilities p∗(x) given by\np∗(x) = { q(x) if x ∈ X ∗, 0 otherwise,\nwhere q is any distribution over X ∗. Given this, the worst case costs C∗(x) are\nC∗(x) =\n{ 1 β log q(x) βp0(x)\nif x ∈ X ∗, −∞ otherwise.\n(13)\nHowever, if X ∗ 6= X , then we get a contradiction, since\nU(x∗)− C∗(x∗) 6≥ U(x)− C∗(x)\nfor all x /∈ X ∗, violating (12). Hence, it must be that for all x ∈ X , U(x)− C(x) = constant, concluding the proof.\nTo get a better understanding on the meaning of this result, it is helpful to consider an example. Figure 4 illustrates four choices of policies and the corresponding optimal adversarial costs. Here it is seen that an agent can protect itself by spreading the probability mass of its policy over many realizations."
    }, {
      "heading" : "4 Discussion",
      "text" : "Starting from the free energy functional, we have shown how to construct an alternative adversarial interpretation that constitutes an equivalent problem. Conceptually, our findings can be summarized as follows:\n1. A regularization of the expected utility encodes assumpions about deviations from the expected utility.\n2. The Legendre-Fenchel transformation reinterprets the free energy as a game against an environmental adversary.\n3. In this transformation, it is seen that the regularization is equivalent to the penalization of adversarial costs.\n4. Stochastic policies guard against adversarial costs. Expected utility alone is linear in the policy and thus encodes deterministic optimal policies."
    }, {
      "heading" : "Indifference and Nash Equilibrium",
      "text" : "Our results establish an interesting relation to game theory. Theorem 4 and (11) immediately imply\nU(x)− C∗(x) = U(x)− 1 β log p(x) βp0(x) = constant\nfor all x ∈ X . This turns out to be an known characterization of the equilibrium distribution (7). However, in light of our present results, it acquires a different twist; it fits with the well-known result that a Nash equilibrium is a strategy\nprofile such that each player chooses a (mixed) strategy that renders the others players indifferent to their choices (Osborne and Rubinstein 1999)."
    }, {
      "heading" : "Other Regularizers",
      "text" : "The presented method is general and can also be applied to other regularizers in order to make the assumptions about the imaginary adversary explicit. For instance, the following list enumerates some examples.\n1. Expected Utility: Because the regularization term is null, the resulting adversary’s objective function is\nmax p min C ∑ x p(x)[U(x)− C(x)]− δC(x)0 .\nHere we see that a null regularization implies an adversary devoid of power, i.e. one that cannot alter the utilities chosen by the agent.\n2. Power Function: If the regularizer is a power function, then we have the dual power function\nmax p min C ∑ x p(x)[U(x)− C(x)] + |C(x)|α.\nThis case is interesting because it encompases ridge and lasso as special cases.\n3. Modern Portfolio Theory: One notable case that is widely used in practice is Modern Portfolio Theory. Here, an investor trades off asset returns versus portfolio risk, encoded into a regularization term that is quadratic in the policy (Markowitz 1952). The transformed objective function is given by\nmax p min C ∑ x p(x)[U(x)− C(x)] + 1 2 λCTΣC."
    }, {
      "heading" : "Conclusions",
      "text" : "Our central result shows how to extract the adversarial cost function implicitly assumed in the agent’s objective function by means of an appropriately chosen Legendre transformation. Conversely, one can also start from the cost function of an adversarial environment and reexpress it as a regularized optimization. While our main motivation was to apply this to the free energy functional, the transformation is general and works also in other well-known frameworks such as in modern portfolio theory. The immediate application is that we can switch between the two representations and pick the one that is easier to solve. This suggests novel algorithms for solving the single-agent planning problem based on ideas from differential game theory (Dockner et al. 2001) and convex programming (Zinkevich 2003). Furthermore, this also suggests that agents that randomize their decisions, are essentially expressing their information limitations by protecting themselves from undesired outcomes. As such, we believe this is a basic result that opens up a series of additional questions regarding the nature of stochastic policies, such as the conditions under which they arise and how they encode risk sensitivity, which need to be further explored in the future."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the anonymous reviewers for their valuable comments and suggestions for improving this manuscript. This study was funded by grants from the U.S. National Science Foundation, Office of Naval Research and Department of Transportation."
    } ],
    "references" : [ {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge Univeristy Press.",
      "citeRegEx" : "Boyd and Vandenberghe,? 2004",
      "shortCiteRegEx" : "Boyd and Vandenberghe",
      "year" : 2004
    }, {
      "title" : "A Survey of Monte Carlo Tree Search Methods",
      "author" : [ "C. Browne", "E. Powley", "D. Whitehouse", "S. Lucas", "P. Cowling", "P. Rohlfshagen", "S. Travener", "D. Perez", "S. Samothrakis", "S. Colton" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games 4(1).",
      "citeRegEx" : "Browne et al\\.,? 2012",
      "shortCiteRegEx" : "Browne et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search",
      "author" : [ "R. Coulom" ],
      "venue" : "Computer and Games.",
      "citeRegEx" : "Coulom,? 2006",
      "shortCiteRegEx" : "Coulom",
      "year" : 2006
    }, {
      "title" : "Differential Games in Economics and Management Science",
      "author" : [ "E. Dockner", "S. Jorgensen", "N. Long", "G. Sorger" ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Dockner et al\\.,? 2001",
      "shortCiteRegEx" : "Dockner et al\\.",
      "year" : 2001
    }, {
      "title" : "Exit Probabilities and Optimal Stochastic Control",
      "author" : [ "W. Fleming" ],
      "venue" : "Applied Mathematics and Optimization 4:329–346.",
      "citeRegEx" : "Fleming,? 1977",
      "shortCiteRegEx" : "Fleming",
      "year" : 1977
    }, {
      "title" : "The free-energy principle: a rough guide to the brain? Trends in Cognitive Science 13:293–301",
      "author" : [ "K. Friston" ],
      "venue" : null,
      "citeRegEx" : "Friston,? \\Q2009\\E",
      "shortCiteRegEx" : "Friston",
      "year" : 2009
    }, {
      "title" : "Modification of UCT with Patterns in Monte-Carlo Go",
      "author" : [ "S. Gelly", "Y. Wang", "R. Munos", "O. Teytaud" ],
      "venue" : "Technical report, Inst. Nat. Rech. Inform. Auto. (INRIA).",
      "citeRegEx" : "Gelly et al\\.,? 2006",
      "shortCiteRegEx" : "Gelly et al\\.",
      "year" : 2006
    }, {
      "title" : "Robustness",
      "author" : [ "L. Hansen", "T. Sargent" ],
      "venue" : "Princeton: Princeton University Press.",
      "citeRegEx" : "Hansen and Sargent,? 2008",
      "shortCiteRegEx" : "Hansen and Sargent",
      "year" : 2008
    }, {
      "title" : "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability",
      "author" : [ "M. Hutter" ],
      "venue" : "Berlin: Springer.",
      "citeRegEx" : "Hutter,? 2004",
      "shortCiteRegEx" : "Hutter",
      "year" : 2004
    }, {
      "title" : "Optimal control as a graphical model inference problem",
      "author" : [ "H. Kappen", "V. Gómez", "M. Opper" ],
      "venue" : "Machine Learning 1:1–11.",
      "citeRegEx" : "Kappen et al\\.,? 2012",
      "shortCiteRegEx" : "Kappen et al\\.",
      "year" : 2012
    }, {
      "title" : "A linear theory for control of non-linear stochastic systems",
      "author" : [ "H. Kappen" ],
      "venue" : "Physical Review Letters 95:200201.",
      "citeRegEx" : "Kappen,? 2005a",
      "shortCiteRegEx" : "Kappen",
      "year" : 2005
    }, {
      "title" : "Path integrals and symmetry breaking for optimal control theory",
      "author" : [ "H. Kappen" ],
      "venue" : "Journal of Statistical Mechanics: Theory and Experiment.",
      "citeRegEx" : "Kappen,? 2005b",
      "shortCiteRegEx" : "Kappen",
      "year" : 2005
    }, {
      "title" : "Machine Super Intelligence",
      "author" : [ "S. Legg" ],
      "venue" : "Ph.D. Dissertation, Department of Informatics, University of Lugano.",
      "citeRegEx" : "Legg,? 2008",
      "shortCiteRegEx" : "Legg",
      "year" : 2008
    }, {
      "title" : "Portfolio Selection",
      "author" : [ "H. Markowitz" ],
      "venue" : "The Journal of Finance 7:77–91.",
      "citeRegEx" : "Markowitz,? 1952",
      "shortCiteRegEx" : "Markowitz",
      "year" : 1952
    }, {
      "title" : "Playing Atari with Deep Reinforcement Learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller" ],
      "venue" : "ArXiv (1312.5602).",
      "citeRegEx" : "Mnih et al\\.,? 2013",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Free Energy and the Generalized Optimality Equations for Sequential Decision Making",
      "author" : [ "P. Ortega", "D. Braun" ],
      "venue" : "European Workshop on Reinforcement Learning (EWRL’10).",
      "citeRegEx" : "Ortega and Braun,? 2012",
      "shortCiteRegEx" : "Ortega and Braun",
      "year" : 2012
    }, {
      "title" : "Thermodynamics as a Theory of Decision-Making with Information Processing Costs",
      "author" : [ "P.A. Ortega", "D.A. Braun" ],
      "venue" : "Proceedings of the Royal Society A 20120683.",
      "citeRegEx" : "Ortega and Braun,? 2013",
      "shortCiteRegEx" : "Ortega and Braun",
      "year" : 2013
    }, {
      "title" : "Monte Carlo Methods for Exact & Efficient Solution of the Generalized Optimality Equations",
      "author" : [ "P. Ortega", "D. Braun", "N. Tishby" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA).",
      "citeRegEx" : "Ortega et al\\.,? 2014",
      "shortCiteRegEx" : "Ortega et al\\.",
      "year" : 2014
    }, {
      "title" : "A Course in Game Theory",
      "author" : [ "M. Osborne", "A. Rubinstein" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Osborne and Rubinstein,? 1999",
      "shortCiteRegEx" : "Osborne and Rubinstein",
      "year" : 1999
    }, {
      "title" : "The Complexity of Markov Decision Processes",
      "author" : [ "C. Papadimitriou", "J. Tsitsiklis" ],
      "venue" : "Mathematics of Operations Research 12(3):441–450.",
      "citeRegEx" : "Papadimitriou and Tsitsiklis,? 1987",
      "shortCiteRegEx" : "Papadimitriou and Tsitsiklis",
      "year" : 1987
    }, {
      "title" : "Relative entropy policy search",
      "author" : [ "J. Peters", "K. Mülling", "Y. Altün" ],
      "venue" : "AAAI.",
      "citeRegEx" : "Peters et al\\.,? 2010",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling Bounded Rationality",
      "author" : [ "A. Rubinstein" ],
      "venue" : "Cambridge, MA: MIT Press.",
      "citeRegEx" : "Rubinstein,? 1998",
      "shortCiteRegEx" : "Rubinstein",
      "year" : 1998
    }, {
      "title" : "Artificial Intelligence: A Modern Approach",
      "author" : [ "S. Russell", "P. Norvig" ],
      "venue" : "Prentice-Hall, Englewood Cliffs, NJ, 3rd edition edition.",
      "citeRegEx" : "Russell and Norvig,? 2010",
      "shortCiteRegEx" : "Russell and Norvig",
      "year" : 2010
    }, {
      "title" : "Rationality and Intelligence",
      "author" : [ "S. Russell" ],
      "venue" : "Mellish, C., ed., Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, 950–957. San Francisco: Morgan Kaufmann.",
      "citeRegEx" : "Russell,? 1995",
      "shortCiteRegEx" : "Russell",
      "year" : 1995
    }, {
      "title" : "The Foundations of Statistics",
      "author" : [ "L. Savage" ],
      "venue" : "New York: John Wiley and Sons.",
      "citeRegEx" : "Savage,? 1954",
      "shortCiteRegEx" : "Savage",
      "year" : 1954
    }, {
      "title" : "Theories of Bounded Rationality",
      "author" : [ "H. Simon" ],
      "venue" : "Radner, C., and Radner, R., eds., Decision and Organization. Amsterdam: North Holland Publ. 161–176.",
      "citeRegEx" : "Simon,? 1972",
      "shortCiteRegEx" : "Simon",
      "year" : 1972
    }, {
      "title" : "A Bayesian Framework for Reinforcement Learning",
      "author" : [ "M. Strens" ],
      "venue" : "ICML.",
      "citeRegEx" : "Strens,? 2000",
      "shortCiteRegEx" : "Strens",
      "year" : 2000
    }, {
      "title" : "A generalized path integral approach to reinforcement learning",
      "author" : [ "E. Theodorou", "J. Buchli", "S. Schaal" ],
      "venue" : "Journal of Machine Learning Research 11:3137–3181.",
      "citeRegEx" : "Theodorou et al\\.,? 2010",
      "shortCiteRegEx" : "Theodorou et al\\.",
      "year" : 2010
    }, {
      "title" : "Perception-Action Cycle",
      "author" : [ "N. Tishby", "D. Polani" ],
      "venue" : "Springer New York. chapter Information Theory of Decisions and Actions, 601–636.",
      "citeRegEx" : "Tishby and Polani,? 2011",
      "shortCiteRegEx" : "Tishby and Polani",
      "year" : 2011
    }, {
      "title" : "Linearly solvable Markov decision problems",
      "author" : [ "E. Todorov" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 19, 1369–1376.",
      "citeRegEx" : "Todorov,? 2006",
      "shortCiteRegEx" : "Todorov",
      "year" : 2006
    }, {
      "title" : "Legendre-Fenchel Transforms in a Nutshell",
      "author" : [ "H. Touchette" ],
      "venue" : "Technical report, Rockefeller University.",
      "citeRegEx" : "Touchette,? 2005",
      "shortCiteRegEx" : "Touchette",
      "year" : 2005
    }, {
      "title" : "Risk Sensitive Path Integral Control",
      "author" : [ "B. van den Broek", "W. Wiegerinck", "H. Kappen" ],
      "venue" : null,
      "citeRegEx" : "Broek et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Broek et al\\.",
      "year" : 2010
    }, {
      "title" : "A Monte-Carlo AIXI Approximation",
      "author" : [ "J. Veness", "M. Ng", "M. Hutter", "W. Uther", "D. Silver" ],
      "venue" : "Journal of Artificial Intelligence Research 40:95–142.",
      "citeRegEx" : "Veness et al\\.,? 2011",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2011
    }, {
      "title" : "Theory of Games and Economic Behavior",
      "author" : [ "J. Von Neumann", "O. Morgenstern" ],
      "venue" : "Princeton: Princeton University Press.",
      "citeRegEx" : "Neumann and Morgenstern,? 1944",
      "shortCiteRegEx" : "Neumann and Morgenstern",
      "year" : 1944
    }, {
      "title" : "Complex Engineering Systems",
      "author" : [ "D. Wolpert" ],
      "venue" : "Perseus Books. chapter Information theory - the bridge connecting bounded rational game theory and statistical physics.",
      "citeRegEx" : "Wolpert,? 2004",
      "shortCiteRegEx" : "Wolpert",
      "year" : 2004
    }, {
      "title" : "Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "ICML, 928–936.",
      "citeRegEx" : "Zinkevich,? 2003",
      "shortCiteRegEx" : "Zinkevich",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Simon as an alternative account to the model of perfect rationality in the face of complex decisions (Simon 1972).",
      "startOffset" : 101,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "In artificial intelligence (AI), this development has been largely motivated by the intractability of exact planning in complex and uncertain environments (Papadimitriou and Tsitsiklis 1987) and the difficulty of finding domain-specific simplifications or approximations that would render planning tractable despite the latest theoretical advancements in understanding the AI problem (Hutter 2004).",
      "startOffset" : 155,
      "endOffset" : 190
    }, {
      "referenceID" : 8,
      "context" : "In artificial intelligence (AI), this development has been largely motivated by the intractability of exact planning in complex and uncertain environments (Papadimitriou and Tsitsiklis 1987) and the difficulty of finding domain-specific simplifications or approximations that would render planning tractable despite the latest theoretical advancements in understanding the AI problem (Hutter 2004).",
      "startOffset" : 384,
      "endOffset" : 397
    }, {
      "referenceID" : 2,
      "context" : "Newly proposed techniques based on Monte Carlo simulations such as Monte Carlo Tree Search (Coulom 2006; Browne et al. 2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability",
      "startOffset" : 91,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "Newly proposed techniques based on Monte Carlo simulations such as Monte Carlo Tree Search (Coulom 2006; Browne et al. 2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability",
      "startOffset" : 91,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "2012), Thompson sampling (Strens 2000) and Free Energy (Kappen 2005b) have become the focus of much AI and reinforcement learning research due to their wide applicability",
      "startOffset" : 55,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "and their unprecedented success in difficult problems such as computer Go (Gelly et al. 2006), universal planning (Veness et al.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 32,
      "context" : "2006), universal planning (Veness et al. 2011), human-level video game playing (Mnih et al.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "2011), human-level video game playing (Mnih et al. 2013) and robot learning (Theodorou, Buchli, and Schaal 2010).",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 16,
      "context" : "The latter group is of special theoretical interest, as it rests on a model of bounded rationality1 that yields randomized optimal policies as a consequence of resource-boundedness (Ortega and Braun 2013).",
      "startOffset" : 181,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "The initial inspiration comes from the maximum entropy principle of statistical mechanics as a way to model stochastic control problems that are linearly-solvable (Fleming 1977; Kappen 2005a; Todorov 2006).",
      "startOffset" : 163,
      "endOffset" : 205
    }, {
      "referenceID" : 10,
      "context" : "The initial inspiration comes from the maximum entropy principle of statistical mechanics as a way to model stochastic control problems that are linearly-solvable (Fleming 1977; Kappen 2005a; Todorov 2006).",
      "startOffset" : 163,
      "endOffset" : 205
    }, {
      "referenceID" : 29,
      "context" : "The initial inspiration comes from the maximum entropy principle of statistical mechanics as a way to model stochastic control problems that are linearly-solvable (Fleming 1977; Kappen 2005a; Todorov 2006).",
      "startOffset" : 163,
      "endOffset" : 205
    }, {
      "referenceID" : 7,
      "context" : "The methods from robust control were also adopted by economists to characterize model misspecification (Hansen and Sargent 2008).",
      "startOffset" : 103,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "Later, it was shown that the free energy can be derived from axioms that treat utilities and information as commensurable quantities (Ortega and Braun 2013).",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 28,
      "context" : "Finally, the free energy has also been motivated from arguments that parallel rate-distortion theory and information geometry by showing that the optimal policy minimizes the decision complexity subject to a constraint on the expected utility (Tishby and Polani 2011).",
      "startOffset" : 243,
      "endOffset" : 267
    }, {
      "referenceID" : 5,
      "context" : "It is also worth mentioning that similar approaches arise in the context of neuroscience (Friston 2009) and in game the-",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 21,
      "context" : "There are several approaches to bounded rationality in the literature, most notably those coming from the field of behavioral economics (Rubinstein 1998), which put emphasis on the procedural elements of decision making.",
      "startOffset" : 136,
      "endOffset" : 153
    }, {
      "referenceID" : 34,
      "context" : "ory (Wolpert 2004).",
      "startOffset" : 4,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "Second, it has been pointed out that decision trees are nested certaintyequivalent operations (Ortega and Braun 2012).",
      "startOffset" : 94,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "Expected utility (Von Neumann and Morgenstern 1944; Savage 1954) is the de facto standard variational principle in artificial intelligence (Russell and Norvig 2010).",
      "startOffset" : 17,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "Expected utility (Von Neumann and Morgenstern 1944; Savage 1954) is the de facto standard variational principle in artificial intelligence (Russell and Norvig 2010).",
      "startOffset" : 139,
      "endOffset" : 164
    }, {
      "referenceID" : 18,
      "context" : "The Nash equilibrium then yields the decision rules (Osborne and Rubinstein 1999):",
      "startOffset" : 52,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : "Russell (Russell 1995).",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 30,
      "context" : "We refer the reader to Touchette’s article (Touchette 2005) for a brief introduction or Boyd and Vanderberghe’s book (Boyd and Vandenberghe 2004) for a thorough treatment.",
      "startOffset" : 43,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "We refer the reader to Touchette’s article (Touchette 2005) for a brief introduction or Boyd and Vanderberghe’s book (Boyd and Vandenberghe 2004) for a thorough treatment.",
      "startOffset" : 117,
      "endOffset" : 145
    }, {
      "referenceID" : 18,
      "context" : "However, in light of our present results, it acquires a different twist; it fits with the well-known result that a Nash equilibrium is a strategy profile such that each player chooses a (mixed) strategy that renders the others players indifferent to their choices (Osborne and Rubinstein 1999).",
      "startOffset" : 264,
      "endOffset" : 293
    }, {
      "referenceID" : 13,
      "context" : "Here, an investor trades off asset returns versus portfolio risk, encoded into a regularization term that is quadratic in the policy (Markowitz 1952).",
      "startOffset" : 133,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "This suggests novel algorithms for solving the single-agent planning problem based on ideas from differential game theory (Dockner et al. 2001) and convex programming (Zinkevich 2003).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 35,
      "context" : "2001) and convex programming (Zinkevich 2003).",
      "startOffset" : 29,
      "endOffset" : 45
    } ],
    "year" : 2014,
    "abstractText" : "Recently, there has been a growing interest in modeling planning with information constraints. Accordingly, an agent maximizes a regularized expected utility known as the free energy, where the regularizer is given by the information divergence from a prior to a posterior policy. While this approach can be justified in various ways, including from statistical mechanics and information theory, it is still unclear how it relates to decisionmaking against adversarial environments. This connection has previously been suggested in work relating the free energy to risk-sensitive control and to extensive form games. Here, we show that a single-agent free energy optimization is equivalent to a game between the agent and an imaginary adversary. The adversary can, by paying an exponential penalty, generate costs that diminish the decision maker’s payoffs. It turns out that the optimal strategy of the adversary consists in choosing costs so as to render the decision maker indifferent among its choices, which is a definining property of a Nash equilibrium, thus tightening the connection between free energy optimization and game theory.",
    "creator" : "TeX"
  }
}