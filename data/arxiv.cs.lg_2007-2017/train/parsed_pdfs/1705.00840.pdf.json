{
  "name" : "1705.00840.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Pointed subspace approach to incomplete data",
    "authors" : [ "Lukasz Struski", "Marek Śmieja", "Jacek Tabor", "Struski Śmieja Tabor" ],
    "emails" : [ "lukasz.struski@uj.edu.pl", "marek.smieja@uj.edu.pl", "jacek.tabor@uj.edu.pl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: incomplete data, SVM, linear transformations"
    }, {
      "heading" : "1. Introduction",
      "text" : "Incomplete data analysis is an important part of data engineering and machine learning, since it appears in many practical problems. In medical diagnosis, a doctor may be unable to complete the patient examination due to the deterioration of health status or lack of patient’s compliance (Burke et al., 1997); in object detection, the system has to recognize the shape from low resolution or corrupted images (Berg et al., 2005); in chemistry, the complete analysis of compounds requires high financial costs (Stahura and Bajorath, 2004). In consequence, the understanding and the appropriate representation of such data is of great practical importance.\nA missing data is typically viewed as a pair (x, Jx), where x ∈ RN is a vector with missing components Jx ⊂ {1, . . . , N}. In the most straightforward approach, one can fill missing attributes with some statistic, e.g. mean, taken from existing data. Although such a strategy can be partially justified when the features are missing at random, we lose the knowledge about unknown attributes1. To preserve this information we usually add a flag indicating which components were missing. More precisely, we supply x with a binary vector 1Jx , in which 1 denotes absent feature while 0 means the present one.\nSummarizing, we perform the embedding (x, Jx) → (x,1Jx) of missing points into a vector space of extended complete data. This allows us to apply typical classification tools,\n1. In the medical data, typically some component is missing if the state of the patient is so bad, that a given numerical procedure cannot be performed. Consequently, the knowledge that given component is missing could say a lot about the state of the patient.\nc© L. Struski, M. Śmieja & J. Tabor.\nar X\niv :1\n70 5.\n00 84\n0v 1\n[ cs\n.L G\n] 2\nM ay\n2 01\nStruski Śmieja Tabor\nlike SVM, with the scalar product defined by\n〈(x, Jx), (y,Ky)〉 = 〈x, y〉+ 〈1Jx ,1Ky〉. (1)\nIn practical classification problems we usually perform various affine transformations of data, as whitening or dimensionality reduction, before training a classifier. Moreover, we may know that the data satisfy some affine constraint. It is nontrivial how to modify the flag vectors so as to keep the correspondence with such affine transformations. Thus, our main problem behind the paper can be stated as follows: How to transform the flag vectors indicating the missing components if we perform the linear (or affine) mapping of data?\nIn this contribution, we show that the answer can be given by viewing the incomplete data as pointed affine subspaces, i.e. the subspace with a distinguished point called basepoint. We first observe that a pair (x, Jx) can be formally associated with a pointed affine subspace of RN :\nx + span(ej)j∈Jx ,\nwhere (ej) N j=1 denotes the canonical base of RN and x is a selected basepoint. In other words, this is the set of all points which coincide with the representative x on the coordinates different from Jx. In consequence, by a generalized missing data point in RN we understand a pointed affine subspace Sx = x + V of RN , where x ∈ RN is a basepoint and V = Sx − x is a linear subspace. Since the basepoint can be selected with a use of various imputation techniques, we propose to choose the most probable point of Sx, i.e. to project a dataset mean onto Sx with respect to Mahalanobis scalar product given by the covariance of data.\nSuch a definition allows us to efficiently extend linear and affine operations from the standard points to missing ones, by taking the image of the subspace and the point. For example, a linear mapping F : w → Aw+b, can be extended to the case of pointed subspace x + V by\nF (x + V ) = F (x) + AV.\nGiven an affine constraint W , we restrict2 x + V by the formula (x + V ) ∩W = x + (V ∩ (W − x)).\n2. Observe that if such a constraint W is given the augmentation of the missing components must be performed in such a way as to choose the representation in W , and consequently we may assume that x ∈ W .\nThere appears another question: how to work with such data, and in particular how to embed the generalized missing data into a vector space in such a way to respect the scalar product (1) given by the flag embedding? Our main observation shows that this can be achieved by identifying a linear subspace V with an orthogonal projection pV : RN → V by considering the embedding (x, V ) → (x, pV ) ∈ RN × RN×N . We show that the scalar product of embeddings coincides with (1), i.e.\n〈(x,1Jx), (y,1Ky)〉 = 〈(x, pspan(eJ :j∈Jx)), (y, pspan(ek:k∈Ky))〉.\nThe paper is organized as follows. The next section covers the related approaches to incomplete data analysis. In third section, we define the generalized missing data, present a strategy of embedding such data into a vector space and propose a new imputation method. We also define a scalar product for such embeddings and show its connections with existing flag approach. In fourth section, we illustrate our method with sample classification results."
    }, {
      "heading" : "2. Related works",
      "text" : "The most common approach to learning from incomplete data is known as deterministic imputation (McKnight et al., 2007). In this two-step procedure, the missing features are filled first, and only then a standard classifier is applied to the complete data (Little and Rubin, 2014). Although the imputation-based techniques are easy to use for practitioners, they lead to the loss of information which features were missing and do not take into account the reasons of missingness. To preserve the information of missing attributes, one can use an additional vector of binary flags, which was discussed in the introduction.\nThe second popular group of methods aims at building a probabilistic model of incomplete data which maximizes the likelihood by applying the EM algorithm (Ghahramani and Jordan, 1994; Schafer, 1997). This allows to generate the most probable values from obtained probability distribution for missing attributes (random imputation) (McKnight et al., 2007) or to learn a decision function directly based on the distributional model. The second option was already investigated in the case of linear regression (Williams et al., 2005), kernel methods (Smola et al., 2005; Williams and Carin, 2005) or by using second order cone programming (Shivaswamy et al., 2006). One can also estimate the parameters of the probability model and the classifier jointly, which was considered in (Dick et al., 2008; Liao et al., 2007). This techniques work very well when the missing data is conditionally independent of the unobserved features given the observations, but there is no guarantee to get a reasonable estimation in more general missing not at random case.\nThere is also a group of methods, which does not make any assumptions about the missing data model and makes a prediction from incomplete data directly. In (Chechik et al., 2008) a modified SVM classifier is trained by scaling the margin according to observed features only. The alternative approaches to learning a linear classifier, which avoid features deletion or imputation, are presented in (Dekel et al., 2010; Globerson and Roweis, 2006). Finally, in (Grangier and Melvin, 2010) the embedding mapping of feature-value pairs is constructed together with a classification objective function.\nIn our contribution, we generalize the imputation-based techniques in such a way to preserve the information of missing features. To select a basepoint we propose to choose the most probable point form a subspace identifying a missing data point, however other\nStruski Śmieja Tabor\nimputation methods can be used as well. Constructed representation allows to apply various affine data transformations preserving classical scalar product before applying typical classification methods."
    }, {
      "heading" : "3. Generalized incomplete data",
      "text" : "In this section, we introduce the subspace approach to incomplete data. First, we define a generalized missing data point, which allows to perform affine transformation of incomplete data. Then, we show how to embed generalized missing data into a vector space and select a basepoint. Finally, we define a scalar product on the embedding space."
    }, {
      "heading" : "3.1. Incomplete data as pointed affine subspaces",
      "text" : "Incomplete data X can be understood as a sequence of pairs (xi, Ji), where xi ∈ RN and Ji ⊂ {1, . . . , N} indicates missing coordinates of xi. Therefore, we can associate a missing data point (x, J) with an affine subspace x+ span(ej)j∈J , where (ej)j is the canonical base of RN . Let us observe that x + span(ej)j∈J is a set of all N -dimensional vectors which coincide with x on the coordinates different from J .\nIn this paper, we focus on transforming incomplete data by affine mappings. For this purpose, we generalize the above representation to arbitrary affine subspaces, or more precisely pointed affine subspaces, which do not have to be generated by canonical bases.\nDefinition 1 A generalized missing data point is defined as a pointed affine subspace Sx = x + V , where x ∈ RN is a basepoint and V = Sx − x is a linear subspace of RN .\nA basepoint can be selected by filling missing attributes with a use of any imputation method, which will be discussed in the next subsection.\nRemark 2 Observe that the notion of pointed affine subspace differs from classical affine subspace. In particular, pointed subspace depends on the selection of basepoint. In consequence, we can create two different generalized missing data points Sy, Sz from the same missing data point (x, J) by using different imputation methods.\nFirst, we show that the above definition is useful for defining linear mappings on incomplete data. Let Sx = x+V be a generalized missing data point and let f : RN 3 w → Aw+b be an affine map. We can transform a generalized missing data point x + V into another missing data point by the formula:\nf(x + V ) = {Aw + b : w ∈ x + V }.\nThe basepoint x is mapped into Ax + b, while the linear part of f(x + V ) is given by\nf(x + V )− f(x) = AV.\nConsequently, we arrive at the definition:\nDefinition 3 For a a generalized missing data point Sx = x + V and an affine mapping f : w → Aw + b we put:\nf(x + V ) = (Ax + b) + AV,\nwhere Ax + b is a basepoint and AV is a linear subspace.\nOne can easily compute and represent AV , if the orthonormal base v1, . . . , vn of V is given, namely we simply orthonormalize the sequence Av1, . . . , Avn."
    }, {
      "heading" : "3.2. Embedding of generalized missing data",
      "text" : "The above representation is useful for understanding and performing affine transformations of incomplete data, such as whitening, dimensionality reduction or incorporating affine constraints to data. Nevertheless, typical machine learning methods require vectors or a kind of kernel (or similarity) matrix as the input. We show how to embed generalized missing data into a vector space.\nA generalized missing data point Sx = x + V consists of a basepoint x ∈ RN which is an element of vector space and a linear subspace V . To represent a subspace V , we propose to use a matrix of orthogonal projection pV onto V . To get an exact form of pV , let us assume that (vj)j∈J is an orthonormal base of V . Then, the projection of y ∈ RN can be calculated by\npV (y) = ∑ j∈J 〈y, vj〉vj = ∑ j∈J vjv T j y = ( ∑ j∈J vjv T j )y,\nwhich implies that\npV = ∑ j∈J vjv T J .\nThe selection of basepoint relies on filling missing attributes with some concrete values, which is commonly known as imputation. In our setting, by the imputation we denote a function Φ : X → RN such that\nΦ(Sx) ∈ Sx,\nfor a generalized missing data Sx. In the case of classical incomplete data, missing attributes are often filled with a mean or a median calculated from existing values for a given attribute. However, these imputations cannot be easily defined in a general case, because the linear part of generalized missing data point might be an arbitrary linear subspace (not necessarily a subspace generated by a subset of canonical base). Let us observe that another popular imputation method, which fills the missing coordinates with zeros can be defined for generalized incomplete data. This is performed by selecting a basepoint of an incomplete data point Sx = x + V as the orthogonal projection of missing data x onto the subspace orthogonal to V , i.e.:\nxV ⊥ = x− pV (x) = x− ∑ j∈J 〈xj , vj〉vj ,\nwhere (vj)j∈J is an arbitrary orthonormal base of V . If V is represented by canonical base then this is equivalent to filling missing attributes with zeros.\nWe propose another technique for setting missing values, which extends zero imputation method. Let us assume that (m,Σ) are the mean and covariance matrix estimated for incomplete dataset X. In this method, a basepoint of x + V is selected as the orthogonal projection of m onto x + V with respect to the Mahalanobis scalar product parametrized by Σ, i.e.\nx (m,Σ) V = x + p Σ V (m− x),\nStruski Śmieja Tabor\nwhere pΣV denotes a projection matrix onto V with respect to Mahalanobis scalar product given by Σ. To obtain the values for m and Σ in practice, one can use existing attributes of incomplete data for the calculation of a sample mean and a covariance matrix. Alternatively, if data satisfy missing at random assumption, then the EM algorithm can be applied to estimate the probability model describing data (Schafer, 1997). We call this technique by the most probable point imputation.\nSummarizing, our embedding is defined as follows:\nDefinition 4 A generalized missing data point is embedded in a vector space by\nSx → (x, pV ) ∈ RN × RN×N ,\nwhere Sx = x + V and x is a basepoint.\nExample 1 To illustrate the effect of missing data imputation and transformation, let us consider the whitening operation:\nWhitening(x) = Σ−1/2(x−m),\nwhere Σ is the covariance, and m the mean of X. For a generalized missing data the above operation is defined by:\nWhitening(x + V ) = Σ−1/2(x−m) + Σ−1/2V.\nIn other words, we map a basepoint in a classical way and transform a subspace V into a linear subspace Σ−1/2V . The illustration is given in Figure 2.\nExample 2 In the case of high dimensional data, we sometimes reduce a dimension of input data space by applying the Principle Component Analysis, which is defined by:\nPCA(x) = W T (x−m),\nwhere m is a mean of a dataset and k columns of W are the leading eigenvectors of covariance matrix Σ. This operation can be extended to the case of generalized missing data by: PCA(x + V ) = W T (x−m) + W TV. An example of the above operation is illustrated in the Figure 3."
    }, {
      "heading" : "3.3. Scalar product for SVM kernel",
      "text" : "To apply most of classification methods it is necessary to define a scalar product (kernel matrix) on a data space. As a natural choice, one could sum the scalar products between basepoints and embedding matrices, i.e.\n〈x + V, y + W 〉 = 〈x, y〉+ 〈pV , pW 〉. (2)\nHowever, for a data space of dimension N , we have ‖pV ‖2 = N , which implies that the weight of projection can dominate the first part of (2) concerning basepoints. Consequently, we decided to introduce an additional parameter to allow reducing the importance of projection part:\nDefinition 5 Let D ∈ [0, 1] be fixed. As a scalar product between two generalized missing data points we put:\n〈x + V, y + W 〉D = 〈x, y〉+ D〈pV , pW 〉. (3)\nLet us observe that the above parametric scalar product can be implemented by taking the embedding x + V → (x, √ DpV ) and then using formula (2) for a scalar product.\nRemark 6 Observe that the value of function (3) strictly depends on the selection of basepoints, which makes it not well defined scalar product in the space of classical affine subspaces. Indeed, x + V defines the same affine subspace as x + v + V , where v ∈ V , but such shifts may lead to different values of the right hand side of (3). However, this is well defined scalar product in the case of pointed affine subspaces, because two different selections of basepoints give different pointed affine subspaces (see Remark 2). In consequence, it might be safely used in the case of generalized missing data points considered in this paper.\nThe following proposition shows how to calculate a scalar product between matrices defining two orthogonal projections onto linear subspaces.\nProposition 7 Let us consider subspaces\nV = span(vj : j ∈ J),W = span(wj : j ∈ K).\nStruski Śmieja Tabor\nwhere vj and wk are orthonormal sequences. If pV , pW denote orthogonal projections onto V,W , respectively, then 〈pV , pW 〉 = ∑\nj∈J,k∈K 〈vj , wk〉2.\nProof By the definition of orthogonal projections and the scalar product between matrices, we have 〈pV , pW 〉 = ∑\nj∈J,k∈K tr((vjv\nT j ) T (wkw T k )). (4)\nMaking use of tr(AB) = tr(BA), we get\ntr((vjv T j ) T (wkw T k )) = tr(vjv T j wkw T k ) = tr(v T j wkw T k vj) = (v T j wk) · (wTk vj) = 〈vj , wk〉2.\nFinally,\n〈pV , pW 〉 = ∑\nj∈J,k∈K 〈vj , wk〉2.\nConcluding, the scalar product between embedding of two generalized missing data points given by Definition 5 can be calculated as:\n〈x + V, y + W 〉D = 〈x, y〉+ D ∑ i,j (pV )ij(pW )ij = 〈x, y〉+ D ∑ j∈J,k∈K 〈vj , wk〉2,\nwhere (vj)j∈J , (wk)k∈K are orthonormal bases of V,W , respectively. The last expression can be more numerically efficient if the dimension of the subspaces (the number of missing attributes) is much smaller than the dimension of the whole space.\nRemark 8 One of typical representations of missing data (x, J) relies on filling unknown attributes and supplying it with a binary flag vector 1J ∈ RN , in which bit 1 denotes coordinate belonging to J . This leads to the embedding of the missing data into a vector space given by (x, J)→ (x,1J) ∈ RN × RN . Then, the scalar product of such embedding can be defined by\n〈(x,1J , )(y,1K)〉 = 〈x, y〉+ 〈1J ,1K〉 = 〈x, y〉+ card(J ∩K). (5)\nIt is worth to noting that the formula (5) coincides with a scalar product defined for generalized missing data (2) (for D = 1). Indeed, if V = span(ej : j ∈ J) and W = span(ek : k ∈ K), for J,K ⊂ {1, . . . , N}, then by Proposition 7 we have,\n〈pV , pW 〉 = ∑\nj∈J,k∈K 〈ej , ek〉2 = ∑ l∈J∩K 〈el, el〉2 = ∑ l∈J∩K 1 = card(J ∩K),\nwhich is exactly the RHS of (5). Therefore, our approach generalizes and theoretically justifies the flag approach to missing data analysis. The importance of our construction lies in its generality, which in particular allows for performing typical affine transformations of data. In the case of flag representation, there is no obvious solution how to perform such mappings on flag vector."
    }, {
      "heading" : "4. Experiments",
      "text" : "To illustrate our approach we applied it in SVM classification experiments, which assumed the use of whitening operation before performing a classification phase. We used examples retrieved from UCI repository combined with two strategies for attributes removal: random and structural. Finally, one real medical dataset was employed, which simulates a real process of missing features.\nFor all cases, the following procedure was applied. First, we set missing features with a use of one of four strategies mentioned in the paper:\n1. Mean: average value of the feature over training set.\n2. Median: median of the feature over training set.\n3. Zero imputation: missing features were filled with zeros.\n4. Most probable imputation: it was described in section 3.2.\nFor a simplicity the mean and covariance matrix were estimated from a training set with a use of norm R package3.\nNext, we performed a whitening of dataset (making use of the parameters returned by norm) based on two approaches:\n1. No information: Feature vectors with imputed missing attributes were whiten.\n2. Subspace: Feature vectors with imputed values were joined with corresponding projection matrices and then the entire vectors were whiten according to the Definition 3.\n3. Since the use of EM method implemented in norm is justified in missing at random case, then one could also estimated a mean and covariance based on existing attributes.\nStruski Śmieja Tabor\nThe above scenarios represent classical imputation and our pointed affine subspace approach. We would like to investigate how the information preserved in the subspace influences the classification results.\nFinally, we calculated the scalar products (kernel matrices) for such representations of data and trained SVM classifier implemented in libsvm (Chang and Lin, 2011). Missing features of test set instances were filled and transformed based on a training set only.\nAll experiments assumed double 5-fold cross validation. More precisely, for every division into train and test sets, the required hyperparameters were tuned using inner 5-fold cross validation applied on training set. The combination of parameters maximizing mean accuracy score (on validation set) was used to learn a final classifier on a entire training set, while the performance was evaluated on a testing set that was not used during training. The accuracy was averaged over all 5 trails. We learned a standard margin parameter C as well as a parameter D in the formula of scalar product for subspace embedding. We performed a grid search in the following ranges: C = {10k : k = −2,−1, 0, 1} and D = { 1\n2k : k = 0, 1, . . . , 10}."
    }, {
      "heading" : "4.1. UCI datasets",
      "text" : "We used three UCI datasets (for datasets with more than two classes we selected two the most numerous classes): breast cancer (BC), ionosphere (IS) and yeast (Y) (Asuncion and Newman, 2007). In the first case, we randomly removed 90% of features. In the second option, we defined a structural process of attributes removal. More precisely, we drawn N points x1, . . . , xN of a dataset X ⊂ RN . Then, for every x ∈ X we removed its i-th attribute with a probability exp(−t‖x−xi‖Σ)), where ‖x‖Σ denotes the Mahalanobis norm of x with respect to Σ and t > 0 was chosen to remove approximately 90% of attributes.\nThe results presented in Table 1 show that there is no benefit from identifying absent attributes when the features were missing completely at random. One can observe that most probable point imputation usually provided the highest accuracy among the imputation strategies.\nIn the case of structurally missing features, Table 2, the proposed subspace approach gave better classification results for all datasets and for all cases of imputations. Moreover, the most probable point imputation outperformed other strategies of filling missing coordinates on two out of three datasets."
    }, {
      "heading" : "4.2. Medical data",
      "text" : "In this application we considered a real angiological dataset acquired from Jagiellonian Center of Experimental Therapeutic containing patients’ examinations, http://jcet.eu/ new_en/. The goal was to find patients with atherosclerosis. Innovative medical tests are very expensive, time-consuming and in some cases they cannot be successfully completed due to the patient’s condition. In consequence, research database contains many empty cells, which is the effect of purely structural process. Since some of parameters are discrete as well as real valued numbers presented in different scales, then a whitening of data is a natural preprocessing step.\nThe results illustrated in Table 3 partially confirm the hypothesis suggested in previous experiment. Indeed, the use of proposed subspace embedding, gave higher accuracy for all\nimputation strategies, but the benefit from its application was not significant. It is difficult to decide which imputation strategy was optimal because all of them provided comparable results."
    }, {
      "heading" : "5. Conclusion",
      "text" : "The paper generalized the existing approach of identifying missing attributes with binary flags. To enable appropriate affine transformations of data, we represented incomplete data as pointed affine subspaces and embedded them into a vector space by linking a pointed subspace with a basepoint joined with a corresponding projection matrix. In the same spirit we proposed to select a basepoint as the most probable point from a subspace, which extends the well-known zero imputation strategy. Such a combination provided the best performance in conducted classification experiments in most cases."
    } ],
    "references" : [ {
      "title" : "Compliance with cardiovascular disease prevention strategies: a review of the research",
      "author" : [ "Lora E Burke", "Jacqueline M Dunbar-Jacob", "Martha N Hill" ],
      "venue" : "Annals of Behavioral Medicine,",
      "citeRegEx" : "Burke et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Burke et al\\.",
      "year" : 1997
    }, {
      "title" : "Libsvm: A library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang and Lin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang and Lin.",
      "year" : 2011
    }, {
      "title" : "Max-margin classification of data with absent features",
      "author" : [ "Gal Chechik", "Geremy Heitz", "Gal Elidan", "Pieter Abbeel", "Daphne Koller" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chechik et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chechik et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning to classify with missing and corrupted features",
      "author" : [ "Ofer Dekel", "Ohad Shamir", "Lin Xiao" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning from incomplete data with infinite imputations",
      "author" : [ "Uwe Dick", "Peter Haider", "Tobias Scheffer" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Dick et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dick et al\\.",
      "year" : 2008
    }, {
      "title" : "Supervised learning from incomplete data via an EM approach",
      "author" : [ "Zoubin Ghahramani", "Michael I Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ghahramani and Jordan.,? \\Q1994\\E",
      "shortCiteRegEx" : "Ghahramani and Jordan.",
      "year" : 1994
    }, {
      "title" : "Nightmare at test time: robust learning by feature deletion",
      "author" : [ "Amir Globerson", "Sam Roweis" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Globerson and Roweis.,? \\Q2006\\E",
      "shortCiteRegEx" : "Globerson and Roweis.",
      "year" : 2006
    }, {
      "title" : "Feature set embedding for incomplete data",
      "author" : [ "David Grangier", "Iain Melvin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Grangier and Melvin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Grangier and Melvin.",
      "year" : 2010
    }, {
      "title" : "Quadratically gated mixture of experts for incomplete data classification",
      "author" : [ "Xuejun Liao", "Hui Li", "Lawrence Carin" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Liao et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2007
    }, {
      "title" : "Statistical analysis with missing data",
      "author" : [ "Roderick J.A. Little", "Donald B Rubin" ],
      "venue" : null,
      "citeRegEx" : "Little and Rubin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Little and Rubin.",
      "year" : 2014
    }, {
      "title" : "Missing data: A gentle introduction",
      "author" : [ "Patrick E McKnight", "Katherine M McKnight", "Souraya Sidani", "Aurelio Jose Figueredo" ],
      "venue" : null,
      "citeRegEx" : "McKnight et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "McKnight et al\\.",
      "year" : 2007
    }, {
      "title" : "Analysis of incomplete multivariate data",
      "author" : [ "Joseph L Schafer" ],
      "venue" : "CRC Press,",
      "citeRegEx" : "Schafer.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schafer.",
      "year" : 1997
    }, {
      "title" : "Second order cone programming approaches for handling missing and uncertain data",
      "author" : [ "Pannagadatta K Shivaswamy", "Chiranjib Bhattacharyya", "Alexander J Smola" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shivaswamy et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Shivaswamy et al\\.",
      "year" : 2006
    }, {
      "title" : "Kernel methods for missing variables",
      "author" : [ "Alexander J Smola", "SVN Vishwanathan", "Thomas Hofmann" ],
      "venue" : "In Proceedings of the International Conference on Artificial Intelligence and Statistics. Citeseer,",
      "citeRegEx" : "Smola et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Smola et al\\.",
      "year" : 2005
    }, {
      "title" : "Virtual screening methods that complement HTS",
      "author" : [ "Florence L Stahura", "Jurgen Bajorath" ],
      "venue" : "Combinatorial Chemistry & High Throughput Screening,",
      "citeRegEx" : "Stahura and Bajorath.,? \\Q2004\\E",
      "shortCiteRegEx" : "Stahura and Bajorath.",
      "year" : 2004
    }, {
      "title" : "Analytical kernel matrix completion with incomplete multi-view data",
      "author" : [ "David Williams", "Lawrence Carin" ],
      "venue" : "In Proceedings of the ICML Workshop on Learning With Multiple Views,",
      "citeRegEx" : "Williams and Carin.,? \\Q2005\\E",
      "shortCiteRegEx" : "Williams and Carin.",
      "year" : 2005
    }, {
      "title" : "Incomplete-data classification using logistic regression",
      "author" : [ "David Williams", "Xuejun Liao", "Ya Xue", "Lawrence Carin" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Williams et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In medical diagnosis, a doctor may be unable to complete the patient examination due to the deterioration of health status or lack of patient’s compliance (Burke et al., 1997); in object detection, the system has to recognize the shape from low resolution or corrupted images (Berg et al.",
      "startOffset" : 155,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : ", 2005); in chemistry, the complete analysis of compounds requires high financial costs (Stahura and Bajorath, 2004).",
      "startOffset" : 88,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "Related works The most common approach to learning from incomplete data is known as deterministic imputation (McKnight et al., 2007).",
      "startOffset" : 109,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "In this two-step procedure, the missing features are filled first, and only then a standard classifier is applied to the complete data (Little and Rubin, 2014).",
      "startOffset" : 135,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "The second popular group of methods aims at building a probabilistic model of incomplete data which maximizes the likelihood by applying the EM algorithm (Ghahramani and Jordan, 1994; Schafer, 1997).",
      "startOffset" : 154,
      "endOffset" : 198
    }, {
      "referenceID" : 11,
      "context" : "The second popular group of methods aims at building a probabilistic model of incomplete data which maximizes the likelihood by applying the EM algorithm (Ghahramani and Jordan, 1994; Schafer, 1997).",
      "startOffset" : 154,
      "endOffset" : 198
    }, {
      "referenceID" : 10,
      "context" : "This allows to generate the most probable values from obtained probability distribution for missing attributes (random imputation) (McKnight et al., 2007) or to learn a decision function directly based on the distributional model.",
      "startOffset" : 131,
      "endOffset" : 154
    }, {
      "referenceID" : 16,
      "context" : "The second option was already investigated in the case of linear regression (Williams et al., 2005), kernel methods (Smola et al.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : ", 2005), kernel methods (Smola et al., 2005; Williams and Carin, 2005) or by using second order cone programming (Shivaswamy et al.",
      "startOffset" : 24,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : ", 2005), kernel methods (Smola et al., 2005; Williams and Carin, 2005) or by using second order cone programming (Shivaswamy et al.",
      "startOffset" : 24,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : ", 2005; Williams and Carin, 2005) or by using second order cone programming (Shivaswamy et al., 2006).",
      "startOffset" : 76,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "One can also estimate the parameters of the probability model and the classifier jointly, which was considered in (Dick et al., 2008; Liao et al., 2007).",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "One can also estimate the parameters of the probability model and the classifier jointly, which was considered in (Dick et al., 2008; Liao et al., 2007).",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "In (Chechik et al., 2008) a modified SVM classifier is trained by scaling the margin according to observed features only.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "The alternative approaches to learning a linear classifier, which avoid features deletion or imputation, are presented in (Dekel et al., 2010; Globerson and Roweis, 2006).",
      "startOffset" : 122,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "The alternative approaches to learning a linear classifier, which avoid features deletion or imputation, are presented in (Dekel et al., 2010; Globerson and Roweis, 2006).",
      "startOffset" : 122,
      "endOffset" : 170
    }, {
      "referenceID" : 7,
      "context" : "Finally, in (Grangier and Melvin, 2010) the embedding mapping of feature-value pairs is constructed together with a classification objective function.",
      "startOffset" : 12,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "Alternatively, if data satisfy missing at random assumption, then the EM algorithm can be applied to estimate the probability model describing data (Schafer, 1997).",
      "startOffset" : 148,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "Finally, we calculated the scalar products (kernel matrices) for such representations of data and trained SVM classifier implemented in libsvm (Chang and Lin, 2011).",
      "startOffset" : 143,
      "endOffset" : 164
    } ],
    "year" : 2017,
    "abstractText" : "Incomplete data are often represented as vectors with filled missing attributes joined with flag vectors indicating missing components. In this paper we generalize this approach and represent incomplete data as pointed affine subspaces. This allows to perform various affine transformations of data, as whitening or dimensionality reduction. We embed such generalized missing data into a vector space by mapping pointed affine subspace (generalized missing data point) to a vector containing imputed values joined with a corresponding projection matrix. Such an operation preserves the scalar product of the embedding defined for flag vectors and allows to input transformed incomplete data to typical classification methods.",
    "creator" : "LaTeX with hyperref package"
  }
}