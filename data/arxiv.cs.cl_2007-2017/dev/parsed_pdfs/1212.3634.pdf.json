{
  "name" : "1212.3634.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TEXT MINING APPLICATIONS",
    "authors" : [ "Hanane FROUD", "Abdelmonaim LACHKAR", "Said ALAOUI OUATIK", "Sidi Mohamed", "Ben Abdellah (USMBA", "Fez" ],
    "emails" : [ "hanane_froud@yahoo.fr,", "abdelmonaime_lachkar@yahoo.fr", "s_ouatik@yahoo.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "KEYWORDS Arabic Language; Latent Semantic Analysis (LSA); Similarity Measures; Root and Light Stemmer."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Arabic Documents Representation is challenge and a crucial stage; it may impact positively or negatively on the accuracy of any Text Mining tasks such as Text Categorization, Summarization, Documents Clustering, Filtering, and Retrieval purposes. Generally, Arabic Text Mining applications usually represent documents as ‘Bags-of-Words’ or Vector Space Model (VSM) [1][2][3], in which text documents are represented as a set of points in a high dimensional vector space. However, VSM has four primary limitations which can be grouped into two problems: the high dimensionality problem and the lack of semantics one. These limitations are: First, in information retrieval application, a long document gets a low similarity to a query because the normalized value of the document becomes high. As a result, a longlength document has little opportunity to match a query. Second, the order of words in a document is still ignored because of the bag of words assumption. The syntactic structure of a document is potentially valuable information. Third, keywords in a query have to be exactly matched with words in documents, and thus the issue of synonymy is not addressed. Fourth and finally, the issue of polysemy is not addressed because VSM only considers word form.\nTherefore, for the Arabic Documents Representation not all features equally represent the document's semantics; in fact, some of these features may be redundant and add nothing to the\nmeaning of the document; others might be synonymous and therefore capturing one of them is enough to enhance the semantic for Documents Representation purposes.\nIn the other hand, the Arabic Documents Representation may also be impacting by the use of different text pre-processing approaches, which affect any Text Mining tasks as we have already concluded in our previous works [7][15].\nThe main goal of this paper is to compare and contrast the effect of two preprocessing techniques, that affect the document's semantics, applied to Arabic corpus: Root-based (Stemming), and Stem-based (Light Stemming) approaches for measuring the semantic between Arabic words with the well known abstractive model -Latent Semantic Analysis (LSA)- with different distance functions and similarity measures [15], to overcome the above problems for Arabic Documents Representation. The LSA model is based on the Singular Value Decomposition SVD. We used SVD technique to reduce the dimensionality of the vector space [4] [5], and to build the word representative matrix. This matrix will be used later to quantify the Arabic words similarity measure. LSA technique [6] [7] is used to quantify the similarity between Arabic words by their tendency to occur in some contexts than others. The context of a word [8] consists of a set of tokens distributed on both sides of the word (after and before the word).\nThe rest of this paper is organized as follows. The next section introduces the concept of \"Latent Semantic Analysis (LSA)\" and its use for measuring similarity between two Arabic words. Section 3 describes the Stemming techniques for the Arabic Language used in the experiments. Section 4 discusses the similarity measures and their semantics. Section 5 explains experiment settings, dataset, results and analysis. In Section 6 we conclude."
    }, {
      "heading" : "2. LATENT SEMANTIC ANALYSIS (LSA)",
      "text" : "Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text. The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to each other.\nLatent Semantic Analysis (LSA) model is the automatic procedure proposed by [9] to construct a vector space. This procedure applies to a vast corpus of texts and includes the three stages below where the corpus of texts is gradually transformed into semantic vector space of several hundred dimensions. The text corpus includes two types of separators, paragraphs boundaries and spaces between words. The paragraph is regarded as the string of characters between two blanks and the word is the string of characters between two spaces.\nThe first step of the procedure is to represent the body as a matrix of co-occurrences. The second is to apply to this matrix a factor analysis called Singular Value Decomposition to get a space. The last step is to eliminate, among the dimensions of space resulting from the singular value decomposition, a number of dimensions, regarded as irrelevant.\n2.1. Building the Co-occurrence Matrix\nFor a given corpus, the number of times each word appears in each paragraph is recognized. The frequencies of co-occurrence between words and paragraphs are calculated. These frequencies are listed in a matrix. In the Column we find each paragraph, in the row, every word. At the intersection of a column and row, each cell contains the frequency of occurrences of a word in a paragraph.\n2.2. The Singular Value Decomposition\nThe Singular Value Decomposition is a general method of decomposition of a linear matrix in independents principal components. As a principal components analysis, this method allows to identify a set of data - here the co-occurrence frequencies - a number of factores uncorrelated with each other and making each account for the variance of the data set. If n factors account from the totality of the variance in frequency of co-occurrence, then the data can be represented in an n-dimensional space, each dimension corresponding to a factor. The table containing the\nwords in rows and the contexts in the columns form a rectangular matrix , where m is the\nnumber of rows and c the number of columns. This rectangular matrix is decomposed into\nthree matrices. It is the product, , and :\nmatrix is a diagonal matrix with n columns and n rows, whose cells contain in the diagonal \"singular values\". The word matrix, U, has m lines with n values. The n values in each row are the coordinates of a vector represented in an n-dimensional space associated with a word corpus. Each word is represented in an n-dimensional space. After this step, the similarity between words can then be calculated.\n2.3. Reducing the Number of Dimensions\nAll dimensions emerged from the Singular Value Decomposition is not relevant. The dimensions associated with the lowest singular values explain only a very small proportion of the variance in the original data. If these dimensions were not eliminated, the model would make errors in estimating the semantic similarity. Since the dimensions are abstract, there are no criteria for elimination of irrelevant dimensions. Consequently, the number of dimensions eliminated must be determined empirically. In most applications [6], the number of relevant\n(1)\nfactors can go from 30 000 to 300. This means that the vectors pass of 30 000 coordinates in a space of 30 000 dimensions to 300 coordinates in a space of 300 dimensions."
    }, {
      "heading" : "3. ARABIC TEXT PREPROCESSING",
      "text" : "3.1. Arabic Language Structure\nThe Arabic language is the language of the Holy Quran. It is one of the six official languages of the United Nations and the mother tongue of approximately 300 million people. It is a Semitic language with 28 alphabet letters. Its writing orientation is from right-to-left. It can be classified into three types: Classical Arabic ( ا ا), Modern Standard Arabic ( ا ا) and Colloquial Arabic dialects ( ا ا).\nClassical Arabic is fully vowelized and it is the language of the holy Quran. Modern Standard Arabic is the official language throughout the Arab world. It is used in official documents, newspapers and magazines, in educational fields and for communication between Arabs of different nationalities. Colloquial Arabic dialects, on the other hand, are the languages spoken in the different Arab countries; the spoken forms of Arabic vary widely and each Arab country has its own dialect.\nModern Standard Arabic has a rich morphology, based on consonantal roots, which depends on vowel changes and in some cases consonantal insertions and deletions to create inflections and derivations which make morphological analysis a very complex task [22]. There is no capitalization in Arabic, which makes it hard to identify proper names, acronyms, and abbreviations.\n3.2. Stemming\nArabic word Stemming is a technique that aim to find the lexical root or stem (Figure 2) for words in natural language, by removing affixes attached to its root, because an Arabic word can have a more complicated form with those affixes. An Arabic word can represent a phrase in English, for example the word ََو آ$َ%َأ (ātata krwnanaā1):”do you remember us?” is decomposed as follows (Table 1):\nTable 1. Arabic Word Decomposition\nPostfix Suffix Root Prefix Antefix نو آ ت أ\nA pronoun meaning “us”\nTermination of conjugation\nremember A letter meaning the tense and the person of conjugation\nPreposition for asking question\n3.3. Root-based versus Stem-based approaches\nArabic stemming algorithms can be classified, according to the desired level of analysis, as root-based approach (Khoja [10]); stem-based approach (Larkey [11]). In this section, a brief review on the two stemming approaches for stemming Arabic Text is presented.\nRoot-Based approach uses morphological analysis to extract the root of a given Arabic word. Many algorithms have been developed for this approach. Al-Fedaghi and Al-Anzi algorithm tries to find the root of the word by matching the word with all possible patterns with all possible affixes attached to it [17]. The algorithm does not remove any prefixes or suffixes. AlShalabi morphology system uses different algorithms to find the roots and patterns [18]. This algorithm removes the longest possible prefix, and then extracts the root by checking the first five letters of the word. This algorithm is based on an assumption that the root must appear in the first five letters of the word. Khoja has developed an algorithm that removes prefixes and suffixes, all the time checking that it’s not removing part of the root and then matches the remaining word against the patterns of the same length to extract the root [10].\nThe aim of the Stem-Based approach or Light Stemmer approach is not to produce the root of a given Arabic word, rather is to remove the most frequent suffixes and prefixes. Light stemmer is mentioned by some authors [19,20,11,21], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.\nIn our work, we believe that the preprocessing of Arabic Documents is challenge and crucial stage. It may impact positively or negatively on the accuracy of any Text Mining tasks; therefore the choice of the preprocessing approaches will lead by necessity to the improvement of any Text Mining tasks very greatly.\nTo illustrate this, in Figure 2, we show an opposite example using Khoja stemmer. It produces a root that is not related to the original word.\nFor example, the word (ت ) which means (organizations) is stemmed to ( ) which means (he was thirsty) instead of the correct root ( ).\nIn order to test the effect of the two stemming approaches above on the similarity measures with the LSA model, we selected tow famous Stemming algorithms: the Morphological Analyzer from Khoja and Garside [10], and the Light Stemmer developed by Larkey [11]."
    }, {
      "heading" : "4. SIMILARITY MEASURES",
      "text" : "In this section we discuss the four similarity measures that were tested in [12], and we include these four measures in our work to effect the measure of the semantic between Arabic words.\n4.1. Metric\nNot every distance measure is a metric. To qualify as a metric, a measure d must satisfy the following four conditions. Let x and y be any two objects in a set and ( , )d x y be the distance between x and y.\n1. The distance between any two points must be non-negative, that is, ( , ) 0d x y ≥ . 2. The distance between two objects must be zero if and only if the two objects are\nidentical, that is, ( , ) 0d x y = if and only if x y= . 3. Distance must be symmetric, that is, distance from x to y is the same as the distance\nfrom y to x, i.e. ( , ) ( , )d x y d y x= .\n4. The measure must satisfy the triangle inequality, which is ( , ) ( , ) ( , )d x z d x y d y z≤ + .\n4.2. Euclidean Distance\nEuclidean distance is widely used in clustering problems, including clustering text. It satisfies\nall the above four conditions and therefore is a true metric. Given two words W a and Wb represented by their vectors ta ur and tb ur\nrespectively, the Euclidean distance of the two words is defined as:\n12 2( , ) ( ) ,, ,1 m D t t w wa t aE b t bt ∑= − = ur ur\nwhere 1, , ( , ..., )Ta a m at w w=\nr\nand 1, ,( , ..., ) T\nb b m bt w w= r\n.\n4.3. Cosine Similarity\nCosine similarity is one of the most popular similarity measure applied to text documents, such as in numerous information retrieval applications [13] and clustering too [14]. Given two words W a and W b represented by their vectors t a ur and tb ur respectively their cosine similarity is:\n. ( , ) , t ta bS IM t taC b t ta b = ×\nur ur ur ur\nur ur\n(2)\n(3)\nwhere 1, ,( , ..., ) T\na a m at w w= r\nand 1, ,( , ..., ) T\nb b m bt w w= r\n. The Cosine similarity varies from -1 to 1.\n4.4. Jaccard Coefficient\nThe Jaccard coefficient, which is sometimes referred to as the Tanimoto coefficient, measures similarity as the intersection divided by the union of the objects. The formal definition is:\n. ( , ) ,2 2\n. t ta bSIM t taJ b t t t ta ab b = + −\nur ur ur ur\nur ur ur ur\nwhere 1 , ,( , . . . , ) T\na a m at w w= r\nand 1 , ,( , . . . , ) T\nb b m bt w w= r\n.\nThe Jaccard coefficient is a similarity measure and it is 1 when the t ta b= ur ur and 0 when t a ur\nand tb ur\nare disjoint.\n4.5. Pearson Correlation Coefficient\nPearson’s correlation coefficient is another measure of the extent to which two vectors are related. There are different forms of the Pearson correlation coefficient formula. Given two words W a and W b represented by their vectors ta ur and tb ur\nrespectively, a commonly used form is:\n,1 , ( , ) ,\n2 2 2 2 ,1 1 , mm w w TF TFt a at t b b SIM t taP b m mm w TF m w TFt a at t t b b ∑ × − ×= = ∑ ∑− −= =      ur ur\nwhere ,1 mT F wa t at= ∑ = and 1 , mT F wtb t b= ∑ =\nThis is also a similarity measure. However, unlike the other measures, it ranges from -1 to +1 and it is 1 when t ta b= ur ur ."
    }, {
      "heading" : "5. EXPERIMENTS RESULTS AND DISCUSSION",
      "text" : "Experiments are applied by using two Arabic testing datasets and by applying two schemes of stemming (Figure 4) : the Larkey's Stemmer developed by [11], and the Khoja's Stemmer [10]. To compute words similarity, we propose to use four schemes of different measures such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient (Figure 4).\n5.1. Dataset\nWe experimented with two testing datasets both are from the website: http://www.spa.gov.sa/ for the Saudi Press Agency: the first one is a heterogeneous dataset; it’s composed of 252 documents from different categories (Economics, Politics, and Sports). The second contains 257 documents belonging to one category (politics) [8]. The complete characteristics of the used corpus are described in Table.2. In the following, for each dataset we experimented with the above four similarity measures using Larkey’s Stemmer (Table.3, Table.4) and using Khoja’s Stemmer (Table.5, Table.6).The Euclidean Distance is a distance measure and is bounded\nin . For an ideal measure of the similarity between two Arabic words, its Euclidean\n(4)\n(5)\ndistance value 0. In the other hand, the Cosine Similarity, Pearson Coefficient and Jaccard Coefficient are similarity measures and they are equal to 1 when the words are similar.\n5.2. Results\nIn this work, we propose to compare the different schemes (two stemming schemes and four different measures) using both Similar Words and Different Words: Similar Words are words that have the same semantic but they may have different stems or different roots. Different Words are words that have not the same semantic even they may have the same stem or the same root. 5.2.1 Results with Larkey’s Stemmer\n5.2.1. a. Test using Similar Words\nIn Tab.3, Cosine, Pearson measures are good and the Euclidean distance is not for the pairs like: (%ر ' ()او, *+$ر) and ( ,ا ا, #,ا ا), this result shows that the similarity between those Arabic words is well detected by the LSA model using Larkey’s Stemmer. Meanwhile, we can observe, in the same table, that the LSA model could not reveal the similarity between ”-.وأو” and ” د $أو”, ” 0 $ ” and ” 12( ” (Cosine, Jaccard and Pearson measures < 0; Euclidean distance> 0).\n5.2.1. b. Test using Different Words\nIn the Tab.4 we presented the obtained results using the LSA model and Larkey’s Stemmer applied to Different Words, in this table we remark that the used similarity measures performs worst for the pairs like : ( و ا, 34 ا) , which proves that those Arabic words are not similar. But for ( . 3 , 5 . ا) the LSA model affects negatively the similarity between this words, we obtained a higher Cosine, Jaccard and Pearson measures and a worst Euclidean distances. 5.2.2 Results with Khoja’s Stemmer\n5.2.2. a. Test using Similar Words\nThe Tab.5 presents the obtained results using the four similarity measures applied to Similar Words with LSA Model using Khoja’s Stemmer. In this case, we can observe that the results show a higher Cosine, Jaccard and Pearson measures, and a worst Euclidean distance. That prove the similarity between the pairs of words like: ( 6, 6أ) and ( ,ا ا, #,ا ا). On the other hand, in the same table, the LSA Model performs worst for the pairs: (%ر ' ()او, *+$ر) and (ع 83 , 5 او) that affects negatively the results obtained in Tab.3.\n5.2.2. b. Test using Different Words\nIn the Tab.6, the higher Cosine, Jaccard and Pearson measures and, in the other side, the worst Euclidean distance proves that the obtained results for the pairs of Arabic words like: (ةر : ا, : ا) are not desirable. In the same table, the bad Cosine, Jaccard and Pearson measures show good results for the words those are not similar like: ( و ا, ) ; ا).\nThe above results show that the obtained results using Larkey’s Stemmer outperformed those obtained using Khoja’s Stemmer, because this latter affects negatively the obtained results with LSA Model [15], when we try to measure the similarity between two different Arabic words that have the same root like: (ةر : ا, : ا), or different root like: (%ر ' ()او, *+$ر), (ع 83 , 5 او). This is mainly due to the aggressiveness of the Stemming in the sense that it reduces words to their 3-letters roots.\nThis affects the semantics as several words with different meanings might have the same root (we can obtain a family of words that can be generated from the same semantic concept from a single root with different patterns).\nThe Larkey’s Stemmer doesn’t produce the root and therefore doesn't affect the semantics of words; but it maps several words, which have the same meaning to a common syntactical form, our observation broadly agreed with [16].\nThough, for the all tables we can see that the variety of the corpus in the experiment n°1 gives more accurate results in both cases (Similar Words or Different Words)."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "In this paper, we proposed to compare and contrast the effect of two preprocessing approaches: Khoja’s Stemmer (Root-based), and Larkey’s Stemmer (Stem-based) for measuring the similarity between Arabic words using Latent Semantic Analysis (LSA). We experimented with different schemes of similarity measures. The Root-based approach finds the three-letter roots for Arabic words without depending on any root or pattern files. The Light Stemming approach removes the common suffixes and prefixes from the words. The obtained results yield three conclusions:\n1. The Larkey’s Stemmer outperforms the Khoja’s Stemmer because this later affects the words meanings.\n2. Jaccard measure performs bad relatively to the other measures.\n3. Cosine and Pearson Correlation measures, and the Euclidean Distance are quite similar for measuring the similarity between the Arabic words.\nWe believe that the comparative study presented in this paper will be very important; it may have a double advantage. First, it may be used generally to support the research in the field any Arabic Text Mining applications. Second, it will be used precisely to support and guides our research group to develop correctly our future works."
    } ],
    "references" : [ {
      "title" : "Clustering by means of Unsupervised Decision Trees or Hierarchical and K-Means-like Algorithm",
      "author" : [ "P. Bellot", "M. El-Bèze" ],
      "venue" : "Proc. of RIAO",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2000
    }, {
      "title" : "Projections for Efficient Document Clustering",
      "author" : [ "H.Schutze", "C. Silverstein" ],
      "venue" : "In Proceedings of SIGIR’97,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of the Acquisition, Induction, and Representation of Knowledge",
      "author" : [ "S.T. LANDAUER" ],
      "venue" : "DUMAIS",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Improving Stemming for Arabic Information Retrieval: Light Stemming and Co-occurrence Analysis",
      "author" : [ "L.S.Larkey", "L.Ballesteros", "M.E.Connell" ],
      "venue" : "The 25th Annual International Conference on Research and Development in Information Retrieval (SIGIR",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Neto.”Modern Information Retrieval",
      "author" : [ "R.B. Yates" ],
      "venue" : "ADDISON-WESLEY, New York,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Fast and Effective Text Mining using Linear-time Document Clustering",
      "author" : [ "B. Larsen", "C. Aone" ],
      "venue" : "In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1999
    }, {
      "title" : "Stemming for Arabic Words Similarity Measures based on Latent Semantic Analysis Model. The 3rd International Conference on Multimedia Computing and Systems (ICMCS'12), May 10-12",
      "author" : [ "H.Froud", "A.Lachkar", "S.Ouatik" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Stemming Versus Light Stemming as Feature Selection Techniques for Arabic Text Categorization",
      "author" : [ "R. Duwairi", "M. Al-Refai", "N. Khasawneh" ],
      "venue" : "Innovations in Information Technology,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "A new algorithm to generate Arabic root-pattern forms",
      "author" : [ "Al-Fedaghi S", "F. Al-Anzi" ],
      "venue" : "In proceedings of the 11th national Computer Conference and Exhibition",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1989
    }, {
      "title" : "A computational morphology system for Arabic",
      "author" : [ "Al-Shalabi R", "M. Evens" ],
      "venue" : "In Workshop on Computational Approaches to Semitic Languages,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "On Arabic search: improving the retrieval effectiveness via a light temming approach",
      "author" : [ "Aljlayl M", "O. Frieder" ],
      "venue" : "In ACM CIKM 2002 International Conference on Information and Knowledge Management,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "Arabic information retrieval at UMass in TREC-10",
      "author" : [ "Larkey L", "M.E. Connell" ],
      "venue" : "Proceedings of TREC",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "Building an Arabic Stemmer for Information Retrieval",
      "author" : [ "Chen A", "F. Gey" ],
      "venue" : "In Proceedings of the 11th Text Retrieval Conference (TREC",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "The Qur'An: An Introduction",
      "author" : [ "Abu-Hamdiyyah", "Mohammad" ],
      "venue" : "Authors Miss. Hanane Froud Phd Student in Laboratory of Information Science and Systems, ECOLE NATIONALE DES SCIENCES APPLIQUÉES,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Generally, Arabic Text Mining applications usually represent documents as ‘Bags-of-Words’ or Vector Space Model (VSM) [1][2][3], in which text documents are represented as a set of points in a high dimensional vector space.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "In the other hand, the Arabic Documents Representation may also be impacting by the use of different text pre-processing approaches, which affect any Text Mining tasks as we have already concluded in our previous works [7][15].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 6,
      "context" : "The main goal of this paper is to compare and contrast the effect of two preprocessing techniques, that affect the document's semantics, applied to Arabic corpus: Root-based (Stemming), and Stem-based (Light Stemming) approaches for measuring the semantic between Arabic words with the well known abstractive model -Latent Semantic Analysis (LSA)- with different distance functions and similarity measures [15], to overcome the above problems for Arabic Documents Representation.",
      "startOffset" : 406,
      "endOffset" : 410
    }, {
      "referenceID" : 1,
      "context" : "We used SVD technique to reduce the dimensionality of the vector space [4] [5], and to build the word representative matrix.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Latent Semantic Analysis (LSA) model is the automatic procedure proposed by [9] to construct a vector space.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "Modern Standard Arabic has a rich morphology, based on consonantal roots, which depends on vowel changes and in some cases consonantal insertions and deletions to create inflections and derivations which make morphological analysis a very complex task [22].",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 3,
      "context" : "Root-based versus Stem-based approaches Arabic stemming algorithms can be classified, according to the desired level of analysis, as root-based approach (Khoja [10]); stem-based approach (Larkey [11]).",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "Al-Fedaghi and Al-Anzi algorithm tries to find the root of the word by matching the word with all possible patterns with all possible affixes attached to it [17].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 9,
      "context" : "AlShalabi morphology system uses different algorithms to find the roots and patterns [18].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "Light stemmer is mentioned by some authors [19,20,11,21], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.",
      "startOffset" : 43,
      "endOffset" : 56
    }, {
      "referenceID" : 11,
      "context" : "Light stemmer is mentioned by some authors [19,20,11,21], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.",
      "startOffset" : 43,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "Light stemmer is mentioned by some authors [19,20,11,21], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.",
      "startOffset" : 43,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "Light stemmer is mentioned by some authors [19,20,11,21], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.",
      "startOffset" : 43,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "In order to test the effect of the two stemming approaches above on the similarity measures with the LSA model, we selected tow famous Stemming algorithms: the Morphological Analyzer from Khoja and Garside [10], and the Light Stemmer developed by Larkey [11].",
      "startOffset" : 254,
      "endOffset" : 258
    }, {
      "referenceID" : 4,
      "context" : "Cosine Similarity Cosine similarity is one of the most popular similarity measure applied to text documents, such as in numerous information retrieval applications [13] and clustering too [14].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "Cosine Similarity Cosine similarity is one of the most popular similarity measure applied to text documents, such as in numerous information retrieval applications [13] and clustering too [14].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 3,
      "context" : "EXPERIMENTS RESULTS AND DISCUSSION Experiments are applied by using two Arabic testing datasets and by applying two schemes of stemming (Figure 4) : the Larkey's Stemmer developed by [11], and the Khoja's Stemmer [10].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 6,
      "context" : "The above results show that the obtained results using Larkey’s Stemmer outperformed those obtained using Khoja’s Stemmer, because this latter affects negatively the obtained results with LSA Model [15], when we try to measure the similarity between two different Arabic words that have the same root like: (ةر : ا, : ا), or different root like: (%ر ' ()او, *+$ر), (ع 83 , 5 او).",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 7,
      "context" : "The Larkey’s Stemmer doesn’t produce the root and therefore doesn't affect the semantics of words; but it maps several words, which have the same meaning to a common syntactical form, our observation broadly agreed with [16].",
      "startOffset" : 220,
      "endOffset" : 224
    } ],
    "year" : 2012,
    "abstractText" : "Representation of semantic information contained in the words is needed for any Arabic Text Mining applications. More precisely, the purpose is to better take into account the semantic dependencies between words expressed by the co-occurrence frequencies of these words. There have been many proposals to compute similarities between words based on their distributions in contexts. In this paper, we compare and contrast the effect of two preprocessing techniques applied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming) approaches for measuring the similarity between Arabic words with the well known abstractive model -Latent Semantic Analysis (LSA)with a wide variety of distance functions and similarity measures, such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient. The obtained results show that, on the one hand, the variety of the corpus produces more accurate results; on the other hand, the Stem-based approach outperformed the Root-based one because this latter affects the words meanings.",
    "creator" : "PDFCreator Version 1.2.3"
  }
}