{
  "name" : "1705.03455.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improving Frame Semantic Parsing with Hierarchical Dialogue Encoders",
    "authors" : [ "Ankur Bapna", "Gokhan Túr", "Dilek Hakkani-Túr", "Larry Heck" ],
    "emails" : [ "ankurbpn@google.com", "gokhant@google.com", "dilekh@google.com", "larryheck@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Goal oriented dialogue systems help users with accomplishing tasks, like making restaurant reservations or booking flights, by interacting with them in natural language. The capability to understand user utterances and break them down into task specific semantics is a key requirement for these systems. This is accomplished in the spoken language understanding module, which typically parses user utterances into semantic frames, composed of domains, intents and slots (Tur and De Mori, 2011), that can then be processed by downstream dia-\nlogue system components. An example semantic frame is shown for a restaurant reservation related query in Figure 1. As the complexity of the task supported by a dialogue system increases, there is a need for an increased back and forth interaction between the user and the agent. For example, a restaurant reservation task might require the user to specify a restaurant name, date, time and number of people required for the reservation. Additionally, based on reservation availability, the user might need to negotiate on date, time, or any other attribute with the agent. This puts the burden of parsing in-dialogue contextual user utterances on the language understanding module. The complexity increases further when the system supports more than one task and the user is allowed to have goals spanning multiple domains within the same dialogue. Without the conversation context, natural language utterances could be ambiguous, and the previous user and system turns could help resolve these ambiguities.\nIn this paper, we explore approaches to improve dialogue context modeling within a Recurrent Neural Network (RNN) based spoken language understanding system. We propose a novel model architecture to improve dialogue context modeling for spoken language understanding on a multi-domain dialogue dataset. The proposed architecture is an extension of Hierarchical Recur-\nar X\niv :1\n70 5.\n03 45\n5v 1\n[ cs\n.C L\n] 8\nM ay\n2 01\n7\nrent Encoder Decoders (HRED) (Sordoni et al., 2015), where we combine the query level encodings with a representation of the current utterance, before feeding it into the session level encoder. We compare the performance of this model to a RNN tagger injected with just the previous turn context and a single hop memory network that uses an attention weighted combination of the dialogue context (Chen et al., 2016; Weston et al., 2014). Furthermore, we describe a dialogue recombination technique to enhance the complexity of the training dataset by injecting synthetic domain switches, to create a better match with the mixed domain dialogues in the test dataset. This is, in principle, a multi-turn extension of (Jia and Liang, 2016). Instead of inducing and composing grammars to synthetically enhance single turn text, we combine single domain dialogue sessions into multi-domain dialogues to provide richer context during training."
    }, {
      "heading" : "2 Related Work",
      "text" : "The task of understanding a user utterance is typically broken down into 3 tasks: domain classification, intent classification and slot-filling (Tur and De Mori, 2011). Most modern approaches to conversational language understanding involve training machine learning models on labeled training data (Young, 2002; Hahn et al., 2011; Wang et al., 2005, among others). More recently, recurrent neural network (RNN) based approaches have been shown to perform exceedingly well on spoken language understanding tasks (Mesnil et al., 2015; Hakkani-Tür et al., 2016; Kurata et al., 2016, among others).\nIn parallel, joint modeling of tasks and addition of contextual signals has been shown to result in performance gains for several applications. Modeling domain, intent and slots in a joint RNN model was shown to result in reduction of overall frame error rates (Hakkani-Tür et al., 2016). Joint modeling of intent classification and language modeling showed promising improvements in intent recognition, especially in the presence of noisy speech recognition (Liu and Lane, 2016). Similarly, models incorporating more context from dialogue history (Chen et al., 2016) or semantic context from the frame (Dauphin et al., 2014) have outperformed models without context and shown potential for greater generalization for spoken language understanding and related tasks. (Dhingra et al., 2016) show improved performance on an informational dialogue agent by incorporating knowledge base context into their dialogue system. Using dialogue context was shown to boost performance for end to end dialogue (Bordes and Weston, 2016) and next utterance prediction (Serban et al., 2015). In the next few sections, we describe the proposed model architecture, the dataset and our dialogue recombination approach. This is followed by experimental results and analysis."
    }, {
      "heading" : "3 Model Architecture",
      "text" : "We compare the performance of 3 model architectures for encoding dialogue context on a multidomain dialogue dataset. Let the dialogue be a sequence of system and user utterances Dt = {u1, u2...ut} and at time step t we are trying to output the parse of a user utterance ut, given Dt.\nLet any utterance uk be a sequence of tokens given by {xk1, xk2...xknk}. We divide the model into 2 components, the context encoder that acts on Dt to produce a vector representation of the dialogue context denoted by ht = H(Dt), and the tagger, which takes the dialogue context encoding ht, and the current utterance ut as input and produces the domain, intent and slot annotations as output."
    }, {
      "heading" : "3.1 Context Encoder Architectures",
      "text" : "In this section we describe the architectures of the context encoders used for our experiments. We compare the performance of 3 different architectures that encode varying levels of dialogue context."
    }, {
      "heading" : "3.1.1 Previous Utterance Encoder",
      "text" : "This is the baseline context encoder architecture. We feed the embeddings corresponding to tokens in the previous system utterance, ut−1 = {xt−11 , x t−1 2 ...x t−1 nt−1 }, into a single Bidirectional RNN (BiRNN) layer with Gated Recurrent Unit (GRU) (Chung et al., 2014) cells and 128 dimensions (64 in each direction). The embeddings are shared with the tagger. The final state of the context encoder GRU is used as the dialogue context.\nht = BiGRUc(ut−1) (1)"
    }, {
      "heading" : "3.1.2 Memory Network",
      "text" : "This architecture is identical to the approach described in (Chen et al., 2016). We encode all dialogue context utterances, {u1, u2...ut−1}, into memory vectors denoted by {m1,m2, ...mt−1} using a Bidirectional GRU (BiGRU) encoder with 128 dimensions (64 in each direction). To add temporal context to the dialogue history utter-\nances, we append special positional tokens to each utterance.\nmk = BiGRUm(uk) for 0 ≤ k ≤ t−1 (2)\nWe also encode the current utterance with another BiGRU encoder with 128 dimensions (64 in each direction), into a context vector denoted by c, as in equation 3. This is conceptually depicted in Figure 2\nc = BiGRUc(ut) (3)\nLet M be a matrix with the ith row given by mi. We obtain the cosine similarity between each memory vector, mi, and the context vector c. The softmax of this similarity is used as an attention distribution over the memory M , and an attention weighted sum of M is used to produce the dialogue context vector ht (Equation 4). This is conceptually depicted in Figure 3.\na = softmax(Mc)\nht = a TM\n(4)"
    }, {
      "heading" : "3.1.3 Hierarchical Dialogue Encoder Network",
      "text" : "We enhance the memory network architecture described above by adding a session encoder (Sordoni et al., 2015) that temporally combines a joint representation of the current utterance encoding, c, (Eq. 3) and the memory vectors, {m1,m2...mt−1}, (Eq. 2). We combine the context vector c with each memory vector mk, for 1 ≤ k ≤ nk, by concatenating and passing them through a feed forward layer (FF) to produce 128 dimensional context encodings, denoted by {g1, g2...gt−1} (Eq. 5).\ngk = sigmoid(FF (mk, c)) for 0 ≤ k ≤ t−1 (5)\nFigure 4: Architecture of the Hierarchical Dialogue Encoder Network. The feed-forward networks share weights across all memories.\nThese context encodings are fed as token level inputs into the session encoder, which is a 128 dimensional BiGRU layer. The final state of the session encoder represents the dialogue context encoding ht (Eq. 6).\nht = BiGRUs({g1, g2, ...gt−1}) (6)\nThe architecture is depicted in Figure 4."
    }, {
      "heading" : "3.2 Tagger Architecture",
      "text" : "For all our experiments we use a stacked BiRNN tagger to jointly model domain classification, intent classification and slot-filling, similar to the approach described in (Hakkani-Tür et al., 2016). We feed learned 256 dimensional embeddings corresponding to the current utterance tokens into the tagger. The first RNN layer uses GRU cells with 256 dimensions (128 in each direction) as in equation 7. The token embeddings are fed into the token level inputs of the first RNN layer to produce the token level outputs o1 = {o11, o12...o1nt}.\no1 = BiGRU1(ut) (7)\nThe second layer uses Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) cells with 256 dimensions (128 in both dimensions). We apply dropout to the outputs of both layers. The initial states of both forward and backward LSTMs of the second tagger layer are initialized with the dialogue encoding ht as in equation 8. The token level outputs of the first RNN layer,\no1, are fed as input into the second RNN layer to produce token level outputs o2 = {o21, o22...o2nt} and the final state s2.\no2, s2 = BiLSTM2(o 1, ht) (8)\nThe final state of the second layer, s2, is used as input to classification layers for domain and intent classification.\npdomain = softmax(Us2)\npintent = sigmoid(V s2) (9)\nThe token level outputs of the second layer, o2, are used as input to a softmax layer that outputs the IOB slot labels. This results in a softmax layer with 2N+1 dimensions for a domain with N slots.\npsloti = softmax(So 2 i ) for 0 ≤ i ≤ nt\n(10) The architecture is depicted in Figure 5."
    }, {
      "heading" : "4 Dataset",
      "text" : "We crowd sourced multi-turn dialogue sessions for 3 tasks: buying movie tickets, searching for a restaurant and reserving tables at a restaurant. Our data collection process comprises of two steps: (i) Generating user-agent interactions comprising of dialog acts and slots based on the interplay of a simulated user and a rule based dialogue policy. (ii) Using a crowd sourcing platform to elicit natural language utterances that align with the semantics of the generated interactions. The goal of the conversational language understanding module of our dialogue system is to map each user utterance into frame based semantics that can be processed by the downstream components. Tables describing the intents and slots present in the dataset can be found in the appendix. We use a stochastic agenda-based user simulator (Schatzmann et al., 2007; Shah et al., 2016) for interplay with our rule based system policy. The user goal is specified in terms of a tuple of slots, which denote the user constraints. Some constraints might be unspecified, in which case the user is indifferent to the value of those slots. At any given turn, the simulator samples a user action from a set of acceptable actions based on (i) the user goal and agenda that includes slots that still need to be specified, (ii) a randomly chosen user\nprofile (co-operative/aggressive, verbose/succinct etc.) and (iii) the previous user and system actions. These generated interactions were then translated to their natural language counterparts and sent out to crowd-workers for paraphrasing. These paraphrased dialogues were automatically annotated with domains, intents and slots where possible, and the remaining dialogues were annotated by an expert annotator. We collected 1319 dialogues for restaurant reservation, 976 dialogues searching for restaurants and 1048 dialogues for buying movie tickets. All of these datasets were used for training. The simulator and policy were also extended to handle multiple goals spanning different domains. In this setup, the user goal for the simulator would include multiple tasks and slot values could be conditioned on the previous task, for example, the simulator would ask for booking a table ”after the movie”, or search for a restaurant ”near the theater”. The set of slots supported by the simulator is enumerated in Table 1. This approach was used to collect 467 dialogues for training, 50 for validation and\n273 for the test set. A sample dialogue from the multi-domain setup can be found in the appendix."
    }, {
      "heading" : "5 Dialogue Recombination",
      "text" : "As described in the previous section, we train our models on a large set of single domain dialogue datasets and a small set of multi-domain dialogues. These models are then evaluated on a test set composed of multi-domain dialogues, where the user attempts to fulfill multiple goals spanning several domains. This results in a data shift that might result in performance degradation. To counter this shift in the data distribution we device a dialogue recombination scheme to generate multi-domain dialogues from single domain training datasets. The key idea behind the recombination approach is the conditional independence of sub-dialogues aimed at performing distinct tasks. We exploit the presence of task intents, or intents that denote a switch in the primary task the user is trying to perform, since they are a strong indicator of a switch in the focus of the dialogue. We exploit\nthe independence of the sub-dialogue following these intents from the previous dialogue context, to generate synthetic dialogues with multi-domain context. The recombination process is described as follows: Let a dialogue d be defined as a sequence of turns and corresponding semantic labels (domain, intent and slot annotations) {(td1, fd1), (td2, fd2), ...(tdnd , fdnd}. To obtain a re-combined dataset composed of dialogues from dataset dataset1 and dataset2, we repeat the following steps 10000 times, for each combination of (dataset1, dataset2) from the three single domain datasets.\n• Sample dialogues x and y from dataset1 and dataset2 respectively.\n• Find the first user utterance labeled with a task intent in y. Let this be turn l.\n• Randomly sample an insertion point in dialogue x. Let this be turn k.\n• The new recombined dialogue is {(tx1, fx1), ...(txk, fxk), (tyl, fyl), ...(tyny , fyny)}.\nA sample dialogue generated using the above procedure is described in table 2."
    }, {
      "heading" : "6 Experiments",
      "text" : "We compare the domain classification, intent classification and slot-filling performances, and the overall frame error rates of the encoder-decoder, memory network and hierarchical dialogue encoder network on the dataset described above. We trained all 3 models with RMSProp for 100000 training steps with a batch size of 100. We started with a learning rate of 0.0003 which was decayed by a factor of 0.95 every 3000 steps. Gradient norms were clipped if they exceed a magnitude of 2.5. We restrict the model vocabularies to contain only tokens occurring more than 10 times in the training set, to prevent over-fitting to training set entities. Digits were replaced with a special ”#” token to allow better generalization to unseen numbers. The dialogue history was padded to 40 utterances for batch processing. We report results with and without the recombined dataset in Table 3."
    }, {
      "heading" : "7 Results",
      "text" : "The encoder decoder model trained on just the previous turn context performs worst on almost all metrics, irrespective of the presence of recombined data. This can be explained by worse performance on in-dialogue utterances, where just the previous turn context isn’t sufficient to accurately identify the domain, and in several cases, the in-\ntents and slots of the utterance. The memory network is the best performing model in the absence of recombined data, indicating that the model is able to encode additional context effectively to improve performance on all tasks, even when only a small amount of multi-domain data is available. The hierarchical dialogue encoder network performs slightly worse than the memory network in the absence of recombined data. This could be explained by the model over-fitting to the single domain context seen during training and failure to utilize context effectively in a multi-domain setting. In the presence of recombined dialogues it outperforms all other implementations. Apart from increasing the noise in the dialogue context, adding recombined dialogues to the training set increases the average turn length of the training data, bringing it closer to that of the test dialogues. We hypothesize that the presence of\nsynthetic context has a regularization-like effect on the models. Similar effects were observed by (Jia and Liang, 2016), where training with longer synthetic utterances resulted in improved performance on a simpler test set. This is also supported by the observation that performance improvements obtained by addition of recombined data increase as the complexity of the model increases."
    }, {
      "heading" : "8 Discussion and Conclusions",
      "text" : "Table 4 demonstrates an example dialogue from the test set, along with the gold and model annotations from all 3 models. We observe that Encoder Decoder (ED) and Hierarchical Dialogue Encoder Network (HDEN) are able to successfully identify the domain, intent and slots, while the Memory Network (MN) fails to identify the movie name. Looking at the attention distributions, we notice that the MN attention is very diffused, whereas\nHDEN is focusing on the most recent last 2 utterances, which directly identify the domain and the presence of the movie slot in the final user utterance. ED is also able to identify the presence of a movie in the final user utterance from the previous utterance context. Table 5 displays another example where the HDEN model outperforms both MN and ED. Constrained to just the previous utterance ED is unable to correctly identify the domain of the user utterance. The MN model correctly identifies the domain, using its strong focus on the task-intent bearing utterance, but it is unable to identify the presence of a restaurant in the user utterance. This highlights its failure to combine context from multiple history utterances. On the other hand, as indicated by its attention distribution on the final two utterances, HDEN is able to successfully combine context from the dialogue to correctly identify the domain and the restaurant name from the user utterance, despite the presence of several outof-vocabulary tokens. The above two examples hint that HDEN performs\nbetter in scenarios where multiple history utterances encode complementary information that could be useful to interpret user utterances. This is usually the case in more natural goal oriented dialogues, where several tasks and sub tasks go in and out of the focus of the conversation (Grosz, 1979). On the other hand, we also observed that HDEN performs significantly worse in the absence of recombined data. Due to its complex architecture and a much larger set of parameters HDEN is prone to over-fitting in low data scenarios. In this paper we propose a novel neural network architecture to encode context from previous utterances in their chronological order for conversational language understanding, and demonstrate that the proposed architecture results in reduction in overall frame error rate on a multi-domain human machine conversation dataset. We also introduce a data augmentation scheme, to generate longer dialogues with richer context, and empirically demonstrate that it results in performance improvement for multiple model architectures."
    } ],
    "references" : [ {
      "title" : "Learning end-to-end goal-oriented dialog",
      "author" : [ "Antoine Bordes", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1605.07683 .",
      "citeRegEx" : "Bordes and Weston.,? 2016",
      "shortCiteRegEx" : "Bordes and Weston.",
      "year" : 2016
    }, {
      "title" : "End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding",
      "author" : [ "Y.-N. Chen", "D. Hakkani-Tür", "G. Tur", "J. Gao", "L. Deng." ],
      "venue" : "Proceedings of the Interspeech. San Francisco, CA.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555 .",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Zero-shot learning and clustering for semantic utterance classification",
      "author" : [ "Y. Dauphin", "G. Tur", "D. Hakkani-Tür", "L. Heck." ],
      "venue" : "Proceedings of the ICLR.",
      "citeRegEx" : "Dauphin et al\\.,? 2014",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end reinforcement learning of dialogue agents for information access",
      "author" : [ "Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng." ],
      "venue" : "arXiv preprint arXiv:1609.00777 .",
      "citeRegEx" : "Dhingra et al\\.,? 2016",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2016
    }, {
      "title" : "Focusing and description in natural language dialogues",
      "author" : [ "Barbara J Grosz." ],
      "venue" : "Technical report, DTIC Document.",
      "citeRegEx" : "Grosz.,? 1979",
      "shortCiteRegEx" : "Grosz.",
      "year" : 1979
    }, {
      "title" : "Comparing stochastic approaches to spoken language understanding in multiple languages",
      "author" : [ "S. Hahn", "M. Dinarelli", "C. Raymond", "F. Lefevre", "P. Lehnen", "R. De Mori", "A. Moschitti", "H. Ney", "G. Riccardi." ],
      "venue" : "IEEE Transactions on Audio, Speech,",
      "citeRegEx" : "Hahn et al\\.,? 2011",
      "shortCiteRegEx" : "Hahn et al\\.",
      "year" : 2011
    }, {
      "title" : "Multidomain joint semantic frame parsing using bidirectional RNN-LSTM",
      "author" : [ "D. Hakkani-Tür", "G. Tur", "A. Celikyilmaz", "Y.-N. Chen", "J. Gao", "L. Deng", "Y.-Y. Wang." ],
      "venue" : "Proceedings of the Interspeech. San Francisco, CA.",
      "citeRegEx" : "Hakkani.Tür et al\\.,? 2016",
      "shortCiteRegEx" : "Hakkani.Tür et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Data recombination for neural semantic parsing",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1606.03622 .",
      "citeRegEx" : "Jia and Liang.,? 2016",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2016
    }, {
      "title" : "Leveraging sentence-level information with encoder LSTM for semantic slot filling",
      "author" : [ "G. Kurata", "B. Xiang", "B. Zhou", "M. Yu." ],
      "venue" : "Proceedings of the EMNLP. Austin, TX.",
      "citeRegEx" : "Kurata et al\\.,? 2016",
      "shortCiteRegEx" : "Kurata et al\\.",
      "year" : 2016
    }, {
      "title" : "Joint online spoken language understanding and language modeling with recurrent neural networks",
      "author" : [ "Bing Liu", "Ian Lane." ],
      "venue" : "CoRR abs/1609.01462. http://arxiv.org/abs/1609.01462.",
      "citeRegEx" : "Liu and Lane.,? 2016",
      "shortCiteRegEx" : "Liu and Lane.",
      "year" : 2016
    }, {
      "title" : "Agenda-based user simulation for bootstrapping a pomdp dialogue system",
      "author" : [ "Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve Young." ],
      "venue" : "Human Language Technologies 2007: The Conference of the North American Chapter of the",
      "citeRegEx" : "Schatzmann et al\\.,? 2007",
      "shortCiteRegEx" : "Schatzmann et al\\.",
      "year" : 2007
    }, {
      "title" : "Hierarchical neural network generative models for movie dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau." ],
      "venue" : "CoRR abs/1507.04808. http://arxiv.org/abs/1507.04808.",
      "citeRegEx" : "Serban et al\\.,? 2015",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2015
    }, {
      "title" : "Interactive reinforcement learning for taskoriented dialogue management",
      "author" : [ "Pararth Shah", "Dilek Hakkani-Tür", "Larry Heck" ],
      "venue" : null,
      "citeRegEx" : "Shah et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion",
      "author" : [ "Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "Jian-Yun Nie." ],
      "venue" : "Proceedings of the 24th",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Spoken language understanding: Systems for extracting semantic information from speech",
      "author" : [ "Gokhan Tur", "Renato De Mori." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Tur and Mori.,? 2011",
      "shortCiteRegEx" : "Tur and Mori.",
      "year" : 2011
    }, {
      "title" : "Spoken language understanding - an introduction to the statistical framework",
      "author" : [ "Y.-Y. Wang", "L. Deng", "A. Acero." ],
      "venue" : "IEEE Signal Processing Magazine 22(5):16–31.",
      "citeRegEx" : "Wang et al\\.,? 2005",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2005
    }, {
      "title" : "Memory networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes." ],
      "venue" : "arXiv preprint arXiv:1410.3916 .",
      "citeRegEx" : "Weston et al\\.,? 2014",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2014
    }, {
      "title" : "Talking to machines (statistically speaking)",
      "author" : [ "S. Young." ],
      "venue" : "Proceedings of the ICSLP. Denver, CO.",
      "citeRegEx" : "Young.,? 2002",
      "shortCiteRegEx" : "Young.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "rent Encoder Decoders (HRED) (Sordoni et al., 2015), where we combine the query level encodings with a representation of the current utterance, before feeding it into the session level encoder.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "We compare the performance of this model to a RNN tagger injected with just the previous turn context and a single hop memory network that uses an attention weighted combination of the dialogue context (Chen et al., 2016; Weston et al., 2014).",
      "startOffset" : 202,
      "endOffset" : 242
    }, {
      "referenceID" : 18,
      "context" : "We compare the performance of this model to a RNN tagger injected with just the previous turn context and a single hop memory network that uses an attention weighted combination of the dialogue context (Chen et al., 2016; Weston et al., 2014).",
      "startOffset" : 202,
      "endOffset" : 242
    }, {
      "referenceID" : 9,
      "context" : "This is, in principle, a multi-turn extension of (Jia and Liang, 2016).",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "Modeling domain, intent and slots in a joint RNN model was shown to result in reduction of overall frame error rates (Hakkani-Tür et al., 2016).",
      "startOffset" : 117,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "Joint modeling of intent classification and language modeling showed promising improvements in intent recognition, especially in the presence of noisy speech recognition (Liu and Lane, 2016).",
      "startOffset" : 170,
      "endOffset" : 190
    }, {
      "referenceID" : 1,
      "context" : "Similarly, models incorporating more context from dialogue history (Chen et al., 2016) or semantic context from the frame (Dauphin et al.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : ", 2016) or semantic context from the frame (Dauphin et al., 2014) have outperformed models without context and shown potential for greater generalization for spoken language understanding and related tasks.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "(Dhingra et al., 2016) show improved performance on an informational dialogue agent by incorporating knowledge base context into their dialogue system.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Using dialogue context was shown to boost performance for end to end dialogue (Bordes and Weston, 2016) and next utterance prediction (Serban et al.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "Using dialogue context was shown to boost performance for end to end dialogue (Bordes and Weston, 2016) and next utterance prediction (Serban et al., 2015).",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "x t−1 nt−1 }, into a single Bidirectional RNN (BiRNN) layer with Gated Recurrent Unit (GRU) (Chung et al., 2014) cells and 128 dimensions (64 in each direction).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "This architecture is identical to the approach described in (Chen et al., 2016).",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "We enhance the memory network architecture described above by adding a session encoder (Sordoni et al., 2015) that temporally combines a joint representation of the current utterance encoding, c, (Eq.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "For all our experiments we use a stacked BiRNN tagger to jointly model domain classification, intent classification and slot-filling, similar to the approach described in (Hakkani-Tür et al., 2016).",
      "startOffset" : 171,
      "endOffset" : 197
    }, {
      "referenceID" : 8,
      "context" : "The second layer uses Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) cells with 256 dimensions (128 in both dimensions).",
      "startOffset" : 52,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "We use a stochastic agenda-based user simulator (Schatzmann et al., 2007; Shah et al., 2016) for interplay with our rule based system policy.",
      "startOffset" : 48,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "We use a stochastic agenda-based user simulator (Schatzmann et al., 2007; Shah et al., 2016) for interplay with our rule based system policy.",
      "startOffset" : 48,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "Similar effects were observed by (Jia and Liang, 2016), where training with longer synthetic utterances resulted in improved performance on a simpler test set.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "This is usually the case in more natural goal oriented dialogues, where several tasks and sub tasks go in and out of the focus of the conversation (Grosz, 1979).",
      "startOffset" : 147,
      "endOffset" : 160
    } ],
    "year" : 2017,
    "abstractText" : "Conversational Language Understanding (CLU) is a key component of goal oriented dialogue systems that would parse user utterances into semantic frame representations. Traditionally CLU does not utilize the dialogue history beyond the previous system turn and contextual ambiguities are resolved by the downstream components. In this paper, we explore novel approaches for modeling dialogue context in a recurrent neural network (RNN) based language understanding system. We propose the Hierarchical Dialogue Encoder Network, that allows encoding context from the dialogue history in chronological order. We compare the performance of our proposed architecture with two context models, one that uses just the previous turn context and another that encodes dialogue context in a memory network, but loses the order of utterances in the dialogue history. Experiments with a multi-domain dialogue dataset demonstrate that the proposed architecture results in reduced semantic frame error rates.",
    "creator" : "LaTeX with hyperref package"
  }
}