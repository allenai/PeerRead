{
  "name" : "1405.2875.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems∗",
    "authors" : [ "Chien-Ju Ho", "Aleksandrs Slivkins", "Jennifer Wortman Vaughan" ],
    "emails" : [ "cjho@ucla.edu.", "slivkins@microsoft.com.", "jenn@microsoft.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n28 75\nv2 [\ncs .D\nS] 2\nS ep\nWe treat this problem as a multi-armed bandit problem, with each “arm” representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature).\nOur results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.\nACM Categories and subject descriptors: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems; F.1.2 [Computation by Abstract Devices]: Modes of Computation—Online computation; J.4 [Social and Behavioral Sciences]: Economics\nKeywords: crowdsourcing; principal-agent; dynamic pricing; multi-armed bandits; regret.\n∗Preliminary version of this paper has been published in the ACM Conference on Economics and Computation (ACM-EC), 2014. The conference version omits many of the proofs, including all of Section 8, the details of the simulation results, and a detailed discussion of related work (Section 9). Moreover, the present version contains revised and expanded Conclusions section, and an updated discussion of the follow-up work.\nCompared to the initial technical report (arXiv:1405.2875v1, May 2014), this version includes updated citations and discussion of the follow-up work.\nMuch of this research was completed while Ho was an intern at Microsoft Research. This research was partially supported by the NSF under grant IIS-1054911. Any opinions, findings, conclusions, or recommendations are those of the authors alone.\n†UCLA, Los Angeles, CA, USA. Email: cjho@ucla.edu. ‡Microsoft Research, New York, NY, USA. Email: slivkins@microsoft.com. §Microsoft Research, New York, NY, USA. Email: jenn@microsoft.com."
    }, {
      "heading" : "1 Introduction",
      "text" : "Crowdsourcing harnesses human intelligence and common sense to complete tasks that are difficult to accomplish using computers alone. Crowdsourcing markets, such as Amazon Mechanical Turk and Microsoft’s Universal Human Relevance System, are platforms designed to match available human workers with tasks to complete. Using these platforms, requesters may post tasks that they would like completed, along with the amount of money they are willing to pay. Workers then choose whether or not to accept the available tasks and complete the work.\nOf course not all human workers are equal, nor is all human-produced work. Some tasks, such as proofreading English text, are easier for some workers than others, requiring less effort to produce high quality results. Additionally, some workers are more dedicated than others, willing to spend extra time to make sure a task is completed properly. To encourage high quality results, requesters may set quality-contingent “bonus” payments on top of the base payment for each task, rewarding workers for producing valuable output. This can be viewed as offering workers a “contract” that specifies how much they will be paid based on the quality of their output.1\nWe examine the requester’s problem of dynamically setting quality-contingent payments for tasks. We consider a setting in which time evolves in rounds. In each round, the requester posts a new contract, a performance-contingent payment rule which specifies different levels of payment for different levels of output. A random, unidentifiable worker then arrives in the market and strategically decides whether to accept the requester’s task and how much effort to exert; the choice of effort level is not directly observable by the requester. After the worker completes the task (or chooses not to complete it), the requester observes the worker’s output, pays the worker according to the offered contract, and adjusts the contract for the next round. The properties of a random worker (formally: the distribution over the workers’ types) are not known to the requester, but may be learned over time. The goal of the requester is to maximize his expected utility, the value he receives from completed work minus the payments made. We call it the dynamic contract design problem.\nFor concreteness, consider a special case in which a worker can strategically choose to perform a task with low effort or with high effort, and the task may be completed either at low quality or at high quality. The low effort incurs no cost and results in low quality, which in turn brings no value to the requester. The high effort leads to high quality with some positive probability (which may vary from one worker to another, and is unknown to the requester). The requester only observes the quality of completed tasks, and therefore cannot infer the effort level. This example captures the two main tenets of our model: that the properties of a random worker are unknown to the requester and that workers’ strategic decisions are unobservable.\nWe treat the dynamic contract design problem as a multi-armed bandit (MAB) problem, with each arm representing a potential contract. Since the action space is large (potentially infinite) and has a well-defined real-valued structure, it is natural to consider an algorithm that uses discretization. Our algorithm, AgnosticZooming, divides the action space into regions, and chooses among these regions, effectively treating each region as a single “meta-arm.” The discretization is defined adaptively, so that the more promising areas of the action space are eventually discretized more finely than the less promising areas. While the general idea of adaptive discretization has\n1For some tasks, such as labeling websites as relevant to a particular search query or not, verifying the quality of work may be as difficult as completing the task. These tasks can be assigned in batches, with each batch containing one or more instances in which the correct answer is already known. Quality-contingent payments can then be based on the known instances.\nappeared in prior work on MAB [Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014, 2011], our approach to adaptive discretization is new and problem-specific. The main difficulty, compared to this prior work, is that an algorithm is not given any information that links the observable numerical structure of contracts and the expected utilities thereof.\nTo analyze performance, we propose a concept called “width dimension” which measures how “nice” a particular problem instance is. We show that AgnosticZooming achieves regret sublinear in the time horizon for problem instances with small width dimension. In particular, if the width dimension is d, it achieves regret O(log T ·T (d+1)/(d+2)) after T rounds. For problem instances with large width dimension, AgnosticZooming matches the performance of the naive algorithm which uniformly discretizes the space and runs a standard bandit algorithm. We illustrate our general results via some corollaries and special cases, including the high-low example described above. We support the theoretical results with simulations.\nFurther, we consider a special case of our setting where each worker only chooses whether to accept or reject a given task. This special case corresponds to a dynamic pricing problem previously studied in the literature. Our results significantly improve over the prior work on this problem.\nOur contributions can be summarized as follows. We define a broad, practically important setting in crowdsourcing markets; identify novel problem-specific structure, for both the algorithm and the regret bounds; distill ideas from prior work to work with these structures; argue that our approach is productive by deriving corollaries and comparing to prior work; and identify and analyze specific examples where our theory applies. The main conceptual contributions are the model itself and the adaptive discretization approach mentioned above. Finally, this paper prompts further research on dynamic contract design along several directions that we outline in the conclusion.\nRelated work. Our work builds on three areas of research. First, our model can be viewed as a multi-round version of the classical principal-agent model from contract theory [Laffont and Martimort, 2002]. A single round of our model corresponds to the basic principal-agent setting, with adverse selection (unknown worker’s type) andmoral hazard (unobservable worker’s decisions). Unlike much of the prior work in contract theory, the prior over worker types is not known to the principal, but may be learned over time. Accordingly, our techniques are very different from those employed in contract theory.\nSecond, our methods build on those developed in the rich literature on MAB with continuous outcome spaces. The closest line of work is that on Lipschitz MAB [Kleinberg et al., 2008], in which the algorithm is given a distance function on the arms, and the expected rewards of the arms are assumed to satisfy Lipschitz-continuity (or a relaxation thereof) with respect to this distance function, [Agrawal, 1995, Kleinberg, 2004, Auer et al., 2007, Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014]. Most related to our techniques is the idea of adaptive discretization [Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014], and in particular, the zooming algorithm [Kleinberg et al., 2008, Slivkins, 2014]. However, the zooming algorithm cannot be applied directly in our setting because the required numerical similarity information is not immediately available. This problem also arises in web search and advertising, where it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007] which can be used to explicitly reconstruct relevant parts of the underlying metric space [Slivkins, 2011, Bull, 2013]. We take a different approach, using a notion of “virtual width” to estimate similarity information. Explicit comparisons between our results and prior MAB work are made throughout the paper.\nFinally, our work follows several other theoretical papers on pricing in crowdsourcing markets\n[Kleinberg and Leighton, 2003b, Badanidiyuru et al., 2012, Singer and Mittal, 2013, Singla and Krause, 2013, Badanidiyuru et al., 2013]. In particular, Badanidiyuru et al. [2012] and Singla and Krause [2013] study a version of our setting with simple, single-price contracts (independent of the output), where the focus is on dealing with a global budget constraint.\nA more thorough literature review (including a discussion of some related empirical work) can be found in Section 9."
    }, {
      "heading" : "2 Our setting: the dynamic contract design problem",
      "text" : "In this section, we formally define the problem that we set out to solve. We start by describing a static model, which captures what happens in a single round of interaction between a requester and a worker. As described above, this is a version of the standard principal-agent model [Laffont and Martimort, 2002]. We then define our dynamic model, an extension of the static model to multiple rounds, with a new worker arriving each round. We then detail the objective of our pricing algorithm and the simplifying assumptions that we make throughout the paper. Finally, we compare our setting to the classic multi-armed bandit problem.\nStatic model. We begin with a description of what occurs during each interaction between the requester and a single worker. The requester first posts a task which may be completed by the worker, and a contract specifying how the worker will be paid if she completes the task. If the task is completed, the requester pays the worker as specified in the contract, and the requester derives value from the completed task; for normalization, we assume that the value derived is in [0, 1]. The requester’s utility from a given task is this value minus the payment to the worker.\nWhen the worker observes the contract and decides whether or not to complete the task, she also chooses a level of effort to exert, which in turn determines her cost (in terms of time, energy, or missed opportunities) and a distribution over the quality of her work. To model quality, we assume that there is a (small) finite set of possible outcomes that result from the worker completing the task (or choosing not to complete it), and that the realized outcome determines the value that the requester derives from the task. The realized outcome is observed by the requester, and the contract that the requester offers is a mapping from outcomes to payments for the worker.\nWe emphasize two crucial (and related) features of the principal-agent model: that the mapping from effort level to outcomes can be randomized, and that the effort level is not directly observed by the requester. This is in line with a standard observation in crowdsourcing that even honest, high-effort workers occasionally make errors.\nThe worker’s utility from a given task is the payment from the requester minus the cost corresponding to her chosen effort level. Given the contract she is offered, the worker chooses her effort level strategically so as to maximize her expected utility. Crucially, the chosen effort level is not directly observable by the requester.\nThe worker’s choice not to perform a task is modeled as a separate effort level of zero cost (called the null effort level) and a separate outcome of zero value and zero payment (called the null outcome) such that the null effort level deterministically leads to the null outcome, and it is the only effort level that can lead to this outcome.\nThe mapping from outcomes to the requester’s value is called the requester’s value function. The mapping from effort levels to costs is called the cost function, and the mapping from effort levels to distributions over outcomes is called the production function. For the purposes of this paper, a worker is completely specified by these two functions; we say that the cost function and the\nproduction function comprise the worker’s type. Unlike some traditional versions of the principalagent problem, in our setting a worker’s type is not observable by the requester, nor is any prior given.\nDynamic model. The dynamic model we consider in this paper is a natural extension of the static model to multiple rounds and multiple workers. We are still concerned with just a single requester. In each round, a new worker arrives. We assume a stochastic environment in which the worker’s type in each round is an i.i.d. sample from some fixed and unknown distribution over types, called the supply distribution. The requester posts a new task and a contract for this task. All tasks are of the same type, in the sense that the set of possible effort levels and the set of possible outcomes are the same for all tasks. The worker strategically chooses her effort level so as to maximize her expected utility from this task. Based on the chosen effort level and the worker’s production function, an outcome is realized. The requester observes this outcome (but not the worker’s effort level) and pays the worker the amount specified by the contract. The type of the arriving worker is never revealed to the requester. The requester can adjust the contract from one round to another, and his total utility is the sum of his utility over all rounds. For simplicity, we assume that the number of rounds is known in advance, though this assumption can be relaxed using standard tricks.\nThe dynamic contract design problem. Throughout this paper, we take the point of view of the requester interacting with workers in the dynamic model. The algorithms we examine dynamically choose contracts to offer on each round with the goal of maximizing the requester’s expected utility. A problem instance consists of several quantities, some of which are known to the algorithm, and some of which are not. The known quantities are the number of outcomes, the requester’s value function, and the time horizon T (i.e., the number of rounds). The latent quantities are the number of effort levels, the set of worker types, and the supply distribution. The algorithm adjusts the contract from round to round and observes the realized outcomes but receives no other feedback.\nWe focus on contracts that are bounded (offer payments in [0, 1]), and monotone (assign equal or higher payments for outcomes with higher value for the requester). Let X be the set of all bounded, monotone contracts. We compare a given algorithm against a given subset of “candidate contracts” Xcand ⊂ X. Letting OPT(Xcand) be the optimal utility over all contracts inXcand, the goal is to minimize the algorithm’s regret R(T |Xcand), defined as T × OPT(Xcand) minus the algorithm’s expected utility.\nThe subset Xcand may be finite or infinite, possibly Xcand = X. The most natural example of a finite Xcand is the set of all bounded, monotone contracts with payments that are integer multiples of some ψ > 0; we call it the uniform mesh with granularity ψ, and denote it Xcand(ψ).\nNotation. Let v(·) be the value function of the requester, with v(π) denoting the value of outcome π. Let O be the set of all outcomes and let m be the number of non-null outcomes. We will index the outcomes as O = {0, 1, 2 , . . . ,m} in the order of increasing value (ties broken arbitrarily), with a convention that 0 is the null outcome.\nLet ci(·) and fi(·) be the cost function and production function for type i. Then the cost of choosing effort level e is ci(e), and the probability of obtaining outcome π having chosen effort e is fi(π|e). Let Fi(π|e) = ∑ π′≥π fi(π\n′|e). Recall that a contract x is a function from outcomes to (non-negative) payments. If contract x is offered to a worker sampled i.i.d. from the supply distribution, V (x) is the expected value to the requester, P (x) ≥ 0 is the expected payment, and U(x) = V (x) − P (x) is the expected utility\nof the requester. Let OPT(Xcand) = supx∈Xcand U(x). Assumption: First-order stochastic dominance (FOSD). Given two effort levels e and e′, we say that e has FOSD over e′ for type i if Fi(π|e) ≥ Fi(π|e′) for all outcomes π, with a strict inequality for at least one outcome.2 We say that type i satisfies the FOSD assumption if for any two distinct effort levels, one effort level has FOSD over the other for type i. We assume that all types satisfy this assumption.\nAssumption: Consistent tie-breaking. If multiple effort levels maximize the expected utility of a given worker for a contract x, we assume the tie is broken consistently in the sense that this worker chooses the same effort level for any contract that leads to this particular tie. This assumption is minor; it can be avoided (with minor technical complications) by adding random perturbations to the contracts. This assumption is implicit throughout the paper."
    }, {
      "heading" : "2.1 Discussion",
      "text" : "Number of outcomes. Our results assume a small number of outcomes. This regime is important in practice, as the quality of submitted work is typically difficult to evaluate in a very fine granularity. Even with m = 2 non-null outcomes, our setting has not been studied before. The special case m = 1 is equivalent to the dynamic pricing problem from Kleinberg and Leighton [2003a]; we obtain improved results for it, too.\nThe benchmark. Our benchmark OPT(·) only considers contracts that are bounded and monotone. In practice, restricting to such contracts may be appealing to all human parties involved. However, this restriction is not without loss of generality: there are problem instances in which monotone contracts are not optimal; see Appendix A for an example. Further, it is not clear whether bounded monotone contracts are optimal among monotone contracts.\nOur benchmark OPT(Xcand) is relative to a given set Xcand, which is typically a finite discretization of the contract space. There are two reasons for this. First, crowdsourcing platforms may require the payments to be multiples of some minimum unit (e.g., one cent), in which case it is natural to restrict our attention to contracts satisfying the same constraint. Second, achieving guarantees relative to OPT(X) for the full generality of our problem appears beyond the reach of our techniques. As in many other machine learning scenarios, it is useful to consider a restricted “benchmark set” – set of alternatives to compare to.3 In such settings, it is considered important to handle arbitrary benchmark sets, which is what we do.\nOne known approach to obtain guarantees relative to OPT(X) is to start with some finiteXcand ⊂ X, design an algorithm with guarantees relative to OPT(Xcand), and then, as a separate result, bound the discretization error OPT(X)− OPT(Xcand). Then the choice of Xcand drives the tradeoff between the discretization error and regret R(T |Xcand), and one can choose Xcand to optimize this tradeoff. However, while one can upper-bound the discretization error in some (very) simple special cases (see Section 5), it is unclear whether this can be extended to the full generality of dynamic contract design.\nAlternative worker models. One of the crucial tenets in our model is that the workers maximize their expected utility. This “rationality assumption” is very standard in Economics, and is\n2This mimics the standard notion of FOSD between two distributions over a linearly ordered set. 3A particularly relevant analogy is contextual bandits with policy sets, e.g., Dudik et al. [2011].\noften used to make the problem amenable to rigorous analysis. However, there is a considerable literature suggesting that in practice workers may deviate from this “rational” behavior. Thus, it is worth pointing out that our results do not rely heavily on the rationality assumption. The FOSD assumption (which is also fairly standard) can be circumvented, too. In fact, all our assumptions regarding worker behavior serve only to enable us to prove Lemma 3.1, and more specifically to guarantee that the collective worker behavior satisfies the following natural property (which is used in the proof of Lemma 3.1): if the requester increases the “increment payment” (as described in the next section) for a particular outcome, the probability of obtaining an outcome at least that good also increases.\nMinimum wage. For ethical or legal reasons one may want to enforce some form of minimum wage. This can be expressed within our model as a minimal payment θ for a completed task, i.e., for any non-null outcome. Our algorithm can be easily modified to accommodate this constraint. Essentially, it suffices to restrict the action space to contracts that pay at least θ for a completed task. Formally, the “increment space” defined in Section 3 should be [θ, 1] × [0, 1]m−1 rather than [0, 1]m, and the “quadrants” of each “cell” are defined by splitting the cell in half in each dimension. All our results easily carry over to this version (restricting Xcand to contracts that pay at least θ for a completed task). We omit further discussion of this issue for the sake of simplicity.\nComparison to multi-armed bandits (MAB). Dynamic contract design can be modeled as special case of the MAB problem with some additional, problem-specific structure. The basic MAB problem is defined as follows. An algorithm repeatedly chooses actions from a fixed action space and collects rewards for the chosen actions; the available actions are traditionally called arms. More specifically, time is partitioned into rounds, so that in each round the algorithm selects an arm and receives a reward for the chosen arm. No other information, such as the reward the algorithm would have received for choosing an alternative arm, is revealed. In an MAB problem with stochastic rewards, the reward of each arm in a given round is an i.i.d. sample from some distribution which depends on the arm but not on the round. A standard measure of algorithm’s performance is regret with respect to the best fixed arm, defined as the difference in expected total reward between a benchmark (usually the best fixed arm) and the algorithm.\nThus, dynamic contract design can be naturally modeled as an MAB problem with stochastic rewards, in which arms correspond to monotone contracts. The prior work on MAB with large / infinite action spaces often assumes known upper bounds on similarity between arms. More precisely, this prior work would assume that an algorithm is given a metric D on contracts such that expected rewards are Lipschitz-continuous with respect to D, i.e., we have upper bounds |U(x) − U(y)| ≤ D(x, y) for any two contracts x, y.4 However, in our setting such upper bounds are absent. On the other hand, our problem has some supplementary structure compared to the standard MAB setting. In particular, the algorithm’s reward decomposes into value and payment, both of which are determined by the outcome, which in turn is probabilistically determined by the worker’s strategic choice of the effort level. Effectively, this supplementary structure provides some “soft” information on similarity between contracts, in the sense that numerically similar contracts are usually (but not always) similar to one another.\n4Such upper bound is informative if and only if D(x, y) < 1.\n3 Our algorithm: AgnosticZooming\nIn this section, we specify our algorithm. We call it AgnosticZooming because it “zooms in” on more promising areas of the action space, and does so without knowing a precise measure of the similarity between contracts. This zooming can be viewed as a dynamic form of discretization. Before stating the algorithm itself, we discuss the discretization of the action space in more detail, laying the groundwork for our approach."
    }, {
      "heading" : "3.1 Discretization of the action space",
      "text" : "In each round, the AgnosticZooming algorithm partitions the action space into several regions and chooses among these regions, effectively treating each region as a “meta-arm.” In this section, we discuss which subsets of the action space are used as regions, and introduce some useful notions and properties of such subsets.\nIncrement space and cells. To describe our approach to discretization, it is useful to think of contracts in terms of increment payments. Specifically, we represent each monotone contract x : O → [0,∞) as a vector x ∈ [0,∞)m, where m is the number of non-null outcomes and xπ = x(π) − x(π − 1) ≥ 0 for each non-null outcome π. (Recall that by convention 0 is the null outcome and x(0) = 0.) We call this vector the increment representation of contract x, and denote it incr(x). Note that if x is bounded, then incr(x) ∈ [0, 1]m. Conversely, call a contract weakly bounded if it is monotone and its increment representation lies in [0, 1]m. Such a contract is not necessarily bounded.\nWe discretize the space of all weakly bounded contracts, viewed as a multi-dimensional unit cube. More precisely, we define the increment space as [0, 1]m with a convention that every vector represents the corresponding weakly bounded contract. Each region in the discretization is a closed, axis-aligned m-dimensional cube in the increment space; henceforth, such cubes are called cells. A cell is called relevant if it contains at least one candidate contract. A relevant cell is called atomic if it contains exactly one candidate contract, and composite otherwise.\nIn each composite cell C, the algorithm will only use two contracts: themaximal corner, denoted x+(C), in which all increment payments are maximal, and the minimal corner, denoted x−(C), in which all increment payments are minimal. These two contracts are called the anchors of C. In each atomic cell C, the algorithm will only use one contract: the unique candidate contract, also called the anchor of C.\nVirtual width. To take advantage of the problem structure, it is essential to estimate how similar the contracts within a given composite cell C are. Ideally, we would like to know the maximal difference in expected utility:\nwidth(C) = supx,y∈C |U(x)− U(y)| .\nWe estimate the width using a proxy, called virtual width, which is expressed in terms of the anchors:\nVirtWidth(C) = ( V (x+(C))− P (x−(C)) ) − ( V (x−(C))− P (x+(C)) ) . (1)\nThis definition is one crucial place where the problem structure is used. (Note that it is not the difference in utility at the anchors.) It is useful due to the following lemma (proved in Section 3.3).\nLemma 3.1. If all types satisfy the FOSD assumption and consistent tie-breaking holds, then width(C) ≤ VirtWidth(C) for each composite cell C.\nRecall that the proof of this lemma is the only place in the paper where we use our assumptions on worker behavior. All further developments hold for any model of worker behavior which satisfies Lemma 3.1."
    }, {
      "heading" : "3.2 Description of the algorithm",
      "text" : "With these ideas in place, we are now ready to describe our algorithm. The high-level outline of AgnosticZooming is very simple. The algorithm maintains a set of active cells which cover the increment space at all times. Initially, there is only a single active cell comprising the entire increment space. In each round t, the algorithm chooses one active cell Ct using an upper confidence index and posts contract xt sampled uniformly at random among the anchors of this cell. After observing the feedback, the algorithm may choose to zoom in on Ct, removing Ct from the set of active cells and activating all relevant quadrants thereof, where the quadrants of cell C are defined as the 2m sub-cells of half the size for which one of the corners is the center of C. In the remainder of this section, we specify how the cell Ct is chosen (the selection rule), and how the algorithm decides whether to zoom in on Ct (the zooming rule).\nLet us first introduce some notation. Consider cell C that is active in some round t. Let U(C) be the expected utility from a single round in which C is chosen by the algorithm, i.e., the average expected utility of the anchor(s) of C. Let nt(C) be the number of times this cell has been chosen before round t. Consider all rounds in which C is chosen by the algorithm before round t. Let Ut(C) be the average utility over these rounds. For a composite cell C, let V + t (C) and P + t (C) be the average value and average payment over all rounds when anchor x+(C) is chosen. Similarly, let V −t (C) and P − t (C) be the average value and average payment over all rounds when anchor x\n−(C) is chosen. Accordingly, we can estimate the virtual width of composite cell C at time t as\nWt(C) = ( V +t (C)− P−t (C) ) − ( V −t (C)− P+t (C) ) . (2)\nTo bound the deviations, we define the confidence radius as\nradt(C) = √ crad log(T )/nt(C), (3)\nfor some absolute constant crad; in our analysis, crad ≥ 16 suffices. We will show that with high probability all sample averages defined above will stay within radt(C) of the respective expectations. If this high probability event holds, the width estimate Wt(C) will always be within 4 radt(C) of VirtWidth(C).\nSelection rule. Now we are ready to complete the algorithm. The selection rule is as follows. In each round t, the algorithm chooses an active cell C with maximal index It(·). It(C) is an upper confidence bound on the expected utility of any candidate contract in C, defined as\nIt(C) =\n{ Ut(C) + radt(C) if C is an atomic cell,\nUt(C) +Wt(C) + 5 radt(C) otherwise. (4)\nZooming rule. We zoom in on a composite cell Ct if\nWt+1(Ct) > 5 radt+1(Ct),\nALGORITHM 1: AgnosticZooming\nInputs: subset Xcand ⊂ X of candidate contracts. Data structure: Collection A of cells. Initially, A = { [0, 1]m }. For each round t = 1 to T\nLet Ct = argmaxC∈A It(C), where It(·) is defined as in Equation (4). Sample contract xt u.a.r. among the anchors of Ct. \\\\ Anchors are defined in Section 3.1. Post contract xt and observe feedback. If |C ∩Xcand| > 1 and 5 radt+1(Ct) < Wt+1(Ct) then\nA ← A∪ {all relevant quadrants of Ct} \\ {Ct}. \\\\ C is relevant if |C ∩Xcand| ≥ 1.\ni.e., the uncertainty due to random sampling, expressed by the confidence radius, becomes sufficiently small compared to the uncertainty due to discretization, expressed by the virtual width. We never zoom in on atomic cells. The pseudocode is summarized in Algorithm 1.\nInteger payments. In practice it may be necessary to only allow contracts in which all payments are integer multiples of some amount ψ, e.g., whole cents. (In this case we can assume that candidate contracts have this property, too.) Then we can redefine the two anchors of each composite cell: the maximal (resp., minimal) anchor is the nearest allowed contract to the maximal (resp., minimal) corner. Width can be redefined as a sup over all allowed contracts in a given cell. With these modifications, the analysis goes through without significant changes. We omit further discussion of this issue."
    }, {
      "heading" : "3.3 Proof of Lemma 3.1 (virtual width)",
      "text" : "For two vectors x,x′ ∈ ℜm, write x′ x if x′ pointwise dominates x, i.e., if x′j ≥ xj for all j. For two monotone contracts x, x′, write x′ x if incr(x′) incr(x).\nClaim 3.2. Consider a worker whose type satisfies the FOSD assumption and two weakly bounded contracts x, x′ such that x′ x. Let e (resp., e′) be the effort levels exerted by this worker when he is offered contract x (resp., x′). Then e does not have FOSD over e′.\nProof. For the sake of contradiction, assume that e has FOSD over e′. Note that e 6= e′. Let i be the worker’s type. Recall that Fi(π|e) denotes the probability of generating an outcome π′ ≥ π given the effort level e. Define F = ( Fi(1|e) , . . . , Fi(m|e) ), and define F′ similarly for e′. Let x and x′ be the increment representations for x and x′. Given contract x, the worker’s expected utility for effort level e is Ui(x|e) = x · F− ci(e). Since e is the optimal effort level given this contract, we have Ui(x|e) ≥ Ui(x|e′), and therefore\nx · F− x · F′ ≥ ci(e)− ci(e′).\nSimilarly, since e′ is the optimal effort level given contract x′, we have\nx′ · F′ − x′ · F ≥ ci(e′)− ci(e).\nCombining the above two inequalities, we obtain\n(x− x′) · (F − F′) ≥ 0. (5)\nNote that if Equation (5) holds with equality then Ui(x|e) = Ui(x|e′) and Ui(x′|e) = Ui(x′|e′), so the worker breaks the tie between e and e′ in a different way for two different contracts. This contradicts the consistent tie-breaking assumption. However, Equation (5) cannot hold with a strict equality, either, because x′ x and (since e has FOSD over e′) we have F F′ and Fπ > F′π for some outcome π > 0. Therefore we obtain a contradiction, completing the proof.\nThe proof of Claim 3.2 is the only place in the paper where we directly use the consistent tie-breaking assumption. (But the rest of the paper relies on this claim.)\nClaim 3.3. Assume all types satisfy the FOSD assumption. Consider weakly bounded contracts x, x′ such that x′ x. Then V (x′) ≥ V (x) and P (x′) ≥ P (x).\nProof. Consider some worker, let i be his type. Let e and e′ be the chosen effort levels for contracts x and x′, respectively. By the FOSD assumption, either e = e′, or e′ has FOSD over e, or e has FOSD over e′. Claim 3.2 rules out the latter possibility.\nDefine vectors F and F′ as in the proof of Claim 3.2. Note that F′ F. Then P = x · F and P ′ = x′ · F′ is the expected payment for contracts x and x′, respectively. Further, letting v denote the increment representation of the requester’s value for each outcome, V = v · F and V ′ = v · F′ is the expected requester’s value for contracts x and x′, respectively. Since x′ x and F′ F, it follows that P ′ ≥ P and V ′ ≥ V . Since this holds for each worker, this also holds in expectation over workers.\nTo finish the proof of Lemma 3.1, fix a contract x ∈ C and observe that V (x+) ≥ V (x) ≥ V (x−) and P (x+) ≥ P (x) ≥ P (x−), where x+ = x+(C) and x− = x−(C) are the two anchors."
    }, {
      "heading" : "4 Regret bounds and discussion",
      "text" : "We present the main regret bound for AgnosticZooming. Formulating this result requires some new, problem-specific structure. Stated in terms of this structure, the result is somewhat difficult to access. To explain its significance, we state several corollaries, and compare our results to prior work.\nThe main result. We start with the main regret bound. Like the algorithm itself, this regret bound is parameterized by the set Xcand of candidate contracts; our goal is to bound the algorithm’s regret with respect to candidate contracts.\nRecall that OPT(Xcand) = supx∈Xcand U(x) is the optimal expected utility over candidate contracts. The algorithm’s regret with respect to candidate contracts is R(T |Xcand) = T OPT(Xcand)− U , where T is the time horizon and U is the expected cumulative utility of the algorithm.\nDefine the badness ∆(x) of a contract x ∈ X as the difference in expected utility between an optimal candidate contract and x: ∆(x) = OPT(Xcand)− U(x). Let Xǫ = {x ∈ Xcand : ∆(x) ≤ ǫ}.\nWe will only be interested in cells that can potentially be used by AgnosticZooming. Formally, we recursively define a collection of feasible cells as follows: (i) the cell [0, 1]m is feasible, (ii) for each feasible cell C, all relevant quadrants of C are feasible. Note that the definition of a feasible cell implicitly depends on the set Xcand of candidate contracts.\nLet Fǫ denote the collection of all feasible, composite cells C such that VirtWidth(C) ≥ ǫ. For Y ⊂ Xcand, let Fǫ(Y ) be the collection of all cells C ∈ Fǫ that overlap with Y , and let Nǫ(Y ) =\n|Fǫ(Y )|; sometimes we will write Nǫ(Y |Xcand) in place of Nǫ(Y ) to emphasize the dependence on Xcand.\nUsing the structure defined above, the main theorem is stated as follows. We prove this theorem in Section 6.\nTheorem 4.1. Consider the dynamic contract design problem with all types satisfying the FOSD assumption and a constant number of outcomes. Consider AgnosticZooming, parameterized by some set Xcand of candidate contracts. Assume T ≥ max(2m+1, 18). There is an absolute constant β0 > 0 such that for any δ > 0,\nR(T |Xcand) ≤ δT +O(log T ) ∑\nǫ=2−j≥δ: j∈N\nNǫ β0(Xǫ|Xcand) ǫ . (6)\nRemark 1. As discussed in Section 2.1, we target the practically important case of a small number of outcomes. The impact of larger m is an exponential dependence on m in the O() notation, and, more importantly, increased number of candidate policies (typically exponential in m for a given granularity).\nRemark 2. Our regret bounds do not depend on the number of worker types, in line with prior work on dynamic pricing. Essentially, this is because bandit approaches tend to depend only on expected reward of a given “arm” (and perhaps also on the variance), not the finer properties of the distribution.\nEquation (6) has a shape similar to several other regret bounds in the literature, as discussed below. To make this more apparent, we observe that regret bounds in “bandits in metric spaces” are often stated in terms of covering numbers. (For a fixed collection F of subsets of a given ground set X, the covering number of a subset Y ⊂ X relative to F is the smallest number of subsets in F that is sufficient to cover Y .) The numbers Nǫ(Y |Xcand) are, essentially, about covering Y with feasible cells with virtual width close to ǫ. We make this point more precise as follows. Let an ǫ-minimal cell be a cell in Fǫ which does not contain any other cell in Fǫ. Let Nminǫ (Y ) be the covering number of Y relative to the collection of ǫ-minimal cells, i.e., the smallest number of ǫ-minimal cells sufficient to cover Y . Then\nNǫ(Y ) ≤ ⌈log 1ψ⌉ N min ǫ (Y ) for any Y ⊂ Xcand and ǫ ≥ 0, (7)\nwhere ψ is the smallest size of a feasible cell.5 Thus, Equation (6) can be easily restated using the covering numbers Nminǫ (·) instead of Nǫ(·). Corollary: Polynomial regret. Literature on regret-minimization often states “polynomial” regret bounds of the form R(T ) = Õ(T γ), γ < 1. While covering-number regret bounds are more precise and versatile, the exponent γ in a polynomial regret bound expresses algorithms’ performance in a particularly succinct and lucid way.\nFor “bandits in metric spaces” the exponent γ is typically determined by an appropriately defined notion of “dimension”, such as the covering dimension,6 which succinctly captures the difficulty of the problem instance. Interestingly, the dependence of γ on the dimension d is typically\n5To prove Equation (7), observe that for each cell C ∈ Fǫ(Y ) there exists an ǫ-minimal cell C ′ ⊂ C, and for each\nǫ-minimal cell C′ there exist at most ⌈log 1 ψ ⌉ cells C ∈ Fǫ(Y ) such that C ′ ⊂ C. 6Given covering numbers Nǫ(·), the covering dimension of Y is the smallest d ≥ 0 such that Nǫ(Y ) = O(ǫ\n−d) for all ǫ > 0.\nof the same shape; γ = (d + 1)/(d + 2), for several different notions of “dimension”. In line with this tradition, we define the width dimension:\nWidthDimα = inf { d ≥ 0 : Nǫ β0(Xǫ|Xcand) ≤ α ǫ−d for all ǫ > 0 } , α > 0. (8)\nNote that the width dimension depends on Xcand and the problem instance, and is parameterized by a constant α > 0. By optimizing the choice of δ in Equation (6), we obtain the following corollary.\nCorollary 4.2. Consider the the setting of Theorem 4.1. For any α > 0, let d = WidthDimα. Then\nR(T |Xcand) ≤ O(α log T ) T (1+d)/(2+d). (9)\nThe width dimension is similar to the “zooming dimension” in Kleinberg et al. [2008] and “near-optimality dimension” in Bubeck et al. [2011a] in the work on “bandits in metric spaces”."
    }, {
      "heading" : "4.1 Comparison to prior work",
      "text" : "Non-adaptive discretization. One approach from prior work that is directly applicable to the dynamic contract design problem is non-adaptive discretization. This is an algorithm, call it NonAdaptive, which runs an off-the-shelf MAB algorithm, treating a set of candidate contracts Xcand as arms.\n7 For concreteness, and following the prior work [Kleinberg and Leighton, 2003a, Kleinberg, 2004, Kleinberg et al., 2008], we use a well-known algorithm UCB1 [Auer et al., 2002] as an off-the-shelf MAB algorithm.\nTo compare AgnosticZooming with NonAdaptive, it is useful to derive several “worst-case” corollaries of Theorem 4.1, replacing Nǫ(Xǫ) with various (loose) upper bounds. 8\nCorollary 4.3. In the setting of Theorem 4.1, the regret of AgnosticZooming can be upper-bounded as follows:\n(a) R(T |Xcand) ≤ δT + ∑\nǫ=2−j≥δ: j∈N Õ(|Xǫ| /ǫ), for each δ ∈ (0, 1). (b) R(T |Xcand) ≤ Õ( √ T |Xcand|).\nHere the Õ() notation hides the logarithmic dependence on T and δ.\nThe best known regret bounds for NonAdaptive coincide with those in Corollary 4.3 up to poly-logarithmic factors. However, the regret bounds in Theorem 4.1 may be significantly better than the ones in Corollary 4.3. We further discuss this in the next section, in the context of a specific example.\nBandits in metric spaces. Consider a variant of dynamic contract design in which an algorithm is given a priori information on similarity between contracts: a function D : Xcand ×Xcand → [0, 1] such that |U(x) − U(y)| ≤ D(x, y) for any two candidate contracts x, y. If an algorithm is given this function D (call such algorithm D-aware), the machinery from “bandits in metric spaces” Kleinberg et al. [2008], Bubeck et al. [2011a] can be used to perform adaptive discretization and obtain a significant advantage over NonAdaptive. We argue that we obtain similar results with AgnosticZooming without knowing the D.\n7To simplify the proofs of the lower bounds, we assume that the candidate contracts are randomly permuted when given to the MAB algorithm.\n8We use the facts that Xǫ ⊂ Xcand, Nǫ(Y ) ≤ N0(Y ), and N min 0 (Y ) ≤ |Y | for all subsets Y ⊂ X.\nIn practice, the similarity information D would be coarse, probably aggregated according to some predefined hierarchy. To formalize this idea, the hierarchy can be represented as a collection F of subsets of Xcand, so that D(x, y) is a function of the smallest subset in F containing both x and y. The hierarchy F should be natural given the structure of the contract space. One such natural hierarchy is the collection of all feasible cells, which corresponds to splitting the cells in half in each dimension. Formally, D(x, y) = f(Cx,y) for some f with f(Cx,y) ≥ width(Cx,y), where Cx,y is the smallest feasible cell containing both x and y.\nGiven this shape of D, let us state the regret bounds for D-aware algorithms in Kleinberg et al. [2008] and Bubeck et al. [2011a]. To simplify the notation, we assume that the action space is restricted to Xcand. The regret bounds have a similar “shape” as that in Theorem 4.1:\nR(T |Xcand) ≤ δT +O(log T ) ∑\nǫ=2−j≥δ: j∈N\nN∗Ω(ǫ)(Xǫ)\nǫ , (10)\nwhere the numbers N∗ǫ (·) have a similar high-level meaning as Nǫ(·), and nearly coincide with Nminǫ (·) when D(x, y) = VirtWidth(Cx,y). One can use Equation (10) to derive a polynomial regret bound like Equation (9).\nFor a more precise comparison, we focus on the results in Kleinberg et al. [2008] . (The regret bounds in Bubeck et al. [2011a] are very similar in spirit, but are stated in terms of a slightly different structure.) The “covering-type” regret bound in Kleinberg et al. [2008] focuses on balls of radius at most ǫ according to distance D, so that N∗ǫ (Y ) is the smallest number of such balls that is sufficient to cover Y . In the special case D(x, y) = VirtWidth(Cx,y) balls of radius ≤ ǫ are precisely feasible cells of virtual width ≤ ǫ. This is very similar (albeit not technically the same) as the ǫ-minimal cells in the definition of Nminǫ (·).\nFurther, the covering numbers N∗ǫ (Y ) determine the “zooming dimension”:\nZoomDimα = inf { d ≥ 0 : N∗ǫ/8(Xǫ) ≤ α ǫ−d for all ǫ > 0 } , α > 0. (11)\nThis definition coincides with the covering dimension in the worst case, and can be much smaller for “nice” problem instances in which Xǫ is a significantly small subset of Xcand. With this definition, one obtains a polynomial regret bound which is version of Equation (9) with d = ZoomDimα.\nWe conclude that AgnosticZooming essentially matches the regret bounds for D-aware algorithms, despite the fact that D-aware algorithms have access to much more information."
    }, {
      "heading" : "5 A special case: the “high-low example”",
      "text" : "We apply the machinery in Section 4 on a special case, and we show that AgnosticZooming significantly outperforms NonAdaptive.\nThe most basic special case is when there is just one non-null outcome. Essentially, each worker makes a strategic choice whether to accept or reject a given task (where “reject” corresponds to the null effort level), and this choice is fully observable. This setting has been studied before [Kleinberg and Leighton, 2003a, Badanidiyuru et al., 2012, Singla and Krause, 2013, Badanidiyuru et al., 2013]; we will call it dynamic task pricing. Here the contract is completely specified by the price p for the non-null outcome. The supply distribution is summarized by the function S(p) = Pr[accept|p], so that the corresponding expected utility is U(p) = S(p)(v− p), where v is the value for the non-null\noutcome. This special case is already quite rich, because S(·) can be an arbitrary non-decreasing function. By using adaptive discretization, we achieve significant improvement over prior work; see Section 8 for further discussion.\nWe consider a somewhat richer setting in which workers’ strategic decisions are not observable; this is a salient feature of our setting, called moral hazard in the contract theory literature. There are two non-null outcomes (low and high), and two non-null effort levels (low and high). Low outcome brings zero value to the requester, while high outcome brings value v > 0. Low effort level inflicts zero cost on a worker and leads to low outcome with probability 1. We assume that workers break ties between effort levels in a consistent way: high better than low better than null. (Hence, as low effort incurs zero cost, the only possible outcomes are low and high.) We will call this the high-low example; it is perhaps the simplest example that features moral hazard.\nIn this example, the worker’s type consists of a pair (ch, θh), where ch ≥ 0 is the cost for high effort and θh ∈ [0, 1] is the probability of high outcome given high effort. Note that dynamic task pricing is equivalent to the special case θh = 1.\nThe following claim states a crucial property of the high-low example.\nClaim 5.1. Consider the high-low example with a fixed supply distribution. Then the probability of obtaining high outcome given contract x Pr[high outcome | contract x] depends only on p = x(high) − x(low); denote this probability by S(p). Moreover, S(p) is non-decreasing in p. Therefore:\n• expected utility is U(x) = S(p)(v − p)− x(low). • discretization error OPT(X)− OPT(Xcand(ψ)) is at most 3ψ, for any ψ > 0. Recall that Xcand(ψ), the uniform mesh with granularity ψ > 0, consists of all bounded, monotone contracts with payments in ψN. For our purposes, the supply distribution is summarized via the function S(·). Denote Ũ(p) = S(p)(v − p). Note that U(x) is maximized by setting x(low) = 0, in which case U(x) = Ũ(p). Thus, if an algorithm knows that it is given a high-low example, it can set x(low) = 0, thereby reducing the dimensionality of the search space. Then the problem essentially reduces to dynamic task pricing with the same S(·).\nHowever, in general an algorithm does not know whether it is presented with the high-low example (because the effort levels are not observable). So in what follows we will consider algorithms that do not restrict themselves to x(low) = 0.\n“Nice” supply distribution. We focus on a supply distribution D that is “nice”, in the sense that S(·) satisfies the following two properties:\n• S(p) is Lipschitz-continuous: |S(p)− S(p′)| ≤ L|p− p′| for some constant L. • Ũ(p) is strongly concave, in the sense that Ũ ′′(·) exists and satisfies Ũ ′′(·) ≤ C < 0.\nHere L and C are absolute constants. We call such D strongly Lipschitz-concave. The above properties are fairly natural. For example, they are satisfied if θh is the same for all worker types and the marginal distribution of ch is piecewise uniform such that the density is between 1λ and λ, for some absolute constant λ ≥ 1.\nWe show that for any choice Xcand ⊂ X, AgnosticZooming has a small width dimension in this setting, and therefore small regret.\nLemma 5.2. Consider the high-low example with a strongly Lipschitz-concave supply distribution. Then the width dimension is at most 12 , for any given Xcand ⊂ X. Therefore, AgnosticZooming with this Xcand has regret R(T |Xcand) = O(log T )T 3/5.\nWe contrast this with the performance of NonAdaptive, parameterized with the natural choice Xcand = Xcand(ψ). We focus on R(T |X): regret w.r.t. the best contract in X. We show that AgnosticZooming achieves R(T |X) = Õ(T 3/5) for a wide range of Xcand, whereas NonAdaptive cannot do better than R(T |X) = O(T 3/4) for any Xcand = Xcand(ψ), ψ > 0.\nLemma 5.3. Consider the setting of Lemma 5.2. Then: (a) AgnosticZooming with Xcand ⊃ Xcand(T−2/5) has regret R(T |X) = O(T 3/5 log T ). (b) NonAdaptive with Xcand = Xcand(ψ) cannot achieve regret R(T |X) < o(T 3/4) over all problem instances, for any ψ > 0. 9"
    }, {
      "heading" : "5.1 Proofs",
      "text" : "Proof of Claim 5.1. Consider a contract x with x(low) = b and x(high) = b + p, and a worker of type (ch, θh). If the worker exerts high effort, she pays cost ch and receives expected payment θh(p+ b) + (1− θh)b, for a total expected payoff pθh + b− ch. Her expected payoff for exerting low effort is b. Therefore she will choose to exert high effort if and only if pθh + b − ch ≥ b, i.e., if ch/θh ≤ p, and choose to exert low effort otherwise. Therefore\nPr[high outcome | contract x] = E (ch,θh)\n[ θh 1{ch/θh≤p} ] .\nThis is a function of p, call it S(p). Moreover, this is a non-decreasing function simply because the expression inside the expectation is non-decreasing in p.\nIt trivially follows that U(x) = S(p)(v − p)− x(low). We can upper-bound the discretization error using a standard approach from the work on dynamic pricing Kleinberg and Leighton [2003b]. Fix discretization granularity ψ > 0. For any ǫ > 0, there exists a contract x∗ ∈ X such that OPT(X)−U(x∗) < ǫ. Round x∗(high) and x∗(low) up and down, respectively, to the nearest integer multiple of ψ; let x ∈ Xcand(ψ) be the resulting contract. Denoting p = x(high)−x(low) and p∗ = x∗(high)−x∗(low), we see that p∗ ≤ p ≤ p∗+2ψ. It follows that U(x) ≥ U(x∗)− 3ψ ≥ OPT(X)− ǫ− 3ψ. Since this holds for any ǫ > 0, we conclude that OPT(X) − OPT(Xcand(ψ)) ≤ 3ψ.\nProof of Lemma 5.2. To calculate the width dimension, we need to count the number of feasible cells in the increment space which (i) has virtual width larger than or equal to O(ǫ) and (ii) overlaps with Xǫ, the set of contracts with badness smaller than ǫ.\nWe first characterize Xǫ. We use xp,b to denote the contract with x(high) = p+b and x(low) = b. The benefit of this representation is that, p and b would then be the two axis in the increment space. Let xp∗,0 be an optimal contract. Since U(xp,b) is strongly concave in p, we know that for any b, there exists constants C1 and C2 such that for any p ∈ [0, 1], C1(p∗−p)2 ≤ U(xp∗,b)−U(xp,b) ≤ C2(p∗−p)2. Also we know that U(xp∗,b) = U(xp∗,0)− b. Therefore.\nXǫ = {xp,b : (p− p∗)2 + b ≤ O(ǫ)} 9This lower bound holds even if UCB1 in NonAdaptive is replaced with any other MAB algorithm.\nWe can also write it as\nXǫ = {xp,b : p∗ − θh( √ ǫ) ≤ p ≤ p∗ + θh( √ ǫ) and b ≤ O(ǫ)}\nIntuitively, Xǫ contains contracts {xp,b} with p not O( √ ǫ) away from p∗ and b not O(ǫ) away from b∗ = 0. Next we characterize the virtual width of a cell. We use Cp,b,d to denote the cell with size d and with anchors {xp,b, x(p+d),(b+d)}. We can derive the expected payment and value on the two anchors as:\n• P+(Cp,b,d) = (p+ d)S(p + d) + b+ d • V +(Cp,b,d) = vS(p + d) • P−(Cp,b,d) = pS(p) + b • V −(Cp,b,d) = vS(p)\nBy definition, we can get that (we use dF to represent S(p+ d)− S(p) for simplification) VirtWidth(Cp,b,d) = (v + p)dF + dS(p) + d dF + d.\nNow we can count the number of feasible cells with virtual width larger than θh(ǫ) which overlaps with Xǫ. Note that since the total number of feasible cells Cp,b,d with large d is small, we can treat the number of cells with large d as a constant. Also, for any relevant cell Cp,b,d, we have p ≈ p∗. Therefore, we only care about feasible cells Cp,b,d with small d and when p is close to p\n∗. Since S(p) is Lipschitz, we have dF = O(d). Therefore, for any relevant cell Cp,d,\nVirtWidth(Cp,b,d) = O(d)\nGiven the above two arguments, we know that the number of cells with virtual width larger than ǫ which also overlaps with Xǫ is O(ǫ/ǫ)×O( √ ǫ/ǫ) = O(ǫ−1/2). Therefore the width dimension is 1/2.\nProof Sketch of Lemma 5.3(b). Consider a version of NonAdaptive that runs an off-the-shelf MAB algorithm ALG on candidate contracts Xcand = Xcand(ψ). For ALG, the “arms” are the candidate contracts; recall that the arms are randomly permuted before they are given to ALG.\nFix ψ > 0. It is easy to construct a problem instance with discretization error Error , OPT(X)−OPT(Xcand(ψ)) ≥ Ω(ψ). Note that Xcand contains N = Ω(ψ−2) suboptimal contracts that are suboptimal w.r.t. OPT(Xcand). (For example, all contracts x with x(low) > 0 are suboptimal.)\nFix any problem instance I of MAB with N suboptimal arms. Using standard lower-bound arguments for MAB, one can show that if one runs ALG on a problem instance obtained by randomly permuting the arms in I, then the expected regret in T rounds is at least Ω( √ NT ).\nTherefore, R(T |Xcand) ≥ Ω( √ NT ). It follows that\nR(T |X) ≥ Ω( √ NT ) + Error · T ≥ Ω( √ T/ψ + ψT ) ≥ Ω(T 3/4)."
    }, {
      "heading" : "6 Proof of the main regret bound (Theorem 4.1)",
      "text" : "We now prove the main result from Section 4. Our high-level approach is to define a clean execution of an algorithm as an execution in which some high-probability events are satisfied, and derive bounds on regret conditional on the clean execution. The analysis of a clean execution does not involve any “probabilistic” arguments. This approach tends to simplify regret analysis.\nWe start by listing some simple invariants enforced by AgnosticZooming:\nInvariant 6.1. In each round t of each execution of AgnosticZooming: (a) All active cells are relevant, (b) Each candidate contract is contained in some active cell, (c) Wt(C) ≤ 5 radt(C) for each active composite cell C. Note that the zooming rule is essential to ensure Invariant 6.1(c)."
    }, {
      "heading" : "6.1 Analysis of the randomness",
      "text" : "Definition 6.2 (Clean Execution). An execution of AgnosticZooming is called clean if for each round t and each active cell C it holds that\n|U(C)− Ut(C)| ≤ radt(C), (12) |VirtWidth(C)−Wt(C)| ≤ 4 radt(C) (if C is composite). (13)\nLemma 6.3. Assume crad ≥ 16 and T ≥ max(1 + 2m, 18). Then: (a) Pr [ Equation (12) holds ∀ rounds t, active cells C ] ≥ 1− 2T−2. (b) Pr [ Equation (13) holds ∀ rounds t, active composite cells C ] ≥ 1− 16T−2. Consequently, an execution of AgnosticZooming is clean with probability at least 1− 1/T . Lemma 6.3 follows from the standard concentration inequality known as “Chernoff Bounds”. However, one needs to be careful about conditioning and other details.\nProof of Lemma 6.3(a). Consider an execution of AgnosticZooming. Let N be the total number of activated cells. Since at most 2m cells can be activated in any one round, N ≤ 1 + 2mT ≤ T 2. Let Cj be the min(j,N)-th cell activated by the algorithm. (If multiple “quadrants” are activated in the same round, order them according to some fixed ordering on the quadrants.)\nFix some feasible cell C and j ≤ T 2. We claim that\nPr [ |U(C)− Ut(C)| ≤ radt(C) for all rounds t | Cj = C ] ≥ 1− 2T−4. (14)\nLet n(C) = n1+T (C) be the total number of times cell C is chosen by the algorithm. For each s ∈ N: 1 ≤ s ≤ n(C) let Us be the requester’s utility in the round when C is chosen for the s-th time. Further, let DC be the distribution of U1, conditional on the event n(S) ≥ 1. (That is, the per-round reward from choosing cell C.) Let U ′1 , . . . , U ′ T be a family of mutually independent random variables, each with distribution DC . Then for each n ≤ T , conditional on the event {Cj = C} ∧ {n(C) = n}, the tuple (U1 , . . . , Un) has the same joint distribution as the tuple (U ′1 , . . . , U ′ n). Consequently, applying Chernoff Bounds to the latter tuple, it follows that\nPr [ ∣∣U(C)− 1n ∑n s=1 Us ∣∣ ≤ √ 1 n crad log(T ) ∣∣∣ {Cj = C} ∧ {n(C) = n} ]\n≥ 1− 2T−2crad ≥ 1− 2T−5.\nTaking the Union Bound over all n ≤ T , and plugging in radt(Cj), nt(Cj), and Ut(Cj), we obtain Equation (14).\nNow, let us keep j fixed in Equation (14), and integrate over C. More precisely, let us multiply both sides of Equation (14) by Pr[Cj = C] and sum over all feasible cells C. We obtain, for all j ≤ T 2:\nPr [ |U(Cj)− Ut(Cj)| ≤ radt(Cj) for all rounds t ] ≥ 1− 2T−4. (15)\n(Note that to obtain Equation (15), we do not need to take the Union Bound over all feasible cells C.) To conclude, we take the Union Bound over all j ≤ 1 + T 2.\nProof Sketch of Lemma 6.3(b). We show that\nPr [ ∣∣V +(C)− V +t (C) ∣∣ ≤ radt(C) ∀ rounds t, active composite cells C ] ≥ 1− 4 T 2 , (16)\nand similarly for V −(), P+() and P−(). Each of these four statements is proved similarly, using the technique from Lemma 6.3(a). In what follows, we sketch the proof for one of the four cases, namely for Equation (16).\nFor a given composite cell C, we are only interested in rounds in which anchor x+(C) is selected by the algorithm. Letting n+t (C) be the number of times this anchor is chosen up to time t, let us define the corresponding notion of “confidence radius”:\nrad+t (C) = 1\n2\n√ crad log T\nn+t (C) .\nWith the technique from the proof of Lemma 6.3(a), we can establish the following highprobability event:\n∣∣V +(C)− V +t (C) ∣∣ ≤ rad+t (C). (17)\nMore precisely, we can prove that\nPr [ Equation (17) holds ∀ rounds t, active composite cells C ] ≥ 1− 2T−2.\nFurther, we need to prove that w.h.p. the anchor x+(C) is played sufficiently often. Noting that E[n+t (C)] = 1 2 nt(C), we establish an auxiliary high-probability event: 10\nn+t (C) ≥ 12 nt(C)− 14 radt(C). (18)\nMore precisely, we can use Chernoff Bounds to show that, if crad ≥ 16,\nPr [ Equation (18) holds ∀ rounds t, active composite cells C ] ≥ 1− 2T−2. (19)\nNow, letting n0 = (crad log T ) 1/3, observe that\nnt(C) ≥ n0 ⇒ n+t (C) ≥ 14 nt(C) ⇒ rad + t (C) ≤ radt(C),\nnt(C) < n0 ⇒ radt(C) ≥ 1 ⇒ ∣∣V +(C)− V +t (C) ∣∣ ≤ radt(C).\nTherefore, once Equations (17) and (18) hold, we have ∣∣V +(C)− V +t (C) ∣∣ ≤ radt(C). This completes the proof of Equation (16).\n10The constant 1 4 in Equation (18) is there to enable a consistent choice of n0 in the remainder of the proof."
    }, {
      "heading" : "6.2 Analysis of a clean execution",
      "text" : "The rest of the analysis focuses on a clean execution. Recall that Ct is the cell chosen by the algorithm in round t.\nClaim 6.4. In any clean execution, I(Ct) ≥ OPT(Xcand) for each round t.\nProof. Fix round t, and let x∗ be any candidate contract. By Invariant 6.1(b), there exists an active cell, call it C∗t , which contains x\n∗. We claim that It(C ∗ t ) ≥ U(x∗). We consider two cases, depending on whether C∗t is atomic.\nIf C∗t is atomic then the anchor is unique, so U(C ∗ t ) = U(x ∗), and It(C ∗ t ) ≥ U(x∗) by the clean execution. If C∗t is composite then\nIt(C ∗ t ) ≥ U(C∗t ) + VirtWidth(C∗t ) by clean execution\n≥ U(C∗t ) + width(C∗t ) by Lemma 3.1 ≥ U(x∗) by definition of width, since x∗ ∈ C∗t .\nWe have proved that It(C ∗ t ) ≥ U(x∗). Now, by the selection rule we have It(Ct) ≥ It(C∗t ) ≥ U(x∗). Since this holds for any candidate contract x∗, the claim follows.\nClaim 6.5. In any clean execution, for each round t, the index It(Ct) is upper-bounded as follows: (a) if Ct is atomic then I(Ct) ≤ U(Ct) + 2 radt(Ct). (b) if Ct is composite then I(Ct) ≤ U(x) +O(radt(Ct)) for each contract x ∈ Ct.\nProof. Fix round t. Part (a) follows because It(Ct) = Ut(Ct) + radt(Ct) by definition of the index, and Ut(Ct) ≤ U(Ct) + radt(Ct) by clean execution.\nFor part (b), fix a contract x ∈ Ct. Then:\nUt(Ct) ≤ U(Ct) + radt(Ct) by clean execution ≤ U(x) + width(Ct) + radt(Ct) by definition of width ≤ U(x) + VirtWidth(Ct) + radt(Ct) by Lemma 3.1 ≤ U(x) +Wt(Ct) + 5 radt(Ct) by clean execution. (20)\nIt(Ct) = Ut(Ct) +Wt(Ct) + 5 radt(Ct) by definition of index\n≤ U(x) + 2Wt(Ct) + 10 radt(Ct) by Equation (20) ≤ U(x) + 20 radt(Ct) by Invariant 6.1(c).\nFor each relevant cell C, define badness ∆(C) as follows. If C is composite, ∆(C) = supx∈C ∆(x) is the maximal badness among all contracts in C. If C is atomic and x ∈ C is the unique candidate contract in C, then ∆(C) = ∆(x).\nClaim 6.6. In any clean execution, ∆(C) ≤ O(radt(C)) for each round t and each active cell C.\nProof. By Claims 6.4 and 6.5, ∆(Ct) ≤ O(radt(Ct)) for each round t. Fix round t and let C be an active cell in this round. If C has never be selected before round t, the claim is trivially true. Else, let s be the most recent round before t when C is selected by the algorithm. Then ∆(C) ≤ O(rads(C)). The claim follows since rads(C) = radt(C).\nClaim 6.7. In a clean execution, each cell C is selected ≤ O(log T/(∆(C))2) times.\nProof. By Claim 6.6, ∆(C) ≤ O(radT (C)). The claim follows from the definition of radT in Equation (3).\nLet n(x) and n(C) be the number of times contract x and cell C, respectively, are chosen by the algorithm. Then regret of the algorithm is\nR(T |Xcand) = ∑ x∈X n(x) ∆(x) ≤ ∑ cells C n(C)∆(C). (21)\nThe next result (Lemma 6.8) upper-bounds the right-hand side of Equation (21) for a clean execution. By Lemma 6.3, this suffices to complete the proof of Theorem 4.1\nLemma 6.8. Consider a clean execution of AgnosticZooming. For any δ ∈ (0, 1), ∑\ncells C n(C)∆(C) ≤ δT +O(log T ) ∑ ǫ=2−j≥δ: j∈N |Fǫ(X2ǫ)| ǫ .\nThe proof of Lemma 6.8 relies on some simple properties of ∆(·), stated below. Claim 6.9. Consider two relevant cells C ⊂ Cp. Then:\n(a) ∆(C) ≤ ∆(Cp). (b) If ∆(C) ≤ ǫ for some ǫ > 0, then C overlaps with Xǫ.\nProof. To prove part (a), one needs to consider two cases, depending on whether cell Cp is composite. If it is, the claim follows trivially. If Cp is atomic, then C is atomic, too, and so ∆(C) = ∆(Cp) = ∆(x), where x is the unique candidate contract in Cp.\nFor part (b), there exists a candidate contract x ∈ C. It is easy to see that ∆(x) ≤ ∆(C) (again, consider two cases, depending on whether C is composite.) So, x ∈ Xǫ.\nProof of Lemma 6.8. Let Σ denote the sum in question. Let A∗ be the collection of all cells ever activated by the algorithm. Among such cells, consider those with badness on the order of ǫ:\nGǫ := { C ∈ A∗ : ∆(C) ∈ [ǫ, 2ǫ) } . By Claim 6.7, the algorithm chooses each cell C ∈ Gǫ at most O(log T/ǫ2) times, so n(C)∆(C) ≤ O(log T/ǫ).\nFix some δ ∈ (0, 1) and observe that all cells C with ∆(C) ≤ δ contribute at most δT to Σ. Therefore it suffices to focus on Gǫ, ǫ ≥ δ/2. It follows that\nΣ ≤ δT +O(log T )∑ǫ=2−i≥δ/2 |Gǫ| ǫ . (22)\nWe bound |Gǫ| as follows. Consider a cell C ∈ Gǫ. The cell is called a leaf if it is never zoomed in on (i.e., removed from the active set) by the algorithm. If C is activated in the round when cell Cp is zoomed in on, Cp is called the parent of C. We consider two cases, depending on whether or not C is a leaf.\n(i) Assume cell C is not a leaf. Since ∆(C) < 2ǫ, C overlaps with X2ǫ by Claim 6.9(b). Note that C is zoomed in on in some round, say in round t− 1. Then\n5 radt(C) ≤ Wt(C) by the zooming rule ≤ VirtWidth(C) + 4 radt(C) by clean execution,\nso radt(C) ≤ VirtWidth(C). Therefore, using Claim 6.6, we have ǫ ≤ ∆(C) ≤ O(radt(C)) ≤ O(VirtWidth(C)).\nIt follows that C ∈ FΩ(ǫ)(X2ǫ).\n(ii) Assume cell C is a leaf. Let Cp be the parent of C. Since C ⊂ Cp, we have ∆(C) ≤ ∆(Cp) by Claim 6.9(a). Therefore, invoking case (i), we have\nǫ ≤ ∆(C) ≤ ∆(Cp) ≤ O(VirtWidth(Cp)).\nSince ∆(C) < 2ǫ, C overlaps with X2ǫ by Claim 6.9(b), and therefore so does Cp. It follows that Cp ∈ FΩ(ǫ)(X2ǫ).\nCombing these two cases, it follows that |Gǫ| ≤ (2m + 1) ∣∣FΩ(ǫ)(X2ǫ) ∣∣. Plugging this into (22) and making an appropriate substitution ǫ → Θ(ǫ) to simplify the resulting expression, we obtain the regret bound in Theorem 4.1"
    }, {
      "heading" : "7 Simulations",
      "text" : "We evaluate the performance of AgnosticZooming through simulations. AgnosticZooming is compared with two versions of NonAdaptive that use, respectively, two standard bandit algorithms: UCB1 [Auer et al., 2002] and Thompson Sampling [Thompson, 1933] (with Gaussian priors). For both UCB1 and AgnosticZooming, we replace the logarithmic confidence terms with small constants. (We find such changes beneficial in practice, for both algorithms; this observation is consistent with prior work [Radlinski et al., 2008, Slivkins et al., 2013].) All three algorithms are run with Xcand = Xcand(ψ), where ψ > 0 is the granularity of the discretization.\nSetup. We consider a version of the high-low example, as described in Section 5. We set the requester’s values to V (high) = 1 and V (low) = .3. The probability of obtaining high outcome given high effort is set to θh = .8. Thus, the worker’s type is characterized by the cost ch for high effort. We consider three supply distributions:\n• Uniform Worker Market : ch is uniformly distributed on [0, 1].\n• Homogeneous Worker Market : ch is the same for every worker.\n• Two-Type Market : ch is uniformly distributed over two values, c′h and c′′h .\nThese first two markets represent the extreme cases when workers are extremely homogeneous or extremely diverse, and the third market is one way to represent the middle ground. For each market, we run each algorithm 100 times. For Homogeneous Worker Market, ch is drawn uniformly at random from [0, 1] for each run. For Two-Type Market, c′h and c ′′ h are drawn independently and uniformly from [0, 1] on each run.\nOverview of the results. Across all simulations, AgnosticZooming performs comparably to or better than NonAdaptive. Its performance does not appear to suffer from large “hidden constants” that appear in the analysis. We find that AgnosticZooming converges faster than NonAdaptive when ψ is near-optimal or smaller; this is consistent with the intuition that AgnosticZooming focuses on exploring the more promising regions. When ψ is large, AgnosticZooming converges slower than NonAdaptive, but eventually achieves the same performance. Further, we find that AgnosticZooming with small ψ performs well compared to NonAdaptive with larger ψ: not much worse initially, and much better eventually.\nOur simulations suggest that if time horizon T is known in advance and one can tune ψ to T , then NonAdaptive can achieve similar performance as AgnosticZooming. However, in real\napplications approximately optimal ψ may be difficult to compute, and the T may not be known in advance.\nDetailed results. Recall that in both UCB1 and AgnosticZooming, the logarithmic confidence terms are replaced with small constants. For UCB1, the confidence term is 1, so that if a given arm a has been played na times, its index is simply the average reward plus 1/ √ na. For AgnosticZooming, we set radt(·) = 1 in the selection rule, and radt(·) = .6 in the zooming rule. For both algorithms, we tried several values and picked those that performed well across all three markets; we found that the performance of both algorithms is not very sensitive to the particular choice of these constants, as long as they are on the order of 1.\nFor each algorithm, we compute the time-averaged cumulative utility after T rounds given granularity ψ, denote it Û(T, ψ), for various values of T and ψ.\nFirst, we fix the time horizon T to 5K rounds, and study how Û(T, ψ) changes with ψ (see Figure 1). We observe that AgnosticZooming either matches or outperforms both versions of NonAdaptive, across all markets and all values of ψ. AgnosticZooming has a huge advantage when ψ is small.\nSecond, we study how the three algorithms perform over time. Specifically, we plot Û(T, ψ) vs. T , for three values of ψ, namely 0.02, 0.8, and 0.2. Since setting to 0.08 is close to optimal in our examples, these values of ψ represent, resp., too small, adequate, and too large. The results are shown in Figure 2. We find that AgnosticZooming converges faster than NonAdaptive when ψ is adequate or small; this is consistent with the intuition that AgnosticZooming focuses on exploring the more promising regions. When ψ is large, AgnosticZooming converges slower than NonAdaptive, but eventually achieves the same performance.\nOur simulations suggest that if time horizon T is known in advance and one can optimize the ψ given this T , then NonAdaptive can achieve similar performance as AgnosticZooming. However, in real applications approximately optimal ψ may be difficult to calculate; further, the T may be unknown in advance.\nThird, we argue that AgnosticZooming performs well with a small ψ: we compare its performance against that for NonAdaptive with different values of ψ. For each algorithm and each choice of ψ, we plot Û(T, ψ) vs. T , see Figure 3. 11 We find that for small T , AgnosticZooming with small ψ converges nearly as fast as NonAdaptive with larger ψ. When T is large, AgnosticZooming with small ψ converges to a better payoff than NonAdaptive with larger ψ.\nFourth, we confirm the intuition that OPT(Xcand(ψ)) decreases with the granularity ψ. To this end, we run AgnosticZooming for 50K rounds, and take the average utility over the last 5K rounds, see Figure 4.\nThe standard errors in all plots are in the order of 0.001 or less. (Note that each point is not only the average of 100 runs but also the average of all previous rounds.)"
    }, {
      "heading" : "8 Application to dynamic task pricing",
      "text" : "We discuss dynamic task pricing, which can be seen as the special case of dynamic contract design in which there is exactly one non-null outcome. We identify an important family of problem instances for which AgnosticZooming out-performs NonAdaptive.\nSome background. The dynamic task pricing problem, in its most basic version, is defined as follows. There is one principal (buyer) who sequentially interacts with multiple agents (sellers). In each round t, an agent arrives, with one item for sale. The principal offers price pt for this item, and the agent agrees to sell if and only if pt ≥ ct, where ct ∈ [0, 1] is the agent’s private cost for this item. The principal derives value v for each item bought; his utility is the value from bought items minus the payment. The time horizon T (the number of rounds) is known. Each private cost ct is an independent sample from some fixed distribution, called the supply distribution. We are interested in the prior-independent version, where the supply distribution is not known to the principal. The algorithm’s goal is to choose the offered prices pt so as to maximize the expected utility of the principal.\nDynamic task pricing can be seen as the special case of dynamic contract design in which there is exactly one non-null outcome (which corresponds to a sale). Indeed, in this special case there is exactly one non-null effort level e without loss of generality (because any non-null effort levels deterministically lead to the non-null outcome).\n11We only show the results for Uniform Worker Market; the results for Homogeneous Worker Market are very similar. We only show the version of NonAdaptive with UCB1, because in our experiments it performs better that Thompson Sampling. (We conjecture that this is because we replaced the logarithmic confidence term in UCB1 with 1.)\nOne crucial simplification compared to the full generality of dynamic contract design is that the discretization error can now be easily bounded from above: 12\nOPT(X) − OPT(Xcand(ψ)) ≤ ψ for each ψ > 0.\nWorst-case regret bounds are implicit in prior work on dynamic inventory-pricing [Kleinberg and Leighton, 2003a].13 Let NonAdaptive(ψ) denote algorithm NonAdaptive with Xcand = Xcand(ψ). Then, by the analysis in Kleinberg and Leighton [2003a], NonAdaptive(ψ) achieves regret R(T ) = Õ(ψT + ψ−2). This is optimized to R(T ) = Õ(T 2/3) if and only if ψ = Õ(T−1/3). Moreover, there is a matching lower bound: R(T ) = Ω(T 2/3) for any algorithm.\nFurther, it is a folklore result that NonAdaptive(ψ) achieves regret R(T ) = Õ(T 2/3) if and only if ψ = Θ̃(T−1/3). (We sketch a lower-bounding example in the proof of Lemma 8.4, to make the paper more self-contained.)\nPreliminaries. Each contract is summarized by a single number: the offered price p for the non-null outcome. Let F (p) be the probability of a worker accepting a task at price p, and let U(p) = F (p) (v − p) be the corresponding expected utility of the algorithm.\nNote that all contracts are trivially monotone and any optimal contract is bounded without loss of generality. It follows that OPT(X) = supp≥0 U(p), the optimal expected utility over all possible prices.\nA cell C is just a price interval C = [p, p′] ⊂ [0, 1], and its virtual width is\nVirtWidth(C) = ( v F (p′)− pF (p) ) − ( v F (p)− p′ F (p′) ) .\nOur results: the general case. We will be using AgnosticZooming with Xcand = X. First, let us prove that this is a reasonable choice in the worst case: namely, that we achieve the optimal Õ(T 2/3) regret.\nLemma 8.1. Consider the dynamic task pricing problem. AgnosticZooming with Xcand = X achieves regret O(T 2/3 log T ).\nProof Sketch. Fix ǫ > 0. The key observation is that if VirtWidth(C) ≥ ǫ then either p′ − p ≥ ǫ4 , or F (p′) − F (p) ≥ ǫ4 . Call C a red cell if the former happens, and blue cell otherwise. Therefore in any collection of mutually disjoint cells of virtual width ≥ ǫ there can be at most O(1ǫ ) red cells and at most O(1ǫ ) blue cells, hence at most O( 1 ǫ ) cells total. It follows that there can be at most O(1ǫ ) active cells of virtual width ≥ ǫ. So, in the notation of Theorem 4.1 we have Nǫ(·) ≤ O(1ǫ ). It follows that the width dimension is at most 1, which in turn implies the desired regret bound.\nOur results: “nice” problem instances. We focus on problem instances with piecewise-uniform costs and bounded density. Formally, we say that an instance of dynamic task pricing has kpiecewise-uniform costs if the interval [0,1] is partitioned into k ∈ N sub-intervals such that the\n12Recall that Xcand(ψ) denotes the set of all prices in [0, 1] that are integer multiples of a given ψ > 0; call this set the additive ψ-mesh.\n13The algorithmic result for dynamic task pricing is an easy modification of the analysis in Kleinberg and Leighton [2003a] for dynamic inventory-pricing. The lower bound in in Kleinberg and Leighton [2003a] can also be “translated” from dynamic inventory-pricing to dynamic task pricing without introducing any new ideas. We omit the details from this version.\nsupply distribution is uniform on each sub-interval. A problem instance has λ-bounded density, λ ≥ 1 if the supply distribution has a probability density function almost everywhere, and the density is between 1λ and λ. Using the full power of Theorem 4.1, we obtain the following regret bound.\nTheorem 8.2. Consider the dynamic task pricing problem with k-piecewise-uniform costs and λbounded density, for some absolute constants k ∈ N and λ > 1. AgnosticZooming with Xcand = X achieves regret R(T ) = Õ(T 3/5).\nProof Sketch. Since the supply distribution has density at most λ, it follows that F (·) is a Lipschitzcontinuous function with Lipschitz constant λ. It follows that each cell of virtual width at least ǫ has diameter at least Ω(ǫ/λ), for any ǫ > 0. (Note that each “cell” is now simply a sub-interval [p, q] ⊂ [0, 1], so its diameter is simply q − p.)\nSecond, we claim that Xǫ is contained in a union of k intervals of diameter O( √ ǫλ). To see this, consider the partition of [0, 1] into k subintervals such that the supply distribution has a uniform density on each subinterval. Let [pj, qj ] be the j-th subinterval. Let p ∗ j be the local optimum of U(·) on this subinterval, and let Xj,ǫ = {x ∈ [pj , qj] : U(p∗j) − U(x) ≤ ǫ}. Then Xǫ ⊂ ∪jXj,ǫ. We can show that Xj,ǫ ⊂ [p∗j − δ, p∗j + δ] for some δ = O( √ ǫλ).\nRecall that Nǫβ0(Xǫ) is the number of feasible cells of virtual width at least ǫβ0 which overlap with Xǫ. It follows that Nǫβ0(Xǫ) is at most k times the maximal number of feasible cells of diameter at least Ω(ǫ/λ) that overlap with an interval of diameter O( √ ǫλ). Therefore: Nǫβ0(Xǫ) = O(kλ3/2ǫ−1/2 log 1ǫ ). Moreover, we have a less sophisticated upper bound on Nǫβ0(Xǫ): it is at most the number of feasible cell of diameter at least Ω(ǫ/λ). So Nǫβ0(Xǫ) = O(λ/ǫ)(log 1 ǫ ). The theorem follows by plugging both upper bounds on Nǫβ0(Xǫ) into Equation (6).\nComparison with NonAdaptive. Consider NonAdaptive(ψ0), where ψ0 = Θ̃(T −1/3) is the granularity required for the optimal worst-case performance. Call a problem instance nice if it has 2-piecewise-uniform costs and λ-bounded density, for some sufficiently large absolute constant λ; say λ = 4 for concreteness. We claim that AgnosticZooming outperforms NonAdaptive(ψ0) on the “nice” problem instances.\nLemma 8.3. NonAdaptive(ψ0) achieves regret R(T ) = Ω(T 2/3) in the worst case over all “nice” problem instances.\nProof Sketch. Recall that for k = 2 the supply distribution has density λ1 on interval [0, p0], and density λ2 on interval [p0, 1], for some numbers λ1, λ2, p0. We pick p0 so that it is sufficiently far from any point in Xcand(ψ0). Note that the function U(·) is a parabola on each of the two intervals. We adjust the densities so that U(·) achieves its maximum at p0, and the maximum of either of the two parabolas is sufficiently far from p0. Then the discretization error of Xcand(ψ0) is at least Ω(ψ0), which implies regret Ω(ψ0T ).\nLower bound for NonAdaptive. We provide a specific lower-bounding example for the worstcase performance of NonAdaptive(ψ), for an arbitrary ψ > 0. Let F be the family of all problem instances with k-piecewise-uniform costs and λ-bounded density, for all k ∈ N and λ = 4.\nLemma 8.4. Let Rψ(T ) be the maximal regret of NonAdaptive(ψ) over all problem instances in F . Then Rψ(T ) = Ω(ψT + √ T/ψ) ≥ Ω(T 2/3).\nProof Sketch. For piecewise-uniform costs, we have F (0) = 0 and F (p) = 1. Assume that the principal derives value v = 1 from each item. Then the expected utility from price p is U(p) = F (p)(1 − p).\nFix ψ > 0. Use the following problem instance. Let Pδ = [25 , 35 ] ∩ {4jψ + δ : j ∈ N}. Set U(p) = 14 for each p ∈ P0. Further, pick some p∗ ∈ Pψ/2 and set U(p∗) = 14 + Ω(ψ). This defines F (p) for p ∈ P ∪ {0, 1, p∗}. For the rest of the prices, define F (·) via linear interpolation. This completes the description of the problem instance.\nWe show that Xψ consists of N = Ω( 1 ψ ) candidate contracts. Therefore, using standard lower-\nbounding arguments for MAB, we obtain R(T |Xcand) ≥ Ω( √ TN) = Ω( √ T/ψ). Further, we show that the discretization error is at least Ω(ψ), implying that R(T ) ≥ R(T |Xcand) + Ω(ψT )."
    }, {
      "heading" : "9 Related work",
      "text" : "This paper is related to three different areas: contract theory, market design for crowdsourcing, and online decision problems. Below we outline connections to each of these areas.\nContract theory. Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002]. In the most basic version of the classic model, a single principal interacts with a single agent whose type (specified by a cost function and production function, as described in Section 2) is generally assumed to be known. The principal specifies a contract mapping outcomes to payments that the principal commits to make to the agent. The agent then chooses an action (i.e., effort level) that stochastically results in an outcome in order to maximize his expected utility given the contract. The principal observes the outcome, but cannot directly observe the agent’s effort level, creating a moral hazard problem. The goal of the principal is to design a contract to maximize her own expected utility, which is the difference between the utility she receives from the outcome and the payment she makes. This maximization can be written as a constrained optimization problem, and it can be shown that linear contracts are optimal.\nThe adverse selection variation of the principal-agent problem relaxes the assumption that the agent’s type is known. Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002]. In this setting, the principal offers a menu of contracts, and the contract chosen by the agent reveals the agent’s type. The problem of selecting a menu of contracts that maximizes the principal’s expected utility can again be formulated as a constrained optimization.\nOur work differs from the classic setting in that we consider a principal interacting with multiple agents, and the principal may adjust her contract over time in an online manner. Several other authors have considered extensions of the classic model to multiple agents. Levy and Vukina [2002] show that with multiple agents it is optimal to set individual linear contracts for each agent rather than a single uniform contract for all agents, but offer a variety of descriptive explanations for why it is more common to see uniform contracts in practice. Babaioff et al. [2006] consider a setting in which one principal interacts with multiple agents, but observes only a single outcome which is a function of all agents’ effort levels. Misra et al. [2012] consider a variant in which the algorithm must decide both how to set a uniform contract for many agents and how to select a subset of agents to hire.\nAlternative online versions of the problem have been considered in the literature as well. In dynamic principal agent problem [Sannikov, 2008, Williams, 2009, Sannikov, 2012], a single principal interacts with a single agent repeatedly over a period of time. The agent can choose to exert different effort at different time, and the outcome at time t is a function of all the efforts exerts by the agent before t. The principal cannot observe the agent’s efforts but can observe the outcome. The goal of the principal is to design an optimal contract over time to maximize his payoff. Our work is different from this line of work since we consider the setting with multiple agents with different, unknown types. Our algorithm needs to learn the distribution of agent types and design an optimal contract accordingly.\nConitzer and Garera [2006] studies the online principal agent problem with a similar setting to ours. However, they focus on empirically comparing different online algorithms, including bandit approaches with uniform discretization, gradient ascent, and Bayesian update approaches to the problem. Our goal is to provide an algorithm with nice theoretical guarantees.\nBohren and Kravitz [2013] studies the setting when the outcome is unverifiable. To address this issue, they propose to assign a bundle of tasks to each worker. To verify the outcome, each task in the bundle is chosen as a verifiable task with some non-trivial probability. A verifiable task can either be a gold standard task with known answer or a task which is assigned to multiple workers for verification. The payment for a task bundle is then conditional only on the outcome of verified tasks. In our setting, we assume the task outcome is verifiable. We can relax this assumption by adopting similar approaches.\nIncentives in crowdsourcing systems. Researchers have recently begun to examine the design of incentive mechanisms to encourage high-quality work in crowdsourcing systems. Jain et al. [2012] explore ways in which to award virtual points to users in online question-and-answer forums to improve the quality of answers. Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets. While we also consider crowdsourcing markets, our work differs in that it focuses on how to design contracts, perhaps the most natural incentive scheme, to incentivize workers to exert effort.\nThe problem closest to ours which has been studied in the context of crowdsourcing systems is the online task pricing problem in which a requester has an unlimited supply of tasks to be completed and a budget B to spend on them [Badanidiyuru et al., 2012, Singer and Mittal, 2013]. Workers with private costs arrive online, and the requester sets a single price for each arriving worker. The goal is to learn the optimal single fixed price over time. Our work can be viewed as a generalization of the task pricing problem, which is a special case of our setting with the number of non-null outcomes m fixed at 1.\nThere has also been empirical work examining how workers’ behavior varies based on the financial incentives offered in crowdsourcing markets. Mason and Watts [2009] study how workers react to changes of performance-independent financial incentives. In their study, increasing financial incentives increases the number of tasks workers complete, but not the quality of their output. Yin et al. [2013] provide a potential explanation for this phenomenon using the concept of “anchoring effect”: a worker’s cost for completing a task is influenced by the first price the worker sees for this task. Horton and Chilton [2010] run experiments to estimate workers’ reservation wage for completing tasks. They show that many workers respond rationally to offered contracts, whereas\nsome of the workers appeared to have some “target payment” in mind. Some recent research studies the effects of performance-based payments (PBPs). Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well. He concludes that the quality of work is better with PBPs than with uniform payments. Yin et al. [2013] show that varying the magnitude of the bonus does not have much effect in certain settings. Ho et al. [2015] perform a more comprehensive set of experiments aimed at determining whether, when, and why PBPs increase the quality of submitted work. Their results suggest that PBPs can increase quality on tasks for which increased time or effort leads to higher quality work. Their results also suggest that workers may interpret a contract as performance-based even if it is not stated as such (since requesters always have the option to reject work). Based on this evidence, they propose a new model of worker behavior that extends the principal-agent model to explicitly reflect workers’ subjective beliefs about their likelihood of being paid.\nOverall, previous empirical work demonstrates that workers in crowdsourcing markets do respond to the change of financial incentives, but that their behavior does not always follow the traditional rational-worker model — similar to people in any real-world market. In our work, we start our analysis with the rational-worker assumption ubiquitous in economic theory, but demonstrate that our results can still hold without these assumptions as long as the collective worker behavior satisfies some natural properties (namely, as long as Lemma 3.1 holds). We note that our results hold under the generalized worker model proposed by Ho et al. [2015], which is consistent with their experimental evidence as discussed above.\nSequential decision problems. In sequential decision problems, an algorithm makes sequential decisions over time.Two directions that are relevant to this paper are multi-armed bandits (MAB) and dynamic pricing.\nMAB have been studied since 1933 [Thompson, 1933] in Operations Research, Economics, and several branches of Computer Science including machine learning, theoretical computer science, AI, and algorithmic economics. A survey of prior work on MAB is beyond the scope of this paper; the reader is encouraged to refer to Cesa-Bianchi and Lugosi [2006] or Bubeck and Cesa-Bianchi [2012] for background on prior-independent MAB, and to Gittins et al. [2011] for background on Bayesian MAB. Below we briefly discuss the lines of work on MAB that are directly relevant to our paper.\nOur setting can be modeled as prior-independent MAB with stochastic rewards: the reward of a given arm i is an i.i.d. sample of some time-invariant distribution, and neither this distribution nor a Bayesian prior on it are known to the algorithm. The basic formulation (with a small number of arms) is well-understood [Lai and Robbins, 1985, Auer et al., 2002, Bubeck and Cesa-Bianchi, 2012]. To handle problems with a large or infinite number of arms, one typically needs side information on similarity between arms. A typical way to model this side information, called Lipschitz MAB Kleinberg et al. [2008], is that an algorithm is given a distance function on the arms, and the expected rewards are assumed to satisfy Lipschitz-continuity (or a relaxation thereof) with respect this distance function, e.g. [Agrawal, 1995, Kleinberg, 2004, Auer et al., 2007, Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014]. Most related to this paper is the idea of adaptive discretization which is often used in this setting [Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014], and particularly the zooming algorithm [Kleinberg et al., 2008, Slivkins, 2014]. In particular, the general template of our algorithm is similar to the one in the zooming algorithm (but our “selection rule” and “zooming rule” are very different, reflecting the lack of a priori known similarity information).\nIn some settings (including ours), the numerical similarity information required for Lipschitz\nMAB is not immediately available. For example, in applications to web search and advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007, Slivkins, 2011, Bull, 2013]. In particular, Slivkins [2011] and Bull [2013] explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy. In a different direction, Bubeck et al. [2011b] study a version of Lipschitz MAB where the Lipschitz constant is not known, and essentially recover the performance of NonAdaptive for this setting.\nDynamic pricing (a.k.a. online posted-price auctions) refers to settings in which a principal interacts with agents that arrive over time and offers each agent a price for a transaction, such as selling or buying an item. The version in which the principal sells items has been extensively studied in Operations Research, typically in a Bayesian setting; see den Boer [2015] for a through literature review. The study of prior-independent, non-parameterized formulations has been initiated in Blum et al. [2003] and Kleinberg and Leighton [2003a] and continued by several others [Besbes and Zeevi, 2009, Babaioff et al., 2015, Besbes and Zeevi, 2012, Wang et al., 2014, Badanidiyuru et al., 2013, 2014]. Further, Badanidiyuru et al. [2012] and Singla and Krause [2013] studied the version in which the principal buys items, or equivalently commissions tasks; we call this version dynamic task pricing. Modulo budget constraints, this is essentially the special case of our setting where in each round a worker is offered the chance to perform a task at a specified price, and can either accept or reject this offer. In particular, the worker’s strategic choice is directly observable. More general settings have been studied in [Badanidiyuru et al., 2013, 2014, Agrawal and Devanur, 2014, Agrawal et al., 2015].14 However, all this work (after the initial papers [Blum et al., 2003, Kleinberg and Leighton, 2003a]) has focused on models with constraints on the principal’s supply or budgets, and does not imply any improved results when specialized to unconstrained settings."
    }, {
      "heading" : "10 Conclusions",
      "text" : "Motivated by applications to crowdsourcing markets, we define the dynamic contract design problem, a multi-round version of the principal-agent model with unobservable strategic decisions. We treat this problem as a multi-armed bandit problem, design an algorithm for this problem, and derive regret bounds which compare favorably to prior work. Our main conceptual contribution, aside from identifying the model, is the adaptive discretization approach that does not rely on Lipschitz-continuity assumptions. We provably improve on the uniform discretization approach from prior work, both in the general case and in some illustrative special cases. These theoretical results are supported by simulations.\nWe believe that the dynamic contract design problem deserves further study, in several directions that we outline below.\n1. It is not clear whether our provable results can be improved, perhaps using substantially different algorithms and relative to different problem-specific structures. In particular, one needs to establish lower bounds in order to argue about optimality; no lower bounds for dynamic contract design are currently known.\n2. Our adaptive discretization approach may be fine-tuned to improve its performance in practice. In particular, the definition of the “index” It(C) of a given feasible cell C may be re-defined in\n14Badanidiyuru et al. [2014] and Agrawal and Devanur [2014] is concurrent and independent work with respect to the conference publication of this paper, and Agrawal et al. [2015] is subsequent work.\nseveral different ways. First, it can use the information from C in a more sophisticated way, similar to the more sophisticated indices for the basic K-armed bandit problem; for example, see Garivier and Cappé [2011]. Second, the index can incorporate information from other cells. Third, it can be defined in a “smoother”, probabilistic way, e.g., as in Thompson Sampling [Thompson, 1933].\n3. Deeper insights into the structure of the (static) principal-agent problem are needed, primarily in order to optimize the choice of Xcand, the set of candidate contracts. The most natural target here is the uniform mesh Xcand(ǫ). To optimize the granularity ǫ, one needs to upper-bound the discretization error OPT(Xcand)− OPT(Xcand(ǫ)) in terms of some function f(ǫ) such that f(ǫ) → 0 as ǫ → 0. The first-order open question is to resolve whether this can be done in the general case, or provide a specific example when it cannot. A related open question concerns the effect of increasing the granularity: upper-bound the difference OPT(Xcand(ǫ))− OPT(Xcand(ǫ′)), ǫ > ǫ′ > 0, in terms of some function of ǫ and ǫ′. Further, it is not known whether the optimal mesh of contracts is in fact a uniform mesh.\nAlso of interest is the effect of restricting our attention to monotone contracts. While we prove that monotone contracts may not be optimal (Appendix A), the significance of this phenomenon is unclear. One would like to characterize the scenarios when restricting to monotone contracts is alright (in the sense that the best monotone contract is as good, or not much worse, than the best contract), and the scenarios when this restriction results in a significant loss. For the latter scenarios, different algorithms may be needed.\n4. A much more extensive analysis of special cases is in order. Our general results are difficult to access (which appears to be an inherent property of the general problem), so the most immediate direction for special cases is deriving lucid corollaries from the current regret bounds. In particular, it is desirable to optimize the choice of candidate contracts. Apart from “massaging” the current results, one can also design improved algorithms and derive specialized lower bounds. Particularly appealing special cases concern supply distributions that are mixtures of a small number of types, and supply distributions that belong to a (simple) parameterized family with unknown parameter.\nGoing beyond our current model, a natural direction is to incorporate a budget constraint, extending the corresponding results on dynamic task pricing. The main difficulty for such settings is that a distribution over two contracts may perform much better than any fixed contract; see Badanidiyuru et al. [2013] for discussion. Effectively, an algorithm needs to optimize over the distributions. As a first step, one can use non-adaptive discretization in conjunction with the general algorithms for bandits with budget constraints (sometimes called “bandits with knapsacks” [Badanidiyuru et al., 2013, Agrawal and Devanur, 2014]). However, it is not clear how to choose an optimal mesh of contracts (as we discussed throughout the paper), and this mesh is not likely to be uniform (because it is not uniform for the special case of dynamic task pricing with a budget [Badanidiyuru et al., 2013]). The eventual target in this research direction is to marry adaptive discretization and the techniques from prior work on “bandits with knapsacks.”"
    }, {
      "heading" : "A Monotone contracts may not be optimal",
      "text" : "In this section we provide an example of a problem instance for which all monotone contracts are suboptimal (at least when restricting attention to only those contracts with non-negative payoffs). In this example, there are three non-null outcomes (i.e., m = 3), and two non-null effort levels, “low” effort and “high” effort, which we denote eℓ and eh respectively. There is only a single worker type. Since there is only one type, we drop the subscript when describing the cost function c. We let c(eℓ) = 0, and let c(eh) be any positive value less than 0.5(v(2)− v(1)). If a worker chooses low effort, the outcome is equally likely to be 1 or 3. If the worker chooses high effort, it is equally likely to be 2 or 3. It is easy to verify that this type satisfies the FOSD assumption. Finally, for simplicity, we assume that all workers break ties between high effort and any other effort level in favor of high effort, and that all workers break ties between low effort and the null effort level in favor of low effort.\nLet’s consider the optimal contract. Since there is just a single worker type and all workers of this type break ties in the same way, we can consider separately the best contract that would make all workers choose the null effort level, the best contract that would make all workers choose low effort, and the best contract that would make all workers choose high effort, and compare the requester’s expected value for each.\nSince c(eℓ) = 0 and workers break ties between low effort and null effort in favor of low effort, there is no contract that would cause workers to choose null effort; workers always prefer low effort to null effort.\nIt is easy to see that the best contract (in terms of requester expected value) that would make workers choose low effort would set x(1) = x(3) = 0 and x(2) sufficiently low that workers would not be enticed to choose high effort; setting x(2) = 0 is sufficient. In this case, the expected value of the requester would be 0.5(v(1) + v(3)).\nNow let’s consider contracts that cause workers to choose high effort. If a worker chooses high effort, the expected value to the requester is\n0.5(v(2) − x(2) + v(3) − x(3)). (23)\nWorkers will choose high effort if and only if\n0.5(x(1) + x(3)) ≤ 0.5(x(2) + x(3)) − c(eh)\nor 0.5x(1) ≤ 0.5x(2) − c(eh). (24)\nSo to find the contract that maximizes the requester’s expected value when workers choose high effort, we want to maximize Equation 23 subject to the constraint in Equation 24. Since x(3) doesn’t appear in Equation 24, we can set it to 0 to maximize Equation 23. Since x(1) does not appear in Equation 23, we can set x(1) = 0 to make Equation 24 as easy as possible to satisfy. We can then see that the optimal occurs when x(2) = 2c(eh).\nPlugging this contact x into Equation 23, the expected utility in this case is 0.5(v(2) + v(3))− c(eh). Since we assumed that c(eh) < 0.5(v(2) − v(1))), this is strictly preferable to the constant 0 contract, and is in fact the unique optimal contract. Since x(2) > x(3), the unique optimal contract is not monotonic."
    } ],
    "references" : [ {
      "title" : "The continuum-armed bandit problem",
      "author" : [ "Rajeev Agrawal" ],
      "venue" : "SIAM J. Control and Optimization,",
      "citeRegEx" : "Agrawal.,? \\Q1926\\E",
      "shortCiteRegEx" : "Agrawal.",
      "year" : 1926
    }, {
      "title" : "Bandits with concave rewards and convex knapsacks",
      "author" : [ "Shipra Agrawal", "Nikhil R. Devanur" ],
      "venue" : null,
      "citeRegEx" : "Agrawal and Devanur.,? \\Q1995\\E",
      "shortCiteRegEx" : "Agrawal and Devanur.",
      "year" : 1995
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : null,
      "citeRegEx" : "Auer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2015
    }, {
      "title" : "Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games",
      "author" : [ "Conitzer", "Nikesh Garera" ],
      "venue" : "Intl. Conf. on Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Conitzer and Garera.,? \\Q2013\\E",
      "shortCiteRegEx" : "Conitzer and Garera.",
      "year" : 2013
    }, {
      "title" : "Efficient optimal leanring for contextual bandits",
      "author" : [ "Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang" ],
      "venue" : "In 27th Conf. on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Dudik et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudik et al\\.",
      "year" : 2011
    }, {
      "title" : "The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond",
      "author" : [ "Aurélien Garivier", "Olivier Cappé" ],
      "venue" : "In 24th Conf. on Learning Theory (COLT),",
      "citeRegEx" : "Garivier and Cappé.,? \\Q2011\\E",
      "shortCiteRegEx" : "Garivier and Cappé.",
      "year" : 2011
    }, {
      "title" : "A game-theoretic analysis of rank-order mechanisms for user-generated content",
      "author" : [ "Arpita Ghosh", "Patrick Hummel" ],
      "venue" : "In EC,",
      "citeRegEx" : "Ghosh and Hummel.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ghosh and Hummel.",
      "year" : 2011
    }, {
      "title" : "Learning and incentives in user-generated content: Multi-armed bandits with endogenous arms",
      "author" : [ "Arpita Ghosh", "Patrick Hummel" ],
      "venue" : "In ICTS,",
      "citeRegEx" : "Ghosh and Hummel.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ghosh and Hummel.",
      "year" : 2013
    }, {
      "title" : "Incentivizing high-quality user-generated content",
      "author" : [ "Arpita Ghosh", "Preston McAfee" ],
      "venue" : "In WWW,",
      "citeRegEx" : "Ghosh and McAfee.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ghosh and McAfee.",
      "year" : 2011
    }, {
      "title" : "Multi-Armed Bandit Allocation Indices",
      "author" : [ "John Gittins", "Kevin Glazebrook", "Richard Weber" ],
      "venue" : null,
      "citeRegEx" : "Gittins et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gittins et al\\.",
      "year" : 2011
    }, {
      "title" : "You’re hired! an examination of crowdsourcing incentive models in human resource tasks",
      "author" : [ "Christopher G. Harris" ],
      "venue" : "In CSDM,",
      "citeRegEx" : "Harris.,? \\Q2011\\E",
      "shortCiteRegEx" : "Harris.",
      "year" : 2011
    }, {
      "title" : "Towards social norm design for crowdsourcing markets",
      "author" : [ "Chien-Ju Ho", "Yu Zhang", "Jennifer Wortman Vaughan", "Mihaela van der Schaar" ],
      "venue" : "HCOMP,",
      "citeRegEx" : "Ho et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2012
    }, {
      "title" : "Incentivizing high quality crowdwork",
      "author" : [ "Chien-Ju Ho", "Aleksandrs Slivkins", "Siddharth Suri", "Jennifer Wortman Vaughan" ],
      "venue" : "In 24th Intl. World Wide Web Conf. (WWW),",
      "citeRegEx" : "Ho et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2015
    }, {
      "title" : "The labor economics of paid crowdsourcing",
      "author" : [ "John J. Horton", "Lydia B. Chilton" ],
      "venue" : "In EC,",
      "citeRegEx" : "Horton and Chilton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Horton and Chilton.",
      "year" : 2010
    }, {
      "title" : "Designing incentives for online question-and-answer forums",
      "author" : [ "Shaili Jain", "Yiling Chen", "David Parkes" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Jain et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2012
    }, {
      "title" : "Nearly tight bounds for the continuum-armed bandit problem",
      "author" : [ "Robert Kleinberg" ],
      "venue" : "In 18th Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Kleinberg.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kleinberg.",
      "year" : 2004
    }, {
      "title" : "The value of knowing a demand curve: Bounds on regret for online posted-price auctions",
      "author" : [ "Robert Kleinberg", "Tom Leighton" ],
      "venue" : "IEEE Symp. on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Kleinberg and Leighton.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kleinberg and Leighton.",
      "year" : 2003
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal" ],
      "venue" : "In 40th ACM Symp. on Theory of Computing (STOC),",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "The value of knowing a demand curve: Bounds on regret for online posted-price auctions",
      "author" : [ "Robert D. Kleinberg", "Frank T. Leighton" ],
      "venue" : "In IEEE Symp. on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Kleinberg and Leighton.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kleinberg and Leighton.",
      "year" : 2003
    }, {
      "title" : "Bandit Based Monte-Carlo Planning",
      "author" : [ "Levente Kocsis", "Csaba Szepesvari" ],
      "venue" : "European Conf. on Machine Learning (ECML),",
      "citeRegEx" : "Kocsis and Szepesvari.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kocsis and Szepesvari.",
      "year" : 2006
    }, {
      "title" : "The Theory of Incentives: The Principal-Agent Model",
      "author" : [ "Jean-Jacques Laffont", "David Martimort" ],
      "venue" : null,
      "citeRegEx" : "Laffont and Martimort.,? \\Q2002\\E",
      "shortCiteRegEx" : "Laffont and Martimort.",
      "year" : 2002
    }, {
      "title" : "Asymptotically efficient Adaptive Allocation Rules",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "Optimal linear contracts with heterogeneous agents",
      "author" : [ "Armando Levy", "Tomislav Vukina" ],
      "venue" : "In European Review of Agricultural Economics,",
      "citeRegEx" : "Levy and Vukina.,? \\Q2002\\E",
      "shortCiteRegEx" : "Levy and Vukina.",
      "year" : 2002
    }, {
      "title" : "Financial incentives and the “performance of crowds",
      "author" : [ "Winter Mason", "Duncan Watts" ],
      "venue" : "HCOMP,",
      "citeRegEx" : "Mason and Watts.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mason and Watts.",
      "year" : 2009
    }, {
      "title" : "Reputation-based incentive protocols in crowdsourcing applications",
      "author" : [ "Yu Zhang", "Mihaela van der Schaar" ],
      "venue" : null,
      "citeRegEx" : "Zhang and Schaar.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang and Schaar.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "First, our model can be viewed as a multi-round version of the classical principal-agent model from contract theory [Laffont and Martimort, 2002].",
      "startOffset" : 116,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "The closest line of work is that on Lipschitz MAB [Kleinberg et al., 2008], in which the algorithm is given a distance function on the arms, and the expected rewards of the arms are assumed to satisfy Lipschitz-continuity (or a relaxation thereof) with respect to this distance function, [Agrawal, 1995, Kleinberg, 2004, Auer et al.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "[Kleinberg and Leighton, 2003b, Badanidiyuru et al., 2012, Singer and Mittal, 2013, Singla and Krause, 2013, Badanidiyuru et al., 2013]. In particular, Badanidiyuru et al. [2012] and Singla and Krause [2013] study a version of our setting with simple, single-price contracts (independent of the output), where the focus is on dealing with a global budget constraint.",
      "startOffset" : 1,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "[Kleinberg and Leighton, 2003b, Badanidiyuru et al., 2012, Singer and Mittal, 2013, Singla and Krause, 2013, Badanidiyuru et al., 2013]. In particular, Badanidiyuru et al. [2012] and Singla and Krause [2013] study a version of our setting with simple, single-price contracts (independent of the output), where the focus is on dealing with a global budget constraint.",
      "startOffset" : 1,
      "endOffset" : 208
    }, {
      "referenceID" : 20,
      "context" : "As described above, this is a version of the standard principal-agent model [Laffont and Martimort, 2002].",
      "startOffset" : 76,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "The special case m = 1 is equivalent to the dynamic pricing problem from Kleinberg and Leighton [2003a]; we obtain improved results for it, too.",
      "startOffset" : 73,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : ", Dudik et al. [2011].",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "The width dimension is similar to the “zooming dimension” in Kleinberg et al. [2008] and “near-optimality dimension” in Bubeck et al.",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "The width dimension is similar to the “zooming dimension” in Kleinberg et al. [2008] and “near-optimality dimension” in Bubeck et al. [2011a] in the work on “bandits in metric spaces”.",
      "startOffset" : 61,
      "endOffset" : 142
    }, {
      "referenceID" : 15,
      "context" : "If an algorithm is given this function D (call such algorithm D-aware), the machinery from “bandits in metric spaces” Kleinberg et al. [2008], Bubeck et al.",
      "startOffset" : 118,
      "endOffset" : 142
    }, {
      "referenceID" : 15,
      "context" : "If an algorithm is given this function D (call such algorithm D-aware), the machinery from “bandits in metric spaces” Kleinberg et al. [2008], Bubeck et al. [2011a] can be used to perform adaptive discretization and obtain a significant advantage over NonAdaptive.",
      "startOffset" : 118,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "Given this shape of D, let us state the regret bounds for D-aware algorithms in Kleinberg et al. [2008] and Bubeck et al.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "Given this shape of D, let us state the regret bounds for D-aware algorithms in Kleinberg et al. [2008] and Bubeck et al. [2011a]. To simplify the notation, we assume that the action space is restricted to Xcand.",
      "startOffset" : 80,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "For a more precise comparison, we focus on the results in Kleinberg et al. [2008] .",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "For a more precise comparison, we focus on the results in Kleinberg et al. [2008] . (The regret bounds in Bubeck et al. [2011a] are very similar in spirit, but are stated in terms of a slightly different structure.",
      "startOffset" : 58,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "For a more precise comparison, we focus on the results in Kleinberg et al. [2008] . (The regret bounds in Bubeck et al. [2011a] are very similar in spirit, but are stated in terms of a slightly different structure.) The “covering-type” regret bound in Kleinberg et al. [2008] focuses on balls of radius at most ǫ according to distance D, so that N∗ ǫ (Y ) is the smallest number of such balls that is sufficient to cover Y .",
      "startOffset" : 58,
      "endOffset" : 276
    }, {
      "referenceID" : 15,
      "context" : "We can upper-bound the discretization error using a standard approach from the work on dynamic pricing Kleinberg and Leighton [2003b]. Fix discretization granularity ψ > 0.",
      "startOffset" : 103,
      "endOffset" : 134
    }, {
      "referenceID" : 15,
      "context" : "Worst-case regret bounds are implicit in prior work on dynamic inventory-pricing [Kleinberg and Leighton, 2003a].13 Let NonAdaptive(ψ) denote algorithm NonAdaptive with Xcand = Xcand(ψ). Then, by the analysis in Kleinberg and Leighton [2003a], NonAdaptive(ψ) achieves regret R(T ) = Õ(ψT + ψ−2).",
      "startOffset" : 82,
      "endOffset" : 243
    }, {
      "referenceID" : 15,
      "context" : "The algorithmic result for dynamic task pricing is an easy modification of the analysis in Kleinberg and Leighton [2003a] for dynamic inventory-pricing.",
      "startOffset" : 91,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : "The algorithmic result for dynamic task pricing is an easy modification of the analysis in Kleinberg and Leighton [2003a] for dynamic inventory-pricing. The lower bound in in Kleinberg and Leighton [2003a] can also be “translated” from dynamic inventory-pricing to dynamic task pricing without introducing any new ideas.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 20,
      "context" : "Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002].",
      "startOffset" : 98,
      "endOffset" : 127
    }, {
      "referenceID" : 20,
      "context" : "Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002].",
      "startOffset" : 124,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002]. In the most basic version of the classic model, a single principal interacts with a single agent whose type (specified by a cost function and production function, as described in Section 2) is generally assumed to be known. The principal specifies a contract mapping outcomes to payments that the principal commits to make to the agent. The agent then chooses an action (i.e., effort level) that stochastically results in an outcome in order to maximize his expected utility given the contract. The principal observes the outcome, but cannot directly observe the agent’s effort level, creating a moral hazard problem. The goal of the principal is to design a contract to maximize her own expected utility, which is the difference between the utility she receives from the outcome and the payment she makes. This maximization can be written as a constrained optimization problem, and it can be shown that linear contracts are optimal. The adverse selection variation of the principal-agent problem relaxes the assumption that the agent’s type is known. Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002]. In this setting, the principal offers a menu of contracts, and the contract chosen by the agent reveals the agent’s type. The problem of selecting a menu of contracts that maximizes the principal’s expected utility can again be formulated as a constrained optimization. Our work differs from the classic setting in that we consider a principal interacting with multiple agents, and the principal may adjust her contract over time in an online manner. Several other authors have considered extensions of the classic model to multiple agents. Levy and Vukina [2002] show that with multiple agents it is optimal to set individual linear contracts for each agent rather than a single uniform contract for all agents, but offer a variety of descriptive explanations for why it is more common to see uniform contracts in practice.",
      "startOffset" : 99,
      "endOffset" : 1900
    }, {
      "referenceID" : 20,
      "context" : "Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002]. In the most basic version of the classic model, a single principal interacts with a single agent whose type (specified by a cost function and production function, as described in Section 2) is generally assumed to be known. The principal specifies a contract mapping outcomes to payments that the principal commits to make to the agent. The agent then chooses an action (i.e., effort level) that stochastically results in an outcome in order to maximize his expected utility given the contract. The principal observes the outcome, but cannot directly observe the agent’s effort level, creating a moral hazard problem. The goal of the principal is to design a contract to maximize her own expected utility, which is the difference between the utility she receives from the outcome and the payment she makes. This maximization can be written as a constrained optimization problem, and it can be shown that linear contracts are optimal. The adverse selection variation of the principal-agent problem relaxes the assumption that the agent’s type is known. Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002]. In this setting, the principal offers a menu of contracts, and the contract chosen by the agent reveals the agent’s type. The problem of selecting a menu of contracts that maximizes the principal’s expected utility can again be formulated as a constrained optimization. Our work differs from the classic setting in that we consider a principal interacting with multiple agents, and the principal may adjust her contract over time in an online manner. Several other authors have considered extensions of the classic model to multiple agents. Levy and Vukina [2002] show that with multiple agents it is optimal to set individual linear contracts for each agent rather than a single uniform contract for all agents, but offer a variety of descriptive explanations for why it is more common to see uniform contracts in practice. Babaioff et al. [2006] consider a setting in which one principal interacts with multiple agents, but observes only a single outcome which is a function of all agents’ effort levels.",
      "startOffset" : 99,
      "endOffset" : 2184
    }, {
      "referenceID" : 20,
      "context" : "Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002]. In the most basic version of the classic model, a single principal interacts with a single agent whose type (specified by a cost function and production function, as described in Section 2) is generally assumed to be known. The principal specifies a contract mapping outcomes to payments that the principal commits to make to the agent. The agent then chooses an action (i.e., effort level) that stochastically results in an outcome in order to maximize his expected utility given the contract. The principal observes the outcome, but cannot directly observe the agent’s effort level, creating a moral hazard problem. The goal of the principal is to design a contract to maximize her own expected utility, which is the difference between the utility she receives from the outcome and the payment she makes. This maximization can be written as a constrained optimization problem, and it can be shown that linear contracts are optimal. The adverse selection variation of the principal-agent problem relaxes the assumption that the agent’s type is known. Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002]. In this setting, the principal offers a menu of contracts, and the contract chosen by the agent reveals the agent’s type. The problem of selecting a menu of contracts that maximizes the principal’s expected utility can again be formulated as a constrained optimization. Our work differs from the classic setting in that we consider a principal interacting with multiple agents, and the principal may adjust her contract over time in an online manner. Several other authors have considered extensions of the classic model to multiple agents. Levy and Vukina [2002] show that with multiple agents it is optimal to set individual linear contracts for each agent rather than a single uniform contract for all agents, but offer a variety of descriptive explanations for why it is more common to see uniform contracts in practice. Babaioff et al. [2006] consider a setting in which one principal interacts with multiple agents, but observes only a single outcome which is a function of all agents’ effort levels. Misra et al. [2012] consider a variant in which the algorithm must decide both how to set a uniform contract for many agents and how to select a subset of agents to hire.",
      "startOffset" : 99,
      "endOffset" : 2363
    }, {
      "referenceID" : 3,
      "context" : "Conitzer and Garera [2006] studies the online principal agent problem with a similar setting to ours.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "Conitzer and Garera [2006] studies the online principal agent problem with a similar setting to ours. However, they focus on empirically comparing different online algorithms, including bandit approaches with uniform discretization, gradient ascent, and Bayesian update approaches to the problem. Our goal is to provide an algorithm with nice theoretical guarantees. Bohren and Kravitz [2013] studies the setting when the outcome is unverifiable.",
      "startOffset" : 0,
      "endOffset" : 393
    }, {
      "referenceID" : 8,
      "context" : "Jain et al. [2012] explore ways in which to award virtual points to users in online question-and-answer forums to improve the quality of answers.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.",
      "startOffset" : 0,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets.",
      "startOffset" : 0,
      "endOffset" : 257
    }, {
      "referenceID" : 6,
      "context" : "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets.",
      "startOffset" : 0,
      "endOffset" : 293
    }, {
      "referenceID" : 6,
      "context" : "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets. While we also consider crowdsourcing markets, our work differs in that it focuses on how to design contracts, perhaps the most natural incentive scheme, to incentivize workers to exert effort. The problem closest to ours which has been studied in the context of crowdsourcing systems is the online task pricing problem in which a requester has an unlimited supply of tasks to be completed and a budget B to spend on them [Badanidiyuru et al., 2012, Singer and Mittal, 2013]. Workers with private costs arrive online, and the requester sets a single price for each arriving worker. The goal is to learn the optimal single fixed price over time. Our work can be viewed as a generalization of the task pricing problem, which is a special case of our setting with the number of non-null outcomes m fixed at 1. There has also been empirical work examining how workers’ behavior varies based on the financial incentives offered in crowdsourcing markets. Mason and Watts [2009] study how workers react to changes of performance-independent financial incentives.",
      "startOffset" : 0,
      "endOffset" : 1402
    }, {
      "referenceID" : 6,
      "context" : "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets. While we also consider crowdsourcing markets, our work differs in that it focuses on how to design contracts, perhaps the most natural incentive scheme, to incentivize workers to exert effort. The problem closest to ours which has been studied in the context of crowdsourcing systems is the online task pricing problem in which a requester has an unlimited supply of tasks to be completed and a budget B to spend on them [Badanidiyuru et al., 2012, Singer and Mittal, 2013]. Workers with private costs arrive online, and the requester sets a single price for each arriving worker. The goal is to learn the optimal single fixed price over time. Our work can be viewed as a generalization of the task pricing problem, which is a special case of our setting with the number of non-null outcomes m fixed at 1. There has also been empirical work examining how workers’ behavior varies based on the financial incentives offered in crowdsourcing markets. Mason and Watts [2009] study how workers react to changes of performance-independent financial incentives. In their study, increasing financial incentives increases the number of tasks workers complete, but not the quality of their output. Yin et al. [2013] provide a potential explanation for this phenomenon using the concept of “anchoring effect”: a worker’s cost for completing a task is influenced by the first price the worker sees for this task.",
      "startOffset" : 0,
      "endOffset" : 1637
    }, {
      "referenceID" : 6,
      "context" : "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets. While we also consider crowdsourcing markets, our work differs in that it focuses on how to design contracts, perhaps the most natural incentive scheme, to incentivize workers to exert effort. The problem closest to ours which has been studied in the context of crowdsourcing systems is the online task pricing problem in which a requester has an unlimited supply of tasks to be completed and a budget B to spend on them [Badanidiyuru et al., 2012, Singer and Mittal, 2013]. Workers with private costs arrive online, and the requester sets a single price for each arriving worker. The goal is to learn the optimal single fixed price over time. Our work can be viewed as a generalization of the task pricing problem, which is a special case of our setting with the number of non-null outcomes m fixed at 1. There has also been empirical work examining how workers’ behavior varies based on the financial incentives offered in crowdsourcing markets. Mason and Watts [2009] study how workers react to changes of performance-independent financial incentives. In their study, increasing financial incentives increases the number of tasks workers complete, but not the quality of their output. Yin et al. [2013] provide a potential explanation for this phenomenon using the concept of “anchoring effect”: a worker’s cost for completing a task is influenced by the first price the worker sees for this task. Horton and Chilton [2010] run experiments to estimate workers’ reservation wage for completing tasks.",
      "startOffset" : 0,
      "endOffset" : 1858
    }, {
      "referenceID" : 10,
      "context" : "Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well. He concludes that the quality of work is better with PBPs than with uniform payments. Yin et al. [2013] show that varying the magnitude of the bonus does not have much effect in certain settings.",
      "startOffset" : 0,
      "endOffset" : 214
    }, {
      "referenceID" : 10,
      "context" : "Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well. He concludes that the quality of work is better with PBPs than with uniform payments. Yin et al. [2013] show that varying the magnitude of the bonus does not have much effect in certain settings. Ho et al. [2015] perform a more comprehensive set of experiments aimed at determining whether, when, and why PBPs increase the quality of submitted work.",
      "startOffset" : 0,
      "endOffset" : 323
    }, {
      "referenceID" : 10,
      "context" : "Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well. He concludes that the quality of work is better with PBPs than with uniform payments. Yin et al. [2013] show that varying the magnitude of the bonus does not have much effect in certain settings. Ho et al. [2015] perform a more comprehensive set of experiments aimed at determining whether, when, and why PBPs increase the quality of submitted work. Their results suggest that PBPs can increase quality on tasks for which increased time or effort leads to higher quality work. Their results also suggest that workers may interpret a contract as performance-based even if it is not stated as such (since requesters always have the option to reject work). Based on this evidence, they propose a new model of worker behavior that extends the principal-agent model to explicitly reflect workers’ subjective beliefs about their likelihood of being paid. Overall, previous empirical work demonstrates that workers in crowdsourcing markets do respond to the change of financial incentives, but that their behavior does not always follow the traditional rational-worker model — similar to people in any real-world market. In our work, we start our analysis with the rational-worker assumption ubiquitous in economic theory, but demonstrate that our results can still hold without these assumptions as long as the collective worker behavior satisfies some natural properties (namely, as long as Lemma 3.1 holds). We note that our results hold under the generalized worker model proposed by Ho et al. [2015], which is consistent with their experimental evidence as discussed above.",
      "startOffset" : 0,
      "endOffset" : 1608
    }, {
      "referenceID" : 7,
      "context" : "A survey of prior work on MAB is beyond the scope of this paper; the reader is encouraged to refer to Cesa-Bianchi and Lugosi [2006] or Bubeck and Cesa-Bianchi [2012] for background on prior-independent MAB, and to Gittins et al. [2011] for background on Bayesian MAB.",
      "startOffset" : 215,
      "endOffset" : 237
    }, {
      "referenceID" : 1,
      "context" : "The basic formulation (with a small number of arms) is well-understood [Lai and Robbins, 1985, Auer et al., 2002, Bubeck and Cesa-Bianchi, 2012]. To handle problems with a large or infinite number of arms, one typically needs side information on similarity between arms. A typical way to model this side information, called Lipschitz MAB Kleinberg et al. [2008], is that an algorithm is given a distance function on the arms, and the expected rewards are assumed to satisfy Lipschitz-continuity (or a relaxation thereof) with respect this distance function, e.",
      "startOffset" : 95,
      "endOffset" : 362
    }, {
      "referenceID" : 19,
      "context" : "For example, in applications to web search and advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007, Slivkins, 2011, Bull, 2013]. In particular, Slivkins [2011] and Bull [2013] explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy.",
      "startOffset" : 150,
      "endOffset" : 286
    }, {
      "referenceID" : 19,
      "context" : "For example, in applications to web search and advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007, Slivkins, 2011, Bull, 2013]. In particular, Slivkins [2011] and Bull [2013] explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy.",
      "startOffset" : 150,
      "endOffset" : 302
    }, {
      "referenceID" : 19,
      "context" : "For example, in applications to web search and advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007, Slivkins, 2011, Bull, 2013]. In particular, Slivkins [2011] and Bull [2013] explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy. In a different direction, Bubeck et al. [2011b] study a version of Lipschitz MAB where the Lipschitz constant is not known, and essentially recover the performance of NonAdaptive for this setting.",
      "startOffset" : 150,
      "endOffset" : 439
    }, {
      "referenceID" : 13,
      "context" : "[2003] and Kleinberg and Leighton [2003a] and continued by several others [Besbes and Zeevi, 2009, Babaioff et al.",
      "startOffset" : 11,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "[2003] and Kleinberg and Leighton [2003a] and continued by several others [Besbes and Zeevi, 2009, Babaioff et al., 2015, Besbes and Zeevi, 2012, Wang et al., 2014, Badanidiyuru et al., 2013, 2014]. Further, Badanidiyuru et al. [2012] and Singla and Krause [2013] studied the version in which the principal buys items, or equivalently commissions tasks; we call this version dynamic task pricing.",
      "startOffset" : 11,
      "endOffset" : 235
    }, {
      "referenceID" : 13,
      "context" : "[2003] and Kleinberg and Leighton [2003a] and continued by several others [Besbes and Zeevi, 2009, Babaioff et al., 2015, Besbes and Zeevi, 2012, Wang et al., 2014, Badanidiyuru et al., 2013, 2014]. Further, Badanidiyuru et al. [2012] and Singla and Krause [2013] studied the version in which the principal buys items, or equivalently commissions tasks; we call this version dynamic task pricing.",
      "startOffset" : 11,
      "endOffset" : 264
    }, {
      "referenceID" : 0,
      "context" : "[2014] and Agrawal and Devanur [2014] is concurrent and independent work with respect to the conference publication of this paper, and Agrawal et al.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "[2014] and Agrawal and Devanur [2014] is concurrent and independent work with respect to the conference publication of this paper, and Agrawal et al. [2015] is subsequent work.",
      "startOffset" : 11,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "First, it can use the information from C in a more sophisticated way, similar to the more sophisticated indices for the basic K-armed bandit problem; for example, see Garivier and Cappé [2011]. Second, the index can incorporate information from other cells.",
      "startOffset" : 167,
      "endOffset" : 193
    } ],
    "year" : 2014,
    "abstractText" : "Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task’s requester, and may be adjusted based on the quality of the completed work, for example, through the use of “bonus” payments. In this paper, we study the requester’s problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each “arm” representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing. ACM Categories and subject descriptors: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems; F.1.2 [Computation by Abstract Devices]: Modes of Computation—Online computation; J.4 [Social and Behavioral Sciences]: Economics",
    "creator" : "gnuplot 4.2 patchlevel 6 "
  }
}