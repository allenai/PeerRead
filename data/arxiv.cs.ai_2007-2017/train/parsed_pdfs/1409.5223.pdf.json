{
  "name" : "1409.5223.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Why Local Search Excels in Expression Simplification",
    "authors" : [ "Ben Ruijl", "Aske Plaat", "Jos Vermaseren", "Jaap van den Herik" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Expression simplifcation, Horner, Stochastic Local Search, Simulated Annealing, MCTS\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "S IMPLIFYING large expressions is important to makenumerical integration tractable. In high energy physics, expressions with millions of terms arise from the calculation of Feynman diagrams. The results are used to compare predictions of Quantum Field Theory to the outcomes of particle collision experiments, for example at CERN or Fermilab. Numerical integration for currentgeneration processes and for next-generation processes required for, e.g., the International Linear Collider will take months. Thus it is important to save computation time by simplifying the underlying expressions.\nThe standard method for reducing the number of operations uses Horner’s rule of lifting variables outside brackets. This reduces the number of multiplications [1], [2]. Afterwards, common subexpression elimination may be applied to reduce the number of operations even further [3]. For a multivariate polynomial, the order in which variables are lifted outside brackets is called a Horner scheme. The problem of finding an optimal Horner scheme is NP-hard [4].\nRecent successes with Monte Carlo Tree Search (MCTS) [5] have shown that the number of operations of expressions can be reduced by at least a factor of 16 for a set of large, real-world, expressions [2]. In MCTS a tree is built selectively over N iterations, expanding only branches that are deemed worthwhile. However, this method introduces an additional parameter, Cp, that governs the amount of exploration versus exploitation\n• Ben Ruijl and Jos Vermaseren are with Nikhef, Science Park 105, 1098 XG Amsterdam, The Netherlands • Ben Ruijl, Aske Plaat and Jaap van den Herik are with Leiden University, Niels Bohrweg 1, 2333 CA Leiden, The Netherlands\nand that has to be tuned manually. The suggested algorithm SA-UCT (Simulated Annealing - Upper Confidence Bounds applied to Trees) alleviates the tuning problem, but does not eliminate it [6].\nIn this work we study the state space properties of Horner schemes and will find that the state space is relatively flat and has only a few local minima. This finding will have many consequences. The main result is that a basic algorithm such as Stochastic Local Search (SLS) may be more suited to the Horner scheme problem than more complex algorithms with many tunable parameters. The idea is surprising, since the problem of finding suitable Horner schemes has at least three complications: first, it is NP-hard, second, we do not know of any local heuristics to guide the search, and third, evaluation of a single state is slow; i.e., it takes several seconds for some of our benchmark expressions.\nThe main contributions of this work are fourfold: (1) it shows a real-world domain where basic stochastic search is more suited than many-parameter search algorithms, (2) it shows that SLS is at least 10 times faster than a manually fine-tuned MCTS, (3) it shows that the Horner scheme state space is flat, and (4) that it usually has only a few local minima.\nOur algorithms are implemented in the next release of the open source symbolic manipulation system FORM [7].\nThe paper is structured as follows. Section 2 gives an informative background on simplification methods, section 3 provides an overview of related algorithms, and section 4 gives the experimental setup. Next, section 5 presents the results of SLS versus SA, provides a good neighborhood structure, measures state space properties and compares the performance of SLS to MCTS. Section 6 formulates the conclusion. Section 7 contains a dis-\nar X\niv :1\n40 9.\n52 23\nv1 [\ncs .A\nI] 1\n8 Se\np 20\n14\n2 cussion and gives a vista on finding other methods of reductions."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "To provide some background, we will now describe two methods to reduce the number of operations, namely Horner schemes and common subexpression elimination [6], [8], followed by some remark on their interplay and the scaling of the computation time."
    }, {
      "heading" : "2.1 Horner schemes",
      "text" : "Horner’s rule is a classic method to reduce the number of multiplications in a polynomial by lifting variables outside brackets [1], [9]. In multivariate polynomials, the order in which the variables are lifted outside brackets is called a Horner scheme. For instance:\nx2z + x3y + x3yz → x2(z + x(y(1 + z))) (1)\nHere, first the variable x is extracted (i.e., x2 and x) and second, the variable y. The number of multiplications is now reduced from 9 to 4. However, the order x, y is chosen arbitrarily. One could also try the order y, x:\nx2z + x3y + x3yz → x2z + y(x3(1 + z)) (2)\nfor which the number of multiplications is 6. Evidently, this is a suboptimal Horner scheme. There are n! orders of extracting variables, where n is the number of variables. The problem of selecting an optimal ordering is NP-hard [4].\nUsually, a heuristic is used that orders the variables according to their frequency in the terms. This heuristic is called “occurrence order”. However, MCTS, which does not use this heuristic, is able to find Horner schemes that reduce the number of operations by at least 50% more than the occurrence order on large test expressions (number of operations up to 7 722 027) [2]."
    }, {
      "heading" : "2.2 Common subexpression elimination",
      "text" : "The number of operations can be reduced further by applying common subexpression elimination (CSEE). This method is well known in the field of compiler construction [3], where it is applied to much smaller expressions than in high energy physics, and in the field of computer chess [10] where it handles the occurrence of common subtrees by using transposition tables. Figure 1 shows an example of a common subexpression in a tree representation of an expression. The shaded subexpression b(a+ e) appears twice, and its removal means removing one superfluous addition and one multiplication.\nCSEE is able to reduce both the number of multiplications and the number of additions, whereas Horner schemes are only able to reduce the number of multiplications.\n+\n×\nb +\na e\n×\nb c +\na e\nFigure 1: A common subexpression (shaded) in an associative and commutative tree representation."
    }, {
      "heading" : "2.3 Interplay",
      "text" : "We note that there is an interplay between Horner schemes and CSEE: a certain “optimal” Horner scheme may reduce the number of multiplications the most, but may expose less common subexpressions than a “mediocre” Horner scheme. Thus, we need to find a way to obtain a Horner scheme that reduces the number of operations the most after both Horner and CSEE have been applied.\nFinding appropriate Horner schemes is not a trivial task, for at least four reasons. First, there are no known local heuristics. For the Traveling Salesman Problem (TSP), the distance between two cities can be used as a heuristic [11], and more specialized heuristics are able to solve symmetric TSP instances with thousands of cities (a historic example is a TSP with 7397 cities [12], [13]). Second, the Horner scheme is applied to an expression. This means that the scheme has a particular context: the nth entry applies to the subexpressions that are created after the first n − 1 entries in the Horner scheme have been applied to the expression. Third, Horner schemes are asymmetric: a scheme has a well-defined beginning and end. This is, e.g., in contrast with TSPs with closed paths (the most common subclass), since they have circle symmetry (translation and mirror symmetry). Fourth, the evaluation of a Horner scheme and CSEE is slow: for some benchmark expressions the evaluation took multiple seconds on a 2.4 GHz computer (see table 1). Our attempted parallelization of the evaluation function was unsuccessful, since the Horner scheme evaluation function is too fine-grained."
    }, {
      "heading" : "2.4 Scaling",
      "text" : "The time it takes to apply a Horner scheme is directly related to the number of variables and the number of terms in the expression. The common subexpression elimination time scales linearly with the number of operations. The difficulty of finding a good Horner scheme is related to the size of the permutation space, i.e., related to the number of variables, but also to the distribution of the variables in the terms. The composition of the variables affects the flatness of the state space and the occurrence of saddle points and local minima, as we shall see in section 5.3.\n3 In [2], and [6], Monte Carlo Tree Search (MCTS) has been successfully used to find a best candidate of all available Horner schemes. In this work we (re)consider which algorithm is best suited for the Horner scheme problem. We discuss six candidate algorithms for the optimization of Horner schemes."
    }, {
      "heading" : "3 RELATED ALGORITHMS",
      "text" : "The Horner scheme problem belongs to the class of permutation problems. Many algorithms for optimizing permutation problems have been suggested in the literature. In order to determine which of these algorithms is best suited for Horner schemes, we briefly discuss the characteristics of a selection of six frequently used algorithms: (Stochastic) Local Search [14], Simulated Annealing [15], Tabu Search [11], Ant Colony Optimization [16], Evolutionary Algorithms [17], and, Monte Carlo Tree Search [5]. Below they are indicated by A to F.\nWe begin to define the concept of a neighbor as it occurs in our problem. The state space of Horner schemes consists of the collection of all possible schemes (a permutation space). If we define transitions between states, then we are able to use graph algorithms to search the state space. All the nodes connected to the current node are called neighbors. There are many options for neighborhood structures (see section 5.2). In the discussion below we refer to the states reached after swapping two variables in the Horner scheme as a “neighbor” (see figure 3). Given n variables, there are n(n − 1)/2 neighbors.\n(A) Local Search [18], [14] is a state space exploration method that starts from an initial state and moves to a neighbor of the current state. The task is to find an extreme value. A full local search explores the values of all neighbors and then moves to the neighbor with the best value. For our domain this is impractical, since the number of neighbors is high and a single evaluation takes multiple seconds. A Stochastic Local Search (SLS) [14] randomly selects a neighbor and moves if the value of the given neighbor is better than the current value. SLS has two parameters: the number of iterations, N , and a neighborhood structure, that defines the transition function. With these characteristics, SLS is a candidate for further research.\n(B) Simulated Annealing (SA) [15] is a classic method that is inspired by the removal of crystal defects by the cooling of metals (annealing). It can be viewed as a generalization of Stochastic Local Search, where SA allows transitions to worse states. SA has several parameters, such as the starting temperature, final temperature, and cooling rate. The major difference between SLS and Simulated Annealing is that SA has the ability to escape local minima, whereas SLS, once in such a situation, will remain stuck permanently. Many papers have been published about the tuning of the SA parameters. For instance, in [19] and [20] suggestions have been made to tune the initial temperature. In section 5.1 we measure the performance of SA compared to SLS.\n(C) Tabu Search performs a local search and keeps track of previous visits [11]. Revisits are (temporarily) disallowed, which allows the search to escape local minima. To improve performance, Tabu Search can be enhanced with short-term, intermediate-term, and longterm memory, at the cost of introducing parameters that have to be tuned manually. In classic Tabu Search, the best neighbor that has not been visited earlier is selected and is added to the tabu list [11]. However, as mentioned above, evaluating all neighbors is impractical. A Tabu Search that does not try all neighbors introduces problems, since unexplored paths to the global minimum cannot be made anymore through states in the tabu list. Tabu Search may be convenient to escape from deep local minima, but as we will explain in subsection 5.3.1, we do not encounter these in our search space. Preliminary tests with a basic Tabu Search and only short-term memory did not yield better results. Therefore, we may conclude that the potential benefit (e.g., escaping from deep local minima) does not outweigh the inclusion of additional parameters. Thus, Tabu Search is not a candidate for further research.\n(D) Ant Colony Optimization [16] has been successful in optimizing various problems, such as TSP instances. The algorithm has two components: a history component, using information from the performance of previous paths, and a heuristic component that prefers transitions to neighbors that are closest. In TSP the heuristic component could be the distance between two cities. For Horner schemes we do not have local information, since there is no notion of distance between two variables in the scheme. Furthermore, known properties of the underlying expression cannot be used to represent a distance, because the expression itself differs depending on the position in the scheme: the subexpressions to which the variable at position i in the scheme is applied, are constructed using the variables prior in the scheme. Thus, the context of the sub-scheme x, y depends completely on the position in the Horner scheme. Therefore, Ant Colony Optimization is not a candidate for further research.\n(E) Evolutionary algorithms work by using mutations and genetic recombinations [17]. The mutations are related to our swap function (see section 5.2) and the recombinations mean merging parts of two Horner schemes in order to obtain a better one. The recombinations work best if the problem can be split into subproblems. For TSP, it is likely that an optimized subpath is also part of the total shortest path. However, as mentioned above, for Horner schemes the quality of a subpath depends on the context and thus on the variables that have been previously chosen. Therefore, recombinations are not likely to improve the quality of the solutions. Hence, evolutionary algorithms are not a candidate for further research.\n(F) Monte Carlo Tree Search (MCTS) [5], [21] has been used successfully in finding high-performance Horner schemes [2]. MCTS builds a search tree selectively, where\n4 at each level in the tree a new variable is added. Only branches that are deemed profitable are explored further. The number of tree updates N is chosen by the user and is related linearly to the amount of time spent in the search. A commonly used criterion for the selection of the best child, UCT (Upper Confidence bounds applied to Trees), introduces an exploration-exploitation constant\nCp that has to be fine-tuned manually [22] (other selection criteria have a similar trade-off parameter). The introduction of Simulated Annealing UCT (SA-UCT), alleviates the tuning of Cp, but does not eliminate it [6].\nEven though MCTS has been successful in finding optimal Horner schemes, we recognize that there are some intrinsic shortcomings to using a tree representation, especially if the depth of the search tree becomes (too) large. We notice that many branches do not reach the bottom when there are more than 20 variables (we recall that the problem depth is equivalent to the number of variables) as is the case with many of our expressions. MCTS determines the scores of a branch by performing a random play-out. If the branch is not constructed all the way to the bottom, the final nodes are therefore random (no optimization). For Horner schemes, the entire scheme is important, so sub-optimal selection of variables at the end of the scheme can have a significant impact. In fact, in [2] a parameter is introduced to select whether the Horner schemes should be built forward or backward (the first variables in the scheme are the last to be applied). The backward approach tries to improve results for expressions where the order of the final variables is more sensitive to improvements than the order of the first variables [6]. However, the underlying problem of poor decision making at the end of the tree remains unsolved. Therefore, we do not consider MCTS as a candidate for further research. The issues with tree representations motivated us to look for a method that is symmetric in its optimization: both the beginning and the end have to be optimized equally well.\nUsing the reasoning above, we may conclude that methods (A) Stochastic Local Search, and (B) Simulated Annealing, are best suited for the Horner scheme problem. Still, we note that the algorithms (C)...(F) mentioned above may be able to outperform SLS and SA, after extensive tuning of their extra parameters. However, we argue that without tuning, these methods do not outperform SLS or SA, since their characteristics are illsuited to the problem.\nSo, we continue our investigation with SLS and SA, and we consider the experimental setup for measurements with SLS and SA below."
    }, {
      "heading" : "4 EXPERIMENTAL SETUP",
      "text" : "We use eight large benchmark expressions, four from mathematics and four from real-world High Energy Physics (HEP) calculations. In table 1 statistics for the expressions are displayed. We show the number of variables, terms, operations, and the evaluation time of applying a Horner scheme and CSEE.\nvariables terms operations eval. time (s) res(7,4) 13 2561 29 163 0.001 res(7,5) 14 11 379 142 711 0.03 res(7,6) 15 43 165 587 880 0.13 res(9,8) 19 4 793 296 83 778 591 25.0 HEP(σ) 15 5716 47 424 0.008 HEP(F13) 24 105 058 1 068 153 0.4 HEP(F24) 31 836 009 7 722 027 3.0 HEP(b) 107 193 767 1 817 520 2.0\nTable 1: The number of variables, terms, operations, and the evaluation time of applying a single Horner scheme and CSEE in seconds, for our eight (unoptimized) benchmark expressions. The time measurement is performed on a 2.4 GHz Xeon computer. All expressions fit in memory (192 GB).\nThe expressions called res(7,4), res(7,5), res(7,6), and res(9,8) are resolvents and are defined by res(m,n) = resx( ∑m i=0 aix i, ∑n i=0 bix i), as described in [23]. The number of variables is m + n + 2. res(9,8) is the largest polynomial we have tested and has been included to test the boundaries of our hardware.\nThe High Energy Physics expressions represent scattering processes for the International Linear Collider, a likely successor to the Large Hadron Collider. A standard method of calculating the probability of certain collision events is by using perturbation theory. As a result, for each order of perturbations, additional expressions are calculated as corrections to previous orders of precision. The HEP polynomials of table 1 are second order corrections to various processes.\nHEP(σ) describes parts of the process e+e− → µ+µ−γ, namely the collision of an electron and positron that creates a muon, an anti-muon, and a photon.\nHEP(F13), HEP(F24), and HEP(b) are obtained from the process e+e− → µ+µ−uū, namely the collision of an electron and positron that creates a muon, anti-muon, an up-quark, and an up-antiquark. The results can be used to obtain next-generation precision measurements for electron-positron scattering."
    }, {
      "heading" : "5 RESULTS",
      "text" : "In this section we present the results of our measurements on the benchmark polynomials. In section 5.1 we measure the difference between Stochastic Local Search and Simulated Annealing. In section 5.2, we study the effects of the two parameters of SLS: the number of iterations and the neighborhood structure. In section 5.3 we investigate two state space properties, namely the occurrence of local minima and the flatness of the state space. In section 5.4 we compare the performance of the Horner schemes that are found by SLS to the results found by MCTS."
    }, {
      "heading" : "5.1 SLS vs. SA",
      "text" : "A Stochastic Local Search has two parameters: the number of iterations N , and the neighborhood structure, which defines the transition function [18]. A Stochastic\n5 0 5000 10000 15000 20000 Initial temperature 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 R e la ti v e i m p ro v e m e n t HEP(σ) HEP(F13) res(7,5) HEP(b)\nFigure 2: The relative improvement (smaller is better) of the number of operations for a given initial temperature Ti, compared to Ti = 0. Each data point is the average of more than 100 SA runs with 1000 iterations, and a swap neighborhood structure. We show the expressions HEP(σ), HEP(F13), res(7,5), and HEP(b). The number of operations is only slightly influenced by the initial temperature, since the best improvement over Ti = 0 is smaller than 5%.\nLocal Search only moves to a neighbor if the evaluation score (number of operations) is improved. As a consequence, SLS could get stuck in local minima. Therefore, we seriously considered to use Simulated Annealing instead of SLS, since SA has the ability to escape from local minima.\nSimulated Annealing is a popular generalization of SLS. It has four additional parameters, namely the initial temperature Ti, the final temperature Tf , the acceptance scheme, and the cooling scheme [15]. The temperature governs the probability of accepting transitions with an energy higher than the energy of the current state. The cooling scheme governs how fast and in what way the temperature is decreased during the simulation (linearly, exponentially, etc.). Exponential cooling is frequently used. The acceptance scheme is most often the Boltzmann probability exp(∆E/T ), that defines the probability of selecting a transition to an inferior state, given the difference in evaluation score ∆E.\nFor SA we consider the following. If the initial temperature is high, transitions to inferior states are permitted, allowing an escape from local minima. In order to determine the effect of the initial temperature Ti on the results, we will perform a sensitivity analysis. We use Boltzmann probability as the acceptance scheme, exponential cooling, a final temperature of 0.01, N = 1000 iterations, and a swap neighborhood structure (for a visualization, see figure 3).\nIn figure 2 we show the relative improvement (smaller is better) of the number of iterations for a given initial temperature Ti compared to Ti = 0 for the expressions HEP(σ), HEP(F13), res(7,5), and HEP(b). Naturally, for\nTi = 0, the relative improvement to itself is 1. For all expressions except HEP(b), we see a region where the improvement is largest: for HEP(σ) it is approximately [1000, 7000], for HEP(F13) it is [12 000, 17 000] and for res(7,5) it is [5000, 20 000]. This improvement is less than 5%. For higher T , too many transitions to inferior states are accepted to obtain good results. HEP(b) seems to be independent of the initial temperature. The fluctuations of 1% are statistical fluctuations.\nThe difference between the best results for all the expressions in figure 2 and the result at T = 0 is less than 5%. Since a Ti = 0 SA search is effectively an SLS search, this means that an almost parameterless Stochastic Local Search (SLS) is able to obtain results that are only slightly inferior. This is surprising, since the way SLS traverses the state space is different from the way by SA. We here reiterate once more, SLS can get stuck in local minima, whereas SA has the possibility to escape. Furthermore, if a saddle point is reached, SA is able to climb over the hill, whereas SLS has to walk around the hill in order to escape. In subsection 5.3.1 we will show that local minima are sparse, and that most of them are actually saddle points (i.e., local “minima” with a way to escape). Consequently, SA performs slightly better not because it can escape from local minima, but because, for some polynomials, walking over a saddle point (SA) is slightly faster to find better states than trying to circumvent the saddle point (SLS).\nThe reason why we prefer SLS over SA even though there is a gain of up to 5%, is that (1) the fundamental algorithmic improvement of SA – the ability to escape from local minima – is not used in practice, as we will see in subsection 5.3.1, and (2) tuning the SA parameters is expensive. Several methods have been suggested to tune the initial temperature, such as [19] and [20], but they often take several hundred iterations to obtain reliable values (which is quite expensive in our case). The small benefit of SA can be obtained by three other ways. First, by performing SLS runs in parallel (see section 5.2), second, by increasing the number of iterations, and third by selecting an initial temperature based on previous information such as figure 2. It is likely that a small improvement is obtained without increasing the run time."
    }, {
      "heading" : "5.2 Neighborhood structure",
      "text" : "The main parameter of SLS is the neighborhood structure. Choosing an appropriate neighborhood structure is crucial, since it determines the shape of the search space and thus influences the search performance. In [24] it is observed that the neighborhood structure can have a significant impact on the quality of the solutions for the Traveling Salesman Problem, the Quadratic Assignment Problem, and the Flow-shop Scheduling Problem.\nThere are many neighborhood structures for permutation problems such as Horner schemes. For example, a transition could swap two variables in the Horner\n6 (a) x y z w z y x w\n(b) x y z w y x z w\n(c) x y z w z x y w\n(d) x y z w z y x w\nFigure 3: The elementary neighborhood structures we use. From top to bottom: (a) a single swap, (b) a single shift, (c) shift of a sublist, and (d) mirroring.\nscheme or move a variable in the scheme. However, there are also neighborhood structures that involve changing larger structures. Figure 3 gives an overview of four basic transitions from which others can be constructed. From top to bottom, it shows (a) a single swap of two variables in the scheme, (b) a shift of a variable, (c) a shift of a sublist, and (d) a mirroring of a sublist. At each iteration of SLS, a transition to a randomly chosen neighbor is proposed. For the single swap transition, this involves the selection of two random variables in the scheme.\nTo examine which neighborhood structure performs best for Horner schemes, we investigate seven (combinations of) neighborhood structures, viz. (1) a single swap, (2) two consecutive swaps, (3) three consecutive swaps, (4) a shift of a single variable, (5) mirroring of a sublist, (6) a sublist shift (which we call ‘many shift’), and (7) mirroring and/or shifting with an equal probability (which we call ‘mirror shift’). Swapping multiple times in succession allows for faster traversal of the state space, but also runs the risk to miss states. Moreover, we have tested hybrid transitions, such as performing two consecutive swaps in the first half of the simulation and resorting to single swaps for the latter half, but we found that these combinations did not perform better. In order to present clear plots, we have omitted the plots resulting from these combinations.\nWe now start investigating two methods of measuring the quality of a neighborhood structure by: (1) the average number of operations obtained by using a neighborhood structure, and (2) the lowest number of operations after performing several runs. Figure 4 shows the distribution of the number of operations of the expression HEP(F13) after 10 000 SLS runs with the neighborhood structure that exchanges two random variables. The average of this distribution is somewhere in the middle, but the actual values that one will measure will be either near 51 000 or near 62 000. Thus, the average is not an appropriate measure.\nSo, we decided to measure the lowest score of several runs, because in practice SLS is run in parallel, and so the results are more in line with those from practical applications. Thus, we are interested in the neighbor-\n50000 52000 54000 56000 58000 60000 62000 64000 66000 68000 Number of operations\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nFr e q u e n cy\nFigure 4: HEP(F13) expression with 10 000 runs and 1 swap as a neighborhood structure. Typical for our domain is that there are often two or more spikes. If the simulation is run multiple times, the probability of finding a value close to the minimum is high.\nhood structure that has the lowest expected value of the minimum of k measurements. Here, we can use the expected value E [min (X0, . . . , Xk−1)]:\nV0 + L−2∑ t=0 (Vt+1 − Vt) (1− cdf(D, t))k (3)\nwhere k is the number of measurements, Xn is the score of the nth measurement, t is an index in the discrete distribution, Vt is the number of operations at t, Dt is the probability of outcome Vt, L is the number of possible outcomes, and cdf the cumulative distribution function. For a derivation, we refer to appendix A. We shall denote the expected value of the minimum of k runs by Emin,k.\nBecause the number of measurements k is in the exponent in eq. (3), Emin,k decreases exponentially with k and finally converges to V0 (see figure 13 in appendix A). As a consequence, neighborhood structures with a high standard deviation are more likely to achieve better results, since at high k the probability of finding a low value at least once is high. We find that four parallel runs (k = 4) yield good results.\nWe will now present detailed results for res(7,6) and HEP(σ) in subsection 5.2.1, and for HEP(F13) and HEP(b) in subsection 5.2.2. The results for res(7,4), res(7,5), and HEP(F24) are similar, and are omitted for brevity. res(9,8) is too time consuming for such a detailed analysis (it would take around 35 days to collect all data)."
    }, {
      "heading" : "5.2.1 Results for res(7,6) and HEP(σ)",
      "text" : "In figure 5 the performance of the neighborhood structures for the expression res(7,6) is shown. We see that shifting a single variable (‘1 shift’) has the best performance at a low number of iterations N , followed by 2 consecutive swaps (‘2 swap’). At around N = 900 all neighborhood structures have converged. Thus, from\n7 100 200 300 400 500 600 700 800 900 1000 N 45000 46000 47000 48000 49000 50000 51000 E (m in , 4 ) 1 swap 2 swap 3 swap mirror 1 shift many shift mirror shift\nFigure 5: The expected number of operations of the minimum of four SLS runs with N iterations for the res(7,6) expression. The 1 shift performs best for low N , followed by the 2 swap. All the neighborhood structures converge at N = 900.\n100 200 300 400 500 600 700 800 900 1000 N\n4000\n4100\n4200\n4300\n4400\n4500\n4600\nE (m\nin , 4 )\n1 swap 2 swap 3 swap mirror 1 shift many shift mirror shift\nFigure 6: The expected number of operations of the minimum of four SLS runs with N iterations for the HEP(σ) expression. The 1 shift performs best for low N . All the neighborhood structures converge at N = 600.\nN = 900 onward it does not matter which structure is chosen.\nIn figure 6 we show the performance of the neighborhood structures for HEP(σ). We see that ‘1 shift’ has the best performance at low N . At N = 600, all the neighborhood structures have converged. The characteristics of this plot are similar to those of res(7,6).\nWe suspect that for a small state space, i.e., a small number of variables, there is not much difference between the neighborhood structures, since the convergence occurs quite early (below N = 1000). Therefore, we look at two expressions with more variables: HEP(F13) with 24 variables, and HEP(b) with 107 variables.\n100 200 300 400 500 600 700 800 900 1000 N\n50000\n52000\n54000\n56000\n58000\n60000\n62000\n64000\nE (m\nin , 4 )\n1 swap 2 swap 3 swap mirror 1 shift many shift mirror shift\nFigure 7: The expected number of operations of the minimum of four SLS runs with N iterations for the HEP(F13) expression. Mirror, many shift, and mirror shift converge to a lower value than the neighborhood structures 1 swap, 2 swap, 3 swap, and 1 shift.\n5.2.2 Results for HEP(F13) and HEP(b) In figure 7 we show the results for HEP(F13). We see that all the neighborhood structures that involve small changes (‘1 swap’, ‘2 swap’, ‘3 swap’, and ‘1 shift’) are outperformed by the neighborhood structures that have larger structural changes (‘mirror’, ‘many shift’, and ‘mirror shift’). The difference is approximately 8%. Both groups seem to have converged independently to different values. However, for larger N , we expect all neighborhood structures to converge to the same value. The point of convergence has shifted to higher N compared to res(7,6), and HEP(σ), since the state space has increased in size from 15! to 24!. From this plot, we may conclude that the state space of HEP(F13) is more suited to be traversed with larger changes.\nIn figure 8 the performance of the neighborhood structures for HEP(b) is shown. We see that two consecutive swaps perform best at low N and that ‘1 swap’, ‘2 swap’, ‘3 swap’, and ‘1 shift’ converge at N = 1000. The neighborhood structures that involve larger structural changes (‘mirror’, ‘many shift’, ‘mirror shift’) perform worse. These results are different from those of HEP(F13): for HEP(b), with an even larger state space than HEP(F13), smaller moves are better suited. This means that the mere number of variables is not a good indicator for the selection of a neighborhood scheme."
    }, {
      "heading" : "5.2.3 Combined results",
      "text" : "For the four benchmark expressions displayed above, and for the other three benchmark expressions, we observe that the relative improvement of the choice of the best neighborhood structure compared to the worst neighborhood structure is never more than 10%. Furthermore, we observe that there are two groups of neighborhood structures when the state space is sufficiently large: a group with small changes to the state (‘1\n8 100 200 300 400 500 600 700 800 900 1000 N 140000 150000 160000 170000 180000 190000 E (m in , 4 ) 1 swap 2 swap 3 swap mirror 1 shift many shift mirror shift\nFigure 8: The expected number of operations of the minimum of four SLS runs with N iterations for the HEP(b) expression. The 2 swap performs best for low N . 1 swap, 2 swap, 3 swap, and 1 shift converge at N = 1000. The other neighborhood structures perform worse.\nswap’, ‘2 swap’, ‘3 swap’, ‘1 shift’), and a group with large structural changes (‘mirror’, ‘many shift’ ‘mirror shift’). These two groups converge before N = 1000 for expressions with small state spaces, such as HEP(σ), but are further apart for expressions with more variables, such as HEP(F13) and HEP(b). The difference in quality in the group itself is often negligible (less than 3%). Thus, as a strategy to apply the appropriate neighborhood structure, we suggest to distribute the number of parallel runs evenly among the two groups: in the case of four runs, two of the runs can be performed using a neighborhood structure from the small change group and two using a structure from the large change group."
    }, {
      "heading" : "5.3 State space properties",
      "text" : "The fact that SLS works so well is surprising. Two wellknown obstacles are (1) a local search can get stuck in local minima which yields inferior results, and (2) the Horner scheme problem does not have local heuristics, so there is no guidance for any best-first search. Remarkably, SLS only needs 1000 iterations for a 107 variable expression (HEP(b)) to obtain good results, whereas a TSP benchmark problem with a comparable state space size, viz. kroA100 [25] with 100 variables, takes more than a million iterations to converge using a manually tuned SA search.\nA thousand iterations is also a small number compared to the size of the state space. The average distance between two arbitrary states is 98 swaps. A thousand iteration SLS search accepts approximately 300 suggested swaps, so at least 33% of all the accepted moves should move towards the global minimum. This scenario would be unlikely if the state space is unsuited for SLS, so perhaps the state space has convenient properties for our purposes. At this moment we consider the following two\nconditions: local minima are rare, and the region of the global minimum is flat. We discuss the two conditions in the following subsections."
    }, {
      "heading" : "5.3.1 Local minima and saddle points",
      "text" : "To obtain an idea on the number of local minima, we measure how often the simulation gets stuck: if there are many local minima, we expect the simulation to get stuck often. In figure 9, we show the distribution of HEP(F13) for 1000, 10 000, and 100 000 SLS runs respectively. For 1000 and 10 000 runs we see two peaks: one at the global minimum near 51 000 and one at an apparent local minimum near 62 000. As the number of iterations is increased, the weight shifts from the apparent local minimum to the global minimum: at 1000 iterations, there is a probability of 27.5% of arriving in the region of the global minimum, whereas this is 36.25% at 10 000 iterations. Apparently the local minimum is ‘leaking’: given sufficient time, the search is able to escape. The figure on the right with 100 000 iterations confirms the escaping possibility: the apparent local minimum has completely disappeared. Thus, the local minimum is in reality a saddle point, since for a true local minimum there is no path with a lower score leading away from the minimum. Since SLS requires many iterations to escape from the saddle point, only a few transitions reduce the number of operations.\nWe observe that apparent local minima disappear for our other benchmark expressions as well. SLS runs with 100 000 iterations approach the global minimum for all of our benchmark expressions. For example, for HEP(F13) mentioned above, the result is 50636±57 and for HEP(σ) the result is 4078 ± 9. The small standard deviations indicate that no runs get stuck in local minima (at least not in local minima significantly higher than the standard deviation).\nFrom these results we may conclude that true local minima, from which a local search cannot escape, are rare for Horner schemes."
    }, {
      "heading" : "5.3.2 Flatness of the state space",
      "text" : "To build an intuition for what the state space looks like, we consider its flatness. We measure how many of the neighbors have a value (number of operations) that does not differ by more than 1%: |xn−x||x| < 1%, where x is the reference state and xn is a neighbor of x. For brevity, we shall refer to this as ‘close’.\nIn figure 10 we show the results for HEP(σ) for the current states during a typical single SLS run. We see that throughout the simulation the percentage of close neighbors is approximately 30%. We compare these results to an SA run of the TSP problem kroA100 (displayed in figure 11). We see that for a random starting state the number of close neighbors is 30% as well, but as the simulation approaches the global minimum (at the right of the graph), the number of close neighbors decreases to 0.9%. As a result, the global minimum for TSP must be very narrow.\nThese results are a first hint that the state space of Horner is flat and terrace-like, whereas the TSP problem is more trough-like, with steep global/local minima. To investigate the flatness more deeply, we have looked at the distribution of the relative difference |xn−x||x| . For the global minimum of the HEP(b) expression, this is depicted in figure 12. We see that about 75% of the neighbors are within 1% and 95% within 5%, which is even higher than for HEP(σ). We observe similar features for the other points in the state space, including hard to escape saddle points.\nThe property that the state space is flat is not only present in physics expressions, but is found in our other four benchmark expressions as well. Additionally, we have generated test expressions that we know to have interesting mathematical structures, such as powers of\nexpressions. For example, for the expression (4a + 9b + 12c2+2d+4e3−2f+8g2−10h+i−j+2k2−3j4+l−15m2)6, 43% of the neighbors, 18% of the second neighbors and 7.3% of the third neighbors are close.\nThe question arises why the number of close neighbors is so high for the HEP(b) expression. For most expressions it is around 30%, but for HEP(b) it is 75%. A closer inspection revealed that the HEP(b) expression has the special property that 90 of the 107 variables never appear in the same term: a term that contain variable x does not contain variable y and vice versa. As a result, the Horner schemes x, y and y, x yield the same expression. The HEP(b) expression is not alone in this property: it represents a class of problems that often appears in electron-positron scattering processes.\nThe fact that some variables do not appear together in\n10\nthe same term is caused by a symmetry of the expression, since rearranging these variables in the scheme does nothing if they are direct neighbors in the scheme. The more symmetrical the expression is, the more likely it is that neighbors have the exact same value or a close value (within 5%). In the case of a uniformly random expression where the number of terms is much greater than the number of variables, we expect that practically all swaps are ineffective. The reason is that there is a high probability that each variable appears in an equal number of terms and has equal mixing.\nMany, if not all, large expressions exhibit the ‘flatness’ property of their state space, since in most cases the number of terms is much larger than the number of variables. For the expressions that we have tested, the ratio of the number of terms and the number of variables is always more than a factor 1000. As a consequence, most variables will appear in many terms, which in turn increases uniformity, resulting in neighboring states with small differences in value (less than 5%)."
    }, {
      "heading" : "5.4 Performance of SLS vs MCTS",
      "text" : "Below, we compare the results of Stochastic Local Search to the previous best results from MCTS, for our eight benchmark expressions res(7,4), res(7,5), res(7,6), res(9,8), HEP(σ), HEP(F13), HEP(F24) and HEP(b). The results of all the MCTS runs except for res(9,8), and HEP(b) are taken from [2].1 The results are displayed in table 2.\nThe results for MCTS with 1000 and 10 000 iterations are obtained after considerable tuning of Cp and after selecting whether the scheme should be constructed forward or in reverse (i.e., the scheme is applied backwards [6]).\n1. We only consider optimizations by Horner schemes and CSEE. Additional optimizations that are mentioned in [2], such as ‘greedy’ optimizations, can just as well be applied to the results of SLS.\nFor smaller problems, we observe the seemingly negative result that the averages of SLS are on a par with or slightly worse than MCTS. However, as a positive result we see that the standard deviations of SLS are higher than MCTS. Consequently, we expect SLS to outperform MCTS if several runs are performed in parallel. Indeed, this is what we see in the last column of table 2. The standard deviations of MCTS are often an order of magnitude smaller than those of SLS, so the benefits of running MCTS in parallel are smaller. We may conclude that although for some small problems MCTS has better average scores, SLS has better minimal behavior if it is run in parallel.\nFor our largest expressions, HEP(F13), HEP(F24) and HEP(b), we observe that SLS with 1000 iterations yields better results than MCTS with 10 000 iterations. For HEP(F24), the average of SLS with 1000 iterations is about 20% better than the average for MCTS with 1000 iterations. In fact, the results are slightly better than MCTS with 10 000 iterations. If we take the Emin,4 into account, the expected value for HEP(F24) is an additional 7% less.\nThe fact that SLS outperforms MCTS when the number of variables is greater than 23, may be due to the fact that there are not sufficient iterations for the branches to reach the bottom, making the choice of the last variables essentially random (see section 3 and [6]). This may also be the reason why for MCTS it is important whether the scheme is constructed forward or in reverse: if most of the performance can be gained by carefully selecting the last variables, building the scheme in reverse will yield better performance.\nSLS is 10 times faster (in clock time) than MCTS, since most of the time is spent in the evaluation function. It is able to make reductions ranging from a factor 7 for our smallest expression, to 26 for our largest expression. The reduction factor becomes larger when there are more operations."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "Throughout the last forty years many algorithms have been devised to solve hard optimization problems. Each new algorithm introduces complexity in the form of parameters that have to be fine-tuned manually to the problem. It is tempting to use the latest successful and complex algorithms, but in doing so, sometimes convenient properties of the problem class can be missed or can remain unused. An analysis of the problem space can help identify which algorithm is best suited. In the case of Horner schemes, we have found that one of the most basic algorithms, Stochastic Local Search, yields the best results.\nStochastic Local Search provides a search method with two parameters: the number of iterations (computation time) and the neighborhood structure. We find that running half of the simulations with a neighborhood structure that makes minor changes to the state (i.e., a single\n11\nshift of a variable), and running the other half with a neighborhood structure that involves larger changes (i.e., the mirroring of a random sublist) is a good strategy for all of our benchmark expressions (see subsection 5.2.3). Consequently, only the computation time remains as a parameter. We find that (1) SLS obtains similar results to MCTS for expressions with around 15 variables, (2) SLS outperforms MCTS for expressions with 24 or more variables, and (3) SLS requires ten times fewer samples than MCTS to obtain similar results. Therefore we may conclude that SLS is more than 10 times faster.\nThe result that a basic algorithm such as SLS performs well is surprising, since Horner schemes have some properties that make the search hard: there are no known local heuristics, and evaluations could take several seconds. In the previous sections we have shown that the performance of SLS is so good because the state space of Horner schemes is flat and has few local minima.\nThe number of operations is linearly related to the time it takes to perform numerical evaluations. The difference between the number of operations for the unoptimized and the optimized expression is more than a factor 24 compared. As a consequence, we are able to perform numerical integration (via repeated numerical evaluations) at least 24 times faster.\nFor High Energy Physics, the contribution is immediate: numerical integration of processes that are currently experimentally verified at CERN can be done significantly faster. Additionally, expensive calculations that would have taken months for the next-generation particle collider ILC can be done in days or even hours.\nOur algorithms will be implemented in the next release of the open source symbolic manipulation system FORM [7]."
    }, {
      "heading" : "7 DISCUSSION / FUTURE WORK",
      "text" : "Currently, our algorithms assume that the expressions are commutative, but our implementation could be expanded to be applied to generic expressions with noncommuting variables. Especially in physics, where tensors are common objects, this is useful. Horner’s rule can only be applied uniquely to commutative variables, but the pulling outside brackets keeps the order of the non-commuting objects intact. Thus, for Horner’s rule\nthe only required change is the selection of commutative variables for the scheme. The common subexpression elimination should honor the ordering of the noncommutative objects. For example, in figure 1, the two highlighted parts are not a common subexpression if the variables are non-commutative (a+e cannot be moved to the left of c). To enable non-cummutative objects, CSEE should only compare connected subsets.\nAdditional work can be put in finding other methods of reductions. For example, expressing certain variables as linear combinations of other variables may reduce the number of operations even further. Many of these patterns cannot be recognized by common subexpression elimination alone. Determining which variables should be expressed as linear combinations of other variables to yield optimal results is an open problem. Perhaps techniques such as Local Stochastic Search are applicable to this subject as well."
    }, {
      "heading" : "APPENDIX A",
      "text" : "EXPECTED VALUE OF MINIMUM In order to provide a wider accessibility, we provide the following derivation of the expected value of the minimum of n samples, used for eq. 3.\nWe draw n numbers X0, . . . , Xn−1 from the discrete probability distribution D, where Dt is the probability of outcome Vt, L is the number of outcomes, where t is an index ranging from 0 to L − 1 and Va < Vb iff a < b (thus D can be viewed as a histogram). We want to know E(min(X0, . . . , Xn−1)). Let:\nφ(t) ≡ P (min(X0, . . . , Xn−1) < Vt) = 1− P (∀Xi ≥ Vt) = 1− P (Xi ≥ Vt)n = 1− (1− P (Xi < Vt))n\n= 1− (1− cdf(D, t− 1))n (4)\nwhere cdf is the cumulative distribution function. We abbreviate min(X0, . . . , Xn−1) to min(n). Using eq. (4), we find an expression for the probability that the minimum is Vt:\nP (min(n) = Vt) =P (min(n) < Vt+1)− P (n) < Vt) =φ(t+ 1)− φ(t) =(1− cdf(D, t− 1))n − (1− cdf(D, t))n\n(5)\n12\nFinally, the expected value is: E(min(n) = ∑ t VtP (min(n) = Vt)\n= L−1∑ t=0 Vt((1− cdf(D, t− 1))n\n− (1− cdf(D, t))n)\n=V0 + L−2∑ t=0 (Vt+1 − Vt)(1− cdf(D, t))n\n(6)\nWe see that the expected value decreases exponentially in n to V0.\nFigure 13 shows the dependence of the expected value of the minimum of n runs for a fair 6-sided die. For n = 1, the result is the average value 3.5. E(min(X0, . . . , Xn−1)) decreases exponentially to 1."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work is supported in part by the ERC Advanced Grant no. 320651, “HEPGAME”."
    } ],
    "references" : [ {
      "title" : "A New Method of Solving Numerical Equations of All Orders by Continuous Approximation",
      "author" : [ "W. Horner" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1959
    }, {
      "title" : "Improving multivariate Horner schemes with Monte Carlo Tree Search",
      "author" : [ "J. Kuipers", "A. Plaat", "J. Vermaseren", "J. van den Herik" ],
      "venue" : "Computer Physics Communications, vol. 184, no. 11, pp. 2391–2395, 2013. [Online]. Available: http://www.sciencedirect. com/science/article/pii/S0010465513001689",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Greedy Algorithms for Optimizing Multivariate Horner Schemes",
      "author" : [ "M. Ceberio", "V. Kreinovich" ],
      "venue" : "SIGSAM Bull., vol. 38, no. 1, pp. 8–15, Mar. 2004. [Online]. Available: http://doi.acm.org/10.1145/980175.980179",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Efficient Selectivity and Backup Operators in Monte- Carlo Tree Search",
      "author" : [ "R. Coulom" ],
      "venue" : "Proceedings of the 5th International Conference on Computers and Games, ser. CG’06. Berlin, Heidelberg: Springer-Verlag, 2007, pp. 72–83. [Online]. Available: http: //dl.acm.org/citation.cfm?id=1777826.1777833",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Combining Simulated Annealing and Monte Carlo Tree Search for Expression Simplification",
      "author" : [ "B. Ruijl", "J. Vermaseren", "A. Plaat", "H.J. van den Herik" ],
      "venue" : "Proceedings of ICAART Conference 2014, vol. 1, no. 1, pp. 724–731, 2014. [Online]. Available: http://arxiv.org/abs/1312.0841",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "1989–present) Form source code",
      "author" : [ "J. Vermaseren" ],
      "venue" : "https://github.com/vermaseren/form",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1989
    }, {
      "title" : "HEPGAME and the Simplification of Expressions",
      "author" : [ "B. Ruijl", "J. Vermaseren", "A. Plaat", "H.J. van den Herik" ],
      "venue" : "2014, in press. [Online]. Available: http://arxiv.org/abs/1405.6369",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The Art of Computer Programming, Volume 2 (3rd Ed.)",
      "author" : [ "D.E. Knuth" ],
      "venue" : "Seminumerical Algorithms",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Algorithms for Games",
      "author" : [ "G.M. Adelson-Velsky", "V.L. Arlazarov", "M.V. Donskoy" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1988
    }, {
      "title" : "Future paths for integer programming and links to artificial intelligence",
      "author" : [ "F. Glover" ],
      "venue" : "Computers & Operations Research, vol. 13, no. 5, pp. 533 – 549, 1986, applications of Integer Programming. [Online]. Available: http://www.sciencedirect. com/science/article/pii/0305054886900481",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "An Effective Heuristic Algorithm for the Traveling-Salesman Problem",
      "author" : [ "S. Lin", "B.W. Kernighan" ],
      "venue" : "Operations Research, vol. 21, no. 2, pp. 498–516, 1973. [Online]. Available: http://dx.doi.org/ 10.2307/169020",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "An effective implementation of the lin-kernighan traveling salesman heuristic",
      "author" : [ "K. Helsgaun" ],
      "venue" : "European Journal of Operational Research, vol. 126, pp. 106–130, 2000.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Optimization by simulated annealing",
      "author" : [ "S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi" ],
      "venue" : "Science, vol. 220, pp. 671–680, 1983.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Ant colony system: A cooperative learning approach to the traveling salesman problem",
      "author" : [ "M. Dorigo", "L.M. Gambardella" ],
      "venue" : "Trans. Evol. Comp, vol. 1, no. 1, pp. 53–66, Apr. 1997. [Online]. Available: http://dx.doi.org/10.1109/4235.585892",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Genetic Algorithms in Search, Optimization and Machine Learning, 1st ed",
      "author" : [ "D.E. Goldberg" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1989
    }, {
      "title" : "Local Search in Combinatorial Optimization, 1st ed",
      "author" : [ "E. Aarts", "J.K. Lenstra", "Eds" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1997
    }, {
      "title" : "Jitta, “Adaptive simulated annealing for maximum temperature.",
      "author" : [ "M. Miki", "T. Hiroyasu" ],
      "venue" : "in SMC. IEEE,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2003
    }, {
      "title" : "Computing the initial temperature of simulated annealing",
      "author" : [ "W. Ben-Ameur" ],
      "venue" : "Comput. Optim. Appl., vol. 29, no. 3, pp. 369– 385, Dec. 2004. [Online]. Available: http://dx.doi.org/10.1023/B: COAP.0000044187.23143.bd",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A Survey of Monte Carlo Tree Search Methods",
      "author" : [ "C. Browne", "E. Powley", "D. Whitehouse", "S. Lucas", "P. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton" ],
      "venue" : "Computational Intelligence and AI in Games, IEEE Transactions on, vol. 4, no. 1, pp. 1–43, 2012.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bandit based Monte-Carlo Planning",
      "author" : [ "L. Kocsis", "C. Szepesvári" ],
      "venue" : "In: ECML-06. LNCS 4212. Springer, 2006, pp. 282–293.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Efficient evaluation of large polynomials",
      "author" : [ "C.E. Leiserson", "L. Li", "M.M. Maza", "Y. Xie" ],
      "venue" : "Proceedings of the Third International Congress Conference on Mathematical Software, ser. ICMS’10. Berlin, Heidelberg: Springer-Verlag, 2010, pp. 342–353. [Online]. Available: http://dl.acm.org/citation.cfm?id=1888390.1888464",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Application of the simulated annealing algorithm to the combinatorial optimisation problem with permutation property: An investigation of generation mechanism",
      "author" : [ "P. Tian", "J. Ma", "D.-M. Zhang" ],
      "venue" : "European Journal of Operational Research, vol. 118, no. 1, pp. 81 – 94, 1999. [Online]. Available: http://www. sciencedirect.com/science/article/pii/S0377221798003087",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This reduces the number of multiplications [1], [2].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "This reduces the number of multiplications [1], [2].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "The problem of finding an optimal Horner scheme is NP-hard [4].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Recent successes with Monte Carlo Tree Search (MCTS) [5] have shown that the number of operations of expressions can be reduced by at least a factor of 16 for a set of large, real-world, expressions [2].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Recent successes with Monte Carlo Tree Search (MCTS) [5] have shown that the number of operations of expressions can be reduced by at least a factor of 16 for a set of large, real-world, expressions [2].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "The suggested algorithm SA-UCT (Simulated Annealing - Upper Confidence Bounds applied to Trees) alleviates the tuning problem, but does not eliminate it [6].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "the open source symbolic manipulation system FORM [7].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "To provide some background, we will now describe two methods to reduce the number of operations, namely Horner schemes and common subexpression elimination [6], [8], followed by some remark on their interplay and the scaling of the computation time.",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "To provide some background, we will now describe two methods to reduce the number of operations, namely Horner schemes and common subexpression elimination [6], [8], followed by some remark on their interplay and the scaling of the computation time.",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "Horner’s rule is a classic method to reduce the number of multiplications in a polynomial by lifting variables outside brackets [1], [9].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "Horner’s rule is a classic method to reduce the number of multiplications in a polynomial by lifting variables outside brackets [1], [9].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "The problem of selecting an optimal ordering is NP-hard [4].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "(number of operations up to 7 722 027) [2].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "This method is well known in the field of compiler construction [3], where it is applied to much smaller expressions than in high energy physics, and in the field of computer chess [10] where it handles the occurrence of common",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "For the Traveling Salesman Problem (TSP), the distance between two cities can be used as a heuristic [11], and more specialized heuristics are able to solve symmetric TSP instances with thousands of cities (a historic example is a TSP with 7397 cities [12], [13]).",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "For the Traveling Salesman Problem (TSP), the distance between two cities can be used as a heuristic [11], and more specialized heuristics are able to solve symmetric TSP instances with thousands of cities (a historic example is a TSP with 7397 cities [12], [13]).",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 11,
      "context" : "For the Traveling Salesman Problem (TSP), the distance between two cities can be used as a heuristic [11], and more specialized heuristics are able to solve symmetric TSP instances with thousands of cities (a historic example is a TSP with 7397 cities [12], [13]).",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 1,
      "context" : "In [2], and [6], Monte Carlo Tree Search (MCTS) has been successfully used to find a best candidate of all available Horner schemes.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "In [2], and [6], Monte Carlo Tree Search (MCTS) has been successfully used to find a best candidate of all available Horner schemes.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "In order to determine which of these algorithms is best suited for Horner schemes, we briefly discuss the characteristics of a selection of six frequently used algorithms: (Stochastic) Local Search [14], Simulated Annealing [15], Tabu Search [11], Ant Colony Optimization [16], Evolutionary Algorithms [17], and, Monte Carlo Tree Search [5].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 9,
      "context" : "In order to determine which of these algorithms is best suited for Horner schemes, we briefly discuss the characteristics of a selection of six frequently used algorithms: (Stochastic) Local Search [14], Simulated Annealing [15], Tabu Search [11], Ant Colony Optimization [16], Evolutionary Algorithms [17], and, Monte Carlo Tree Search [5].",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 13,
      "context" : "In order to determine which of these algorithms is best suited for Horner schemes, we briefly discuss the characteristics of a selection of six frequently used algorithms: (Stochastic) Local Search [14], Simulated Annealing [15], Tabu Search [11], Ant Colony Optimization [16], Evolutionary Algorithms [17], and, Monte Carlo Tree Search [5].",
      "startOffset" : 272,
      "endOffset" : 276
    }, {
      "referenceID" : 14,
      "context" : "In order to determine which of these algorithms is best suited for Horner schemes, we briefly discuss the characteristics of a selection of six frequently used algorithms: (Stochastic) Local Search [14], Simulated Annealing [15], Tabu Search [11], Ant Colony Optimization [16], Evolutionary Algorithms [17], and, Monte Carlo Tree Search [5].",
      "startOffset" : 302,
      "endOffset" : 306
    }, {
      "referenceID" : 3,
      "context" : "In order to determine which of these algorithms is best suited for Horner schemes, we briefly discuss the characteristics of a selection of six frequently used algorithms: (Stochastic) Local Search [14], Simulated Annealing [15], Tabu Search [11], Ant Colony Optimization [16], Evolutionary Algorithms [17], and, Monte Carlo Tree Search [5].",
      "startOffset" : 337,
      "endOffset" : 340
    }, {
      "referenceID" : 15,
      "context" : "(A) Local Search [18], [14] is a state space exploration method that starts from an initial state and moves to a neighbor of the current state.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "(B) Simulated Annealing (SA) [15] is a classic method that is inspired by the removal of crystal defects by the cooling of metals (annealing).",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "For instance, in [19] and [20] suggestions have been made to tune the initial temperature.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : "For instance, in [19] and [20] suggestions have been made to tune the initial temperature.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "(C) Tabu Search performs a local search and keeps track of previous visits [11].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "In classic Tabu Search, the best neighbor that has not been visited earlier is selected and is added to the tabu list [11].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "(D) Ant Colony Optimization [16] has been successful in optimizing various problems, such as TSP instances.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "(E) Evolutionary algorithms work by using mutations and genetic recombinations [17].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "(F) Monte Carlo Tree Search (MCTS) [5], [21] has been used successfully in finding high-performance Horner",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "(F) Monte Carlo Tree Search (MCTS) [5], [21] has been used successfully in finding high-performance Horner",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "schemes [2].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 19,
      "context" : "A commonly used criterion for the selection of the best child, UCT (Upper Confidence bounds applied to Trees), introduces an exploration-exploitation constant Cp that has to be fine-tuned manually [22] (other selection criteria have a similar trade-off parameter).",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 4,
      "context" : "The introduction of Simulated Annealing UCT (SA-UCT), alleviates the tuning of Cp, but does not eliminate it [6].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "In fact, in [2] a parameter is introduced to select whether the Horner schemes should be built forward or backward (the first variables in the scheme are the last to be applied).",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "The backward approach tries to improve results for expressions where the order of the final variables is more sensitive to improvements than the order of the first variables [6].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 20,
      "context" : "The expressions called res(7,4), res(7,5), res(7,6), and res(9,8) are resolvents and are defined by res(m,n) = resx( ∑m i=0 aix , ∑n i=0 bix ), as described in [23].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 15,
      "context" : "A Stochastic Local Search has two parameters: the number of iterations N , and the neighborhood structure, which defines the transition function [18].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 12,
      "context" : "It has four additional parameters, namely the initial temperature Ti, the final temperature Tf , the acceptance scheme, and the cooling scheme [15].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "For all expressions except HEP(b), we see a region where the improvement is largest: for HEP(σ) it is approximately [1000, 7000], for HEP(F13) it is [12 000, 17 000] and for res(7,5) it is [5000, 20 000].",
      "startOffset" : 149,
      "endOffset" : 165
    }, {
      "referenceID" : 14,
      "context" : "For all expressions except HEP(b), we see a region where the improvement is largest: for HEP(σ) it is approximately [1000, 7000], for HEP(F13) it is [12 000, 17 000] and for res(7,5) it is [5000, 20 000].",
      "startOffset" : 149,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "For all expressions except HEP(b), we see a region where the improvement is largest: for HEP(σ) it is approximately [1000, 7000], for HEP(F13) it is [12 000, 17 000] and for res(7,5) it is [5000, 20 000].",
      "startOffset" : 189,
      "endOffset" : 203
    }, {
      "referenceID" : 16,
      "context" : "Several methods have been suggested to tune the initial temperature, such as [19] and [20], but they often take several hundred iterations to obtain reliable values (which is quite expensive in our case).",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "Several methods have been suggested to tune the initial temperature, such as [19] and [20], but they often take several hundred iterations to obtain reliable values (which is quite expensive in our case).",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "In [24] it is observed that the neighborhood structure can have a significant impact on the quality of the solutions for the Traveling Salesman Problem, the Quadratic Assignment Problem, and the Flow-shop Scheduling Problem.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "The results of all the MCTS runs except for res(9,8), and HEP(b) are taken from [2].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : ", the scheme is applied backwards [6]).",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "Additional optimizations that are mentioned in [2], such as ‘greedy’ optimizations, can just as well be applied to the results of SLS.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "The fact that SLS outperforms MCTS when the number of variables is greater than 23, may be due to the fact that there are not sufficient iterations for the branches to reach the bottom, making the choice of the last variables essentially random (see section 3 and [6]).",
      "startOffset" : 264,
      "endOffset" : 267
    }, {
      "referenceID" : 1,
      "context" : "The MCTS results for all expressions except res(9,8) and HEP(b) are from [2].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "lease of the open source symbolic manipulation system FORM [7].",
      "startOffset" : 59,
      "endOffset" : 62
    } ],
    "year" : 2014,
    "abstractText" : "Simplifying expressions is important to make numerical integration of large expressions from High Energy Physics tractable. To this end, Horner’s method can be used. Finding suitable Horner schemes is assumed to be hard, due to the lack of local heuristics. Recently, MCTS was reported to be able to find near optimal schemes. However, several parameters had to be fine-tuned manually. In this work, we investigate the state space properties of Horner schemes and find that the domain is relatively flat and contains only a few local minima. As a result, the Horner space is appropriate to be explored by Stochastic Local Search (SLS), which has only two parameters: the number of iterations (computation time) and the neighborhood structure. We found a suitable neighborhood structure, leaving only the allowed computation time as a parameter. We performed a range of experiments. The results obtained by SLS are similar or better than those obtained by MCTS. Furthermore, we show that SLS obtains the good results at least 10 times faster. Using SLS, we can speed up numerical integration of many real-world large expressions by at least a factor of 24. For High Energy Physics this means that numerical integrations that took weeks can now be done in hours.",
    "creator" : "LaTeX with hyperref package"
  }
}