{
  "name" : "1412.7063.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DIVERSE EMBEDDING NEURAL NETWORK LANGUAGE MODELS",
    "authors" : [ "Kartik Audhkhasi", "Abhinav Sethy", "Bhuvana Ramabhadran" ],
    "emails" : [ "kaudhkha@us.ibm.com", "asethy@us.ibm.com", "bhuvana@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Diversity of systems trained to perform a given machine learning task is crucial to obtaining a performance improvement upon fusion (Kuncheva (2004)). The problem of language modeling, that aims to design predictive models of text, is no exception. Several language models (LMs) have been proposed over many years of research (Rosenfield (2000)). Simple N-gram models estimate the conditional probability of the i-th word wi in a sentence given the previous N − 1 words (wi−N+1, . . . , wi−1) as\nP̂N-gram(wi|wi−N+1, . . . , wi−1) = C(wi, wi−1, . . . , wi−N+1)\nC(wi−1, . . . , wi−N+1) (1)\nwhere C(.) computes the count of a given word phrase or N-gram in the training text1. More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function\nP̂NN(wi|wi−N+1, . . . , wi−1) = f(wi−1, . . . , wi−N+1; Θ) (2)\nparameterized by Θ2. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al. (2011)) often significantly improves performance. Table 1 shows the perplexity3 of 4-gram and NNLMs on a standard split of the Penn Treebank data set (Marcus et al. (1993)). Interpolation of a NNLM with a 4-gram LM gives a 16.9% reduction in perplexity over a single NNLM even though the two LMs have relatively close perplexities. We use the correlation coefficient between the posterior probabilities of the predicted word over the test set from the two models as a simple measure to predict whether the models are diverse. If the posterior probabilities are highly correlated, then the models are less diverse and smaller gains are expected from fusion. The posteriors from the N-gram and NNLM have a correlation coefficient of 0.869 which is significantly lower than the correlation coefficient of 0.988 for a pair of randomly initialized NNLMs. This higher diversity of the NNLM and N-gram LM combination results in significant perplexity improvement upon interpolation.\n1Almost all N-gram models are smoothed in various ways to assign non-zero probability estimates for word phrases unseen in training data. We use Kneser-Ney smoothing (Kneser & Ney (1995)) for all N-grams models in this paper.\n2We will describe the typical architecture of a NNLM in the next section 3Perplexity is a standard measure of LM performance and is 2−L where L is the negative average log-\nlikelihood of the test set text estimated by the LM.\nar X\niv :1\n41 2.\n70 63\nv5 [\ncs .C\nL ]\n1 5\nA pr\n2 01\n5\nRandom initialization and modifying the neural net topology in terms of the embedding and hidden layer size can be used to build diverse NNLM models. As we can see from Table 1, 4 randomly initialized NNLMs (4 NN-RandInit) when fused together provide a 5% improvement in perplexity over the baseline. Recurrent NNLM models of different topologies can be fused to get significant gains as well as demonstrated in (Mikolov et al. (2011)). The remarkable benefit of such simple diversitypromoting strategies leads us to the central question of this paper - Is there a way to explicitly enforce diversity during NNLM model training? We show that modifying the NNLM architecture and augmenting the training loss function achieves that. The fact that a NNLM learns a low-dimensional continuous space embedding of input words motivates the architecture and training of our proposed model - Diverse Embedding Neural Network (DENN) LM.\nWe first give an overview of conventional NNLMs in the next section. Section 3 presents the DENNLM architecture and its training loss function. We presents experiments and results in Section 4 and conclude the paper in Section 5."
    }, {
      "heading" : "2 FEED-FORWARD NEURAL NETWORK LANGUAGE MODEL (NNLM)",
      "text" : "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi, . . . ,wi−N+1 denote the 1-in-V vectors of the history words. Let RH denote the D × V matrix that projects a history word vectors onto Ddimensional vectors RHwi, . . . ,RHwi−N+1. D is typically much smaller than the size V of the vocabulary with typical values being V = 10, 000 and D = 500.\nThis resulting (N − 1)D-dimensional continuous representation of input words feeds into a neural network with one hidden layer that estimates the 1-in-V vector of the target word wi. The hidden neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron activation function is a V -way softmax. The back-propagation algorithm (Rumelhart et al. (1986)) trains an NNLM, often using stochastic gradient descent where the gradient is computed over several random subsets or batches of N-grams from the input training data.\nResearchers have proposed several variants of the feed forward NNLM, especially to model a longer input word history. Prominent examples include the recurrent neural network LM (Mikolov et al. (2010)) and bidirectional long-short term memory (LSTM) LM (Sundermeyer et al. (2012)). These models offer improved performance but can be slower and more difficult to train. Even though we restrict our attention to feed forward NNLMs in this paper, the general principles proposed are applicable to other NNLM architectures as well. The next section introduces the diverse embedding NNLM (DENNLM).\n5A one-hot vector of the i-th word in the vocabulary contains 1 at index i and 0 everywhere else."
    }, {
      "heading" : "3 DIVERSE EMBEDDING NNLM",
      "text" : "A diverse embedding NNLM (DENNLM) aims to learn multiple diverse representations of the input words rather than a single representation. It is first important to understand the intuition of a representation in the context of a NNLM. Given a set of N input word vectors wi−1, . . . ,wi−N+1, consider the set of D-dimensional vectors RHwi−1, . . . ,RHwi−N+1. The pairwise distances between vectors of this set constitute the representation of the input words. A good representation captures the contextual and semantic similarity between pairs of words. Similar words are located close to each other in representation while dissimilar words are located far apart. A NNLM uses this representation of the input words to predict the next word. The most natural way to ensure diversity of two NNLMs is through the diversity in the representation itself (representational diversity). The next section discusses an intuitive score to capture representational diversity."
    }, {
      "heading" : "3.1 DIVERSITY BETWEEN NNLM EMBEDDINGS",
      "text" : "Maximizing the representational diversity between two NNLMs first requires an objective score to capture this diversity. Consider the representation RHwi−1, . . . ,RHwi−N+1 of the N − 1 input words to the NNLM. Since this representation lies in a D-dimensional Euclidean space, the set of all (N − 1)(N − 2)/2 pairwise angles between N − 1 words are sufficient to uniquely define the representation even under any affine transformation such as translation, rotation, and scaling. The matrix of pairwise cosine angles between all pairs of points in the representation defined by RH1 is\nC(RH1) =  1 . . .\nwTi−1R T H1RH1wi−N+1\n||RH1wi−1|| ||RH1wi−N+1|| wTi−2R T H1RH1wi−1\n||RH1wi−2|| ||RH1wi−2|| . . . wTi−2R T H1RH1wi−N+1\n||RH1wi−2|| ||RH1wi−N+1|| ... . . . ...\nwTi−N+1R T H1RH1wi−1\n||RH1wi−N+1|| ||RH1wi−1|| . . . 1\n .\nThis matrix completely defines the representation of the word history produced by RH . It is also independent of the dimensionality of the representation, i.e. the number of rows of RH . This is useful when comparing two representations with different dimensionality of the same set of input data points. Given two such representations computed using matrices RH1 and RH2, we define the\nrepresentational diversity as the negative correlation between cosine angles dRep(C(RH1),C(RH2)) = −vec ( C(RH1) )T vec ( C(RH2) ) (3)\nacross the two representations, where vec raster-scans a matrix into a column vector. We note that dRep is bounded because the cosine is bounded between −1 and 1. Representational diversity between RH1 and RH2 increases as dRep decreases. In our implementation of distance diversity, for computation efficiency reasons we consider distances over a randomly chosen set of 500 words in each minibatch instead of the full vocabulary. Our experiments show that using the entire vocabulary for diversity computation gave only minor improvements in perplexity at the expense of much longer training time.\nWe are currently exploring several other potential ways to compute diversity between two NNLMs beyond the representational diversity score presented in this paper. The next section discusses the DENNLM architecture and training loss function."
    }, {
      "heading" : "3.2 DENNLM ARCHITECTURE AND TRAINING LOSS FUNCTION",
      "text" : "As discussed earlier, a DENNLM attempts to learn multiple diverse low-dimensional representations of the input word history. Figure 2 shows the schematic diagram of a DENNLM with two diverse representations. The two representations pass through two separate NNs and produce separate predictions of the next word. The model merges the two predictions to produce the final prediction.\nIt is important to note that the DENNLM is equivalent to a single NNLM with the following blockstructured representation matrix\nRH =  R1H 00 R1HR2H 0 0 R2H  and block-diagonal connection weight matrices\nW = ( W1 0 0 W2 ) RP = ( R1P 0 0 R2P ) .\nThe presence of zero entries in the above matrices implies that a DENNLM has a smaller number of parameters than a comparable NNLM. The equivalence between a DENNLM and a conventional NNLM makes the implementation of a DENNLM easy within conventional NNLM toolkits.\nMerely constraining the architecture of a NNLM does not guarantee that it learns multiple diverse representations. Hence we augment the training loss function of a conventional NNLM with additional terms to promote diversity. We use the negative log-likelihood or cross entropy for V -way classification as the loss function for a conventional NNLM. A DENNLM instead uses the following augmented loss function:\nLDENNLM = −β T∑\ni=1\nlog ( M∑\nm=1\nαmpm(wi|wi−1, . . . , wi−N+1) )\n− (1− β) T∑\ni=1 M∑ m=1 αm log ( pm(wi|wi−1, . . . , wi−N+1) ) − γdRep(C(RH1), . . . ,C(RHM )) (4)\nThe first term of the above loss function is a mixture model loss that ensures that the fused prediction is accurate. Using only the first term trains a mixture of neural networks. However, this will not ensure high discrimination ability of the representations learned by the individual networks because the different representations will only capture the modes of the data distribution. We thus include the second term, motivated by a recent work on deeply supervised neural networks (Lee et al. (2014)), where the authors augment the conventional loss at the final layer with discriminative loses computed at previous layers. The second term in (4) plays a similar role and makes the individual representations discriminative as well. The final term is the representational diversity of the NNLM as described in Section 3.1. Minimizing the DENNLM loss function in (4) gives representations that are jointly discriminative, individually discriminative, and diverse.\nWe can add L1 and/or L2 regularization penalties to the loss function as well, which are often useful to prevent over-fitting in case the network size is large compared to the number of training set Ngrams. The next section discusses the experimental setup and results."
    }, {
      "heading" : "4 EXPERIMENTS AND RESULTS",
      "text" : "We conducted language modeling experiments on a standard split of the Penn Treebank (UPenn) data set (Marcus et al. (1993)), as used in previous works such as (Mikolov et al. (2011)). The UPenn data set has a vocabulary of 10K words, and contains approximately 1M N-grams in the training set. We used the UPenn data set since it is well-studied for language modeling and also small enough to conduct several experiments for understanding the DENNLM model better. We implemented the DENNLM in Theano (Bergstra et al. (2011)) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (Tieleman & Hinton (2012)), a variant of stochastic gradient descent. We tuned all hyper-parameters on the standard UPenn heldout set.\nTable 4 shows the test set perplexities of the baseline and the DENNLMs. We kept the NN model size comparable by reducing the size of each component NNLM. Our results show a significant improvement in perplexity by using a DENNLM over the 4-gram model, a single NNLM, and interpolation of randomly initialized NNLMs. The posterior correlation coefficients are significantly less than 0.99, which is the correlation coefficient for randomly initialized NNLMs.\nNote that the DENNLM significantly outperforms a standard NNLM of size (600,800) which has similar number of parameters. It is also clearly better then a randomly initialized set of 4 NNLM models. The perplexity results in Table 4 for our diverse feed-forward NNLMs are especially encouraging given the fact that the more advanced recurrent neural network (RNN) LM gives a perplexity of 124.7 by itself and 105.7 upon interpolation with a 5-gram LM on the same task (Mikolov et al. (2011))."
    }, {
      "heading" : "4.1 SENSITIVITY TO HYPERPARAMETERS",
      "text" : "We further studied the dependence of DENNLM performance on some sets of hyper-parameters and list our observations below:\nTo further understand the impact of hyper-parameters on DENNLM diversity and perplexity, we performed a thorough grid search of β and γ in (4), the learning rate of RMSProp, and the weight of an L2 penalty on the DENNLM connection weights. We then computed the log perplexity and average cross-correlation between posteriors of individual models in a DENNLM. Figure 3 shows the scatter plot between average posterior cross-correlation and percent improvement of the DENNLM log perplexity over the best model’s perplexity. A strong negative correlation of −0.97 in this figure shows that more diverse models give a bigger improvement in log perplexity upon interpolation. This highlights the merit of training diverse NNLM and the fact that one can achieve this diversity by either informed objective functions such as the one in (4) or an exhaustive hyper-parameter search. The latter becomes especially tedious for deep and complex neural networks trained on big data sets."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this work, we introduced a neural network architecture and training objective function that encourages the individual models to be diverse in terms of their output distribution as well as the underlying word representations. We demonstrated its usefulness on the well-studied UPenn language modeling task. The proposed training criterion is general enough and does not constrain the NNLM models to be of any specific architecture. Given the promising results, our next step is to evaluate the improved language models on speech recognition and spoken-term detection tasks. We\nalso plan to explore the utility of these diverse representations to measure semantic similarity and for sentence completion, where word embedding-based models have been shown to be effective."
    }, {
      "heading" : "6 ACKNOWLEDGEMENT",
      "text" : "This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense U.S. Army Research Laboratory (DoD / ARL) contract number W911NF-12C-0012. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government."
    } ],
    "references" : [ {
      "title" : "Neural probabilistic language models",
      "author" : [ "Y. Bengio", "H. Schwenk", "J. Senécal", "F. Morin", "J. Gauvain" ],
      "venue" : "In Innovations in Machine Learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2006
    }, {
      "title" : "Deep learning on gpus with python",
      "author" : [ "J. Bergstra", "F. Bastien", "O. Breuleux", "P. Lamblin", "R. Pascanu", "O. Delalleau", "G. Desjardins", "D. Warde-Farley", "I.J. Goodfellow", "A. Bergeron", "Bengio", "Y. Theano" ],
      "venue" : "In Big Learn workshop,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2011
    }, {
      "title" : "A bit of progress in language modeling",
      "author" : [ "J.T. Goodman" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Goodman,? \\Q2001\\E",
      "shortCiteRegEx" : "Goodman",
      "year" : 2001
    }, {
      "title" : "Improved backing-off for m-gram language modeling",
      "author" : [ "R. Kneser", "H. Ney" ],
      "venue" : "In Proc. ICASSP,",
      "citeRegEx" : "Kneser and Ney,? \\Q1995\\E",
      "shortCiteRegEx" : "Kneser and Ney",
      "year" : 1995
    }, {
      "title" : "Combining pattern classifiers: methods and algorithms",
      "author" : [ "L.I. Kuncheva" ],
      "venue" : null,
      "citeRegEx" : "Kuncheva,? \\Q2004\\E",
      "shortCiteRegEx" : "Kuncheva",
      "year" : 2004
    }, {
      "title" : "Building a large annotated corpus of English: The Penn Treebank",
      "author" : [ "M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafiát", "L. Burget", "J. Cernockỳ", "S. Khudanpur" ],
      "venue" : "In Interspeech,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Empirical evaluation and combination of advanced language modeling techniques",
      "author" : [ "T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernockỳ" ],
      "venue" : "In Interspeech,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2011
    }, {
      "title" : "Two decades of statistical language modeling: Where do we go from here",
      "author" : [ "R. Rosenfield" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Rosenfield,? \\Q2000\\E",
      "shortCiteRegEx" : "Rosenfield",
      "year" : 2000
    }, {
      "title" : "LSTM neural networks for language modeling",
      "author" : [ "M. Sundermeyer", "R. Schlüter", "H. Ney" ],
      "venue" : "In Interspeech,",
      "citeRegEx" : "Sundermeyer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sundermeyer et al\\.",
      "year" : 2012
    }, {
      "title" : "Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "Tieleman and Hinton,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Diversity of systems trained to perform a given machine learning task is crucial to obtaining a performance improvement upon fusion (Kuncheva (2004)).",
      "startOffset" : 133,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "Diversity of systems trained to perform a given machine learning task is crucial to obtaining a performance improvement upon fusion (Kuncheva (2004)). The problem of language modeling, that aims to design predictive models of text, is no exception. Several language models (LMs) have been proposed over many years of research (Rosenfield (2000)).",
      "startOffset" : 133,
      "endOffset" : 345
    }, {
      "referenceID" : 0,
      "context" : "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P̂NN(wi|wi−N+1, .",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P̂NN(wi|wi−N+1, . . . , wi−1) = f(wi−1, . . . , wi−N+1; Θ) (2) parameterized by Θ2. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al.",
      "startOffset" : 61,
      "endOffset" : 319
    }, {
      "referenceID" : 0,
      "context" : "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P̂NN(wi|wi−N+1, . . . , wi−1) = f(wi−1, . . . , wi−N+1; Θ) (2) parameterized by Θ2. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al. (2011)) often significantly improves performance.",
      "startOffset" : 61,
      "endOffset" : 342
    }, {
      "referenceID" : 0,
      "context" : "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P̂NN(wi|wi−N+1, . . . , wi−1) = f(wi−1, . . . , wi−N+1; Θ) (2) parameterized by Θ2. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al. (2011)) often significantly improves performance. Table 1 shows the perplexity3 of 4-gram and NNLMs on a standard split of the Penn Treebank data set (Marcus et al. (1993)).",
      "startOffset" : 61,
      "endOffset" : 507
    }, {
      "referenceID" : 0,
      "context" : "More complex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probability as a non-linear function P̂NN(wi|wi−N+1, . . . , wi−1) = f(wi−1, . . . , wi−N+1; Θ) (2) parameterized by Θ2. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al. (2011)) often significantly improves performance. Table 1 shows the perplexity3 of 4-gram and NNLMs on a standard split of the Penn Treebank data set (Marcus et al. (1993)). Interpolation of a NNLM with a 4-gram LM gives a 16.9% reduction in perplexity over a single NNLM even though the two LMs have relatively close perplexities. We use the correlation coefficient between the posterior probabilities of the predicted word over the test set from the two models as a simple measure to predict whether the models are diverse. If the posterior probabilities are highly correlated, then the models are less diverse and smaller gains are expected from fusion. The posteriors from the N-gram and NNLM have a correlation coefficient of 0.869 which is significantly lower than the correlation coefficient of 0.988 for a pair of randomly initialized NNLMs. This higher diversity of the NNLM and N-gram LM combination results in significant perplexity improvement upon interpolation. Almost all N-gram models are smoothed in various ways to assign non-zero probability estimates for word phrases unseen in training data. We use Kneser-Ney smoothing (Kneser & Ney (1995)) for all N-grams models in this paper.",
      "startOffset" : 61,
      "endOffset" : 1497
    }, {
      "referenceID" : 6,
      "context" : "Recurrent NNLM models of different topologies can be fused to get significant gains as well as demonstrated in (Mikolov et al. (2011)).",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)).",
      "startOffset" : 153,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi, . . . ,wi−N+1 denote the 1-in-V vectors of the history words. Let RH denote the D × V matrix that projects a history word vectors onto Ddimensional vectors RHwi, . . . ,RHwi−N+1. D is typically much smaller than the size V of the vocabulary with typical values being V = 10, 000 and D = 500. This resulting (N − 1)D-dimensional continuous representation of input words feeds into a neural network with one hidden layer that estimates the 1-in-V vector of the target word wi. The hidden neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron activation function is a V -way softmax. The back-propagation algorithm (Rumelhart et al. (1986)) trains an NNLM, often using stochastic gradient descent where the gradient is computed over several random subsets or batches of N-grams from the input training data.",
      "startOffset" : 153,
      "endOffset" : 916
    }, {
      "referenceID" : 0,
      "context" : "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi, . . . ,wi−N+1 denote the 1-in-V vectors of the history words. Let RH denote the D × V matrix that projects a history word vectors onto Ddimensional vectors RHwi, . . . ,RHwi−N+1. D is typically much smaller than the size V of the vocabulary with typical values being V = 10, 000 and D = 500. This resulting (N − 1)D-dimensional continuous representation of input words feeds into a neural network with one hidden layer that estimates the 1-in-V vector of the target word wi. The hidden neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron activation function is a V -way softmax. The back-propagation algorithm (Rumelhart et al. (1986)) trains an NNLM, often using stochastic gradient descent where the gradient is computed over several random subsets or batches of N-grams from the input training data. Researchers have proposed several variants of the feed forward NNLM, especially to model a longer input word history. Prominent examples include the recurrent neural network LM (Mikolov et al. (2010)) and bidirectional long-short term memory (LSTM) LM (Sundermeyer et al.",
      "startOffset" : 153,
      "endOffset" : 1284
    }, {
      "referenceID" : 0,
      "context" : "A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi, . . . ,wi−N+1 denote the 1-in-V vectors of the history words. Let RH denote the D × V matrix that projects a history word vectors onto Ddimensional vectors RHwi, . . . ,RHwi−N+1. D is typically much smaller than the size V of the vocabulary with typical values being V = 10, 000 and D = 500. This resulting (N − 1)D-dimensional continuous representation of input words feeds into a neural network with one hidden layer that estimates the 1-in-V vector of the target word wi. The hidden neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron activation function is a V -way softmax. The back-propagation algorithm (Rumelhart et al. (1986)) trains an NNLM, often using stochastic gradient descent where the gradient is computed over several random subsets or batches of N-grams from the input training data. Researchers have proposed several variants of the feed forward NNLM, especially to model a longer input word history. Prominent examples include the recurrent neural network LM (Mikolov et al. (2010)) and bidirectional long-short term memory (LSTM) LM (Sundermeyer et al. (2012)).",
      "startOffset" : 153,
      "endOffset" : 1363
    }, {
      "referenceID" : 4,
      "context" : "We conducted language modeling experiments on a standard split of the Penn Treebank (UPenn) data set (Marcus et al. (1993)), as used in previous works such as (Mikolov et al.",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "We conducted language modeling experiments on a standard split of the Penn Treebank (UPenn) data set (Marcus et al. (1993)), as used in previous works such as (Mikolov et al. (2011)).",
      "startOffset" : 102,
      "endOffset" : 182
    }, {
      "referenceID" : 1,
      "context" : "We implemented the DENNLM in Theano (Bergstra et al. (2011)) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (Tieleman & Hinton (2012)), a variant of stochastic gradient descent.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "We implemented the DENNLM in Theano (Bergstra et al. (2011)) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (Tieleman & Hinton (2012)), a variant of stochastic gradient descent.",
      "startOffset" : 37,
      "endOffset" : 196
    }, {
      "referenceID" : 1,
      "context" : "We implemented the DENNLM in Theano (Bergstra et al. (2011)) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (Tieleman & Hinton (2012)), a variant of stochastic gradient descent. We tuned all hyper-parameters on the standard UPenn heldout set. Table 4 shows the test set perplexities of the baseline and the DENNLMs. We kept the NN model size comparable by reducing the size of each component NNLM. Our results show a significant improvement in perplexity by using a DENNLM over the 4-gram model, a single NNLM, and interpolation of randomly initialized NNLMs. The posterior correlation coefficients are significantly less than 0.99, which is the correlation coefficient for randomly initialized NNLMs. Note that the DENNLM significantly outperforms a standard NNLM of size (600,800) which has similar number of parameters. It is also clearly better then a randomly initialized set of 4 NNLM models. The perplexity results in Table 4 for our diverse feed-forward NNLMs are especially encouraging given the fact that the more advanced recurrent neural network (RNN) LM gives a perplexity of 124.7 by itself and 105.7 upon interpolation with a 5-gram LM on the same task (Mikolov et al. (2011)).",
      "startOffset" : 37,
      "endOffset" : 1253
    } ],
    "year" : 2015,
    "abstractText" : "We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higherdimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.",
    "creator" : "LaTeX with hyperref package"
  }
}