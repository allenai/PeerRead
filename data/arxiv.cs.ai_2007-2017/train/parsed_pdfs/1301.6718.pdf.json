{
  "name" : "1301.6718.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Complexity of Policy Iteration",
    "authors" : [ "Satinder Singh" ],
    "emails" : [ "}@research.att.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Decision-making problems in uncertain or stochastic domains are often formulated as Markov decision processes (MD Ps). Pol icy iteration (PI) is a popular algorithm for searching over policy-space, the size of which is exponential in the number of states. We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor. In this paper we prove the first such non-trivial, worst-case, upper bounds on the number of iterations required by PI to converge to the optimal policy. Our analysis also sheds new light on the manner in which PI progresses through the space of policies.\n1 Introduction\nThe problem of decision-making in uncertain or stochastic environments is central to artificial intel ligence (AI) [7, 6]. The framework of Markov deci sion processes (MDPs) developed in the operations re search community [1] is increasingly used within AI to formulate such problems. In this formulation, the environment is assumed to be in one of a finite-set of states, the decision-making agent has a choice of ac tions in each state of the environment, executing an action causes a stochastic change in the state of the environment, and the agent receives a stochastic re ward in return for executing the action. The agent's goal is to choose actions so as to maximize a cumu lative discounted measure of rewards over some time\n*On sabbatical from Tel-Aviv University.\nhorizon. Here we consider the planning problem in which we are given a full description of the MDP and have to compute the optimal action-selection policy.\nOne reason for the popularity of the MDP framework within AI is the availability of a number of well-studied classes of algorithms for planning in MDPs: linear programming [2], value iteration [1], and policy itera tion [3]. Linear programming and value iteration are known to compute the optimal policy in time poly nomial in the size of the representation of the MDP and the discount factor [4, 2]. While no direct analy sis of policy iteration is available, one can bound the number of steps of \"greedy\" policy iteration (which greedily accepts all single-state action changes that are improvements) by the number of steps of value itera tion. This implies that policy iteration also runs in time polynomial in the size of the representation and the discount factor [3, 2].\nHowever, our goal is to derive bounds for solving MDPs that do not depend on the discount factor. For value iteration the dependence on the discount factor is unavoidable. For linear programming, in general, it is a major open problem whether there exists a strongly polynomial algorithm, i.e., runs in time polynomial in the number of parameters and independent of the size of the representation of the parameters. For PI we can bound the number of steps independent from the representation size and discount factor as follows: PI is guaranteed to improve the policy at every step and therefore the total number of steps is trivially upper bounded by the total number of policies. This bound is of course independent of the discount factor. However, note that the total number of policies is exponential in the number of states.\nIn this paper, we prove the first non-trivial upper\n402 Mansour and Singh\nbound on the worst-case number of steps PI can take. For the specific case of n states and two actions the total number of policies is exactly 2\". We show that \"greedy\" PI will take at most 0( 2;) steps. We also de fine a randomized PI (which accepts each single-state action change that is an improvement with probabil ity 0.5) and prove that in the worst-case it will take at most 0(2°·78\") steps. For the general case of k ac tions we show an bound of 0 ( k\" J n) for greedy PI and 0([(1 + Ek)k/2]\") for random PI (where fk is small for large k and will be defined later). Note that these bounds are independent of the size of representation of the specific parameters of the MDP and in particular do not depend on the discount factor. Our analysis also sheds new light on the manner in which PI pro gresses through the space of policies.\nWe view our results as the first step towards a better understanding of Pl. This is an important issue be cause there is strong empirical evidence in favor of PI over value iteration and linear programming in solving MDPs [4]. While in practice it is difficult to construct MDPs for which greedy PI takes more than n steps, no general rigorous lower bounds are known. A lower bound is known for a particular form of PI, called se quential PI (which at each step accepts only one of the single-state action changes that are improvements) - in the worst-case, sequential PI can take 11(2\") steps on a two-action MDP, when the adversary controls which improvements are selected [5, 4]. However it is not clear whether results about sequential PI trans fer to other forms of PI, e.g., greedy or random PI. In fact our results show that there is a gap between the worst-case complexities of sequential and both greedy and random Pl.\nThis paper is organized as follows: Section 2 defines the MDP model and its notation. Section 3 introduces the general scheme of policy iteration and proves a few general results concerning it. Section 4 derives an upper-bound on the time complexity of greedy PI, while Section 5 derives an upper-bound to the time complexity of random PI (both for two-action MDPs). Section 6 extends our results to a general multi-action MDP. Section 7 concludes with a summary of our con tributions and open problems.\n2 Model\nIn this section we define the Markov decision process (MDP) framework.\nDefinition 1 An MDP is a tuple (S, A, P, R): S is a finite set of states the environment can be in, A is a fi nite set of actions available to the agent, P is the table of tmnsition probabilities, where P(s'ls, a) is the proba bility of a tmnsition to state s' upon executing action a in state s, and R is the reward function, where R(s, a) is the expected reward received by the agent upon exe cuting action a in state s.\nWe define the agent's return to be the discounted sum of rewards over an infinite horizon, i.e., we use the infinite-horizon discounted framework in this paper. More formally, the agent's return is 2.::�0 -/ r t where rt is reward received at time step t, and 0 ::::; \"' < 1 is a discount factor that makes future reward less valuable than immediate reward. The agent's goal is to select actions so as to maximize its expected return. In infinite-horizon discounted MDPs the agents expected return is maximized by a policy (a mapping from states to actions), called the optimal policy.\nUseful quantities in analyzing MDP-decision-making are value functions: one defined over states and the other defined over state-action pairs.\nDefinition 2 Let V\" ( s) be the expected return if the start-state is s and the agent executes policy 7r forever. Let Q\" ( s, a) be the expected return if the start-state is s and the agent executes action a to begin with and thereafter follows policy 1r.\nNote that by the definition above Q\" (s, a) = R(s, a)+ \"f2.:,,P(s'ls, a)V\"(s'). The agent's goal, restated in terms of value functions, is that of finding an optimal policy rr* that satisfies 7r* = argmax\" V\". The opti mal value function V\"* is denoted simply as V* and the associated Q-value function as Q*. Note that there can be more than one optimal policy, however, V* and Q* are unique.\nThe total number of policies in an MDP is k\", where n = lSI and k = lA I. In most of the paper we discuss the case that there are only two actions, i.e. IAI = 2, which implies that the number of policies is bounded by 2n. At the end of the paper we discuss the general case, where IAI = k.\n3 General Policy Iteration\nGeneral policy iteration works as follows. At each iter ation consider changing the action at each state while keeping the actions for all the other states fixed to the\ncurrent policy. Some such single-state action changes will improve upon the current policy. Different vari ants of policy iteration differ in which single-state im provements they accept at each step.\nBefore we can describe general PI, we must define what it means for one policy to be better than another. We define a partial order between the policies as follows.\nDefinition 3 For two policies, rr and rr', we have rr > rr' if for each state s, V\"(s) � V\" ' (s), and for some state s , V\" (s) > V\" ' (s). If for every state s we have V\" (s) = V\" ' (s) then rr � rr'.\nThe partial ordering tells us when a policy is better than another and when they are incomparable. Clearly any optimal policy is better than all suboptimal poli cies and equivalent (�) to all other optimal policies. This partial order is central to our analysis.\nGiven a policy, rr, we can define, using the function Q\", the single-state improvements that could improve that policy. The following definition gives the necessary notation that we use later.\nDefinition 4 Given a policy rr, let the modification set T\" C S x A be the set of all pairs (s, a) such that changing the action of rr to a in state s improves the return of the policy, i.e. Q\"(s, a) > V\" (s). We define states(T\") to be the states that appear in T\", i. e. , { s : ( s, a) E T\"}. If each state appears only once m T\" we say that T\" is well defined.\nLet 1r be a policy such that T\" is well defined. (Note that if the MDP has only two action then for any policy rr we have that T\" is well defined.) For a set U C T\" let modify( 1r, U) define a policy rr' whose ac tions are the same as those of policy rr on states not in states(U) and rr'(s) =a for (s, a) E U.\nFigure 1 presents the general policy iteration algo rithm. In every iteration there are two basic steps: the first, Improvement Selection Step, selects which single-state improvements to make, and the second, Policy Improvement Step, modifies the policy accord ingly. Different methods for selecting subsets ofT\" to modify the policy lead to different PI algorithms.\nThe following two theorems are well known properties of general policy iteration. The first claims that ac cepting any non-zero number of single-state improve ments can only improve the policy, and the second claims that there a! ways exists at least one single-state improvement that improves the policy, unless the pol-\nOn the Complexity of Policy Iteration 403\nicy is already optimal. (For proofs see, e.g., [2].)\nTheorem 1 For any U C T\", let rr' = modify(rr, U). If U =f. 0 then rr' >- rr.\nTheorem 2 For any sub-optimal1r, T\" =f 0.\nThe above two theorems immediately imply that all instantiations of general PI strictly improve the policy at every iteration. Therefore, every iteration from a current to a next policy at least skips, or rules out, all the policies that are equal to, or better than, the current policy and worse than the next policy. How many such policies are there at every iteration? There is at least one such policy: the current policy itself. This, of course, implies an upper bound of kn steps. For specific improvement-selection methods, defined in the following sections, we perform a more careful anal ysis of the number of equal or better policies that get ruled out at each iteration. The more policies we can rule out at each iteration the better ·the upper-bound will be. Our analysis will be mainly based on proper ties derived from Theorems 1 and 2.\nIn the rest of this section, we prove a few properties that hold for all instances of Pl. The first is actually a property of the partial order itself: in general two policies may be incomparable but if they differ only in one state then they must be comparable.\nLemma 3 Let rr and 1r1 be two policies whose actions differ in only one state s, i. e. , rr(u) = 1r1(u) for u =f. s. Then either 1r >- 1r1, 1r1 >- 1r, or 1r � 11\"1•\nProof: If Q\"(s, 1r'(s)) > V\" (s) then tr' >- rr. If Q\" (s, 1r1 (s)) < V\" (s) then 1r >- tr'. Otherwise 1r � 1!\"1• 0\nThe following lemma gives an interesting connection between the optimality of a policy 1r and the states in its modification set T\".\nLemma 4 For any policy 1r, and any policy rr' that is identical to tr on states in states(T\"), either 1r >- 1r1, or 7r � 1r1•\nProof: Consider an MDP M' such that the only ac tion possible from a state s E states(T\") is 1r(s). Clearly both 1r and rr' are valid policies for M'. On the other hand in M' there is no local improvement for 1r, i.e. TM, = 0. By Theorem 2, 1r is optimal for M'. Therefore 1r >- 1r1 (or 1r � 1r1). 0\nFor an MDP with two actions we can show that PI,\n404 Mansour and Singh\nat different iterations, considers an improvement over a different subset of the states. This result is general to any PI.\nLemma 5 During a run of general policy iteration al gorithm on a two-action MDP, there are no i and j, i < j, such that states(T\";) <; states(T\"i).\nProof: We prove the lemma by contradiction. As sume that there exists i and j, i < j such that states(T\";) <; states(T\"i). LetT= states(T\";) <; states(T\"i). Let U' = {(s, a): a= 1r;(s) and 7r;(s) -=f 7rj(s) and s E T}. Clearly U' <; T\"i, since there are only two actions. Then 11\"1 =modify( 11\"j, U') is identi cal with 1r; on the states in T = states(T\";). There fore, by Lemma 4 we have that 1r; >- 11\"1 or 1r; � 1r1• This contradicts the fact that 1r1 >- 1l\"j >- 1!\";. D\nSo far we have showed that a subset of states can ap pear at most once in general policy iteration, when the MDP has only two actions. This still leaves open the possibility that all subsets appear in the run of the algorithm, and thus we observe all 2n policies, for a two-action MDP. The next step is to show that each time we perform modify on a large subset of the states we rule out many policies.\n4 Greedy Policy Iteration\nGreedy policy iteration is PI with select(T) = T, namely, we perform all the possible single-state action improvements at each policy improvement step. (We assume that T is well defined, which is always the case for two-action MDPs. For the general case see Sec tion 6.)\nThe next lemma shows that each time we perform a modify operation we rule out a number of policies that\nis at least the the size of the modification set.\nLemma 6 Let 1r be a policy such that T\" is well de fined, and 11\"1 = modify( 1r, T\"). Then there are at least IT\" I policies 11\";, 1 ::; i ::; IT\" I, such that 11\"1 � 11\"; >- 1r.\nProof: We show by induction on m, that if IT\" I ?: m then there are at least m policies 11\";, 1 ::; i ::; m, such that 11\"1 � 1r; >- 1!\". The base of the induction, m = 1, follows from Theorem 1.\nFor the inductive step we assume that the claim holds for m-1 and we show that it holds form. Assume that IT\" I ?: m. Consider all the single state modifications to 1r using T\", i.e., consider all Zj, such that Zj C T\" and I Zj I = 1. Let U 1 = Zj such that for any Z;, we have that modify(1r, Zj) '/- modify(1r, Z;). (Note that Zj is not necessarily unique, since we have a partial order.) Let 1r1 = modify(1r, U1). With out loss of generality, let U1 = {(s1, h)}. For any other pair ( s;, b;) E T\" , for i > 1, we show that (s;,bi) E T\"'· Let 1ri = modify(7r1,{(s;,b;)}). By Lemma 3 we know that either 1r1 >- 1r( or 1ri � 11\"!· We would like to claim that the relation 1r1 >- 1rj is not possible.\nFor contradiction assume that 1r1 >- 1r(. Con sider q; modify(7r,{(s;,b;)}). Note that 1ri = modify( 1r, { ( s1, h), ( s;, b;)}), and therefore by Lemma 3 we know that either q; >- 1r( or 1r( � q;. If 1ri � q; then 1r1 >- 1r( � q; contradicting the min imality of 1r1. Therefore q; >- 1r(. Let 1r( sl) = a1 and 1r(s;) = a;. Since 11\"! >- 1ri, this implies that (s;, a;) E T\"i and similarly, since q; >- 1ri, this im plies that (s1, a!) E T\";. By Theorem 1 this implies that\ncontradicting the fact that 1ri � 1r. Hence, 1ri � 1r1. This implies that ( s;, b;) E T\"1, for i > 1. Therefore, we have IT\"' I 2: IT\" I - 1 2: m - 1. The lemma follows from the inductive hypothesis on 1r1. D We can now state and prove our upper-bound on the number of steps of greedy policy iteration.\nTheorem 7 The greedy policy iteration algorithm considers at most 0(2n In) different policies for a 2- action MDP.\nProof: The analysis has two parts. The first part includes the case where the set T\" is small. For this case we simply show that there are very few such poli cies. The second case will include the cases when T\" is large. For this case we show that each iteration elim inates O(n) policies, that have not been eliminated before.\nWe define a set to be small if IT\" I :S nl3. By Lemma 5 we do not consider the same set of states twice. This bounds the number of such modifications by\nwhere the second inequality holds for n 2: 3. (The first inequality follows from the fact that for k < nl3 we have that G) I (k�1) > 2.) For policies 1!\"; such that IT\"; I 2: nl3, by Lemma 6 we have that at least nl3 policies better than or equal to our current policy are ruled out after this iteration. This implies that the total number of policies that we consider is bounded by,\nwhere the first term is the number of policies with small number of improvements and the second term is a bound on the number of policies with a large number of improvements. D\n5 Random Policy Iteration\nFormally, random policy iteration defines select(T) as a random subset of T where each subset has prob ability 2-ITI. (We assume that Tis well defined. For the general case seee Section 6.) Intuitively, we can think of random policy iteration as deciding to accept\nOn the Complexity of Policy Iteration 405\neach local improvement with probability half. Even though we allow for the empty subset for convenient proofs, in practice one may ignore such iterations.\nThe property that we would like to prove is that for a two-action MDP the number of policies that we rule out after considering each policy is at least 21T\"q-l rather than only IT\"; I (as in greedy policy iteration). This enables us to improve our bound on the running time significantly.\nWe first show another property of general PI: that no policy 1!\"1 incomparable to 1r; is ever considered after iteration i.\nLemma 8 Consider a run of a general policy iteration algorithm, and let 1r; be the policy at iteration i. Let 1!\"1 be a policy such that rr' f 1r;. For any j > i we have that ITj -=J rr'.\nProof: By Theorem 1 we know that for each j we have that 1t\"j � 1t\"j-l· By transitivity, we have that 1t\"j � 1t\"i+l, for j > i + 1. Since 1!\"1 f 1r;, it implies that �-::j:;�. D\nFrom the above lemma we know that the only policies that we can reach after 1!\"; are policies that are com parable with 1r;. This implies that any policy which is either strictly inferior to 1r;, or incomparable to 1!\"; will never be considered. The next step is to argue that the number of policies that we rule out at phase i has an expected value of at least 21r·q-l. We first prove a general property of selecting a random element in a partial order.\nLemma 9 Let � be a partial order over II. If we chose a random element r E II, with uniform prob ability, then the expected number of elements s E II such that s � r is at most 111112.\nProof: For any element v E II we associate two sets. Tit includes all the elements s such that s � v, and II; includes all the elements s such that v � s. For every pair of elements v1 � v2 we have that v1 E Tit, and v2 E II;,. This implies that\nL IIItl = L III; I :S 1�12. vEII vEII\nTherefore the expected value of lilt I is at most 111112. D\nThe following corollary combines Lemma 8 and Lemma 9.\n406 Mansour and Singh\nCorollary 10 Consider a run of the random policy it eration algorithm on a two-action MDP. Let 1r; be the policy at iteration i, then the expected number of poli cies rr', such that rri+1 >- rr' >- rr; is at least 21T\"•I-l.\nUnfortunately it is not true that at each step we ex pect to rule out !1(21T\"'i) policies, with high proba bility. Rather we can say that there is some constant probability that this will happen, and then claim that in a run with m iterations we should have, with high probability, this occurring n(m) times.\nTheorem 11 The random policy iteration algorithm, for a two-action MDP, considers at most 0(2°·78\") dif ferent policies, with probability 1 - 2-2\"(•).\nProof: As before we consider two cases, that of small sets and that of large sets. We define a set to be small if IT\"• I :::; pn, where the constant p > 0 will be selected later. As before we bound the number of iterations with small sets by I:f:o (7) :::; 2H(p)n+l, where H(p) is the binary entropy, i.e. H(p) = - p log p - (1 - p) log(l - p).\nNow we are interested in bounding the number of it erations with large sets. Assume that we have m such iterations. By Corollary 10 the expected number of policies we rule out is at least 2Pn-l policies, at each such iteration. This implies that with probability 1/3 we rule out at least 2pn-2 policies. (If this occurs with probability strictly less than 1/3, then the ex pected number of policies we rule out is strictly less than (1/3)2Pn + (2/3)2Pn-2 = 2Pn-1, which contra dicts Corollary 10.)\nAn iteration with a large set is good if it chooses a set that rules out at least 2pn-2 policies. From above, the probability that an iteration is good is at least 1/3. A run is called typical if at least m/4 of the m iterations with large sets are good. The number of large set iterations in a typical run is bounded by 2(l-p)n+4. The total number of iterations in a typical run is bounded by,\nfor p = 0. 227 and sufficiently large n.\nThe probability that a run is not typical is at most e-(1/3-1/4)2m. We are interested in runs in which m 2: 2(l-p)n+4, in which case the probability is bounded by 2-2\"(•) 0\n6 Multi-Action MDPs\nIn this section we extend the results from two-action to k actions, where k 2: 2. Recall that since we have k actions the total number of policies is kn.\nFirst we observe that when there are more than two actions, it might be the case that we have in T\" a number of different pairs with the same state, i.e. T\" is not well defined. We assume that T\" is reduced to L\", such that each state appears only in one pair, i.e. L\" is well defined. Formally, L\" C T\" and states(£\") = states(T\"). We do not make any other assumption on the way L\" is chosen, and assume that the various PI algorithms perform U +--select(£\"). Using the Lemma4 we can derive the following lemma.\nLemma 12 During a run of a general policy itera tion algorithm, there are no i and j, i < j, such that states(T\"') � states(T\"i) and for every s E states(T\"') we have rr;(s) = 1t\"j(s).\nProof: The proof is by contradiction. Assume that such i and j exists. By Lemma 4 we have that rr; >- \"Trj or 1r; :::e \"Trj. By Theorem 1 we have that \"Trj >- rr;, since j > i, and therefore we have a contradiction. 0\nThe above lemma is the main difference between the two-action case and the multi-action case. This dif ference results in slightly worse bounds. As in the two action case, our analysis separates the modifica tion sets to small and large. The following corollary of Lemma 12 is used to bound the number of small modifications.\nCorollary 13 During a run of a general policy it eration algorithm, the number of iterations in which 1£\" • I :::; d is bounded by I:t =O (j) ki .\nWe start by bounding the number of iterations, in the worst case, performed by the greedy policy iteration algorithm. Note that Lemma 6 applies to L\", since L\" is well defined. The following theorem bounds the number of iterations for the greedy policy iteration algorithm in the multi-action case. (The proof is in the same spirit as the two-action case, but the constants are different.)\nTheorem 14 The greedy policy iteration algorithm considers at most O(kn /n) different policies.\nProof: As in the proof of Theorem 7, the analysis has two parts. The first part includes the case where the\nset L\" is small. For this case we simply show that there are very few such policies. The second case includes the case when L\" is large. For this case we show that each iteration eliminates !1(n) policies, that have not been eliminated before.\nWe define a modification set to be small if IL\" I ::; pn, where p = 1/10. By Corollary 13, the number of small modification sets is bounded by,\n� (�)kj::; 2�:)kpn::; l:, for k � 2 and n � 1.\nFor policies 1!\"; such that IL\"; I � pn, by Lemma 6 we have that at least pn policies better than or equal to our current policy are ruled out after this iteration. This implies that the maximum number of policies that greedy PI considers is bounded by,\nk\" k\" k\" 3-+- = 13-, n pn n\nwhere the first term is the number of policies with small number of improvements and the second term is a bound on the number of policies with a large number of improvements. D\nWe now show the bound for random policy iteration. First note that Lemma 8 holds for L\", since L\" is well defined. In addition Lemma 9 is a general property of partial orders. Therefore, we can derive a corollary similar to Corollary 10.\nCorollary 15 Let 1!\"; be the policy at iteration i, then the expected number of policies 11\"1, such that 1l\"i+I >- 11\"1 >- 1!\"; is at least 2IL\"q-1•\nNow we can derive the theorem for the random policy iteration algorithm for the multi-action case.\nTheorem 16 The random policy iteration algorithm considers at m ost\ndifferent policies, with probability 1 - 2-n((k/2)\").\nProof: As in Theorem 11 we consider two cases, that of small sets and that of large sets. We define a set to be small if IL\"; I ::; pn, where p = 1- 2/ log k. By Corollary 13, the number of iterations with small sets is bounded by Ef�o (7)ki ::; 2\" kP\".\nOn the Complexity of Policy Iteration 407\nNow we are interested in bounding the number of it erations with large sets. Assume that we have m such iterations. By Corollary 10 the expected number of policies we rule out is at least 2pn-I policies, at each such iteration. This implies that with probability 1/3 we rule out at least 2Pn-2 policies. An iteration with a large modification set is good if it chooses a set that rules out at least 2P\"- 2 policies. From above, the probability that an iteration is good is at least 1/3. A run is called typical if at least m/4 of the m iterations with large sets are good. The number of large set iterations in a typical run is bounded by k\" /2P\"-4. The total number of iterations in a typical run is bounded by,\n<\nfor k � 2 and n � 1.\nThe probability that a run is not typical is at most c(!/3-1/4)2m. We are interested in runs in which m :0:: (k/2)\", in which case the probability is bounded by 2-n((k/2) \"). 0\n7 Conclusion\nIn this paper we developed a proof technique for de riving upper-bounds on the number of steps required by policy iteration to find an optimal policy. Using our proof technique we are able to establish non-trivial upper-bounds for two important variations of policy iterations.\nFor greedy policy iteration we proved an upper-bound of a en·), and for random policy iteration we proved an upper-bound of 0(2°·78n), both in the case that the MDP has two actions. This should be contrasted with the lower-bound of !1(2\") for sequential policy iteration [5, 4]. For the case of k actions we give upper bounds of O( kn\n\") and 0([(1 + <'k )k/2]\" ), for the greedy and random policy iteration algorithms, respectively.\nWe have no reason to believe that our bounds are tight. One case where our bounds seems to be \"losing\" con siderably is the following. When counting policies that we rule out we consider only policies that we can reach from 1r; using its modification set T\"; . However, in many cases we can rule out additional policies. An other constraint that we were not able to utilize is the\n408 Mansour and Singh\nbenefit of having small modification sets. For example if T\" • = { ( s, a)} then in the two-action case we can rule out half of the possible modification sets. More precisely, we will never have to update the action of state s again. Unfortunately, we did not find a way to take advantage of this property, and we use Lemma 5 only in the sense that the modification sets cannot be equal, rather than the subset property.\nIt would have been of great benefit if we had good lower bounds for general policy iteration, but unfortu nately we do not know of any bound other than the trivial lower-bound of n. The gap between upper and lower bounds is still very large and is an interesting subject for future research.\nReferences\n[1] D. P. Bertsekas. Dynamic Programming: Deter ministic and Stochastic Models. Prentice-Hall, En glewood Cliffs, NJ, 1987.\n[2] D. P. Bertsekas. Dynamic Programming and Op timal Control. Athena Scientific, Belmont, MA, 1995.\n[3] R. Howard. Dynamic Programming and Markov Processes. MIT Press, Cambridge, MA, 1960.\n[4] M. L. Littman. Algorithms for Sequential Decision Making. PhD thesis, Brown University, 1996.\n[5] M. Melekopoglou and A. Condon. On the complex ity of policy iteration for stochastic games. Techni cal Report CS-TR-90-941, Computer Sciences De partment, University of Wisconsin, Madison, 1990.\n[6] S. J. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall, Englewood Cliffs, New Jersey, 1995.\n[7] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cam bridge, MA, 1998."
    } ],
    "references" : [ {
      "title" : "Dynamic Programming: Deter­ ministic and Stochastic Models. Prentice-Hall, En­",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1987
    }, {
      "title" : "Dynamic Programming and Op­ timal Control",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1995
    }, {
      "title" : "Dynamic Programming and Markov Processes",
      "author" : [ "R. Howard" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1960
    }, {
      "title" : "Algorithms for Sequential Decision Making",
      "author" : [ "M.L. Littman" ],
      "venue" : "PhD thesis, Brown University,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1996
    }, {
      "title" : "On the complex­ ity of policy iteration for stochastic games",
      "author" : [ "M. Melekopoglou", "A. Condon" ],
      "venue" : "Techni­ cal Report CS-TR-90-941,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1990
    }, {
      "title" : "Artificial Intelligence: A Modern Approach",
      "author" : [ "S.J. Russell", "P. Norvig" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1995
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "The problem of decision-making in uncertain or stochastic environments is central to artificial intel­ ligence (AI) [7, 6].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "The problem of decision-making in uncertain or stochastic environments is central to artificial intel­ ligence (AI) [7, 6].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "The framework of Markov deci­ sion processes (MDPs) developed in the operations re­ search community [1] is increasingly used within AI to formulate such problems.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "One reason for the popularity of the MDP framework within AI is the availability of a number of well-studied classes of algorithms for planning in MDPs: linear­ programming [2], value iteration [1], and policy itera­ tion [3].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "One reason for the popularity of the MDP framework within AI is the availability of a number of well-studied classes of algorithms for planning in MDPs: linear­ programming [2], value iteration [1], and policy itera­ tion [3].",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : "One reason for the popularity of the MDP framework within AI is the availability of a number of well-studied classes of algorithms for planning in MDPs: linear­ programming [2], value iteration [1], and policy itera­ tion [3].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 3,
      "context" : "Linear programming and value iteration are known to compute the optimal policy in time poly­ nomial in the size of the representation of the MDP and the discount factor [4, 2].",
      "startOffset" : 169,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "Linear programming and value iteration are known to compute the optimal policy in time poly­ nomial in the size of the representation of the MDP and the discount factor [4, 2].",
      "startOffset" : 169,
      "endOffset" : 175
    }, {
      "referenceID" : 2,
      "context" : "This implies that policy iteration also runs in time polynomial in the size of the representation and the discount factor [3, 2].",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "This implies that policy iteration also runs in time polynomial in the size of the representation and the discount factor [3, 2].",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "This is an important issue be­ cause there is strong empirical evidence in favor of PI over value iteration and linear programming in solving MDPs [4].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "A lower­ bound is known for a particular form of PI, called se­ quential PI (which at each step accepts only one of the single-state action changes that are improvements) in the worst-case, sequential PI can take 11(2\") steps on a two-action MDP, when the adversary controls which improvements are selected [5, 4].",
      "startOffset" : 307,
      "endOffset" : 313
    }, {
      "referenceID" : 3,
      "context" : "A lower­ bound is known for a particular form of PI, called se­ quential PI (which at each step accepts only one of the single-state action changes that are improvements) in the worst-case, sequential PI can take 11(2\") steps on a two-action MDP, when the adversary controls which improvements are selected [5, 4].",
      "startOffset" : 307,
      "endOffset" : 313
    }, {
      "referenceID" : 1,
      "context" : ", [2].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 4,
      "context" : "This should be contrasted with the lower-bound of !1(2\") for sequential policy iteration [5, 4].",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "This should be contrasted with the lower-bound of !1(2\") for sequential policy iteration [5, 4].",
      "startOffset" : 89,
      "endOffset" : 95
    } ],
    "year" : 2011,
    "abstractText" : "Decision-making problems in uncertain or stochastic domains are often formulated as Markov decision processes (MD Ps). Pol­ icy iteration (PI) is a popular algorithm for searching over policy-space, the size of which is exponential in the number of states. We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor. In this paper we prove the first such non-trivial, worst-case, upper bounds on the number of iterations required by PI to converge to the optimal policy. Our analysis also sheds new light on the manner in which PI progresses through the space of policies.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}