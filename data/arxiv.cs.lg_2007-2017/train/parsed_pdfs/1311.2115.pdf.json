{
  "name" : "1311.2115.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An adaptive low dimensional quasi-Newton sum of functions optimizer",
    "authors" : [ "Jascha Sohl-Dickstein", "Ben Poole", "Surya Ganguli" ],
    "emails" : [ "JASCHA@STANFORD.EDU", "POOLE@CS.STANFORD.EDU", "SGANGULI@STANFORD.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "A common problem in computer science is to find a vector x∗ ∈ RM which minimizes a function F (x), where F (x) is a sum of N computationally cheaper differentiable subfunctions fi (x),\nF (x) = N∑ i=1 fi (x) , (1)\nx∗ = argmin x F (x) . (2)\nMany optimization tasks fit this form (Boyd & Vandenberghe, 2004), including training of autoencoders, support vector machines, and logistic regression algorithms, as well as parameter estimation in probabilistic models (SohlDickstein et al., 2011). In statistics, x∗ as defined by Equations 1 and 2 is referred to as an M-estimator (Huber, 1981).\nThere are two general approaches to efficiently optimizing a function of this form. The first is to use a quasi-Newton method (Dennis Jr & Moré, 1977), of which BFGS (Dennis Jr & Moré, 1977) or LBFGS (Liu & Nocedal, 1989) are the most common choice. Quasi-Newton methods use the history of gradient evaluations to build up an approximation to the inverse Hessian of the objective function F (x). By making descent steps which are scaled by the approximate inverse Hessian, and which are therefore longer in directions of shallow curvature and shorter in directions of steep curvature, quasi-Newton methods can be orders of magnitude faster than steepest descent. Additionally, quasiNewton techniques typically require adjusting few or no hyperparameters, because they use the measured curvature of the objective function to set step lengths and directions. However, direct application of quasi-Newton methods requires calculating the gradient of the full objective function F (x) at every proposed parameter setting x, which can be very computationally expensive, especially for large N .\nThe second approach is to use a variant of Stochastic Gradient Descent (SGD) (Robbins & Monro, 1951; Bottou, 1991). In SGD, only one subfunction’s gradient is evaluated per update step, and a small step is taken in the negative gradient direction. More recent descent techniques like IAG (Blatt et al., 2007), SAG (Roux et al., 2012), and MISO (Mairal, 2013) instead take update steps in the average gradient direction. For each update step, they evaluate the gradient of one subfunction, and update the average gradient using its new value. If the subfunctions are similar, then SGD can also be orders of magnitude faster than steepest descent on the full batch. However, because a different subfunction is evaluated for each update step, the gradients for each update step cannot be combined in a straightforward way to estimate the inverse Hessian of the full objective function. Additionally, efficient optimization with SGD typically involves tuning a number of hyperpaar X\niv :1\n31 1.\n21 15\nv1 [\ncs .L\nG ]\n9 N\nov 2\n01 3\nrameters, which can be a painstaking and frustrating process. (Ngiam & Coates, 2011) compares the performance of stochastic gradient and quasi-Newton methods on neural network training, and finds both to be competitive.\nCombining quasi-Newton and stochastic gradient methods could improve optimization time, and reduce the need to tweak optimization hyperparameters. This problem has been approached from a number of directions. In (Schraudolph et al., 2007; Sunehag et al., 2009) a stochastic variant of LBFGS is proposed. In (Martens, 2010), (Byrd et al., 2011), and (Vinyals & Povey, 2011) stochastic versions of Hessian-free optimization are implemented and applied to optimization of deep networks. In (Lin et al., 2008) a trust region Newton method is used to train logistic regression and linear SVMs using minibatches. Stochastic meta-descent (Schraudolph, 1999), AdaGrad (Duchi et al., 2010), and SGD-QN (Bordes et al., 2009) rescale the gradient independently for each dimension, and can be viewed as accumulating something similar to a diagonal approximation to the Hessian. All of these techniques treat the Hessian on a subset of the data as a noisy approximation to the full Hessian. To reduce noise, they rely on regularization and large minibatches to descend F (x) despite these noisy Hessian observations. Thus, unfortunately each update step requires the evaluation of many subfunctions and/or yields a highly regularized (i.e. diagonal) approximation to the full Hessian.\nWe develop a novel second-order quasi-Newton technique that only requires the evaluation of a single subfunction per update step. In order to achieve this substantial simplification, we treat the full Hessian of each subfunction as a direct target for estimation, thereby maintaining a separate quadratic approximation of each subfunction. This approach differs from all previous work, which in contrast treats the Hessian of each subfunction as a noisy approximation to the full Hessian. Our approach allows us to combine Hessian information from multiple subfunctions in a much more natural and efficient way than previous work, and avoid the use of large minibatches per update step to accurately estimate the full Hessian. Moreover, we develop a novel method to maintain computational tractability of this quasi-Newton method in the face of high dimensional optimization problems (large M ), by storing and manipulating the subfunctions in a shared, adaptive low dimensional subspace, determined by the recent history of the gradients and positions.\nThus our optimization method can usefully estimate and utilize powerful second-order information inherent in the total function F (x) while simultaneously combatting two potential sources of computational intractability: large numbers of subfunctions (large N) and a high-dimensional optimization domain (large M). Moreover, the use of a sec-\nond order approximation means that minimal or no adjustment of hyperparameters is required. We refer to the resulting algorithm as Sum of Functions Optimizer (SFO). We demonstrate that the combination of techniques and new ideas inherent in SFO results in faster optimization on five disparate example problems. Finally, we release the optimizer and the test suite as an open source Python package."
    }, {
      "heading" : "2. Algorithm",
      "text" : "Our goal is to combine the benefits of stochastic and quasiNewton optimization techniques. We first describe the general procedure by which we optimize the parameters x. We then describe the process by which an independent online Hessian approximation is maintained for each subfunction. This is followed by an explanation of the construction of the shared low dimensional subspace which makes the algorithm tractable for large problems. Finally, we end this section with a review of implementation details."
    }, {
      "heading" : "2.1. Approximating Functions",
      "text" : "We define a series of functions Gt (x) intended to approximate F (x),\nGt (x) = N∑ i=1 gti (x) , (3)\nwhere the superscript t indicates the learning iteration. The functions gti (x) will be stored, and one of them will be updated per learning step. Each gti (x) serves as a quadratic approximation to the corresponding fi (x)."
    }, {
      "heading" : "2.2. Update Steps",
      "text" : "As is illustrated in Figure 1, optimization is performed by repeating the steps:\n1. Choose a vector xt by minimizing the approximating objective function Gt−1 (x),\nxt = argmin x Gt−1 (x) . (4)\nNote that Gt−1 (x) can be minimized in closed form, since it is a sum of quadratic functions gt−1i (x).\n2. Choose an index j ∈ {1...N}, and update the corresponding approximating subfunction gti (x) using a second order power series around xt, while leaving all other subfunctions unchanged,\n2 (x) (red dashed lines). The sum of the approximating functions G\nt−1 (x) (solid red line)\napproximates the full objective F (x). (b) The next parameter setting xt is chosen by minimizing the approximating function Gt−1 (x) from the prior update step. See Equation 4. (c) After each parameter update, the quadratic approximation for one of the subfunctions is updated using a second order expansion around the new parameter vector xt. See Equation 5. The constant and first order term in the expansion are evaluated exactly, and the second order term is estimated by performing BFGS on the subfunction’s history. In this case the approximating subfunction gt1 (x) is updated (long dashed red line). This update is also reflected in the full approximating function Gt (x) (solid red line). Optimization proceeds by repeating these two illustrated update steps.\ngti (x) =  gt−1i (x) i 6= j fi (xt)+ (x− xt)T f ′i (xt)\n+ 12 (x− x t) T Hti (x− xt)  i = j . (5)"
    }, {
      "heading" : "2.3. Online Hessian Approximation",
      "text" : "The constant and first order term in Equation 5 are set by evaluating the subfunction and gradient, fj (xt) and f ′j (x\nt). We set the quadratic term Htj by using the BFGS (Dennis Jr & Moré, 1977) algorithm to generate an online approximation to the true Hessian of subfunction j based on its history of gradient evalutaions1.\nFor the subfunction j, we construct two matrices, ∆f ′ and ∆x. Each column of ∆f ′ holds the change in the gradient of subfunction j between successive evaluations of that subfunction, including all evaluations up until the present time. Each column of ∆x holds the corresponding change in the position x between successive evaluations. Both matrices are truncated after a number of columns L, meaning that they include information from only the prior L + 1 gradient evaluations for each subfunction. For all results in this paper, L = 10 (identical to the default history length for the LBFGS implementation used in Section 4).\n1We additionally experimented with Symmetric Rank 1 (Dennis Jr & Moré, 1977) updates to the approximate Hessian, but found they performed consistently worse than BFGS."
    }, {
      "heading" : "2.3.1. BFGS UPDATES",
      "text" : "The BFGS algorithm functions by iterating through the columns in ∆f ′ and ∆x, from oldest to most recent. Let s be the column index, and Bs be the approximate Hessian for subfunction j after processing column s. For each s, the approximate Hessian matrix Bs is set so that it obeys the secant equation ∆f ′s = Bs∆xs for the corresponding columns, where ∆f ′s and ∆xs are taken to refer to the sth columns of the gradient difference and position difference matrix respectively.\nIn addition to satisfying the secant equation, Bs is chosen such that the difference between it and the prior estimate Bs−1 has the smallest weighted Frobenius norm2. This produces the update equation\nBs = Bs−1 + ∆f ′s∆f ′T s\n∆f ′Ts ∆xs − Bs−1∆xs∆x\nT s Bs−1\n∆xTs Bs−1∆xs . (6)\nThe final update is used as the approximate Hessian for subfunction j, Htj = Bmax(s).\n2The weighted Frobenius norm is defined as ||E||F,W =\n||WEW||F . For BFGS, W = B − 1 2 s (Papakonstantinou, 2009). Equivalently, in BFGS the Frobenius norm is minimized after linearly mapping the new approximate Hessian into the identity matrix."
    }, {
      "heading" : "2.3.2. THE FIRST BFGS STEP",
      "text" : "The initial approximate Hessian matrix used in BFGS is set to a scaled identity matrix, so that B0 = βI. The scaling factor β is set to the smallest non-zero eigenvalue of a matrix Q,\nβ = min λQ>0 λQ. (7)\nwhere λQ indicates the eigenvalues of Q. Q is the symmetric matrix with the smallest Frobenius norm which is consistent with the squared secant equations for all columns in ∆f ′ and ∆x. That is,\nQ = [ (∆x) +T (∆f ′) T ∆f ′(∆x) + ] 1 2 , (8)\nwhere + indicates the pseudoinverse, and 12 indicates the matrix square root. All of the eigenvalues of Q are nonnegative. Equations 7 and 8 are computed in the subspace defined by ∆f ′ and ∆x, reducing computational cost (see Table 1).\nThe use of the smallest non-zero eigenvalue can be motived by observing that β sets the approximate Hessian in all unexplored directions in parameter space. Gradient descent routines tend to progress from directions with large slopes and curvatures, and correspondingly large eigenvalues, to directions with shallow slopes and curvatures, and smaller eigenvalues. The typical eigenvalue in an unexplored direction is thus expected to be smaller than in the previously explored directions. Equation 7 sets β to the smallest eigenvalue of Q in an explored direction, and may thus be a reasonable guess for the curvature in unexplored directions."
    }, {
      "heading" : "2.3.3. ENFORCING POSITIVE DEFINITENESS",
      "text" : "It is typical in quasi-Newton techniques to enforce that the Hessian approximation remain positive definite. In SFO each Hti is constrained to be positive definite by performing an eigendecomposition, and setting any eigenvalues which are too small to the median positive eigenvalue. If λmax is the maximum eigenvalue of Hti, then any eigenvalues smaller than γλmax are set to be equal to medianλ>0 λ. The median is used because it provides a measure of “typical” curvature, and when an eigenvalue is negative or extremely small it is an indication that it cannot be trusted as a measure of curvature. For all experiments shown here, γ = 10−8."
    }, {
      "heading" : "2.3.4. PROPERTIES",
      "text" : "If the Hessian is constant, then BFGS will eventually converge to the true Hessian (Dennis Jr & Moré, 1977). However, the updates in Equation 6 can make Bs inconsistent with the secant equation for earlier steps r, r < s. This\nmakes BFGS particularly effective in settings where the Hessian is changing over the course of the learning trajectory, since more recent gradient evaluations tend to overwrite older evaluations."
    }, {
      "heading" : "2.4. A Shared, Adaptive Low-Dimensional Representation",
      "text" : "The dimensionality M of x ∈ RM is typically large. As a result, the memory and computational cost of working directly with the matrices Hti ∈ RM×M and the history terms for each subfunction ∆f ′ and ∆x is typically prohibitive. To reduce the dimensionality M to a tractable value, all history is stored and all updates computed in a lower dimensional subspace, with dimensionality between Kmin and Kmax. The subspace is constructed such that it includes the most recent gradient and position for every subfunction. By construction, it therefore includes the steepest gradient descent direction.\nFor the results in this paper,Kmin = 2N andKmax = 3N . The subspace is represented by the orthonormal columns of a matrix Pt ∈ RM×Kt , (Pt)T Pt = I. Kt is the subspace dimensionality at optimization step t."
    }, {
      "heading" : "2.4.1. EXPANDING THE SUBSPACE WITH A NEW OBSERVATION",
      "text" : "At each optimization step, an additional column is added to the subspace, expanding it to include the most recent gradient direction. This is done by first finding the component in the gradient vector which lies outside the existing subspace, and then appending that component to the current subspace,\nqorth = f ′ j\n( xt ) −Pt−1 ( Pt−1 )T f ′j ( xt ) , (9)\nPt = [ Pt−1\nqorth ||qorth||\n] , (10)\nwhere j is the subfunction updated at time t. The new position xt is included automatically, since the position update was computed within the subspace Pt−1. Vectors embedded in the subspace Pt−1 can be updated to lie in Pt simply by appending a 0, since the first Kt−1 dimensions of Pt consist of Pt−1."
    }, {
      "heading" : "2.4.2. RESTRICTING THE SIZE OF THE SUBSPACE",
      "text" : "In order to prevent the dimensionality Kt of the subspace from growing too large, whenever Kt > Kmax, the subspace is collapsed to only include the most recent gradient and position measurements from each subfunction. The orthonormal matrix representing this collapsed subspace is computed by a QR decomposition on the most recent gradients and positions. A new collapsed subspace is thus com-\nputed as, P′ = orth ([ f ′1 ( xτ t 1 ) · · · f ′N ( xτ t N ) xτ t 1 · · ·xτ t N ]) ,\n(11)\nwhere τ ti indicates the learning step at which the ith subfunction was most recently evaluated, prior to the current learning step t. Vectors embedded in the prior subspace P are projected into the new subspace P′ by multiplication with a projection matrix T = (P′)T P. Vector components which point outside the subspace defined by the most recent positions and gradients are lost in this projection.\nNote that the subspace P′ lies within the subspace P. The QR decomposition and the projection matrix T are thus both computed within P, reducing the computational cost (see Section 3.1)."
    }, {
      "heading" : "2.5. Choosing a Target Subfunction",
      "text" : "The subfunction j to update in Equation 5 is chosen as,\nj = argmax i\n[ xt − xτi ]T Ht [ xt − xτi ] , (12)\nwhere τi indicates the time at which subfunction i was last evaluated.\nThat is, the updated subfunction is the one which was last evaluated farthest from the current location, using the approximate Hessian as a metric. This is motivated by the observation that the approximating functions which were computed farthest from the current location tend to be the functions which are least accurate at the current location, and therefore the most useful to update. This contrasts with the cyclic choice of subfunction in (Blatt et al., 2007), and the random choice of subfunction in (Roux et al., 2012).\nIn exploratory experiments we found that choosing the function to evaluate based on distance led to better optimization. For instance, for the protein logistic regression objective in Section 4, the objective value after 25 effective passes through the data is 1.045 for SFO using the distance metric. Using the random function ordering however it is only 1.066, and using cyclic function ordering it is 1.191."
    }, {
      "heading" : "2.6. Growing the Batch Size",
      "text" : "For many problems of the form in Equation 1, the gradient information is nearly identical between the different subfunctions early in learning. In order to achieve faster initial convergence we begin with a small number of active subfunctions. We then increment the number of subfunctions every time the average gradient shrinks to within a factor α of the standard error in the average gradient. This comparison is performed using the inverse approximate Hessian as the metric. That is, we increment the batch size by one\nwhenever\n( f̄ ′t )T Ht −1 f̄ ′t < α ∑ i (f ′t i ) T Ht −1 f ′ti\n(N t − 1)N t , (13)\nwhere N t is the active batch size at time t, Ht is the full Hessian, and f̄ ′t is the average gradient,\nf̄ ′t = 1\nN t ∑ i fi ′ (xti) . (14)\nFor all the experiments shown here, α = 1, and the initial batch size is N1 = 2. The active batch size is also increased by 1 when a bad update is detected, as described in Section 2.7."
    }, {
      "heading" : "2.7. Detecting bad updates",
      "text" : "A heuristic detects extremely bad updates, and resets xt to its previous value xt−1 when they are detected. This is triggered whenever the value of a subfunction has increased since its previous evaluation, and also exceeds its predicted value by more than the reduction in the summed approximating function (ie fj (xt) − gt−1 (xt) > Gt−1 ( xt−1 ) − Gt−1 (xt))."
    }, {
      "heading" : "2.8. Initialization",
      "text" : "An approximate Hessian can only be computed as described in Section 2.3 after multiple gradient evaluations. If a subfunction j only has one gradient evaluation, then its approximate Hessian Htj is set to the identity times the median eigenvalue of the average Hessian of the other active subfunctions. If j is the very first subfunction to be evaluated, Htj is initialized as the identity matrix times a large positive constant (106)."
    }, {
      "heading" : "3. Properties",
      "text" : ""
    }, {
      "heading" : "3.1. Computational Cost",
      "text" : "The computational cost per full pass through the data for each portion of the algorithm is given in Table 1. Typically the largest terms are the O (QN) term from evaluating the objective and gradient for all N subfunctions, and theO ( MN2 ) term resulting from projecting positions and gradients into and out of the low dimensional subspace. This algorithm is thus suited to the case that the O (Q) cost of a single subfunction evaluation is larger than the O (MN) cost of projecting an M dimensional vector into an O (N) dimensional subspace. Note that N can be reduced, and the algorithmic overhead reduced, by merging subfunctions or choosing larger minibatches. Without the use of the low dimensional subspace, the leading term in the computational cost of SFO would be the far larger O ( M2N ) per pass.\nFor many problems the cost of a single subfunction evaluation is proportional to the minibatch size, O (Q) = O ( M DN ) , where D is the size of the full data batch. In this case, the ideal minibatch size to minimize total computational cost per iteration is N ∼ √ D."
    }, {
      "heading" : "3.2. Convergence",
      "text" : "Concurrent work by (Mairal, 2013) considers a similar algorithm to that described in 2.2, but with Hti a scalar constant rather than a matrix. Proposition 6.1 in (Mairal, 2013) shows that in the case that each gi majorizes its respective fi, and subject to some additional smoothness constraints, Gt (x) monotonically decreases, and x∗ is an asymptotic stationary point. Proposition 6.2 in (Mairal, 2013) further shows that for strongly convex fi, the algorithm exhibits a linear convergence rate to x∗.\nThe same convergence results hold for SFO with nearidentical proofs, but requiring some modifications to the algorithm. For the proofs to hold: The eigenvalues of Hti must be bounded from above by some constant. It must be possible for gi to majorize fi, and Hti must be chosen so as to guarantee this majorization (eg, by addition of diagonal regularizer). The subfunction update order (Section 2.5) must be made random, rather than the current choice of the most distant subfunction in Section 2.5.\nWe conjecture that the convergence rate for SFO is superlinear. This is because in addition to matching function value and gradient at the current location (Equation 5), we additionally converge on the true Hessian in our approximating functions."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "We compared our optimization technique to several competing optimization techniques for several objective functions. The results are illustrated in Figure 2, and the objectives are described below. For all problems our method outperformed all other techniques in the comparison. Code generating the plots in Figure 2 is included in the Supple-\nmentary Material3. For all experiments we chose a number of subfunctions N = 100.\nSFO refers to Sum of Functions Optimizer, and is the new algorithm presented in this paper. SAG refers to Stochastic Average Gradient method, with the trailing number providing the Lipschitz constant. SGD refers to Stochastic Gradient Descent, with the trailing number indicating the step size. ADAGrad indicates the AdaGrad algorithm, with the trailing number indicating the initial step size. LBFGS refers to the limited memory BFGS algorithm. LBFGS minibatch repeatedly chooses one tenth of the subfunctions, and runs LBFGS for ten iterations on them.\nFor SAG, SGD, and ADAGrad the hyperparameter was chosen by a grid search. The winning hyperparameter value, and the hyperparameter values immediately larger and smaller in the grid search, are shown in the plots and legends for each model in Figure 2. In SGD+momentum the two hyperparameters for both step size and momentum coefficient were chosen by a grid search, but only the winning parameter values are shown. The grid-searched momenta were 0.5, 0.9, 0.95, and 0.99, and the grid-searched step lengths were integer powers of ten between 10−3 and 103."
    }, {
      "heading" : "4.1. Logistic Regression",
      "text" : "We chose the logistic regression objective, L2 regularization penalty, and training dataset to be identical to the protein homology test case in the recent Stochastic Average Gradient paper (Roux et al., 2012), to allow for direct comparison of techniques. The one difference is that our total objective function is divided by the number of samples per minibatch, but unlike in (Roux et al., 2012) is not also divided by the number of minibatches. This different scaling places the hyperparameters for logistic regression in the\n3All figures in the paper can be reproduced simply by downloading code and training data, typing “python figures.py”, and then waiting a week for all optimizers to run on all objective functions for all hyperparameters.\nsame range as for our other experiments."
    }, {
      "heading" : "4.2. Autoencoder",
      "text" : "We trained a contractive autoencoder, which penalizes the Frobenius norm of the Jacobian of the encoder function, on MNIST digits. Autoencoders of this form have been successfully used for learning deep representations in neural networks (Rifai et al., 2011). Sigmoid nonlinearities were used for both encoder and decoder. The regularization penalty was set to 1, and did not depend on the number of hidden units. The reconstruction error was divided by the number of training examples. There were 784 visible units, and 256 hidden units."
    }, {
      "heading" : "4.3. Multilayer Perceptron",
      "text" : "We trained a deep neural network to classify digits on the MNIST digit recognition benchmark. We used a similar architecture as (Hinton & Srivastava, 2012), but with a smaller number of units to allow all competing optimizers time to run. Our network consisted of: 784 input units, one hidden layer of 120 units, one hidden layer of 12 units, and 10 output units. We ran the experiment using both rectified linear and sigmoidal units. The objective used was the standard softmax regression on the output units."
    }, {
      "heading" : "4.4. Deep Convolutional Network",
      "text" : "We trained a deep convolutional network on CIFAR-10 using max pooling and rectified linear units. The architecture we experimented with contains two convolutional layers containing 48 and 128 units respectively, followed by one fully connected layer of 240 units. (this plot will be updated to include the SGD + momentum optimizer when the corresponding grid search over hyperparameters completes)"
    }, {
      "heading" : "5. Future Directions",
      "text" : "We perform optimization in an O (N) dimensional subspace. It may be possible, however, to drastically reduce the dimensionality of the active subspace without significantly reducing optimization performance. For instance, the subspace could be determined by accumulating, in an online fashion, the leading eigenvectors of the covariance matrix of the gradients of the subfunctions, as well as the leading eigenvectors of the covariance matrix of update steps. This would allow the algorithm to run more quickly even for large numbers of subfunctions, and also reduce memory requirements.\nMost portions of the presented algorithm are naively parallelizable. The gti (x) functions can be updated asynchronously, and can even be updated using old position\ninformation. Therefore, developing a parallelized version of this algorithm could make it a useful tool for massive scale optimization problems. Similarly, it may be possible to adapt this algorithm to an online / infinite data context by cycling through a finite set of active subfunctions.\nQuadratic functions are often a poor match to the geometry of the objective function (Pascanu et al., 2012). Neither the dynamically updated subspace nor the use of independent approximating subfunctions gti (x) which are fit to the true subfunctions fi (x) depend on the functional form of gti (x). Exploring non-quadratic approximating subfunctions has the potential to greatly improve performance.\nSection 2.3.2 initializes the approximate Hessian using a diagonal matrix. Instead, it might be effective to initialize the approximate Hessian for each subfunction using the average approximate Hessian from all other subfunctions. Where the measurements from a single subfunction disagreed with this initialization they would overwrite it. This would take advantage of the fact that the Hessians for different subfunctions are very similar for many objective functions.\nFinally, the natural gradient (Amari, 1998) can greatly accelerate optimization by removing the effect of dependencies and relative scalings between parameters. The natural gradient can be simply combined with other optimization methods by performing a change of variables, such that in the new parameter space the natural gradient and the ordinary gradient are identical (Sohl-Dickstein, 2012). It should be straightforward to incorporate this change-ofvariables technique into SFO."
    } ],
    "references" : [ {
      "title" : "Natural Gradient Works Efficiently in Learning",
      "author" : [ "Amari", "Shun-Ichi" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Amari and Shun.Ichi.,? \\Q1998\\E",
      "shortCiteRegEx" : "Amari and Shun.Ichi.",
      "year" : 1998
    }, {
      "title" : "A convergent incremental gradient method with a constant step size",
      "author" : [ "Blatt", "Doron", "Hero", "Alfred O", "Gauchman", "Hillel" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Blatt et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blatt et al\\.",
      "year" : 2007
    }, {
      "title" : "SGD-QN: Careful quasi-Newton stochastic gradient descent",
      "author" : [ "Bordes", "Antoine", "Bottou", "Léon", "Gallinari", "Patrick" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2009
    }, {
      "title" : "On the use of stochastic hessian information in optimization methods for machine learning",
      "author" : [ "Byrd", "RH Richard H", "Chin", "GM Gillian M", "Neveitt", "Will", "Nocedal", "Jorge" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Byrd et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Byrd et al\\.",
      "year" : 2011
    }, {
      "title" : "Quasi-Newton methods, motivation and theory",
      "author" : [ "Dennis Jr.", "John E", "Moré", "Jorge J" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "Jr et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Jr et al\\.",
      "year" : 1977
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi", "John", "Hazan", "Elad", "Singer", "Yoram" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "GE Hinton", "N. Srivastava" ],
      "venue" : "arXiv preprint arXiv:",
      "citeRegEx" : "Hinton and Srivastava,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton and Srivastava",
      "year" : 2012
    }, {
      "title" : "Robust statistics. Wiley, New York, 1981. URL http://scholar.google.com/scholar?hl= en&q=P.J.+Huber%2C+Robust+Statistics% 2C+Wiley%2C+New+York%2C+1981.&btnG= &as_sdt=1%2C5&as_sdtp=#0",
      "author" : [ "Huber", "PJ" ],
      "venue" : null,
      "citeRegEx" : "Huber and PJ.,? \\Q1981\\E",
      "shortCiteRegEx" : "Huber and PJ.",
      "year" : 1981
    }, {
      "title" : "Trust region newton method for logistic regression",
      "author" : [ "Lin", "Chih-Jen", "Weng", "Ruby C", "Keerthi", "S Sathiya" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Lin et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2008
    }, {
      "title" : "On the limited memory BFGS method for large scale optimization",
      "author" : [ "Liu", "Dong C DC", "Nocedal", "Jorge" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Liu et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 1989
    }, {
      "title" : "Optimization with First-Order Surrogate Functions",
      "author" : [ "J. Mairal" ],
      "venue" : "arXiv preprint arXiv:1305.3120,",
      "citeRegEx" : "Mairal,? \\Q2013\\E",
      "shortCiteRegEx" : "Mairal",
      "year" : 2013
    }, {
      "title" : "Deep learning via Hessian-free optimization",
      "author" : [ "Martens", "James" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Martens and James.,? \\Q2010\\E",
      "shortCiteRegEx" : "Martens and James.",
      "year" : 2010
    }, {
      "title" : "On optimization methods for deep learning",
      "author" : [ "J Ngiam", "A. Coates" ],
      "venue" : null,
      "citeRegEx" : "Ngiam and Coates,? \\Q2011\\E",
      "shortCiteRegEx" : "Ngiam and Coates",
      "year" : 2011
    }, {
      "title" : "Historical Development of the BFGS Secant Method and Its Characterization Properties",
      "author" : [ "Papakonstantinou", "JM" ],
      "venue" : null,
      "citeRegEx" : "Papakonstantinou and JM.,? \\Q2009\\E",
      "shortCiteRegEx" : "Papakonstantinou and JM.",
      "year" : 2009
    }, {
      "title" : "On the difficulty of training Recurrent Neural Networks",
      "author" : [ "Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua" ],
      "venue" : "URL http://arxiv.org/abs/",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2012
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Robbins", "Herbert", "Monro", "Sutton" ],
      "venue" : "The Annals of Mathematical Statistics, pp",
      "citeRegEx" : "Robbins et al\\.,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins et al\\.",
      "year" : 1951
    }, {
      "title" : "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets",
      "author" : [ "Roux", "N Le", "M Schmidt", "F. Bach" ],
      "venue" : "URL",
      "citeRegEx" : "Roux et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2012
    }, {
      "title" : "URL http://eprints",
      "author" : [ "Schraudolph", "Nicol", "Yu", "Jin", "Günter", "Simon. A stochastic quasi-Newton method for online convex optimization." ],
      "venue" : "pascal-network.org/archive/00003992/.",
      "citeRegEx" : "Schraudolph et al\\.,? 2007",
      "shortCiteRegEx" : "Schraudolph et al\\.",
      "year" : 2007
    }, {
      "title" : "Local gain adaptation in stochastic gradient descent",
      "author" : [ "Schraudolph", "Nicol N" ],
      "venue" : "In Artificial Neural Networks,",
      "citeRegEx" : "Schraudolph and N.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schraudolph and N.",
      "year" : 1999
    }, {
      "title" : "The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use",
      "author" : [ "Sohl-Dickstein", "Jascha" ],
      "venue" : "URL http: //arxiv.org/abs/1205.1828",
      "citeRegEx" : "Sohl.Dickstein and Jascha.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sohl.Dickstein and Jascha.",
      "year" : 2012
    }, {
      "title" : "Minimum Probability Flow Learning",
      "author" : [ "Sohl-Dickstein", "Jascha", "Battaglino", "Peter B", "DeWeese", "Michael R" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Sohl.Dickstein et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sohl.Dickstein et al\\.",
      "year" : 2011
    }, {
      "title" : "Variable metric stochastic approximation theory",
      "author" : [ "Sunehag", "Peter", "Trumpf", "Jochen", "S V N Vishwanathan", "Schraudolph", "Nicol" ],
      "venue" : "arXiv preprint arXiv:0908.3529,",
      "citeRegEx" : "Sunehag et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sunehag et al\\.",
      "year" : 2009
    }, {
      "title" : "Krylov subspace descent for deep learning",
      "author" : [ "Vinyals", "Oriol", "Povey", "Daniel" ],
      "venue" : "arXiv preprint arXiv:1111.4259,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "More recent descent techniques like IAG (Blatt et al., 2007), SAG (Roux et al.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : ", 2007), SAG (Roux et al., 2012), and MISO (Mairal, 2013) instead take update steps in the average gradient direction.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : ", 2012), and MISO (Mairal, 2013) instead take update steps in the average gradient direction.",
      "startOffset" : 18,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "In (Schraudolph et al., 2007; Sunehag et al., 2009) a stochastic variant of LBFGS is proposed.",
      "startOffset" : 3,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : "In (Schraudolph et al., 2007; Sunehag et al., 2009) a stochastic variant of LBFGS is proposed.",
      "startOffset" : 3,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "In (Martens, 2010), (Byrd et al., 2011), and (Vinyals & Povey, 2011) stochastic versions of Hessian-free optimization are implemented and applied to optimization of deep networks.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "In (Lin et al., 2008) a trust region Newton method is used to train logistic regression and linear SVMs using minibatches.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "Stochastic meta-descent (Schraudolph, 1999), AdaGrad (Duchi et al., 2010), and SGD-QN (Bordes et al.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : ", 2010), and SGD-QN (Bordes et al., 2009) rescale the gradient independently for each dimension, and can be viewed as accumulating something similar to a diagonal approximation to the Hessian.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "This contrasts with the cyclic choice of subfunction in (Blatt et al., 2007), and the random choice of subfunction in (Roux et al.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : ", 2007), and the random choice of subfunction in (Roux et al., 2012).",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "Concurrent work by (Mairal, 2013) considers a similar algorithm to that described in 2.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "1 in (Mairal, 2013) shows that in the case that each gi majorizes its respective fi, and subject to some additional smoothness constraints, G (x) monotonically decreases, and x∗ is an asymptotic stationary point.",
      "startOffset" : 5,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "2 in (Mairal, 2013) further shows that for strongly convex fi, the algorithm exhibits a linear convergence rate to x∗.",
      "startOffset" : 5,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "We chose the logistic regression objective, L2 regularization penalty, and training dataset to be identical to the protein homology test case in the recent Stochastic Average Gradient paper (Roux et al., 2012), to allow for direct comparison of techniques.",
      "startOffset" : 190,
      "endOffset" : 209
    }, {
      "referenceID" : 16,
      "context" : "The one difference is that our total objective function is divided by the number of samples per minibatch, but unlike in (Roux et al., 2012) is not also divided by the number of minibatches.",
      "startOffset" : 121,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "Quadratic functions are often a poor match to the geometry of the objective function (Pascanu et al., 2012).",
      "startOffset" : 85,
      "endOffset" : 107
    } ],
    "year" : 2013,
    "abstractText" : "We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information accessible by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability even for high dimensional optimization problems by developing an adaptive scheme to store and manipulate these quadratic approximations in a shared, time evolving low dimensional subspace, determined by the recent history of gradient evaluations. This algorithm contrasts with earlier stochastic second order techniques, which treat the Hessian of each contributing function only as a noisy approximation to the full Hessian, rather than as a target for direct estimation. Our approach reaps the benefits of both SGD and quasi-Newton methods; each update step requires only a single subfunction evaluation (like SGD but unlike previous stochastic second order methods), while little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods but not for SGD). For convex problems the convergence rate of the proposed technique is at least linear. We demonstrate improved convergence on five diverse optimization problems.",
    "creator" : "LaTeX with hyperref package"
  }
}