{"conference": "ICLR 2017 conference submission", "title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: - Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results.  - Interpretation: The representations learned by deep learning models should align with medical knowledge. To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies.  Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism.  We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "reviews": [{"is_meta_review": true, "comments": "SUMMARY.\nThis paper presents a method for enriching medical concepts with their parent nodes in an ontology.\nThe method employs an attention mechanism over the parent nodes of a medical concept to create a richer representation of the concept itself.\nThe rationale of this is that for  infrequent medical concepts the attention mechanism will rely more on general concepts, higher in the ontology hierarchy, while for frequent ones will focus on the specific concept.\nThe attention mechanism is trained together with a recurrent neural network and the model accuracy is tested on two tasks.\nThe first task aims at prediction the diagnosis categories at each time step, while the second task aims at predicting whether or not a heart failure is likely to happen after the T-th step.\n\nResults shows that the proposed model works well in condition of data insufficiency.\n\n----------\n\nOVERALL JUDGMENT\nThe proposed model is simple but interesting.\nThe ideas presented are worth to expand but there are also some points where the authors could have done better.\nThe learning of the representation of concepts in the ontology is a bit naive, for example the authors could have used some kind of knowledge base factorization approach to learn the concepts, or some graph convolutional approach.\nI do not see why the the very general factorization methods for knowledge bases do not apply in the case of ontology learning.\nI also found strange that the representation of leaves are fine tuned while the inner nodes are not, it is a specific reason to do so?\n\nRegarding the presentation, the paper is clear and the qualitative evaluation is insightful.\n\n\n----------\n\nDETAILED COMMENTS\n\nFigure 2. Please use the same image format with the same resolution.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The reviewers all agreed that the paper was well written, that the proposed approach is very sensible and intuitive and that the experiments are convincing. However, they are concerned that the proposed work is of limited interest to the ICLR community. The technical contribution is not significant enough for any of the reviewers to strongly recommend an acceptance.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Changes in the last revision", "OTHER_KEYS": "Edward Choi", "comments": "Major changes in the revision\n-- Figures 2 are replaced with tables for better readability\n-- Related works have been expanded\n-- Incorporated the reviewers\u2019 comments in the experiment section.\n-- Experiments for GRAM+ and RNN+ in heart failure prediction were re-run\n\nMore details\nWe discovered that, for GRAM+ and RNN+ in HF prediction with varying amounts of training data, we initialized the basic embeddings e_i\u2019s and the embedding matrix W_emb with GloVe vectors that were trained always on the 100% training data instead of the downsampled training data. This could have exaggerated the AUC of both models. Therefore we conducted a correct experiment where the initialization was done with GloVe vectors trained on the downsampled training data. We found that GRAM+\u2019s performance did not show noticeable difference, but RNN+\u2019s performance dropped approximately 0.5-1% AUC.\n", "IS_META_REVIEW": false, "DATE": "12 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper addresses the problem of data sparsity in the healthcare domain by leveraging hierarchies of medical concepts organized in ontologies. The paper focuses on sequential prediction given a patient\u2019s medical record (a sequence of medical codes, some of which might occur very rarely). Instead of simply assigning each medical code an independent embedding before feeding it to an RNN, the proposed approach assigns each node in the medical ontology a \u201cbasic\u201d embedding, and composes a \u201cfinal\u201d embedding for each medical code by taking a learned weighted average (via an attention mechanism) of the medical code\u2019s ancestors in the ontology. Notably, the paper is well written and the approach is quite intuitive.\nI have the following comments: \n- Why is the patient\u2019s visit taken as just the sum of medical codes found in the visit, and not say the average or a learned weighted average? Wouldn\u2019t this bias for/against the number of codes in the visit?\n- I don\u2019t see why basic embeddings are not fine tuned as well. Did you find that to hurt performance? Do you have an explanation for that?\n- Looking at Figure 2, the results seem very close and the figures are not very clear (figure (b) top is missing). Also, I am wondering how significant the differences are so it would be nice to comment on that.\nFinally, I think this is an interesting application paper applying well-established deep learning techniques. The paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources. However, I would like the authors to comment on what there paper offers as new insights to the ICLR community and why they think ICLR is a good avenue for their work.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "How that compares to your KDD 2016 paper ", "OTHER_KEYS": "(anonymous)", "comments": "Nice paper, I have one comment however. \n\nI read your KDD 2016 paper about \"Multi-layer Representation Learning for Medical Concepts\" where you presented a Med2vec model to map patients' visits and codes into a different space. You showed that the new representation is effective and interpretable. \n\nYour current submission to ICLR is very similar to your KDD paper except that you have regularized the model using DAG of concepts as a prior knowledge. \n1) Am I right on that? \n2) why you did not compare your current model to Med2vec? You can use Med2vec as input to any other model such as RNN or CNN for classification. This case, we can have a clear picture about the contribution of the prior knowledge to the model accuracy. If there is no significant difference in the results then why we would care about the prior knowledge. ", "IS_META_REVIEW": false, "DATE": "22 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting approach for learning input representations in RNN", "is_meta_review": false, "comments": "I read the authors' response and maintain my rating.\n\n---\n\nThis paper introduces an approach for integrating a direct acyclic graph structure of the data into word / code embeddings, in order to leverage domain knowledge and thus help train an RNN with scarce data. It is applied to codes of medical visits. Each code is part of an ontology, which can be represented by a DAG, where codes correspond to leaf nodes, and where different codes may share common ancestors (non-leaf nodes) in the DAG. Instead of embedding merely the leaf nodes, one can also embed the non-leaf nodes, and the embeddings of the code and its ancestors can be combined using a convex sum. That convex sum can be seen as an attention mechanism over the representation. The attention weights depend on the embeddings and the weights of an MLP, meaning that the model can separate learning the code embeddings and the interaction between the codes. Embedding codes are pretrained using GloVe, then fine-tuned.\n\nThe model is properly evaluated on two medical datasets, with several variations to isolate the contribution of the DAG (GRAM or GRAM+ vs. RNN or RandomDAG) and of pretraining the embeddings (RNN+ vs RNN, GRAM+ vs GRAM). Both are shown to help achieve the best performance and the evaluation methodology seems thorough.\n\nThe paper is also well written, and the case for MLP attention instead of a plain dot product of embeddings was made by the authors.\n\nMy only two comments would be:\n1) Why is there a softmax in equation 4, given that the loss is multivariate cross-entropy (in the predicted visit, several codes could be equal to 1), not a a single-class cross-entropy?\n2) What is the embedding dimension m?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "is_meta_review": false, "comments": "SUMMARY.\nThis paper presents a method for enriching medical concepts with their parent nodes in an ontology.\nThe method employs an attention mechanism over the parent nodes of a medical concept to create a richer representation of the concept itself.\nThe rationale of this is that for  infrequent medical concepts the attention mechanism will rely more on general concepts, higher in the ontology hierarchy, while for frequent ones will focus on the specific concept.\nThe attention mechanism is trained together with a recurrent neural network and the model accuracy is tested on two tasks.\nThe first task aims at prediction the diagnosis categories at each time step, while the second task aims at predicting whether or not a heart failure is likely to happen after the T-th step.\n\nResults shows that the proposed model works well in condition of data insufficiency.\n\n----------\n\nOVERALL JUDGMENT\nThe proposed model is simple but interesting.\nThe ideas presented are worth to expand but there are also some points where the authors could have done better.\nThe learning of the representation of concepts in the ontology is a bit naive, for example the authors could have used some kind of knowledge base factorization approach to learn the concepts, or some graph convolutional approach.\nI do not see why the the very general factorization methods for knowledge bases do not apply in the case of ontology learning.\nI also found strange that the representation of leaves are fine tuned while the inner nodes are not, it is a specific reason to do so?\n\nRegarding the presentation, the paper is clear and the qualitative evaluation is insightful.\n\n\n----------\n\nDETAILED COMMENTS\n\nFigure 2. Please use the same image format with the same resolution.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Compatibility function", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "embeddings", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F. Stewart, Jimeng Sun", "KEYWORDS": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "accepted": false, "id": ""}
