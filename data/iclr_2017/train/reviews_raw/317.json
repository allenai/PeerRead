{"conference": "ICLR 2017 conference submission", "title": "Amortised MAP Inference for Image Super-resolution", "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "reviews": [{"is_meta_review": true, "comments": "The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.\nResults are nicely demonstrated on several datasets.\n\nI like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn?", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "All the reviewers agreed that the paper is original, of high quality, and worth publishing.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Comment", "OTHER_KEYS": "Martin Arjovsky", "comments": "I just wanted to comment that I found this to be a really great paper. There's a lot of new ideas, and the writing and experiments are extremely well executed.\n\nThe analysis of section 3-3.1 is novel (to the best of my knowledge) and very valuable, as it's interesting to see in play the DAE ideas in 3.3. Figure 1 (sec 2.1) is extremely clarifying. The criticism of section 5.6 is very interesting, and might lead to a new direction of research if handled with care, the question about the precise mathematical meaning of \"producing plausible samples\" is extremely important and far from solved.\n\nMinor comments / questions:\n- Have you tried comparing with an architecture such as the ones used in segmentation or structured prediction? The use of the mean field CRF approach will lead you to pick a mode and get sharp predictions as has been done for a long time, since you are training a conditional unimodal distribution with the inverse KL. The issue of continuous variables can be ameliorated by discretizing as in pixel CNN for example. As another bonus, these architectures are very stable and can be trained with much much bigger models.\n- Page 3. Item 2. Typo on employs -> employ", "IS_META_REVIEW": false, "DATE": "23 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Novel methodology", "is_meta_review": false, "comments": "The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments:\n\n1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.\n\n2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still need the {HR, LR} image pairs. Does it mean that we merely transfer this training procedure to the training of affine projection?\n\n3) The paper presents many results of the framework, including the results of natural images from ImageNet. Can the author also provide the results of Set5, Set14 or BSD100, which are conventional test dataset for SR, so that we can perform a fair comparison with previous work.\n\n4) I see that the size of the results of nature images presented in this paper are limited to 128*128. Can this framework perform well on images with larger size? Because SR will encounter input with arbitrary size.\n\n5) A normal GAN will have a noise term as a latent space, so that it can be better illustrated as learning a distribution. Do the author try the noise vector?\n\nOverall, this paper provides a new framework for SR with solid theoretical analysis. The idea is novel and the author explore many methods. Though there still exist questions like the necessity and more experiments are needed. I think this work will will provide good inspiration to the community.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "is_meta_review": false, "comments": "Sincere apologies for the late review.\n\nThis paper argues to approach Super-Resolution as amortised MAP estimation. A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout. Further three different methods to solve the resulting cross-entropy problem in Eq.9 are proposed and tested. \n\nSummary: Very good paper, very well written and presented. Experimental results are sufficient, the paper presents well chosen toy examples and real world applications. From my understanding the contributions for the field of super-resolutions are novel (3.2,3.3,3.4), parts that are specific for the training of GANs may have appeared in different variants elsewhere (see also discussion). I believe that this paper will be relevant to future work on super-resolution, the finding that GAN based model training yields most visually appealing results suggests further work in this domain. \n\nManuscript should be proof-read once more, there were some very few typos that may be worth fixing.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting paper", "is_meta_review": false, "comments": "The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.\nResults are nicely demonstrated on several datasets.\n\nI like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Instance noise", "OTHER_KEYS": "Xun Huang", "comments": "Improved GAN (Salimans et al., 2016) also adds gaussian noise to inputs of discriminators in their implementation (", "IS_META_REVIEW": false, "DATE": "07 Nov 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Casper Kaae S\u00f8nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, Ferenc Husz\u00e1r", "KEYWORDS": "Probabilisticly motivated image superresolution using a projection to the subspace of valid solutions", "accepted": true, "id": ""}
