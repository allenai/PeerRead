{
  "name" : "1312.6885.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep learning for class-generic object detection",
    "authors" : [ "Brody Huval", "Adam Coates", "Andrew Ng" ],
    "emails" : [ "BRODYH@STANFORD.EDU", "ACOATES@STANFORD.EDU", "ANG@STANFORD.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The task of separating objects from background is fundamental for many computer vision tasks. This has led to much research on localizing and classifying objects by using object segmentation, object detection, and region proposals. Currently, most detectors are trained individually for each object class, which requires a class label and a bounding box for all images. Unfortunately, in this approach it is difficult to transfer information from previously trained detectors to novel classes where bounding box labels may not be available. This situation is common in current datasets, which often have many class labels but incomplete bounding box labels. In this work, we aim to overcome these challenges by training separately from bounding box labels and class labels, enabling our system to learn even when only one of these labels is available. This approach harnesses the notion of object-ness (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013) to build a deep neural network (Krizhevsky et al., 2012) able to detect novel objects where bounding box labels have not been provided.\nOne successful approach to object detection is to train a single detector for each class of objects (for example, the Deformable Parts Model (DPM) (Felzenszwalb et al., 2010)). In this approach, one discriminatively trains a set of detec-\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\ntors on each individual class. This strategy generally has proven useful on the Pascal VOC detection challenge due to the limited number of classes, each of which includes many bounding box labels. In other cases, however, where we may have an abundance of class labels, but few or no bounding box labels, it is not clear how to apply this same strategy. For example, the Image-Net dataset has 14 million class labels but only about 7% are labeled with bounding boxes (Deng et al., 2009).\nRecently, region proposal algorithms have shown good performance in object detection pipelines by proposing class-generic locations for further classification (Endres & Hoiem, 2010; Alexe et al., 2012; Girshick et al., 2013; Uijlings et al., 2013). They attempt to measure object-ness within an image by training on all bounding boxes labels, regardless of class, in hopes of building a single detector for all classes.\nWhile training from only bounding box labels potentially enables a detector to locate novel classes never seen before, it may perform poorly due to having too few training examples and failing to exploit the wealth of class labels available in datasets like Image-Net. We propose to train a detector to localize objects while also exploiting object class labels by separating the recognition and detection problems. We show that by pretraining our detector on class labels and then on object locations, we can increase its performance in detecting previously seen objects, while nearly retaining its ability to localize objects for which we have no bounding box labels."
    }, {
      "heading" : "2. Related works",
      "text" : "(Szegedy et al., 2013) have used a similar neural network for object detection in Pascal VOC. Like our approach, they avoided the use of sliding windows or region proposals, and instead directly used a deep neural network for predicting object locations. However, their work focuses on only a handful of classes from Pascal VOC, and five different networks are trained for each class. In contrast, we train a single network able to provide class-generic object detections.\nRegion proposal algorithms are typically shallow methods\nar X\niv :1\n31 2.\n68 85\nv1 [\ncs .C\nV ]\n2 4\nD ec\n2 01\n3\nthat focus on high-recall object-ness detection (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013). Therefore, they return hundreds to thousands of potential bounding boxes for evaluation, which is still a large reduction over the number of evaluations required for the sliding window approach. These potential locations are then input to a stronger classification algorithm such as a deep Convolutional Neural Network (CNN) to identify the object. In our work, we focus on high-precision detection, using nonmax supression to reduce our predictions to a set of likely detected objects."
    }, {
      "heading" : "3. Object Detection from Neural Networks",
      "text" : ""
    }, {
      "heading" : "3.1. Model",
      "text" : "The Convolutional Neural Network (CNN) we use is similar to that proposed by (Krizhevsky et al., 2012) for object classification. The network consists of five layers of convolution followed by two densely connected layers. Every layer applies a Rectified Linear Unit (ReLU) as its nonlinearity. Only the first, second, and fifth layers use Local Contrast Normalization (LCN) and max pooling. The final output is a 4096 dimensional feature vector for the image. See (Krizhevsky et al., 2012) for details."
    }, {
      "heading" : "3.2. Bounding Box Training",
      "text" : "In the image classification results from (Krizhevsky et al., 2012), the final feature vector is input to a softmax layer which provides a probability distribution over class labels. In our work, we instead use a softmax layer to provide a probability distribution over a discretized space of bounding boxes. This 4-dimensional space encodes the x-y position, the scale, and the aspect ratio of a bounding box.\nBecause the softmax layer treats every output label independently, the network will receive the same loss for bounding boxes with high or low overlap with the ground truth, which is not ideal. This also leads to low counts of each label during training. To resolve this, instead of placing a one or zero at each label, we place a Gaussian distribution centered at the correct label. The result is a smaller loss when a bounding box similar to the ground truth is predicted. To allow multiple bounding boxes in each image, a Gaussian distribution is placed at each location and the labels are re-normalized to sum to one. During evaluation, multiple boxes are predicted by applying non-max suppression to the resulting probability distribution over bounding boxes."
    }, {
      "heading" : "3.3. Classification pretraining",
      "text" : "When training a CNN for image recognition, the network is learning discriminative filters that help in detecting\nthe different classes, while ignoring potentially distracting generic backgrounds. As a result, the task of recognition and detection are related, and information from one task can improve results on the other. To implement this intuition, we optionally pretrain our network using the image recognition task. Our results will confirm the usefulness of this intuition: Pretraining on image recognition turns out to increase performance on class-generic object detection. Conversely, pretraining on object detection can increase image recognition performance."
    }, {
      "heading" : "4. Experiments",
      "text" : "To perform our experiments, we use multiple GPUs for training (Coates et al., 2013; Krizhevsky et al., 2012)."
    }, {
      "heading" : "4.1. Dataset",
      "text" : "All experiments were performed on the Imagenet 2012 Localization Challenge (Deng et al., 2009). This dataset provides 1.2 million classification images with 592, 000 including bounding box labels over 1000 different categories/synsets."
    }, {
      "heading" : "4.2. Evaluation",
      "text" : "To show that our network is capable of detecting objects, despite never having seen their bounding boxes, we randomly chose 100 object classes out of the 1000 ImageNet Challenge classes and train without their bounding boxes. We then evaluate the performance on a validation set only containing the 100 object classes held out during training. This was performed with one network starting from random weights, and another starting from a network pretrained on image recognition for all 1000 object classes. Our results are shown in Table 1. To measure the drop in performance from not having the bounding boxes from 100 classes available, the performance for both networks, pretrained and random, are reported when trained on all 1000 bounding box classes, shown in Table 2. Precision recall curves are also shown for these four cases in Figure 1. Examples of correct and incorrect detections from our network on held out bounding box classes are shown in Figure 2.\nFinally, we looked at the impact of training to detect objects (without the corresponding class labels) on image recogni-\nTable 2: AUC after training on all bounding box labels.\nAUC pretrained 0.545 random 0.498\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP re\nci si\no n\npretrained, all bounding boxes random, all bounding boxes pretrained, without bounding boxes random, without bounding boxes\nFigure 1: Precision Recall curves.\ntion performance. We found that by initializing an image recognition network with the weights from a trained object detection network, we reduce our top-5 error by 1%."
    }, {
      "heading" : "5. Conclusion",
      "text" : "By using a single deep neural network we have investigated a method for object-ness detection, capable of exploiting both class and bounding box labels. Our network is able to generalize to classes for which it has never seen bounding box labels while benefitting from class labels when available. In addition, we have also found that the object-ness detection task yields a modest improvement in the ImageNet recognition challenge, which does not involve detection."
    } ],
    "references" : [ {
      "title" : "Measuring the objectness of image windows",
      "author" : [ "Alexe", "Bogdan", "Deselaers", "Thomas", "Ferrari", "Vittorio" ],
      "venue" : null,
      "citeRegEx" : "Alexe et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Alexe et al\\.",
      "year" : 2012
    }, {
      "title" : "Deep learning with cots hpc systems",
      "author" : [ "Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Coates et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2013
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Category independent object proposals",
      "author" : [ "Endres", "Ian", "Hoiem", "Derek" ],
      "venue" : "In Computer Visionâ€“ECCV",
      "citeRegEx" : "Endres et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Endres et al\\.",
      "year" : 2010
    }, {
      "title" : "Object detection with discriminatively trained part-based models",
      "author" : [ "Felzenszwalb", "Pedro F", "Girshick", "Ross B", "McAllester", "David", "Ramanan", "Deva" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Felzenszwalb et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Felzenszwalb et al\\.",
      "year" : 2010
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra" ],
      "venue" : "arXiv preprint arXiv:1311.2524,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Deep neural networks for object detection",
      "author" : [ "Szegedy", "Christian", "Toshev", "Alexander", "Erhan", "Dumitru" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2013
    }, {
      "title" : "Selective search for object recognition",
      "author" : [ "JRR Uijlings", "KEA van de Sande", "T Gevers", "Smeulders", "AWM" ],
      "venue" : "International Journal of Computer Vision, pp",
      "citeRegEx" : "Uijlings et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Uijlings et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This approach harnesses the notion of object-ness (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013) to build a deep neural network (Krizhevsky et al.",
      "startOffset" : 50,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "This approach harnesses the notion of object-ness (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013) to build a deep neural network (Krizhevsky et al.",
      "startOffset" : 50,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : ", 2013) to build a deep neural network (Krizhevsky et al., 2012) able to detect novel objects where bounding box labels have not been provided.",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "One successful approach to object detection is to train a single detector for each class of objects (for example, the Deformable Parts Model (DPM) (Felzenszwalb et al., 2010)).",
      "startOffset" : 147,
      "endOffset" : 174
    }, {
      "referenceID" : 2,
      "context" : "For example, the Image-Net dataset has 14 million class labels but only about 7% are labeled with bounding boxes (Deng et al., 2009).",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "Recently, region proposal algorithms have shown good performance in object detection pipelines by proposing class-generic locations for further classification (Endres & Hoiem, 2010; Alexe et al., 2012; Girshick et al., 2013; Uijlings et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 247
    }, {
      "referenceID" : 5,
      "context" : "Recently, region proposal algorithms have shown good performance in object detection pipelines by proposing class-generic locations for further classification (Endres & Hoiem, 2010; Alexe et al., 2012; Girshick et al., 2013; Uijlings et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 247
    }, {
      "referenceID" : 8,
      "context" : "Recently, region proposal algorithms have shown good performance in object detection pipelines by proposing class-generic locations for further classification (Endres & Hoiem, 2010; Alexe et al., 2012; Girshick et al., 2013; Uijlings et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 247
    }, {
      "referenceID" : 7,
      "context" : "(Szegedy et al., 2013) have used a similar neural network for object detection in Pascal VOC.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "that focus on high-recall object-ness detection (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013).",
      "startOffset" : 48,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "that focus on high-recall object-ness detection (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013).",
      "startOffset" : 48,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "The Convolutional Neural Network (CNN) we use is similar to that proposed by (Krizhevsky et al., 2012) for object classification.",
      "startOffset" : 77,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "See (Krizhevsky et al., 2012) for details.",
      "startOffset" : 4,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "In the image classification results from (Krizhevsky et al., 2012), the final feature vector is input to a softmax layer which provides a probability distribution over class labels.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "To perform our experiments, we use multiple GPUs for training (Coates et al., 2013; Krizhevsky et al., 2012).",
      "startOffset" : 62,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "To perform our experiments, we use multiple GPUs for training (Coates et al., 2013; Krizhevsky et al., 2012).",
      "startOffset" : 62,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "All experiments were performed on the Imagenet 2012 Localization Challenge (Deng et al., 2009).",
      "startOffset" : 75,
      "endOffset" : 94
    } ],
    "year" : 2013,
    "abstractText" : "We investigate the use of deep neural networks for the novel task of class-generic object detection. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. In addition, we show that bounding box labels yield a 1% performance increase on the ImageNet recognition challenge.",
    "creator" : "LaTeX with hyperref package"
  }
}