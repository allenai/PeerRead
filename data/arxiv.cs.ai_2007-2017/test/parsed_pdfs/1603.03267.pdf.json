{
  "name" : "1603.03267.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hierarchical Linearly-Solvable Markov Decision Problems (Extended Version with Supplementary Material)",
    "authors" : [ "Anders Jonsson", "Vicenç Gómez" ],
    "emails" : [ "anders.jonsson@upf.edu", "vicen.gomez@upf.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Hierarchical reinforcement learning (HRL) is a general framework for addressing large-scale reinforcement learning problems. It exploits the task (or action) structure of a problem by considering policies over temporally extended actions that typically involve a reduced subset of the state components. For example, the MAXQ approach (Dietterich 2000) decomposes a Markov decision process (MDP) and its value function into a hierarchy of smaller MDPs such that the value function of the target MDP corresponds to an additive combination of the value functions of the smaller MDPs. Another example is the options approach for which different tasks can be learned simultaneously in an online fashion (Sutton and Precup 1998). HRL methods have also been used to explain human and animal behavior (Botvinick, Niv, and Barto 2009).\nIndependently, a class of stochastic optimal control problems was introduced for which the actions and cost function are restricted in ways that make the Bellman equation linear and thus more efficiently solvable (Todorov 2006; Kappen 2005). This class of problems is known in the discrete setting as linearly-solvable MDPs (LMDPs), in the continuous setting as path-integral control or more generally, as Kullback-Leibler (KL) control (Kappen, Gómez, and Opper 2012). Optimal control computation for this class of problems is equivalent to a KL-divergence minimization.\nThe original LMDP formulation considers a single action that changes the stochastic laws of the environment. An alternative interpretation (that we adopt in this work) is to consider a stochastic policy over deterministic actions. LMDPs have many interesting properties. For example, optimal control laws for LMDPs can be linearly combined to derive composite optimal control laws efficiently (Todorov 2009a). Also, the power iteration method, used to solve LMDPs, is equivalent to the popular belief propagation algorithm used for probabilistic inference in dynamic graphical models (Kappen, Gómez, and Opper 2012). The optimal value function for LMDPs can be learned efficiently using an offpolicy learning algorithm, Z-learning, that operates directly in the state space instead of in the product space of states and actions (Todorov 2009b).\nIn the continuous setting, the KL-divergence reduces to the familiar quadratic energy cost, widely used in robotic applications. Examples of such applications include robot navigation (Kinjo, Uchibe, and Doya 2013) and motor skill reinforcement learning (Theodorou, Buchli, and Schaal 2010). This class of problems is also relevant in other disciplines, such as cognitive science and decision making theory (Friston et al. 2013; Ortega and Braun 2013). However, in general, application of LMDPs to real-world problems is challenging, mainly due to the curse dimensionality (AbbasiYadkori et al. 2015; Matsubara, Gómez, and Kappen 2014; Todorov 2009c).\nIn this paper, we propose to combine both HRL and LMDP frameworks and formulate a reinforcement learning problem as a hierarchy of LMDPs. Surprisingly, despite LMDPs were introduced already ten years ago, no unifying framework that combines both methodologies has been proposed yet. The benefits of this combination are two-fold. On one hand, HRL problems expressed in this way can benefit from the same properties that LMDPs enjoy. For example, one can use Z-learning as an efficient alternative to the stateof-the-art HRL methods. Another example is task compositionality, by which a composite task can be learned at no cost given the optimal solution for the different composing tasks. This is useful in tasks that have several terminal states, as we will show later. On the other hand, LMDPs can also benefit from the HRL framework, for example, by addressing the curse of dimensionality in an alternative way to the pre-\n1\nar X\niv :1\n60 3.\n03 26\n7v 1\n[ cs\n.A I]\n1 0\nM ar\n2 01\nviously mentioned approaches or by simultaneous intra-task learning from HRL.\nThe paper is organized as follows. We review HRL and LMDPs in Section 2. The main contribution of this work is the hierarchical formulation for LMDPs, which we present in Section 3. We empirically illustrate its benefits on two benchmarks in Section 4 We conclude this work in Section 5."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this section we introduce preliminaries and notation. We first define MDPs and Semi-MDPs, then explain the idea behind MAXQ decomposition, and finally describe linearlysolvable MDPs."
    }, {
      "heading" : "2.1 MDPs and Semi-MDPs",
      "text" : "An MDP M = 〈S,A, P,R〉 consists of a set of states S, a set of actions A, a transition probability distribution P : S × A× S → [0, 1] satisfying ∑ s′ P (s\n′|s, a) = 1 for each state-action pair (s, a) ∈ S × A, and an expected reward function R : S × A → R. The aim is to learn an optimal policy π : S → A, i.e. a mapping from states to actions that maximizes expected future reward.\nMDPs are usually solved by defining a value function V : S → R that estimates the expected future reward in each state. In the undiscounted case, the optimal value is obtained by solving the Bellman optimality equation:\nV (s) = max a∈A {R(s, a) + Es′ [V (s′)]}\n= max a∈A\n{ R(s, a) +\n∑ s′ P (s′|s, a)V (s′)\n} .\nTo bound the optimal value function, we only consider first exit problems that define a set of terminal states T ⊆ S. A function g : T → R defines the final reward V (t) ≡ g(t) of each terminal state. As an alternative to the value function V , one can define an action-value function Q : S×A→ R that estimates the expected future reward for each state-action pair (s, a) ∈ S ×A.\nThe Bellman optimality equation can be solved globally using algorithms such as value iteration and policy iteration. However, for large state spaces this is not feasible. Alternative algorithms make local updates to the value function online. Arguably, the most popular online algorithm for MDPs is Q-learning (Watkins 1989). Given a transition (st, at, rt, st+1) from state st to state st+1 when taking action at and receiving reward rt, Q-learning makes the following update to the estimate Q̂ of the optimal action-value function:\nQ̂(st, at)← (1− α)Q̂(st, at) + α(rt + max a Q̂(st+1, a)),\nwhere α is a learning rate. A Semi-MDP generalizes an MDP by including actions that take more than one time-step to complete. In this case,\nthe Bellman optimality equation becomes\nV (s) = max a∈A Eτ\n{ R(s, a, τ) +\n∑ s′ P (s′, τ |s, a)V (s′) } = max\na∈A ∑ τ ∑ s′ P (s′, τ |s, a) {R(s, a, τ) + V (s′)} ,\nwhere τ is the duration of action a,R(s, a, τ) is the expected reward when a applied in s lasts for τ steps, andP (s′, τ |s, a) is the probability of transitioning to s′ in τ steps. By defining R(s, a) ≡ ∑ τ,s′ P (s\n′, τ |s, a)R(s, a, τ) and P (s′|s, a) =∑ τ P (s ′, τ |s, a), we get\nV (s) = max a∈A\n{ R(s, a) +\n∑ s′ P (s′|s, a)V (s′)\n} ,\ni.e. a Semi-MDP can be treated and solved as an MDP."
    }, {
      "heading" : "2.2 MAXQ Decomposition",
      "text" : "MAXQ decomposition (Dietterich 2000) decomposes an MDP M = 〈S,A, P,R〉 into a finite set of tasks M = {M0, . . . ,Mn}with root taskM0, i.e. solvingM0 is equivalent to solving M . Each task Mi = 〈Ti, Ai, R̃i〉, 0 ≤ i ≤ n, consists of a termination set Ti ⊂ S, an action set Ai ⊂ M and a pseudo-reward R̃i : Ti → R.\nA task Mi is primitive if it has no subtasks, i.e. if Ai = ∅. A primitive task Mi corresponds to an action a ∈ A of the original MDP M , defined such that Mi is always applicable, terminates after one time step and has pseudo-reward 0 everywhere. A non-primitive task Mi can only be applied in non-terminal states (i.e. states not in Ti). Terminating in state t ∈ Ti produces pseudo-reward R̃i(t). Mi corresponds to a Semi-MDP with action set Ai, i.e. actions are other tasks.\nMAXQ defines a task graph with tasks in M as nodes. There is an edge between nodes Mi and Mj if and only if Mj ∈ Ai, i.e. ifMj is an action of taskMi. To avoid infinite recursion, the task graph has to be acyclic. Figure 1 shows a simplified task graph of the Taxi domain, commonly used to illustrate MAXQ decomposition.\nThe aim of MAXQ decomposition is to learn a hierarchical policy π = (π0, . . . , πn), i.e. a separate policy πi : S → Ai for each individual task Mi, 0 ≤ i ≤ n. Each taskMi defines its own value function Vi that, for each state s ∈ S, estimates the expected cumulative reward until Mi terminates. The reward associated with applying action Mj ∈ Ai in state s of task Mi equals the value of Mj in\ns, i.e. Ri(s,Mj) = Vj(s). Hence the Bellman optimality equation for Mi decomposes as\nVi(s) = max Mj∈Ai\n{ Vj(s) +\n∑ s′ P (s′|s,Mj)Vi(s′)\n} ,\nwhere P (s′|s,Mj) is the probability of transitioning from s to s′ when applying the (possibly composite) action Mj . If Mj is a primitive task corresponding to an action a ∈ A of the original MDP M , its value is the expected immediate reward, i.e. Vj(s) = R(s, a). The pseudo-reward R̃i is only used for learning the policy πi ofMi and does not contribute to the value function.\nDietterich (2000) proposed an online algorithm for MAXQ decomposition called MAXQ-Q learning. The algorithm maintains two value functions for each task Mi: an estimate V̂i of the value function Vi defined above, and an estimate Ṽi of the expected cumulative reward that includes the pseudo-reward R̃i. The estimate Ṽi defines the policy πi for Mi, while the estimate V̂i is passed as reward to parent tasks of Mi. MAXQ-Q learning achieves recursive optimality, i.e. each policy πi is locally optimal with respect to Mi. Dietterich (1999) also showed how to use state abstraction to simplify learning in MAXQ decomposition."
    }, {
      "heading" : "2.3 Linearly-Solvable MDPs",
      "text" : "Linearly-solvable MDPs (LMDPs) were first introduced by Todorov (2006; 2009b). The original formulation has no explicit actions, and control consists in changing a predefined uncontrolled probability distribution over next states. An alternative interpretation is to view the resulting probability distribution as a stochastic policy over deterministic actions. Todorov’s idea was to transform the discrete optimization problem over actions to a continuous optimization problem over transition probabilities, which is convex and analytically tractable.\nFormally, an LMDP L = 〈S, P,R〉 consists of a set of states S, an uncontrolled transition probability distribution P : S × S → [0, 1] satisfying ∑ s′ P (s\n′|s) = 1 for each state s ∈ S, and an expected reward function R : S → R. Given a state s ∈ S and any next state distribution D, we define the set of next states under D as N(s,D) = {s′ : D(s′|s) > 0}. For first-exit problems, LMDPs also have a subset of terminal states T ⊂ S.\nThe control in LMDPs is a probability distribution a(·|s) over next states; for a given next state s′ ∈ S, a(s′|s) can be non-zero only if P (s′|s) is non-zero. The rewardR(s, a) for applying control a in state s is\nR(s, a) = R(s)− λ ·KL(a(·|s)‖P (·|s)) = R(s)− λ · Es′∼a(·|s) [ log\na(s′|s) P (s′|s)\n] ,\nwhere R(s) is the (non-positive) reward associated with state s. KL(a(·|s)‖P (·|s)) is the Kullback-Leibler divergence between a and P , penalizing controls that are significantly different from P . Typically, P is a random walk and λ acts as a temperature parameter. Large values of λ\n(high temperature) lead to solutions which are more stochastic, since deviating from the random dynamics is penalized more. Conversely, very small values of λ (low temperature) result in deterministic policies, since the state-dependent term dominates the immediate cost. LMDPs, in a sense, replace deterministic policies defined over stochastic actions with stochastic policies defined over deterministic actions. In what follows, unless otherwise stated, the next state s′ is always drawn from the distribution a(·|s).\nWe can now define the Bellman optimality equation: 1 λ V (s) = 1 λ max a∈A(s) {R(s, a) + Es′ [V (s′)]}\n= 1\nλ R(s) + max a∈A(s) Es′\n[ 1\nλ V (s′)− log a(s ′|s) P (s′|s)\n] .\nFor a given state s ∈ S, the set A(s) consists of control inputs that satisfy ∑ s′ a(s\n′|s) = 1 and a(s′|s) > 0 → P (s′|s) > 0 for each s′ ∈ S. To bound the values V (s) in the absense of a discount factor, terminal states are absorbing, i.e. P (t|t) = 1 for each t ∈ T .\nIntroducing Z(s) = eV (s)/λ we obtain\n1 λ V (s) = 1 λ R(s) + max a Es′\n[ − log a(s\n′|s) P (s′|s)Z(s′) ] = 1\nλ R(s)−min a Es′\n[ log\na(s′|s) P (s′|s)Z(s′)\n] .\nTo obtain a KL divergence on the right-hand side, introduce a normalization term G[Z](s) = ∑ s′ P (s\n′|s)Z(s′) and insert it into the Bellman equation:\n1 λ V (s) = 1 λ R(s)−min a Es′\n[ log\na(s′|s)G[Z](s) P (s′|s)Z(s′)G[Z](s) ] = 1\nλ R(s) + log G[Z](s)\n−min a KL\n( a(·|s) ∥∥∥∥P (·|s)Z(·)G[Z](s) ) .\nThe KL term achieves a minimum of 0 when the distributions are equal, i.e. the optimal policy is\na∗(s′|s) = P (s ′|s)Z(s′) G[Z](s) .\nExponentiating the Bellman equation gives\nZ(s) = eR(s)/λG[Z](s). We can write this equation in matrix form as\nz = ΩΠz, (1)\nwhere Ω is a diagonal matrix with the terms eR(s)/λ on the diagonal and Π is the transition probability matrix derived from the distribution P . Unlike the Bellman optimality equation, this is a system of linear equations.\nSince Equation (1) is linear, we can solve its eigenvector problem using, for example, the power iteration method. As an alternative, Todorov (2006; 2009b) proposed an online learning algorithm for LMDPs called Z-learning. Similar to Q-learning for MDPs, the idea of Z-learning is to follow a trajectory, record transitions and perform incremental updates to the value function.\nSince LMDPs have no explicit actions, each transition (st, rt, st+1) consists of a state st, a next state st+1 and a reward rt recorded during the transition. Z-learning maintains an estimate Ẑ of the optimal Z value, and this estimate is updated after each transition as\nẐ(st)← (1− α)Ẑ(st) + αert/λẐ(st+1), (2)\nwhere α is a learning rate. Naive Z-learning samples transitions from the passive dynamics P , which essentially amounts to a random walk and leads to slow learning. A better alternative is to use importance sampling to guide exploration by sampling transitions from a more informed distribution. A natural choice is the estimated optimal policy â derived from Ẑ, resulting in the following corrected update rule (Todorov 2006):\nẐ(st)← (1− α)Ẑ(st) + αert/λẐ(st+1)wâ(st, st+1),\nwâ(st, st+1) = P (st+1|st) â(st+1|st) . (3)\nNote that the importance weight wâ(st, st+1) requires access to the passive dynamics P ."
    }, {
      "heading" : "2.4 LMDPs With Transition-Dependent Rewards",
      "text" : "In the original formulation of LMDPs, reward is statedependent. To develop a hierarchical framework based on LMDPs, we have to account for the fact that each task may accumulate different amounts of reward. Hence reward is transition-dependent, depending not only on the current state but also on the next state. In this section we extend LMDPs to transition-dependent reward, i.e. the expected reward functionR : S×S → R is defined over pairs of states. Then the rewardR(s, a) of applying control a in state s is\nR(s, a) = Es′ [R(s, s′)]− λ ·KL(a(·|s)‖P (·|s))\n= Es′\n[ R(s, s′)− λ · log a(s\n′|s) P (s′|s)\n] .\nThe Bellman equation becomes\n1 λ V (s) = 1 λ max a {R(s, a) + Es′ [V (s′)]}\n= max a Es′\n[ 1\nλ (R(s, s′) + V (s′))− log a(s ′|s) P (s′|s)\n] .\nLetting Z(s) = eV (s)/λ and O(s, s′) = eR(s,s ′)/λ yields\n1 λ V (s) = −min a Es′\n[ log\na(s′|s) P (s′|s)O(s, s′)Z(s′)\n] .\nNormalizing as G[Z](s) = ∑ s′ P (s\n′|s)O(s, s′)Z(s′) yields V (s)/λ = log G[Z](s) and results in the policy\na∗(s′|s) = P (s ′|s)O(s, s′)Z(s′) G[Z](s) .\nExponentiating the Bellman equation gives Z(s) = G[Z](s) which can be written in matrix form as\nz = Γz, (4)\nwhere each entry of Γ equals Γ(s, s′) = P (s′|s)O(s, s′). To solve Equation (4) we can either apply the power iteration method or Z-learning. It is trivial to extend Z-learning to LMDPs with transition-dependent reward. Each transition is still a triplet (st, rt, st+1), and the only difference is that the reward rt now depends on the next state st+1 as well as the current state st. If we compare to the target value Z(s) = G[Z](s), we see that the update rule in Equation (2) causes Ẑ to tend towards the optimal Z value when using the uncontrolled distribution P to sample transitions. The update for importance sampling in Equation (3) can also be directly applied to LMDPs with transition-dependent reward.\n3 Hierarchical LMDPs In this section we formalize a framework for hierarchical LMDPs based on MAXQ decomposition. We assume that there exists an underlying LMDP L = 〈S, P,R〉 and a set of tasks L = {L0, . . . , Ln}, with L0 being the root task. Each task Li = 〈Ti, Ai, R̃i〉, 0 ≤ i ≤ n, consists of a termination set Ti ⊂ S, a set of subtasks Ai ⊂ L and a pseudo-reward R̃i : Ti → R. Each state t ∈ Ti is absorbing and produces reward R̃i(t). For clarity of presentation, we first assume that each task Li is deterministic, i.e. for each state s ∈ S \\ Ti, Li terminates in a unique state ti(s) ∈ Ti. We later show how to extend hierarchical LMDPs to nondeterministic tasks.\nIn MAXQ decomposition, since the actions of the original MDP M = 〈S,A, P,R〉 are included as primitive tasks, the action set Ai of task Mi contains a subset of actions in A. The analogy for hierarchical LMDPs is that each task Li contains a subset of the allowed transitions of the original LMDP L = 〈S, P,R〉, i.e. transitions with non-zero probability according to P . Intuitively, the optimal control ai of each task Li can be viewed as a stochastic policy that selects between deterministic tasks and deterministic next states.\nWe associate each task Li with an LMDP 〈S, Pi, Ri〉with transition-dependent rewards. Task Li is primitive if and only if Ai = ∅. For each state s ∈ S, let Ns ⊆ N(s, P ) be the subset of next states of the original LMDP L that are also present in Li. Further let As = {Lj ∈ Ai | s /∈ Tj} be the set of subtasks of Li that are applicable in s, i.e. for which s is not a terminal state. Clearly, As = ∅ if Li is primitive.\nFor a given state s ∈ S, the passive dynamics Pi and immediate reward Ri of the task LMDP Li are defined in terms of transitions due to Ns\nPi(s ′|s) = P (s ′|s)∑ s′′∈Ns P (s ′′|s) · |Ns| |Ns|+ |As| , ∀s′ ∈ Ns,\nRi(s, s ′) = R(s), ∀s′ ∈ Ns,\nand transitions due to As\nPi(tj(s)|s) = 1\n|Ns|+ |As| , ∀Lj ∈ As, (5)\nRi(s, tj(s)) = Vj(s), ∀Lj ∈ As. (6) Just as in MAXQ decomposition, the reward associated with applying a subtask Lj ∈ Ai of Li in state s equals the value\nfunction of Lj in s, i.e. Ri(s, tj(s)) = Vj(s). The transition (s, tj(s)) associated with subtask Lj ∈ Ai has uniform probability, while transitions in Ns have probabilities proportional to those of P and produce the same reward as in the original LMDP L.\nFor each task Li, the value function Vi estimates the expected cumulative reward until Li terminates and defines the immediate reward for higher-level tasks. We can write the Bellman optimality equation for Li as\nVi(s)\nλ = max ai Es′\n[ 1\nλ (Ri(s, s\n′) + Vi(s ′))− log ai(s ′|s) Pi(s′|s)\n] .\n(7)\nThe task graph for hierarchical LMDPs is defined as for MAXQ decomposition, and has to be acyclic. Figure 2 shows the LMDP task graph of the Taxi domain. Compared to Figure 1, primitive actions no longer appear as tasks, and new sink nodes, e.g. NAVIGATE(t), correspond to primitive tasks of the hierarchical LMDP.\nThe above definitions implicitly consider the following assumptions that differ from the MAXQ formulation and are required for hierarchical LMDPs:\n1. First, we assume that terminal states of subtasks are mutually exclusive and do not overlap with next states in Ns. The reason is that an LMDP is not allowed to have more than one transition between two states with different rewards: if this happens, the optimal policy is not identifiable, since one has to collapse both transitions into one and determine what the resulting immediate reward should be, an ill-defined problem.\n2. Another difference is that each LMDP task Li needs the equivalent of a no-op action, i.e. Pi(s|s) > 0, so that the corresponding Markov chain is aperiodic, needed for convergence of the power-iteration method.\n3. Finally, unlike MAXQ decomposition, the value function Vi in Equation (7) includes KL terms due to differences between the control ai and uncontrolled dynamics Pi. The reward Ri(s, tj(s)) = Vj(s) also includes subtask dependent KL terms. Consequently, the value function V0(s) of the root task L0 includes KL terms from all other tasks. Although this introduces an approximation, we can control the relative importance of KL terms by adjusting the value of λ."
    }, {
      "heading" : "3.1 Task Compositionality for Non-Deterministic Tasks",
      "text" : "In this subsection, we extend the definition of hierarchical LMDPs to non-deterministic tasks. As before, we associate with each task Li an LMDP 〈S, Pi, Ri〉 with the important difference that Li (and it subtasks) can have more than one terminal state. Primitive subtasks (or transitions Ns) are addressed as before, and we omit them for clarity. We thus have to define passive dynamics and immediate rewards for nonprimitive subtasks Lj that can have more than one terminal state.\nDenote tj,k(s) ∈ Tj , 1 ≤ k ≤ |Tj |, as the k-th terminal state of subtask Lj . We define the counterparts of Equations (5) and (6) for multiple terminal states as\nPi(tj,k(s)|s) = P j(tj,k(s)|s) |Ns|+ |As| , ∀Lj ∈ As, tj,k(s) ∈ Tj ,\n(8) Ri(s, tj,k(s)) = Vj,k(s), ∀Lj ∈ As, tj,k(s) ∈ Tj .\n(9)\nwhere the transition probability P j(tj,k(s)|s) of subtask Lj from state s to terminal state tj,k(s) and the value function Vj,k(s) can be expressed using compositionality of optimal control laws for LMDPs (Todorov 2009a), as described below. Note that P j is different from the immediate transition probabilities Pj for subtask Lj , and that the total transition probability of subtask Lj is still 1/(|Ns| + |As|), but it is now distributed among the possible terminal states according to P j(tj,k(s)|s).\nFor each terminal state tj,k(s) ∈ Tj of task Lj , we define a separate task Lj,k . The new tasks are identical to Lj and have |Tj | terminal states that differ only in their pseudorewards R̃j,k. For task Lj,k, the pseudo-reward for tj,k(s) (goal) is zero and the remaining |Tj |−1 terminal states have the same (negative) pseudo-rewardC, e.g. R̃j,k(tj,k(s)) = 0 and R̃j,k(tj,l) = C, l 6= k.\nConsider the optimal policy a∗j,k(·|s) and the optimal value Zj,k(s) of each of the individual tasks. Using compositionality, the original task Lj with multiple terminal states can be expressed as a weighted sum of the individual tasks Lj,k. In particular, the composite optimal Zj and policy a∗j are (Todorov 2009a)\nZj(s) = 1\n|Tj | |Tj |∑ k=1 Zj,k(s),\na∗j (·|s) = |Tj |∑ k=1 Zj,k(s) Zj(s) a∗j,k(·|s),\nwhere the mixing weights for composing tasks are uniform and equals 1/|Tj |, since each task Lj,k assigns the same pseudo-reward C to non-goal terminal states.\nThe value function in Equation (9) is then given by\nVj,k(s) = λ log Zj,k(s)\nZj(s) ,\nand the transition probability for Equation (8) is defined recursively for all states s as\nP j(tj,k(s)|tj,k(s)) = 1, P j(tj,k(s)|tj,l(s)) = 0, l 6= k,\nP j(tj,k(s)|s) = ∑ s′ a∗j (s ′|s)P j(tj,k(s)|s′), s /∈ Tj .\nThis defines the hierarchical LMDP framework for nondeterministic tasks. Note that each individual task Lj,k is still deterministic; its purpose is to avoid terminating in a state different from tj,k(s)."
    }, {
      "heading" : "3.2 Hierarchical Learning Algorithms",
      "text" : "The aim of a hierarchical LMDP is to learn an estimated hierarchical control policy â = 〈â0, . . . , ân〉, i.e. an individual control policy âi for each task Li, 0 ≤ i ≤ n. Similar to MAXQ decomposition, there are two main alternatives for learning a hierarchical policy:\n1. Learn each individual policy âi separately in a bottom-up fashion.\n2. Learn all policies simultaneously using a hierarchical execution in a top-down fashion.\nImplementing an algorithm of the first type is straightforward: since each individual task Li is an LMDP, we can using the power iteration method or Z-learning. Since all subtasks of Li are solved before Li itself, the rewards of Li are known and fixed when solving Li.\nTo implement an algorithm of the second type, similar to MAXQ-Q learning, we start at the root task L0 and sample a subtask Li to execute using the current estimate â0 of the policy forL0. We then executeLi until termination, possibly applying subtasks of Li along the way. When Li terminates, we return the control to the root task L0 and another subtask Lj is sampled using â0. This continues until we reach an absorbing state ofL0. During this process, the value function estimates of each task are updated using Z-learning.\nAs in MAXQ-Q learning, if a task Li has pseudo-rewards different from 0, we have to learn two estimates of the value function for Li: one estimate V̂i of the optimal value function Vi that excludes the pseudo-reward R̃i, and another estimate Ṽi that includes the pseudo-reward R̃i. The estimate Ṽi defines the policy âi of Li, while V̂i is passed as reward to parent tasks of Li."
    }, {
      "heading" : "3.3 Intra-Task Z-Learning",
      "text" : "In hierarchical MDPs, the aim is to learn a separate policy for each individual task. Since Q-learning is an off-policy algorithm, it is possible to use transitions recorded during one task to learn the policy of another task. Such intra-task learning is known to converge faster (Sutton and Precup 1998). In this section we describe an algorithm for intra-task Zlearning.\nAs described in Subsection 2.3 , we can use importance sampling to improve exploration. Let (st, rt, st+1) be a transition sampled using the estimated policy âj of task Lj , and consider an update to the estimated value Ẑi of another task\nLi. Even though the sample distribution âj is different from the estimated policy âi of Li, we consider the update in Equation (3)\nẐi(st)← (1− α)Ẑi(st) + αert/λẐi(st+1)wâi(st, st+1). (10)\nTo see why the update rule is correct, simply substitute the expressions for wâi and âi:\nẐi(st)← (1− α)Ẑi(st) + αert/λẐi(st+1) P (st+1|st) âi(st+1|st)\n= (1− α)Ẑi(st)\n+ αert/λ P (st+1|st)Ẑi(st+1) P (st+1|st)Ẑi(st+1) G[Ẑi](st)\n= (1− α)Ẑi(st) + αert/λG[Ẑi](st).\nIn other words, instead of moving Ẑi(st) in the direction of ert/λẐi(st+1), the update rule moves Ẑi(st) in the direction of ert/λG[Ẑi](st), which is precisely the desired value of Ẑi(st). In particular, the importance weight can be shared by the different tasks in intra-task learning.\nFor LMDPs with transition costs, the same update rule can be used, but substituting the expressions for wâi and âi leads to a slightly different result:\nẐi(st)← (1− α)Ẑi(st) + αert/λẐi(st+1) P (st+1|st) âi(st+1|st)\n= (1− α)Ẑi(st)\n+ α P (st+1|st)ert/λẐi(st+1)G[Ẑi](st) P (st+1|st)O(st, st+1)Ẑi(st+1)\n= (1− α)Ẑi(st) + αE[G[Ẑi](st)].\nRecall that O(st, st+1) = eR(st,st+1)/λ. The expectation E[G[Ẑi](st)] results from the fact that the observed reward rt and expected reward R(st, st+1) may be different, but are equal in expectation. For LMDPs with transition costs, G[Ẑi](st) is the desired value of Ẑi(st)."
    }, {
      "heading" : "3.4 State Abstraction",
      "text" : "In hierarchical LMDPs, we can apply the same forms of state abstraction as for MAXQ decomposition (Dietterich 1999). The most common form of state abstraction is projection or max node irrelevance. This form of state abstraction assumes that the state is factored, i.e. S = S1×· · ·×Sk where S1, . . . , Sk are the domains of k state variables. Max node irrelevance identifies state variables that are irrelevant for a given task, implying that the values of these state variables remain the same until completion of the task. Irrelevant state variables can be ignored while learning the value function.\nDietterich (1999) identified other conditions under which it is safe to perform state abstraction. One condition, leaf\nirrelevance, does not apply to hierarchical LMDPs since actions are no longer included as leaves of the task graph. Another condition, result distribution irrelevance, does apply to hierarchical LMDPs: when two or more states have the same transition probabilities with respect to a given task Li, we only need to estimate a single value V̂i (or Ṽi) for these states."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate the proposed framework in two tasks commonly used in hierarchical MDPs: the taxi domain (Dietterich 2000) and an autonomous vehicle guided task (AGV) (Ghavamzadeh and Mahadevan 2007). We compare the following methods:\nZ: Z-learning using naive sampling (i.e. random walk) without correction term, as in Equation (2).\nZ-IS: Z-learning with importance sampling but without intra-task learning, as in Equation (3),\nZ-IS-IL: Z-learning with importance sampling and intratask learning, as in Equation (10).\nQ-G: -greedy Q-learning without intra-task learning. Q-G-IL: -greedy Q-learning with intra-task learning.\nThe Z-learning variants are evaluated using task LMDPs as described in Section 3 To compare with the Q-learning variants, for each task LMDP we construct a traditional MDP following the methodology of Todorov (2009b). The resulting traditional MDP is guaranteed to have the same optimal value function as the original LMDP. Following Todorov (2006), we use dynamic learning rates, which decay as α(τ) = c/(c + τ), where c is optimized separately for each algorithm and τ is the current trial. The parameter for Q-learning is also optimized for best performance.\nTo compare performance, we calculate, for each iteration, the `1-norm of the differences between the learned and the optimal value function, which can be computed exactly in the tasks we consider here. More details about the experiments are described in the supplementary material."
    }, {
      "heading" : "4.1 The Taxi Domain",
      "text" : "The taxi domain is defined on a grid, with four distinguished locations. There is a passenger at one of the four locations, and that passenger wishes to be transported to one of the other three locations. There is also a taxi that must navigate to the passenger’s location, pick up the passenger, navigate to the destination location, and put down the passenger there. We use a variant of the taxi domain (Dietterich 2000) with much larger state space, a 15× 15 grid.\nWe decompose the taxi domain as shown in Figure 2. Just like Dietterich (2000), we apply state abstraction in the form of projection to the navigate tasks, ignoring the passenger’s location and destination. This results in state spaces of sizes 625 and 3,125 for the navigation and full task, respectively.\nThe primitive tasks (NAVIGATE) contain all state transitions associated with navigation actions: NORTH, SOUTH, EAST, WEST and IDLE (i.e. the no-op action). There are four of these primitive tasks, one for each location (corner) in the grid. The corresponding LMDPs are very similar to the grid example of Todorov (2006): the passive dynamics is a random walk and the state-cost term is zero for the terminal states (the corresponding corner) and 1 elsewhere.\nFigure 3 shows the performance of different learning methods in the primitive task of this domain. The best results are obtained with Z-learning with importance sampling and intra-task learning (Z-IS-IL). The Z-learning variants outperform the Q-learning variants mainly because Z-learning, unlike Q-learning, does not need a maximization operator or state-action values (Todorov 2006). Remarkably, Z-IS (without intra-task learning) still performs better than Q-learning with intra-task learning. Naive Z-learning (Z) performs better than greedy Q-learning (Q-G) because in this particular task, random exploration is still useful to learn a location at one corner of the grid.\nThe full task is composed of the four possible navigation subtasks plus the transitions resulting from applying the original PICKUP and PUTDOWN actions and the IDLE transition. Figure 4 shows the results in the full task. Since there is only one such task, intra-task learning does not apply. In this case, random exploration converges very slowly,\nas the curve of Z indicates. Also, the advantage of Z-learning with importance sampling over -greedy Q-learning is less pronounced than in the primitive tasks.\nFrom these results, we can conclude that the proposed extensions of Z-learning outperform the state-of-the-art learning methods for this domain."
    }, {
      "heading" : "4.2 The Autonomous Guided Vehicle (AGV) Domain",
      "text" : "The second domain we consider is a variant of the AGV domain (Ghavamzadeh and Mahadevan 2007). In this problem, an AGV has to transport parts between machines in a warehouse. Different parts arrive to the warehouse at uncertain times, and these parts can be loaded from the warehouse and delivered to the specific machines that can process and assemble them. Once a machine terminates, the AVG can pick up the assembly and bring it to the unload location of the warehouse.\nThe state space of the full problem has nine components: three components for the position of the AGV (x, y and angle), one for the type of the part that the AGV is carrying and five to represent the different parts that are available to pick up from the warehouse of from the assembly locations. To convert the overall problem into a first-exit task, we do not allow new parts to arrive at the warehouse, and the task is to assemble all parts and deliver them to the unload station. For more details, see the supplementary material.\nAn important feature of this problem is that the AVG can only navigate using transitions corresponding to primitive actions FORWARD, TURN-LEFT, TURN-RIGHT and STAY. Unlike the taxi domain, this significantly constrains the trajectories required to navigate from one location to another in the warehouse. Similar to the taxi domain, we define six primitive NAVIGATE tasks for navigating to the six dropoff and pickup locations in the warehouse. As before, we apply state abstraction in the form of projection to these tasks, ignoring the location of all parts and assemblies.\nFigure 5 shows the result of different learning methods on the NAVIGATE tasks. They are similar to those in the taxi\ndomain, although Q-learning with intra-task learning performs comparatively better, while naive Z-learning performs comparatively worse. The latter result can be explained by the need of guided exploration for navigating in this domain.\nSince the total number of states is large, we also apply state abstraction in the form of result distribution irrelevance to the overall task. Since NAVIGATE tasks always terminate in a predictable state, it is not necessary to maintain a value function for other than dropoff and pickup locations.\nWe have also implemented an online algorithm similar to MAXQ-Q learning. Instead of using the value function of subtasks to estimate transition rewards, we execute each subtask until termination, recording the reward along the way. The reward accumulated during the subtask is then used as the observed immediate reward for the abstract task. For performance we measure the throughput, i.e. the number of assemblies delivered to the unload station per time step. Figure 5 shows the relative performance of Z-learning with importance sampling and Q-learning for this MAXQQ variant. We omit naive Z-learning, since the throughput of a random walk is constant over time. The number of time steps includes all primitive transitions, including those of the NAVIGATE subtasks. As the figure shows, Z-learning converges more quickly to a suboptimal policy compared to Qlearning, illustrating the benefits of hierarchical LMDPs.\n5 Conclusions We have presented a framework for hierarchical reinforcement learning that combines the MAXQ decomposition and formulates each task as a linearly-solvable MDP. The framework has been illustrated in two domains, in which the hierarchical, intra-task Z-learning algorithm outperforms the state-of-the-art methods for hierarchical MDPs.\nHierarchical Linearly-Solvable Markov Decision Problems (Supplementary Material)\nAnders Jonsson and Vicenç Gómez\nDepartment of Information and Communication Technologies Universitat Pompeu Fabra\nRoc Boronat 138, 08018 Barcelona, Spain {anders.jonsson,vicen.gomez}@upf.edu"
    }, {
      "heading" : "A Experimental Setup",
      "text" : "To compare with the Q-learning variants, for each task LMDP we construct a traditional MDP following the methodology of Todorov (2009b). For each state s, we define a symbolic action with transition probability distribution matching the optimal a∗(·|s), which is computed using power iteration method in the original LMDP. We also define as many other symbolic actions as number of possible states following s. Their transition probabilities are obtained from a∗(·|s) by circular shifting. The reward of the symbolic actions is R(s) + λKL(a∗(·|s)||P (·|s)). The resulting traditional MDP is guaranteed to have the same optimal value function that the original LMDP.\nFollowing Todorov (2006), we use dynamic learning rates, which decay as α(τ) = c/(c+ τ), where the constant c is optimized separately for each algorithm and τ is the current trial. The parameter for Q-learning is also optimized for best performance.\nTo compare performance, we calculate, for each iteration, the `1-norm of the differences between the learned and the optimal value function,\nA.1 The Taxi Domain The taxi domain is defined on a grid, with four distinguished locations. There is a passenger at one of the four locations, and that passenger wishes to be transported to one of the other three locations. There is also a taxi that must navigate to the passenger’s location, pick up the passenger, navigate to the destination location, and put down the passenger there. We use a variant of the taxi domain (Dietterich 2000) with much larger state space, a 15×15 grid, as shown in Figure 7.\nThe state space is composed of (x, y, c), where x and y are the horizontal and vertical coordinates of the taxi location and c = 0, . . . , 4 is the location of the passenger 0, . . . , 3 for the different pickup locations and 4 when the passenger is in the taxi. Just like Dietterich (2000), we apply state abstraction in the form of projection to the navigate tasks, ignoring the passenger’s location and destination. This results in state spaces of sizes 625 and 3,125 for the navigation and full\ntask, respectively. The primitive tasks (NAVIGATE) are composed of all state transitions corresponding to navigation actions, namely, NORTH, SOUTH, EAST, WEST and IDLE (i.e. the no-op action). There are four of these primitive tasks, one for each location (corner) in the grid. The corresponding LMDPs are very similar to the grid example of (2006): the passive dynamics is a random walk and the state-cost term is zero for the terminal states (the corresponding corner) and 1 elsewhere.\nIn terms of scalability, the bottleneck of the algorithm is the numerical precision required for computing the exact optimal value function. This precision strongly depends on the absolute difference between the maximum and the minimum values in the matrices Π (or Γ). In order to obtain good es-\n9\ntimates, one needs to set λ sufficiently small, as mentioned in Todorov (2006), which increases the required numerical precision. Although we obtained correct policies for larger grids, For 50 × 50 grids we started to have numerical problems, since the threshold required to check convergence of the power iteration method is ≈ 10−300.\nA.2 The Autonomous Guided Vehicle (AGV) Domain\nThe second domain we consider is a variant of the AGV domain (Ghavamzadeh and Mahadevan 2007). In this problem, an AGV has to transport parts between machines in a warehouse. Different parts arrive to the warehouse at uncertain times, and these parts can be loaded from the warehouse and delivered to the specific machines that can process and assemble them. Once a machine terminates, the AVG can pick up the assembly and bring it to the unload location of the warehouse.\nWe simplified the original problem by reducing the number of machines from 4 to 2 and setting the processing time of machines to 0 to make the task fully observable (see Figure 8).\nThe state space of the full problem has the following components: • x coordinate of the AGV position • y coordinate of the AGV position • c orientation of the AGV (up,right,down,left) • Num. parts at the input buffer of Machine 1 (0, 1, 2) • Num. parts at the output buffer of Machine 1 (0, 1, 2) • Num. parts at the input buffer of Machine 2 (0, 1, 2) • Num. parts at the output buffer of Machine 2 (0, 1, 2)\n• Part of type 1 available at the warehouse (0, 1) • Part of type 2 available at the warehouse (0, 1) To convert the overall problem into a first-exit task, we do not allow new parts to arrive at the warehouse, and the task is to assemble all parts and deliver them to the unload station. The resulting task has approximately 75, 000 states.\nThe bottleneck of the algorithm is again a numerical precision error for calculating the optimal value function using the power iteration method.\nReferences [Abbasi-Yadkori et al. 2015] Abbasi-Yadkori, Y.; Bartlett, P. L.; Chen, X.; and Malek, A. 2015. Large-scale Markov decision problems with KL control cost and its application to crowdsourcing. In 32nd International Conference on Machine Learning (ICML) 2015, 1053–1062.\n[Botvinick, Niv, and Barto 2009] Botvinick, M. M.; Niv, Y.; and Barto, A. C. 2009. Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective. Cognition 113(3):262–280.\n[Dietterich 1999] Dietterich, T. G. 1999. State abstraction in MAXQ hierarchical reinforcement learning. In Advances in Neural Information Processing Systems 12 (NIPS), 994– 1000.\n[Dietterich 2000] Dietterich, T. G. 2000. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artificial Intelligence Research 13:227– 303.\n[Friston et al. 2013] Friston, K.; Schwartenbeck, P.; FitzGerald, T.; Moutoussis, M.; Behrens, T.; and Dolan, R. J. 2013. The anatomy of choice: active inference and agency. Frontiers in Human Neuroscience 7:598.\n[Ghavamzadeh and Mahadevan 2007] Ghavamzadeh, M., and Mahadevan, S. 2007. Hierarchical average reward reinforcement learning. Journal of Machine Learning Research 8:2629–2669.\n[Kappen, Gómez, and Opper 2012] Kappen, H. J.; Gómez, V.; and Opper, M. 2012. Optimal control as a graphical model inference problem. Machine Learning 87(2):159– 182.\n[Kappen 2005] Kappen, H. J. 2005. Linear theory for control of nonlinear stochastic systems. Physical Review Letters 95:200201.\n[Kinjo, Uchibe, and Doya 2013] Kinjo, K.; Uchibe, E.; and Doya, K. 2013. Evaluation of linearly solvable Markov decision process with dynamic model learning in a mobile robot navigation task. Frontiers in Neurorobotics 7:1–13.\n[Matsubara, Gómez, and Kappen 2014] Matsubara, T.; Gómez, V.; and Kappen, H. J. 2014. Latent Kullback Leibler control for continuous-state systems using probabilistic graphical models. In 30th Conference on Uncertainty in Artificial Intelligence (UAI), 583–592. AUAI Press.\n[Ortega and Braun 2013] Ortega, P. A., and Braun, D. A. 2013. Thermodynamics as a theory of decision-making with\ninformation-processing costs. In Proceedings of the Royal Society of London A, volume 469, 20120683. The Royal Society.\n[Sutton and Precup 1998] Sutton, R. S., and Precup, D. 1998. Intra-option learning about temporally abstract actions. In Proceedings of the 15th International Conference on Machine Learning (ICML), 556–564. Morgan Kaufman.\n[Theodorou, Buchli, and Schaal 2010] Theodorou, E.; Buchli, J.; and Schaal, S. 2010. A generalized path integral control approach to reinforcement learning. Journal of Machine Learning Research 11:3137–3181.\n[Todorov 2006] Todorov, E. 2006. Linearly-solvable Markov decision problems. Advances in Neural Information Processing Systems 19 (NIPS) 1369–1376.\n[Todorov 2009a] Todorov, E. 2009a. Compositionality of optimal control laws. Advances in Neural Information Processing Systems 22 (NIPS) 1856–1864.\n[Todorov 2009b] Todorov, E. 2009b. Efficient computation of optimal actions. Proceedings of the National Academy of Sciences of the United States of America 106(28):11478– 11483.\n[Todorov 2009c] Todorov, E. 2009c. Eigenfunction approximation methods for linearly-solvable optimal control problems. In Proceedings of the 2nd IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, 161–168.\n[Watkins 1989] Watkins, C. J. C. H. 1989. Learning from Delayed Rewards. Ph.D. Dissertation, King’s College, Cambridge, UK."
    } ],
    "references" : [ {
      "title" : "P",
      "author" : [ "Abbasi-Yadkori, Y.", "Bartlett" ],
      "venue" : "L.; Chen, X.; and Malek, A.",
      "citeRegEx" : "Abbasi.Yadkori et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A",
      "author" : [ "M.M. Botvinick", "Y. Niv", "Barto" ],
      "venue" : "C.",
      "citeRegEx" : "Botvinick. Niv. and Barto 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "1999",
      "author" : [ "Dietterich", "T. G" ],
      "venue" : "State abstraction in MAXQ hierarchical reinforcement learning. In Advances in Neural Information Processing Systems 12 (NIPS), 994–",
      "citeRegEx" : "Dietterich 1999",
      "shortCiteRegEx" : null,
      "year" : 1000
    }, {
      "title" : "T",
      "author" : [ "Dietterich" ],
      "venue" : "G.",
      "citeRegEx" : "Dietterich 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "R",
      "author" : [ "K. Friston", "P. Schwartenbeck", "T. FitzGerald", "M. Moutoussis", "T. Behrens", "Dolan" ],
      "venue" : "J.",
      "citeRegEx" : "Friston et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Mahadevan",
      "author" : [ "M. Ghavamzadeh" ],
      "venue" : "S.",
      "citeRegEx" : "Ghavamzadeh and Mahadevan 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "H",
      "author" : [ "Kappen" ],
      "venue" : "J.; Gómez, V.; and Opper, M.",
      "citeRegEx" : "Kappen. Gómez. and Opper 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "H",
      "author" : [ "Kappen" ],
      "venue" : "J.",
      "citeRegEx" : "Kappen 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Evaluation of linearly solvable Markov decision process with dynamic model learning in a mobile robot navigation task. Frontiers in Neurorobotics 7:1–13",
      "author" : [ "Uchibe Kinjo", "K. Doya 2013] Kinjo", "E. Uchibe", "K. Doya" ],
      "venue" : null,
      "citeRegEx" : "Kinjo et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kinjo et al\\.",
      "year" : 2013
    }, {
      "title" : "H",
      "author" : [ "T. Matsubara", "V. Gómez", "Kappen" ],
      "venue" : "J.",
      "citeRegEx" : "Matsubara. Gómez. and Kappen 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "D",
      "author" : [ "P.A. Ortega", "Braun" ],
      "venue" : "A.",
      "citeRegEx" : "Ortega and Braun 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Precup",
      "author" : [ "R.S. Sutton" ],
      "venue" : "D.",
      "citeRegEx" : "Sutton and Precup 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A generalized path integral control approach to reinforcement learning",
      "author" : [ "Buchli Theodorou", "E. Schaal 2010] Theodorou", "J. Buchli", "S. Schaal" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Theodorou et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Theodorou et al\\.",
      "year" : 2010
    }, {
      "title" : "C",
      "author" : [ "Watkins", "C. J" ],
      "venue" : "H.",
      "citeRegEx" : "Watkins 1989",
      "shortCiteRegEx" : null,
      "year" : 1989
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We present a hierarchical reinforcement learning framework that formulates each task in the hierarchy as a special type of Markov decision process for which the Bellman equation is linear and has analytical solution. Problems of this type, called linearly-solvable MDPs (LMDPs) have interesting properties that can be exploited in a hierarchical setting, such as efficient learning of the optimal value function or task compositionality. The proposed hierarchical approach can also be seen as a novel alternative to solving LMDPs with large state spaces. We derive a hierarchical version of the socalled Z-learning algorithm that learns different tasks simultaneously and show empirically that it significantly outperforms the state-of-the-art learning methods in two classical hierarchical reinforcement learning domains: the taxi domain and an autonomous guided vehicle task.",
    "creator" : "LaTeX with hyperref package"
  }
}