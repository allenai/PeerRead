{
  "name" : "1606.06234.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CNNLab: a Novel Parallel Framework for Neural Networks using GPU and FPGA",
    "authors" : [ "Maohua Zhu", "Liu Liu", "Chao Wang", "Yuan Xie" ],
    "emails" : [ "liu}@umail.ucsb.edu", "cswang@ustc.edu.cn", "yuanxie@ece.ucsb.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION In the past several years, machine learning has become pervasive in various research fields and commercial applications with achieved satisfactory products. In particular, the emerging of deep learning speeded up the development of machine learning and artificial intelligence. Consequently, deep learning has become a research hotspot in research organizations and the companies[1]. In general, deep learning uses a multi-layer neural network model to extract high-level features into a combination of low-level abstractions to find the distributed data features, to solve complex problems in machine learning. Currently, the most widely used neural models of deep learning are Deep Neural Networks (DNNs) and Convolution Neural Networks (CNNs), which have excellent capability in solving picture recognition, voice recognition, and other complex machine learning tasks.\nHowever, with the increasing accuracy requirements and complexity for the practical applications, the size of the networks becomes explosively large scale (for example, the Google cat-recognizing system has 1 Billion neuronal connections). The explosive volume of data makes the data centers quite power consuming. Therefore, it poses significant\nchallenges to implementing high-performance deep learning networks with low power cost, especially for large-scale deep learning neural network models.\nThe state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4]. GPU has been well recognized for its high performance in massive computing capacity. Compared with GPU acceleration, hardware accelerators like FPGA and ASIC can achieve at least satisfying performance with lower power consumption. However, both FPGA and ASIC have relatively limited computing resources, memory, and I/O bandwidths, therefore it is challenging to develop complex and massive deep neural networks using hardware accelerators. Up to now, the problem of providing efficient middleware support for different architectures has not been adequately solved.\nAnother challenge is the diversity and programming in deep learning applications. Due to the design complexity of the deep learning algorithms, architectures, and accelerators, it requires significant programming effort to make satisfying utilization of the accelerations in diverse application domains. If the computation is solved by the programmer manually, the quality of scheduling depends on the experiences of the programmer, who has limited knowledge of the hardware. To alleviate the burden of the high-level programmers, we can demonstrate the effectiveness of an efficient middleware support in deep learning research paradigm. To tackle these problems, in this paper, we present CNNLab, which is a middleware architecture targeting the state-of-the-art GPU and FPGA-based deep learning acceleration engines. Our main contributions are the following:\n• We introduce a novel framework into the state-of-the-art architecture for deep learning applications. Applications can be mapped heterogeneous accelerators within a wellstructured interface to improve the flexibility and scalability. CNNLab maps the applications into computing kernels using CUDA and OpenCL programming interfaces. • CNNLab is based on a heterogeneous hybrid system which includes the software processor, GPU accelerator, and FPGA-based hardware accelerator to speed up the\nar X\niv :1\n60 6.\n06 23\n4v 1\n[ cs\n.L G\n] 2\n0 Ju\nn 20\n16\nkernel computational parts of deep learning algorithms. In particular, we utilize an efficient middleware support to bridge the gap between high-level neural networks and hardware accelerators. • We construct a hardware prototype using state-of-theart Nvidia K40 GPU and Altera FPGA platforms. Experimental results on real hardware demonstrate that CNNLab can achieve remarkable speedup with insignificant overheads. More importantly, to explore the tradeoffs between different implementations using FPGA and GPU, we leverage the trade-offs by analyzing the quantitative results for the running time, throughput, power, energy and performance density of the accelerators.\nThe rest of the paper is organized as follows: Section II summarizes the problem description and motivation. Section III discusses the CNNLab architecture, including architecture, the programming model, and the hierarchical layers. After that, Section IV describes the GPU and FPGA implementation of the accelerators. We analyze the detailed results and discusses the trade-offs between the GPU and FPGA based implementations. Section V outlines the related study of the neural network accelerators. Finally, Section VI describes the conclusion and further works."
    }, {
      "heading" : "II. PROBLEM DESCRIPTION AND MOTIVATION",
      "text" : "Deep Learning has recently gained great popularity in the machine learning community due to their potential in solving previously difficult learning problems. Even though Deep and Convolutional Neural Networks have diverse forms, they share similar properties that a generic description can be formalized. First, these algorithms consist of a large number of layers, which are normally executed in sequence so they can be implemented and evaluated separately. Second, each layer usually contains several sub-layers called feature maps; we then use the terms feature input maps and feature output maps. Overall, there are three main kinds of layers: most of the hierarchy is composed of convolutional and pooling layers, and there is a classifier at the top of the network consisting of one or multiple layers. The role of convolutional layers is to apply one or several local filters to data from the input layer. Consequently, the connectivity between the input and output feature map is local. Consider the case where the input is an image, and the convolution is a 2D transform between a KxKy subset of the input layer and a kernel of the same dimensions, as illustrated in Fig. 1. The kernel values are the synaptic weights between an input layer and an output layer. In\ngeneral, a DNN has two computational steps, including prediction process and training. Prediction process is a feedforward computation which computes the output for each given input with network settings. Training process includes pre-training which locally tunes the connection weights between the units in adjacent layers and global training which globally tunes the connection weights with the back-propagation (BP) algorithm.\nFirstly, we introduce the prediction process which is a feedforward computation. The process computes in accordance with traditional neural network layer by layer, and the outputs of current layer are the inputs of the next layer. The deep neural network is composed of an input layer, an output layer, and multiple hidden layers to get representations of the data with multiple levels of abstraction. The prediction computation of DNNs is a bottom-up feed-forward process, where the output of the lower layer is the input of its upper layer. To present the prediction computation of deep neural networks clearly, we consider a layer (called L) with No neurons, the lower layer of which has Ni neurons. The connectivity between layers is full, so each pair of neurons own their private weight values, resulting an NixNo weight matrix. L reads in Ni inputs (x1, x2, ..., xNi ) from its lower layer, and then produces No outputs (y1, y2, ..., yNo ). The calculation of a neuron yk (k = 1, 2, ...No) in L can be represented as f( ∑Ni\nj=1Wjkxj +bk) where xj is a neuron of the lower layer, f is the activation function, Wjk is the weight coefficient of xj and yk , and bk means the offset value. Since ∑Ni j=1Wjkxj can be regarded as a multiplication between a row vector −→X and a column vector −→Wk , the computation of the whole layer can be formalized as a vector-matrix multiplication and activation function process, shown as Equation (1):\n−→ Y = f ( −→ X ∗W ) (1)\nwhere: −→ X = (1 , x1 , x2 , ..., xNi ), −→ Y = (y1 , y2 , ..., yNo ) (2)\nW =  b1 b2 · · · bNo W11 W12 · · · W1No ... ... . . . ...\nWNi1 WNi2 · · · WNiNo  (3) f (x ) = 1\n1 + e−x , sigmoid function (4)\nTo accelerate the kernel function of the CNN processing, this paper presents CNNLab architecture, which uses both GPU and FPGA as the platform to explore the tradeoff of the performance/power metrics."
    }, {
      "heading" : "III. THE CNNLAB ABSTRACTION",
      "text" : ""
    }, {
      "heading" : "A. Data Model and Processing Flow",
      "text" : "In this section, we present the modeling framework of the CNNLab architecture. Fig. 2 illustrates the common infrastructure of CNNLab with high-level perspective. The frontend cloud users access the CNNlab platform via a uniform programming interface with the user definitions of the CNN\nmodel, whereby each layer is packaged and exposed in an API-like manner. Meanwhile, in the back-end, each layer is composed of specific functionalities provided by software libraries and resource pool through consistent communication interfaces. The functionality for each layer is combined with multiple data resources with a sequence of input/output parameters. It should be noted that the detailed composition and the middleware scheduling is invisible to front-end software users. At runtime, the application is first decomposed into multiple layers under the definition of specific parameters, which are then scheduled at runtime. Whenever a pending layer has obtained its requisite input parameters, it can be offloaded to a particular accelerator for immediate execution.\nFig. 3 presents a high-level overview of CNNLab processing flow in following steps. As the first step, the Deep Learning Specialist provides as inputs a high-level description of a ConvNet architecture together with information about the target CNNLab platform with the aid of a general layerbased model that we designed. Next, the structure of the NN input model will undergo the design space exploration and trade-off analysis in the middleware support, considering the requirements of the application. The design space is searched, and this process yields a succession of hardware mappings of the NN model onto the particular FPGA-based or GPU-based platforms, using OpenCL or CUDA programming interface, respectively."
    }, {
      "heading" : "B. User Defined Computation",
      "text" : "To describe the CNN model, we define each layer associated with a tuple of parameters. Currently, the following types of layers are supported, which are the ones that have been most commonly used in the ConvNet literature:\n1) Convolutional Layer: The model of Convolutional Layer is abstracted as\n< MI ,MK ,MO, S, T > (5)\nwhere • MI and MO are the input/output matrix of each convolu-\ntional layer, which includes height × width × dimension. • MK refers to the kernel size each accelerator can pro-\ncessed with, which includes height × width × dimension. • S is the stride which defines the step between successive\nconvolution windows. • T is the type of nonlinear function to be applied, e.g.\nsigmoid, tanh or ReLU. 2) Normalization Layer: The model of Normalization\nLayer is abstracted as\n< MI , T, S, α, β > (6)\nwhere • MI is the input matrix of the normalization layer, which\nincludes height × width × dimension. • T is the type of normalization operation to be applied. • S is the local size applied in the nonlinear layer. • α and β are the parameters used in LRN computation. 3) Pooling Layer: The model of Pooling Layer is abstracted as < MI ,MO, T, S,N > (7)\nwhere • MI and MO are the input/output matrix of the pooling\nlayer, which includes height × width × dimension. • T is the type of pooling operation to be applied, i.e. either\nmax or average. • N is the number of pooling kernels in the pooling layer. • S is the stride which defines the step between successive\npooling windows.\n4) FC Layer: The model of FC Layer is abstracted as\n< MI ,KO > (8)\nwhere • MI is the input matrix of each convolutional layer, which\nincludes height × width × dimension for FC-dropout layer.\n• KO stands for the output of the FC layer."
    }, {
      "heading" : "C. Programming Model and Wrapper",
      "text" : "Based on the flexible programming framework in CNN lab, the tasks can be distributed to either GPU and FPGA-based accelerators. In particular, the middleware support should provide a uniform runtime for different hardware architectures. Fig. 4 illustrates the general programming framework based on the runtime kernels. The API forwards the requests via the scheduling middleware on the host code. The host code can offload part of the execution threads to CUDA kernels or OpenCL kernels, depending on the accelerator architecture and the design space exploration. Different kernels share a virtual memory space for communication among the parameters of accelerators. The scheduling process and run-time support are invisible to the programmers as the API provides an efficient bridge for high-level applications to hardware implementations. In particular, we use two example code snippets using both CUDA (cuDNN V5) and OpenCL as a demonstration.\nFig. 5 illustrates an example code segment using GPU and FPGA-based accelerators. The general processing flow\nincludes following steps: 1) Platform Initialization, 2) Set Input and Kernel Descriptors, 3) Computation using Accelerators, and 4) Data Synchronization after Execution. To offload the kernels to the GPU and FPGA-based accelerators, the OpenCL contexts and cuDNN contexts are invoked with specific primitives. The code segments is general so as to be ported to other GPU or FPGA based hardware platforms."
    }, {
      "heading" : "IV. EXPERIMENTS RESULTS AND TRADE-OFF ANALYSIS",
      "text" : ""
    }, {
      "heading" : "A. Platform Setup and Network Description",
      "text" : "To measure the performance and overheads of CNNLab architecture, we implemented a hardware prototype with hybrid heterogeneous systems:\nCPU: An Intel Corei7-4770 processor clocked at 3.4 GHz was used as the CPU controller. The CPU processor assigns the computational tasks to the acceleration kernels via PCIE X8 edge connector for interconnection.\nFPGA: An Intel-Altera DE5 was used to implement the design of deep learning module. Altera Quartus II toolchain to evaluate the speedup and hardware cost, as well as the PowerPlay to estimate the power consumption. The running frequencies of the accelerators on FPGA range from 171MHz to 304MHz (see Table III for detail).\nGPU: A Nvidia K40 was used as the GPU accelerator, with 12,288 MB memory capacity, peak bandwidth of device memory at 288 GB/s, and peak single-precision floating point performance at 4.29TFLOPS. We use the state-of-the-art\nTABLE I DESCRIPTION OF THE EXPERIMENTAL NEURAL NETWORK MODEL\nLayer Name Layer Type Description Conv1 Conv-ReLU Input: 3x224x224, Kernel: 96x3x11x11, Output: 96x55x55, Stride: 4\nConv2 Conv-ReLU Input: 96x27x27, Kernel: 256x96x5x5, Output: 256x27x27, Stride: 1\nConv3 Conv-ReLU Input: 256x13x13, Kernel: 384x256x3x3, Output: 384x13x13, Stride: 1\nConv4 Conv-ReLU Input: 384x13x13, Kernel: 384x384x3x3, Output: 384x13x13, Stride: 1\nConv5 Conv-ReLU Input: 384x13x13, Kernel: 256x384x3x3, Output: 256x13x13, Stride: 1\nFC6 FC-dropout Input: 256x6x6, Output: 4096\nFC7 FC-dropout Input: 4096, Output: 4096\nFC8 FC-softmax Input: 4096, Output: 1000\n1\n10\n100\n1000\n10000\n100000 (a) Running Time (ms)\nK40-cuDNN DE5-FPGA\n10000\n0\n5\n10\n15\n20\n25 (d) Energy Consumed(J)\nK40-cuDNN DE5-FPGA\n1\n10\n100\n1000\n100 (b) Effective Throughput(GFLOPS)\nK40-cuDNN DE5-FPGA\n0\n5\n10\n15\n20\n25 (e) Throughput/Power(GFLOPS/W)\nK40-cuDNN DE5-FPGA\n1\n10\n100\n1000\n(c) Average Power(W)\nK40-cuDNN DE5-FPGA\n1\n10\n100\n1000\n10000\n100000 (f) Operation/Energy(GFLOP/J)\nK40-cuDNN DE5-FPGA\nFig. 6. Evaluation and Trade-off Analysis between GPU and FPGA based Acceleration Engines.\ncuDNN V5 as the CUDA programming models (released in April 2016).\nTable I introduces the experimental neural network model, including 5 Convolutional Layers and 3 FC Layers. We use ReLU as the nonlinear function in the Convolutional layer. For each layer, the parameters introduced in Section III.B is realized with different configurations."
    }, {
      "heading" : "B. Results Analysis and Trade-offs between FPGA and GPU",
      "text" : "We analyze the trade-offs between the two approaches on the following aspects: Performance(including execution time and throughput), Cost (including power and energy), and performance density (including throughput per watt, and throughput per joule) respectively.\nPerformance. Fig. 6 (a) presents the running time for the eight layers. GPU has better performance than FPGA on all the layers, and the speedup can achieve up to 1000x for FC layers. Regarding the eight layers, the speedup for convolutional layers (1-5) is lower than the FC layers (6-8), which contains matrix multiplication operations. We also evaluate\nand compare the throughput, as illustrated in (b). Results are similar to the running time that the GPU can achieve significant higher throughput than FPGA. For example, the peak throughput for GPU is 1632 GFLOPS in Conv 4 layer, while the peak throughput for FPGA is only 25.56 GFLOPS in Conv 2 layer.\nPower and Energy. To establish the cost model for both approaches, we evaluate the power and energy consumption for GPU and FPGA-based accelerators. Fig. 6 (c) illustrates the comparison of the power cost. The average power for GPU is 97W while the power of FPGA-based accelerator for the convolutional layer is only 2.23W. To this end, FPGA is power saving due to the limited hardware resources and low working frequency (300MHz). Concerning the energy, both approaches have similar energy consumption when running convolutional layers. For example, the average energy for FPGA is 10.24J, while GPU cost 8.67J on average. In comparison, FPGA takes significantly higher energy for FC layers than GPU, as presented in (d). For FC layers, the average energy consumption for FPGA is 12.24J, while GPU\nonly takes 0.64J on average. Results demonstrate that GPU can achieve better energy efficiency on FC layers due to the optimization of matrix multiplication operations.\nPerformance Density. Based on the performance and the power cost, we derive the performance density for both methods. First, for Throughput/Power metrics, GPU and FPGA has similar performance density in convolutional layers, that the GPU achieves 14.12 GFLOPS/W while FPGA gets 10.58 GFLOPS/W. For the FC layers, GPU substantially outperforms FPGA by achieving the average density at 14.20 GFLOPS/W, while FPGA only has 0.82 GFLOPS/W. Regarding the energy metrics, we measure the Operation/Energy (GFLOP/J) as the metric. In this case, GPU far outperforms the GPU by achieving 14732 GFLOP/J for all the layers on average, while FPGA only gets 41.35 GFLOP/J for the convolutional layer on average, and 3.19 GFLOP/J for FC layers.\nAbove results reveals that GPU can achieve higher speedup and throughput while FPGA saves more power consumption. Regarding the energy consumption and performance density, both approaches get similar results for convolutional computation, and GPU outperforms FPGA in the calculation for FC layers."
    }, {
      "heading" : "C. Comparison between Different GPU Models",
      "text" : "Above results demonstrate that GPU can achieve significantly higher throughput and performance density, especially for the FC layers. In this section, to evaluate the impact of the different GPU library models, we use both cuDNN and cuBLAS library to implement the FC layers in forward computation and back propagation, as illustrated in Table II.\nFig. 7 and Fig. 8 present the comparison between cuDNN and cuBLAS for forward computation and back propagation, respectively. We have following observations based on the experimental results:\n• In general, the cuBLAS library kernels achieve higher speedup (calculated by execution time) than the cuDNN library kernels. In particular, the speedup for cuBLAS against cuDNN is 1.69x in forward computation and 24.89x in BP. In comparison, the throughput for cuBLAS\nis 1.77x higher than cuDNN in forward computation, but cuDNN achieves 1.57x than cuBLAS in BP calculation. • The cuDNN and cuBLAS libraries have similar power consumptions for forward computation (79.12W and 78.73W on average, respectively), while for the BP, cuBLAS takes significantly more power saving than cuDNN, with the average power 78.77W and 123.40W respectively. Accordingly, the energy consumption of cuBLAS is much lower than the cuDNN, with the average\nenergy 0.70J and 31.19J respectively. • Regarding the performance density, we calculate the\nThroughput/Power and Operation/Energy accordingly. Results demonstrate cuBLAS substantially outperforms the cuDNN library on performance density metrics."
    }, {
      "heading" : "D. Resources Usage and Running Frequency",
      "text" : "Table III lists the resources and power consumption for the modules in CNNLab accelerator. In particular, Of these NN layers, convolutional layer takes most significant logic devices as it requires computational power. In particular, the convolutional layer needs 73% of the hardware logics, 63% DSP blocks, and 56% RAM blocks. In comparison, pooling\nlayer only takes 17% logic resources and 11% RAM blocks. Regarding the running frequency, the convolutional layer has the lowest frequency at 171.29MHz, while pooling achieves the highest frequency at 304.50MHz accordingly."
    }, {
      "heading" : "V. RELATED WORK",
      "text" : "The neural network model has been an emerging field during the past few years [5]. In this section, we summarize the related acceleration engines, including cloud computing, GPU, and FPGA, respectively."
    }, {
      "heading" : "A. Cloud based Acceleration",
      "text" : "Distributed computing platforms have been widely recognized as the scalable, and easy-to-deploy measures [6]. Project Adam [7] describes the design and implementation of a distributed system comprised of commodity server machines to train large-scale deep learning models. SINGA [8] is a distributed deep learning system for training big models over large datasets. DistBelief [9] is a software framework that can utilize computing clusters with a good number of machines to train large models."
    }, {
      "heading" : "B. GPU based Accelerators",
      "text" : "GPU has been widely applied to the acceleration engine for data-intensive applications. For example, Coates et. al [10] present a high-performance computing system with a cluster of GPU servers, using Infiniband interconnects and MPI. NGPU [11] brings GPU accelerators together without hindering SIMT execution or adding excessive hardware overhead. Li et al. [12] propose an efficient GPU implementation of the large-scale recurrent neural network and demonstrate the power of scaling up the recurrent neural network with GPUs. Vasilache et al. examine the performance profile using fbfft of CNN training on the current generation of GPU [13]. Teng et al. describe an efficient DBN implementation on the GPU, including the pre-training and fine-tuning processes [14]. Recently, GeePS is a scalable deep learning architecture on distributed GPUs with specific parameters [15]."
    }, {
      "heading" : "C. FPGA and Hardware based Accelerators",
      "text" : "To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18]. For the IC based accelerator,\nDiannao [3] is one of the pioneers works solidifying the neural networks on the hardware circuits. Origami [19] present a tape-out accelerator with silicon measurements of power-, area- and I/O efficiency. Meanwhile, FPGA is more flexible due to the integration of the reconfigurable logic devices. Therefore, it can fit changing applications and parameters in neural networks [20], [21]. For example, Zhang et al. [2] explores the bandwidth for the parameters facing the limitation of an FPGA chip. Suda et al. [22] presents a design space exploration method OpenCL programming model approach, which can explore the trade-offs the parameters in the network topologies.\nBesides the ASIC and FPGA-based accelerators, there have been numerous directions using emerging hardware technologies, such as Memristive Boltzmann Machine [23], and Processing-in-Memory techniques [24]. Energy efficient inference engine (EIE) uses compression by pruning the redundant connections and having multiple connections share the same weight [25]."
    }, {
      "heading" : "VI. CONCLUSIONS AND FUTURE WORK",
      "text" : "FPGA and GPU have been demonstrated as very powerful and flexible platform for data-intensive neural network processing in machine learning applications. In this paper, we have presented CNNLab, a middleware support for GPU and FPGA-based framework to accelerate the neural network computing models. It can offload the tasks into different accelerators in the guidance of the neural network model and constraints. To achieve the trade-offs between the GPU and FPGA-based platform, we constructed the real prototype using Intel-Altera DE5 FPGA board and Nvidia K40 GPU platform. We measure the execution time, throughput, power consumption, energy cost, and performance density, respectively.\nExperimental Results show that the GPU has better speedup (100x) and throughput (100x) against FPGA-based accelerator while FPGA is more power saving (50x) than GPU. More importantly, in our case study, the energy consumption fo GPU and FPGA are similar in convolutional computation, while GPU is more energy efficient in FC layer calculation. Regarding the performance density, both approaches achieve similar Throughput/Power metrics in convolutional layers (10GFLOPS/W for FPGA, and 14GFLOPS/W for GPU), but GPU has higher Operation/Energy than FPGAbased accelerators, especially for FC computation. Regarding the improvement between different GPU CUDA programming models, we also evaluate the metrics for the state-of-the-art cuDNN and cuBLAS, respectively. Results show that cuBLAS is more energy efficient with significantly higher speedup and lower power consumption.\nAlthough the experimental results are inspiring, there are some future promising directions. First, the speedup of the accelerators can be further improved by compressed network models. Second, the hardware accelerator can be assisted with a large scale data processing framework like Spark or TensorFlow platforms."
    } ],
    "references" : [ {
      "title" : "Deep leaning",
      "author" : [ "G.H. Yann LeCun", "Yoshua Bengio" ],
      "venue" : "Nature, vol. 521, pp. 436–444, May 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Optimizing fpga-based accelerator design for deep convolutional neural networks",
      "author" : [ "C. Zhang", "P. Li", "G. Sun", "Y. Guan", "B. Xiao", "J. Cong" ],
      "venue" : "FPGA ’15, pp. 161–170, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning",
      "author" : [ "T. Chen", "Z. Du", "N. Sun", "J. Wang", "C. Wu", "Y. Chen", "O. Temam" ],
      "venue" : "ASPLOS ’14, pp. 269–284, 2014.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Neural acceleration for gpu throughput processors",
      "author" : [ "A. Yazdanbakhsh", "J. Park", "H. Sharma", "P. Lotfi-Kamran", "H. Esmaeilzadeh" ],
      "venue" : "MI- CRO’15, pp. 482–493, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "S. Han", "H. Mao", "W.J. Dally" ],
      "venue" : "ICLR’16, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng" ],
      "venue" : "ICML’12, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Project adam: Building an efficient and scalable deep learning training system",
      "author" : [ "T. Chilimbi", "Y. Suzue", "J. Apacible", "K. Kalyanaraman" ],
      "venue" : "OSDI’14, pp. 571–582, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Singa: A distributed deep learning platform",
      "author" : [ "B.C. Ooi", "K.-L. Tan", "S. Wang", "W. Wang", "Q. Cai", "G. Chen", "J. Gao", "Z. Luo", "A.K. Tung", "Y. Wang", "Z. Xie", "M. Zhang", "K. Zheng" ],
      "venue" : "MM ’15, pp. 685–688, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le", "A.Y. Ng" ],
      "venue" : "NIPS’12, pp. 1232–1240, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep learning with cots hpc systems",
      "author" : [ "A. Coates", "B. Huval", "T. Wang", "D. Wu", "B. Catanzaro", "N. Andrew" ],
      "venue" : "ICML’13, vol. 28, pp. 1337– 1345, May 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Neural acceleration for gpu throughput processors",
      "author" : [ "A. Yazdanbakhsh", "J. Park", "H. Sharma", "P. Lotfi-Kamran", "H. Esmaeilzadeh" ],
      "venue" : "MI- CRO’15, pp. 482–493, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Large scale recurrent neural network on gpu",
      "author" : [ "B. Li", "E. Zhou", "B. Huang", "J. Duan", "Y. Wang", "N. Xu", "J. Zhang", "H. Yang" ],
      "venue" : "IJCNN’14, pp. 4062–4069, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fast convolutional nets with fbfft: A gpu performance evaluation",
      "author" : [ "N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun" ],
      "venue" : "ICLR’15, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Optimized deep belief networks on cuda gpus",
      "author" : [ "T. Li", "Y. Dou", "J. Jiang", "Y. Wang", "Q. Lv" ],
      "venue" : "IJCNN’15, pp. 1–8, July 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Geeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server",
      "author" : [ "H. Cui", "H. Zhang", "G.R. Ganger", "P.B. Gibbons", "E.P. Xing" ],
      "venue" : "EuroSys ’16, pp. 4:1–4:16, 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "14.1 a 126.1mw real-time natural ui/ux processor with embedded deep-learning core for low-power smart glasses",
      "author" : [ "S. Park", "S. Choi", "J. Lee", "M. Kim", "J. Park", "H.J. Yoo" ],
      "venue" : "ISSCC’16, pp. 254–255, Jan 2016.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "14.5 eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks",
      "author" : [ "Y.H. Chen", "T. Krishna", "J. Emer", "V. Sze" ],
      "venue" : "ISSCC’16, pp. 262–263, Jan 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "14.6 a 1.42tops/w deep convolutional neural network recognition processor for intelligent ioe systems",
      "author" : [ "J. Sim", "J.S. Park", "M. Kim", "D. Bae", "Y. Choi", "L.S. Kim" ],
      "venue" : "ISSCC’16, pp. 264–265, Jan 2016.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Origami: A convolutional network accelerator",
      "author" : [ "L. Cavigelli", "D. Gschwend", "C. Mayer", "S. Willi", "B. Muheim", "L. Benini" ],
      "venue" : "GLSVLSI ’15, pp. 199–204, 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep learning on fpgas: Past, present, and future",
      "author" : [ "G. Lacey", "G.W. Taylor", "S. Areibi" ],
      "venue" : "arXiv preprint arXiv:1602.04283, 2016.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "fpgaconvnet: A framework for mapping convolutional neural networks on fpgas",
      "author" : [ "S.I. Venieris", "C.-S. Bouganis" ],
      "venue" : "FCCM’16, pp. 40–47, 2016.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Throughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks",
      "author" : [ "N. Suda", "V. Chandra", "G. Dasika", "A. Mohanty", "Y. Ma", "S. Vrudhula", "J.-s. Seo", "Y. Cao" ],
      "venue" : "FPGA, pp. 16–25, 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Memristive boltzmann machine: A hardware accelerator for combinatorial optimization and deep learning",
      "author" : [ "M.N. Bojnordi", "E. Ipek" ],
      "venue" : "HPCA’16, pp. 1–13, March 2016.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A reconfigurable digital neuromorphic processor with memristive synaptic crossbar for cognitive computing",
      "author" : [ "Y. Kim", "Y. Zhang", "P. Li" ],
      "venue" : "J. Emerg. Technol. Comput. Syst., vol. 11, pp. 38:1–38:25, Apr. 2015.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Eie: Efficient inference engine on compressed deep neural network",
      "author" : [ "S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally" ],
      "venue" : "ISCA’16, 2016.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Consequently, deep learning has become a research hotspot in research organizations and the companies[1].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "The neural network model has been an emerging field during the past few years [5].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "Distributed computing platforms have been widely recognized as the scalable, and easy-to-deploy measures [6].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "Project Adam [7] describes the design and implementation of a distributed system comprised of commodity server machines to train large-scale deep learning models.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "SINGA [8] is a distributed deep learning system for training big models over large datasets.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "DistBelief [9] is a software framework that can utilize computing clusters with a good number of machines to train large models.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "al [10] present a high-performance computing system with a cluster of GPU servers, using Infiniband interconnects and MPI.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "NGPU [11] brings GPU accelerators together without hindering SIMT execution or adding excessive hardware overhead.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "[12] propose an efficient GPU implementation of the large-scale recurrent neural network and demonstrate the power of scaling up the recurrent neural network with GPUs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "examine the performance profile using fbfft of CNN training on the current generation of GPU [13].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "describe an efficient DBN implementation on the GPU, including the pre-training and fine-tuning processes [14].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "Recently, GeePS is a scalable deep learning architecture on distributed GPUs with specific parameters [15].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 16,
      "context" : "To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 17,
      "context" : "To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "Diannao [3] is one of the pioneers works solidifying the neural networks on the hardware circuits.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 18,
      "context" : "Origami [19] present a tape-out accelerator with silicon measurements of power-, area- and I/O efficiency.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 19,
      "context" : "Therefore, it can fit changing applications and parameters in neural networks [20], [21].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "Therefore, it can fit changing applications and parameters in neural networks [20], [21].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "[2] explores the bandwidth for the parameters facing the limitation of an FPGA chip.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 21,
      "context" : "[22] presents a design space exploration method OpenCL programming model approach, which can explore the trade-offs the parameters in the network topologies.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "Besides the ASIC and FPGA-based accelerators, there have been numerous directions using emerging hardware technologies, such as Memristive Boltzmann Machine [23], and Processing-in-Memory techniques [24].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 23,
      "context" : "Besides the ASIC and FPGA-based accelerators, there have been numerous directions using emerging hardware technologies, such as Memristive Boltzmann Machine [23], and Processing-in-Memory techniques [24].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 24,
      "context" : "Energy efficient inference engine (EIE) uses compression by pruning the redundant connections and having multiple connections share the same weight [25].",
      "startOffset" : 148,
      "endOffset" : 152
    } ],
    "year" : 2016,
    "abstractText" : "Designing and implementing efficient, provably correct parallel neural network processing is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. However, the diversity and large-scale data size have posed a significant challenge to construct a flexible and high-performance implementation of deep learning neural networks. To improve the performance and maintain the scalability, we present CNNLab, a novel deep learning framework using GPU and FPGA-based accelerators. CNNLab provides a uniform programming model to users so that the hardware implementation and the scheduling are invisible to the programmers. At runtime, CNNLab leverages the trade-offs between GPU and FPGA before offloading the tasks to the accelerators. Experimental results on the state-of-the-art Nvidia K40 GPU and Altera DE5 FPGA board demonstrate that the CNNLab can provide a universal framework with efficient support for diverse applications without increasing the burden of the programmers. Moreover, we analyze the detailed quantitative performance, throughput, power, energy, and performance density for both approaches. Experimental results leverage the trade-offs between GPU and FPGA and provide useful practical experiences for the deep learning research community.",
    "creator" : "LaTeX with hyperref package"
  }
}