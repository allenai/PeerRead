{
  "name" : "1603.07252.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Summarization by Extracting Sentences and Words",
    "authors" : [ "Jianpeng Cheng", "Mirella Lapata" ],
    "emails" : [ "jianpeng.cheng@ed.ac.uk", "mlap@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Traditional approaches to extractive summarization rely heavily on humanengineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation."
    }, {
      "heading" : "1 Introduction",
      "text" : "The need to access and digest large amounts of textual data has provided strong impetus to develop automatic summarization systems aiming to create shorter versions of one or more documents, whilst preserving their information content. Much effort in automatic summarization has been devoted to sentence extraction, where a summary is created by identifying and subsequently concatenating the most salient text units in a document.\nMost extractive methods to date identify sentences based on human-engineered features. These include surface features such as sentence position and length (Radev et al., 2004), the words in the title, the presence of proper nouns, content features such as word frequency (Nenkova et al., 2006), and event features such as action nouns (Filatova and Hatzivassiloglou, 2004). Sentences are typically assigned a score indicating the strength of presence of these features. Several methods\nhave been used in order to select the summary sentences ranging from binary classifiers (Kupiec et al., 1995), to hidden Markov models (Conroy and O’Leary, 2001), graph-based algorithms (Erkan and Radev, 2004; Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010).\nIn this work we propose a data-driven approach to summarization based on neural networks and continuous sentence features. There has been a surge of interest recently in repurposing sequence transduction neural network architectures for NLP tasks such as machine translation (Sutskever et al., 2014), question answering (Hermann et al., 2015), and sentence compression (Rush et al., 2015). Central to these approaches is an encoderdecoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding.\nWe develop a general framework for singledocument summarization which can be used to extract sentences or words. Our model includes a neural network-based hierarchical document reader or encoder and an attention-based content extractor. The role of the reader is to derive the meaning representation of a document based on its sentences and their constituent words. Our models adopt a variant of neural attention to extract sentences or words. Contrary to previous work where attention is an intermediate step used to blend hidden units of an encoder to a vector propagating additional information to the decoder, our model applies attention directly to select sentences or words of the input document as the output summary. Similar neural attention architectures have been previously used for geometry reasoning (Vinyals et al., 2015), with the name Pointer Networks.\nOne stumbling block to applying neural network models to extractive summarization is the\nar X\niv :1\n60 3.\n07 25\n2v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\n01 6\nlack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization (Woodsend and Lapata, 2010; Svore et al., 2007) and reading comprehension (Hermann et al., 2015) we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction1. Previous approaches have used small scale training data in the range of a few hundred examples.\nOur work touches on several strands of research within summarization and neural sequence modeling. The idea of creating a summary by extracting words from the source document was pioneered in Banko et al. (2000) who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words. Our word-based model is similar in spirit, however, it operates over continuous representations, produces multi-sentence output, and jointly selects summary words and organizes them into sentences. A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm. Our work also uses continuous representations to express the meaning of sentences and documents, but importantly employs neural networks more directly to perform the actual summarization task. Rush et al. (2015) propose a neural attention model for abstractive sentence compression which is trained on pairs of headlines and first sentences in an article. In contrast, our model summarizes documents rather than individual sentences, and produces multi-sentential discourse. A major architectural difference is that our decoder selects output symbols from the document of interest rather than the entire vocabulary.\nWe evaluate our models both automatically (in terms of ROUGE) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that\n1The corpora are available at http://.\nour summarizers achieve performance comparable to state-of-the-art systems employing handengineered features and sophisticated linguistic constraints."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "In this section we formally define the summarization tasks considered in this paper. Given a document D consisting of a sequence of sentences {s1, · · · ,sm} and a word set {w1, · · · ,wn}, we are interested in obtaining summaries at two levels of granularity, namely sentences and words.\nSentence extraction aims to create a summary from D by selecting a subset of j sentences (where j < m). We do this by scoring each sentence within D and predicting a label yL ∈ {0,1} indicating whether the sentence should be included in the summary. As we apply supervised training, the objective is to maximize the likelihood of all sentence labels yL = (y1L, · · · ,ymL ) given the input document D and model parameters θ:\nlog p(yL|D;θ) = m\n∑ i=1 log p(yiL|D;θ) (1)\nAlthough extractive methods yield naturally grammatical summaries and require relatively little linguistic analysis, the selected sentences make for long summaries containing much redundant information. For this reason, we also develop a model based on word extraction which seeks to find a subset of words 2 in D and their optimal ordering so as to form a summary ys = (w′1, · · · ,w′k),w′i ∈ D. Compared to sentence extraction which is a sequence labeling problem, this task occupies the middle ground between full abstractive summarization which can exhibit a wide range of rewrite operations and extractive summarization which exhibits none. We formulate word extraction as a language generation task with an output vocabulary restricted to the original document. In our supervised setting, the training goal is to maximize the likelihood of the generated sentences, which can be further decomposed by enforcing conditional dependencies among their constituent words:\nlog p(ys|D;θ)= k\n∑ i=1 log p(w′i|D,w′1,· · ·,w′i−1;θ) (2)\n2The vocabulary can also be extended to include a small set of commonly-used (high-frequency) words.\nIn the following section, we discuss the data elicitation methods which allow us to train neural networks based on the above defined objectives."
    }, {
      "heading" : "3 Training Data for Summarization",
      "text" : "Data-driven neural summarization models require a large training corpus of documents with labels indicating which sentences (or words) should be in the summary. Until now such corpora have been limited to hundreds of examples (e.g., the DUC 2002 single document summarization corpus) and thus used mostly for testing (Woodsend and Lapata, 2010). To overcome the paucity of annotated data for training, we adopt a methodology similar to Hermann et al. (2015) and create two large-scale datasets, one for sentence extraction and another one for word extraction.\nIn a nutshell, we retrieved3 hundreds of thousands of news articles and their corresponding highlights from DailyMail (see Figure 1 for an example). The highlights (created by news editors) are genuinely abstractive summaries and therefore not readily suited to supervised training. To create the training data for sentence extraction, we reverse approximated the gold standard label of each document sentence given the summary based on their semantic correspondence (Woodsend and Lapata, 2010). Specifically, we designed a rulebased system that determines whether a document sentence matches a highlight and should be labeled with 1 (must be in the summary), and 0 otherwise. The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, the number of entities appearing in the highlight and in the document sentence. We adjusted the weights of the rules on 9,000 documents with manual sentence labels created by Woodsend and Lapata (2010), and evaluated the preprocessing rules on a held-out set of 216 documents coming from the same dataset. Our method obtained an accuracy of 85% on the held-out data and was\n3The script for constructing our datasets is modified from the one released in Hermann et al. (2015).\nsubsequently used to label 200K documents (with approximately 30% of the sentences in each document being deemed summary-worthy).\nFor the creation of the word extraction dataset, we examine the lexical overlap between the highlights and the news article. In cases where all highlight words (after stemming) come from the original document, the document-highlight pair constitutes a valid training example and is added to the word extraction dataset. For out-ofvocabulary (OOV) words, we try to find a semantically equivalent replacement present in the news article. Specifically, we query their synonyms (more specifically, words with similar meanings) with pre-trained word embeddings4 and check if a neighbor is in the original document and therefore constitutes a valid substitution. If we cannot find any substitutes, we discard the documenthighlight pair. Following this procedure, we obtained a word extraction dataset containing 170K articles, again from the DailyMail."
    }, {
      "heading" : "4 Summarization Model",
      "text" : "The key components of our summarization model include a neural network-based document reader and an attention-based content extractor. We first describe the document reader, and then present the details of our sentence and word extractors."
    }, {
      "heading" : "4.1 Document Reader",
      "text" : "The role of the reader is to derive the meaning representation of the document from its constituent sentences, each of which is treated as a sequence of words. We first obtain representation vectors at the sentence level using a single-layer convolutional neural network (CNN) with a max-overtime pooling operation (Kalchbrenner and Blunsom, 2013; Zhang and Lapata, 2014; Kim et al., 2016). Next, we build representations for documents using a standard recurrent neural network (RNN) that recursively composes sentences. The CNN operates at the word level, leading to the\n4We used the Python Gensim library and the 300-dimensional GoogleNews vectors.\nacquisition of sentence-level representations that are then used as inputs to the RNN that acquires document-level representations, in a hierarchical fashion. We describe these two sub-components of the text reader below.\nConvolutional Sentence Encoder We opted for a convolutional neural network model for representing sentences for two reasons. Firstly, CNNs can be trained effectively (without any long-term dependencies in the model) and secondly, they have been successfully used for sentence-level classification tasks such as sentiment analysis (Kim, 2014). Let d denote the dimension of word embeddings, and s a document sentence consisting of a sequence of n words (w1, · · · ,wn) which can be represented by a dense column matrix W ∈ Rn×d . We apply a temporal narrow convolution between W and a kernel K ∈ Rc×d of width c as follows:\nfij = tanh(W j: j+c−1⊗K+b) (3)\nwhere ⊗ equates to the Hadamard Product followed by a sum over all elements. fij denotes the j-th element of the i-th feature map fi and b is the bias. We perform max pooling over time to obtain a single feature (the ith feature) representing the sentence under the kernel K with width c:\nsi,K = max j fij (4)\nIn practice, we use multiple feature maps to compute a list of features that match the dimensionality of a sentence under each kernel width. In addition, we apply multiple kernels with different widths to obtain a set of different sentence vectors. Finally, we sum these sentence vectors to obtain the final sentence representation. The CNN model is schematically illustrated in Figure 2 (bottom). In the example, the sentence embeddings have six dimensions, so six feature maps are used under each kernel width. The blue feature maps have width two and the red feature maps have width three. The sentence embeddings obtained under each kernel width are summed to get the final sentence representation (denoted by green).\nRecurrent Document Encoder At the document level, a recurrent neural network composes a sequence of sentence vectors into a document vector. This is a somewhat simplistic attempt at capturing document organization at the level of sentence to sentence transitions. The RNN\nhas an LSTM activation unit for ameliorating the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document d = (s1, · · · ,sm), the hidden state at time step t, denoted by ht, is updated as:\nit ft ot ĉt\n=  σ σ σ\ntanh\nW ·[ht−1st ]\n(5)\nct = ft ct−1 + it ĉt (6)\nht = ot tanh(ct) (7)\nwhere W is a learnable weight matrix. Next, we discuss a special attention mechanism for extracting sentences and words given the recurrent document encoder just described, starting from the sentence extractor."
    }, {
      "heading" : "4.2 Sentence Extractor",
      "text" : "In the standard neural sequence-to-sequence modeling paradigm (Bahdanau et al., 2015), an attention mechanism is used as an intermediate step to decide which input region to focus on in order to generate the next output. In contrast, our sentence extractor applies attention to directly extract salient sentences after reading them.\nThe extractor is another recurrent neural network that labels sentences sequentially, taking into account not only whether they are individually relevant but also mutually redundant. The complete architecture for the document encoder and the sentence extractor is shown in Figure 2. As\ncan be seen, the next labeling decision is made with both the encoded document and the previously labeled sentences in mind. Given encoder hidden states (h1, · · · ,hm) and extractor hidden states (h̄1, · · · , h̄m) at time step t, the decoder attends the t-th sentence by relating its current decoding state to the corresponding encoding state:\nh̄t = LSTM(pt−1st−1, h̄t−1) (8)\np(yL(t) = 1|D) = σ(Weh̄t +Wrht) (9)\nwhere We and Wr are model weights. pt−1 represents the degree to which the extractor believes the previous sentence should be extracted and memorized (pt−1=1 if the system is certain; 0 otherwise).\nIn practice, there is a discrepancy between training and testing such a model. During training we know the true label pt−1 of the previous sentence, whereas at test time pt−1 is unknown and has to be predicted by the model. The discrepancy can lead to quickly accumulating prediction errors, especially when mistakes are made early in the sequence labeling process. To mitigate this, we adopt a curriculum learning strategy (Bengio et al., 2015): at the beginning of training when pt−1 cannot be predicted accurately, we set it to the true label of the previous sentence; as training goes on, we gradually shift its value to the predicted label p(yL(t−1) = 1|d)."
    }, {
      "heading" : "4.3 Word Extractor",
      "text" : "Compared to sentence extraction which is a purely sequence labeling task, word extraction is closer to a generation task where relevant content must be selected and then rendered fluently and grammatically. A small modification to the structure of the sequential labeling model makes it suitable for generation: instead of predicting a label for the next sentence at each time step, the model directly outputs the next word in the summary. The\nmodel uses a hierarchical attention architecture: at time step t, the decoder softly5 attends each document sentence and subsequently attends each word in the document and computes the probability of the next word to be included in the summary p(w′t = wi|d,w′1, · · · ,w′t−1) with a softmax classifier:\nh̄t = LSTM(w′t−1, h̄t−1)6 (10)\natj = z T tanh(Weh̄t +Wrh j),h j ∈ D (11)\nbtj = softmax(a t j) (12)\nh̃t = m\n∑ j=1 btjh j (13)\nuti = v T tanh(We′ h̃t +Wr′wi),wi ∈ D (14) p(w′t = wi|D,w′1, · · · ,w′t−1) = softmax(uti) (15) In the above equations, wi corresponds to the vector of the i-th word in the input document, whereas z, We, Wr, v, We′ , and Wr′ are model weights. The model architecture is shown in Figure 3.\nThe word extractor can be viewed as a conditional language model with a vocabulary constraint. In practice, it is not powerful enough to enforce grammaticality due to the lexical diversity and sparsity of the document highlights. A possible enhancement would be to pair the extractor with a neural language model, which can be pretrained on a large amount of unlabeled documents and then jointly tuned with the extractor during decoding (Gulcehre et al., 2015). A simpler alternative which we adopt is to use n-gram features collected from the document to rerank candidate summaries obtained via beam decoding. We incorporate the features in a log-linear reranker whose feature weights are optimized with minimum error rate training (Och, 2003)."
    }, {
      "heading" : "4.4 Pre-training the Convolutional Sentence Encoder",
      "text" : "In the above we presented an end-to-end document reader and sentence extractor. In a direct supervised training with labeled examples, the parameters of the system, including all word and sentence\n5A simpler model would use hard attention to select a sentence first and then a few words from it as a summary, but this would render the system non-differentiable for training. Although hard attention can be trained with the REINFORCE algorithm (Williams, 1992), it requires the sampling of discrete actions and could lead to very high variance.\n6We empirically found that feeding the previous sentencelevel attention vector as an additional input to the LSTM could lead to a small improvement of the system. This is not shown in the equation.\nembeddings, may converge to a local optimum that minimizes the classification error but generalizes poorly. To improve the model, we propose to pre-train the convolutional sentence encoder to capture sentence semantics. Specifically, we use each sentence embedding derived from the CNN to predict every word in the sentence. This corresponds to an extended Paragraph Vector model (Le and Mikolov, 2014) with a convolutional encoder."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "In this section we present our experimental setup for assessing the performance of our summarization models. We discuss the datasets used for training and evaluation, give implementation details, briefly introduce comparison models, and explain how system output was evaluated.\nDatasets We trained our sentence- and wordbased summarization models on the two datasets created from DailyMail news. Each dataset was split into approximately 90% for training, 5% for validation, and 5% for testing. We evaluated the models on the DUC-2002 single document summarization task. In total, there are 567 documents belonging to 59 different clusters of various news topics. Each document is associated with two versions of 100-word manual summaries produced by human annotators. We also evaluated our models on 500 articles from the DailyMail test set (with the human authored highlights as goldstandard). We sampled article-highlight pairs so that the highlights include a minimum of 3 sentences.\nImplementation Details We trained our models with Adam (Kingma and Ba, 2014) with initial learning rate 0.001. The two momentum parameters were set to 0.99 and 0.999 respectively. We performed mini-batch training with a batch size of 20 documents. All input documents were padded to the same length with an additional mask variable storing the real length for each document. The size of word, sentence, and document embeddings were set to 150, 300, and 750, respectively. For the convolutional sentence model, we followed Kim et al. (2016) and used a list of kernel sizes {1, 2, 3, 4, 5, 6, 7}. For the recurrent document model and the sentence extractor, we used as regularization dropout with probability 0.5 on the LSTM input-to-hidden layers and the scoring layer. The depth of each LSTM module was 1. All LSTM parameters were randomly initialized over a uniform distribution within [-0.05, 0.05].\nThe word vectors were initialized with 150 dimensional pre-trained embeddings.7\nProper nouns pose a problem for embeddingbased approaches, especially when these are rare or unknown (e.g., at test time). Rush et al. (2015) address this issue by adding a new set of features and a log-linear model component to their system. We instead force the model to inspect the context surrounding an entity and its relative position in the sentence in order to discover extractive patterns, placing less emphasis on the meaning of the entity itself. Specifically, we perform named entity recognition with the package provided by Hermann et al. (2015) and maintain a set of randomly initialized entity embeddings. During training, the index of the entities is permuted to introduce some noise but also robustness in the data. A similar data augmentation approach has been used for reading comprehension (Hermann et al., 2015).\nA common problem with extractive methods based on sentence labeling is that there is no constraint on the number of sentences being selected at test time. We address this by reranking the positively labeled sentences with the probability scores obtained from the softmax layer (rather than the label itself). We use the three sentences with the highest scores as the summary. Another issue relates to the word extraction model which is challenging to batch since each document possesses a distinct vocabulary. We sidestep this during training by performing negative sampling (Mikolov et al., 2013) which trims the vocabulary of different documents to the same length. At each decoding step the model is trained to differentiate the true target word from 20 noise samples. Note that at test time we still loop through the words in the input document to decide which word to output next.\nSystem Comparisons We compared the output of our models to various summarization methods. These included the standard baseline of simply selecting the “leading” three sentences from each document as the summary. We also built a sentence extraction baseline classifier using logistic regression and human engineered features. The classifier was trained on the same datasets as our neural network models with the following features: sentence length, sentence position, number of entities in the sentence, sentence-to-\n7We used the word2vec (Mikolov et al., 2013) skip-gram model with context window size 6, negative sampling size 10 and hierarchical softmax 1. The model was trained on the Google 1-billion word benchmark (Chelba et al., 2014).\nsentence cohesion, and sentence-to-document relevance. Sentence-to-sentence cohesion was computed by calculating for every document sentence its embedding similarity with every other sentence in the same document. The feature was the normalized sum of these similarity scores. Sentence embeddings were obtained by averaging the constituent word embeddings. Sentence-to-document relevance was computed similarly. We calculated for each sentence its embedding similarity with the document (represented as bag-of-words), and normalized the score. The word embeddings used in this baseline are the same as the pre-trained ones used for our neural models.\nIn addition, we included a neural abstractive summarization baseline. This system has a similar architecture to our word extraction model except that it uses an open vocabulary during decoding. It can also be viewed as a hierarchical documentlevel extension of the abstractive sentence summarizer proposed by Rush et al. (2015). We trained this model with negative sampling to avoid the excessive computation of the normalization constant.\nFinally, we compared our models to three previously published systems which have shown competitive performance on the DUC2002 single document summarization task. The first approach is the phrase-based extraction model of Woodsend and Lapata (2010). Their system learns to produce highlights from parsed input (phrase structure trees and dependency graphs); it selects salient phrases and recombines them subject to length, coverage, and grammar constraints enforced via integer linear programming (ILP). Like ours, this model is trained on document-highlight pairs, and produces telegraphic-style bullet points rather than full-blown summaries. The other two systems, TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010), produce more typical summaries and represent the state of the art. TGRAPH is a graph-based sentence extraction model, where the graph is constructed from topic models and the optimization is performed by constrained ILP. URANK adopts a unified ranking system for both single- and multidocument summarization.\nEvaluation We evaluated the quality of the summaries automatically using ROUGE (Lin and Hovy, 2003). We report unigram and bigram overlap (ROUGE-1,2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency.\nIn addition, we evaluated the generated summaries by eliciting human judgments for 20 randomly sampled DUC 2002 test documents. Participants were presented with a news article and summaries generated by a list of systems. These include two neural network systems (sentenceand word-based extraction), the neural abstractive system described earlier, the lead baseline, the phrase-based ILP model8 of Woodsend and Lapata (2010), and the human authored summary. Subjects were asked to rank the summaries from best to worst (with ties allowed) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?). We elicited human judgments using Amazon’s Mechanical Turk crowdsourcing platform. Participants (self-reported native English speakers) saw 2 random articles per session. We collected 5 responses per document."
    }, {
      "heading" : "6 Results",
      "text" : "Table 1 (top half) summarizes our results on the DUC 2002 test dataset using ROUGE. NN-SE represents our neural sentence extraction model, NN-WE our word extraction model, and NN-ABS the neural abstractive baseline. The table also includes results for the LEAD baseline, the logistic regression classifier (LREG), and three previously published systems (ILP, TGRAPH, and URANK).\nThe NN-SE outperforms the LEAD and LREG baselines with a significant margin, while per-\n8We are grateful to Kristian Woodsend for giving us access to the output of his system. Unfortunately, we do not have access to the output of TGRAPH or URANK for inclusion in the human evaluation.\nforming slightly better than the ILP model. This is an encouraging result since our model has only access to embedding features obtained from raw text. In comparison, LREG uses a set of manually selected features, while the ILP system takes advantage of syntactic information and extracts summaries subject to well-engineered linguistic constraints, which are not available to our models. Overall, our sentence extraction model achieves performance comparable to the state of the art without sophisticated constraint optimization (ILP, TGRAPH) or sentence ranking mechanisms (URANK). We visualize the sentence weights of the NN-SE model in the top half of Figure 4. As can be seen, the model is able to locate text portions which contribute most to the overall meaning of the document.\nROUGE scores for the word extraction model are less promising. This is somewhat expected given that ROUGE is n-gram based and not very well suited to measuring summaries which contain a significant amount of paraphrasing and may deviate from the reference even though they express similar meaning. However, a meaningful comparison can be carried out between NN-WE and\nNN-ABS which are similar in spirit. We observe that NN-WE consistently outperforms the purely abstractive model. As NN-WE generates summaries by picking words from the original document, decoding is easier for this model compared to NN-ABS which deals with an open vocabulary. The extraction-based generation approach is more robust for proper nouns and rare words, which pose a serious problem to open vocabulary models. An example of the generated summaries for NN-WE is shown at the lower half of Figure 4.\nTable 1 (lower half) also shows system results on the 500 DailyMail news articles (test set). In general, we observe similar trends to DUC 2002, with NN-SE performing the best in terms of all ROUGE metrics. Note that scores here are generally lower compared to DUC 2002. This is due to the fact that the gold standard summaries (aka highlights) tend to be more laconic and as a result involve a substantial amount of paraphrasing.\nThe results of our human evaluation study are shown in Table 2. Specifically, we show, proportionally, how often our participants ranked each system 1st, 2nd and so on. Perhaps unsurprisingly, the human-written descriptions were considered best and ranked 1st 27% of the time, however closely followed by our NN-SE model which was ranked 1st 22% of the time. The ILP system was mostly ranked in 2nd place (38% of the time). The rest of the systems occupied lower ranks. We further converted the ranks to ratings on a scale of 1 to 6 (assigning ratings 6. . .1 to rank placements 1. . .6). This allowed us to perform Analysis of Variance (ANOVA) which revealed a reliable effect of system type. Specifically, post-hoc Tukey\ntests showed that NN-SE and ILP are significantly (p < 0.01) better than LEAD, NN-WE, and NN-ABS but do not differ significantly from each other or the human goldstandard."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this work we presented a data-driven summarization framework based on an encoder-extractor architecture. We developed two classes of models based on sentence and word extraction. We showed that our models can be trained on large scale datasets and learn informativeness features based on continuous representations without recourse to linguistic annotations. Directions for future work are many and varied. One way to improve the word-based model, would be to combine a chart based decoding algorithm (Cohn and Lapata, 2009) with the neural word extractor. It would also be interesting to apply the neural models presented here in a phrase-based setting similar to Lebret et al. (2015)."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Proceedings of ICLR",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Headline generation based on statistical translation",
      "author" : [ "Banko et al.2000] Michele Banko", "Vibhu O. Mittal", "Michael J. Witbrock" ],
      "venue" : "In Proceedings of the 38th ACL,",
      "citeRegEx" : "Banko et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2000
    }, {
      "title" : "Scheduled sampling for sequence prediction with recurrent neural networks",
      "author" : [ "Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bengio et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "One billion word benchmark for measuring progress in statistical language modeling",
      "author" : [ "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson" ],
      "venue" : "arXiv preprint arXiv:1312.3005",
      "citeRegEx" : "Chelba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chelba et al\\.",
      "year" : 2014
    }, {
      "title" : "Sentence compression as tree transduction",
      "author" : [ "Cohn", "Lapata2009] Trevor Anthony Cohn", "Mirella Lapata" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Cohn et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cohn et al\\.",
      "year" : 2009
    }, {
      "title" : "Text summarization via hidden Markov models",
      "author" : [ "Conroy", "O’Leary2001] Conroy", "O’Leary" ],
      "venue" : "In Proceedings of the 34th Annual ACL SIGIR,",
      "citeRegEx" : "Conroy et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Conroy et al\\.",
      "year" : 2001
    }, {
      "title" : "Lexpagerank: Prestige in multidocument text summarization",
      "author" : [ "Erkan", "Radev2004] Güneş Erkan", "Dragomir R. Radev" ],
      "venue" : "In Proceedings of the 2004 EMNLP,",
      "citeRegEx" : "Erkan et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Erkan et al\\.",
      "year" : 2004
    }, {
      "title" : "Event-based extractive summarization",
      "author" : [ "Filatova", "Vasileios Hatzivassiloglou" ],
      "venue" : "Text Summarization Branches Out: Proceedings of the ACL-04",
      "citeRegEx" : "Filatova et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Filatova et al\\.",
      "year" : 2004
    }, {
      "title" : "On using monolingual corpora in neural machine translation",
      "author" : [ "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "HueiChi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Gulcehre et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2015
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Recurrent convolutional neural networks for discourse compositionality",
      "author" : [ "Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom" ],
      "venue" : "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2013
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush" ],
      "venue" : "In Proceedings of the 30th AAAI,",
      "citeRegEx" : "Kim et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In Proceedings of the 2014 EMNLP,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980",
      "author" : [ "Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Summarization based on embedding distributions",
      "author" : [ "Masaki Noguchi", "Taichi Yatsuka" ],
      "venue" : "In Proceedings of the 2015 EMNLP,",
      "citeRegEx" : "Kobayashi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2015
    }, {
      "title" : "A trainable document summarizer",
      "author" : [ "Kupiec et al.1995] Julian Kupiec", "Jan O. Pedersen", "Francine Chen" ],
      "venue" : "In Proceedings of the 18th Annual International ACM SIGIR,",
      "citeRegEx" : "Kupiec et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Kupiec et al\\.",
      "year" : 1995
    }, {
      "title" : "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053",
      "author" : [ "Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2014
    }, {
      "title" : "Phrase-based image captioning",
      "author" : [ "Lebret et al.2015] Rémi Lebret", "Pedro O Pinheiro", "Ronan Collobert" ],
      "venue" : "In Proceedings of the 32nd ICML,",
      "citeRegEx" : "Lebret et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lebret et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic evaluation of summaries using n-gram co-occurrence statistics",
      "author" : [ "Lin", "Hovy2003] Chin-Yew Lin", "Eduard H. Hovy" ],
      "venue" : "In Proceedings of HLT NAACL,",
      "citeRegEx" : "Lin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2003
    }, {
      "title" : "Language independent extractive summarization",
      "author" : [ "Rada Mihalcea" ],
      "venue" : "In Proceedings of the ACL Interactive Poster and Demonstration Sessions,",
      "citeRegEx" : "Mihalcea.,? \\Q2005\\E",
      "shortCiteRegEx" : "Mihalcea.",
      "year" : 2005
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization",
      "author" : [ "Nenkova et al.2006] Ani Nenkova", "Lucy Vanderwende", "Kathleen McKeown" ],
      "venue" : "In Proceedings of the 29th Annual",
      "citeRegEx" : "Nenkova et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nenkova et al\\.",
      "year" : 2006
    }, {
      "title" : "Minimum error rate training in statistical machine translation",
      "author" : [ "Franz Josef Och" ],
      "venue" : "In Proceedings of the 41st ACL,",
      "citeRegEx" : "Och.,? \\Q2003\\E",
      "shortCiteRegEx" : "Och.",
      "year" : 2003
    }, {
      "title" : "Topical coherence for graph-based extractive summarization",
      "author" : [ "Hans-Martin Ramsl", "Michael Strube" ],
      "venue" : "In Proceedings of the 2015 EMNLP,",
      "citeRegEx" : "Parveen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Parveen et al\\.",
      "year" : 2015
    }, {
      "title" : "Mead-a platform for multidocument multilingual text summarization",
      "author" : [ "Radev et al.2004] Dragomir Radev", "Timothy Allison", "Sasha Blair-Goldensohn", "John Blitzer", "Arda Celebi", "Stanko Dimitrov", "Elliott Drabek", "Ali Hakim", "Wai Lam", "Danyu Liu" ],
      "venue" : null,
      "citeRegEx" : "Radev et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Radev et al\\.",
      "year" : 2004
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Proceedings of the 2015 EMNLP,",
      "citeRegEx" : "Rush et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Oriol Vinyals", "Quoc VV Le" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Enhancing singledocument summarization by combining RankNet and third-party sources",
      "author" : [ "Svore et al.2007] Krysta Svore", "Lucy Vanderwende", "Christopher Burges" ],
      "venue" : "In Proceedings of the 2007 EMNLP-CoNLL,",
      "citeRegEx" : "Svore et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Svore et al\\.",
      "year" : 2007
    }, {
      "title" : "Towards a unified approach to simultaneous single-document and multidocument summarizations",
      "author" : [ "Xiaojun Wan" ],
      "venue" : "In Proceedings of the 23rd COLING,",
      "citeRegEx" : "Wan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wan.",
      "year" : 2010
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Automatic generation of story highlights",
      "author" : [ "Woodsend", "Lapata2010] Kristian Woodsend", "Mirella Lapata" ],
      "venue" : "In Proceedings of the 48th ACL,",
      "citeRegEx" : "Woodsend et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Woodsend et al\\.",
      "year" : 2010
    }, {
      "title" : "Extractive summarization by maximizing semantic volume",
      "author" : [ "Fei Liu", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 2015 EMNLP,",
      "citeRegEx" : "Yogatama et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yogatama et al\\.",
      "year" : 2015
    }, {
      "title" : "Chinese poetry generation with recurrent neural networks",
      "author" : [ "Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata" ],
      "venue" : "In Proceedings of 2014 EMNLP,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "These include surface features such as sentence position and length (Radev et al., 2004), the words in the title, the presence of proper nouns, content features such as word frequency (Nenkova et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 22,
      "context" : ", 2004), the words in the title, the presence of proper nouns, content features such as word frequency (Nenkova et al., 2006), and event features such as action nouns (Filatova and Hatzivassiloglou, 2004).",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "Several methods have been used in order to select the summary sentences ranging from binary classifiers (Kupiec et al., 1995), to hidden Markov models (Conroy and O’Leary, 2001), graph-based algorithms (Erkan and Radev, 2004; Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : ", 1995), to hidden Markov models (Conroy and O’Leary, 2001), graph-based algorithms (Erkan and Radev, 2004; Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010).",
      "startOffset" : 84,
      "endOffset" : 123
    }, {
      "referenceID" : 27,
      "context" : "There has been a surge of interest recently in repurposing sequence transduction neural network architectures for NLP tasks such as machine translation (Sutskever et al., 2014), question answering (Hermann et al.",
      "startOffset" : 152,
      "endOffset" : 176
    }, {
      "referenceID" : 9,
      "context" : ", 2014), question answering (Hermann et al., 2015), and sentence compression (Rush et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : ", 2015), and sentence compression (Rush et al., 2015).",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 28,
      "context" : "Inspired by previous work on summarization (Woodsend and Lapata, 2010; Svore et al., 2007) and reading comprehension (Hermann et al.",
      "startOffset" : 43,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : ", 2007) and reading comprehension (Hermann et al., 2015) we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm.",
      "startOffset" : 21,
      "endOffset" : 68
    }, {
      "referenceID" : 32,
      "context" : "A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm.",
      "startOffset" : 21,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "The idea of creating a summary by extracting words from the source document was pioneered in Banko et al. (2000) who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "The idea of creating a summary by extracting words from the source document was pioneered in Banko et al. (2000) who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words. Our word-based model is similar in spirit, however, it operates over continuous representations, produces multi-sentence output, and jointly selects summary words and organizes them into sentences. A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm. Our work also uses continuous representations to express the meaning of sentences and documents, but importantly employs neural networks more directly to perform the actual summarization task. Rush et al. (2015) propose a neural attention model for abstractive sentence compression which is trained on pairs of headlines and first sentences in an article.",
      "startOffset" : 93,
      "endOffset" : 885
    }, {
      "referenceID" : 9,
      "context" : "To overcome the paucity of annotated data for training, we adopt a methodology similar to Hermann et al. (2015) and create two large-scale datasets, one for sentence extraction and another one for word extraction.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "To overcome the paucity of annotated data for training, we adopt a methodology similar to Hermann et al. (2015) and create two large-scale datasets, one for sentence extraction and another one for word extraction. In a nutshell, we retrieved3 hundreds of thousands of news articles and their corresponding highlights from DailyMail (see Figure 1 for an example). The highlights (created by news editors) are genuinely abstractive summaries and therefore not readily suited to supervised training. To create the training data for sentence extraction, we reverse approximated the gold standard label of each document sentence given the summary based on their semantic correspondence (Woodsend and Lapata, 2010). Specifically, we designed a rulebased system that determines whether a document sentence matches a highlight and should be labeled with 1 (must be in the summary), and 0 otherwise. The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, the number of entities appearing in the highlight and in the document sentence. We adjusted the weights of the rules on 9,000 documents with manual sentence labels created by Woodsend and Lapata (2010), and evaluated the preprocessing rules on a held-out set of 216 documents coming from the same dataset.",
      "startOffset" : 90,
      "endOffset" : 1241
    }, {
      "referenceID" : 9,
      "context" : "3The script for constructing our datasets is modified from the one released in Hermann et al. (2015). subsequently used to label 200K documents (with approximately 30% of the sentences in each document being deemed summary-worthy).",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "We first obtain representation vectors at the sentence level using a single-layer convolutional neural network (CNN) with a max-overtime pooling operation (Kalchbrenner and Blunsom, 2013; Zhang and Lapata, 2014; Kim et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 229
    }, {
      "referenceID" : 13,
      "context" : "Firstly, CNNs can be trained effectively (without any long-term dependencies in the model) and secondly, they have been successfully used for sentence-level classification tasks such as sentiment analysis (Kim, 2014).",
      "startOffset" : 205,
      "endOffset" : 216
    }, {
      "referenceID" : 0,
      "context" : "In the standard neural sequence-to-sequence modeling paradigm (Bahdanau et al., 2015), an attention mechanism is used as an intermediate step to decide which input region to focus on in order to generate the next output.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "To mitigate this, we adopt a curriculum learning strategy (Bengio et al., 2015): at the beginning of training when pt−1 cannot be predicted accurately, we set it to the true label of the previous sentence; as training goes on, we gradually shift its value to the predicted label p(yL(t−1) = 1|d).",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "A possible enhancement would be to pair the extractor with a neural language model, which can be pretrained on a large amount of unlabeled documents and then jointly tuned with the extractor during decoding (Gulcehre et al., 2015).",
      "startOffset" : 207,
      "endOffset" : 230
    }, {
      "referenceID" : 23,
      "context" : "We incorporate the features in a log-linear reranker whose feature weights are optimized with minimum error rate training (Och, 2003).",
      "startOffset" : 122,
      "endOffset" : 133
    }, {
      "referenceID" : 30,
      "context" : "Although hard attention can be trained with the REINFORCE algorithm (Williams, 1992), it requires the sampling of discrete actions and could lead to very high variance.",
      "startOffset" : 68,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "For the convolutional sentence model, we followed Kim et al. (2016) and used a list of kernel sizes {1, 2, 3, 4, 5, 6, 7}.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "A similar data augmentation approach has been used for reading comprehension (Hermann et al., 2015).",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "Rush et al. (2015) address this issue by adding a new set of features and a log-linear model component to their system.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "Specifically, we perform named entity recognition with the package provided by Hermann et al. (2015) and maintain a set of randomly initialized entity embeddings.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : "We sidestep this during training by performing negative sampling (Mikolov et al., 2013) which trims the vocabulary of different documents to the same length.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "7We used the word2vec (Mikolov et al., 2013) skip-gram model with context window size 6, negative sampling size 10 and hierarchical softmax 1.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "The model was trained on the Google 1-billion word benchmark (Chelba et al., 2014).",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : "It can also be viewed as a hierarchical documentlevel extension of the abstractive sentence summarizer proposed by Rush et al. (2015). We trained this model with negative sampling to avoid the excessive computation of the normalization constant.",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 24,
      "context" : "The other two systems, TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010), produce more typical summaries and represent the state of the art.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : ", 2015) and URANK (Wan, 2010), produce more typical summaries and represent the state of the art.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "It would also be interesting to apply the neural models presented here in a phrase-based setting similar to Lebret et al. (2015).",
      "startOffset" : 108,
      "endOffset" : 129
    } ],
    "year" : 2016,
    "abstractText" : "Traditional approaches to extractive summarization rely heavily on humanengineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.",
    "creator" : "LaTeX with hyperref package"
  }
}