{
  "name" : "1612.00694.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ESE: Efficient Speech Recognition Engine with Compressed LSTM on FPGA",
    "authors" : [ "Song Han", "Junlong Kang", "Huizi Mao", "Yiming Hu", "Xin Li", "Yubin Li", "Dongliang Xie", "Hong Luo", "Song Yao", "Yu Wang", "Huazhong Yang", "William J. Dally", "DeePhi Tech" ],
    "emails" : [ "songhan@stanford.edu,", "dally@stanford.edu,", "song.yao@deephi.tech,", "yu-wang@mail.tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep neural network has surpassed the traditional acoustic model and become the state-of-the-art method for speech recognition [1, 2]. Long Short-Term Memory (LSTM) [3], Gated Recurrent Unit (GRU) [4] and vanilla recurrent neural networks (RNNs) are popular in speech recognition. In this work, we designed a hardware accelerator called ESE for the most complex one: the LSTM.\nESE takes the approach of EIE [5] one step further to address a more general problem of accelerating not only feed forward neural networks but also recurrent neural networks and LSTM. The recurrent nature of RNN produces complicated data dependency, which is more challenging than feed forward neural nets. To deal with this problem, we designed a data flow that can effectively schedule the complex RNN operations using multiple EIE cores.\nAmong all factors contribute to the monthly bill of a data center, power consumption is the major one. Since memory reference consumes more than two orders of magnitude higher energy than ALU operations, we focus on reducing the memory footprint.\nIn order to achieve this, we design a novel method to optimize across the algorithm, software and hardware. At algorithm level, ESE revisited pruning algorithm from the hardware efficiency\n1st International Workshop on Efficient Methods for Deep Neural Networks at NIPS 2016, Barcelona, Spain. Full paper to appear at FPGA 2017.\nar X\niv :1\n61 2.\n00 69\n4v 1\n[ cs\n.C L\n] 1\nD ec\n2 01\nDeep Model Compression\n35x-49x smaller same accuracy\nBlocking Encoding\nrelative-indexed CSC format with codebook\nCustomized Accelerator\n13x speedup, 3400x lower energy than GPU\nAlgorithm Software Hardware\nAcceleration Load Balancing\nCompression Hardware\nCompression Pruning /\nWeight Sharing\nLoad Balance-Aware Pruning\nAcceleration Sparsity, Load\nBalancing\nCompression Hardware\nCompression Pruning /\nWeight Sharing\nperspective, by introducing load-balance-aware pruning. Next we design a scheduler that can effectively schedule the compressed LSTM model using spMV as basic building block, with memory reference fully overlapp d with computation. At hardware level, we design a new architecture that can works directly on the compressed model that could be efficiently mapp d to FPGA. ESE achieves high efficiency by load balancing and partition ng both the computation and storage.\n2 Model Compression by Load-Balance-Aware Pruning ~a\n0 a1 0 a3\n⇥ ~b PE0\nPE1\nPE2\nPE3 0 BBBBBBBBBBBBB@ w0,0w0,1 0 w0,3 0 0 w1,2 0 0 w2,1 0 w2,3 0 0 0 0 0 0 w4,2w4,3 w5,0 0 0 0 0 0 0 w6,3\n0 w7,1 0 0\n1 CCCCCCCCCCCCCA = 0 BBBBBBBBBBBBB@ b0 b1 b2 b3 b4 b5 b6\nb7\n1 CCCCCCCCCCCCCA ReLU) 0 BBBBBBBBBBBBB@ b0 b1 0 b3 0 b5 b6\n0\n1 CCCCCCCCCCCCCA\nUnbalanced\nw0,0 w0,1 0 w0,3 0 0 w1,2 0 0 w2,1 0 w2,3 0 0 0 0 0 0 w4,2 w4,3 w5,0 0 0 0 w6,0 0 0 w6,3 0 w7,1 0 0\n~a 0 a1 0 a3\n⇥ ~b PE0\nPE1\nPE2\nPE3 0 BBBBBBBBBBBBB@ w0,0w0,1 0 w0,3 0 0 w1,2 0 0 w2,1 0 w2,3 0 0 0 0 0 0 w4,2w4,3 w5,0 0 0 0 0 0 0 w6,3\n0 w7,1 0 0\n1 CCCCCCCCCCCCCA = 0 BBBBBBBBBBBBB@ b0 b1 b2 b3 b4 b b\nb7\n1 CC CCCCCCCCCCA ReLU) 0 BBBBBBBBBBBBB@ b0 b1 0 b3 0 b5 b6\n0\n1 CCCCCCCCCCCCCA\n1\n5 cycles 2 cycles 4 cycles 1 cycle\nOverall: 5 cycles\nBalanced\nOverall: 3 cycles\n3 cycles 3 cycles 3 cycles 3 cycles\n~a 0 a1 0 a3\n⇥ ~b PE0\nPE1\nPE2\nPE3 0 BBBBBBBBBBBBB@ w0,0w0,1 0 w0,3 0 0 w1,2 0 0 w2,1 0 w2,3 0 0 0 0 0 0 w4,2w4,3 w5,0 0 0 0 0 0 0 w6,3\n0 w7,1 0 0\n1 CCCCCCCCCCCCCA = 0 BBBBBBBBBBBBB@ b0 b1 b2 b3 b4 b5 b6\nb7\n1 CCCCCCCCCCCCCA ReLU) 0 BBBBBBBBBBBBB@ b0 b1 0 b3 0 b5 b6\n0\n1 CCCCCCCCCCCCCA\n1\n~a 0 a1 0 a3\n⇥ ~b PE0\nPE1\nPE2\nPE3 0 BBBBBBBBBBBBB@ w0,0w0,1 0 w0,3 0 0 w1,2 0 0 w2,1 0 w2,3 0 0 0 0 0 0 w4,2w4,3 w5,0 0 0 0 0 0 0 w6,3\n0 w7,1 0 0\n1 CCCCCCCCCCCCCA = 0 BBBBBBBBBBBBB@ b0 b1 b2 b3 b4 b5 b6\nb7\n1 CCCCCCCCCCCCCA ReLU) 0 BBBBBBBBBBBBB@ b0 b1 0 b3 0 b5 b6\n0\n1 CCCCCCCCCCCCCA\n1\nw0,0 0 0 w0,3 0 0 w1,2 0 0 w2,1 0 w2,3 0 0 w3,2 0 0 0 w4,2 w ,0 0 0 w5,3 w ,0 0 0 0 0 w7,1 0 w7,3\nFigure 2: Load Balance Aware Pruning and its Benefit for Parallel Processing\nPrevious pruning methods removed the redundant connections based on the absolute value of the weights [6, 7], which lead to a potential problem of unbalanced non-zero weights distribution. In our hardware implementation, matrix is divided into different sub-matrices and assigned to the corresponding processing elements (PEs), so that the multiplication could be executed in parallel. However, only non-zero weights are stored and computed, thus PEs with less non-zero weights have to wait for those with more non-zero weights. The workload imbalance over PEs would result in a gap between the real performance and peak performance.\nTo solve this problem, we propose the load-balance-aware pruning, which produces the same compression rate among all the sub-matrices. In this way, the workload of each PE is roughly the same, and no waiting is needed any more. As shown in Fig. 2, the matrix is divided into four colors, and each color belongs to a PE for parallel processing. With conventional pruning, PE0 might have five non-zero weights while PE3 may have only three. The total processing time is the longest, which is 5 cycles. With load-balance-aware pruning, all PEs have three non-zero weights, thus only 3 cycles are needed to carry out the operation. Both cases have the same non-zero weights in total, but load-balance-aware pruning need fewer cycles. The difference of prediction accuracy with and without load-balance-aware pruning is very small, as shown in Fig. 3. There are some noise around 70%, and we put lots of experiments around 90%, which is the sweet point, and find the performance is very similar. We highlight this practical pruning strategy for hardware efficiency.\nWe evaluate a LSTM on the TIMIT dataset. As shown in Fig. 3, the sweet point sparsity is around 90%. On the sparsity point of 92.6%, the load-balance model achieves a Phone Error Rate (PER) of\n2\nSTATE STATE_1\nOutput\nInput Xt\nWixXt\nW\nWfxXt WcxXt WiCCt-1\nCt-1\nSTATE_2\nWirYt-1 WfrYt-1 WcrYt-1 WCfCt-1 It Ft\nWYt-1Wc Ct-1 Wc B\nSTATE_3\nWoxXt Gt\nWXt B\nSTATE_4\nWorYt-1 Ct WocCt Ht\nYt-1 W Wc\nSTATE_5\nOt Mt\nB\nSTATE_6\nW\nYt\n20.7%, which is only 0.4% higher than the original model. We further experiment on a proprietary dataset which is much larger: it has 1000 hours of training speech data, 100 hours of validation speech data, and 10 hours of test speech data, we find that we can prune away 92% of the parameters without hurting Word Error Rate (WER), which aligns with our result on TIMIT dataset. In our later discussions we will use a conservative density of 10%(90% sparse).\nWe compress the model by quantizing 32-bit floating point weights into 12-bit integer. Plus the 4bits for sparse index, each weight is still aligned at 16bit. Activations are quantized to 16-bit integer. We use linear quantization strategy on both the weights and activations."
    }, {
      "heading" : "3 Computation Scheduling",
      "text" : "Compressed LSTM model is highly irregular, and thus accelerators on dense LSTMs cannot effectively take advantage of sparsity [8, 9]. LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.\nWe propose an ESE scheduler, shown in Fig.4, in which computation and data-fetching are fully overlapped. Operations in the first three lines fetch the pointers, weights and biases from memory to prepare for computation. Operations in the fourth line are matrix-vector multiplications. And operations in the fifth line are element-wise multiplications (indigo blocks) or accumulations (orange blocks). Operations in the horizontal direction have to be executed sequentially, while those in the vertical direction are able be executed concurrently. For example, we can calculate Wfryt−1 and it concurrently, because the two operations are not dependent on each other in the LSTM network, and they can be executed by two independent computation units in hardware system. Wiryt−1/Wicct−1 and it have to be executed sequentially, because it is dependent on the former operations in LSTM network.\nCPU External Memory DATA BUS PCIE Controller MEM Controller ES E Co nt ro lle r Input Buffer Output Buffer\nPE\nChannel 1 PE PE\nPE\nChannel 0 PE PE\nPE\nChannel N PE PE ESE Accelerator FPGA SpMVExternal Memory Software Program ActQueue\nSigmoid /Tanh Act Buffer Buf Buf Weight Buffer Buf Buf SpmatRead Pointer Buffer Buf Buf PtrRead Adder Tree ElemMul ElemMul Ct Ht Buffer Yt\nProcessing Element (PE)\nFIFO FIFO\nAccu\nMt MEM\nWXt/Yt-1"
    }, {
      "heading" : "4 Hardware Architecture",
      "text" : "Fig.5(a) shows the overview architecture of ESE system. It is a CPU+FPGA heterogeneous architecture to accelerate LSTM networks. Fig.5(b) shows the architecture of one channel with multiple PEs. It is composed of several major components:\nActivation Vector Queue (ActQueue). ActQueue consists of several FIFOs. Each FIFO stores elements of the input voice vector aj for each PE. ActQueue is shared by all the PEs in one channel, while each FIFO is owned by each PE independently. ActQueue’s fuction is to decouple the load imbalance across different PEs.\nSparse Matrix Read (SpmatRead). Pointer Read Unit (PtrRead) and Sparse Matrix Read (SpmatRead) manage the encoded weight matrix storage and output. The start and end pointers pj and pj+1 for column j determine the start location and length of elements in one encoded weight column that should be fetched for each element of a voice vector. SpmatRead uses pointers pj and pj+1 to look up the non-zero elements in weight column j.\nSparse Matrix-vector Multiplication (SpMV). SpMV unit multiplies the activation by a column on weight, and the current partial result is written into partial result ActBuffer. Accumulator sums the new output of SpMV and previous data stored in Act Buffer. Multiplier instantiated in the design can perform 16bit×12bit functions. Element-wise Multiplication (ElemMul). ElemMul in Fig.5(b) generates one vector by consuming two vectors. Each element in the output vector is the element-wise multiplication of two input vectors. There are 16 multipliers instantiated for element-wise multiplications in each channel.\nAdder Tree. Adder Tree performs summation by consuming the intermediate data produced by other units or bias data from input buffer.\nSigmoid/Tanh. These non-linear modules are implemented with quantized look-up table.\nConstrained by FPGA resource, ESE cannot buffer all the weights on-chip, thus hiding latency is important. ESE adopts double buffering to overlap the time of data transfer and computation."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "We implemented ESE on Xilinx XCKU060 FPGA. ESE is clocked at 200MHz, and there is a large room for improvement. We evaluate the speedup and energy efficiency of ESE and compared with CPU and GPU. Our baseline LSTM runs on Intel Core i7-5930K CPU and Pascal Titan X GPU. We use MKL BLAS/cuBLAS for dense LSTM, and MKL SPARSE/cuSPARSE for sparse LSTM.\nThe experimental results of LSTM on ESE, CPU, and GPU are shown in Table 1. The model is pruned to 10% non-zeros. There are fewer than 12.2% non-zeros even taking padding zeros into account. On ESE, the total throughput is 282 GOPS with the sparse LSTM, which corresponds to 2.52 TOPS on the dense LSTM. On CPU and GPU, we combine some matrices together to improve the performance. Processing the LSTM with 1024 hidden elements, ESE takes 82.7 us, CPU takes\nParameters Pruned Away\nFigure 6: Sparse LSTM model running on ESE can be 6.2× faster than the dense model.\n6017.3/3569.9 us (dense/sparse), and GPU takes 240.2/287.4 us (dense/sparse). ESE is 43× faster than CPU 3× faster than GPU. We measured power consumption of CPU, GPU and ESE. CPU power is measured by the pcm-power utility. GPU power is measured with nvidia-smi utility. We measure the power consumption of ESE by taking difference with/without the FPGA board installed. ESE takes 41 watts, CPU takes 111 watts(38 watts when using MKLSparse), GPU takes 202 watts (136 watts when using cuSparse). Considering both performance and power consumption, ESE is 197.0×/40.0× (dense/sparse) more energy efficient than CPU, and 14.3×/11.5× (dense/sparse) more energy efficient than GPU. Though sparse LSTM makes GPU more energy efficient, it is still one magnitude lower than ESE.\nWe analyze the trade off between sparsity and speedup in Fig. 6. The speedup increases as more parameters get pruned away. The sparse model pruned to 10% achieved 6.2× speedup over the baseline dense model on ESE. Besides, load-balance-aware pruning makes the performance 11% higher, as shown in the red and green line,"
    }, {
      "heading" : "6 Conclusion",
      "text" : "We present Efficient Speech Recognition Engine (ESE) that works directly on compressed LSTM model. ESE is optimized across the algorithm-software-hardware boundary: we first propose a method to compress the LSTM model by 12× without sacrificing the prediction accuracy, which greatly saves the memory bandwidth of FPGA implementation. Then we design a scheduler that can map the complex LSTM operations on FPGA and achieve parallelism. Finally we propose a hardware architecture that efficiently deals with the irregularity caused by compression. Working directly on the compressed model enables ESE to achieve 282 GOPS (equivalent to 2.52 TOPS for dense LSTM) on Xilinx XCKU060 FPGA board. ESE outperforms Core i7 CPU and Pascal Titan X GPU by factors of 43× and 3× on speed, and it is 40× and 11.5× more energy efficient than the CPU and GPU respectively."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We would like to thank Wei Chen, Zhongliang Liu, Guanzhe Huang, Yong Liu, Yanfeng Wang, Xiaochuan Wang and other researchers from Sogou for their suggestions and providing real-world speech data for model compression performance test."
    } ],
    "references" : [ {
      "title" : "Deep speech: Scaling up end-to-end speech recognition",
      "author" : [ "Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "Andrew Ng" ],
      "venue" : "arXiv, preprint arXiv:1412.5567,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "Dario Amodei" ],
      "venue" : "arXiv, preprint arXiv:1512.02595,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
      "author" : [ "Hasim Sak" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.1259,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Eie: efficient inference engine on compressed deep neural network",
      "author" : [ "Song Han", "Xingyu Liu", "Huizi Mao", "Jing Pu", "Ardavan Pedram", "Mark A Horowitz", "William J Dally" ],
      "venue" : "arXiv preprint arXiv:1602.01528,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Learning both weights and connections for efficient neural networks",
      "author" : [ "Song Han", "Jeff Pool", "John Tran", "William J Dally" ],
      "venue" : "In Proceedings of Advances in Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Deep Compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J Dally" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Accelerating recurrent neural networks in analytics server: Comparison of fpga, cpu, gpu, and asic",
      "author" : [ "Eriko Nurvitadhi", "Jaewoong Sim", "David Sheffield", "Asit Mishra", "Srivatsan Krishnan", "Debbie Marr" ],
      "venue" : "In Field Programmable Logic (FPL),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Recurrent neural networks hardware implementation on",
      "author" : [ "Andre Xian Ming Chang", "Berin Martini", "Eugenio Culurciello" ],
      "venue" : "FPGA. CoRR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Sparse matrix-vector multiplication on fpgas",
      "author" : [ "Ling Zhuo", "Viktor K. Prasanna" ],
      "venue" : "In FPGA,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "A high memory bandwidth fpga accelerator for sparse matrixvector multiplication",
      "author" : [ "J. Fowers", "K. Ovtcharov", "K. Strauss" ],
      "venue" : "In FCCM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "A scalable sparse matrix-vector multiplication kernel for energy-efficient sparse-blas on FPGAs",
      "author" : [ "Richard Dorrance", "Fengbo Ren" ],
      "venue" : "In FPGA,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction Deep neural network has surpassed the traditional acoustic model and become the state-of-the-art method for speech recognition [1, 2].",
      "startOffset" : 142,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction Deep neural network has surpassed the traditional acoustic model and become the state-of-the-art method for speech recognition [1, 2].",
      "startOffset" : 142,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "Long Short-Term Memory (LSTM) [3], Gated Recurrent Unit (GRU) [4] and vanilla recurrent neural networks (RNNs) are popular in speech recognition.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "Long Short-Term Memory (LSTM) [3], Gated Recurrent Unit (GRU) [4] and vanilla recurrent neural networks (RNNs) are popular in speech recognition.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "ESE takes the approach of EIE [5] one step further to address a more general problem of accelerating not only feed forward neural networks but also recurrent neural networks and LSTM.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Figure 2: Load Balance Aware Pruning and its Benefit for Parallel Processing Previous pruning methods removed the redundant connections based on the absolute value of the weights [6, 7], which lead to a potential problem of unbalanced non-zero weights distribution.",
      "startOffset" : 179,
      "endOffset" : 185
    }, {
      "referenceID" : 6,
      "context" : "Figure 2: Load Balance Aware Pruning and its Benefit for Parallel Processing Previous pruning methods removed the redundant connections based on the absolute value of the weights [6, 7], which lead to a potential problem of unbalanced non-zero weights distribution.",
      "startOffset" : 179,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "Compressed LSTM model is highly irregular, and thus accelerators on dense LSTMs cannot effectively take advantage of sparsity [8, 9].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "Compressed LSTM model is highly irregular, and thus accelerators on dense LSTMs cannot effectively take advantage of sparsity [8, 9].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 10,
      "context" : "LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 11,
      "context" : "LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.",
      "startOffset" : 181,
      "endOffset" : 184
    } ],
    "year" : 2016,
    "abstractText" : "Long Short-Term Memory (LSTM) is widely used in speech recognition. In order to achieve higher prediction accuracy, machine learning scientists have built larger and larger models. Such large model is both computation intensive and memory intensive. Deploying such bulky model results in high power consumption given latency constraint and leads to high total cost of ownership (TCO) of a data center. In order to speedup the prediction and make it energy efficient, we first propose a load-balance-aware pruning method that can compress the LSTM model size by 20× (10× from pruning and 2× from quantization) with negligible loss of the prediction accuracy. The pruned model is friendly for parallel processing. Next, we propose scheduler that encodes and partitions the compressed model to each PE for parallelism, and schedule the complicated LSTM data flow. Finally, we design the hardware architecture, named Efficient Speech Recognition Engine (ESE) that works directly on the compressed model. Implemented on Xilinx XCKU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working directly on the compressed LSTM network, corresponding to 2.52 TOPS on the uncompressed one, and processes a full LSTM for speech recognition with a power dissipation of 41 Watts. Evaluated on the LSTM for speech recognition benchmark, ESE is 43× and 3× faster than Core i7 5930k CPU and Pascal Titan X GPU implementations. It achieves 40× and 11.5× higher energy efficiency compared with the CPU and GPU respectively.",
    "creator" : "LaTeX with hyperref package"
  }
}