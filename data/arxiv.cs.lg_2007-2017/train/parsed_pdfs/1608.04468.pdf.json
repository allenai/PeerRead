{
  "name" : "1608.04468.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unbiased Learning-to-Rank with Biased Feedback",
    "authors" : [ "Thorsten Joachims", "Adith Swaminathan", "Tobias Schnabel" ],
    "emails" : [ "tj@cs.cornell.edu", "adith@cs.cornell.edu", "tbs49@cornell.edu" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Batch training of retrieval systems requires annotated test collections that take substantial effort and cost to amass. While economically feasible for Web Search, eliciting relevance annotations from experts is infeasible or impossible for most other ranking applications (e.g., personal collection search, intranet search). For these applications, implicit feedback from user behavior is an attractive source of data. Unfortunately, existing approaches for Learning-toRank (LTR) from implicit feedback – and clicks on search results in particular – have several limitations or drawbacks.\nUnder review\nACM ISBN 123-4567-24-567/08/06. . . $15.00\nDOI: 10.475/123 4\nFirst, the näıve approach of treating a click/no-click as a positive/negative relevance judgment is severely biased. In particular, the order of presentation has a strong influence on where users click [11]. This presentation bias leads to an incomplete and skewed sample of relevance judgments that is far from uniform, thus leading to biased learning-to-rank.\nSecond, treating clicks as preferences between clicked and skipped documents has been found to be accurate [9, 11], but it can only infer preferences that oppose the presented order. This again leads to severely biased data, and learning algorithms trained with these preferences tend to reverse the presented order unless additional heuristics are used [9].\nThird, probabilistic click models (see [4]) have been used to model how users produce clicks, and they can take position and context biases into account. By estimating latent parameters of these generative click models, one can infer the relevance of a given document for a given query. However, inferring reliable relevance judgments typically requires that the same query is seen multiple times, which is unrealistic in many retrieval settings (e.g., personal collection search) and for tail queries.\nFourth, allowing the LTR algorithm to randomize what is presented to the user, like in online learning algorithms [16, 6] and batch learning from bandit feedback (BLBF) [24] can overcome the problem of bias in click data in a principled manner. However, requiring that rankings be actively perturbed during system operation whenever we collect training data decreases ranking quality and, therefore, incurs a cost compared to observational data collection.\nIn this paper we present a theoretically principled and empirically effective approach for learning from observational implicit feedback that can overcome the limitations outlined above. By drawing on counterfactual estimation techniques from causal inference [8], we first develop a provably unbiased estimator for evaluating ranking performance using biased feedback data. Based on this estimator, we propose a Propensity-Weighted Empirical Risk Minimization (ERM) approach to LTR, which we implement efficiently in a new learning method we call Propensity SVM-Rank. While our approach uses a click model, the click model is merely used to assign propensities to clicked results in hindsight, not to extract aggregate relevance judgments. This means that our Propensity SVM-Rank does not require queries to repeat, making it applicable to a large range of ranking scenarios. Finally, our methods can use observational data and we do not require that the system randomizes rankings during data collection, except for a small pilot experiment to estimate the propensity model. ar X\niv :1\n60 8.\n04 46\n8v 1\n[ cs\n.I R\n] 1\n6 A\nug 2\n01 6\nWhen deriving our approach, we provide theoretical justification for each step, leading to a rigorous end-to-end approach that does not make unspecified assumptions or employs heuristics. This provides a principled basis for further improving components of the approach (e.g., the click propensity model, the ranking performance measure, the learning algorithm). We present an extensive empirical evaluation testing the limits of the approach on synthetic click data, finding that it performs robustly over a large range of bias, noise, and misspecification levels. Furthermore, we field our method in a real-world application on an operational search engine, finding that it is robust in practice and manages to substantially improve retrieval performance."
    }, {
      "heading" : "2. RELATED WORK",
      "text" : "There are two groups of approaches for handling biases in implicit feedback for learning-to-rank. The first group assumes the feedback collection step is fixed, and tries to interpret the observationally collected data so as to minimize bias effects. Approaches in the second group intervene during feedback collection, trying to present rankings that will lead to less biased feedback data overall.\nApproaches in the first group commonly assume some model of user behavior in order to explain bias effects. For example, in a cascade model [5], users are assumed to sequentially go down a ranking and click on a document if it is relevant. Clicks, under this model, let us learn preferences between skipped and clicked documents. Learning from these relative preferences lowers the impact of some biases [9]. Other click models ([5, 3, 1], also see [4]) have been proposed, and are trained to maximize log-likelihood of observed clicks. In these click modeling approaches, performance on downstream learning-to-rank algorithms is merely an afterthought. In contrast, we separate click propensity estimation and learning-to-rank in a principled way and we optimize for ranking performance directly. Our framework allows us to plug-and-play more sophisticated user models in place of the simple click models we use in this work.\nThe key technique used by approaches in the second group to obtain more reliable click data are randomized experiments. For instance, randomizing documents across all ranks lets us learn unbiased relevances for each document, and swapping neighboring pairs of documents [15] lets us learn reliable pairwise preferences. Similarly, randomized interleaving can detect preferences between different rankers reliably [2]. Different from online learning via bandit algorithms and interleaving [29, 21], batch learning from bandit feedback (BLBF) [24] still uses randomization during feedback collection, and then performs offline learning. Our problem formulation can be interpreted as being half way between the BLBF setting (loss function is unknown and no assumptions on loss function) and learning-to-rank from editorial judgments (components of ranking are fully labeled and loss function is given) since we know the form of the loss function but labels for only some parts of the ranking are revealed. All approaches that use randomization suffer from two limitations. First, randomization typically degrades ranking quality during data collection; second, deploying non-deterministic ranking functions introduces bookkeeping overhead. In this paper, the system can be deterministic and we merely exploit and model stochasticity in user behavior. Moreover, our framework also allows (but does not require)\nthe use of randomized data collection in order to mitigate the effect of biases and improve learning.\nOur approach uses inverse propensity scoring (IPS), originally employed in causal inference from observational studies [18], and more recently also in whole page optimization [28], IR evaluation with manual judgments [19], and recommender evaluation [12, 20]. We use randomized interventions similar to [5, 23, 27] to estimate propensities in a position discount model. Unlike the uniform ranking randomization of [27] (with its high performance impact) or swapping adjacent pairs as in [5], we swap documents in different ranks to the top position randomly as in [23]. See Section 5.3 for details.\nFinally, our approach is similar in spirit to [27], where propensity-weighting is used to correct for selection bias when discarding queries without clicks during learning-torank. The key insight of our work is to recognize that inverse propensity scoring can be employed much more powerfully, to account for position bias, trust bias, contextual effects, document popularity etc. using appropriate click models to estimate the propensity of each click rather than the propensity for a query to receive a click as in [27]."
    }, {
      "heading" : "3. FULL-INFO LEARNING TO RANK",
      "text" : "Before we derive our approach for LTR from biased implicit feedback, we first review the conventional problem of LTR from editorial judgments. In conventional LTR, we are given a sample X of i.i.d. queries xi ∼ P(x) for which we assume the relevances rel(x, y) of all documents y are known. Since all relevances are assumed to be known, we call this the Full-Information Setting. The relevances can be used to compute the loss ∆(y|x) (e.g., negative DCG) of any ranking y for query x. Aggregating the losses of individual rankings by taking the expectation over the query distribution, we can define the overall risk of a ranking system S that returns rankings S(x) as\nR(S) = ∫ ∆(S(x)|x) dP(x). (1)\nThe goal of learning is to find a ranking function S ∈ S that minimizes R(S) for the query distribution P(x). Since R(S) cannot be computed directly, it is typically estimated via the empirical risk\nR̂(S) = 1 |X| ∑\nxi∈X\n∆(S(xi)|xi).\nA common learning strategy is Empirical Risk Minimization (ERM) [25], which corresponds to picking the system Ŝ ∈ S that optimizes the empirical risk\nŜ = argminS∈S\n{ R̂(S) } ,\npossibly subject to some regularization in order to control overfitting. There are several LTR algorithms that follow this approach (see [14]), and we use SVM-Rank [9] as a representative algorithm in this paper.\nThe relevances rel(x, y) are typically elicited via expert judgments. Apart from being expensive and often infeasible (e.g., in personal collection search), expert judgments come with at least two other limitations. First, since it is clearly impossible to get explicit judgments for all documents, pooling techniques [22] are used such that only the most promising documents are judged. While cutting down\non judging effort, this introduces an undesired pooling bias because all unjudged documents are typically assumed to be irrelevant. The second limitation is that expert judgments rel(x, y) have to be aggregated over all intents that underlie the same query string, and it can be challenging for a judge to properly conjecture the distribution of query intents to assign an appropriate rel(x, y)."
    }, {
      "heading" : "4. PARTIAL-INFO LEARNING TO RANK",
      "text" : "Learning from implicit feedback has the potential to overcome the above-mentioned limitations of full-information LTR. By drawing the training signal directly from the user, it naturally reflects the user’s intent, since each user acts upon their own relevance judgement subject to their specific context and information need. It is therefore more appropriate to talk about query instances xi that include contextual information about the user, instead of query strings x. For a given query instance xi, we denote with ri(y) the userspecific relevance of result y for query instance xi. One may argue that what expert assessors try to capture with rel(x, y) is the mean of the relevances ri(y) over all query instances that share the query string, so, using implicit feedback for learning is able to remove a lot of guesswork about what the distribution of users meant by a query.\nHowever, when using implicit feedback as a relevance signal, unobserved feedback is an even greater problem than missing judgments in the pooling setting. In particular, implicit feedback is distorted by presentation bias, and it is not missing completely at random [13]. To nevertheless derive well-founded learning algorithms, we adopt the following counterfactual model. It closely follows [19], which unifies several prior works on evaluating information retrieval systems.\nFor concreteness and simplicity, assume that relevances are binary, ri(y) ∈ {0, 1}, and our performance measure of interest is the sum of the ranks of the relevant results\n∆(y|xi, ri) = ∑ y∈y rank(y|y) · ri(y). (2)\nAnalogous to (1), we can define the risk of a system as\nR(S) = ∫ ∆(S(x)|x, r) dP(x, r). (3)\nIn our counterfactual model, there exists a true vector of relevances ri for each incoming query instance (xi, ri) ∼ P(x, r). However, only a part of these relevances is observed for each query instance, while typically most remain unobserved. In particular, given a presented ranking ȳi we are more likely to observe the relevance signals (e.g., clicks) for the top-ranked results than for results ranked lower in the list. Let oi denote the 0/1 vector indicating which relevance values were revealed, oi ∼ P(o|xi, ȳi, ri). For each element of oi, denote with Q(oi(y) = 1|xi, ȳi, ri) the marginal probability of observing the relevance ri(y) of result y for query xi, if the user was presented the ranking ȳi. We refer to this probability value as the propensity of the observation. We will discuss how oi and Q can be obtained in Section 5.\nUsing this counterfactual modeling setup, we can get an unbiased estimate of ∆(y|xi, ri) for any new ranking y (typically different from the presented ranking ȳi) via the inverse\npropensity scoring (IPS) estimator [7, 18, 8] ∆̂IPS(y|xi, ȳi, oi) = ∑\ny:oi(y)=1\nrank(y|y)·ri(y) Q(oi(y)=1|xi, ȳi, ri)\n= ∑\ny:oi(y)=1∧ ri(y)=1\nrank(y|y) Q(oi(y)=1|xi, ȳi, ri) .\nThis is an unbiased estimate of ∆(y|xi, ri) for any y, if Q(oi(y) = 1|xi, ȳi, ri) > 0 for all y that are relevant ri(y) = 1 (but not necessarily for the irrelevant y).\nEoi [∆̂IPS(y|xi, ȳi, oi)]\n= Eoi  ∑ y:oi(y)=1 rank(y|y)·ri(y) Q(oi(y)=1|xi, ȳi, ri)  =\n∑ y∈y Eoi [ oi(y)·rank(y|y)·ri(y) Q(oi(y)=1|xi, ȳi, ri)) ] =\n∑ y∈y Q(oi(y) = 1|xi, ȳi, ri) · rank(y|y) · ri(y) Q(oi(y) = 1|xi, ȳi, ri)\n= ∑ y∈y rank(y|y) ri(y)\n= ∆(y|xi, ri).\nThe second step uses linearity of expectation, and the fourth step uses Q(oi(y) = 1|xi, ȳi, ri) > 0.\nAn interesting property of ∆̂IPS(y|xi, ȳi, oi) is that only those results y with [oi(y) = 1 ∧ ri(y) = 1] (i.e. clicked results, as we will see later) contribute to the estimate. We therefore only need the propensities Q(oi(y) = 1|xi, ȳi, ri) for relevant results. Since we will eventually need to estimate the propensities Q(oi(y) = 1|xi, ȳi, ri), an additional requirement for making ∆̂IPS(y|xi, ȳi, oi) computable while remaining unbiased is that the propensities only depend on observable information (i.e., unconfoundedness, see [8]).\nTo define the empirical risk to optimize during learning, we begin by collecting a sample of N query instances xi, recording the partially-revealed relevances ri as indicated by oi, and the propensities Q(oi(y) = 1|xi, ȳi, ri) for the observed relevant results in the ranking ȳi presented by the system. Then, the empirical risk of a system is simply the IPS estimates averaged over query instances:\nR̂IPS(S) = 1\nN N∑ i=1 ∑ y:oi(y)=1∧\nri(y)=1\nrank(y|S(xi)) Q(oi(y)=1|xi, ȳi, ri) . (4)\nSince ∆̂IPS(y|xi, ȳi, oi) is unbiased for each query instance, the aggregate R̂IPS(S) is also unbiased for R(S) from (3),\nE[R̂IPS(S)] = R(S).\nFurthermore, it is easy to verify that R̂IPS(S) converges to the true R(S) under mild additional conditions (i.e., propensities bounded away from 0) as we increase the sample size N of query instances. So, we can perform ERM using this propensity-weighted empirical risk,\nŜ = argminS∈S { R̂IPS(S) } .\nFinally, using standard results from statistical learning theory [25], consistency of the empirical risk paired with capacity control implies consistency also for ERM. In intuitive\nterms, this means that given enough training data, the learning algorithm is guaranteed to find the best system in S."
    }, {
      "heading" : "5. FEEDBACK PROPENSITY MODELS",
      "text" : "In Section 4, we showed that the relevance signal ri, the observation pattern oi, and the propensities of the observations Q(oi(y) = 1|xi, ȳi, ri) are the key components for unbiased LTR from biased observational feedback. We now outline how these quantities can be elicited and modeled in a typical search-engine application. However, the general framework of Section 4 extends beyond this particular application, and beyond the particular feedback model below."
    }, {
      "heading" : "5.1 Position-Based Propensity Model",
      "text" : "Search engine click logs provide a sample of query instances xi, the presented ranking ȳi and a (sparse) clickvector where each ci(y) ∈ {0, 1} indicates whether result y was clicked or not. To derive propensities of observed clicks, we will employ a click propensity model. For simplicity, we consider a straightforward examination model analogous to [17], where a click on a search result depends on the probability that a user examines a result (i.e., ei(y)) and then decides to click on it (i.e., ci(y)) in the following way:\nP (ei(y) = 1| rank(y|ȳ)) · P (ci(y) = 1| ri(y), ei(y) = 1).\nIn this model, examination depends only on the rank of y in ȳ. So, P (ei(y) = 1| rank(y|ȳi)) can be represented by a vector of examination probabilities pr, one for each rank r. These examination probabilities can model presentation bias documented in eye-tracking studies [11], where users are more likely to see results at the top of the ranking than those further down.\nFor the probability of click on an examined result P (ci(y) = 1| ri(y), ei(y) = 1), we first consider the simplest model where clicking is a deterministic noise-free function of the users private relevance assessment ri(y). Under this model, users click if and only if the result is examined and relevant (ci(y) = 1 ↔ [ei(y) = 1 ∧ ri(y) = 1]). This means that for examined results (i.e., ei(y) = 1) clicking is synonymous with relevance (ei(y) = 1 → [ci(y) = ri(y)]). Furthermore, it means that we observe the value of ri(y) perfectly when ei(y) = 1 (ei(y) = 1 → oi(y) = 1), and that we gain no knowledge of the true ri(y) when a result is not examined (ei(y) = 0 → oi(y) = 0). Therefore, examination equals observation and Q(oi(y)|xi, ȳi, ri) ≡ P (ei(y)| rank(y|ȳi)).\nUsing these equivalences, we can simplify the IPS estimator from (4) by substituting pr as the propensities and by using ci(y) = 1↔ [oi(y) = 1 ∧ ri(y) = 1]\nR̂IPS(S) = 1\nn n∑ i=1 ∑ y:ci(y)=1 rank(y|S(xi)) prank(y|ȳi) . (5)\nR̂IPS(S) is an unbiased estimate of R(S) under the positionbased propensity model if pr > 0 for all ranks. While absence of a click does not imply that the result is not relevant (i.e., ci(y) = 0 6→ ri(y) = 0), the IPS estimator has the nice property that such explicit negative judgments are not needed to compute an unbiased estimate of R(S) for the loss in (2). Similarly, while absence of a click leaves us unsure about whether the result was examined (i.e., ei(y) =?), the IPS estimator only needs to know the indicators oi(y) = 1 for results that are also relevant (i.e., clicked results).\nFinally, note the conceptual difference in how we use this standard examination model compared to most prior work. We do not try to estimate an average relevance rating rel(x, y) by taking repeat instances of the same query x, but we use the model as a propensity estimator to de-bias individual observed user judgments ri(y) to be used directly in ERM."
    }, {
      "heading" : "5.2 Incorporating Click Noise",
      "text" : "In Section 5.1, we assumed that clicks reveal the user’s true ri in a noise-free way. This is clearly unrealistic. In addition to the stochasticity in the examination distribution P (ei(y) = 1| rank(y|y)), we now also consider noise in the distribution that generates the clicks. In particular, we no longer require that a relevant result is clicked with probability 1 and an irrelevant result is clicked with probability 0, but instead, for 1 ≥ + > − ≥ 0,\nP (ci(y) = 1| ri(y) = 1, oi(y) = 1) = +, P (ci(y) = 1| ri(y) = 0, oi(y) = 1) = −.\nThe first line means that users click on a relevant result only with probability +, while the second line means that users may erroneously click on an irrelevant result with probability −. An alternative and equivalent way of thinking about click noise is that users still click deterministically as in the previous section, but based on a noisily corrupted version r̃i of ri. This means that all reasoning regarding observation (examination) events oi and their propensities pr still holds, and that we still have that ci(y) = 1 → oi(y) = 1. What does change, though, is that we no longer observe the “correct” ri(y) but instead get feedback according to the noise-corrupted version r̃i(y). What happens to our learning process if we estimate risk using (5), but now with r̃i?\nFortunately, the noise does not affect ERM’s ability to find the best ranking system given enough data. While using noisy clicks leads to biased empirical risk estimates w.r.t. the true ri (i.e., E[R̂IPS(S)] 6= R(S)), in expectation this bias is order preserving for R(S) such that the risk minimizer remains the same.\nE[R̂IPS(S1)] > E[R̂IPS(S2)]\n⇔ Ex,r,ȳ EoEc|o  ∑ y:c(y)=1 rank(y|S1(x))−rank(y|S2(x)) prank(y|ȳ) >0 ⇔ Ex,r\n[∑ y P(c(y) = 1|o(y) = 1, r(y))δ rank(y|x) ] > 0\n⇔ Ex,r [∑ y δ rank(y|x) · ( + r(y) + −(1− r(y))) ] > 0\n⇔ Ex,r [∑ y δ rank(y|x) · (( + − −) r(y) + −) ] > 0\n∗⇔ Ex,r [∑ y δ rank(y|x) · ( + − −) r(y) ] > 0\n⇔ Ex,r [∑ y δ rank(y|x) · r(y) ] > 0\n⇔ R(S1) > R(S2),\nwhere δ rank(y|x) is short for rank(y|S1(x))−rank(y|S2(x)) and we use the fact that − ∑ y∈ȳ δ rank(y|x) = 0 in the step marked ∗. This implies that our propensity-weighted ERM\nis a consistent approach for finding a ranking function with the best true R(S),\nŜ = argminS∈S {R(S)}\n= argminS∈S { E[R̂IPS(S)] } , (6)\neven when the objective is corrupted by click noise as specified above."
    }, {
      "heading" : "5.3 Propensity Estimation",
      "text" : "As the last step of defining the click propensity model, we need to address the question of how to estimate its parameters (i.e. the vector of examination probabilities pr) for a particular search engine. The following shows that we can get estimates using data from a simple intervention similar to [27], but without the strong negative impact of presenting uniformly random results to some users. This also relates to the Click@1 metric proposed by [3].\nFirst, note that it suffices to estimate the pr up to some positive multiplicative constant, since any such constant does not change how the IPS estimator (5) orders different systems. We therefore merely need to estimate how much pr changes relative to pk for some“landmark”rank k. This suggests the following experimental intervention for estimating pr: before presenting the ranking to the user, swap the result at rank k with the result at rank r. If we denote with y′ the results originally in rank k, our click model before and after the intervention indicates that\nP (ci(y ′) = 1|no-swap) = pk · P (ci(y′) = 1| ei(y′) = 1) P (ci(y ′) = 1|swap-k-and-r) = pr · P (ci(y′) = 1| ei(y′) = 1)\nwhere\nP (ci(y ′) = 1| ei(y′) = 1) = ∑\nv∈{0,1}\nP (ci(y ′)=1| ri(y′)=v, ei(y′)=1) · P (ri(y′)=v)\nis constant regardless of the intervention. This means that the clickthrough rates P (ci(y\n′) = 1|swap-k-and-r), which we can estimate from the intervention data, are proportional to the parameters pr for any r. By performing the swapping intervention between rank k and all other ranks r, we can estimate all the pr parameters.\nThis swap-intervention experiment is of much lower impact than the uniform randomization proposed in [27] for a different propensity estimation problem, and careful consideration of which rank k to choose can further reduce impact of the swap experiment. From a practical perspective, it may also be unnecessary to separately estimate pr for each rank. Instead, one may want to interpolate between estimates at well-chosen ranks and/or employ smoothing. Finally, note that the intervention only needs to be applied on a small subset of the data used for fitting the click propensity model, while the actual data used for training the ERM learning algorithm does not require any interventions."
    }, {
      "heading" : "5.4 Alternative Feedback Propensity Models",
      "text" : "The click propensity model we define above is arguably one of the simplest models one can employ for propensity modeling in LTR, and there is broad scope for extensions.\nFirst, one could extend the model by incorporating other biases, for example, trust bias [11] which affects perceived relevance of a result based on its position in the ranking.\nThis can be captured by conditioning the click probabilities also on the position P(ci(y\n′) = 1| ri(y′), ei(y′) = 1, rank(y|ȳi)). We have already explored that the model can be extended to include trust bias, but it is omitted due to space constraints. Furthermore, it is possible to model saliency biases [30] by replacing the pr with a regression function.\nSecond, we conjecture that a wide range of other click models (e.g., cascade model [5] and others [5, 3, 1, 4]) can be adapted as propensity models. The main requirement is that we can compute marginal click probabilities for the clicked documents in hindsight, which is computationally feasible for many of the existing models.\nThird, we may be able to define and train new types of click models. In particular, for our propensity ERM approach we only need the propensities Q(oi(y) = 1|xi, ȳi, ri) for observed and relevant documents to evaluate the IPS estimator, but not for irrelevant documents. This can be substantially easier than a full generative model of how people reveal relevance judgments through implicit feedback. In particular, this model can condition on all the revealed relevances ri(yj) in hindsight, and it does not need to treat them as latent variables.\nFinally, the ERM learning approach is not limited to binary click feedback, but applies to a large range of feedback settings. For example, the feedback may be explicit star ratings in a movie recommendation system, and the propensities may be the results of self-selection by the users as in [20]. In such an explicit feedback setting, oi is fully known, which simplifies propensity estimation substantially."
    }, {
      "heading" : "6. PROPENSITY-WEIGHTED SVM-RANK",
      "text" : "We now derive a concrete learning method that implements propensity-weighted LTR. It is based on SVM-Rank [9, 10], but we conjecture that propensity-weighted versions of other LTR methods can be derived as well.\nConsider a dataset of n examples of the following form. For each query-result pair (xj , yj) that is clicked, we compute the propensity qi = Q(oi(y) = 1|xi, ȳi, ri) of the click according to our click propensity model. We also record the candidate set Yj of all results for query xj . Typically, Yj contains a few hundred documents – selected by a stage-one ranker [26] – that we aim to rerank. Note that each click generates a separate training example, even if multiple clicks occur for the same query.\nGiven this propensity-scored click data, we define Propensity SVM-Rank as a generalization of conventional SVMRank. Propensity SVM-Rank learns a linear scoring function f(x, y) = w·φ(x, y) that can be used for ranking results, where w is a weight vector and φ(x, y) is a feature vector that describes the match between query x and result y.\nPropensity SVM-Rank optimizes the following objective,\nŵ = argminw,ξ 1 2 w · w + C n n∑ j=1 1 qj ∑ y∈Yj ξjy\ns.t. ∀y ∈ Y1\\{y1} : w · [φ(x1, y1)− φ(x1, y)] ≥ 1−ξ1y ...\n∀y ∈ Yn\\{yn} : w · [φ(xn, yn)− φ(xn, y)] ≥ 1−ξny ∀j∀y : ξjy ≥ 0.\nC is a regularization parameter that is typically selected via cross-validation. The training objective optimizes an\nupper bound on the regularized IPS estimated empirical risk of (5), since each line of constraints corresponds to the rank of a relevant document (minus 1). In particular, for any feasible (w, ξ)\nrank(yi|y)− 1 = ∑ y 6=yi 1w·[φ(xi,y)−φ(xi,yi)]>0\n≤ ∑ y 6=yi max(1− w · [φ(xi, yi)− φ(xi, y)], 0)\n≤ ∑ y 6=yi ξiy.\nWe can solve this type of Quadratic Program efficiently via a one-slack formulation [10], and we are using SVMRank1 with appropriate modifications to include IPS weights 1/qj . The resulting code will be available online.\nIn the empirical evaluation, we compare against the naive application of SVM-Rank, which minimizes the rank of the clicked documents while ignoring presentation bias. In particular, Naive SVM-Rank sets all the qi uniformly to the same constant (e.g., 1)."
    }, {
      "heading" : "7. EMPIRICAL EVALUATION",
      "text" : "We take a two-pronged approach to evaluating our approach empirically. First, we use synthetically generated click data to explore the behavior of our methods over the whole spectrum of presentation bias severity, click noise, and propensity misspecification. Second, we explore the realworld applicability of our approach by evaluating on an operational search engine using real click-logs from live traffic."
    }, {
      "heading" : "7.1 Synthetic Data Experiments",
      "text" : "To be able to explore the full spectrum of biases and noise, we conducted experiments using click data derived from the Yahoo Learning to Rank Challenge corpus (set 1). This corpus contains a large number of manually judged queries, where we binarized relevance by assigning ri(y) = 1 to all documents that got rated 3 or 4, and ri(y) = 0 for ratings 0, 1, 2. We adopt the train, validation, test splits in the corpus. This means that queries in the three sets are disjoint, and we never train on any data from queries in the test set. To have a gold standard for reporting test-set performance, we measure performance on the binarized full-information ratings using (2).\nTo generate click data from this full-information dataset of ratings, we first trained a normal Ranking SVM using 1 percent of the full-information training data to get a ranking function S0. We employ S0 as the “Production Ranker”, and it is used to “present” rankings ȳ when generating the click data. We generate clicks using the rankings ȳ and ground-truth binarized relevances from the Yahoo dataset according to the following process. Depending on whether we are generating a training or a validation sample of click data, we first randomly draw a query x from the respective full-information dataset. For this query we compute ȳ = S0(x) and generate clicks based on the model from Section 5. Whenever a click is generated, we record a training example with its associated propensity Q(o(y) = 1|x, ȳ, r). For the\n1https://www.joachims.org/svm light/svm rank.html\nexperiments, we model presentation bias via Q(o(y) = 1|x, ȳ, r) = prank(y|ȳ) = ( 1\nrank(y|ȳ)\n)η . (7)\nThe parameter η lets us control the severity of the presentation bias. We also introduce noise into the clicks according to the model described in Section 5. When not mentioned otherwise, we use the parameters η = 1, − = 0.1, and + = 1, which leads to click data where about 33% of the clicks are noisy clicks on irrelevant results and where the result at rank 10 has a 10% probability of being examined. We also explore other bias profiles and noise levels in the following experiments.\nIn all experiments, we select any parameters (e.g., C) of the learning methods via cross-validation on a validation set. The validation set is generated using the same click model as the training set, but using the queries in the validation-set portion of the Yahoo dataset. For Propensity SVM-Rank, we always use the (unclipped) IPS estimator (5) to estimate validation set performance. Keeping with the proportions of the original Yahoo data, the validation set size is always about 15% the size of the training set.\nThe primary baseline we compare against is a naive application of SVM-Rank that simply ignores the bias in the click data. We call this method Naive SVM-Rank. It is equivalent to a standard ranking SVM [9], but is most easily explained as equivalent to Propensity SVM-Rank with all qj set to 1. Analogously, we use the corresponding naive version of (5) with propensities set to 1 to estimate validation set performance for Naive SVM-Rank."
    }, {
      "heading" : "7.2 How does ranking performance scale with training set size?",
      "text" : "We first explore how the test-set ranking performance changes as the learning algorithm is given more and more click data. The resulting learning curves are given in Figure 1, and the performance of S0 is given as a baseline. The click data has presentation bias according to (2) with η = 1\nand noise − = 0.1. For small datasets, results are averaged over 5 draws of the click data.\nWith increasing amounts of click data, Propensity SVMRank approaches the skyline performance of the fullinformation SVM-Rank trained on the complete training set of manual ratings without noise. This is in stark contrast to Naive SVM-Rank which fails to account for the bias in the data and does not reach this level of performance. Furthermore, Naive SVM-Rank cannot make effective use of additional data and its learning curve is essentially flat. This is consistent with the theoretical insight that estimation error in Naive SVM-Rank’s empirical risk R̂(S) is dominated by asymptotic bias due to biased clicks, which does not decrease with more data and leads to suboptimal learning. The unbiased risk estimate R̂IPS(S) of Propensity SVM-Rank, however, has estimation error only due to finite sample variance, which is decreased by more data and leads to consistent learning.\nWhile unbiasedness is an important property when click data is plenty, the increased variance of R̂IPS(S) can be a drawback for small datasets. This can be seen in Figure 1, where Naive SVM-Rank outperforms Propensity SVM-Rank for small datasets. This can be remedied using techniques like “propensity clipping” [23], where small propensities are clipped to some threshold value τ to trade bias for variance.\nR̂CIPS(S) = 1\nn ∑ xi ∑ y∈S(xi) rank(y|S(xi)) · ri(y) max{τ,Q(oi(y)=1|xi, ȳi, ri)} .\nFigure 1 shows the learning curve of Propensity SVM-Rank with clipping, cross-validating both the clipping threshold τ and C. Clipping indeed improves performance for small datasets. While τ = 1 is equivalent to Naive SVM-Rank, the validation set is too small (and hence, the finite sample error of the validation performance estimate too high) to reliably select this model in every run. In practice, however, we expect click data to be plentiful such that lack of training data is unlikely to be a persistent issue."
    }, {
      "heading" : "7.3 How much presentation bias can be tolerated?",
      "text" : "We now vary the severity of the presentation bias via η to understand its impact on Propensity SVM-Rank. Fig-\nure 2 shows that inverse propensity weighting is beneficial whenever substantial bias exists. Furthermore, increasing the amount of training data by a factor of 5 leads to further improvement for the Propensity SVM-Rank, while the added training data has no effect on Naive SVM-Rank. This is consistent with our arguments from Section 4 – more training data does not help when bias dominates estimation error, but it can reduce estimation error from variance in the unbiased risk estimate of Propensity SVM-Rank."
    }, {
      "heading" : "7.4 How robust are the methods to click noise?",
      "text" : "Figure 3 shows that Propensity SVM-Rank also enjoys a substantial advantage when it comes to noise. When increasing the noise level in terms of − from 0 up to 0.3 (resulting in click data where 59.8% of all clicks are on irrelevant documents), Propensity SVM-Rank increasingly outperforms Naive SVM-Rank. And, again, the unbiasedness of the empirical risk estimate allows Propensity SVM-Rank to benefit from more data."
    }, {
      "heading" : "7.5 How robust is Propensity SVM-Rank to misspecified propensities?",
      "text" : "So far all experiments have assumed that Propensity SVMRank has access to accurate propensities. In practice, however, propensities need to be estimated and are subject to model assumptions. We now evaluate how robust Propensity SVM-Rank is to misspecified propensities. Figure 4 shows the performance of Propensity SVM-Rank when the training data is generated with η = 1, but the propensities used by Propensity SVM-Rank are misspecified using the η given in the x-axis of the plot. The plot shows that even misspecified propensities can give substantial improvement over naively ignoring the bias, as long as the misspecification is “conservative” – i.e., overestimating small propensities is tolerable (which happens when η < 1), but underestimating small propensities can be harmful (which happens when η > 1). This is consistent with theory, and clipping is one particular way of overestimating small propensities that can even improve performance. Overall, we conclude that even a mediocre propensity model can improve over the naive approach – after all, the naive approach can be thought of as a particularly poor propensity model that implicitly assumes no presentation bias and uniform propensities."
    }, {
      "heading" : "7.6 Real-World Experiment",
      "text" : "We now examine the performance of Propensity SVMrank when trained on real-world click logs and deployed in a live search engine for scientific articles [anonymized for submission]. The search engine uses a linear scoring function as outlined in Section 6. Query-document features φ(x, y) are represented by a 1000−dimensional vector, and the production ranker used for collecting training clicks employs a hand-crafted weight vector w (denoted Prod). Observed clicks on rankings served by this ranker over a period of 21 days provide implicit feedback data for LTR as outlined in Section 6.\nTo estimate the propensity model, we consider the simple position-based model of Section 5.1 and we collect new click data via randomized interventions for 7 days as outlined in Section 5.3 with landmark rank k = 1. Before presenting the ranking, we take the top-ranked document and swap it with the document at a uniformly at random chosen rank j ∈ {1, . . . 21}. The ratio of observed click-through rates (CTR) on the formerly top-ranked document now at position j vs. its CTR at position 1 gives a noisy estimate of pj/p1 in the position-based click model. We additionally smooth these estimates by interpolating with the overall observed CTR at position j (normalized so that CTR@1 = 1). This yields pr that approximately decay with rank r with the smallest pr ' 0.12. For r > 21, we impute pr = p21.\nWe partition the click-logs into a train-validation split: the first 16 days are the train set and provide 5437 clickevents for SVM-rank, while the remaining 5 days are the validation set with 1755 click events. The hyper-parameter C is picked via cross validation. Analogous to Section 7.1, we use the IPS estimator for Propensity SVM-Rank, and naive estimator with Q(o(y) = 1|x, ȳ, r) = 1 for Naive SVM-Rank. With the best hyper-parameter settings, we re-train on all 21 days worth of data to derive the final weight vectors for either method.\nWe fielded these learnt weight vectors in two online interleaving experiments [2], the first comparing Propensity SVM-Rank against Prod and the second comparing Propensity SVM-Rank against Naive SVM-Rank. The results are summarized in Table 1. We find that Propensity SVMRank significantly outperforms the hand-crafted production ranker that was used to collect the click data for training\n(two-tailed binomial sign test p = 0.001 with relative risk 0.71 compared to null hypothesis). Furthermore, Propensity SVM-Rank similarly outperforms Naive SVM-Rank, demonstrating that even a simple propensity model provides benefits on real-world data (two-tailed binomial sign test p = 0.006 with relative risk 0.77 compared to null hypothesis). Note that Propensity SVM-Rank not only significantly, but also substantially outperforms both other rankers in terms of effect size – and the synthetic data experiments suggest that additional training data will further increase its advantage."
    }, {
      "heading" : "8. CONCLUSIONS",
      "text" : "This paper introduced a principled approach for learningto-rank under biased feedback data. Drawing on counterfactual modeling techniques from causal inference, we present a theoretically sound Empirical Risk Minimization framework for LTR. We instantiate this framework with a PropensityWeighted Ranking SVM, and provide extensive empirical evidence that the resulting learning method is robust to selection biases, noise, and model misspecification. Furthermore, our real-world experiments on a live search engine show that the approach leads to substantial retrieval improvements, without any heuristic or manual interventions in the learning process."
    }, {
      "heading" : "9. FUTURE RESEARCH",
      "text" : "Beyond the specific learning methods and propensity models we propose, this paper may have even bigger impact for its theoretical contribution of developing the general counterfactual model for LTR, thus articulating the key components necessary for LTR under biased feedback. First, the insight that propensity estimates are crucial for ERM learning opens a wide area of research on designing better propensity models. Second, the theory demonstrates that LTR methods should optimize propensity-weighted ERM objectives, raising the question of which other learning methods beyond the Ranking SVM can be adapted to the Propensity ERM approach. Third, we conjecture that a Propensity ERM approach can be developed also for pointwise LTR methods using techniques from [19], and possibly even for listwise LTR.\nBeyond learning from implicit feedback, propensityweighted ERM techniques may prove useful even for optimizing offline IR metrics on manually annotated test collections. First, they can eliminate pooling bias, since the use of sampling during judgment elicitation puts us in a controlled setting where propensities are known (and can be optimized [19]) by design. Second, propensities estimated via click models can enable click-based IR metrics like click-DCG to better correlate with test set DCG."
    }, {
      "heading" : "10. ACKNOWLEDGMENTS",
      "text" : "This work was supported in part through NSF Awards IIS-1247637, IIS-1513692, IIS-1615706, and a gift from Bloomberg. We thank Maarten de Rijke, Alexey Borisov, Artem Grotov, and Yuning Mao for valuable feedback and discussions."
    }, {
      "heading" : "11. REFERENCES",
      "text" : "[1] A. Borisov, I. Markov, M. de Rijke, and P. Serdyukov.\nA neural click model for web search. In Proceedings of the 25th International Conference on World Wide Web, pages 531–541, 2016.\n[2] O. Chapelle, T. Joachims, F. Radlinski, and Y. Yue. Large-scale validation and analysis of interleaved search evaluation. ACM Transactions on Information Systems (TOIS), 30(1):6:1–6:41, 2012.\n[3] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In International Conference on World Wide Web (WWW), pages 1–10. ACM, 2009.\n[4] A. Chuklin, I. Markov, and M. de Rijke. Click Models for Web Search. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2015.\n[5] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In International Conference on Web Search and Data Mining (WSDM), pages 87–94. ACM, 2008.\n[6] K. Hofmann, A. Schuth, S. Whiteson, and M. de Rijke. Reusing historical interaction data for faster online learning to rank for ir. In International Conference on Web Search and Data Mining (WSDM), pages 183–192, 2013.\n[7] D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American Statistical Association, 47(260):663–685, 1952.\n[8] G. Imbens and D. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press, 2015.\n[9] T. Joachims. Optimizing search engines using clickthrough data. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 133–142, 2002.\n[10] T. Joachims. Training linear SVMs in linear time. In ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (KDD), pages 217–226, 2006.\n[11] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Transactions on Information Systems (TOIS), 25(2), April 2007.\n[12] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms. In International Conference on Web Search and Data Mining (WSDM), pages 297–306, 2011.\n[13] R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data. John Wiley, 2002.\n[14] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225–331, Mar. 2009.\n[15] K. Raman and T. Joachims. Learning socially optimal information systems from egoistic users. In European Conference on Machine Learning (ECML), pages 128–144, 2013.\n[16] K. Raman, T. Joachims, P. Shivaswamy, and T. Schnabel. Stable coactive learning via perturbation. In International Conference on Machine Learning (ICML), pages 837–845, 2013.\n[17] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: Estimating the click-through rate for new ads. In International Conference on World Wide Web (WWW), pages 521–530. ACM, 2007.\n[18] P. R. Rosenbaum and D. B. Rubin. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41–55, 1983.\n[19] T. Schnabel, A. Swaminathan, P. Frazier, and T. Joachims. Unbiased comparative evaluation of ranking functions. In ACM International Conference on the Theory of Information Retrieval (ICTIR), 2016.\n[20] T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. Recommendations as treatments: Debiasing learning and evaluation. In International Conference on Machine Learning (ICML), 2016.\n[21] A. Schuth, H. Oosterhuis, S. Whiteson, and M. de Rijke. Multileave gradient descent for fast online learning to rank. In International Conference on Web Search and Data Mining (WSDM), pages 457–466, 2016.\n[22] K. Sparck-Jones and C. J. V. Rijsbergen. Report on the need for and provision of an “ideal” information retrieval test collection. Technical report, University of Cambridge, 1975.\n[23] A. L. Strehl, J. Langford, L. Li, and S. Kakade. Learning from logged implicit exploration data. In Proceedings of the 24th Annual Conference on Neural Information Processing Systems, pages 2217–2225, 2010.\n[24] A. Swaminathan and T. Joachims. Batch learning from logged bandit feedback through counterfactual risk minimization. Journal of Machine Learning Research (JMLR), 16:1731–1755, Sep 2015. Special Issue in Memory of Alexey Chervonenkis.\n[25] V. Vapnik. Statistical Learning Theory. Wiley, Chichester, GB, 1998.\n[26] L. Wang, J. J. Lin, and D. Metzler. A cascade ranking model for efficient ranked retrieval. In ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 105–114. ACM, 2011.\n[27] X. Wang, M. Bendersky, D. Metzler, and M. Najork. Learning to rank with selection bias in personal search. In ACM Conference on Research and Development in Information Retrieval (SIGIR). ACM, 2016.\n[28] Y. Wang, D. Yin, L. Jie, P. Wang, M. Yamada, Y. Chang, and Q. Mei. Beyond ranking: Optimizing whole-page presentation. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining, WSDM ’16, pages 103–112, 2016.\n[29] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In International Conference on Machine Learning (ICML), pages 151–159, 2009.\n[30] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias: examining result attractiveness as a source of presentation bias in clickthrough data. In International Conference on World Wide Web (WWW), pages 1011–1018. ACM, 2010."
    } ],
    "references" : [ {
      "title" : "A neural click model for web search",
      "author" : [ "A. Borisov", "I. Markov", "M. de Rijke", "P. Serdyukov" ],
      "venue" : "In Proceedings of the 25th International Conference on World Wide Web,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Large-scale validation and analysis of interleaved search evaluation",
      "author" : [ "O. Chapelle", "T. Joachims", "F. Radlinski", "Y. Yue" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "A dynamic bayesian network click model for web search ranking",
      "author" : [ "O. Chapelle", "Y. Zhang" ],
      "venue" : "In International Conference on World Wide Web (WWW),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Click Models for Web Search. Synthesis Lectures on Information Concepts, Retrieval, and Services",
      "author" : [ "A. Chuklin", "I. Markov", "M. de Rijke" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "An experimental comparison of click position-bias models",
      "author" : [ "N. Craswell", "O. Zoeter", "M. Taylor", "B. Ramsey" ],
      "venue" : "In International Conference on Web Search and Data Mining (WSDM),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Reusing historical interaction data for faster online learning to rank for ir",
      "author" : [ "K. Hofmann", "A. Schuth", "S. Whiteson", "M. de Rijke" ],
      "venue" : "In International Conference on Web Search and Data Mining (WSDM),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "A generalization of sampling without replacement from a finite universe",
      "author" : [ "D.G. Horvitz", "D.J. Thompson" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1952
    }, {
      "title" : "Causal Inference for Statistics, Social, and Biomedical Sciences",
      "author" : [ "G. Imbens", "D. Rubin" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "T. Joachims" ],
      "venue" : "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "In ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search",
      "author" : [ "T. Joachims", "L. Granka", "B. Pan", "H. Hembrooke", "F. Radlinski", "G. Gay" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms",
      "author" : [ "L. Li", "W. Chu", "J. Langford", "X. Wang" ],
      "venue" : "In International Conference on Web Search and Data Mining (WSDM),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Statistical Analysis with Missing Data",
      "author" : [ "R.J.A. Little", "D.B. Rubin" ],
      "venue" : "John Wiley,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "T.-Y. Liu" ],
      "venue" : "Foundations and Trends in Information Retrieval,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Learning socially optimal information systems from egoistic users",
      "author" : [ "K. Raman", "T. Joachims" ],
      "venue" : "In European Conference on Machine Learning (ECML),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Stable coactive learning via perturbation",
      "author" : [ "K. Raman", "T. Joachims", "P. Shivaswamy", "T. Schnabel" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Predicting clicks: Estimating the click-through rate for new ads",
      "author" : [ "M. Richardson", "E. Dominowska", "R. Ragno" ],
      "venue" : "In International Conference on World Wide Web (WWW),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "The central role of the propensity score in observational studies for causal effects",
      "author" : [ "P.R. Rosenbaum", "D.B. Rubin" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1983
    }, {
      "title" : "Unbiased comparative evaluation of ranking functions",
      "author" : [ "T. Schnabel", "A. Swaminathan", "P. Frazier", "T. Joachims" ],
      "venue" : "In ACM International Conference on the Theory of Information Retrieval (ICTIR),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Recommendations as treatments: Debiasing learning and evaluation",
      "author" : [ "T. Schnabel", "A. Swaminathan", "A. Singh", "N. Chandak", "T. Joachims" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Multileave gradient descent for fast online learning to rank",
      "author" : [ "A. Schuth", "H. Oosterhuis", "S. Whiteson", "M. de Rijke" ],
      "venue" : "In International Conference on Web Search and Data Mining (WSDM),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Report on the need for and provision of an “ideal” information retrieval test collection",
      "author" : [ "K. Sparck-Jones", "C.J.V. Rijsbergen" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1975
    }, {
      "title" : "Learning from logged implicit exploration data",
      "author" : [ "A.L. Strehl", "J. Langford", "L. Li", "S. Kakade" ],
      "venue" : "In Proceedings of the 24th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Batch learning from logged bandit feedback through counterfactual risk minimization",
      "author" : [ "A. Swaminathan", "T. Joachims" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1998
    }, {
      "title" : "A cascade ranking model for efficient ranked retrieval",
      "author" : [ "L. Wang", "J.J. Lin", "D. Metzler" ],
      "venue" : "In ACM Conference on Research and Development in Information Retrieval (SIGIR),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Learning to rank with selection bias in personal search",
      "author" : [ "X. Wang", "M. Bendersky", "D. Metzler", "M. Najork" ],
      "venue" : "In ACM Conference on Research and Development in Information Retrieval (SIGIR)",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Beyond ranking: Optimizing whole-page presentation",
      "author" : [ "Y. Wang", "D. Yin", "L. Jie", "P. Wang", "M. Yamada", "Y. Chang", "Q. Mei" ],
      "venue" : "In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining, WSDM ’16,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "Beyond position bias: examining result attractiveness as a source of presentation bias in clickthrough data",
      "author" : [ "Y. Yue", "R. Patel", "H. Roehrig" ],
      "venue" : "In International Conference on World Wide Web (WWW),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In particular, the order of presentation has a strong influence on where users click [11].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "Second, treating clicks as preferences between clicked and skipped documents has been found to be accurate [9, 11], but it can only infer preferences that oppose the presented order.",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "Second, treating clicks as preferences between clicked and skipped documents has been found to be accurate [9, 11], but it can only infer preferences that oppose the presented order.",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "This again leads to severely biased data, and learning algorithms trained with these preferences tend to reverse the presented order unless additional heuristics are used [9].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "Third, probabilistic click models (see [4]) have been used to model how users produce clicks, and they can take position and context biases into account.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "Fourth, allowing the LTR algorithm to randomize what is presented to the user, like in online learning algorithms [16, 6] and batch learning from bandit feedback (BLBF) [24] can overcome the problem of bias in click data in a principled manner.",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "Fourth, allowing the LTR algorithm to randomize what is presented to the user, like in online learning algorithms [16, 6] and batch learning from bandit feedback (BLBF) [24] can overcome the problem of bias in click data in a principled manner.",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "Fourth, allowing the LTR algorithm to randomize what is presented to the user, like in online learning algorithms [16, 6] and batch learning from bandit feedback (BLBF) [24] can overcome the problem of bias in click data in a principled manner.",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "By drawing on counterfactual estimation techniques from causal inference [8], we first develop a provably unbiased estimator for evaluating ranking performance using biased feedback data.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "For example, in a cascade model [5], users are assumed to sequentially go down a ranking and click on a document if it is relevant.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "Learning from these relative preferences lowers the impact of some biases [9].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "Other click models ([5, 3, 1], also see [4]) have been proposed, and are trained to maximize log-likelihood of observed clicks.",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "Other click models ([5, 3, 1], also see [4]) have been proposed, and are trained to maximize log-likelihood of observed clicks.",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "Other click models ([5, 3, 1], also see [4]) have been proposed, and are trained to maximize log-likelihood of observed clicks.",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "Other click models ([5, 3, 1], also see [4]) have been proposed, and are trained to maximize log-likelihood of observed clicks.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "For instance, randomizing documents across all ranks lets us learn unbiased relevances for each document, and swapping neighboring pairs of documents [15] lets us learn reliable pairwise preferences.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "Similarly, randomized interleaving can detect preferences between different rankers reliably [2].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 28,
      "context" : "Different from online learning via bandit algorithms and interleaving [29, 21], batch learning from bandit feedback (BLBF) [24] still uses randomization during feedback collection, and then performs offline learning.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 20,
      "context" : "Different from online learning via bandit algorithms and interleaving [29, 21], batch learning from bandit feedback (BLBF) [24] still uses randomization during feedback collection, and then performs offline learning.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "Different from online learning via bandit algorithms and interleaving [29, 21], batch learning from bandit feedback (BLBF) [24] still uses randomization during feedback collection, and then performs offline learning.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "Our approach uses inverse propensity scoring (IPS), originally employed in causal inference from observational studies [18], and more recently also in whole page optimization [28], IR evaluation with manual judgments [19], and recommender evaluation [12, 20].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 27,
      "context" : "Our approach uses inverse propensity scoring (IPS), originally employed in causal inference from observational studies [18], and more recently also in whole page optimization [28], IR evaluation with manual judgments [19], and recommender evaluation [12, 20].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "Our approach uses inverse propensity scoring (IPS), originally employed in causal inference from observational studies [18], and more recently also in whole page optimization [28], IR evaluation with manual judgments [19], and recommender evaluation [12, 20].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 11,
      "context" : "Our approach uses inverse propensity scoring (IPS), originally employed in causal inference from observational studies [18], and more recently also in whole page optimization [28], IR evaluation with manual judgments [19], and recommender evaluation [12, 20].",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 19,
      "context" : "Our approach uses inverse propensity scoring (IPS), originally employed in causal inference from observational studies [18], and more recently also in whole page optimization [28], IR evaluation with manual judgments [19], and recommender evaluation [12, 20].",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 4,
      "context" : "We use randomized interventions similar to [5, 23, 27] to estimate propensities in a position discount model.",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "We use randomized interventions similar to [5, 23, 27] to estimate propensities in a position discount model.",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "We use randomized interventions similar to [5, 23, 27] to estimate propensities in a position discount model.",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "Unlike the uniform ranking randomization of [27] (with its high performance impact) or swapping adjacent pairs as in [5], we swap documents in different ranks to the top position randomly as in [23].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "Unlike the uniform ranking randomization of [27] (with its high performance impact) or swapping adjacent pairs as in [5], we swap documents in different ranks to the top position randomly as in [23].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "Unlike the uniform ranking randomization of [27] (with its high performance impact) or swapping adjacent pairs as in [5], we swap documents in different ranks to the top position randomly as in [23].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 26,
      "context" : "Finally, our approach is similar in spirit to [27], where propensity-weighting is used to correct for selection bias when discarding queries without clicks during learning-torank.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "using appropriate click models to estimate the propensity of each click rather than the propensity for a query to receive a click as in [27].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : "A common learning strategy is Empirical Risk Minimization (ERM) [25], which corresponds to picking the system Ŝ ∈ S that optimizes the empirical risk",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "There are several LTR algorithms that follow this approach (see [14]), and we use SVM-Rank [9] as a representative algorithm in this paper.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "There are several LTR algorithms that follow this approach (see [14]), and we use SVM-Rank [9] as a representative algorithm in this paper.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "First, since it is clearly impossible to get explicit judgments for all documents, pooling techniques [22] are used such that only the most promising documents are judged.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "In particular, implicit feedback is distorted by presentation bias, and it is not missing completely at random [13].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "It closely follows [19], which unifies several prior works on evaluating information retrieval systems.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "Using this counterfactual modeling setup, we can get an unbiased estimate of ∆(y|xi, ri) for any new ranking y (typically different from the presented ranking ȳi) via the inverse propensity scoring (IPS) estimator [7, 18, 8]",
      "startOffset" : 214,
      "endOffset" : 224
    }, {
      "referenceID" : 17,
      "context" : "Using this counterfactual modeling setup, we can get an unbiased estimate of ∆(y|xi, ri) for any new ranking y (typically different from the presented ranking ȳi) via the inverse propensity scoring (IPS) estimator [7, 18, 8]",
      "startOffset" : 214,
      "endOffset" : 224
    }, {
      "referenceID" : 7,
      "context" : "Using this counterfactual modeling setup, we can get an unbiased estimate of ∆(y|xi, ri) for any new ranking y (typically different from the presented ranking ȳi) via the inverse propensity scoring (IPS) estimator [7, 18, 8]",
      "startOffset" : 214,
      "endOffset" : 224
    }, {
      "referenceID" : 7,
      "context" : ", unconfoundedness, see [8]).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 24,
      "context" : "Finally, using standard results from statistical learning theory [25], consistency of the empirical risk paired with capacity control implies consistency also for ERM.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : "For simplicity, we consider a straightforward examination model analogous to [17], where a click on a search result depends on the probability that a user examines a result (i.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "These examination probabilities can model presentation bias documented in eye-tracking studies [11], where users are more likely to see results at the top of the ranking than those further down.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "The following shows that we can get estimates using data from a simple intervention similar to [27], but without the strong negative impact of presenting uniformly random results to some users.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "This also relates to the Click@1 metric proposed by [3].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "This swap-intervention experiment is of much lower impact than the uniform randomization proposed in [27] for a different propensity estimation problem, and careful consideration of which rank k to choose can further reduce impact of the swap experiment.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "First, one could extend the model by incorporating other biases, for example, trust bias [11] which affects perceived relevance of a result based on its position in the ranking.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 29,
      "context" : "Furthermore, it is possible to model saliency biases [30] by replacing the pr with a regression function.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : ", cascade model [5] and others [5, 3, 1, 4]) can be adapted as propensity models.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : ", cascade model [5] and others [5, 3, 1, 4]) can be adapted as propensity models.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : ", cascade model [5] and others [5, 3, 1, 4]) can be adapted as propensity models.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : ", cascade model [5] and others [5, 3, 1, 4]) can be adapted as propensity models.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : ", cascade model [5] and others [5, 3, 1, 4]) can be adapted as propensity models.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "For example, the feedback may be explicit star ratings in a movie recommendation system, and the propensities may be the results of self-selection by the users as in [20].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 8,
      "context" : "It is based on SVM-Rank [9, 10], but we conjecture that propensity-weighted versions of other LTR methods can be derived as well.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "It is based on SVM-Rank [9, 10], but we conjecture that propensity-weighted versions of other LTR methods can be derived as well.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : "Typically, Yj contains a few hundred documents – selected by a stage-one ranker [26] – that we aim to rerank.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "We can solve this type of Quadratic Program efficiently via a one-slack formulation [10], and we are using SVMRank with appropriate modifications to include IPS weights 1/qj .",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "It is equivalent to a standard ranking SVM [9], but is most easily explained as equivalent to Propensity SVM-Rank with all qj set to 1.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "This can be remedied using techniques like “propensity clipping” [23], where small propensities are clipped to some threshold value τ to trade bias for variance.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "We fielded these learnt weight vectors in two online interleaving experiments [2], the first comparing Propensity SVM-Rank against Prod and the second comparing Propensity SVM-Rank against Naive SVM-Rank.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "Third, we conjecture that a Propensity ERM approach can be developed also for pointwise LTR methods using techniques from [19], and possibly even for listwise LTR.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "First, they can eliminate pooling bias, since the use of sampling during judgment elicitation puts us in a controlled setting where propensities are known (and can be optimized [19]) by design.",
      "startOffset" : 177,
      "endOffset" : 181
    } ],
    "year" : 2016,
    "abstractText" : "Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of data in human-interactive systems. While implicit feedback has many advantages (e.g., it is inexpensive to collect, user centric, and timely), its inherent biases are a key obstacle to its effective use. For example, position bias in search rankings strongly influences how many clicks a result receives, so that directly using click data as a training signal in Learning-to-Rank (LTR) methods yields sub-optimal results. To overcome this bias problem, we present a counterfactual inference framework that provides the theoretical basis for unbiased LTR via Empirical Risk Minimization despite biased data. Using this framework, we derive a Propensity-Weighted Ranking SVM for discriminative learning from implicit feedback, where click models take the role of the propensity estimator. In contrast to most conventional approaches to de-bias the data using click models, this allows training of ranking functions even in settings where queries do not repeat. Beyond the theoretical support, we show empirically that the proposed learning method is highly effective in dealing with biases, that it is robust to noise and propensity model misspecification, and that it scales efficiently. We also demonstrate the real-world applicability of our approach on an operational search engine, where it substantially improves retrieval performance.",
    "creator" : "LaTeX with hyperref package"
  }
}