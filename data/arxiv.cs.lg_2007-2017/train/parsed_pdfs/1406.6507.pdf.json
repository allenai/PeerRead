{
  "name" : "1406.6507.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Weakly-supervised Discovery of Visual Pattern Configurations",
    "authors" : [ "Hyun Oh Song", "Yong Jae Lee", "Stefanie Jegelka", "Trevor Darrell" ],
    "emails" : [ "song@eecs.berkeley.edu", "yjlee22@eecs.berkeley.edu", "stefje@eecs.berkeley.edu", "trevor@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The growing amount of sparsely and noisily labeled image data promotes the need for robustly learning detection methods that can cope with a minimal amount of supervision. A prominent example of this scenario is the abundant availability of labels at the image level (i.e., whether a certain object is present in the image or not), while detailed annotations of the exact location of the object are tedious and expensive and, consequently, scarce. Learning methods that handle imagelevel labels circumvent the need for such detailed annotations and therefore have the potential to effectively utilize the massive and ever-growing textually annotated visual data available on the Web. In addition, such weakly supervised methods can be more robust than fully supervised ones if the detailed annotations are noisy or ill-defined.\nMotivated by these developments, recent work has explored learning methods that decreasingly rely on strong supervision. Early ideas for weakly supervised detection [30, 9] paved the way by successfully learning part-based object models, albeit on simple object-centric datasets (e.g., Caltech-101). Since then, a number of approaches [19, 24, 27] have attempted to learn models on more realistic and challenging data sets that feature large intra-category appearance variations and background clutter. To cope with those difficulties, these methods typically generate multiple candidate regions and retain the ones that occur most frequently in the positively labeled images. However, due to intra-category variations and deformations, the identified (single) patches often correspond to only a part of the object, such as a human face instead of the entire body. Such mislocalizations are a frequent problem for weakly supervised detection methods. Figure 4 illustrates some examples.\nMislocalization and too large or too small bounding boxes are problematic in two respects. First, they obviously affect the accuracy of the detection method. Detection is commonly phrased as multiple instance learning and addressed with non-convex optimization methods that alternatingly guess the location of the objects as positive examples (since the true location is unknown) and train a detector based on those guesses. Such methods are therefore heavily influenced by good initial localizations. Second, a common approach is to train the detector in stages, while adding informative “hard” negative examples to the training data. If we are not given accurate true object localizations in the training data, these hard examples must be derived from the detections identified in earlier rounds, and these initial detections may only use image-level annotations. The higher the accuracy\nar X\niv :1\n40 6.\n65 07\nv1 [\ncs .C\nV ]\n2 5\nJu n\nof the initial localizations, the more informative is the augmented training data – and this is key to the accuracy of the final learned model.\nIn this work, we address the issue of mislocalizations by identifying characteristic, discriminative configurations of multiple patches (rather than a single one). This part-based approach is motivated by the observation that automatically discovered single “discriminative” patches often correspond to object parts. In addition, wrong background patches (e.g., patches of water or sky) occur throughout the positive images, but do not re-occur in typical configurations. In particular, we propose an effective method that takes as input a set of images with labels of the form “the object is present”/“not present”, and automatically identifies characteristic part configurations of the given object.\nTo identify such co-occurrences, we use two main criteria. First, useful patches are discriminative, i.e., they occur in many positively labeled images, but not in the negatively labeled ones. To identify such patches, we use a discriminative covering formulation similar to [27]. However, our end goal is to discover multiple patches in each image that represent different parts, i.e., they may be close but may not be overlapping too much. In covering formulations, one may discourage overlaps by saying that for two overlapping regions, one “covers” the other, i.e., they are treated as identical. But this is a transitive relation, and the density of possible regions in detection would imply that all regions are identical, strongly discouraging the selection of more than one part per image. Partial covers face the challenge of scale invariance. Hence, we take a different approach and formulate an independence constraint. This second criterion ensures that we select regions that may be close but non-redundant and not fully overlapping. We show that this constrained selection problem corresponds to maximizing a submodular function subject to a matroid intersection constraint, which leads to approximation algorithms with theoretical worst-case bounds. Given candidate parts identified by those two criteria, we effectively find frequently co-occurring configurations that take into account relative position, scale and viewpoint.\nWe demonstrate multiple benefits of the discovered configurations. First, we observe that combinations of patches can produce more accurate spatial coverage of the full object, especially when the most discriminative pattern corresponds to an object part. Second, any overlapping region between the co-occurring visual patterns is likely to cover a part (but not the full) of the object of interest (see intersecting regions between green and yellow boxes in Figure 5); thus, they can be used to generate very informative hard negatives for training, and those can reduce localization errors at test time.\nIn short, our main contribution is a novel weakly-supervised object detection method that automatically discovers frequent configurations of discriminative visual patterns and exploits them for training more robust object detectors. In our experiments on the challenging PASCAL VOC dataset, we find the inclusion of our discriminative, automatically detected configurations to outperform all state-of-the-art methods."
    }, {
      "heading" : "1.1 Related work",
      "text" : "Weakly-supervised object detection. Training object detectors is usually done in a fully-supervised fashion using tight bounding box annotations that cover the object of interest (e.g., [8]). To reduce laborious bounding box annotation costs, recent weakly-supervised approaches [30, 9, 19, 24, 27] train detectors using binary object-presence labels without any object-location information.\nEarly efforts [30, 9] focused on simple datasets that have a single prominent object in each image (e.g., Caltech-101). More recent approaches [19, 24, 27] focus on the more challenging PASCAL dataset, which contains multiple objects in each image and large intra-category appearance variations. Of these, Song et al. [27] achieve state-of-the-art results by finding discriminative image patches that occur frequently in the positive images but rarely in the negative images using deep Convolutional Neural Network (CNN) features [15] and a submodular cover formulation. We use a similar approach to identify discriminative patches. But, contrary to [27] who assume patches to contain entire objects, we allow patches to contain full objects or merely object parts. Thus, we aim to automatically piece together those patches to produce better full-object estimates. To this end, we augment the covering formulation and identify patches that are both representative and explicitly mutually different. We will see that this leads to more robust object estimates and further allows our system to intelligently select “hard negatives” (mislocalized objects), both of which improve detection performance.\nVisual data mining. Existing approaches discover high-level object categories [26, 12, 5], mid-level patches [22, 3, 14], or low-level foreground features [16] by grouping similar visual patterns (i.e., images, patches, or contours) according to their texture, color, shape, etc. Recent methods [3, 14] use weakly-supervised labels to discover discriminative visual patterns. We use related ideas, but formulate the problem as a submodular optimization over matroids, which leads to approximation algorithms with theoretical worst-case guarantees. Covering formulations have also been used in [1, 2], but after running a trained object detector. An alternative discriminative approach, but less scalable than covering, uses spectral methods [32].\nModeling co-occurring visual patterns. Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17]. Co-occurring patterns are usually represented as doublets [22], higher-order constellations [30, 9] or star-shaped models [8]. Among these, our work is most inspired by [30, 9], which learns part-based models using only weak-supervision. However, we use more informative features and a different formulation, and show results on more difficult datasets. Our work is also related by [17], which discovers high-level object compositions (“visual phrases” [6]) using ground-truth bounding box annotations. In contrast to [17], we aim to discover part compositions to represent full objects and do so without any bounding box annotations."
    }, {
      "heading" : "2 Approach",
      "text" : "Our goal is to find a discriminative set of parts or patches that co-occur in many of the positively labeled images in the same configuration. We address this goal in two steps. First, we find a set of patches that are discriminative, i.e., they tend to occur mainly in positive images. Second, we use an efficient approach to find co-occurring configurations of pairs of such patches. Our approach easily extends beyond pairs; for simplicity and to retain configurations that occur frequently enough, we here restrict ourselves to pairs.\nDiscriminative candidate patches. For identifying discriminative patches, we begin with a construction similar to that of Song et al. [27]. Let P be the set of positively labeled images. Each image I contains candidate boxes {bI,1, . . . , bI,m} found via selective search [28]. For each bI,i, we find its closest matching neighbor bI′,j in each other image I ′ (regardless of the image label). The K closest of those neighbors form the neighborhood N (bI,i), the remaining ones are discarded. Discriminative patches will have neighborhoods mainly within images in P , i.e., if B(P) is the set of all patches from images inP , thenN (b)∩B(P) ≈ K. To identify a small, diverse and representative set of such patches, like [27], we construct a bipartite graph G = (U ,V, E), where both U and V contain copies of B(P). Each patch b ∈ V is connected to the copy of its nearest neighbors in U – these will be K or less, depending on whether the K nearest neighbors of b occur in B(P) or in negatively labeled images. The most representative patches maximize the covering function\nF (S) = |Γ(S)|, (1) where Γ(S) = {u ∈ U | (b, u) ∈ E for some b ∈ S} is the neighborhood of S ⊆ V in the bipartite graph. Figure 1 shows a cartoon illustration. The function F is monotone and submodular, and the C maximizing elements (for a given C) can be selected greedily [18].\nHowever, if we aim to find part configurations, we must select multiple, jointly informative patches per image. Patches selected to merely maximize coverage can still be redundant, since the most frequently occurring ones are often highly overlapping. A straightforward modification would be to treat highly overlapping patches as identical. This identification would still admit a submodular covering model as in Equation (1). But, in our case, the candidate patches are very densely packed in the image, and, by transitivity, we would have to make all of them identical. In consequence, this would completely rule out the selection of more than one patch in an image and thereby prohibit the discovery of any co-occurring configurations. Therefore, we take a different approach, differing from [27] whose goal is to identify single patches, and not part-based configurations. We constrain our selection such that no two patches b, b′ ∈ V can be picked whose neighborhoods overlap by more than a fraction of θ. By overlap, we mean that the patches in the neighborhoods of b, b′ overlap significantly (they need not be identical). This notion of diversity is reminiscent of NMS and similar to that in [3], but we here phrase and analyze it as a constrained submodular optimization problem. Our constraint can be expressed in terms of a different graph GC = (V, EC) with nodes V . In GC , there is an edge between b and b′ if their\nneighborhoods overlap prohibitively, as illustrated in Figure 1. Our family of feasible solutions is\nM = {S ⊆ V | ∀ b, b′ ∈ S there is no edge (b, b′) ∈ EC}. (2) In other words,M is the family of all independent sets in GC . We aim to maximize\nmaxS⊆V F (S) s.t. S ∈M. (3) This problem is NP-hard. We solve it approximately via the following greedy algorithm. Begin with S0 = ∅, and, in iteration t, add b ∈ argmaxb∈V\\S |Γ(b) \\ Γ(St−1)|. As we add b, we delete all of b’s neighbors in GC from V . We continue until V = ∅. If the neighborhoods Γ(b) are disjoint, then this algorithm amounts to the following simplified scheme: we first sort all b ∈ V in non-increasing order by their degree Γ(b), i.e., their number of neighbors in B(P), and visit them in this order. We always add the currently highest b in the list to S, then delete it from the list, and with it all its immediate (overlapping) neighbors. The following lemma states an approximation factor for the greedy algorithm, where ∆ is the maximum degree of any node in GC . Lemma 1. The solution Sg returned by the greedy algorithm is a 1/(∆ + 2) approximation for Problem (2): F (Sg) ≥ 1∆+2F (S∗). If Γ(b) ∩ Γ(b′) = ∅ for all b, b′ ∈ V , then the worst-case approximation factor is 1/(∆ + 1).\nThe proof relies on phrasingM as an intersection of matroids. Definition 1 (Matroid). A matroid (V, Ik) consists of a ground set V and a family Ik ⊆ 2V of “independent sets” that satisfy three axioms: (1) ∅ ∈ Ik; (2) downward closedness: if S ∈ Ik then T ∈ Ik for all T ⊆ S; and (3) the exchange property: if S, T ∈ Ik and |S| < |T |, then there is an element v ∈ T \\ S such that S ∪ {v} ∈ Ik.\nProof. (Lemma 1) We will argue that Problem (2) is the problem of maximizing a monotone submodular function subject to the constraint that the solution lies in the intersection of ∆+1 matroids. With this insight, the approximation factor of the greedy algorithm for submodular F follows from [10] and that for non-intersecting Γ(b) from [13], since in the latter case the problem is that of finding a maximum weight vector in the intersection of ∆ + 1 matroids.\nIt remains to argue thatM is an intersection of matroids. Our matroids will be partition matroids (over the ground set V) whose independent sets are of the formMk = {S | |S ∩ e| ≤ 1, for all e ∈ Ek}. To define those, we partition the edges in GC into disjoint sets Ek, i.e., no two edges in Ek share a common node. The Ek can be found by an edge coloring – one Ek andMk for each color k. By Vizing’s theorem [29], we need at most ∆ + 1 colors. The matroidMk demands that for each edge e ∈ Ek, we may only select one of its adjacent nodes. All matroids together say that for any edge e ∈ E , we may only select one of the adjacent nodes, and that is the constraint in Equation (2), i.e. M = ⋂∆+1k=1 Mk. We do not ever need to explicitly compute Ek andMk; all we need to do is check membership in the intersection, and this is equivalent to checking whether a set S is an independent set in GC , which is done by the deletions in the algorithm.\nFrom the constrained greedy algorithm, we obtain a set S ⊂ V of discriminative patches. Together with its neighborhood Γ(b), each patch b ∈ V forms a representative cluster. Figure 2 shows some example patches derived from the labels “aeroplane” and “motorbike”. The discovered patches intuitively look like “parts” of the objects, and are frequent but sufficiently different.\nFinding frequent configurations. The next step is to find frequent configurations of co-occurring clusters, e.g., the head patch of a person on top of the torso patch, or a bicycle with visible wheels. A “configuration” consists of patches from two clusters Ci, Cj , their relative location, and their viewpoint and scale. In practice, we give preference to pairs that by themselves are very relevant and maximize a weighted combination of co-occurrence count and coverage max{Γ(Ci),Γ(Cj)}. All possible configurations of all pairs of patches amount to too many to explicitly write down and count. Instead, we follow an efficient procedure for finding frequent configurations. Our approach is inspired by [17], but does not require any supervision. We first find configurations that occur in at least two images. To do so, we consider each pair of images I1, I2 that have at least two co-occurring clusters. For each correspondence of cluster patches across the images, we find a corresponding transform operation (translation, rescale, viewpoint change). This results in a point in a 4D transform space, for each cluster correspondence. We quantize this space into B bins. Our candidate configurations will be pairs of cluster correspondences ((bI1,1, bI2,1), (bI1,2, bI2,2)) ∈ (Ci×Ci), (Cj×Cj) that fall in the same bin, i.e., share the same transform, and have the same relative location. Between a given pair of images, there can be multiple such pairs of correspondences. We keep track of those via a multi-graph GP = (P, EP ) that has a node for each image I ∈ P . For each correspondence ((bI1,1, bI2,1), (bI1,2, bI2,2)) ∈ (Ci × Ci), (Cj × Cj), we draw an edge (I1, I2) and label it by the clusters Ci, Cj and the common relative position. As a result, there can be multiple edges (I1, Ij) in GP with different edge labels. The most frequently occurring configuration can now be read out by finding the largest connected component in GP induced by retaining only edges with the same label. We use the largest component(s) as the characteristic configurations for a given image label (object class). If the component is very small, then there is not enough information to determine co-occurrences, and we simply use the most frequent single cluster. (This may also be determined by a statistical test.) The final single “correct” localization will be the smallest bounding box that contains the full configuration.\nDiscovering mislocalized hard negatives. Discovering frequent configurations can not only lead to better localization estimates of the full object, but they can also be used to generate mislocalized estimates as “hard negatives” when training the object detector. We exploit this idea as follows. Let b1, b2 be a discovered configuration within a given image. These patches typically constitute parts or, in some cases, a part and the full object. Our foreground estimate is the smallest box that includes both b1 and b2. Hence, any region within the foreground estimate that does not overlap simultaneously with both b1 and b2 will capture only a fragment of the foreground object. We extract the four largest such rectangular regions (see white boxes in Figure 3) as “hard” negative examples.\nSpecifically, we parameterize any rectangular region with [xl, xr, yt, yb], i.e., its x-left, x-right, y-top, and y-bottom coordinate values. Let the bounding box of bi be [xli, x r i , y t i , y b i ], and foreground estimate [x l f , x r f , y t f , y b f ], and let x l = max(xl1, x l 2), x r = min(xr1, x r 2), y t = max(yt1, y t 2), y b = min(yb1, y b 2). We generate four hard negatives: [xlf , x l, ybf , y t f ], [x r, xrf , y b f , y t f ], [x l f , x r f , y t f , y t], [xlf , x r f , y b, ybf ]. If either b1 or b2 is very small\nin size relative to the foreground, the resulting hard negatives can have high overlap with the foreground, which will introduce undesirable noise (false negatives) when training the detector. Thus, we shrink any hard negative that overlaps with the foreground estimate by more than 50%, until its overlap is 50% (we adjust the boundary that does not coincide with any of the foreground estimation boundaries).\nFinally, it is important to note that simply taking arbitrary rectangular regions that overlap with the foreground estimation box by some threshold will not always generate useful hard negatives. If the overlap threshold is too low, the selected regions will be uninformative, and if the overlap threshold is too high, the selected regions will cover too much of the foreground. Our approach selects informative hard negatives more robustly by ruling out the overlapping region between the configuration patches, which is very likely be part of the foreground object.\nMining positives and training the detector. While the discovered configurations typically lead to better foreground localization, their absolute count can be relatively low compared to the total number of positive images. This is due to inaccuracies in the initial patch discovery stage: for a frequent configuration to be discovered, both of its patches must be accurately found. Thus, we also mine additional positives from the set of remaining positive images P ′ that did not produce any of the discovered configurations.\nTo do so, we train an initial object detector, using the foreground estimates derived from our discovered configurations as positive examples, and the corresponding discovered hard negative regions as negatives. In addition, we mine negative examples as in [8]. We run the detector on all selective search regions in P and retain the region with the highest detector score as an additional positive training example. Our final detector is trained on this augmented training data, and iteratively improved by latent SVM (LSVM) updates (see [8, 27] for details)."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we analyze (1) detection performance of the models trained with the discovered configurations, and (2) impact of the discovered hard negatives on detection performance.\nImplementation details. We employ a recent region based detection framework [11, 27] and use the same fc7 features from the CNN model [4] on region proposals [28] throughout the experiment. For discriminative patch discovery, we use K = |P|/2, θ = K/20. For correspondence detection, we discretize the 4D transform space of {x: relative horizontal shift, y: relative vertical shift, s: relative scale, p: relative aspect ratio} with ∆x = 30 px,∆y = 30 px,∆s = 1 px/px,∆p = 1 px/px. We choose this binning scheme by visually examining few qualitative examples so that scale, aspect ratio agreement between the two paired instances are more strict, while their translation agreement is more loose in order to handle deformable objects. More details regarding be transform space binning can be found in [20].\nDiscovered configurations. Figure 5 qualitatively illustrates discovered configurations (green and yellow boxes) and foreground estimates (magenta boxes) that have high degree in graph GP for all classes in the PASCAL dataset. Our method consistently finds meaningful combinations such as a wheel and body of bicycles, face and torso of people, locomotive basement and upper body parts of trains/buses, window and body frame of cars, etc. Some failures include cases where the algorithm latches onto different objects co-occurring in consistent configurations such as the lamp and sofa combination (right column, second row from the bottom in Figure 5).\nWeakly-supervised object detection. Following the evaluation protocol of the PASCAL VOC dataset, we report detection results on the PASCAL test set using detection average precision. For a\ndirect comparison with the state-of-the-art weakly-supervised object detection method [27], we do not use the extra instance level annotations such as pose, difficult, truncated and restrict the supervision to the image level object presence annotations. Table 1 compares our detection results against two baseline methods [23, 27] which report the result on the full dataset. As shown in Table 1, our method improves detection performance on the majority of the classes (consistent improvement on rigid man-made object classes). It is worth noting that our method shows significant improvement on the person class (arguably most important category in the PASCAL dataset). Figure 4 shows some example high scoring detection results on the test set.\nImpact of discovered hard negatives. To analyze the effect of our discovered hard negatives, we compare to two baseline cases: (1) not adding any negative examples from positives images (2) adding image regions around the foreground estimate as conventionally implemented in fully supervised object detection algorithms [7, 11]. We use the criterion from [11], where all image regions in positive images with overlap score (intersection area over union area with respect to foreground regions) less than 0.3 are used as “neighboring” negative image regions on positive images. Table 2 shows the effect of our hard negative examples in terms of detection average precision, for all classes (mAP). The experiment shows that adding “neighboring negative regions” does not lead to noticeable improvement over not adding any negative regions from positive images, while adding our automatically discovered hard negative regions improves the detection performance more substantially.\nConclusion. We presented a novel weakly-supervised object detection method that discovers frequent configurations of discriminative visual patterns. We showed that the discovered configurations provide more accurate spatial coverage of the full object and provide a way to generate useful hard negatives. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset."
    } ],
    "references" : [ {
      "title" : "On detection of multiple object instances using hough transforms",
      "author" : [ "O. Barinova", "V. Lempitsky", "P. Kohli" ],
      "venue" : "IEEE TPAMI,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Active detection via adaptive submodularity",
      "author" : [ "Y. Chen", "H. Shioi", "C. Fuentes-Montesinos", "L. Koh", "S. Wich", "A. Krause" ],
      "venue" : "ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "What Makes Paris Look like Paris",
      "author" : [ "C. Doersch", "S. Singh", "A. Gupta", "J. Sivic", "A.A. Efros" ],
      "venue" : "In SIGGRAPH,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "arXiv e-prints, arXiv:1310.1531 [cs.CV],",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Clustering by Composition Unsupervised Discovery of Image Categories",
      "author" : [ "A. Faktor", "M. Irani" ],
      "venue" : "ECCV,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Recognition Using Visual Phrases",
      "author" : [ "A. Farhadi", "A. Sadeghi" ],
      "venue" : "CVPR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A Discriminatively Trained, Multiscale, Deformable Part Model",
      "author" : [ "P. Felzenszwalb", "D. McAllester", "D. Ramanan" ],
      "venue" : "CVPR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Object Detection with Discriminatively Trained Part Based Models",
      "author" : [ "P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan" ],
      "venue" : "TPAMI, 32(9),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Object Class Recognition by Unsupervised Scale-Invariant Learning",
      "author" : [ "R. Fergus", "P. Perona", "A. Zisserman" ],
      "venue" : "CVPR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functions - II",
      "author" : [ "M. Fisher", "G. Nemhauser", "L. Wolsey" ],
      "venue" : "Math. Prog. Study, 8:73–87,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "arXiv e-prints,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Unsupervised learning of categories from sets of partially matching image features",
      "author" : [ "K. Grauman", "T. Darrell" ],
      "venue" : "CVPR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The efficacy of the “greedy” algorithm",
      "author" : [ "T. Jenkyns" ],
      "venue" : "Proc. of 7th South Eastern Conference on Combinatorics, Graph Theory and Computing, pages 341–350,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Blocks that Shout: Distinctive Parts for Scene Classification",
      "author" : [ "M. Juneja", "A. Vedaldi", "C.V. Jawahar", "A. Zisserman" ],
      "venue" : "CVPR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "A. Krizhevsky", "I.S.G. Hinton" ],
      "venue" : "NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Foreground Focus: Unsupervised Learning From Partially Matching Images",
      "author" : [ "Y.J. Lee", "K. Grauman" ],
      "venue" : "IJCV, 85,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Automatic Discovery of Groups of Objects for Scene Understanding",
      "author" : [ "C. Li", "D. Parikh", "T. Chen" ],
      "venue" : "CVPR,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functions—I",
      "author" : [ "G. Nemhauser", "L. Wolsey", "M. Fisher" ],
      "venue" : "Mathematical Programming, 14(1):265–294,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Scene recognition and weakly supervised object localization with deformable part-based models",
      "author" : [ "M. Pandey", "S. Lazebnik" ],
      "venue" : "ICCV,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "From Appearance to Context-Based Recognition: Dense Labeling in Small Images",
      "author" : [ "D. Parikh", "C.L. Zitnick", "T. Chen" ],
      "venue" : "CVPR,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficient Mining of Frequent and Distinctive Feature Configurations",
      "author" : [ "T. Quack", "V. Ferrari", "B. Leibe", "L.V. Gool" ],
      "venue" : "ICCV,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Unsupervised Discovery of Mid-level Discriminative Patches",
      "author" : [ "S. Singh", "A. Gupta", "A.A. Efros" ],
      "venue" : "ECCV,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Weakly supervised object detector learning with model drift detection",
      "author" : [ "P. Siva", "T. Xiang" ],
      "venue" : "ICCV,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "In defence of negative mining for annotating weakly labelled data",
      "author" : [ "P. Siva", "C. Russell", "T. Xiang" ],
      "venue" : "ECCV,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Video Data Mining Using Configurations of Viewpoint Invariant Regions",
      "author" : [ "J. Sivic", "A. Zisserman" ],
      "venue" : "CVPR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Discovering object categories in image collections",
      "author" : [ "J. Sivic", "B.Russell", "A.Efros", "A.Zisserman", "W.Freeman" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2005
    }, {
      "title" : "On learning to localize objects with minimal supervision",
      "author" : [ "H.O. Song", "R. Girshick", "S. Jegelka", "J. Mairal", "Z. Harchaoui", "T. Darrell" ],
      "venue" : "ICML,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Selective search for object recognition",
      "author" : [ "J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders" ],
      "venue" : "In IJCV,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "On an estimate of the chromatic class of a p-graph",
      "author" : [ "V. Vizing" ],
      "venue" : "Diskret. Analiz., 3:25–30,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Unsupervised Learning of Models for Recognition",
      "author" : [ "M. Weber", "M. Welling", "P. Perona" ],
      "venue" : "ECCV,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Efficient Kernels for Identifying Unbounded-order Spatial Features",
      "author" : [ "Y. Zhang", "T. Chen" ],
      "venue" : "CVPR,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Contrastive learning using spectral methods",
      "author" : [ "J. Zou", "D. Hsu", "D. Parkes", "R. Adams" ],
      "venue" : "NIPS,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Early ideas for weakly supervised detection [30, 9] paved the way by successfully learning part-based object models, albeit on simple object-centric datasets (e.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "Early ideas for weakly supervised detection [30, 9] paved the way by successfully learning part-based object models, albeit on simple object-centric datasets (e.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : "Since then, a number of approaches [19, 24, 27] have attempted to learn models on more realistic and challenging data sets that feature large intra-category appearance variations and background clutter.",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "Since then, a number of approaches [19, 24, 27] have attempted to learn models on more realistic and challenging data sets that feature large intra-category appearance variations and background clutter.",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "Since then, a number of approaches [19, 24, 27] have attempted to learn models on more realistic and challenging data sets that feature large intra-category appearance variations and background clutter.",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "To identify such patches, we use a discriminative covering formulation similar to [27].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : ", [8]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 29,
      "context" : "To reduce laborious bounding box annotation costs, recent weakly-supervised approaches [30, 9, 19, 24, 27] train detectors using binary object-presence labels without any object-location information.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "To reduce laborious bounding box annotation costs, recent weakly-supervised approaches [30, 9, 19, 24, 27] train detectors using binary object-presence labels without any object-location information.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "To reduce laborious bounding box annotation costs, recent weakly-supervised approaches [30, 9, 19, 24, 27] train detectors using binary object-presence labels without any object-location information.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : "To reduce laborious bounding box annotation costs, recent weakly-supervised approaches [30, 9, 19, 24, 27] train detectors using binary object-presence labels without any object-location information.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 26,
      "context" : "To reduce laborious bounding box annotation costs, recent weakly-supervised approaches [30, 9, 19, 24, 27] train detectors using binary object-presence labels without any object-location information.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 29,
      "context" : "Early efforts [30, 9] focused on simple datasets that have a single prominent object in each image (e.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "Early efforts [30, 9] focused on simple datasets that have a single prominent object in each image (e.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "More recent approaches [19, 24, 27] focus on the more challenging PASCAL dataset, which contains multiple objects in each image and large intra-category appearance variations.",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "More recent approaches [19, 24, 27] focus on the more challenging PASCAL dataset, which contains multiple objects in each image and large intra-category appearance variations.",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "More recent approaches [19, 24, 27] focus on the more challenging PASCAL dataset, which contains multiple objects in each image and large intra-category appearance variations.",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "[27] achieve state-of-the-art results by finding discriminative image patches that occur frequently in the positive images but rarely in the negative images using deep Convolutional Neural Network (CNN) features [15] and a submodular cover formulation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[27] achieve state-of-the-art results by finding discriminative image patches that occur frequently in the positive images but rarely in the negative images using deep Convolutional Neural Network (CNN) features [15] and a submodular cover formulation.",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 26,
      "context" : "But, contrary to [27] who assume patches to contain entire objects, we allow patches to contain full objects or merely object parts.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 25,
      "context" : "Existing approaches discover high-level object categories [26, 12, 5], mid-level patches [22, 3, 14], or low-level foreground features [16] by grouping similar visual patterns (i.",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "Existing approaches discover high-level object categories [26, 12, 5], mid-level patches [22, 3, 14], or low-level foreground features [16] by grouping similar visual patterns (i.",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "Existing approaches discover high-level object categories [26, 12, 5], mid-level patches [22, 3, 14], or low-level foreground features [16] by grouping similar visual patterns (i.",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "Existing approaches discover high-level object categories [26, 12, 5], mid-level patches [22, 3, 14], or low-level foreground features [16] by grouping similar visual patterns (i.",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "Existing approaches discover high-level object categories [26, 12, 5], mid-level patches [22, 3, 14], or low-level foreground features [16] by grouping similar visual patterns (i.",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "Existing approaches discover high-level object categories [26, 12, 5], mid-level patches [22, 3, 14], or low-level foreground features [16] by grouping similar visual patterns (i.",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "Existing approaches discover high-level object categories [26, 12, 5], mid-level patches [22, 3, 14], or low-level foreground features [16] by grouping similar visual patterns (i.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "Recent methods [3, 14] use weakly-supervised labels to discover discriminative visual patterns.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "Recent methods [3, 14] use weakly-supervised labels to discover discriminative visual patterns.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Covering formulations have also been used in [1, 2], but after running a trained object detector.",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Covering formulations have also been used in [1, 2], but after running a trained object detector.",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 31,
      "context" : "An alternative discriminative approach, but less scalable than covering, uses spectral methods [32].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 8,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 24,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 20,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 30,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 15,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 7,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 5,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 21,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 16,
      "context" : "Modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance [30, 9, 25, 21, 31, 16, 8, 6, 22, 17].",
      "startOffset" : 157,
      "endOffset" : 194
    }, {
      "referenceID" : 21,
      "context" : "Co-occurring patterns are usually represented as doublets [22], higher-order constellations [30, 9] or star-shaped models [8].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : "Co-occurring patterns are usually represented as doublets [22], higher-order constellations [30, 9] or star-shaped models [8].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Co-occurring patterns are usually represented as doublets [22], higher-order constellations [30, 9] or star-shaped models [8].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "Co-occurring patterns are usually represented as doublets [22], higher-order constellations [30, 9] or star-shaped models [8].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 29,
      "context" : "Among these, our work is most inspired by [30, 9], which learns part-based models using only weak-supervision.",
      "startOffset" : 42,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "Among these, our work is most inspired by [30, 9], which learns part-based models using only weak-supervision.",
      "startOffset" : 42,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "Our work is also related by [17], which discovers high-level object compositions (“visual phrases” [6]) using ground-truth bounding box annotations.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "Our work is also related by [17], which discovers high-level object compositions (“visual phrases” [6]) using ground-truth bounding box annotations.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "In contrast to [17], we aim to discover part compositions to represent full objects and do so without any bounding box annotations.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "[27].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : ", bI,m} found via selective search [28].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 26,
      "context" : "To identify a small, diverse and representative set of such patches, like [27], we construct a bipartite graph G = (U ,V, E), where both U and V contain copies of B(P).",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "The function F is monotone and submodular, and the C maximizing elements (for a given C) can be selected greedily [18].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 26,
      "context" : "Therefore, we take a different approach, differing from [27] whose goal is to identify single patches, and not part-based configurations.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "This notion of diversity is reminiscent of NMS and similar to that in [3], but we here phrase and analyze it as a constrained submodular optimization problem.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "With this insight, the approximation factor of the greedy algorithm for submodular F follows from [10] and that for non-intersecting Γ(b) from [13], since in the latter case the problem is that of finding a maximum weight vector in the intersection of ∆ + 1 matroids.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "With this insight, the approximation factor of the greedy algorithm for submodular F follows from [10] and that for non-intersecting Γ(b) from [13], since in the latter case the problem is that of finding a maximum weight vector in the intersection of ∆ + 1 matroids.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 28,
      "context" : "By Vizing’s theorem [29], we need at most ∆ + 1 colors.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "Our approach is inspired by [17], but does not require any supervision.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "In addition, we mine negative examples as in [8].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "Our final detector is trained on this augmented training data, and iteratively improved by latent SVM (LSVM) updates (see [8, 27] for details).",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 26,
      "context" : "Our final detector is trained on this augmented training data, and iteratively improved by latent SVM (LSVM) updates (see [8, 27] for details).",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "We employ a recent region based detection framework [11, 27] and use the same fc7 features from the CNN model [4] on region proposals [28] throughout the experiment.",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "We employ a recent region based detection framework [11, 27] and use the same fc7 features from the CNN model [4] on region proposals [28] throughout the experiment.",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "We employ a recent region based detection framework [11, 27] and use the same fc7 features from the CNN model [4] on region proposals [28] throughout the experiment.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "We employ a recent region based detection framework [11, 27] and use the same fc7 features from the CNN model [4] on region proposals [28] throughout the experiment.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "More details regarding be transform space binning can be found in [20].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : "[23] 13.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] 27.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "direct comparison with the state-of-the-art weakly-supervised object detection method [27], we do not use the extra instance level annotations such as pose, difficult, truncated and restrict the supervision to the image level object presence annotations.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "Table 1 compares our detection results against two baseline methods [23, 27] which report the result on the full dataset.",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "Table 1 compares our detection results against two baseline methods [23, 27] which report the result on the full dataset.",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "Green: our method, Red: [27]",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "To analyze the effect of our discovered hard negatives, we compare to two baseline cases: (1) not adding any negative examples from positives images (2) adding image regions around the foreground estimate as conventionally implemented in fully supervised object detection algorithms [7, 11].",
      "startOffset" : 283,
      "endOffset" : 290
    }, {
      "referenceID" : 10,
      "context" : "To analyze the effect of our discovered hard negatives, we compare to two baseline cases: (1) not adding any negative examples from positives images (2) adding image regions around the foreground estimate as conventionally implemented in fully supervised object detection algorithms [7, 11].",
      "startOffset" : 283,
      "endOffset" : 290
    }, {
      "referenceID" : 10,
      "context" : "We use the criterion from [11], where all image regions in positive images with overlap score (intersection area over union area with respect to foreground regions) less than 0.",
      "startOffset" : 26,
      "endOffset" : 30
    } ],
    "year" : 2014,
    "abstractText" : "The increasing prominence of weakly labeled data nurtures a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}