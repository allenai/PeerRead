{
  "name" : "1412.0854.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semantic HMC for Big Data Analysis",
    "authors" : [ "Thomas Hassan", "Rafael Peixoto", "Christophe Cruz", "Aurlie Bertaux", "Nuno Silva" ],
    "emails" : [ "thomas.hassan@checksem.fr", "rafpp@isep.ipp.pt", "christophe.cruz@u-bourgogne.fr", "aurelie.bertaux@iut-dijon.u-bourgogne.fr", "nps@isep.ipp.pt" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—classification; multi-classify; Big-Data; ontology; semantic technologies; machine learning\nI. INTRODUCTION\nNowadays, discovering knowledge and insights over web data is a major task for most corporations to increase their competitiveness. Determining the value of information relative to a particular customer is a complex task addressed by the business intelligence/data-mining field [1]. In the context of Big Data, this task is even more challenging, due to its characteristics. An increasing number of Vs has been used to characterize Big Data [2], [3]: Volume, Velocity, Variety and Value. Volume concerns the large amount of data that is generated and stored through the years by social media, sensor data, etc[2]. Velocity concerns both to the production and the process to meet a demand because Big Data is not only a huge volume of data but it must be processed quickly. Variety relates to the various types of data composing the Big Data. These types include semi-structured and unstructured data representing 90% of his content [4] such as audio, video, webpage, and text, as well as traditional structured data, etc. Value measures how valuable the information to a Big Data consumer is. Value is the most important feature of Big Data and his raison dłtre because if data dont have value then is useless. An IDC report [5] proposes the value extraction from very large volumes of a wide variety of data, by enabling the high-velocity capture, discovery, and/or analysis. Sheth [6] proposes deriving Value via harnessing the challenges posed by Volume, Variety, and Velocity using semantic techniques and technologies. This requires organized ways to harness and overcome the four V-challenges by using metadata and employ semantics and intelligent processing. Our aim is to extract Value from Big Data by harnessing a huge Volume and Variety of data that change constantly (Velocity) by using a\nnovel unsupervised ontology learning process based on HMC called Semantic HMC. Hierarchical Multi-Label Classification (HMC) is the combination of Multi-Label classification and Hierarchical classification [7]. In HMC, the items can be assigned to different hierarchical paths and simultaneously may belong to different class labels in the same hierarchical level [7]. The ontology [8] plays a key role in defining terms and meanings used to represent the knowledge, reducing the gap between the users and the HMC process. This paper does not aim to improve the state of the art in multi-classification, nor the automatic hierarchy construction. Instead it proposes a scalable process to semantically learn the ontology by adopting scalable machine learning processes and Rule-based reasoning [9] to classify the data items and therefore extract value from Big Data. The contributions of this work are twofold:\n• Scalable ontology learning process based on HMC (Semantic HMC).\n• Big Data Analysis using a Semantic HMC. The rest of the paper covers three sections. The second section describes how to use the Semantic HMC to extract value from Big Data. The third section describes the Semantic HMC process proposal. Finally, the last section draws conclusions and suggests further research."
    }, {
      "heading" : "II. USING SEMANTIC HMC TO DERIVE VALUE IN BIG DATA CONTEXT",
      "text" : "Our approach is to exploit value from very large volumes of data that are in constant generation using a Semantic HMC approach. The Semantic HMC process learns the Tbox (Taxonomy and Rules) part of the ontology from the huge Volume and Variety of initial data. Once this learning phase is finished, the classification system incrementally learns the Tbox from the new incoming items (Abox) to provide and respond to the Velocity (and the others V) dimension(s). The result of this Semantic HMC process is a rich ontology with the items (instances) classified according to the learned concept hierarchy (i.e. the taxonomy of the ontology). Corporations recurrently use concept hierarchies as taxonomies to represent their valuable information [10]. Our vision is to use the concept hierarchies from corporations to validate the value\nar X\niv :1\n41 2.\n08 54\nv1 [\ncs .A\nI] 2\nD ec\n2 01\nof the learned ontology for a specific corporation (Fig. 1). Higher similarity between the concept hierarchy of the learned ontology and the concept hierarchy used by a corporation suggest better alignment between the HMC results and the corporations knowledge and goals. Consequently, data items classified as valuable concepts ultimately present better value to the corporation than those not matching the corporations concepts."
    }, {
      "heading" : "III. SEMANTIC HMC PROCESS",
      "text" : "Our Semantic HMC process is generic for a large Variety of unstructured data items (e.g. text, images) and scalable for a large Volume of data. The process is unsupervised such that no previously labeled/classified examples or rules to relate the data items with the labels exist. The label (i.e. concepts) hierarchy and the rules are automatically learned from the data through scalable Machine Learning techniques. To infer the most specific concepts for each data item and all subsuming concepts we use rule-based reasoning that exhaustively applies a set of rules to a set of triples to infer conclusions [9], i.e. the items classifications. This Rule-based reasoning approach allows the parallelization and distribution of work by large clusters of inexpensive machines through Big Data technologies as Map-reduce [11]. Web Scale Reasoners [12] currently uses Rule-Based reasoning to reach high scalability by load parallelization and distribution, thus addressing the Velocity and Volume dimensions of Big Data. This proposed process consists of 5 individually scalable steps (Fig. 2) matching the requirements of Big Data processing:\n• Indexation parses and creates an index of data items. • Vectorization calculates term-frequency vectors of the\nindexed items. • Hierarchization creates a concept hierarchy based on term\nfrequency. • Resolution creates the reasoning rules to relate data items\nwith the hierarchy concepts based on term frequency. • Realization first populates the ontology with items and\nthen for each item determines the most specific hierarchy concept and all his subsuming concepts.\nA. Indexation\nThe indexation step parses and index data items. As one of our main focus points is the scalability of the architecture, the indexation is a mandatory step. Each item type has its specific parser to efficiently retrieve useful information for the other steps reducing the Limited Context Analysis problem. The Limited Content Analysis problem [13][14] is defined by the difficulty in extracting reliable automated information from various content (e.g. text, images, sound, etc.), which can greatly reduce the quality of the classifications. By reducing the Limited Content Analysis we improve the Semantic HMC capability to handle more Variety of data.\nB. Vectorization\nThe vectorization step vectorizes the terms in the indexed items by calculating two types of term frequency vectors :\n• Term frequency in each item using the frequency of a term in an item measured by TF-IDF. TF-IDF uses the frequency of a term in an item (TF) and the inverse number of items in which the term appears (IDF) [15]. • Term frequency in all items using the appearing frequency of a term in all documents [15].\nThe following steps use the term vectors calculated in this step to learn the concepts Hierarchy and the Rules."
    }, {
      "heading" : "C. Hierarchization",
      "text" : "The hierarchization step will select relevant terms as relevant concepts and also will generate the broader-narrower relations between these concepts. To select the concepts in the hierarchy, a quality measure must be used. There are several methods for creating hierarchical relations between concepts including [16], [17]:\n• Hierarchical clustering that starts with one cluster and progressively merges clusters that are closest.\n• Subsumption methods that construct the concept broadernarrower relations based on the co-occurrence of concepts.\nAny of these methods can be used to create the hierarchical relations. The advantages and drawbacks of each method is deeply studied in [16]."
    }, {
      "heading" : "D. Resolution",
      "text" : "The resolution process will create the ontology rules used to relate the hierarchy concepts and the items using the term-frequency vectors. The rules creation process will use thresholds as proposed in [18] to select the most relevant terms for each hierarchy concept that will be used in the rules. The main difference is that instead of translating the rules into logical constraints of an ontology captured in Description Logic, these rules will be translated into rules in the Semantic Web Rule Language (SWRL). The main interest in using SWRL rules is to reduce the reasoning effort, thus improving the scalability and performance of the system. We aim to use a huge amount of simple SWRL rules that will be applied to the ontology in order to classify items."
    }, {
      "heading" : "E. Realization",
      "text" : "The realization phase will populate the learned concept hierarchy with data items. First the ontology is populated with new items to label in an assertion level (Abox). To do the classification/labeling of the items, the SWRL Rules generated in the Resolution step are used. Then a Rule-Based inference engine will use the SWRL rules and the hierarchy to infer the most specific concepts for each data item and all subsuming concepts. This leads to a multi-label classification of the documents based in a hierarchy of labels (Hierarchical Multi-label Classification)."
    }, {
      "heading" : "IV. CONCLUSION",
      "text" : "In this paper we present our vision to extract value from Big Data using a Semantic HMC process and propose a scalable five-step architecture to automatically classify unstructured items. We use machine learning to learn an ontology with SWRL rules to automatically classify items of Big Data. The Semantic HMC process prototype is under development and we expect to show the implementation and results in further work. Our current work consists in evaluating the resulting ontology, considering three different aspects: the process scalability (performance), the quality of the hierarchy, and the quality of the classification process (i.e. concept tagging of items)."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This project is founded by the company Actualis SARL, the French agency ANRT and through the Portuguese COMPETE Program under the project AAL4ALL (QREN13852)."
    } ],
    "references" : [ {
      "title" : "Data Mining: Practical machine learning tools and techniques",
      "author" : [ "I.H. Witten", "E. Frank" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "Big Data: A Survey",
      "author" : [ "M. Chen", "S. Mao", "Y. Liu" ],
      "venue" : "Mobile Networks and Applications, pp. 171–209, 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Linked data, big data, and the 4th paradigm",
      "author" : [ "P. Hitzler", "K. Janowicz" ],
      "venue" : "Semantic Web, vol. 4, pp. 233–235, 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The Future Revolution on Big Data",
      "author" : [ "A. Syed", "K. Gillela", "C. Venugopal" ],
      "venue" : "Future, vol. 2, pp. 2446–2451, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Extracting value from chaos",
      "author" : [ "J. Gantz", "D. Reinsel" ],
      "venue" : "IDC iview, pp. 1–12, 2011.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Transforming Big Data into Smart Data",
      "author" : [ "A. Sheth" ],
      "venue" : "2014 IEEE 30th International Conference on Data Engineering, pp. 2–2, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-label classification on tree-and DAGstructured hierarchies",
      "author" : [ "W. Bi", "J. Kwok" ],
      "venue" : "Yeast, pp. 1–8, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Knowledge engineering: Principles and methods",
      "author" : [ "R. Studer", "V.R. Benjamins", "D. Fensel" ],
      "venue" : "Data & Knowledge Engineering, pp. 161–197, 1998.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "QueryPIE: Backward Reasoning for OWL Horst over Very Large Knowledge Bases",
      "author" : [ "J. Urbani", "F. van Harmelen", "S. Schlobach", "H. Bal" ],
      "venue" : "ISWC’11. Springer-Verlag, 2011, pp. 730–745.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Organising knowledge: taxonomies, knowledge and organisational effectiveness",
      "author" : [ "P. Lambe" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "MapReduce : Simplified Data Processing on Large Clusters",
      "author" : [ "J. Dean", "S. Ghemawat" ],
      "venue" : "Communications of the ACM, pp. 1–13, 2008.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Three Laws Learned from Web-scale Reasoning",
      "author" : [ "J. Urbani" ],
      "venue" : "2013 AAAI Fall Symposium Series, 2013.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Content-based recommender systems: State of the art and trends",
      "author" : [ "P. Lops", "M. de Gemmis", "G. Semeraro" ],
      "venue" : "Recommender Systems Handbook. Springer, 2011, pp. 73–105.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Recommender systems survey",
      "author" : [ "J. Bobadilla", "F. Ortega", "A. Hernando", "A. Gutiérrez" ],
      "venue" : "Knowledge-Based Systems, pp. 109–132, 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Term-weighting approaches in automatic text retrieval",
      "author" : [ "G. Salton", "C. Buckley" ],
      "venue" : "Information processing & management, pp. 513—-523, 1988.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Domain taxonomy learning from text: The subsumption method versus hierarchical clustering",
      "author" : [ "J. de Knijff", "F. Frasincar", "F. Hogenboom" ],
      "venue" : "Data & Knowledge Engineering, pp. 54–69, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A Semantic Approach for Extracting Domain Taxonomies from Text",
      "author" : [ "K. Meijer", "F. Frasincar", "F. Hogenboom" ],
      "venue" : "Decision Support Systems, 2014.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Using DL-Reasoner for Hierarchical Multilabel Classification applied to Economical e-News",
      "author" : [ "D. Werner", "N. Silva", "C. Cruz" ],
      "venue" : "Science and Information Conference, 2014, p. 8.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Determining the value of information relative to a particular customer is a complex task addressed by the business intelligence/data-mining field [1].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "An increasing number of Vs has been used to characterize Big Data [2], [3]: Volume, Velocity, Variety and Value.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "An increasing number of Vs has been used to characterize Big Data [2], [3]: Volume, Velocity, Variety and Value.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "Volume concerns the large amount of data that is generated and stored through the years by social media, sensor data, etc[2].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "These types include semi-structured and unstructured data representing 90% of his content [4] such as audio, video, webpage, and text, as well as traditional structured data, etc.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "An IDC report [5] proposes the value extraction from very large volumes of a wide variety of data, by enabling the high-velocity capture, discovery, and/or analysis.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "Sheth [6] proposes deriving Value via harnessing the challenges posed by Volume, Variety, and Velocity using semantic techniques and technologies.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "Hierarchical Multi-Label Classification (HMC) is the combination of Multi-Label classification and Hierarchical classification [7].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "In HMC, the items can be assigned to different hierarchical paths and simultaneously may belong to different class labels in the same hierarchical level [7].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 7,
      "context" : "The ontology [8] plays a key role in defining terms and meanings used to represent the knowledge, reducing the gap between the users and the HMC process.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "Instead it proposes a scalable process to semantically learn the ontology by adopting scalable machine learning processes and Rule-based reasoning [9] to classify the data items and therefore extract value from Big Data.",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 9,
      "context" : "Corporations recurrently use concept hierarchies as taxonomies to represent their valuable information [10].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "To infer the most specific concepts for each data item and all subsuming concepts we use rule-based reasoning that exhaustively applies a set of rules to a set of triples to infer conclusions [9], i.",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 10,
      "context" : "This Rule-based reasoning approach allows the parallelization and distribution of work by large clusters of inexpensive machines through Big Data technologies as Map-reduce [11].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 11,
      "context" : "Web Scale Reasoners [12] currently uses Rule-Based reasoning to reach high scalability by load parallelization and distribution, thus addressing the Velocity and Volume dimensions of Big Data.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "The Limited Content Analysis problem [13][14] is defined by the difficulty in extracting reliable automated information from various content (e.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "The Limited Content Analysis problem [13][14] is defined by the difficulty in extracting reliable automated information from various content (e.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "TF-IDF uses the frequency of a term in an item (TF) and the inverse number of items in which the term appears (IDF) [15].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "• Term frequency in all items using the appearing frequency of a term in all documents [15].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "There are several methods for creating hierarchical relations between concepts including [16], [17]:",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "There are several methods for creating hierarchical relations between concepts including [16], [17]:",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "The advantages and drawbacks of each method is deeply studied in [16].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "The rules creation process will use thresholds as proposed in [18] to select the most relevant terms for each hierarchy concept that will be used in the rules.",
      "startOffset" : 62,
      "endOffset" : 66
    } ],
    "year" : 2014,
    "abstractText" : "Analyzing Big Data can help corporations to improve their efficiency. In this work we present a new vision to derive Value from Big Data using a Semantic Hierarchical Multi-label Classification called Semantic HMC based in a nonsupervised Ontology learning process. We also proposea Semantic HMC process, using scalable Machine-Learning techniques and Rule-based reasoning.",
    "creator" : "LaTeX with hyperref package"
  }
}