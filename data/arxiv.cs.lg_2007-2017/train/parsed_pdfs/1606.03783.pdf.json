{
  "name" : "1606.03783.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Retrieving and Ranking Similar Questions from Question-Answer Archives Using Topic Modelling and Topic Distribution Regression",
    "authors" : [ "Pedro Chahuara", "Thomas Lampert", "Pierre Gançarski", "P. Gançarski" ],
    "emails" : [ "pedro.chahuara@unistra.fr", "lampert@unistra.fr", "pierre.gancarski@unistra.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Collaborative Question Answering, Question and Answer Retrieval, LDA, Neural Network, Topic Modelling, Regression"
    }, {
      "heading" : "1 Introduction",
      "text" : "During the last decade internet based Collaborative Question Answering (CQA) platforms have increased in popularity. These platforms offer a social environment for people to seek answers to questions, and where the answers are offered by other community members. Users pose questions in natural language, as opposed to queries in web search engines, and community members propose answers in addition to voting and rating the information posted on the platform. Some of the most popular CQA sites are Yahoo! Questions, Quora, and StackExchange. Besides public CQA websites, similar systems can be found in industry, for example in retail and business websites where users can pose questions about a company’s product and a group of specialists can give support.\nThis content has attracted the attention of researchers from a number of domains [1–7] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted. Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8]. The latter is the problem that is covered in the present work. As such, the system helps to remove the delay needed for other community members to answer; and the list of related questions provides material for users to acquire more knowledge on the topic of their question. ar X\niv :1\n60 6.\n03 78\n3v 1\n[ cs\n.I R\n] 1\n2 Ju\nn 20\n16\nSolving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the ‘lexical chasm’ [9]. For instance the questions “Where can I watch movies on the internet for free?” and “Are there any sites for streaming films?” are semantically related but lexically different. The opposite case is also possible—questions having words in common may have different semantic meanings. Besides the need for accurately identifying a question’s semantics, a solution to the problem must deal with noisy information such as: misspelt words, polysemy, and short questions.\nSimilar questions are typically found by comparing the query question to the content of existing questions as it has been shown that finding similar questions based solely on their answers does not perform well [1,5]. Nevertheless Xue et al. demonstrated that combining information derived from existing questions and their answers outperforms the other strategies [5]. In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy. These approaches, however, have thus far only been used to model the questions in the archives. As such, the contribution of the present work is twofold: firstly, the application of Latent Dirichlet Allocation (LDA) to model topics among the questions and answers in the archive; and secondly, the use of a regression step to estimate the appropriate QA topic distribution from that of a novel question.\nThis paper is organized as follows: Section 2 presents the state of the art, Section 3 the study’s methodology, Section 4 the experimental setup and results, which are discussed in Section 5, and conclusions are presented in Section 6."
    }, {
      "heading" : "2 Related Work",
      "text" : "The principal challenge when retrieving related questions and answers in a QA database given a new question is the lexical gap that may exist between two semantically similar questions. In general, a method that intends to solve the problem of question retrieval should be composed at least of two main parts: a document representation that can properly express the semantics and context of QAs in the database; and a mechanism for comparing the similarity of documents given their representations. The most widespread document representation methods in the literature are those based on bag-of-words (BOW), which explicitly represents each of the document’s words. Comparison is achieved by computing the number of matching words between two BOW representations. There exist several variations of this class of methods, each weighting words that have specific properties in the dataset, such as tf-idf and BM25 [12]. This class of methods is able to measure two documents’ lexical similarity but it does not capture information regarding their semantics and context.\nIn QA databases, questions and answers are often short and contain many word variations resulting from grammatical inflection, misspelling, and informal abbreviations. As a consequence, BOW representations in QA corpora produce a vector representation that can be too sparse. Besides sparsity, BOW represen-\ntations do not provide a measure of co-occurrence or shared contextual information, which can increase the similarity of related documents.\nAn approach that overcomes these limitations is the translation model, first proposed for use in this context by Jeon et al. [1]. Their method consists of two stages: first a set of semantically similar questions are found by matching their answers using a query-likelihood language model; and subsequently, word translation probabilities are estimated using the IBM translation model 1 [13]. Several extensions have been proposed [5–7] including the use of external corpora [14,15] such as Wikipedia. Xue et al. [5] propose an extension that combines the IBM translation model (applied to the questions) with a query likelihood language model (applied to the answers). Translation-based models have become the stateof-the-art in query retrieval [10, 16] but they suffer from some limitations: they do not capture word co-occurrences nor word distributions in the corpora.\nIn the last decade Topic Modeling has become an important method for text analysis. Since the topics that characterise a document can be considered a semantic representation, it is possible to use topic distributions inferred using a method such as Latent Dirichlet Allocation (LDA) [17] to measure the semantic similarity between documents in a corpora. Consequently, several approaches for applying topic modelling to QA archives have been proposed: Zhang et al. [2] retrieve similar questions by measuring lexical and topical similarities [2]; Cai et al. [10] combine the result of LDA and translation models; Vasiljević et al. [11] explore combining a document’s word count and topic model similarity into one measure; and Yang et al. [8] form a generative probabilistic method to jointly model QA topic distributions and user expertise. In all of the above-mentioned topic modelling approaches similarity is calculated using the questions that exist in the database.\nThis work explores the possibility of deriving topic distributions from existing questions and answers, and proposes a method to relate these to the topic distribution of a novel question. Some work has been done in this direction; Zolaktaf et al. [18] model the question topics and then use them to condition the answer topics. This work proposes to model question and question-answer topics independently and then to learn a mapping between them. Furthermore, it extends topic modeling to include distributed word representations."
    }, {
      "heading" : "3 Methodology",
      "text" : "A corpora C of size L = |C| consists of many question-answer pairs: C = {(q1, a1), (q2, a2), . . . , (qL, aL)}, where Q = {q1, q2, . . . , qL} and A = {a1, a2, . . . , aL}, ∀(qi, ai) ∈ C : qi ∈ Q, ai ∈ A, are question and answer sets (respectively).\nQuestions in a CQA corpora tend to be shorter than answers and may contain few relevant words, which limits a model’s ability to discover underlying trends. An approach to mitigate this is to assume that each question qi contains its text, and possibly keywords, a title, and a description. This assumption is not a requirement as meta-data may not always be available; however its absence may limit the ability of a model to represent the questions. We discuss this further in\nthis section and propose methods to overcome the problem of question sparsity. Furthermore, each question in a QA corpora may have multiple answers and these are concatenated to form each element ai as they all provide contextual information that can be exploited to determine the question’s relevance.\nFigure 1 presents the proposed methodology. The task of similar question retrieval implies ranking the pairs contained in the QA Corpora (C) according to their similarity to a query question q∗, producing a partially ordered set C ′ such that its first element has the highest similarity (the top, say, ten elements of which can then be returned as suggestions). In the learning phase of the proposed methodology, the QA corpora is used to train two topic models (Section 3.1): LDA on the set Q, and LDA on the set QA, in which each pair (qi, ai) ∈ C is concatenated to form a single document. This results in topic distributions associated with the sets Q and QA and each element contained therein (θQi and θQAi respectively). A regression model is trained using the samples θ Q i and θQAi (Train NN) to learn the translation function between the Q and QA topic distributions (Section 3.2). During inference the Q set LDA model is used to determine the topic distribution of a query question (θQ∗ ) which is translated to a QA topic distribution (θQA∗ ) using the regression model. Finally, a similarity measure (Section 3.2) is used to rank the QA Corpora (QA) according the similarity between each pair’s topic distribution (θQAi ) and the query question’s QA topic distribution (θQA∗ ) obtained from the regression model. The LDA and regression models are discussed in more detail in the following subsections."
    }, {
      "heading" : "3.1 Latent Dirichlet Allocation & Distributed Word Representation",
      "text" : "In this work we assert that topic modeling provides a representation of the elements in C that facilitates the discovery of semantically similar questions; particularly when these similar questions do not have words in common.\nLatent Dirichlet Allocation (LDA) [17] is a generative probabilistic model that enables us to describe a collection of discrete observations in terms of latent variables. The plate notation representing LDA is presented in Figure 2. When applied to a corpora, LDA models the generation of each document by means of two stochastic independent processes and can be summarised as follows 1. For each document d in the collection D, randomly choose a distribution over\ntopics θd ∼ Dir(α), where α is the Dirichlet prior. 2. For each word wn in document d:\n(a) choose a topic from the distribution over topics in Step 1. zd,n ∼ Mult(θd); (b) choose a word from the vocabulary distribution wd,n ∼ Mult(φzd,n). After learning a corpora’s latent variables a topic is represented as a multinomial distribution of words, and a document by a multinomial distribution of topics.\nThe LDA algorithm described above treats words as explicit constraints, which inhibits its effectiveness when words are rare. A solution is to treat words as features [19] and the method used to calculate a word’s features then influences its topic membership. This allows us to exploit a word’s semantic similarity to augment information in short questions by giving similar topic membership probabilities to semantically equivalent words. For example, the words “educator”, “education”, “educational”, and “instruction” should have similar probabilities within a certain topic, even if some of these words appear rarely in the corpus.\nMikolov et al. [20] introduced the continuous bag-of-words and Skip-gram neural network models that produce a continuous-valued vectorial word representation by exploiting the content of large textual databases. Distances between these vectors are proportional to the semantic difference of the words they represent, and thus these vectors can be used as features in many NLP tasks. In this work, the Word2vec vector representation is used to group semantically related words; its use for this application was first proposed by Petterson et al. [19].\nIn the original LDA algorithm, a word is generated by the process wd,n ∼ Mult(φzd,n) where φzd,n is the multinomial distribution (Mult) of word probabilities in topic zd,n over the whole vocabulary. In order to introduce the distributed representation of words, we define a function v : R → Rr that maps a word to its vectorial representation learnt by Word2vec, where r the number\nof latent features used for the distributed word representation (in practice this function is represented by a matrix ω ∈ RN×r, where N is the vocabulary size). Given two words w and w′ their semantic similarity can be found by applying the cosine similarity function, see Eq. (4), to their vectorial representations, i.e. Similarity(v(w), v(w′)). A set of words that are similar to w, Ωw, can be obtained by defining a threshold τ such thatΩw = {w′ | Similarity(v(w), v(w′)) > τ}. This set can be used to define an alternative distribution of word probabilities φ′zd,n for topic zd,n, in which the probability of a word w is given by\nφ′zd,n(w) = 1\nc ∑ w′∈Ωw exp ( φzd,n(w ′) Similarity (v(w), v(w′)) ) , (1)\nwhere c is a normalisation factor. This modified distribution gives a high probability to semantically related words. Finally we consider each word w to be sampled from a linear combination of the original and modified distributions\nwd,n ∼ λMult(φzd,n) + (1− λ) Mult(φ′zd,n), 0 ≤ λ ≤ 1. (2)\nWe fixed λ to 0.9 so that the results of standard LDA are not excessively altered. In order to implement this modification, the Gibbs Sampling-based algorithm proposed by [21] was adapted so that at each step the probability of topic t being present in document d given word w is estimated as follows:\np(z = t | w) = αβ βV + n.|t + nt|dβ βV + n.|t +\n( α+ nt|d ) λnw|t\nβV + n.|t\n+ 1− λ c ∑ w′∈Ωw exp ( nw′|t Similarity (v(w), v(w ′)) n.|t ) , (3)\nwhere nw|t is the number of words w assigned to topic t, nt|d is the total number of words in document d assigned to topic t, n.|t = ∑ w nw|t, α = 35/T is the Dirichlet prior of the per document topic distribution (for number of topics T ), and β = 0.01 [21, 22]. Small values of α and β result in a fine-grained decomposition into topics that address specific areas [21,22].\nThis method is applied to two document collections (Figure 1), Q and QA, which results in two topic models: TQ = {1, . . . ,KQ} in which each question qi is represented by the distribution of topics θQi ; and TQA = {1, . . . ,KQA} in which each pair (qi, ai) is represented by the distribution of topics θ QA i ."
    }, {
      "heading" : "3.2 Nonlinear Multinomial Regression",
      "text" : "When a query question q∗ is entered the left-to-right method [23] is used to infer its topic distribution, θQ∗ . A regression model is therefore needed to obtain an estimate of θQ∗ mapped to a distribution of topics in the QA set, θ QA ∗ . Mapping the distribution of question topics to the distribution of question-answer topics avoids problems that occur when limited vocabularies are used in a question.\nThis information is augmented with that derived from the set of answer terms, thus by mapping a query question to the space of question-answers it is possible to calculate its similarity using words that do not exist in the question vocabulary (and therefore are not represented in the topic distribution TQ). Performing this mapping also provides a means to model the relationship between question semantics and existing question-answer semantics (which will be discussed further in Section 5): given a query question q∗ the model estimates a topic distribution in the space of concatenated questions and answers, which can be compared to the distributions of existing QA pairs.\nDetermining the topic distribution in the space of documents comprising questions and answers, given the topic distribution of a new question is a problem of multinomial regression. For which we use a multilayer perceptron neural network (NN), which are nonlinear multinomial regression models [24, 25]. The NN is trained using the set of topic distributions for each document in Q and QA, θQi and θ QA i (respectively) where i = 1, . . . , L, and therefore the input and output layers have as many nodes as the number of topics used to model these sets, KQ and KQA (respectively). Sigmoid activation functions are used in the hidden layer and softmax in the output layer to ensure that outputs sum to one.\nIn application the input of the NN is the topic distribution of the query question according to latent topic model of the existing questions, represented by θQ∗ , and its output is an estimate of its distribution in the QA latent topic model, θQA∗ . The cosine similarity measure allows us to rank existing questions qi according to their similarity to θ QA ∗ , i.e.\nSimilarity ( θQA∗ , θ QA i ) =\nθQA∗ · θQAi ‖θQA∗ ‖‖θQAi ‖ , (4)\nwhere ‖x‖ is the length of vector x, and therefore the most similar existing questions appear at the top of the ranked list that is output by the system."
    }, {
      "heading" : "4 Evaluation",
      "text" : "This section describes the data, experimental setup, and comparison algorithms used to evaluate the proposed approach."
    }, {
      "heading" : "4.1 Data",
      "text" : "Four categories, derived from two different CQA sources, are used for the evaluation. The first two are the Health and Computers & Internet (referred to herein as Computers) categories in the publicly available Yahoo! Questions L6 (Yahoo! Answers Comprehensive Questions and Answers version 1.0) dataset1. The second two are the Physics and Geographic Information Systems (GIS) categories taken from the publicly available StackExchange (SE) dataset2. The question\n1 Available from http://webscope.sandbox.yahoo.com/catalog.php?datatype=l 2 Available from https://archive.org/details/stackexchange\nsets extracted from the Yahoo! dataset were created by concatenating the question text and description (when available), and the question sets extracted from the SE dataset were created by concatenating the question title, tags, and text. The answer sets were created by concatenating all the answers provided by different users for a particular question. Table 1 summarises these datasets.\nPreprocessing was performed before data use: stop words were removed using Mallet’s standard English list (543 words), non-English characters were removed, and lemmatization was performed to reduce the number of inflected word forms.\nFifty randomly selected questions from each category were used for testing and the remaining pairs were used as training data. Therefore four models were calculated using each algorithm, one for each category. The output of each model (the top ten most similar results for each test question) were manually labelled as relevant or not and this was used to calculate the evaluation statistics.\nThe Word2vec model requires training in order to learn the word embedding space, and this was realised using an additional corpus of Google news and Yahoo! Questions QA pairs (from categories other than those presented previously). The reason for including documents form Yahoo! Questions in this corpus is that it enables words that are specific to the dataset—such as abbreviations, misspellings, and technical jargon—to be learnt.\nA modified version of Mallet, which implements the Gibbs sampling method proposed by Yao et al. [21], was used for Topic Modeling. The number of topics were empirically set to 140 and 160 for the Q and QA sets (respectively) and the size of the neural network’s hidden layer was empirically set to 180 using 100 questions-answer pairs (these were subsequently removed from the corpus)."
    }, {
      "heading" : "4.2 Results",
      "text" : "The proposed method, referred to henceforth as LDA+, was compared to four state-of-the-art algorithms: Translation1, the IBM translation approach proposed by Jeon et al. [1]; Translation2, the combined translation and querylikelihood language model proposed by Xue et al. [5]; an autoencoder based method proposed by Socher et al. [26]; to establish the benefit of word2vec, LDA∗ (as described within Section 3 excluding word2vec); and to establish the benefit of the regression stage, LDA† (as described within Section 3 excluding the regression step).\nMean Average Precision (MAP) and Precision at N (P@N) are used to summarise retrieval performance within each category. The autoencoder was found to be computationally infeasible when applied to the described datasets and therefore its retrieval performance is not presented. The results obtained using the remaining methods are presented in Table 2. A cursory validation of these results was performed by comparing the translation methods’ figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].\nThe results show that in all of the datasets, LDA+ outperforms all other methods. However, the difference is much more pronounced when the length of the question and answers increase (as is the case in the SE datasets). In this situation, the translation methods fail to find relevant documents whereas all of the LDA methods do (due to the increase in information). It is difficult to separate the performances of LDA with Word2vec and LDA with regression (LDA† and LDA∗), but when combined (LDA+) a performance increase is observed."
    }, {
      "heading" : "5 Discussion",
      "text" : "Within the translation based approaches [1, 5] the translation probabilities of equal source and target words are fixed to 1. This forces questions that share words in common with the query question to be highly ranked. Conversely,\nLDA†, LDA∗, and LDA+ perform matching based upon shared topics, and inherently accounts for words that represent multiple concepts by decreasing their probabilities in the topics that they appear. To illustrate this, Table 3 presents an example of retrieved questions using LDA+ and the two translation based approaches3 (the points discussed in this section were observed in all of the categories but to save space we present examples from the Health category). In the first example, presented in the top half of the table, the QA pairs retrieved by LDA+ do not contain the words “lift” and “weight” even though they are relevant to the query. The excessive contribution from the word “weight” causes the translation models to retrieve questions that are related to body weight instead of weight lifting. The second example illustrates a query in which all the retrieved QA pairs are relevant. As before, the translation methods result in questions that have words in common with the query question (as does LDA+); in this case Translation2 associates a high translation probability between “hair” and “mustache” (sic).\nTable 4 demonstrates the benefit of performing the multinomial regression. It presents the representative words (those that have high probability in the topic’s word distribution) of three of the topics derived from the question (Q) set and the question+answers (QA) set. It demonstrates that the topics derived from the QA set better represent the themes that appear in health documents, whilst the topics of derived from the Q set are less distinguishable. For example, the words in Topic 3 appear to represent depression, however, the words derived from the QA set are more coherent. This is because of the limited vocabulary used in questions and their typically short length.\nFurthermore, the topics derived from the Q set tend to represent the semantics of expressions commonly used in questions (and not in answers), for example the phrases “an effective method” and “effective treatment”. The word\n3 Mistakes in the questions are original to the data\n“effective” in the topics derived from the QA set is associated with the topic representing medical products. Consequently, when a question such as “What is an effective sleeping aid?” is posed to a model trained on the QA set, topics in which the words “method” and “treatment” have high probability would not be considered. The model trained on the Q set, however, results in a high probability of Topic 1, and the regression stage of LDA+ causes this to be mapped to the distribution in which the words “treatment” and “method” have higher probabilities. Another example is provided by Topic 2, here the word “result” is often mentioned in questions posed by those who have performed medical tests, while in answers the word usually refers to the results of health research studies."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This paper has presented a novel model that fuses topic modelling with Word2vec and a regression stage for ranking relevant questions-answer pairs within Collaborative Question Answering platforms. The performance of the proposed method has been evaluated using several real-world datasets, and it has been shown to outperform translation based methods and LDA with each innovation separately in all cases. Most notably when the dataset contains long questions and answers. It achieves this by allowing the model to overcome the differences in vocabulary used in questions and answers, helps to deal with the sparsity often encountered in questions (due to their relatively short length), and allows the method to exploit all available information."
    } ],
    "references" : [ {
      "title" : "Finding similar questions in large question and answer archives",
      "author" : [ "J. Jeon", "B.W. Croft", "J. Ho Lee" ],
      "venue" : "CIKM",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "A topic clustering approach to finding similar questions from large question and answer archives",
      "author" : [ "Zhang", "W.N" ],
      "venue" : "PLoS ONE",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "A syntactic tree matching approach to finding similar questions in community-based QA services",
      "author" : [ "K. Wang", "Z. Ming", "T.S. Chua" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Question answering passage retrieval using dependency relations",
      "author" : [ "H. Cui", "R. Sun", "K. Li", "M.Y. Kan", "T.S. Chua" ],
      "venue" : "In: SIGIR",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Retrieval models for question and answer archives",
      "author" : [ "X. Xue", "J. Jeon", "W.B. Croft" ],
      "venue" : "In: SIGIR",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Bridging lexical gaps between queries and questions on large online Q&A collections with compact translation models",
      "author" : [ "Lee", "J.T" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "Combining lexical semantic resources with question & answer archives for translation-based answer finding",
      "author" : [ "D. Bernhard", "I. Gurevych" ],
      "venue" : "In: ACL-IJCNLP. Volume",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "CQArank: Jointly model topics and expertise in community question answering",
      "author" : [ "L Yang" ],
      "venue" : "CIKM",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Bridging the lexical chasm: statistical approaches to answer-finding",
      "author" : [ "A. Berger", "R. Caruana", "D. Cohn", "D. Freitag", "V. Mittal" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "Learning the latent topics for question retrieval in community QA",
      "author" : [ "L. Cai", "G. Zhou", "K. Liu", "J. Zhao" ],
      "venue" : "IJCNLP",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "The application of the topic modeling to question answer retrieval",
      "author" : [ "J. Vasiljević", "M. Ivanović", "T. Lampert" ],
      "venue" : "ICIST",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval",
      "author" : [ "S.E. Robertson", "S. Walker" ],
      "venue" : "In: SIGIR",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1994
    }, {
      "title" : "The mathematics of statistical machine translation: paramter estimation",
      "author" : [ "P Brown" ],
      "venue" : "Computational Linguistics",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1993
    }, {
      "title" : "Improving question retrieval in community question answering using world knowledge",
      "author" : [ "G Zhou" ],
      "venue" : "In: IJCAI",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Entity based Q&A retrieval",
      "author" : [ "A. Singh" ],
      "venue" : "EMNLP",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Statistical machine translation improves question retrieval in community question answering via matrix factorization",
      "author" : [ "G Zhou" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "Blei", "D.M" ],
      "venue" : "JMLR",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Modeling community questionanswering archives",
      "author" : [ "Z. Zolaktaf", "F. Riahi", "M. Shafiei", "E. Milios" ],
      "venue" : "Proceedings of the Workshop on Computational Social Science and the Wisdom of Crowds at NIPS",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Word features for latent dirichlet allocation",
      "author" : [ "J Petterson" ],
      "venue" : "In: NIPS. Volume",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "ICLR Workshop",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Efficient methods for topic model inference on streaming document collections",
      "author" : [ "L. Yao", "D. Mimno", "A. McCallum" ],
      "venue" : "In: SIGKDD",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Evaluation methods for topic models",
      "author" : [ "H. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno" ],
      "venue" : "In: ICML",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Pattern recognition and neural networks",
      "author" : [ "B. Ripley" ],
      "venue" : "Camb UP,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1996
    }, {
      "title" : "Neural networks and the multinomial logit for brand choice modelling: a hybrid approach",
      "author" : [ "Y. Bentz", "D. Merunka" ],
      "venue" : "J. Forec",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2000
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "R Socher" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This content has attracted the attention of researchers from a number of domains [1–7] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "This content has attracted the attention of researchers from a number of domains [1–7] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "This content has attracted the attention of researchers from a number of domains [1–7] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "This content has attracted the attention of researchers from a number of domains [1–7] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "This content has attracted the attention of researchers from a number of domains [1–7] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "This content has attracted the attention of researchers from a number of domains [1–7] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "This content has attracted the attention of researchers from a number of domains [1–7] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].",
      "startOffset" : 98,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].",
      "startOffset" : 98,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the ‘lexical chasm’ [9].",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the ‘lexical chasm’ [9].",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the ‘lexical chasm’ [9].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "Similar questions are typically found by comparing the query question to the content of existing questions as it has been shown that finding similar questions based solely on their answers does not perform well [1,5].",
      "startOffset" : 211,
      "endOffset" : 216
    }, {
      "referenceID" : 4,
      "context" : "Similar questions are typically found by comparing the query question to the content of existing questions as it has been shown that finding similar questions based solely on their answers does not perform well [1,5].",
      "startOffset" : 211,
      "endOffset" : 216
    }, {
      "referenceID" : 4,
      "context" : "demonstrated that combining information derived from existing questions and their answers outperforms the other strategies [5].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.",
      "startOffset" : 65,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.",
      "startOffset" : 65,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.",
      "startOffset" : 65,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "There exist several variations of this class of methods, each weighting words that have specific properties in the dataset, such as tf-idf and BM25 [12].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "Their method consists of two stages: first a set of semantically similar questions are found by matching their answers using a query-likelihood language model; and subsequently, word translation probabilities are estimated using the IBM translation model 1 [13].",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 4,
      "context" : "Several extensions have been proposed [5–7] including the use of external corpora [14,15] such as Wikipedia.",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "Several extensions have been proposed [5–7] including the use of external corpora [14,15] such as Wikipedia.",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Several extensions have been proposed [5–7] including the use of external corpora [14,15] such as Wikipedia.",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "Several extensions have been proposed [5–7] including the use of external corpora [14,15] such as Wikipedia.",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "Several extensions have been proposed [5–7] including the use of external corpora [14,15] such as Wikipedia.",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "[5] propose an extension that combines the IBM translation model (applied to the questions) with a query likelihood language model (applied to the answers).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "Translation-based models have become the stateof-the-art in query retrieval [10, 16] but they suffer from some limitations: they do not capture word co-occurrences nor word distributions in the corpora.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "Translation-based models have become the stateof-the-art in query retrieval [10, 16] but they suffer from some limitations: they do not capture word co-occurrences nor word distributions in the corpora.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "Since the topics that characterise a document can be considered a semantic representation, it is possible to use topic distributions inferred using a method such as Latent Dirichlet Allocation (LDA) [17] to measure the semantic similarity between documents in a corpora.",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "[2] retrieve similar questions by measuring lexical and topical similarities [2]; Cai et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] retrieve similar questions by measuring lexical and topical similarities [2]; Cai et al.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "[10] combine the result of LDA and translation models; Vasiljević et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] explore combining a document’s word count and topic model similarity into one measure; and Yang et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[8] form a generative probabilistic method to jointly model QA topic distributions and user expertise.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "[18] model the question topics and then use them to condition the answer topics.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Latent Dirichlet Allocation (LDA) [17] is a generative probabilistic model that enables us to describe a collection of discrete observations in terms of latent variables.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "A solution is to treat words as features [19] and the method used to calculate a word’s features then influences its topic membership.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "[20] introduced the continuous bag-of-words and Skip-gram neural network models that produce a continuous-valued vectorial word representation by exploiting the content of large textual databases.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "In order to implement this modification, the Gibbs Sampling-based algorithm proposed by [21] was adapted so that at each step the probability of topic t being present in document d given word w is estimated as follows:",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "01 [21, 22].",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 20,
      "context" : "Small values of α and β result in a fine-grained decomposition into topics that address specific areas [21,22].",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : "When a query question q∗ is entered the left-to-right method [23] is used to infer its topic distribution, θ ∗ .",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "For which we use a multilayer perceptron neural network (NN), which are nonlinear multinomial regression models [24, 25].",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 23,
      "context" : "For which we use a multilayer perceptron neural network (NN), which are nonlinear multinomial regression models [24, 25].",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 20,
      "context" : "[21], was used for Topic Modeling.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[1]; Translation2, the combined translation and querylikelihood language model proposed by Xue et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5]; an autoencoder based method proposed by Socher et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 24,
      "context" : "[26]; to establish the benefit of word2vec, LDA∗ (as described within Section 3 excluding word2vec); and to establish the benefit of the regression stage, LDA† (as described within Section 3 excluding the regression step).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "A cursory validation of these results was performed by comparing the translation methods’ figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].",
      "startOffset" : 245,
      "endOffset" : 261
    }, {
      "referenceID" : 9,
      "context" : "A cursory validation of these results was performed by comparing the translation methods’ figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].",
      "startOffset" : 245,
      "endOffset" : 261
    }, {
      "referenceID" : 13,
      "context" : "A cursory validation of these results was performed by comparing the translation methods’ figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].",
      "startOffset" : 245,
      "endOffset" : 261
    }, {
      "referenceID" : 13,
      "context" : "A cursory validation of these results was performed by comparing the translation methods’ figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].",
      "startOffset" : 245,
      "endOffset" : 261
    }, {
      "referenceID" : 14,
      "context" : "A cursory validation of these results was performed by comparing the translation methods’ figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].",
      "startOffset" : 245,
      "endOffset" : 261
    }, {
      "referenceID" : 0,
      "context" : "Within the translation based approaches [1, 5] the translation probabilities of equal source and target words are fixed to 1.",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "Within the translation based approaches [1, 5] the translation probabilities of equal source and target words are fixed to 1.",
      "startOffset" : 40,
      "endOffset" : 46
    } ],
    "year" : 2016,
    "abstractText" : "Presented herein is a novel model for similar question ranking within collaborative question answer platforms. The presented approach integrates a regression stage to relate topics derived from questions to those derived from question-answer pairs. This helps to avoid problems caused by the differences in vocabulary used within questions and answers, and the tendency for questions to be shorter than answers. The performance of the model is shown to outperform translation methods and topic modelling (without regression) on several real-world datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}