{
  "name" : "1503.04069.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "LSTM: A Search Space Odyssey",
    "authors" : [ "Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutnı́k", "Bas R. Steunebrink", "Jürgen Schmidhuber" ],
    "emails" : [ "KLAUS@IDSIA.CH", "RUPESH@IDSIA.CH", "HKOU@IDSIA.CH", "BAS@IDSIA.CH", "JUERGEN@IDSIA.CH" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Recurrent neural networks with Long Short-Term Memory (which we will concisely refer to as LSTMs) have emerged as an effective and scalable model for several learning\nproblems related to sequential data. Earlier methods for attacking these problems were usually hand-designed workarounds to deal with the sequential nature of data such as language and audio signals. Since LSTMs are effective at capturing long-term temporal dependencies without suffering from the optimization hurdles that plague simple recurrent networks (SRNs) (Hochreiter, 1991; Bengio et al., 1994), they have been used to advance the state of the art for many difficult problems. This includes handwriting recognition (Graves et al., 2009; Pham et al., 2013; Doetsch et al., 2014) and generation (Graves et al., 2013), language modeling (Zaremba et al., 2014) and translation (Luong et al., 2014), acoustic modeling of speech (Sak et al., 2014), speech synthesis (Fan et al., 2014), protein secondary structure prediction (Sønderby & Winther, 2014), analysis of audio (Marchi et al., 2014), and video data (Donahue et al., 2014) among others.\nThe central idea behind the LSTM architecture is a memory cell which can maintain its state over time, and non-linear gating units which regulate the information flow into and out of the cell. Most modern studies incorporate many improvements that have been made to the LSTM architecture since its original formulation (Hochreiter & Schmidhuber, 1995; 1997). However, LSTMs are now applied to many learning problems which differ significantly in scale and nature from the problems that these improvements were initially tested on. A systematic study of the utility of various computational components which comprise LSTMs (see Figure 1) was missing. This paper fills that gap and systematically addresses the open question of improving the LSTM architecture.\nWe evaluate the most popular LSTM architecture (vanilla LSTM; Section 2) and eight different variants thereof on three benchmark problems: acoustic modeling, handwrit-\nar X\niv :1\n50 3.\n04 06\n9v 1\n[ cs\n.N E\n] 1\n3 M\ning recognition and polyphonic music modeling. Each one differs from the vanilla LSTM by a single change. This allows us to isolate the effect of each of these changes on the performance of the architecture. Random search (Anderson, 1953; Solis & Wets, 1981; Bergstra & Bengio, 2012) is used to find the best performing hyperparameters for each variant on each problem, enabling a reliable comparison of the performance of different variants. We also provide insights gained about hyperparameters and their interaction using fANOVA (Hutter et al., 2014)."
    }, {
      "heading" : "2. Vanilla LSTM",
      "text" : "The LSTM architecture most commonly used in literature was originally described by Graves & Schmidhuber (2005).1 We refer to it as vanilla LSTM and use it as a reference for comparison of all the variants. The vanilla LSTM incorporates changes by Gers et al. (1999) and Gers & Schmidhuber (2000) into the original LSTM (Hochreiter & Schmidhuber, 1997) and uses full gradient training. Section 3 provides descriptions of these major LSTM changes.\nA schematic of the vanilla LSTM block can be seen in Figure 1. It features three gates (input, forget and output), block input, a single cell (the Constant Error Carousel), an output activation function, and peephole connections. The output of the block is recurrently connected back to the block input and all of the gates.\nThe vector formulas for a vanilla LSTM layer forward pass are given below. The corresponding Back-Propagation Through Time (BPTT) formulas can be found in supple-\n1But note that some studies omit peephole connections.\nmentary material. Here xt is the input vector at time t, the W are rectangular input weight matrices, the R are square recurrent weight matrices, the p are peephole weight vectors and b are bias vectors. Functions σ, g and h are point-wise non-linear activation functions: logistic sigmoid ( 11+e−x ) is used for as activation function of the gates and hyperbolic tangent is usually used as the block input and output activation function. The point-wise multiplication of two vectors is denoted with :\nzt = g(Wzx t + Rzy t−1 + bz) block input\nit = σ(Wix t + Riy t−1 + pi ct−1 + bi) input gate f t = σ(Wfx t + Rfy t−1 + pf ct−1 + bf) forget gate ct = it zt + f t ct−1 cell state ot = σ(Wox t + Roy t−1 + po ct + bo) output gate\nyt = ot h(ct) block output"
    }, {
      "heading" : "3. History of LSTM",
      "text" : ""
    }, {
      "heading" : "3.1. Original Formulation",
      "text" : "This initial version of the LSTM block (Hochreiter & Schmidhuber, 1995; 1997) included (possibly multiple) cells, input and output gates, but no forget gate and no peephole connections. The output gate, unit biases, or input activation function were omitted for certain experiments. Training was done using a mixture of Real Time Recurrent Learning (RTRL) and Backpropogation Through Time (BPTT). Only the gradient of the cell was propagated back through time, and the gradient for the other recurrent con-\nnections was truncated. Thus, that study did not use the exact gradient for training. Another feature of that version was the use of full gate recurrence, which means that all the gates received recurrent inputs from all gates at the previous time-step in addition to the recurrent inputs from the block outputs. This feature did not appear in any of the later papers."
    }, {
      "heading" : "3.2. Forget Gate",
      "text" : "The first paper to suggest a modification of the LSTM architecture introduced the forget gate (Gers et al., 1999), enabling the LSTM to reset its own state. This allowed learning of continual tasks such as embedded Reber grammar."
    }, {
      "heading" : "3.3. Peephole Connections",
      "text" : "Gers & Schmidhuber (2000) argued that in order to learn precise timings, the cell needs to control the gates. So far, this was only possible through an open output gate. Peephole connections (connections from the cell to the gates, blue in Figure 1) were added to the architecture in order to make precise timings easier to learn. Additionally, the output activation function was omitted, as there was no evidence that it was essential for solving the problems that LSTM had been tested on so far."
    }, {
      "heading" : "3.4. Full Gradient",
      "text" : "The final modification towards the vanilla LSTM was done by Graves & Schmidhuber (2005). This study presented the full backpropagation through time (BPTT) training for LSTM networks with the architecture described in Section 2, and presented results on the TIMIT benchmark. Using full BPTT had the added advantage that LSTM gradients could be checked using finite differences, making practical implementations more reliable."
    }, {
      "heading" : "3.5. Other Variants",
      "text" : "Since its introduction the vanilla LSTM has been the most commonly used architecture, but other variants have been suggested too. Before the introduction of full BPTT training, Gers et al. (2002) utilized a training method based on Extended Kalman Filtering which enabled the LSTM to be trained on some pathological cases at the cost of high computational complexity. Schmidhuber et al. (2007) proposed using a hybrid evolution-based method instead of BPTT for training but retained the vanilla LSTM architecture.\nBayer et al. (2009) evolved different LSTM block architectures that maximize fitness on context-sensitive grammars. Sak et al. (2014) introduced a linear projection layer that projects the output of the LSTM layer down before recurrent and forward connections in order to reduce the amount of parameters for LSTM networks with many blocks. By\nintroducing a trainable scaling parameter for the slope of the gate activation functions, Doetsch et al. (2014) were able to improve the performance of LSTM on an offline handwriting recognition dataset. In what they call Dynamic Cortex Memory, Otte et al. (2014) improved convergence speed of LSTM by adding recurrent connections between the gates of a single block (but not between the blocks).\nCho et al. (2014) proposed a simplification of the LSTM architecture called Gated Recurrent Unit (GRU). They used neither peephole connections nor output activation functions, and coupled the input and the forget gate into an update gate. Finally, their output gate (called reset gate) only gates the recurrent connections to the block input (Wz). Chung et al. (2014) performed an initial comparison between GRU and LSTM and reported mixed results."
    }, {
      "heading" : "4. Evaluation Setup",
      "text" : "The focus of our study is to compare different LSTM variants, and not to achieve state-of-the-art results. Therefore, our experiments are designed to keep the setup simple and the comparisons fair. The vanilla LSTM is used as a baseline and evaluated together with eight of its variants. Each variant adds, removes or modifies the baseline in exactly one aspect, which allows to isolate their effect. Three different datasets from different domains are used to account for cross-domain variations.\nSince hyperparameter space is large and impossible to traverse completely, random search was used in order to obtain the best-performing hyperparameters (Bergstra & Bengio, 2012) for every combination of variant and dataset. Thereafter, all analyses focused on the 10% best performing trials for each variant and dataset (Section 5.1), making the results representative for the case of reasonable hyperparameter tuning efforts. Random search was also chosen for the added benefit of providing enough data for analyzing the general effect of various hyperparameters on the performance of each LSTM variant (Section 5.2)."
    }, {
      "heading" : "4.1. Datasets",
      "text" : "Each dataset is split into three parts: a training set, a validation set, which is used for early stopping and for optimizing the hyperparameters, and a test set for the final evaluation. Details of preprocessing for each dataset are provided in the supplementary material."
    }, {
      "heading" : "4.1.1. TIMIT",
      "text" : "The TIMIT Speech corpus (Garofolo et al., 1993) is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable. Our experiments focus on the frame-wise classification task for this dataset,\nwhere the objective is to classify each audio-frame as one of 61 phones. The performance is measured as classification error percentage. The training, testing and validation sets are split in line with Halberstadt (1998) into 3696, 400 and 192 sequences, having 304 frames on average."
    }, {
      "heading" : "4.1.2. IAM ONLINE",
      "text" : "The IAM Online Handwriting Database (Liwicki & Bunke, 2005) consists of English sentences as time series of pen movements that have to be mapped to characters. The network uses four input features: the change in x and y pen positions, the time since the current stroke started and a binary value indicating whether the pen is lifted. The performance is measured in terms of the Character Error Rate (CER) after decoding. The size of the dataset was halved by subsampling, which makes the experiments to run 2× faster without harming the performance. The training, testing and validation sets contained 5355, 2956, 3859 sequences with an average length of 334 frames."
    }, {
      "heading" : "4.1.3. JSB CHORALES",
      "text" : "JSB Chorales (Allan & Williams, 2005) is a polyphonic music modeling dataset. The preprocessed data consists of sequences of binary vectors and the task is next-step prediction. The performance metric used is the negative loglikelihood on the validation/test set. The complete dataset consists of 229, 76, and 77 sequences respectively with an average length of 61."
    }, {
      "heading" : "4.2. Network Architectures & Training",
      "text" : "Bidirectional LSTM (Graves & Schmidhuber, 2005), which consists of two hidden layers, one processing the input forwards and the other one backwards in time, both connected to a single softmax output layer, was used for TIMIT and IAM Online tasks. A normal LSTM with one hidden layer and a sigmoid output layer was used for the JSB Chorales task. As loss function we employed CrossEntropy Error for TIMIT and JSB Chorales, while for the IAM Online task the Connectionist Temporal Classification (CTC) Error by Graves et al. (2006) was used. The initial weights for all networks were drawn from a normal distribution with standard deviation of 0.1. Training was done using Stochastic Gradient Descent with Nesterov-style momentum (Sutskever et al., 2013) with updates after each sequence. The learning rate was rescaled by a factor of (1 − momentum). Gradients were computed using full BPTT for LSTMs (Graves & Schmidhuber, 2005). Training stopped after 150 epochs or once there was no improvement on the validation set for more than fifteen epochs."
    }, {
      "heading" : "4.3. LSTM Variants",
      "text" : "The vanilla LSTM from Section 2 is referred as Vanilla (V). The derived eight variants of the V architecture are the following:\n1. No Input Gate (NIG) 2. No Forget Gate (NFG) 3. No Output Gate (NOG) 4. No Input Activation Function (NIAF) 5. No Output Activation Function (NOAF) 6. No Peepholes (NP) 7. Coupled Input and Forget Gate (CIFG) 8. Full Gate Recurrence (FGR)\nThe first six variants are self-explanatory. The CIFG variant uses only one gate for gating both the input and the cell recurrent self-connection (an LSTM modification proposed in GRU (Cho et al., 2014)). This is equivalent to setting ft = 1− it instead of learning the forget gate weights independently. The FGR variant adds recurrent connections between all the gates as in the original formulation of the LSTM (Hochreiter & Schmidhuber, 1997). It adds nine additional recurrent weight matrices, thus significantly increasing the number of parameters."
    }, {
      "heading" : "4.4. Hyperparameter Search",
      "text" : "While there are other methods to efficiently search for good hyperparameters (cf. Snoek et al. 2012; Hutter et al. 2011), random search has a couple of advantages for our setting: it is easy to implement, trivial to parallelize and covers the search space more uniformly, thereby improving the follow-up analysis of hyperparameter importance.\nEach hyperparameter search consists of 200 trials (for a total of 5400 trials) of randomly sampling the following hyperparameters:\n• number of LSTM blocks per hidden layer: log-uniform samples from [20, 200];\n• learning rate: log-uniform samples from [10−6, 10−2]; • momentum: 1− log-uniform samples from [0.01, 1.0]; • standard deviation of Gaussian input noise: uniform\nsamples from [0, 1].\nIn the case of the TIMIT dataset, two additional (boolean) hyperparameters were considered (not tuned for the other two datasets):\nThe first one was the choice between traditional momentum and Nesterov-style momentum (Sutskever et al., 2013). Our analysis showed that this had no measurable effect on performance so the latter was arbitrarily chosen for all\nfurther experiments. The second one was whether to clip the gradients to the range [−1, 1]. This turned out to hurt overall performance,2 therefore the gradients were never clipped in the case of the other two datasets.\nNote that, unlike an earlier small scale study (Chung et al., 2014), the number of parameters was not kept fixed for all variants. Since different variants can utilize their parameters differently, fixing this number can bias comparisons."
    }, {
      "heading" : "5. Results & Discussion",
      "text" : "Each of the 5400 experiments was run on one of 128 AMD Opteron CPUs at 2.5 GHz and took 24.3 h on average to complete. This sums up to a total single-CPU computation time of just below 15 years. For TIMIT and JSB Chorales the test set performance of the best setup were 29.6% classification error (CIFG) and a log-likelihood of -8.38 (NIG) respectively. For the IAM Online dataset our best result was a Character Error Rate of 9.26% (NP) on the test set. The best previously published result is 11.5% CER by Graves et al. (2008) using a different and much more extensive preprocessing. 3"
    }, {
      "heading" : "5.1. Comparison of the Variants",
      "text" : "The twenty best runs (according to validation set performance) for each variant were compared with those for the baseline architecture (V). Welch’s t-test at a significance level of p = 0.05 was used4 to determine whether the mean test set performance of each variant was significantly different from that of the baseline. A summary of the results is shown in Figure 2. The box for each variant for which the mean differs significantly from the baseline is highlighted in blue. The mean number of parameters used by those twenty best performing networks are also shown as grey bar plots in the background."
    }, {
      "heading" : "5.1.1. GENERAL OBSERVATIONS",
      "text" : "The first important observation based on Figure 2 is that removing the output activation function (NOAF) or the forget gate (NFG) significantly hurt performance on all three datasets. Apart from the CEC, the ability to forget old information and the squashing of the cell state appear to be critical for the LSTM architecture. Indeed, without the out-\n2Although this may very well be the result of the range having been chosen too tightly.\n3Note that these numbers differ from the best test-set performances that can be found in Figure 2. This is the case because here we only report the single best performing setup as determined on the validation set. In Figure 2 on the other hand we show the test-set performance of the 20 best setups for each variant.\n4We applied the Bonferroni adjustment to correct for performing eight different tests (one for each variant).\nput activation function, the block output can in principle grow unbounded. Coupling the input and the forget gate avoids this problem and might render the use of an output non-linearity less important, which could explain why GRU performs well without it.\nInput and forget gate coupling (CIFG) did not significantly change mean performance on any of the datasets, although the best performance improved slightly on music modelling. Similarly, removing peephole connections (NP) also did not lead to significant changes, but the best performance improved slightly for handwriting recognition. Both of these variants simplify LSTMs and reduce the computational complexity, so it might be worthwhile to incorporate these changes into the architecture.\nAdding full gate recurrence (FGR) did not significantly change performance on TIMIT or IAM Online, but led to worse results on the JSB Chorales dataset. Given that this variant greatly increases the number of parameters, we generally advice against using it. Note that this feature was present in the original proposal of the LSTM (Hochreiter & Schmidhuber, 1995; 1997), but has been absent in all following studies."
    }, {
      "heading" : "5.1.2. TASK-SPECIFIC OBSERVATIONS",
      "text" : "Removing the input gate (NIG), the output gate (NOG) and the input activation function (NIAF) led to a significant reduction in performance on speech and handwriting recognition. However, there was no significant effect on music modelling performance. A small (but statistically insignif-\nicant) average performance improvement was observed for the NIG and NIAF architectures on music modeling. We hypothesize that these behaviors will generalize to similar problems such as language modeling. For supervised learning on continuous real-valued data (such as speech and handwriting recognition), the input gate, output gate and input activation function are all crucial for obtaining good performance."
    }, {
      "heading" : "5.2. Impact of Hyperparameters",
      "text" : "The fANOVA framework for assesing hyperparameter importance by Hutter et al. (2014) is based on the observation that marginalizing over dimensions can be done efficiently in regression trees. This allows predicting the marginal error for one hyperparameter while averaging over all the others. Traditionally this would require a full hyperparameter grid search, whereas here the hyperparameter space can be sampled at random.\nAverage performance for any slice of the hyperparameter space is obtained by first training a regression tree and then summing over its predictions along the corresponding subset of dimensions. To be precise, a random regressionforest of 100 trees is trained and their prediction performance is averaged. This improves the generalization and allows for an estimation of uncertainty of those predictions. The obtained marginals can then be used to decompose the variance into additive components using the functional ANalysis Of VAriance (fANOVA) method (Hooker, 2007) which provides an insight into the overall importance of hyperparameters and their interactions."
    }, {
      "heading" : "5.2.1. ANALYSIS OF VARIANCE",
      "text" : "Figure 3 shows what fraction of the test set performance variance can be attributed to different hyperparameters. It is obvious that the learning rate is by far the most important hyperparameter, always accounting for more than two thirds of the variance. The next most important hyperparameter is the hidden layer size, followed by the input noise, leaving the momentum with less than one percent of the variance. Higher order interactions play an important role in the case of TIMIT, but are much less important for the other two data sets. The hyperparameter interplay is further discussed in Section 5.2.6."
    }, {
      "heading" : "5.2.2. LEARNING RATE",
      "text" : "Learning rate is the most important hyperparameter, therefore it is very important to understand how to set it correctly in order to achieve good performance. Figure 4 shows (in blue) how setting the learning rate value affects the predicted average performance on the test set. It is important to note that this is an average over all other hyperparameters and over all the trees in the regression-forest. The\nshaded area around the curve indicates the standard deviation over tree predictions (not over other hyperparameters), thus quantifying the reliability of the average. The same is shown in green with the predicted average training time.\nThe plots in Figure 3 show that the optimal value for the learning rate is dependent on the dataset. For each dataset, there is a large basin (up to two orders of magnitude) of good learning rates inside of which the performance does not vary much. A related but unsurprising observation is that there is a sweet-spot for the learning rate at the high end of the basin.5 In this region, the performance is good and the training time is small. So while searching for a good learning rate for the LSTM, it is sufficient to do a coarse search by starting with a high value (e.g. 1.0) and dividing it by ten until performance stops increasing.\nFigure 3 also shows that the fraction of variance caused by the learning rate is much bigger than the fraction due to interaction between learning rate and hidden layer size (some part of the “higher order” piece, for more see Section 5.2.6). This suggests that the learning rate can be quickly tuned on a small network and then used to train a large one."
    }, {
      "heading" : "5.2.3. HIDDEN LAYER SIZE",
      "text" : "Not surprisingly the hidden layer size is an important hyperparameter affecting the LSTM network performance. As expected, larger networks perform better, and the required training time increases with the network size."
    }, {
      "heading" : "5.2.4. INPUT NOISE",
      "text" : "Additive Gaussian noise on the inputs, a traditional regularizer for neural networks, has been used for LSTM as well. However, we find that not only does it almost always hurt performance, it also slightly increases training times. The only exception is TIMIT, where a small dip in error for the range of [0.2, 0.5] is observed."
    }, {
      "heading" : "5.2.5. MOMENTUM",
      "text" : "One unexpected result of this study is that momentum affects neither performance nor training time in any significant way. This follows from the observation that for none of the datasets, momentum accounted for more than 1% of the variance of test set performance. It should be noted that for TIMIT the interaction between learning rate and momentum accounts for 2.5% of the total variance, but as with learning rate × hidden size (cf. Section 5.2.6) it does not reveal any interpretable structure. This may be the result of our choice to scale learning rates dependent on momentum (Section 4.2). These observations suggest that momentum\n5Note that it is outside the plotted range for IAM Online and JSB Chorales.\ndoes not offer substantial benefits when training LSTMs with online stochastic gradient descent. It may, however, be more important in the case of batch training, where the gradients are less noisy."
    }, {
      "heading" : "5.2.6. INTERACTION OF HYPERPARAMETERS",
      "text" : "Here we focus on the higher order interactions for the TIMIT dataset, for which they were strongest, but our analysis revealed very similar behavior for the other datasets:\nlearning rate× hidden size = 6.7% learning rate× input noise = 4.4% hidden size× input noise = 2.0%\nlearning rate×momentum = 1.5% momentum× hidden size = 0.6% momentum× input noise = 0.4%\nThe interaction between learning rate and the hidden size is the strongest one, but Figure 5 does not reveal any systematic dependence between the two. In fact it may be the case that more samples would be needed in order to properly analyse the fine interplay between them, but given our observations so far this might not be worth the effort. In any case, it is clear that varying the hidden size does not change the region of optimal learning rate."
    }, {
      "heading" : "6. Conclusion",
      "text" : "This paper reports the results of a large scale study on variations of the LSTM architecture. We conclude that:\n• The most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets and using any of eight possible modifications does not significantly improve the LSTM performance.\n• Certain modifications such as coupling the input and forget gates or removing peephole connections simplify LSTM without significantly hurting performance.\n• The forget gate and the output activation function are the critical components of the LSTM block. While the first is crucial for LSTM performance, the second is necessary whenever the cell state is unbounded.\n• Learning rate and network size are the most crucial tunable LSTM hyperparameters. Surprisingly, the use of momentum was found to be unimportant (in our setting of online gradient descent). Gaussian noise on the inputs was found to be moderately helpful for TIMIT, but harmful for other datasets.\n• The analysis of hyperparameter interactions revealed that even the highest measured interaction (between learning rate and network size) is quite small. This implies that the hyperparameters can be tuned independently. In particular, the learning rate can be calibrated first using a fairly small network, thus saving a lot of experimentation time.\nNeural networks can be tricky to use for many practitioners compared to other methods whose properties are already well understood. This has remained a hurdle for\nnewcomers to the field since a lot of practical choices are based on the intuitions of experts, and experiences gained over time. With this study, we have attempted to back some of these intuitions with experimental results. We have also presented new insights, both on architecture selection and hyperparameter tuning for LSTM networks which have emerged as the method of choice for solving complex sequence learning problems. In future work, we will explore more complex modifications of the LSTM architecture."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by the Swiss National Science Foundation grants “Theory and Practice of Reinforcement Learning 2” (#138219) and “Advanced Reinforcement Learning” (#156682), and by EU projects “NASCENCE” (FP7-ICT-317662) and “NeuralDynamics” (FP7-ICT-270247)."
    }, {
      "heading" : "A. LSTM formulas",
      "text" : "Here we repeat the vectorized formulas for a vanilla LSTM layer forward pass from the paper, and then present the formulas for the backward pass. We also provide formulas for all the studied variants.\nA.1. Forward Pass\nHere we reproduce the formulas of the forward pass from the paper, but we split all gates and the block input into activity before (?̄) and after non-linearity (?).\nLet N be the number of LSTM blocks and M the number of inputs. Then we get the following weights:\n• Input weights: Wz , Ws, Wf , Wo ∈ RN×M\n• Recurrent weights: Rz , Rs, Rf , Ro ∈ RN×N\n• Peephole weights: ps, pf , po ∈ RN\n• Bias weights: bz , bs, bf , bo ∈ RN\nAs in the paper we have xt as the input vector at time t, σ, g and h are pointwise non-linear functions with σ(x) =\n1 1+e−x being the logistic sigmoid. The pointwise multiplication of two vectors is denoted with .\nz̄t = Wzx t + Rzy t−1 + bz\nzt = g(z̄t) block input\nīt = Wix t + Riy t−1 + pi ct−1 + bi it = σ(̄it) input gate\nf̄ t = Wfx t + Rfy t−1 + pf ct−1 + bf f t = σ(f̄ t) forget gate\nct = zt it + ct−1 f t cell ōt = Wox t + Roy t−1 + po ct + bo\not = σ(ōt) output gate\nyt = h(ct) ot block output\nA.2. Backpropagation Through Time\nHere ∆t is the vector of deltas passed down from the layer above. If E is the loss function it formally corresponds\nto ∂E∂yt , but not including the recurrent dependencies. The deltas inside the LSTM block are then calculated as follows:\nδyt = ∆t + RTz δz t+1 + RTi δi t+1 + RTf δf t+1 + RTo δo t+1 δot = δyt h(ct) σ′(ōt) δct = δyt ot h′(ct) + po δot + pi δit+1\n+ pf δf t+1 + δct+1 f t+1\nδf t = δct ct−1 σ′(f̄ t) δit = δct zt σ′(̄it) δzt = δct it g′(z̄t)\nThe deltas for the inputs are only needed if there is a layer below that needs training:\nδxt = WTz δz t + WTi δi t + WTf δf t + WTo δo t\nFinally the gradients for the weights are calculated like this, where again ? can be any of {z, i, f ,o}:\nδW? = T∑ t=0 〈δ?t,xt〉 δpi = T−1∑ t=0 ct δit+1\nδR? = T−1∑ t=0 〈δ?t+1,yt〉 δpf = T−1∑ t=0 ct δf t+1\nδb? = T∑ t=0 δ?t δpo = T∑ t=0 ct δot\nHere 〈?1, ?2〉 denotes the outer product of two vectors.\nA.3. Variants\nWe only report differences to the formulas from Section A.1:\n1. No Input Gate (NOG): it = 1\n2. No Forget Gate (NFG): f t = 1\n3. No Output Gate (NIG): ot = 1\n4. No Input Activation Function (NIAF): g(x) = x\n5. No Output Activation Function (NOAF): h(x) = x\n6. Coupled Input and Forget Gate (CIFG):\nf t = 1− it\n7. No Peepholes (NP):\nīt = Wix t + Riy t−1 + bi\nf̄ t = Wfx t + Rfy t−1 + bf\nōt = Wox t + Roy t−1 + bo\n8. Full Gate Recurrence (FGR):\nīt =Wix t + Riy t−1 + pi ct−1 + bi + Riii t−1 + Rfif t−1 + Roio t−1\nf̄ t =Wfx t + Rfy t−1 + pf ct−1 + bf + Rif i t−1 + Rff f t−1 + Rofo t−1\nōt =Wox t + Roy t−1 + po ct−1 + bo + Rioi t−1 + Rfof t−1 + Rooo t−1"
    }, {
      "heading" : "B. Datasets",
      "text" : "This section provides details on the datasets and their preprocessing that were used in the LSTM comparison tasks.\nB.1. TIMIT\nWe use the TIMIT Speech corpus (Garofolo et al., 1993) for framewise phone classification. The full set of 61 phones were used as targets. From the raw audio we extract 12 Mel Frequency Cepstrum Coefficients (MFCCs) (Mermelstein, 1976) + energy over 25ms hamming-windows with stride of 10ms and a pre-emphasis coefficient of 0.97. These 13 inputs along with their first and second derivatives comprise the 39 inputs to the network and are normalized to have mean 0 and variance of 1.\nWe restrict our study to an established subset of the full TIMIT corpus as detailed by Halberstadt (1998). In short that means we only use the core tests set and drop the SA samples from the training set. For validation we use some of the discarded samples from the full test set.\nB.2. IAM Online\nThe IAM On-Line Handwriting Database (IAM-OnDB; Liwicki & Bunke 2005)6 was used for the handwriting experiments in the IAM Online task. The IAM-OnDB dataset splits into one training, two validation sets and one test set, having 775, 192, 216 and 544 boards each. Each board, see 6(a), contains multiple hand-written lines. Each line splits into strokes represented by sequences of 3-dimensional vectors of x, y (a pen position) and t (time) coordinates. Begins and ends of the characters within each stroke are not explicitely marked. The stroke data were joint together and a fourth dimension that contains value of 1 at the time of the pen lifting (a transition to the next stroke) and zeroes at all other time steps. Each handwriting line is accompanied with a target character sequence, see 6(b) assembled from the following 81 ASCII characters:\nabcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ 0123456789 !\"#&\\’()*+,-./[]:;?\nThe board labeled as a08-551z (in the training set) contains a sequence of 11 percent (%) characters that does not have an image in the strokes and the percent character does not occur in any other board. The board was removed from the experiments.\n6The IAM-OnDB was obtained from http://www. iam.unibe.ch/fki/databases/iam-on-linehandwriting-database\nThe two validation sets were joint together. The training, validation and testing sets contain 5 355, 2 956 and 3 859 lines. The sequences were subsampled to half the length (they still contain enough information but it speeds up the training). Instead of absolute pen positions their differences were used. The data was standardized. No additional preprocessing (like base-line straightening, cursive correction etc.) was used. The CTC-error function by Graves et al. (2006) was used for labeling the 81 characters and bestpath decoding was used for determining the Character Error Rate.\nB.3. JSB Chorales\nJSB Chorales is a collection of 382 four-part harmonized chorales by J. S. Bach (Allan & Williams, 2005), consisting of 202 chorales in major keys and 180 chorals in minor keys. We used the preprocessed piano-rolls provided by Boulanger-Lewandowski et al. (2012) currently available at http://www-etud.iro.umontreal. ca/˜boulanni/icml2012. These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note."
    }, {
      "heading" : "C. Additional plots",
      "text" : "Here we present some additional plots that didn’t make it into the paper.\nC.1. Full Boxplot for all Variants\nIn Figure 7 a box-whiskers-plot of the performance over all 200 runs (in contrast to only the top 20 as in the paper) is shown for every variant.\nC.2. Performance and Time Scatterplots\nA scatterplot of training time vs performance for all runs can be seen in Figure 9, 10, and 11. The individual variants are shown with different markers. We were hoping to identify some clusters, along the pareto front of that tradeoff. But no such structure could be found.\nC.3. Hyperparameter Interactions\nIn Figure 8 we visualize the interaction between all pairs of hyperparamters. It is divided vertically into three subplots, one for every dataset (TIMIT, IAM Online, and JSB Chorales). The subplots itself are divided horizontally into two parts, each containing a lower triangular matrix of heatmaps. The rows and columns of these matrices represent the different hyperparameters (learning rate, momentum, hidden size, and input noise) and there is one heatmap for every combination. The color encodes the performance as measured by the Classification Error for TIMIT, Character Error Rate for IAM Online and Negative Log-Likelihood for the JSB Chorales Dataset. For all datasets low (blue) is better than high (red).\nEach heatmap in the left part shows marginal performance for different values of the respective two hyperparamers. This is the average performance predicted by the decision forest when marginalizing over all other hyperparameters. So each one is the 2D version of the performance plots from Figure 4 in the paper.\nThe right side employs the idea of ANOVA to better illustrate the interaction between the hyperparameters. So we removed the variance of performance that can be explained by varying a single hyperparameter and only plot what is left. For the case that two hyperparameters do not interact\nat all (are perfectly independent) that residual would be all zero (grey).\nSo look for example at the pair hidden size and learning rate on the left side for the TIMIT dataset. We can see that in general performance varies strongly along the x-axis (learning rate), first decreasing and then increasing again. This is what we would expect knowing the valley-shape of the learning rate from Figure 4 in the paper. Along the yaxis (hidden size) performance seems to decrease slightly from top to bottom. Again this is roughly what we would expect.\nNow let’s look at the same pair on the right side. This plot shows how the heatmap on the left differs from the case of the two hyperparameters being independent. So here a blue pixel means, that the marginal error for this combination of learning rate and hidden size is lower (better) than you would expect. You will notice the scale is much smaller for the right side (-3 to 3 as opposed to 32 to 60) and still many of the heatmaps are close to grey."
    } ],
    "references" : [ {
      "title" : "Harmonising chorales by probabilistic inference",
      "author" : [ "References Allan", "Moray", "Williams", "Christopher KI" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Allan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Allan et al\\.",
      "year" : 2005
    }, {
      "title" : "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription",
      "author" : [ "Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal" ],
      "venue" : null,
      "citeRegEx" : "Boulanger.Lewandowski et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Boulanger.Lewandowski et al\\.",
      "year" : 2012
    }, {
      "title" : "DARPA TIMIT AcousticPhonetic Continuous Speech Corpus CD-ROM",
      "author" : [ "JS Garofolo", "LF Lamel", "WM Fisher", "JG Fiscus", "DS Pallett", "Dahlgren", "NL" ],
      "venue" : "National Institute of Standards and Technology, NTIS Order No PB91-505065,",
      "citeRegEx" : "Garofolo et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Garofolo et al\\.",
      "year" : 1993
    }, {
      "title" : "Heterogeneous acoustic measurements and multiple classifiers for speech recognition",
      "author" : [ "Halberstadt", "Andrew K" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "Halberstadt and K.,? \\Q1998\\E",
      "shortCiteRegEx" : "Halberstadt and K.",
      "year" : 1998
    }, {
      "title" : "IAM-OnDB-an on-line English sentence database acquired from handwritten text on a whiteboard",
      "author" : [ "Liwicki", "Marcus", "Bunke", "Horst" ],
      "venue" : "In Document Analysis and Recognition,",
      "citeRegEx" : "Liwicki et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Liwicki et al\\.",
      "year" : 2005
    }, {
      "title" : "Distance measures for speech recognition: Psychological and instrumental",
      "author" : [ "P. Mermelstein" ],
      "venue" : "Pattern Recognition and Artificial Intelligence,",
      "citeRegEx" : "Mermelstein,? \\Q1976\\E",
      "shortCiteRegEx" : "Mermelstein",
      "year" : 1976
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "TIMIT The TIMIT Speech corpus (Garofolo et al., 1993) is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable.",
      "startOffset" : 30,
      "endOffset" : 53
    } ],
    "year" : 2015,
    "abstractText" : "Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (≈ 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.",
    "creator" : "LaTeX with hyperref package"
  }
}