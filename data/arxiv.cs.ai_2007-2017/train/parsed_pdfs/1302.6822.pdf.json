{
  "name" : "1302.6822.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Logic for Default Reasoning About Probabilities",
    "authors" : [ "Manfred Jaeger" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nIt has often been noted that \"probability\" is a term with dual use: it can be applied to the frequency of occurrence of a specific property in a large sample of objects, and to the degree of belief granted to a propo sition. While some have argued that only one of these two in terpretations captures the true meaning of probability [Jay78], others have tried to analyze both usages of the term in their own right, and to clarify the relationship between the two aspects of probability. Carnap was among the first to do this ([Car50]). Even though his interest lies primarily with probabilities as subjective beliefs (or \"degrees of confirmation\"), he also formulates direct (inductive) inference as a prin ciple to arrive at subjective beliefs on the basis of given relative frequencies: when it is known that objects from a class cl also are members of a class c2 with a frequency p, and a specific object a is believed to be long to cl' then the given statistical information may be used as a justification for assigning p as a degree of belief to a's belonging to c2. When, instead of firmly believing that a is an element of C1, one only has several conflicting pieces of evi dence about the true nature of a, these can be com bined to form a degree of belief for a being in c2 by\nusing Jeffrey's rule [Jef65], as illustrated in the follow ing example.\nExample 1.1 Scanning channels on TV we have tuned in to a mystery film. It looks interesting, but we only want to continue watching it, if a happy ending seems likely. By what we have seen so far, we judge the film to be ei ther American, French, or English, with a likelihood of 0.2, 0.6, and 0.2 respectively. From our extensive past experience with mystery films we know that 8 out of 10 American films have a happy end, while this figure is 1 and 5 out of 10 for French and English productions, respectively. Jeffrey's rule in this situation states, that our degree of belief in a happy ending of the film we are currently watching should be given by\n0.2 X 0.8 + 0.6 X 0.1 + 0.2 X 0.5 = 0.32. (1) Hence, we better switch channels.\nIt must be noted at this place that calling the infer ence in this example by the name of Jeffrey's rule is a somewhat loose terminology: when Jeffrey originally stated his rule he was concerned with updating prior subjective beliefs to posterior subjective beliefs in the light of newly obtained evidence, not with using statis tical information to define degrees of belief. Hence, the terms for the conditional probabilities of a happy end given the origin of a film that appear in (1) would be some prior conditional beliefs in Jeffrey's rule, rather than statistical expressions. If, however, the funda mental assumption is made that in the absence of any specific information about an object, the subjective be liefs held about the object are governed by the statis tical information available for the domain from which it is taken, then conditional belief and conditional sta tistical probability can be equated, and the rule for deriving degrees of belief from statistical information exemplified by (1) be identified with Jeffrey's rule. Note, too, that this assumption also underlies the di rect inference principle, which from this perspective then can be seen as a special case of Jeffrey's rule - special both in the way prior beliefs are defined\nA Logic for Default Reasoning About Probabilities 353\nthrough relative frequencies and in that the new ev idence concerning the nature of an object takes the form of one certain fact. The kind of probabilistic inference illustrated in exam ple 1.1 might be called default reasoning about prob abilities. While this should be clearly distinguished from logical default reasoning (e.g. [McC80], [Rei80]), it shares the nonmonotinicity of these logics: in the light of additional (probabilistic or definite) informa tion, earlier inferences may be retracted. Recently, proposals have been made to incorporate the two kinds of probabilistic statements in an extension of first-order predicate logic [Ha190], [Bac90J. Here, statistical information and subjective beliefs are mod elled by probability measures on the domains of first order structures and sets of possible worlds, respec tively. While this is an intuitively appealing interpre tation of the formulas, it does not allow for the kind of reasoning exemplified by Jeffrey's rule. The proba bility measures on the domain and the possible worlds can be chosen independently in such a way that all the formulas in the given knowledge base are satisfied, but no interaction between the two kinds of proba bilistic statements takes place. Indeed, Halpern writes [Hal90]: \"Although 2'3 ( «P) [the combined probabilistic logic in question] allows arbitrary alternation of the two types of probability, the semantics does support the intuition that these really are two fundamentally different types of probability.\" An additional strategy to arrive at subjective be liefs on the basis of statistical information is devel oped in [Bac91], [GHK92a], [GHK92b], [BGHK92], and [BGHK93]. This strategy, which is based on di rect inference, has the great disadvantage that it does not allow for any given subjective beliefs to be used for arriving at new degrees of belief. Hence, even Jef frey's rule is beyond the scope of reasoning that can be carried out in this framework. On the other hand, very specific degrees of belief are assigned to propo sitions even in the absence of any information: on the basis of an empty knowledge base, the proposi tions American(this-film) and American(this-film)l\\ Happy_end(this-film) would be assigned a degree of belief of 0.5 and 0.25 respectively. The formalism presented in this paper, though mo tivated by similar intuitions as the above mentioned, exhibits rather different properties. Among them are:\n• The expressive power of the language used is smaller than in [Hal90], [Bac90]. Notably, expres sions about statistical and subjective probabilities can only be combined in a restricted way.\n• Both statistical and subjective probabilities can be specified in a knowledge base, and new proba bilities of both types be inferred. While the statis tical probabilities entailed by the knowledge base essentially depend on the given statistical infor mation only, the resulting subjective beliefs de-\npend crucially on both types of probabilistic state ments.\n• When only partial statistical information is avail able (as is usually the case), no default assump tions about the statistical probabilities are made. As a result, it will usually only be possible to infer probability intervals rather than unique probabil ity values from a knowledge base\nThe basic idea on which the formalism to be de fined in the following sections is based, is to inter pret both types of probabilistic expressions by prob ability measures on a common probability space. In the example above it is noticeable that both the statements about the relative frequency for happy endings and the subjective assignment of likelihood to the predicates American, French and English, are basically constraints on a probabil ity measure on the formulas in the vocabulary S::;: {American, French, English, Happy_end}, where in some cases a constant \"this_film\" enters as a param eter. When deductions from the knowledge base are made, it is again probabilities on these formulas that are to be inferred. Default reasoning about probabil ities can now be viewed as the process of selecting a probability measure on the expressions this-film E ¢> (with if; E Ls, i.e. a first-order formula over S) that most closely resembles the probability measure gener ally assigned to Ls on the basis of the given statistical information. While it would be desirable, to work with probability measures on the abstract syntactic structure Ls itself (as has been done for terminological logics in [Jae94]), it proves much easier, in the more general framework of first-order predicate logic, to use probability mea sures on the domain of an interpretation to induce a probability measure on Ls.\n2 SYNTAX\nAs mentioned above, it will not be possible to freely combine expressions about the two different lcinds of probabilities. Hence, two distinct extensions of the syntax of first-order logic have to be provided. The following notational conventions will be used in the sequel: tuples (vo, . . . , Vn- d , (ao, . . . , ak-d of variable or constant symbols are abbreviated by v, a. When it is necessary to explicitly note the length of a tuple, the notation �' � may be used. ¢>(v) is used to denote a formula ¢> whose free variables are among Vo, ... ,Vn-1· Definition 2.1 Let S be a vocabulary containing relation-, function-, and constant-symbols. A statis tical formula in S is any formula that can be con structed from S by the syntax rules of first-order pred icate logic with equality together with the new rule:\n354 Jaeger\n• If ¢( v) and 1/;( v) are statistical formulas in S, {Vi1, ... ,vik} � {vo, ... ,v,_I} , and p E [0 , 1], then\n[¢(v) 11/J(v)] k 2:: p (v&) is a statistical formula in S. The free variables in this formula are the free variables of ¢ and 1/J without { Vi1, • • • , Vik}.\nThe set of statistical formulas in S is denoted by LS'. A statistical formula with no free variables is a statistical sentence.\nDefinition 2.2 Let s be as above, and { ao, ... , an-I} a set of constant symbols not in S. A subjective prob ability sentence for a in S is any sentence of the form\nprob(¢[a]I1/J[a]) 2:: p\nwith ¢(v), 1/;(v) E 15, and p E [0, 1]. L�(a) denotes the set of all these sentences.\nThe abbreviations [¢ 11/J](...) :::; p and prob(¢ 11/1) :::; p may be used for [...,¢ 11/Jk .. J 2:: 1- p and prob(...,¢ I 1/J) 2:: 1 - p respectively. Similarly, [¢ I 1/Jk .. J = p, [¢ I 1/Jk .. J < p and [¢ I 1/;](. .. ) > pare defined. Also, prob(¢ 11/J) = p may be substituted for the pair of sentences prob( ¢ I 1/J) 2:: p and prob( ¢ I 'ljJ) :::; p. Note, however, that prob( ¢ I 'ljJ) < p would have to be defined by means of the negation of prob( ¢ I 'ljJ) 2:: p, and such a negation is not within the syntax given by definition 2.2. Finally, [¢]c .. ) 2:: p and prob(¢) 2:: pare used for [¢I -r]( ... ) 2:: p and prob(¢ I -r) 2:: p, where Tis any tautology. Definition 2. 1 is standard and can be found similarly in [Kei85], [Hal90], [Bac90]. Definition 2.2 differs from its counterparts in [Hal90] and [Bac90] in that prob(¢ I 1/J) 2:: p is seen as a statement about a distinguished subset of the constant symbols appearing in ¢ and '1/J, and ¢, 'ljJ are not allowed to contain, in turn, a formula of the form prob(¢' I '1/J') 2:: p'.\nA knowledge base KB in the language here defined con sists of a finite set �o- of statistical sentences (which will typically also contain some purely first-order sen tences), and a finite set �,6(a) of subjective probability sentences for a.\nExample 2.3 The probabilistic knowledge from our introductory example can be �mbolized by a knowl edge base KB /I = �Movies U � (II) where /1 is a con stant symbol standing for the unknown film we are concerned with, and\n�Movies= [Happy_endv I Americanv 1\\ Mysteryv](v) = 0.8 (2) [Happy_endv I Englishv 1\\ Mysteryv](v) = 0.5 (3) [Happy_endv I Frenchv 1\\ Mysteryv](u) = 0.1 (4)\n�,6(!1) :=\nprob(Americanf1/\\ Mysteryf1) = 0.2 prob(Englishj1/\\ Mysteryj1) = 0.2 prob(Frenchj1 /\\ Mysteryj1) = 0.6\n3 SEMANTICS 3.1 OUTLINE\n(5) (6) (7)\nA semantical structure in which �o- can be interpreted is defined along the same lines as in [Kei85], [Hal90], [Bac90]:\nDefinition 3.1 A statistical $-structure is a structure (M, I), where\n• M = (M, !J.Jt, Jl) is a probability space with do main M, supplied with a cr-algebra !JJt and a prob ability measure Jl on !J.Jt.\n• I is an interpretation function that maps the relation-, function-, and constant-symbols in S to relations, functions and elements of M in such a way that for every formula ¢( v) E 15, the inter pretation I(¢) � M\"' is measurable with respect to the product cr-algebra !J.Jt\"'.\nThe condition imposed on I in this definition may seem highly restrictive; in fact, one may wonder whether statistical structures in the sense of definition 3.1 ac tually exist. While it is beyond the scope of this paper to embark on a thorough measure-theoretic discussion of the questions here involved, it should be pointed out, that whenever M is finite or countably infinite, then !JJt can be taken to be the power set 2M, and every subset of M\"' is measurable with respect to !J.Jt\"'. What about an interpretation for �,6(a)? As was indi cated in the introduction, subjective probability sen tences can be seen as making assertions about a prob ability measure on the formulas in the vocabulary S, with a just a name or parameter for this measure. Put another way, in the context of a fixed structure ( M ,I), a in this interpretation may be viewed as a random variable with values in Mn, and �,a(a) is a set of con straints on its distribution. Since these constraints only concern subsets of Mn that are definable by for mulas in 15, which, by definition 3.1, all belong to !J.Jtn, this leads to the following definition.\nDefinition 3.2 A probabilistic S-structure for a is a structure\n(M, I, va), where (M, I) is a statistical $-structure, and va is a probability measure on !JJt\"' .\nSome conditions are immediate for when a probabilis tic S-structure shall be called a model of a knowledge base KB = �o- u �,a(a): for a statistical $-structure\nA Logic for Default Reasoning About Probabilities 355\n(M, I), a valuation function v, and a statistical for mula(), the relation ((M, I), v) f= () is defined by augmenting the standard definition for first-order logic with the rule\n((M, I), v) f= [¢(v) 1 1/J(v)] k � p iff (vi) Jl( {(m1, ... , mk) E Mk I\nk k ((M,I), v[rn I vi]) f= ¢(v) !\\ 1/J(v)})\n� p x ltk({(m1, ... ,mk) EM\" I ((M,I), v(� I .Ji]) F 1/J(v)}).\nSimilarly, it will be required that for a probabilistic structure for a\n(M, I, Va) F prob(¢[a]l w[a]) � p only holds, if va( { (m1, ... , mn) E Mn I (M,I) F ¢[rn] !\\ 1/J[rn)})\n� p X Va({(ml,· . . ,mn) E Mn I (M,I) I= w[rnl}). However, this mere satisfaction of the constraints in ci>fi(a) is insufficient for va, as it does not establish any connection between the measures ltn and va. If the intuition is to be formalized, that lla should resemble ltn as much as possible within the limits drawn by ci>I'(a), then something more is required. First of all, the notion of \"resemblance\" has to be made precise. To this problem the following section is dedi cated.\n3.2 CROSS ENTROPY\nCross entropy ([Kul59]) commonly is interpreted as a \"measure of information dissimilarity\" for two proba bility measures [Sho86]. Usually, cross entropy is used in a rule to update a prior estimate for the probability distribution of some variable to a posterior estimate when some new information about the variable's ac tual distribution has been obtained. However, both its information theoretic interpretation and its unique properties make cross entropy also the most promising tool for bridging the gap between general statistical knowledge and subjective beliefs.\nFor a u-algebra 9J1 the set of probability measures on 9J1 is denoted by .0.VJ7. Let J.l, v E .0.9J1 with v « /t, i.e. for every A E 9J1: �t(A) = 0 =? v(A) = 0. In this case there exists a density function f for 11 with respect to J.l, and the cross entropy of 11 with respect to It can be defined by\nCE(v, �t) = J jlnj dji,. If v '¢:. jt, define CE(v, �t) := oo. For J..t E .0.VJ7 and a closed (with regard to the variation distance 1 ) and\n1The variation distance of VJ, v2 E .C:..OOl is defined as the integral J I /J - h I dv where v E tl.9TI is such that 1.11 « 11 and v2 « v (e.g. v == 1/2(vt + v2)), and f; is a density for v; with respect to v.\nconvex subset N <;;; .0..9Jl, which contains at least one 11 with CE(11, �t) < oo, there is a unique vo EN such that CE(vo,J..t) < CE(v, jt) for all11 E N, v # 11o [Csi75]. Denote this 11o by 1l'N (It). In the case that N is defined by a finite set of con straints\n{v(A;) � p x v(B;) I A;,B;EVJl, p;E[O,l], i=l, . . . ,k}\nthe following theorem reduces the problem of comput ing 1l'N (J.l) to aCE-minimization on a finite probability space.\nFor a subalgebra !JJl' <;;; VJl and J..t E .0.9JL the notation J..t I VJ7' is used for the restriction of J.l to VR'. N I VJ7' stands for { 11 I VJ7' I v E N}.\nTheorem 3.3 Let VJ70 be a finite subalgebra of 9J1 generated by a partition {A11 ... , Ad � 9J1 of M. Let J..L E .0.9Jl, and N <;;; .0.9J1 be defined by a set of con straints on v:nG, i.e. for all 11 E ll!JJl:\n11EN ¢::> viVJ1°ENf!J.Jil. Then 1l'N (It) is defined iff 7l' N r 91!0 (It r !JJl0) is defined, and in this case for every C E VJl:\nk nN(J..t)(C) = L 7l'Nrmo(�ti9Jtl)(A;) �-t(C I A;).\ni=l\nThe proof of this theorem is basically an application of property 9 from [SJ81]. Corollary 3.4 Let {A1, ... , Ad <;;; VJl be a partition of M, let N be defined by a set of constraints\nk {v(A;) = p; I i = l, . . . ,k}, L Pi = 1.\ni=l Then, for every probability measure J..L on 9J1 with p; > 0 =? p(A;) > 0 (i = 1, . . . , k), 7rN(P) is the prob ability measure obtained by applying Jeffrey's rule to It and the given constraints.\nCorollary 3.4 is a first indication that cross entropy might be the appropriate tool to model default reason ing about probabilities, and may serve as a preliminary justification for making cross entropy minimization the central element of the semantics for Ls u L�(a) now to be defined.\n3.3 THE FINAL SEMANTICS\nDefinition 3.5 Let (M, I, va) be a probabilistic S structure for a, KB = cl>\"\" U ci>i3(a) a knowledge base. (M, I, va) is a model of KB iff\n• (M, I) f= �.,. as defined in section 3.1.\n• With Bel(a) the set of probability measures on VJln that satisfy the constraints in ci>fl(a):\nlla = 1l'Bel(a)(J..tn).\n356 Jaeger\nBel(a) always is a closed (in the topology defined by the variation distance) and convex subset of .6.9Rn. To make sure that this will be the case is the reason for the restrictive syntax of L�(a). If it was allowed to express •prob(¢ I 1/J) � p for instance, then Bel(a) would need no longer be closed. Permitting disjunc tions prob( ... ) � p V prob( . . . ) � q destroys convex ity. Hence, by the remarks in section 3.2, there ex ists a measure lla satisfying the condition of definition 3.5 iff Bel(a) contains at least one measure 11 with 11 « Jl.n. When this is not the case, then the statisti cal S-structure (M, I) can not be extended to a model of KB. Should this be the case for all (M, I) f= 1>\", then KB does not have a model. Note that Bel(a) is defined by constraints on the finite subalgebra of mn generated by the finitely many sub sets of Mn defined by the formulas appearing in cpf'(a). Hence, theorem 3.3 applies to 11'Bel(a)(?ln), and even though p, and va generally are probability measures on infinite probability spaces, cross entropy minimization only has to be performed on finite probability spaces. The logic defined by definitions 2.1 ,2.2 and 3.5 is de noted guf3. For a knowledge base KB and a sentence(} E LsUL�(aJ the relation KB f= B is defined as usual. gcri> is monotonic with respect to 1>\", but non monotonic with respect to q>f'(a): if �\"' 2 1>\" and 1>\"\" U q>i3(a) f= (}, then �\"\" U q>i3(a) f= (} for every (} E Ls u L�(aJ. If, on the other hand, �!3(a) 2 q,i3(a), then 1>\"' u q>i3(a) F= e does not imply 1>\" u <i>!3{a) F= (}_\n4 WHY CROSS ENTROPY?\nCross entropy minimization, in the past, has received a considerable amount of attention as a rule for updating probability measure. Notably, Shore and Johnson have provided an axiomatic description of minimum cross entropy updating [SJ80], [SJ83]. They show that, if a function f is used to define for a closed and convex set N of continuous or discrete probability measures and a prior p,:\n7r�(p,) := {v EN I f(v, p,) = inf{f(v', ,u) !11' E N}}, and the mapping (,u, N) 1--7 1r� (p,) satisfies a set of five axioms, then the function f must in fact be equivalent to cross entropy. It is beyond the scope of this paper to also give an axiomatic justification for putting cross entropy min imization at the core of definition 3.5 by formulat ing a set of conditions that the consequence relation f= for guf3 should satisfy, and then show that only cross entropy minimization will fulfill these conditions. Instead, the two theorems contained in this section demonstrate that using cross entropy leads to very de sirable properties for .£\"13, and indicate, when looked at as axioms rather than theorems, what an axiomatic\njustification for the use of cross entropy in the seman tics of .£\"!3 would look like. The two theorems are directly derived from the two central axioms in [SJ80], subset independence and sys tem independence. The first one rephrases the prop erty of subset independence to a statement about log ical entailment in gcrf'.\nTheorem 4.1 Let ¢1(v), .. . ,(/Jk(v) E Ls. Let KB= 1>\"\" U q>i3(a) with\n1>\"' F Vv(¢1 (v)V ... v¢k(v)) (here v is the exclusive disjunction). Let\nq>P(a.) {prob (¢;[a]) �Pi I i = 1, .. . , k} U �Pf(a) U ... U I!>�( a)'\nwhere each I!>�( a) is of the form {prob(I/Jij[a]l ¢iJ[a]) � PiJ I j = 1, . . . , lk}\nfor some ¢;J with q,u f= ¢ij -+ ¢;. Then, for every i E {1,,.,, k} and every subjective probability formula (}of the form prob(I/J[a]l¢i[a]) � p:\n1>\"\" U 1>�(a) f= B � KB F 0.\nBy theorem 4.1, reasoning by cases is possible in .zcri3 under certain circumstances: if �i3(a) contains subjec tive beliefs that are each conditioned on one of several mutually exclusive hypotheses for a, then valid infer ences about subjective beliefs conditioned on one of these hypotheses can be made by ignoring the infor mation about the other hypotheses.\nExample 4.2 The prospects for a happy ending of the mystery film we have been watching not being very bright, we switch to a different channel where another film is running. This one can be easily identified as an American production, but it could be either a romance or a mystery:\nprob(Americanf2 1\\ Romancef2) prob(Americanf2 1\\ Mystery f2)\nAlso, we are ready to believe that\nprob(Happy_endf2 I\n0.5, (8) 0.5. (9)\nAmericanf2 1\\ Romancef2) = 0.95 (10)\nSuppose we are interested in estimating\nprob(Happy_endf21 Americanf2/\\ Mysteryf2). (*)\nBefore we are able to apply the statistical rule (2) in order to obtain this estimate, we make the additional observation that should f2 be a mystery, then it is not particularly likely to contain action scenes, contrary to what we generally expect from mystery films:\nprob(Actionf21 Americanf2/\\ Mysteryf2) = 0.5, (11) [Actionv I Americanv 1\\ Mysteryv](v) = 0.7. (12)\nA Logic for Default Reasoning About Probabilities 357\nThis information is relevant for our estimate of ( * ) because the existence of action scenes is correlated to a happy end by [Actionv I\nAmericanv 1\\ Mysteryv 1\\ -,Happy_endv](v) = 0.5. (13) Let KB /2 = �Movies u (bf3C f2l consist of q,Movies from example 2.3 and the new sentences (8)-(13). KB 12 is of the form defined in theorem 4.1 with\n¢1(v) = Americanv 1\\ Romancev, ¢2(v) Americanv 1\\ Mysteryv, and ¢3(v) •(¢1 (v) V ¢2(v)).\nBy theorem 4.1 we know that everything we can infer about ( * ) from the smaller knowledge base obtained by removing (8)-(10) from KB 12, also is valid with respect to KB 12. By elementary computations it can be seen that\n�Movies I= [Happy_endv I Americanv 1\\ Mysteryv 1\\ Actionv](u) = �, (14)\n.f.Movies F [Happy_endv I Americanv 1\\ Mysteryv 1\\ ...,Actionv](v) = �- (15)\nHence, with theorem 3.3 KB12 I= prob(Happy_endf21 Americanf2 1\\ Mysteryf2)\n6 2 16 = 0.5 X 7 + 0.5 X S = 21. (16)\nAlso, combining (8)-(10) and (16) we get\nprob(Happy_endf2) 16 = 0.5 X 0.95 + 0.5 X 21 � 0.856. (17)\nThe following theorem is derived from the system in dependence axiom.\nTheorem 4.3 Let KB= q,cr U q>f3(a), where q,f3(a) = q,f3(a.o, .. . ,a.k-d U .:pf3(a.h····a.,._,) 1\ni.e. the set of subjective probability formulas for a consists of two disjoint sets for (ao, ... ,ak-d and (ak, . . . , an-d· Suppose that q,cr U .:pf3(a.o, ... ,a.,_,) F\nprob(¢I[ao, . . . ,ak-d 11/Jdao, ... , ak-d) ;:::p1, (18) .:perU (bf3(ak,. .. ,an-d F\nprob(¢2[ak, ... ,an-I] I1/J2[ak, . . . ,an-d) ;:::p2. (19) Then KB f= prob(¢I [ao , ... , ak-d 1\\ ¢2[ak, . .. , an-d 11/Jt[ao, . . . , ak-d 1\\ 1/J2[ak,-.-, an-d) ;::: P1P2- (20) Theorem 4.3 remains true, when the inequality in (18) (20) is replaced with equality.\nCorollary 4.4 For KB as in the preceding theorem k and for every subjective belief formula B E L�(a): q,cr U .:pf3(ao,---,a.k-d F () ::::;.- KB F B.\nRoughly speaking, theorem 4.3 states, that when .:pi'( a) does not contain any information connecting one con stant a; with another constant aj, then these constants are interpreted as independent. Especially, ignoring the information about aJ still leads to valid inferences about a;.\nExample 4.5 Ultimately, we want to know which of the two films fl and f2 is likely to be the better one. Better is a predicate for which we have the axioms\n'livo•Bettervovo (21) 'livovl( vo \"I v1 � (Bettervov1 +-7 ·Betterv1 vo)) (22)\nand a useful statistic: [Bettervov1 I Happy_endvo 1\\ •Happy_endvd(vo,vi)\n= 0.95. (23) Let KB fl/2 be the union of KB fl , KB 12 and the sen tences (21 )-(23). From (21 )-(23) [Bettervov1 I vo =I- Vt 1\\ Happy _endvo\n/\\Happy_endvd(vo,vi) = 0.5 (24) [Bettervov1 I vo \"I v1 1\\ •Happy_endvo\nf\\-,Happy_endvl](vo,v,) 0.5 (25) [Bettervov1 I •Happy_endvo\n/\\Happy_endvl](vo,v1) = 0.05 (26) can be derived by exploiting the fact that we are dealing with product measures, and therefore, for all p E [0 , 1] : f= [Bettervov1 I vo =I- v1 1\\ Happy _endvo\n/\\Happy_endvl](vo,vl) ;::: p +-7 [Betterv1 vo I vo =I- v1 1\\ Happy _endvo\n/\\Happy_endvt](vo,vl) ;::: p. By our previous results (1) (formally justified by corol lary 3.4) and (17), and theorem 4.3, the probabil ities of the conditioning events in (23)-(26) for f1 and j2 are known to be 0.32 x (1 - 0.856) = 0.046, 0.32 X 0.856 = 0.274, (1 - 0.32) X (1 - 0.856) = 0.098 and (1 - 0.32) x 0.856 = 0.582 respectively. One fi nal application of Jeffrey's rule, sanctioned by theorem 3.3, then yields\nKB flf2 f= prob(Better f1f2) = 0.95 X 0.046 + 0.5 X 0.274 + 0.5 X 0.098 + 0.05 X 0.582 = 0.259,\nwhich is a suitable result to settle the question about which film we are going to watch.\nObviously, this example has been an extremely sim ple illustration of the given definitions and theorems throughout: neither will it be possible, in more real istic examples, to reduce cross entropy minimization\n358 Jaeger\nto an application of Jeffrey's rule, nor will the result ing probabilities usually be unique values rather than intervals.\n5 RELATED WORK\nIn [PV89] and [PV92] Paris and Vencovska consider basically the same inference problem as is discussed in the present paper. They assume that two types of probabilistic constraints on expressions in proposi tional logic are given: one type referring to general pro portions, the other to subjective beliefs about an indi vidual. Their approach to dealing with the dichotomy of the probabilistic information is quite different from the one here presented: it is proposed to transform the constraints on the subjective beliefs about an object a to statistical constraints conditioned on a newly intro duced propositional variable A representing an ideal reference class for a, i.e. the set of all elements that are \"similar to\" a. Then an additional constraint is added that the absolute probability of this set is very small. Thus, all the constraints can be viewed as being on one single probability distribution. Paris and Vencovska then explore different inference processes that can be applied to these constraints in order to obtain a single probability distribution on the propositional formulas. Most notably, they consider the maximum entropy ap proach, and show that when it is used the resulting conditional probability distribution on the variable A is just the distribution on the formulas not containing A that minimizes cross entropy with respect to the global distribution on these formulas under the con straints for a (more precisely, this will be the case for the limiting distribution when the absolute probability of A tends to zero) .\nThe techniques of probabilistic inference explored by Paris and Vencovska are quite different from the one discussed in this paper in that, as demanded by the uniform encoding of statistical and subjective prob abilities, one process of inference is applied to both kinds of information simultaneously. This makes Paris and Vencovska's paradigm for probabilistic inference a somewhat less likely framework for default reasoning about probabilities, where it is the key issue to give an interpretation of the subjective beliefs as a function of the interpretation of the statistical information.\nHowever, the mere semantic principle of interpreting subjective beliefs via conditional probabilities on a new reference class also allows for a separate processing of the constraints given for the domain in general and the constraints given with respect to the reference class. Thus, the two approaches of interpreting the subjec tive beliefs held about an object as either the condi tional distribution on a special reference class, or as an alternative measure on the domain as in .51af3, ba sically allow for the same scope of probabilistic rea soning. If it is intended, though, to clearly distinguish the reasoning about the statistics from the reasoning about beliefs - a separation pushed to the extreme in\nthe probabilistic logics of Bacchus et al. and Halpern - the second approach probably will lead to greater conceptual clarity.\n6 CONCLUSION\n.51af3 is a logic that models the forming of subjective beliefs about objects on the basis of statistical infor mation about the domain and already existing beliefs. The novelty of the approach here presented lies in the idea of interpreting constant symbols as probability measures over the domain, which leads to semantics that seem to be better suited to describe the interac tion of statistical and belief probabilities than possible worlds semantics. In order to make effective use of cross entropy minimization, a fairly restrictive syntax with regard to expressing subjective beliefs was intro duced.\nIt should be pointed out, though, that .51af3 is open to generalizations in various ways. Disjunctions and negations of subjective probability sentences might be allowed, in which case the condition Va = 'irBeL(a)(Jln) in definition 3.5 has to be replaced by the demand that va is one of the measures in the closure of Bel(a) that minimizes cross entropy with respect to Jl·\nAlso, interpreting constant symbols as probability measures over the domain is a feasible way to inter pret formulas in which statements of subjective belief and statistical relations are arbitrarily nested, thus al lowing to express statements like\n[prob(Betterf1v);::: 0.9] (v) ;::: 0.2 (\"for some (;::: 0.2) v it is believed that f1 is very likely (;::: 0.9) to be better than v). When formulas like these are allowed, however, it is more difficult to define what their proper default interpretation should be, because the interaction of statistical information and subjective beliefs can no longer be viewed as es sentially one-way only.\nAcknowledgement The author is greatful for some helpful remarks and suggestions received from an anonymous referee. Par ticularly, they contained a valuable clarification re garding the interrelation of direct inference and Jef frey's rule.\nReferences [Bac90]\n[Bac91]\nF. Bacchus. Representing and Reasoning With Probabilistic Knowledge. MIT Press, 1990.\nF. Bacchus. Default reasoning from statis tics. In Proc. National Conference on Ar tificial Intelligence ( AAAI-91), pages 392- 398, 1991.\n[BGHK92] F. Bacchus, A. Grove, J.Y. Halpern, and D. Koller. From statistics to beliefs. In\nA Logic for Default Reasoning About Probabilities 359\nProc. of National Conference on Artificial Intelligence (AAAI-92), 1992.\n[BGHK93] F. Bacchus, A. Grove, J.Y. Halpern, and D. Koller. Statistical foundations for de fault reasoning. In Proc. of International Joint Conference on Artificial Intelligence (IJCAI-93), 1993.\n[Car50] R. Carnap. Logical Foundations of Prob ability. The University of Chicago Press, 1950.\n[Csi75] I. Csiszar. !-divergence geometry of proba bility distributions and minimization prob lems. Annals of Probability, 3:146-158, 1975.\n[GHK92a] A.J. Grove, J.Y. Halpern, and D. Koller. Asymptotic conditional probabilities for first-order logic. In Proc. 24th ACM Symp. on Theory of Computing, 1992.\n[GHK92b] A.J. Grove, J.Y. Halpern, and D. Koller. Random worlds and maximum entropy. In Proc. 7th IEEE Symp. on Logic in Com puter Science, 1992.\n[Hal90] J .Y. Halpern. An analysis of first-order logics of probability. Artificial Intelligence, 46:311-350, 1990.\n[Jae94] M. Jaeger. Probabilistic reasoning in ter minological logics. In J. Doyle, E. Sande wall, and P. Torasso, editors, Principles of Knowledge Representation an Reason ing: Proceedings of the Fourth Interna tional Conference (KR94). Morgan Kauf mann, San Mateo, CA, 1994.\n[Jay78] E.T. Jaynes. Where do we stand on maximum entropy? In R.D. Levine and M. Tribus, editors, The Maximum Entropy Formalism, pages 15-118. MIT Press, 1978.\n[Jef65] R.C. Jeffrey. The Logic of Decision. McGraw-Hill, 1965.\n[Kei85] H.J. Keisler. Probability quantifiers. In J. Barwise and S. Feferman, editors, Model- Theoretic Logics, pages 509-556. Springer-Verlag, 1985.\n[Kul59] S. Kullback. Information Theory and Statistics. Wiley, 1959.\n[McC80] J. McCarthy. Circumscription - a form of non-monotonic reasoning. Artificial Intel ligence, 13:27-39, 1980.\n[PV89] J.B. Paris and A. Vencovska. On the ap plicability of maximum entropy to inexact reasoning. International Journal of Ap proximate Reasoning, 3:1-34, 1989.\n[PV92] J .B. Paris and A. Vencovska. A method for updating that justifies minimum cross entropy. International Journal of Approx imate Reasoning, 7:1-18, 1992.\n[Rei80]\n[Sho86]\n[SJ80]\n[SJ81]\n[SJ83]\nR. Reiter. A logic for default reasoning. Artificial Intelligence, 13:81-132, 1980.\nJ .E. Shore. Relative entropy, probabilistic inference, and ai. In L.N. Kanal and J.F. Lemmer, editors, Uncertainty in Artificial Intelligence. Elsevier, 1986.\nJ.E. Shore and R.W. Johnson. Axiomatic derivation of the principle of maximum en tropy and the principle of minimum cross entropy. IEEE Transactions on Informa tion Theory, IT-26(1):26-37, 1980.\nJ.E. Shore and R.W. Johnson. Proper ties of cross-entropy minimization. IEEE Transactions on Information Theory, IT27(4):472-482, 1981.\nJ.E. Shore and R.W. Johnson. Comments on and correction to \"Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy\". IEEE Transactions on Information The ory, IT-29(6):942-943, 1983."
    } ],
    "references" : [ {
      "title" : "Representing and Reasoning With Probabilistic Knowledge",
      "author" : [ "F. Bacchus" ],
      "venue" : "In Proc. National Conference on Ar­",
      "citeRegEx" : "Bacchus.,? \\Q1990\\E",
      "shortCiteRegEx" : "Bacchus.",
      "year" : 1990
    }, {
      "title" : "Statistical foundations for de­ fault reasoning",
      "author" : [ "F. Bacchus", "A. Grove", "J.Y. Halpern", "D. Koller" ],
      "venue" : "Proc. of International Joint Conference on Artificial Intelligence (IJCAI-93)",
      "citeRegEx" : "BGHK93",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Logical Foundations of Prob­ ability",
      "author" : [ "R. Carnap" ],
      "venue" : "The University of Chicago Press",
      "citeRegEx" : "Car50",
      "shortCiteRegEx" : null,
      "year" : 1950
    }, {
      "title" : "-divergence geometry of proba­ bility distributions and minimization prob­ lems",
      "author" : [ "I. Csiszar" ],
      "venue" : "Annals of Probability,",
      "citeRegEx" : "Csiszar.,? \\Q1975\\E",
      "shortCiteRegEx" : "Csiszar.",
      "year" : 1975
    }, {
      "title" : "Asymptotic conditional probabilities for first-order logic",
      "author" : [ "A.J. Grove", "J.Y. Halpern", "D. Koller" ],
      "venue" : "Proc. 24th ACM Symp. on Theory of Computing",
      "citeRegEx" : "GHK92a",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Random worlds and maximum entropy",
      "author" : [ "A.J. Grove", "J.Y. Halpern", "D. Koller" ],
      "venue" : "Proc. 7th IEEE Symp. on Logic in Com­ puter Science",
      "citeRegEx" : "GHK92b",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Artificial Intelligence",
      "author" : [ "J .Y. Halpern. An analysis of first-order logics of probability" ],
      "venue" : "46:311-350,",
      "citeRegEx" : "Hal90",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Jaynes",
      "author" : [ "E.T" ],
      "venue" : "Where do we stand on maximum entropy? In R.D. Levine and M. Tribus, editors, The Maximum Entropy Formalism, pages 15-118. MIT Press,",
      "citeRegEx" : "Jay78",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "The Logic of Decision",
      "author" : [ "R.C. Jeffrey" ],
      "venue" : "McGraw-Hill",
      "citeRegEx" : "Jef65",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Probability quantifiers",
      "author" : [ "H.J. Keisler" ],
      "venue" : "J. Barwise and S. Feferman, editors, Model- Theoretic Logics, pages 509-556. Springer-Verlag",
      "citeRegEx" : "Kei85",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Information Theory and Statistics",
      "author" : [ "S. Kullback" ],
      "venue" : "Wiley",
      "citeRegEx" : "Kul59",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Circumscription - a form of non-monotonic reasoning",
      "author" : [ "J. McCarthy" ],
      "venue" : "Artificial Intel­ ligence, 13:27-39",
      "citeRegEx" : "McC80",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "On the ap­ plicability of maximum entropy to inexact reasoning",
      "author" : [ "J.B. Paris", "A. Vencovska" ],
      "venue" : "International Journal of Ap­ proximate Reasoning, 3:1-34",
      "citeRegEx" : "PV89",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A method for updating that justifies minimum cross entropy",
      "author" : [ "J .B. Paris", "A. Vencovska" ],
      "venue" : "International Journal of Approx­ imate Reasoning,",
      "citeRegEx" : "Paris and Vencovska.,? \\Q1992\\E",
      "shortCiteRegEx" : "Paris and Vencovska.",
      "year" : 1992
    }, {
      "title" : "A logic for default reasoning",
      "author" : [ "R. Reiter" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Reiter.,? \\Q1980\\E",
      "shortCiteRegEx" : "Reiter.",
      "year" : 1980
    }, {
      "title" : "Relative entropy, probabilistic inference, and ai",
      "author" : [ "J .E. Shore" ],
      "venue" : "Uncertainty in Artificial Intelligence. Elsevier,",
      "citeRegEx" : "Shore.,? \\Q1986\\E",
      "shortCiteRegEx" : "Shore.",
      "year" : 1986
    }, {
      "title" : "Axiomatic derivation of the principle of maximum en­ tropy and the principle of minimum cross­ entropy",
      "author" : [ "J.E. Shore", "R.W. Johnson" ],
      "venue" : "IEEE Transactions on Informa­ tion Theory,",
      "citeRegEx" : "Shore and Johnson.,? \\Q1980\\E",
      "shortCiteRegEx" : "Shore and Johnson.",
      "year" : 1980
    }, {
      "title" : "Proper­ ties of cross-entropy minimization",
      "author" : [ "J.E. Shore", "R.W. Johnson" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Shore and Johnson.,? \\Q1981\\E",
      "shortCiteRegEx" : "Shore and Johnson.",
      "year" : 1981
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "While some have argued that only one of these two in­ terpretations captures the true meaning of probability [Jay78], others have tried to analyze both usages of the term in their own right, and to clarify the relationship between the two aspects of probability.",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "Carnap was among the first to do this ([Car50]).",
      "startOffset" : 39,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "When, instead of firmly believing that a is an element of C1, one only has several conflicting pieces of evi­ dence about the true nature of a, these can be com­ bined to form a degree of belief for a being in c2 by using Jeffrey's rule [Jef65], as illustrated in the follow­ ing example.",
      "startOffset" : 237,
      "endOffset" : 244
    }, {
      "referenceID" : 11,
      "context" : "[McC80], [Rei80]), it shares the nonmonotinicity of these logics: in the light of additional (probabilistic or definite) informa­ tion, earlier inferences may be retracted.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "An additional strategy to arrive at subjective be­ liefs on the basis of statistical information is devel­ oped in [Bac91], [GHK92a], [GHK92b], [BGHK92], and [BGHK93].",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "An additional strategy to arrive at subjective be­ liefs on the basis of statistical information is devel­ oped in [Bac91], [GHK92a], [GHK92b], [BGHK92], and [BGHK93].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "An additional strategy to arrive at subjective be­ liefs on the basis of statistical information is devel­ oped in [Bac91], [GHK92a], [GHK92b], [BGHK92], and [BGHK93].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "• The expressive power of the language used is smaller than in [Hal90], [Bac90].",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "1 is standard and can be found similarly in [Kei85], [Hal90], [Bac90].",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "1 is standard and can be found similarly in [Kei85], [Hal90], [Bac90].",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "2 differs from its counterparts in [Hal90] and [Bac90] in that prob(¢ I 1/J) 2:: p is seen as a statement about a distinguished subset of the constant symbols appearing in ¢ and '1/J, and ¢, 'ljJ are not allowed to contain, in turn, a formula of the form prob(¢' I '1/J') 2:: p' .",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "A semantical structure in which �o- can be interpreted is defined along the same lines as in [Kei85], [Hal90], [Bac90]:",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "A semantical structure in which �o- can be interpreted is defined along the same lines as in [Kei85], [Hal90], [Bac90]:",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "Cross entropy ([Kul59]) commonly is interpreted as a \"measure of information dissimilarity\" for two proba­ bility measures [Sho86].",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "In [PV89] and [PV92] Paris and Vencovska consider basically the same inference problem as is discussed in the present paper.",
      "startOffset" : 3,
      "endOffset" : 9
    } ],
    "year" : 2011,
    "abstractText" : "A logic is defined that allows to express in­ formation about statistical probabilities and about degrees of belief in specific proposi­ tions. By interpreting the two types of proba­ bilities in one common probability space, the semantics given are well suited to model the influence of statistical information on the for­ mation of subjective beliefs. Cross entropy minimization is a key element in these se­ mantics, the use of which is justified by show­ ing that the resulting logic exhibits some very reasonable properties.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}