{
  "name" : "1703.08748.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LEPOR: An Augmented Machine Translation Evaluation Metric",
    "authors" : [ "Lifeng Han", "Aaron Master", "LiFeng Han", "Lidia S. Chao", "Derek F. Wong" ],
    "emails" : [ "hanlifengaaron@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "LEPOR: An Augmented Machine Translation Evaluation Metric\nby"
    }, {
      "heading" : "Lifeng Han, Aaron",
      "text" : "Master of Science in Software Engineering\n2014"
    }, {
      "heading" : "Faculty of Science and Technology",
      "text" : "University of Macau\nLEPOR: AN AUGMENTED MACHINE TRANSLATION EVALUATION\nMETRIC\nby\nLiFeng Han, Aaron\nA thesis submitted in partial fulfillment of the\nrequirements for the degree of\nMaster of Science in Software Engineering\nFaculty of Science and Technology\nUniversity of Macau\n2014\nApproved by ___________________________________________________\nSupervisor\n__________________________________________________\n__________________________________________________\n__________________________________________________\nDate __________________________________________________________\nIn presenting this thesis in partial fulfillment of the requirements for a Master's degree at the University of Macau, I agree that the Library and the Faculty of Science and Technology shall make its copies freely available for inspection. However, reproduction of this thesis for any purposes or by any means shall not be allowed without my written permission. Authorization is sought by contacting the author at\nAddress:FST Building 2023, University of Macau, Macau S.A.R.\nTelephone: +(00)853-63567608\nE-mail: hanlifengaaron@gmail.com\nSignature ______________________\nDate __________________________\n2014.07.10 th\nUniversity of Macau\nAbstract\nLEPOR: AN AUGMENTED MACHINE TRANSLATION EVALUATION\nMETRIC\nby LiFeng Han, Aaron\nThesis Supervisors: Dr. Lidia S. Chao and Dr. Derek F. Wong\nMaster of Science in Software Engineering\nMachine translation (MT) was developed as one of the hottest research topics\nin the natural language processing (NLP) literature. One important issue in\nMT is that how to evaluate the MT system reasonably and tell us whether the\ntranslation system makes an improvement or not. The traditional manual\njudgment methods are expensive, time-consuming, unrepeatable, and\nsometimes with low agreement. On the other hand, the popular automatic MT\nevaluation methods have some weaknesses. Firstly, they tend to perform well\non the language pairs with English as the target language, but weak when\nEnglish is used as source. Secondly, some methods rely on many additional\nlinguistic features to achieve good performance, which makes the metric\nunable to replicateand apply to other language pairs easily. Thirdly, some\npopular metrics utilize incomprehensive factors, which result in low\nperformance on some practical tasks.\nIn this thesis, to address the existing problems, we design novel MT evaluation\nmethods and investigate their performances on different languages. Firstly, we\ndesign augmented factors to yield highly accurate evaluation.Secondly, we\ndesign a tunable evaluation model where weighting of factors can be\noptimized according to the characteristics of languages. Thirdly, in the\nenhanced version of our methods, we design concise linguistic feature using\nPOS to show that our methods can yield even higher performance when using\nsome external linguistic resources. Finally, we introduce the practical\nperformance of our metrics in the ACL-WMT workshop shared tasks, which\nshow that the proposed methods are robust across different languages.\ni"
    }, {
      "heading" : "TABLE OF CONTENTS",
      "text" : "LISTOF FIGURES ....................................................................................................... vi\nLIST OF TABLES ....................................................................................................... vii\nLIST OF ABBREVIATIONS .....................................................................................viii\nACKNOWLEDGEMENTS .......................................................................................... ix\nCHAPTER 1: INTRODUCTION ............................................................................. 1\n1.1 MT Events .......................................................................................... 1\n1.2 Importance of MT Evaluation ............................................................ 2\n1.3 Guidance of the Thesis Layout .......................................................... 3\nCHAPTER 2: MACHINE TRANSLATION EVALUATIONS ............................... 5\n2.1 Human Assessment Methods for MT ................................................ 5\n2.1.1 Traditional Manual Judgment Methods ............................................. 5\n2.1.2 Advances of Human Assessment Methods ........................................ 8\n2.2 Automatic Evaluation Metrics for MT ............................................ 11\n2.2.1 Metrics Based on Lexical Similarity................................................ 11\n2.2.1.1 Edit Distance Metrics ....................................................................... 12\n2.2.1.2 Precision Based Metrics ................................................................... 14\n2.2.1.3 Recall Based Metrics ....................................................................... 16\nii\n2.2.1.4 Combination of Precision and Recall .............................................. 17\n2.2.1.5 Word Order Utilization .................................................................... 21\n2.2.2 Combination with Linguistic Features ............................................. 22\n2.2.2.1 Syntactic Similarity .......................................................................... 23\n2.2.2.2 Semantic Similarity .......................................................................... 26\n2.2.2.3 Language Model Utilization ............................................................ 31\n2.2.3 Combination of Different Metrics ................................................... 32\n2.3 Evaluation Methods of MT Evaluation............................................ 33\n2.3.1 Statistical Significance ..................................................................... 33\n2.3.2 Evaluating the Human Judgments ................................................... 33\n2.3.3 Correlating the Manual and Automatic Evaluation ......................... 35\n2.3.3.1 Pearson Correlation Coefficient ....................................................... 35\n2.3.3.2 Spearman Correlation Coefficient ................................................... 37\n2.3.3.3 Kendall’s τ ....................................................................................... 38\nCHAPTER 3: LEPOR – PROPOSED MODEL ..................................................... 40\n3.1 Enhanced Factors ............................................................................. 41\n3.1.1 Length Penalty ................................................................................. 41\n3.1.2 N-gram Position Difference Penalty ................................................ 42\n3.1.3 Harmonic Mean of Precision and Recall ......................................... 46\n3.2 Metrics Scores of Designed Methods .............................................. 48\niii\nCHAPTER 4: IMPROVED MODELS WITH LINGUISTIC FEATURES ........... 49\n4.1 New Factors ..................................................................................... 49\n4.2 Variants of LEPOR .......................................................................... 50\n4.2.1 hLEPOR ........................................................................................... 50\n4.2.2 nLEPOR ........................................................................................... 50\n4.3 Utilization of Linguistic Feature ...................................................... 51\nCHAPTER 5: EVALUATION................................................................................ 53\n5.1 Experimental Setting ........................................................................ 53\n5.1.1 Corpora ............................................................................................ 53\n5.1.2 Existing Metrics for Comparison ..................................................... 54\n5.1.3 Evaluation Criteria ........................................................................... 55\n5.2 Experimental Results ....................................................................... 55\n5.2.1 Initial MT Evaluation Results .......................................................... 55\n5.2.2 MT Evaluation Results with Improved Methods ............................. 56\nCHAPTER 6: EVALUATION ON ACL-WMT SHARED TASK ........................ 59\n6.1 Task Introduction in WMT 2013 ..................................................... 59\n6.2 Submitted Methods .......................................................................... 60\n6.3 Official Evaluation Results .............................................................. 61\n6.3.1 System-level Evaluation Results...................................................... 61\n6.3.1.1 The Official Results of English-to-other MT Evaluation ................ 61\niv\n6.3.1.2 The Official Results of other-to-English MT Evaluation ................ 63\n6.3.2 Segment-level MT Evaluation Results ............................................ 65\nCHAPTER 7: QUALITY ESTIMATION OF MT ................................................. 68\n7.1 Quality Estimation without using Reference Translations .............. 68\n7.2 Latest QE Tasks ............................................................................... 71\n7.3 Proposed Methods in the Advanced QE .......................................... 72\n7.3.1 Proposed Methods in WMT-13 QE Task ........................................ 72\n7.3.2 QE Model using Universal Phrase Category ................................... 74\nCHAPTER 8: CONCLUSION AND FUTURE WORK ........................................ 79\n8.1 Issues in Manual Judgments ............................................................ 79\n8.2 Issues in Automatic Evaluation ....................................................... 80\n8.3 Future Work ..................................................................................... 81\nBIBLIOGRAPHY ........................................................................................................ 82\nAPPENDIX A: Large Tables ..................................................................................... 109\nVITA .......................................................................................................................... 111\nSecond Prize in National Post-Graduate Mathematical Contest in Modeling\n(NPGMCM2011) ............................................................................................... 111\nOpen source tools:.............................................................................................. 111\nGoogle scholar citation: ..................................................................................... 111\nv\nvi"
    }, {
      "heading" : "LISTOF FIGURES",
      "text" : "Figure 3-1: N-gram Word Alignment Algorithm. ....................................................... 63\nFigure 3-2: Constituent structure for an English sentence. .......................................... 65\nFigure 3-3: NPD Calculation Example. ....................................................................... 65\nFigure 3-4: N-gram Word Alignment Example with Multi-references. ...................... 66\nFigure 4-1: N-gram Block Alignment Example. ......................................................... 69\nFigure 4-2: N-gram POS Sequence Alignment Example. ........................................... 72\nFigure 7-1: Parsing of the French and English Sentences. .......................................... 96\nFigure 7-2: Conversion of the Phrase Tags into Universal Categories. ....................... 96\nvii"
    }, {
      "heading" : "LIST OF TABLES",
      "text" : "Table 2-1: Fluency and Adequacy Criteria. ................................................................. 28\nTable 5-1: Participated MT Systems in WMT 2011. ................................................... 74\nTable 5-2: Spearman Correlation Scores of LEPOR and Others. ................................ 76\nTable 5-3: Tuned Parameters of hLEPOR Metric. ...................................................... 77\nTable 5-4: Spearman Correlation Scores of hLEPOR and Others. .............................. 78\nTable 6-1: Participated MT Systems in WMT 2013. ................................................... 80\nTable 6-2: System-level Pearson Correlation Scores. .................................................. 82\nTable 6-3: System-level Spearman Correlation Scores ............................................... 83\nTable 6-4: System-level Pearson Correlation on other-to-English Language Pairs .... 84\nTable 6-5: System-level Spearman Correlation on other-to-English Language Pairs. 85\nTable 6-6: Segment-level Kendall’s tau Correlation scores on WMT13 English-to-\nother Language Pairs.................................................................................................... 86\nTable 6-7: Segment-level Kendall’s tau Correlation scores on WMT13 other-to-\nEnglish Language Pairs................................................................................................ 87\nTable 7-1: Developed POS Mapping for Spanish and Universal Tagset..................... 93\nTable 7-2: Tuned Parameters of HPPR in the Development Stage. ............................ 98\nTable 7-3: Evaluation Results of HPPR on WMT 2012 Corpora. ............................... 98\nviii"
    }, {
      "heading" : "LIST OF ABBREVIATIONS",
      "text" : "ACL Association for Computational Linguistics\nHMM Hidden Markov Models\nITG Inverse Transducer Grammar\nLM Language Model\nML Machine Learning\nMT Machine Translation\nMTE Machine Translation Evaluation\nNP Noun Phrase\nNLP Natural Language Processing\nPOS Part-of-Speech\nPP Preposition Phrase\nSIG Special Interest Group\nSMT Statistical Machine Translation\nVP Verb Phrase\nWMT International Workshop of SMT\nix"
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "I would like to thank my thesis supervisors, Dr. Derek F. Wong and Dr. Lidia S. Chao,\nwho gave me many valuable advices and suggestions during my research periods.\nThey always have encouraged and supported me to go further. This work would not\nhave been possible without their guidance.\nI also give my thanks to my colleagues in our NLP 2 CT-lab as following.\nThanks to Mr. Liang Tian & Mr. Xiaodong Zeng (Samuel), from whom I learnt a\nlot during the three years research in lab. Thanks to Mr. Liangye He (Yervant), Mr. Yi\nLu, and Mr. Junwen Xing (Anson), who gave me a lot of technical and programming\nhelp in our lab during my research.\nThanks to Mr. Longyue Wang (Vincent) who inspired me a lot in the lab. He is a\nhard-working guy, and we worked in our lab all through the night for the paper\nsubmission before the deadline of some international conferences, and this kind of\nexperiment occurred several times. It was a really tiring work such during all the\nnights we spent in the lab; however, it was also such a happy experience after we\nsubmitted the paper on time.\nThanks to our lab members Mr. Fan Sun (Jeff), Miss. Qiuping Huang (Michelle),\nMiss. Ling Zhu (Lynn), Mr. Shuo Li (Shawn), Mr. Hao Zong (Peter), Mr. Yiming\nWang, Mr. Jiaji Zhou, Mr. Chunhui Lu (Olivio), Mr. Yuchu Lin, Miss. Ian, Mr. Ban,\nMr. Kin, Miss. Wendy, and Mr. Jacky, et al. who accompanied my master stage and\nmade the life in the lab wonderful.\nx\nThanks a lot to Mr. Kevin Tang and Mr. Francisco Oliveira for their technical help\nin our lab. I must have made a lot of trouble to them when my computer went wrong\nin the lab. It is very kind of them to deal with the problems with a lot of patience.\nI would like to give my thanks to many kind people outside of the NLP 2 CTlab as\nfollows.\nMy thanks go to Prof. Deng Ding from the Department of Mathematics, who\nhelped me a lot in the national postgraduate mathematical modelling competition, and\nour team finally gained the national second prize.\nThanks to Mr. Vincent Li from the ICTO of UM, who gave me a lot of help including financial support before I joined the NLP 2 CTlab and I worked in ICTO for\nthree months under his guidance.\nIn addition, I want to give my thanks to my friends from other departments in UM.\nThanks to Mr. Zhibo Wang, Miss. Shulin Lv, Mr. Jianhao Lv, and Mr. Zhi Li et al.\nfrom the Department of Mathematics; Mr. Gary Wong (Enlong Huang) from the\nDepartment of E-commerce; Mr. Hongqiang Zhu, Mr. Yunsheng Xie (Shawn), Miss.\nLiyu Deng, Miss. Hong Zhang, Miss. Selin et al. from the Department of English, Mr.\nXingqiang Peng from the Department of Electrical and Computer Engineering (ECE),\nMr. Ziqian Ma, Mr. Dong Li et al. from the Department of Electromechanical\nEngineering (EME), Mr. Shuang Lin, Miss. Le Dong et al. from the Department of\nCivil Engineering, Miss. Xiaonan Bai, Mr. Li Li, Mr. Xiaozhou Shen et al. from the\nFSH, Mr. Haipeng Lei, Mr. Hefeng Zhou, Mr. Haitao Li, Miss. Xiaohui Huang et al.\nxi\nfrom the Institute of Chinese Medical Sciences (ICMS). Thanks for their fulfilling the\ngood memory of my master stage in UM.\nMy special thanks to my roommate, Mr. Wei Hong, for his kind tolerance and\nstaying with me throughout the master stage. I receive many good suggestions from\nhim.\nThanks to Mr. Gust (Augustine), my swimming coach, who teaches me to swim\nwith kind patience. When I got tired in my lab, I went to the Olympic swimming pool\nin Taipa of Macau and he taught me how to float and relax in water. Swimming\nbecamealmost my only hobby in the master stage, and kept my body in health.\nSwimming will become a life-long hobby for me. He likestraveling and thinking and\ntold me a lot of knowledge about the people all around the world.\nThanks to Mr. Tristan and Ms. Nadya from Germany, who kindly invited me to\nstay at their place during the international conference of GSCL, and they kindly\ncooked the delicious Germany-style breakfast and dinner for me during my stay. I\nenjoyed the stay at their place. They went to the station to pick me up when I reached\nGermany and sent me to the station when I left Germany. I also remember the two\nlovely pets Pjofur & Pora, the nice movie in the night before I left Darmstadt.\nFurthermore, I would like to give my thanks to Prof. Xiuwen Xu from the Hebei\nNormal University (HNU) who cared me a lot in the past days. Thanks to Prof.\nJianguo Lei, Prof. Lixia Liu, and Prof. Wenming Li from HNU, who kindly wrote the\nrecommendation letters for me when I was applying for UM.\nxii\nIt will be my great honour if you read this thesis in one day. Please kindly forgive\nme if I lost your name here, because there are really so many people I should give my\nthanks to.\nFinally, I give my deepest gratitude to my family. Thanks a lot to my parents for\ngiving me a life such that I have the chance to explore the world. Thanks for their\nend-less loving and support to me. Thanks to my sister Miss. Litao Han (Grace), who\nencouraged me a lot. They are one most important motivation of me to work hard.\nThere is still a long way in ahead of us to go together and I have confidence that it\nwill be a beautiful long way.\nxiii"
    }, {
      "heading" : "DEDICATION",
      "text" : "I wish to dedicate this thesis to my parents and my sister..\n1\nCHAPTER 1: INTRODUCTION\nThe machine translation (MT) began as early as in the 1950s (Weaver, 1955), and\ngained a quick development since the 1990s due to the development of computer\ntechnology, e.g. augmented storage capacity and the computational power, and the\nenlarged bilingual corpora (Mariño et al., 2006). We will first introduce several MT\nevents that promote the MT technology, and then it is the importance of MT evalution\n(MTE). Subsequently, we give a brief introduction of each chapter in the thesis.\n1.1 MT Events\nThere are several events that promote the development of MT and MT evaluation\nresearch.\nOne of which is the Open machine translation (OpenMT) Evaluation series of National Institute of Standards and Technology (NIST) that are very prestigious evaluation campaigns, the corpora including Arabic-English and Chinese-English language pairs from 2001 to 2009. The OpenMT evaluation series aim to help advance the state of the art MT technologies (NIST, 2002, 2009; Li, 2005) and make the system output to be an adequate and fluent translation of the original text that includes all forms.\nThe innovation of MT and the evaluation methods is also promoted by the annual Workshop on Statistical Machine Translation (WMT) (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012) organized by the special interest group in machine translation (SIGMT) of the Association for Computational Linguistics (ACL) since 2006. The evaluation campaigns focus on European languages. There are roughly two tracks in the annual WMT workshop including the\n2\ntranslation task and evaluation task. From 2012, they added a new task of Quality Estimation of MT without given reference translations (unsupervised evaluation). The tested language pairs are clearly divided into two parts, English-to-other and other-toEnglish, relating to French, German, Spanish (Koehn and Monz, 2006), Czech (Callison-Burch et al., 2007, 2010, 2011, 2012), Hungarian (Callison-Burch et al., 2008, 2009), and Haitian Creole, featured task translating Haitian Creole SMS messages that were sent to an emergency response hotline, due to the Haitian earthquake (Callison-Burch et al., 2011).\nAnother promotion is the International Workshop of Spoken Language Translation\n(IWSLT) that is organized annually since 2004 (Eck and Hori, 2005; Paul, 2008, 2009;\nPaul, et al., 2010; Federico et al., 2011). This campaign has a stronger focus on\nspeech translation including the English and Asian languages, e.g. Chinese, Japanese\nand Korean.\n1.2 Importance of MT Evaluation\nDue to the wide-spread development of MT systems, the MT evaluation becomes more and more important to tell us how well the MT systems perform and whether they make some progress. However, the MT evaluation is difficult because multiple reasons. The natural languages are highly ambiguous and different languages do not always express the same content in the same way (Arnold, 2003), and language variability results in no single correct translation.\nThe earliest human assessment methods for machine translation include the intelligibility and fidelity. They were used by the Automatic Language Processing Advisory Committee (ALPAC) around 1966 (Carroll, 1966a and 1966b). The afterwards proposed human assessment methods include adequacy, fluency, and comprehension (improved intelligibility) by Defense Advanced Research Projects\n3\nAgency (DARPA) of US (White et al., 1994; White, 1995). However, the human judgments are usually expensive, time-consuming, and un-repeatable.\nTo overcome the weeknesses of manual judgments, the early automatic evaluation metrics include the word error rate (WER) (Su et al., 1992), and position independent word error rate (PER) (Tillmann et al., 1997). WER and PER are based on the Levenshtein distance that is the number of editing steps of insertions, deletions, and substitutions to match two sequences. The nowadays commonly used automatic metrics include BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Banerjee and Lavie, 2005), etc. However, there remain some weaknesses in the existing automatic MT evaluation metrics, such as lower performances on the language pairs with English as source language, highly relying on large amount of linguistic features for good performance, and incomprehensive factors, etc.\nAs discussed in the works of Liu et al. (2011) and Wang and Manning (2012b), the accurate and robust evaluation metrics are very important for the development of MT technology. To address some of the existing problems in the automatic MT evaluation metrics, we first design reinforced evaluation factors to achieve robustness; then we design tunable parameters to address the language bias performance; finally, we investigate some concise linguistic features to enhance the performance our metrics. The practical performances in the ACL-WMT shared task show that our metrics were robust and achieved some improvements on the language pairs with English as source language.\n1.3 Guidance of the Thesis Layout\nThis thesis is constructed as the following describes.\nChapter One is the introduction of MT development and importance of MT evaluation. The weeknesses of existing automatic MT evaluation metrics are briefly mentioned and we give a brief introduction about how we will address the problem.\n4\nChapter Two is the background and related work. It contains the knowledge of manual judgment methods, automatic evaluation methods, and the evaluation methods for automatic evaluation metrics.\nChapter Three proposes the designed automatic evaluation method LEPOR of this thesis, including the introduction of each factor in the metric. The main factors in the LEPOR metric include enhanced sentence length penalty, n-gram position difference penalty, and the harmonic mean of precision and recall. We designed several different strategies to group the factors together.\nChapter Four is the improved models of the designed LEPOR metric. This chapter contains the new factors and linguistic features developed in the metric and several variants of LEPOR.\nChapter Five is the experimental evaluation of the designed LEPOR metric. It introduces the used corpora, evaluation criteria, experimental results, and comparisons with existing metrics.\nChapter Six introduces the participation in the annual internaltional workshop of statistical MT (WMT). This chapter contains the submitted metrics and the official results in the shared tasks.\nChapter Seven is the latest development of quality estimation (QE) for MT. It\nintroduces the difference of QE and traditional MT evaluations. This chapter also\nmentions our designed methods in the QE tasks.\nChapter Eight draws the conclusion and future work of this thesis.\n5\nCHAPTER 2: MACHINE TRANSLATION EVALUATIONS\nIn this chapter, we first introduce the human judgment methods for MT; then we\nintroduce the existing automatic evaluation metrics; finally, it is the evaluation criteria\nfor MT evaluation.White (1995) proposes the concept of Black box evaluation. Black\nBox evaluation measures the quality of a system based solely upon its output, without\nrespect to the internal mechanisms of the translation system. The coordinate\nmethodology with it is the Glass Box evaluation, which measures the quality of a\nsystem based upon internal system properties. In this work, we mainly focus on the\nblack box MT evaluation.\n2.1 Human Assessment Methods for MT\nThis section introduces the human evaluation methods for MT, sometimes called as\nthe manual judgments. We begin with the traditional human assessment methods and\nend with the advanced human assessment methods.\n2.1.1 Traditional Manual Judgment Methods\nThe traditional human assessments include intelligibility, fidelity, fluency, adequacy,\nand comprehension, etc. There are also some further developments of these methods.\nThe earliest human assessment methods for MT can be traced back to around 1966, which includethe intelligibility, measuring how understandable the sentence is, and fidelity, measuring how much information the translated sentence retains as compared to the original, used by the automatic language processing advisory committee (ALPAC) (Carroll, 1966aand1966b). ALPAC was established in 1964 by the US\n6\ngovernment to evaluate the progress in computational linguistics in general and machine translation.The requirement that a translation be intelligible means that as far as possible the translation should be read as normal, well-edited prose and be readily understandable in the same way that such a sentencewould be understandable if originally composed in the translation language. The requirement that a translation be of high fidelity or accuracy includes that the translation should as little as possible twist, distort, or controvert the meaning intended by the original.\nOn the other hand, fidelity is measured indirectly, “informativeness” of the original relative to the translation. The translated sentence is presented, and after reading it and absorbing the content, the original sentence is presented. The judges are asked to rate the original sentence on informativeness.The fidelity is measured on a scale of 0- to-9 spanning from less information, not informative at all, no really new meaning added, a slightly different “twist” to the meaning on the word level, a certain amount of information added about the sentence structure and syntactical relationships, clearly informative, very informative, to extremely informative.\nThanks to the development of computer technology in 1990s, the machine translation\ndeveloped fast and the human assessment methods also did around the 1990s. As part\nof the Human Language Technologies Program, the Advanced Research Projects\nAgency (ARPA) created the methodology to evaluate machine translation systems\nusing the adequacy, fluency and comprehension (Church et al., 1991) as the\nevaluation criteria in MT evaluation campaigns for the full automatic MT systems\n(FAMT) (White et al., 1994; White, 1995). All the three components are plotted\nbetween 0 and 1 according to the formulas (White, 1995):\n(2-1)\n7\n(2-2) (2-3)\nThe evaluator is asked to look at each fragment, usually less than a sentence in length, delimited by syntactic constituent and containing sufficient information, and judge the adequacy on a scale 1-to-5, and the results are computed by averaging the judgments over all of the decisions in the translation set and mapped onto a 0-to-1 scale. Adequacy is similar to the fidelity assessment used by ALPAC.\nThe fluency evaluation is compiled with the same manner as for the adequacy except for that the evaluator is to make intuitive judgments on a sentence by sentence basis for each translation.The evaluators are asked to determine whether the translation is good English without reference to the correct translation. The fluency evaluation is to determine whether the sentence is well-formed and fluent in context.\nThe modified comprehension develops into the “Informativeness”, whose objective is to measure a system’s ability to produce a translation that conveys sufficient information, such that people can gain necessary information from it. Developed from the reference set of expert translations, six questions have six possible answers respectively including “none of above” and “cannot be determined”. The results are computed as the number of right answers for each translation, averaged for all outputs of each system and mapped into a 0-to-1 scale.\nThere are some further developments of the above manual judgments with some\nexamples as below.\nLinguistics Data Consortium (LDC, 2005) develops two five-point-scales representing fluency and adequacy for the annual NIST Machine Translation Evaluation Workshop, which become the widely used methodology when manually\n8\nevaluating MT is to assign values, e.g. utilized in the WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007) and IWSLP evaluation campaigns (Eck and Hori, 2005; Paul et al., 2010).\nThe five point scale for adequacy indicates howmuch of the meaning expressed in the reference translation is also expressed in a hypothesis translation; the second five point scale indicates how fluent the translation is, involving both grammatical correctness and idiomatic word choices. When translating into English the values correspond to the Table 2-1.\nOther related works include Bangalore et al. (2000) and Reeder (2004), Callison-\nBurch et al. (2007), Przybocki et al. (2008), Specia et al. (2011), Roturierand\nBensadoun (2011), etc.\n2.1.2 Advances of Human Assessment Methods\nThe advanced manual assessments include task oriented method, extended criteria,\nbinary system comparison, utilization of post-editing and linguistic tools, and online\nmanual evaluation, etc.\nWhite and Taylor (1998) develop a task-oriented evaluation methodology for Japanese-to-English translation to measure MT systems in light of the tasks for which\n9\ntheir output might be used. They seek to associate the diagnostic scores assigned to theoutput used in the DARPA evaluation with a scale of language-dependent tasks suchas scanning, sorting, and topic identification.\nKing et al. (2003) extend a large range of manual evaluation methods for MT systems,which, in addition to the early talked accuracy, include suitability, whether even accurate results are suitable in the particular context in which the system is to be used; interoperability, whether with other software or with hardware platforms; reliability, i.e. don’t break down all the time or take a long time to run again after breaking down; usability, easy to get the interfaces, easy to learn and operate, and looks pretty; efficiency, when needed, keep up with the flow of dealt documents; maintainability, being able to modify the system in order to adapt it to particular users; and portability, one version of a system can be replaced by a new one.\nBased on the ideas that the final goal of most evaluations is to rank the different systems and human judge can normally choose the best one out of two translations, Vilar et al. (2007) design a novel human evaluation scheme by the direct comparison of pairs of translation candidates. The proposed method has lower dependencies on extensive evaluation guidelines and typically focuses on the ranking of different MT systems.\nA measure of quality is to compare translation from scratch and post-editing the result of an automatic translation. This type of evaluation is however time consuming and depends on the skills of the translator and post-editor. One example of a metric that is designed in such a manner is the human translation error rate (HTER) (Snover et al., 2006), based on the number of editing steps, computing the editing steps between an automatic translation and a reference translation. Here, a human annotator has to find the minimum number of insertions, deletions, substitutions, and shifts to convert the system output into an acceptable translation. HTER is defined as the number of editing steps divided by the number of words in the acceptable translation.\n10\nNaskar et al. (2011) describean evaluation approach DELiC4MT, diagnostic evaluation using linguistic checkpoints for MT, to conduct the MT evaluation with a flexible framework, which is experienced with three language pairs from German, Italian and Dutch into English. It makes use of many available component and representation standards, e.g. the GIZA++ POS taggers and word aligner (Och and Ney, 2003), public linguistic parsing tool, the KYOTO Annotation Format (Bosma et al., 2009) to represent textual analysis, and the Kybots (Vossen et al.,2010) to define the evaluation targets (linguistic checkpoint). The diagnostic evaluation scores reveal that the rule-based systems Systran and FreeTranslation are not far behind the SMT systems Google Translate and Bing Translator, and show some crucial knowledge to the MT developers in determining which linguistic phenomena their systems are good at dealing with.\nFedermann (2012) describes Appraise, an open-source toolkit supporting manual evaluation of machine translation output. The system allows collecting human judgments on translation outputandimplementing annotation tasks such as 1) quality checking, 2) translation ranking, 3) error classification, and 4) manual post-editing. It features an extensible, XML-based format for import/export and can be easily adapted to new annotation tasks. Appraise also includes automatic computation of interannotator agreements allowing quick access to evaluation results.\nOther related works include Miller and Vanni (2005), Bentivogli et al. (2011), Paul et\nal. (2012), etc.\n2.2 Automatic Evaluation Metrics for MT\nManual evaluation suffers some disadvantages such as time-consuming, expensive,\nnot tunable, and not reproducible. Some researchers also find that the manual\njudgments sometimes result in low agreement (Callison-Burch et al., 2011). For\n11\ninstance, in the WMT 2011 English-Czech task, multi-annotator agreement kappa\nvalue is very low, and even the same strings produced by two systems are ranked\ndifferently each time by the same annotator. Due to the weaknesses in human\njudgments, automatic evaluation metrics have been widely used for machine\ntranslation. Typically, they compare the output of machine translation systems against\nhuman translations but there are also some metrics that do not use the reference\ntranslation. Common metrics measure the overlap in words and word sequences, as\nwell as word order and edit distance (Cherry, 2010). Advanced metrics also take\nlinguistic features into account such as syntax, semantics, e.g. POS, sentence structure,\ntextual entailment, paraphrase, synonyms and named entities, and language models.\n2.2.1 Metrics Based on Lexical Similarity\nThis section discusses the automatic MT evaluation metrics employing the lexical\nsimilarity including the factors of edit distance, precision, recall, and word order.\nSome of the metrics also employ the n-gram co-occurrence (Doddington, 2002)\ninformation and others use the unigram matching only. Some metrics mentioned in\nthis section also utilize the linguistic features.\n2.2.1.1 Edit Distance Metrics\nBy calculating the minimum number of editing steps to transform output to reference,\nSu et al. (1992) introduce the word error rate (WER) metric from speech recognition\ninto MT evaluation. This metric takes word order into account, and the operations\ninclude insertion (adding word), deletion (dropping word) and replacement (or\nsubstitution, replace one word with another) using the Levenshtein distance, the\n12\nminimum number of editing steps needed to match two sequences.The measuring\nformula is shown as below whose value ranges from 0 (the best) to 1 (the worst).\n(2-4) Because WER is to compare the raw translation output of a MT system with the final revised version that is used directly as a reference, this method can reflect real quality gap between the system performance and customer expectation. The Multi-reference WER is later defined in (Nießen et al., 2000). They computean “enhanced” WER as follows: a translation is compared to all translations that have been judged “perfect” and the most similar sentence is used for the computation of the edit distance.\nOne of the weak points of the WER is the fact that word ordering is not taken into\naccount appropriately.The WER scores very low when the word order of system\noutput translation is “wrong” according to the reference. In the Levenshtein distance,\nthe mismatches in word order require the deletion and re-insertion of the misplaced\nwords.However, due to the diversity of language expression, some so-called “wrong”\norder sentences by WER also prove to be good translations. To address this problem\nin WER, the position-independent word error rate (PER) (Tillmann et al., 1997)\nignores word order when matching output and reference.Without taking into account\nof the word order, PER counts the number of times that identical words appear in both\nsentences. Depending on whether the translated sentence is longer or shorter than the\nreference translation, the rest of the words are either insertions or deletions. PER is\nguaranteed to be less than or equal to the WER.\n(2-5)\n13\n(2-6) where means the number of words that appear in the reference but not in the output, and means the difference value of the and when the output is longer. Another way to overcome the unconscionable penalty on word order in the Levenshtein distance is adding a novel editing step that allows the movement of word sequences from one part of the output to another. This is something a human posteditor would do with the cut-and-paste function of a word processor. In this light, Snover et al. (2006) design the translation edit rate (TER) metric that is also based on Levenshtein distance but adds block movement (jumping action) as an editing step. The weakness is that finding the shortest sequence of editing steps is a computationally hard problem.\nOther related researches using the edit distances as features include (Akiba, et al.,\n2001), (Akiba, et al., 2006), (Leusch et al., 2006), TERp (Snover et al., 2009), Dreyer\nand Marcu (2012), and (Wang and Manning, 2012a), etc.\n2.2.1.2 Precision Based Metrics\nPrecision is a widely used criterion in the MT evaluation tasks. For instance, if we use\nthe to specify the number of correct words in the output sentence and the as the total number of the output sentence, then the precision score of this sentence can be calculated by their quotient value.\n(2-7) The commonly used evaluation metric BLEU (bilingual evaluationunderstudy) (Papineni et al., 2002) is based on the degree of n-gram overlapping between the\n14\nstrings of words produced by the machine and the human translation references at the corpus level. BLEU computes the precision for n-gram of size 1-to-4 with the coefficient of brevity penalty. The theory under this design is that if most of the outputs are right but with too short output (e.g. many meanings of the source sentences lost), then the precision value may be very high but this is not a good translation; the brevity-penalty coefficient will decrease the final score to balance this phenomenon.\n∑ (2-8) { (2-9)\nwhere is the total length of candidate translation corpus (the sum of sentences’ length), and refers to the sum of effective reference sentence length in the corpus. The effective sentence means that if there are multi-references for each candidate sentence, then the nearest length as compared to the candidate sentence is selected as the effective one.\nThe n-gram matching of candidate translation and reference translation is first performed at sentence level. The unigram matching is designed to capture the adequacy and the n-gram matching is to achieve the fluency evaluation. Then the successfully matched n-gram numbers are added sentence by sentence and the n-gram precisions and brevity penalty values in the formula are calculated at the corpus level instead of sentence level.\nBLEU is now still one of the commonly used metrics by researchers to show their improvements in their researches including the translation quality and evaluation metrics. For example, Nakov and Ng (2012) show their improved language model on machine translation quality for resource-poor language by the gaining of up to several points of BLEU scores; Sanchez-Martınez and Forcada (2009) describe a method for the automatic inference of structural transfer rules to be used in a shallow-transfer MT\n15\nsystem from small parallel corpora with the verifying metrics TER and BLEU; Li et al. (2011) propose a feedback selecting algorithm for manually acquired rules employed in a Chinese to English MT system stating the improvement of SMT quality by 17.12% and by 5.23 in terms ofcase-insensitive BLEU-4 score over baseline. Actually, BLEU has a wider applicability than just MT. Alqudsi et al. (2012) extend its use to evaluate the generation of natural language and the summarization of systems.\nIn the BLEU metric, the n-gram precision weight is usually selected as uniform weight . However, the 4-gram precision value is usually very low or even zero when the test corpus is small. Furthermore, the geometric average results in 0 score whenever one of the component n-grams scores is 0. To weight more heavily those ngrams that are more informative, Doddington (2002) proposes the NIST metric with the information weight added.\n(2-10) Furthermore, he replace the geometric mean of co-occurrences with the arithmetic average of n-gram counts, extend the n-gram into 5-gram (N=5), and select the average length of reference translations instead of the nearest length. The arithmetic mean ensures that the co-occurrence for different n-gram can be weighted. ∑ {∑ ∑ } { [ ( ̅̅ ̅̅ ̅̅ )]} (2-11) where means the words sequence, ̅̅ ̅̅ ̅̅ is the average number of words in a reference translation, averaged over all reference translations. The experiments show that NIST provides improvement in score stability and reliability, and higher correlation with human adequacy judgments than BLEU on four languages, Chinese, French, Japanese and Spanish. However, NIST correlates lower with the human Fluency judgments than BLEU on the other three corpora except for Chinese.\n16\nCombining BLEU with weights of statistical salience from vector space model (Babych et al., 2003), which is similar to TF.IDF score (SaltonandLesk, 1968), (Babych, 2004) and (Babych and Hartley, 2004aand2004b) describe an automated MT evaluation toolkit weighted N-gram model (WNM) and implement a rough model of legitimate translation variation (LTV). The method has been tested by correlation with human scores on DARPA 94 MT evaluation corpus (White et al, 1994).\nOther research works based precision include Liu and Gildea (2007 and 2006), etc.\n2.2.1.3 Recall Based Metrics\nRecall is another crucial criterion in the MT evaluation. For instance, if the means the number of words in the reference sentence, and the also specify the number of correct words in the output sentence, then the recall value\nis calculated as:\n(2-12) Different with precision criterion, which reflects the accuracy of the system output, recall value reflects the loyalty of the output to the reference (or input) (Melamed et al., 2003).\nROUGE (Lin and Hovy 2003; Lin 2004a) is a recall-oriented automated evaluation metric, which is initially developed for summaries. Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities. A series of workshops on automatic text summarization (WAS, 2002) are held as special topic sessions in ACL. Following the adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, Lin (2004a) conducts a study of a similar idea for evaluating summaries. The experiments show that automatic evaluation using unigram co-occurrences, i.e.\n17\nROUGE, between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; on the other hand, direct application of the BLEU evaluation procedure does not always give good results. They also explore the effect of sample size in (Lin, 2004b) and apply the ROUGE into automatic machine translation evaluation in the work (Lin and Och, 2004a and 2004b). Furthermore, Lin and Och (2004) introduce a family of ROUGE including three measures, of which ROUGE-S is a skip bigram F-measure, ROUGE-L and ROUGE-W are measures based on the length of the longest common subsequence of the sentences. ROUGE-S has a similar structure to the bigram PER and they expect ROUGE-L and ROUGEWto have similar properties to WER.\nOther related works include (Leusch et al., 2006) and (Lavie et al., 2004) that talk\nabout the significance of recall values in automatic evaluation of machine translation.\n2.2.1.4 Combination of Precision and Recall\nAs mentioned in the precious section of this paper, the precision value reflects the\naccuracy, how much proportion of the output is correct, of the automatic MT system\nand the recall value reflects the loyalty, how much meaning is lost or remained, of the\noutput to the inputs, both of which are the crucial criteria to judge the quality of the\ntranslations. To evaluate the MT quality more reasonable, it is not difficult to think of\nthe combination of these two factors.\nF-measure is the combination of precision (P) and recall (R), which is firstly employed in the information retrieval and latterly has been adopted by the information extraction, MT evaluation and other tasks. Let’ssee a set of formula first.\n18\n(2-13)\nThe variable measures the effectiveness of retrieval with respect to a user who attaches times as much importance to recall as precision. The effectiveness measure ( value) is defined in (van Rijsbergen. 1979). In the effectiveness measure, there is a parameter which sets the trade-off between Precision and Recall. When an equal trade-off is desired, is set to 0.5. The first full definition of the F-measure ( ) to evaluation tasks of information extraction technology was given by (Chinchor, 1992) in the fourth message understanding conference (MUC-4).\nTraditional F-measure or balanced F-score ( score) is exactly the harmonic mean of precision and recall (put the same trade-off on precision and recall, ) (Sasaki and Fellow, 2007).\n(2-14) If we bring the precision and recall formula introduced in the precious sections into the F-measure, we can get the following inferred formula.\n(2-15) (2-16)\nwhere means assign the weight and respectively to Precision and Recall. We should note that the unigram precision, recall and F-score do not take word order into consideration.\nRiezler and Maxwell III(2005) investigate some pitfalls regarding the discriminatory power of MT evaluation metrics and the accuracy of statistical significancetests. In a\n19\ndiscriminative re-ranking experiment for phrase-based SMT,they showthat the NIST metric is more sensitive than BLEU or F-score despite their incorporationof aspects of fluency or meaning adequacy into MT evaluation.Pointing out a well-known problem of randomly assessing significance in multiple pairwise comparisons, they recommend for multiple comparisons of subtle differences to combine the NIST score for evaluation with the approximate randomization test for significance testing, at stringent rejection levels.\nF-measure is based on the unigram matching and two sentences containing the same words always get the same F-measure rating regardless of the correct order of the words in the sentence. To eliminate this drawbacks, BLEU and NIST reward the correct word order by double-counting all sub-runs, where the factor is the contiguous sequence of matching words in the matching M (M is usually a sentence). On the other hand, GTM (general text matching) that is proposed in (Turian et al., 2003) rewards the contiguous sequences of correctly translated words by the assigned weight to the . GTM is based on the F-measure but adds the maximum match size (MMS) information in the calculation of precision and recall. √∑ (2-17) They first define the weight of a to be the , then they generalize the definition of match size as . The reward is controlled by parameter . The contiguous sequences of words are rewarded and penalized respectively when and . When , the GTM score achieves the same performance with the original F-measure. GTM calculates word overlap between a reference and a solution, without double counting duplicate words. Furthermore, BLEU and NIST are difficult to gain insight from the experiment scores, whereas GTM performs the measures that have an intuitive graphical interpretation and can facilitate insights into how MT systems might be improved.\n20\nBLEU is an n-gram precision based metric and performs the exact words matching. However, Banerjee and Lavie (2005) find that the recall value plays a more important role than precision to obtain higher correlation with human judgments and design a novel evaluation metric METEOR. METEOR is based on general concept of flexible unigram matching, unigram precision and unigram recall, e.g. unigram F-measure, including the match of words that are simple morphological variants of each other by the identical stem and words that are synonyms of each other. METEOR assigns 9 times as importance of recall as precision value, i.e., in the F-measure . To measure how well-ordered the matched words in the candidate translation are in relation to the human reference, METEOR introduces a novel penalty coefficient by employing the number of matched chunks.\n(2-18) (2-19)\nWhen there are no bigram or longer matches between the candidate translation and the reference, there are as many chunks as there are unigram matches. Experiments tested on LDC TIDES 2003 Arabic-to-English and Chinese-to-English show that all of the individual factors included within METEOR contribute to improved correlation with human judgments, which means that METEOR achieve higher correlation score than the unigram precision, unigram recall and unigram F-measure, in addition to the BLEU and NIST metrics. The enhanced version of METEOR (Lavie and Agarwal, 2007) also employs the paraphrases, using WordNet a popular ontology of English words, into the matching period. The weakness of METEOR is the computationally expensive word alignment.\n21\nOther related works using the combination of precision and recall include Lita et al.\n(2005), Chen and Kuhn (2011), Chen et al. (2012a), etc.\n2.2.1.5 Word Order Utilization\nThe right word order places an important role to ensure a high quality translation\noutput. However, the language diversity also allows different appearances or\nstructures of the sentence. How to successfully achieve the penalty on really wrong\nword order (wrongly structured sentence) instead of on the “correctly” different order,\nthe candidate sentence that has different word order with the reference is well\nstructured, compared with the reference translation, attracts a lot of interests from\nresearchers in the NLP literature. In fact, the Levenshtein distance and n-gram based\nmeasures contain the word order information. Here, we introduce severale valuation\nmetrics that are not based on Levenshtein distance but also take the word order in to\nconsideration.\nFeaturing the explicit assessment of word order and word choice, Wong and Kit (2008 and 2009) develop the evaluation metric ATEC, assessment of text essential characteristics, which is also based on precision and recall criteria but with the designed position difference penalty coefficient attached. The word choice is assessed by matching word forms at various linguistic levels, including surface form, stem, sound and sense, and further by weighing the informativeness of each word. The word order is quantified in term of the discordance of word position and word sequence between the translation candidate and its reference. The evaluation on the Metrics MATR08, the LDC MTC2 and MTC4 corpora demonstrates an impressive positive correlation to human judgments at the segment level.The parameter-optimized version of ATEC and its performance is described in (Wong and Kit,2010).\n22\nOther related works include Zhou et al. (2008), Isozaki et al.(2010), Chen et al.\n(2012b), Popovic (2012), etc.\n2.2.2 Combination with Linguistic Features\nAlthough some of the previous mentioned metrics employ the linguistic information\ninto consideration, e.g. the semantic information synonyms and stemming in\nMETEOR, the lexical similarity mainly focus on the exact matches of the surface\nwords in the output translation. The advantages of the metrics based on lexical\nsimilarity are that they perform well in capturing the translation fluency as mentioned\nin (Lo et al., 2012), and they are very fast and low cost. On the other hand, there are\nalso some weaknesses, for instance, the syntactic information is rarely considered and\nthe underlying assumption that a good translation is one that shares the same lexical\nchoices as the reference translations is not justified semantically. Lexical similarity\ndoes not adequately reflect similarity in meaning. Translation evaluation metric that\nreflects meaning similarity needs to be based on similarity of semantic structure not\nmerely flat lexical similarity.\nIn this section we focus on the introduction of linguistic features utilized into the\nevaluation including the syntactic features and semantic information.\n2.2.2.1 Syntactic Similarity\nThe lexical similarity metrics tend to perform on the local level without considering\nthe overall grammaticality of the sentence or sentence meaning. To address this\nproblem, the syntax information should be considered. Syntactic similarity methods\nusually employ the features of morphological part-of-speech information, phrase\n23\ncategories or sentence structure generated by the linguistic tools such as language\nparser or chunker."
    }, {
      "heading" : "POS information",
      "text" : "In grammar, a part of speech, also called a lexical category, is a linguistic category of words or lexical items, which is generally defined by the syntactic or morphological behaviour of the lexical item. Common linguistic categories of lexical items include noun, verb, adjective, adverb, and preposition, etc. To reflect the syntactic quality of automatically translated sentences, some researchers employ the POS information into their evaluation.\nUsing the IBM model one, Popovic et al. (2011) evaluate the translation quality by calculating the similarity scores of source and target (translated) sentence without using reference translation based on the morphemes, 4-gram POS and lexicon probabilities. This evaluation metric MP4IBM1 relies on the large parallel bilingual corpus to extract the lexicon probability, precise POS tagger to gain the details about verb tenses, cases, number, gender, etc., and other linguistic tool to split words into morphemes. The experiments show good performance with English as the source language but very weak performance when English is the target language. For instance, the correlation score with human judgments is 0.12 and 0.08 respectively on Spanish-to-English and French-to-English WMT corpus (Popovic et al., 2011), which means very low correlation.\nOther similar works using POS information include the Giménez and Márquez (2007), Popovic and Ney (2007), Dahlmeier et al. (2011) and Liu et al. (2010), etc."
    }, {
      "heading" : "Phrase information",
      "text" : "To measure a MT system’s performance in translating new text-types, such as in what ways the system itself could be extended to deal with new text-types, Povlsen, et al.\n24\n(1998) perform a research work focusing on the study of English-to-Danish machinetranslation system PaTrans (Bech 1997), which covers the domain of petrochemicaland mechanical patent documents. The overall evaluation and quality criterion is defined in terms ofhow much effort it takes to post-edit the text after having been translated by the MT system. A structured questionnaire rating different error types is given to the post-editors. In addition to the lexical analysis such as the identifying of deverbalnouns, adjectives, homograghs (with different target translations) and words (translate into nexus adverbs), the syntactic constructions and semantic features are explored with more complex linguistic knowledge, such as the identifying of valency and non-valency bond prepositions, fronted adverbial subordinate clauses, prepositional phrases, and part-of-speech disambiguation with constraint-grammar parser ENGCG (Voutilainen et al., 1992). In order to achieve consistency and reliability, the analysis of thenew text-types is automated as far as possible. In the experiments, a reference text, known as a good text, is first analysedusing the procedure in order to provide a benchmark againstwhich to assess the results from analysing the new text-types.After running the evaluation, a representative subset of the new text-types is then selectedand translated by a slightly revisedversion of the MT system and assessed by the post-editors usingthe same questionnaire.Their evaluation is a first stage in an iterative process, in which the suite of programs is extended to account for the newly identified gaps in coverage and the evaluation of the text type carried out again.\nAssuming that the similar grammatical structures should occur on both source and translations, Avramidis et al. (2011) perform the evaluation on source (German) and target (English) sentence employing the features of sentence length ratio, unknown words, phrase numbers including noun phrase, verb phrase and prepositional phrase. The used tools include Probabilistic Context Free Grammar (PCFG) parser (Petrov et al., 2006) and statistical classifiers using Naive Bayes and K-Nearest Neighbour algorithm. However, the experiment shows lower performance than the BLEU score.\n25\nThe further development of this metric (DFKI) is introduced in (Avramidis, 2012) where the features are expanded with verbs, nouns, sentences, subordinate clauses and punctuation occurrences to derive the adequacy information.\nHan et al. (2013d) develop a phrase tagset mapping between English and French treebanks, and perform the MT evaluation work on the developed universal phrase tagset instead of the surface words of the sentences. The experiments on ACL-WMT (2011 and 2012) corpora, without using reference translations, yield promising correlation scores as compared to the traditional evaluation metrics BLEU and TER.\nOther similar works using the phrase similarity include the (Li et al., 2012) that uses noun phrase and verb phrase from chunking and (Echizen-ya and Araki, 2010) that only uses the noun phrase chunking in automatic evaluation."
    }, {
      "heading" : "Sentence structure information",
      "text" : "To address the overall goodness of the translated sentence’s structure, Liu and Gildea (2005) employ constituent labels and head-modifier dependencies from language parser as syntactic features for MT evaluation. They compute the similarity of dependency trees between the candidate translation and the reference translations using their designed methods HWCM (Headword Chain Based Metric), STM (subtree metric), DSTM (dependency sub-tree metric), TKM (kernel-based sub-tree metric) and DTKM (dependency tree kernel metric). The overall experiments prove that adding syntactic information can improve the evaluation performance especially for predicting the fluency of hypothesis translations.\nFeaturing that valid syntactic variations in the translationcan avoid the unfairly penalize, Owczarzak et al. (2007) develop a MT evaluation method using labelled dependencies that are produced by a lexical-functional grammar parser in contrast to the string-based methods. Similarly, the dependency structure relation is also\n26\nemployed in the feature set by the work (Ye et al., 2007), which performs the MT evaluation as a ranking problem.\nOther works that using syntactic information into the evaluation are shown in (Lo and\nWu, 2011a) and (Lo et al., 2012) that use an automatic shallow parser, (Mutton et al.,\n2007) that focuses on the fluency criterion, and (Fishel et al., 2012; Avramidis, 2012;\nFelice and Specia, 2012) that use different linguistic features.\n2.2.2.2 Semantic Similarity\nAs contrast to the syntactic information, which captures the overall grammaticality or\nsentence structure similarity, the semantic similarity of the automatic translations and\nthe source sentences (or references) can be measured by the employing of some\nsemantic features, such as the named entity, synonyms, semantic roles, paraphrase\nand textual entailment."
    }, {
      "heading" : "Named entity information",
      "text" : "To capture the semantic equivalence of sentences or text fragments, the named entity knowledge is brought from the literature of named-entity recognition, also called as entity extraction or entity identification, which is aiming to identify and classify atomic elements in the text into different entity categories (Marsh and Perzanowski, 1998; Guo et al., 2009). The commonly used entity categories include the names of persons, locations, organizations and times, etc.\nIn the MEDAR, an international cooperation between the EU and the Mediterranean region on Speech and Language Technologies for Arabic, 2011 evaluation campaign, two SMT systems based on Moses (Koehn et al., 2007) are used as baselines respectively for English-to-Arabic and Arabic-to-English directions. The Baseline-1 system adapts SRILM (Stockle, 2002), GIZA++ (Och & Ney, 2003) and a\n27\nmorphological analyzer to Arabic, whereas Baseline-2 system also utilizes OpenNLP 1 toolkit to perform named entity detection, in addition to other packagesthat provides tokenizing, POS tagging and base phrase chunking for Arabic text (Hamon and Choukri, 2011). The experiments show that the low performances from the perspective of named entities, many entities are either not translated or not well translated, cause a drop in fluency and adequacy.\nOther such related works include Buck (2012),Raybaud et al. (2011), and Finkel et al. (2005), etc."
    }, {
      "heading" : "Synonym information",
      "text" : "Synonyms are used to specify the words that have the same or close meanings. One of the widely used synonym database in NLP literature is the WordNet (Miller et al., 1990; Fellbaum, 1998), which is an English lexical database grouping English words into sets of synonyms. WordNet classifies the words mainly into four kinds of part-ofspeech (POS) categories including Noun, Verb, Adjective, and Adverb without prepositions, determiners, etc. Synonymous words or phrases are organized using the unit of synset. Each synset is a hierarchical structure with the words in different levels according to their semantic relations. For instance, the words in upper level belong to the words (hypernym) in lower level.\nUtilizing the WordNet and the semantic distance designed by (Wu and Palmer,1994) to identify near-synonyms, Wong and Kit (2012) develop a document level evaluation metric with lexical cohesion device information. They define the lexical cohesion as the content words of synonym and near-synonym that appear in a document. In their experiments, the employed lexical cohesion has a weak demand for language resource as compared to the other discourse features such as the grammatical cohesion, so it is much unaffected by grammatical errors that usually appear in translation outputs. The\n1 http://opennlp.apache.org/index.html\n28\nperformances on the corpora of MetricsMATR 2008 (Przybockiet al., 2009) and MTC-4 (Ma, 2006) show high correlation rate with manually adequacy judgments. Furthermore, the metrics BLEU and TER also achieve improved scores through the incorporating of the designed document-level lexical cohesion features.\nOther works employing the synonym features include Chan and Ng (2008), Agarwal and Lavie (2008), and Liu and Ng (2012), etc."
    }, {
      "heading" : "Semantic roles",
      "text" : "The semantic roles are employed by some researchers as linguistic features in the MT evaluation. To utilize the semantic roles, the sentences are usually first shallow parsed and entity tagged. Then the semantic roles used to specify the arguments and adjuncts that occur in both the candidate translation and reference translation. For instance, the semantic roles introduced by Giménez and Márquez (2007, 2008) include causative agent, adverbial adjunct, directional adjunct, negation marker, and predication adjunct, etc. In the further development, Lo and Wu (2011a and 2011b) design the metric MEANT to capture the predicate-argument relations as the structural relations in semantic frames, which is not reflected by the flat semantic role label features in the work of (Giménez and Márquez, 2007). Furthermore, instead of using uniform weights, Lo, Tumuluru and Wu (2012) weight the different types of semantic roles according to their relative importance to the adequate preservation of meaning, which is empirically determined. Generally, the semantic roles account for the semantic structure of a segment and have proved effective to assess adequacy in the above papers."
    }, {
      "heading" : "Textual entailment",
      "text" : "Textual entailment is usually used as a directive relation between text fragments.If the truth of one text fragment TA follows another text fragment TB, then there is a directional relation between TA and TB (TB=>TA). Instead of the pure logical or\n29\nmathematical entailment, the textual entailment in natural language processing (NLP) is usually performed with a relaxed or loose definition (Dagan et al., 2005; 2006). For instance, according to text fragment TB, if it can be inferred that the text fragment TA is most likely to be true then the relationship TB=>TA also establishes. That the relation is directive also means that the inverse inference (TA=>TB) is not ensured to be true (Dagan and Glickman, 2004).\nTo address the task of handling unknown terms in SMT, Mirkin et al. (2009) proposea Source-Language entailment model. Firstly they utilize the source-language monolingual models and resources to paraphrase the source text prior to translation. They further present a conceptual extension to prior work by allowing translations of entailed texts rather than paraphrases only. This method is experimented on some 2500 sentences with unknown terms and substantially increases the number of properly translated texts.\nOther works utilizing the textual entailment can be referred to Pado et al. (2009a and 2009b), Lo and Wu (2011a), Lo et al. (2012), Aziz et al. (2010), and Castillo and Estrella (2012), etc."
    }, {
      "heading" : "Paraphrase features",
      "text" : "Paraphrase is to restatement the meaning of a passage or text utilizing other words, which can be seen as bidirectional textual entailment (Androutsopoulos and Malakasiotis, 2010). Instead of the literal translation, word by word and line by line, used by metaphrase, paraphrase represents a dynamic equivalent. For instance, “He is a great man” may be paraphrased as “He has ever done a lot of great things”. Further knowledge of paraphrase from the aspect of linguistics is introduced in the works of (McKeown, 1979; Meteer and Shaked, 1988; Dras, 1999; Barzilay and Lee, 2003). As an example, let’s see the usage of paraphrase and other linguistic features in the improvement of TER evaluation metric.\n30\nWhile Translation Edit Rate (TER) metric (Snover 2006) has been shown to correlate well with human judgments of translation quality, it has several flaws, including the use of only a single reference translation and the measuring of similarity only by exact word matches between the hypothesis and the reference (Snover et al., 2011). These flaws are addressed through the use of Human-Mediated TER (HTER), but are not captured by the automatic metric.To address this problem, Snover et al. (2009a; 2009b) describea new evaluation metric TER-Plus (TERp). TERp uses all the edit operations of TER, Matches, Insertions, Deletions, Substitutions and Shifts, as well as three new edit operations, Stem Matches, Synonym Matches (Banerjee and Lavie 2005)and Phrase Substitutions (Zhou et al., 2006; Kauchak 2006). TERp identifies words in the hypothesis and reference that share the same stem using the Porter stemming algorithm (Porter, 1980). Two words are determined to be synonyms if they share the same synonym set according to WordNet (Fellbaum, 1998). Sequences of ghypothesis if that phrase pair occurs in the TERp phrase table.They presenta correlation study comparing TERp to BLEU, METEOR and TER, and illustrate that TERp can better evaluate translation adequacy.\nOther works using the paraphrase information can be seen in (Owczarzak et al., 2006),\n(Zhou et al., 2006), and (Kauchak and Barzilay, 2006), etc.There are also many\nresearchers who combine the syntactic and semantic features together in the MT\nevaluation, such as Peral and Ferrandez (2003), Gimenez and Marquez (2008), Wang\nand Manning (2012b), de Souza et al. (2012), etc.\n2.2.2.3 Language Model Utilization\nThe language models are also utilized by the MT and MT evaluation researchers. A\nstatistical language model usually assigns a probability to a sequence of words by\nmeans of a probability distribution.\n31\nGamon et al. (2005) propose LM-SVM (language-model, support vector machine) method investigating the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations. They measure the correlation between automatically-generated scores and human judgments, and evaluate the performance of the system when used as a classifier for identifying highly dysfluent and illformed sentences. They show that they can substantially improve the correlation between language model perplexity scores and human judgment by combining these perplexity scores with class probabilities from a machine-learned classifier. The classifier uses linguistic features and is trained to distinguish human translations from machine translations.\nThere are also some other research works that use the linguistic features, such as Wong and Kit (2011), Aikawa and Rarrick (2011),Reeder F. (2006a), etc.\nGenerally, the linguistic features mentioned above, including both syntactic and\nsemantic features, are usually combined in two ways, either by following a machine\nlearning approach (Albrecht and Hwa, 2007; Leusch and Ney, 2009), or trying to\ncombine a wide variety of metrics in a more simple and straight forward way, such as\nGiménez and Márquez (2008), Specia and Giménez (2010), and Comelles et al.\n(2012), etc.\n2.2.3 Combination of Different Metrics\nThis sub-section introduces some metrics that are designed by the combination, or\noffering a framework for the combination of existing metrics.\nAdequacy-oriented metrics, such as BLEU, measure n-gram overlap of MT outputs and their references, but do not represent sentence-level information. In contrast, fluency-oriented metrics, such as ROUGE-W, compute longest common sub-\n32\nsequences, but ignore words not aligned by the longest common sub-sequence. To address these problems, Liu and Gildea (2006) describea new metric based on stochastic iterative string alignment (SIA) for MT evaluation, which achieves good performance by combining the advantages of n-gram-based metrics and loosesequence-based metrics. SIA uses stochastic word mapping to allow soft or partial matches between the MT hypotheses and the references. It works especially well in fluency evaluation. This stochastic component is shown to be better than PORTERSTEM and WordNet in their experiments. They also analyse the effect of other components in SIA and speculate that they can also be used in other metrics to improve their performance.\nOther related works include Gimenez and Amigo (2006), Parton et al. (2011),\nPopovic (2011), etc.\n2.3 Evaluation Methods of MT Evaluation\nThis section introduces the evaluation methods for the MT evaluation. They include\nthe statistical significance, evaluation of manual judgments, correlation with manual\njudgments, etc.\n2.3.1 Statistical Significance\nIf different MT systems produce translations with different qualities on a data set,\nhow can we ensure that they indeed own different system quality? To explore this\nproblem, Koehn (2004) performs a research work on the statistical significance test\nfor machine translation evaluation. The bootstrap resampling method is used to\ncompute the statistical significance intervals for evaluation metrics on small test sets.\nStatistical significance usually refers to two separate notions, of which one is the p-\n33\nvalue, the probability that the observed data will occur by chance in a given single\nnull hypothesis, and another is the Type I error, false positive, rate of a statistical\nhypothesis test, the probability of incorrectly rejecting a given null hypothesis in\nfavour of a second alternative hypothesis (Hald, 1998). The fixed number 0.05 is\nusually referred to as the significance level, i.e. the level of significance.\n2.3.2 Evaluating the Human Judgments\nSince the human judgments are usually trusted as the golden standards that the\nautomatic evaluation metrics should try to approach, the reliability and coherence of\nhuman judgments is very important. Cohen’s kappa agreement coefficient is one of\nthe commonly used evaluation methods (Cohen,1960). For the problem in nominal\nscale agreement between two judges, there are two relevant quantities:\n(2-20) (2- 21)\nThe test of agreement comes then with regard to the of the units of which the hypothesis of no association would predict disagreement between the judges. The coefficient is simply the proportion of chance-expected disagreements which do not occur, or alternatively, it is the proportion of agreement after chance agreement is removed from consideration:\n(2-22) where represents the proportion of the cases in which beyond-chance agreement occurs and is the numerator of the coefficient. The interval of 0-to-0.2 is\n34\nslight, 0.2-to-0.4is fair, 0.4-to-0.6 is moderate, 0.6-to-0.8 is substantial,and 0.8-to-1.0 is almost perfect (Landisand Koch, 1977).\nIn the annual ACL-WMT workshop (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012), they also use this agreement formula to calculate the Inter- and Intraannotator agreement inthe ranking task to ensure their process as a valid evaluation setup. To ensure they have enough data to measure agreement, they occasionally show annotator items that are repeated from previously completed items. These repeated items are drawn from ones completed by the same annotator and from different annotators. They measure pairwise agreement among annotators using following formula:\n(2-23) where P(A) is the proportion of times that the annotators agree, and P(E) is the\nproportion of times that they will agree by chance. The agreement value k has a value\nof at most 1, higher rates of agreement resulting in higher k value.\n2.3.3 Correlating the Manual and Automatic Evaluation\nThis section introduces the correlation criteria that are usually utilized to measure the\ncloseness between manual judgments and automatic evaluations. They cover the\nPearson correlation coefficient, Spearman correlation coefficient, and Kendall’s . 2.3.3.1 Pearson Correlation Coefficient\nPearson correlation coefficient (Pearson, 1900) is usually used as the formula to\ncalculate the system-level correlations between automatic evaluation results and\nhuman judgments.\n35\nPearson's correlation coefficient when applied to a population is commonly represented by the Greek letter (rho). The correlation between random variables X and Y denoted as is measured as follow (Montgomery and Runger,1994; Montgomery and Runger, 2003).\n√ (2-24) Because the standard deviations of variable X and Y are higher than 0( and ), if the covariance between X and Y is positive, negative or zero, the correlation score between X and Y will correspondingly result in positive, negative or zero, respectively. Forany two random variables, the correlation score of them varies in the following interval:\n(2-25) The correlation just scales the covariance by the standard deviation of each variable. Consequently the correlation is a dimensionless quantity that can be used to compare the linear relationships between pairs of variables in different units.In the above formula, the covariance between the random variables X and Y, denoted as cov(X, Y) or , is defined as: [ ] (2-26)\nTo learn the above formula, let’s first see a definition: a discrete random variable is a random variable with a finite (or countably infinite) range; a continuous random variable is a random variable with an interval (either finite or infinite) of real numbers for its range. The mean or expected value of the discrete random variable X, denoted as or E(X), is ∑ (2-27)\nA random variable X has a discrete uniform distribution if each of the n values in its range, say , has equal probability. Then,\n36\n∑ (2-28) The variance of discrete random variable X, denoted as or V(X), is\n∑ (2-29) ∑ ∑ (2-30)\nThe standard deviation of X is √ . Finally, based on a sample of paired data (X, Y) as , , the Pearson correlation coefficient is: ∑ √∑ √∑ ( ) (2-31) where and specify the means of discrete random variable X and Y respectively. As the supplementary knowledge, we also list the mean and variance formula for the continuous random variable X. The mean or expected value of continuous variable X is ∫ (2-32) The variance of continuous variableX is ∫ ∫ (2-33) 2.3.3.2 Spearman Correlation Coefficient\nIn order to distinguish the reliability of different MT evaluation metrics, Spearman\nrank correlation coefficient (a simplified version of Pearson correlation coefficient) is also commonly used to calculate the system level correlation, especially for recent\n37\nyears WMT task (Callison-Burch et al., 2011, 2010, 2009, 2008). When there are no\nties, Spearman rank correlation coefficient, which is sometimes specified as (rs) is\ncalculated as: ∑ (2-34) where is the difference-value (D-value) between the two corresponding rank variables – in  ⃑ and  ⃑⃑ describing the system , and n is the number of variables in the system. In the MT evaluation task, the Spearman rank correlation coefficient method is usually used by the authoritative ACL WMT to evaluate the correlation of MT evaluation metrics with the human judgments. There are some problems existing in this method. For instance, let two MT evaluation metrics MA and MB with their\nevaluation scores ⃑⃑⃑⃑ ⃑⃑⃑ and ⃑⃑⃑⃑ ⃑⃑ ⃑ respectively reflecting the MT systems ⃑⃑⃑ . Before the calculation of correlation with human judgments, they will be changed as ⃑⃑⃑⃑ ⃑⃑ ⃑̆ and ⃑⃑⃑⃑ ⃑⃑ ⃑̆ with the same rank sequence using Spearman method. Thus, the two evaluation systems will get the same correlation score with\nhuman judgments. But the two metrics reflect different results indeed: MA gives the\noutstanding score (0.95) to system and puts very low scores (0.50 and 0.45) on other two systems and ; on the other hand, MB thinks the three MT systems have similar performances (scores from 0.74 to 0.77). This information is lost using\nthe Spearman rank correlation methodology.\n38\n2.3.3.3 Kendall’s τ\nKendall’s (Kendall, 1938) has been used in recent years for the correlation between automatic order and reference order (Callison-Burch et al., 2012, 2011, 2010). It is\ndefined as:\n(2-35) The latest version of Kendall’s is introduced in (Kendall and Gibbons, 1990). Lebanon and Lafferty (2002) give an overview work for Kendall’s showing its application in calculating how much the system orders differ from the reference order. More concretely, Lapata (2003) proposes the use of Kendall’s , a measure of rank correlation, estimating the distance between a system-generated and a humangenerated gold-standard order\nKendall’s is less widely used than Spearman’s rank correlation coefficient ( ). The two measures have different underlying scales, and, numerically, they are not directly comparable to each other. Siegel and Castellan (1988) express the relationship of the two measures in terms of the inequality:\n(2-36) More importantly, Kendall’s and Spearman’s rank correlation coefficient have different interpretations. Kendall’s can be interpreted as a simple function of the probability of observing concordant and discordant pairs (Kerridge 1975). In other\nwords, it is the difference between the probability, that in the observed data two\nvariables are in the same order, versus the probability, that they are in different orders."
    }, {
      "heading" : "On the other hand, no simple meaning can be attributed to Spearman’s rank",
      "text" : "correlation coefficient . The latter is similar to the Pearson correlation coefficient\n39\ncomputed for values consisting of ranks. It is difficult to draw any meaningful\nconclusions with regard to information ordering based on the variance of ranks. In\npractice, while both correlations frequently provide similar answers, there\naresituations where they diverge. For example, the statistical distribution of approaches the normal distribution faster than (Kendall and Gibbons, 1990), thus offering an advantage for small to moderate sample studies with fewer data points.\nThis is crucial when experiments are conducted with a small number of subjects or\ntest items. Another related issue concerns sample size. Spearman’s rank correlation\ncoefficient is a biased statistic (Kendall and Gibbons, 1990). The smaller the sample,\nthe more diverges from the true population value, usually underestimating it. In contrast, Kendall’s does not provide a biased estimate of the true correlation.\n40\nCHAPTER 3: LEPOR – PROPOSED MODEL\nThe weaknesses of Manual judgments are apparent, such as time consuming,\nexpensive, unrepeatable, and low agreement sometimes (Callison-Burch et al., 2011).\nOn the other hand, there are also some weaknesses of existing automatic MT\nevaluation methods. Firstly, they usually show good performance on certain language\npairs (e.g. EN as target) and weak on others (e.g. EN as source). This is partly due to\nthe rich English resource people can utilize to aid the evaluation, such as dictionary,\nsynonym, paraphrase, etc. it may be also due to the different characteristics of the\nlanguage pairs and they need different strategies. For instance, TER metric (Snover et\nal., 2006) achieved 0.89 (ES-EN) vs 0.33 (DE-EN) correlation score with human\njudgments on WMT-2011 tasks. Secondly, some metric rely on many linguistic\nfeatures for good performances. This makes it not easy to repeat the experiment by\nother researchers, and it also makes the metric difficult to achieve generalization for\nother languages. For instance, MP4IBM1 metric (Popovic et al., 2011) utilizes large\nbilingual corpus, POS taggers, linguistic tools for morphemes/ POS / lexicon\nprobabilities, etc. This metric can show good performance on its focused language\npairs (English-German), but low performance on others. Thirdly, some metrics utilize\nincomprehensive factors. For example, the state-of-the-art BLEU metric (Papineni et\nal., 2002) is based on n-gram precision score only. Some researchers also held the\nopinion that the higher BLEU score is not necessarily indicative of better translation\n(Callison-Burch et al., 2006).\n41\nTo address some of the existing problems in the automatic MT evaluation metrics, in\nthis chapter, we introduce our designed models, including the augmented factors and\nthe metrics (Han et al., 2012).\n3.1 Enhanced Factors\nIn this section, we introduce the three enhanced factors of our methods including\nenhanced length penalty, n-gram position difference penalty, and n-gram precision\nand recall.\n3.1.1 Length Penalty\nIn the widely used metric BLEU (Papineni et al., 2002), it utilizes a brevity penalty\nfor shorter sentence; however, the redundant (or longer) sentences are not penalized\nproperly. To achieve a penalty score for the MT system, which tends to yield\nredundant information, we design a new version of the sentence length penalty factor,\nthe enhanced length penalty . In the Equation, means Length penalty, which is defined to embrace the penalty for both longer and shorter system outputs compared with the reference translations, and it is calculated as:\n{ (3-1) where and mean the sentence length of output candidate translation and reference translation respectively. When the output length of sentence is equal to that of the reference one, will be one which means no penalty. However, when the output\n42\nlength is larger or smaller than that of the reference one, will be little than one which means a penalty on the evaluation value of LEPOR. And according to the characteristics of exponential function mathematically, the larger of numerical difference between and , the smaller the value of will be. BLEU is measured at corpus level, which means the penalty score is directly the\nsystem level score, with and referring to the length of corpus level output and reference translations. Our length penalty score is measured in different way, first by\nsentence level.\n3.1.2 N-gram Position Difference Penalty\nThe word order information is introduced in the research work of (Wong and Kit,\n2008); however, they utilized the traditional nearest matching strategy and did not\ngive a formulized measuring function.\nWe design a different way to measure the position different, i.e. the n-gram position difference penalty factor, which means our matching is based on n-gram alignment considering neighbour information of the candidate matches. Furthermore, we give clear formulized measuring function.To measure this factor, there are mainly two stages, which include n-gram word alignment and score measuring. Let’s see it step by step as below.\nThe n-gram position difference penalty, ,is defined as: (3-2)\nwhere means n-gram position difference penalty. The value is designed to compare the words order in the sentences between reference translation and output translation. The value is normalized. Thus we can take all MT systems into account whose effective value varies between 0 and 1, and when\n43\nequals 0, the will be 1 which represents no penalty and is quite reasonable. When the increases from 0 to 1, the value decreases from 1 to , which is based on the mathematical analysis. Consequently, the final LEPOR value will be smaller. According to this thought, the is defined as:\n∑ (3-3)\nwhere represents the length of system output sentence and means the n-gram position -value (difference value) of aligned words between output and reference sentences. Every word from both output translation and reference should be\naligned only once (one-to-one alignment). Case (upper or lower) is irrelevant. When\n44\nthere is no match, the value of will be zero as default for this output translation word.\nTo calculate the NPD value, there are two steps: aligning and calculating. To begin\nwith, the Context-dependent n-gram Word Alignment task: we take the context-\ndependent factor into consideration and assign higher priority on it, which means we\ntake into account the surrounding context (neighbouring words) of the potential word\nto select a better matching pairs between the output and the reference. If there are both\nnearby matching or there is no matched context around the potential words pairs, then\nwe consider the nearest matching to align as a backup choice. The alignment direction\nis from output sentence to the reference translations. Assuming that represents the current word in output sentence and (or ) means the th word to the previous or following . While (or ) means the words matching in the references, and (or ) has the similar meaning as but in reference sentence. is the position difference value between the matching wordsin outputs and references. The operation process and pseudo code of\nthe context-dependent n-gram word alignment algorithm are shown in Figure 1 (with\n“→” as the alignment). Taking 2-gram (n = 2) as an example, let’s see explanation in\nFigure 3-1. We label each word with its absolute position, then according to the\ncontext-dependent n-gram method, the first word “A” in the output sentence has no\nnearby matching with the beginning word “A” in reference, so it is aligned to the fifth\nword “a” due to their matched neighbor words “stone”and “on” within one and two steps respectively away from current position. Then the fourth word “a” in\n45\nthe output will align the first word “A” of the reference due to the one-to-one\nalignment.The alignments of other words in the output are obvious.\nIn the second step (calculating step), we label each word with its position number\ndivided by the corresponding sentence length for normalization, and then using the Eq.\n(4) to finish the calculation. We also use the example in Figure 6-2 for the introduction:\nIn the example, when we label the word position of output sentence we divide the numerical position (from 1 to 6) of the current word by the reference sentence length 6. Similar way is applied in labeling the reference sentence. After we get the value, using the Eq. (3), the values of can be calculated.\n46\nWhen there is multi-references (more than one reference sentence), for instance 2\nreferences, we take the similar approach but with a minor change. The alignment\ndirection isreminded the same (from output to reference), and the candidate\nalignments that have nearby matching words also embrace higher priority. If the\nmatching words from Reference-1 and Reference-2 both have the nearby matching\nwith the output word, then we select the candidate alignment that makes the final value smaller. See below (also 2-gram) for explanation:\nThe beginning output words “the” and “stone” are aligned simply for the single\nmatching. The output word “on” has nearby matching with the word “on” both in\nReference-1 and Reference-2, due to the words “the” (second to previous) and “a”\n(first in the following) respectively. Then we should select its alignment to the word “on” in Reference-1, not Reference-2 for the further reason| | | | and this selection will obtain a smaller value. The remaining two words “a” and “bird” in output sentence are aligned using the same principle.\n3.1.3 Harmonic Mean of Precision and Recall\nIn the BLEU metric, there is only precision factor without recall. Generally, precision\nand recall are two parallel factors. Precision reflects the probability of how much the\n47\noutput content is correct, while recall reflects the probability of how much of the\nanswer is included by the output. So, both of the two aspects are important in\nevaluation. On the other hand, METEOR (Banerjee and Lavie, 2005) puts fixed\nhigher weight on recall as compared with precision score. For different language pairs,\nthe importance of precision and recall differ. To make a generalized factor for wide\nspread language pairs, we design the tunable parameters for precision and recall, i.e.,\nthe weighted harmonic mean of precision and recall.\nThe weighted harmonic mean of precision and recall ( and ), in the equation, is calculated as: (3-4)\nwhere and are two parameters we designed to adjust the weight of (recall) and (precision). The two metrics are calculated by: (3-5) (3-6)\nwhere represents the number of aligned (matching) words and marks appearing both in translations and references, and specify the sentence length of system output and reference respectively (Melamed et al., 2003).\n48\n3.2 Metrics Scores of Designed Methods\nWe name our metric as LEPOR, automatic machine translation evaluation metric\nconsidering the enhanced Length Penalty, Precision, n-gram Position difference\nPenalty and Recall (Han et al., 2012). To begin with, the sentence level score is the\nsimple product value of each factor. ∏ (3-7) Then, we design two strategies to measure the system level (document level) scores.\nOne is the arithmetic mean of each sentence level score, called as ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅. The other one ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ is the product value of system level factors score, which means that we first measure the system level factor scores as the arithmetic mean of sentence level factor scores, and then the system level LEPOR metric score is the product value of system level factors.\n̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ∑ (3-8) ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ∏ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ (3-9) ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ∑ (3-10)\nIn this initial version, the designed system level ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ reflects the system level metric score, and ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ reflects the system level factors score.\n49\nCHAPTER 4: IMPROVED MODELS WITH LINGUISTIC FEATURES\nThischapter introduces our improved model of our metrics, such as the new factors,\nvariants of LEPOR, and utilization of concise linguistic features (Han et al., 2013a).\n4.1 New Factors\nTo consider more about the content information, we design the new factors including\nn-gram precision and n-gram recall. These two factors are also measured first at\nsentence level, which is different with BLEU.\nLet’s see the n-gram scores. Here, n is the number of words in the block matching.\n(4-1) (4-2) (4-3)\nLet’s see an example of bigram matching, and it is the similar strategies for the block\nmatching with .\n50\n4.2 Variants of LEPOR\nThis section introduces two variants of LEPOR metric. The first one is based on\ntunable parameters designed for factor level, and the second one is based on n-gram\nmetric score (Han et al., 2013a; Han et al., 2014).\n4.2.1 hLEPOR\nTo achieve higher correlation with manual judgments when dealing with special\nlanguage pairs, we design tunable parameters to tune the weights of factors. It is\nachieved by using the weighted harmonic mean again. In this way, we try to seize the\nimportant characteristics of focused languages.\nLet’s see the formula below. The parameters , and are the weights of three factors respectively. ( ) (4-4) ∑ ∑ (4-5) In this version, the system-level scores are also measured by the two strategies introduced above. The corresponding formulas are:\n̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ∑ (4-6) ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ̅̅̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ̅̅ ̅̅ ̅̅ (4-7) 4.2.2 nLEPOR\nThe n-gram metric score is based on the utilization of weighted n-gram precision and\nn-gram recall factors. Let’s see the designed formula, where HPR is measured using\n51\nweighted n-gram precision and recall formula introduced previously. This variant is\ndesigned for the languages that request high fluency.\n∑ (4-8) ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ∑ (4-9) ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ̅̅̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ∑ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ (4-10)\n4.3 Utilization of Linguistic Feature\nThe linguistic features have been shown very helpful in many previous researches.\nHowever, many linguistic features relying on large external data may result in low\nrepeatable. In this section, we investigate the part of speech (POS) information in our\nmodel. We first attach the POS tags of each word or token of the sentences by POS\ntagger or parsing tools, and extract the POS sequence from both the MT output and\nthe reference translations. Then, we apply our algorithms on the POS sequence as the\nsame way on words sequence. In this way, we gain two different kinds of similarity\nscores, the word level and the POS level. The final metric score will be the weighted\ncombination of these two scores.\n52\nFigure 4-2: N-gram POS Sequence Alignment Example.\nLet’s see an example with the algorithm applied on POS sequences in the figure.\nIn this way, some words can be aligned by POS. It sometimes performs as synonym information, e.g. the words “say” and “claim” in the example are successful aligned.\nThe final scores of our methods using the linguistic features are the combination of word level and linguistic level scores:\n(4-11) ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ (4-12)\nThe and are measured using the same algorithm on POS sequence and word sequence respectively.\n53\nCHAPTER 5: EVALUATION\nThis chapter introduces the experimental performance of our designed MT evaluation\nmethods, including the results of initial metric LEPOR and the enhanced model\nhLEPOR.\n5.1 Experimental Setting\nWe first introduce the corpora preparation, the selected state-of-the-art metrics to\ncompare, and the evaluation criteria.\n5.1.1 Corpora\nWe utilize the standard WMT shared task corpora in our experiments. For the\ndevelopment set, we use the WMT 2008 corpora. The development corpora are to\ntune the parameters in our metrics to achieve a higher correlation with manual\njudgments. For the testing corpora, we use the WMT 2011 corpora.\nBoth the WMT 2008 2 and WMT 2011 3 corpora contain the language pairs of English (EN) to other (ES: Spanish, DE: German, FR: French and CS: Czech) and the inverse translation direction, i.e. other to English.\nThere are 2,028 and 3,003 sentences respectively for each language document in the\nWMT 2008 and WMT 2011 MT testing corpora. The effective number of participated\nMT systems in WMT 2011 for each language pair is shown in the Table 5-1.\n2 http://www.statmt.org/wmt08/ 3 http://www.statmt.org/wmt11/\n54\n5.1.2 Existing Metrics for Comparison\nTo compare with our initial version metric, the LEPOR, we selected three state-of-\nthe-art metrics as comparisons, including precision based metric BLEU(Papineni et al.,\n2002), edit distance based metric TER (Snover et al., 2006), and METEOR (version\n1.3) (Denkowski and Lavie, 2011), which used synonym and stemming as external\nlinguistic features. See Section 2.2 for detailed introduction of the metrics.\nFurthermore, we also selected two latest metrics the AMBER and MP4IBM1 as\ncomparisons. AMBER (Chen and Kuhn, 2011) is a modified version of BLEU,\nattaching more kinds of penalty coefficients and combining the n-gram precision and\nrecall. MP4IBM1 (Popovic et al., 2011) is based on morphemes, POS (4-grams) and\nlexicon probabilities, etc.\nTo investigate the performance of our improved metric hLEPOR, i.e. the metric with\nmore tunable parameters and concise POS as linguistic feature, we added two more\nmetrics in the comparison list including ROSE and MPF. This is due to the fact that\nboth ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011) metrics also utilized the\nPOS information as linguistic feature and they are very related with our work.\n55\n5.1.3 Evaluation Criteria\nEach year, there is a manual judgment task in the WMT after the participants\nsubmitted their MT outputs, and we regard the manual judgments as the golden one.\nIn this light, we measure the correlation score between manual judgments and\nautomatic evaluations. The evaluation criterion we utilized is the widely used system\nlevel spearman correlation score. See Section 2.3, the evaluation criteria for MT\nevaluation, for detailed formula.\n5.2 Experimental Results\nThis section introduces the experiments results of the MT evaluation, including the\nresults and initial metric and the results of improved metric.\n5.2.1 Initial MT Evaluation Results\nThe MT evaluation results using our initial metric LEPOR are demonstrated in Table\n5-2, the correlation score with human judgments. The is the arithmetic mean of sentence level score, and is the product value of system level factor scores. Please see Section 3.2, i.e. the metric scores, for the detailed metric formula.\nThe parameters and are tuned to be the value 9 and 1 respectively for all the language pairs, except for Czech-to-English with the value 1 and 9.\nThe metrics are rankedby their mean (hybrid) performance on the eight corpora from\nthe best to the worst. It shows that BLEU, AMBER (modified version of BLEU) and\nMeteor-1.3 perform unsteady with better correlation on some translation languages\n56\nand worse on others, resulting in medium level generally. TER and MP4IBM1 get the\nworst scores by the mean correlation.\nThe evaluation results also demonstrate that the first simplified version of our metric without using external resources yielded three top-one correlation scores on CZ-EN / ES-EN / EN-ES language pairs. Furthermore, LEPOR showed robust performance across languages, resulting in top one Mean-score 0.77.\nIt also releases the information that although the test metrics yield high system-level\ncorrelations with human judgments on certain language pairs, e.g. all correlations\nabove 0.83 on Czech-to-English, they are far from satisfactory by synthetically mean\nscores on total eight corpora, spanning from 0.58 to 0.77 only, and there is clearly a\npotential for further improvement.\n5.2.2 MT Evaluation Results with Improved Methods\nIn this improved version hLEPOR, the metric based on factor level weighted\nharmonic mean, with concise linguistic feature, the tuned values of many parameters\n57\non the development set are shown in Table 5-3.In the parameters table, the token “(W)”\nand “(POS)” mean this set of parameters are on the word level and extracted POS\nlevel respectively. The ratio “HPR:ELP:NPP” represents the different weights of three\nmain factors in our metric, i.e. the harmonic mean of precision and recall, the\nenhanced length penalty, and the n-gram position difference penalty. The ratio “ ” means the weights of recall and precision. The ratio “ ” represents the different weights of word level score and the POS level score. The token “N/A” means the POS\ninformation was not utilized on that language pair, so there is only word level score.\nThe testing results, correlation score with manual judgments, are demonstrated in\nTable 5-4.\nThe evaluation results using correlation score with manual judgments demonstrate\nthat our enhanced model hLEPOR yielded the highest score on the language pair\nGerman-to-English and higher scores on other language pairs. Our initial metric\nLEPOR remains the highest score on Spanish-to-English and Czech-to-English\nlanguage pairs. The MPF and ROSE metrics achieved the highest scores on English-\nto-French and English-to-Spanish respectively. However, in the overall performance,\nour improved model hLEPOR reached the top one level with the mean score 0.83,\nwhich is much higher than the initial version LEPOR with 0.77 score.\n59\nCHAPTER 6: EVALUATION ON ACL-WMT SHARED TASK\nThis chapter introduces our participation in the shared tasks of WMT 2013, the Eighth\nInternational Workshop of Statistical Machine Translation accompanied with ACL\nconference, including our submitted metrics and the official evaluation results (Han et\nal., 2013b).\n6.1 Task Introduction in WMT 2013\nIn the WMT 2013, there are mainly three kinds of shared tasks, i.e. the MT task, the\nMT evaluation task, and the quality estimation (QE) task. This section introduces our\nparticipation in the MT evaluation task. The MT evaluation task is to evaluate the\ntranslation qualities of submitted MT systems, including manual judgments and\nautomatic MT evaluation.\nIn addition to the traditional language corpora, i.e. English, Spanish, German, French, and Czech, there is one new language Russian participated in the WMT 2013; thus, there are two new language pairs in translation, the English-to-Russian and Russianto-English. In these two newly added language pairs, there are not very many training or developing data.\nFor each language pair, the evaluation task is to evaluate the translation quality of one\nsingle document that contains 3,000 sentences. The participated MT systems in each\nlanguage pair are shown in Table 6-1 (Bojar et al., 2013). The evaluation criteria for\nthe automatic MT emulation metrics are the correlation score with human judgements,\n60\nincluding Spearman, Pearson and Kendall’s tau. See Section 2.3 for the detailed\nevaluation criteria.\n6.2 Submitted Methods\nWe submitted two versions of our methods to the shared tasks in WMT-2013. The\nsubmitted metrics are LEPOR_v3.1 and nLEPOR_baseline. The LEPOR_v3.1 is actually the ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ metric and the nLEPOR_baseline is the n-gram based ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅metric with default parameter values. See Section 4.2 “variants of LEPOR” for detailed formula introduction. The parameters in the hLEPOR metric\nutilized are the same set as in the last section, testing for WMT-2011 corpora. For the\nnLEPOR_baseline metric, we utilized the unigram harmonic mean in the factor\nHPR.The parameters and in nLEPOR are tuned to be the value 9 and 1 respectively for all the language pairs, except for Czech-to-English with the value 1\nand 9.\n61\n6.3 Official Evaluation Results\nThere are 18 and 20 effective automatic MT evaluation metrics for the English-to-\nOther and Other-to-English translation directions respectively. Some of the metrics\nonly performed on single direction, such as English-to-other or other-to-English. For\ninstance, the UMEANT and Deprif metrics only submitted evaluation results for\nother-to-English direction; and the ACTa only submitted the evaluation results for\nEnglish-to-French and English-to-German. However, we submitted our evaluation\nresults on both translation directions.\n6.3.1 System-level Evaluation Results\nThis section introduces our metrics performances in system level correlation scores\nwith manual judgments. We firstly show the English-to-other direction and then the\nother-to-English direction.\n6.3.1.1 The Official Results of English-to-other MT Evaluation\nTable 6-2 and Table 6-3 demonstrate the official results using Pearson and Spearman\ncorrelation scores respectively. Table 6-2 shows LEPOR_v3.1 and nLEPOR_baseline\nyield the highest and the second highest average Pearson correlation score 0.86 and\n0.85 respectively with human judgments at system-level on five English-to-other\nlanguage pairs. LEPOR_v3.1 and nLEPOR_baseline also yield the highest Pearson\ncorrelation score on English-to-Russian (0.77) and English-to-Czech (0.82) language\npairs respectively. The testing results of LEPOR_v3.1 and nLEPOR_baseline show\nbetter correlation scores as compared to METEOR (0.81), BLEU (0.80) and TER-\nmoses (0.75) on English-to-other language pairs, which is similar with the training\nresults. On the other hand, using the Spearman rank correlation coefficient,\nSIMPBLEU_RECALL yields the highest correlation score 0.85 with human\njudgments. Our metric LEPOR_v3.1 also yields the highest Spearman correlation\nscore on English-to-Russian (0.85) language pair, which is similar with the result\nusing Pearson correlation and shows its robust performance on this language pair.\n63\n6.3.1.2 The Official Results of other-to-English MT Evaluation\nTable 6-4 and Table 6-5 demonstrate the official results using Pearson and Spearman\ncorrelation scores respectively for other-to-English translation direction.\nMETEOR yields the highest average correlation scores 0.95 and 0.94 respectively\nusing Pearson and Spearman rank correlation methods on other-to-English language\npairs. The average performance of nLEPOR_baseline is a little better than\nLEPOR_v3.1 on the five language pairs of other-to-English even though it is also\nmoder-ate as compared to other metrics. However, using the Pearson correlation\nmethod, nLEPOR_baseline yields the average correlation score 0.87 which already\nwins the BLEU (0.86) and TER (0.80) as shown in Table 6-4.\n65\n6.3.2 Segment-level MT Evaluation Results\nIn addition to the system level MT evaluation, our metric can also be utilized for the\nsegment level MT evaluation; because our metrics first measure the sentence score\nand then the document score. However, many automatic MT evaluation metrics can\nonly perform on system level (document score), thus, the participated automatic MT\nevaluation metrics for segment level strategy were much fewer than the system level.\nThis section introduces our performance in the segment level MT evaluation. The\nevaluation criterion is the Kendall’s tau.\nFrom the Table 6-6 and Table 6-7, the overall segment-level performance of LEPOR\nis moderate with the average Kendall’s tau correlation score 0.10 and 0.19\nrespectively on English-to-other and other-to-English directions. This is due to the\nfact that we trained our metrics at system-level in this shared metrics task. The\nsegment level evaluation scores are actually the bonuses of our participated metrics.\n68\nCHAPTER 7: QUALITY ESTIMATION OF MT\nThis chapter introduces the advanced technology in MT evaluation, which is usually\ncalled as Quality Estimation (QE) of MT. The Quality Estimation tasks make some\ndifferences from the traditional evaluation, such as extracting reference-independent\nfeatures from input sentences and the translation, obtaining quality score based on\nmodels produced from training data, predicting the quality of an unseen translated text\nat system run-time, filtering out sentences which are not good enough for post\nprocessing, and selecting the best translation among multiple systems, etc. In this\nchapter, we firstly introduce some QE methods without using reference translations.\nThen, we introduce the latest QE tasks, and finally, it is our proposed methods in the\nQE research area.\n7.1 Quality Estimation without using Reference Translations\nIn recent years, some MT evaluation methods that do not use the manually offered\ngolden reference translations are proposed. The unsupervised MT evaluation is\nusually called as Quality Estimation. Some of the related works have been mentioned\nin previous sections. We introduce some works that have not been discussed in\nprevious sections.\nGamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level without using the reference translations. Their system can also perform as a classifier to identify the worst-translated (highly dysfluent and ill-formed) sentences. The SVM classifier is used with linguistic features such as the trigram partof-speech tags, context-free grammar productions (the phrase sequences), semantic\n69\nanalysis features, and semantic modification relations, etc. The experiment on English-to-French corpus show that the described methods achieve lower correlation score with human judgments as compared to the BLEU metric, which uses the reference translations. However, when formulated as a classification task for identifying the worst-translated sentences, the combination of language model and SVM scores outperforms BLEU.\nThetraditional metrics BLEU and NIST are known to have good correlation with human evaluation at the corpus level, but this is not the case at the segment level. Specia et al. (2010) addressthe problem of evaluating the quality of MT as a prediction task, where reference-independent features are extracted from the input sentences and their translation, and a quality score is obtained based on models produced from training data. They showthat this approach yields better correlation at segment-level with human evaluation as compared to commonly used metrics, even with models trained on different MT systems, language-pairs and text domains.\nSuzuki (2011) develops a post-editing system based on phrase-based SMT (Moses) and applies it into a sentence-level automatic quality evaluator for machine translation in the absence of reference translations. The 28 features they used are from the partial least squares regression analysis on translation sentences (without the using of source sentences) such as 2-gram and 3-gram language model probability, 2-gram and 3- gram backward language model probability, POS 2-gram and 3-gram language model probability, noun phrase and verb phrase from grammar parser, etc. The experiment on Japanese-to-English patent translation, using the criteria adequacy and fluency, shows the validity of the designed methods.\nMehdad, et al. (2012) treat the MT evaluation as a cross-lingual textual entailment problem, and design the evaluation focusing on adequacy, semantic equivalence between source sentence and target translation, without using reference translations. The designed method is built on the advances in cross-lingual textual entailment\n70\nrecognition. They use support vector machine to learn models for classification and regression with a linear kernel and default parameters setting. The performances are carried out on English-Spanish language pairs. The large feature set is from the partof-speech tagger, dependency parsers and named entity recognizers, etc. The used features include Surface Form, number of words, punctuation and the ratios, etc.; Shallow Syntactic, ratios of POS tags in source and target; Syntactic, number and ratios of dependency roles; Phrase Table, lexical phrases extracted from bilingual parallel corpus; Dependency Relation, syntactic constraints; and Semantic Phrase Table, named entity, etc. Theexperiment on WMT 07 corpora shows higher correlation score with human adequacy annotation than METEOR but lower than BLEU and TER.\nOther related works about evaluation without using reference translations include\n(Blatz et al., 2004) and (Quirk, 2004) that perform the early attempt to evaluate the\ntranslation quality avoiding reference translations by the utilizing of large number of\nsource, target, and system-dependent features to discriminate the “good” and “bad”\ntranslations; Albrecht and Hwa (2007) utilizethe regression method and pseudo\nreferences; Specia and Gimenez (2010) combine the confidence estimation and\nreference-based metrics together for the segment-level MT evaluation; Popović et al.\n(2011) perform the MT evaluation using the IBM model-1 with the information of\nmorphemes, 4-gram POS and lexicon probabilities; Avramidis (2012) performs an\nautomatic sentence-level ranking of multiple machine translation using the features of\nverbs, nouns, sentences, subordinate clauses and punctuation occurrences to derive\nthe adequacy information. The detailed introduction and descriptions of the MT\nQuality Estimation tasks can be reached in (Callison-Burch et al., 2012) and (Felice\nand Specia, 2012).\n71\n7.2 Latest QE Tasks\nThe latest quality estimation tasksof MT can be found in WMT12 (Callison-Burch et al., 2012) and WMT13 4 . For the ranking task, they defined a novel task evaluation\nmetric that provides some advantages over the traditional ranking metrics. The\ndesigned criterion DeltaAvg assumes that the reference test set has a number\nassociated with each entry that represents its extrinsic value. For instance, using the\neffort scale, they associatea value between 1 and 5 with each sentence, representing\nthe quality of that sentence. Given these values, their metric does not need an explicit\nreference ranking, the way the Spearman ranking correlation does. The goal of the\nDeltaAvg metric is to measure how valuable a proposed ranking (hypothesis ranking)\nis according to the extrinsic values associated with the test entries. [ ] ∑ (7-1) For the scoring task, they usetwo task evaluation metrics that have been traditionally used for measuring performance for regression tasks: Mean Absolute Error (MAE) as a primary metric, and Root of Mean Squared Error (RMSE) as a secondary metric. For a given test set S with entries , , they denoteby the proposed score for entry (hypothesis), and by the reference value for entry (goldstandard value). They formally define the metrics as follows.\n∑ (7-2) √∑ ( ) (7-3)\n4 http://www.statmt.org/wmt13/quality-estimation-task.html\n72\nwhere . Both these metrics are nonparametric, automatic and deterministic (and therefore consistent), and extrinsically interpretable.\nThere appear to be significant differences between considering the quality estimation\ntask as a ranking problem versus a scoring problem. The ranking based approach\nappears to be somewhat simpler and more easily amenable to automatic solutions, and\nat the same time provides immediate benefits when integrated into larger applications,\nfor instance, the post-editing application described in (Specia, 2011). The scoring-\nbased approach is more difficult, as the high error rate even of oracle-based solutions\nindicates. It is also well-known from human evaluations of MT outputs that human\njudges also have a difficult time agreeing on absolute-number judgements to\ntranslations. The experiences in creating the current datasets confirmthat, even with\nhighly-trained professionals, it is difficult to arrive at consistent judgements. The\nWMT tasks planto have future investigations on how to achieve more consistent ways\nof generating absolute-number scores that reflect the quality of automated translations.\n7.3 Proposed Methods in the Advanced QE\nFirstly, we introduce our methods in the WMT-13 QE tasks (Han et al., 2013c). Then,\nwe introduce our research works out side of the task.\n7.3.1 Proposed Methods in WMT-13 QE Task\nIn the WMT-2013 QE shared task (Bojar et al., 2013), we participated the tasks of\nsentence-level English-to-Spanish QE, system selection for English-to-Spanish and\nEnglish-to-German translation, and the word-level QE for binary and multi-class error\nclassification.\n73"
    }, {
      "heading" : "ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB X .",
      "text" : "For the sentence-level EN-ES QE task, we designed the English and Spanish POS tagset mapping as shown in Table 7-1. The 75 Spanish POS tags yielded by the Treetagger (Schmid, 1994) are mapped to the 12 universal tags developed in (Petrov et al., 2012). Furthermore, we designed a novel evaluation method, the enhanced BLEU (EBLEU) as bellow:\n∑ (7-4) { (7-5)\nThe EBLEU formula is designed with the factors of modified length penalty ( ), n-gram precision and recall, the and representing the lengths of hypothesis (target) sentence and source sentence respectively.\nFor the system selection task, we investigated the probability model Naïve Bayes (NB) and support vector machine (SVM) classification algorithms in the QE performances using the features of Length penalty, Precision, Recall and Rank values.\nFor the word-level error classification task, we investigated a discriminative\nundirected probabilistic graphical model Conditional random field (CRF), in addition\n74\nto the NB algorithm. The official results show that the NB algorithm can show overall\nbetter performance than the CRF for error classification tasks.\n7.3.2 QE Model using Universal Phrase Category\nWe have designed a universal phrase tagset and utilized it into the MT evaluation\nwithout relying on reference translations (Han et al., 2013d). The universal tags we\nrefined are 9 commonly used categories, such as NP, VP, AJP, AVP, PP, S, CONJP,\nCOP, and X. The NP tag covers noun phrase, Wh- leading noun phrase, quantifier\nphrase, prenominal modifiers within an NP, head of the NP, classifier phrase, and\nlocalizer phrase, Spanish multiword proper name (MPN). The VP tag covers verbal\nphrase, coordinated verb compound (VCD), verb-resultative and verb-directional\ncompounds (VRD), verb compounds forming a modifier + head relationship (VSB),\nverb compounds formed by VV+VC (VCP), verb nucleus (VN.fr), coordinated verb\nphrase (CVP.de), etc. The AJP tag covers adjective phrase, Wh-adjective phrase,\ndeterminer phrase (DP.cn), coordinated adjective phrase (CAP.de), multi-token\nadjective (MTA.de), etc. The AVP tag covers adverb phrase, Wh-adverb phrase,\nEnglish particle (PRT), Chinese “XP+地” phrase, coordinated adverbial phrase, etc.\nThe PP tag covers prepositional phrase, Wh-prepositional phrase, German coordinated\nadposition (CAC), German coordinated adpositional phrase (CPP), German\ncoordinated complementiser (CCP), Spanish multitoken preposition (MTP), etc. The\nS tag covers sentence, sub-sentence, clause introduced by subordinating conjunction,\ndirect question introduced by Wh-word or Wh-phrase, fragment, reduced relative\nclause, parenthetical, incomplete sentences, etc. The CONJP tag covers conjunction\nphrase, multitoken conjunction, etc. The tag COP covers English unlike coordinated\n75\nphrase, Chinese unidentical coordination phrase, French coordinated phrase\n(COORD), German coordination (CO), etc. The X tag covers URL, punctuations, list\nmarker, interjection, Chinese chunks of text that are redundant in a sentence, German\nNegra idiosyncratic unit (ISU), German Negra quasi-language (QL), etc.\nThese 9 phrase categories are the most frequently appearing ones in the existing treebanks. We design these 9 phrase categories as the universal phrase tagset, and conduct the mappings from the tags of existing treebanks to the universal ones. The studied 25 treebanks cover 21 languages, i.e., Arabic, Catalan, Chinese, Danish, English, Estonian, French, German, Hebrew, Hindi, Hungarian, Icelandic, Italian, Japanese, Korean, Portuguese, Spanish, Swedish, Thai, Urdu, and Vietnamese. The mapping results are shown in the tables of Appendix A.\nTo utilize the designed universal phrase tags into QE research. We firstly, parse the\nsource and target (automatic translated) sentences and extract their phrase sequences.\nThen, we convert the phrase sequences into universal tags using our designed\nmapping. Finally, we measure the similarity score on the converted source and target\nphrase sequences. Let’s see an example with French-to-English MT evaluation.\nFigure 7-1 is the parsing for the source French and target English sentences. Figure 7-\n2 is the extracted phrase sequences of the two sentences and the conversion into\nuniversal phrase tag categories.\n76\n77\nFinally, we designed the metric HPPR to measure the similarity of the phrase tag sequences. This metric is the harmonic mean of N1-gram position difference penalty , N2-gram precision and N3-gram recall . ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ (7-6) ̅̅ ̅̅ ̅̅ ̅̅ ̅̅ ̅ ∑ (7-7) (∑ ) (7-8) ∑ (7-9)\nWe conducted some experiments using our designed HPPR methods. The corpora used in the experiments are from the international workshop ofstatistical machine translation (WMT). To avoid theoverfitting problem, the WMT 2011 corpora are used as the development set to tune the weights of factors in the formula to make the evaluation results closeto the human judgments. Then the WMT 2012 corpora are used as the testingset with the formula that has the same parameters tuned in the developmentstage.\nThere are 18 and 15 systems respectively in WMT 2011 and WMT 2012\nproducingthe French-to-English translation documents, each document containing\n3003 sentences. Each year, there are hundreds of human annotators to evaluatethe MT\nsystem outputs, and the human judgments task usually costs hundredsof hours of\nlabor. The human judgments are used to validate the automaticmetrics. The system-\nlevel Spearman correlation coefficient of the different evaluationresults will be\ncalculated as compared to the human judgments. Thestate-of-the-art evaluation\nmetrics BLEU (measuring the closeness between thehypothesis and reference\ntranslations) and TER (measuring the editing distance) are selected for the\ncomparison with the designed model HPPR.The values of N2 and N3 are both\nselected as 3 due to the fact that the4-gram chunk match usually results in 0 score.\nThe tuned factor weights inthe formula are shown in Table 7-2. The experiment\nresults on the testing corpora are shown in Table 7-3, where the phrase “Use\nReference?”means whether this metric uses reference translations in the evaluation.\nThe experiment results on the testing corpora show that HPPR without using\nreference translations has yielded comparable correlation score 0.63 with human\njudgments even though lower than the reference-aware metrics. This proves to be a\npromising investigation.\n79\nCHAPTER 8: CONCLUSION AND FUTURE WORK\nTo facilitate the development of MT itself, it is crucial to tell the MT researchers and\ndevelopers whether his or her system achieves an improvement after conducting some\nrevisions, such as new algorithms or features. This work introduces our proposed\nmethods for the automatic MT evaluation. To address some of the weaknesses in the\nexisting MT evaluation methods, we designed augmented factors, tunable parameters\nand concise linguistic features to yield reliable evaluation. Furthermore, our methods\ncan be easily employed to different language pairs, or new language pairs due to the\nconcise external resources utilized. For the existing weaknessses of MT evaluation,\nsuch as the evaluation with non-English in target language and the low resource\nlanguage pairs, our proposed methods have shown some improvements as compared\nwith the state-of-the-art metrics. We also introduced our designed model for the\nquality estimation of MT and the experiments show that our proposed methods\nwithout using reference translations yielded promising results as compared with the\nreference-aware metrics. In spite of the many efforts from the MT evaluation\nresearchers, there remain some issues for the future research as bellow.\n8.1 Issues in Manual Judgments\nSo far, the human judgment scores of MT results are usually considered as the golden\nstandard that the automatic evaluation metrics should try to approach. However, some\nimproper handlings in the process also yield problems. For instance, in the ACL\nWMT 2011 English-Czech task, the multi-annotator agreement kappa value k is very\n80\nlow and even the exact same string produced by two systems is ranked differently\neach time by the same annotator (Bojar et al., 2011). Secondly, the evaluation results\nare highly affected by the manual reference translations. How to ensure the quality of\nreference translations and the agreement level of human judgments are two important\nproblems.\n8.2 Issues in Automatic Evaluation\nFirst, automatic evaluation metrics are indirect measures (Moran and Lewis, 2011)of\ntranslation quality, because that they are usually using the various string distance\nalgorithms to measure the closeness between the machine translation system outputs\nand the manually offered reference translations and they are based on the calculating\nof correlation score with manual MT evaluation.\nFurthermore, the existing automatic evaluation metrics tend to ignore the relevance of words (Koehn, 2010). For instance, the name entities and core concepts are more important than punctuations and determiners but most automatic evaluation metrics put the same weight on each word of the sentences.\nThird, existing automatic evaluation metrics usually yield meaningless score, which is very test set specific and the absolute value is not informative. For instance, what is the meaning of -16094 score by the MTeRater metric (Parton et al., 2011) or 1.98 score by ROSE (Song and Cohn, 2011)?\nFourth, some of the existing automatic metrics only use the surface words information without any linguistic features, which makes them result in low correlation with human judgment and receives much criticism from the linguists; on the other hand, some metrics utilize too many language specific linguistic features, which make it\n81\ndifficult to promote them on other language pairs. How to handle the balance between the two aspects is a challenge before researchers.\nThe automatic evaluation metrics should try to achieve the goals of low cost, tunable,\nconsistent, meaningful, and correct, of which the first three aspects are easily\nachieved but the rest two goals, i.e. meaningful and correct, and the robustness in\ndifferent language pairs are usually the challenges in front of us.\n8.3 Future Work\nThis work tried to advance the MT evaluation by using augmented factors and concise\nlinguistic features. In our future work, we plan to investigate the MT evaluation\nperformance from some different aspects.\nFirstly, we want to utilize our designed universal phrase tagset into the MT evaluation on more language pairs. In this work, we only employ the universal tagset into French-English MT evaluation, and it has shown some promising results.\nSecondly, we plan to enhance the performance of the designed LEPOR MT evaluation models with extended linguistic features, especially semantic features, such as synonyms, paraphrasing and text entailments.\nThirdly, we want to investigate some machine learning technologies into MT\nevaluation. For instance, we plan to utilize the deep learning method to convert the\nsurface words into the vector form. In this way, we can measure the similarity of\nsource and target languages on the vector level instead of word or sentence level. The\nreference translation can be a waiver in this framework.\n82"
    }, {
      "heading" : "NP, CLP, QP, LCP,",
      "text" : "WHNP np NP\nNPper, NPloc,\nNPtmp, NP,\nNP.foc\nVP VP VP\nVP, VCD, VCP,\nVNV, VPT, VRD,\nVSB\nvp VN, VP, VPpart,\nVPinf\nVP.foc, VP,\nVPcnd, VPfin\nAJP ADJP ADJP, WHADJP ADJP, DP, DNP ap, adjp AP AP.foc, AP,\nAPcnd\nAVP ADVP, WHADVP ADVP, WHAVP,\nPRT, WHADVP ADVP, DVP advp AdP\nADVP.foc,\nADVP\nPP PP, WHPP PP, WHPP PP pp PP\nPP, PP.foc,\nPPnom, PPgen,\nPPacc\nS S, SBAR, SBARQ,\nSINV, SQ\nS, SBAR, SBARQ,\nSINV, SQ, PRN,\nFRAG, RRC"
    }, {
      "heading" : "IP, CP, PRN, FRAG,",
      "text" : "INC\nfcl, icl, acl, cu,\nx, sq\nSENT, Ssub, Sint,\nSrel, S S, SS\nCONJP CONJP\nCOP UCP UCP COORD\nX X X, INTJ, LST"
    }, {
      "heading" : "LST, FLR, DFL,",
      "text" : "INTJ, URL, X ITJ, GR, err\nUniversal Phrase Tag\nDanish Arboretum\nTreebank\nGerman\nNegraTreebank\n(Skut et al., 1997)\nSpanish UAM\nTreebank (Moreno et\nal., 1999)\nHungarian\nSzeged\nTreebank\nSpanish Treebank\n(Volk, 2009)\nSwedish\nTalbanken05 (Nivre et al.,\n2006)\nNP np NP, CNP, MPN, NM HOUR, NP, QP,\nSCORE, TITLE NP, QP NP, MPN CNP, NP\nVP vp, acl VP, CVP, VZ, CVZ VP VP, INF_,\nINF0 SVC CVP, VP\nAJP ajp AP, CAP, MTA ADJP ADJP AP AP, CAP, AVP dvp AVP, CAVP, AA ADVP, PRED-\nCOMPL\nADVP, PA_,\nPA0 AVP AVP, CAVP,\nPP pp PP, CAC, CPP, CCP PP PP PP, MTP CPP, PP S fcl, icl S, CS, CH, DL,\nPSEUDO CL, S S S, INC CS, S\nCONJP cp C0 MTC\nCOP CO CP\nCS, CNP, CPP, CAP,\nCAVP, CAC, CCP,\nCO\nCONJP, CXP\nX par ISU, QL FP, XP NAC, XP\n110\nPhrase Tagset Mapping between Universal Tagset and Existing Treebanks: Continue\nUniversal Phrase Tag\nArabic PENN\nTreebank (Bies and Maamouri, 2003.)\nKorean Penn Treebank (Han et al., 2001; 2002)\nEstonian Arborest\nTreebank\nIcelandic IcePaHC Treebank\n(Wallenberg et\nal., 2011)\nItalian ISST Treebank\n(Montemagni et al., 2000;\n2003)\nPortuguese Tycho\nBrahe Treebank (Galves & Faria,\n2010)\nNP NP, NX, QP,\nWHNP NP\nAN>, <AN, NN>,\n<NN, NP, QP, WNP SN\nNP, NP-ACC, NP-\nDAT, NP-GEN,\nNP-SBJ, IP-SMC,\nNP-LFD, NP-\nADV, NP-VOC,\nNP-PRN\nVP VP VP VN>, <VN,\nINF_N>, <INF_N VP IBAR VB, VB-P\nAJP ADJP, WHADJP ADJP, DANP ADJP, ADJP-\nSPR SA ADJP, ADJP-SPR\nAVP ADVP, WHADVP ADVP, ADCP AD>, <AD\nADVP, ADVP-\nDIR, ADVPLOC, ADVP-\nTMP, RP\nSAVV ADVP, WADVP\nPP PP, WHPP PP, WPP, PP-BY,\nPP-PRN SP, SPD, SPDA\nPP, PP-ACC, PPSBJ, PP-LFD, PP-\nPRN, PP-LOC\nS S, SBAR, SBARQ,\nSQ S\nF, SV2, SV3, SV5, FAC,\nFS, FINT, F2\nRRC, CP, CP-\nREL, IP-MAT, IPINF, IP-SUB, CP-\nADV, CP-THT\nCONJP CONJP, NAC CONJP CP, COMPC CONJP\nCOP UCP PN>, <PN FC, COORD\nX PRN, PRT, FRAG,\nINTJ, X INTJ, PRN, X, LST, XP <P, P>, <Q, Q> LATIN FP, COMPT, COMPIN\nUniversal Phrase Tag\nHindi-Urdu\nTreebank (Bhatt et\nal., 2012)\nCatalan AnCora\nTreebank (Civit and\nMarti, 2004; Taulé et al.,\n2008)\nSwedish Treebank ()\nVietnamese\nTreebank\n(Nguyen et al.,\n2009)\nThai CG Treebank\n(Ruangrajitpakorn et al.,\n2009)\nHebrew (Sima’an\net a., 2001)\nNP\nNP, NP-P, NP-NST, SC-A, SC-P, NP-P-\nPred\nSn NP NP, WHNP, QP np, num, spnum NP-gn-(H)\nVP VP, VP-Pred, V’ Gv VP VP PREDP, VP, VP-\nMD, VP-INF\nAJP AP, AP-Pred Sa AP AP, WHAP ADJP-gn-(H) AVP DegP sadv, neg AVP RP, WHRP ADVP\nPP Sp PP PP, WHPP pp PP\nS\nS, S*, S.NF.C, S.NF.A,\nS.NF.P, S.F.C,\nS.F.AComp, S.F.AConc,\nS.F.Acons, S.F.Acond,\nS.F.R,\nROOT, S S, SQ, SBAR s, ws, root FRAG, FRAGQ,\nS, SBAR, SQ\nCONJP conj.subord, coord\nCOP CCP, XP-CC\nX CP\ninterjeccio,\nmorfema.verbal,\nmorf.pron\nXP XP, YP, MDP INTJ, PRN\n111"
    }, {
      "heading" : "VITA",
      "text" : "Aaron Li-Feng Han\nUniversity of Macau\n2014"
    }, {
      "heading" : "Award",
      "text" : "Second Prize in National Post-Graduate Mathematical Contest in Modeling (NPGMCM2011) 2011. National Postgraduate Mathematical Contest In Modeling Committee\nThe first prizes and second prizes occupy 20.45% of total 2,245 teams from 242\nuniversities and research institutes all over the P.R.C. country, including more than 300\ndoctoral students.(paper).http://www.shumo.com/home/html/1317.html\nProfessional Information\nOpen source tools: Homepage: https://github.com/aaronlifenghan\nGoogle scholar citation:\nHomepage: http://scholar.google.com/citations?hl=en&user=_vf3E2QAAAAJ"
    }, {
      "heading" : "Selected Publications:",
      "text" : "1. Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao, Liangye He and Yi Lu.\nUnsupervised Quality Estimation Model for English to German Translation and Its Application in Extensive Supervised Evaluation. The Scientific World Journal, Issue: Recent Advances in Information Technology. Page 1-12, April 2014.Hindawi Publishing Corporation. ISSN:1537-744X. http://www.hindawi.com/journals/tswj/aip/760301/\n112\n2. Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao, Liangye He, Yi Lu, Junwen Xing\nand Xiaodong Zeng. Language-independent Model for Machine Translation Evaluation with Reinforced Factors. Proceedings of the 14th International Conference of Machine Translation Summit (MT Summit), pp. 215-222. Nice, France. 2 - 6 September 2013. International Association for Machine Translation. http://www.mt-archive.info/10/MTS2013-Han.pdf\n3. Aaron Li-Feng Han, Derek Wong, Lidia S. Chao, Yi Lu, Liangye He, Yiming Wang,\nJiaji Zhou. A Description of Tunable Machine Translation Evaluation Systems in WMT13 Metrics Task. Proceedings of the ACL 2013 EIGHTH WORKSHOP ON STATISTICAL MACHINE TRANSLATION (ACL-WMT), pp. 414-421, 8-9 August 2013. Sofia, Bulgaria. Association for Computational Linguistics. http://www.aclweb.org/anthology/W13-2253\n4. Aaron Li-Feng Han, Yi Lu, Derek F. Wong, Lidia S. Chao, Liangye He, Junwen Xing.\nQuality Estimation for Machine Translation Using the Joint Method of Evaluation Criteria and Statistical Modeling. Proceedings of the ACL 2013 EIGHTH WORKSHOP ON STATISTICAL MACHINE TRANSLATION (ACL-WMT), pp. 365-372. 8-9 August 2013. Sofia, Bulgaria. Association for Computational Linguistics. http://www.aclweb.org/anthology/W13-2245\n5. Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao, Liangye He, Shuo Li and Ling Zhu.\nPhrase Tagset Mapping for French and English Treebanks and Its Application in Machine Translation Evaluation. Language Processing and Knowledge in the Web. Lecture Notes in Computer Science Volume 8105, 2013, pp 119-131. Volume Editors: Iryna Gurevych, Chris Biemann and Torsten Zesch. Springer-Verlag Berlin Heidelberg. http://dx.doi.org/10.1007/978-3-642-40722-2_13\n6. Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao, Liangye He, Ling Zhu and Shuo Li.\nA Study of Chinese Word Segmentation Based on the Characteristics of Chinese. Language Processing and Knowledge in the Web. Lecture Notes in Computer Science Volume 8105, 2013, pp 111-118. Volume Editors: Iryna Gurevych, Chris Biemann and Torsten Zesch. Springer-Verlag Berlin Heidelberg. http://dx.doi.org/10.1007/978-3-64240722-2_12\n113\n7. Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao, Liangye He. Automatic Machine\nTranslation Evaluation with Part-of-Speech Information. Text, Speech, and Dialogue. Lecture Notes in Computer Science Volume 8082, 2013, pp 121-128. Volume Editors: I. Habernal and V. Matousek. Springer-Verlag Berlin Heidelberg. http://dx.doi.org/10.1007/978-3-642-40585-3_16\n8. Aaron Li-Feng Han, Derek Fai Wong and Lidia Sam Chao. Chinese Named Entity\nRecognition with Conditional Random Fields in the Light of Chinese Characteristics. Language Processing and Intelligent Information Systems. Lecture Notes in Computer Science Volume 7912, 2013, pp 57-68. M.A. Klopotek et al. (Eds.): IIS 2013. SpringerVerlag Berlin Heidelberg. http://dx.doi.org/10.1007/978-3-642-38634-3_8\n9. Aaron Li-Feng Han, Derek F. Wong and Lidia S. Chao. LEPOR: A Robust Evaluation\nMetric for Machine Translation with Augmented Factors. Proceedings of the 24th International Conference on Computational Linguistics (COLING): Posters, pages 441– 450, Mumbai, December 2012. Association for Computational Linguistics. http://aclweb.org/anthology//C/C12/C12-2044.pdf"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "LEPOR: AN AUGMENTED MACHINE TRANSLATION EVALUATION METRIC by LiFeng Han, Aaron Thesis Supervisors: Dr. Lidia S. Chao and Dr. Derek F. Wong Master of Science in Software Engineering Machine translation (MT) was developed as one of the hottest research topics in the natural language processing (NLP) literature. One important issue in MT is that how to evaluate the MT system reasonably and tell us whether the translation system makes an improvement or not. The traditional manual judgment methods are expensive, time-consuming, unrepeatable, and sometimes with low agreement. On the other hand, the popular automatic MT evaluation methods have some weaknesses. Firstly, they tend to perform well on the language pairs with English as the target language, but weak when English is used as source. Secondly, some methods rely on many additional linguistic features to achieve good performance, which makes the metric unable to replicateand apply to other language pairs easily. Thirdly, some popular metrics utilize incomprehensive factors, which result in low performance on some practical tasks. In this thesis, to address the existing problems, we design novel MT evaluation methods and investigate their performances on different languages. Firstly, we design augmented factors to yield highly accurate evaluation.Secondly, we design a tunable evaluation model where weighting of factors can be optimized according to the characteristics of languages. Thirdly, in the enhanced version of our methods, we design concise linguistic feature using POS to show that our methods can yield even higher performance when using some external linguistic resources. Finally, we introduce the practical performance of our metrics in the ACL-WMT workshop shared tasks, which show that the proposed methods are robust across different languages.",
    "creator" : "Online2PDF.com"
  }
}