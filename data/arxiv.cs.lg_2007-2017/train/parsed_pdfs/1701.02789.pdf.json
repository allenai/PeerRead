{
  "name" : "1701.02789.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Identifying Best Interventions through Online Importance Sampling",
    "authors" : [ "Rajat Sen", "Karthikeyan Shanmugam", "Alexandros G. Dimakis", "Sanjay Shakkottai", "Thomas J. Watson" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Causal graphs [28] are useful for representing causal relationships among interacting variables in large systems [7]. Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18]. There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system. The change either alters the relationship between the parental causes and the variable, or decouples it from the parental causes entirely. Our focus is on optimizing over a given set of interventions.\nAn illustrative example includes online advertising [7], where there is a collection of click-through rate scoring algorithms that provide an estimate of the probability that an user clicks on an ad displayed at a specific position. The interventions occur through the choice of click-through rate scoring algorithm; the algorithm choice directly impacts ad placement and pricing, and through a complex network of interactions, affects the revenue generated through advertisements. The revenue is used to determine the best scoring algorithm (optimize for the best intervention); see Figure 1. Another example is in biological gene-regulatory networks [6], where a large number of genomes interact amongst each other and also interact with environmental factors. The objective here is to understand the best perturbation of some genomes in terms of its effect on the expression of another subset of genomes (target) in cellular systems.\nThis paper focuses on the following setting: We are given apriori knowledge about the structure and strength of interactions over a small part of the causal graph. In addition, there is freedom to intervene (from a set of allowable interventions) at a certain node in the known part of the graph, and collect data under the chosen intervention; further we can alter the interventions over time and observe the corresponding effects. Given a set of potential interventions to optimize over, the key question of interest is: How to choose the best sequence of T allowable interventions in order to discover which intervention maximizes the expectation of a downstream target node?\nar X\niv :1\n70 1.\n02 78\n9v 3\n[ st\nat .M\nL ]\n9 M\nar 2\nDetermining the best intervention in the above setting can be cast as a best arm identification bandit problem, as noted in [22]. The possible interventions to optimize over are the arms of the bandit, while the sample value of the target node under an intervention is the reward.\nMore formally, suppose that V is a node in a causal graph G(V, E) (as shown in Fig. 2), with the parents of V denoted by pa(V ). In Fig. 1, V corresponds to the click-through rate and its parents are user-query and ads-chosen. This essentially means that V is causally determined by a function of pa(V ) and some exogenous random noise. This dependence is characterized by the conditional P(V |pa(V ))1. Then a (soft) intervention mathematically corresponds to changing this conditional probability distribution i.e. probabilistically forcing V to certain states given its parents. In the computational advertising example, the interventions correspond to changing the click through rate scoring algorithm i.e P(click through rate|ads chosen, user query), whose input-output characteristics are well-studied. Further, suppose that the effect of an intervention is observed at a node Y which is downstream of V in the topological order (w.r.t G) -refer to Fig. 2. Then, our key question is stated as follows: Given a collection of interventions {P0(V |pa(V )), . . .PK−1(V |pa(V ))}, find the best intervention among these K possibilites that maximizes E[Y ] under a fixed budget of T (intervention, observation) pairs."
    }, {
      "heading" : "1.1 Main Contributions",
      "text" : "(Successive Rejects Algorithm) We provide an efficient successive rejects multi-phase algorithm. The algorithm uses clipped importance sampling. The clipper level is set adaptively in each phase in order to trade-off bias and variance. Our procedure yields a major improvement over the algorithm in [22] (both in theoretical guarantees and in practice), which sets the clippers and allocates samples in a static manner. (Gap Dependent Error Guarantees under Budget Constraints) In the classic best arm identification problem [3], Audibert et al. derive gap dependent bounds on the probability of error given a fixed sample budget. Specifically, let ∆(i) be the i-th largest gap (difference) in the expected reward from that of the best\n1Formally if node V has parents V1, V2, then this distribution is the conditional P(V = v|V1 = v1, V2 = v2) for all v, v1, v2.\narm (e.g. ∆(1) is the difference between the best arm expected reward and the second best reward). Then, it has been shown in [3] that the number of samples needed scales as (upto poly log factors) maxi(i/∆ 2 (i)).\nIn our setting, a fundamental difference from the classical best arm setting [3] is the information leakage across the arms, i.e, samples from one arm can inform us about the expected value of other arms because of the shared causal graph. We show that this information leakage yields significant improvement both in theory and practice. We derive the first gap dependent (gaps between the expected reward at the target under different interventions) bounds on the probability of error in terms of the number of samples T , cost budget B on the relative fraction of times various arms are sampled and the divergences between soft intervention distributions of the arms.\nIn our result (upto poly log factors) the factor i is replaced by the ’effective variance’ of the estimator for arm (i), i.e. we obtain (with informal notation) maxi σ 2 i /∆ 2 (i). σi can be much smaller than √ i (the corresponding term in the results of [3]). Our theoretical guarantees quantify the improvement obtained by leveraging information leakage, which has been empirically observed in [7]. We discuss in more detail in Sections 3.3 and B (in the appendix), about how these guarantees can be exponentially better than the classical ones. We derive simple regret (refer to Section 3.1) bounds analogous to the gap dependent error bounds.\n(Novel f-divergence measure for analyzing Importance Sampling) We provide a novel analysis of clipped importance sampling estimators, where pairwise f -divergences between the distributions {Pk(V |pa(V ))}, for a carefully chosen function f(.) (see Section C.1) act as the ‘effective variance’ term in the analysis for the estimators (similar to Bernstein’s bound [4]).\n(Extensive Empirical Validation) We demonstrate that our algorithm outperforms the prior works [22, 3] on the Flow Cytometry data-set [32] (in Section 4.1). We exhibit an innovative application of our algorithm for model interpretation of the Inception Deep Network [38] for image classification (refer to Section 4.2).\nRemark 1. The techniques in this paper can be directly applied to more general settings like (i) the intervention source (V ) can be a collection of nodes (V) and the changes affect the distribution P (V|pa(V)), where pa(V) is the union of all the parents; (ii) the importance sampling can be applied at a directed cut separating the sources and the targets, provided the effect of the interventions, on the nodes forming the cut can be estimated. Moreover, our techniques can be applied without the complete knowledge of the source distributions. We explain the variations in more detail in Section A in the appendix."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "The problem lies at the intersection of causal inference and best arm identification in bandits. There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8]. It was shown recently in [8] that the results of [3] are optimal. The key difference from our work is that, in these models, there is no information leakage among the arms.\nThere has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature. One notable work that partially inspired our work is [7] where the causal graph underlying a computational advertising system (like in Bing, Google etc.) is known and the primary interest is to find out how a change in the system would affect some other variable.\nAt the intersection of causality and bandits, [22] is perhaps most relevant to our setting. It studies the problem of identifying the best hard interventions on multiple variables (among many), provided the distribution of the parents of the target is known under those interventions. Simple regret bound of order O(1/ √ T ) was derived. We assume soft interventions that affect the mechanism between a ’source’ node and its parents, far away from the target (similar to the case of computational advertising considered in [7]). Further, we derive the first gap dependent bounds (that can be exponentially small in T ), generalizing the results of [3]. Our formulation can handle general budget constraints on the bandit arms and also recover the problem independent bounds of [22] (orderwise). Budget constraints in bandit settings have been explored before in [1, 34].\nIn the context of machine learning, importance sampling has been mostly used to recondition input data to adhere to conditions imposed by learning algorithms [37, 23, 39]."
    }, {
      "heading" : "2 Problem Setting",
      "text" : "A causal graph G(V, E) specifies causal relationships among the random variables representing the vertices of the graph V. The relationships are specified by the directed edges E ; an edge Vi → Vj implies that Vi ∈ V is a direct parental cause for the effect Vj ∈ V. With some abuse of notation, we will denote the random variable associated with a node V ∈ V by V itself. We will denote the parents of a node V by pa(V ). The causal dependence implies that V = fV (U ∈ pa(V ), V ), where V is an independent exogenous noise variable. One does not get to measure the functions fV in practice. The noise variable and the above functional dependence induce a conditional probability distribution P(V |pa(V )). Further, the joint distribution of {V }V ∈V decomposes into product of conditional distributions according to G viewed as a Bayesian Network, i.e. P({V }V ∈V) =\n∏ V ∈V\nP(V |pa(V )). Interventions in a causal setting can be categorized into two kinds:\n1. Soft Interventions: At node V , the conditional distribution relating pa(V ) and V is changed to P̃ (V |pa(V )).\n2. Hard Interventions: We force the node V to take a specific value x. The conditional distribution P̃ (V |pa(V )) is set to a point mass function 1V=x.\nIn this work, we consider the problem of identifying the best soft intervention, i.e. the one that maximizes the expected value of a certain target variable. The problem setting is best illustrated in Figure 2. Consider a causal graph G(V, E) that specifies directed causal relationships between the variables V. Let Y be a target random variable which is downstream in the graph G; the expected value of this target variable is the quantity of interest. Consider another random variable V along with its parents pa(V ). We assume that there are K possible soft interventions. Each soft intervention is a distinct conditional distribution that dictates the relationship pa(V ) → V . During a soft intervention k ∈ [K] ([K] = {0, 1, ...,K − 1}), the conditional distribution of V given its parents is set to Pk(V |pa(V )) and all other relationships in the causal graph are unchanged.\nIt is assumed that the conditional distributions Pk(V |pa(V )) and marginals for pa(V ) for k ∈ [K] are known from past experiments or existing domain knowledge. We only observe samples of Y, V and pa(V ),\nwhile the rest of the variables in the causal graph may be unobserved under different interventions. For simplicity we assume that the variables V, pa(V ) are discrete while the target variable Y may be continuous/discrete and has bounded support in [0, 1]. Further, we assume that the various conditionals, i.e. Pk (V |pa(V )) are absolutely continuous with respect to each other. In the case of discrete distributions, the non-zero supports of these distributions are identical. However, our algorithm can be easily generalized for continuous distributions on V and pa(V ) (as in our experiments in Section 4.1). In this setting, we are interested in the following natural questions: Which of the K soft interventions yield the highest expected value of the target (E[Y ]) and what is the misidentification error that can be achieved with a finite total budget T for samples ?\nRemark 2. Although we may know apriori the joint distribution of pa(V ) and V under different interventions, how the change affects another variable Y in the causal graph is unknown and must be learnt from samples. The task is to transfer prior knowledge to identify the best intervention.\nBandit Setting: The K different soft interventions can be thought of as the K arms of a bandit problem. Let the reward of arm k be denoted by: µk = Ek [Y ], where Ek [Y ] is the expected value of Y under the soft intervention when the conditional distribution of V given its parents pa(V ) is set to Pk(V |pa(V )) (soft intervention k) while keeping all other things in G unchanged. We assume that there is only one best arm. Let k∗ be the arm that yields the highest expected reward and µ∗ be the value of the corresponding expected reward i.e. k∗ = arg maxk µk and µ\n∗ = µk∗ . Let the optimality gap of the kth arm be defined as ∆k = µ∗−µk. We shall see that the these gaps {∆k}K−1k=0 and the relationship between distributions {Pk(V |pa(V ))}K−1k=0 are important parameters in the problem. Let the minimum gap be ∆ = mink 6=k∗ ∆k. Fixed Budget for Samples: In this paper, we work under the fixed budget setting of best arm identification [3]. Let Tk be the number of times the k\nth intervention is used to obtain samples. We require that∑K k=0 Tk = T . Let νk = Tk T be the fraction of times the k th intervention is played. Additional Cost Budget on Interventions: In the context of causal discovery, some interventions require a lot more resources or experimental effort than the others. We find such examples in the context of online advertisement design [7]. Therefore, we introduce two variants of an additional cost constraint that influences the choice of interventions. (i) Difficult arm budget (S1): Some arms are deemed to be difficult. Let B ⊂ [K] be the set of difficult arms. We require that the total fraction of times the difficult arms are played does not exceed B i.e. ∑ k∈B νk ≤ B. (ii) Cost Budget (S2): This is the most general budget setting that captures the variable costs of sampling each arm [34]. We assume that there is a cost ck associated with sampling arm k. It is required that the average cost of sampling does not exceed a cost budget B .ie.∑K−1 k=0 ckνk ≤ B. c = [c1, .., ck] along with the total budget T completely defines this budget setting. It should be noted that S1 is a special case of S2.\nWe note that unless otherwise stated, we work with the most general setting in S2. We state some of our results in the setting S1 for clearer exposition.\nObjectives: There are two main quantities of interest:\n(Probability of Error): This is the probability of failing to identify the best soft intervention (arm). Let\nk̂(T,B) be the arm that is predicted to be the best arm at the end of the experiment. Then the probability of error e(T,B) [3, 8] is given by,\ne(T,B) = P ( k̂(T,B) 6= k∗ )\n(Simple Regret): Another important quantity that has been analyzed in the best arm identification setting\nis the simple regret [22]. The simple regret is given by r(T,B) = ∑ k 6=k∗ ∆kP ( k̂(T,B) = k ) ."
    }, {
      "heading" : "3 Our Main Results",
      "text" : "In this section we provide our main theoretical contributions. In Section 3.2, we provide a successive rejects style algorithm that leverages the information leakage between the arms via importance sampling. Then, we provide theoretical guarantees on the probability of mis-identification (e(T,B)) and simple regret (r(T,B)) for our algorithm in Section 3.3. In order to explain our algorithm and our results formally, we first describe several key ideas in our algorithm and introduce important definitions in Section 3.1."
    }, {
      "heading" : "3.1 Definitions",
      "text" : "Quantifying Information Leakage: Our setting is one in which there is information leakage among the K arms of the bandit. Recall that each arm corresponds to a different conditional distribution imposed on a node V given its parents pa(V ), while the rest of the relationships in the causal graph G remain unchanged. Since the different candidate conditional distributions Pk(V |pa(V )) are known from prior knowledge (and are absolutely continuous with respect to each other), it is possible to utilize samples obtained under an arm j to obtain an estimate for the expectation under some other arm i (i.e Ei [Y ]). A popular method for utilizing this information leakage among different distributions is through importance sampling, which has been used in counterfactual analysis in similar causal settings [22, 7]. Importance Sampling: Suppose we get samples from arm j ∈ [K] and we are interested in estimating Ei[Y ]. In this context it helpful to express Ei[Y ] in the following manner:\nEi [Y ] = Ej [ Y\nPi(V |pa(V )) Pj(V |pa(V ))\n] (1)\n(1) is trivially true because the only change to the joint distribution of all the variables in the causal graph G under arm i and j is at the factor P(V |pa(V )). Suppose we observe t samples of {Y, V, pa(V )} from the arm j, denoted by {Yj(s), Vj(s), pa(V )j(s)}ts=1. Here Xj(s) denotes the sample from random variable X at time step s, while the sub-script j just denotes that the samples are collected under arm j. Under the observation of Equation (1), one might assume that the naive estimator,\nŶ ′i (j) = 1\nt\nt∑\ns=1\nYj(s) Pi(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n(2)\nprovides a good estimate for µi = Ei [Y ]. However, the confidence guarantees on such an estimate can be arbitrarily bad even if Y is bounded. This is because the factor Pi(V |pa(V ))/Pj(V |pa(V )) can be very large for several instances of (V, pa(V )). Therefore, usual measure concentrations (e.g. the Azuma-Hoeffding inequality) would not yield good confidence intervals. This has been noted in [22] in a similar setting, where a static clipper has been applied to the weighted samples to control the variance. However, a static clipper introduces a fixed bias, and thus it is not suitable for obtaining gap dependent simple regret bounds. Instead, in our algorithm we a multi-phase approach and use dynamic clipping to adaptively control the bias vs. variance trade-off in a phase dependent manner, which leads to significantly better gap dependent bounds2. We now define some key quantities.\nDefinition 1. Let f(·) be a non-negative convex function such that f(1) = 0. For two joint distributions pX,Y (x, y) and qX,Y (x, y) (and the associated conditionals), the conditional f -divergence Df (pX|Y ‖qX|Y ) is given by:\nDf (pX|Y ‖qX|Y ) = EqX,Y [ f ( pX|Y (X|Y ) qX|Y (X|Y ) )] .\nRecall that Pi is the conditional distribution of node V given the state of its parents pa(V ). Thus, Df (Pi‖Pj) is the conditional f -divergence between the conditional distributions Pi and Pj . Now we define some log-divergences that are are crucial in the our analysis.\nDefinition 2. (Mij measure) Consider the function f1(x) = x exp(x − 1) − 1. We define the following log-divergence measure: Mij = 1 + log(1 +Df1(Pi‖Pj)), ∀i, j ∈ [K].\nThese log divergences help us in controlling the bias variance trade-off in importance sampling as shown in Section C.2 in the appendix. We also note that estimates of the f -divergence measure can be had directly from empirical data (without the knowledge of the full distributions) using techniques like that of [30].\n2We note that the authors in [22] discuss the possibility of a multi-phase approach, where clipper levels could change across phases. However, they do not pursue this direction (no specific algorithm or results) as their objective is to derive gap independent bounds (minimax regret).\nAggregating Interventional Data: We describe an efficient estimator of Ek[Y ] (∀k ∈ [K]) that combines available samples from different arms. This estimator adaptively weights samples depending on the relative Mij measures, and also uses clipping to control variance by introducing bias. The estimator is given by (3).\nSuppose we obtain τi samples from arm i ∈ [K]. Let the total number of samples from all arms be denoted by τ . Further, let us index all the samples by s ∈ {1, 2, .., τ}, and Tk ⊂ {1, 2, .., τ} be the indices of all the samples collected from arm k. Let Xj(s) denotes the sample collected for random variable X under intervention j, at time instant s. Finally, let Zk = ∑ j∈[K] τj/Mkj . We denote the estimate of µk by Ŷ k ( indicates the level of confidence desired). Our estimator is:\nŶ k = 1\nZk\nK∑\nj=0\n∑\ns∈Tj\n1\nMkj Yj(s) Pk(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n× 1 {\nPk(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n≤ 2 log(2/ )Mkj } . (3)\nIn other words, Ŷ k is the weighted average of the clipped samples, where the samples from arm j are weighted by 1/Mkj and clipped at 2 log(2/ )Mkj . The choice of controls the bias-variance tradeoff which we will adaptively change in our algorithm."
    }, {
      "heading" : "3.2 Algorithm",
      "text" : "Now, we describe our main algorithmic contribution - Algorithm 1 and 2. Algorithm 1 starts by having all the K arms under consideration and then proceeds in phases, possibly rejecting one or more arms at the end of each phase.\nAt every phase, Estimator (3) with a phase specific choice of the parameter (i.e. controlling bias variance trade-off), is applied to all arms under consideration. Using a phase specific threshold on these estimates, some arms are rejected at the end of each phase. A random arm among the ones surviving at the end of all phases is declared to be the optimal. We now describe the duration of various phases.\nRecall the parameters T - Total sample budget available and B - average cost budget constraint. Let n(T ) = dlog 2× log 10 √ T e. Let log(n) = ∑ni=1(1/i). We will have an algorithm with n(T ) phases numbered by ` = 1, 2., , n(T ). Let τ(`) be the total number of samples in phase `. We set τ(l) = T/(llog(n(T ))) for l ∈ {1, .., n(T )}. Note that ∑l τ(l) = T . Let R be the set of arms remaining to compete with the optimal arm at the beginning of phase ` which is continuously updated. Allocation of Budget: Let τk(`) be the samples allocated to arm k in phase `. Let τ(`) be the vector consisting of entries {τk (`)}. The vector of allocated samples, i.e. τ(`) is decided by Algorithm 3. Intuitively, an arm that provides sufficient information about all the remaining arms needs to be given more budget than other less informative arms. This allocation depends on the average budget constraints and the relative log divergences between the arms (Definition 2). Algorithm 3 formalizes this intuition, and ensures that variance of the worst estimator (of the form (3)) for the arms in R is as good as possible (quantified in Theorem 4 and Lemma 4 in the appendix).\nThe inverse of the maximal objective of the LP in Algorithm 3 acts as effective standard deviation uniformly for all the estimators for the remaining arms in R. It is analogous to the variance terms appearing in Bernstein-type concentration bounds (refer to Lemma 4 in the appendix).\nDefinition 3. The effective standard deviation for budget B and arm set R ⊆ [K] is defined as σ∗(B,R) = 1/v∗(B,R) from Algorithm 3 with input B and arm set R.\nAlgorithm 3 minimizes the variance terms in the confidence bounds for the estimates of the arms that are in contention i.e. Ŷ k for all k ∈ R. This minimization is performed subject to the constraints on the fractional budget of each arms (recall B from Section 2). Note that Algorithm 3 only needs to ensure good confidence bounds for the arms that are remaining (k ∈ R). This gets easier as the number of arms remaining (i.e. |R|) decreases. Therefore the effective variances σ∗(B,R) become progressively better with every phase.\nRemark 3. Note that Line 6 uses only the samples acquired in that phase. Clearly, a natural extension is to modify the algorithm to re-use all the samples acquired prior to that step. We give that variation in Algorithm 2. We prove all our guarantees for Algorithm 1. We conjecture that the second variation has tighter guarantees (dropping a multiplicative log factor) in the sample complexity requirements.\nAlgorithm 1 Successive Rejects with Importance Sampling -v1 (SRISv1) - Given total budget T and the cost budget B (along with ci’s) picks the best arm.\n1: SRIS(B, {Mkj}, T ) 2: R = [K]. 3: Form the matrix A ∈ RK×K such that Akj = 1Mkj . 4: for ` = 1 to n(T ) do 5: τ(`) = ALLOCATE (c, B,A,R, τ(`)) (Algorithm 3) 6: Use arm k, τk(`) times and collect samples (Y, V, pa(V )). 7: for k ∈ R do 8: Let Ŷk be the estimator for arm k as in (3) calculated with {Mkj}, = 2−(`−1) and the samples obtained in Line 6. 9: end for\n10: Let ŶH = arg maxk∈R Ŷk. 11: R = R− {k ∈ R : ŶH > Ŷk + 5/2l}. 12: if |R| = 1 then 13: return: the arm in R. 14: end if 15: end for 16: return: A randomly chosen arm from R.\nAlgorithm 2 Successive Rejects with Importance Sampling -v2 (SRISv2) - Given total budget T and the cost budget B (along with c) picks the best arm.\n1: Identical to Algorithm 1 except for Line 6 where all samples acquired in all the phases till that Line is used.\nAlgorithm 3 Allocate - Allocates a given budget τ among the arms to reduce variance.\n1: ALLOCATE(c, B,A,R, τ) 2: Solve the following LP:\n1\nσ∗(B,R) = v ∗(B,R) = max ν min k∈R [Aν ]k (4)\ns.t.\nK∑\ni=0\nciνi ≤ B and K∑\nj=0\nνj = 1, νi ≥ 0.\nwhere [Aν ]k denotes the k th element in the vector Aν .\n3: Assign τj = ν ∗ j (B,R)τ"
    }, {
      "heading" : "3.3 Theoretical Guarantees",
      "text" : "We state our main results as Theorem 1 and Theorem 2, which provide guarantees on probability of error and simple regret respectively. Our results can be interpreted as a natural generalization of the results in [3], when there is information leakage among the arms. This is the first gap dependent characterization.\nTheorem 1. (Proved formally as Theorem 5) Let ∆ = min k 6=k∗\n∆k. Let σ ∗(.) be the effective standard deviation\nas in Definition 3. The probability of error for Algorithm 1 satisfies:\ne(T,B) ≤ 2K2 log2(20/∆) exp ( − T\n2H̄log(n(T ))\n) (5)\nwhen the budget for the total number of samples is T and ∆ ≥ 10/ √ T . Here,\nH̄ = max k 6=k∗\nlog2(10/∆k) 3\n( σ∗(B,R∗(∆k))\n∆k\n)2 (6)\nand R∗(∆k) = {s : log2 ( 10 ∆s ) ≥ blog2 ( 10 ∆k ) c} is the set of arms whose distance from the optimal arm is roughly at most twice that of arm k.\nComparison with the result in [3]: Let R̃(∆k) = {s : ∆s ≤ ∆k}, i.e. the set of arms which are closer to the optimal than arm k. Let H̃ = max\nk 6=k∗ |R̃(∆k)| ∆2k . The result in [3] can be stated as: The error in finding\nthe optimal arm is bounded as: e(T ) ≤ O ( K2 exp ( − T−K\nlog(K)H̃\n)) .\nOur work is analogous to the above result (upto poly log factors) except that H̄ appears instead of H̃. In Section B.1 (in the appendix), we demonstrate through simple examples that σ∗(B,R∗(∆k)) can be significantly smaller than √ R̃(∆k) (the corresponding term in e(T ) above) even when there are no average budget constraints. Moreover, our results can be exponentially better in the presence of average budget constraints (examples in Section B.1). Now we present our bounds on simple regret in Theorem 2.\nTheorem 2. (Proved formally as Theorem 5) Let σ∗(.) be the effective standard deviation as in Definition 3. The simple regret of Algorithm 1 when the number of samples is T satisfies:\nr(T,B) ≤ 10√ T 1 { ∃k 6= k∗ s.t ∆k < 10/ √ T } + 2K2 ∑\nk 6=k∗: ∆k≥10/ √ T\n∆k log2\n( 20\n∆k\n) exp ( − T\n2H̄klog(n(T ))\n) (7)\nHere, H̄k = max{l:∆l≥∆k} log2(10/∆l)\n3\n(∆l/10)2v∗(B,R∗(∆l))2 and R ∗(∆k) = {s : log2 ( 10 ∆s ) ≥ blog2 ( 10 ∆k ) c}.\nComparison with the result in [22]: In [22], the simple regret scales as O(1/ √ T ) and does not adapt to the gaps. We provide gap dependent bounds that can be exponentially better than that of [22] (when ∆k’s are not too small and the first term in (7) is zero). Moreover our bounds generalize to gap independent bounds that match O(1/ √ T ). Further details are provided in Section B.2 (in the appendix).\nWe defer the theoretical analysis to Section C. Theorem 1 and Theorem 2 are subparts of our main technical theorem (Theorem 5), which is proved in Section C.5."
    }, {
      "heading" : "4 Empirical Validation",
      "text" : "We empirically validate the performance of our algorithms in two real data settings. In Section 4.1, we study the empirical performance of our algorithm on the flow cytometry data-set [32]. In Section 4.2, we apply our algorithms for the purpose of model interpretability of the Inception Deep Network [38] in the context of image classification. Section 4.3 is dedicated to synthetic experiments. In Section D (in the appendix) we provide more details about our experiments. In the appendix we empirically show that our divergence metric is fundamental and replacing it with other divergences is sub-optimal."
    }, {
      "heading" : "4.1 Flow Cytometry Data-Set",
      "text" : "The flow cytometry data-set [32] consists of multi-variate measurements of protein interactions in a single cell, under different experimental conditions (soft interventions). This data-set has been extensively used for validating causal inference algorithms. Our experiments are aimed at identifying the best intervention among many, given some ground truth about the causal graph. For, this purpose we borrow the causal graph from Fig. 5(c) in [26] (shown in Fig. 3a) and consider it to be the ground truth.\nParametric linear models have been popularly used for causal inference on this data-set [25, 11]. We fit a GLM gamma model [14] between the activation of each node and its parents in Fig. 3a using the observational data. In Section D.1 (in the appendix) we provide further details showing that the sampled distributions in the fitted model are extremely close to the empirical distributions from the data. The soft interventions signifying the arms are generated by changing the distribution of a source node pkc in the GLM. The objective is to identify the intervention that yields the highest output at the target node erk3 We provide empirical results for two sets of interventions at the source node. Both these experiments have been performed with 15 arms each representing different distributions at pkc.\nBudget Restriction: The experiments are performed in the budget setting S1, where all arms except arm 0 are deemed to be difficult. We plot our results as a function of the total samples T , while the fractional budget of the difficult arms (B) is set to 1/ √ T . Therefore, we have ∑ k 6=0 Tk ≤ √ T . This essentially belongs to the case when there is a lot of data that can be acquired for a default arm while any new change requires significant cost in acquiring samples.\nCompeting Algorithms: We test our algorithms on different problem parameters and compare with related prior work [3, 22]. The algorithms compared are (i) SRISv1: Algorithm 1 introduced in Section 3.2. The divergences, Df1(Pi||Pj) are estimated from sampled data using techniques from [30]; (ii)SRISv2: Algorithm 2 as detailed in Section 3.2; (iii) SR: Successive Rejects Algorithm from [3] adapted to the budget setting. The division of the total budget T into K − 1 phases is identical, while the individual arm budgets are decided in each phase according to the budget restrictions; (iv) CR: Algorithm 2 from [22]. The optimization problem for calculating the mixture parameter η is not efficiently solvable for general distributions and budget settings. Therefore, the mixture proportions are set by Algorithm 3.\nIn these experiments, the budget restrictions imply that arm 0 can be pulled much more than the other arms. Intuitively the divergences of the arms from arm 0 as well as the gap ∆ defines the hardness of identification. Fig. 3b represents a difficult scenario where the divergences Mk0 > 400 for many arms (large divergences imply low information leakage) and ∆ = 0.01 (small ∆ increases hardness). In Fig. 3c (easier\n3The activations of the node erk have been scaled so that the mean is less than one. Note that the marginal distribution still has an exponential tail, and thus does not strictly adhere to our boundedness assumption on the target variable. However, the experiments suggest that our algorithms still perform extremely well.\nscenario) the divergences Mk0 < 20 for most arms while the gap is same as before. We see that SRISv2 outperforms all the other algorithms by a large margin, especially in the low sample regime."
    }, {
      "heading" : "4.2 Interpretability of Inception Deep Network",
      "text" : "In this section we use our algorithm for model interpretation of the pre-trained Inception-v3 network [38] for classifying images. Model Interpretation essentially addresses: ’why does a learning model classify in a certain way?’, which is an important question for complicated models like deep nets [31].\nWhen an RGB image is fed to Inception, it produces an ordered sequence of 1000 labels (e.g ’drums’, ’sunglasses’) and generally the top-10 labels are an accurate description of the objects in the image. To address interpretability, we segment the image into a number of superpixels/segments (using segmentation algorithms like SLIC [2]) and infer which superpixels encourage the neural net to output a certain label (henceforth referred to as label-I; e.g ’drum’) in top-k (e.g. k = 10), and to what extent.\nGiven a mixture distribution over the superpixels of an image (Figure 4a), a few superpixels are randomly sampled from the distribution with replacement. Then a new image is generated where all other superpixels of the original image are blurred out except the ones selected. This image is then fed to Inception, and it is observed whether label-I appears within the top-k labels. A mixture distribution is said to be a good interpretation for label-I if there is a high probability that label-I appears for an image generated by this mixture distribution. To empirically test the goodness of a mixture distribution, we would generate (using this mixture distribution) a number of random images, and determine the fraction of images for which label-I appears; a large fraction indicates that the mixture distribution is a good interpretation of label-I.\nMotivated by the above discussion, we generate a large number (3200) of mixture distributions, with the goal of finding the one that best interprets label-I. To highlight the applicability of our algorithm, we allow images to be generated for only 200 of these mixture distributions; in other words, most of the mixture distributions cannot be directly tested. Nevertheless, we determine the best from among the entire collection\nof mixture distributions (learning counterfactuals). Specifically in our experiments, we consider the image in Figure 4a, partition it into 43 superpixels, and generate images from mixture distributions by sampling 5 superpixels (with replacement). We generate 3000 arm distributions which lie in the 43-dimensional simplex but have sparse support (sparsity of 10 in our examples). The support of these distributions are randomly generated by techniques like markov random walk (encourages contiguity), random choice, etc. as detailed in Section D.2 in the appendix. However, we are only allowed to sample using a different set of 200 arms that are dense distributions chosen uniformly at random from the 43-dimensional simplex. The distributions are generated in a manner which is completely agnostic to the image content. The total sample budget (T ) is 2500.\nFigure 4 shows images in which the segments are weighted in proportion to the optimal distribution (obtained by SRISv2) for the interpretation of three different labels. This showcases the true counterfactual power of the algorithm, as the set of arms that can be optimized over are disjoint from the arms that can be sampled from. Moreover the sample budget is less than the number of arms. This is an extreme special case of budget setting S2. We see that our algorithm can generate meaningful interpretations for all the labels with relatively less number of runs of Inception. Even sampling 10 times from each of the arms to be optimized over would require 30, 000 runs of Inception for a single image and label, while we use only 2500 runs by leveraging information leakage."
    }, {
      "heading" : "4.3 Synthetic Experiments",
      "text" : "In this section, we empirically validate the performance our algorithm through synthetic experiments. We carefully design our simulation setting which is simple, but at the same time sufficient to capture the various tradeoffs involved in the problem. An important point to note is that our algorithm is not aware of the actual effect of the changes on the target (gaps between expectations) but it only knows the divergence among the candidate soft interventions. Sometimes, a change with large divergence from an existing one may not maximize the effect we are looking for. Conversely, smaller divergence may sometimes lead you closer to the optimal. We demonstrate that our algorithm performs well in all the experiments, as compared to previous works [3, 22]. Experimental Setup: We set up our experiments according to the simple causal graph in Figure 5. V is assumed to be a random variable taking values in {0, 1, 2, · · · ,m−1}. The various arms P0(V ),P1(V ), ...PK−1(V ) are discrete distributions with support [m]. We will vary m and K over the course of our experiments.\nY is assumed to be a function of V and some random noise which is external to the system. In our experiments, we set the function as follows:\nY =\n{ f(V ) if = 0\n1− f(V ) if = 1\nwhere f : [m]→ {0, 1} is an arbitrary function. We set P( = 1) = 0.01 in all our experiments. The discrete candidate distributions are modified to explore various tradeoffs between the gaps and the effective standard deviation parameters. Budget Restriction: The experiments are performed in the budget setting S1, where all arms except arm 0 are deemed to be difficult. We plot our results as a function of the total samples T , while the fractional budget of the difficult arms (B) is set to 1/ √ T . Therefore, we have ∑ k 6=0 Tk ≤ √ T . This essentially belongs to the case when there is a lot of data that can be acquired for a default arm while any new change requires significant cost in acquiring samples. Experiments: In our experiments, we choose f to be the parity function, when V ∈ [m], is represented in base 2. Note that arm 0 is the arm that can be sampled O(T ) times while the rest of the arms can only be sampled O( √ T ) times due to the above budget constraints. So, the divergence of the arm 0 from other arms is crucial alongside the gaps. We perform our experiments in different regimes that get progressively easier. In these experiments, we function in various regimes of the divergences between the other arms and arm 0, and the gaps from the optimal arm in terms of target value. When there is no information leakage, the samples are divided among the K arms. So, the loss in having multiple arms can be expressed as a scaling √ K in standard deviation. Recall the log divergence measure Mk0 which is a measure of information leakage from arm 0 to another arm k. Therefore, in the following, when we say high divergence from arm 0, it means that Mk0/ √ K is high for most arms k 6= 0.\nHigh Divergence and Low ∆: This is the hardest of all settings. Here, we set m = 20 and K = 30. Here, we have Mk0 to be pretty high for all the arms k 6= 0. This means that the arm 0, which can be pulled O(T ) times provides highly noisy estimates for other arms. We have Mk0/ √ K ∼ 30 for most arms. Moreover, the minimum gap from the best arm ∆ = 0.04, which is pretty small. This implies that it is harder to distinguish the best arm.\nThe results are demonstrated in Figure 6. Figure 6a displays the simple regret. We see that both SRISv1 and SRISv2 outperform the others by a large extent, in this hard setting, even when the number of samples are very low. In Figure 6b we plot the probability of error in exactly identifying the best arm. We see that none of the algorithms successfully identify the best arm, in the small sample regime, as the gap ∆ is very low. However, our algorithms quickly zero in on arms that are almost as good as the optimal, and therefore the simple regret is well-behaved. Our algorithm performs this well even when the divergences are big, because it is able to reject the arms that have high ∆i in the early phases, very effectively.\nHigh Divergence and High ∆: This is easier than the previous setting. Here, we set m = 10 and K = 20. Here, we have Mk0 to be very high for all the arms k 6= 0. Thus arm 0 provides very noisy estimates on other arms. We have Mk0/ √ K 50 for many arms. However, the minimum gap from the best arm ∆ = 0.15, which is not too small. This implies that it might be easier to distinguish the best arm.\nThe results are demonstrated in Figure 7. Figure 7a displays the simple regret. We see that in the small sample regime SRISv1 and SRISv2 outperform the others by a large extent. In the high sample regime,\nSRISv2 is still the best, while SR and SRISv1 are close behind. In Figure 7b we plot the probability of error in exactly identifying the best arm. We see that SRISv2 performance very well in identifying the best arm even though arm 0 gives highly noise estimates. It is interesting to note that CR does not perform well. This can be attributed to the non-adaptive clipper in CR, that incurs a significant bias because arm 0 has high-divergences from most of the other arms.\nLow Divergence and Low ∆: This is another moderately hard setting, similar to the previous one. Here, we set m = 20 and K = 30. Here, we have Mk0 to be not too high for the arms k 6= 0. This means that the arm 0, which can be pulled O(T ) times is moderately good for estimating the other arms. Here, Mk0/ √ K ≤ 10 for most arms k. However, the minimum gap from the best arm ∆ = 0.04, which is small. This implies that it might be hard to distinguish the best arm. The results are demonstrated in Figure 8. Figure 8a displays the simple regret. We see that in the small sample regime SRISv1 and SRISv2 outperforms the others by a large extent. In the high sample regime, SRISv2 is still the best, while CR is close behind. In Figure 8a we plot the probability of error in exactly identifying the best arm. We see that most of the algorithms have moderately bad probability of error as the gap ∆ is small. However, the algorithms SRISv2 and SRISv1 are quickly able to zero down on arms close to optimal as shown in the simple regret in the small sample regime.\nLow Divergence and High ∆: This is the easiest of all settings. Here, we set m = 10 and K = 20. Here, arms 0 has P0(V ) pretty close to the uniform distribution on [m]. Therefore, it is very well-posed\nfor estimating the means of all other arms. In fact we have Mk0/ √ K < 2 for many arms. Moreover, the minimum gap from the best arm ∆ = 0.15, which is not too small. This implies that it might be very easy to distinguish the best arm.\nThe results are demonstrated in Figure 9. Figure 9a displays the simple regret. We see that SRISv2 and CR perform extremely well closely followed by SRISv1. In Figure 9b we plot the probability of error in exactly identifying the best arm. Again SRISv2 and CR have almost zero probability of error and SRISv1 is close behind. This is because ∆ is pretty large. In this example, we observe that all the algorithms that use information leakage are better than SR, because arm 0 is well-behaved. CR performs almost as well as SRISv2 in this example, as the static clipper is never invoked because almost always the ratios in the importance sampler are well bounded.\nIn conclusion, it should be noted that our algorithms perform well in all the different settings, because they are able to adapt to the problem parameters (similar to [3]) and at the same time leverage the information leakage (similar to [22])."
    }, {
      "heading" : "5 Discussion and Future Work",
      "text" : "In this paper, we analyze the problem of identifying the best arm at a node V in a causal graph (various known conditionals Pk(V |pa(V ))) in terms of its effect on a target variable Y further downstream, possibly in a less understood portion of the larger causal network. We characterize the hardness of this problem in terms of the relative divergences of the various conditionals that are being tested and the gaps between the expected value of the target under the various arms. We provide the first problem dependent simple regret and error bounds for this problem, that is a natural generalization of [3], but with information leakage between arms. We provide an efficient successive rejects style algorithm that achieves these guarantees, by leveraging the leakage of information, through carefully designed clipped importance samplers. Further, we introduce a new f -divergence measure that may be relevant for analyzing importance sampling estimators in the causal context. This may be of independent interest. We believe that our work paves the way for various interesting problems with significant practical implications. In the following, we state a few open questions in this regard:\nTighter guarantees on SRISv2: In Section 4, we have observed that a slightly modified version of our algorithm SRISv2 performs the best among all the competing algorithms including SRISv1. The only difference of SRISv2 from Algorithm 1, is that in line 6 the estimators used in a phase also uses samples from past phases, but clipped according to the criterion in the current phase. We believe that this algorithm has tighter error and simple regret guarantees. We conjecture that at least one of the log(1/∆k), in the definition of H̄ in (6) can be eliminated, thus leading to better guarantees.\nEstimating the marginals of the parents: In Algorithm 1, either the marginals of the parents of V , that is P(pa(V )) is required in order to calculate the f -divergences in Definition 2, or prior data involving the parents is required to estimate the f -divergences directly from data. However, we believe it is possible to model this estimation, directly into the online framework, as data about the marginals of the parents are available through the samples in all the arms, as these marginals remain unchanged.\nProblem Dependent Lower Bound: In [22], a problem independent lower bound of O(1/ √ T ) has been provided for a special causal graph. However, the problem parameter dependent lower bound like that of [3] still remains an open problem. We believe that the lower bound will depend on the divergences between the distributions and the gaps between the rewards of the arms, similar to the term in (6).\nGeneral Learning Framework: Our work paves the way for a more general setting for learning counterfactual effects. Importance sampling is a fairly general tool and can be ideally applied at any set of nodes of a causal graph. So, in principle it is possible to study the effect of a change at V on a target Y , by using importance sampling between the changed marginal distributions at an intermediate cut S that blocks every path from V to Y . In fact, this is explored in a non-bandit context in [7]. An important question is: What is the most suitable cut to be used? [22] uses the cut closest to Y , i.e. immediate parents of Y . However, the marginals of the cut under different changes need to be estimated this ’far’ from the source closer to the target. Therefore, there is a tradeoff that involves a delicate balance between the estimation errors of the changes at an intermediate cut between V and Y , and the reduction in importance sampling divergences between cut distributions closer to the target Y . We believe understanding this is quite important to fully exploit partial/full knowledge about causal graph structure to answer causal strength questions from data observed."
    }, {
      "heading" : "B.1 Comparison with [3]",
      "text" : "Let R̃(∆k) = {s : ∆s ≤ ∆k}, i.e. the set of arms which are closer to the optimal than arm k. Let H̃ = max\nk 6=k∗ |R̃(∆k)| ∆2k . The result for the best arm identification with no information leakage in [3] can be stated\nas: The error in finding the optimal arm is bounded as:\ne(T ) ≤ O ( K2 exp ( − T −K\nlog(K)H̃\n)) (8)\nOne intuitive interpretation for H̃ is that it is the maximum among the number of samples (neglecting some log factors) required to conclude that arm k is suboptimal from among the arms which are closer to the optimal than itself. Intuitively, this is because when there is no information leakage, one requires 1/∆2k samples to distinguish between the k-th optimal arm and the optimal arm. Further, the kth arm is played only 1/k fraction of the times since we do not know the identity of the k-th optimal arm.\nOur main result in Theorem 1 can be seen to be a generalization of the existing result for the case when there is information leakage between the arms (various changes in a causal graph).\nThe term σ∗(B,R∗(∆k)) in our setting is the ‘effective standard deviation’ due to information leakage. There is a similar interpretation of our result (ignoring the log factors): Since there is information leakage, the expression (σ ∗)2\n(∆k)2 characterizes the number of samples required to weed out arm k out of contention from\namong competing arms (arms that are at a distance at most twice than that of arm k from the optimal arm). The interpretation of ’effective variance’ is justified using importance sampling which is detailed in Section C.3. Further, in our framework σ∗ also incorporates any budget constraint that comes with the problem, i.e. any apriori constraint on the relative fraction of times different arms need to be pulled.\nFor ease of exposition let (k) denote the index of the k-th best arm (for k = 1, ..,K) and ∆(k) denotes\nthe corresponding gap. In this setting, the terms H̃ (from the result in [3]) and H̄ can be written as:\nH̃ = max k 6=1\nk\n∆2(k)\nH̄ = max k 6=1 σ∗(B,R∗(∆(k)))2 ∆2(k) .\nσ∗(B,R∗(∆(k))) can be smaller than √ k due to information leakage as every single arm pull contributes to another arm’s estimate. Therefore, these provide better guarantees than [3]. To see the improvement over the previous result in [3], we consider a special case when the cost budget B is infinity and there is only the the sample budget T . In addition, let us assume that the log divergences are such that: Mij ≤ ηMii = η << √ |R|, ∀i 6= j. Let R = R∗(∆(k)). If η > √ |R|, the optimal solution for (4) is a bit complicated to interpret. Consider the feasible allocation νi = 1 |R| , ∀i ∈ R in (4). Evaluating\nthe objective function for this feasible allocation, it is possible to show that σ∗ ≤ η1−1/|R| << √ |R|. Hence, unless the variance due to information leakage is too bad, the effective variance is smaller than that of the case with no information leakage.\nThe improvement over the no information leakage setting, is even more pronounced under budget constraints. Consider the setting S1, and assume that the fractional budget of the difficult arms, B = o(1). This implies that the total number of samples available for difficult arms is o(T ). The budget constrained case has not been analyzed in [3], however in the absence of information leakage, one would expect that the arms with the least number of samples would be the most difficult to eliminate, and therefore the error guarantees would scale as exp(−O(BT )/H̃) ∼ exp(−o(T )/H̃) (excluding log factors). On the other hand, our algorithm can leverage the information leakage and the error guarantee would scale as exp(−O(T )/H̄), which can be order-wise better if the effective standard deviations are well-behaved."
    }, {
      "heading" : "B.2 Comparison with [22]",
      "text" : "In [22], the algorithm is based on clipped importance samples, where the clipper is always set at a static level of O( √ T ) (excluding log factors). The simple regret guarantee in [22] scales as O( √ (m(η)/T ) log T ), where\nm(η) is a global hardness parameter. The guarantees do not adapt to the problem parameters, specifically the gaps {∆k}k∈[K].\nOn the contrary, we provide problem dependent bounds, which differentiates the arms according to its gap from the optimal arm and its effective standard deviation parameter. The terms H̄k can be interpreted as the hardness parameter for rejecting arm k. Note that H̄k depends only on the arms that are at least as bad in terms of their gap from the optimal arm. Moreover the guarantees are adapted to our general budget constraints, which is absent in [22]. It can be seen that when ∆k’s do not scale in T , then our simple regret is exponentially small in T (dependent on H̄k’s) and can be much less than O(1/ √ T ). The guarantee also\ngeneralizes to the problem independent setting when ∆k’s scale as O(1/ √ T )."
    }, {
      "heading" : "C Proofs",
      "text" : "In this section we present the theoretical analysis of our algorithm. Before we proceed to the proof of our main theorems, we derive some key lemmas that are useful in analyzing clipped importance sampled estimators."
    }, {
      "heading" : "C.1 Clipped Importance Sampling Estimator",
      "text" : "In Section 3.1 we have introduced the concept of importance sampling that helps us in using samples collected under one arm to estimate the means of other arms. As noted in Section 3.1, a naive unbiased importance sampled estimator can potentially have unbounded variances thus leading to poor guarantees. We now introduce clipped importance samplers and provide a novel analysis of these estimators that alleviates the variance related issues.\nClipped Importance Samplers: The naive estimator of (2) is not suitable for yielding good confidence intervals. It has been observed in the context of importance sampling, that clipping the estimator in (2) at a carefully chosen value, can yield better confidence guarantees even though the resulting estimator will become slightly biased [7]. Before we introduce the precise estimator, let us define a key quantity that will be useful for the analysis.\nDefinition 4. We define ηi,j( ) as follows:\nηi,j( ) =\n{ argmin\nη : Pi ( Pi(V |pa(V )) Pj(V |pa(V )) > η ) ≤ 2 } (9)\nfor all i, j ∈ [K], where > 0.\nWe shall see that the ηi,j( ) is related to the conditional f -divergence between Pi(V |pa(V )) and Pj(V |pa(V )) for the carefully chosen function f1(.) as introduced in Section 3.1.\nNow we are at a position to provide confidence guarantees on the following clipped estimator:\nŶ (η) i (j) =\n1\nt\nt∑\ns=1\nYj(s) Pi(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n× 1 {\nPi(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n≤ ηij( ) } . (10)\nLemma 1. The estimate Ŷ (η) i (j) for η = ηi,j( ) satisfies the following:\n1.\nEj [ Ŷ (η) i (j) ] ≤ µi ≤ Ej [ Ŷ (η) i (j) ] +\n2 (11)\n2.\nP ( µi − δ − /2 ≤ Ŷ (η)i (j) ≤ µi + δ ) ≥ 1− 2 exp ( − δ 2t\n2ηi,j( )2\n) . (12)\nProof. We have the following chain:\nEj [ Y\nPi(V |pa(V ) Pj(V |pa(V ))\n]\n= Ej [ Y\nPi(V |pa(V )) Pj(V |pa(V )) 1 { Pi(V |pa(V )) Pj(V |pa(V )) ≤ ηi,j( ) }] + Ej [ Y Pi(V |pa(V )) Pj(V |pa(V )) 1 { Pi(V |pa(V )) Pj(V |pa(V )) > ηi,j( ) }]\n(a) ≤ Ej [ Y\nPi(V |pa(V )) Pj(V |pa(V )) 1 { Pi(V |pa(V )) Pj(V |pa(V )) ≤ ηi,j( ) }] + Pi ( Pi(V |pa(V )) Pj(V |pa(V )) > ηi,j( ) )\n≤ Ej [ Y\nPi(V |pa(V )) Pj(V |pa(V )) 1 { Pi(V |pa(V )) Pj(V |pa(V )) ≤ ηi,j( ) }] + 2\nHere, (a) is because Y ∈ [0, 1]. This yields the first part of the lemma:\nEj [ Ŷ (η) i (j) ] ≤ µi ≤ Ej [ Ŷ (η) i (j) ] +\n2 (13)\nwhere η = ηi,j( ). Note that all the terms in the summation of (10) are bounded by ηi,j( ). Therefore, by an application of Azuma-Hoeffding we obtain:\nP ( |Ŷ (η)i (j)− Ej [ Ŷ (η) i (j) ] | > δ ) ≤ 2 exp ( − δ 2t\n2ηi,j( )2\n) (14)\nCombining Equation (13) and (14), we obtain the first part of our lemma.\nC.2 Relating ηij(·) with f-divergence Now we are left with relating ηi,j( ) to a particular f -divergence (Df1 defined in Section 3.1) between Pi(V |pa(V )) and Pj(V |pa(V )). We have the following relation,\nEi [ exp ( Pi(V |pa(V )) Pj(V |pa(V )) )] = [1 +Df1 (Pi‖Pj)] e. (15)\nThe following lemma expresses the quantity ηi,j( ) as a separable function of Df1 (Pi‖Pj) and , and is one of the key tools used in subsequent analysis. Lemma 2. It holds that, ηi,j( ) ≤ log ( 2 ) + 1 + log (1 +Df1 (Pi‖Pj)). Furthermore,\nηi,j( ) ≤ 2 log ( 2 ) [1 + log(1 +Df1(Pi‖Pj))] (16)\nwhen, ≤ 1.\nProof. We have the following chain:\nPi (\nPi(V |pa(V )) Pj(V |pa(V )) > η\n) (17)\n= Pi ( exp ( Pi(V |pa(V )) Pj(V |pa(V )) ) > exp(η) )\n(a) ≤ Ei [ exp ( Pi(V |pa(V )) Pj(V |pa(V )) )] exp(−η)\n(18)\n(a) - We used Markov’s inequality. Suppose, we have the right hand side to be at most /2. Then we have,\nEi [ exp ( Pi(V |pa(V )) Pj(V |pa(V )) )] exp(−η) ≤ /2 (19)\nNow using (15), we have:\nη ≥ log ( 2 ) + 1 + log(1 +Df1(Pi‖Pj)) (20)\n=⇒ Pi (\nPi(V |pa(V )) Pj(V |pa(V )) > η\n) ≤ /2\nFrom, the definition of ηi,j( ), we have:\nηi,j( ) ≤ log ( 2 ) + 1 + log(1 +Df1(Pi‖Pj))\na ≤ 2 log\n( 2 ) [1 + log(1 +Df1(Pi‖Pj))] , ∀ ≤ 1 (21)\n(a) - This is due to the inequality p+ q ≤ 2pq when q ≥ 1 and p ≥ loge(2).\nNow, we introduce the main result of this section as Theorem 3. Recall thatMij = 1+log(1+Df1(Pi‖Pj)).\nTheorem 3. The estimate Ŷ (η) i (j) for η = 2 log(2/ )Mij satisfies the following confidence guarantees:\nP ( µi − δ − /2 ≤ Ŷ (η)i (j) ≤ µi + δ ) ≥ 1− 2 exp ( − δ 2t\n8 log(2/ )2M2ij\n) .\nProof. The proof is immediate from Lemmas 2 and 1."
    }, {
      "heading" : "C.3 Aggregating Heterogenous Clipped Estimators",
      "text" : "In Section C.1, we have seen how samples from one of the candidate distribution can be used for estimating the target mean under another arm. Therefore, it is possible to obtain information about the target mean under the kth arm (Ek[Y ]) from the samples of all the other arms. It is imperative to design an efficient estimator of Ek[Y ] (∀k ∈ [K]) that seamlessly uses the samples from all arms, possibly with variable weights depending on the relative divergences between the distributions. In this section we will come up with one such estimator, based on the insight gained in Section C.1.\nRecall the quantities Mkj = 1 + log(1 +Df1(Pk‖Pj)) (∀k, j ∈ [K]). These quantities will be the key tools in designing the estimators in this section. Suppose we obtain τi samples from arm i ∈ [K]. Let the total number of samples from all arms put together be τ .\nLet us index all the samples by s ∈ {1, 2, .., τ}. Let Tk ⊂ {1, 2, .., τ} be the indices of all the samples collected from arm k. Further, let Zk = ∑ j∈[K] τj/Mkj . Now, we are at the position to introduce the estimator for µk, which we will denoted by Ŷ k ( is an indicator of the level of confidence desired):\nŶ k = 1\nZk\nK∑\nj=0\n∑\ns∈Tj\n1\nMkj Yj(s) Pk(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n× 1 {\nPk(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n≤ 2 log(2/ )Mkj } . (22)\nIn other words, Ŷ k is the weighted average of the clipped samples, where the samples from arm j are weighted by 1/Mkj and clipped at 2 log(2/ )Mkj .\nLemma 3.\nµ̂k := E [ Ŷ k ] ≤ µk ≤ E [ Ŷ k ] +\n2 (23)\nProof. We note that Ŷ k can be written as:\nŶ k = 1\nZk\nK∑\nj=0\nτj Mkj Ỹ kj (24)\nHere, Ỹ kj = 1 τj ∑ s∈Tj Yj(s) Pk(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s)) ×1 { Pk(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s)) ≤ 2 log(2/ )Mkj } . Using Lemma 2 it is easy to observe that E[Ỹ k ] ≤ µk ≤ E[Ỹ k ]+ 2 as ηkj( ) ≤ 2 log(2/ )Mkj . Now, (24) together with this implies the lemma as Zk = ∑ j∈[K] τj/Mkj .\nTheorem 4. The estimator Ŷ k of (22) satisfies the following concentration guarantee:\nP ( µk − δ − /2 ≤ Ŷ k ≤ µk + δ ) ≥ 1− 2 exp ( − δ 2τ\n8(log(2/ ))2 ( Zk τ\n)2)\nProof. For the sake of analysis, let us consider the rescaled version Ȳ k = (Zk/τ)Ŷ k which can be written as:\nȲ k = 1\nτ\nK∑\nj=0\n∑\ns∈Tj\n1\nMkj Yj(s) Pk(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n× 1 {\nPk(Vj(s)|pa(V )j(s)) Pj(Vj(s)|pa(V )j(s))\n≤ 2 log(2/ )Mkj } . (25)\nSince Yj(s) ≤ 1, we have every random variable in the sum in (25) bounded by 2 log(2/ ) Let, µ̄k = E[Ȳ k ]. Therefore by Chernoff’s bound, we have the following chain:\nP ( |Ȳk − µ̄k| ≤ δ ) ≤ 2 exp ( − δ 2τ\n8(log(2/ ))2\n)\n=⇒ P ( |Ȳk τ\nZk − µ̄k\nτ Zk | ≤ δ τ Zk\n) ≤ 2 exp ( − δ 2τ\n8(log(2/ ))2\n)\n=⇒ P ( |Ŷk − µ̂k| ≤ δ τ\nZk\n) ≤ 2 exp ( − δ 2τ\n8(log(2/ ))2\n)\n=⇒ P ( |Ŷk − µ̂k| ≤ δ ) ≤ 2 exp ( − δ 2τ\n8(log(2/ ))2 ( Zk τ )2) (26)\nNow we can combine Equations (26) and (23) we get:\nP ( µk − δ − /2 ≤ Ŷ k ≤ µk + δ ) ≥ 1− 2 exp ( − δ 2τ\n8(log(2/ ))2 ( Zk τ\n)2)\nIn Theorem 4, we observe that the first part of the exponent scales as O( 2τ/(log(2/ ))2) if we set δ = O( ), which is very close to the usual Chernoff’s bound with τ i.i.d samples. The performance of this estimator therefore depends on the factor (Zk/τ) which depends on the fixed quantities Mkj (∀j) and the allocation of the samples τj . In the next section, we will come up with a strategy to allocate the samples so that the estimators Ŷ k have good guarantees for all the arms k."
    }, {
      "heading" : "C.4 Allocation of Samples",
      "text" : "In Section C.3, Theorem 4 tells us that the confidence guarantees on the estimator depends on how the samples are allocated between the arms. To be more precise, the term (Zk/τ) in Equation (26), affects the performance of the estimator for µk (Ŷ k ). We would like to maximize (Zk/τ) for all arms k ∈ [K].\nLet the total budget be τ . Let R be the set of arms that remain in contention for the best optimal arm. Consider the matrix A ∈ RK×K such that Akj = 1/Mkj for all k, j ∈ [K]. Then, we decide the fraction of times arm k gets pulled, i.e. νk to maximize Zk using the Algorithm 3.\nLemma 4. Allocation τ in Algorithm 3 ensures that (Zk/τ) ≥ 1σ∗(B,R) for all k ∈ R.\nThis is essentially the best allocation of the individual arm budgets in terms of ensuring good error bounds on the estimators Ŷ k for all k ∈ R. Since S1 is a special case of S2, to obtain the allocation for S1 one needs to set the cost values ci set to 1 for i ∈ B (difficult arms) in the above formulation and 0 otherwise."
    }, {
      "heading" : "C.5 Putting it together: Online Analysis",
      "text" : "We analyze Algorithm 1 phase by phase. With some abuse of notation, we redefine various quantities to be used in the analysis of the algorithm. Each quantity depends on the phase indices, as follows:\n• R(l): Set of arms remaining after phase l − 1 ends.\n• Ŷk(l): The value of the estimator (in Algorithm 1) for arm k at the end of phase l.\n• ŶH(l): The value of the highest estimate maxk Ŷk(l) (in Algorithm 1).\n• A(l) ⊆ R(l): Set of arms given by:\nA(l) := { k ∈ R(l) : ∆k > 10\n2l\n} . (27)\n• Sl: Success event of phase l defined as:\nSl := ∩k∈R(l),k 6=k∗ { Ŷk(l) ≤ µk + 1\n2l−1\n} ∩ { µk − 3\n2l ≤ Ŷk∗(l)\n} . (28)\nNow we will establish that the occurrence of the event Sl implies that all arms in A(l) gets eliminated at the end of phase l, while at the same time the optimal arm survives. Consider an arm k ∈ A(l). Given Sl has happened we have:\nŶH(l) ≥ Ŷk∗(l) ≥ µk∗ − 3\n2l\nŶk(l) ≤ µk + 1\n2l−1\nThis further implies that ŶH(l) − Ŷk(l) ≥ ∆k − 5/2l > 5/2l. Therefore all the arms in A(l) are eliminated given Sl. Following similar logic it is also possible to show that the optimal arm survives. If ŶH(l) = Yk∗(l) then it survives certainly. Now, given Sl, only arms in R(l) \\ A(l) can be the ones with the highest means. Consider arms k ∈ R(l) \\ A(l). Again given Sl we have:\nŶk∗(l) ≥ µk∗ − 3\n2l\nŶk(l) ≤ µk + 1\n2l−1\nTherefore, we have Yk(l)− Yk∗(l) ≤ 5/2l −∆k < 5/2l. Therefore, the optimal arm survives. It would seem that now it would be easy to analyze the probability of the event Sl, using Theorem 4. However, the bound in Theorem 4 depends on the sequence of arms eliminated so far in each phase. Therefore it is imperative to analyze S1:l, that is the event that all phases from 1, 2, .., l succeed. Let Bl = P(Scl |S1:l−1). So, we have the chain:\nP(S1:l) ≥ P(S1:l|S1:l−1)P(S1:l−1) ≥ P(S1:l|S1:l−1)P(S1:l−1|S1:l−2)P(S1:l−2)\n≥ l−2∏\ni=0\nP(S1:l−i|S1:l−i−1)P (S1)\n=\nl∏\ns=1\n(1−Bs)\n≥ 1− l∑\ns=1\nBs\nThe advantage of analyzing the probability of S1:l is that given S1:l we know the exact sequences of the arms that have been eliminated till Phase l. This gives us exact control on the exponents in the bound of Theorem 4. Given S1:s−1 we have,\nR(s) ⊆ R∗(s) := { k : ∆k ≤ 10\n2s−1\n} .\nRecall that the budget for the samples of each arm in any phase s, is decide by solving the LP in Algorithm 3. Therefore, given S1:s−1, we have σ∗(B,R(s)) ≥ σ∗(B,R∗(s)). Therefore, we have the following key lemma.\nLemma 5. We have:\nBl := P (Scl |S1:l−1) ≤ 2|R∗(l)| exp ( −2 −2(l−1)τ(l)v∗(B,R∗(l))2\n8l2\n) (29)\nProof. Note that in this phase we set ηkj = 2lMkj . Setting = 2 −(l−1) and δ = 2−(l−1) in Theorem 4 and by Lemma 4 we have:\nP ( µk − 3\n2l ≤ Ŷk(l) ≤ µk +\n1\n2l−1\n) ≥ 1− 2 exp ( −2 −2(l−1)τ(l)v∗(B,R∗(l))2\n8l2\n) (30)\nNote that the samples considered in phase l are independent of the event S1:l−1. Doing a union bound of the event complementary to the success event in (28), for all the remaining arms in R∗(`) implies the result in the Lemma.\nNow we are at a position to introduce our main results as Theorem 5.\nTheorem 5. Consider a problem instance with K candidate arms {Pk(V |pa(V )}K−1k=0 . Let the gaps from the optimal arm be ∆k for k ∈ [K]. Let us define the following important quantities:\nR∗(∆k) = { s : ⌊ log2 ( 10\n∆s\n)⌋ ≥ ⌊\nlog2\n( 10\n∆k\n)⌋} (31)\nH̄k = max{l:∆l≥∆k} log2(10/∆l)\n3\n(∆l/10)2v∗(B,R∗(∆l))2 (32)\nH̄ = max k 6=k∗\nlog2(10/∆k) 3\n(∆k/10)2v∗(B,R∗(∆k))2 (33)\nAlgorithm 1 satisfies the following guarantees: 1. The simple regret is bounded as:\nr(T,B) ≤ 2K2 ∑\nk 6=k∗: ∆k≥10/ √ T\n∆k log2\n( 20\n∆k\n) exp ( − T\n2H̄klog(n(T ))\n)\n+ 10√ T 1 { ∃k 6= k∗ s.t ∆k < 10/ √ T }"
    }, {
      "heading" : "2. The error probability is bounded as:",
      "text" : "e(T,B) ≤ 2K2 log2(20/∆) exp ( − T\n2H̄log(n(T ))\n)\nThe bound on the error probability only holds if ∆k ≥ 10/ √ T for all k 6= k∗.\nProof. Recall that the simple regret is given by:\nr(T,B) = ∑\nk 6=k∗ ∆kP\n( k̂(T,B) = k ) (34)\nLet us introduce some further notation. Let us define the phase at which an arm is ideally deleted as follows:\nγk := γ(∆k) := l if 10\n2l < ∆k ≤\n10\n2l−1 (35)\nTherefore we have the following chain:\nP ( k̂(T,B) = k ) a ≤ P ( Sc1:γk )\n≤ γk∑\nl=1\nBl\n≤ γk∑\nl=1\n2|R∗(l)| exp ( −2 −2(l−1)τ(l)v∗(B,R∗(l))2\n8l2\n)\nprovided ∆k ≥ 10/ √ T . Justification for (a) - If arm k is chosen finally, it implies that it is not eliminated at phase γk. Therefore the regret of the algorithm is given by:\nr(T,B) ≤ ∑\n{k 6=k∗:∆k≥10/ √ T}\n∆k\n( γk∑\nl=1\n2|R∗(l)| exp ( −2 −2(l−1)τ(l)v∗(B,R∗(l))2\n8l2\n)) (36)\n+ 10√ T 1 { ∃k 6= k∗ s.t ∆k < 10/ √ T }\n(37)\nLet `1, `2..`s = γk such thatR∗(`) changes value only at these phases. Let us set `s+1 = `s+1 for convenience in notation. Combining this notation with (36) we have:\nr(T,B) ≤ ∑\n{k 6=k∗:∆k≥10/ √ T}\n∆k\n( s∑\ni=1\n2|R∗(`i)| (`i+1 − `i) exp ( −2 −2`iTv∗(B,R∗(li))2\n2`3i log(n(T ))\n)) (38)\n+ 10√ T 1 { ∃k 6= k∗ s.t ∆k < 10/ √ T }\nConsider the phase `i when ideally at least an arm leaves. Let one of those arms be l. Recall that, γl is the phase where the arm ideally leaves according to (35). Therefore, γl = `i. Also it is easy to observe that: R∗(∆l) = R∗(`i). We have,\n`i ≥ log2(10/∆l) (39)\nas `i+1 − `i ≤ log2(20/∆k), i ≤ s. Further, for every `i < γk, there is at least one distinct arm l : γl = `i. This is because an arm leaves only once ideally. Further, we associate `s with arm k although other arms may leave at the phase `s = γk. Further, all arms l associated with `i < γk are such that ∆l ≥ ∆k. This is because of (35) and the fact that `i < `s = γk. Therefore, the r.h.s in Equation (38) is upper bounded as\nfollows:\nr(T,B) ≤ ∑\n{k 6=k∗:∆k≥10/ √ T}\n∆k\n  ∑\n{l:∆l≥∆k} 2|R∗(∆l)| log2(20/∆k) exp\n( − (∆l/10)\n2Tv∗(B,R∗(∆l))2 2 log2(10/∆l) 3log(n(T ))\n))\n+ 10√ T 1 { ∃k 6= k∗ s.t ∆k < 10/ √ T }\n(a) ≤ ∑\n{k 6=k∗:∆k≥10/ √ T}\n∆k\n  ∑\n{l:∆l≥∆k} 2|R∗(∆l)|\n  log2(20/∆k) exp ( − T\n2H̄klog(n(T ))\n)\n+ 10√ T 1 { ∃k 6= k∗ s.t ∆k < 10/ √ T }\n(b) ≤ 2K2 ∑\n{k 6=k∗:∆k≥10/ √ T}\n∆k log2(20/∆k) exp\n( − T\n2H̄klog(n(T ))\n)\n+ 10√ T 1 { ∃k 6= k∗ s.t ∆k < 10/ √ T }\nHere (a) is by definition of H̄k while (b) is because |R∗(∆l)| ≤ K and there are at most K terms in the summation.\nAnother quantity of interest here is the error probability. We will only provide bounds on the error probability e(T,B) when we have ∆k > 10/ √ T for all k 6= k∗. Let ∆ = mink 6=k∗ ∆k and γ∗ = γ(∆). In this case we have:\ne(T,B) ≤ 1− P (S1:γ∗)\n≤ γ∗∑\nl=1\nBl (40)\n≤ γ∗∑\nl=1\n2|R∗(l)| exp ( −2 −2(l−1)τ(l)v∗(B,R∗(l))2\n8l2\n)\na ≤\nγ∗∑\nl=1\n2|R∗(l)| exp ( −2 −2lTv∗(B,R∗(l))2\n2l3log(n(T ))\n) (41)\n(a)- This follows from the definition of τ(l). As before let `1 = 1, `2..`m ≤ γ∗ such that R∗(`) changes value only at these phases. Let us set `m+1 = `m + 1 for convenience in notation. Then, e(T,B) in (41) is upper bounded by:\ne(T,B) ≤ m∑\ni=1\n2|R∗(`i)| (`i+1 − `i) exp ( −2 −2`iTv∗(B,R∗(li))2\n2`3i log(n(T ))\n) (42)\nConsider the phase `i when ideally at least an arm leaves. Let one of those arms be k. Recall that, γk is the phase where the arm ideally leaves according to (35). Therefore, γk = `i. Also it is easy to observe that: R∗(∆k) = R∗(`i). Then, `i ≥ log2(10/∆k) (43) Further, for every `i, there is a distinct and different k : γk = `i. This is because an arm leaves only once ideally. Therefore, according to (43) and (42) we have:\ne(T,B) ≤ ∑\nk 6=k∗ 2|R∗(∆k)|γ∗ exp\n( − (∆k/10)\n2Tv∗(B,R∗(∆k))2 2 log2(10/∆k) 3log(n(T ))\n)\n≤ 2K2 log2(20/∆) exp ( − T\n2H̄log(n(T ))\n) (44)\nHere, we have used the definition of H̄ and the fact that |R∗(∆k)| ≤ K and γ∗ ≤ log(20/∆)"
    }, {
      "heading" : "D More Experiments",
      "text" : "In this section we provide more details about our experiments."
    }, {
      "heading" : "D.1 More on Flow Cytometry Experiments",
      "text" : "In this section we give further details on the flow cytometry experiments. As detailed in the main paper we use the causal graph in Fig. 5(c) in [26] (shown in Fig. 3a) as the ground truth. Then we fit a GLM gamma model [14] between each node in the graph and its parents using the observational data-set. The GLM model produces a highly accurate representation of the flow cytometry data-set. In Fig. 11a we plot the histogram for the activation of an internal node pip2 from the real data and samples generated from the GLM probabilistic model. It can be seen that the histograms are very close to each other.\nIn Fig. 11b we plot the performance of SRISv2 when the divergence metric is replaced by KL-divergence. In one of the plots SRISv2 is modified by setting Mij = 1 + KL(Pi||Pj). It can be seen that the performance degrades, which signifies that our divergence metric is fundamental to the problem."
    }, {
      "heading" : "D.2 More on Interpretation of Inception Deep Network",
      "text" : "In this section we describe the methodology of our model interpretation technique in more detail. In Section 4.2 we have described how the best arm algorithm can be used to pick a distribution over the superpixels of an image, that has the maximum likelihood of producing a certain classification from Inception. Here, we describe how the distributions over the superpixels are generated and how they are used subsequently. The arm distributions are essentially points in the n-dimensional simplex (where n is the number of superpixels into which the image is segmented). These distributions are generated in a randomized fashion using the following methods:\n1. Generate a point uniformly at random in the n-dimensional simplex.\n2. Randomly choose l < n superpixels. Make the distribution uniform over them and 0 elsewhere.\n3. Randomly choose l < n superpixels. The probability distribution is a uniformly chosen random point over the l-dimensional simplex with support on the l chosen superpixels and 0 elsewhere.\n4. Start a random walk from a few superpixels which traverses to adjacent superpixels at each step. Stop the random-walk after a certain number of steps and choose the superpixels touched by the random\nwalk. Then choose a uniform distribution over the super pixel support or choose a random distribution from the simplex of probability distributions over the support of the chosen superpixels (like in the previous point) and 0 elsewhere. This method uses the geometry of the image.\nNote that all the above methods do not depend on the specific content of the images. Using the above methods L pull arms are chosen which are used to collect the rewards. Further there are K opt arms that are the interventions to be optimized over. When an arm is used to sample, then m n superpixels are chosen with replacement from the distribution of that arm. These pixels are preserved in the original image while everything else is blurred out before feeding this into the neural network. Thus if the distribution corresponding to an arm is P , then the actual distribution to be used for the importance sampling is the product distribution Pm. Note that the pull arms are separate from the opt arms. The true counterfactual power of our algorithm is showcased in this experiment, as we are able to optimize over a large number of interventions that are never physically performed."
    } ],
    "references" : [ {
      "title" : "Low-cost learning via active data procurement",
      "author" : [ "Jacob Abernethy", "Yiling Chen", "Chien-Ju Ho", "Bo Waggoner" ],
      "venue" : "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Best arm identification in multi-armed bandits",
      "author" : [ "Jean-Yves Audibert", "Sébastien Bubeck" ],
      "venue" : "In COLT- 23th Conference on Learning Theory-2010, pages 13–p,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Probability inequalities for the sum of independent random variables",
      "author" : [ "George Bennett" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1962
    }, {
      "title" : "Causal models in the social sciences",
      "author" : [ "Hubert M Blalock" ],
      "venue" : "Transaction Publishers,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1985
    }, {
      "title" : "A predictive model for transcriptional control of physiology in a free living",
      "author" : [ "Richard Bonneau", "Marc T Facciotti", "David J Reiss", "Amy K Schmid", "Min Pan", "Amardeep Kaur", "Vesteinn Thorsson", "Paul Shannon", "Michael H Johnson", "J Christopher Bare" ],
      "venue" : "cell. Cell,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Counterfactual reasoning and learning systems: the example of computational advertising",
      "author" : [ "Léon Bottou", "Jonas Peters", "Joaquin Quinonero Candela", "Denis Xavier Charles", "Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Y Simard", "Ed Snelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Tight (lower) bounds for the fixed budget best arm identification bandit problem",
      "author" : [ "Alexandra Carpentier", "Andrea Locatelli" ],
      "venue" : "arXiv preprint arXiv:1605.09004,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "On the optimal sample complexity for best arm identification",
      "author" : [ "Lijie Chen", "Jian Li" ],
      "venue" : "arXiv preprint arXiv:1511.03774,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Reconstructing causal biological networks through active learning",
      "author" : [ "Hyunghoon Cho", "Bonnie Berger", "Jian Peng" ],
      "venue" : "PloS one,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Almost optimal intervention sets for causal discovery",
      "author" : [ "Frederick Eberhardt" ],
      "venue" : "In Proceedings of 24th Conference in Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Best arm identification: A unified approach to fixed budget and fixed confidence",
      "author" : [ "Victor Gabillon", "Mohammad Ghavamzadeh", "Alessandro Lazaric" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Generalized linear models and extensions",
      "author" : [ "James William Hardin", "Joseph M Hilbe", "Joseph Hilbe" ],
      "venue" : "Stata press,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Two optimal strategies for active learning of causal networks from interventional data",
      "author" : [ "Alain Hauser", "Peter Bühlmann" ],
      "venue" : "In Proceedings of Sixth European Workshop on Probabilistic Graphical Models,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Experiment selection for causal discovery",
      "author" : [ "Antti Hyttinen", "Frederick Eberhardt", "Patrik Hoyer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "lil’ucb: An optimal exploration algorithm for multi-armed bandits",
      "author" : [ "Kevin G Jamieson", "Matthew Malloy", "Robert D Nowak", "Sébastien Bubeck" ],
      "venue" : "In COLT,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Causal diagrams in systems epidemiology",
      "author" : [ "Michael Joffe", "Manoj Gambhir", "Marc Chadeau-Hyam", "Paolo Vineis" ],
      "venue" : "Emerging themes in epidemiology,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "On the complexity of best arm identification in multi-armed bandit models",
      "author" : [ "Emilie Kaufmann", "Olivier Cappé", "Aurélien Garivier" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Large-scale genetic perturbations reveal regulatory networks and an abundance of gene-specific repressors",
      "author" : [ "Patrick Kemmeren", "Katrin Sameith", "Loes AL van de Pasch", "Joris J Benschop", "Tineke L Lenstra", "Thanasis Margaritis", "Eoghan O?Duibhir", "Eva Apweiler", "Sake van Wageningen", "Cheuk W Ko" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Gene regulatory networks in plants: learning causality from time and perturbation",
      "author" : [ "Gabriel Krouk", "Jesse Lingeman", "Amy Marshall Colon", "Gloria Coruzzi", "Dennis Shasha" ],
      "venue" : "Genome biology,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Causal bandits: Learning good interventions via causal inference",
      "author" : [ "Finnian Lattimore", "Tor Lattimore", "Mark D Reid" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms",
      "author" : [ "Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang" ],
      "venue" : "In Proceedings of the fourth ACM international conference on Web search and data mining,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "High-dimensional learning of linear causal networks via inverse covariance estimation",
      "author" : [ "Po-Ling Loh", "Peter Bühlmann" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Methods for causal inference from gene perturbation experiments and validation",
      "author" : [ "Nicolai Meinshausen", "Alain Hauser", "Joris M Mooij", "Jonas Peters", "Philip Versteeg", "Peter Bühlmann" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Cyclic causal discovery from continuous equilibrium data",
      "author" : [ "Joris Mooij", "Tom Heskes" ],
      "venue" : "arXiv preprint arXiv:1309.6849,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "Distinguishing cause from effect using observational data: methods and benchmarks",
      "author" : [ "Joris M Mooij", "Jonas Peters", "Dominik Janzing", "Jakob Zscheischler", "Bernhard Schölkopf" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Causality: Models, Reasoning and Inference",
      "author" : [ "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "Kullback-leibler divergence estimation of continuous distributions",
      "author" : [ "Fernando Pérez-Cruz" ],
      "venue" : "In Information Theory,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Why should i trust you?: Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin" ],
      "venue" : "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2016
    }, {
      "title" : "Causal proteinsignaling networks derived from multiparameter single-cell data",
      "author" : [ "Karen Sachs", "Omar Perez", "Dana Pe’er", "Douglas A Lauffenburger", "Garry P Nolan" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2005
    }, {
      "title" : "Learning causal graphs with small interventions",
      "author" : [ "Karthikeyan Shanmugam", "Murat Kocaoglu", "Alexandros G Dimakis", "Sriram Vishwanath" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Dynamic ad allocation: Bandits with budgets",
      "author" : [ "Aleksandrs Slivkins" ],
      "venue" : "arXiv preprint arXiv:1306.0155,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2013
    }, {
      "title" : "Causation, Prediction, and Search",
      "author" : [ "Peter Spirtes", "Clark Glymour", "Richard Scheines" ],
      "venue" : "A Bradford Book,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2001
    }, {
      "title" : "On the application of probability theory to agricultural experiments",
      "author" : [ "Jerzy Splawa-Neyman", "DM Dabrowska", "TP Speed" ],
      "venue" : "essay on principles",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1990
    }, {
      "title" : "Covariate shift adaptation by importance weighted cross validation",
      "author" : [ "Masashi Sugiyama", "Matthias Krauledat", "Klaus-Robert MÃžller" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2007
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Causal graphs [28] are useful for representing causal relationships among interacting variables in large systems [7].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 22,
      "context" : "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 32,
      "context" : "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.",
      "startOffset" : 408,
      "endOffset" : 426
    }, {
      "referenceID" : 15,
      "context" : "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.",
      "startOffset" : 408,
      "endOffset" : 426
    }, {
      "referenceID" : 17,
      "context" : "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.",
      "startOffset" : 408,
      "endOffset" : 426
    }, {
      "referenceID" : 4,
      "context" : "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.",
      "startOffset" : 408,
      "endOffset" : 426
    }, {
      "referenceID" : 18,
      "context" : "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.",
      "startOffset" : 408,
      "endOffset" : 426
    }, {
      "referenceID" : 5,
      "context" : "An illustrative example includes online advertising [7], where there is a collection of click-through rate scoring algorithms that provide an estimate of the probability that an user clicks on an ad displayed at a specific position.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "Another example is in biological gene-regulatory networks [6], where a large number of genomes interact amongst each other and also interact with environmental factors.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "Figure 1: Computational advertising example borrowed from [7].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "Determining the best intervention in the above setting can be cast as a best arm identification bandit problem, as noted in [22].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 19,
      "context" : "Our procedure yields a major improvement over the algorithm in [22] (both in theoretical guarantees and in practice), which sets the clippers and allocates samples in a static manner.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "(Gap Dependent Error Guarantees under Budget Constraints) In the classic best arm identification problem [3], Audibert et al.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "Then, it has been shown in [3] that the number of samples needed scales as (upto poly log factors) maxi(i/∆ 2 (i)).",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "In our setting, a fundamental difference from the classical best arm setting [3] is the information leakage across the arms, i.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "σi can be much smaller than √ i (the corresponding term in the results of [3]).",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "Our theoretical guarantees quantify the improvement obtained by leveraging information leakage, which has been empirically observed in [7].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "1) act as the ‘effective variance’ term in the analysis for the estimators (similar to Bernstein’s bound [4]).",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "(Extensive Empirical Validation) We demonstrate that our algorithm outperforms the prior works [22, 3] on the Flow Cytometry data-set [32] (in Section 4.",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "(Extensive Empirical Validation) We demonstrate that our algorithm outperforms the prior works [22, 3] on the Flow Cytometry data-set [32] (in Section 4.",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "(Extensive Empirical Validation) We demonstrate that our algorithm outperforms the prior works [22, 3] on the Flow Cytometry data-set [32] (in Section 4.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "We exhibit an innovative application of our algorithm for model interpretation of the Inception Deep Network [38] for image classification (refer to Section 4.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].",
      "startOffset" : 173,
      "endOffset" : 187
    }, {
      "referenceID" : 7,
      "context" : "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].",
      "startOffset" : 173,
      "endOffset" : 187
    }, {
      "referenceID" : 14,
      "context" : "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].",
      "startOffset" : 173,
      "endOffset" : 187
    }, {
      "referenceID" : 6,
      "context" : "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].",
      "startOffset" : 173,
      "endOffset" : 187
    }, {
      "referenceID" : 6,
      "context" : "It was shown recently in [8] that the results of [3] are optimal.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "It was shown recently in [8] that the results of [3] are optimal.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 24,
      "context" : "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 31,
      "context" : "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 25,
      "context" : "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 29,
      "context" : "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "One notable work that partially inspired our work is [7] where the causal graph underlying a computational advertising system (like in Bing, Google etc.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "At the intersection of causality and bandits, [22] is perhaps most relevant to our setting.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "We assume soft interventions that affect the mechanism between a ’source’ node and its parents, far away from the target (similar to the case of computational advertising considered in [7]).",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "Further, we derive the first gap dependent bounds (that can be exponentially small in T ), generalizing the results of [3].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "Our formulation can handle general budget constraints on the bandit arms and also recover the problem independent bounds of [22] (orderwise).",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "Budget constraints in bandit settings have been explored before in [1, 34].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 30,
      "context" : "Budget constraints in bandit settings have been explored before in [1, 34].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : "In the context of machine learning, importance sampling has been mostly used to recondition input data to adhere to conditions imposed by learning algorithms [37, 23, 39].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : "In the context of machine learning, importance sampling has been mostly used to recondition input data to adhere to conditions imposed by learning algorithms [37, 23, 39].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "For simplicity we assume that the variables V, pa(V ) are discrete while the target variable Y may be continuous/discrete and has bounded support in [0, 1].",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "Fixed Budget for Samples: In this paper, we work under the fixed budget setting of best arm identification [3].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "We find such examples in the context of online advertisement design [7].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 30,
      "context" : "(ii) Cost Budget (S2): This is the most general budget setting that captures the variable costs of sampling each arm [34].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "Then the probability of error e(T,B) [3, 8] is given by, e(T,B) = P ( k̂(T,B) 6= k∗ )",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Then the probability of error e(T,B) [3, 8] is given by, e(T,B) = P ( k̂(T,B) 6= k∗ )",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "(Simple Regret): Another important quantity that has been analyzed in the best arm identification setting is the simple regret [22].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "A popular method for utilizing this information leakage among different distributions is through importance sampling, which has been used in counterfactual analysis in similar causal settings [22, 7].",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "A popular method for utilizing this information leakage among different distributions is through importance sampling, which has been used in counterfactual analysis in similar causal settings [22, 7].",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 19,
      "context" : "This has been noted in [22] in a similar setting, where a static clipper has been applied to the weighted samples to control the variance.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 26,
      "context" : "We also note that estimates of the f -divergence measure can be had directly from empirical data (without the knowledge of the full distributions) using techniques like that of [30].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : "2We note that the authors in [22] discuss the possibility of a multi-phase approach, where clipper levels could change across phases.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Our results can be interpreted as a natural generalization of the results in [3], when there is information leakage among the arms.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Comparison with the result in [3]: Let R̃(∆k) = {s : ∆s ≤ ∆k}, i.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "The result in [3] can be stated as: The error in finding the optimal arm is bounded as: e(T ) ≤ O ( K exp ( − T−K log(K)H̃ )) .",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "Comparison with the result in [22]: In [22], the simple regret scales as O(1/ √ T ) and does not adapt to the gaps.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "Comparison with the result in [22]: In [22], the simple regret scales as O(1/ √ T ) and does not adapt to the gaps.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "We provide gap dependent bounds that can be exponentially better than that of [22] (when ∆k’s are not too small and the first term in (7) is zero).",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 28,
      "context" : "1, we study the empirical performance of our algorithm on the flow cytometry data-set [32].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 34,
      "context" : "2, we apply our algorithms for the purpose of model interpretability of the Inception Deep Network [38] in the context of image classification.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "(a) Causal Graph for Cytometry Data [26] 0 100 200 300 400 500 600 700 800 900 0.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 28,
      "context" : "The flow cytometry data-set [32] consists of multi-variate measurements of protein interactions in a single cell, under different experimental conditions (soft interventions).",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "5(c) in [26] (shown in Fig.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 22,
      "context" : "Parametric linear models have been popularly used for causal inference on this data-set [25, 11].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "Parametric linear models have been popularly used for causal inference on this data-set [25, 11].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 11,
      "context" : "We fit a GLM gamma model [14] between the activation of each node and its parents in Fig.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "Competing Algorithms: We test our algorithms on different problem parameters and compare with related prior work [3, 22].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "Competing Algorithms: We test our algorithms on different problem parameters and compare with related prior work [3, 22].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 26,
      "context" : "The divergences, Df1(Pi||Pj) are estimated from sampled data using techniques from [30]; (ii)SRISv2: Algorithm 2 as detailed in Section 3.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "2; (iii) SR: Successive Rejects Algorithm from [3] adapted to the budget setting.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "The division of the total budget T into K − 1 phases is identical, while the individual arm budgets are decided in each phase according to the budget restrictions; (iv) CR: Algorithm 2 from [22].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 34,
      "context" : "2 Interpretability of Inception Deep Network In this section we use our algorithm for model interpretation of the pre-trained Inception-v3 network [38] for classifying images.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 27,
      "context" : "Model Interpretation essentially addresses: ’why does a learning model classify in a certain way?’, which is an important question for complicated models like deep nets [31].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 1,
      "context" : "We demonstrate that our algorithm performs well in all the experiments, as compared to previous works [3, 22].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "We demonstrate that our algorithm performs well in all the experiments, as compared to previous works [3, 22].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "In conclusion, it should be noted that our algorithms perform well in all the different settings, because they are able to adapt to the problem parameters (similar to [3]) and at the same time leverage the information leakage (similar to [22]).",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 19,
      "context" : "In conclusion, it should be noted that our algorithms perform well in all the different settings, because they are able to adapt to the problem parameters (similar to [3]) and at the same time leverage the information leakage (similar to [22]).",
      "startOffset" : 238,
      "endOffset" : 242
    }, {
      "referenceID" : 1,
      "context" : "We provide the first problem dependent simple regret and error bounds for this problem, that is a natural generalization of [3], but with information leakage between arms.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "Problem Dependent Lower Bound: In [22], a problem independent lower bound of O(1/ √ T ) has been provided for a special causal graph.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "However, the problem parameter dependent lower bound like that of [3] still remains an open problem.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "In fact, this is explored in a non-bandit context in [7].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "An important question is: What is the most suitable cut to be used? [22] uses the cut closest to Y , i.",
      "startOffset" : 68,
      "endOffset" : 72
    } ],
    "year" : 2017,
    "abstractText" : "Motivated by applications in computational advertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node V in an acyclic causal directed graph, to maximize the expected value of a target node Y (located downstream of V ). Our setting imposes a fixed total budget for sampling under various interventions, along with cost constraints on different types of interventions. We pose this as a best arm identification bandit problem with K arms where each arm is a soft intervention at V, and leverage the information leakage among the arms to provide the first gap dependent error and simple regret bounds for this problem. Our results are a significant improvement over the traditional best arm identification results. We empirically show that our algorithms outperform the state of the art in the Flow Cytometry data-set, and also apply our algorithm for model interpretation of the Inception-v3 deep net that classifies images.",
    "creator" : "LaTeX with hyperref package"
  }
}