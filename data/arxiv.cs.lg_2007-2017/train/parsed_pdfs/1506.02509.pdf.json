{
  "name" : "1506.02509.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "leizhang@cqu.edu.cn)", "csdzhang@comp.polyu.edu.hk)" ],
    "sections" : [ {
      "heading" : null,
      "text" : " Abstract—Deep learning with a convolutional neural network (CNN) has been proved to be very effective in feature extraction and representation of images. For image classification problems, this work aim at finding which classifier is more competitive based on high-level deep features of images. In this report, we have discussed the nearest neighbor, support vector machines and extreme learning machines for image classification under deep convolutional activation feature representation. Specifically, we adopt the benchmark object recognition dataset from multiple sources with domain bias for evaluating different classifiers. The deep features of the object dataset are obtained by a well-trained CNN with five convolutional layers and three fully-connected layers on the challenging ImageNet. Experiments demonstrate that the ELMs outperform SVMs in cross-domain recognition tasks. In particular, state-of-the-art results are obtained by kernel ELM which outperforms SVMs with about 4% of the average accuracy. The features and codes are available in http://www.escience.cn/people/lei/index.html\nKeywords—Deep learning; image classification; support vector machine; extreme learning machine; object recognition\nI. INTRODUCTION ecently, deep learning as the hottest learning technique has been widely explored in machine learning, computer vision, natural language processing and data mining. In the\nearly, convolutional neural network (CNN), as the most important deep net in deep learning, has been applied to document recognition and face recognition [1, 2]. Moreover, some deep learning algorithms with multi-layer fully connected networks (e.g. multi-layer perceptrons, MLP) for auto-encoder have been proposed, for examples, stacked auto encoders (SAE) [3], deep belief networks (DBN) [4] and deep Boltzmann machines (DBM) [5]. However, in large-scale learning problems, e.g. image classification in computer vision, CNNs with convolutioanl layers, pooling layers and fully-connected layers are widely investigated for its strong deep feature representation ability and state-of-the-art performance in challenged big datasets like ImageNet, Pascal VOC, etc. In the latest progress of deep learning, researchers have broken the new record in face verification by using CNNs with different structures [6, 7, 8, 9]. The latest verification accuracy on LFW data is 99.7% by Face++ team. Besides the faces, CNN has also\nL. Zhang is with College of Communication Engineering, Chongqing University, Chongqing 400044, China and also with Department of Computing, The Hong Kong Polytechnic University, Hong Kong (email: leizhang@cqu.edu.cn) D. Zhang is with Department of Computing, The Hong Kong Polytechnic University, Hong Kong. (email: csdzhang@comp.polyu.edu.hk)\nachieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17]. From these works, CNNs have been proved to be highly effective for deep feature representation with large-scale parameters. The main advantages of deep learning can be shown in three facets. 1) Feature representation. CNN integrates feature extraction (raw pixels) and model learning together, without using any other advanced low-level feature descriptors. 2) Large-scale learning. With the adjustable network structures, big data in millions can be learned by a CNN at one time. 3) Parameter learning. Due to the scalable network structures, millions of parameters can be trained. Therefore, CNN based deep method can be state-of-the-art parameter learning technique.\nIn this report, we would like to discuss about the deep feature representation capability of CNN by using traditional classification method with high-level deep features of images, and find which classifier is the best under the deep representation. Therefore, we mainly exploit the nearest neighbor (NN) [18], support vector machine (SVM) [19], least-square support vector machine (LSSVM) [20], extreme learning machine (ELM) [21] and kernel extreme learning machine (KELM) [22]. These classifiers are well-known in many different applications. Specially, ELM was initially proposed for generalized single-hidden-layer feed-forward neural networks and overcome the local minima, learning rate, stopping criteria and learning epochs that exist in gradient-based methods such as back-propagation (BP) algorithm. In recent years, ELMs are widely used due to some significant advantages such as learning speed, ease of implementation and minimal human intervention. The potential for large scale learning and artificial intelligence is preserved. The main steps of ELM include the random projection of hidden layer with random input weights and analytically determined solution by using Moore-Penrose generalized inverse. With similar impact with SVM, it has been proved to be efficient and effective for regression and classification tasks [23, 24]. The latest work about the principles and brain-alike learning of ELM has been presented [25]. Many improvement and new applications of ELMs have been proposed by researchers. The newest work about ELM for deep auto-encoder, local receptive fields for deep learning, transfer learning, and semi-supervised learning have also been proposed [26, 27, 28, 29, 30]. With the Mercer condition applied, a kernel ELM (KELM) that computes a kernel matrix of hidden layers has also been proposed [22]. A salient feature\nSVM and ELM: Who Wins? Object Recognition with Deep Convolutional Features from\nImageNet Lei Zhang and David Zhang\nR\nof KELM is that the random input weights and bias can be avoided.\nIn this report, we will present a study of NN, SVM, LSSVM, ELM and KELM for object recognition on the deep convolutional activation features trained by CNN on ImageNet, and have an insight of which one is the best for classification on deep representation.\nThe rest of this paper is organized as follows. Section 2 presents a method review of support vector machines and extreme learning machines. Section 3 shows the training and testing protocol of CNN for deep representation of images. Section 4 presents the experiments and results. Finally, Section 5 concludes this paper."
    }, {
      "heading" : "II. OVERVIEW OF SVMS AND ELMS",
      "text" : ""
    }, {
      "heading" : "A. Support Vector Machine",
      "text" : "In this section, the principle of SVM for classification problems is briefly reviewed. More details can be referred to [19].\nGiven a training set of N data points { , } , where the label ∈ {−1, 1}, = 1, ⋯ , . According to the structural risk minimization principle, SVM aims at solving the following risk bound minimization problem with inequality constraint.\n   iiii\nN i i\nbyts\nC i\n\n \n    1,0..\n, 2 1min\nT\n1 2\n,\nxw\nw w (1)\nwhere φ(∙) is a linear/nonlinear mapping function, w and b are the parameters of classifier hyper-plane.\nGenerally, for optimization, the original problem (7) of SVM can be transformed into its dual formulation with equality constraint by using Lagrange multiplier method. One can construct the Lagrange function\n       \n\n\n\n\nN i ii N i iiii\nN\ni iiii\nby\nCbL\n11 T\n1\n2\n1-\n2 1,;,,\n\n\nxw\nww (2)\nwhere ≥ 0 and ≥ 0 are Lagrange multipliers. The solution can be given by the saddle point of Lagrange function (2) by solving\n iiib bLiii   ,;,,minmax ,,, w w (3)\nBy calculating the partial derivatives of Lagrange function (2) with respect to w, b and ξi, one can obtain\n     \n     \n   \n\n \n\n \n\n \n\n \n\n\nCbL\ny b\nbL\nybL\ni i\niii\nN\ni ii iii\nN\ni iii iii\n \n\n \n \n00,;,,\n00 ,;,,\n0,;,,\n1\n1\nw\nw\nxw w\nw\n(4)\nThen one can rewrite (3) as\n   \nCyts\nyy\ni N\ni ii\nji jijiji i i\n\n\n\n\n \n\n0,0..\n2 1max\n1\n, T xxααα α (5)\nBy solving α of the dual problem (5) with a quadratic programming, the goal of SVM is to construct the following decision function (classifier),\n            byf\nM\ni iii1 ,sgn xxαx  (6)\nwhere (∙) is a kernel function. ( , ) = φ( ) φ( ) = for linear SVM and ( , ) = exp(−‖ − ‖ σ⁄ ) for RBF-SVM."
    }, {
      "heading" : "B. Least Square Support Vector Machine",
      "text" : "LSSVM is an improved and simplified version of SVM. The details can be referred to [20]. We briefly introduce the basic principle of LSSVM for classification problems. By introducing the square error and equality constraint, LSSVM can be formulated as\n   Nibyts C\niii\nN\ni ii\n,,1,1..\n, 2 1 2 1min\nT\n1 22\n,\n       xw w w (7)\nThe Lagrange function of (7) can be defined as\n      \n\n\n\n\nN i iiii\nN\ni iii\nby\nCbL\n1 T\n1 22\n1-\n2 1 2 1;,,\n\n\nxw\nww (8)\nwhere is the Lagrange multiplier. The optimality conditions can be obtained by computing the partial derivatives of (8) with respect to the four variables as\n     \n \n          \n    \n\n \n\n \n\n \n\n \n\n \n\n\n01-0 ;,,\n0 ;,,\n00 ;,,\n0 ;,,\nT\n1\n1\niii i\nii\nii i\nii\nN\ni ii ii\nN\ni iii ii\nby bL\nC bL\ny b\nbL\ny bL\n \n\n \n\n \n \nxw w\nw\nw\nxw w\nw\n(9)\nThe equation group (9) can be written in linear equation as\n    \n\n\n    \n\n\n     \n\n    \n\n     \n\n     \n\n \n\n1\n0 0 0 α ξ b w IYZ II Y ZI  0 00 000 00 C T\nT\n(10)\nwhere     T11 ,, NN yy xxZ   ,  T1 ,, Nyy Y ,  T1,,1  1 ,  T1 ,, N ξ ,  T1 ,, N α . The solution\nof α and b can also be given by\n  \n\n  \n                 1 0 α b IZZY Y0  1T TC (11)\nLet TZZΩ  , with the Mercer condition, there is\n      Nlkyyyy lklklklklk ,,1,,,T,  xxxx  (12)\nBy substituting (12) into (11), the solution can be obtained by solving a linear equation instead of a quadratic programming problem in SVM. The final decision function of LSSVM is the same as SVM shown as (6)."
    }, {
      "heading" : "C. Extreme Learning Machine",
      "text" : "ELM aims to solve the output weights of a single layer feed-forward neural network (SLFN) by minimizing the squared loss of predicted errors and the norm of the output weights in both classification and regression problems. We briefly introduce the principle of ELM for classification problems. Given a dataset = [ , , ⋯ , ] ∈ ℜ × of N samples with label = [ , , ⋯ , ] ∈ ℜ × , where d is the dimension of sample and c is the number of classes. Note that if\n( = 1, ⋯ , ) belongs to the k-th class, the k-th position of ( = 1, ⋯ , ) is set as 1, and -1 otherwise. The hidden layer\noutput matrix H with L hidden neurons can be computed as\n             \n\n\n   \n\n\n \nLNLNN\nLL\nbhbhbh\nbhbhbh\nxwxwxw\nxwxwxw H\nT 2 T 21 T 1\n1 T 21 T 211 T 1\n\n\n\n(13)\nwhere ℎ(∙) is the activation function of hidden layer, = [ , ⋯ , ] ∈ ℜ × and = [ , ⋯ , ] ∈ ℜ are randomly generated input weights and bias between the input layer and hidden layer. With such a hidden layer output matrix H, ELM can be formulated as follows\n  TTTT 1\n22 ,,1,.. 2 1 2 1min ξTβHξtβx ξβ β      Nihts C\niii\nN\ni icL\n\n(14)\nwhere ∈ ℜ × denotes the output weights between hidden layer and output layer, = [ , ⋯ , ] denotes the prediction error matrix with respect to the training data, and C is a penalty constant on the training errors.\nThe closed form solution of (14) can be easily solved. First, if the number N of training patterns is larger than L, the gradient equation is over-determined, and the closed form solution of (14) can be obtained as\nTH I HHTHβ T 1 T* \n        C LL (15)\nwhere × denotes the identity matrix with size of L, and H is the Moore-Penrose generalized inverse of H. If the number N of training patterns is smaller than L, an under-determined least square problem would be handled. In\nthis case, the solution of (14) can be obtained as\nT I HHHTHβ 1 TT* \n        C NN (16)\nwhere × denotes the identity matrix. Then the predicted output of a new observation z can be computed as\n   \n   \n  \n\n      \n       \n \n \nLNif C h\nLNif C h h\nNN\nLL\n,\n,\n1 TT\nT 1 T\n*\nT I HHHz\nTHIHHz βzy (17)"
    }, {
      "heading" : "D. Kernelized Extreme Learning Machine",
      "text" : "One can also apply Mercer condition to ELM and thus a\nKELM is formulated. The KELM can be described as follows. Let = ∈ ℜ × , where Ω , = ℎ( )ℎ = , and (∙) is the kernel function. With the expression of solution β (16), the predicted output of a new observation z can be computed as\n \n \n \n  T\nI Ω\nxz\nxz\nT I HHHz\nβzy\n1 T\n1\n1\n1 TT\n*\n,\n,  \n \n          \n\n   \n \n      \n\nC\nC h\nh\nNN\nNN\n\n \n(18)\nNote that due to the kernel matrix of training data is ∈ ℜ × , therefore, the number L of hidden neurons is not explicit and the decision function of KELM can be expressed uniquely in (18)."
    }, {
      "heading" : "III. TRAINING AND TESTING PROTOCOL",
      "text" : ""
    }, {
      "heading" : "A. CNN training on ImageNet",
      "text" : "In this report, we aim at proposing a comparative\ninvestigation on SVMs and ELMs for classification based on deep convolutional features. Therefore, we adopt the deep convolutional activated features (DeCAF) from [17] for experiments. The structures of CNN for training on the ImageNet with 1000 categories are the same as the proposed CNN in [10]. The basic structure of the adopted is illustrated in Fig.1, which includes 5 convolutional layers and 3 fully-connected layers. Further details of the CNN training architecture and features can be referred to [10, 17]."
    }, {
      "heading" : "B. CNN Testing",
      "text" : "The well-trained network parameters shown in Fig.1 are used for deep representation of the 4DA (domain adaptation) dataset [31, 32]. The CNN outputs of the 6-th (f6) and 7-th (f7) fully-connected layers are used as inputs of SVMs and ELMs for classification, respectively. The 4DA dataset includes four domains such as Caltech 256 (C), Amazon (A), Webcam (W) and Dslr (D) sampled from different sources, in which 10 object classes are selected. As can be seen from Fig.1, the dimension of features from f6 and f7 is 4096. The detail of 4DA dataset with deep features is summarized in Table I. Some examples of the dataset for each domain have been illustrated in Fig.2."
    }, {
      "heading" : "C. Classification",
      "text" : "The 4DA dataset is commonly used for evaluating domain adaptation and transfer learning tasks. So, in this report, we investigate the classification ability of deep representation on domain shifted data. We adopt the deep features for SVMs/ELMs training, and compare the classification accuracy. The specific experimental setup is described in Experiments section."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "A. Experimental Setup",
      "text" : "In the experiment, three settings are investigated respectively,\nas follows. 1) Setting 1: single-domain recognition task.\nFor example, we train a model on the training data of Amazon, and report the test accuracy on the remaining data of Amazon. As shown in Table I (ns/c), 20, 8, 8, and 8 samples per class are randomly selected for training from Amazon, DSLR, Webcam and Caltech domains, respectively, and the remaining are used as test samples for each domain. 20 random train/test splits are run, and the average recognition accuracy for each method is reported. 2) Setting 2: cross-domain recognition tasks--source only.\nWe perform a cross-domain recognition task. For example, we train a SVM/ELM on the Amazon and test on DSLR, i.e.\nA→D. Totally, 12 cross-domain tasks among the four domains are conducted. Note that the training data is source data only (source only) without leveraging the data from target domain. The number of training data is 20, 8, 8 and 8 per class for Amazon, DSLR, Webcam and Caltech domains, respectively, when used as source domain. 20 random train/test splits are run, and the average recognition accuracy for each method is reported. 3) Setting 3: cross-domain recognition tasks--source and target.\nSimilar to Setting 2, we perform a cross-domain recognition task. For example, we train a SVM/ELM on the Amazon and test on DSLR, i.e. A→D. Totally, 12 cross-domain tasks among the four domains are conducted. However, the difference from Setting 2 lies in that the training data includes the labeled source data and few labeled target data. The number of training data is 20, 8, 8 and 8 per class for Amazon, DSLR, Webcam and Caltech domains, respectively, when used as source domain. The number of few labeled target data is 3 per class for each domain when they are used as target domain, as shown in Table I (nt/c). 20 random train/test splits are run, and the average recognition accuracy for each method is reported."
    }, {
      "heading" : "B. Parameter Setting",
      "text" : "To make sure that the best result of each method can be obtained, we have adjusted the parameters. For SVM the penalty coefficient C and kernel parameter σ are set as 1000 and\n1, respectively, by using Libsvm-3.12 toolbox. For LSSVM, the two coefficients are automatically optimized with a grid search by using LSSVM-1.7 toolbox. For ELM, the penalty coefficient C and the number L of hidden neurons are set as 100 and 5000, respectively. For KELM, the penalty coefficient C and kernel parameter σ are set as 100 and 0.01, respectively. Note that the penalty coefficient C and kernel parameter σ for SVM, ELM, and KELM are adjusted from the set C={1, 100, 10000} and σ={0.0001, 0.01, 1, 100}."
    }, {
      "heading" : "C. Experimental Results",
      "text" : "(1) Results of Setting 1.\nFor experimental Setting 1, the average accuracy of 20 randomly generated train/test splits for five methods including NN, SVM, LSSVM, ELM and KELM are reported in Table II. We can observe that the recognition performance based on the deep features from the 6-th layer (f6) and 7-th layer (f7) is slightly different. The best two methods are highlighted with bold face. From the comparisons, we can find that ELMs outperforms SVMs and NN methods for all domains, and KELM shows a more competitive performance. Specifically, by comparing KELM and SVM, the improvement in accuracy for the deep features f6 is 0.8%, 0.2%, 1.1% and 2.1% for Amazon, DSLR, Webcam, and Caltech, respectively. For the deep features f7, the improvement is 1.0%, 0.6%, 0.8%, and 2.5%, respectively.\n(2) Results of Setting 2.\nTable III presents the average recognition accuracy of 20 randomly generated train/test splits based on the experimental setting 2. Totally, 12 cross-domain recognition tasks are conducted. The first two highest accuracies are highlighted in bold face. We can observe that 1) the recognition performance with deep feature f7 clearly outperforms that of f6, which demonstrates the effectiveness of “deep”; 2) the performance of ELM and KELM is significantly better than SVM and LSSVM, the average improvement of 12 tasks of KELM is 4% better than that of SVM. The results demonstrate that for more difficult problems (i.e. cross-domain tasks), the ELM based methods show a more competitive and robust advantage for classification. More obvious, the accuracies by using the five methods for each cross-domain task are illustrated in Fig. 3, from which the superiority of ELMs especially KELM is clearly demonstrated compared with others methods for each tasks under deep features from f7 and f6.\nTABLE III RECOGNITION ACCURACY OF EACH METHOD WITH SETTING 2, WHERE THE TRAINING DATA IS FROM SOURCE DOMAIN ONLY (A: AMAZON, C: CALTECH 256, W: WEBCAM, D: DSLR)\nMethod CNN_layer A→D C→D W→D A→C W→C D→C D→A W→A C→A C→W D→W A→W\nNN f6 71.9±0.9 72.0±1.7 92.7±0.5 76.8±0.3 56.6±0.9 64.4±0.4 75.1±0.7 64.0±0.6 78.1±0.8 61.5±1.1 95.8±0.4 65.1±1.0 f7 78.7±0.5 75.6±1.3 96.9±0.4 77.2±0.4 66.2±0.5 70.7±0.4 75.0±0.7 66.3±0.8 83.6±0.4 60.7±1.2 95.2±0.4 68.5±0.8\nSVM f6 79.6±0.7 75.1±1.8 96.7±0.4 79.5±0.4 59.5±0.9 67.3±1.2 77.0±1.0 66.8±1.0 85.8±0.4 67.1±1.1 95.4±0.4 70.6±0.8 f7 80.6±0.8 76.4±1.4 96.7±0.4 79.6±0.4 68.1±0.6 74.3±0.6 81.8±0.5 73.4±0.7 86.5±0.5 67.8±1.1 95.3±0.5 71.0±0.8 LSSVM f6 77.1±0.9 76.8±1.2 96.1±0.3 77.5±0.6 61.1±0.7 70.6±1.0 80.0±0.8 68.2±1.1 86.5±0.4 67.8±1.2 96.4±0.4 65.5±0.8 f7 82.6±0.5 79.2±0.8 95.9±0.4 79.8±0.5 66.0±1.3 73.7±0.9 80.8±0.7 72.0±1.1 87.4±0.3 69.9±1.1 95.1±0.3 69.4±0.6\nELM f6 80.6±0.6 79.5±1.2 96.7±0.2 80.4±0.3 67.2±0.5 75.6±0.5 83.7±0.4 72.2±0.9 87.3±0.4 70.1±0.9 97.2±0.3 71.1±0.6 f7 82.3±0.5 81.2±0.7 97.0±0.4 81.8±0.3 74.0±0.3 79.5±0.2 85.8±0.3 76.7±0.9 88.3±0.2 72.3±0.9 96.8±0.3 72.4±0.8\nKELM f6 82.3±0.5 80.7±0.9 96.5±0.3 82.6±0.3 69.5±0.4 77.8±0.4 85.3±0.4 73.8±1.1 88.0±0.4 72.3±1.0 97.6±0.2 72.9±0.7 f7 84.0±0.4 82.2±0.9 97.3±0.3 83.4±0.2 75.7±0.3 81.1±0.2 87.1±0.2 78.2±0.8 89.1±0.3 73.3±0.9 96.9±0.3 74.7±0.8\n(2) Results of Setting 3.\nThe results under experimental Setting 3 are reported in Table IV, from which we can find that ELMs especially KELM outperform other methods. Due to that few labeled data from target domain are leveraged in model training with domain adaptation, so the recognition accuracies are much higher than that from Table III. The average differences between ELMs and SVMs are therefore reduced from 4% in Setting 2 to 1.5% in Setting 3. For better visualization of the difference, we provide a Fig.4 which describes the recognition accuracies of all methods for each cross-domain task. We can see that KELM always shows the best performance.\nTABLE IV RECOGNITION ACCURACY OF EACH METHOD WITH SETTING 3, WHERE THE TRAINING DATA IS FROM BOTH SOURCE AND TARGET DOMAINS (A: AMAZON, C: CALTECH 256, W: WEBCAM, D: DSLR)\nMethod CNN_layer A→D C→D W→D A→C W→C D→C D→A W→A C→A C→W D→W A→W\nNN f6 89.4±0.7 90.1±0.8 97.0±0.4 78.1±0.4 69.0±0.9 72.8±0.8 83.8±0.5 83.3±0.7 85.4±0.4 86.9±0.6 97.2±0.4 86.1±0.8 f7 93.0±0.5 90.9±0.9 98.6±0.2 78.9±0.4 73.6±0.6 75.6±0.4 86.7±0.5 84.0±0.5 87.9±0.2 87.8±0.9 96.3±0.2 89.1±0.6\nSVM f6 94.5±0.4 92.9±0.8 99.1±0.2 84.0±0.3 81.7±0.5 83.0±0.3 90.5±0.2 90.1±0.2 90.0±0.2 91.5±0.6 97.9±0.3 90.4±0.8 f7 94.0±0.6 92.7±0.8 98.9±0.2 83.4±0.4 81.2±0.4 82.7±0.4 90.9±0.3 90.6±0.2 90.3±0.2 90.6±0.8 98.0±0.2 91.1±0.8 LSSVM f6 92.6±0.5 93.1±0.6 98.8±0.2 82.3±0.5 80.7±0.5 82.3±0.4 90.9±0.2 89.7±0.2 90.3±0.1 90.9±0.6 97.8±0.3 87.7±0.8 f7 91.9±0.5 92.4±0.8 98.4±0.2 82.9±0.4 81.7±0.3 82.6±0.5 90.9±0.4 90.0±0.2 90.7±0.2 90.4±0.5 97.2±0.3 89.5±0.7\nELM f6 94.6±0.5 93.7±0.6 99.2±0.2 83.4±0.3 81.2±0.3 83.5±0.3 91.1±0.2 90.3±0.2 90.5±0.1 91.6±0.7 98.3±0.2 90.5±0.6 f7 94.9±0.4 93.0±0.6 99.0±0.2 84.1±0.2 82.2±0.4 84.1±0.2 91.7±0.2 90.8±0.2 90.9±0.1 91.5±0.7 97.9±0.2 91.7±0.7\nKELM f6 95.7±0.4 94.1±0.6 99.2±0.2 85.0±0.3 83.0±0.3 84.9±0.2 91.9±0.2 90.8±0.2 91.1±0.1 92.2±0.7 98.6±0.2 91.3±0.6 f7 95.5±0.4 93.9±0.6 99.1±0.1 85.4±0.3 83.4±0.3 85.3±0.3 92.1±0.2 91.5±0.2 91.5±0.1 91.9±0.6 98.2±0.3 92.2±0.6"
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "In the report, we present a systematic comparison between SVMs and ELMs for object recognition with multiple domains based on the deep convolutional activation features trained by CNN on a subset of 1000-category images from ImageNet. We aim at exploring the most appropriate classifiers for high-level deep features in classification. In experiments, the deep features of 10-category object images of 4 domains from the 6-th layer and 7-th layer of CNN are used as the inputs of general classifiers including NN, SVM, LSSVM, ELM and KELM, respectively. The recognition accuracies for each method under three different experimental settings are reported. A number of experimental results clearly demonstrate that ELMs outperform SVM based classifiers in different settings. In particular, KELM shows state-of-the-art recognition performance among the presented 5 popular classifiers."
    }, {
      "heading" : "ACKNOWLEDGEMENT",
      "text" : "This work was supported in part by the National Natural Science Foundation of China under Grant 61401048 and 61305144, in part by the Hong Kong Scholar Program under\nGrant XJ2013044, and in part by the China Post-Doctoral Science Foundation under Grant 2014M550457."
    } ],
    "references" : [ {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Face recognition: a convolutional neural-network approach",
      "author" : [ "S. Lawrence", "C.L. Giles", "T. Ah Chung", "A.D. Back" ],
      "venue" : "IEEE Transactions on Neural Networks, vol. 8, no. 1, 1997.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science, vol. 313, no. 5786, pp. 504-507, 2006.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G. Hinton", "S. Osindero", "Y. The" ],
      "venue" : "Neural Comput. Vol. 18, no. 7, pp. 1527-1554, 2006.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Deep Boltzman machines",
      "author" : [ "R. Salakhutdinov", "G. Hinton" ],
      "venue" : "Proc. Int’l Conf. Artif. Intell. Statist., pp. 448-455, 2009.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning hierarchical representations for face verification with convolutional deep belief networks",
      "author" : [ "G.B. Huang", "H. Lee", "E. Learned-Miller" ],
      "venue" : "Proc. IEEE Int’l Computer Vision and Pattern Recognition, pp. 2518-2525, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Hybrid Deep Learning for Face Verification",
      "author" : [ "Y. Sun", "X. Wang", "X. Tang" ],
      "venue" : "Proc. IEEE Int’l Conf. Computer Vision, 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
      "author" : [ "Y. Taigman", "M. Yang", "M.A. Ranzato", "L. Wolf" ],
      "venue" : "Proc. IEEE Int’l Computer Vision and Pattern Recognition, 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Naïve-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not?",
      "author" : [ "E. Zhou", "Z. Cao", "Q. Yin" ],
      "venue" : "arXiv: 1501.04690,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS, 2012.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-column deep neural networks for image classification",
      "author" : [ "D. Ciresan", "U. Meier", "J. Schmidhuber" ],
      "venue" : "Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, pp. 3642-3649, 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Large-Scale Video Classification with Convolutional Neural Networks",
      "author" : [ "A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung" ],
      "venue" : "Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, pp. 1725-1732, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition",
      "author" : [ "A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson" ],
      "venue" : "Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, pp. 512-519, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Accurate Object Detection and Semantic Segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, pp. 580-587, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv: 1406.4729.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1406
    }, {
      "title" : "What is the best multi-stage architecture for object recognition?",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "arXiv: 1310.1531, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Nearest neighbor pattern classification",
      "author" : [ "T. Cover", "P. Hart" ],
      "venue" : "IEEE Trans. Information Theory, vol. 13, no. 1, pp. 21-27, 1967.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Statistical learning theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "John Wiley: New York, 1998.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Least Squares Support Vector Machine Classifiers",
      "author" : [ "J.A.K. Suykens", "J. Vandewalle" ],
      "venue" : "Neural Processing Letters, vol. 9, no. 3, pp. 293-300, 1999.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Extreme learning machine: Theory and applications",
      "author" : [ "G.B. Huang", "Q.Y. Zhu", "C.K. Siew" ],
      "venue" : "Neurocomputing, vol. 70, pp. 489-501, 2006.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Extreme Learning Machine for Regression and Multiclass Classification",
      "author" : [ "G.B. Huang", "H. Zhou", "X. Ding", "R. Zhang" ],
      "venue" : "IEEE Trans. Systems, Man, Cybernetics: Part B, vol. 42, no. 2, pp. 513-529, 2012.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Optimization method based on extreme learning machine for classification",
      "author" : [ "G.B. Huang", "X.J. Ding", "H.M. Zhou" ],
      "venue" : "Neurocomputing, vol. 74, no. 1-3, pp. 155-163, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Evolutionary Cost-sensitive Extreme Learning Machine and Subspace Extension",
      "author" : [ "L. Zhang", "D. Zhang" ],
      "venue" : "arXiv:1505.04373, 2015.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "What are Extreme Learning Machines? Filling the Gap between Frank Rosenblatt’s Dream and John von Neumann’s Puzzle",
      "author" : [ "G.B. Huang" ],
      "venue" : "Cognitive Computation, vol. 7, pp. 263-278, 2015.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Representational Learning with Extreme Learning Machine for Big Data",
      "author" : [ "L.L.C. Kasun", "H. Zhou", "G.B. Huang", "C.M. Vong" ],
      "venue" : "IEEE Intelligent Systems, vol. 28, no. 6, pp. 31-34, 2013.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Local Receptive Fields Based Extreme Learning Machine",
      "author" : [ "G.-B. Huang", "Z. Bai", "L.L.C. Kasun", "C.M. Vong" ],
      "venue" : "IEEE Computational Intelligence Magazine, vol. 10, no. 2, pp. 18-29, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Domain Adaptation Transfer Extreme Learning Machines",
      "author" : [ "L. Zhang", "D. Zhang" ],
      "venue" : "Proceedings in Adaptation, Learning and Optimization, vol. 3, pp. 103-119, 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Domain Adaptation Extreme Learning Machines for Drift Compensation in E-nose Systems",
      "author" : [ "L. Zhang", "D. Zhang" ],
      "venue" : "IEEE Transactions on Instrumentation and Measurement, vol. 64, no. 7, pp. 1790-1801, 2015.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1801
    }, {
      "title" : "Semi-Supervised and Unsupervised Extreme Learning Machines",
      "author" : [ "G. Huang", "S. Song", "J.N.D. Gupta", "C. Wu" ],
      "venue" : "IEEE Transactions on Cybernetics, vol. 44, no. 12, pp. 2405-2417, 2014.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell" ],
      "venue" : "ECCV, 2010.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Geodesic flow kernel for unsupervised domain adaptation",
      "author" : [ "B. Gong", "Y. Shi", "F. Sha", "K. Grauman" ],
      "venue" : "CVPR, pp. 2066-2073, 2012.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In the early, convolutional neural network (CNN), as the most important deep net in deep learning, has been applied to document recognition and face recognition [1, 2].",
      "startOffset" : 161,
      "endOffset" : 167
    }, {
      "referenceID" : 1,
      "context" : "In the early, convolutional neural network (CNN), as the most important deep net in deep learning, has been applied to document recognition and face recognition [1, 2].",
      "startOffset" : 161,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "multi-layer perceptrons, MLP) for auto-encoder have been proposed, for examples, stacked auto encoders (SAE) [3], deep belief networks (DBN) [4] and deep Boltzmann machines (DBM) [5].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "multi-layer perceptrons, MLP) for auto-encoder have been proposed, for examples, stacked auto encoders (SAE) [3], deep belief networks (DBN) [4] and deep Boltzmann machines (DBM) [5].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "multi-layer perceptrons, MLP) for auto-encoder have been proposed, for examples, stacked auto encoders (SAE) [3], deep belief networks (DBN) [4] and deep Boltzmann machines (DBM) [5].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "In the latest progress of deep learning, researchers have broken the new record in face verification by using CNNs with different structures [6, 7, 8, 9].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "In the latest progress of deep learning, researchers have broken the new record in face verification by using CNNs with different structures [6, 7, 8, 9].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "In the latest progress of deep learning, researchers have broken the new record in face verification by using CNNs with different structures [6, 7, 8, 9].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 8,
      "context" : "In the latest progress of deep learning, researchers have broken the new record in face verification by using CNNs with different structures [6, 7, 8, 9].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "hk) achieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "hk) achieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "hk) achieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "hk) achieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "hk) achieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "hk) achieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : "hk) achieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "hk) achieved very competitive results on ImageNet for image classification and Pascal VOC data [10-17].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 17,
      "context" : "Therefore, we mainly exploit the nearest neighbor (NN) [18], support vector machine (SVM) [19], least-square support vector machine (LSSVM) [20], extreme learning machine (ELM) [21] and kernel extreme learning machine (KELM) [22].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "Therefore, we mainly exploit the nearest neighbor (NN) [18], support vector machine (SVM) [19], least-square support vector machine (LSSVM) [20], extreme learning machine (ELM) [21] and kernel extreme learning machine (KELM) [22].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "Therefore, we mainly exploit the nearest neighbor (NN) [18], support vector machine (SVM) [19], least-square support vector machine (LSSVM) [20], extreme learning machine (ELM) [21] and kernel extreme learning machine (KELM) [22].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 20,
      "context" : "Therefore, we mainly exploit the nearest neighbor (NN) [18], support vector machine (SVM) [19], least-square support vector machine (LSSVM) [20], extreme learning machine (ELM) [21] and kernel extreme learning machine (KELM) [22].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 21,
      "context" : "Therefore, we mainly exploit the nearest neighbor (NN) [18], support vector machine (SVM) [19], least-square support vector machine (LSSVM) [20], extreme learning machine (ELM) [21] and kernel extreme learning machine (KELM) [22].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 22,
      "context" : "With similar impact with SVM, it has been proved to be efficient and effective for regression and classification tasks [23, 24].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 23,
      "context" : "With similar impact with SVM, it has been proved to be efficient and effective for regression and classification tasks [23, 24].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 24,
      "context" : "The latest work about the principles and brain-alike learning of ELM has been presented [25].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : "The newest work about ELM for deep auto-encoder, local receptive fields for deep learning, transfer learning, and semi-supervised learning have also been proposed [26, 27, 28, 29, 30].",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 26,
      "context" : "The newest work about ELM for deep auto-encoder, local receptive fields for deep learning, transfer learning, and semi-supervised learning have also been proposed [26, 27, 28, 29, 30].",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 27,
      "context" : "The newest work about ELM for deep auto-encoder, local receptive fields for deep learning, transfer learning, and semi-supervised learning have also been proposed [26, 27, 28, 29, 30].",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 28,
      "context" : "The newest work about ELM for deep auto-encoder, local receptive fields for deep learning, transfer learning, and semi-supervised learning have also been proposed [26, 27, 28, 29, 30].",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 29,
      "context" : "The newest work about ELM for deep auto-encoder, local receptive fields for deep learning, transfer learning, and semi-supervised learning have also been proposed [26, 27, 28, 29, 30].",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "With the Mercer condition applied, a kernel ELM (KELM) that computes a kernel matrix of hidden layers has also been proposed [22].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "More details can be referred to [19].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "The details can be referred to [20].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "Therefore, we adopt the deep convolutional activated features (DeCAF) from [17] for experiments.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "The structures of CNN for training on the ImageNet with 1000 categories are the same as the proposed CNN in [10].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "Further details of the CNN training architecture and features can be referred to [10, 17].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "Further details of the CNN training architecture and features can be referred to [10, 17].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 30,
      "context" : "1 are used for deep representation of the 4DA (domain adaptation) dataset [31, 32].",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "1 are used for deep representation of the 4DA (domain adaptation) dataset [31, 32].",
      "startOffset" : 74,
      "endOffset" : 82
    } ],
    "year" : 2015,
    "abstractText" : "Abstract—Deep learning with a convolutional neural network (CNN) has been proved to be very effective in feature extraction and representation of images. For image classification problems, this work aim at finding which classifier is more competitive based on high-level deep features of images. In this report, we have discussed the nearest neighbor, support vector machines and extreme learning machines for image classification under deep convolutional activation feature representation. Specifically, we adopt the benchmark object recognition dataset from multiple sources with domain bias for evaluating different classifiers. The deep features of the object dataset are obtained by a well-trained CNN with five convolutional layers and three fully-connected layers on the challenging ImageNet. Experiments demonstrate that the ELMs outperform SVMs in cross-domain recognition tasks. In particular, state-of-the-art results are obtained by kernel ELM which outperforms SVMs with about 4% of the average accuracy. The features and codes are available in http://www.escience.cn/people/lei/index.html",
    "creator" : null
  }
}