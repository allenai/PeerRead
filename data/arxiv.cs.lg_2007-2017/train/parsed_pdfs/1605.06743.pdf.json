{
  "name" : "1605.06743.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Inductive Bias of Deep Convolutional Networks through Pooling Geometry",
    "authors" : [ "Nadav Cohen", "Amnon Shashua" ],
    "emails" : [ "cohennadav@cs.huji.ac.il", "shashua@cs.huji.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A central factor in the application of machine learning to a given task is the inductive bias, i.e. the choice of hypotheses space from which learned functions are taken. The restriction posed by the inductive bias is necessary for practical learning, and reflects prior knowledge regarding the task at hand. Perhaps the most successful exemplar of inductive bias to date manifests itself in the use of convolutional networks ([18]) for computer vision tasks. These hypotheses spaces are delivering unprecedented visual recognition results (e.g. [17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]). Unfortunately, our formal understanding of the inductive bias behind convolutional networks is limited – the assumptions encoded into these models, which seem to form an excellent prior knowledge for imagery data, are for the most part a mystery.\nExisting works studying the inductive bias of deep networks (not necessarily convolutional) do so in the context of depth efficiency, essentially arguing that for a given amount of resources, more layers result in higher expressiveness. More precisely, depth efficiency refers to a situation where a function realized by a deep network of polynomial size, requires exponential size in order to be realized (or approximated) by a shallower network. In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]). Nonetheless, despite the wide attention it is receiving, depth efficiency does not convey the complete story behind the inductive bias of deep networks. While it does suggest that depth brings forth functions that are otherwise unattainable, it does not explain why these functions are useful. Loosely speaking, the hypotheses space of a polynomially sized deep\nar X\niv :1\n60 5.\n06 74\n3v 1\n[ cs\n.N E\n] 2\n2 M\nay 2\nnetwork covers a small fraction of the space of all functions. We would like to understand why this small fraction is so successful in practice.\nA specific family of convolutional networks gaining increased attention is that of convolutional arithmetic circuits. These models follow the standard paradigm of locality, weight sharing and pooling, yet differ from the most conventional convolutional networks in that their point-wise activations are linear, with non-linearity originating from product pooling. Recently, [8] analyzed the depth efficiency of convolutional arithmetic circuits, showing that besides a negligible set, all functions realizable by a deep network require exponential size in order to be realized (or approximated) by a shallow one. This result, termed complete depth efficiency, stands in contrast to previous results along this line, which merely showed existence of functions efficiently realizable by deep networks but not by shallow ones. In subsequent work, [6] proved that for the popular convolutional rectifier networks, i.e. convolutional networks with rectified linear (ReLU, [22]) activation and max or average pooling, depth efficiency exists but it is not complete, meaning such networks are inferior to convolutional arithmetic circuits in terms of depth efficiency. Motivated by these results, and by the fact that convolutional arithmetic circuits are equivalent to SimNets, a new deep learning architecture that has recently demonstrated promising empirical performance ([5, 7]), we focus in this paper on convolutional arithmetic circuits as representatives of the class of convolutional networks.\nWe approach the study of inductive bias from the direction of function inputs. Specifically, we study the ability of convolutional arithmetic circuits to model correlation between regions of their input. To analyze the correlations of a function, we consider different partitions of input regions into disjoint sets, and ask how separable the function is w.r.t. these partitions. Separability of a function w.r.t. a partition of its input is measured through the notion of separation rank ([2]), which can be viewed as quantifying how far the function is from being equal to a product of factors, each depending on input regions from only one side of the partition. High separation rank (low separability) implies that the function induces strong correlation between sides of the partition, and vice versa.\nOur analysis shows that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial or linear (in network size) separation ranks for others. The network’s pooling geometry effectively determines which input partitions are favored in terms of separation rank, i.e. which partitions enjoy exponentially high separation ranks with polynomially sized networks, and which require networks to be exponentially large. With the standard choice of square contiguous pooling windows, interleaved (entangled) partitions are favored over coarse ones that divide the input into large distinct areas. We thus conclude that in terms of modeled correlations, pooling geometry controls the inductive bias, and the particular design commonly employed in practice orients it towards the statistics of natural images (nearby pixels more correlated than distant ones).\nWith regards to depth efficiency, we show that separation ranks under favored input partitions are exponentially high for all but a negligible set of functions realizable by a deep network. Shallow networks on the other hand, treat all partitions equally and support only linear (in network size) separation ranks. Therefore, almost all functions that may be realized by a deep network require a replicating shallow network to have exponential size. By this we return to the complete depth efficiency result of [8], but with an added important insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.\nThe remainder of the paper is organized as follows. Sec. 2 provides a brief presentation of necessary background material from the field of tensor analysis. Sec. 3 describes the convolutional arithmetic circuits we analyze, and their relation to tensor decompositions. In sec. 4 we convey the concept of separation rank, on which we base our analyses in sec. 5 and 6. Finally, sec. 7 concludes."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "The analyses carried out in this paper rely on concepts and results from the field of tensor analysis. In this section we establish the minimal background required in order to follow our arguments 1 , referring the interested reader to [12] for a broad and comprehensive introduction to the field.\n1 The definitions we give are actually concrete special cases of more abstract algebraic definitions as given in [12]. We limit the discussion to these special cases since they suffice for our needs and are easier to grasp.\nThe core concept in tensor analysis is a tensor, which for our purposes may simply be thought of as a multi-dimensional array. The order of a tensor is defined to be the number of indexing entries in the array, which are referred to as modes. The dimension of a tensor in a particular mode is defined as the number of values that may be taken by the index in that mode. For example, a 4-by-3 matrix is a tensor of order 2, i.e. it has two modes, with dimension 4 in mode 1 and dimension 3 in mode 2. If A is a tensor of order N and dimension Mi in each mode i ∈ [N ] := {1, . . . , N}, the space of all configurations it can take is denoted, quite naturally, by RM1×···×MN . A fundamental operator in tensor analysis is the tensor product, which we denote by ⊗. It is an operator that intakes two tensors A ∈ RM1×···×MP and B ∈ RMP+1×···×MP+Q (orders P and Q respectively), and returns a tensor A ⊗ B ∈ RM1×···×MP+Q (order P + Q) defined by: (A ⊗ B)d1,...,dP+Q = Ad1,...,dP · BdP+1,...,dP+Q . Notice that in the case P = Q = 1, the tensor product reduces to the standard outer product between vectors, i.e. if u ∈ RM1 and v ∈ RM2 , then u⊗ v is no other than the rank-1 matrix uv> ∈ RM1×M2 . We now introduce the important concept of matricization, which is essentially the rearrangement of a tensor as a matrix. Suppose A is a tensor of order N and dimension Mi in each mode i ∈ [N ], and let (I, J) be a partition of [N ], i.e. I and J are disjoint subsets of [N ] whose union gives [N ]. We may write I = {i1, . . . , i|I|} where i1 < · · · < i|I|, and similarly J = {j1, . . . , j|J|} where j1 < · · · < j|J|. The matricization of A w.r.t. the partition (I, J), denoted JAKI,J , is the∏|I| t=1Mit-by∏|J| t=1Mjt matrix holding the entries of A such that Ad1,...,dN is placed in row index\n1 + ∑|I| t=1(dit − 1) ∏|I| t′=t+1Mit′ and column index 1 + ∑|J| t=1(djt − 1) ∏|J| t′=t+1Mjt′ . If I = ∅ or\nJ = ∅, then by definition JAKI,J is a row or column (respectively) vector of dimension ∏N t=1Mt\nholding Ad1,...,dN in entry 1 + ∑N t=1(dt − 1) ∏N t′=t+1Mt′ .\nA well known matrix operator is the Kronecker product, which we denote by . For two matrices A ∈ RM1×M2 and B ∈ RN1×N2 , A B is the matrix in RM1N1×M2N2 holding AijBkl in row index (i−1)N1+k and column index (j−1)N2+ l. LetA and B be tensors of orders P andQ respectively, and let (I, J) be a partition of [P +Q]. The basic relation that binds together the tensor product, the matricization operator, and the Kronecker product, is:\nJA⊗ BKI,J = JAKI∩[P ],J∩[P ] JBK(I−P )∩[Q],(J−P )∩[Q] (1) where I − P and J − P are simply the sets obtained by subtracting P from each of the elements in I and J respectively. In words, eq. 1 implies that the matricization of the tensor product betweenA and B w.r.t. the partition (I, J) of [P +Q], is equal to the Kronecker product between two matricizations: that of A w.r.t. the partition of [P ] induced by the lower values of (I, J), and that of B w.r.t. the partition of [Q] induced by the higher values of (I, J)."
    }, {
      "heading" : "3 Convolutional arithmetic circuits",
      "text" : "The convolutional arithmetic circuit architecture on which we focus in this paper is the one considered in [8], portrayed in fig. 1(a). Instances processed by a network are represented as N -length sequences of s-dimensional vectors. They are generally thought of as images, with the s-dimensional vectors corresponding to local patches. For example, instances could be 32-by-32 RGB images, with local patches being 5× 5 regions crossing the three color bands. In this case, assuming a patch is taken around every pixel in an image (boundaries padded), we have N = 1024 and s = 75. Throughout the paper, we denote a general instance by X = (x1, . . . ,xN ), with xi ∈ Rs standing for its patches. The first layer processing an instance is referred to as representation. It consists of applying M representation functions fθ1 . . .fθM : Rs → R to all patches, thereby creating M feature maps. In the case where representation functions are chosen as fθd(x) = σ(w > d x + bd), with parameters θd = (wd, bd) ∈ Rs × R and some point-wise activation σ(·), the representation layer reduces to a standard convolutional layer. More elaborate settings are also possible, for example modeling the representation as a cascade of convolutional layers with pooling in-between. Following the representation, a network includes L hidden layers indexed by l = 0. . .L− 1. Each hidden layer l begins with a 1× 1 conv operator, which is simply a three-dimensional convolution with rl channels and 1× 1 filter spatiality. This is followed by spatial pooling, that decimates feature maps by taking products of non-overlapping two-dimensional windows that cover the spatial extent. The last of the L hidden layers (l = L− 1) reduces feature maps to singletons (its pooling operator is global), creating\na vector of dimension rL−1. This vector is mapped into Y network outputs through a final dense linear layer.\nAltogether, the architectural parameters of a network are the type of representation functions (fθd ), the pooling window shapes and sizes (which in turn determine the number of hidden layers L), and the number of channels in each layer (M for representation, r0. . .rL−1 for hidden layers, Y for output). Given these architectural parameters, the learnable parameters of a network are the representation weights (θd for channel d), the conv weights (al,γ for channel γ of hidden layer l), and the output weights (aL,y for output node y).\nFor a particular setting of weights, every node (neuron) in a given network realizes a function from (Rs)N to R. The receptive field of a node refers to the indexes of input patches on which its function may depend. For example, the receptive field of node j in channel γ of conv operator at hidden layer 0 is {j}, and that of an output node is [N ], corresponding to the entire input. Denote by h(l,γ,j) the function realized by node j of channel γ in conv operator at hidden layer l, and let I(l,γ,j) ⊂ [N ] be its receptive field. By the structure of the network it is evident that I(l,γ,j) does not depend on γ, so we may write I(l,j) instead. Moreover, assuming pooling windows are uniform across channels (as customary for convolutional networks), and taking into account the fact that they do not overlap, we conclude that I(l,j1) and I(l,j2) are necessarily disjoint if j1 6=j2. A simple induction over l = 0. . .L − 1 then shows that h(l,γ,j) may be expressed as h(l,γ,j)(xi1 , . . . ,xiT ) = ∑M d1...dT=1 A(l,γ,j)d1...dT ∏T t=1 fθdt (xit), where {i1, . . . , iT } stands for the receptive field I(l,j), and A(l,γ,j) is a tensor of order T = |I(l,j)| and dimension M in each mode, with entries given by polynomials in the network’s conv weights ({al,γ}l,γ). Taking the induction one step further (from last hidden layer to network output), we obtain the following expression for functions realized by network outputs:\nhy (x1, . . . ,xN ) = ∑M\nd1...dN=1 Ayd1,...,dN ∏N i=1 fθdi (xi) (2)\ny ∈ [Y ] here is an output node index, and hy is the function realized by that node. Ay is a tensor of order N and dimension M in each mode, with entries given by polynomials in the network’s conv weights {al,γ}l,γ and output weights aL,y . Hereafter, terms such as function realized by a network or coefficient tensor realized by a network, are to be understood as referring to hy or Ay respectively. Next, we present explicit expressions for Ay under two canonical networks – deep and shallow.\nDeep network. Consider a network as in fig. 1(a), with pooling windows set to cover four entries each, resulting in L = log4N hidden layers. The linear weights of such a network are {a0,γ ∈ RM}γ∈[r0] for conv operator in hidden layer 0, {al,γ ∈ Rrl−1}γ∈[rl] for conv operator in hidden layer l = 1. . .L− 1, and {aL,y ∈ RrL−1}y∈[Y ] for dense output operator. They determine the coefficient tensor Ay (eq. 2) through the following recursive decomposition:\nφ1,γ︸︷︷︸ order 4\n= ∑r0\nα=1 a1,γα · ⊗4a0,α , γ ∈ [r1]\n· · · φl,γ︸︷︷︸\norder 4l\n= ∑rl−1\nα=1 al,γα · ⊗4φl−1,α , l ∈ {2. . .L− 1}, γ ∈ [rl]\n· · · Ay︸︷︷︸\norder 4L N\n= ∑rL−1\nα=1 aL,yα · ⊗4φL−1,α (3)\nal,γα and a L,y α here are scalars representing entry α in the vectors a l,γ and aL,y respectively, and the symbol ⊗ with a superscript stands for a repeated tensor product, e.g. ⊗4a0,α := a0,α ⊗ a0,α ⊗ a0,α ⊗ a0,α. To verify that under pooling windows of size four Ay is indeed given by eq. 3, simply plug the rows of the decomposition into eq. 2, starting from bottom and continuing upwards. For context, eq. 3 describes what is called a hierarchical tensor decomposition (see chapter 11 in [12]), with underlying tree over modes being a full quad-tree (corresponding to the fact that the network’s pooling windows cover four entries each).\nShallow network. The second network we pay special attention to is shallow, comprising a single hidden layer with global pooling – see illustration in fig. 1(b). The linear weights of such a network are {a0,γ ∈ RM}γ∈[r0] for hidden conv operator and {a1,y ∈ Rr0}y∈[Y ] for dense output operator. They determine the coefficient tensor Ay (eq. 2) as follows:\nAy = ∑r0\nγ=1 a1,yγ · ⊗Na0,γ (4)\nwhere a1,yγ stands for entry γ of a 1,y , and again, the symbol⊗ with a superscript represents a repeated tensor product. The tensor decomposition in eq. 4 is an instance of the classic CP decomposition, also known as rank-1 decomposition (see [16] for a historic survey).\nTo conclude this section, we relate the background material above, as well as our contribution described in the upcoming sections, to the work of [8]. The latter shows that for arbitrary coefficient tensors Ay, functions hy as in eq. 2 form a universal hypotheses space. It is then shown that convolutional arithmetic circuits as in fig. 1(a) realize such functions by applying tensor decompositions to Ay, with the type of decomposition determined by the structure of a network (number of layers, number of channels in each layer etc.). The deep network (fig. 1(a) with size-4 pooling windows and L = log4N hidden layers) and the shallow network (fig. 1(b)) presented hereinabove are two special cases, whose corresponding tensor decompositions are given in eq. 3 and 4 respectively. The central result in [8] relates to inductive bias through the notion of depth efficiency – it is shown that in the parameter space of a deep network, all weight settings but a set of measure zero give rise to functions that can only be realized (or approximated) by a shallow network if the latter has exponential size. This result does not relate to the characteristics of instances X = (x1, . . . ,xN ), it only treats the ability of shallow networks to replicate functions realized by deep networks.\nIn this paper we draw a line connecting the inductive bias to the nature of X , by studying the relation between a network’s architecture and its ability to model correlation among patches xi. Specifically, in sec. 4 we consider partitions (I, J) of [N ] (I ·∪J = [N ], where ·∪ stands for disjoint union), and present the notion of separation rank as a measure of the correlation modeled between patches indexed by I and ones indexed by J . In sec. 5.1 the separation rank of a network’s function hy w.r.t. a partition (I, J) is proven to be equal to the rank of JAyKI,J – the matricization w.r.t. (I, J) of the coefficient tensor Ay. Sec. 5.2 derives lower and upper bounds on this rank for a deep network, showing that it supports exponential separation ranks with polynomial size for certain partitions, whereas for others it is required to be exponentially large. Subsequently, sec. 5.3 establishes an upper bound on rankJAyKI,J for shallow networks, implying that these must be exponentially large in order to model exponential separation rank under any partition, and thus cannot efficiently replicate a deep network’s correlations. Our analysis is concluded in sec. 6, which discusses the pooling geometry of a deep network as a means for controlling the inductive bias by determining a correspondence between partitions (I, J) and spatial partitions of the input."
    }, {
      "heading" : "4 Separation rank",
      "text" : "In this section we define the concept of separation rank for functions realized by convolutional arithmetic circuits (sec. 3), i.e. functions that take as input X = (x1, . . . ,xN ) ∈ (Rs)N . The separation rank serves as a measure of the correlations such functions induce between different sets of input patches, i.e. different subsets of the variable set {x1, . . . ,xN}. Let (I, J) be a partition of input indexes, i.e. I and J are disjoint subsets of [N ] whose union gives [N ]. We may write I = {i1, . . . , i|I|} where i1 < · · · < i|I|, and similarly J = {j1, . . . , j|J|} where j1 < · · · < j|J|. For a function h : (Rs)N → R, the separation rank w.r.t. the partition (I, J) is defined as follows: 2\nsep(h; I, J) := min { R ∈ N ∪ {0} : ∃g1. . .gR : (Rs)|I| → R, g′1. . .g′R : (Rs)|J| → R s.t. (5)\nh(x1, . . . ,xN ) = ∑R\nν=1 gν(xi1 , . . . ,xi|I|)g\n′ ν(xj1 , . . . ,xj|J|) } In words, it is the minimal number of summands that together give h, where each summand is equal to a product of two functions – one that intakes only patches indexed by I , and another that\n2 If I = ∅ or J = ∅ then by definition sep(h; I, J) = 1 (unless h ≡ 0, in which case sep(h; I, J) = 0).\nintakes only patches indexed by J . One may wonder if it is at all possible to express h through such summands, i.e. if the separation rank of h is finite. From the theory of tensor products between L2 spaces (see [12] for a comprehensive coverage), we know that any h∈L2((Rs)N ), i.e. any h that is measurable and square integrable, may be approximated arbitrarily well by summations of the form∑R ν=1 gν(xi1 , . . . ,xi|I|)g ′ ν(xj1 , . . . ,xj|J|). Exact realization however is only guaranteed at the limit R → ∞, thus in general the separation rank of h need not be finite. Nonetheless, as we show in sec. 5, for the class of functions we are interested in, namely functions realizable by convolutional arithmetic circuits, separation ranks are always finite.\nThe concept of separation rank was introduced in [2] for numerical treatment of high-dimensional functions. Since then, it has been employed for various applications, including quantum chemistry ([13]), particle engineering ([11]), machine learning ([3]), and more. The separation rank measures how separable function variables are, or conversely, how correlated they are in determining function values. When separation rank equals 1, function variables are completely separable, meaning they are completely uncorrelated in determining function values. On the other hand, high separation rank indicates that function variables are difficult to separate, i.e. the function induces strong correlation between them. In our context, the separation rank of a function h : (Rs)N → R w.r.t. the partition (I, J) of [N ] (eq. 5), measures the strength of the modeled correlation between two sets of input patches – those indexed by I (xi1 . . .xi|I| ), and those indexed by J (xj1 . . .xj|J| )."
    }, {
      "heading" : "5 Correlation analysis",
      "text" : "In this section we analyze convolutional arithmetic circuits (sec. 3) in terms of the correlations they model between sides of different input partitions, i.e. in terms of the separation ranks (sec. 4) they support under different partitions (I, J) of [N ]. We begin in sec. 5.1, establishing a correspondence between separation ranks and coefficient tensor matricization ranks. This correspondence is then used in sec. 5.2 and 5.3 to analyze the deep and shallow networks (respectively) presented in sec. 3. We note that we focus on these particular networks merely for simplicity of presentation – the analysis can easily be adapted to account for alternative networks with different depths and pooling schemes."
    }, {
      "heading" : "5.1 From separation rank to matricization rank",
      "text" : "Let hy be a function realized by a convolutional arithmetic circuit, with corresponding coefficient tensor Ay (eq. 2). Denote by (I, J) an arbitrary partition of [N ], i.e. I ·∪J = [N ]. We are interested\nin studying sep(hy; I, J) – the separation rank of hy w.r.t. (I, J) (eq. 5). As claim 1 below states, assuming representation functions {fθd}d∈[M ] are linearly independent (there is no reason to choose them otherwise 3 ), this separation rank is equal to the rank of JAyKI,J – the matricization of the coefficient tensor Ay w.r.t. the partition (I, J). Our problem thus translates to studying ranks of matricized coefficient tensors. Claim 1. Let hy be a function realized by a convolutional arithmetic circuit (fig. 1(a)), with corresponding coefficient tensor Ay (eq. 2). Assume that the network’s representation functions (fθd) are linearly independent, and consider an arbitrary partition (I, J) of [N ]. Then sep(hy; I, J) = rankJAyKI,J .\nProof. See app. A.1.\nAs the linear weights of a network vary, so do the coefficient tensors it gives rise to (Ay). Accordingly, for a particular partition (I, J), a network does not correspond to a single value of rankJAyKI,J , but rather supports a range of values. We analyze this range by quantifying its maximum, which reflects the strongest correlation that the network can model between the input patches indexed by I and those indexed by J . One may wonder if the maximal value of rankJAyKI,J is the appropriate statistic to measure, as a-priori, it may be that rankJAyKI,J is maximal for very few of the network’s weight settings, and much lower for all the rest. Apparently, as claim 2 below shows, this is not the case, and in fact rankJAyKI,J is maximal under almost all of the network’s weight settings. Claim 2. Consider a convolutional arithmetic circuit (fig. 1(a)) with corresponding coefficient tensor Ay (eq. 2). Ay depends on the network’s linear weights – {al,γ}l,γ and aL,y, thus for a given partition (I, J) of [N ], rankJAyKI,J is a function of these weights. This function obtains its maximum almost everywhere (w.r.t. Lebesgue measure).\nProof. See app. A.2."
    }, {
      "heading" : "5.2 Deep network",
      "text" : "In this subsection we study correlations modeled by the deep network presented in sec. 3 (fig. 1(a) with size-4 pooling windows and L = log4N hidden layers). In accordance with sec. 5.1, we do so by characterizing the maximal ranks of coefficient tensor matricizations under different partitions.\nRecall from eq. 3 the hierarchical decomposition expressing a coefficient tensor Ay realized by the deep network. We are interested in matricizations of this tensor under different partitions of [N ]. Let (I, J) be an arbitrary partition, i.e. I ·∪J = [N ]. Matricizing the last row of eq. 3 w.r.t. (I, J), while applying the relation in eq. 1, gives:\nJAyKI,J = ∑rL−1\nα=1 aL,yα ·\nq φL−1,α ⊗ φL−1,α ⊗ φL−1,α ⊗ φL−1,α y I,J\n= ∑rL−1\nα=1 aL,yα ·\nq φL−1,α ⊗ φL−1,α y I∩[2·4L−1],J∩[2·4L−1]\nq φL−1,α ⊗ φL−1,α y (I−2·4L−1)∩[2·4L−1],(J−2·4L−1)∩[2·4L−1]\nApplying eq. 1 again, this time to matricizations of the tensor φL−1,α ⊗ φL−1,α, we obtain: JAyKI,J = ∑rL−1\nα=1 aL,yα ·\nq φL−1,α y I∩[4L−1],J∩[4L−1]\nq φL−1,α y (I−4L−1)∩[4L−1],(J−4L−1)∩[4L−1] q φL−1,α\ny (I−2·4L−1)∩[4L−1],(J−2·4L−1)∩[4L−1]\nq φL−1,α y (I−3·4L−1)∩[4L−1],(J−3·4L−1)∩[4L−1]\nFor every k ∈ [4] define IL−1,k := (I − (k − 1) · 4L−1) ∩ [4L−1] and JL−1,k := (J − (k − 1) · 4L−1) ∩ [4L−1]. In words, (IL−1,k, JL−1,k) represents the partition induced by (I, J) on the k’th\n3 The case of linearly dependent representation functions is degenerate, equivalent to reducing the number of channels in the representation layer (M ).\nquadrant of [N ], i.e. on the k’th size-4L−1 group of input patches. We now have the following matricized version of the last level in eq. 3:\nJAyKI,J = ∑rL−1\nα=1 aL,yα · 4 t=1 JφL−1,αKIL−1,t,JL−1,t\nwhere the symbol with a running index stands for an iterative Kronecker product. To derive analogous matricized versions for the upper levels of eq. 3, we define for l ∈ {0. . .L−1}, k ∈ [N/4l]:\nIl,k := (I − (k − 1) · 4l) ∩ [4l] Jl,k := (J − (k − 1) · 4l) ∩ [4l] (6) That is to say, (Il,k, Jl,k) represents the partition induced by (I, J) on the set of indexes {(k−1) ·4l+ 1, . . . , k · 4l}, i.e. on the k’th size-4l group of input patches. With this notation in hand, traversing upwards through the levels of eq. 3, with repeated application of the relation in eq. 1, one arrives at the following matrix decomposition for JAyKI,J :\nJφ1,γKI1,k,J1,k︸ ︷︷ ︸ M |I1,k|-by-M |J1,k|\n= ∑r0\nα=1 a1,γα · 4 t=1 Ja0,αKI0,4(k−1)+t,J0,4(k−1)+t , γ ∈ [r1]\n· · ·\nJφl,γKIl,k,Jl,k︸ ︷︷ ︸ M |Il,k|-by-M |Jl,k|\n= ∑rl−1\nα=1 al,γα · 4 t=1 Jφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t , l ∈ {2. . .L− 1}, γ ∈ [rl]\n· · ·\nJAyKI,J︸ ︷︷ ︸ M |I|-by-M |J|\n= ∑rL−1\nα=1 aL,yα · 4 t=1 JφL−1,αKIL−1,t,JL−1,t (7)\nEq. 7 expresses JAyKI,J – the matricization w.r.t. the partition (I, J) of a coefficient tensorAy realized by the deep network, in terms of the network’s conv weights {al,γ}l,γ and output weights aL,y. As discussed in sec. 5.1, our interest lies in the maximal rank that this matricization can take. Theorem 1 below provides lower and upper bounds on this maximal rank, by making use of eq. 7, and of the rank-multiplicative property of the Kronecker product (rank(A B) = rank(A)·rank(B)). Theorem 1. Let (I, J) be a partition of [N ], and JAyKI,J be the matricization w.r.t. (I, J) of a coefficient tensor Ay (eq. 2) realized by the deep network (fig. 1(a) with size-4 pooling windows). For every l ∈ {0. . .L− 1} and k ∈ [N/4l], define Il,k and Jl,k as in eq. 6. Then, the maximal rank that JAyKI,J can take (when network weights vary) is:\n• No smaller than min{r0,M}S , where S := |{k ∈ [N/4] : I1,k 6= ∅ ∧ J1,k 6= ∅}|.\n• No greater than min{Mmin{|I|,|J|}, rL−1 ∏4 t=1 c\nL−1,t}, where c0,k := 1 for k ∈ [N ], and cl,k := min{Mmin{|Il,k|,|Jl,k|}, rl−1 ∏4 t=1 c l−1,4(k−1)+t} for l ∈ [L− 1], k ∈ [N/4l].\nProof. See app. A.3.\nThe lower bound in theorem 1 is exponential in S, the latter defined to be the number of size-4 patch groups that are split by the partition (I, J), i.e. whose indexes are divided between I and J . Partitions that split many of the size-4 patch groups will thus lead to a large lower bound. For example, consider the partition (Iodd, Jeven) defined as follows:\nIodd = {1, 3, . . . , N − 1} Jeven = {2, 4, . . . , N} (8)\nThis partition splits all size-4 patch groups (S = N/4), leading to a lower bound that is exponential in the number of patches (N ).\nThe upper bound in theorem 1 is expressed via constants cl,k, defined recursively over levels l = 0. . .L − 1, with k ranging over 1. . .N/4l for each level l. What prevents cl,k from growing double-exponentially fast (w.r.t. l) is the minimization with Mmin{|Il,k|,|Jl,k|}. Specifically, if min{|Il,k| , |Jl,k|} is small, i.e. if the partition induced by (I, J) on the k’th size-4l group of patches is unbalanced (most of the patches belong to one side of the partition, and only a few belong to the\nother), cl,k will be of reasonable size. The higher this takes place in the hierarchy (i.e. the larger l is), the lower our eventual upper bound will be. In other words, if partitions induced by (I, J) on size-4l patch groups are unbalanced for large values of l, the upper bound in theorem 1 will be small. For example, consider the partition (I low, Jhigh) defined by:\nI low = {1, . . . , N/2} Jhigh = {N/2 + 1, . . . , N} (9)\nUnder (I low, Jhigh), all partitions induced on size-4L−1 patch groups (quadrants of [N ]) are completely one-sided (min{|IL−1,k|, |JL−1,k|} = 0 for all k ∈ [4]), resulting in the upper bound being no greater than rL−1 – linear in network size.\nTo summarize this discussion, theorem 1 states that with the deep network, the maximal rank of a coefficient tensor matricization w.r.t. (I, J), highly depends on the nature of the partition (I, J) – it will be exponentially large for partitions such as (Iodd, Jeven), that split many size-4 patch groups, while being only polynomial (or linear) for partitions like (I low, Jhigh), under which size-4l patch groups are unevenly divided for large values of l. Since the rank of a coefficient tensor matricization w.r.t. (I, J) corresponds to the strength of correlation modeled between input patches indexed by I and those indexed by J (sec. 5.1), we conclude that the ability of a polynomially sized deep network to model correlation between sets of input patches highly depends on the nature of these sets."
    }, {
      "heading" : "5.3 Shallow network",
      "text" : "We now turn to study correlations modeled by the shallow network presented in sec. 3 (fig. 1(b)). In line with sec. 5.1, this is achieved by characterizing the maximal ranks of coefficient tensor matricizations under different partitions.\nRecall from eq. 4 the CP decomposition expressing a coefficient tensor Ay realized by the shallow network. For an arbitrary partition (I, J) of [N ], i.e. I ·∪J = [N ], matricizing this decomposition with repeated application of the relation in eq. 1, gives the following expression for JAyKI,J – the matricization w.r.t. (I, J) of a coefficient tensor realized by the shallow network:\nJAyKI,J = ∑r0\nγ=1 a1,yγ ·\n( |I|a0,γ )( |J|a0,γ )> (10)\n|I|a0,γ and |J|a0,γ here are column vectors of dimensions M |I| and M |J| respectively, standing for the Kronecker products of a0,γ ∈ RM with itself |I| and |J | times (respectively). Eq. 10 immediately leads to two observations regarding the ranks that may be taken by JAyKI,J . First, they depend on the partition (I, J) only through its division size, i.e. through |I| and |J |. Second, they are no greater than min{Mmin{|I|,|J|}, r0}, meaning that the maximal rank is linear (or less) in network size. In light of sec. 5.1 and 5.2, these findings imply that in contrast to the deep network, which with polynomial size supports exponential separation ranks under favored partitions, the shallow network treats all partitions (of a given division size) equally, and can only give rise to an exponential separation rank if its size is exponential.\nSuppose now that we would like to use the shallow network to replicate a function realized by a polynomially sized deep network. So long as the deep network’s function realizes an exponential separation rank under at least one of the favored partitions (e.g. (Iodd, Jeven) – eq. 8), the shallow network would have to be exponentially large in order to replicate it 4 , i.e. depth efficiency takes place. Since all but a negligible set of the functions realizable by the deep network give rise to maximal separation ranks (claim 2), we obtain the complete depth efficiency result of [8]. However, unlike [8], which did not provide any explanation for the usefulness of functions brought forth by depth, we obtain an insight into their utility – they are able to efficiently model strong correlation between favored partitions of the input.\n4 Our definition of the shallow network includes weight sharing in its hidden conv operator. By eq. 4, this implies that the network is not universal (may only realize symmetric coefficient tensors), and thus might not be able to replicate the deep network’s function, no matter how large we allow it to be. However, even if we consider the more powerful, universal variant of the shallow network which is not constrained by weight sharing, the exact same results would hold. In particular, the separation ranks of the network would still be linear in its size, and we would still have (complete) depth efficiency."
    }, {
      "heading" : "6 Inductive bias through pooling geometry",
      "text" : "The deep network presented in sec. 3, whose correlations we analyzed in sec. 5.2, was defined as having size-4 pooling windows, i.e. pooling windows covering four entries each. We have yet to specify the shapes of these windows, or equivalently, the spatial (two-dimensional) locations of nodes grouped together in the process of pooling. In compliance with standard convolutional network design, we now assume that the network’s (size-4) pooling windows are contiguous square blocks, i.e. have shape 2× 2. Under this configuration, the network’s functional description (eq. 2 with Ay given by eq. 3) induces a spatial ordering of input patches 5 , which may be described by the following recursive process:\n• Set the index of the top-left patch to 1. • For l = 1, . . ., L = log4N : Replicate the already-assigned top-left 2l−1-by-2l−1 block of\nindexes, and place copies on its right, bottom-right and bottom. Then, add a 4l−1 offset to all indexes in the right copy, a 2 · 4l−1 offset to all indexes in the bottom-right copy, and a 3 · 4l−1 offset to all indexes in the bottom copy.\nWith this spatial ordering (illustrated in fig. 1(c)), partitions (I, J) of [N ] convey a spatial pattern. For example, the partition (Iodd, Jeven) (eq. 8) corresponds to the pattern illustrated on the left of fig. 1(c), whereas (I low, Jhigh) (eq. 9) corresponds to the pattern illustrated on the right. Our analysis (sec. 5.2) shows that the deep network is able to model strong correlation under (Iodd, Jeven), while being inefficient for modeling correlation under (I low, Jhigh). More generally, partitions for which S, defined in theorem 1, is high, convey patterns that split many 2 × 2 patch blocks, i.e. are highly entangled. These partitions enjoy the possibility of strong correlation. On the other hand, partitions for which min{|Il,k| , |Jl,k|} is small for large values of l (see eq. 6 for definition of Il,k and Jl,k) convey patterns that divide large 2l × 2l patch blocks unevenly, i.e. separate the input to distinct contiguous regions. These partitions, as we have seen, suffer from limited low correlations.\nWe conclude that with 2× 2 pooling, the deep network is able to model strong correlation between input regions that are highly entangled, at the expense of being inefficient for modeling correlation between input regions that are far apart. Had we selected a different pooling regime, the preference of input partition patterns in terms of modeled correlation would change. For example, if nodes grouped together by a pooling window were matching elements in the four quadrants of a feature map (in which case pooling windows are highly noncontiguous), the correlations corresponding to the two patterns illustrated in fig. 1(c) would swap – the entangled pattern on the left would now suffer from limited low correlation, whereas the coarse one on the right would enjoy the possibility of strong correlation. The choice of pooling shapes thus serves as a means for controlling the inductive bias in terms of correlations modeled under different input partitions. Square contiguous windows, as commonly employed in practice, lead to a preference of interleaved partitions over coarse ones, in compliance with our intuition regarding the statistics of natural images (nearby pixels are more correlated than distant ones)."
    }, {
      "heading" : "7 Discussion",
      "text" : "Through the notion of separation rank, we studied the relation between the architecture of a convolutional arithmetic circuit, and its ability to model correlations among input regions. For a given input partition, the separation rank quantifies how far a function is from being separable, i.e. from being equal to a product of terms, each depending on input regions from only one side of the partition.\nOur analysis shows that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial or linear (in network size) separation ranks for others. The network’s pooling window shapes effectively determine which input\n5 The network’s functional description assumes a one-dimensional full quad-tree grouping of input patch indexes. That is to say, it assumes that in the first pooling operation (hidden layer 0), the nodes corresponding to patches x1,x2,x3,x4 are pooled into one group, those corresponding to x5,x6,x7,x8 are pooled into another, and so forth. Similar assumptions hold for the deeper layers. For example, in the second pooling operation (hidden layer 1), the node with receptive field {1, 2, 3, 4}, i.e. the one corresponding to the quadruple of patches {x1,x2,x3,x4}, is assumed to be pooled together with the nodes whose receptive fields are {5, 6, 7, 8}, {9, 10, 11, 12} and {13, 14, 15, 16}.\npartitions are favored in terms of separation rank, i.e. which partitions enjoy exponentially high separation ranks with polynomially sized networks, and which require networks to be exponentially large. Pooling geometry thus serves as a means for controlling the inductive bias of deep networks. The particular pooling scheme commonly employed in practice – square contiguous windows, favors interleaved partitions over ones that divide the input to distinct areas, and thus orients the inductive bias towards the statistics of natural images (nearby pixels more correlated than distant ones).\nAs opposed to deep networks, shallow networks support only linear (in network size) separation ranks. Therefore, in order to replicate a function realized by a deep network (exponential separation rank), a shallow network must be exponentially large. By this we derive the depth efficiency result of [8], but in addition, provide an insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.\nFinally, the selection of correlation-wise favored input partitions through a deep network’s pooling geometry, enables tailoring the inductive bias to data that does not necessarily comply with the statistics of natural images. Suppose for example that we would like to detect symmetry between the left and right halves of a CT scan. With the standard arrangement of pooling windows (contiguous square blocks), the left-vs.-right halving partition is unfavored, and thus a very large network would be required in order to model the desired correlation. By modifying the network’s pooling regime (to incorporate noncontiguous windows), we may turn this partition favorable, and thus enable use of a much smaller network."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partly funded by Intel grant ICRI-CI no. 9-2012-6133 and by ISF Center grant 1790/12. Nadav Cohen is supported by a Google Fellowship in Machine Learning."
    }, {
      "heading" : "A Deferred proofs",
      "text" : "A.1 Proof of claim 1\nWe prove the equality in two steps, first showing that sep(hy; I, J)≤rankJAyKI,J , and then establishing the converse. The first part of the proof is elementary, and does not make use of the representation functions’ (fθd ) linear independence. The second part does rely on this assumption, and employs slightly more advanced mathematical machinery. Throughout the proof, we assume without loss of generality that the partition (I, J) of [N ] is such that I takes on smaller values, while J takes on larger ones. That is to say, we assume that I = {1, . . . , |I|} and J = {|I|+ 1, . . . , N}. 6\nTo prove that sep(hy; I, J)≤rankJAyKI,J , denote by R the rank of JAyKI,J . The latter is an M |I|-by-M |J| matrix, thus there exist vectors u1. . .uR ∈ RM |I| and v1. . .vR ∈ RM |J| such that JAyKI,J = ∑R ν=1 uνv > ν . For every ν ∈ [R], let Bν be the tensor of order |I| and dimension M in each mode whose arrangement as a column vector gives uν , i.e. whose matricization w.r.t. the partition ([|I|], ∅) is equal to uν . Similarly, let Cν , ν ∈ [R], be the tensor of order |J | = N − |I| and dimension M in each mode whose matricization w.r.t. the partition (∅, [|J |]) (arrangement as a row vector) is equal to v>ν . It holds that:\nJAyKI,J = ∑R\nν=1 uνv\n> ν\n= ∑R\nν=1 JBνK[|I|],∅ JCνK∅,[|J|] = ∑R\nν=1 JBνKI∩[|I|],J∩[|I|] JCνK(I−|I|)∩[|J|],(J−|I|)∩[|J|] = ∑R\nν=1 JBν ⊗ CνKI,J = r∑R\nν=1 Bν ⊗ Cν z I,J\nwhere the third equality relies on the assumption I = {1, . . . , |I|}, J = {|I|+ 1, . . . , N}, the fourth equality makes use of the relation in eq. 1, and the last equality is based on the linearity of the matricization operator. Since matricizations are merely rearrangements of tensors, the fact that JAyKI,J = J ∑R ν=1 B\nν ⊗CνKI,J implies Ay = ∑R ν=1 B ν ⊗ Cν , or equivalently, Ayd1...dN = ∑R ν=1 B ν d1...d|I|\n· Cνd|I|+1...dN for every d1. . .dN ∈ [M ]. Plugging the latter into eq. 2 gives:\nhy (x1, . . . ,xN ) = ∑M\nd1...dN=1 Ayd1,...,dN ∏N i=1 fθdi (xi)\n= ∑M\nd1...dN=1\n∑R ν=1 Bνd1...d|I| · C ν d|I|+1...dN ∏N i=1 fθdi (xi)\n= ∑R\nν=1 (∑M d1...d|I|=1 Bνd1...d|I| ∏|I| i=1 fθdi (xi) ) · (∑M\nd|I|+1...dN=1 Cνd|I|+1...dN ∏N i=|I|+1 fθdi (xi) ) (11)\nFor every ν ∈ [R], define the functions gν : (Rs)|I| → R and g′ν : (Rs)|J| → R as follows:\ngν(x1, . . . ,x|I|) := ∑M\nd1...d|I|=1 Bνd1...d|I| ∏|I| i=1 fθdi (xi)\ng′ν(x1, . . . ,x|J|) := ∑M\nd1...d|J|=1 Cνd1...d|J| ∏|J| i=1 fθdi (xi)\n6 To see that this does not limit generality, denote I = {i1, . . . , i|I|} and J = {j1, . . . , j|J|}, and define an auxiliary function h′y by permuting the entries of hy such that those indexed by I are on the left and those indexed by J on the right, i.e. h′y(xi1 , . . . ,xi|I| ,xj1 , . . . ,xj|J|) = hy(x1, . . . ,xN ). Obviously sep(hy; I, J) = sep(h′y; I\n′, J ′), where the partition (I ′, J ′) is defined by I ′ = {1, . . . , |I|} and J ′ = {|I| + 1, . . . , N}. Analogously to the definition of h′y , let A′y be the tensor obtained by permuting the modes of Ay such that those indexed by I are on the left and those indexed by J on the right, i.e. A′ydi1 ...di|I|dj1 ...dj|J| = Ayd1,...,dN . It is not difficult to see that matricizingA\n′y w.r.t. (I ′, J ′) is equivalent to matricizingAy w.r.t. (I, J), i.e. JA′yKI′,J′ = JAyKI,J , and in particular rankJA′yKI′,J′ = rankJAyKI,J . Moreover, since by definition Ay is a coefficient tensor corresponding to hy (eq. 2), A′y will be a coefficient tensor that corresponds to h′y . Now, our proof will show that sep(h′y; I ′, J ′) = rankJA′yKI′,J′ , which, in light of the equalities above, implies sep(hy; I, J) = rankJAyKI,J , as required.\nSubstituting these into eq. 11 leads to: hy (x1, . . . ,xN ) = ∑R\nν=1 gν(x1, . . . ,x|I|)g\n′ ν(x|I|+1, . . . ,xN )\nwhich by definition of the separation rank (eq. 5), implies sep(hy; I, J)≤R. By this we have shown that sep(hy; I, J)≤rankJAyKI,J , as required.\nFor proving the converse inequality, i.e. sep(hy; I, J)≥rankJAyKI,J , we rely on basic concepts and results from functional analysis, or more specifically, from the topic of L2 spaces. While a full introduction to this topic is beyond our scope (the interested reader is referred to [25]), we briefly lay out here the minimal background required in order to follow our proof. For any n ∈ N, L2(Rn) is formally defined as the Hilbert space of Lebesgue measurable square-integrable real functions over Rn 7 , equipped with standard (point-wise) addition and scalar multiplication, as well as the inner product defined by integration over point-wise multiplication. For our purposes, L2(Rn) may simply be thought of as the (infinite-dimensional) vector space of functions g : Rn → R satisfying ∫ g2 <∞, with inner product defined by 〈g1, g2〉 := ∫ g1·g2. Our proof will make use of the following basic facts related to L2 spaces:\nFact 1. If V is a finite-dimensional subspace of L2(Rn), then any g∈L2(Rn) may be expressed as g = p+ δ, with p∈V and δ∈V ⊥ (i.e. δ is orthogonal to all elements in V ). Moreover, such a representation is unique, so in the case where g∈V , we necessarily have p = g and δ ≡ 0. Fact 2. If g∈L2(Rn), g′∈L2(Rn′), then the function (x1,x2)7→g(x1)·g′(x2) belongs to L2(Rn × Rn′). Fact 3. Let V and V ′ be finite-dimensional subspaces of L2(Rn) and L2(Rn′) respectively, and define U⊂L2(Rn × Rn′) to be the subspace spanned by {(x1,x2) 7→p(x1)·p′(x2) : p∈V, p′∈V ′}. Given g∈L2(Rn), g′∈L2(Rn′), consider the function (x1,x2) 7→g(x1)·g′(x2) in L2(Rn × Rn′). This function belongs to U⊥ if g∈V ⊥ or g′∈V ′⊥. Fact 4. If g1. . .gm∈L2(Rn) are linearly independent, then for any k ∈ N, the set of functions {(x1, . . . ,xk) 7→∏k i=1 gdi(xi)}d1...dk∈[m] is linearly independent in L 2((Rn)k).\nTo facilitate application of the theory of L2 spaces, we assume that the network’s representation functions – fθ1 . . .fθM : R\ns → R, are members of L2(Rs). In particular, we assume them to be square-integrable. This may seem as a limitation at first glance, as for example neural representation functions fθd(x) = σ(w > d x+ bd), with parameters θd = (wd, bd) ∈ Rs ×R and sigmoidal activation σ(·) 8 , are not square-integrable. However, since in practice the input to a representation function is bounded (e.g. it represents image patches by holding intensity values), we may view the function as having compact support, which, as long as the function is continuous (holds for all cases of interest), ensures square-integrability. With the assumption fθ1 . . .fθM∈L\n2(Rs) in place, the expression given in eq. 2 for hy , along with fact 2 above, imply that hy∈L2((Rs)N ). We thus apply the definition of separation rank (eq. 5) to hy through the outlook of L2 spaces. That is to say, sep(hy; I, J) is understood to be the minimal non-negative integer R such that there exist g1. . .gR∈L2((Rs)|I|) and g′1. . .g′R∈L2((Rs)|J|) for which:\nhy(x1, . . . ,xN ) = ∑R\nν=1 gν(x1, . . . ,x|I|)g\n′ ν(x|I|+1, . . . ,xN ) (12)\nWe would like to show that sep(hy; I, J)≥rankJAyKI,J . Our strategy for achieving this will be to start from eq. 12, and derive an expression for JAyKI,J comprising a sum of R rank-1 matrices. As an initial step along this path, define the following finite-dimensional subspaces:\nV := span { (x1, . . . ,x|I|) 7→ ∏|I| i=1 fθdi (xi) } d1...d|I|∈[M ] ⊂ L2 ( (Rs)|I| ) (13)\nV ′ := span { (x1, . . . ,x|J|) 7→ ∏|J| i=1 fθdi (xi) } d1...d|J|∈[M ] ⊂ L2 ( (Rs)|J| ) (14)\nU := span { (x1, . . . ,xN ) 7→ ∏N i=1 fθdi (xi) } d1...dN∈[M ] ⊂ L2 ( (Rs)N ) (15)\nNotice that hy∈U (eq. 2), and that U is the span of products from V and V ′, i.e.:\nU = span{(x1, . . . ,xN ) 7→p(x1, . . . ,x|I|)·p′(x|I|+1, . . . ,xN ) : p∈V, p′∈V ′} (16)\nReturning to eq. 12, we apply fact 1 to obtain orthogonal decompositions of g1. . .gR w.r.t. V , and of g′1. . .g′R w.r.t. V ′. This gives p1. . .pR∈V , δ1. . .δR∈V ⊥, p′1. . .p′R∈V ′ and δ′1. . .δ′R∈V ′⊥, such that gν = pν + δν and\n7 More precisely, elements of the space are equivalence classes of functions, where two functions are considered equivalent if the set in Rn on which they differ has measure zero.\n8 σ is sigmoidal if it is monotonic with limz→−∞ σ(z) = c and limz→+∞ σ(z) = C for some c 6=C in R.\ng′ν = p ′ ν + δ ′ ν for every ν ∈ [R]. Plug this into eq. 12:\nhy(x1, . . . ,xN ) = ∑R\nν=1 gν(x1, . . . ,x|I|)·g′ν(x|I|+1, . . . ,xN )\n= ∑R\nν=1\n( pν(x1, . . . ,x|I|) + δν(x1, . . . ,x|I|) ) · ( p′ν(x|I|+1, . . . ,xN ) + δ ′ ν(x|I|+1, . . . ,xN )\n) = ∑R ν=1 pν(x1, . . . ,x|I|)·p′ν(x|I|+1, . . . ,xN )\n+ ∑R\nν=1 pν(x1, . . . ,x|I|)·δ′ν(x|I|+1, . . . ,xN )\n+ ∑R\nν=1 δν(x1, . . . ,x|I|)·p′ν(x|I|+1, . . . ,xN )\n+ ∑R\nν=1 δν(x1, . . . ,x|I|)·δ′ν(x|I|+1, . . . ,xN )\nGiven that U is the span of products from V and V ′ (eq. 16), and that pν∈V, δν∈V ⊥, p′ν∈V ′, δ′ν∈V ′⊥, one readily sees that the first term in the latter expression belongs to U , while, according to fact 3, the second, third and fourth terms are orthogonal to U . We thus obtained an orthogonal decomposition of hy w.r.t. U . Since hy is contained in U , the orthogonal component must vanish (fact 1), and we amount at:\nhy(x1, . . . ,xN ) = ∑R\nν=1 pν(x1, . . . ,x|I|)·p′ν(x|I|+1, . . . ,xN ) (17)\nFor every ν ∈ [R], let Bν and Cν be coefficient tensors of pν and p′ν w.r.t. the functions that span V and V ′ (eq. 13 and 14), respectively. Put formally, Bν and Cν are tensors of orders |I| and |J | (respectively), with dimension M in each mode, meeting:\npν(x1, . . . ,x|I|) = ∑M\nd1...d|I|=1 Bνd1...d|I| ∏|I| i=1 fθdi (xi)\np′ν(x1, . . . ,x|J|) = ∑M\nd1...d|J|=1 Cνd1...d|J| ∏|J| i=1 fθdi (xi)\nSubstitute into eq. 17:\nhy (x1, . . . ,xN ) = ∑R\nν=1 (∑M d1...d|I|=1 Bνd1...d|I| ∏|I| i=1 fθdi (xi) ) · (∑M\nd|I|+1...dN=1 Cνd|I|+1...dN ∏N i=|I|+1 fθdi (xi) ) = ∑R ν=1 ∑M d1...dN=1 Bνd1...d|I| · C ν d|I|+1...dN ∏N i=1 fθdi (xi)\n= ∑M\nd1...dN=1\n(∑R ν=1 Bνd1...d|I| · C ν d|I|+1...dN )∏N i=1 fθdi (xi)\nCompare this expression for hy to that given in eq. 2:∑M d1...dN=1 (∑R ν=1 Bνd1...d|I| · C ν d|I|+1...dN )∏N i=1 fθdi (xi) = ∑M d1...dN=1 Ayd1,...,dN ∏N i=1 fθdi (xi) (18) At this point we utilize the given linear independence of fθ1 . . .fθM∈L\n2(Rs), from which it follows (fact 4) that the functions spanning U (eq. 15) are linearly independent in L2((Rs)N ). Both sides of eq. 18 are linear combinations of these functions, thus their coefficients must coincide:\nAyd1,...,dN = ∑R ν=1 Bνd1...d|I| · C ν d|I|+1...dN , ∀d1. . .dN ∈ [M ] ⇐⇒ A y = ∑R ν=1 Bν ⊗ Cν\nMatricizing the tensor equation on the right w.r.t. (I, J) gives: JAyKI,J = r∑R\nν=1 Bν ⊗ Cν z I,J\n= ∑R\nν=1 JBν ⊗ CνKI,J = ∑R\nν=1 JBνKI∩[|I|],J∩[|I|] JCνK(I−|I|)∩[|J|],(J−|I|)∩[|J|] = ∑R\nν=1 JBνK[|I|],∅ JCνK∅,[|J|]\nwhere the second equality is based on the linearity of the matricization operator, the third equality relies on the relation in eq. 1, and the last equality makes use of the assumption I = {1, . . . , |I|}, J = {|I| + 1, . . . , N}.\nFor every ν ∈ [R], JBνK[|I|],∅ is a column vector of dimension M |I| and JCνK∅,[|J|] is a row vector of dimension M |J|. Denoting these by uν and v>ν respectively, we may write:\nJAyKI,J = ∑R\nν=1 uνv\n> ν\nThis shows that rankJAyKI,J≤R. Since R is a general non-negative integer that admits eq. 12, we may take it to be minimal, i.e. to be equal to sep(hy; I, J) – the separation rank of hy w.r.t. (I, J). By this we obtain rankJAyKI,J≤sep(hy; I, J), which is what we set out to prove.\nA.2 Proof of claim 2\nThe claim is framed in measure theoretical terms, and in accordance, so will its proof be. While a complete introduction to measure theory is beyond our scope (the interested reader is referred to [15]), we briefly convey here the intuition behind the concepts we will be using, as well as facts we rely upon. The Lebesgue measure is defined over sets in a Euclidean space, and may be interpreted as quantifying their “volumes”. For example, the Lebesgue measure of a hypercube is one, of the entire space is infinity, and of a finite set of points is zero. In this context, when a phenomenon is said to occur almost everywhere, it means that the set of points in which it does not occur has Lebesgue measure zero, i.e. is negligible. An important result we will make use of (proven in [4] for example) is the following. Given a polynomial defined over n real variables, the set of points in Rn on which it vanishes is either the entire space (when the polynomial in question is the zero polynomial), or it must have Lebesgue measure zero. In other words, if a polynomial is not identically zero, it must be different from zero almost everywhere.\nHeading on to the proof, we recall from sec. 3 that the entries of the coefficient tensor Ay (eq. 2) are given by polynomials in the network’s conv weights {al,γ}l,γ and output weights aL,y . Since JAyKI,J – the matricization of Ay w.r.t. the partition (I, J), is merely a rearrangement of the tensor as a matrix, this matrix too will have entries given by polynomials in the network’s linear weights. Now, denote by r the maximal rank taken by JAyKI,J as network weights vary, and consider a specific setting of weights for which this rank is attained. We may assume without loss of generality that under this setting, the top-left r-by-r block of JAyKI,J is non-singular. The corresponding minor, i.e. the determinant of the sub-matrix (JAyKI,J)1:r,1:r , is thus a polynomial defined over {al,γ}l,γ and aL,y which is not identically zero. In light of the above, this polynomial is different from zero almost everywhere, implying that rank(JAyKI,J)1:r,1:r = r almost everywhere. Since rankJAyKI,J≥rank(JAyKI,J)1:r,1:r , and since by definition r is the maximal rank that JAyKI,J can take, we have that rankJAyKI,J is maximal almost everywhere.\nA.3 Proof of theorem 1\nThe matrix decomposition in eq. 7 expresses JAKI,J in terms of the network’s linear weights – {a0,γ ∈ RM}γ∈[r0] for conv operator in hidden layer 0, {a\nl,γ ∈ Rrl−1}γ∈[rl] for conv operator in hidden layer l = 1. . .L−1, and aL,y ∈ RrL−1 for node y of dense output operator. We prove lower and upper bounds on the maximal rank that JAKI,J can take as these weights vary. Our proof relies on the rank-multiplicative property of the Kronecker product (rank(A B) = rank(A)·rank(B) for any real matrices A and B – see [1] for proof), but is otherwise elementary.\nBeginning with the lower bound, consider the following weight setting (eγ here stands for a vector holding 1 in entry γ and 0 at all other entries, 0 stands for a vector holding 0 at all entries, and 1 stands for a vector holding 1 at all entries, with the dimension of a vector to be understood by context):\na0,γ = { eγ , γ≤min{r0,M} 0 , otherwise (19)\na1,γ = { 1 , γ = 1 0 , otherwise\nal,γ = { e1 , γ = 1 0 , otherwise for l = 2. . .L− 1\naL,y = e1\nLet n ∈ [N/4]. Recalling the definition of Il,k and Jl,k from eq. 6, consider the sets I1,n and J1,n, as well as I0,4(n−1)+t and J0,4(n−1)+t for t ∈ [4]. (I1,n, J1,n) is a partition of [4], i.e. I1,n ·∪J1,n = [4], and for every t ∈ [4] we have I0,4(n−1)+t = {1} and J0,4(n−1)+t = ∅ if t belongs to I1,n, and otherwise I0,4(n−1)+t = ∅ and J0,4(n−1)+t = {1} if t belongs to J1,n. This implies that for an arbitrary vector v, the matricization\nJvKI0,4(n−1)+t,J0,4(n−1)+t is equal to v if t∈I1,n, and to v> if t∈J1,n. Accordingly, for any γ ∈ [r0]:\n4 t=1 Ja0,γKI0,4(n−1)+t,J0,4(n−1)+t =  (a0,γ a0,γ a0,γ a0,γ) , |I1,n| = 4 |J1,n| = 0 (a0,γ a0,γ a0,γ)(a0,γ)> , |I1,n| = 3 |J1,n| = 1 (a0,γ a0,γ)(a0,γ a0,γ)> , |I1,n| = 2 |J1,n| = 2 (a0,γ)(a0,γ a0,γ a0,γ)> , |I1,n| = 1 |J1,n| = 3 (a0,γ a0,γ a0,γ a0,γ)> , |I1,n| = 0 |J1,n| = 4\nAssume that γ ≤ min{r0,M}. By our setting a0,γ = eγ , so the above matrix holds 1 in a single entry and 0 in all the rest. Moreover, if the matrix is not a row or column vector, i.e. if both I1,n and J1,n are non-empty, the column index and row index of the entry holding 1 are both unique w.r.t. γ, i.e. they do not repeat as γ ranges over 1 . . .min{r0,M}. We thus have:\nrank (∑min{r0,M} γ=1 4 t=1 Ja0,γKI0,4(n−1)+t,J0,4(n−1)+t ) = { min{r0,M} , I1,n 6= ∅ ∧ J1,n 6= ∅ 1 , I1,n = ∅ ∨ J1,n = ∅\nSince we set a1,1 = 1 and a0,γ = 0 for γ > min{r0,M}, we may write:\nrank (∑r0 γ=1 a1,1γ · 4 t=1 Ja0,γKI0,4(n−1)+t,J0,4(n−1)+t ) = { min{r0,M} , I1,n 6= ∅ ∧ J1,n 6= ∅ 1 , I1,n = ∅ ∨ J1,n = ∅\nThe latter matrix is by definition equal to Jφ1,1KI1,n,J1,n (see top row of eq. 7), and so for every n ∈ [N/4]:\nrank q φ1,1 y I1,n,J1,n = { min{r0,M} , I1,n 6= ∅ ∧ J1,n 6= ∅ 1 , I1,n = ∅ ∨ J1,n = ∅ (20)\nNow, the fact that we set aL,y = e1 and al,1 = e1 for l = 2. . .L− 1, implies that the second to last levels of the decomposition in eq. 7 collapse to:\nJAyKI,J = N/4\nt=1\nJφ1,1KI1,t,J1,t\nApplying the rank-multiplicative property of the Kronecker product, and plugging in eq. 20, we obtain: rankJAyKI,J = ∏N/4\nt=1 rankJφ1,1KI1,t,J1,t = min{r0,M}S\nwhere S := |{t ∈ [N/4] : I1,t 6= ∅ ∧ J1,t 6= ∅}|. This equality holds for the specific weight setting we defined in eq. 19. Maximizing over all weight settings gives the sought after lower bound:\nmax {al,γ}l,γ ,aL,y\nrankJAyKI,J ≥ min{r0,M}S\nMoving on to the upper bound, we show by induction over l = 1. . .L− 1 that for any k ∈ [N/4l] and γ ∈ [rl], the rank of Jφl,γKIl,k,Jl,k is no greater than cl,k, regardless of the chosen weight setting. For the base case l = 1 we have:\nJφ1,γKI1,k,J1,k = ∑r0\nα=1 a1,γα · 4 t=1 Ja0,αKI0,4(k−1)+t,J0,4(k−1)+t TheM |I1,k|-by-M |J1,k| matrix Jφ1,γKI1,k,J1,k is given here as a sum of r0 rank-1 terms, thus obviously its rank is no greater than min{Mmin{|I1,k|,|J1,k|}, r0}. Since by definition c0,t = 1 for all t ∈ [N ], we may write:\nrankJφ1,γKI1,k,J1,k ≤ min { Mmin{|I1,k|,|J1,k|}, r0 ∏4 t=1 c0,4(k−1)+t }\nc1,k is defined by the right hand side of this inequality, so our inductive hypotheses holds for l = 1. For l > 1: Jφl,γKIl,k,Jl,k = ∑rl−1\nα=1 al,γα · 4 t=1 Jφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t Taking ranks:\nrankJφl,γKIl,k,Jl,k = rank (∑rl−1\nα=1 al,γα · 4 t=1\nJφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t )\n≤ ∑rl−1\nα=1 rank ( 4 t=1 Jφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t )\n= ∑rl−1\nα=1 ∏4 t=1 rankJφl−1,αKIl−1,4(k−1)+t,Jl−1,4(k−1)+t\n≤ ∑rl−1\nα=1 ∏4 t=1 cl−1,4(k−1)+t\n= rl−1 ∏4\nt=1 cl−1,4(k−1)+t\nwhere we used rank sub-additivity in the second line, the rank-multiplicative property of the Kronecker product in the third line, and our inductive hypotheses for l− 1 in the fourth line. Since the number rows and columns in Jφl,γKIl,k,Jl,k is M |Il,k| and M |Jl,k| respectively, we may incorporate these terms into the inequality, obtaining:\nrankJφl,γKIl,k,Jl,k ≤ min { Mmin{|Il,k|,|Jl,k|}, rl−1 ∏4 t=1 cl−1,4(k−1)+t }\nThe right hand side here is equal to cl,k by definition, so our inductive hypotheses indeed holds for all l = 1. . .L− 1. To establish the sought after upper bound on the rank of JAyKI,J , we recall that the latter is given by:\nJAyKI,J = ∑rL−1\nα=1 aL,yα · 4 t=1 JφL−1,αKIL−1,t,JL−1,t\nCarrying out a series of steps similar to before, while making use of our inductive hypotheses for l = L− 1: rankJAyKI,J = rank (∑rL−1\nα=1 aL,yα · 4 t=1\nJφL−1,αKIL−1,t,JL−1,t )\n≤ ∑rL−1\nα=1 rank ( 4 t=1 JφL−1,αKIL−1,t,JL−1,t )\n= ∑rL−1\nα=1 ∏4 t=1 rankJφL−1,αKIL−1,t,JL−1,t\n≤ ∑rL−1\nα=1 ∏4 t=1 cL−1,t\n= rL−1 ∏4\nt=1 cL−1,t\nSince JAyKI,J has M |I| rows and M |J| columns, we may include these terms in the inequality, thus reaching the upper bound we set out to prove."
    } ],
    "references" : [ {
      "title" : "Introduction to matrix analysis, volume 960",
      "author" : [ "Richard Bellman", "Richard Ernest Bellman" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1970
    }, {
      "title" : "Numerical operator calculus in higher dimensions",
      "author" : [ "Gregory Beylkin", "Martin J Mohlenkamp" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Multivariate regression and machine learning with sums of separable functions",
      "author" : [ "Gregory Beylkin", "Jochen Garcke", "Martin J Mohlenkamp" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "The zero set of a polynomial",
      "author" : [ "Richard Caron", "Tim Traynor" ],
      "venue" : "WSMR Report 05-02,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Simnets: A generalization of convolutional networks",
      "author" : [ "Nadav Cohen", "Amnon Shashua" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), Deep Learning Workshop,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Convolutional rectifier networks as generalized tensor decompositions",
      "author" : [ "Nadav Cohen", "Amnon Shashua" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Deep simnets",
      "author" : [ "Nadav Cohen", "Or Sharir", "Amnon Shashua" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "On the expressive power of deep learning: A tensor analysis",
      "author" : [ "Nadav Cohen", "Or Sharir", "Amnon Shashua" ],
      "venue" : "Conference On Learning Theory (COLT),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Shallow vs. deep sum-product networks",
      "author" : [ "Olivier Delalleau", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "The power of depth for feedforward neural networks",
      "author" : [ "Ronen Eldan", "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1512.03965,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "On the efficient evaluation of coalescence integrals in population balance models",
      "author" : [ "Wolfgang Hackbusch" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Computational Mathematics",
      "author" : [ "Wolfgang Hackbusch" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Multiresolution quantum chemistry in multiwavelet bases",
      "author" : [ "Robert J Harrison", "George I Fann", "Takeshi Yanai", "Gregory Beylkin" ],
      "venue" : "In Computational Science-ICCS",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2003
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Tensor Decompositions and Applications",
      "author" : [ "Tamara G Kolda", "Brett W Bader" ],
      "venue" : "SIAM Review (),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Convolutional networks for images, speech, and time series",
      "author" : [ "Yann LeCun", "Yoshua Bengio" ],
      "venue" : "The handbook of brain theory and neural networks,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1995
    }, {
      "title" : "Learning real and boolean functions: When is deep better than shallow",
      "author" : [ "Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio" ],
      "venue" : "arXiv preprint arXiv:1603.00988,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "On the number of linear regions of deep neural networks",
      "author" : [ "Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "On the number of inference regions of deep feed forward networks with piece-wise linear activations",
      "author" : [ "Razvan Pascanu", "Guido Montufar", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "I-theory on depth vs width: hierarchical function composition",
      "author" : [ "Tomaso Poggio", "Fabio Anselmi", "Lorenzo Rosasco" ],
      "venue" : "Technical report, Center for Brains, Minds and Machines (CBMM),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Functional analysis. international series in pure and applied mathematics",
      "author" : [ "Walter Rudin" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1991
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Going Deeper with Convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Perhaps the most successful exemplar of inductive bias to date manifests itself in the use of convolutional networks ([18]) for computer vision tasks.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : "[17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 24,
      "context" : "[17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 23,
      "context" : "[17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "[17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).",
      "startOffset" : 151,
      "endOffset" : 178
    }, {
      "referenceID" : 20,
      "context" : "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).",
      "startOffset" : 151,
      "endOffset" : 178
    }, {
      "referenceID" : 18,
      "context" : "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).",
      "startOffset" : 151,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).",
      "startOffset" : 151,
      "endOffset" : 178
    }, {
      "referenceID" : 21,
      "context" : "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).",
      "startOffset" : 151,
      "endOffset" : 178
    }, {
      "referenceID" : 17,
      "context" : "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).",
      "startOffset" : 151,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "Recently, [8] analyzed the depth efficiency of convolutional arithmetic circuits, showing that besides a negligible set, all functions realizable by a deep network require exponential size in order to be realized (or approximated) by a shallow one.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "In subsequent work, [6] proved that for the popular convolutional rectifier networks, i.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "convolutional networks with rectified linear (ReLU, [22]) activation and max or average pooling, depth efficiency exists but it is not complete, meaning such networks are inferior to convolutional arithmetic circuits in terms of depth efficiency.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Motivated by these results, and by the fact that convolutional arithmetic circuits are equivalent to SimNets, a new deep learning architecture that has recently demonstrated promising empirical performance ([5, 7]), we focus in this paper on convolutional arithmetic circuits as representatives of the class of convolutional networks.",
      "startOffset" : 207,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "Motivated by these results, and by the fact that convolutional arithmetic circuits are equivalent to SimNets, a new deep learning architecture that has recently demonstrated promising empirical performance ([5, 7]), we focus in this paper on convolutional arithmetic circuits as representatives of the class of convolutional networks.",
      "startOffset" : 207,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "a partition of its input is measured through the notion of separation rank ([2]), which can be viewed as quantifying how far the function is from being equal to a product of factors, each depending on input regions from only one side of the partition.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "By this we return to the complete depth efficiency result of [8], but with an added important insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "In this section we establish the minimal background required in order to follow our arguments 1 , referring the interested reader to [12] for a broad and comprehensive introduction to the field.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "1 The definitions we give are actually concrete special cases of more abstract algebraic definitions as given in [12].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "The convolutional arithmetic circuit architecture on which we focus in this paper is the one considered in [8], portrayed in fig.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "3 describes what is called a hierarchical tensor decomposition (see chapter 11 in [12]), with underlying tree over modes being a full quad-tree (corresponding to the fact that the network’s pooling windows cover four entries each).",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "4 is an instance of the classic CP decomposition, also known as rank-1 decomposition (see [16] for a historic survey).",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "To conclude this section, we relate the background material above, as well as our contribution described in the upcoming sections, to the work of [8].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "The central result in [8] relates to inductive bias through the notion of depth efficiency – it is shown that in the parameter space of a deep network, all weight settings but a set of measure zero give rise to functions that can only be realized (or approximated) by a shallow network if the latter has exponential size.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "From the theory of tensor products between L spaces (see [12] for a comprehensive coverage), we know that any h∈L((R) ), i.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "The concept of separation rank was introduced in [2] for numerical treatment of high-dimensional functions.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "Since then, it has been employed for various applications, including quantum chemistry ([13]), particle engineering ([11]), machine learning ([3]), and more.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "Since then, it has been employed for various applications, including quantum chemistry ([13]), particle engineering ([11]), machine learning ([3]), and more.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "Since then, it has been employed for various applications, including quantum chemistry ([13]), particle engineering ([11]), machine learning ([3]), and more.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "α=1 a α · q φL−1,α y I∩[4L−1],J∩[4L−1] q φL−1,α y (I−4L−1)∩[4L−1],(J−4L−1)∩[4L−1] q φL−1,α y (I−2·4L−1)∩[4L−1],(J−2·4L−1)∩[4L−1] q φL−1,α y (I−3·4L−1)∩[4L−1],(J−3·4L−1)∩[4L−1] For every k ∈ [4] define IL−1,k := (I − (k − 1) · 4L−1) ∩ [4L−1] and JL−1,k := (J − (k − 1) · 4L−1) ∩ [4L−1].",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 3,
      "context" : "L−1}, k ∈ [N/4]: Il,k := (I − (k − 1) · 4) ∩ [4] Jl,k := (J − (k − 1) · 4) ∩ [4] (6) That is to say, (Il,k, Jl,k) represents the partition induced by (I, J) on the set of indexes {(k−1) ·4+ 1, .",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "L−1}, k ∈ [N/4]: Il,k := (I − (k − 1) · 4) ∩ [4] Jl,k := (J − (k − 1) · 4) ∩ [4] (6) That is to say, (Il,k, Jl,k) represents the partition induced by (I, J) on the set of indexes {(k−1) ·4+ 1, .",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : ", N} (9) Under (I , J), all partitions induced on size-4L−1 patch groups (quadrants of [N ]) are completely one-sided (min{|IL−1,k|, |JL−1,k|} = 0 for all k ∈ [4]), resulting in the upper bound being no greater than rL−1 – linear in network size.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "Since all but a negligible set of the functions realizable by the deep network give rise to maximal separation ranks (claim 2), we obtain the complete depth efficiency result of [8].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "However, unlike [8], which did not provide any explanation for the usefulness of functions brought forth by depth, we obtain an insight into their utility – they are able to efficiently model strong correlation between favored partitions of the input.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "By this we derive the depth efficiency result of [8], but in addition, provide an insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.",
      "startOffset" : 49,
      "endOffset" : 52
    } ],
    "year" : 2016,
    "abstractText" : "Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional arithmetic circuits to model correlations among regions of their input. Correlations are formalized through the notion of separation rank, which for a given input partition, measures how far a function is from being separable. We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others. The network’s pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias. Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images. In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth – they are able to efficiently model strong correlation under favored partitions of the input.",
    "creator" : "LaTeX with hyperref package"
  }
}