{
  "name" : "1602.00172.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Learning For Smile Recognition",
    "authors" : [ "Patrick O. Glauner" ],
    "emails" : [ "patrick.glauner@uni.lu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Deep Learning For Smile Recognition\nPatrick O. Glauner\nInterdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg\n2721 Luxembourg, Luxembourg Email: patrick.glauner@uni.lu\nsnt.uni.lu\nInspired by recent successes of deep learning in computer vision, we propose a novel application of deep convolutional neural networks to facial expression recognition, in particular smile recognition. A smile recognition test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches based on hand-crafted features with accuracies ranging from 65.55% to 79.67%. The novelty of this approach includes a comprehensive model selection of the architecture parameters, allowing to find an appropriate architecture for each expression such as smile. This is feasible because all experiments were run on a Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations on a CPU.\nKeywords: Computer Vision; Deep Learning; Facial expression recognition; GPU acceleration."
    }, {
      "heading" : "1. Introduction",
      "text" : "Neural networks are celebrating a comeback under the term ”deep learning” for the last ten years by training many hidden layers allowing to selflearn complex feature hierarchies. This makes them of particular interest for computer vision, in which feature description is a long-standing issue. Many advances have been reported in this period, including new training methods and a paradigm shift of training from CPUs to GPUs. As a result, those advances allow to train more reliable models much faster. This has for example resulted in breakthroughs3 in signal processing. Nonetheless, deep neural networks are not a magic bullet and successful training is still heavily based on experimentation.\nThe Facial Action Coding System (FACS)1 is a system to taxonomize any facial expression of a human being by their appearance on the face. Action units describe muscles or muscle groups in the face, are set or un-\nar X\niv :1\n60 2.\n00 17\n2v 1\n[ cs\n.C V\n] 3\n0 Ja\nn 20\n16\nset and the activation may be on different intensity levels. State-of-the art approaches in this field mostly rely on hand-crafted features leaving a lot of potential for higher accuracies. In contrast to other fields such as face or gesture recognition, only very few works on deep learning applied to facial expression recognition have been reported so far2 in which the architecture parameters are fixed. We are not aware of publications in which the architecture of a deep neural network for facial expression recognition is subject to extensive model selection. This allows to learn appropriate architectures per action unit."
    }, {
      "heading" : "2. Deep neural networks",
      "text" : "Training neural networks is difficult, as their cost functions have many local minima. The more hidden layers, the more difficult the training of a neural network. Hence, training tends to converge to a local minimum, resulting in poor generalization of the network. In order to overcome these issues, a variety of new concepts have been proposed in the literature, of which only a few can be named in this chapter. Unsupervised pre-training methods, such as autoencoders8 allow to initialize the weights well in order for backpropagation to quickly optimize them. The Rectified Linear Unit (ReLU)7 and dropout10 are new regularization methods. The new training methods and other new concepts can also lead to significant improvements of shallow neural networks with just a few hidden layers. Convolutional neural networks (CNNs) were initially proposed by LeCun5 for the recognition of hand-written digits. A CNN consists of two layers: a convolutional layer, followed by a subsampling layer. Inspired by biological processes and exploiting the fact that nearby pixels are strongly correlated, CNNs are relatively insensitive to small translations or rotations of the image input.\nTraining deep neural networks is slow due to the number of parameters in the model. As the training can be described in a vectorized form, it is possible to massively parallelize it. Modern GPUs have thousands of cores and are therefore an ideal candidate for the execution of the training of neural networks. Significant speedups of factor 10 or higher9 have been reported. A difficulty is to write GPU code. In the last few years, more abstract libraries have been released."
    }, {
      "heading" : "3. DISFA database",
      "text" : "The Denver Intensity of Spontaneous Facial Action (DISFA)6 database consists of 27 videos of 4844 frames each, with 130,788 images in total. Action\nunit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all. Fig. 1 contains a sample image of DISFA.\nIn the original paper on DISFA6 multi-class SVMs were trained for the different levels 0-5 of action unit intensity. Test accuracies for the individual levels and for the binary action unit recognition problem are reported for three different hand-crafted feature description techniques. In those three cases, accuracies of 65.55%, 72.94% and 79.67% for smile recognition are reported."
    }, {
      "heading" : "4. Smile recognition",
      "text" : "In the following experiments, an aligned version of DISFA is used. In this aligned version, the faces have been cropped and annotated with facial landmark points. Facial landmark points allow to compute a bounding box to fit the mouth in all images. In the experiments, two inputs are used: the mouth and face, downscaled to 85 × 69 and 128 × 104 pixels, respectively. Both inputs are used to assess if the mouth alone is as expressive as or even more expressive than the entire face for smile recognition.\n4.1. Model\nThe architecture of the network is as follows: The input images are fed into a convolution comprising a convolutional and a subsampling layer. That convolution may be followed by more convolutions to become gradually\nmore invariant to distortions in the input. In the second stage, a regular neural network follows the convolutions in order to discriminate the features learned by the convolutions. The output layer consists of two units for smile or no smile. The novelty of this approach is that the exact number of convolutions, number of hidden layers and size of hidden layers are not fixed but subject to extensive model selection in Sec. 4.3.\n4.2. Experiment setting\nDue to training time constraints, some parameters have been fixed to reasonable and empirical values, such as the size of convolutions (5× 5 pixels, 32 feature maps) and the size of subsamplings (2 × 2 pixels using max pooling). All layers use ReLU units, except of softmax being used in the output layer. The learning rate is fixed to α = 0.01 and not subject to model selection as it would significantly prolong the model selection. The same considerations apply to the momentum, which is fixed to µ = 0.9.\nThe entire database has been randomly split into a 60%/20%/20% training/validation/test ratio. Training neural networks comes with uncertainties, mostly due to the random initialization of the weights, but also due to that random split of the data. Evaluations have shown that for 10 similar experiments carried out, the standard deviation of the test accuracy is 0.041725%. Because of this low standard deviation, performing each experiment exactly once has only a very low bias and is therefore relatively safe to do for reasons of faster training time. Throughout the experiments, the classification rate is used as the accuracy measure.\nThe model is implemented using Lasagne4 and the generated CUDA code is executed on a Tesla K40c9 as training on a GPU allows to perform a comprehensive model selection in a feasible amount of time. Stochastic gradient descent with a batch size of 500 is used.\n4.3. Parameter optimization\nTable 1 contains the four parameters to be optimized: the number of convolutions, the number of hidden layers, the number of units per hidden layer and the dropout factor. Each parameter was optimized independently due to training time constraints. This may not lead to an optimal model, but has proven to work empirically well. Each model was trained for 50 epochs in the model selection.\nFor both inputs, Table 2 contains the final models selected. For the mouth input, there is a preference to more convolutions and more hidden\nParameter Values Default value\n#Convolutions 1, 2, 3 1 #Hidden layers 1, 2, 3 1\n#Units / hidden layer 100, 200, 300, 400 100 Dropout 0, 0.1, 0.5, 0.7 0.5\nlayers. This is the case because slight translations or rotations in the mouth input have stronger consequences on the classification result. In the entire face, that sort of distortions may be less of a problem because other parts of the face such as the cheeks contribute to smile recognition, too.\n4.4. Results and discussion\nBoth final models were trained for 1000 epochs. The test accuracies of both models started to converge after about 300 epochs. For the mouth and face inputs, the best accuracies were achieved after 700 and 1000 epochs with 99.45% and 99.34%, respectively. Both models significantly outperform the state-of-the-art SVM baselines reported in Sec. 3 ranging from 65.55% to 79.67%. Overall, there is no strong preference for either the mouth or face input. Further experiments with a reduced dataset containing only 70% of the images that have no action unit(s) set at all support this hypothesis. Concretely, the test accuracies for the mouth and face input reduced to 99.24% and 99.26%, respectively. Thus, the difference between the two models has been further reduced and this time giving a very low preference for the face input. Nonetheless, this difference is not representative as it is within the experiment error standard deviation reported in Sec. 4.2.\nTraining time per epoch are 82 seconds and 41 seconds for the mouth and face input models, respectively. Experiments have shown that the training time mostly depends on the number of convolutions. Using the Tesla K40c GPU has allowed to speed up the training time by factor ten over the use of a CPU to execute the CPU code generated by the library. This clearly\ndemonstrates the importance of training on a GPU to do a comprehensive model selection in a feasible amount of time."
    }, {
      "heading" : "5. Conclusions and future work",
      "text" : "Deep learning is an umbrella term for training neural networks with potentially many hidden layers using new training methods allowing to learn complex feature hierarchies from data. Applied to action unit recognition and smile recognition in particular, a deep convolutional neural network model with an overall accuracy of 99.45% significantly outperforms existing approaches. The underlying extensive model selection allows to find for each action unit an appropriate architecture in order to maximize test accuracies. In the future, we will extend the model to images from multiple databases and to make predictions in image sequences."
    } ],
    "references" : [ {
      "title" : "Facial Action Coding System: A Technique for the Measurement of Facial Movement",
      "author" : [ "P. Ekman", "W. Friesen" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1978
    }, {
      "title" : "Deep Learning based FACS Action Unit Occurrence and Intensity Estimation",
      "author" : [ "A. Gudi" ],
      "venue" : "Vicarious Perception Technologies",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
      "author" : [ "G. Hinton" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "LeNet-5, convolutional neural networks. http://yann",
      "author" : [ "Y. LeCun" ],
      "venue" : "lecun.com/exdb/lenet/. Retrieved: April",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Disfa: A spontaneous facial action intensity database",
      "author" : [ "S.M. Mavadati", "M.H. Mahoor", "K. Bartlett", "P. Trinh", "J.F. Cohn" ],
      "venue" : "IEEE Transactions on Affective Computing,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Rectified Linear Units Improve Restricted Boltzmann",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Deep Learning Tutorial. http://deeplearning.stanford.edu/ tutorial",
      "author" : [ "A. Ng" ],
      "venue" : "Retrieved: February",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "author" : [ "N. Srivastava" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Inspired by recent successes of deep learning in computer vision, we propose a novel application of deep convolutional neural networks to facial expression recognition, in particular smile recognition. A smile recognition test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches based on hand-crafted features with accuracies ranging from 65.55% to 79.67%. The novelty of this approach includes a comprehensive model selection of the architecture parameters, allowing to find an appropriate architecture for each expression such as smile. This is feasible because all experiments were run on a Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations on a CPU.",
    "creator" : "TeX"
  }
}