{
  "name" : "1609.06773.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "JOINT CTC-ATTENTION BASED END-TO-END SPEECH RECOGNITION USING MULTI-TASK LEARNING",
    "authors" : [ "Suyoun Kim", "Takaaki Hori", "Shinji Watanabe" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— end-to-end, speech recognition, connectionist temporal classification, attention, multi-task learning"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9]. The traditional hybrid approach, Deep Neural Networks - Hidden Markov Models (DNN-HMM), factorizes the system into several components trained separately (i.e. acoustic model, contextdependent phone model, pronunciation model, and language model) based on conditional independence assumptions (including Markov assumptions) and approximations [10, 11]. Unlike such hybrid approaches, the end-to-end model learns acoustic frames to character mappings in one step towards the final objective of interest, and attempts to rectify the suboptimal issues that arise from the disjoint training procedure.\nRecent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6]. Both methods address the problem of variable-length\nThe work is performed during Suyoun Kim is at MERL.\ninput and output sequences. The key idea of CTC is to use intermediate label representation allowing repetitions of labels and occurrences of blank labels to identify less informative frames. The CTC loss can be efficiently calculated by the forward-backward algorithm, but it still predicts targets for every frame, and assumes that the targets are conditionally independent of each other.\nAnother approach, the attention-based encoder-decoder directly learns a mapping from acoustic frame to character sequences. At each output time step, the model emits a character conditioned on the inputs and the history of the target character. Since the attention model does not use any conditional independence assumption, it has often shown to improve Character Error Rate (CER) than CTC when no external language model is used [7]. However, in realenvironment speech recognition tasks, the model shows poor results because the alignment estimated in the attention mechanism is easily corrupted due to the noise. Another issue is that the model is hard to be learned from scratch due to the misalignment on longer input sequences, and therefore a windowing technique is commonly used to limit the area explored by the attention mechanism [7], but several parameters for windowing need to be determined manually depending on the training data.\nTo overcome the above misalignment issues, this paper proposes a novel end-to-end speech recognition method to improve performance and accelerate learning by using a joint CTC-attention model within the multi-task learning framework. The key to our approach is that we use a shared-encoder representation trained by both CTC and attention model objectives simultaneously. We think that the weakness of the attention model is due to lack of left-to-right constraints as used in DNN-HMM and CTC, making it difficult to train the encoder network with proper alignments in the case of noisy data and/or long input sequences. Our proposed method improves the performance by rectifying the alignment problem using the CTC loss function based on the forward-backward algorithm. Along with improving performance, our framework significantly speeds up learning with fast convergence. We evaluate our model on the WSJ and CHiME-4 tasks, and show that our system outperforms both the CTC and attention models in CER and learning speed."
    }, {
      "heading" : "2. JOINT CTC-ATTENTION MECHANISM",
      "text" : "In this section, we review the CTC in Section 2.1 and the attentionbased encoder-decoder in Section 2.2, addressing the variable (T ) length input frames, x = (x1, · · · , xT ), andU length output characters, y = (y1, · · · , yU ), where yu ∈ {1, · · · ,K}. K is the number of distinct labels. Then, our joint CTC-attention based end-to-end framework will be described in Section 2.3.\nar X\niv :1\n60 9.\n06 77\n3v 1\n[ cs\n.C L\n] 2\n1 Se\np 20\n16"
    }, {
      "heading" : "2.1. Connectionist temporal classification (CTC)",
      "text" : "The key idea of CTC [12] is to use intermediate label representation π = (π1, · · · , πT ), allowing repetitions of labels and occurrences of a blank label (−), which represents the special emission without labels, i.e., πt ∈ {1, · · · ,K} ∪ {−}. CTC trains the model to maximize P (y|x), the probability distribution over all possible label sequences Φ(y′):\nP (y|x) = ∑\nπ∈Φ(y′)\nP (π|x), (1)\nwhere y′ is a modified label sequence of y, which is made by inserting the blank symbols between labels for allowing blanks in the output, i.e., yu ∈ {1, · · · ,K} ∪ {−}.\nCTC is applied on top of Recurrent Neural Networks (RNNs), which is interpreted as the label distribution including the blank. The probability of label sequence P (π|x) is approximately computed by the product of the probability of each label based on the conditional independence assumption:\nP (π|x) ≈ T∏\nt=1\nP (πt|x) = T∏\nt=1\nqt(πt) (2)\nwhere qt(πt) denotes the softmax activation of πt label in RNN output layer q at time t.\nThe CTC loss to be minimized is defined as the negative log likelihood of the ground truth character sequence y∗, i.e.\nLCTC ,− lnP (y∗|x). (3)\nThe probability distribution P (y|x) can be computed efficiently using the forward-backward algorithm as\nP (y|x) = |y′|∑ u=1 αt(u)βt(u) qt(y′u) , (4)\nwhere αt(u) is the forward variable, representing the total probability of all possible prefixes (y′1:u) that end with the u-th label, and βt(u) is the backward variable of all possible suffixes (y′u:U ) and vice versa. The network can then be trained with standard backpropagation by taking the derivative of the loss function with respect to qt(k) for any k label including the blank.\nSince CTC does not explicitly model inter-label dependencies based on the conditional independence assumption in Eq. (2), the model is limited to character-level language information. Therefore, lexicon or language models are commonly incorporated, like the hybrid framework [2, 3]."
    }, {
      "heading" : "2.2. Attention-based encoder-decoder",
      "text" : "Unlike the CTC approach, the attention model directly predicts each target without requiring intermediate representation or any assumptions, improving CER as compared to CTC when no external language model is used [7]. The model emits each label distribution at u conditioning on previous labels according to the following recursive equations:\nP (y|x) = ∏ u P (yu|x, y1:u−1) (5)\nh = Encoder(x) (6) yu ∼ AttentionDecoder(h, y1:u−1). (7)\nThe framework consists of two RNNs: Encoder and AttentionDecoder, so that it is able to learn two different lengths of sequences based on the cross-entropy criterion. Encoder transforms x, to highlevel representation h = (h1, · · · , hL) in Eq. (6), then AttentionDecoder produces the probability distribution over characters, yu, conditioned on h and all the characters seen previously y1:u−1 in Eq. (7). L is the number of skipped input frames, and L < T . Here, a special start-of-sentence(sos)/end-of-sentence(eos) token is added to the target set, so that the decoder completes the generation of the hypothesis when (eos) is emitted. The loss function of the attention model is computed from Eq. (5) as:\nLAttention , − lnP (y∗|x) = − ∑ u lnP (y∗u|x, y∗1:u−1) (8)\nwhere y∗1:u−1 is the ground truth of the previous characters. The attention mechanism aids in the decoding procedure by integrating all the inputs h into cu based on their attention weight vectors au ∈ RL+ over input L identifying where to focus at output step u. The following equations represent how to compute au and cu:\neu,l =  content-based: wT tanh(Wsu−1 + V hl + b)\nlocation-based: fu = F ∗ αu−1 wT tanh(Wsu−1 + V hl + Ufu,l + b)\n(9)\nau,l = exp(γeu,l)∑ l exp(γeu,l)\n(10)\ncu = ∑ l au,lhl (11)\nwhere w,W, V, F, U, b are trainable parameters, su−1 is the decoder state, γ is the sharpening factor [5], and * denotes convolution.\nau can be computed by the softmax of energy eu,l from two types of attention mechanisms: content-based and location-based [5] in Eq. (9). Both depend on the decoder state, su−1, and the content of input, hl. The location-based attention mechanism additionally uses convolutional features fu,l from the previous attention au−1.\nWith cu, su−1, and yu−1, the decoder generates next label yu and updates the state as:\nyu ∼ Generate(cu, su−1) (12) su = Recurrency(su−1, cu, yu), (13)\nwhere the Generate and Recurrency functions indicate a feedforward network and a recurrent network, respectively.\nIn practice, the approach has two main issues. (1) The model is weak on noisy speech data. The attention model is easily affected by noises, and generates misalignments because the model does not have any constraint that guides the alignments be monotonic as in DNN-HMM and CTC. (2) Another issue is that it is hard to be trained from scratch on larger input sequences via purely data-driven methods. To make training faster, the author [5, 7] constrains the attention mechanism to only consider inputs within a narrow range. However, this modification may limit the model’s capability to extract useful information from long character sequences."
    }, {
      "heading" : "2.3. Proposed model: Joint CTC-attention (MTL)",
      "text" : "The idea of our model is to use a CTC objective function as an auxiliary task to train the attention model encoder within the multitask learning (MTL) framework. Figure 1 illustrates the overall ar-\nchitecture of our framework, where the encoder network is shared with CTC and attention models. Unlike the attention model, the forward-backward algorithm of CTC can enforce monotonic alignment between speech and label sequences. We therefore expect that our framework is more robust in acquiring appropriate alignments in noisy conditions. Another advantage of using CTC as an auxiliary task is that the network is learned quickly. In our experiments, rather than solely depending on data-driven attention methods to estimate the desired alignments in long sequences, the forward-backward algorithm in CTC helps to speed up the process of estimating the desired alignment without the aid of rough estimates of the alignment which requires manual effort. The proposed objective is represented as follows by using both attention model in Eq. (8) and CTC in Eq. (3):\nLMTL = λLCTC + (1− λ)LAttention, (14)\nwith a tunable parameter λ : 0 ≤ λ ≤ 1."
    }, {
      "heading" : "3. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1. Data",
      "text" : "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16]. The CHiME-4 corpus was recorded using a tablet device in everyday environments - a cafe, a street junction, public transport, and a pedestrian area. As input features, we used 40 mel-scale filterbank coefficients, with their first and second order temporal derivatives to obtain a total of 120 feature values per frame. Evaluation was done on (1) ”eval92” for WSJ, and (2) ”et05 real isolated 1ch track” for CHiME-4. Hyperparameter selection was performed on the (1) ”dev93” for WSJ, and (2) ”dt05 multi isolated 1ch track” for CHiME-4. None of our experiments used any language model or lexicon information. For the attention model, we used only 32 distinct labels: 26 characters, apostrophe, period, dash, space, noise, and sos/eos tokens. The CTC\nmodel uses the blank instead of sos/eos, and our MTL model uses both sos/eos and the blank."
    }, {
      "heading" : "3.2. Training and Decoding",
      "text" : "Our model used four layers of 320 Bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder, and one layer of 320 LSTM in the decoder. The top two layers read every second of hidden states in the network below, reducing the utterance length by the factor of 4, L = T/4. Ten centered convolution filters of width 100 were used in the location-based attention model to extract\nthe features from the previous step alignment. We use the sharpening factor γ = 2. Each linear projection layer is followed by the BLSTM layer. The AdaDelta algorithm [19] with gradient clipping [20] was used for optimization. All the weights are initialized with the range [-0.1, 0.1] of uniform distribution. For our MTL, we tested three different task weights, λ: 0.2, 0.5, and 0.8.\nFor decoding, we used a beam search algorithm similar to [21] with the beam size 20 to reduce the computation cost. We adjusted the score by adding a length penalty, length(hyp) ∗ 0.3 for CHiME4 and length(hyp) ∗ 0.1 for WSJ experiments. Note that we do not use any lexicon or language models. Our framework is implemented with the Chainer library [22, 23]."
    }, {
      "heading" : "3.3. Results",
      "text" : "The results in Table 1 show that our proposed model MTL significantly outperformed both CTC and the attention model in CER on both the noisy CHiME-4 and clean WSJ tasks. Our model showed 7.0 - 9.5% and 6.6 - 10.3% relative improvements on validation and evaluation set, respectively. As we expected, the attention model showed relatively poor results on noisy corpus CHiME-4 compared to clean corpora WSJ. We observed that the benefit from our Joint CTC-attention increased in noisy condition, and when larger weights on CTC loss (i.e. λ = 0.8) achieved the best performance in CHiME-4, while λ = 0.5 showed the best performance in clean WSJ0.\nOne noticeable thing is that our framework significantly outperformed both the CTC and attention model even on clean corpora WSJ1 and WSJ0. It is possible that the CTC improved generalisation by less relying on the gold-standard transcription information,\nas it does not explicitly use character inter-dependencies. This point needs to be verified with additional experiments in future work.\nApart from the CER improvements, MTL can also be very helpful in accelerating the learning of the desired alignment. Figure 2 shows the learning curves of character accuracy on the validation sets of CHiME-4 over training epochs. Note that the accuracies of the attention and our MTL model were obtained with given gold standard history. As we use large λ giving more weight to CTC loss, the network learns quickly and converges early. Figure 3 visualizes the attention alignments between characters and acoustic frames over training epoch. We observed that our MTL model learned the desired alignment in an early training stage, the 5th epoch, while the attention model could not learn the desired alignment even at the 9th epoch. This result indicates that the CTC loss guided the alignment to be monotonic in our MTL approach."
    }, {
      "heading" : "4. CONCLUSIONS",
      "text" : "We have introduced a novel, general method for end-to-end speech recognition based on the multi-task learning approach using the CTC and the attention encoder-decoder. Our method improves performance by training a shared encoder using an auxiliary CTC objective function. Moreover, it significantly speeds up the process of learning the desired alignment without requiring manual restriction of the range of inputs, even in longer sequences. Our method has outperformed both CTC and an attention model on a speech recognition task in real-world noisy conditions as well as in clean conditions. This work can potentially be applied to any sequence-to-sequence learning framework."
    }, {
      "heading" : "5. REFERENCES",
      "text" : "[1] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764–1772.\n[2] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., “Deep speech: Scaling up endto-end speech recognition,” arXiv preprint arXiv:1412.5567, 2014.\n[3] Yajie Miao, Mohammad Gowayyed, and Florian Metze, “Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,” in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167–174.\n[4] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “End-to-end continuous speech recognition using attention-based recurrent nn: First results,” arXiv preprint arXiv:1412.1602, 2014.\n[5] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Advances in Neural Information Processing Systems, 2015, pp. 577–585.\n[6] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals, “Listen, attend and spell,” arXiv preprint arXiv:1508.01211, 2015.\n[7] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio, “End-to-end attentionbased large vocabulary speech recognition,” arXiv preprint arXiv:1508.04395, 2015.\n[8] Liang Lu, Xingxing Zhang, and Steve Renais, “On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5060–5064.\n[9] William Chan and Ian Lane, “On online attention-based speech recognition and joint mandarin character-pinyin training,” Interspeech 2016, pp. 3404–3408, 2016.\n[10] Abdel-rahman Mohamed, George E Dahl, and Geoffrey Hinton, “Acoustic modeling using deep belief networks,” Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 14–22, 2012.\n[11] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82–97, 2012.\n[12] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber, “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.\n[13] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.\n[14] Linguistic Data Consortium, “Csr-ii (wsj1) complete,” Linguistic Data Consortium, Philadelphia, vol. LDC94S13A, 1994.\n[15] John Garofalo, David Graff, Doug Paul, and David Pallett, “Csr-i (wsj0) complete,” Linguistic Data Consortium, Philadelphia, vol. LDC93S6A, 2007.\n[16] Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer, “An analysis of environment, microphone and data simulation mismatches in robust speech recognition,” in Computer Speech and Language, to appear.\n[17] Sepp Hochreiter and Jürgen Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[18] Alan Graves, Navdeep Jaitly, and Abdel-rahman Mohamed, “Hybrid speech recognition with deep bidirectional lstm,” in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273–278.\n[19] Matthew D Zeiler, “Adadelta: an adaptive learning rate method,” arXiv preprint arXiv:1212.5701, 2012.\n[20] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, “On the difficulty of training recurrent neural networks,” arXiv preprint arXiv:1211.5063, 2012.\n[21] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le, “Sequence to sequence learning with neural networks,” in Advances in neural information processing systems, 2014, pp. 3104–3112.\n[22] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton, “Chainer: a next-generation open source framework for deep learning,” in Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015.\n[23] Preferred Networks, “Chainer,” in ”http://chainer.org/”."
    } ],
    "references" : [ {
      "title" : "Towards end-to-end speech recognition with recurrent neural networks",
      "author" : [ "Alex Graves", "Navdeep Jaitly" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764–1772.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep speech: Scaling up endto-end speech recognition",
      "author" : [ "Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates" ],
      "venue" : "arXiv preprint arXiv:1412.5567, 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding",
      "author" : [ "Yajie Miao", "Mohammad Gowayyed", "Florian Metze" ],
      "venue" : "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167–174.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "End-to-end continuous speech recognition using attention-based recurrent nn: First results",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.1602, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015, pp. 577–585.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Listen, attend and spell",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1508.01211, 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "End-to-end attentionbased large vocabulary speech recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1508.04395, 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition",
      "author" : [ "Liang Lu", "Xingxing Zhang", "Steve Renais" ],
      "venue" : "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5060–5064.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On online attention-based speech recognition and joint mandarin character-pinyin training",
      "author" : [ "William Chan", "Ian Lane" ],
      "venue" : "Interspeech 2016, pp. 3404–3408, 2016.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Acoustic modeling using deep belief networks",
      "author" : [ "Abdel-rahman Mohamed", "George E Dahl", "Geoffrey Hinton" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 14–22, 2012.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82–97, 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Csr-ii (wsj1) complete",
      "author" : [ "Linguistic Data Consortium" ],
      "venue" : "Linguistic Data Consortium, Philadelphia, vol. LDC94S13A, 1994.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Csr-i (wsj0) complete",
      "author" : [ "John Garofalo", "David Graff", "Doug Paul", "David Pallett" ],
      "venue" : "Linguistic Data Consortium, Philadelphia, vol. LDC93S6A, 2007.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An analysis of environment, microphone and data simulation mismatches in robust speech recognition",
      "author" : [ "Emmanuel Vincent", "Shinji Watanabe", "Aditya Arie Nugraha", "Jon Barker", "Ricard Marxer" ],
      "venue" : "Computer Speech and Language, to appear.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Hybrid speech recognition with deep bidirectional lstm",
      "author" : [ "Alan Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed" ],
      "venue" : "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273–278.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701, 2012.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1211.5063, 2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le" ],
      "venue" : "Advances in neural information processing systems, 2014, pp. 3104–3112.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Chainer: a next-generation open source framework for deep learning",
      "author" : [ "Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton" ],
      "venue" : "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Chainer",
      "author" : [ "Preferred Networks" ],
      "venue" : "”http://chainer.org/”.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 0
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 3,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 4,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 5,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 6,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 7,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 8,
      "context" : "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 181,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "acoustic model, contextdependent phone model, pronunciation model, and language model) based on conditional independence assumptions (including Markov assumptions) and approximations [10, 11].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "acoustic model, contextdependent phone model, pronunciation model, and language model) based on conditional independence assumptions (including Markov assumptions) and approximations [10, 11].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].",
      "startOffset" : 134,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].",
      "startOffset" : 134,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].",
      "startOffset" : 134,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].",
      "startOffset" : 134,
      "endOffset" : 147
    }, {
      "referenceID" : 12,
      "context" : "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].",
      "startOffset" : 184,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].",
      "startOffset" : 184,
      "endOffset" : 197
    }, {
      "referenceID" : 4,
      "context" : "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].",
      "startOffset" : 184,
      "endOffset" : 197
    }, {
      "referenceID" : 5,
      "context" : "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].",
      "startOffset" : 184,
      "endOffset" : 197
    }, {
      "referenceID" : 6,
      "context" : "Since the attention model does not use any conditional independence assumption, it has often shown to improve Character Error Rate (CER) than CTC when no external language model is used [7].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 6,
      "context" : "Another issue is that the model is hard to be learned from scratch due to the misalignment on longer input sequences, and therefore a windowing technique is commonly used to limit the area explored by the attention mechanism [7], but several parameters for windowing need to be determined manually depending on the training data.",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 11,
      "context" : "The key idea of CTC [12] is to use intermediate label representation π = (π1, · · · , πT ), allowing repetitions of labels and occurrences of a blank label (−), which represents the special emission without labels, i.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "Therefore, lexicon or language models are commonly incorporated, like the hybrid framework [2, 3].",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Therefore, lexicon or language models are commonly incorporated, like the hybrid framework [2, 3].",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "Unlike the CTC approach, the attention model directly predicts each target without requiring intermediate representation or any assumptions, improving CER as compared to CTC when no external language model is used [7].",
      "startOffset" : 214,
      "endOffset" : 217
    }, {
      "referenceID" : 4,
      "context" : "where w,W, V, F, U, b are trainable parameters, su−1 is the decoder state, γ is the sharpening factor [5], and * denotes convolution.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "au can be computed by the softmax of energy eu,l from two types of attention mechanisms: content-based and location-based [5] in Eq.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "To make training faster, the author [5, 7] constrains the attention mechanism to only consider inputs within a narrow range.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "To make training faster, the author [5, 7] constrains the attention mechanism to only consider inputs within a narrow range.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "Our model used four layers of 320 Bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder, and one layer of 320 LSTM in the decoder.",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "Our model used four layers of 320 Bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder, and one layer of 320 LSTM in the decoder.",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "The AdaDelta algorithm [19] with gradient clipping [20] was used for optimization.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 19,
      "context" : "The AdaDelta algorithm [19] with gradient clipping [20] was used for optimization.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "For decoding, we used a beam search algorithm similar to [21] with the beam size 20 to reduce the computation cost.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "Our framework is implemented with the Chainer library [22, 23].",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "Our framework is implemented with the Chainer library [22, 23].",
      "startOffset" : 54,
      "endOffset" : 62
    } ],
    "year" : 2016,
    "abstractText" : "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoderdecoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the attention model has shown poor results especially in noisy condition and is hard to be trained in the initial training stage with long input sequences, as compared with CTC. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-toright constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 6.6-10.3% relative improvements in Character Error Rate (CER).",
    "creator" : "LaTeX with hyperref package"
  }
}