{
  "name" : "1704.04341.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Environment-Independent Task Specifications via GLTL",
    "authors" : [ "Michael L. Littman", "Ufuk Topcu", "Jie Fu", "Charles Isbell", "Min Wen", "James MacGlashan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a new task-specification language for Markov decision processes that is designed to be an improvement over reward functions by being environment independent. The language is a variant of Linear Temporal Logic (LTL) that is extended to probabilistic specifications in a way that permits approximations to be learned in finite time. We provide several small environments that demonstrate the advantages of our geometric LTL (GLTL) language and illustrate how it can be used to specify standard reinforcementlearning tasks straightforwardly."
    }, {
      "heading" : "1 Introduction",
      "text" : "The thesis of this work is that (1) rewards are an excellent way of controlling the behavior of agents, but (2) rewards are difficult to use for specifying behaviors in an environment-independent way, therefore (3) we need intermediate representations between behavior specifications and reward functions.\nThe intermediate representation we propose is a novel variant of linear temporal logic that is modified to be probabilistic so as to better support reinforcement-learning tasks. Linear temporal logic has been used in the past to specify reward functions that depend on temporal sequences (Bacchus et al., 1996); here, we expand the role to provide a robust and consistent semantics that allows desired behaviors to be specified in an environment-independent way. Briefly, our approach involves the specification of tasks via temporal operators that have a constant probability of expiring on each step. As such, it bears a close relationship to the notion of discounting in standard Markov decision process (MDP) reward functions (Puterman, 1994).\nAt a philosophical level, we are viewing behavior specification as a kind of programming problem. That is, if we think of a Markov decision process (MDP) as an input, a\nreward function as a program, and a policy as an output, then reinforcement learning can be viewed as a process of program interpretation. We would like the same program to work across all possible inputs."
    }, {
      "heading" : "1.1 Specifying behavior via reward functions",
      "text" : "An MDP consists of a finite state space, action space, transition function, and reward function. Given an environment, an agent should behave in a way that maximizes cumulative discounted expected reward. The problems of learning and planning in such environments have been vigorously studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009). A reinforcement-learning (RL) agent needs to learn to maximize cumulative discounted expected reward starting with an incomplete model of the MDP itself.\nFor “programming” reinforcement-learning agents, the state of the art is to define a reward function and then for the learning agent to interact with the environment to discover ways to maximize its reward. Reward-based specifications have proven to be extremely valuable for optimal planning in complex, uncertain environments (Russell & Norvig, 1994). However, we can show that reward functions, as they are currently structured, are very difficult to work with as a way of reliably specifying tasks. The best use case for reward functions is when the utilities of all actions and outcomes can be expressed in a consistent unit, for example, time or money or energy. In reality, however, putting a meaningful dollar figure on scuffing a wall or dropping a clean fork is challenging. When informally adding negative rewards to undesirable outcomes, it is difficult to ensure a consistent semantics over which planning and reasoning can be carried out. Further, reward values often need to be changed if the environment itself changes— they are not environment independent. Therefore, to get a system to exhibit a desired behavior, it can be necessary to try different reward structures and carry out learning multiple times in the target environment, greatly undermining the purpose of autonomous learning in the first place. ar X iv :1\n70 4.\n04 34\n1v 1\n[ cs\n.A I]\n1 4\nA pr\n2 01\nConsider the simple example MDP in Figure 1. The agent is choosing between a1 and a2 in the initial state s0. Choosing a1 causes the agent to pass through bad state b1 for one step, then to continue on to the goal g. Action a2, however, results in a probabilistic transition to s1 (with probability 1−p) or bad state b2 (with slip probability p). From s1, the agent can continue on to the goal. If it reaches b2, it gets stuck there forever.\nLet’s say our desired behavior is “maximize the probability of reaching g without hitting a bad state”. (A bad state could be something like colliding with a wall or bumping up against a table.) The probability of success of a1 is zero and a2 is 1− p. Thus, for any 0 ≤ p < 1, it is better to take action a2.\nWhat reward function encourages this behavior? For concreteness, let’s assume a discount of γ = 0.8 and a reward of +1 for reaching the goal. We can assign bad states a value of −r. In the case where p = 0.1, setting r > 0.16 encourages the desired behavior.\nConsider, though, what happens if the slip probability is p = 0.3. Now, there is no value of r for which a2 is preferred to a11. That is, it has become impossible to find a reward function that creates the correct incentives for the desired behavior to be optimal.\nThis example is perhaps a bit contrived, but we have observed the same phenomenon in large and natural state spaces as well. The reason for this result is that reward functions force us to express utility in terms of the discounted expected visit frequency of states. In this case, we are stuck trying to make a tradeoff between the certainty of encountering a bad state once and the possibility of encountering a bad state repeatedly. Since we are trying to maximize the probability of zero encounters with a bad state, the expected number of encounters is only useful for distinguishing zero from more than zero—the objective cannot be translated into a reward function when bad states\n1Actually, r < −24/50 works for this example, but that is tantamount to rewarding the agent for bumping into things— something bound to result in other problems.\nare unavoidable."
    }, {
      "heading" : "1.2 Specifying behavior via LTL",
      "text" : "An alternative to specifying tasks via reward functions is to use a formal specification like linear temporal logic or LTL (Manna & Pnueli, 1992; Baier & Katoen, 2008).\nLinear temporal logic formulas are built up from a set of atomic propositions; the logic connectives: negation (¬), disjunction (∨), conjunction (∧) and material implication (→); and the temporal modal operators: next (©), always ( ), eventually (♦) and until (U). A wide class of properties including safety ( ¬b), goal guarantee (♦g), progress ( ♦g), response ( (b → ♦g)), and stability (♦ g), where b and g are atomic propositions, can be expressed as LTL formulas. More complicated specifications can be obtained from the composition of such simple formulas. For example, the specification of “repeatedly visit certain locations of interest in a given order while avoiding certain other unsafe or undesirable locations” can be obtained through proper composition of simpler safety and progress formulas (Manna & Pnueli, 1992; Baier & Katoen, 2008).\nReturning to the example in Figure 1, the task is to avoid b states until g is reached: ¬bUg. Given an LTL specification and an environment, an agent, for example, should adopt a behavior that maximizes the probability that the specification is satisfied. One advantage of this approach is its ability to specify tasks that cannot be expressed using simple reward functions (like the example MDP in Section 1.1). Indeed, in the context of reinforcement-learning problems, we have found it very natural to express standard MDP task specifications using LTL.\nStandard MDP tasks can be expressed well using these temporal operators. For example:\n• Goal-based tasks like mountain car (Moore, 1991): If p represents the attribute of being at the goal (the top of the hill, say), ♦p corresponds to eventually reaching the goal.\n• Avoidance-type tasks like cart pole (Barto et al., 1983): If q represents the attribute of being in the failure state (dropping the pole, say), ¬q corresponds to always avoiding the failure state.\n• Sequence tasks like taxi (Dietterich, 2000): If p represents some task being completed (getting the passenger, say) and q represents another task being completed (delivering the passenger, say), ♦(p∧♦q) corresponds to eventually completing the first task, then, from there, eventually completing the second task.\n• Stabilizing tasks like pendulum swing up (Atkeson, 1994): If p represents the property that needs to be stabilized (the pendulum being above the vertical, say),\n♦ p corresponds to eventually achieving and continually maintaining the desired property.\n• Approach-avoid tasks like the 4×3 grid (Russell & Norvig, 1994): If p represents the attribute of being at the goal (the upper right corner the grid, say), and q represents the attribute of being at a bad state (the state below it, say), ¬qUp corresponds to avoiding the bad state en route to the goal.\nOn the other hand, there are barriers to straightforwardly adopting temporal logic-based languages in a reinforcement-learning setup. The most significant is that we can show that it is simply impossible to learn to satisfy classical LTL specifications in some cases. A key property for being able to learn near-optimal policies efficiently in the context of reward-based MDPs is what is known as the Simulation Lemma (Kearns & Singh, 1998). Informally, it says that, for any MDP and any > 0, there exists an ′ > 0 such that finding optimal policies in an ′-close model of the real environment results in behavior that is -close to optimal in the real environment.\nUnfortunately, tasks specified via LTL do not have this property. In particular, there is an MDP and an > 0 such that no ′-close approximation for ′ > 0 is sufficient to produce a policy with -close satisfaction probability.\nConsider the MDP in Figure 2. If we want to find a behavior that nearly maximizes the probability of satisfying the specification g (stay in the good state forever), we need accurate estimates of p1 and p2. If p1 = p2 = 1 or p1 < 1 and p2 < 1, either policy is equally good. If p1 = 1 and p2 < 1, only action a1 is near optimal. If p2 = 1 and p1 < 1, only action a2 is near optimal. As there is no finite bound on the number of learning trials needed to distinguish p1 = 1 from p1 < 1, a near optimal behavior cannot be found in worst-case finite time. LTL expressions are simply too unforgiving to be used with any confidence in a learning setting.\nIn this work, we develop a hybrid approach for specifying behavior in reinforcement learning that combines the\nstrengths of both reward functions and temporal logic specifications."
    }, {
      "heading" : "2 Learning To Satisfy LTL",
      "text" : "While provable guarantees of efficiency and optimality have been at the core of the literature on learning (Fiechter, 1994; Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Li et al., 2011), correctness with respect to complicated, high-level task specifications—during the learning itself or in the behavior resulting from the learning phase— has attracted limited attention (Abbeel & Ng, 2005)."
    }, {
      "heading" : "2.1 Geometric linear temporal logic",
      "text" : "We present a variant of LTL we call geometric linear temporal logic (GLTL) that builds on the logical and temporal operators in LTL while ensuring learnability. The idea of GLTL is roughly to restrict the period of validity of the temporal operators to bounded windows—similar to the bounded semantics of LTL (Manna & Pnueli, 1992). To this end, GLTL introduces operators of the form of ♦µb with the atomic proposition b, which is interpreted as “b eventually holds within k time steps where k is a random variable following a geometric distribution with parameter µ.” Similar semantics stochastically restricting the window of validity for other temporal operators are also introduced.\nThis kind of geometric decay fits very nicely with MDPs for a few reasons. It can be viewed as a generalization of reward discounting, which is already present in many MDP models. It also avoids unnecessarily expanding the specification state space by only requiring extra states to represent events and not simply the passage of time.\nUsing G1(µ) to represent the geometric distribution with parameter µ, the temporal operators are:\n• ♦µp: p is achieved in the next k steps, k ∼ G1(µ).\n• µq: q holds for the next k steps, k ∼ G1(µ).\n• q Uµq: q must hold at least until p becomes true, which itself must be achieved in the next k steps, k ∼ G1(µ).\nReturning to our earlier example from Figure 2, evaluating the probability of satisfaction for g requires infinite precision in the learned transition probabilities in the environment. Consider instead evaluating µg in this environment. An encoding of the specification for this example is shown in Figure 3 (Third). We call it a specification MDP, as it specifies the task using states (derived from the formula), actions (representing conditions), and probabilities (capturing the stochasticity of operator expiration). This example says that, from the initial state q0, encountering any state where g is not true results in immediately failing the specification. In contrast, encountering any state\nwhere g is true results in either continued evaluation (with probability µ) or success (with probability 1− µ). Success represents the idea that the temporal window in which g must hold true has expired without g being violated.\nComposing these two MDPs leads to the composite MDP in Figure 3 (Fourth). The true satisfaction probability for action ai is 1−µ(1−µ+pi) . Thus, if µ = .9, the dependence of this value on is .1.1+ , which is well behaved for all values of . The sensitivity of the computed satisfaction probability has a maximum 1/(1− µ)2 dependence on the accuracy of the estimate of . Thus, GLTL is considerably more friendly to learning than is LTL.\nReturning to the MDP example in Figure 1, we find that GLTL is also more expressive than rewards. The GLTL formula ¬q Uµp can be translated to a specification MDP. Essentially, the idea is that encountering a bad state (q) even once or running out of time results in specification failure. Maximizing the satisfaction of this GLTL formula results in taking action a1 regardless of the value of p. That is, it is an environment-independent specification of the task.\nThe reason the GLTL formulation is able to succeed where standard rewards fail is that the GLTL formula results in an augmentation of the state space so that the reward function can depend on whether a bad state has yet been encountered. On the first encounter, a penalty can be issued. After the first encounter, no additional penalty is added. By composing the environment MDP with this bit of internal memory, the task can be expressed provably correctly and in an environment-independent way."
    }, {
      "heading" : "3 Related Work",
      "text" : "Discounting has been used in previous temporal models. In quantitative temporal logic, it gives more weight to the satisfaction of a logic property in the near future than the far future. De Alfaro et al. (2003, 2004) augment computation tree logic (CTL) with discounting and develop fixpointbased algorithms for checking such properties for probabilistic systems and games. Almagor et al. (2014) explicitly refine the “eventually” operator of LTL to a discounting operator such that the longer it takes to fulfill the task the smaller the value of satisfaction. Further, they show that discounted LTL is more expressive than discounted CTL. They use both discounted until and undiscounted until for expressing traditional eventually as well as its discounted version. However, algorithms for model checking and synthesis discounted LTL for probabilistic systems and games are yet to be developed.\nLTL has been used extensively in robotics domains. Work on the trustworthiness of autonomous robots, automated verification and synthesis with provable correctness with respect to temporal logic-based specifications in motion, task, and mission planning have attracted considerable at-\ntention recently. The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011). While temporal logic had initially focused on reasoning about temporal and logical relations, its dialects with probabilistic modalities have been used increasingly for robotics applications (Baier & Katoen, 2008; De Alfaro, 1998; Kwiatkowska et al., 2002)."
    }, {
      "heading" : "4 Generating Specification MDPs",
      "text" : "Similar to LTL, GLTL formulas are built from a set of atomic propositions AP , Boolean operators ∧ (conjunction), ¬ (negation) and temporal operator Uµ (µ-until). Useful operators such as∨ (disjunction),♦µ (µ-eventually) and µ (µ-always) can be derived from these basic operators.\nGLTL formulas can be converted to the corresponding specification MDPs recursively, with the operator precedence listed in descending order in Table 1. Operators of the same precedence are read from right to left. For example, µ1♦µ2ϕ = ( µ1(♦µ2ϕ)), ϕ1Uµ1ϕ2Uµ2ϕ3 = (ϕ1Uµ1(ϕ2Uµ2ϕ3)).\nAssume ϕ,ϕ1, ϕ2 are GLTL formulas in the following discussion.\n• b, where b ∈ AP is an atomic proposition: A specification MDP Mb = ({sini, acc, rej}, {a}, T,R) for b can be constructed such that, if p holds at sini, the transition (sini, a, acc) is taken with probability 1; otherwise, the transition (sini, a, rej) is taken with probability 1.\n• ¬ϕ: A specification MDP M¬ϕ can be constructed from a specification MDP Mϕ by swapping the terminal states acc and rej.\n• ϕ1 ∧ ϕ2: A specification MDP Mϕ1∧ϕ2 = (S,A, T,R) can be constructed from specification MDPs Mϕ1 = (S1, A1, T1, R1)\nand Mϕ2 = (S2, A2, T2, R2) such that (1) S = (S1\\{rej1}) × (S2\\{rej2}) ⋃ {rej}, and the accepting state is acc = (acc1, acc2); (2) A = A1 × A2; (3) for all transitions (s1, a1, s′1) of Mϕ1 and (s2, a2, s ′ 2) of Mϕ2 , if either s ′ 1 = rej1 or s′2 = rej2, let T ((s1, s2), (a1, a2), rej) = T1(s1, a1, s ′ 1)T2(s2, a2, s ′ 2); otherwise, T ((s1, s2), (a1, a2), (s′1, s ′ 2)) = T1(s1, a1, s ′ 1)T2(s2, a2, s ′ 2).\n• ϕ1 ∨ ϕ2 = ¬(¬ϕ1 ∧ ¬ϕ2).\n• ϕ1Uµϕ2: The operator µ-until has two operands, ϕ1 and ϕ2, which generate specification MDPs Mϕ1 = (S1, A1, T1, R1) and Mϕ2 = (S2, A2, T2, R2). The new specification MDP Mϕ1Uµϕ2 = (S,A, T,R) is constructed from Mϕ1 and Mϕ2 : S = (S1\\{acc1, rej1}) × (S2\\{acc2, rej2}) ⋃ {acc, rej},\nwhere acc and rej are the accepting and rejecting state, respectively, and sini = (sini1 , s ini 2 ) ∈ S is the initial state; A = A1 × A2; for all s = (s1, s2) ∈ S\\{acc, rej}, a = (a1, a2) ∈ A, s′1 ∈ S1 and s′2 ∈ S2, if T1(s1, a1, s′1) > 0 and T2(s2, a2, s ′ 2) > 0, a transition (s, a, s′) is added to Mϕ1Uµϕ2 with probability T (s, a, s′) as specified in Table 2.\nHere are some intuitions behind the construction of T . The formula ϕ1Uµϕ2 means that, within some stochastically decided time period k, we would like to successfully implement task ϕ2 in at most k steps without ever failing in task ϕ1. If we observe a success in Mϕ2 (that is, the specification reaches acc2) before ϕ1 fails (that is the sepcification reaches rej1), Mϕ1Uµϕ2 goes to state acc for sure; if we observe a failure in Mϕ1 (that, the specifcation reaches rej1) before succeeding in Mϕ2 (that is, the specification reaches acc2), Mϕ1Uµϕ2 goes to state rej for sure. In all other cases, Mϕ1Uµϕ2 primarily keeps track of the transitions inMϕ1 andMϕ2 , with a tiny probability of failing immediately, which corresponds to the operator expiring.\n• ♦µϕ2: As in the semantics of LTL, µ-eventually ♦µϕ2 = True Uµϕ2. Hence, given a specification\nTable 2: Transition (s, a, s′) in Mϕ1Uµϕ2 constructed from a transition (s1, a1, s′1) in Mϕ1 and a transition (s2, a2, s ′ 2)\nin Mϕ2 . Here, p(s ′|s′1, s′2) = T (s,a,s′) T1(s1,a1,s′1)T2(s2,a2,s ′ 2)\n. That is, to get the transition probability, multiple the p column by the corresponding T1 and T2 transition probabilities.\ns′1 s ′ 2 s ′ p(s′|s′1, s′2) acc1 acc2 acc 1 acc1 rej2 (s′1, s ′ 2) 1− µ\nrej µ\nacc1 S2\\{acc2, rej2} (sini1 , s ′ 2) 1− µ rej µ rej1 acc2 acc 1 rej1 S2\\{acc2} rej 1\nS1\\{acc1, rej1} acc2 acc 1 S1\\{acc1, rej1} rej2 (s′1, s ini 2 ) 1− µ\nrej µ\nS1\\{acc1, rej1} S2\\{acc2, rej2} (s′1, s ′ 2) 1− µ\nrej µ\nMDP Mϕ2 = (S2, A2, T2, R2) for ϕ2, we can construct a specification MDP M♦µϕ2 = (S,A, T,R) for ♦µϕ2: S = S2, sini = sini2 , acc = acc2, rej = rej2; A = A2; transitions of M♦µϕ2 are modified from those of Mϕ2 as in Table 3. Informally, ♦µϕ2 is satisfied if we succeed in task ϕ2 within the stochastic observation time period.\n• µϕ2: µ-always ϕ2 is equivalent to ¬♦µ¬ϕ2 = ¬(♦µ(¬ϕ2)). In other words, µϕ2 is satisfied if we did not witness a failure of ϕ2 within the stochastic observation time period. The transitions of a specification MDP M µϕ2 can be constructed from Table 3, or directly from Table 4.\nUsing the transitions as described, a given GLTL formula can be converted into a specification MDP. To satisfy the specification in a given environment, a joint MDP is created as follows:\n1. Take the cross product of the MDP representing the environment and the specification MDP.\nThe resulting policy is one that maximizes the probability of satisfying the given formula where the random events are both the transitions in the environment and the stochastic transitions in the specification MDP. Such policies tend to prefer satisfying formulas quickly, as that increases the chance of successful completion before operators expire."
    }, {
      "heading" : "5 Example Domain",
      "text" : "Consider the following formula:\n(¬blueUµred) ∧ (♦µ(red ∧ ♦µgreen)).\nIt specifies a task of reaching a red state without encountering a blue state and, once a red state is reached, going to a green state.\nFigure 5 illustrates a grid world environment in which this task can be carried out. It consists of different colored grid cells. The agent can move to one of the four adject cells to its current position with a north, south, east, or west action. However, selecting an action for one direction has a 0.02 probability of moving in the one of the three other directions. This stochastic movement causes the agent to keep\nits distance from dangerous grid cells that could result in task failure, whenever possible. The solid line in the figure traces the path of the optimal policy of following this specification in the grid. As can be seen, the agent moves to red and then green. Note that this behavior can be very difficult to encode in a standard reward function as both green and red need to be given positive reward and therefore either would be a sensible place for the agent to stop.\nFigure 5 illustrates a grid world environment in which the blue cells create a partial barrier between the red and green cells. As a result of the “until” in the specification, the agent goes around the blue wall to get to the red cell. However, since the prohibition against blue cells is lifted once the red cell is reached, it goes directly through the barrier to reach green.\nThese 25-state environments become 98-state MDPs when combined with the specification MDP."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In contrast to standard MDP reward functions, we have provided an environment-independent specification for tasks. We have shown that this specification language can capture standard tasks used in the MDP community and that it can be automatically incorporated into an environment MDP to create a fixed MDP to solve. Maximizing reward in this resulting MDP maximizes the probability of satisfying the task specification.\nFuture work includes inverse reinforcement learning of task specifications and techniques for accelerating planning."
    } ],
    "references" : [ {
      "title" : "Exploration and apprenticeship learning in reinforcement learning",
      "author" : [ "Abbeel", "Pieter", "Ng", "Andrew Y" ],
      "venue" : "In Proceedings of the 22nd International Conference on Machine Learning,",
      "citeRegEx" : "Abbeel et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Abbeel et al\\.",
      "year" : 2005
    }, {
      "title" : "Discounting in ltl",
      "author" : [ "Almagor", "Shaull", "Boker", "Udi", "Kupferman", "Orna" ],
      "venue" : "In Tools and Algorithms for the Construction and Analysis of Systems,",
      "citeRegEx" : "Almagor et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Almagor et al\\.",
      "year" : 2014
    }, {
      "title" : "Using local trajectory optimizers to speed up global optimization in dynamic programming",
      "author" : [ "Atkeson", "Christopher G" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Atkeson and G.,? \\Q1994\\E",
      "shortCiteRegEx" : "Atkeson and G.",
      "year" : 1994
    }, {
      "title" : "Rewarding behaviors",
      "author" : [ "Bacchus", "Fahiem", "Boutilier", "Craig", "Grove", "Adam" ],
      "venue" : "In Proceedings of the Thirteenth National Conference on Artificial Intelligence,",
      "citeRegEx" : "Bacchus et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bacchus et al\\.",
      "year" : 1996
    }, {
      "title" : "Principles of Model Checking",
      "author" : [ "Baier", "Christel", "Katoen", "Joost-Pieter" ],
      "venue" : null,
      "citeRegEx" : "Baier et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Baier et al\\.",
      "year" : 2008
    }, {
      "title" : "Neuronlike adaptive elements that can solve difficult learning control problems",
      "author" : [ "Barto", "Andrew G", "Sutton", "Richard S", "Anderson", "Charles W" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics,",
      "citeRegEx" : "Barto et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Barto et al\\.",
      "year" : 1983
    }, {
      "title" : "Decision-theoretic planning: Structural assumptions and computational leverage",
      "author" : [ "Boutilier", "Craig", "Dean", "Thomas", "Hanks", "Steve" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Boutilier et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Boutilier et al\\.",
      "year" : 1999
    }, {
      "title" : "Formal verification of probabilistic systems",
      "author" : [ "De Alfaro", "Luca" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Alfaro and Luca.,? \\Q1998\\E",
      "shortCiteRegEx" : "Alfaro and Luca.",
      "year" : 1998
    }, {
      "title" : "Discounting the future in systems theory",
      "author" : [ "De Alfaro", "Luca", "Henzinger", "Thomas A", "Majumdar", "Rupak" ],
      "venue" : "In Automata, Languages and Programming,",
      "citeRegEx" : "Alfaro et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Alfaro et al\\.",
      "year" : 2003
    }, {
      "title" : "Model checking discounted temporal properties",
      "author" : [ "De Alfaro", "Luca", "Faella", "Marco", "Henzinger", "Thomas A", "Majumdar", "Rupak", "Stoelinga", "Mariëlle" ],
      "venue" : null,
      "citeRegEx" : "Alfaro et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Alfaro et al\\.",
      "year" : 2004
    }, {
      "title" : "Hierarchical reinforcement learning with the MAXQ value function decomposition",
      "author" : [ "Dietterich", "Thomas G" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Dietterich and G.,? \\Q2000\\E",
      "shortCiteRegEx" : "Dietterich and G.",
      "year" : 2000
    }, {
      "title" : "Ltl control in uncertain environments with probabilistic satisfaction",
      "author" : [ "Ding", "Xu Chu", "Smith", "Stephen L", "Belta", "Calin", "Rus", "Daniela" ],
      "venue" : "guarantees. CoRR,",
      "citeRegEx" : "Ding et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient reinforcement learning",
      "author" : [ "Fiechter", "Claude-Nicolas" ],
      "venue" : "Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory,",
      "citeRegEx" : "Fiechter and Claude.Nicolas.,? \\Q1994\\E",
      "shortCiteRegEx" : "Fiechter and Claude.Nicolas.",
      "year" : 1994
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Kearns", "Michael", "Singh", "Satinder" ],
      "venue" : "In Proceedings of the 15th International Conference on Machine Learning, pp",
      "citeRegEx" : "Kearns et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 1998
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Kearns", "Michael J", "Singh", "Satinder P" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kearns et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 2002
    }, {
      "title" : "Temporal-logic-based reactive mission and motion planning",
      "author" : [ "H. Kress-Gazit", "G.E. Fainekos", "G.J. Pappas" ],
      "venue" : "IEEE Tans. on Robotics,",
      "citeRegEx" : "Kress.Gazit et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kress.Gazit et al\\.",
      "year" : 2009
    }, {
      "title" : "Correct, reactive robot control from abstraction and temporal logic specifications",
      "author" : [ "H. Kress-Gazit", "T. Wongpiromsarn", "U. Topcu" ],
      "venue" : "IEEE RAM,",
      "citeRegEx" : "Kress.Gazit et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kress.Gazit et al\\.",
      "year" : 2011
    }, {
      "title" : "Prism: Probabilistic symbolic model checker",
      "author" : [ "Kwiatkowska", "Marta", "Norman", "Gethin", "Parker", "David" ],
      "venue" : "In Computer Performance Evaluation: Modelling Techniques and Tools,",
      "citeRegEx" : "Kwiatkowska et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kwiatkowska et al\\.",
      "year" : 2002
    }, {
      "title" : "Control of Markov decision processes from PCTL specifications",
      "author" : [ "M. Lahijanian", "S.B. Andersson", "C. Belta" ],
      "venue" : "In Proc. of the American Control Conference,",
      "citeRegEx" : "Lahijanian et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lahijanian et al\\.",
      "year" : 2011
    }, {
      "title" : "Knows what it knows: A framework for self-aware learning",
      "author" : [ "Li", "Lihong", "Littman", "Michael L", "Walsh", "Thomas J", "Strehl", "Alexander L" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Li et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "Synthesis of reactive switching protocols from temporal logic specifications",
      "author" : [ "Liu", "Jun", "Ozay", "Necmiye", "Topcu", "Ufuk", "Murray", "Richard M" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Liu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "The Temporal Logic of Reactive & Concurrent Sys",
      "author" : [ "Manna", "Zohar", "Pnueli", "Amir" ],
      "venue" : null,
      "citeRegEx" : "Manna et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Manna et al\\.",
      "year" : 1992
    }, {
      "title" : "Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued spaces",
      "author" : [ "Moore", "Andrew W" ],
      "venue" : "In Proc. Eighth International Machine Learning Workshop,",
      "citeRegEx" : "Moore and W.,? \\Q1991\\E",
      "shortCiteRegEx" : "Moore and W.",
      "year" : 1991
    }, {
      "title" : "Markov Decision Processes— Discrete Stochastic Dynamic Programming",
      "author" : [ "Puterman", "Martin L" ],
      "venue" : null,
      "citeRegEx" : "Puterman and L.,? \\Q1994\\E",
      "shortCiteRegEx" : "Puterman and L.",
      "year" : 1994
    }, {
      "title" : "Artificial Intelligence: A Modern Approach",
      "author" : [ "Russell", "Stuart J", "Norvig", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Russell et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Russell et al\\.",
      "year" : 1994
    }, {
      "title" : "Reinforcement learning in finite MDPs: PAC analysis",
      "author" : [ "Strehl", "Alexander L", "Li", "Lihong", "Littman", "Michael L" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Strehl et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning from Delayed Rewards",
      "author" : [ "Watkins", "Christopher J.C. H" ],
      "venue" : "PhD thesis, King’s College,",
      "citeRegEx" : "Watkins and H.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins and H.",
      "year" : 1989
    }, {
      "title" : "Robust control of uncertain markov decision processes with temporal logic specifications",
      "author" : [ "Wolff", "Eric M", "Topcu", "Ufuk", "Murray", "Richard M" ],
      "venue" : "In Proc. of the IEEE Conference on Decision and Control,",
      "citeRegEx" : "Wolff et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wolff et al\\.",
      "year" : 2012
    }, {
      "title" : "Receding horizon temporal logic planning",
      "author" : [ "T. Wongpiromsarn", "U. Topcu", "R.M. Murray" ],
      "venue" : "IEEE T. on Automatic Control,",
      "citeRegEx" : "Wongpiromsarn et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wongpiromsarn et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Linear temporal logic has been used in the past to specify reward functions that depend on temporal sequences (Bacchus et al., 1996); here, we expand the role to provide a robust and consistent semantics that allows desired behaviors to be specified in an environment-independent way.",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "The problems of learning and planning in such environments have been vigorously studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009).",
      "startOffset" : 126,
      "endOffset" : 186
    }, {
      "referenceID" : 25,
      "context" : "The problems of learning and planning in such environments have been vigorously studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009).",
      "startOffset" : 126,
      "endOffset" : 186
    }, {
      "referenceID" : 19,
      "context" : "While provable guarantees of efficiency and optimality have been at the core of the literature on learning (Fiechter, 1994; Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Li et al., 2011), correctness with respect to complicated, high-level task specifications—during the learning itself or in the behavior resulting from the learning phase— has attracted limited attention (Abbeel & Ng, 2005).",
      "startOffset" : 107,
      "endOffset" : 191
    }, {
      "referenceID" : 1,
      "context" : "Almagor et al. (2014) explicitly refine the “eventually” operator of LTL to a discounting operator such that the longer it takes to fulfill the task the smaller the value of satisfaction.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).",
      "startOffset" : 202,
      "endOffset" : 364
    }, {
      "referenceID" : 15,
      "context" : "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).",
      "startOffset" : 202,
      "endOffset" : 364
    }, {
      "referenceID" : 20,
      "context" : "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).",
      "startOffset" : 202,
      "endOffset" : 364
    }, {
      "referenceID" : 27,
      "context" : "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).",
      "startOffset" : 202,
      "endOffset" : 364
    }, {
      "referenceID" : 11,
      "context" : "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).",
      "startOffset" : 202,
      "endOffset" : 364
    }, {
      "referenceID" : 18,
      "context" : "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).",
      "startOffset" : 202,
      "endOffset" : 364
    }, {
      "referenceID" : 16,
      "context" : "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).",
      "startOffset" : 202,
      "endOffset" : 364
    }, {
      "referenceID" : 17,
      "context" : "While temporal logic had initially focused on reasoning about temporal and logical relations, its dialects with probabilistic modalities have been used increasingly for robotics applications (Baier & Katoen, 2008; De Alfaro, 1998; Kwiatkowska et al., 2002).",
      "startOffset" : 191,
      "endOffset" : 256
    } ],
    "year" : 2017,
    "abstractText" : "We propose a new task-specification language for Markov decision processes that is designed to be an improvement over reward functions by being environment independent. The language is a variant of Linear Temporal Logic (LTL) that is extended to probabilistic specifications in a way that permits approximations to be learned in finite time. We provide several small environments that demonstrate the advantages of our geometric LTL (GLTL) language and illustrate how it can be used to specify standard reinforcementlearning tasks straightforwardly.",
    "creator" : "LaTeX with hyperref package"
  }
}