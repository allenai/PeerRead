{
  "name" : "1506.08928.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty",
    "authors" : [ "Changkyu Song", "Sejong Yoon", "Vladimir Pavlovic" ],
    "emails" : [ "vladimir}@cs.rutgers.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The need for algorithms and methods that can handle large data in a distributed setting has grown significantly in recent years. Specifically, such settings may arise in two prototypical scenarios: (a) induced distributed data: distribute and parallelize computationally demanding optimization tasks to connected computational nodes using a data distributed model and (b) intrinsically distributed data: data is collected across a connected network of sensors (e.g., mobile devices, camera networks), where some or all of the computation can be performed in individual sensor nodes without requiring centralized data pooling. Several distributed learning approaches have been proposed to meet these needs. In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].\nIn the distributed optimization setting, the distributed nodes process data locally by solving small optimization problems and aggregate the result by exchanging the (possibly compressed) local solutions (e.g., local model parameter estimates) to arrive at a consensus global result. However, the nature of distributed learning models, particularly in the fully distributed setting where no network topology is presumed, inherently requires repetitive communications between the device nodes. Therefore, it is desirable to reduce the amount of information exchanged and simultaneously improve computational efficiency through faster convergence of such distributed algorithms.\nTo this end, the contributions of this paper are three fold.\n• We propose two variants of ADMM for the consensus-based distributed learning faster than the standard ADMM. Our method extends an acceleration approach for ADMM [10] by an efficient variable penalty parameter update strategy. This strategy results in improved convergence properties of ADMM and also works in a fully distributed fashion.\nar X\niv :1\n50 6.\n08 92\n8v 1\n[ cs\n.L G\n] 3\n0 Ju\n• We extend our proposed method to automatically determine the maximum number of iterations allocated to successive updates by employing a budget magement scheme. This strategy results in adaptive parameter tuning for ADMM, removing the need for arbitrary parameter settings, and effectively induces a varying network communication topology.\n• We apply the proposed method to a prototypical vision and learning problem, the distributed PPCA for structure-from-motion, and demonstrate its empirical utility over the traditional ADMM."
    }, {
      "heading" : "2 Problem Description and Related Works",
      "text" : "The problem we consider in this paper can be formulated as a consensus-based optimization problem [11]. A general consensus-based optimization problem can be written as\narg min θi J∑ i=1 fi(θi), s.t. θi = θj ,∀i 6= j (1)\nwhere we want to find the set of optimal parameters θi, i = 1..J that minimizes the sum of convex objective functions fi(θi), where J denotes the total number of the functions. This problem is typically a reformulation of a centralized optimization task arg min f(θ) with a decomposable objective f(θ) = ∑J i=1 fi(θ). Given the consensus formulation, the original problem can be solved by decomposing the problem into J subproblems so that J processors can cooperate to solve the overall problem by changing the equality constraint to θi = θ̄ where θ̄ denotes a globally shared parameter. The optimization can be approached efficiently by exploiting the alternating direction method of multiplier (ADMM) [1].\nThe above consensus formulation is particularly suitable for many optimization problems that appear in computer vision. For instance, since fi(θi) can be any convex function, we can also consider a probabilistic model with the joint negative log likelihood fi(θi) = − log p(xi, zi|θi) between the observation xi and the corresponding latent variable zi. Assuming (xi, zi) are independent and identically distributed, finding the maximum likelihood estimate of the shared paramter θ̄ can then be formulated as the optimization problem we described above for many exponential family parametric densities. Moreover, the function need not be a likelihood, but can also be a typical decomposable and regularized loss that occurs in many vision problems such as denoising or dictionary learning.\nIt is often very convenient to consider the above consensus optimization problem from the perspective of optimization on graphs. For instance, the centralized i.i.d. Maximum Likelihood learning can be viewed as the optimization on the graph in Fig. 1a. Edges in this graph depict functional (in)dependencies among variables, commonly found in representations such as Markov Random Fields [9] or Factor Graphs [12]. In this context, to fully decompose f(·) and eliminate the need for a processing center completely, one can introduce auxiliary variables ρij on every edge to break the dependency between θi and θj [13, 14] as shown in Fig. 1b. This generalizes to arbitrary graphs, where the connectivity structure may be implied by node placement or communication constraints (camera networks), imaging constraints (pixel neighborhoods in images or frames in a video sequence), or other contextual constraints (loss and regularization structure).\nIn general, given a connected graph G = (V, E) with the nodes i, j ∈ V and the edges eij = (i, j) ∈ E , the consensus optimization problem becomes\nmin ∑ i∈V fi(θi), s.t. θi = ρij , ρij = θj , j ∈ Bi (2)\nSolving that problem is equivalent to optimizing the augmented Lagrangian L(Θ) = ∑ i∈V Li(Θi),\nLi(Θi) = fi(θi) + ∑ j∈Bi { λ>ij1(θi − ρij) + λ>ij2(ρij − θj) } + η 2 ∑ j∈Bi { ‖θi − ρij‖2 + ‖ρij − θj‖2 } , (3)\nwhere Θ = {Θi : i ∈ V}, Θi = {θi, ρi, λi} are parameters to find, λi = {λij1, λij2 : j ∈ Bi}, λij1, λij2 are Lagrange multipliers, Bi = {j|eij ∈ E} is the set of one hop neighbors of node i, η > 0 is a fixed scalar penalty constraint, and ‖ · ‖ is induced norm. The ADMM approach suggests that the optimization can be done in coordinate descent fashion taking gradient of each variable while fixing all the others."
    }, {
      "heading" : "2.1 Convergence Speed of ADMM",
      "text" : "The currently known convergence rate of ADMM is O(1/T ) where T is the number of iterations [15]. Even though O(1/T ) is the best known bound, it has been observed empirically that ADMM converges faster in many applications. Moreover, the computation time per each iteration may dominate the total algorithm running time. Thus many speed up techniques for ADMM have been proposed that are application specific. One way is to come up with a predictor-corrector step for the coordinate descent [16] using some available acceleration method such as [17]. It guarantees quadratic convergence for strongly convex fi(·). Another way is to replace the gradient descent optimization with a stochastic one [18, 19]. This approach has recently gained attention as it greatly reduces the computation per iteration. However, these methods usually require the coordinating center node thus may not readily applicable to the decentralized setting. Moreover, we want to preserve the application range of ADMM and avoid introducing additional assumptions on fi(·). One way to improve convergence speed of ADMM is through the use of different constraint penalty in each iteration. For example, [10] proposed ADMM with self-adaptive penalty, and it improved the convergence speed as well as made its performance less dependent on initial penalty values. The idea of [10] is to change the constraint penalty taking account of the relative magnitudes of primal and dual residuals of ADMM as follows\nηt+1 =  ηt · (1 + τ t) , if ‖rt‖2 > µ‖st‖2 ηt · (1 + τ t)−1 , if ‖st‖2 > µ‖rt‖2 ηt , otherwise\n(4)\nwhere t is the iteration index, µ > 1, τ t > 0 are parameters, rt and st are the primal and dual residuals, respectively1. The primal residual measures the violation of the consensus constraints and the dual residual measures the progress of the optimization in the dual space. This update converges when τ t satisfies ∑∞ t=0 τ\nt <∞, i.e. we stop updating ηt after a finite number of iterations. Typical choice for parameters are suggested as µ = 10 and τ t = 1 at all t iterations. The strength of this approach is that conservative changes in the penalty are guaranteed to converge [1, 20]. However, like other ADMM speed up approaches mentioned above, this update scheme relies on the global computation of the primal and the dual residuals and requires the ηt stored in nodes to be homogeneous over entire network thus it is not a fully decentralized scheme. Moreover, the choice of parameters as well as the maximum number of iterations require manually tuning."
    }, {
      "heading" : "3 Proposed Methods",
      "text" : "We present our proposed ADMM penalty update schemes in three steps. First, we extend the aforementioned update scheme of (4) to be applicable on fully decentralized setting. Next, we propose the novel penalty parameter update strategy for ADMM speed up that does not require manual tuning of τ t. Finally, we extend the strategy so that we can automatically select the maximum number of penalty update iterations.\n1Please refer [1], page 18 and 51 for their definitions."
    }, {
      "heading" : "3.1 ADMM with Varying Penalty (ADMM-VP)",
      "text" : "Throughout the paper, the superscript t in all terms with subscript i denote either the objective function or parameter at t-th iteration for node i. In order to extend (4) for a fully distributed setting, we first introduce ηti , the penalty for i-th node at t-th iteration. Next, we need to compute local primal and dual residuals for each node i. In the fully distributed learning framework of [13, 14], the dual auxiliary variable vanishes from derivation. However, to compute the residuals, we need to keep track of the dual variable, which is essentially the average of local estimates, explicitly over iterations. The squared residual norms for the i-th node are defined as\n‖rti‖22 = ‖θti − θ̄ti‖22, ‖sti‖22 = (ηti)2‖θ̄ti − θ̄t−1i ‖ 2 2, θ̄ t i =\n1 |Bi| ∑ j∈Bi θtj . (5)\nNote the difference from the standard residual definitions for consensus ADMM [1], used in (4), where the dual variable is considered as a single, globally accessible variable, θ̄t instead of local θ̄ti . This allows each node to change its η t i based on its own local residuals. The penalty update scheme is similar to (4) but ηt, ‖rt‖2 and ‖st‖2 are replaced with ηti , ‖rti‖2 and ‖sti‖2, respectively. Lastly, [10] stopped changing ηt after t > 50. However, in ADMM-VP, if we stop the same way, we end up with heterogeneously fixed penalty values which impacts the convergence of ADMM by yielding heavy oscillations near the saddle point. Therefore we reset all penalty values in all nodes to a pre-defined value (e.g. η0, the initial penalty parameter) after a fixed number of iterations. As we fix the penalty values homogeneously after a finite number of iterations, it becomes the standard ADMM after that point thus the convergence of ADMM-VP update is guaranteed."
    }, {
      "heading" : "3.2 ADMM with Adaptive Penalty (ADMM-AP)",
      "text" : "We further extend ηi by introducing a bi-directional graph with a penalty constraint parameter ηij specific to directed edge eij from node i to j. The modified augmented Lagrangian Li is similar to (3) except that we replace η with ηij . The penalty constraint controls the amount each constraint contributes to the local minimization problem. The penalty constraint parameter ηij is determined by evaluating the parameter θj from node j with the objective function fi(·) of node i as\nηt+1ij =\n{ η0 · (1 + τ tij) , if t < tmax\nη0 , otherwise (6)\nwhere tmax is the maximum number of iterations for the update as proposed in [10] and\nτ tij = κti(θ t i)\nκti(θ t j) − 1 , κti(θ) = ( f ti (θ)− fmini fmaxi − fmini + 1 ) , (7)\nfmaxi = max{f ti (θti), f ti (θtj) : j ∈ Bi} , fmini = min{f ti (θti), f ti (θtj) : j ∈ Bi} . (8)\nThe interpretation of this update strategy is straightforward. In each iteration t, each i-th node will evaluate its objective using its own estimate of θti and the estimates from other nodes θ t j (we use ρtij instead of actual θ t j to retain locality of each node from the neighbors). Then, we assign more weight to the neighbor with better parameter estimate for the local fi(·) (i.e. larger penalty ηtij if fi(θj) < fi(θi)) with the above update scheme. The intuition behind the ADMM-AP update is to emphasize the local optimization during early stages and then deal with the consensus update at later, subsequence stages. If all local parameters yield similarly valued local objectives fi(·), the onus is placed on consensus. This makes ADMM-AP different from pre-initialization that does the local optimization using the local observations and ignores the consensus constraints.\nNote that unlike the update strategy of (4), we do not need to specify τ t and the update weight is automatically chosen according to the normalized difference in the local objective evaluation among neighboring parameters. The proposed algorithm also emphasizes the objective minimization over the minimization that solely depends on the norms of primal and dual residuals of constraints. The hope is that we not only achieve the consensus of the parameters of the model but also a good estimate with respect to the objective.\nOn the other hand, the convergence property of [10] still holds for the proposed algorithm. Following Remark 4.2 of [10], the requirement for the convergence is to satisfy the update ratio to be fixed after\nsome tmax <∞ iteartions. Moreover, the proposed update ensures bounding by ηt+1ij /ηtij ∈ [0.5, 2], which matches with the increase and decrease amount suggested in [1, 10]. One may use tmax = 50 as in [10]."
    }, {
      "heading" : "3.3 ADMM with Network Adaptive Penalty (ADMM-NAP)",
      "text" : "To extend the proposed method for automatically deciding the maximum number of penalty updates, the penalty update for the ADMM becomes\nηt+1ij =\n{ η0 · (1 + τ tij) , if ∑t u=1 |τuij | < T tij\nη0 , otherwise. (9)\nFig. 1c depicts how the proposed model have different structures from centralized and traditional distributed models, and how nodes share their parameters via network.\nIn addition to the adaptive penalty update, the inequality condition on the summation of τuij , u = 1..t encodes the spent budget that the edge eij can change ηij . All nodes have its upper bound T tij and everytime it makes a change to ηij , it has to pay exactly the amount they changed. If the edge has changed too much, too often, the update strategy will block the edge from changing ηij any more.\nThe update scheme is guaranteed to convergence if T tij is simply set to constant T for all i, j, t or if τ tij = 0 for t > t\nmax. However, with a different objective function and different network connectivity, a different upper bound should be imposed. This is because a given upper bound T or maximum iteration tmax could be too small for a certain node to fully take an advantage of our adaptation strategy or they could be too big so that it converges much slowly because of the continuously changing ηtij . To this end, we propose updating strategy for T tij as following:\nT t+1ij =  T t ij + α nT , if ∑t u=1 |τuij | ≥ T tij and |fi(θti)− fi(θ t−1 i )| > β\nT tij , otherwise (10)\nwhere T 0ij is set by an initial parameter T and α, β ∈ (0, 1) are parameters. Whenever T t+1 ij > T tij , we increase n by 1. Once ∑t u=1 |τuij | ≥ T tij but its objective value is still significantly changing, i.e. |fi(θti) − fi(θ t−1 i )| > β, T t+1 ij is increased by α\nnT . Note that the independent upper bound T tij for each ηtij update on the edge eij makes it sensitive to the various network topology, but it still satisfies the convergence condition because\nlim t→∞ T tij ≤ ∞∑ n=1 αn−1 T = 1 1− α T . (11)"
    }, {
      "heading" : "3.4 Combined Update Strategies (ADMM-VP + AP, ADMM-VP + NAP)",
      "text" : "Observing (4) and the proposed update schemes (6) and (9), one can easily come up with a combined update strategy by replacing τ t in (4) with τ tij . Based on preliminary experiments, we found that this replacement yields little utility. Instead, we suggest another penalty update strategy combining ADMM-VP and ADMM-AP as\nηt+1ij =  ηtij · (1 + τ tij) · 2 , if ‖rti‖2 > µ‖sti‖2 ηtij · (1 + τ tij) · (1/2) , if ‖sti‖2 > µ‖rti‖2 ηtij , otherwise\n(12)\nwhich we denote as ADMM-VP + AP. We reset ηtij = η 0 when t > tmax. In order to combine ADMM-VP and ADMM-NAP, we consider the summation condition of τ tij as in (9). We denote this strategy as ADMM-VP + NAP."
    }, {
      "heading" : "4 Distributed Maximum Likelihood Learning",
      "text" : "In this section, we show how our method can be applied to an existing distributed learning framework in the context of distributed probabilistic principal component analysis (D-PPCA). D-PPCA can be\nviewed as fundamental approach to a general matrix factorization task in the presence of potentially missing data, with many applications in machine learning."
    }, {
      "heading" : "4.1 Probabilistic Principal Component Analysis",
      "text" : "The Probabilistic PCA (PPCA) [21] has many applications in vision problems, including structure from motion, dictionary learning, image inpainting, etc. We here restrict our attention to the linear PPCA without any loss of generalization. The centralized PPCA is formulated as the task of projecting the source data x according to x = Wz+µ+ where x ∈ RD is the observation column vector, z ∈ RM is the latent variable following z ∼ N (0, I), W ∈ RD×M is the projection matrix that maps x to z, µ ∈ RD allows non-zero mean, and the Gaussian observation noise ∼ N (0, a−1I) with the noise precision a. When a−1 = 0, PPCA recovers the standard PCA. The posterior estimate of the latent variable z given the observation x is\np(z|x) ∼ N (M−1W>(x− µ), a−1M−1), (13)\nwhere M = W>W + a−1I. The parameters W, µ, and a can be estimated using a number of methods, including SVD and Expectation Maximization (EM) algorithm."
    }, {
      "heading" : "4.2 Distributed PPCA",
      "text" : "The distributed extension of PPCA (D-PPCA) [14] can be derived by applying ADMM to the centralized PPCA model above. Each node learns its local copy of PPCA parameters with its set of local observations Xi = {xin|n = 1..Ni} where xin denotes the n-th observation in i-th node and Ni is the number of observations available in the node. Then, they exchange the parameters using the Lagrange multipliers and impose consensus constraints on the parameters. The global constrained optimization is\nmin Θi − log p(Xi|Θi) s.t. Θi = ρΘij , ρΘij = Θj , (14)\nwhere i ∈ V, j ∈ Bi, Θi = {Wi,µi, ai} is the set of local parameters and ρΘij = {ρWij , ρ µ ij , ρ a ij} is the set of auxiliary variables for the parameters. For the details regarding how the decentralized model is optimized, see [14]."
    }, {
      "heading" : "4.3 D-PPCA with Network Adpative Penalty",
      "text" : "The augmented Lagrangian applying the proposed ADMM with Network Adpative Penalty is similar to [14] except that η becomes ηij . with λi, γi, βi are Lagrange multipliers for the PPCA parameters for node i. The adaptive penalty constraint ηtij controls the speed of parameter propagation dynamically so that the overall optimization empirically converges faster than [14]. One can solve this optimization using the distributed EM approach [13]. The E-step of the D-PPCA is the same as centralized counterpart [21]. The M-step is similar to [14] except we use separate ηij for each edge. Since the update formulas for the three parameters are similar, we present the µi update as an example. First, µi can be updated as\nµt+1i = ai Ni∑ n=1 (xin −WiE[zin])− 2γti + ∑ j∈Bi ηij ( µti + µ t j ) · Niai + 2 ∑ j∈Bi ηtij −1 , (15) where E[zin] denotes the posterior estimates of the n-th latent variable of node i. Note that unlike D-PPCA where we computed the normalization factor as Niai + 2η|Bi| where | · | is the cardinality, we add up ηtij ,∀j ∈ Bi. The corresponding Lagrange multiplier can be computed as penaltyweighted summation of consensus errors γt+1i = γ t i + (1/2) ∑ j∈Bi η t ij ( µt+1i − µ t+1 j ) . Once all the parameters and the Lagrange multipliers are updated, we update ηij and Tij using (9) and (10), respectively. Algorithm 1 in the appendix summarizes the overall steps for the D-PPCA with Network Adpative Penalty."
    }, {
      "heading" : "5 Experiments",
      "text" : "We first analyze and compare the proposed methods (ADMM-VP, ADMM-AP, ADMM-NAP, ADMM-VP + AP, ADMM-VP + NAP) with the baseline method using synthetic data. Next, we\napply our method to a distributed structure from motion problem using two benchmark real world datasets. For the baseline, we compare with the standard ADMM-based D-PPCA [14] denoted as ADMM. Unless noted otherwise, we used η0 = 10. To assess convergence, we compare the relative change of (14) to a fixed threshold (10−3 in this case) for the D-PPCA experiments as in [14]."
    }, {
      "heading" : "5.1 Synthetic Data",
      "text" : "We generated 500 samples of 20 dimensional observations from a 5-dim subspace following N (0, I), with the Gaussian measurement noise following N (0, 0.2 · I). For the distributed settings, the samples are assigned to each node evenly. All experiments are ran with 20 independent random initializations. We measured the number of iterations to convergence and the maximum subspace angle error versus the ground truth defined as the maximum of subspace angles between each node’s projection matrix and the ground truth projection matrix. We examined the impact of different graph topologies and different graph sizes. We tested three network topologies: complete, ring and cluster (a connected graph consists of two complete graphs linked with an edge). For the graph size, we tested on 12, 16 and 20 nodes settings.\nTop three plots in Fig. 2 depict results over varying number of nodes while fixing the graph topology as the complete graph. We plot the median result out of the 20 independent initializations. We observed that the speed up with the proposed method, particularly for ADMM-VP and its variants, becomes more significant as the number of nodes increases. This suggests the proposed method can be of particular use as the size of an application problem increases. Fig. 2c to Fig. 2e in the figure show the performance in the context of different network topologies. Our proposed methods converge faster or at the same rate as the standard ADMM. The proposed method works most robustly in the complete graph setting. In other words as the graph connectivity increases, the convergence property of the proposed method improves. Note also that ADMM-VP works best in complete graph while ADMM-AP / NAP are better than the ADMM-VP in weakly connected networks. This makes sense as ADMM-VP depends on residual computation and the proposed local residual computation become less accurate compared to the complete graph when the global residual can be computed."
    }, {
      "heading" : "5.2 Distributed Affine Structure from Motion",
      "text" : "We tested the performance of our method on five objects of Caltech Turntable [22] and Hopkins 155 [23] dataset as in [14]. The goal here is to jointly estimate the 3D structure of the objects as\nwell as the camera motion, however in a distributed camera network setting. The input measurement matrix is defined as 2×F byN where F denotes the number of frames andN denotes the number of points. By applying PCA, we can decompose the input into the camera pose Wi and the 3D structure E[zin], n = 1..Ni. For the detailed experimental setting, refer to [14, 24]. As the performance measure, we used the maximum subspace angle error versus the centralized SVD-reconstructed structure. The network setting assumes five cameras on a complete graph.\nFig. 5 shows the result on the Caltech Turntable dataset. First, we compare Fig. 3a and Fig. 3b. One can see that when the graph is less connected (Fig. 3a), the proposed adaptive penalty method can boost ADMM-VP which cannot utilize the full residual information of fully connected case (Fig. 3b), as explained in synthetic data experiments. Next, we compare Fig. 3b and Fig. 3c. The network topologies are the same (complete) but tmax value required for ADMM-VP, ADMM-AP, ADMM-VP + AP is different in these two groups of experiments. When tmax = 50 (Fig. 3b), all methods can accelerate throughout the iterations. However, when tmax = 5 (Fig. 3c), the methods that depend on tmax cannot accelerate after 5 iterations thus showing behavior similar to the baseline ADMM. On the other hand, ADMM-NAP based methods can accelerate by adaptively modifying the maximum number of penalty updates. Note that one can choose any small value of T and Tij is increased automatically using (10).\nFor the Hopkins 155 dataset, we compared methods on 135 objects using the same approach as [14]. For each method considered, we computed the mean number of iterations until convergence. Since some objects in the dataset are point trajectories of non-rigid structure, it is inevitable for simple linear models to fail for those objects. Thus we omitted objects yielded more than 15 degrees when calculating the mean. For each object, we tested 5 independent random initializations. For ADMM-AP, ADMM-NAP and ADMM-VP + NAP, we found no significant speed up over the baseline ADMM. For ADMM-VP and ADMM-VP + AP, we could obtain 40.2%, 37.3% speed up, respectively if we use complete network. In ring network, the amount of improvement becomes smaller. This small or no improvement of speed is mainly due to the fact that the baseline ADMM converges fast enough (typically < 100 iterations) thus there is little room for the proposed methods to speed up the optimization. As observed from the synthetic experiments and Caltech dataset, the acceleration of the proposed methods occurs at the earlier iterations of the optimization. Thus if one can come up with a better convergence checking criterion depending on the application, the proposed methods can be a very viable choice due to its parameter-free nature."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduced a novel adaptive penalty update methods for ADMM that can be applied to consensus distributed learning frameworks. Contrary to previous approaches, our adaptive penalty update methods, ADMM-AP and ADMM-NAP does not depend on the parameters that require manual tuning. Using both synthetic and real data experiments, we showed the empirical effectiveness of the methods over the baseline. In addition, we found that the performance of ADMM-VP decreases with weakly connected graphs, and in those cases, ADMM-AP and ADMM-NAP can be useful.\nThe proposed methods do leave some room for improvements. For the problems when the standard ADMM can converge fast enough, the proposed methods may show less than significant gains. A better convergence criterion may help stop the proposed algorithms at earlier iterations (e.g. a criterion that can stop algorithms to remove long tails in Fig. 2b or Fig. 2c)."
    }, {
      "heading" : "A D-PPCA with Network Adaptive Penalty",
      "text" : "Here we summarize the distributed probabilistic principal component analysis (D-PPCA) [14] algorithm modified to use the proposed network adaptive penalty update scheme (ADMM-NAP). We follow the notations from the previous sections. The D-PPCA with Network Adaptive Penalty algorithm is summarized in Algorithm 1.\nAlgorithm 1 D-PPCA with Network Adpative Penalty\nRequire: For every node i randomly initialize W0i ,µ0i , a0i and set λ0i = 0, γ0i = 0, β0i = 0, ηij = η for j ∈ Bi\n1: for t = 0, 1, 2, · · · until convergence do 2: for all i ∈ V do 3: Compute E[zin] and E[zinz>in] 4: Compute Wt+1i ,µ (t+1) i , a (t+1) i 5: end for 6: for all i ∈ V do 7: Broadcast W(t+1)i ,µ (t+1) i , and a (t+1) i to ∀j ∈ Bi 8: end for 9: for all i ∈ V do\n10: Compute λ(t+1)i , γ (t+1) i , and β (t+1) i 11: end for 12: for all i ∈ V do 13: Update ηij for j ∈ Bi via (9) 14: Update Tij for j ∈ Bi via (10) 15: end for 16: end for"
    }, {
      "heading" : "B Results on Caltech Turntable Dataset",
      "text" : "We present example image frames from the Caltech Turntable [22] dataset used in [14]. We compare the proposed methods, ADMM with Varying Penalty (ADMM-VP), ADMM with Adaptive Penalty (ADMM-AP) and ADMM with Network Adaptive Penalty (ADMM-NAP) and their combination (ADMM-VP + AP, ADMM-VP + NAP) with the standard ADMM based D-PPCA [14] using the same experimental setting. Fig. 4 shows an example frame, feature points extracted from the frame and the centralized SVD-based reconstructed structure we used as ground truth. In the paper, we showed the results of Standing.\nFig. 5 summarizes the results on the remaining four objects. The findings and analysis explained in the main paper on the object Standing also apply to these four remaining objects. First, we compare the top and the middle rows. One can see that when the graph is less connected (ring, top row) the proposed adaptive penalty method can boost ADMM-VP which cannot utilize the full residual information of fully connected case (complete, middle row), as explained in synthetic data experiments.\nSecond, we compare the middle and the bottom rows. The network topologies are the same as complete but tmax value required for ADMM-VP, ADMM-AP, ADMM-VP + AP is different from these two groups of experiments. When tmax = 50 (middle row), all methods can accelerate throughout the iterations. However, when tmax = 5 (bottom row), the methods that depend on tmax cannot accelerate after 5 iterations thus show similar behaviour as the baseline ADMM. On the other hand, ADMM-NAP based methods could accelerate by adaptively modifying the maximum number of penalty updates."
    } ],
    "references" : [ {
      "title" : "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers",
      "author" : [ "Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Fixed-rank representation for unsupervised visual learning",
      "author" : [ "Risheng Liu", "Zhouchen Lin", "Fernando De la Torre", "Zhixun Su" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Non-negative low rank and sparse graph for semi-supervised learning",
      "author" : [ "Liansheng Zhuang", "Haoyuan Gao", "Zhouchen Lin", "Yi Ma", "Xin Zhang", "Nenghai Yu" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "A Convex Optimization Framework for Active Learning",
      "author" : [ "Ehsan Elhamifar", "Guillermo Sapiro", "Allen Y. Yang", "S. Shankar Sastry" ],
      "venue" : "In IEEE International Conference on Computer Vision,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Learning by associating ambiguously labeled images",
      "author" : [ "Zinan Zeng", "Shijie Xiao", "Kui Jia", "Tsung-Han Chan", "Shenghua Gao", "Dong Xu", "Yi Ma" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Robust estimation of 3d human poses from a single image",
      "author" : [ "Chunyu Wang", "Yizhou Wang", "Zhouchen Lin", "Alan L. Yuille", "Wen Gao" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Recognizing complex events in videos by learning key static-dynamic evidences",
      "author" : [ "Kuan-Ting Lai", "Dong Liu", "Ming-Syan Chen", "Shih-Fu Chang" ],
      "venue" : "In Proceedings of the European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Fast and Exact: ADMM-Based Discriminative Shape Segmentation with Loopy Part Models",
      "author" : [ "H. Boussaid", "I. Kokkinos" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Distributed Non-Convex ADMMinference in Large-scale Random Fields",
      "author" : [ "Ondrej Miksik", "Vibhav Vineet", "Patrick Pérez", "Philip H.S. Torr" ],
      "venue" : "In British Machine Vision Conference (BMVC),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Alternating Direction Method with Self-Adaptive Penalty Parameters for Monotone Variational Inequalities",
      "author" : [ "B.S. He", "H. Yang", "S.L. Wang" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2000
    }, {
      "title" : "Parallel and Distributed Computation: Numerical Methods",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1989
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "Christopher Bishop" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Distributed Clustering Using Wireless Sensor Networks",
      "author" : [ "Pedro A. Forero", "Alfonso Cano", "Georgios B. Giannakis" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Distributed probabilistic learning for camera networks with missing data",
      "author" : [ "Sejong Yoon", "Vladimir Pavlovic" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "On the O(1/n) Convergence Rate of the Douglas-Rachford Alternating Direction Method",
      "author" : [ "Bingsheng He", "Xiaoming Yuan" ],
      "venue" : "SIAM Journal of Numerical Analysis,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Fast Alternating Direction Optimization Methods",
      "author" : [ "Tom Goldstein", "Brendan O’Donoghue", "Simon Setzer", "Richard Baraniuk" ],
      "venue" : "SIAM Journal of Imaging Science,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate o(1/k)",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Soviet Math. Dokl.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1983
    }, {
      "title" : "Stochastic alternating direction method of multipliers",
      "author" : [ "H. Ouyang", "N. He", "L. Tran", "A. Gray" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Dual averaging and proximal gradient descent for online alternating direction multiplier method",
      "author" : [ "T. Suzuki" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Monotone operators and the proximal point algorithm",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1976
    }, {
      "title" : "Probabilistic Principal Component Analysis",
      "author" : [ "Michael E. Tipping", "Chris M. Bishop" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1999
    }, {
      "title" : "Evaluation of Features Detectors and Descriptors based on 3D Objects",
      "author" : [ "Pierre Moreels", "Pietro Perona" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms",
      "author" : [ "Roberto Tron", "Rene Vidal" ],
      "venue" : "In IEEE International Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Distributed Computer Vision Algorithms Through Distributed Averaging",
      "author" : [ "Roberto Tron", "Rene Vidal" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 2,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 3,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 4,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 5,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 6,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 7,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 8,
      "context" : "In particular, the alternating direction method of multiplier (ADMM) [1] is an optimization technique that has been very often used in computer vision and machine learning to handle model estimation and learning in either of the two large data settings [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 9,
      "context" : "Our method extends an acceleration approach for ADMM [10] by an efficient variable penalty parameter update strategy.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "2 Problem Description and Related Works The problem we consider in this paper can be formulated as a consensus-based optimization problem [11].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "The optimization can be approached efficiently by exploiting the alternating direction method of multiplier (ADMM) [1].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "Edges in this graph depict functional (in)dependencies among variables, commonly found in representations such as Markov Random Fields [9] or Factor Graphs [12].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "Edges in this graph depict functional (in)dependencies among variables, commonly found in representations such as Markov Random Fields [9] or Factor Graphs [12].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 12,
      "context" : "In this context, to fully decompose f(·) and eliminate the need for a processing center completely, one can introduce auxiliary variables ρij on every edge to break the dependency between θi and θj [13, 14] as shown in Fig.",
      "startOffset" : 198,
      "endOffset" : 206
    }, {
      "referenceID" : 13,
      "context" : "In this context, to fully decompose f(·) and eliminate the need for a processing center completely, one can introduce auxiliary variables ρij on every edge to break the dependency between θi and θj [13, 14] as shown in Fig.",
      "startOffset" : 198,
      "endOffset" : 206
    }, {
      "referenceID" : 14,
      "context" : "1 Convergence Speed of ADMM The currently known convergence rate of ADMM is O(1/T ) where T is the number of iterations [15].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "One way is to come up with a predictor-corrector step for the coordinate descent [16] using some available acceleration method such as [17].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "One way is to come up with a predictor-corrector step for the coordinate descent [16] using some available acceleration method such as [17].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 17,
      "context" : "Another way is to replace the gradient descent optimization with a stochastic one [18, 19].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "Another way is to replace the gradient descent optimization with a stochastic one [18, 19].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "For example, [10] proposed ADMM with self-adaptive penalty, and it improved the convergence speed as well as made its performance less dependent on initial penalty values.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "The idea of [10] is to change the constraint penalty taking account of the relative magnitudes of primal and dual residuals of ADMM as follows",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "The strength of this approach is that conservative changes in the penalty are guaranteed to converge [1, 20].",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "The strength of this approach is that conservative changes in the penalty are guaranteed to converge [1, 20].",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "Please refer [1], page 18 and 51 for their definitions.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 12,
      "context" : "In the fully distributed learning framework of [13, 14], the dual auxiliary variable vanishes from derivation.",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "In the fully distributed learning framework of [13, 14], the dual auxiliary variable vanishes from derivation.",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Note the difference from the standard residual definitions for consensus ADMM [1], used in (4), where the dual variable is considered as a single, globally accessible variable, θ̄ instead of local θ̄ i .",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "Lastly, [10] stopped changing η after t > 50.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 9,
      "context" : "where t is the maximum number of iterations for the update as proposed in [10] and τ t ij = κi(θ t i) κi(θ t j) − 1 , κi(θ) = ( f t i (θ)− f i fmax i − fmin i + 1 ) , (7) f i = max{f t i (θ i), f t i (θ j) : j ∈ Bi} , f i = min{f t i (θ i), f t i (θ j) : j ∈ Bi} .",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "On the other hand, the convergence property of [10] still holds for the proposed algorithm.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "2 of [10], the requirement for the convergence is to satisfy the update ratio to be fixed after",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "5, 2], which matches with the increase and decrease amount suggested in [1, 10].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "5, 2], which matches with the increase and decrease amount suggested in [1, 10].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "One may use t = 50 as in [10].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "1 Probabilistic Principal Component Analysis The Probabilistic PCA (PPCA) [21] has many applications in vision problems, including structure from motion, dictionary learning, image inpainting, etc.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "2 Distributed PPCA The distributed extension of PPCA (D-PPCA) [14] can be derived by applying ADMM to the centralized PPCA model above.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "For the details regarding how the decentralized model is optimized, see [14].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "3 D-PPCA with Network Adpative Penalty The augmented Lagrangian applying the proposed ADMM with Network Adpative Penalty is similar to [14] except that η becomes ηij .",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "The adaptive penalty constraint η ij controls the speed of parameter propagation dynamically so that the overall optimization empirically converges faster than [14].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "One can solve this optimization using the distributed EM approach [13].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "The E-step of the D-PPCA is the same as centralized counterpart [21].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "The M-step is similar to [14] except we use separate ηij for each edge.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "For the baseline, we compare with the standard ADMM-based D-PPCA [14] denoted as ADMM.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "To assess convergence, we compare the relative change of (14) to a fixed threshold (10−3 in this case) for the D-PPCA experiments as in [14].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : "2 Distributed Affine Structure from Motion We tested the performance of our method on five objects of Caltech Turntable [22] and Hopkins 155 [23] dataset as in [14].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "2 Distributed Affine Structure from Motion We tested the performance of our method on five objects of Caltech Turntable [22] and Hopkins 155 [23] dataset as in [14].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : "2 Distributed Affine Structure from Motion We tested the performance of our method on five objects of Caltech Turntable [22] and Hopkins 155 [23] dataset as in [14].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "For the detailed experimental setting, refer to [14, 24].",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "For the detailed experimental setting, refer to [14, 24].",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "For the Hopkins 155 dataset, we compared methods on 135 objects using the same approach as [14].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "A D-PPCA with Network Adaptive Penalty Here we summarize the distributed probabilistic principal component analysis (D-PPCA) [14] algorithm modified to use the proposed network adaptive penalty update scheme (ADMM-NAP).",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : "B Results on Caltech Turntable Dataset We present example image frames from the Caltech Turntable [22] dataset used in [14].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "B Results on Caltech Turntable Dataset We present example image frames from the Caltech Turntable [22] dataset used in [14].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "We compare the proposed methods, ADMM with Varying Penalty (ADMM-VP), ADMM with Adaptive Penalty (ADMM-AP) and ADMM with Network Adaptive Penalty (ADMM-NAP) and their combination (ADMM-VP + AP, ADMM-VP + NAP) with the standard ADMM based D-PPCA [14] using the same experimental setting.",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 13,
      "context" : "(e) StorageBin (102 points) Figure 4: The Caltech Turntable dataset objects used in [14] and the centralized SVD-based affine structure from motion result.",
      "startOffset" : 84,
      "endOffset" : 88
    } ],
    "year" : 2015,
    "abstractText" : "We propose new methods to speed up convergence of the Alternating Direction Method of Multipliers (ADMM), a common optimization tool in the context of large scale and distributed learning. The proposed method accelerates the speed of convergence by automatically deciding the constraint penalty needed for parameter consensus in each iteration. In addition, we also propose an extension of the method that adaptively determines the maximum number of iterations to update the penalty. We show that this approach effectively leads to an adaptive, dynamic network topology underlying the distributed optimization. The utility of the new penalty update schemes is demonstrated on both synthetic and real data, including a computer vision application of distributed structure from motion.",
    "creator" : "LaTeX with hyperref package"
  }
}