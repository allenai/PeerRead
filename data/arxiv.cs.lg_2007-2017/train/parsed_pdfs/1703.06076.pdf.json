{
  "name" : "1703.06076.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Machine learning approach for early detection of autism by combining questionnaire and home video screening",
    "authors" : [ "Halim Abbas", "Ford Garberson", "Eric Glover", "Dennis P Wall" ],
    "emails" : [ "g@ericglover.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Existing screening tools for early detection of autism are expensive, cumbersome, time-intensive, and sometimes fall short in predictive value. In this work, we apply Machine Learning (ML) to gold standard clinical data obtained across thousands of children at risk for autism spectrum disorders to create a low-cost, quick, and easy to apply autism screening tool that performs as well or better than most widely used standardized instruments. This new tool combines two screening methods into a single assessment, one based on short, structured parent-report questionnaires and the other on tagging key behaviors from short, semi-structured home videos of children. To overcome the scarcity, sparsity, and imbalance of training data, we apply creative feature selection, feature engineering, and novel feature encoding techniques. We allow for inconclusive determination where appropriate in order to boost screening accuracy when conclusive. We demonstrate a significant accuracy improvement over standard screening tools in a clinical study sample of 162 children."
    }, {
      "heading" : "1 Introduction",
      "text" : "Patient data pose unique challenges for Machine Learning. Of particular note is the application of machine learning to designing simple, low-cost screening tools for a variety of medical conditions. Labeled data usually originates from tightly controlled clinical environments and is hence clean but sparse, unbalanced, and of a different context to the data available when applying the screening techniques. This limits the reliability and generalizability of ML models trained on such data when applied in the less controlled settings of a typical screening tool. In this paper we highlight these challenges and present useful strategies for addressing them, focusing on the critical problem of affordable, reliable, early detection of autism in children 2-6 years old.\nDiagnosis within the first few years of life dramatically improves the outlook of children with autism, as it allows for treatment while the child’s brain is still rapidly developing1, 2. Unfortunately, in the United States autism is typically not diagnosed earlier than age 4, with approximately 27% of cases remaining undiagnosed at age 83. This delay in diagnosis is driven primarily by a lack of effective screening tools and a shortage of specialists to evaluate at risk children.\nMost autism screeners in use today are based on score-sheets with questions for the parent or the medical practitioner, that produce results by comparing summation scores to predetermined thresholds. Notable examples are the Modified Checklist for Autism in Toddlers, Revised (M-CHAT)4, a checklist-based screening tool for autism that is intended to be administered during developmental screenings with children between the ages of 16 and 30 months, and the Child Behavior Checklist (CBCL)5 which is a parent-completed screening tool. Cognoa6 provides a data driven alternative screening method powered by machine learning and validated by multiple clinical studies 7, 8. To date, Cognoa has been used by over 150,000 parents in the US and internationally using web and native smartphone mobile app tools. The majority of Cognoa users are parents of young children between 18 and 30 months.\nIn this paper we present two new machine learning screeners that are reliable, cost-effective, easy enough to complete in minutes, and achieve higher accuracy than existing screeners. One is based on a short questionnaire about the child which is answered by the parent. The other is based on identification of specific behaviors by trained analysts after watching two or three short videos of the child at their home environment captured by parents using a mobile device.\nThe parent questionnaire screener keys on behavioral patterns typically probed in a standard autism diagnostic instrument, the Autism Diagnostic Interview – Revised (ADI-R)9. This clinical tool consists of a parent interview of 93 multi-part questions with multiple choice and numeric responses which are delivered by a trained professional in a clinical setting. While this instrument is considered a gold-standard, and gives consistent results across examiners, the cost and time to administer it can be\nar X\niv :1\n70 3.\n06 07\n6v 1\n[ cs\n.C Y\n] 1\n5 M\nar 2\nprohibitive. Section 3.1 details our approach to using clinical ADI-R instrument data to create a screener based on a much shorter questionnaire presented directly to parents without supervision.\nThe video screener keys on behavioral patterns typically probed in another diagnostic tool, the Autism Diagnostic Observation Schedule (ADOS)10. ADOS is widely considered a gold standard and is one of the most common behavioral instruments used to aid in diagnosis of autism11. It consists of an interactive, highly standardized examination of the child by trained clinicians in a tightly controlled setting. The ADOS is a multi-modular diagnostic instrument, with different modules for subjects in different developmental stages. Section 3.2 details our approach to using ADOS clinical data to create a video-based screener that relies on analysts evaluating short videos of children filmed by their parents at home.\nThe use of ADI-R and ADOS score-sheets as inputs to train autism screening classifiers was introduced, studied, and clinically validated in previous work12–15. This paper builds upon that work, devising new algorithms that are more accurate and more robust against confounding biases between training and application data, and describing the improvements in the context of best practices for machine learning as applied to clinical science.\nIn Section 3.3, we combine the parent questionnaire and video-based screeners into a single, higher reliability screening tool. Techniques for adapting the binary screeners to allow for a third, “inconclusive” determination are also presented, allowing higher screening accuracy for those children who do receive “autism” or “not-autism” diagnoses at the cost of lower effective coverage.\nFinally, the screeners are applied to a sample of 162 at-risk children who have undergone full clinical examination and received a clinical diagnosis. The performance of our new screening tools surpasses the more conventional MCHAT and CBCL, and the combination screener is shown to achieve higher accuracy than either individual screener."
    }, {
      "heading" : "2 Data",
      "text" : "Training data were compiled from multiple repositories of ADOS and ADI-R score-sheets of boys and girls at 18 to 84 months of age including Boston Autism Consortium, Autism Genetic Resource Exchange, Autism Treatment Network, Simons Simplex Collection, and Vanderbilt Medical Complex. Since such repositories are highly imbalanced towards the incidence of autism, the controls across the datasets were supplemented with balancing data obtained by conducting ADI-R interviews on a random sample of children deemed at low risk for autism from Cognoa’s user base.\nThe clinical validation sample consisted of 230 children who presented to one of three autism centers in the United States at 18 to 72 months of age. Every child received an ADOS as well as standard screeners like M-CHAT and CBCL as appropriate, and a diagnosis was ultimately ascertained by a licensed psychologist. This sample corresponds to a clinical study performed by Kanne et. al.8 in 2016. For 162 of those children, the parents also used their mobile devices to complete the short parental questionnaire and submit the short videos required for the screeners discussed in this paper. Sample breakdown by age group and diagnosis for both the training and clinical validation datasets is shown in Table 1."
    }, {
      "heading" : "3 Approach",
      "text" : "We trained two independent ML classifiers and combined their outputs into a single screening assessment. The parent questionnaire classifier was trained using data from historical item-level ADI-R score-sheets with labels corresponding to established clinical diagnoses. The video classifier was trained using ADOS instrument score-sheets and diagnostic labels. In each case, progressive sampling was used to verify sufficient training volume as detailed in Appendix A.\nThe ADI-R and ADOS instruments are designed to be administered by trained professionals in highly standardized clinical settings and typically take hours. Contrastingly, our screening methods are deliberately designed to be administered at home by parents without expert supervision, and to take only minutes to complete. This change of environment, further detailed in Appendix B, causes significant data degradation and biases resulting in an expected loss of screening accuracy. For each classifier, mindful adjustments to ML methodology were used to mitigate these issues, as explained below.\n2/11"
    }, {
      "heading" : "3.1 Parent questionnaire",
      "text" : "Multiple model variants representing incremental improvements over a generic ML classification approach are shown below. Each model was independently parameter-tuned with a bootstrapped grid search. Class labels were used to stratify the cross-validation folds, and (age, label) pairs were used to weight-balance the samples."
    }, {
      "heading" : "3.1.1 Generic ML baseline variant",
      "text" : "A random forest was trained over the ADI-R instrument data. Each of the instrument’s 155 data columns was treated as a categorical variable and one-hot encoded. The subject’s age and gender were included as features as well. Of the resulting set of features, the top 20 were selected using feature-importance ranking in the decision forest."
    }, {
      "heading" : "3.1.2 Robust feature selection variant",
      "text" : "Due to the small size and sparsity of the training dataset, generic feature selection over hundreds of binary features was not robust, and the selected features (along with the performance of the resulting model) fluctuated from run to run due to the stochastic nature of the learner’s underlying bagging approach. Many ADI-R questions are highly correlated, leading to multiple competing sets of feature selection choices that were seemingly equally powerful during training, but which had different performance characteristics when the underlying sampling bias was exposed via full bootstrapped cross-validation. This resulted in a wide performance range of the variant in Section 3.1.1 as shown in Table 3.\nRobust feature selection overcame that limitation using a two-step approach. First, a 100-count bootstrapped feature selection was run, with a weight balanced 90% random sample selected in each iteration. The top 20 features were selected each time, and a rank-invariant tally was kept for the number of times each feature made it to a top-20 list. Next, the top 30 features in the tally were kept as candidates and all other features were discarded. A final feature-selection run was used to pick the best 20 of these candidate features. Over multiple runs, this approach was found to be more robust to statistical fluctuations, usually selecting the same set of features when run multiple times."
    }, {
      "heading" : "3.1.3 Age silo variant",
      "text" : "This variant built upon the improvements of Section 3.1.2, by exploiting the importance of the dichotomy between pre-phrasal and fully-phrasal language capability in at-risk children. Language development is significant in this domain as it is known to affect the nature in which autism presents, and consequently the kinds of behavioral clues to look for in order to screen for it.\nThis variant achieved better performance by training separate classifiers for children in the younger and older age groups of Table 1. The age dichotomy of < 4, ≥ 4 was chosen to serve as the best proxy for language ability. Feature selection, model parameter-tuning, and cross-validation were run independently for each age group classifier. Before siloing by age group, the classifier was limited to selecting features that work well across children of both developmental stages. Siloing enabled the classifiers to specialize on features that are most developmentally appropriate within each age group."
    }, {
      "heading" : "3.1.4 Severity-level feature encoding variant",
      "text" : "Building upon Section 3.1.3, this variant achieved better performance by replacing one-hot feature encoding with a more context-appropriate technique. One-hot encoding does not distinguish between values that correspond to increasing levels of severity of a behavioral symptom, and values that do not convey a clear concept of severity. This is especially troublesome since a typical ADI-R instrument question includes answer choices from both types of values. For example, ADI-R question 37, which focuses on the child’s tendency to confuse and mix up pronouns, allows for answer codes 0,1,2,3,7,8, and 9. Among those choices, 0 through 3 denote increasing degrees of severity in pronominal confusion, while 7 denotes any other type of pronominal confusion not covered in 0-3 regardless of severity. Codes 8 and 9 denote the non-applicability of the question (for example to a child still incapable of phrasal speech) or the lack of an answer (for example if the question was skipped) respectively. When coding the answers to such question, generic one-hot encoding would allow for non-symptomatic answer codes to be selected as screening features based on phantom correlations present in the dataset.\nSeverity-level encoding converts all answer codes that do not convey a relevant semantic concept to a common value, thereby reducing the chance of useless feature selection, and reducing the number of features to choose from. In addition, severity-level encoding condenses the signal according to increasing ranges of severity. This more closely resembles the way medical practitioners interpret such answer choices in real life, and helps alleviate the problem of sparsity over each of the one-hot encoded features in the dataset. Severity-grouped feature levels also reduce over-fitting and serve to emphasize the patterns of correlation between the features and the target, with signals from multiple answer codes grouped into single binary features in a semantically-appropriate way. The associated severity-level encoding for the possible responses to the exemplary ADI-R question 37 is shown in Table 2."
    }, {
      "heading" : "3.1.5 Aggregate features variant",
      "text" : "Building upon Section 3.1.4, this variant achieved better performance by incorporating aggregate features such as the minimum, maximum, and average severity level, as well as number of answer choices by severity level across the questions corresponding\n3/11\nto the 20 selected features. These new features were especially helpful due to the sparse, shallow, and wide nature of the training set, whereupon any semantically meaningful condensation of the signal can be useful to the trained classifier. The summation of answer choices by severity is a method used by specialists who administer the ADI-R instrument in real life, and is therefore known to carry semantic benefit."
    }, {
      "heading" : "3.1.6 Inconclusive results variant",
      "text" : "Patients with more complex symptom presentation are known to pose challenges to developmental screening. These children often screen as false positives or false negatives, resulting in an overall degradation of screening accuracy that is observed by all standard methods and has become acceptable in the industry. Given that our low-cost instruments do not rely on sophisticated observations to differentiate complex symptom cases, our approach was to avoid assessing them altogether, and to try instead to spot and label them as “inconclusive”.\nBuilding upon 3.1.5, two methods to implement this strategy were devised. The first was to train a binary classifier with a continuous output score, then replace the cutoff threshold with a cutoff range, with values within the cutoff range considered inconclusive. A grid search was used to determine the optimal cutoff range representing a tradeoff between inconclusive determination rate and accuracy over conclusive subjects. The second approach was to train and cross-validate a simple binary classifier, label the correctly and incorrectly predicted samples as conclusive or inconclusive respectively, and then build a second classifier to predict whether a subject would be incorrectly classified by the first classifier. At runtime, the second classifier was used to spot and label inconclusives. The conclusives were sent for classification by a third, binary classifier trained over the conclusive samples only. Both methods for labeling inconclusive results yielded similar performance. Therefore the simpler method of using a threshold range in the machine learning output was used to report inconclusive results for this paper."
    }, {
      "heading" : "3.2 Video",
      "text" : "The second of our two-method approach to autism screening is an ML classifier that uses input answers about the presence and severity of target behaviors among subjects. Analysts provide this information upon viewing two or three 1-minute home videos of children in semi-structured settings, that are taken by parents on their mobile phones. The classifier was trained on item-level data from two of the ADOS modules (module 1: preverbal, module 2: phrased speech) and corresponding clinical diagnosis.\nTwo decision forest ML classifiers were trained corresponding to each ADOS module. For each classifier, 10 questions were selected using robust feature selection as in Section 3.1.2, and allowance was made for inconclusive outcomes as in Section 3.1.6. Each model was independently parameter-tuned with a bootstrapped grid search. Class labels were used to stratify the cross-validation folds, and (age,label) pairs were used to weight-balance the samples.\nProblems related to the change of environment from training to application (detailed in Appendix B) are especially significant in the case of video screening because ADOS involves a 45 minute direct observation of the child by experts, whereas our screening was based on unsupervised short home videos. Specifically, we expect the likelihood of inconclusive or unobserved behaviors and symptoms to be much higher in the application than in the training data, and the assessed level of severity or frequency of observed symptoms to be less reliable in the application than in the training data. The following improvements were designed to help overcome these limitations."
    }, {
      "heading" : "3.2.1 Presence of behavior encoding",
      "text" : "To minimize potential bias from a video analyst misreading the severity of a symptom in a short cell phone video, this encoding scheme improves feature reliability at the expense of feature information content by collapsing all severity gradations of a question into one binary value representing the presence vs absence of the behavior or symptom in question. Importantly, a value of 1 denotes the presence of behavior, regardless of whether the behavior is indicative of autism or of normalcy. This rule ensures that a value of 1 corresponds to a reliable observation, whereas a 0 does not necessarily indicate the absence of a symptom but possibly the failure to observe the symptom within the short window of observation."
    }, {
      "heading" : "3.2.2 Missing value injection to balance the non presence of features for the video screener training data",
      "text" : "While collapsing severity gradations into a single category overcomes noisy severity assessment, it does not help with the problem of a symptom not present or unnoticeable in a short home video. For this reason, it is important that the learning algorithm treat a value of 1 as semantically meaningful, and a value of 0 as inconsequential. To help accomplish that we\n4/11\naugmented the training set with duplicate samples that had some feature values flipped from 1 to 0. The injection of 0s was randomly performed with probabilities such that the sample-weighted ratio of positive to negative samples for which the value of any particular feature is 0 about 50%. Such ratios ensure that the trees in a random forest will be much less likely to draw conclusions from the absence of a feature."
    }, {
      "heading" : "3.3 Combination",
      "text" : "The numerical response of each of the parent questionnaire and video classifiers were combined using logistic regression. Since each of the individual methods was siloed, separate combinators were trained per age group silo. For each combinator, optimal inconclusive output criteria were chosen as discussed Section 3.1.6. The performance characteristics of the overall screening process compared to standard alternative screeners is shown in Section 4.3."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Parent questionnaire performance on training data",
      "text" : "Bootstrapped cross-validation performance metrics for the optimally parameter-tuned version of each of the variants in 3.1 are reported in Table 3. The results for baseline variant 3.1.1 are reported as a confidence interval rather than a single value, because the unreliability of generic feature selection leads to different sets of features selected from run to run, with varying performance results."
    }, {
      "heading" : "4.2 Parent questionnaire performance on clinical data",
      "text" : "Parents of children included in the clinical study answered short, age-appropriate questionnaires chosen using the feature selection approach outlined in 3.1.2. The performance metrics for each of the classification variants that build upon that feature selection scheme are shown in Table 4.\nROC curves in Figure 1 show how our parent questionnaire classification approach outperforms some of the established screening tools like MCHAT and CBCL on the clinical sample."
    }, {
      "heading" : "4.3 Combination screening performance on clinical data",
      "text" : "ROC curves in Figure 2 show how combining the questionnaire and video classifiers into a single assessment further boosted performance on the clinical study sample. Figure 3 shows how allowing for inconclusive determination up to 25% of the time\n5/11\nresults in further accuracy improvement over the conclusive cases."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Machine learning can play a very important role in improving the effectiveness of clinical screeners. We have achieved a significant improvement over established screening tools for autism in young children as demonstrated in a clinical trial. We have also shown some important pitfalls when applying machine learning in a clinical setting, and quantified the benefit of applying proper solutions to address them.\n6/11\n7/11"
    }, {
      "heading" : "A Progressive sampling on training datasets",
      "text" : "Progressive sampling runs were performed to assert available training data is sufficient to build stable ML classifiers. These runs were performed for each classifier variant for both the questionnaire and video based training samples. In each run, boot-strapped cross-validation was used to compute the AUC metric of an optimized random-forest trained over increasingly larger proportions of the training set. The size of the training set was demonstrated to be sufficient for stable learning of ensemble decision trees, as shown in the plots of Figure 4. A similar conclusion was reached for the parental questionnaire classifiers for young and old children, using similar progressive sampling runs of its corresponding training sets."
    }, {
      "heading" : "B Differences between training and application environments",
      "text" : "Since the classification labels used to train medical screening algorithms would have to be come from professional medical diagnoses, it is not feasible to amass large training sets that are class-balanced and sufficiently representative of the general population of children. Our approach is to start with historical medical instrument records of previously diagnosed subjects, and use those as training data for screeners that will rely on information acquired outside the clinical setting. Since the algorithms are trained using features obtained during rigorous clinical evaluations, but applied on data representing proxies of said features acquired in a less controlled setting, we expect the performance of the classifiers to degrade considerably when used as a mobile screener.\nMore specifically, Table 5 details the various mechanisms by which confounding biases may creep into the application data in ways not present in the training data. It should be noted that inaccuracies introduced by such biases cannot be probed by cross validation or similar analysis of the training data alone."
    }, {
      "heading" : "C Age binned results",
      "text" : "Results split for young and old kids separately are shown in Figure 5 through 8.\n9/11\n10/11\n11/11"
    } ],
    "references" : [ {
      "title" : "Socioeconomic inequality in the prevalence of autism spectrum disorder: evidence from a U.S. cross-sectional study",
      "author" : [ "M Durkin" ],
      "venue" : "PloS One",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Prevalence and characteristics of autism spectrum disorder among children aged 8 years — autism and developmental disabilities monitoring network, 11 sites",
      "author" : [ "D. Christensen", "J. Baio", "K Braun" ],
      "venue" : "United States,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "insights from studies of high-risk infants",
      "author" : [ "Zwaigenbaum", "L. et al. Clinical assessment", "management of toddlers with suspected autism spectrum disorder" ],
      "venue" : "Pediatrics 123, 1383–1391",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Diagnosing autism spectrum disorders in primary care",
      "author" : [ "R. Bernier", "A. Mao", "J. Yen" ],
      "venue" : "Practitioner 255(1745),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Manual for the ASEBA preschool forms & profiles",
      "author" : [ "T. Achenbach", "L. Rescorla" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "Clinical evaluation of a novel and mobile autism risk assessment",
      "author" : [ "M. Duda", "J. Daniels", "D. Wall" ],
      "venue" : "J Autism Dev Disord",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Evaluating a mobile-health screening tool for rapid identification of autism spectrum disorder risk in young children",
      "author" : [ "S. Kanne", "L. Carpenter", "Z. Warren" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "a revised version of a diagnostic interview for caregivers of individuals with possible pervasive developmental disorders",
      "author" : [ "C. Lord", "M. Rutter", "Le Couteur", "A. Autism diagnostic interview-revised" ],
      "venue" : "Journal of Autism and Developmental Disorders 24, 659–685",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "a standardized observation of communicative and social behavior",
      "author" : [ "Lord", "C. et al. Autism diagnostic observation schedule" ],
      "venue" : "J Autism Dev Disord 19, 185–212",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A multisite study of the clinical diagnosis of different autism spectrum disorders",
      "author" : [ "C Lord" ],
      "venue" : "Archives of General Psychiatry",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Use of artificial intelligence to shorten the behavioral diagnosis of autism",
      "author" : [ "D. Wall", "R. Dally", "R. Luyster", "J. Jung", "T. Deluca" ],
      "venue" : "PLoS One 7(8):e43855",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "The potential of accelerating early detection of autism through content analysis of YouTube videos",
      "author" : [ "V Fusaro" ],
      "venue" : "PLoS One 16;9(4):e93533",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Clinical evaluation of a novel and mobile autism risk assessment",
      "author" : [ "M. Duda", "J. Daniels", "D. Wall" ],
      "venue" : "Journal of autism developmental disorders",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Existing screening tools for early detection of autism are expensive, cumbersome, time-intensive, and sometimes fall short in predictive value. In this work, we apply Machine Learning (ML) to gold standard clinical data obtained across thousands of children at risk for autism spectrum disorders to create a low-cost, quick, and easy to apply autism screening tool that performs as well or better than most widely used standardized instruments. This new tool combines two screening methods into a single assessment, one based on short, structured parent-report questionnaires and the other on tagging key behaviors from short, semi-structured home videos of children. To overcome the scarcity, sparsity, and imbalance of training data, we apply creative feature selection, feature engineering, and novel feature encoding techniques. We allow for inconclusive determination where appropriate in order to boost screening accuracy when conclusive. We demonstrate a significant accuracy improvement over standard screening tools in a clinical study sample of 162 children.",
    "creator" : "LaTeX with hyperref package"
  }
}