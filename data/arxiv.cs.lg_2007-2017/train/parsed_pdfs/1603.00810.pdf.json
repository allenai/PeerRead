{
  "name" : "1603.00810.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Character-based Neural Machine Translation",
    "authors" : [ "Marta R. Costa-jussà", "José A. R. Fonollosa" ],
    "emails" : [ "marta.ruiz@upc.edu", "jose.fonollosa@upc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages.\nIn this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affixaware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme completely avoids the problem of unknown source words and provides improved results even when the source language is not morphologically rich. The number of target words is still limited by the standard word-based softmax output layer. However the number of unknowns at the output of the translation network is dramatically reduced (by a relative 66%) with a significant overall improvement over both neural and phrase-based baselines. Improvements up to 3 BLEU points are obtained in the German-English WMT task."
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine Translation (MT) is the set of algorithms that aim at transforming a source language into a target language. For the last 20 years, one of the most popular approaches has been statistical phrase-based MT, which uses a combination of\nfeatures to maximise the probability of the target sentence given the source sentence (Koehn et al., 2003). Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results.\nAmong its different strengths neural MT does not need to pre-design feature functions beforehand; optimizes the entire system at once because it provides a fully trainable model; uses word embeddings (Sutskever et al., 2014) so that words (or minimal units) are not independent anymore; and is easily extendable to multimodal sources of information (Elliott et al., 2015). As for weaknesses, neural MT has a strong limitation in vocabulary due to its architecture and it is difficult and computationally expensive to tune all parameters in the deep learning structure.\nIn this paper, we use the neural MT baseline system from (Bahdanau et al., 2015), which follows an encoder-decoder architecture with attention, and introduce elements from the characterbased neural language model (Kim et al., 2016). The translation unit continues to be the word, and we continue using word embeddings related to each word as an input vector to the bidirectional recurrent neural network (attention-based mechanism). The difference is that now the embeddings of each word are no longer an independent vector, but are computed from the characters of the corresponding word. The system architecture has changed in that we are using a convolutional neural network (CNN) and a highway network over characters before the attention-based mechanism of the encoder. This is a significant difference from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters).\nSubword-based representations have already\nar X\niv :1\n60 3.\n00 81\n0v 1\n[ cs\n.C L\n] 2\nM ar\n2 01\n6\nbeen explored in Natural Language Processing (NLP), e.g. for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015). These previous works show different advantages of using character-level information. In our case, with the new characterbased neural MT architecture, we take advantage of intra-word information, which is proven to be extremely useful in other NLP applications (Santos and Zadrozny, 2014; Ling et al., 2015), especially when dealing with morphologically rich languages. When using the character-based source word embeddings in MT, there ceases to be unknown words in the source input, while the size of the target vocabulary remains unchanged. Although the target vocabulary continues with the same limitation as in the standard neural MT system, the fact that there are no unknown words in the source helps to reduce the number of unknowns in the target. Moreover, the remaining unknown target words can now be more successfully replaced with the corresponding source-aligned words. As a consequence, in addition to obtaining a clear reduction of unknown words (66% relative), we obtain a significant improvement in terms of translation quality (up to 3 BLEU points).\nThe rest of the paper is organized as follows. Section 2 briefly explains the architecture of the neural MT that we are using as a baseline system. Section 3 describes the changes introduced in the baseline architecture in order to use characterbased embeddings instead of the standard lookupbased word representations. Section 4 reports the experimental framework and the results obtained in the German-English WMT task. Finally, section 5 concludes with the contributions of the paper and further work."
    }, {
      "heading" : "2 Neural Machine Translation",
      "text" : "Neural MT uses a neural network approach to compute the conditional probability of the target sentence given the source sentence (Cho et al., 2014; Bahdanau et al., 2015). The approach used in this work (Bahdanau et al., 2015) follows the encoder-decoder architecture.First, the encoder reads the source sentence s = (s1, ..sI) and encodes it into a sequence of hidden states h = (h1, ..hI). Then, the decoder generates a\ncorresponding translation t = t1, ..., tJ based on the encoded sequence of hidden states h. Both encoder and decoder are jointly trained to maximize the conditional log-probability of the correct translation.\nThis baseline autoencoder architecture is improved with a attention-based mechanism (Bahdanau et al., 2015), in which the encoder uses a bi-directional gated recurrent unit (GRU). This GRU allows for a better performance with long sentences. The decoder also becomes a GRU and each word tj is predicted based on a recurrent hidden state, the previously predicted word tj−1, and a context vector. This context vector is obtained from the weighted sum of the annotations hk, which in turn, is computed through an alignment model αjk (a feedforward neural network). This neural MT approach has achieved competitive results against the standard phrase-based system in the WMT 2015 evaluation (Jean et al., 2015)."
    }, {
      "heading" : "3 Character-based Machine Translation",
      "text" : "Word embeddings have been shown to boost the performance in many NLP tasks, including machine translation. However, the standard lookupbased embeddings are limited to a finite-size vocabulary for both computational and sparsity reasons. Moreover, the orthographic representation of the words is completely ignored. The standard learning process is blind to the presence of stems, prefixes, suffixes and any other kind of affixes in words.\nAs a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or POS tagging (Ling et al., 2015; Santos and Zadrozny, 2014).\nFor our experiments in neural MT, we selected the best character-based embedding architecture proposed by Kim et al. (Kim et al., 2016) for language modeling. As the Figure 1 shows, the computation of the representation of each word starts with a character-based embedding layer that associates each word (sequence of characters) with a sequence of vectors. This sequence of vectors is then processed with a set of 1D convolution filters of different lengths (from 1 to 7 characters) followed with a max pooling layer. For each convolutional filter, we keep only the output with the maximum value. The concatenation of these\nmax values already provides us with a representation of each word as a vector with a fixed length equal to the total number of convolutional kernels. However, the addition of two highway layers was shown to improve the quality of the language model in (Kim et al., 2016) so we also kept these additional layers in our case. The output of the second Highway layer will give us the final vector representation of each source word, replacing the standard source word embedding in the neural machine translation system.\n! \" # $ % & ' ( )\n*)+,)$-)./0.\n-1&\"&-2)\".\n)3')!!#$45\n6,(2#7().\n-/$8/(,2#/$.\n0#(2)\"5./0.!#00)\")$2.\n()$4125\n6&9./,27,2./0.\n)&-1.0#(2)\"\n:#41;&<.=&<)\"\n:#41;&<.=&<)\"\n:#41;&<.=&<)\"5\nFigure 1: Character-based word embedding\nIn the target size we are still limited in vocabulary by the softmax layer at the output of the network and we kept the standard target word embeddings in our experiments. However, the results seem to show that the affix-aware representation of the source words has a positive influence on all the components of the network. The global optimization of the integrated model forces the translation model and the internal vector representation of the target words to follow the affix-aware codification of the source words."
    }, {
      "heading" : "4 Experimental framework",
      "text" : "This section reports the data used, its preprocessing, baseline details and results with the enhanced character-based neural MT system."
    }, {
      "heading" : "4.1 Data",
      "text" : "We used the German-English WMT data1 including the EPPS, NEWS and Commoncrawl. Preprocessing consisted of tokenizing, truecasing,\n1http://www.statmt.org/wmt15/translation-task.html\nnormalizing punctuation and filtering sentences with more than 5% of their words in a language other than German or English. Statistics are shown in Table 1."
    }, {
      "heading" : "4.2 Baseline systems",
      "text" : "The phrase-based system was built using Moses (Koehn et al., 2007), with standard parameters such as grow-final-diag for alignment, GoodTuring smoothing of the relative frequencies, 5- gram language modeling using Kneser-Ney discounting, and lexicalized reordering, among others. The neural-based system was built using the software from DL4MT2 available in github. We generally used settings from previous work (Jean et al., 2015): networks have an embedding of 620 and a dimension of 1024, a batch size of 32, and no dropout. We used a vocabulary size of 90 thousand words in German-English. Also, as proposed in (Jean et al., 2015) we replaced unknown words (UNKs) with the corresponding source word using the alignment information."
    }, {
      "heading" : "4.3 Results",
      "text" : "Table 3 shows the BLEU results for the baseline systems (including phrase and neural-based, NN) and the character-based neural MT (CHAR). We also include the results for the CHAR and NN systems with post-processing of unknown words, which consists in replacing the UNKs with the corresponding source word (+Src), as suggested in (Jean et al., 2015). BLEU results improve by almost 1.5 points in German-to-English and by more than 3 points in English-to-German. The reduction in the number of unknown words is of 66% relative in German-to-English and 15% relative in English-to-German. This reduction in unknown\n2http://dl4mt.computing.dcu.ie/\nwords is not proportional to the improvement in BLEU, but it is relevant in both directions. Note the number of out-of-vocabulary words of the test set is shown in Table 1. In the cases with postprocessing of unknown words, it is not applicable to compute the number of unknowns since they have all been replaced with the corresponding aligned source word.\nThe character-based embedding seems to have an impact in learning a better translation model at various levels, which may include better alignment, reordering, morphological generation and disambiguation. Table 2 shows some examples of the kind of improvements that the character-based neural MT system is capable of achieving compared to baseline systems. Examples 1 and 2 show how the reduction of source unknowns improves the adequacy of the translation. Examples 3 and 4 show how the character-based approach is able to handle morphological variations. Finally, example 5 shows an appropriate semantic disambiguation."
    }, {
      "heading" : "5 Conclusions",
      "text" : "Neural MT offers a new perspective in the way MT is managed. Its main advantages when com-\npared with previous approaches, e.g. statistical phrase-based, are that the translation is faced with trainable features and optimized in an end-to-end scheme. However, there still remain many challenges left to solve, such as dealing with the limitation in vocabulary size.\nIn this paper we have proposed a modification to the standard encoder/decoder neural MT architecture to use unlimited-vocabulary character-based source word embeddings. This modification allows for a dramatic reduction in the number of unknown words and a high improvement in translation quality. The reduction of unknown words reaches a 66% relative in the German-to-English direction and the improvement in BLEU is about 1.5 points in German-to-English and more than 3 points in English-to-German.\nAs further work, we are currently studying different alternatives to extend the character-based approach to the target side of the neural MT system."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the 7th Framework Program of the European Commission through the International Outgoing Fellowship Marie Curie Action (IMTraP-2011-29951) and also by the Spanish Ministerio de Economı́a y Competitividad, contract TEC2015-69266-P."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved transitionbased parsing by modeling characters instead of words with lstms",
      "author" : [ "Chris Dyer", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Ballesteros et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "Compositional Morphology for Word Representations and Language Modelling",
      "author" : [ "Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Botha et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Botha et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint learning of character and word embeddings",
      "author" : [ "Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huan-Bo Luan" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches",
      "author" : [ "Cho et al.2014] Kyunghyun Cho", "Bart van van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : "In Proc. of the Eighth Workshop on Syntax,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Normalizing tweets with edit scripts and recurrent neural embeddings",
      "author" : [ "Grzegorz Chrupala" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Chrupala.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chrupala.",
      "year" : 2014
    }, {
      "title" : "Multi-language image description with neural sequence models. CoRR, abs/1510.04709",
      "author" : [ "Stella Frank", "Eva Hasler" ],
      "venue" : null,
      "citeRegEx" : "Elliott et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Elliott et al\\.",
      "year" : 2015
    }, {
      "title" : "Montreal neural machine translation systems for wmt15",
      "author" : [ "Jean et al.2015] Sebastien Jean", "Orhan Firat", "Kyunghun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : "In Proc. of the 10th Workshop on Statistical Machine Translation,",
      "citeRegEx" : "Jean et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom" ],
      "venue" : "In Proc. of the Conference on Empirical Methods in Natural Language Processing, Seattle",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2013
    }, {
      "title" : "Characteraware neural language models",
      "author" : [ "Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush" ],
      "venue" : "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI’16)",
      "citeRegEx" : "Kim et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Statistical Phrase-Based Translation",
      "author" : [ "Koehn et al.2003] Philipp Koehn", "Franz Joseph Och", "Daniel Marcu" ],
      "venue" : "In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Koehn et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "Moses: Open Source Toolkit for Statistical Machine Translation",
      "author" : [ "Herbst." ],
      "venue" : "Proc. of the 45th Annual Meeting of the Association for Computational Linguistics, pages 177–180.",
      "citeRegEx" : "Herbst.,? 2007",
      "shortCiteRegEx" : "Herbst.",
      "year" : 2007
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning character-level representations for part-of-speech tagging",
      "author" : [ "Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Santos et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural machine translation of rare words with subword units. CoRR, abs/1508.07909",
      "author" : [ "Barry Haddow", "Alexandra Birch" ],
      "venue" : null,
      "citeRegEx" : "Sennrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "the most popular approaches has been statistical phrase-based MT, which uses a combination of features to maximise the probability of the target sentence given the source sentence (Koehn et al., 2003).",
      "startOffset" : 180,
      "endOffset" : 200
    }, {
      "referenceID" : 15,
      "context" : "Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results.",
      "startOffset" : 51,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results.",
      "startOffset" : 51,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results.",
      "startOffset" : 51,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "beddings (Sutskever et al., 2014) so that words (or minimal units) are not independent anymore; and is easily extendable to multimodal sources of information (Elliott et al.",
      "startOffset" : 9,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : ", 2014) so that words (or minimal units) are not independent anymore; and is easily extendable to multimodal sources of information (Elliott et al., 2015).",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "system from (Bahdanau et al., 2015), which follows an encoder-decoder architecture with attention, and introduce elements from the characterbased neural language model (Kim et al.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : ", 2015), which follows an encoder-decoder architecture with attention, and introduce elements from the characterbased neural language model (Kim et al., 2016).",
      "startOffset" : 140,
      "endOffset" : 158
    }, {
      "referenceID" : 14,
      "context" : "from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : ", 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters).",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al.",
      "startOffset" : 101,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : ", 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : ", 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015).",
      "startOffset" : 73,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "In our case, with the new characterbased neural MT architecture, we take advantage of intra-word information, which is proven to be extremely useful in other NLP applications (Santos and Zadrozny, 2014; Ling et al., 2015), especially when dealing with morphologically rich languages.",
      "startOffset" : 175,
      "endOffset" : 221
    }, {
      "referenceID" : 4,
      "context" : "Neural MT uses a neural network approach to compute the conditional probability of the target sentence given the source sentence (Cho et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "Neural MT uses a neural network approach to compute the conditional probability of the target sentence given the source sentence (Cho et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "The approach used in this work (Bahdanau et al., 2015) follows the encoder-decoder architecture.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "This baseline autoencoder architecture is improved with a attention-based mechanism (Bahdanau et al., 2015), in which the encoder uses a bi-directional gated recurrent unit (GRU).",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "sults against the standard phrase-based system in the WMT 2015 evaluation (Jean et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "ing (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al.",
      "startOffset" : 4,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "ing (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al.",
      "startOffset" : 4,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : ", 2015), parsing (Ballesteros et al., 2015) or POS tagging (Ling et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : ", 2015) or POS tagging (Ling et al., 2015; Santos and Zadrozny, 2014).",
      "startOffset" : 23,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "(Kim et al., 2016) for language modeling.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "However, the addition of two highway layers was shown to improve the quality of the language model in (Kim et al., 2016) so we also kept these additional layers in our case.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Also, as proposed in (Jean et al., 2015) we replaced unknown words (UNKs) with the corresponding source word using the alignment information.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "We also include the results for the CHAR and NN systems with post-processing of unknown words, which consists in replacing the UNKs with the corresponding source word (+Src), as suggested in (Jean et al., 2015).",
      "startOffset" : 191,
      "endOffset" : 210
    } ],
    "year" : 2016,
    "abstractText" : "Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affixaware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme completely avoids the problem of unknown source words and provides improved results even when the source language is not morphologically rich. The number of target words is still limited by the standard word-based softmax output layer. However the number of unknowns at the output of the translation network is dramatically reduced (by a relative 66%) with a significant overall improvement over both neural and phrase-based baselines. Improvements up to 3 BLEU points are obtained in the German-English",
    "creator" : "LaTeX with hyperref package"
  }
}