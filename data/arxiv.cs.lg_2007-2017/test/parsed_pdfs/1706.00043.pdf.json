{
  "name" : "1706.00043.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Biased Importance Sampling for Deep Neural Network Training",
    "authors" : [ "Angelos Katharopoulos", "François Fleuret" ],
    "emails" : [ "name.surname@idiap.ch" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The dramatic increase in available training data has made the use of Deep Neural Networks feasible, which in turn has significantly improved the state-of-the-art in many fields, in particular Computer Vision and Natural Language Processing. Due to the complexity of the resulting optimization problem, computational cost is now the core issue in training these large architectures.\nWhen training such a model, it appears to any practitioner that not all samples are equally important; many of them are properly handled after a few epochs of training, and most could be ignored at that point without impacting the resulting final model.\nFor convex optimization problems, many works [Bordes et al., 2005, Zhao and Zhang, 2015, Needell et al., 2014, Canévet et al., 2016, Richtárik and Takáč, 2013] have taken advantage of the difference in importance among the samples to improve the convergence speed of stochastic optimization methods. On the contrary, important sampling is rarely used in conjunction with Deep Learning models.\nZhao and Zhang [2015] and Alain et al. [2015] prove that sampling according to the gradient norm minimizes the variance of the gradient estimates and improves the convergence speed of SGD.\nHowever, computing the gradient norm requires to compute second-order quantities during the backward pass, which is computationally prohibitive. We propose to use the loss instead, but since computing it still requires a full forward pass, using it directly on all the samples remains intractable. So to reduce computation even more, we propose to use the prediction of a small network, trained alongside the deep model we want to eventually train, to predict an approximation of the importance\nar X\niv :1\n70 6.\n00 04\n3v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\nof the training samples. The complexity of this surrogate allows us to modulate the cost/accuracy trade-off.\nFinally, we show a relationship between importance sampling and maximum-loss minimization, which can be used to improve the generalization ability of the trained Deep Neural Network. We evaluate the proposed method both on image and text datasets.\nIn summary, the contributions of this work are:\n• The use of the loss instead of the gradient norm to estimate the importance of a sample • The creation of a model able to approximate the loss for a low computational overhead • The development of an online algorithm that minimizes a soft max-loss in the training set\nthrough importance sampling"
    }, {
      "heading" : "2 Related Work",
      "text" : "Importance sampling for convex problems has received significant attention over the years. Bordes et al. [2005] developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines. Later Needell et al. [2014] and more recently Zhao and Zhang [2015] developed more general importance sampling methods that improve the convergence of Stochastic Gradient Descent. In particular, the latter has crucially connected the convergence speed of SGD with the variance of the gradient estimator and has shown that the target sampling distribution is the one that minimizes this variance.\nAlain et al. [2015] are the first ones, to our knowledge, that have attempted to use importance sampling for training Deep Neural Networks. They sample according to the exact gradient norm as computed by a cluster of GPU workers. Even with a cluster of GPUs they have to constrain the networks that they use to fully connected layers in order to be able to compute the gradient norm in a reasonable time.\nCanévet et al. [2016] worked on improving the sampling procedure for importance sampling. They imposed a prior tree structure on the weights, and use a sampling procedure inspired by the Monte Carlo Tree Search algorithm, that handles properly the exploration / exploitation dilemma and converges asymptotically to the probability distribution of the weights. However, this method fully relies on the existence of the tree structure, that should reflect the regularity of the importance on the samples, which is in itself a quite complicated embedding problem.\nFinally, there is another class of methods related to importance sampling that can be perceived as using an importance metric quite antithetical to most common methods. Curriculum learning [Bengio et al., 2009] and its evolution self-paced learning [Kumar et al., 2010] present the classifier with easy samples first (samples that are likely to have a small loss) and gradually introduce harder and harder samples."
    }, {
      "heading" : "3 Importance Sampling",
      "text" : "It has already been mentioned that Importance sampling aims at increasing the convergence speed of SGD by reducing the variance of the gradient estimates. In the following sections, we analyze how it works and present an efficient procedure that can be used to train a Deep Learning model.\nMoreover, we show that our importance sampling method has a close relation to maximum loss minimization: by using an exponentiation of the loss as importance instead of the loss itself, we can minimize a smooth max-loss, while controlling the variance."
    }, {
      "heading" : "3.1 Exact Importance Sampling",
      "text" : "Let xi, yi be the i-th input-output pair from the training set, Ψ(·; θ) a Deep Learning model parameterized by the vector θ, and L(·, ·) the loss function to minimize during training. The goal of training is to find\nθ∗ = arg min θ\n1\nN N∑ i=1 L(Ψ(xi; θ), yi) (1)\nwhere N corresponds to the number of examples in the training set. Using Stochastic Gradient Descent with learning rate η, we iteratively update the parameters of our model, between two consecutive iterations t and t+ 1, with\nθt+1 = θt − ηαi∇θtL(Ψ(xi; θt), yi) (2) where i is a discrete random variable sampled according to a distribution P with probabilities pi and αi is a sample weight. For instance, plain SGD with uniform sampling is achieved with αi = 1 and pi = 1 N for all i.\nIf we define the convergence speed S of SGD as the reduction of the distance of the parameter vector θ from the optimal parameter vector θ∗ in two consecutive iterations t and t+ 1\nS = −EP [ ‖θt+1 − θ∗‖22 − ‖θt − θ∗‖ 2 2 ] , (3)\nand if we have\nEP [αi∇θtL(Ψ(xi; θt), yi)] = ∇θt 1\nN N∑ i=1 L(Ψ(xi; θt), yi), (4)\nand define Gi = αi∇θtL(Ψ(xi; θt), yi), then we get (this is a different derivation of the result by Wang et al. [2016])\nS = −EP [ (θt+1 − θ∗)T (θt+1 − θ∗)− (θt − θ∗)T (θt − θ∗) ] (5)\n= −EP [ θTt+1θt+1 − 2θt+1θ∗ + (θ∗)T θ∗ − θTt θt + 2θtθ∗ − (θ∗)T θ∗ ] (6)\n= −EP [ (θt − ηGi)T (θt − ηGi) + 2ηGTi θ∗ − θTt θt ] (7)\n= −EP [ −2η (θt − θ∗)Gi + η2GTi Gi ] (8)\n= 2η (θt − θ∗)EP [Gi]− η2 EP [Gi]T EP [Gi]− η2Tr (VP [Gi]) . (9)\nWith this last expression, we observe that it is possible to gain a speedup by sampling from the distribution that minimizes Tr (VP [Gi]). Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi ∝ ‖∇θtL(Ψ(xi; θt), yi)‖2. However, computing the norm of the gradient for each sample is computationally intensive. Alain et al. [2015] use a distributed cluster of workers and constrain their models to fully connected networks while Zhao and Zhang [2015] only consider convex problems and sample according to the Lipschitz constant of the loss of each sample, which is an upper bound of the gradient norm.\nTo mitigate the computational requirement, we propose to use the loss itself as the importance metric instead of the gradient norm.\nAlthough providing the network with a larger number of confusing examples makes intuitive sense, we also provide theoretical arguments and empirical evidence that in practice the ordering of samples according to the gradient norm is reflected by their ordering according to the loss, and that this is sufficient for the loss-based importance sampling to exhibit the properties of the gradient-norm importance sampling discussed above, and perform better than the uniform sampling, as shown in the experimental results.\nTo sum up, we propose the following importance sampling scheme that creates an unbiased estimator of the gradient vector of Batch Gradient Descent with lower variance:\npi ∝ L(Ψ(xi; θt), yi) (10)\nαi = 1\nNpi . (11)"
    }, {
      "heading" : "3.2 Relation to Max Loss Minimization",
      "text" : "Minimizing the average loss over the training set does not necessarily result in the best model for classification. Shalev-Shwartz and Wexler [2016] argue that minimizing the maximum loss can lead to better generalization performance, especially if there exist a few “rare” samples in the training set.\nIn this section, we show that introducing a minor bias inducing modification in the sample weights αi, we are able to focus on the high-loss samples with a variable intensity up to the point of minimizing the maximum loss.\nInstead of choosing the sample weights αi such that we get an unbiased estimator of the gradient, we define them according to\nαi = 1\nNpki , with k ∈ (−∞, 1], (12)\nand with Li = L(Ψ(xi; θ), yi) and pi ∝ Li, we get\nEP [αi∇θLi] = N∑ i=1 piαi∇θLi (13)\n= N∑ i=1 p1−ki N ∇θLi (14)\n= C N∑ i=1 L1−ki N ∇θLi (15)\n= C ′ N∑ i=1 1 N ∇θL2−ki . (16)\nWe show that we have an unbiased estimator of the gradient of the original loss function raised to the power 2− k ≥ 1. When 2− k 1 we are essentially minimizing the maximum loss but as it will be analyzed in the experiments, smaller values can be helpful both in increasing the convergence speed and improving the generalization error."
    }, {
      "heading" : "3.3 Approximate Importance Sampling",
      "text" : "Although by using the loss instead of the gradient norm, we simplify the problem and make it straightforward for use with Deep Learning models, calculating the loss for a portion of the training set is still prohibitively resource intensive. To alleviate this problem and make importance sampling practical we propose approximating the loss with another model, which we train alongside our Deep Learning model.\nLet Ht = {(j, τ, Lτj ) | j ∈ {0, 1, . . . , N}, τ ≤ t} be the history of the losses where the triplet (j, τ, Lτj ) denotes the sample index j, the iteration index τ and the value of the loss for sample j at iteration τ (samples seen in the same mini-batch appear in the history as triplets with the same τ .).\nOur goal is to learn a model M(xi, yi,Ht−1) ≈ L(Ψ(xi; θt), yi) that has negligible computational complexity compared to Ψ(xi; θt). The above formulation defines a large number of models including approximations of the original neural network Ψ(·). In order to create lightweight models that do not impact the performance of a single forward-backward pass (or impact it minimally) we focus on models that use the class information and the history of the losses. Specifically, we consider models that map the history and the class to two separate representations that are then combined with a simple linear projection.\nTo generate a representation for the history, we run an LSTM over the previous losses of a sample and return the hidden state. The use of an LSTM allows the history of other samples to influence the representation through the shared weights. Let this mapping be represented by Mh(j,Ht−1;πh) parameterized by πh. Regarding the class mapping, we use a simple embedding, namely we map each class to a specific vector in RD. Let this mapping be My(yj ;πy) parameterized by πy . Finally, we solve the following optimization problem and learn a function that predicts the importance of each sample for the next training iteration.\nπ∗, π∗h, π ∗ y = arg min\nπ,πh,πy N∑ i=1 1 N ( M ( Mh(i,Ht−1;πh),My(yi;πy);π ) − L ( Ψ(xi; θt), yi ))2 (17)\nThe precise training procedure is described in pseudocode in algorithm 1 where, for succinctness, we use M(·;π) to denote the composition and the parameters of the models M(·), Mh(·) and My(·).\nAlgorithm 1 Approximate importance sampling 1: Assume inputs η, π0, θ0, k ∈ (−∞, 1], X = {x1, x2, . . . , xN} and Y = {y1, y2, . . . , yN} 2: t← 0 3: repeat 4: S ∼ Uniform(1, N) . Sample a portion of the dataset for further speedup 5: pi ∝M(i, yi,Ht)∀i ∈ S 6: s ∼ Multinomial(P ) 7: α← 1\nNpks 8: θt+1 ← θt − ηα∇θtL(Ψ(xs; θt), ys) 9: πt+1 ← πt − η∇πtM(s, ys,Ht;πt)\n10: Ht+1 ← Ht ∪ {(s, t, L(Ψ(xs; θt), ys))} 11: t← t+ 1 12: until convergence"
    }, {
      "heading" : "3.3.1 Smoothing",
      "text" : "Modern Deep Learning models often contain stochastic layers, such as Dropout, that given a set of constant parameters and a sample can result into vastly different outputs for different runs. This fact, combined with the inevitable approximation error of our importance model, can result into pathological cases of samples being predicted to have a small importance (thus large weight αi) but ending up having high loss.\nTo alleviate this problem we use additive smoothing to influence the sampling distribution towards uniform sampling. We observe experimentally, that a good rule of thumb is to add a constant c such that c ≤ 12N ∑N i=1 L(Ψ(xi; θt), yi) for all iterations t."
    }, {
      "heading" : "4 Experiments",
      "text" : "We analyze experimentally how our importance sampling scheme improves convergence speed and generalization performance, first for image classification with a deep convolution network, on MNIST [LeCun et al., 1998] in § 4.1 and CIFAR10 [Krizhevsky, 2009] in § 4.2, and then for word prediction using deep recurrent network on the Penn Treebank dataset [Marcus et al., 1993] in § 4.3.\nWe compare three different sampling strategies: uniform sampling which is our baseline, oracle which uses the model itself to calculate the importance, and approx which uses our approximation defined in § 3.3. For the hyperparameter k, which controls the smooth max-loss (analyzed in § 3.2), we choose the values k = 1 and k = 0.5, and for some additional analysis with MNIST, we also tried with k = 0.75.\nThe approximation is implemented using two stacked LSTMs with a hidden state size of 32. The input to the first LSTM layer is at most the 10 previously observed loss values of a sample (features of one dimension). Regarding the class label, it is initially projected in R32 and subsequently concatenated with the hidden state of the LSTM. The resulting 64 dimensional feature is used to predict the loss with a simple linear layer.\nTo prevent any over-estimation of performance related to the choice of the step size, we optimized it on the test set, for the uniform sampling baseline with a grid-search in [0.001, 0.01] with step 0.001, hence giving the baseline an unfair advantage over our methods.\nIn all the experiments, we deviate slightly from our theoretical analysis and algorithm by sampling mini-batches instead of single samples in line 6, and using the Adam optimizer [Kingma and Ba, 2014] instead of plain Stochastic Gradient Descent in lines 8 and 9 of Algorithm 1.\nExperiments were conducted using Keras Chollet et al. [2015] with TensorFlow Abadi et al. [2016], and the code to reproduce the experiments will be provided under an open source license when the paper will be published.\n0 2000 4000 6000 Minibatches\n(a) Training\n58500 59000 59500 60000 Samples\n(b) Final Loss distribution\n2000 4000 6000 Minibatches\n(c) Classification error\nFigure 1: Averaged results of 10 independent runs on MNIST. Figure 1a shows the moving average (solid line) and moving standard deviation (light areas) of the training loss. Figure 1b depicts the per sample loss in the training set after training (the samples are sorted in ascending order according to the loss). Finally, with the last Figure 1c, we show the generalization performance of each run on the test set. The effects of the hyperparameter k (see § 3.2) are obvious, both in terms of minimizing the maximum loss in the training set (1b) and achieving better generalization performance (1c)."
    }, {
      "heading" : "4.1 Image classification with MNIST",
      "text" : "Our first series of experiments were conducted on MNIST. Given the simplicity of the dataset, we chose for model a down-sized variant of the VGG architecture [Simonyan and Zisserman, 2014], with two convolutional layers with 32 filters of size 3× 3, a max-pooling layer and a dropout layer with a dropout rate of 0.25, without batch normalization, followed by the same combination of layers with twice the number of filters. Finally, we add two fully connected layers with 512 neurons and dropout with rate 0.5, and a final classification layer. All the layers use the ReLU activation function.\nEach network is trained for 6, 000 iterations with a mini-batch size of 128. We compute the loss and accuracy on the test set every 300 mini-batches. For each set of parameters we run 10 independent runs with different random seeds and report the average. We can see in Figure 1a that importance sampling methods reduce the variance and have significantly smaller oscillations during training than uniform sampling, which translates into faster convergence. Figure 1b shows that lower k indeed aggressively reduces the loss of individual samples, which translates in higher accuracy, as shown on Figure 1c. It is noteworthy that training with k = 0.5 leads to less than 1% error after a single epoch and that, as shown in Table 1, an error of 0.6% is reached 15 epochs earlier than with uniform. Finally, we notice in Table 1, that approx also improves the convergence speed and generalization error compared to uniform by achieving 0.65% error 5 epochs earlier.\n0 10000 20000 30000 40000 50000 Minibatches\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nT ra\nin in\ng lo\nss\n(a) Training\n0 10000 20000 30000 40000 Minibatches\n8\n10\n12\n14\nT es\nt E\nrr or\n(% )\n(b) Biased vs Unbiased sampling\n0 10000 20000 30000 40000 50000 Minibatches\n8\n10\n12\n14\nT es\nt E\nrr or\n(% )\nuniform\noracle-k=1\noracle-k=0.5\napprox-k=1\napprox-k=0.5\n(c) Classification error\nFigure 2: Averaged results of 3 independent runs on CIFAR10. Figure 2a shows the moving average (solid line) and the moving standard deviation (shaded areas) of the training loss. Figure 2b depicts the moving average of the test classification error of the four importance sampling schemes. Finally, Figure 2c compares the generalization performance of uniform, oracle and approximate importance sampling."
    }, {
      "heading" : "4.2 Image classification with CIFAR10",
      "text" : "Our second series of experiments were conducted on CIFAR10, which is a more challenging dataset, commonly used to evaluate new Deep Learning methods. We developed a VGG-inspired network (with batch normalization), which consists of three convolution-pooling blocks with 64, 128 and 256 filters, two fully connected layers of sizes 1024 and 512, and a classification layer. Dropout is used in a similar manner as for the MNIST network with rates 0.25 after each pooling layer and 0.5 between the fully connected layers. The activation function in all layers is ReLU.\nEach network is trained for 50, 000 iterations using a batch size of 128 samples. After 35, 000 iterations we decrease the learning rate, by multiplying it by 10−1. We perform minor data augmentation by creating 500, 000 images generated by random flipping, horizontal and vertical shifting of the original images. For the importance sampling strategies, we use smoothing as described in § 3.3.1. In particular, the importance of each sample is incremented by 12 L̄, where L̄ is the mean of the training loss computed by the exponential moving average of the mini-batch losses. Furthermore, we run each method with 3 different random seeds and report the mean.\nFigure 2 depicts the results of our experiment on CIFAR10. In Figure 2a, we reproduce the results observed in the MNIST experiment in terms of variance reduction. In addition, we note that the proposed approximation is also able to reduce the gradient variance and achieve faster convergence in the training set. Furthermore, in Figure 2b we observe that biased sampling has better classification performance both using the oracle and the approximation, thus validating our initial assumption that this kind of bias can result in improved generalization ability. Finally, in the last Figure 2c, we can see that importance sampling affects the training primarily during the first epochs. This phenomenon occurs due to the large initial learning rate (necessary for fast convergence) which results in large gradient variance. Specifically, we see that uniform sampling achieves less than 10% error only after the learning rate decrease at epoch 116 instead of 78 for our approximation and 48 for the oracle. The results in Table 1 also show clear improvement compared to uniform for every method, both oracle and approx. Specifically, our approximation with k = 0.5 reaches the performance of uniform sampling at 100 epochs almost 50 epochs faster."
    }, {
      "heading" : "4.3 Word prediction with Penn Tree Bank",
      "text" : "To assess the generality of our method, we conducted experiments on a language modeling task. We used the Penn Treebank language dataset, as preprocessed by Mikolov et al. [2011]1, and a recurrent neural network as the language model. Initially, we split the dataset into sentences and add an “End of sentence” token (resulting in a vocabulary of 10,001 words). For each word we use the previous 20 words, if available, as context for the neural network. Our language model is similar to the small LSTM used by Zaremba et al. [2014] with 256 units, a word embedding in R64 and dropout with rate 0.5.\n1http://www.fit.vutbr.cz/~imikolov/rnnlm/\n0 20000 40000 60000 80000 100000 Minibatches\n4.5\n5.0\n5.5\n6.0\nT ra\nin in\ng lo\nss\n(a) Training\n0 20000 40000 60000 80000 100000 Minibatches\n4.5\n5.0\n5.5\n6.0\nT ra\nin in\ng lo\nss\n(b) Class based importance sampling\n0 20 40 Minibatches × 2000\n120\n130\n140\n150\n160\n170\n180\nV al\nid at\nio n\nP er\np le\nxi ty\nuniform\noracle-k=0.5\napprox-k=0.5\npcg-k=0.5\n(c) Validation Perplexity\nFigure 3: Averaged results of 3 independent runs on Penn Treebank. Figure 3a shows the moving average (solid line) and the moving standard deviation (shaded area) of the training loss. Figure 3b depicts the variance reduction achieved with a simpler univariate Gaussian model per class. Finally, Figure 3c compares the perplexity on the validation set between training with importance sampling and uniform sampling.\nIn terms of training the language models, we use a batch size of 128 words and train each network for 100,000 iterations. For the importance sampling strategies, we use constant smoothing instead of adaptive as in the CIFAR10 experiment. To choose the smoothing constant, we experiment with the values {0.5, 1, 2.5} and choose 0.5 because it performs better in terms of variance minimization during training. Finally, we run each method with 3 different random seeds and report the mean.\nThe results of the language modeling experiment are presented in Figure 3. In Figure 3a, it is clear that the variance is reduced using both the oracle and our approximation of the loss, and that the gap in performance between the two is insignificant. We theorize that the rich class information can be utilized to model the loss more effectively. To validate this theory we also conduct an experiment modeling the loss by a univariate Gaussian for each class (denoted as “pcg”) and use the mean as the importance. The corresponding results are depicted in Figure 3b. We observe a reduction in variance but it is less significant than with our approximation.\nFinally, both in Figure 3c and Table 1, we observe that importance sampling improves the convergence speed as well as the generalization error in the language modeling task. Surprisingly, the oracle performs worse than our approximation in this task, which can be explained by the filtering across mini-batches that the approximation provides. Nevertheless, it still improves on uniform with significant variance reduction (see Figure 3a) that could be used, for instance to increase the learning rate."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have proposed an importance sampling scheme suitable for use with Deep Learning models and an optional biased gradient estimator that can focus on hard examples in the training set and improve the generalization performance of our models. Furthermore, we have shown that the loss can be approximated with a model with significantly lower complexity paving the way for efficient and practical training of Deep models with importance sampling.\nSubjects for further research include the experimentation with other models for approximate importance sampling as well as the combination of our proposed importance sampling method with optimized implementations that can significantly improve wall clock training time."
    }, {
      "heading" : "A Justification for sampling with the loss",
      "text" : "The goal of this analysis is to justify the use of the loss as the importance metric instead of the gradient norm and provide additional evidence (besides the experiments in the paper) that it is an improvement over uniform sampling.\nInitially, in § A.1 we will show that sampling with the loss is better at minimizing an upper bound to the variance of the gradients than uniform sampling.\nSubsequently, in § A.2 we provide additional empirical evidence that the loss is a surrogate for the gradient norm by computing the exact gradient norm for a limited number of samples and comparing it to the loss.\nA.1 Theoretical justification\nInitially, we will show in lemma 2 that the most common loss functions for classification and regression define the same ordering as their gradient norm. Subsequently, we will use this derivation, in § A.1.1, together with an upper bound to the variance of the gradients and show that sampling according to the loss reduces this upper bound compared to uniform sampling.\nFirstly, we prove two lemmas that will be later used in the analysis.\nLemma 1. Let f(x) : R→ R be a strictly convex monotonically decreasing function then f(x1) > f(x2) ⇐⇒ ∣∣∣∣ ∂f∂x1 ∣∣∣∣ > ∣∣∣∣ ∂f∂x2 ∣∣∣∣ ∀ x1, x2 ∈ R. (18)\nProof. By the definition of f(x) we have\n∂2f ∂x2 > 0 ∀ x (19)\n∂f ∂x ≤ 0 ∀ x, (20)\nwhich means that ∂f∂x is monotonically increasing and non-positive. In that case we have\nx1 < x2 ⇐⇒ f(x1) > f(x2) (21)\nx1 < x2 ⇐⇒ ∂f\n∂x1 <\n∂f\n∂x2 ⇐⇒ ∣∣∣∣ ∂f∂x1 ∣∣∣∣ > ∣∣∣∣ ∂f∂x2 ∣∣∣∣ (22) therefore proving the lemma.\nLemma 2. Let L(ψ) : D → R be either the negative log likelihood or the squared error loss function defined respectively as\nL1(ψ) = −yT log(ψ) y ∈ {0, 1}d s.t. yT y = 1 D = [0, 1]d s.t. ‖ψ‖1 = 1 (23) L2(ψ) = ‖y − ψ‖22 y ∈ Rd D = Rd (24)\nwhere y is the target vector. Then\nL(ψ1) > L(ψ2) ⇐⇒ ‖∇ψL(ψ1)‖ > ‖∇ψL(ψ2)‖ (25)\nProof. In the case of the squared error loss we have\n‖∇ψL(ψ)‖22 = ‖−2(y − ψ)‖ 2 2 = 4L(ψ), (26)\nthus proving the lemma.\nFor the log likelihood loss we can use the fact that only one dimension of y can be non-zero and prove it using lemma 1 because f(x) = −log(x) is a strictly convex monotonically decreasing function.\nA.1.1 Main analysis\nThe goal for importance sampling is to minimize Tr (V [∇θL(Ψ(xi; θ), yi)]) = E [ ‖∇θL(Ψ(xi; θ), yi)‖22 ] . (27)\nTo perform importance sampling, we sample according to the distribution P with probabilities pi and use per sample weights αi = 1Npi in order to have an unbiased estimator of the gradients. Consequently, the variance of the gradients is\nEP [ ‖αi∇θL(Ψ(xi; θ), yi)‖22 ] = N∑ i=1 piα 2 i ‖∇θL(Ψ(xi; θ), yi)‖22 (28)\n= N∑ i=1 1 Npi 1 N ‖∇θL(Ψ(xi; θ), yi)‖22 (29)\n= N∑ i=1 αi 1 N ‖∇θL(Ψ(xi; θ), yi)‖22 . (30)\nAssuming that the neural network is Lipschitz continuous (assumption that holds when the weights are not infinite) with constant K, we derive the following upper bound to the variance\nEP [ ‖αi∇θL(Ψ(xi; θ), yi)‖22 ] ≤ N∑ i=1 αi 1 N ‖∇θΨ(xi; θ)‖22 ∥∥∇Ψ(xi;θ)L(Ψ(xi; θ), yi)∥∥22 (31) ≤ K2\nN∑ i=1 αi 1 N ∥∥∇Ψ(xi;θ)L(Ψ(xi; θ), yi)∥∥22 . (32) Since we have a finite set of samples, there exists a constant c such that\nL(Ψ(xi; θ), yi) + c ≥ ∥∥∇Ψ(xi;θ)L(Ψ(xi; θ), yi)∥∥∀ i ∈ {1, 2, . . . , N}. (33)\nHowever, using lemma 2 we know that this upper bound is better than uniform becauseL(Ψ(xi; θ), yi) and ∥∥∇Ψ(xi;θ)L(Ψ(xi; θ), yi)∥∥ grow and shrink in tandem. In particular the following equation holds, N∑ i=1 ( L(Ψ(xi; θ), yi) + c−\n∥∥∇Ψ(xi;θ)L(Ψ(xi; θ), yi)∥∥) < N∑ i=1 ( max ∥∥∇Ψ(xi;θ)L(Ψ(xi; θ), yi)∥∥− ∥∥∇Ψ(xi;θ)L(Ψ(xi; θ), yi)∥∥) . (34)\nA.2 Empirical justification\nIn this section, we provide empirical evidence regarding the use of the loss instead of the gradient norm as the importance metric. Specifically, we conduct experiments computing the exact gradient norm and the loss value for the first 20,000 samples during training. The gradient norm is normalized in each mini-batch to account for the changes in the norm of the weights.\nSubsequently, we plot the loss sorted by the gradient norm. If there exists C such that L(Ψ(xi; θ), yi) = C ‖∇θL(Ψ(xi; θ), yi)‖ we should see approximately a line. In case of order preservation we should see a monotonically increasing function.\nIn figure 4, we observe a correlation between the gradient norm and the loss. In all cases, samples with high gradient norm also have high loss. In the Penn Treebank dataset, (Figure 4c), there exist some samples with high loss but very low gradient norm. This can be explained because LSTMs use the tanh(·) activation function which can have very low gradient but incorrect output. We note that this cannot hurt performance, it just means that we waste some CPU/GPU cycles on samples that are incorrectly classified but will not affect the parameters heavily."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "author" : [ "M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin" ],
      "venue" : "arXiv preprint arXiv:1603.04467,",
      "citeRegEx" : "Abadi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Abadi et al\\.",
      "year" : 2016
    }, {
      "title" : "Variance reduction in sgd by distributed importance sampling",
      "author" : [ "G. Alain", "A. Lamb", "C. Sankar", "A. Courville", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06481,",
      "citeRegEx" : "Alain et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Alain et al\\.",
      "year" : 2015
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston" ],
      "venue" : "In Proceedings of the 26th annual international conference on machine learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Fast kernel classifiers with online and active learning",
      "author" : [ "A. Bordes", "S. Ertekin", "J. Weston", "L. Bottou" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2005
    }, {
      "title" : "Importance sampling tree for large-scale empirical expectation",
      "author" : [ "O. Canévet", "C. Jose", "F. Fleuret" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Canévet et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Canévet et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky" ],
      "venue" : "Master’s thesis,",
      "citeRegEx" : "Krizhevsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2009
    }, {
      "title" : "Self-paced learning for latent variable models",
      "author" : [ "M.P. Kumar", "B. Packer", "D. Koller" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kumar et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2010
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Y. LeCun", "C. Cortes", "C.J. Burges" ],
      "venue" : null,
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Empirical evaluation and combination of advanced language modeling techniques",
      "author" : [ "T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Černockỳ" ],
      "venue" : "In Twelfth Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm",
      "author" : [ "D. Needell", "R. Ward", "N. Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Needell et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Needell et al\\.",
      "year" : 2014
    }, {
      "title" : "On optimal probabilities in stochastic coordinate descent methods",
      "author" : [ "P. Richtárik", "M. Takáč" ],
      "venue" : "arXiv preprint arXiv:1310.3438,",
      "citeRegEx" : "Richtárik and Takáč.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richtárik and Takáč.",
      "year" : 2013
    }, {
      "title" : "Minimizing the maximal loss: How and why",
      "author" : [ "S. Shalev-Shwartz", "Y. Wexler" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz and Wexler.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Wexler.",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Accelerating deep neural network training with inconsistent stochastic gradient descent",
      "author" : [ "L. Wang", "Y. Yang", "M.R. Min", "S. Chakradhar" ],
      "venue" : "arXiv preprint arXiv:1603.05544,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "W. Zaremba", "I. Sutskever", "O. Vinyals" ],
      "venue" : "arXiv preprint arXiv:1409.2329,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic optimization with importance sampling for regularized loss minimization",
      "author" : [ "P. Zhao", "T. Zhang" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Zhao and Zhang.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao and Zhang.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "For convex optimization problems, many works [Bordes et al., 2005, Zhao and Zhang, 2015, Needell et al., 2014, Canévet et al., 2016, Richtárik and Takáč, 2013] have taken advantage of the difference in importance among the samples to improve the convergence speed of stochastic optimization methods. On the contrary, important sampling is rarely used in conjunction with Deep Learning models. Zhao and Zhang [2015] and Alain et al.",
      "startOffset" : 46,
      "endOffset" : 415
    }, {
      "referenceID" : 1,
      "context" : "Zhao and Zhang [2015] and Alain et al. [2015] prove that sampling according to the gradient norm minimizes the variance of the gradient estimates and improves the convergence speed of SGD.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "Curriculum learning [Bengio et al., 2009] and its evolution self-paced learning [Kumar et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : ", 2009] and its evolution self-paced learning [Kumar et al., 2010] present the classifier with easy samples first (samples that are likely to have a small loss) and gradually introduce harder and harder samples.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "Bordes et al. [2005] developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "Bordes et al. [2005] developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines. Later Needell et al. [2014] and more recently Zhao and Zhang [2015] developed more general importance sampling methods that improve the convergence of Stochastic Gradient Descent.",
      "startOffset" : 0,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "Bordes et al. [2005] developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines. Later Needell et al. [2014] and more recently Zhao and Zhang [2015] developed more general importance sampling methods that improve the convergence of Stochastic Gradient Descent.",
      "startOffset" : 0,
      "endOffset" : 210
    }, {
      "referenceID" : 1,
      "context" : "Alain et al. [2015] are the first ones, to our knowledge, that have attempted to use importance sampling for training Deep Neural Networks.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Alain et al. [2015] are the first ones, to our knowledge, that have attempted to use importance sampling for training Deep Neural Networks. They sample according to the exact gradient norm as computed by a cluster of GPU workers. Even with a cluster of GPUs they have to constrain the networks that they use to fully connected layers in order to be able to compute the gradient norm in a reasonable time. Canévet et al. [2016] worked on improving the sampling procedure for importance sampling.",
      "startOffset" : 0,
      "endOffset" : 427
    }, {
      "referenceID" : 15,
      "context" : "and define Gi = αi∇θtL(Ψ(xi; θt), yi), then we get (this is a different derivation of the result by Wang et al. [2016])",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi ∝ ‖∇θtL(Ψ(xi; θt), yi)‖2.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi ∝ ‖∇θtL(Ψ(xi; θt), yi)‖2.",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi ∝ ‖∇θtL(Ψ(xi; θt), yi)‖2. However, computing the norm of the gradient for each sample is computationally intensive. Alain et al. [2015] use a distributed cluster of workers and constrain their models to fully connected networks while Zhao and Zhang [2015] only consider convex problems and sample according to the Lipschitz constant of the loss of each sample, which is an upper bound of the gradient norm.",
      "startOffset" : 0,
      "endOffset" : 231
    }, {
      "referenceID" : 1,
      "context" : "Alain et al. [2015] and Zhao and Zhang [2015] show that this distribution has probabilities pi ∝ ‖∇θtL(Ψ(xi; θt), yi)‖2. However, computing the norm of the gradient for each sample is computationally intensive. Alain et al. [2015] use a distributed cluster of workers and constrain their models to fully connected networks while Zhao and Zhang [2015] only consider convex problems and sample according to the Lipschitz constant of the loss of each sample, which is an upper bound of the gradient norm.",
      "startOffset" : 0,
      "endOffset" : 351
    }, {
      "referenceID" : 13,
      "context" : "Shalev-Shwartz and Wexler [2016] argue that minimizing the maximum loss can lead to better generalization performance, especially if there exist a few “rare” samples in the training set.",
      "startOffset" : 0,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "We analyze experimentally how our importance sampling scheme improves convergence speed and generalization performance, first for image classification with a deep convolution network, on MNIST [LeCun et al., 1998] in § 4.",
      "startOffset" : 193,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "1 and CIFAR10 [Krizhevsky, 2009] in § 4.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "2, and then for word prediction using deep recurrent network on the Penn Treebank dataset [Marcus et al., 1993] in § 4.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "In all the experiments, we deviate slightly from our theoretical analysis and algorithm by sampling mini-batches instead of single samples in line 6, and using the Adam optimizer [Kingma and Ba, 2014] instead of plain Stochastic Gradient Descent in lines 8 and 9 of Algorithm 1.",
      "startOffset" : 179,
      "endOffset" : 200
    }, {
      "referenceID" : 4,
      "context" : "In all the experiments, we deviate slightly from our theoretical analysis and algorithm by sampling mini-batches instead of single samples in line 6, and using the Adam optimizer [Kingma and Ba, 2014] instead of plain Stochastic Gradient Descent in lines 8 and 9 of Algorithm 1. Experiments were conducted using Keras Chollet et al. [2015] with TensorFlow Abadi et al.",
      "startOffset" : 180,
      "endOffset" : 340
    }, {
      "referenceID" : 0,
      "context" : "[2015] with TensorFlow Abadi et al. [2016], and the code to reproduce the experiments will be provided under an open source license when the paper will be published.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "Given the simplicity of the dataset, we chose for model a down-sized variant of the VGG architecture [Simonyan and Zisserman, 2014], with two convolutional layers with 32 filters of size 3× 3, a max-pooling layer and a dropout layer with a dropout rate of 0.",
      "startOffset" : 101,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "We used the Penn Treebank language dataset, as preprocessed by Mikolov et al. [2011]1, and a recurrent neural network as the language model.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "We used the Penn Treebank language dataset, as preprocessed by Mikolov et al. [2011]1, and a recurrent neural network as the language model. Initially, we split the dataset into sentences and add an “End of sentence” token (resulting in a vocabulary of 10,001 words). For each word we use the previous 20 words, if available, as context for the neural network. Our language model is similar to the small LSTM used by Zaremba et al. [2014] with 256 units, a word embedding in R and dropout with rate 0.",
      "startOffset" : 63,
      "endOffset" : 439
    } ],
    "year" : 2017,
    "abstractText" : "Importance sampling has been successfully used to accelerate stochastic optimization in many convex problems. However, the lack of an efficient way to calculate the importance still hinders its application to Deep Learning. In this paper, we show that the loss value can be used as an alternative importance metric, and propose a way to efficiently approximate it for a deep model, using a small model trained for that purpose in parallel. This method allows in particular to utilize a biased gradient estimate that implicitly optimizes a soft max-loss, and leads to better generalization performance. While such method suffers from a prohibitively high variance of the gradient estimate when using a standard stochastic optimizer, we show that when it is combined with our sampling mechanism, it results in a reliable procedure. We showcase the generality of our method by testing it on both image classification and language modeling tasks using deep convolutional and recurrent neural networks. In particular, in case of CIFAR10 we reach 10% classification error 50 epochs faster than when using uniform sampling.",
    "creator" : "LaTeX with hyperref package"
  }
}