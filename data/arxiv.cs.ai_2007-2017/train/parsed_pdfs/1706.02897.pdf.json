{
  "name" : "1706.02897.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bandit Models of Human Behavior: Reward Processing in Mental Disorders",
    "authors" : [ "Djallel Bouneffouf", "Irina Rish", "Guillermo A. Cecchi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 6.\n02 89\n7v 1\n[ cs\n.A I]\n7 J\nun 2\npropose here a general parametric framework for multi-armed bandit problem, which extends the standard Thompson Sampling approach to incorporate reward processing biases associated with several neurological and psychiatric conditions, including Parkinson’s and Alzheimer’s diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. We demonstrate empirically that the proposed parametric approach can often outperform the baseline Thompson Sampling on a variety of datasets. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions."
    }, {
      "heading" : "1 Introduction",
      "text" : "In daily-life decision making, from choosing a meal at a restaurant to deciding on a place to visit during a vacation, and so on, people often face the classical exploration versus exploitation dilemma, requiring them to choose between following a good action chosen previously (exploitation) and obtaining more information about the environment which can possibly lead to better actions in the future, but may also turn out to be a bad choice (exploration).\nThe exploration-exploitation trade-off is typically modeled as the multi-armed bandit (MAB) problem, stated as follows: given N possible actions (“arms”), each associated with a fixed, unknown and independent reward probability distribution [1,2], an agent selects an action at each time point and receives a reward, drawn from the corresponding distribution, independently of the previous actions.\nIn order to better understand and model human decision-making behavior, scientists usually investigate reward processing mechanisms in healthy subjects [3]. However, neurogenerative and psychiatric disorders, often associated with reward processing disruptions, can provide an additional resource for deeper understanding of human decision making mechanisms. Furthermore, from the perspective of evolutionary psychiatry, various mental disorders, including depression, anxiety, ADHD, addiction and even schizophrenia can be considered as “extreme points” in a continuous spectrum of behaviors and traits developed for various purposes during evolution, and somewhat less extreme versions of those traits can be actually beneficial in specific environments (e.g., ADHD-like fast-switching attention can be life-saving in certain environments, etc.). Thus, modeling decision-making biases and traits associated with various disorders may actually enrich the existing computational decision-making models, leading to potentially more flexible and better-performing algorithms.\nHerein, we focus on reward-processing biases associated with several mental disorders, including Parkinson’s and Alzheimer disease, ADHD, addiction and chronic pain. Our questions are: is it possible to extend standard stochastic bandit algorithms to mimic human behavior in such disorders? Can such generalized approaches outperform standard bandit algorithms on specific tasks?\nWe show that both questions can be answered positively. We build upon the Thompson Sampling, a state-of-art approach to multi-arm bandit problem, and extend it to a parametric version which allows to incorporate various reward-processing biases known to be associated with particular disorders. For example, it was shown that (unmedicated) patients with Parkinson’s disease appear to learn better from negative rather than from positive rewards [4]; another example is addictive behaviors which may be associated with an inability to forget strong stimulus-response associations from the past, i.e. to properly discount past rewards [5], and so on. More specifically, we propose a parameteric model which introduces weights on incoming positive and negative rewards, and on reward histories, extending the standard parameter update rules in Bernoulli Thompson Sampling; tuning the parameter settings allows us to better capture specific rewardprocessing biases.\nOur empirical results demonstrate that the proposed approach outperforms the baseline Thomp-\nson Sampling on a variety of UCI benchmarks Furthermore, we show how parameter-tuning in the proposed model allows to mimic certain aspects of the behavior associated with mental disorders mentioned above, and thus may provide a valuable tool for improving our understanding of such disorders.\nThe rest of this paper is organized as follows. Section 2 reviews related work. Section 3 describes the MAB model and the proposed algorithm. The experimental evaluation for different setting is presented in Section 5. The last section concludes the paper and identifies directions for future works."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Reward Processing in Mental Disorders",
      "text" : "The literature on the reward processing abnormalities in particular neurological and psychiatric disorders is quite extensive; below we summarize some of the recent developments in this fastgrowing field.\nParkinson’s disease (PD). It is well-known that the neuromodulator dopamine plays a key role in reinforcement learning processes. PD patients, who have depleted dopamine in the basal ganglia, tend to have impaired performance on tasks that require learning from trial and error. For example, [4] demonstrate that off-medication PD patients are better at learning to avoid choices that lead to negative outcomes than they are at learning from positive outcomes, while dopamine medication typically used to treat PD symptoms reverses this bias.\nAlzheimer’s disease (AD). This is the most common cause of dementia in the elderly and, besides memory impairment, it is associated with a variable degree of executive function impairment and visuospatial impairment. As discussed in [3], AD patients have decreased pursuit of rewarding behaviors, including loss of appetite; these changes are often secondary to apathy, associated with diminished reward system activity. Furthermore, poor performance on certain tasks is correlated with memory impairments.\nFrontotemporal dementia, behavioral variant (bvFTD). Frontotemporal dementia (bvFTD)\ntypically involves a progressive change in personality and behavior including disinhibition, apathy, eating changes, repetitive or compulsive behaviors, and loss of empathy [3], and it is hypothesized that those changes are associated with abnormalities in reward processing. For example, changes in eating habits with a preference for sweet, carbohydrate rich foods and overeating in bvFTD patients can be associated with abnormally increased reward representation for food, or impairment in the negative (punishment) signal associated with fullness.\nAttention-deficit/hyperactivity disorder (ADHD). Authors in [6] suggest that the strength of the association between a stimulus and the corresponding response is more susceptible to degradation in ADHD patients, which suggests problems with storing the stimulus-response associations. Among other functions, storing the associations requires working memory capacity, which is often impaired in ADHD patients.\nAddiction. In [5], it is demonstrated that patients suffering from addictive behavior are not able to forget the stimulus-response associations, which causes them to constantly seek the stimulus which generated such association.\nChronic pain. In [7], it is suggested that chronic pain results in a hypodopaminergic (low dopamine) state that impairs motivated behavior, resulting into a reduced drive in chronic pain patients to pursue the rewards. Decreased reward response may underlie a key system mediating the anhedonia and depression common in chronic pain.\nA variety of computational models was proposed for studying the disorders of reward pro-\ncessing in specific disorders, including, among others [4,8,9,10,5,11].\nHowever, none of the above studies is proposing a unifying model that can represent a wide range of reward processing disorders; moreover, none of the above studies used the multi-arm bandit model simulating human online decision-making."
    }, {
      "heading" : "2.2 Multi-Armed Bandit (MAB)",
      "text" : "The multi-armed bandit (MAB) problem models a sequential decision-making process, where at each time point a player selects an action from a given finite set of possible actions, attempting to maximize the cumulative reward over time.\nMAB is frequently used in reinforcement learning to study the exploration/exploitation tradeoff, and is an active area of research since the 1950s. Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14]. Recently, there has been a surge of interest in a Bayesian formulation [15], involving the algorithm known as Thompson sampling [16]. Theoretical analysis in [17] shows that Thompson sampling for Bernoulli bandits asymptotically achieves the optimal performance limit. Empirical analysis of Thompson sampling, including problems more complex than the Bernoulli bandit, demonstrates that its performance is highly competitive with other approaches [15,18].\nPsychological study done in [19] shows that, instead of maximizing output by a deliberate mean-variance trade-off, participants approach dynamic decision-making problems by utilizing a probabilitymatching heuristic. Thus, their behavior is better described by the Thompson sampling choice rule than by the Upper Confidence Bound (UCB) approach [2]. However, none of the above studies bandit models of the behavior of patients with mental disorders and impaired reward processing.\nTo the best of our knowledge, this work is the first one to propose a generalized version of Thompson Sampling algorithmwhich incorporates a range of reward processing biases associated\nwith various mental disorders and shows how different parameter settings of the proposed model lead to behavior mimicking a wide range of impairments in multiple neurological and psychiatric disorders. Most importantly, our bandit algorithm based on generalization of Thompson sampling outperforms the baseline method on multiple datsasets."
    }, {
      "heading" : "3 Background and Definitions",
      "text" : "The Stochastic Multi-Armed Bandit. Given a slot machine with N arms representing potential actions, the player must chose one of the arms to play at each time step t = 1, 2, 3, ..., T . Choosing an arm i yields a random real-valued reward according to some fixed (unknown) distribution with support in [0, 1]. The reward is observed immediately after playing the arm. The MAB algorithm must decide which arm to play at each time step t, based on the outcomes during the previous t− 1 steps. Let µi denote the (unknown) expected reward for arm i. The goal is to maximize the expected total reward during T iterations, i.e., E[ ∑ T\nt=1 µi(t)], where i(t) is the arm played in step t, and the expectation is over the random choices of i(t) made by the algorithm. We could also use the equivalent performance measure known as the expected total regret, i.e. the amount of total reward lost because of playing according to a specific algorithm rather than choosing the optimal arm in each step.\nThe expected total regret is formally defined as:\nE[R(T )] = E[ T∑\nt=1\n(µ∗ − µi(t))] = ∑\ni\n∆iE[ki(T )]. (1)\nwhere µ∗ := maxiµi, ∆i := µ ∗ − µi, and ki(t) denote the number of times arm i has been\nplayed up to step t. Thompson Sampling. Thompson sampling (TS) [20], also known as Basyesian posterior sampling, is a classical approach to multi-arm bandit problem, where the reward ri(t) for choosing an arm i at time t is assumed to follow a distribution Pr(rt|µ̃) with the parameter µ̃. Given a prior Pr(µ̃) on these parameters, their posterior distribution is given by the Bayes rule, Pr(µ̃|rt) ∝ Pr(rt|µ̃)Pr(µ̃) [17].\nA particular case of the Thompson Sampling approach, presented in Algorithm 1, assumes a Bernoulli bandit problem, with rewards being 0 or 1, and the parameters following the Beta prior. TS initially assumes arm i to have prior Beta(1, 1) on µi (the probability of success). At time t, having observed Si(t) successes (reward = 1) and Fi(t) failures (reward = 0), the algorithm updates the distribution on µi as Beta(Si(t), Fi(t)). The algorithm then generates independent samples θi(t) from these posterior distributions of the µi, and selects the arm with the largest sample value."
    }, {
      "heading" : "4 Proposed Approach: Human-Based Thompson Sampling",
      "text" : "We will now introduce a more general rule for updating the parameters of Beta distribution in steps 10 and 11 of the Algorithm 1; this parameteric rule incorporates weights on the prior and the current number of successes and failures, which will allow to model a wide range of reward processing biases associated with various disorders. More specifically, the proposed Human-Based\nAlgorithm 1: Thompson Sampling\n1: Foreach arm i = 1, ..., K 2: set Si(t) = 1, Fi(t) = 1 3: End for 4: Foreach t = 1, 2, ..., T do 5: Foreach i = 1, 2, ..., K do 6: Sample θi(t) from Beta(Si(t), Fi(t)) 7: End do\n8: Play arm it = argmaxiθi(t), obtain reward r(t) 9: if r(t) = 1, then 10: Si(t) = Si(t) + 1 11: else Fi(t) = Fi(t) + 1 12: End do\nThompson Sampling (HBTS), outlined in Algorithm 2, replaces binary incremental updates in lines 10 and 11 of TS (Algorithm 1) with their corresponding weighted version (lines 10 and 11 in Algorithm 2), using the four weight parameters: τ and φ are the weights of the previously accumulated positive and negative rewards, respectively, while α and β represent the weights on the positive and negative rewards at the current iteration.\nAlgorithm 2: Human-Based Thompson Sampling (HBTS)\n1: Foreach arm i = 1, ..., K 2: set Si(t) = 1, Fi(t) = 1 3: End for 4: Foreach t = 1, 2, ..., T do 5: Foreach i = 1, 2, ..., K do 6: Sample θi(t) from Beta(Si(t), Fi(t)) 7: End do\n8: Play arm it = argmaxiθi(t), obtain reward r(t) 9: if ri(t) = 1, then 10: Si(t) = τSi(t) + αri(t) 11: else Fi(t) = φFi(t) + β(1− ri(t)) 12: End do"
    }, {
      "heading" : "4.1 Reward Processing Models with Different Biases",
      "text" : "In this section we describe how specific constraints on the model parameters in the proposed algorithm can yield different reward processing biases discussed earlier, and introduce several instances of the HBTS model, with parameter settings reflecting particular biases. The parameter settings are summarized in Table 2, where we use list our models associated with specific disorders.\nIt is important to underscore that the above models should be viewed as only a first step towards a unifying approach to reward processing disruptions, which requires further extensions, as\nwell as tuning and validation on human subjects. Our main goal is to demonstrate the promise of our parametric approach at capturing certain decision-making biases, as well as its computational advantages over the standard TS, due to the increased generality and flexibility facilitated by multi-parametric formulation. Note that the standard Thompson sampling (TS) approach correspond to setting the four (hyper)parameters used in our model to 1. Next, we introduce the model which incorporates some mild forgetting of the past rewards or losses, using 0.5 weights, just as an example, and calibrating the other models with respect to this one; we refer to this model as M for “moderate” forgetting, which serves here as a proxy for somewhat “normal” reward processing, without extreme reward-processing biases associated with disorders. We will use the subscriptM to denote the parameters of this model.\nWe will now introduced several models inspired by certain reward-processing biases in a range of mental disorders. It is important to note that, despite using disorder names for these models, we are not claiming that they provide accurate models of the corresponding disorders, but rather disorder-inspired versions of our general parametric family of models.\nParkinson’s disease (PD). Recall that PD patients are typically better at learning to avoid negative outcomes than at learning to achieve positive outcomes [4]; one way to model this is to over-emphasize negative rewards, by placing a high weight on them, as compared to the reward processing in healthy individuals. Specifically, we will assume the parameter β for PD patients to be much higher than normal βM (e.g., we use β = 100 here), while the rest of the parameters will be in the same range for both healthy and PD individuals.\nFrontotemporal Dementia (bvFTD). Patients with bvFTD are prone to overeating which may represent increased reward representation. To model this impairment in bvFTD patients, the parameter of the model could be modified as follow: αM << α (e.g., α = 100 as shown in Table 2), where α is the parameter of the bvFTD model has, and the rest of these parameters are equal to the normal one.\nAlzheimer’s disease (AD). To model apathy in patients with Altzheimer’s, including downplaying rewards and losses, we will assume that the parameters φ and τ are somewhat smaller than normal, φ < φM and τ < τM (e.g, set to 0.1 in Table 2), which models the tendency to forget both positive and negative rewards.\nADHD. Recall that ADHD may be involve impairments in storing stimulus-response associations. In our ADHD model, the parameters φ and τ are smaller than normal, φM > φ and τM > τ , which models forgetting of both positive and negative rewards. Note that while this model appears similar to Altzheimer’s model described above, the forgetting factor will be less\npronounced, i.e. the φ and τ parameters are larger than those of the Altzheimer’s model (e.g., 0.2 instead of 0.1, as shown in Table 2).\nAddiction.As mentioned earlier, addiction is associated with inability to properly forget (positive) stimulus-response associations; we model this by setting the weight on previously accumulated positive reward (“memory” ) higher than normal, τ > τM , e.g. τ = 1, while τM = 0.5. Chronic Pain. We model the reduced responsiveness to rewards in chronic pain by setting α < αM so there is a decrease in the reward representation, and φ > φM so the negative rewards are not forgotten (see table 2).\nOf course, the above models should be treated only as first approximations of the reward processing biases in mental disorders, since the actual changes in reward processing are much more complicated, and the parameteric setting must be learned from actual patient data, which is a nontrivial direction for future work. Herein, we simply consider those models as specific variations of our general method, inspired by certain aspects of the corresponding diseases, and focus primarily on the computational aspects of our algorithm, demonstrating that the proposed parametric extension of TS can learn better than the baseline TS due to added flexibility."
    }, {
      "heading" : "5 Empirical Evaluation",
      "text" : "In order to evaluate the proposed framework empirically and compare its performance with the standard Thompson Sampling, we used the following four classification datasets from the UCI Machine Learning Repository1: Covertype, CNAE-9, Internet Advertisements and Poker Hand. A brief summary of the datasets is listed in Table 2.\nIn order to simulate an infinite data stream, we draw samples randomly without replacement, from each dataset, restarting the process each time we draw the last sample. In each round, the algorithm receives the reward 1 if the instance is classified correctly, and 0 otherwise.We compute the total number of classification errors as a performance metric. Note that we do not use the features (context) here, as we try to simulate the classical multi-arm bandit environment (rather than contextual bandit), and use the class labels only. As the result, we obtain a non-stationary environment, since even if P (reward|context) is fixed, switching from a sample to a sample (i.e., from a context to a context) results into different P (reward) at each time point.\nIn order to test the ability of our models to reflect decision-making biases in various disorders, as well as to evaluate the advantages of our model in comparison with the baseline TS, under different test conditions, we consider the following settings:\n- Positive reward environment: we modify the reward function so that the agent receives only positive rewards (the lines 11 is not executed). This environment allows us to evaluate how our models deal with positive reward.\n1 https://archive.ics.uci.edu/ml/datasets.html\n- Negative reward environment: we modify the reward function so that the agent receives only negative rewards (the lines 10 is not executed). This environment helps to evaluate the negativereward processing by our models.\n- Normal environment: the agent can see both negative and positive rewards.\nThe average error rate results on the UCI datasets, for each type of the environment, and over 10 runs of each algorithm, are shown in Table 3. We compute the error rate by dividing the total accumulated regret by the number of iterations. The best results for each dataset are shown in bold. Note that our parametric approach always outperforms the standatrd TS method: AD (addiction) model is best in positive reward environment, M (moderate) version is best in negative environment, and bvFTD happens to outperform other models in regular (positive and negative) reward environment. While further modeling and validation on human subjects may be required to validate neuroscientific value of the proposed models, they clearly demonstrate computational advantages over the classical TS approach for the bandit problem.\nWe now present the detailed results for all algorithms and for each of the three environments, in Tables 4, 5, and 6. Lowest errors for each dataset (across each row) are again shown in bold. Note that, in all three environments, and for each of the four datasets, the baseline Thompson Sampling was always inferior to the proposed parametric family of methods, for each specific settings, different versions of our HBTS framework were performing best.\nPositive Reward Environment. Table 4 summarizes the results for positive reward setting. Note that most versions of the proposed approach frequently outperform the standard Thompson sampling. ADHD model yields best results on two datasets out of four, while AD (addiction) and M (moderate) models are best at one of each remaining datasets, respectively.\nNote that PD (Parkinson’s) and bvFTD (behavioral-variant fronto-teporal dementia) yield the worst results on most datasets. The behavior of PDmodel is therefore consistent with the literature on Parkinson’s disease, which suggests, as mentioned earlier, that Parkinson’s patients do not learn as well from positive rewards as they do from negative ones.\nRanking the algorithms with respect to their mean error rate, we note that the top three performing algorithms were AD (addiction), ADHD and AZ (Alzheimer’s), in that order. One can hypothesise that these observations are consistent with the fact that those disorders did not demonstrate such clear impairment in learning from positive rewards as, for example, Parkinson’s.\nNegative Reward Environment. As shown in Table 5, for negative reward environment, we again observe that the proposed algorithms alwyas work better than the state of the art Thompson sampling.\nOverall, M (moderate) model performs best in this environment, on three out of four datasets. Note that PD (Parkinson’s) and CP (chronic pain) models outperform many other models, performing much better with negative rewards than they did woith the positive ones, which is consistent with the literature discussed before. AD (addiction) is the worst-performing out of HBTS algorithms, which may relate to its bias towards positive-reward driving learning, but impaired ability to learn from negative rewards.\nRanking the algorithms with respect to their mean error rate, we note that the two best-\nperforming algorithms were ADHD and AZ (Alzheimer’s), in that order.\nNormal Reward Environment. Similarly to the other two environments, the baseline Thompson Sampling is always inferior to the proposed algorithms, as shown in Table 6). Interestingly, model M was never a winner, either, and different disorder models performed best for different data sets. PD and CP showworst performance, suggesting that negative-reward driven learning is impairing."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This paper proposes a novel parametric family of algorithms for multi-arm bandit problem, extending the classical Thompson Sampling approach to model a wide range of potential reward processing biases. Our approach draws an inspiration from extensive literature on decisionmaking behavior in neurological and psychiatric disorders stemming from disturbances of the reward processing system. The proposed model is shown to consistently outperform the baseline Thompson Sampling method, on all data and experiment settings we explored, demonstrating better adaptation to each domain due to high flexibility of our multi-parameter model which allows to tune the weights on incoming positive and negative rewards, as well as the weights on memories about the prior reward history. Our empirical results support multiple prior observations about reward processing biases in a range of mental disorders, thus indicating the potential of the proposed model and its future extensions to capture reward-processing aspects across various neurological and psychiatric conditions. Our future work directions include extending our model to the more realistic contextual bandit setting, as well as testing the model on human decision making data."
    } ],
    "references" : [ {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics 6(1)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning 47(2-3)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Reward processing in neurodegenerative disease",
      "author" : [ "D.C. Perry", "J.H. Kramer" ],
      "venue" : "Neurocase 21(1)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "By carrot or by stick: cognitive reinforcement learning in parkinsonism",
      "author" : [ "M.J. Frank", "L.C. Seeberger", "R.C. O’reilly" ],
      "venue" : "Science 306(5703)",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling",
      "author" : [ "A.D. Redish", "S. Jensen", "A. Johnson", "Z. Kurth-Nelson" ],
      "venue" : "Psychological review 114(3)",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Does reward frequency or magnitude drive reinforcement-learning in attention-deficit/hyperactivity disorder? Psychiatry research",
      "author" : [ "M. Luman", "C.S. Van Meel", "J. Oosterlaan", "J.A. Sergeant", "H.M. Geurts" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Mesolimbic dopamine signaling in acute and chronic pain: implications for motivation, analgesia, and addiction",
      "author" : [ "A.M. Taylor", "S. Becker", "P. Schweinhardt", "C. Cahill" ],
      "venue" : "Pain 157(6)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Frontotemporal dementia: what can the behavioral variant teach us about human brain organization? The Neuroscientist",
      "author" : [ "W.W. Seeley", "J. Zhou", "E.J. Kim" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Computational psychiatry of adhd: neural gain impairments across marrian levels of analysis",
      "author" : [ "T.U. Hauser", "V.G. Fiore", "M. Moutoussis", "R.J. Dolan" ],
      "venue" : "Trends in neurosciences 39(2)",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A neurocomputational model for cocaine addiction",
      "author" : [ "A. Dezfouli", "P. Piray", "M.M. Keramati", "H. Ekhtiari", "C. Lucas", "A. Mokri" ],
      "venue" : "Neural computation 21(10)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Beyond pain: modeling decision-making deficits in chronic pain",
      "author" : [ "L.E. Hess", "A. Haimovici", "M.A. Muñoz", "P. Montoya" ],
      "venue" : "Frontiers in behavioral neuroscience 8",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On-line learning with malicious noise and the closure algorithm",
      "author" : [ "P. Auer", "N. Cesa-Bianchi" ],
      "venue" : "Ann. Math. Artif. Intell. 23(1-2)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM J. Comput. 32(1)",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Multi-armed bandit problem with known trend",
      "author" : [ "D. Bouneffouf", "R. Féraud" ],
      "venue" : "Neurocomputing 205",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An empirical evaluation of thompson sampling",
      "author" : [ "O. Chapelle", "L. Li" ],
      "venue" : "Advances in neural information processing systems.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W. Thompson" ],
      "venue" : "Biometrika 25",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1933
    }, {
      "title" : "Analysis of thompson sampling for the multi-armed bandit problem",
      "author" : [ "S. Agrawal", "N. Goyal" ],
      "venue" : "COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27, 2012, Edinburgh, Scotland.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Freshness-aware thompson sampling",
      "author" : [ "D. Bouneffouf" ],
      "venue" : "Neural Information Processing - 21st International Conference, ICONIP 2014, Kuching, Malaysia, November 3-6, 2014. Proceedings, Part III.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning and decisions in contextual multi-armed bandit tasks",
      "author" : [ "E. Schulz", "E. Konstantinidis", "M. Speekenbrink" ],
      "venue" : "In Proceedings of the 37th annual conference of the cognitive science society.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : "Biometrika 25(3/4)",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1933
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The exploration-exploitation trade-off is typically modeled as the multi-armed bandit (MAB) problem, stated as follows: given N possible actions (“arms”), each associated with a fixed, unknown and independent reward probability distribution [1,2], an agent selects an action at each time point and receives a reward, drawn from the corresponding distribution, independently of the previous actions.",
      "startOffset" : 241,
      "endOffset" : 246
    }, {
      "referenceID" : 1,
      "context" : "The exploration-exploitation trade-off is typically modeled as the multi-armed bandit (MAB) problem, stated as follows: given N possible actions (“arms”), each associated with a fixed, unknown and independent reward probability distribution [1,2], an agent selects an action at each time point and receives a reward, drawn from the corresponding distribution, independently of the previous actions.",
      "startOffset" : 241,
      "endOffset" : 246
    }, {
      "referenceID" : 2,
      "context" : "In order to better understand and model human decision-making behavior, scientists usually investigate reward processing mechanisms in healthy subjects [3].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "For example, it was shown that (unmedicated) patients with Parkinson’s disease appear to learn better from negative rather than from positive rewards [4]; another example is addictive behaviors which may be associated with an inability to forget strong stimulus-response associations from the past, i.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : "to properly discount past rewards [5], and so on.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "For example, [4] demonstrate that off-medication PD patients are better at learning to avoid choices that lead to negative outcomes than they are at learning from positive outcomes, while dopamine medication typically used to treat PD symptoms reverses this bias.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "As discussed in [3], AD patients have decreased pursuit of rewarding behaviors, including loss of appetite; these changes are often secondary to apathy, associated with diminished reward system activity.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Frontotemporal dementia (bvFTD) typically involves a progressive change in personality and behavior including disinhibition, apathy, eating changes, repetitive or compulsive behaviors, and loss of empathy [3], and it is hypothesized that those changes are associated with abnormalities in reward processing.",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 5,
      "context" : "Authors in [6] suggest that the strength of the association between a stimulus and the corresponding response is more susceptible to degradation in ADHD patients, which suggests problems with storing the stimulus-response associations.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "In [5], it is demonstrated that patients suffering from addictive behavior are not able to forget the stimulus-response associations, which causes them to constantly seek the stimulus which generated such association.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "In [7], it is suggested that chronic pain results in a hypodopaminergic (low dopamine) state that impairs motivated behavior, resulting into a reduced drive in chronic pain patients to pursue the rewards.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 8,
      "context" : "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "A variety of computational models was proposed for studying the disorders of reward processing in specific disorders, including, among others [4,8,9,10,5,11].",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].",
      "startOffset" : 68,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].",
      "startOffset" : 68,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].",
      "startOffset" : 111,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].",
      "startOffset" : 111,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "Optimal solutions have been provided using a stochastic formulation [1,2], or using an adversarial formulation [12,13,14].",
      "startOffset" : 111,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "Recently, there has been a surge of interest in a Bayesian formulation [15], involving the algorithm known as Thompson sampling [16].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "Recently, there has been a surge of interest in a Bayesian formulation [15], involving the algorithm known as Thompson sampling [16].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 16,
      "context" : "Theoretical analysis in [17] shows that Thompson sampling for Bernoulli bandits asymptotically achieves the optimal performance limit.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "Empirical analysis of Thompson sampling, including problems more complex than the Bernoulli bandit, demonstrates that its performance is highly competitive with other approaches [15,18].",
      "startOffset" : 178,
      "endOffset" : 185
    }, {
      "referenceID" : 17,
      "context" : "Empirical analysis of Thompson sampling, including problems more complex than the Bernoulli bandit, demonstrates that its performance is highly competitive with other approaches [15,18].",
      "startOffset" : 178,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "Psychological study done in [19] shows that, instead of maximizing output by a deliberate mean-variance trade-off, participants approach dynamic decision-making problems by utilizing a probabilitymatching heuristic.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "Thus, their behavior is better described by the Thompson sampling choice rule than by the Upper Confidence Bound (UCB) approach [2].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Choosing an arm i yields a random real-valued reward according to some fixed (unknown) distribution with support in [0, 1].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "Thompson sampling (TS) [20], also known as Basyesian posterior sampling, is a classical approach to multi-arm bandit problem, where the reward ri(t) for choosing an arm i at time t is assumed to follow a distribution Pr(rt|μ̃) with the parameter μ̃.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "Given a prior Pr(μ̃) on these parameters, their posterior distribution is given by the Bayes rule, Pr(μ̃|rt) ∝ Pr(rt|μ̃)Pr(μ̃) [17].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "Recall that PD patients are typically better at learning to avoid negative outcomes than at learning to achieve positive outcomes [4]; one way to model this is to over-emphasize negative rewards, by placing a high weight on them, as compared to the reward processing in healthy individuals.",
      "startOffset" : 130,
      "endOffset" : 133
    } ],
    "year" : 2017,
    "abstractText" : "Drawing an inspiration from behavioral studies of human decision making, we propose here a general parametric framework for multi-armed bandit problem, which extends the standard Thompson Sampling approach to incorporate reward processing biases associated with several neurological and psychiatric conditions, including Parkinson’s and Alzheimer’s diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. We demonstrate empirically that the proposed parametric approach can often outperform the baseline Thompson Sampling on a variety of datasets. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions.",
    "creator" : "LaTeX with hyperref package"
  }
}