{
  "name" : "1704.00217.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification",
    "authors" : [ "Lianhui Qin", "Zhisong Zhang", "Hai Zhao", "Zhiting Hu", "Eric P. Xing" ],
    "emails" : [ "zzs2011}@sjtu.edu.cn,", "zhaohai@cs.sjtu.edu.cn,", "epxing}@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Discourse relations connect linguistic units such as clauses and sentences to form coherent semantics. Identification of discourse relations can benefit a variety of downstream applications including question answering (Liakata et al., 2013), machine translation (Li et al., 2014), text summarization (Gerani et al., 2014), and so forth.\nConnectives (e.g., but, so, etc) are one of the most critical linguistic cues for identifying discourse relations. When explicit connectives are present in the text, a simple frequency-based mapping is sufficient to achieve over 85% classification accuracy (Xue et al., 2016). In contrast, implicit discourse relation recognition has long been seen as a challenging problem, with the best accuracy so far still lower than 50%. In the implicit case, discourse relations are not lexicalized\nby connectives, but to be inferred from relevant sentences (i.e., arguments). For example, the following two adjacent sentences Arg1 and Arg2 imply relation Cause (i.e., Arg2 is the cause of Arg1).\n[Arg1]: Never mind.\n[Arg2]: You already know the answer.\n[Implicit connective]: Because\n[Discourse relation]: Cause\nVarious attempts have been made to directly infer underlying relations by modeling the semantics of the arguments, ranging from feature-based methods (Lin et al., 2009; Pitler et al., 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c). Despite impressive performance, the absence of strong explicit connective cues has made the inference extremely hard and hindered further improvement. In fact, even the human annotators would make use of connectives to aid relation annotation. For instance, the popular Penn Discourse Treebank (PDTB) benchmark data (Prasad et al., 2008) was annotated by first inserting a connective expression (i.e., implicit connective, as shown in the above example) manually, and determining the abstract relation by combining both the implicit connective and contextual semantics.\nTherefore, the huge performance gap between explicit and implicit parsing (namely, 85% vs 50%), as well as the human annotation practice, strongly motivates to incorporate connective information to guide the reasoning process. This paper aims to advance implicit parsing by making use of annotated implicit connectives available in training data. Few recent work has explored such combination. Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation. However, the pipeline approach usually\nar X\niv :1\n70 4.\n00 21\n7v 1\n[ cs\n.C L\n] 1\nA pr\n2 01\n7\nsuffers from error propagation, and the method itself has relied on hand-crafted features which do not necessarily generalize well. Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016). Our work is orthogonal and complementary to this line.\nIn this paper, we propose a novel neural method that incorporates implicit connectives in a principled adversarial framework. We use deep neural models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning behavior by extracting similarly salient features. We therefore setup a secondary network in addition to the implicit relation classifier, building upon connectiveaugmented inputs and serving as a feature learning model for the implicit classifier to emulate.\nMethodologically, however, feature imitation in our problem is challenging due to the semantic gap induced by adding the connective cues. It is necessary to develop an adaptive scheme to flexibly drive learning and transfer discriminability. We devise a novel adversarial approach which enables a self-calibrated imitation mechanism. Specifically, we build a discriminator which distinguishes between the features by the two counterpart networks. The implicit relation network is then trained to correctly classify relations and simultaneously to fool the discriminator, resulting in an adversarial framework. The adversarial mechanism has been an emerging method in different context, especially for image generation (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016; Chen et al., 2016c). Our adversarial framework is unique to address neural feature emulation between two models. Besides, to the best of our knowledge, this is the first adversarial approach in the context of discourse parsing. Compared to previous connective exploiting work (Zhou et al., 2010; Xu et al., 2012), our method provides a new integration paradigm and an end-to-end procedure that avoids inefficient feature engineering and error propagation.\nWe evaluate our method on the PDTB 2.0 benchmark in a variety of experimental settings.\nThe proposed adversarial model greatly improves over standalone neural networks and previous best-performing approaches. We also demonstrate that our implicit recognition network successfully imitates and extracts crucial hidden representations.\nWe begin by briefly reviewing related work in section 2. Section 3 presents the proposed adversarial model. Section 4 shows substantially improved experimental results over previous methods. Section 5 discusses extensions and future work."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Implicit Discourse Relation Recognition",
      "text" : "There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges.\nPrior work has attempted to leverage connective information. Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an implicit network, serving as an adaptive regularization mechanism for enhanced robustness and generalization. Our framework is also end-to-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples."
    }, {
      "heading" : "2.2 Adversarial Networks",
      "text" : "Adversarial method has gained impressive success in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic images through competition between an image generator and a real/fake discriminator. Professor forcing (Lamb et al., 2016) applies a similar idea to improve longterm generation of a recurrent neural language model. Other approaches (Chen et al., 2016b; Hu et al., 2017) extend the framework for controllable image/text generation. Li et al. (2015); Salimans et al. (2016) propose feature matching which trains generators to match the statistics of real/fake examples. Their features are extracted by the discriminator rather than the classifier networks as in our case. Our work differs from the above since we consider the context of discriminative modeling. Adversarial domain adaptation forces a neural network to learn domain-invariant features using a classifier that distinguishes the domain of the network’s input data based on the hidden feature. Our adversarial framework is distinct in that besides the implicit relation network we construct a second neural network serving as a teacher model for feature emulation.\nTo the best of our knowledge, this is the first to employ the idea of adversarial learning in the context of discourse parsing. We propose a novel connective exploiting scheme based on feature imitation, and to this end derive a new adversar-\nial framework, achieving substantial performance gain over existing methods. The proposed approach is generally applicable to other tasks for utilizing any indicative side information. We give more discussions in section 5."
    }, {
      "heading" : "3 Adversarial Method",
      "text" : "Discourse connectives are key indicators for discourse relation. In the annotation procedure of the PDTB implicit relation benchmark, annotators inserted implicit connective expressions between adjacent sentences to lexicalize abstract relations and help with final decisions. Our model aims at making full use of the provided implicit connectives at training time to regulate learning of implicit relation recognizer, encouraging extraction of highly discriminative semantics from raw arguments, and improving generalization at test time. Our method provides a novel adversarial framework that leverages connective information in a flexible adaptive manner, and is efficiently trained end-to-end through standard back-propagation.\nThe basic idea of the proposed approach is simple. We want our implicit relation recognizer, which predicts the underlying relation of sentence arguments without discourse connective, to have prediction behaviors close to a connectiveaugmented relation recognizer which is provided with a discourse connective in addition to the arguments. The connective-augmented recognizer is in analogy to an annotator with the help of connectives as in the human annotation process, and the\nimplicit recognizer would be improved by learning from such an “informed” annotator. Specifically, we want the latent features extracted by the two models to match as closely as possible, which explicitly transfers the discriminability of the connective-augmented representations to implicit ones.\nTo this end, instead of manually selecting a closeness metric, we take advantage of the adversarial framework by constructing a two-player zero-sum game between the implicit recognizer and a rival discriminator. The discriminator attempts to distinguish between the features extracted by the two relation models, while the implicit relation model is trained to maximize the accuracy on implicit data, and at the same time to confuse the discriminator.\nIn the next we first present the overall architecture of the proposed approach (section 3.1), then develop the training procedure (section 3.2). The components are realized as deep (convolutional) neural networks, with detailed modeling choices discussed in section 3.3."
    }, {
      "heading" : "3.1 Model Architecture",
      "text" : "Let (x, y) be a pair of input and output of implicit relation classification, where x = (x1,x2) is a pair of sentence arguments, and y is the underlying discourse relation. Each training example also includes an annotated implicit connective c that best expresses the relation. Figure 1 shows the architecture of our framework.\nThe neural model for implicit relation classification (i-CNN in the figure) extracts latent representation from the arguments, denoted as HI(x1,x2), and feeds the feature into a classifier C for final prediction C(HI(x1,x2)). For ease of notation, we will also use HI(x) to denote the latent feature on data x.\nThe second relation network (a-CNN) takes as inputs the sentence arguments along with an implicit connective, to induce the connectiveaugmented representation HA(x1,x2, c), and obtains relation prediction C(HA(x1,x2, c)). Note that the same final classifierC is used for both networks, so that the feature representations by the two networks are ensured to be within the same semantic space, enabling feature emulation as presented shortly.\nWe further pair the implicit network with a rival discriminator D to form our adversarial game.\nThe discriminator is to differentiate between the reasoning behaviors of the implicit network i-CNN and the augmented network a-CNN. Specifically, D is a binary classifier that takes as inputs a latent feature H derived from either i-CNN or aCNN given appropriate data (where implicit connectives is either missing or present, respectively). The output D(H) estimates the probability that H comes from the connective-augmented a-CNN rather than i-CNN."
    }, {
      "heading" : "3.2 Training Procedure",
      "text" : "The system is trained through an alternating optimization procedure that updates the components in an interleaved manner. In this section, we first present the training objective for each component, and then give the overall training algorithm.\nLet θD denote the parameters of the discriminator. The training objective ofD is straightforward, i.e., to maximize the probability of correctly distinguishing the input features:\nmax θD LD = E(x,c,y)∼data\n[ logD(HA(x, c);θD)+\nlog(1−D(HI(x);θD)) ] , (1)\nwhere E(x,c,y)∼data[·] denotes the expectation in terms of the data distribution.\nWe denote the parameters of the implicit network i-CNN and the classifier C as θI and θC , respectively. The model is then trained to (a) correctly classify relations in training data and (b) produce salient features close to connectiveaugmented ones. The first objective can be fulfilled by minimizing the usual cross-entropy loss:\nLI,C(θI ,θC) = E(x,y)∼data [ J ( C(HI(x;θI);θC), y )] ,\n(2) where J(p, y) = − ∑\nk I(y = k) log pk is the cross-entropy loss between predictive distribution p and ground-truth label y. We achieve objective (b) by minimizing the discriminator’s chance of correctly telling apart the features:\nLI(θI) = Ex∼data [ log ( 1−D(HI(x;θI)) )] . (3)\nThe parameters of the augmented network aCNN, denoted as θA, can be learned by simply fitting to the data, i.e., minimizing the cross-entropy loss as follows:\nLA(θA) = E(x,c,y)∼data [ J ( C(HA(x, c;θA)), y )] . (4)\nAlgorithm 1 Adversarial Model for Implicit Recognition Input: Training data {(x, c, y)n}\nParameters: λ1, λ2 – balancing parameters 1: Initialize {θI ,θC} and {θA} by minimizing\nEq.(2) and Eq.(4), respectively 2: repeat 3: Train the discriminator through Eq.(1) 4: Train the relation models through Eq.(5) 5: until convergence\nOutput: Adversarially enhanced implicit relation network i-CNN with classifier C for prediction\nAs mentioned above, here we use the same classifierC as for the implicit network, forcing a unified feature space of both networks. We combine the above objectives Eqs.(2)-(4) of the relation classifiers and minimize the joint loss:\nmin θI ,θA,θC\nLI,A,C = LI,C(θI ,θC) + λ1LI(θI) + λ2LA(θA),\n(5)\nwhere λ1 and λ2 are two balancing parameters calibrating the weights of the classification losses and the feature-regulating loss. In practice, we pretrain the implicit and augmented networks independently by minimizing Eq.(2) and Eq.(4), respectively. In the adversarial training process, we found setting λ2 = 0 gives stable convergence. That is, the connective-augmented features are fixed after the pre-training stage.\nAlgorithm 1 summarizes the training procedure, where we interleave the optimization of Eq.(1) and Eq.(5) at each iteration. More practical details are provided in section 4. We instantiate all modules as neural networks (section 3.3) which are differentiable, and perform the optimization efficiently through standard stochastic gradient descent and back-propagation.\nThrough Eq.(1) and Eq.(3), the discriminator and the implicit relation network follow a minimax competition, which drives both to improve until the implicit feature representations are close to the connective-augmented latent representations, encouraging the implicit network to extract highly discriminative features from raw sentence arguments for relation classification. Alternatively, we can see Eq.(3) as an adaptive regularization on the implicit model, which, compared to pre-fixed regularizors such as `2-regularization, provides a more flexible, self-calibrated mechanism to improve generalization ability."
    }, {
      "heading" : "3.3 Component Structures",
      "text" : "We have presented our adversarial framework for implicit relation classification. We now discuss the model realization of each component. All components of the framework are parameterized with neural networks. Distinct roles of the modules in the framework lead to different modeling choices.\nRelation Classification Networks Figure 2 illustrates the structure of the implicit relation network i-CNN. We use a convolutional network as it is a common architectural choice for discourse parsing. The network takes as inputs the word vectors of the tokens in each sentence argument, and maps each argument to intermediate features through a shared convolutional layer. The result-\ning representations are then concatenated and fed into a max pooling layer to select most salient features as the final representation. The final classifier C is a simple fully-connected layer followed by a softmax classifier.\nThe connective-augmented network a-CNN has a similar structure as i-CNN, wherein implicit connective is appended to the second sentence as input. The key difference from i-CNN is that here we adopt average k-max pooling, which takes the average of the top-k maximum values in each pooling window. The reason is to prevent the network from solely selecting the connective induced features (which are typically the most salient features) which would be the case when using max pooling, but instead force it to also attend to contextual features derived from the arguments. This facilitates more homogeneous output features of the two networks, and thus facilitates feature imitation. In all the experiments we fixed k = 2.\nDiscriminator The discriminator is a binary classifier to identify the correct source of an input feature vector. To make it a strong rival to the feature imitating network (i-CNN), we model the discriminator as a multi-layer perceptron (MLP) enhanced with gated mechanism for efficient information flow (Srivastava et al., 2015; Qin et al., 2016c), as shown in Figure 3."
    }, {
      "heading" : "4 Experiments",
      "text" : "We demonstrate the effectiveness of our approach both quantitatively and qualitatively with extensive experiments. We evaluate prediction performance on the PDTB benchmark in different settings. Our method substantially improves over a diverse set of previous models, especially in the practical multi-class classification task. We perform in-depth analysis of the model behaviors, and show our adversarial framework successfully enables the implicit relation model to imitate and learn discriminative features."
    }, {
      "heading" : "4.1 Experiment Setup",
      "text" : "We use PDTB 2.01, one of the largest manually annotated discourse relation corpus. The dataset contains 16,224 implicit relation instances in total, with three levels of senses: Level-1 Class, Level-2 Type, and Level-3 Subtypes. The 1st level consists of four major relation Classes: COMPARI-\n1http://www.seas.upenn.edu/∼pdtb/\nSON, CONTINGENCY, EXPANSION and TEMPORAL. The 2nd level contains 16 Types.\nTo make extensive comparison with prior work of implicit discourse relation classification, we evaluate on two popular experimental settings: 1) multi-class classification for 2nd-level types (Lin et al., 2009; Ji and Eisenstein, 2015), and 2) oneversus-others binary classifications for 1st-level classes (Pitler et al., 2009). We describe the detailed configurations in the following respective sections. We will focus our analysis on the multiclass classification setting, which is most realistic in practice and serves as a building block for a complete discourse parser such as that for the shared tasks of CoNLL-2015 and 2016 (Xue et al., 2015, 2016).\nModel Training We provide detailed model and training configurations in the supplementary materials, and only mention a few of them here. Throughout the experiments i-CNN and a-CNN contains 3 sets of convolutional filters with the filter sizes selected on the dev set. The final singlelayer classifier C contains 512 neurons. The discriminator D consists of 4 fully-connected layers, with 2 gated pathways from layer 1 to layer 3 and layer 4 (Figure 3).\nFor adversarial model training, it is critical to keep balance between the progress of the two players. We use a simple strategy which at each iteration optimizes the discriminator and the implicit relation network on a randomly-sampled minibatch. We found this is enough to stabilize the training. The neural parameters are trained using AdaGrad (Duchi et al., 2011) with an initial learning rate of 0.001. For the balancing parameters in Eq.(5), we set λ1 = 0.1, while λ2 = 0. That is, after the initialization stage the weights of the connective-augmented network a-CNN are fixed. This has been shown capable of giving stable and good predictive performance for our system."
    }, {
      "heading" : "4.2 Implicit Relation Classification",
      "text" : "We will mainly focus on the general multi-class classification problem in two alternative settings adopted in prior work, showing the superiority of our model over previous state of the arts. We perform in-depth comparison with carefully designed baselines, providing empirical insights into the working mechanism of the proposed framework. For broader comparisons we also report the performance in the one-versus-all setting.\nMulti-class Classifications We first adopt the standard PDTB splitting convention following (Lin et al., 2009), denoted as PDTB-Lin, where sections 2-21, 22, and 23 are used as training, dev, and test sets, respectively. The most frequent 11 types of relations are selected in the task. During training, instances with more than one annotated relation types are considered as multiple instances, each of which has one of the annotations. At test time, a prediction that matches one of the gold types is considered as correct. The test set contains 766 examples. Please refer to (Lin et al., 2009) for more details. An alternative, slightly different multi-class setting is used in (Ji and Eisenstein, 2015), denoted as PDTB-Ji, where sections 2-20, 0-1, and 21-22 are used as training, dev, and test sets, respectively. The resulting test set contains 1039 examples. We also evaluate in this setting for thorough comparisons.\nTable 1 shows the classification accuracy in both of the settings. We see that our model (Row 10) achieves state-of-the-art performance, greatly outperforming previous methods (Rows 6- 9) with various modeling paradigms, including the linguistic feature-based model (Lin et al., 2009), pure neural methods (Qin et al., 2016c), and combined approach (Ji and Eisenstein, 2015).\nTo obtain better insights into the working mechanism of our method, we further compare with a set of carefully selected baselines as shown in Rows 1-5. 1) “Word-vector” sums over the word vectors for sentence representation, showing the base effect of word embeddings. 2) “CNN” is a standalone convolutional net having the exact same architecture with our implicit relation network. Our model trained within the pro-\nposed framework provides significant improvement, showing the benefits of utilizing implicit connectives at training time. 3) “Ensemble” has the same neural architecture with the proposed framework except that the input of a-CNN is not augmented with implicit connectives. This essentially is an ensemble of two implicit recognition networks. We see that the method performs even inferior to the single CNN model. This further confirms the necessity of exploiting connective information. 4) “Multi-task” is the convolutional net augmented with an additional task of simultaneously predicting the implicit connectives based on the network features. As a straightforward way of incorporating connectives, we see that the method slightly improves over the stand-alone CNN, while falling behind our approach with a large margin. This indicates that our proposed feature imitation is a more effective scheme for making use of implicit connectives. 5) At last, “`2-reg” also implements feature mimicking by imposing an `2 distance penalty between the implicit relation features and connective-augmented features. We see that the simple model has obtained improvement over previous best-performing systems in both settings, further validating the idea of imitation. However, in contrast to the fixed `2 regularization, our adversarial framework provides an adaptive mechanism, which is more flexible and performs better as shown in the table.\nOne-versus-all Classifications We also report the results of four one-versus-all binary classifications for more comparisons with prior work. We follow the conventional experimental setting (Pitler et al., 2009) by selecting sections 2-20, 21-22, and 0-1 as training, dev, and test sets. More detailed data statistics are provided in\nthe supplementary materials. Following previous work, the F1 scores are reported in Table 2. Our method outperforms most of the prior systems in all the tasks. We achieve state-of-the-art performance in recognition of the\nExpansion relation, and obtain comparable scores with the best-performing methods in each of the other relations, respectively. Notably, our feature imitation scheme greatly improves over (Zhou et al., 2010) which leverages implicit connectives as an intermediate prediction task. This provides additional evidence for the effectiveness of our approach."
    }, {
      "heading" : "4.3 Qualitative Analysis",
      "text" : "We now take a closer look into the modeling behavior of our framework, by investigating the process of the adversarial game during training, as well as the feature imitation effects.\nFigure 4 demonstrates the training progress of different components. The a-CNN network keeps high predictive accuracy as implicit connectives are given, showing the importance of connective\ncues. The rise-and-fall patterns in the accuracy of the discriminator clearly show its competition with the implicit relation network i-CNN as training goes. At first few iterations the accuracy of the discriminator increases quickly to over 0.9, while at late stage the accuracy drops to around 0.6, showing that the discriminator is getting confused by i-CNN (an accuracy of 0.5 indicates full confusion). The i-CNN network keeps improving in terms of implicit relation classification accuracy, as it is gradually fitting to the data and simultaneously learning increasingly discriminative features by mimicking a-CNN. The system exhibits similar learning patterns in the two different settings, showing the stability of the training strategy.\nWe finally visualize the output feature vectors of i-CNN and a-CNN using the t-SNE method (Maaten and Hinton, 2008) in Figure 5. Without feature imitation, the extracted features by the two networks are clearly separated (Figure 5(a)). In contrast, as shown in Figures 5(b)(c), the feature vectors are increasingly mixed as training proceeds. Thus our framework has successfully driven i-CNN to induce similar representations with a-CNN, even though connectives are not present."
    }, {
      "heading" : "5 Discussions",
      "text" : "We have developed an adversarial neural framework that facilitates an implicit relation network to extract highly discriminative features by mimicking a connective-augmented network. Our method achieved state-of-the-art performance for implicit discourse relation classification. Besides implicit connective examples, our model can naturally exploit enormous explicit connective data to further improve discourse parsing.\nThe proposed adversarial feature imitation scheme is also generally applicable to other context to incorporate indicative side information available at training time for enhanced inference. Our framework shares a similar spirit of the iterative knowledge distillation method (Hu et al., 2016a,b) which train a “student” network to mimic the classification behavior of a knowledgeinformed “teacher” network. Our approach encourages imitation on the feature level instead of the final prediction level. This allows our approach to apply to regression tasks, and more interestingly, the context in which the student and teacher networks have different prediction out-\nputs, e.g., performing different tasks, while transferring knowledge between each other can be beneficial. Besides, our adversarial mechanism provides an adaptive metric to measure and drive the imitation procedure."
    }, {
      "heading" : "Appendix A Model architectures and training configurations",
      "text" : "In this section we provide the detailed architecture configurations of each component we used in the experiments.\n• Table 3 lists the filter configurations of the convolutional layer in i-CNN and a-CNN in different tasks, tuned on dev sets. As described in section 3.3 in the paper, following the convolutional layer is a max pooling layer in i-CNN, and an average k-max pooling layer with k = 2 in a-CNN.\n• The final single-layer classifier C contains 512 neurons and uses “tanh” as activation function.\n• The discriminator D consists of 4 fully-connected layers, with 2 gated pathways from layer 1 to layer 3 and layer 4 (see Figure 3 in the paper). The size of each layer is set to 1024 and is fixed in all the experiments.\n• We set the dimension of the input word vectors to 300 and initialize with pre-trained word2vec (Mikolov et al., 2013). The maximum length of sentence argument is set to 80, and truncation or zero-padding is applied when necessary.\nAll experiments were performed on a Linux machine with eight 4.0GHz CPU cores and 32GB RAM. We implemented neural networks based on Tensorflow2, a popular deep learning platform."
    }, {
      "heading" : "Appendix B One-vs-all Classifications",
      "text" : "Table 4 lists the statistics of the data.\n2https://www.tensorflow.org"
    } ],
    "references" : [ {
      "title" : "Aggregated word pair features for implicit discourse relation disambiguation",
      "author" : [ "Or Biran", "Kathleen McKeown." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Sofia, Bulgaria,",
      "citeRegEx" : "Biran and McKeown.,? 2013",
      "shortCiteRegEx" : "Biran and McKeown.",
      "year" : 2013
    }, {
      "title" : "Comparing word representations for implicit discourse relation classification",
      "author" : [ "Chloé Braud", "Pascal Denis." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 2201–2211.",
      "citeRegEx" : "Braud and Denis.,? 2015",
      "shortCiteRegEx" : "Braud and Denis.",
      "year" : 2015
    }, {
      "title" : "Learning connective-based word representations for implicit discourse relation identification",
      "author" : [ "Chloé Braud", "Pascal Denis." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 203–",
      "citeRegEx" : "Braud and Denis.,? 2016",
      "shortCiteRegEx" : "Braud and Denis.",
      "year" : 2016
    }, {
      "title" : "Implicit discourse relation detection via a deep architecture with gated relevance network",
      "author" : [ "Jifan Chen", "Qi Zhang", "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Chen et al\\.,? 2016a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel." ],
      "venue" : "Advances in Neural Information Processing Sys-",
      "citeRegEx" : "Chen et al\\.,? 2016b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial deep averaging networks for cross-lingual sentiment classification",
      "author" : [ "Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Weinberger", "Claire Cardie." ],
      "venue" : "arXiv preprint arXiv:1606.01614 .",
      "citeRegEx" : "Chen et al\\.,? 2016c",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research 12(Jul):2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Domain-adversarial training of neural networks",
      "author" : [ "Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand", "Victor Lempitsky." ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Ganin et al\\.,? 2016",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2016
    }, {
      "title" : "Abstractive summarization of product reviews using discourse",
      "author" : [ "Shima Gerani", "Yashar Mehdad", "Giuseppe Carenini", "T. Raymond Ng", "Bita Nejat" ],
      "venue" : null,
      "citeRegEx" : "Gerani et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gerani et al\\.",
      "year" : 2014
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2672–2680.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Harnessing deep neural networks with logic rules",
      "author" : [ "Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric P Xing." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). Berlin, Germany,",
      "citeRegEx" : "Hu et al\\.,? 2016a",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "Controllable text generation",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing." ],
      "venue" : "arXiv preprint arXiv:1703.00955 .",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep neural networks with massive learned knowledge",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P Xing." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). Austin, USA.",
      "citeRegEx" : "Hu et al\\.,? 2016b",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "One vector is not enough: Entity-augmented distributed semantics for discourse relations",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL) 3:329– 344.",
      "citeRegEx" : "Ji and Eisenstein.,? 2015",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2015
    }, {
      "title" : "A latent variable recurrent neural network for discourse-driven language models",
      "author" : [ "Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Ji et al\\.,? 2016",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2016
    }, {
      "title" : "Closing the gap: Domain adaptation from explicit to implicit discourse relations",
      "author" : [ "Yangfeng Ji", "Gongbo Zhang", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal,",
      "citeRegEx" : "Ji et al\\.,? 2015",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2015
    }, {
      "title" : "Professor forcing: A new algorithm for training recurrent networks",
      "author" : [ "Alex M Lamb", "Anirudh Goyal", "Ying Zhang", "Saizheng Zhang", "Aaron C Courville", "Yoshua Bengio." ],
      "venue" : "Advances In Neural Information Processing Systems. pages 4601–4609.",
      "citeRegEx" : "Lamb et al\\.,? 2016",
      "shortCiteRegEx" : "Lamb et al\\.",
      "year" : 2016
    }, {
      "title" : "Leveraging synthetic discourse data via multi-task learning for implicit discourse relation recognition",
      "author" : [ "Man Lan", "Yu Xu", "Zhengyu Niu." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Lan et al\\.,? 2013",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2013
    }, {
      "title" : "Assessing the discourse factors that influence the quality of machine translation",
      "author" : [ "Junyi Jessy Li", "Marine Carpuat", "Ani Nenkova." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-",
      "citeRegEx" : "Li et al\\.,? 2014",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Implicit discourse relation classifica",
      "author" : [ "Sui" ],
      "venue" : null,
      "citeRegEx" : "2016.,? \\Q2016\\E",
      "shortCiteRegEx" : "2016.",
      "year" : 2016
    }, {
      "title" : "The penn discourse treebank",
      "author" : [ "nie L Webber" ],
      "venue" : null,
      "citeRegEx" : "Webber.,? \\Q2008\\E",
      "shortCiteRegEx" : "Webber.",
      "year" : 2008
    }, {
      "title" : "Shallow discourse parsing using convolutional neural network",
      "author" : [ "Lianhui Qin", "Zhisong Zhang", "Hai Zhao." ],
      "venue" : "Proceedings of the CoNLL-16 shared task. Berlin, Germany, pages 70–77.",
      "citeRegEx" : "Qin et al\\.,? 2016b",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2016
    }, {
      "title" : "A stacking gated neural architecture for implicit discourse relation classification",
      "author" : [ "Lianhui Qin", "Zhisong Zhang", "Hai Zhao." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 2263–",
      "citeRegEx" : "Qin et al\\.,? 2016c",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving the inference of implicit discourse relations via classifying explicit discourse connectives",
      "author" : [ "Attapol Rutherford", "Nianwen Xue." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Rutherford and Xue.,? 2015",
      "shortCiteRegEx" : "Rutherford and Xue.",
      "year" : 2015
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2226–2234.",
      "citeRegEx" : "Salimans et al\\.,? 2016",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Highway networks",
      "author" : [ "Rupesh Kumar Srivastava", "Klaus Greff", "Jürgen Schmidhuber." ],
      "venue" : "arXiv preprint arXiv:1505.00387 .",
      "citeRegEx" : "Srivastava et al\\.,? 2015",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Connective prediction using machine learning for implicit discourse relation",
      "author" : [ "Yu Xu", "Man Lan", "Yue Lu", "Zheng Yu Niu", "Chew Lim Tan" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "The CoNLL-2015 shared task on shallow discourse parsing",
      "author" : [ "Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Rashmi Prasad", "Christopher Bryant", "Attapol Rutherford." ],
      "venue" : "Proceedings of the Nineteenth Conference on Computational Natural Lan-",
      "citeRegEx" : "Xue et al\\.,? 2015",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2015
    }, {
      "title" : "The CoNLL-2016 shared task on shallow discourse parsing",
      "author" : [ "Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Bonnie Webber", "Attapol Rutherford", "Chuan Wang", "Hongmin Wang." ],
      "venue" : "Proceedings of the Twentieth Conference on Computational Natural",
      "citeRegEx" : "Xue et al\\.,? 2016",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2016
    }, {
      "title" : "Variational neural discourse relation recognizer",
      "author" : [ "Biao Zhang", "Deyi Xiong", "jinsong su", "Qun Liu", "Rongrong Ji", "Hong Duan", "Min Zhang" ],
      "venue" : "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting discourse connectives for implicit discourse relation recognition",
      "author" : [ "Zhi-Min Zhou", "Yu Xu", "Zheng-Yu Niu", "Man Lan", "Jian Su", "Chew Lim Tan." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (Coling",
      "citeRegEx" : "Zhou et al\\.,? 2010",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : ", 2013), machine translation (Li et al., 2014), text summarization (Gerani et al.",
      "startOffset" : 29,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : ", 2014), text summarization (Gerani et al., 2014), and so forth.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 28,
      "context" : "When explicit connectives are present in the text, a simple frequency-based mapping is sufficient to achieve over 85% classification accuracy (Xue et al., 2016).",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : ", 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c).",
      "startOffset" : 52,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : ", 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c).",
      "startOffset" : 52,
      "endOffset" : 91
    }, {
      "referenceID" : 30,
      "context" : "Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).",
      "startOffset" : 76,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).",
      "startOffset" : 76,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).",
      "startOffset" : 76,
      "endOffset" : 165
    }, {
      "referenceID" : 2,
      "context" : "Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016).",
      "startOffset" : 76,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : "The adversarial mechanism has been an emerging method in different context, especially for image generation (Goodfellow et al., 2014) and domain adaptation (Ganin et al.",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : ", 2014) and domain adaptation (Ganin et al., 2016; Chen et al., 2016c).",
      "startOffset" : 30,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : ", 2014) and domain adaptation (Ganin et al., 2016; Chen et al., 2016c).",
      "startOffset" : 30,
      "endOffset" : 70
    }, {
      "referenceID" : 30,
      "context" : "Compared to previous connective exploiting work (Zhou et al., 2010; Xu et al., 2012), our method provides a new integration paradigm and an end-to-end procedure that avoids inefficient feature engineering and error propagation.",
      "startOffset" : 48,
      "endOffset" : 84
    }, {
      "referenceID" : 26,
      "context" : "Compared to previous connective exploiting work (Zhou et al., 2010; Xu et al., 2012), our method provides a new integration paradigm and an end-to-end procedure that avoids inefficient feature engineering and error propagation.",
      "startOffset" : 48,
      "endOffset" : 84
    }, {
      "referenceID" : 30,
      "context" : "large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al.",
      "startOffset" : 151,
      "endOffset" : 209
    }, {
      "referenceID" : 13,
      "context" : ", 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016).",
      "startOffset" : 52,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : ", 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016).",
      "startOffset" : 52,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al.",
      "startOffset" : 75,
      "endOffset" : 166
    }, {
      "referenceID" : 23,
      "context" : "Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al.",
      "startOffset" : 75,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al.",
      "startOffset" : 75,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al.",
      "startOffset" : 75,
      "endOffset" : 166
    }, {
      "referenceID" : 17,
      "context" : ", 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016).",
      "startOffset" : 29,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : ", 2016), and word representation (Braud and Denis, 2016).",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "2014) and domain adaptation (Ganin et al., 2016).",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic images through competition between an image generator and a real/fake discriminator.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "Other approaches (Chen et al., 2016b; Hu et al., 2017) extend the framework for controllable image/text generation.",
      "startOffset" : 17,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "Other approaches (Chen et al., 2016b; Hu et al., 2017) extend the framework for controllable image/text generation.",
      "startOffset" : 17,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "Other approaches (Chen et al., 2016b; Hu et al., 2017) extend the framework for controllable image/text generation. Li et al. (2015); Sali-",
      "startOffset" : 18,
      "endOffset" : 133
    }, {
      "referenceID" : 25,
      "context" : "formation flow (Srivastava et al., 2015; Qin et al., 2016c), as shown in Figure 3.",
      "startOffset" : 15,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "formation flow (Srivastava et al., 2015; Qin et al., 2016c), as shown in Figure 3.",
      "startOffset" : 15,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "To make extensive comparison with prior work of implicit discourse relation classification, we evaluate on two popular experimental settings: 1) multi-class classification for 2nd-level types (Lin et al., 2009; Ji and Eisenstein, 2015), and 2) oneversus-others binary classifications for 1st-level classes (Pitler et al.",
      "startOffset" : 192,
      "endOffset" : 235
    }, {
      "referenceID" : 6,
      "context" : "The neural parameters are trained using AdaGrad (Duchi et al., 2011) with an initial learning rate of 0.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "66 8 Ji and Eisenstein (2015) 44.",
      "startOffset" : 5,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "66 8 Ji and Eisenstein (2015) 44.59 9 Qin et al. (2016a) 43.",
      "startOffset" : 5,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "An alternative, slightly different multi-class setting is used in (Ji and Eisenstein, 2015), denoted",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : ", 2009), pure neural methods (Qin et al., 2016c), and combined approach (Ji and Eisenstein, 2015).",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : ", 2016c), and combined approach (Ji and Eisenstein, 2015).",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : ", 2009), where first 8 epochs are for initialization stage (thus the discriminator is fixed and not shown); Bottom: the PDTB-Ji setting (Ji and Eisenstein, 2015), where first 3 epochs are for initialization.",
      "startOffset" : 136,
      "endOffset" : 161
    }, {
      "referenceID" : 17,
      "context" : "76 Qin et al. (2016c) 41.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "(2016c) 41.55 57.32 71.50 35.43 Zhang et al. (2016) 35.",
      "startOffset" : 1,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "(2016c) 41.55 57.32 71.50 35.43 Zhang et al. (2016) 35.88 50.56 71.48 29.54 Zhou et al. (2010) 31.",
      "startOffset" : 1,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "(2016c) 41.55 57.32 71.50 35.43 Zhang et al. (2016) 35.88 50.56 71.48 29.54 Zhou et al. (2010) 31.79 47.16 70.11 20.30 Liu and Li (2016) 36.",
      "startOffset" : 1,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "84 Chen et al. (2016a) 40.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 30,
      "context" : "Notably, our feature imitation scheme greatly improves over (Zhou et al., 2010) which leverages implicit connectives as an intermediate prediction task.",
      "startOffset" : 60,
      "endOffset" : 79
    } ],
    "year" : 2017,
    "abstractText" : "Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.",
    "creator" : "LaTeX with hyperref package"
  }
}