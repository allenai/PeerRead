{
  "name" : "1611.07954.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SENTATIONS OF NEURAL READERS",
    "authors" : [ "Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester" ],
    "emails" : [ "haiwang@ttic.edu", "tonishi@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Reading comprehension is a type of question answering task where the answer is to be found in a passage about particular entities and events not otherwise familiar to the reader. In particular, the entities and events should not be mentioned in structured databases of general knowledge. Reading comprehension problems are intended to measure a systems ability to extract semantic information about entities and relations directly from unstructured text. Several large scale reading comprehension datasets have been introduced recently. In particular the CNN & DailyMail datasets (Hermann et al., 2015), the Children’s Book Test (CBT) (Hill et al., 2016), and the Who-did-What dataset (Onishi et al., 2016). The large sizes of these datasets enable the application of deep learning. These are all cloze-style datasets where a question is constructed by deleting a word or phrase from an article summary (in CNN/DailyMail), from a sentence in a Children’s story (in CBT), or by deleting a person from the first sentence of a different news article on the same entities and events (in Who-did-What).\nA variety of neural models for machine comprehension (neural readers) have been developed recently. Here we divide these readers into two classes — aggregation readers and explicit reference readers. Aggregation readers compute a vector representation of the passage involving a questionsensitive attention. They then select an answer based on the passage vector. Aggregation readers include Memory Networks (Weston et al.; Sukhbaatar et al., 2015), the Attentive Reader (Hermann et al., 2015) and the Stanford Reader (Chen et al., 2016).\nExplicit reference readers, on the other hand, avoid computing a vector representation of the passage. Instead they rely on a kind of coreference annotation —a specification at each position in the passage of whether or not that position references a candidate answer, and if so, which answer is referenced. These readers compute an attention, as in aggregation readers, but rather than compute a passage vector they simply select the most attended-to answer. Explicit reference readers include the Attention Sum Reader (Kadlec et al., 2016), the Gated Attention Reader (Dhingra et al., 2016), the Attention-over-Attention Reader (Cui et al., 2016) and others (a list can be found in section 6).\n∗Authors contributed equally 1code will be available: https://github.com/sohuren\nar X\niv :1\n61 1.\n07 95\n4v 1\n[ cs\n.C L\n] 2\n3 N\nov 2\n01 6\nSomewhat surprisingly, aggregation readers can perform as well as explicit reference readers. Here we analyze how this happens and argue for the emergence of logical structure in aggregation readers.\nIn all of these models a hidden state vector is computed for each position in the passage. We propose that the hidden state vector represents a direct sum of a “statement vector” and an “entity vector”. This logical structure can be written as H = S ⊕ E where H is the space of hidden vectors and S and E are orthogonal subspaces corresponding to “statements” and “entities” respectively. We then have that a hidden vector h has a unique decomposition as h = Φ + e. We interpret this as saying that statement Φ is true of entity e.\nSections 2 and 3 review various existing datasets and models respectively. Section 4 presents the logical structure interpretation of aggregation readers and the empirical evidence supporting it. Section 5 proposes new models that enforce the direct sum structure of the hidden state vectors. It is shown that these new models perform well on the Who-did-What dataset provided that reference annotations are added as input features. Section 5 also describes additional linguistic features that can be added to the input embeddings and show that these improve performance of existing models resulting in the best single-model performance to date on the Who-did-What datasets."
    }, {
      "heading" : "2 A BRIEF SURVEY OF DATASETS",
      "text" : "Before presenting various models for machine comprehension we give a general formulation of the machine comprehension task. We take an instance of the task be a four tuple (q, p, a,A), where q is a question given as sequence of words containing a special taken for a “blank” to be filled in, p is a document consisting of a sequence of words, A is a set of possible answers and a ∈ A is the ground truth answer. All words are drawn from a vocabulary V . We assume that all possible answers are words from the vocabulary, that is A ⊆ V , and that the ground truth answer appears in the document, that is a ∈ p. The problem can be described as that of selecting the answer a ∈ A that answers question q based on information from p.\nWe will now briefly summarize important features of the related datasets in reading comprehension.\nCNN & DailyMail: Hermann et al. (2015) constructed these datasets from a large number of news articles from the CNN and Daily Mail news websites. The main article is used as the context, while the cloze style question is formed from one short highlight sentence appearing in conjunction with the published article. To avoid the model using external world knowledge when answering the question, the named entities in the entire dataset were replaced by anonymous entity IDs which were then further shuffled for each example. This forces models to rely on the context document to answer each question. In this anonymized corpus the entity identifiers are taken to be a part of the vocabulary and the answer set A consists of the entity identifiers occurring in the passage. Who-did-What (WDW): The Who-did-What dataset (Onishi et al., 2016) contains 127,000 multiple choice cloze questions constructed from the LDC English Gigaword newswire corpus (David & Cieri, 2003). In contrast with CNN and Daily Mail, it avoids using article summaries for question formation. Instead, each problem is formed from two independent articles: one is given as the passage to be read and a different article on the same entities and events is used to form the question. Further, Who-did-What avoids anonymization, as each choice is a person named entity. In this dataset the answer set A consists of the person named entities occurring in the passage. Finally, the problems have been filtered to remove a fraction that are easily solved by simple baselines. It has two training sets. The larger training set (“relaxed”) is created using less baseline filtering, while the smaller training set (“strict”) uses the same filtering as the validation and test sets.\nChildren’s Book Test (CBT) Hill et al. (2016) developed the CBT dataset in a slightly different fashion to the CNN/DailyMail datasets. They take any sequence of 21 consecutive sentences from a children’s book: the first 20 sentences are used as the passage, and the goal is to infer a missing word in the 21st sentence. The task complexity varies with the type of the omitted word (verb, preposition, named entity, or common noun). According to the original study on this dataset (Hill et al., 2016), n-gram and recurrent neural network language models are sufficient for predicting verbs or prepositions. However, for named entities and common nouns, current solvers are still far from human performance.\nOther Related Datasets. It is also worth mentioning several related datasets. The MCTest dataset (Richardson et al., 2013) consists of children’s stories and questions written by crowdsourced workers. The dataset only contains 660 documents and is too small to train deep models. The bAbI dataset (Weston et al., 2016) is constructed automatically using synthetic text generation and can be perfectly answered by hand-written algorithms (Lee et al., 2016). The SQuAD dataset (Rajpurkar et al., 2016) consists passage-question pairs where the passage is a wikipedia article and the questions are written by crowdsourced workers. Although crowdsourcing is involved, the dataset contains over 200,000 problems. But the answer is often a word sequence which is dificult to handle with the reader models considered here. The LAMBADA dataset (Denis et al., 2016) is a word prediction dataset which requires a broad discourse context and the correct answer might not in the context. Nonetheless, when the correct answer is in the context, neural readers can be applied effectively (Chu et al., 2016)."
    }, {
      "heading" : "3 AGGREGATION READERS AND EXPLICIT REFERENCE READERS",
      "text" : "Here we classify readers into aggregation readers and explicit reference readers. Aggregation readers appeared first in the literature and include Memory Networks (Weston et al.; Sukhbaatar et al., 2015), the Attentive Reader (Hermann et al., 2015), and the Stanford Reader (Chen et al., 2016). Aggregation readers are defined by equations (4) and (6) below. Explicit reference readers incluce the Attention-Sum Reader (Kadlec et al., 2016), the Gated-Attention Reader (Dhingra et al., 2016), and the Attention-over-Attention Reader (Cui et al., 2016). Explicit reference readers are defined by equation (10) below. We first present the Stanford Reader as a paradigmatic aggregation Reader and the Attention-Sum Reader as a paradigmatic explicit reference reader."
    }, {
      "heading" : "3.1 AGGREGATION READERS",
      "text" : "Stanford Reader. The the Stanford Reader (Chen et al., 2016) computes a bi-directional LSTM representation of both the passage and the question.\nh = biLSTM(e(p)) (1) q = [fLSTM(e(q))|q|, bLSTM(e(q))1] (2)\nIn equations (1) and (2) we have that e(p) is the sequence of word embeddings e(wi) for wi ∈ p and similarly for e(q). The expression biLSTM(s) denotes the sequence of hidden state vectors resulting from running a bi-directional LSTM on the vector sequence s. We write biLSTM(s)i for the ith vector in this sequence. Similarly fLSTM(s) and bLSTM(s) denote the sequence of vectors resulting from running a forward LSTM and a backward LSTM respectively and [·, ·] denotes vector concatenation. The Stanford Reader, and various other readers, then compute a bilinear attention over the passage which is then used to construct a single weighted vector representation of the passage.\nαt = softmax t\nh>t Wα q (3) o = ∑ t αtht (4)\nFinally, they compute a probability distribution over the answers P (a|p, q,A).\np(a|d, q,A) = softmax a∈A eo(a) >o (5)\nâ = argmax a∈A\neo(a) >o (6)\nHere eo(a) is an “output embedding” of the answer a. On the CNN dataset the Stanford Reader trains an output embedding for each the roughly 500 entity identifiers used in the dataset. In cases where the answer might be any word in V an output embedding must be trained for the entire vocabulary. The reader is trained with log-loss ln 1/P (a|p, q,A) where a is the correct answer. At test time the reader is scored on the percentage of problems where â = a.\nMemory Networks. Memory Networks (Weston et al.; Sukhbaatar et al., 2015) use (4) and (6) but have more elaborate methods of constructing “memory vectors” ht not involve LSTMs. Memory\nnetworks use (4) and (6) but replace (5) with\nP (w|p, q,A) = P (w|p, q) = softmax w∈V eo(w) T o. (7)\nIt should be noted that (7) trains output vectors over the whole vocabulary rather than just those items occurring in the choice setA. This is empirically significant in non-anonymized datasets such as CBT and Who-did-What where choices at test time may never have occurred as choices in the training data.\nAttentive Reader. The Stanford Reader was derived from the Attentive Reader (Hermann et al., 2015). The Attentive Reader uses αt = softmaxt MLP([ht, q]) instead of (3). Here MLP(x) is the output of a multi layer perceptron (MLP) given input x. Also, the answer distribution in the attentive reader is defined over the full vocabulary rather than just the candidate answer set A.\nP (w|p, q,A) = P (w|p, q) = softmax w∈V eo(w) TMLP([o, q]) (8)\nEquation (8) is similar to (7) in that it leads to the training of output vectors for the full vocabulary rather than just those items appearing in choice sets in the training data. As in memory networks, this leads to improved performance on non-anonymized data sets."
    }, {
      "heading" : "3.2 EXPLICIT REFERENCE READERS",
      "text" : "Attention-Sum Reader. In the Attention-Sum Reader (Kadlec et al., 2016) h and q are computed with equations (1) and (2) as in the Stanford Reader but using GRUs rather than LSTMs. The attention αt is computed similarly to (3) but using a simple inner product αt = softmaxt h>t q rather than a trained bilinear form. Most significanlty, however, equations (5) and (6) are replaced by the following where t ∈ R(a, p) indicates that a reference to candidate answer a occurs at position t in p.\nP (a|p, q,A) = ∑\nt∈R(a,p)\nαt (9)\nâ = argmax a ∑ t∈R(a,p) αt (10)\nHere we think of R(a, p) as the set of references to a in the passage p. It is important to note that (9) is an equality and that P (a|p, q,A) is not normalized to the members of R(a, p). When training with the log-loss objective this drives the attention αt to be normalized — to have support only on the positions t with t ∈ R(a, p) for some a. See the heat maps in the appendix. Gated-Attention Reader. The Gated Attention Reader Dhingra et al. (2016) involves a K-layer biGRU architecture defined by the following equations.\nq` = [fGRU(e(q))|q|, bGRU(e(q))1] 1 ≤ ` ≤ K h1 = biGRU(e(p))\nh` = biGRU(h`−1 q`−1) 2 ≤ ` ≤ K Here the question embeddings q` for different values of ` are computed with different GRU model parameters. Here h q abbreviates the sequence h1 q, h2 q, . . . h|p| q. Note that for K = 1 we have only q1 and h1 as in the attention-sum reader. An attention is then computed over the final layer hK with αt = softmaxt (hKt ) > qK in the attention-sum reader. This reader uses (9) and (10).\nAttention-over-Attention Reader, The Attention-over-Attention Reader (Cui et al., 2016) uses a more elaborate method to compute the attention αt. We will use t to range over positions in the passage and j to range over positions in the question. The model is then defined by the following equations.\nh = biGRU(e(p)) q = biGRU(e(q))\nαt,j = softmaxt h > t qj βt,j = softmaxj h > t qj\nβj = 1 |p| ∑ t βt,j αt = ∑ j βjαt,j\nNote that the final equation defining αt can be interpreted as applying the attention βj to the attentions αt,j . This reader uses (9) and (10)."
    }, {
      "heading" : "4 EMERGENT LOGICAL STRUCTURE",
      "text" : "Our logical structure interpretation was inspired by the anonymization done in the CNN/DailyMail dataset. To undermine the use of language models the named entities in this data set are replaced by anonymous entity identifiers such as ent381. After anonymization a typical sentence might be “the ent381 producer allegedly struck by ent212 will not press charges against the ent153 host”. Furthermore, at training time these identifiers are typically randomly shuffled so that the same problem can be used at training time under different mappings between the actual named entities and the identifiers that replace them. Clearly the identifiers themselves cannot be assigned any semantics other than their identity. We can think of them as pointers or semantics-free constant symbols. Despite this undermining of semantics, aggregation readers using (4) and (6) are able to perform well. This indicates that the vector o appearing in (5) and (6) contains some kind of “pointer” to the desired entity identifier. More specifically, it seems natural to assume that for t ∈ R(a, p) we have that the hidden vector ht of the Stanford Reader has a strong inner product with eo(a) and a weak inner product with eo(a′) for a′ 6= a. This suggests the following for some fixed positive constant c.\neo(a) >ht = { c if t ∈ R(a, p) 0 otherwise (11)\nThis gives\nargmax a\neo(a) >o = argmax\na eo(a) > ∑ t αtht\n= argmax a ∑ t αt eo(a) >ht = argmax a ∑ t∈R(a,p) αt\nand hence (6) and (10) agree. Empirical evidence for (11) is given in the first two rows of table 1. The first row empirically measures the “constant” c in (11) by measuring e0(a)>ht for those cases where t ∈ R(a, p). The second row measures “0” in (11) by measuring eo(a)>ht in those cases where t 6∈ R(a, p). Additional evidence for (11) is given in figure 1 showing that the output vectors eo(a) for different entity identifiers a are nearly orthogonal. Orthogonality of the output vectors is required by (11) provided that each output vector eo(a) is in the span of the hidden state vectors ht,p for which t ∈ R(a, p). Intuitively, the mean of all vectors ht,p with t ∈ R(a, p) should be approximately equal to eo(a). Of course empirically this will only be approximately true. As further support for (11) we give heat maps for eo(a)ht for different identifiers a and heat maps for αt for different readers in the appendix.\nSince the model is trained under permutations of the entity identifiers, one would also expect the attention αt to be independent of the choice of the identifier permutation. In particular, for two passages with different identifier permutations resulting in hidden state vector sequences h and h′ we should have q>ht = q>h′t. This suggest that ht, in addition to containing a component indicating an entity identifier, also contains a component independent of the entity identifier. In addition to (11), one would expect\nq>(hi + eo(a)) = q >hi. (12)\nThis equation is equivalent to q>eo(a) = 0. Experimentally, however, we cannot expect q>eo(a) to be exactly zero and (12) seems to provides a more experimentally meaningful test. Empirical\nevidence for (12) is given in the third and fourth row of table 1. The third row measures the cosine of the angle between the question vector q and the hidden state ht averaged over passage positions t at which some entity identifier occurs. The fourth row measures the cosine of the angle between q and eo(a) averaged over the entity identifiers a.\nPredictions (11) and (12) suggest a dependent sum structure for the hidden state vector space. Let H be the vector space spanned by the hidden states ht. Our logical interpretation can be written as\nH = S ⊕ E (13) where S is a subspace of “statement vectors” and E is an orthogonal subspace of “entity pointers”. Each hidden state vector h ∈ H then has a unique decomposition as h = Ψ + e for Ψ ∈ S and e ∈ E. Empirical evidence for (13) is given by the performance of models that enforce this direct sum structure as presented in section 5.\nA question asks for a value of x such that a statement Φ[x] is implied by the passage. Hence we should expect the question vector to represent a statement — we expect q ∈ S. For a question Φ we might even suggest the following vectorial interpretation of entailment.\nΨ[x] implies Φ[x] iff Ψ>Φ ≥ ||Φ||1. This interpretation is exactly correct if some of the dimensions of the vector space correspond to predicates, Φ is a 0-1 vector representing a conjunction predicates, and Ψ is also 0-1 on these dimensions indicating whether a predicate is implied by the context. Of course in practice one expects the dimension to be smaller than the number of possible predicates."
    }, {
      "heading" : "5 POINTER ANNOTATION READERS",
      "text" : "It is of course important to note that anonymization provides reference information — anonymization assumes that one can determine coreference so as to replace coreferent phrases with the same entity identifier. Anonymization allows the reference set R(a, p) to be directly read off of the passage. Still, an aggregation reader must learn to recover this explicit reference structure.\nAggregation readers can have difficulty when anonymization is not done. The Stanford Reader achieves just better than 45% on Who-did-What dataset while Attention Sum Reader can get near 60%. But if we anonymize the Who-did-What dataset and then re-train the Stanford Reader, the accuracy jumps to near 65%. Anonymization has two effects. First, it greatly reduces the number of output word eo(a) to be learned — we need only learn output embeddings for the relatively small number of entity identifiers needed. Second, anonymization suppresses the semantics of the reference phrases and leaves only a semantics-free entity identifier. This suppression of semantics may facilitate the separation of the hidden state vector space H into a direct sum S ⊕E with q ∈ S and eo(a) ∈ E.\nWe can think of anonymization as providing additional linguistic input for the reader — it explicitly marks positions of candidate answers and establishes coreference. A natural question is whether this information can be provided without anonymization by simply adding additional coreference features to the input. Here we evaluate two architectures inspired by this question. This evaluation is done on the Who-did-What dataset which is not anonymized. In each architecture we add features to the input to mark the occurrences of candidate answers. These models are simpler than the Stanford reader but perform comparably. This comparable performance in table 2 further supports our analysis of logical structure in aggregation readers.\nOne-Hot Pointer Annotation: The Stanford Reader involves both input embeddings of words and output embeddings of entity identifiers. In the Who-did-What dataset each problem has at most five choices in the multiple choice answer list. This means that we need only five entity identifiers and we can use a five dimensional one-hot vector representation for answer identifiers. If an answer choice exists at position t in the passage let it be the index of that choice on the choice list. If no choice occurs t take it to be zero. Take e′(i) to be the zero vector if i = 0 and otherwise to be the one-hot vector for i. We defined pointer annotation to be the result of adding e′(it) as additional features to the input embedding.\ne(wt) = [e(wt), e ′(it)] (14)\nWe then define a one-hot pointer reader by designates five dimensions of the hidden state as indicators of the answer and take the probability of choice i to be defined as\np(i|d, q) = softmax i oi (15)\nwhere o is computed by (4).\nGeneral Pointer Annotation: In the CNN dataset there are roughly 500 entity identifier and a onehot representation is not desirable. Instead we can let e′(i) be a fixed set of “pointers vectors” — vectors distributed widely on the unit sphere so that for i 6= j we have that e′(i)>e′(j) is small. We again use (14) but replace (15) with\np(i|d, q) = softmax i\n[0, e′(i)]>o (16)\nIn the general pointer reader the pointer embeddings e′(i) are held fixed and not trained.\nLinguistic Features. Each model can be modified to include additional input features for each input token in the question and passage. More specifically we can add the following features to the word embeddings.\n• Binary feature: whether current token occurs in the question.\n• Real value feature: the frequency of current token in the passage. • Real value feature: position of the token’s first occurrence in the passage as a percentage\nof the passage length.\n• Binary feature: whether the text surrounding token match the text surrounding the placeholder in the question. We only have features for matching both left and right one word.\n• One hot vector: Part-of-speech (POS) tagging. We didn’t use such feature on CNN&DailyMail dataset.\n• One hot vector: Name Entity Recognition (NER). We didn’t use such feature on CNN&DailyMail dataset."
    }, {
      "heading" : "6 A SURVEY OF RECENT RESULTS",
      "text" : "The performance of various recent readers on CNN, DailyMail and CBTest are summarized in Table 3. For purposes of comparison we only present results on single models. Model ensembles generally perform better than single models but are require more computation to train making comparisons more difficult. More experimental details can be found in appendix.\nIn table 3, all the high-performance approaches are proposed very recently. Blue color represents the second highest accuracy and bold font indicates the state-of-the-art accuracy. Note that the result of Stanford Reader we report here is the one without relabeling since relabeling procedure doesn’t follow the protocol used in Hermann et al. (2015)."
    }, {
      "heading" : "7 DISCUSSION",
      "text" : "Explicit reference architectures rely on reference resolution — a specification of which phrases in the given passage refer to candidate answers. Our experiments indicate that all existing readers benefit greatly from this externally provided information. Aggregation readers seem to demonstrate a stronger learning ability in that they essentially learn to mimic explicit reference readers by identifying reference annotation and using it appropriately. This is done most clearly in the pointer reader architectures. Furthermore, we have argued for, and given experimental evidence for, an interpretation of aggregation readers as learning emergent logical structure — a factoring of neural\nrepresentations into a direct sum of a statement (predicate) representation and an entity (argument) representation.\nAt a very high level our analysis and experiments support a central role for reference resolution in reading comprehension. Automating reference resolution in neural models, and demonstrating its value on appropriate datasets, would seem to be an important area for future research.\nOf course there is great interest in “learning representations”. The current state of the art in reading comprehension is such that systems still benefit from externally provided linguistic features including externally annotated reference resolution. It would seem desirable to develop fully automated neural readers that perform as well as readers using externally provided annotations. It is of course important to avoid straw man baselines when making any such claim.\nWe are hesitant to make any more detailed comments on the differences between the architectural details of the readers discussed in this paper. The differences in scores between the leading readers are comparable to differences in scores that can be achieved by aggressive search over meta parameters or the statistical fluctuations in the quality of models learned by noisy statistical training procedures. More careful experiments over a longer period of time are needed. More dramatic improvements in performance would of course provide better support for particular innovations."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thanks the support of NVIDIA Corporation with the donation of GPUs used for this work."
    }, {
      "heading" : "8 APPENDIX",
      "text" : ""
    }, {
      "heading" : "8.1 EXPERIMENT DETAILS",
      "text" : "We implemented the neural readers using Theano (de ric Bastien et al., 2012) and Blocks (van Merrienboer et al., 2015) and train them on single Nvidia Tesla K40 GPU. Negative log-likelihood is employed as training criterion. We used stochastic gradient descent (SGD) with the ADAM update rule (Kingma & Ba, 2015) and set the learning rate 0.0005.\nFor Stanford Reader and One-Hot Pointer Reader, we simply follows the Stanford Reader’s setting and didn’t tune it on each dataset. For gated attention reader, the lookup table was randomly initialized with uniform distribution from the interval [-0.2, 0.2] on CBT dataset, but on CNN&DailyMail, the lookup table was initialized by Glove vector (Jeffrey et al., 2014) trained on the train&validatation set (we found that the pre-trained word vector doesn’t improve the accuracy but will accelerate the training) on CNN&DailyMail. On WDW dataset, the lookup table was initialized by pre-trained Glove vector2. It should be noticed that if we initialize the lookup table with pre-trained Glove vector from //nlp.stanford.edu/data/glove.6B.zip, it will slightly boost the accuracy compared with using the Glove vector trained on train&validation set. Input to hidden state weights were initialized by random orthogonal matrices (Saxe et al., 2013) and biases were initialized to zero. Hidden to hidden state weights were initialized by identity matrices to force the model can remember longer information. To compute the attention weight, we αt = ht>Wαq and initialize Wα with random uniform distribution. We also used the gradient clipping (Razvan et al., 2013) with threshold of 10 and batches of size 32.\nDuring training we randomly shuffled all examples within each epoch. To speedup training, we always pre-fetched 10 batches worth of examples and sorted them according to document length as did by Kadlec et al. (2016). When trained on CNN, DailyMail and WDW (anonymization case) dataset, we randomly reshuffled the entity identifier to match the procedure proposed in Hermann et al. (2015).\nDuring training we evaluated the accuracy after each epoch and stopped the training when the accuracy on the validation set started decreasing. We tried limiting the vocabulary to the most frequent tokens but didn’t observed any performance improvement compared with using all the distinct tokens as vocabulary. Since part of our experiments need to check the word embedding assignment issues, finally we use all the distinct tokens as vocabulary. To find the optimal embedding and hidden state dimension, we tried several groups of different combinations, the optimal value and corresponding training statistics in Gated Attention readers are summarized in Table. 4. When anonymize the Whodid-What dataset, we can either use simple string match to replace answer in question and story with entity identifier, or we can use Name Entity Recognition(NER) tools3 to detect name entities and then replace the answer name entities in question and story with entity identifier, we found the later one generally will bring 2 % improvement compared with simple string match. More experimental details can be found in code."
    }, {
      "heading" : "8.2 HEAT MAP OF STANFORD READER FOR DIFFERENT ANSWER CANDIDATES",
      "text" : "We randomly choose one article from CNN dataset and show softmax(eo(a)ht) for t ∈ [0, |p|] for each answer candidate a in figure.2, figure.3, figure.4, figure.5 and figure.6. Red color indicates\n2http://nlp.stanford.edu/data/glove.6B.zip 3http://nlp.stanford.edu/software/CRF-NER.shtml\nlarger probability and orange indicates smaller probability and the remaining indicates very low probability that can be ignored. From those figures, we can see that our assumption that eo(a) is used to pick up its occurrence is reasonable.\n@entity0 ( @entity1 ) six survivors of the @entity0 kosher supermarket siege in january are suing a @entity5 media outlet for what they call dangerous live broadcasting during the hostage - taking . according to @entity0 prosecutor 's spokeswoman @entity10 , the lawsuit was filed march 27 and a preliminary investigation was opened by the prosecutor 's office wednesday . the media outlet , @entity1 affiliate @entity16 , is accused of endangering the lives of the hostages , who were hiding in a cold room during the attack , by broadcasting their location live during the siege . @entity23 in a statement friday said one of its journalists \" mentioned only once the presence of a woman hidden inside the @entity27 , on the basis of police sources on the ground . \" \" immediately , the chief editor felt that this information should not be released . it therefore has subsequently never been repeated on air or posted on - screen . @entity16 regrets that the mention of this information could cause concern to the hostages , as well as their relatives , that their lives were in danger , \" the statement said . gunman @entity47 , also suspected in the slaying of a police officer , stormed the @entity27 @entity51 supermarket on january 9 , killing four people and taking others hostage . he was killed in the police operation to end the siege . a 24 - year - old supermarket employee , @entity57 - born @entity56 , was hailed as a hero afterward when it emerged that he had risked his life to hide 15 customers from @entity47 in the cold room . the hostage - taking was the culmination of three days of terror in @entity0 that began with the january 7 shooting of 12 people at the offices of @entity5 satirical magazine @entity69 . the two brothers blamed for that attack , @entity72 and @entity73 , were killed on january 9 after a violent standoff at an industrial site . the terror attacks claimed the lives of 17 people and put @entity5 on a heightened state of alert . @entity1 's @entity80 reported from @entity0 , and @entity81 wrote from @entity82 . @entity1 's @entity83 contributed to this report . query: they hid in a cold room during the attack in @entity0 by gunman @placeholder\nFigure 2: Heat map of softmax(eo(a)ht) when a = entity0."
    }, {
      "heading" : "8.3 HEAT MAP OF DIFFERENT READERS",
      "text" : "We randomly choose one article from CNN dataset and show the attention map αt = softmax(q>Waht) for different readers (in Attention Sum and Gated Attention Reader, Wα is identity matrix). From figure 7, figure 8 and figure 9, we can see that different readers essential put the weights on the entity identifiers.\n@entity0 ( @entity1 ) six survivors of the @entity0 kosher supermarket siege in january are suing a @entity5 media outlet for what they call dangerous live broadcasting during the hostage - taking . according to @entity0 prosecutor 's spokeswoman @entity10 , the lawsuit was filed march 27 and a preliminary investigation was opened by the prosecutor 's office wednesday . the media outlet , @entity1 affiliate @entity16 , is accused of endangering the lives of the hostages , who were hiding in a cold room during the attack , by broadcasting their location live during the siege . @entity23 in a statement friday said one of its journalists \" mentioned only once the presence of a woman hidden inside the @entity27 , on the basis of police sources on the ground . \" \" immediately , the chief editor felt that this information should not be released . it therefore has subsequently never been repeated on air or posted on - screen . @entity16 regrets that the mention of this information could cause concern to the hostages , as well as their relatives , that their lives were in danger , \" the statement said . gunman @entity47 , also suspected in the slaying of a police officer , stormed the @entity27 @entity51 supermarket on january 9 , killing four people and taking others hostage . he was killed in the police operation to end the siege . a 24 - year - old supermarket employee , @entity57 - born @entity56 , was hailed as a hero afterward when it emerged that he had risked his life to hide 15 customers from @entity47 in the cold room . the hostage - taking was the culmination of three days of terror in @entity0 that began with the january 7 shooting of 12 people at the offices of @entity5 satirical magazine @entity69 . the two brothers blamed for that attack , @entity72 and @entity73 , were killed on january 9 after a violent standoff at an industrial site . the terror attacks claimed the lives of 17 people and put @entity5 on a heightened state of alert . @entity1 's @entity80 reported from @entity0 , and @entity81 wrote from @entity82 . @entity1 's @entity83 contributed to this report . query: they hid in a cold room during the attack in @entity0 by gunman @placeholder\nFigure 3: Heat map of softmax(eo(a)ht) when a = entity1.\n@entity0 ( @entity1 ) six survivors of the @entity0 kosher supermarket siege in january are suing a @entity5 media outlet for what they call dangerous live broadcasting during the hostage - taking . according to @entity0 prosecutor 's spokeswoman @entity10 , the lawsuit was filed march 27 and a preliminary investigation was opened by the prosecutor 's office wednesday . the media outlet , @entity1 affiliate @entity16 , is accused of endangering the lives of the hostages , who were hiding in a cold room during the attack , by broadcasting their location live during the siege . @entity23 in a statement friday said one of its journalists \" mentioned only once the presence of a woman hidden inside the @entity27 , on the basis of police sources on the ground . \" \" immediately , the chief editor felt that this information should not be released . it therefore has subsequently never been repeated on air or posted on - screen . @entity16 regrets that the mention of this information could cause concern to the hostages , as well as their relatives , that their lives were in danger , \" the statement said . gunman @entity47 , also suspected in the slaying of a police officer , stormed the @entity27 @entity51 supermarket on january 9 , killing four people and taking others hostage . he was killed in the police operation to end the siege . a 24 - year - old supermarket employee , @entity57 - born @entity56 , was hailed as a hero afterward when it emerged that he had risked his life to hide 15 customers from @entity47 in the cold room . the hostage - taking was the culmination of three days of terror in @entity0 that began with the january 7 shooting of 12 people at the offices of @entity5 satirical magazine @entity69 . the two brothers blamed for that attack , @entity72 and @entity73 , were killed on january 9 after a violent standoff at an industrial site . the terror attacks claimed the lives of 17 people and put @entity5 on a heightened state of alert . @entity1 's @entity80 reported from @entity0 , and @entity81 wrote from @entity82 . @entity1 's @entity83 contributed to this report . query: they hid in a cold room during the attack in @entity0 by gunman @placeholder\nFigure 4: Heat map of softmax(eo(a)ht) when a = entity16.\n@entity0 ( @entity1 ) six survivors of the @entity0 kosher supermarket siege in january are suing a @entity5 media outlet for what they call dangerous live broadcasting during the hostage - taking . according to @entity0 prosecutor 's spokeswoman @entity10 , the lawsuit was filed march 27 and a preliminary investigation was opened by the prosecutor 's office wednesday . the media outlet , @entity1 affiliate @entity16 , is accused of endangering the lives of the hostages , who were hiding in a cold room during the attack , by broadcasting their location live during the siege . @entity23 in a statement friday said one of its journalists \" mentioned only once the presence of a woman hidden inside the @entity27 , on the basis of police sources on the ground . \" \" immediately , the chief editor felt that this information should not be released . it therefore has subsequently never been repeated on air or posted on - screen . @entity16 regrets that the mention of this information could cause concern to the hostages , as well as their relatives , that their lives were in danger , \" the statement said . gunman @entity47 , also suspected in the slaying of a police officer , stormed the @entity27 @entity51 supermarket on january 9 , killing four people and taking others hostage . he was killed in the police operation to end the siege . a 24 - year - old supermarket employee , @entity57 - born @entity56 , was hailed as a hero afterward when it emerged that he had risked his life to hide 15 customers from @entity47 in the cold room . the hostage - taking was the culmination of three days of terror in @entity0 that began with the january 7 shooting of 12 people at the offices of @entity5 satirical magazine @entity69 . the two brothers blamed for that attack , @entity72 and @entity73 , were killed on january 9 after a violent standoff at an industrial site . the terror attacks claimed the lives of 17 people and put @entity5 on a heightened state of alert . @entity1 's @entity80 reported from @entity0 , and @entity81 wrote from @entity82 . @entity1 's @entity83 contributed to this report . query: they hid in a cold room during the attack in @entity0 by gunman @placeholder\nFigure 5: Heat map of softmax(eo(a)ht) when a = entity27.\n@entity0 ( @entity1 ) six survivors of the @entity0 kosher supermarket siege in january are suing a @entity5 media outlet for what they call dangerous live broadcasting during the hostage - taking . according to @entity0 prosecutor 's spokeswoman @entity10 , the lawsuit was filed march 27 and a preliminary investigation was opened by the prosecutor 's office wednesday . the media outlet , @entity1 affiliate @entity16 , is accused of endangering the lives of the hostages , who were hiding in a cold room during the attack , by broadcasting their location live during the siege . @entity23 in a statement friday said one of its journalists \" mentioned only once the presence of a woman hidden inside the @entity27 , on the basis of police sources on the ground . \" \" immediately , the chief editor felt that this information should not be released . it therefore has subsequently never been repeated on air or posted on - screen . @entity16 regrets that the mention of this information could cause concern to the hostages , as well as their relatives , that their lives were in danger , \" the statement said . gunman @entity47 , also suspected in the slaying of a police officer , stormed the @entity27 @entity51 supermarket on january 9 , killing four people and taking others hostage . he was killed in the police operation to end the siege . a 24 - year - old supermarket employee , @entity57 - born @entity56 , was hailed as a hero afterward when it emerged that he had risked his life to hide 15 customers from @entity47 in the cold room . the hostage - taking was the culmination of three days of terror in @entity0 that began with the january 7 shooting of 12 people at the offices of @entity5 satirical magazine @entity69 . the two brothers blamed for that attack , @entity72 and @entity73 , were killed on january 9 after a violent standoff at an industrial site . the terror attacks claimed the lives of 17 people and put @entity5 on a heightened state of alert . @entity1 's @entity80 reported from @entity0 , and @entity81 wrote from @entity82 . @entity1 's @entity83 contributed to this report . query: they hid in a cold room during the attack in @entity0 by gunman @placeholder\nFigure 6: Heat map of softmax(eo(a)ht) when a = entity47.\n( @entity3 ) suspected @entity2 militants this week attacked civilians inside @entity5 for the first time in a month , killing at least 16 villagers , a military spokesman told @entity3 saturday . six attackers were killed by @entity5 forces , said maj. @entity10 , an operations officer with a special military unit set up to fight @entity2 . the attackers came thursday \" in the hundreds ... torched @entity14 village in the @entity15 , \" he said . @entity14 is a village that borders @entity17 and has been identified as a recruiting ground for @entity2 . regional gov. @entity19 said the insurgents have been attacking border villages in @entity5 in search of supplies . @entity5 troops retook cattle that was stolen by the attackers in @entity14 , @entity10 said . the last attack in @entity5 by the @entity29 - based militants was march 10 , when the assailants struck the locality of @entity32 in a failed attempt to overrun a military base . @entity2 , whose name translates as \" @entity44 education is sin , \" has been waging a years - long campaign of terror aimed at instituting its extreme version of @entity42 law in @entity29 . @entity2 's tactics have intensified in recent years , from battling @entity29 government soldiers to acts disproportionately affecting civilians -- such as raids on villages , mass kidnappings , assassinations , market bombings and attacks on churches and unaffiliated mosques . much of this violence has taken place in @entity29 , but neighboring countries -- @entity5 included -- have also been hit increasingly hard . journalist @entity61 in @entity63 , @entity5 , contributed to this report . query: @placeholder is based in @entity29 but has attacked across the border of several neighbors\nFigure 7: Heat map αt for Stanford Reader\n( @entity3 ) suspected @entity2 militants this week attacked civilians inside @entity5 for the first time in a month , killing at least 16 villagers , a military spokesman told @entity3 saturday . six attackers were killed by @entity5 forces , said maj. @entity10 , an operations officer with a special military unit set up to fight @entity2 . the attackers came thursday \" in the hundreds ... torched @entity14 village in the @entity15 , \" he said . @entity14 is a village that borders @entity17 and has been identified as a recruiting ground for @entity2 . regional gov. @entity19 said the insurgents have been attacking border villages in @entity5 in search of supplies . @entity5 troops retook cattle that was stolen by the attackers in @entity14 , @entity10 said . the last attack in @entity5 by the @entity29 - based militants was march 10 , when the assailants struck the locality of @entity32 in a failed attempt to overrun a military base . @entity2 , whose name translates as \" @entity44 education is sin , \" has been waging a years - long campaign of terror aimed at instituting its extreme version of @entity42 law in @entity29 . @entity2 's tactics have intensified in recent years , from battling @entity29 government soldiers to acts disproportionately affecting civilians -- such as raids on villages , mass kidnappings , assassinations , market bombings and attacks on churches and unaffiliated mosques . much of this violence has taken place in @entity29 , but neighboring countries -- @entity5 included -- have also been hit increasingly hard . journalist @entity61 in @entity63 , @entity5 , contributed to this report . query: @placeholder is based in @entity29 but has attacked across the border of several neighbors\nFigure 8: Heat map αt for Gated Attention Reader\n( @entity3 ) suspected @entity2 militants this week attacked civilians inside @entity5 for the first time in a month , killing at least 16 villagers , a military spokesman told @entity3 saturday . six attackers were killed by @entity5 forces , said maj. @entity10 , an operations officer with a special military unit set up to fight @entity2 . the attackers came thursday \" in the hundreds ... torched @entity14 village in the @entity15 , \" he said . @entity14 is a village that borders @entity17 and has been identified as a recruiting ground for @entity2 . regional gov. @entity19 said the insurgents have been attacking border villages in @entity5 in search of supplies . @entity5 troops retook cattle that was stolen by the attackers in @entity14 , @entity10 said . the last attack in @entity5 by the @entity29 - based militants was march 10 , when the assailants struck the locality of @entity32 in a failed attempt to overrun a military base . @entity2 , whose name translates as \" @entity44 education is sin , \" has been waging a years - long campaign of terror aimed at instituting its extreme version of @entity42 law in @entity29 . @entity2 's tactics have intensified in recent years , from battling @entity29 government soldiers to acts disproportionately affecting civilians -- such as raids on villages , mass kidnappings , assassinations , market bombings and attacks on churches and unaffiliated mosques . much of this violence has taken place in @entity29 , but neighboring countries -- @entity5 included -- have also been hit increasingly hard . journalist @entity61 in @entity63 , @entity5 , contributed to this report . query: @placeholder is based in @entity29 but has attacked across the border of several neighbors\nFigure 9: Heat map αt for Attention Sum Reader"
    } ],
    "references" : [ {
      "title" : "A thorough examination of the cnn/daily mail reading comprehension task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D Manning" ],
      "venue" : "In Proceedings of the ACL,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Broad context language modeling as reading comprehension",
      "author" : [ "Zewei Chu", "Hai Wang", "Kevin Gimpel", "David McAllester" ],
      "venue" : null,
      "citeRegEx" : "Chu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention-overattention neural networks for reading comprehension",
      "author" : [ "Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu" ],
      "venue" : null,
      "citeRegEx" : "Cui et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2016
    }, {
      "title" : "English gigaword ldc2003t05",
      "author" : [ "Graff David", "Christopher Cieri" ],
      "venue" : "Philadelphia: Linguistic Data Consortium,",
      "citeRegEx" : "David and Cieri.,? \\Q2003\\E",
      "shortCiteRegEx" : "David and Cieri.",
      "year" : 2003
    }, {
      "title" : "The lambada dataset: Word prediction requiring a broad discourse context",
      "author" : [ "Paperno. Denis", "Germn Kruszewski", "Angeliki Lazaridou", "Quan Ngoc Pham", "Raffaella Bernardi", "Sandro Pezzelle", "Marco Baroni", "Gemma Boleda", "Raquel Fernndez" ],
      "venue" : "In Proceedings of the ACL,",
      "citeRegEx" : "Denis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Denis et al\\.",
      "year" : 2016
    }, {
      "title" : "Gated-attention readers for text comprehension",
      "author" : [ "Bhuwan Dhingra", "Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Dhingra et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2016
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karm Moritz Hermann", "Tom Kocisk", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The goldilocks principle: Reading childrens books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Proceedings of the 4th International Conference on Learning Representations,",
      "citeRegEx" : "Hill et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Pennington Jeffrey", "Richard Socher", "Christopher D. Manning" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,",
      "citeRegEx" : "Jeffrey et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jeffrey et al\\.",
      "year" : 2014
    }, {
      "title" : "Text understanding with the attention sum reader network",
      "author" : [ "Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Kadlec et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "In Proceedings of the 3rd International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Dynamic entity representation with max-pooling improves machine reading",
      "author" : [ "Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui" ],
      "venue" : "In Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACLHLT),",
      "citeRegEx" : "Kobayashi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2016
    }, {
      "title" : "Reasoning in vector space: An exploratory study of question answering",
      "author" : [ "Moontae Lee", "Xiaodong He", "Scott Wen tau Yih", "Jianfeng Gao", "Li Deng", "Paul Smolensky" ],
      "venue" : "Proceedings of the 4th International Conference on Learning Representations,",
      "citeRegEx" : "Lee et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Reasoning with memory augmented neural networks for language comprehension",
      "author" : [ "Tsendsuren Munkhdalai", "Hong Yu" ],
      "venue" : null,
      "citeRegEx" : "Munkhdalai and Yu.,? \\Q2016\\E",
      "shortCiteRegEx" : "Munkhdalai and Yu.",
      "year" : 2016
    }, {
      "title" : "Who did what: A large-scale person-centered cloze dataset",
      "author" : [ "Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester" ],
      "venue" : "In Proceedings of the EMNLP,",
      "citeRegEx" : "Onishi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Onishi et al\\.",
      "year" : 2016
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang" ],
      "venue" : "In Proceedings of International Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Rajpurkar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Pascanu Razvan", "Tomas Mikolov", "Yoshua Bengio" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Razvan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Razvan et al\\.",
      "year" : 2013
    }, {
      "title" : "Mctest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher JC Burges", "Erin Renshaw" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,",
      "citeRegEx" : "Richardson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Andrew M. Saxe", "James L. McClelland", "Surya Ganguli" ],
      "venue" : "Arxiv,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Reasonet: Learning to stop reading in machine comprehension",
      "author" : [ "Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Iterative alternating neural attention for machine reading",
      "author" : [ "Alessandro Sordonif", "Phillip Bachmanf", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Sordonif et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sordonif et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural language comprehension with the epireader",
      "author" : [ "Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman" ],
      "venue" : null,
      "citeRegEx" : "Trischler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2016
    }, {
      "title" : "Separating answers from queries for neural reading comprehension",
      "author" : [ "Dirk Weissenborn" ],
      "venue" : null,
      "citeRegEx" : "Weissenborn.,? \\Q2016\\E",
      "shortCiteRegEx" : "Weissenborn.",
      "year" : 2016
    }, {
      "title" : "Towards ai complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M. Rush", "Bart van Merrinboer", "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "In Proceedings of the 4th International Conference on Learning Representations,",
      "citeRegEx" : "Weston et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2016
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Fre de ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio" ],
      "venue" : "NIPS Workshop Deep Learning and Unsupervised Feature Learning,",
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "In particular the CNN & DailyMail datasets (Hermann et al., 2015), the Children’s Book Test (CBT) (Hill et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : ", 2015), the Children’s Book Test (CBT) (Hill et al., 2016), and the Who-did-What dataset (Onishi et al.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : ", 2016), and the Who-did-What dataset (Onishi et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "Aggregation readers include Memory Networks (Weston et al.; Sukhbaatar et al., 2015), the Attentive Reader (Hermann et al.",
      "startOffset" : 44,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : ", 2015), the Attentive Reader (Hermann et al., 2015) and the Stanford Reader (Chen et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and the Stanford Reader (Chen et al., 2016).",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "Explicit reference readers include the Attention Sum Reader (Kadlec et al., 2016), the Gated Attention Reader (Dhingra et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : ", 2016), the Gated Attention Reader (Dhingra et al., 2016), the Attention-over-Attention Reader (Cui et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : ", 2016), the Attention-over-Attention Reader (Cui et al., 2016) and others (a list can be found in section 6).",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "Who-did-What (WDW): The Who-did-What dataset (Onishi et al., 2016) contains 127,000 multiple choice cloze questions constructed from the LDC English Gigaword newswire corpus (David & Cieri, 2003).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "According to the original study on this dataset (Hill et al., 2016), n-gram and recurrent neural network language models are sufficient for predicting verbs or prepositions.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "CNN & DailyMail: Hermann et al. (2015) constructed these datasets from a large number of news articles from the CNN and Daily Mail news websites.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "CNN & DailyMail: Hermann et al. (2015) constructed these datasets from a large number of news articles from the CNN and Daily Mail news websites. The main article is used as the context, while the cloze style question is formed from one short highlight sentence appearing in conjunction with the published article. To avoid the model using external world knowledge when answering the question, the named entities in the entire dataset were replaced by anonymous entity IDs which were then further shuffled for each example. This forces models to rely on the context document to answer each question. In this anonymized corpus the entity identifiers are taken to be a part of the vocabulary and the answer set A consists of the entity identifiers occurring in the passage. Who-did-What (WDW): The Who-did-What dataset (Onishi et al., 2016) contains 127,000 multiple choice cloze questions constructed from the LDC English Gigaword newswire corpus (David & Cieri, 2003). In contrast with CNN and Daily Mail, it avoids using article summaries for question formation. Instead, each problem is formed from two independent articles: one is given as the passage to be read and a different article on the same entities and events is used to form the question. Further, Who-did-What avoids anonymization, as each choice is a person named entity. In this dataset the answer set A consists of the person named entities occurring in the passage. Finally, the problems have been filtered to remove a fraction that are easily solved by simple baselines. It has two training sets. The larger training set (“relaxed”) is created using less baseline filtering, while the smaller training set (“strict”) uses the same filtering as the validation and test sets. Children’s Book Test (CBT) Hill et al. (2016) developed the CBT dataset in a slightly different fashion to the CNN/DailyMail datasets.",
      "startOffset" : 17,
      "endOffset" : 1789
    }, {
      "referenceID" : 17,
      "context" : "The MCTest dataset (Richardson et al., 2013) consists of children’s stories and questions written by crowdsourced workers.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "The bAbI dataset (Weston et al., 2016) is constructed automatically using synthetic text generation and can be perfectly answered by hand-written algorithms (Lee et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : ", 2016) is constructed automatically using synthetic text generation and can be perfectly answered by hand-written algorithms (Lee et al., 2016).",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "The SQuAD dataset (Rajpurkar et al., 2016) consists passage-question pairs where the passage is a wikipedia article and the questions are written by crowdsourced workers.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "The LAMBADA dataset (Denis et al., 2016) is a word prediction dataset which requires a broad discourse context and the correct answer might not in the context.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "Nonetheless, when the correct answer is in the context, neural readers can be applied effectively (Chu et al., 2016).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "Aggregation readers appeared first in the literature and include Memory Networks (Weston et al.; Sukhbaatar et al., 2015), the Attentive Reader (Hermann et al.",
      "startOffset" : 81,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : ", 2015), the Attentive Reader (Hermann et al., 2015), and the Stanford Reader (Chen et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : ", 2015), and the Stanford Reader (Chen et al., 2016).",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "Explicit reference readers incluce the Attention-Sum Reader (Kadlec et al., 2016), the Gated-Attention Reader (Dhingra et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : ", 2016), the Gated-Attention Reader (Dhingra et al., 2016), and the Attention-over-Attention Reader (Cui et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : ", 2016), and the Attention-over-Attention Reader (Cui et al., 2016).",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "The the Stanford Reader (Chen et al., 2016) computes a bi-directional LSTM representation of both the passage and the question.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "Memory Networks (Weston et al.; Sukhbaatar et al., 2015) use (4) and (6) but have more elaborate methods of constructing “memory vectors” ht not involve LSTMs.",
      "startOffset" : 16,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "The Stanford Reader was derived from the Attentive Reader (Hermann et al., 2015).",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "In the Attention-Sum Reader (Kadlec et al., 2016) h and q are computed with equations (1) and (2) as in the Stanford Reader but using GRUs rather than LSTMs.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "Attention-over-Attention Reader, The Attention-over-Attention Reader (Cui et al., 2016) uses a more elaborate method to compute the attention αt.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "The Gated Attention Reader Dhingra et al. (2016) involves a K-layer biGRU architecture defined by the following equations.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "Who did What Val Test Attention Sum Reader (Onishi et al., 2016) 59.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "8 Gated Attention Reader (Onishi et al., 2016) 60.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "6 Attention Sum (Kadlec et al., 2016) 68.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "4 Gated Attention (Dhingra et al., 2016) 73.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "9 AoA Reader (Cui et al., 2016) 73.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "4 DER Network (Kobayashi et al., 2016) 71.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "9 - - - - - Epi Reader (Trischler et al., 2016) 73.",
      "startOffset" : 23,
      "endOffset" : 47
    }, {
      "referenceID" : 20,
      "context" : "4 Iterative Reader (Sordonif et al., 2016) 72.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "2 QANN (Weissenborn, 2016) - 73.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "1 MemNets (Sukhbaatar et al., 2015) 63.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "0 Attentive Reader (Hermann et al., 2015) 61.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "0 - - - Stanford Reader (Chen et al., 2016) 72.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "0 - - - - - ReasoNet (Shen et al., 2016) 72.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "Note that the result of Stanford Reader we report here is the one without relabeling since relabeling procedure doesn’t follow the protocol used in Hermann et al. (2015).",
      "startOffset" : 148,
      "endOffset" : 170
    } ],
    "year" : 2016,
    "abstractText" : "Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the existence of logical structure in the hidden state vectors of “aggregation readers” such as the Attentive Reader and Stanford Reader. The logical structure of aggregation readers reflects the architecture of “explicit reference readers” such as the Attention-Sum Reader, the GatedAttention Reader and the Attention-over-Attention Reader. This relationship between aggregation readers and explicit reference readers presents a case study in emergent logical structure. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on Who-did-What datasets1.",
    "creator" : "LaTeX with hyperref package"
  }
}