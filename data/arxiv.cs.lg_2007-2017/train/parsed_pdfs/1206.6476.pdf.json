{
  "name" : "1206.6476.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Similarity Learning for Provably AccurateSparse Linear Classification",
    "authors" : [ "Aurélien Bellet", "Amaury Habrard", "Marc Sebban" ],
    "emails" : [ "aurelien.bellet@univ-st-etienne.fr", "amaury.habrard@univ-st-etienne.fr", "marc.sebban@univ-st-etienne.fr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The notion of (dis)similarity plays an important role in many machine learning problems such as classification, clustering or ranking. For this reason, researchers have studied, in practical and formal ways, what it means for a pairwise similarity function to be “good”. Since manually tuning such functions can be difficult and tedious for real-world problems,\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\na lot of work has gone into automatically learning them from labeled data, leading to the emergence of supervised similarity and metric learning. Generally speaking, these approaches are based on the reasonable intuition that a good similarity function should assign a large (resp. small) score to pairs of points of the same class (resp. different class). Following this idea, they aim at finding the parameters (usually a matrix) of the function such that it satisfies best these local pair-based constraints. Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well. Some work has also gone into learning arbitrary similarity functions with no PSD constraint to make the problem easier to solve (Chechik et al., 2009; Qamar, 2010). The (dis)similarities learned with the above-mentioned methods are typically plugged in a k-Nearest Neighbor (k-NN) classifier (whose decision rule is based on a local neighborhood) and often lead to greater accuracy than the standard Euclidean distance, although no theoretical evidence supports this behavior. However, it seems unclear whether they can be successfully used in the context of global classifiers, such as linear separators.\nRecently, Balcan et al. (2008) introduced the formal notion of (ǫ, γ, τ)-good similarity function, which does not require positive semi-definiteness and is less restrictive than local pair-based constraints. Indeed, it basically says that for most points, the average similarity scores to some points of the same class should be greater than to some points of different class. Assuming this property holds, generalization guarantees can be derived in terms of the error of a sparse linear\nclassifier built from this similarity function.\nIn this paper, we use the notion of (ǫ, γ, τ)-goodness to design a new similarity learning algorithm. This novel approach, called SLLC (Similarity Learning for Linear Classification), has several advantages: (i) it is tailored to linear classifiers, (ii) theoretically well-founded, (iii) does not require positive semi-definiteness, and (iv) is in a sense less restrictive than pair-based settings. We formulate the problem of learning a good similarity function as an efficient convex quadratic program which optimizes a bilinear similarity. Furthermore, by using the Kernel Principal Component Analysis (KPCA) trick (Chatpatanasiri et al., 2010), we are able to kernelize our algorithm and thereby learn more powerful similarity functions and classifiers in the nonlinear feature space induced by a kernel. On the theoretical point of view, we show that our approach has uniform stability (Bousquet & Elisseeff, 2002), which leads to generalization guarantees in terms of the (ǫ, γ, τ)-goodness of the learned similarity. Lastly, we provide an experimental study on seven datasets of various domains and compare SLLC with two widelyused metric learning approaches. This study demonstrates the practical effectiveness of our method and shows that it is fast, robust to overfitting and induces very sparse classifiers, making it suitable for dealing with high-dimensional data.\nThe rest of the paper is organized as follows. Section 2 reviews some past work in similarity and metric learning and introduces the theory of (ǫ, γ, τ)-good similarities. Section 3 presents our approach, SLLC, and the KPCA trick used to kernelize it. Section 4 provides a theoretical analysis of SLLC, leading to the derivation of generalization guarantees. Finally, Section 5 features an experimental study on various datasets."
    }, {
      "heading" : "2. Notations and Related Work",
      "text" : "We denote vectors by lower-case bold symbols (x) and matrices by upper-case bold symbols (A). We consider labeled points z = (x, ℓ) drawn from an unknown distribution P over Rd × {−1, 1}. A similarity function is defined by K : Rd × Rd → [−1, 1]. We denote the L2-norm by ‖ · ‖2 and the Frobenius norm by ‖ · ‖F . Lastly, [1− c]+ = max(0, 1− c) denotes the hinge loss."
    }, {
      "heading" : "2.1. Metric and Similarity Learning",
      "text" : "Supervised metric and similarity learning aims at finding the parameters (usually a matrix) of a (dis)similarity function such that it best satisfies local constraints derived from the class labels. These constraints are typically pair-based (“examples x and\nx′ should be similar/dissimilar”) or triplet-based (“x should be more similar to x′ than to x′′”). A great deal of work has focused on learning a (squared) Mahalanobis distance defined by d2M(x,x\n′) = (x − x′)TM(x− x′) and parameterized by the PSD matrix M ∈ Rd×d. A Mahalanobis distance implicitly corresponds to computing the Euclidean distance after some linear projection of the data. The PSD constraint ensures that this interpretation holds and makes dM a (pseudo)metric, which enables k-NN speed-ups based on (for instance) the triangle inequality. The methods available in the literature mainly differ by their choices of objective/loss function and regularizer on M. For instance, Schultz & Joachims (2003) require examples to be closer to similar examples than to dissimilar ones by a certain margin. Weinberger & Saul (2009) define an objective function related to the kNN error on the training set. Davis et al. (2007) regularize using the LogDet divergence (for its automatic enforcement of PSD) while Ying et al. (2009) use the (2,1)-norm (which favors a low-rank M). There also exist purely online methods (Shalev-Shwartz et al., 2004; Jain et al., 2008). The bottleneck of many of these approaches is to enforce the PSD constraint on M, although some manage to reduce this computational burden by developing specific solvers. There has also been some interest in learning arbitrary similarity functions with no PSD requirement (Chechik et al., 2009; Qamar, 2010). All of the previously-mentioned learned similarities are used in the context of nearestneighbors approaches (sometimes in clustering), which are based on local neighborhoods. In practice, they often outperform standard similarities, although no theoretical argument supports this behavior.\nHowever, these local constraints do not seem appropriate to learn a similarity function for use in global classifiers, such a linear separators. The theory presented in the next section introduces a new, different notion of the goodness of a similarity function, and shows that such a good similarity achieves bounded error in linear classification. This opens the door to similarity learning for improving linear classifiers."
    }, {
      "heading" : "2.2. Learning with Good Similarity Functions",
      "text" : "In recent work, Balcan et al. (2008) introduced a new theory of learning with good similarity functions, based on the following definition.\nDefinition 1 (Balcan et al., 2008) A similarity function K is an (ǫ, γ, τ)-good similarity function in hinge loss for a learning problem P if there exists a (random) indicator function R(x) defining a (probabilistic) set of “reasonable points” such that the\nfollowing conditions hold:\n1. E(x,ℓ)∼P [[1− ℓg(x)/γ]+] ≤ ǫ, where g(x) = E(x′,ℓ′)∼P [ℓ\n′K(x,x′)|R(x′)], 2. Prx′ [R(x ′)] ≥ τ .\nThinking of this definition in terms of number of margin violations, we can interpret the first condition as “an ǫ proportion of examples x are on average 2γ more similar to random reasonable examples of the same class than to random reasonable examples of the opposite class” and the second condition as “at least a τ proportion of the examples should be reasonable”. This definition is interesting in three respects. First, it does not impose positive semi-definiteness nor symmetry, which are requirements that may rule out the most natural similarity functions for some tasks. Second, it is based on an average over some points, which is less restrictive than pair or triplet-based settings. Third, satisfying Definition 1 is sufficient to learn well (Theorem 1).\nTheorem 1 (Balcan et al., 2008) Let K be an (ǫ, γ, τ)-good similarity function in hinge loss for a learning problem P . For any ǫ1 > 0 and 0 ≤ δ ≤ γǫ1/4, let S = {x′1, . . . ,x′dland} be a (potentially unlabeled) sample of dland = 2 τ ( log(2/δ) + 16 log(2/δ)(ǫ1γ)2 ) landmarks drawn from P . Consider the mapping φS : R\nd → Rdland defined as follows: φSi (x) = K(x,x′i), i ∈ {1, . . . , dland}. Then, with probability at least 1− δ over the random sample S, the induced distribution φS(P ) in Rdland has a linear separator α of error at most ǫ+ ǫ1 at margin γ.\nIn other words, if we are given an (ǫ, γ, τ)-good similarity function for a learning problem P and enough points (the “landmarks”), there exists a linear separator α with error arbitrary close to ǫ, which lies in the explicit “φ-space” (the space of the similarity scores to the landmarks). As Balcan et al. mention, using du (potentially unlabeled) landmark examples and dl labeled examples, we can estimate this separator α ∈ Rdu by solving the following linear program:1\nmin α\ndl ∑\ni=1\n 1− du ∑\nj=1\nαjℓiK(xi,x ′ j)\n\n\n+\n+ λ‖α‖1. (1)\nNote that Problem (1) is essentially an L1-regularized linear SVM (Zhu et al., 2003) with an empirical similarity map (Balcan et al., 2008), and can be efficiently\n1The original formulation proposed by Balcan et al. (2008) was actually L1-constrained. We turned it into an equivalent L1-regularized one.\nsolved. The L1-regularization induces sparsity (zero coordinates) in α, which reduces the number of training examples the classifier is based on, speeding up prediction. We can control the amount of sparsity by using the parameter λ (the larger λ, the sparser α).\nTo sum up, the performance of the linear classifier theoretically depends on how well the similarity function satisfies Definition 1. However, for some problems, standard similarity functions may satisfy the definition poorly, leading to weak guarantees. To deal with this limitation, Kar & Jain (2011) propose to automatically adapt the goodness criterion to the problem at hand. In this paper, we take a different approach: we see Definition 1 as a novel, theoretically well-founded objective function for similarity learning."
    }, {
      "heading" : "3. Learning Good Similarity Functions for Linear Classification",
      "text" : "We consider KA(x,x ′) = xTAx′, a bilinear similarity parameterized by the matrix A ∈ Rd×d, which is not constrained to be PSD nor symmetric. This form of similarity function was successfully used in the context of large-scale online image similarity learning (Chechik et al., 2009). KA has the advantage of being efficiently computable when the inputs x and x′ are sparse vectors. In order to satisfy the condition KA ∈ [−1, 1], we assume that inputs are normalized such that ||x||2 ≤ 1, and we require ||A||F ≤ 1."
    }, {
      "heading" : "3.1. Similarity Learning Formulation",
      "text" : "Our goal is to optimize the (ǫ, γ, τ)-goodness of KA on a finite-size sample. To this end, we are given a training sample of NT labeled points T = {zi = (xi, ℓi)}NTi=1 and a sample of NR labeled reasonable points R = {zk = (xk, ℓk)}NRk=1. In practice, R is a subset of T with NR = τ̂NT (τ̂ ∈ ]0, 1]). In the lack of background knowledge, it can be drawn randomly or according to some criterion (e.g., diversity (Kar & Jain, 2011)). Given R and a margin γ, let also V (A, zi, R) = [1− ℓi 1γNR ∑NR\nk=1 ℓkKA(xi,xk)]+ denote the empirical goodness of KA with respect to a single training point zi, and ǫT =\n1 NT ∑NT i=1 V (A, zi, R) the\nempirical goodness over T .\nNow, we want to learn the matrixA that minimizes ǫT . This can be done by solving the following regularized problem, referred to as SLLC (Similarity Learning for Linear Classification):\nmin A∈Rd×d\nǫT + β‖A‖2F\nwhere β is a regularization parameter. Note that an equivalent constrained formulation can be obtained by\nrewriting the sum of NT hinge losses in the objective function as NT margin constraints and introducing NT slack variables in the objective.\nSLLC is radically different from classic metric and similarity learning algorithms, which are based on pair or triplet-based constraints. It learns a global similarity rather than a local one, since R is the same for each training example. Moreover, the constraints are easier to satisfy since they are defined over an average of similarity scores to the points in R instead of over a single pair or triplet. This means that one can fulfill a constraint without satisfying the margin for each point in R individually. SLLC has also a number of desirable properties: (i) This is a convex quadratic program, which can be solved efficiently using standard solvers. No costly semi-definite programming is required, as opposed to many Mahalanobis distance learning methods. (ii) In the constrained formulation, there is only one constraint per training example (instead of one for each pair or triplet), i.e., a total of NT constraints and NT + d\n2 variables. (iii) The size of R does not affect the complexity of the constraints. (iv) If xi is sparse, then the associated constraint is sparse as well (some variables of the problem do not appear)."
    }, {
      "heading" : "3.2. Kernelization of SLLC",
      "text" : "The framework presented in the previous section is theoretically well-founded with respect to Balcan et al.’s theory and has some generalization guarantees, as we will see in the next section. Moreover, it has the advantage of being very simple: we learn a global linear similarity and use it to build a global linear classifier. In order to learn more powerful similarities (and therefore classifiers), we propose to kernelize the approach by learning them in the nonlinear feature space induced by a kernel. Kernelization allows linear classifiers such as Support Vector Machines or some Mahalanobis distance learning algorithms (e.g., Shalev-Shwartz et al., 2004; Davis et al., 2007) to learn nonlinear decision boundaries or transformations. However, kernelizing a metric learning algorithm is not trivial: a new formulation of the problem has to be derived, where interface to the data is limited to inner products, and sometimes a different implementation is necessary. Moreover, when kernelization is possible, one must learn a NT ×NT matrix. As NT gets large, the problem becomes intractable unless dimensionality reduction is applied.\nFor these reasons, we instead use the KPCA trick, recently proposed by Chatpatanasiri et al. (2010). It provides a straightforward way to kernelize a metric learning algorithm while performing dimensionality re-\nduction at no additional cost. The idea is to use Kernel Principal Component Analysis (Schölkopf et al., 1998) to project the data into a new space using a nonlinear kernel function, and to keep only a chosen number of dimensions (those that capture best the overall variance of the data). The data are then projected into this new feature space, and the (unchanged) metric learning algorithm can be used to learn a metric in that space. Chatpatanasiri et al. (2010) showed that the KPCA trick is theoretically sound for unconstrained metric and similarity learning algorithms (they proved representer theorems), which includes SLLC. Throughout the rest of this paper, we will only consider the kernelized version of SLLC.\nGenerally speaking, kernelizing a metric learning algorithm may cause or increase overfitting, especially when data are scarce and/or high-dimensional. However, since our entire framework is linear and global, we expect our method to be quite robust to this effect. This will be doubly confirmed in the rest of this paper: experimentally in Section 5, but also theoretically with the derivation in the following section of generalization guarantees independent from the size of the projection space."
    }, {
      "heading" : "4. Theoretical Analysis",
      "text" : "In this section, we present a theoretical analysis of our approach. Our main result is the derivation of a generalization bound (Theorem 3) guaranteeing the consistency of SLLC and thus the (ǫ, γ, τ)-goodness for the considered task. In our framework, the similarity is optimized according to a set R of reasonable points coming from the training sample. Therefore, these reasonable points may not follow the distribution from which the training sample has been generated. To cope with this situation, we propose to derive a generalization bound according to the framework of uniform stability (Bousquet & Elisseeff, 2002), which does not assume an i.i.d. draw at the pair level."
    }, {
      "heading" : "4.1. Uniform Stability",
      "text" : "Roughly speaking, an algorithm is stable if its output does not change significantly under a small modification of the training sample. This variation is required to be bounded in O(1/NT ) in terms of infinite norm.\nDefinition 2 (Bousquet & Elisseeff, 2002) A learning algorithm has a uniform stability in κNT w.r.t. a loss function L, with κ a positive constant, if\n∀T,∀i, 1 ≤ i ≤ NT , sup z\n|L(MT , z)−L(MT i , z)| ≤ κ\nNT ,\nwhere MT is the model learned from the sample T ,"
    }, {
      "heading" : "MT i the model learned from the sample T",
      "text" : "i. T i is obtained from T by replacing the ith example zi ∈ T by another example z′i independent from T and drawn from P . L(M, z) is the loss for an example z.\nIn this definition, T i characterizes the notion of small modification of the training sample. When this definition is fulfilled, Bousquet & Elisseeff (2002) have shown that the following generalization bound holds.\nTheorem 2 (Bousquet & Elisseeff, 2002) Let δ > 0 and NT > 1. For any algorithm with uniform stability κ/NT using a loss function bounded by 1, with probability 1−δ over the random draw of T :\nL(MT ) < L̂T (MT ) + κ\nNT + (2κ+ 1)\n√\nln 1/δ\n2NT ,\nwhere L(MT ) is the expected loss and L̂T (MT ) its empirical estimate over T ."
    }, {
      "heading" : "4.2. Generalization Bound",
      "text" : "For convenience, given a bilinear model KA, we denote by AR both the similarity defined by the matrix A and its associated set of reasonable points R (when it is clear from the context we may omit the subscript R). Given a similarity AR, V (AR, z, R) plays the role of the loss function over one example z. The loss over the sample T is defined as ǫT (AR) =\n1 NT ∑NT i=1 V (AR, zi, R), and corresponds\nto the empirical goodness. Lastly, the expected loss over the true distribution is given by ǫ(AR) = Ez=(x,l)∼PV (AR, z, R), and corresponds to the goodness in generalization. When it is clear from the context we may simply use ǫT and ǫ.\nIn our case, to prove the uniform stability property we need to show that\n∀T,∀i, sup z |V (A, z, R)− V (Ai, z, Ri)| ≤ κ NT ,\nwhere A is learned from T , R ⊆ T , Ai is the matrix learned from T i and Ri ⊆ T i is the set of reasonable points associated to T i. Note that R and Ri are of equal size and can differ in at most one example, depending on whether zi or z ′ i belong to their corresponding set of reasonable points. For the sake of simplicity, we consider V bounded by 1 (which can be easily obtained by dividing it by the constant 1 + 1γ ). To show this property, we need the following results.\nLemma 1 For any labeled examples z = (x, ℓ), z′ = (x′, ℓ′) and any models AR, A ′ R′ , the following holds:\nP1: |KA(x,x′)| ≤ 1,"
    }, {
      "heading" : "P2: |KA(x,x′)−KA′(x,x′)| ≤ ‖A−A′‖F ,",
      "text" : "P3: |V (A, z, R)− V (A′, z, R′)| ≤ 1| PNR k=1 ℓkKA(x,xk)\nγNR −\nPN R′\nj=1 ℓ′kKA′ (x,x ′ k)\nγNR′ | (1-admissibility property of V ).\nProof P1 comes from |KA(x,x′)| ≤ ‖x‖2‖A‖F‖x′‖2, the normalization on examples (‖x‖2 ≤ 1) and the requirement on matrices (‖A‖F ≤ 1).\nFor P2, we observe that |KA(x,x ′) − KA′(x,x ′)| = |KA−A′(x,x ′)|, and we use the normalization ‖x‖2 ≤ 1.\nP3 follows directly from |ℓ| = 1 and the 1-lipschitz property\nof the hinge loss: |[X]+ − [Y ]+| ≤ |X − Y |. 2\nLet FT = ǫT (A)+β‖A‖2F be the objective function of SLLC w.r.t. a sample T and a set of reasonable points R ⊆ T . The following lemma bounds the deviation between A and Ai.\nLemma 2 For any models A and Ai that are minimizers of FT and FT i respectively, we have:\n‖A−Ai‖F ≤ 1\nβNT γ .\nProof We follow closely the proof of Lemma 20 of (Bousquet & Elisseeff, 2002) and omit some details due to the limitation of space. Let ∆A = Ai −A and 0 ≤ t ≤ 1, M1 = ‖A‖ 2 F −‖A+ t∆A‖ 2 F + ‖A i‖2F −‖A i − t∆A‖2F and M2 = 1\nβNT (ǫT (AR)−ǫT ((A+t∆A)R)+ǫT i((A+t∆A)R)−\nǫT i(AR)). Using the fact that FT and FT i are convex functions, that A and Ai are their respective minimizers and property P3, we have M1 ≤ M2. Fixing t = 1/2, we obtain M1 = ‖A − A\ni‖2F , and using property P3 and the normalization ‖x‖2 ≤ 1, we get:\nM2 ≤ 1\nβNT γ (‖\n1 2 ∆A‖F + ‖ − 1 2 ∆A‖F ) = ‖A−Ai‖F βNT γ .\nThis leads to the inequality ‖A −Ai‖2F ≤ ‖A−Ai‖F\nβNT γ from\nwhich Lemma 2 is directly derived. 2\nWe now have all the material needed to prove the stability property of our algorithm.\nLemma 3 Let NT and NR be the number of training examples and reasonable points respectively, NR = τ̂NT with τ̂ ∈]0, 1]. SLLC has a uniform stability in κ NT with κ = 1γ ( 1 βγ + 2 τ̂ ) = τ̂+2βγ τ̂βγ2 , where β is the regularization parameter and γ the margin.\nProof For any sample T of size NT , any 1 ≤ i ≤ NT , any labeled examples z = (x, ℓ) and z′i = (xi, ℓ ′ i) ∼ P :\n|V (A, z, R)− V (Ai, z, Ri)|\n≤\n˛ ˛ ˛ ˛ ˛ ˛ 1 γNR NR X\nk=1\nℓkKA(x,xk)− 1\nγNRi\nN Ri X\nk=1\nℓkKAi(x,xk)\n˛ ˛ ˛ ˛ ˛ ˛\n=\n˛ ˛ ˛ ˛ ˛ ˛ 1 γNR 0 @ 0 @ NR X\nk=1,k 6=i\n(ℓkKA(x,xk)−KAi(x,xk))\n1\nA+\nℓiKA(x,xi)− ℓ ′ iKAi(x,x ′ i)\n1\nA\n˛ ˛ ˛ ˛ ˛ ˛\n≤ 1\nγNR\n0\n@\n0\n@\nNR X\nk=1,k 6=i\n(|ℓk|‖A−A i‖F )\n1\nA+\n|ℓiKAi(x,xi)|+ |ℓ ′ iKA(x,x ′ i)|\n1\nA\n≤ 1 γNR ( NR − 1 βNT γ + 2) ≤ 1 γNR ( NR βNT γ + 2).\nThe first inequality follows from P3. The second comes from the fact that R and Ri differ in at most one element, corresponding to the example zi in R and the example z ′ i replacing zi in R i. The last inequalities are obtained by the use of the triangle inequality, P1, P2, Lemma 2, and the fact that the labels belong to {−1, 1}. Since NR = τ̂NT , we get |V (A, z, R)− V (Ai, z, Ri)| ≤ 1 γNT ( 1 βγ + 2 τ̂ ). 2\nApplying Thm 2 with Lemma 2 gives our main result.\nTheorem 3 Let γ > 0, δ > 0 and NT > 1. With probability at least 1 − δ, for any model AR learned with SLLC, we have:\nǫ ≤ ǫT+ 1\nNT\n(\nτ̂ + 2βγ\nτ̂βγ2\n)\n+\n(\n2(τ̂ + 2βγ)\nτ̂βγ2 + 1\n)\n√\nln 1/δ\n2NT .\nThm 3 highlights three important properties of SLLC. First, it converges in O(1/ √ NT ), which is a standard convergence rate for uniform stability. Second, it is independent from the dimensionality of the data. This is due to the fact that ‖A‖F is bounded by a constant. Third, Thm 3 bounds the goodness in generalization of the learned similarity function. By minimizing ǫT with SLLC, we minimize ǫ and thus the error of the resulting linear classifier, as stated by Thm 1."
    }, {
      "heading" : "5. Experiments",
      "text" : "We propose a comparative study of our method and two widely-used Mahalanobis distance learning algorithms: Large Margin Nearest Neighbor (LMNN) from Weinberger & Saul (2009) and Information-Theoretic Metric Learning (ITML) from Davis et al. (2007).2 Roughly speaking, LMNN optimizes the k-NN error on the training set (with a safety margin) whereas ITML aims at best satisfying pair-based constraints while\n2We used the code provided by the authors.\nminimizing the LogDet divergence between the learned matrix M and the identity matrix. We conduct this experimental study on seven classic binary datasets of varying domain, size and difficulty, mostly taken from the UCI Machine Learning Repository. Their properties are summarized in Table 1. Some of them, such as Breast, Ionosphere or Pima, have been extensively used to evaluate metric learning methods."
    }, {
      "heading" : "5.1. Experimental Setup",
      "text" : "We compare the following methods: (i) the cosine similarity KI in KPCA space, as a baseline, (ii) SLLC, (iii) LMNN in the original space, (iv) LMNN in KPCA space, (v) ITML in the original space, and (vi) ITML in KPCA space.3 All attributes are scaled to [−1/d; 1/d] to ensure ‖x‖2 ≤ 1. To generate a new feature space using KPCA, we use the Gaussian kernel with parameter σ equal to the mean of all pairwise training data Euclidean distances (a standard heuristic, used for instance by Kar & Jain (2011)). Ideally, we would like to project the data to the feature space of maximum size (equal to the number of training examples), but to keep the computations tractable we only retain three times the number of features of the original data (four times for the low-dimensional datasets), as shown in Table 1.4 On Cod-RNA, KPCA was run on a randomly drawn subsample of 10% of the training data. Unless predefined training and test sets are available (as for Splice, Svmguide1 and Cod-RNA), we randomly generate 70/30 splits of the data, and average the results over 100 runs. Training sets are further partitioned 70/30 for validation purposes. We tune the following parameters by cross-validation: β, γ ∈ {10−7, . . . , 10−2} for SLLC, λITML ∈ {10−4, . . . , 104} for ITML, and λ ∈ {10−3, . . . , 102} for learning the linear classifiers, choosing the value offering the best accuracy. We choose R to be the entire training set, i.e., τ̂ = 1 (interestingly, cross-validation of τ̂ did not improve the results significantly). We take k = 3 and µ = 0.5 for LMNN, as done in (Weinberger & Saul, 2009). For ITML, we generate NT random constraints for a fair comparison with SLLC."
    }, {
      "heading" : "5.2. Results",
      "text" : "Classification performance We report the results obtained with the sparse linear classifier of Problem (1) suggested by Balcan et al. (2008) but also those obtained with 3-NN since LMNN and ITML are designed\n3KI , LMNN and ITML are normalized to ensure their values belong to [−1, 1].\n4Note that the amount of variance captured thereby was greater than 90% for all datasets.\nfor k-NN use.5 In linear classification (Table 2), SLLC achieves the highest accuracy on 5 out of 7 datasets and competitive performance on the remaining 2. At the same time, on all datasets, SLLC leads to extremely sparse classifiers. The sparsity of the classifier corresponds to the number of training examples that are involved in classifying a new example. Therefore, SLLC leads to much simpler and yet often more accurate classifiers than those built from other similarities. Furthermore, sparsity allows faster predictions, especially when data are plentiful and/or high-dimensional (e.g., Cod-RNA or Splice). Often enough, the learned linear classifier has sparsity 1, which means that classifying a new example boils down to computing its similarity score to a single training example and compare the value with a threshold. Note that we tried large values of λ to obtain sparser classifiers from KI , LMNN and ITML, but this yielded dramatic drops in accuracy. The extreme sparsity brought by SLLC comes from the fact that the constraints are based on an average of similarity scores over the same set of points for all training examples. Surprisingly, in 3-NN classification (Table 3), SLLC achieves the best results on 4 datasets. It is, however, outperformed by LMNN or ITML on the 3 biggest problems. For most tasks, the accuracy obtained in linear classification is better or similar to that obtained with 3-NN (highlighting the fact that similarity learning for linear classification is of interest) while prediction is many orders of magnitude faster due to the sparsity of the linear separators.\n5When necessary, we take the opposite value of a similarity to get a measure of distance, and vice versa.\nAlso note that a good similarity for k-NN classification can achieve poor results in linear classification (LMNN on Cod-RNA), and vice versa (SLLC on Svmguide1).\nRobustness to overfitting The fact that SLLC performs well on small datasets is partly due to its robustness to overfitting, highlighted in Figure 1. As expected, LMNN and ITML, which are optimized locally and plugged in a local nonlinear classifier, tend to overfit as the dimensionality grows. On the other hand, SLLC suffers from very limited overfitting due to its global and linear setting.\nTime comparison Note that SLLC is solved using the standard convex programming package Mosek while LMNN and ITML have their own specific and sophisticated solver. Despite this fact, SLLC is several orders of magnitude faster than LMNN (see Table 4) because its number of constraints is much smaller.\nHowever, it remains slower than ITML."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we presented SLLC, a novel approach to the problem of similarity learning by making use of both Balcan et al.’s theory of (ǫ, γ, τ)-good similarity functions and the KPCA trick. We derived a generalization bound based on the notion of uniform stability that is independent from the size of the input space, and thus from the number of dimensions selected by KPCA. It guarantees the goodness in generalization of the learned similarity, and therefore the accuracy of the resulting linear classifier for the considered task. We experimentally demonstrated the effectiveness of SLLC and also showed that the learned similarities induce extremely sparse classifiers. Combined with the independence from dimensionality and the robustness to overfitting, it makes the approach very efficient and suitable for high-dimensional data. Future work could include a “full” kernelization of SLLC (i.e., express the problem solely in terms of inner products), studying the influence of other regularizers on M (for instance, using the nuclear norm to learn low-rank matrices), developing a specific solver to match ITML’s speed and the derivation of an online algorithm."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to acknowledge support from the ANR LAMPADA 09-EMER-007-02 project and the PASCAL 2 Network of Excellence."
    } ],
    "references" : [ {
      "title" : "Improved Guarantees for Learning via Similarity Functions",
      "author" : [ "Balcan", "M.-F", "A. Blum", "N. Srebro" ],
      "venue" : "In COLT, pp",
      "citeRegEx" : "Balcan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2008
    }, {
      "title" : "A new kernelization framework for Mahalanobis distance learning",
      "author" : [ "R. Chatpatanasiri", "T. Korsrilabutr", "P. Tangchanachaianan", "B. Kijsirikul" ],
      "venue" : "algorithms. Neurocomputing,",
      "citeRegEx" : "Chatpatanasiri et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chatpatanasiri et al\\.",
      "year" : 2010
    }, {
      "title" : "An Online Algorithm for Large Scale Image Similarity Learning",
      "author" : [ "G. Chechik", "U. Shalit", "V. Sharma", "S. Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Chechik et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chechik et al\\.",
      "year" : 2009
    }, {
      "title" : "Information-theoretic metric learning",
      "author" : [ "J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Davis et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Davis et al\\.",
      "year" : 2007
    }, {
      "title" : "Online Metric Learning and Fast Similarity Search",
      "author" : [ "P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jain et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2008
    }, {
      "title" : "Similarity-based Learning via Data Driven Embeddings",
      "author" : [ "P. Kar", "P. Jain" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kar and Jain,? \\Q2011\\E",
      "shortCiteRegEx" : "Kar and Jain",
      "year" : 2011
    }, {
      "title" : "Generalized Cosine and Similarity Metrics: A supervised learning approach based on nearestneighbors",
      "author" : [ "A.M. Qamar" ],
      "venue" : "PhD thesis, University of Grenoble,",
      "citeRegEx" : "Qamar,? \\Q2010\\E",
      "shortCiteRegEx" : "Qamar",
      "year" : 2010
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "B. Schölkopf", "A. Smola", "Mller", "K.-R" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning a Distance Metric from Relative Comparisons",
      "author" : [ "M. Schultz", "T. Joachims" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Schultz and Joachims,? \\Q2003\\E",
      "shortCiteRegEx" : "Schultz and Joachims",
      "year" : 2003
    }, {
      "title" : "Online and batch learning of pseudo-metrics",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2004
    }, {
      "title" : "Distance Metric Learning for Large Margin",
      "author" : [ "K.Q. Weinberger", "L.K. Saul" ],
      "venue" : "Nearest Neighbor Classification. JMLR,",
      "citeRegEx" : "Weinberger and Saul,? \\Q2009\\E",
      "shortCiteRegEx" : "Weinberger and Saul",
      "year" : 2009
    }, {
      "title" : "Sparse Metric Learning via Smooth Optimization",
      "author" : [ "Y. Ying", "K. Huang", "C. Campbell" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ying et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ying et al\\.",
      "year" : 2009
    }, {
      "title" : "1-norm Support Vector Machines",
      "author" : [ "J. Zhu", "S. Rosset", "T. Hastie", "R. Tibshirani" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well.",
      "startOffset" : 51,
      "endOffset" : 189
    }, {
      "referenceID" : 3,
      "context" : "Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well.",
      "startOffset" : 51,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well.",
      "startOffset" : 51,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : "Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well.",
      "startOffset" : 51,
      "endOffset" : 189
    }, {
      "referenceID" : 2,
      "context" : "Some work has also gone into learning arbitrary similarity functions with no PSD constraint to make the problem easier to solve (Chechik et al., 2009; Qamar, 2010).",
      "startOffset" : 128,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "Some work has also gone into learning arbitrary similarity functions with no PSD constraint to make the problem easier to solve (Chechik et al., 2009; Qamar, 2010).",
      "startOffset" : 128,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "Recently, Balcan et al. (2008) introduced the formal notion of (ǫ, γ, τ)-good similarity function, which does not require positive semi-definiteness and is less restrictive than local pair-based constraints.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, by using the Kernel Principal Component Analysis (KPCA) trick (Chatpatanasiri et al., 2010), we are able to kernelize our algorithm and thereby learn more powerful similarity functions and classifiers in the nonlinear feature space induced by a kernel.",
      "startOffset" : 75,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "There also exist purely online methods (Shalev-Shwartz et al., 2004; Jain et al., 2008).",
      "startOffset" : 39,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "There also exist purely online methods (Shalev-Shwartz et al., 2004; Jain et al., 2008).",
      "startOffset" : 39,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "There has also been some interest in learning arbitrary similarity functions with no PSD requirement (Chechik et al., 2009; Qamar, 2010).",
      "startOffset" : 101,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "There has also been some interest in learning arbitrary similarity functions with no PSD requirement (Chechik et al., 2009; Qamar, 2010).",
      "startOffset" : 101,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "Davis et al. (2007) regularize using the LogDet divergence (for its automatic enforcement of PSD) while Ying et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "Davis et al. (2007) regularize using the LogDet divergence (for its automatic enforcement of PSD) while Ying et al. (2009) use the (2,1)-norm (which favors a low-rank M).",
      "startOffset" : 0,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "In recent work, Balcan et al. (2008) introduced a new theory of learning with good similarity functions, based on the following definition.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Definition 1 (Balcan et al., 2008) A similarity function K is an (ǫ, γ, τ)-good similarity function in hinge loss for a learning problem P if there exists a (random) indicator function R(x) defining a (probabilistic) set of “reasonable points” such that the",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "Theorem 1 (Balcan et al., 2008) Let K be an (ǫ, γ, τ)-good similarity function in hinge loss for a learning problem P .",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "Note that Problem (1) is essentially an L1-regularized linear SVM (Zhu et al., 2003) with an empirical similarity map (Balcan et al.",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : ", 2003) with an empirical similarity map (Balcan et al., 2008), and can be efficiently The original formulation proposed by Balcan et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : ", 2003) with an empirical similarity map (Balcan et al., 2008), and can be efficiently The original formulation proposed by Balcan et al. (2008) was actually L1-constrained.",
      "startOffset" : 42,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "This form of similarity function was successfully used in the context of large-scale online image similarity learning (Chechik et al., 2009).",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "Kernelization allows linear classifiers such as Support Vector Machines or some Mahalanobis distance learning algorithms (e.g., Shalev-Shwartz et al., 2004; Davis et al., 2007) to learn nonlinear decision boundaries or transformations.",
      "startOffset" : 121,
      "endOffset" : 176
    }, {
      "referenceID" : 7,
      "context" : "The idea is to use Kernel Principal Component Analysis (Schölkopf et al., 1998) to project the data into a new space using a nonlinear kernel function, and to keep only a chosen number of dimensions (those that capture best the overall variance of the data).",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "For these reasons, we instead use the KPCA trick, recently proposed by Chatpatanasiri et al. (2010). It provides a straightforward way to kernelize a metric learning algorithm while performing dimensionality reduction at no additional cost.",
      "startOffset" : 71,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "For these reasons, we instead use the KPCA trick, recently proposed by Chatpatanasiri et al. (2010). It provides a straightforward way to kernelize a metric learning algorithm while performing dimensionality reduction at no additional cost. The idea is to use Kernel Principal Component Analysis (Schölkopf et al., 1998) to project the data into a new space using a nonlinear kernel function, and to keep only a chosen number of dimensions (those that capture best the overall variance of the data). The data are then projected into this new feature space, and the (unchanged) metric learning algorithm can be used to learn a metric in that space. Chatpatanasiri et al. (2010) showed that the KPCA trick is theoretically sound for unconstrained metric and similarity learning algorithms (they proved representer theorems), which includes SLLC.",
      "startOffset" : 71,
      "endOffset" : 677
    }, {
      "referenceID" : 3,
      "context" : "We propose a comparative study of our method and two widely-used Mahalanobis distance learning algorithms: Large Margin Nearest Neighbor (LMNN) from Weinberger & Saul (2009) and Information-Theoretic Metric Learning (ITML) from Davis et al. (2007). Roughly speaking, LMNN optimizes the k-NN error on the training set (with a safety margin) whereas ITML aims at best satisfying pair-based constraints while We used the code provided by the authors.",
      "startOffset" : 228,
      "endOffset" : 248
    }, {
      "referenceID" : 0,
      "context" : "Classification performance We report the results obtained with the sparse linear classifier of Problem (1) suggested by Balcan et al. (2008) but also those obtained with 3-NN since LMNN and ITML are designed KI , LMNN and ITML are normalized to ensure their values belong to [−1, 1].",
      "startOffset" : 120,
      "endOffset" : 141
    } ],
    "year" : 2012,
    "abstractText" : "In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of (ǫ, γ, τ)-good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to stateof-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.",
    "creator" : "LaTeX with hyperref package"
  }
}