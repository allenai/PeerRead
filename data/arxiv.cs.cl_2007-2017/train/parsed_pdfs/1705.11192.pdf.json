{
  "name" : "1705.11192.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols",
    "authors" : [ "Serhii Havrylov" ],
    "emails" : [ "s.havrylov@uva.nl", "ititov@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "With the rapid advances in machine learning in recent years, the goal of enabling intelligent agents to communicate with each other and with humans is turning from a hot topic of philosophical debates into a practical engineering problem. It is believed that supervised learning alone is not going to provide a solution to this challenge (Mikolov et al., 2015). Moreover, even learning natural language from an interaction between humans and an agent may not be the most efficient and scalable approach. These, as well as other considerations (e.g., achieving a better understanding of principles guiding evolution and emergence of natural languages), have motivated previous research into setups where agents invent a communication protocol which lets them succeed in a given collaborative task (see (Kirby, 2002; Wagner et al., 2003) for an overview of earlier work).\nWe continue this line of research and specifically consider a setting where the collaborative task is a game. Neural network models have been shown to be able to successfully induce a communication protocol for this setting (Lazaridou et al., 2016; Jorge et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016). One important difference with these previous approaches is that we assume that messages exchanged between the agents are variable-length strings of symbols rather than atomic categories (as in the previous work). Our protocol would be more similar to natural language and, as such, would have many advantages over using atomic categories. For example, it can support compositionality (Werning et al., 2011) and provide an easy way to regulate the amount of information passed in a message. Interestingly, in our experiments, we also find that agents develop a protocol faster when we allow them to use longer sequences of symbols. Somewhat surprisingly, we observe that the language derived by our method favours multiple encodings of the same information,\nSubmitted to NIPS 2017. The extended abstract was presented at ICLR 2017 workshop track.\nar X\niv :1\n70 5.\n11 19\n2v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\nreminiscent of synonyms or paraphrases in natural languages. Moreover, with messages being strings of symbols (i.e. words), it is now possible to inject supervision to ensure that the invented protocol is close enough to a natural language and, thus, potentially interpretable by humans.\nIn our experiments, we focus on a referential game, where the goal for one agent is to explain which image the other agent should select. Our setting can be formulated as follows:\n1. There is a collection of images {in}Nn=1 from which target image t is sampled as well as K distracting images {dk}Kk=1. 2. There are two agents: a sender Sφ and a receiver Rθ. 3. After seeing a target image t, the sender has to come up with a message mt, which is\nrepresented by a sequence of symbols from the vocabulary V of a size |V |. The maximum possible length of a sequence is L.\n4. Given a message mt and a set of images, which consists of distracting images and a target image, the goal of the receiver is to identify the target image correctly.\nThis setting is inspired by Lazaridou et al. (2016) but there are important differences: for example, we use sequences rather than single symbols, and our sender, unlike theirs, does not have access to distracting images. This makes our setting both arguably more realistic and more challenging from the learning perspective.\nGenerating message mt requires sampling from categorical distributions over vocabulary, which makes backpropagating the error through the message impossible. It is tempting to formulate this game as a reinforcement learning problem. However, the number of possible messages1 is proportional to |V |L. Therefore, it seems unlikely that such model can be learned with naïve Monte Carlo methods, as the variance of estimated gradients, would be very high. Also, in this setup, the receiver Rθ would correspond to the non-stationary environment in which sender Sφ acts, making the learning problem even more challenging. Instead, we propose an effective approach, where we use straight-through Gumbel-softmax estimators (Jang et al., 2016; Bengio et al., 2013) allowing for end-to-end differentiation, despite using only discrete messages in training. We demonstrate that this approach is much more effective than the reinforcement learning framework employed in previous approaches to referential games, both in terms of convergence times and the resulting communication success.\nOur main contributions can be summarized as follows:\n• we are the first to show that structured protocols (i.e. strings of symbols) can be induced from scratch by optimizing reward in collaborative tasks;\n• we demonstrate that relaxations based on straight-through estimators are more effective than reinforcement learning for our task;\n• we show that the induced protocol shares some properties with natural languages."
    }, {
      "heading" : "2 Model",
      "text" : ""
    }, {
      "heading" : "2.1 Agents’ architectures",
      "text" : "The sender and the receiver are implemented as LSTM networks (Hochreiter and Schmidhuber, 1997). Figure 1 shows the sketch of model architecture, where diamond-shaped, dashed and solid arrows represent sampling, copying and deterministic functions respectively. The inputs to the sender are target image t and the special token <S> that denotes the start of a message. Given these inputs, the sender generates next token wi in a sequence by sampling from the categorical distribution Cat(pti) where p t i = softmax(Wh s i + b). Here, h s i is the hidden state of sender’s LSTM and can be calculated as2 hsi = LSTM(h s i−1, wi−1). In the first time step we have h s 0 = η(f(t)), where η(·) is an affine transformation of image features f(·) extracted from a convolutional neural network (CNN). Message mt is obtained by sequentially sampling until the maximum possible length L is reached or the special token <S> is generated.\n1In our experiments |V | = 10000 and L is up to 14. 2We omitted the cell state in the equation for brevity.\nThe inputs to the receiver are the generated message mt and a set of images that contain the target image t and distracting images {dk}Kk=1. Receiver interpretation of the message is given by the affine transformation g(·) of the last hidden state hrl of the LSTM network that reads the message. The loss function for the whole system can be written as:\nLφ,θ(t) = Emt∼pφ(·|t) [ K∑ k=1 max[0, 1− f(t)T g(hrl ) + f(dk)T g(hrl )] ] (1)\nThe energy functionE(v,mt) = −f(v)T g(hrl (mt)) can be used to define the probability distribution over a set of images p(v|mt) ∝ e−E(v,mt). Communication between two agents is successful if the target image has the highest probability according to this distribution."
    }, {
      "heading" : "2.2 Learning",
      "text" : "It is relatively easy to learn the receiver agent. It is end-to-end differentiable, so gradients of the loss function with respect to its parameters can be estimated efficiently. The receiver-type model was investigated before by Chrupała et al. (2015) and known as Imaginet. It was used to learn visually grounded representations of language from coupled textual and visual input. The real challenge is to learn the sender agent. Its computational graph contains sampling, which makes it nondifferentiable. In what follows in this section, we discuss methods for estimating gradients of the loss function in Equation (1)."
    }, {
      "heading" : "2.2.1 REINFORCE",
      "text" : "REINFORCE is a likelihood-ratio method (Williams, 1992) that provides a simple way of estimating gradients of the loss function with respect to parameters of the stochastic policy. We are interested in optimizing the loss function from Equation (1). The REINFORCE algorithm enables the use of gradient-based optimization methods by estimating gradients as:\n∂Lφ,θ ∂φ = Epφ(·|t) [ lt ∂log pφ(mt|t) ∂φ ] (2)\nWhere lt is a learning signal, an inner part of the expectation in Equation (1). However, computing the gradient precisely may not be feasible due to the enormous number of message configurations. Usually, a Monte Carlo approximation of the expectation is used. Training models with REINFORCE can be difficult, due to the high variance of the estimator. We observed more reliable learning when using stabilizing techniques proposed by Mnih and Gregor (2014). Namely, we use a baseline, defined as a moving average of the reward, to control variance of the estimator; this results in centering the learning signal lt. We also use a variance-based adaptation of the learning rate that consists of dividing the learning rate by a running estimate of the reward standard deviation. This trick ensures that the learning signal is approximately unit variance, making the learning process less sensitive to dramatic and non-monotonic changes in the centered learning signal."
    }, {
      "heading" : "2.2.2 Gumbel-softmax estimator",
      "text" : "In the typical RL task formulation, an acting agent does not have access to the complete environment specification, or, even if it does, the environment is non-differentiable. In our setup, an agent that was trained by any REINFORCE-like algorithm would hugely underuse available information about the environment. As a solution, we consider replacement of one-hot encoded symbols w ∈ V sampled from a categorical distribution with a continuous relaxation w̃ obtained from the Gumbel-softmax distribution (Jang et al., 2016; Maddison et al., 2016).\nConsider a categorical distribution with event probabilities p1, p2, ..., pK , the Gumbel-softmax trick proceeds as follows: obtain K samples {uk}Kk=1 from uniformly distributed variable u ∼ U(0, 1), transform each sample with function gk = − log (− log (uk)) to get samples from Gumbel distribution, then compute a continuous relaxation:\nw̃k = exp ((log pk + gk)/τ)∑K i=1 exp ((log pi + gi)/τ)\n(3)\nWhere τ is the temperature that controls accuracy of the approximation argmax with softmax function. As the temperature τ is approaching 0, samples from the Gumbel-softmax distribution are becoming one-hot encoded, and the Gumbel-softmax distribution starts to be identical to the categorical distribution (Jang et al., 2016).\nAs a result of this relaxation, the game becomes completely differentiable and can be trained using the backpropagation algorithm. However, communicating with real values allows the sender to encode much more information into a message compared to using a discrete one and is unrealistic if our ultimate goal is communication in natural language. Also, due to the recurrent nature of the receiver agent, using discrete tokens during test time can lead to completely different dynamics compared to the training time. This manifests itself in a huge gap between training and testing performance (up to 20% of communication success in our experiments)."
    }, {
      "heading" : "2.2.3 Straight-through Gumbel-softmax estimator",
      "text" : "To prevent the issues mentioned above, we discretize w̃ back with argmax in the forward pass that then becomes an ordinary sample from the original categorical distribution. Nevertheless, we use continuous relaxation in the backward pass, effectively assuming ∂L∂w ≈ ∂L ∂w̃ . This biased estimator is known as the straight-through Gumbel-softmax (ST-GS) estimator (Jang et al., 2016; Bengio et al., 2013). As a result of applying this trick, there is no difference in message usage during training and testing stages, which contrasts with previous differentiable frameworks for learning communication protocols (Foerster et al., 2016).\nBecause of using ST-GS, the forward pass does not depend on the temperature. However, it still affects the gradient values during the backward pass. As discussed before, low values for τ provide better approximations of argmax. Because the derivative of argmax is 0 everywhere except at the boundary of state changes, a more accurate approximation would lead to the severe vanishing gradient problem. Nonetheless, with ST-GS we can afford to use large values for τ , which would usually lead to faster learning. To reduce the burden of performing extensive hyperparameter search for the temperature, similarly to Gulcehre et al. (2017), we consider learning the inverse-temperature with a multilayer perceptron:\n1\nτ(hsi ) = log(1 + exp(wTτ h s i )) + τ0, (4)\nwhere τ0 controls maximum possible value for the temperature. In our experiments, we found that learning process is not very sensitive to the hyperparameter as long as τ0 is less than 1.0."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Tabula rasa communication",
      "text" : "We used the Microsoft COCO dataset (Chen et al., 2015) as a source of images. Prior to training, we randomly selected 10% of the images from the MSCOCO 2014 training set as validation data\nand kept the rest as training data. As a result of this split, more than 74k images were used for training and more than 8k images for validation. To evaluate the learned communication protocol, we used the MSCOCO 2014 validation set that consists of more than 40k images. In our experiments images are represented by outputs of the relu7 layer from the pretrained 16-layer VGG convolutional network (Simonyan and Zisserman, 2014).\nWe set the following hyperparameters without tuning: the embedding dimensionality is 256, the dimensionality of LSTM layers is 512, the vocabulary size is 10000, the number of distracting images is 127, the batch size is 128. We used Adam (Kingma and Ba, 2014) as an optimizer, with default hyperparameters and the learning rate of 0.001. To investigate benefits of learning temperature, first, we found the optimal temperature that is equal to 1.2 by performing a search over interval [0.5; 2.0] with the step size equal to 0.1. As we mentioned before, the learning process with temperature defined by Equation (4) is not very sensitive to τ0 hyperparameter. Nevertheless, we conducted hyperparameter search over interval [0.0; 2.0] with step size 0.1 and found that model τ0 = 0.2 has the best performance. The differences in the performance were not significant unless the τ0 was bigger than 1.0.\nWe tested two encoding strategies: greedy argmax and sampling. Figure 2 shows the communication success rate as a function of the maximum message length L. Because results for models with learned temperature are very similar to the counterparts with fixed (manually tuned) temperatures, we omitted them from the figure for clarity. However, in average, models with learned temperatures outperform vanilla versions by 0.8%. As expected, argmax encoding slightly but consistently outperforms the sampling strategy. Surprisingly, REINFORCE beats GS-ST for the setup with L = 1. We may speculate that in this relatively easy setting being unbiased (as REINFORCE) is more important than having a low variance (as GS-ST).\nInterestingly, the number of updates that are required to achieve training convergence with the GS-ST estimator decreases when we let the sender use longer messages (i.e. for larger L). This behaviour is slightly surprising as one could expect that it is harder to learn the protocol when the space of messages is larger. In other words, using longer sequences helps to learn a communication protocol faster. However, this is not at all the case for the REINFORCE estimator: it usually takes x5 more updates to converge compared to GS-ST, and also there is no clear dependency between the number of updates needed to converge and the maximum possible length of a message.\nWe also plot the perplexity of the encoder, it is relatively high and increasing with sentence length for GS-ST, whereas for REINFORCE the perplexity increase is not as rapid. This implies redundancy in the encodings: there exist multiple paraphrases that encode the same semantic content. A noteworthy feature of GS-ST with learned temperature is that perplexity values of all encoders for different L are always smaller than corresponding values for vanilla GS-ST."
    }, {
      "heading" : "3.2 Qualitative analysis of the learned language",
      "text" : "To better understand the nature of the learned language, we inspected a small subset of sentences that were produced by the model with maximum possible message length equal to 5. Figure 3 shows\nsome samples from the MSCOCO 2014 validation set that correspond to (5747 * * * *) code.3 Images in this subset depict animals. On the other hand, it seems that images for (* * * 5747 *) code do not correspond to any predefined category. This suggests that word order is crucial in the developed language. Particularly, word 5747 on the first position encodes presence of an animal in the image. The same figure shows that message (5747 5747 7125 * *) corresponds to a particular type of bears. This suggests that developed language implements some kind of hierarchical coding. This is interesting by itself because the model was not constrained explicitly to use any hierarchical encoding scheme. The model shows similar behaviour for images in the food domain. It is important to mention that these results are not cherry picked. In the case of the food domain, we took a random photo of a pizza and generated a message. Then we iterated over the dataset and randomly selected images with messages that share prefixes of 1, 2 and 3 symbols with the given message. The same methodology was used for the animal domain."
    }, {
      "heading" : "3.3 Indirect grounding of artificial language in natural language",
      "text" : "As the ultimate goal is to ensure that communication is accomplished with a language that is understandable by humans, we should favour protocols that resemble, in some respect, a natural language. One possible solution is to use the Kullback-Leibler (KL) divergence regularization DKL (qφ(m|t)‖pNL(m)), from the natural language to the learned protocol. As we do not have access to pNL(m), we train a language model pω using available samples (i.e. texts) and approximate the original KL divergence with DKL (qφ(m|t)‖pω(m)). This regularization provides indirect supervision by encouraging generated messages to have a high probability in natural language but at the same time maintaining high entropy for the communication protocol. Note that this is a weak form of grounding, as it does not force agents to preserve ‘meanings’ of words: the same word can refer to a very different concept in the induced artificial language and in the natural language.\nWe implemented the language model pω(m) by using an LSTM recurrent neural network. To acquire data for estimating the parameters of a language model, we took image captions of randomly selected (50%) images from the previously created training set. These images were not used for training the sender and the receiver. Another half of the set was used for training agents. We evaluated the learned communication protocol on the MSCOCO 2014 validation set.\nTo get an estimate of communication success when using natural language, we trained the receiver with pairs of images and captions. This model is similar to Imaginet (Chrupała et al., 2015). Also, inspired by their analysis, we report the omission score. The omission score of a word is equal to difference between the target image probability given the original message and the probability given a message with the removed word. The sentence omission score is the maximum over all word omission scores in the given sentence. The score quantifies the change in the target image probability after removing the most important word. Natural languages have content words that name objects (i.e. nouns) and encode their qualities (e.g., adjectives). One can expect that a protocol that uses a\n3* means any word from the vocabulary or end-of-sentence passing.\ndistinction between content words and function words would have a higher omission score than a protocol that distributes information evenly across tokens. As Table 1 shows, the grounded language has the communication success rate similar to natural language. However, it has a slightly lower omission score. The unregularized model has the lowest omission score which probably means that symbols in the developed protocol have similar nature to characters or syllables rather than words."
    }, {
      "heading" : "3.4 Variational Autoencoder interpretation",
      "text" : "The described indirect grounding of the artificial language in a natural language can be interpreted as a particular instantiation of a variational autoencoder (VAE) (Kingma and Welling, 2013). There are no gold standard messages for images. Thus, a message can be treated as a variable-length sequence of discrete latent variables. On the other hand, image representations are always given. Hence they are equivalent to the observed variable in the VAE framework. The trained language model pω(m) serves as a prior over latent variables. The receiver agent is analogous to the generative part of the VAE, although, it uses a slightly different loss for the reconstruction error (hinge loss instead of log-likelihood). The sender agent is equivalent to an inference network used to approximate the posteriors in VAEs."
    }, {
      "heading" : "3.5 Direct grounding of artificial language in natural language",
      "text" : "Minimizing the KL divergence from the natural language to the learned protocol favors generated messages both to have a high probability according to the distribution of natural language and have a high entropy. The goal is to ensure that statistical properties of the protocol are similar to those of natural language. However, words are not likely to preserve their original meaning (e.g. the word ‘red’ may not refer to ‘red’ in the protocol). To address this, we considered a more direct form of supervision, additionally training the sender on the image captioning task (Vinyals et al., 2015).\nTo simulate the semi-supervised setup, we divided the previously created training set into two parts. The randomly selected 25% of the dataset were used to train the sender on the image captioning task Lcaption. The rest 75% were used to train the sender and the receiver to solve the referential game Lgame. The final loss is a weighted sum of losses for the two tasks L = Lcaption + λLgame. We did not perform any preprocessing of the gold standard captions apart from lowercasing. It is important to mention that in this setup the communication loss is equivalent to the variational lower bound of mutual information (Barber and Agakov, 2003) of image features and the corresponding caption.\nWe used the greedy decoding strategy to sample image descriptions. As Table 2 shows, both systems have comparable performance across different image captioning metrics. We believe that the model did not achieve better peroformance as discriminative captions are different in nature compared to reference captions. In fact generating discriminative descriptions may be useful for certain applications (e.g., generating reference expressions in navigation instructions (Byron et al., 2009)) but is hard to evaluate them intrinsically. Note that using the communication loss yield, in average, longer captions. It is not surprising, taking into account the mutual information interpretation of the referential game, a longer sequence can retain more information about image features."
    }, {
      "heading" : "4 ST-GS estimator as a pseudogradient estimator",
      "text" : "The direction δ is a pseudogradient of J(u) if the condition δT∇J(u) > 0 is satisfied. Polyak and Tsypkin (1973) have shown that, given certain assumptions about the learning rate, a very broad class of pseudogradient methods converge to the critical point of function J .\nTo examine whether the direction provided by ST-GS is a pseudogradient, we used a stochastic perturbation gradient estimator that can approximate a dot product between arbitrary direction δ in the parameter space and the true gradient:\nJ(u+ δ)− J(u− δ) 2 = δT∇J(u) +O( 2) (5)\nIn our case J(u) is a Monte Carlo approximation of Equation (1). To reduce the variance in dot product estimation (Bhatnagar et al., 2012), the same Gumbel noise samples were used for evaluating forward and backward perturbations of J(u). By observing the estimated sign of the dot product, we found that after 400 parameter updates there is almost always (> 99%) an acute angle between the direction provided by GS-ST and the true gradient. This suggests that GS-ST gradient can be used as a pseudogradient for our referential game problem."
    }, {
      "heading" : "5 Related work",
      "text" : "There is a long history of work on language emergence in multi-agent systems (Kirby, 2002; Wagner et al., 2003; Steels, 2005; Nolfi and Mirolli, 2009; Golland et al., 2010). The recent generation relied on deep learning techniques. More specifically, Foerster et al. (2016) proposed a differentiable inter-agent learning (DIAL) framework where it was used to solve puzzles in a multi-agent setting. The agents in their work were allowed to communicate by sending one-bit messages. Jorge et al. (2016) adopted DIAL to solve the interactive image search task with two agents participating in the task. These actors successfully developed a language consisting of one-hot encoded atomic symbols. By contrast, Lazaridou et al. (2016) applied the policy gradient method to learn agents that are involved in a referential game. Unlike us, they used atomic symbols rather than sequences of tokens.\nThe idea of using the Gumbel-softmax distribution for learning language in a multi-agent environment was concurrently considered by Mordatch and Abbeel (2017). They studied a simulated two-dimensional environment in continuous space and discrete time with several agents where, in addition to performing physical actions, agents can also utter verbal communication symbols at every timestep. Similarly to us, the induced language exhibits compositional structure and to a large degree interpretable. Das et al. (2017), also in concurrent work, investigated a cooperative ‘image guessing’ game with two agents communicating in natural language. They use the policy gradient method for learning, hence their framework can benefit from the approach proposed in this paper. The important difference with our approach is that they pretrain model on an available dialog dataset. By contrast, we induce the communication protocol from scratch.\nVAE-based approaches that use sequences of discrete latent variables were studied recently by Miao and Blunsom (2016) and Kočiskỳ et al. (2016) for text summarization and semantic parsing, correspondingly. The variational lower bound for these models involves expectation over sequences of symbols, so the learning strategy proposed here may have a positive impact in their applications."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have shown that agents, modeled using neural networks, can successfully invent a language that consists of sequences of discrete tokens. Despite the common belief that it is hard to train such models, we proposed an efficient learning strategy that relies on the straight-through Gumbel-softmax estimator. We have performed analysis of the learned language and corresponding learning dynamics. We have also considered two methods for injecting prior knowledge about natural language. In the future work, we would like to extend this approach to modeling practical dialog systems and, thus, reduce the amount of necessary human supervision."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This project is supported by SAP ICN and ERC Starting Grant BroadSem (678254). We would like to thank Jelle Zuidema for helpful comments."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Learning to communicate through interaction, rather than relying on explicit super-<lb>vision, is often considered a prerequisite for developing a general AI. We study a<lb>setting where two agents engage in playing a referential game and, from scratch,<lb>develop a communication protocol necessary to succeed in this game. Unlike<lb>previous work, we require that messages they exchange, both at train and test time,<lb>are in the form of a language (i.e. sequences of discrete symbols). We compare a<lb>reinforcement learning approach and one using a differentiable relaxation (straight-<lb>through Gumbel-softmax estimator (Jang et al., 2016)) and observe that the latter is<lb>much faster to converge and it results in more effective protocols. Interestingly, we<lb>also observe that the protocol we induce by optimizing the communication success<lb>exhibits a degree of compositionality and variability (i.e. the same information can<lb>be phrased in different ways), both properties characteristic of natural languages.<lb>As the ultimate goal is to ensure that communication is accomplished in natural<lb>language, we also perform experiments where we inject prior information about<lb>natural language into our model and study properties of the resulting protocol.",
    "creator" : "LaTeX with hyperref package"
  }
}