{"conference": "ICLR 2017 conference submission", "title": "Chess Game Concepts Emerge under Weak Supervision: A Case Study of Tic-tac-toe", "abstract": "This paper explores the possibility of learning chess game concepts under weak supervision with convolutional neural networks, which is a topic that has not been visited to the best of our knowledge. We put this task in three different backgrounds: (1) deep reinforcement learning has shown an amazing capability to learn a mapping from visual inputs to most rewarding actions, without knowing the concepts of a video game. But how could we confirm that the network understands these concepts or it just does not? (2) cross-modal supervision for visual representation learning draws much attention recently. Is this methodology still applicable when it comes to the domain of game concepts and actions? (3) class activation mapping is widely recognized as a visualization technique to help us understand what a network has learnt. Is it possible for it to activate at non-salient regions? With the simplest chess game tic-tac-toe, we report interesting results as answers to those three questions mentioned above. All codes, pre-processed datasets and pre-trained models will be released.", "reviews": [{"is_meta_review": true, "comments": "Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Unclear ", "is_meta_review": false, "comments": "Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. \n\nAuthors claim that this:\n1. is a very interesting finding. \n2. CNN has figured out game rules. \n3. Cross modal supervision is applicable to higher-level semantics. \n\nI don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only \"one\" stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. \n\nFor (3), author's definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It's unclear what points authors are trying to make. \n\nFor (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of \"what do do\" and \"what will happen\". They claim by supervising for \"what will happen\", the CNN can automatically learn about \"what to do\". This is extensively studied in the model predictive control literature. Where model is \"what will happen next\", and the model is used to infer a control law - \"what to do\". However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. \n\nFor further analysis of what the CNN has learnt I would recommend:\n(a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning).\n\n(b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of \"generalizable\" features the CNN pays attention to. \n\nIn summary, understanding why CNN's make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above). \n\n\n \n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Novel experiments, but the results and significance are not clear", "is_meta_review": false, "comments": "Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Still not sure what to take away from these experiments", "is_meta_review": false, "comments": "1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying \"If you place a black square in the middle right, black will win\" or \"if you place a white square in the upper left, white will win\". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.\n\nThe focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more clear about what each class is). So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited. The focus ends up being on the empty bottom right square (because certainly you can't exhibit class 1 if the bottom right square is occupied). The CNN also needs to condition its decision on other parts of the board -- it needs to know whether there will be 3 in a row from some direction. But maybe that conditioning is weaker?\n\nThat's kind of interesting but I'm not sure about the deeper statements about discovering game rules that the paper hints at. I'm also not sure about the connection of this work to weakly supervised learning or multi-modal learning.\n\nThe paper is pretty well written, overall, with some grammatical mistakes, but I simply don't see the surprising discovery of this work. \n\nI also have some concerns about how contrived this scenario is -- using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method.\n\nI am not an expert in reinforcement learning (which isn't happening in this paper, but is in related works on CNN game playing), so maybe I'm not appreciating the paper appropriately.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"TITLE": "Relation to AlphaGo", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "11 Dec 2016", "is_meta_review": false}, {"TITLE": "Regarding interpretation of results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "10 Dec 2016", "is_meta_review": false}, {"TITLE": "Initial Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}], "SCORE": 3, "authors": "Hao Zhao, Ming Lu, Anbang Yao, Yurong Chen, Li Zhang", "KEYWORDS": "investigating whether a CNN understands concepts from a new perspective", "accepted": false, "id": ""}
