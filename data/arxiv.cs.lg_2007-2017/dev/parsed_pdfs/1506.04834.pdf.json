{
  "name" : "1506.04834.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tree-structured composition in neural networks without tree-structured architectures",
    "authors" : [ "Samuel R. Bowman", "Christopher D. Manning", "Christopher Potts" ],
    "emails" : [ "sbowman@stanford.edu", "manning@stanford.edu", "cgpotts@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural networks that encode sentences as realvalued vectors have been successfully used in a wide array of NLP tasks, including translation (Sutskever et al., 2014), parsing (Dyer et al., 2015), and sentiment analysis (Tai et al., 2015). These models are generally either sequence models based on recurrent neural networks, which build representations incrementally from left to right (Elman, 1990; Sutskever et al., 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al., 2011).\nWhile both model classes perform well on many tasks, and both are under active development, tree models are often presented as the more principled choice, since they align with standard linguistic assumptions about constituent structure and\nthe compositional derivation of complex meanings. Nevertheless, tree models have not shown the kinds of dramatic performance improvements over sequence models that their billing would lead one to expect: head-to-head comparisons with sequence models show either modest improvements (Tai et al., 2015) or none at all (Li et al., 2015).\nWe propose a possible explanation for these results: standard sequence models can learn to exploit recursive syntactic structure in generating sentence meaning representations, thereby learning to use the structure that tree-structured models are explicitly designed around. This first requires that sequence models are able to identify syntactic structure in natural language. We believe this is plausible, on the basis of other recent research (Vinyals et al., 2014; Karpathy et al., 2015). In this paper, we evaluate whether LSTM models are able to use that structure to guide interpretation, focusing on cases where the relevant syntactic structure is clearly indicated in the data.\nWe compare standard tree and sequence models on their handling of recursive structure by training the models on sentences whose length and recursion depth are limited, and then testing them on longer and more complex sentences, such that only models that exploit the recursive structure will be able to generalize in a way that yields correct interpretations for these test sentences. Our methods are based on Bowman et al. (2015), in which we describe an experiment and corresponding artificial dataset which tests this ability in two tree models. We adapt that experiment here to sequence models by decorating the statements with an explicit bracketing, and we use this design to compare an LSTM sequence model with three tree models, with a focus on what data each model needs in order to learn the needed generalizations.\nAs in Bowman et al., we find that standard tree neural networks are able to make the necessary generalizations, with their performance decaying\nar X\niv :1\n50 6.\n04 83\n4v 1\n[ cs\n.C L\n] 1\n6 Ju\nn 20\n15\ngradually as the structures in the test set grow in size. We additionally find that extending the training set to include larger structures mitigates this decay. Then considering sequence models, we find that a single-layer LSTM is also able to generalize to unseen large structures, but that it does this only when trained on a larger and more complex training set than is needed by the tree models.\nOur results engage with those of Vinyals et al. (2014) and Dyer et al. (2015), who find that sequence models can learn to recognize syntactic structure in natural language, at least when trained on explicitly syntactic tasks. The simplest model presented in Vinyals et al. uses an LSTM sequence model to encode each sentence as a vector, and then generates a linearized parse (a sequence of brackets and constituent labels) with high accuracy using only the information present in the vector. This shows that the LSTM is able to identify the correct syntactic structures and also hints that it is able to develop a generalizable method for encoding these structures in vectors. However, the massive size of the dataset needed to train the model—250M tokens—leaves open the possibility that it primarily learns to generate only tree structures that it has already seen, representing them as simple hashes—which would not capture unseen tree structures—rather than as structured objects. Our experiments, though, show that LSTMs can learn to understand tree structures when given enough data, suggesting that there is no fundamental obstacle to learning this kind of structured representation."
    }, {
      "heading" : "2 Recursive structure in artificial data",
      "text" : "Reasoning about entailment The data that we use define a version of the recognizing textual entailment task, in which the goal is to determine what kind of logical consequence relation holds between two sentences, drawing on a small fixed vocabulary of relations such as entailment, contradiction, and synonymy. This task is well suited to evaluating neural network models for sentence interpretation: models must develop comprehensive representations of the meanings of each sentence to do well at the task, but the data do not force these representations to take a specific form, allowing the model to learn whatever kind of representations it can use most effectively.\nThe data we use are labeled with the seven mutually exclusive logical relations of MacCartney\nand Manning (2009), which distinguish entailment in two directions (@, A), equivalence (≡), exhaustive and non-exhaustive contradiction (∧, |), and two types of semantic independence (#, `).\nThe artificial language The language described in Bowman et al. (§4) is designed to highlight the use of recursive structure with minimal additional complexity. Its vocabulary consists only of six unanalyzed word types (p1, p2, p3, p4, p5, p6), and, or, and not. Sentences of the language can be straightforwardly interpreted as statements of propositional logic (where the six unanalyzed words are variables), and labeled sentence pairs can be interpreted as theorems of that logic. Some example pairs are provided in Table 1.\nCrucially, the language is defined such that any sentence can be embedded under negation or conjunction to create a new sentence, allowing for arbitrary-depth recursion, and the scope of negation and conjunction are determined only by bracketing with parentheses (rather than bare word order). The compositional structure of each sentence can thus be an arbitrary tree, and interpreting a sentence correctly requires using that structure.\nThe data come with parentheses representing a complete binary bracketing. Our models use this information in two ways. For the tree models, the parentheses are not word tokens, but rather used in the expected way to build the tree. For the sequence model, the parentheses are words with associated learned embeddings. This provides the models with equivalent data, so their ability to handle unseen structures can be fairly compared.\nThe corpus The sentence pairs in the corpus are divided into thirteen bins according to the number of logical connectives (and, or, not) in the longer of the two sentences in the pair. We test the model on each bin separately (58k total examples, using an 80/20% train/test split) in order to\nevaluate how its performance depends on the complexity of the sentences. In three experiments, we train our models on the training portions of bins 0–3 (62k examples), 0–4 (90k), and 0–6 (160k), and test on every bin but the trivial bin 0. Capping the size of the training sentences allows us to evaluate how the models interpret the sentences: if their performance falls off abruptly above the cutoff, it is reasonable to assume that the models are depending heavily on specific sentence structures, and cannot generalize to new structures. If their performance decays gradually1 with no such abrupt change, then it must have learned a more generally valid interpretation function for the language which respects its recursive structure."
    }, {
      "heading" : "3 Testing sentence models on entailment",
      "text" : "We use the architecture depicted in Figure 1a, which builds on the one used in Bowman et al. (2015). The model architecture uses two copies of a single sentence model (a tree or sequence model) to encode the premise and hypothesis (left and right side) sentences, and then uses those encodings as the features for a multilayer classifier which predicts one of the seven relations. Since the encodings are computed separately, the sentence models are forced to encode every element of the sentence meaning that the downstream model will need.\nClassifier The classifier component of the model consists of a combining layer which takes the two sentence representations as inputs, followed by two neural network layers, then a softmax classifier. For the combining layer, we use a neural tensor network (NTN, Chen et al. 2013) layer, which sums the output of a plain recursive/recurrent neural network layer with a vector computed using two multiplications with a learned (full rank) third-order tensor parameter:\n~yNN = tanh(M\n[ ~x(l)\n~x(r)\n] +~b )(1)\n~yNTN = ~yNN + tanh(~x (l)TT[1...n]~x(r))(2)\nOur model is largely identical to the Bowman et al. model, but adds the two additional tanh NN\n1Since sentences are fixed-dimensional vectors of fixedprecision floating point numbers, all models will make errors on sentences above some length, and L2 regularization (which helps overall performance) exacerbates this by discouraging the model from using the kind of numerically precise, nonlinearity-saturating functions that generalize best.\nlayers, which we found help performance across the board, and also uses the NTN combination layer when evaluating all three models, rather than just the TreeRNTN model, so as to ensure that the three sentence models are compared in as similar a setting as possible.\nSentence models The sentence encoding component of the model transforms the (jointly learned) embeddings of the input words into a single sentence vector. We experiment with tree models (Figure 1b) with TreeRNN (eqn. 1), TreeRNTN (eqn. 2), and TreeLSTM (Tai et al., 2015) activation functions. In addition, we use a sequence model (Figure 1c) with an LSTM activation function (Hochreiter and Schmidhuber, 1997) implemented as in Zaremba et al. (2015). Experiments with simpler non-LSTM RNN sequence models tended to badly underfit the training data, and are not included here.\ndeep$3 50d(TreeRNN 50d(TreeRNTN 50d(LSTM 1 1 1 0.9359 2 0.99242 0.98485 0.98485 3 0.95132 0.97895 0.93684 4 0.86342 0.93123 0.75167 5 0.79125 0.89025 0.65541 6 0.74852 0.86128 0.57122 7 0.6944 0.8073 0.50764 8 0.67253 0.78242 0.47253 9 0.61883 0.75772 0.45988 10 0.55799 0.75055 0.4267 11 0.56949 0.70508 0.4203 12 0.50459 0.66667 0.3639\ndeep$4 50d(TreeRNN 50d(TreeRNTN 50d(LSTM 1 1 1 0.98718 2 0.9899 0.99242 0.98485 3 0.98947 0.98684 0.93947 4 0.9618 0.96084 0.87775 5 0.9386 0.94781 0.76132 6 0.91098 0.91395 0.68694 7 0.8871 0.90323 0.60357 8 0.87363 0.87033 0.55495 9 0.80556 0.86883 0.50463\n10 0.7768 0.84683 0.44639 11 0.7728 0.80339 0.48814 12 0.7461 0.78899 0.43119\ndeep$6 50d(TreeRNN 50d(TreeRNTN 50d(LSTM 1 1 0.98718 0.97436 2 1 0.99747 0.98485 3 0.99605 0.98289 0.94868 4 0.97803 0.96562 0.91213 5 0.97774 0.95088 0.87337 6 0.97404 0.94436 0.82493 7 0.95586 0.91851 0.78268 8 0.94396 0.8967 0.73077 9 0.89815 0.88272 0.66821\n40% 50% 60% 70% 80% 90% 100% 1 2 3 4 5 6 7 8 9 10 11 12 A cc ur ac y Size of longer expression\n40%\n50%\n60% 70% 80% 90% 100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc ur ac y\nSize of longer expression\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc\nur ac\ny\nSize of longer expression\n50d TreeRNN\n50d TreeRNTN\n50d LSTM\ndeep$3 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM From(mon(morn,(epoch(72 1 0.98718 0.9615 1 0.98718 2 0.98737 0.957 0.9873 0.97727 3 0.93158 0.925 0.95 0.91974 4 0.84241 0.8548 0.8834 0.76313 5 0.77667 0.825 0.8188 0.66769 6 0.72552 0.744 0.7744 0.55341 7 0.70458 0.7393 0.7657 0.54924 8 0.66044 0.6813 0. 022 0.4967 9 0.58642 0.6512 0.6543 0.42438\n10 0.56893 0.632 0.652 0.4026 11 0.50847 0.555 0.6 0.4508 12 0.51988 0.571 0.584 0.4097\ndeep$4 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM 1 1 1 1 1 2 0.9873 0.98737 0.99242 0.9848 3 0.9815 0.98158 0.98553 0.975 4 0.9436 0.94174 0.95798 0.9159 5 0.901 0.89793 0.9317 0.8158 6 0.8471 0.85905 0.88056 0.6928 7 0.8446 0.84041 0.87946 0.6519 8 0.8033 0.7956 0.82747 0.578 9 0.7623 0.74383 0.76235 0.5108\n10 0.737 0.768 0.7833 0.507 11 0.728 0.7457 0.7559 0.477 12 0.733 0.7186 0.7125 0.437\ndeep$6 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM 1 0.98718 0.98718 1 0.98718 2 1 0.98737 1 0.98485 3 0.99474 0.99342 0.99474 0.97368 4 0.99045 0.97517 0.98663 0.94842 5 0.98235 0.95625 0.97237 0.90867 6 0.97033 0.92804 0.9451 0.8635 7 0.96435 0.9253 0.94822 0.83447 8 0.93846 0.89011 0.91648 0.76813 9 0.91512 0.89352 0.8858 0.69599\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc\nur ac\ny\nSize of longer expression\n50d TreeRNN\n50d TreeRNTN 50d TreeLSTM 50d LSTM\n40 1 2 3 4 5 6 7 8 9 10 11 12\nA cc\nur ac\ny\nSize of longer expression\n50d TreeRNN 50d TreeRNTN 50d TreeLSTM 50d LSTM\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc\nur ac\ny\nSize of longer expression\n50d TreeRNN\n50d TreeRNTN 50d TreeLSTM 50d LSTM\n(a) Training on sz. ≤3.\ndeep$3 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM From(mon(morn,(epoch(72 1 0.98718 0.9615 1 0.98718 2 0.98737 0.957 0.9873 0.97727 3 0.93158 0.925 0.95 0.91974 4 0.84241 0.8548 0.8834 0.76313 5 0.77667 0.825 0.8188 0.66769 6 0.72552 0.744 0.7744 0.55341 7 0.70458 0.7393 0.7657 0.54924 8 0.66044 0.6813 0.7022 0.4967 9 0.58642 0.6512 0.6543 0.42438\n10 0.56893 0.632 0.652 0.4026 11 0.50847 0.555 0.6 0.4508 12 0.51988 0.571 0.584 0.4097\ndeep$4 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM5 d(LSTM 1 1 1 1 1 2 0.9873 0.98737 0.99242 0.9848 3 0.9815 0.98158 0.98553 0.975 4 0.9436 0.94174 0.95798 0.9159 5 0.901 0.89793 0.9317 0.8158 6 0.8471 0.85905 0.88056 0.6928 7 0.8446 0.84041 0.87946 0.6519 8 0.8033 0.7956 0.82747 0.578\n0.7623 0.74383 0.76235 0.5108 10 0.737 0.768 0.7833 0.507 11 0. 28 0.7457 0.7559 0.477 12 0.733 0.7186 0.7125 0.437\ndeep$6 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM\n1 0.98718 0.98718 1 0.98718 2 1 0.98737 1 0.98485 3 0.99474 0.99342 0.99474 0.97368 4 0.99045 0.97517 0.98663 0.94842 5 0.98235 0.95625 0.97237 0.90867 6 0.97033 0.92804 0.9451 0.8635 7 0.96435 0.9253 0.94822 0.83447 8 0.93846 0.89011 0.91648 0.76813 9 0.91512 0.89352 0.8858 0.69599\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc\nur ac\ny\nSize of longer expression\n50d TreeRNN\n50d TreeRNTN\n50d TreeLSTM 50d LSTM\n40%\n50%\n60% 70% 80% 90% 100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc ur ac y\nSize of longer expression\n50d TreeRNN 50d TreeRNTN 50d TreeLSTM 50d LSTM\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc\nur ac\ny\nSize of longer expression\n50d TreeRNN 50d TreeRNTN 50d TreeLSTM 50d LSTM\n(b) Training on sz. ≤4.\ndeep$3 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM From(mon(morn,(epoch(72 1 0.98718 0.9615 1 0.98718 2 0.98737 0.957 0.9873 0.97727 3 0.93158 0.925 0.95 0.91974 4 .84241 0.8548 0.8834 0.76313 5 0.77667 0.825 0.8188 0.66769 6 0.72552 0.744 0.7744 0.55341 7 0.70458 0.7393 0.7657 0.54924 8 0.66044 0.6813 0.7022 0.4967 9 0.58642 0.6512 0.6543 0.42438 10 0.56893 0.632 0.652 0.4026 11 0.50847 0.555 0.6 0.4508 12 0.51988 0.571 0.584 0.4097\ndeep$4 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM 1 1 1 1 1 2 0.9873 0.98737 0.99242 0.9848 3 0.9815 0.98158 0.98553 0.975 4 0.9436 0.94174 0.95798 0.9159 5 0.901 0.89793 0.9317 0.8158 6 0.8471 0.85905 0.88056 0.6928 7 0.8446 0.84041 0.87946 0.6519 8 0.8033 0.7956 0.82747 0.578 9 0.7623 0.74383 0.76235 0.5108\n10 0.737 0.768 0.7833 0.507 11 0.728 0.7457 0.7559 0.477 12 0.733 0.7186 0.7125 0.437\ndeep$6 50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM 1 0.98718 0.98718 1 0.98718 2 1 0.98737 1 0.98485 3 0.99474 0.99342 0.99474 0.97368 4 0.99045 0.97517 0.98663 0.94842 5 0.98235 0.95625 0.97237 0.90867 6 0.97033 0.92804 0.9451 0.8635 7 0.96435 0.9253 0.94822 0.83447 8 0.93846 0.89011 0.91648 0.76813 9 0.91512 0.89352 0.8858 0.69599\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc\nur ac\ny\nSize of longer expression\n50d TreeRNN 50d TreeRNTN 50d TreeLSTM 50d LSTM\n40% 50% 60% 70% 80% 90% 100% 1 2 3 4 5 6 7 8 9 10 11 12 A cc ur ac y Size of longer expression\n50d TreeRNN 50d TreeRNTN 50d TreeLSTM 50d LSTM\n40%\n50%\n60% 70% 80% 90% 100%\n1 2 3 4 5 6 7 8 9 10 11 12\nA cc ur ac y\nSize of longer expression\n50d TreeRNN 50d TreeRNTN 50d TreeLSTM 50d LSTM\n(c) Training on sz. ≤6.\ndeep$3 50d(TreeRNN 50d(TreeRNT 50d(TreeLSTM50d(LSTM From(Sat(morn,(pass(100000 1 0.98718 1 1 1 2 0.9873 0.9798 0.98 9 0.99242 3 0.91447 0.95263 0.95921 0.94605 4 0.84336 0.89016 0.88921 0.77 46 5 0.78511 0.82655 0.83269 0.6492 6 0.72 74 0.77448 0.78338 0.544 1 7 0.71647 0.73769 0.7691 0.531 1 8 0.65275 0.66044 0.70659 0.498 9 0.62963 0.63272 0.666 7 0.42901 10 93 0.60613 0.6652 0.4004 11 0.4983 0.57966 0.6 0.4711 12 0.5198 0.59327 0.5963 0.425\ndeep$4 50d(TreeRNN 50d(TreeRNT 50d(TreeLSTM50d(LSTM 1 1 1 1 2 0.99242 0.98232 1 3 0.96184 0.96447 0.98158 4 0.91882 0.9169 0.95415 5 0.88565 0.87951 0.91558 6 0.8457 0.837 4 0.86424 7 0.81579 0.8107 0.85314 8 0.764 4 0.76264 0.81648 9 0.73611 0.73611 0.75154\n10 0.7133 0.7396 0.78993 11 0.6779 0.6711 0.7152 12 0.6758 0.691 0.721\ndeep$6 50d(TreeRNN 50d(TreeRNT 50d(TreeLSTM50d(LSTM 1 0.98718 0.88462 1 0.98718 2 0.98737 0.9 65 0.99747 0.97222 3 0.978 5 0.8355 0.99605 0.96184 4 0.95129 0.81089 0.98663 0.91786 5 0.9263 0.7958 0.967 7 0.86723 6 0.905 9 0.77374 0.9421 0.81009 7 0.89049 0.76 1 0.9405 0.78183 8 0.85385 0.72527 0.90549 0.72527 9 0.8302 0.7 3 0.885 0.65278\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1 2 3 4 5 6 7 8 9 10 11 2\nA cc\nur ac\ny\nSize of lon er xpression\n50d TreeRNN 50d TreeRNT 50d TreeLSTM 50d LSTM\n40% 50% 60% 70% 80% 90% 100% 1 2 3 4 5 6 7 8 9 10 11 12 A cc ur ac y Size of longer exp ession\n50d TreeRNN 50d TreeRNTN 50d TreeLSTM 50d LSTM\n40%\n50%\n60% 70% 80% 90% 100%\n1 2 3 4 5 6 7 8 9 10 11 2\nA cc ur ac y\nSize of longer xpression\n50d TreeRNN 50d TreeRNT 50d TreeLSTM 50d LSTM\nFigure 2: Test accuracy on three experiments with increasingly rich training sets. The horizontal axis on each graph divides the test set expression pairs into bins by the number of logical operators in the more complex of the two expressions in the pair. The dotted line shows the maximum size in each training set.\n10 0.92341 0. 4 26 0.8993 .6914 11 0.91186 0.85424 0.8406 0.6406 12 0.92049 0.82875 0.8623 0.636\n50d(TreeRNN 50d(TreeRNTN 50d(TreeLSTM50d(LSTM 0 0.5252 0.5252 0.5252 0.5 52 0.03 0.77193 0.59579 0.84893 0.57186 \"@6k\" 0.11 0.86057 0.60628 0.9284 0.57494 \"@22k\" 0.33 0.91477 0.93175 0.9532 0.83991 \"@66k\"\n1 0.96053 0.9299 0.96746 0.89303 \"@200k\"\nSize of longer expression\n50%\n60%\n7\n8\n90\n100%\n0% 3% 11% 33% 100%\nA cc\nur ac\ny ac\nro ss\na ll\nbi ns\nAmount of training data used (nonlinear scale)\n50d TreeRNN\n50d TreeRNTN\n50d TreeLSTM\n50d LSTM\nFigure 3: Learning curve for the ≤6 experiment.\nTraining We randomly initialize all embeddings and layer parameters, and train them using minibatch stochastic gradient descent with AdaDelta (Zeiler, 2012) learning rates. Our objective is the standard negativ l g likelihood classification objective with L2 regularization (tuned on a separate train/test split). All models were trained for 72 epochs, after which all had largely converged without significantly declining from their peak performances."
    }, {
      "heading" : "4 Results and discussion",
      "text" : "The results are shown in Figure 2. The tree models fit the training data well, reaching 99.6, 98.5, and 97.7% overall accuracy respectively in the ≤6 setting, with the LSTM underfitting slightly at 94.4%. In that setting, all models generalized well to structures of familiar length, with the tree models all surpassing 97% on examples of size 4, and the LSTM reaching 94.8%. On the longer test sentences, the tree models decayed smoothly in performance across the board, while the LSTM decayed more quickly and more abruptly, with a striking difference in the≤4 setting, where LSTM performance falls 10% from 4 to 5, compared to 4.4% for the next worse model. However, the LSTM improves considerably with more ample\ntraining data in the ≤6 condition, showing only a 3% drop and generalization results better than the best model’s in the ≤3 setting.\nAll four models robustly beat the simple baselines reported in Bowman et al.: the most frequent class occurs just over 50% of the time and a neural bag of words model does reasonably on the shortest examples but falls below 60% by bin 4.\nThe learning curve (Figure 3) suggests that additional data is unlikely to change these basic results, though the disparity in performance at the 11% level is striking: the LSTM sequence model and the TreeRNTN (the model with the most parameters by far) lag far behind the other two."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We find that all four models can exploit a recursively defined language to interpret sentences with complex unseen structures, and that tree models’ biases allow them to learn to do this more effectively from less data. We interpret these results as evidence that both tree and sequence architectures can play valuable roles in the construction of sentence models over data with recursive syntactic structure: tree architectures provide an explicit bias that makes it possible to learn from relatively small and impoverished data sets, while sequence-based architectures can solve the same problems—given sufficient data—without being constrained by this bias. Finally, we suggest that, because of the well-supported linguistic claim that the kind of recursive structure that we study here is key to the understanding of real natural languages, there is likely to be value in developing sequence models that can more efficiently exploit this structure without fully sacrificing the flexibility that makes them succeed."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We gratefully acknowledge support from a Google Faculty Research Award, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA875013-2-0040, the National Science Foundation under grant no. IIS 1159679, and the Department of the Navy, Office of Naval Research, under grant no. N00014-10-1-0109. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Google, Bloomberg L.P., DARPA, AFRL NSF, ONR, or the US government."
    } ],
    "references" : [ {
      "title" : "Recursive neural networks can learn logical semantics",
      "author" : [ "Samuel R. Bowman", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors",
      "author" : [ "Danqi Chen", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Chen et al\\.,? 2013",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Transitionbased dependency parsing with stack long shortterm memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith." ],
      "venue" : "Proc. ACL.",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L. Elman." ],
      "venue" : "Cognitive science, 14(2):179–211.",
      "citeRegEx" : "Elman.,? 1990",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "Learning task-dependent distributed representations by backpropagation through structure",
      "author" : [ "Christoph Goller", "Andreas Kuchler." ],
      "venue" : "Proc. IEEE International Conference on Neural Networks.",
      "citeRegEx" : "Goller and Kuchler.,? 1996",
      "shortCiteRegEx" : "Goller and Kuchler.",
      "year" : 1996
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Visualizing and understanding recurrent networks",
      "author" : [ "Andrej Karpathy", "Justin Johnson", "Fei-Fei Li." ],
      "venue" : "arXiv:1506.02078.",
      "citeRegEx" : "Karpathy et al\\.,? 2015",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2015
    }, {
      "title" : "When are tree structures necessary for deep learning of representations? arXiv:1503.00185",
      "author" : [ "Jiwei Li", "Dan Jurafsky", "Eudard Hovy" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "An extended model of natural logic",
      "author" : [ "Bill MacCartney", "Christopher D. Manning." ],
      "venue" : "Proc. of the Eighth International Conference on Computational Semantics.",
      "citeRegEx" : "MacCartney and Manning.,? 2009",
      "shortCiteRegEx" : "MacCartney and Manning.",
      "year" : 2009
    }, {
      "title" : "Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "author" : [ "Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning." ],
      "venue" : "Proc. EMNLP.",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Proc. NIPS.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proc. ACL.",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "arXiv:1412.7449.",
      "citeRegEx" : "Vinyals et al\\.,? 2014",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Zaremba et al\\.,? 2015",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2015
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "arXiv:1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Neural networks that encode sentences as realvalued vectors have been successfully used in a wide array of NLP tasks, including translation (Sutskever et al., 2014), parsing (Dyer et al.",
      "startOffset" : 140,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : ", 2014), parsing (Dyer et al., 2015), and sentiment analysis (Tai et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : ", 2015), and sentiment analysis (Tai et al., 2015).",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "These models are generally either sequence models based on recurrent neural networks, which build representations incrementally from left to right (Elman, 1990; Sutskever et al., 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al.",
      "startOffset" : 147,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : "These models are generally either sequence models based on recurrent neural networks, which build representations incrementally from left to right (Elman, 1990; Sutskever et al., 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al.",
      "startOffset" : 147,
      "endOffset" : 184
    }, {
      "referenceID" : 4,
      "context" : ", 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al., 2011).",
      "startOffset" : 164,
      "endOffset" : 211
    }, {
      "referenceID" : 9,
      "context" : ", 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al., 2011).",
      "startOffset" : 164,
      "endOffset" : 211
    }, {
      "referenceID" : 11,
      "context" : "Nevertheless, tree models have not shown the kinds of dramatic performance improvements over sequence models that their billing would lead one to expect: head-to-head comparisons with sequence models show either modest improvements (Tai et al., 2015) or none at all (Li et al.",
      "startOffset" : 232,
      "endOffset" : 250
    }, {
      "referenceID" : 7,
      "context" : ", 2015) or none at all (Li et al., 2015).",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "We believe this is plausible, on the basis of other recent research (Vinyals et al., 2014; Karpathy et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "We believe this is plausible, on the basis of other recent research (Vinyals et al., 2014; Karpathy et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "Our methods are based on Bowman et al. (2015), in which we describe an experiment and corresponding artificial dataset which tests this ability in two tree models.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "Our results engage with those of Vinyals et al. (2014) and Dyer et al.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "(2014) and Dyer et al. (2015), who find that sequence models can learn to recognize syntactic structure in natural language, at least when trained on explicitly syntactic tasks.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "We use the architecture depicted in Figure 1a, which builds on the one used in Bowman et al. (2015). The model architecture uses two copies of a single sentence model (a tree or sequence model) to encode the premise and hypothesis (left and right side) sentences, and then uses those encodings as the features for a multilayer classifier which predicts one of the seven relations.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "2), and TreeLSTM (Tai et al., 2015) activation functions.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "In addition, we use a sequence model (Figure 1c) with an LSTM activation function (Hochreiter and Schmidhuber, 1997) implemented as in Zaremba et al.",
      "startOffset" : 82,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "In addition, we use a sequence model (Figure 1c) with an LSTM activation function (Hochreiter and Schmidhuber, 1997) implemented as in Zaremba et al. (2015). Experiments with simpler non-LSTM RNN sequence models tended to badly underfit the training data, and are not included here.",
      "startOffset" : 83,
      "endOffset" : 157
    }, {
      "referenceID" : 14,
      "context" : "Training We randomly initialize all embeddings and layer parameters, and train them using minibatch stochastic gradient descent with AdaDelta (Zeiler, 2012) learning rates.",
      "startOffset" : 142,
      "endOffset" : 156
    } ],
    "year" : 2017,
    "abstractText" : "Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find that the sequence model can learn the underlying patterning. The sequence model is better in that it learns the value of tree structure from the data in an emergent way, while the treestructured model is better in being able to learn with greater statistical efficiency due to its informative prior model structure.",
    "creator" : "TeX"
  }
}