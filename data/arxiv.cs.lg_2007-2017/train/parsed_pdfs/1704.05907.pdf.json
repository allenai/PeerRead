{
  "name" : "1704.05907.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "firstname.lastname@nrc-cnrc.gc.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n05 90\n7v 1\n[ cs\n.C L\n] 1\n9 A\npr 2\n01 7\nclassification. Our method automatically creates various views of its input text, each taking the form of soft attention weights that distribute the classifier’s focus among a set of base features. For a bag-of-words representation, each view focuses on a different subset of the text’s words. Aggregating many such views results in a more discriminative and robust representation. Through a novel architecture that both stacks and concatenates views, we produce a network that emphasizes both depth and width, allowing training to converge quickly. Using our multi-view architecture, we establish new state-of-theart accuracies on two benchmark tasks."
    }, {
      "heading" : "1 Introduction",
      "text" : "State-of-the-art deep neural networks leverage task-specific architectures to develop hierarchical representations of their input, with each layer building a refined abstraction of the layer that came before it (Conneau et al., 2016). For text classification, one can think of this as a single reader building up an increasingly refined understanding of the content. In a departure from this philosophy, we propose a divide-and-conquer approach, where a team of readers each focus on different aspects of the text, and then combine their representations to make a joint decision.\nMore precisely, the proposed Multi-View Network (MVN) for text classification learns to generate several views of its input text. Each view is formed by focusing on different sets of words through a view-specific attention mechanism. These views are arranged sequentially, so each subsequent view can build upon or deviate\nfrom previous views as appropriate. The final representation that concatenates these diverse views should be more robust to noise than any one of its components. Furthermore, different sentences may look similar under one view but different under another, allowing the network to devote particular views to distinguishing between subtle differences in sentences, resulting in more discriminative representations.\nUnlike existing multi-view neural network approaches for image processing (Zhu et al., 2014; Su et al., 2015), where multiple views are provided as part of the input, our MVN learns to automatically create views from its input text by focusing on different sets of words. Compared to deep Convolutional Networks (CNN) for text (Zhang et al., 2015; Conneau et al., 2016), the MVN strategy emphasizes network width over depth. Shorter connections between each view and the loss function enable better gradient flow in the networks, which makes the system easier to train. Our use of multiple views is similar in spirit to the weak learners used in ensemble methods (Breiman, 1996; Friedman et al., 1998; Wolpert, 1992), but our views produce vector-valued intermediate representations instead of classification scores, and all our views are trained jointly with feedback from the final classifier.\nExperiments on two benchmark data sets, the Stanford Sentiment Treebank (Socher et al., 2013) and the AG English news corpus (Zhang et al., 2015), show that 1) our method achieves very competitive accuracy, 2) some views distinguish themselves from others by better categorizing specific classes, and 3) when our base bag-of-words feature set is augmented with convolutional features, the method establishes a new state-of-the-art for both data sets."
    }, {
      "heading" : "2 Multi-View Networks for Text",
      "text" : "The MVN architecture is depicted in Figure 1. First, individual selection vectors s+ are created, each formed by a distinct softmax weighted sum over the word vectors of the input text. Next, these selections are sequentially transformed into views v, with each view influencing the views that come after it. Finally, all views are concatenated and fed into a two-layer perceptron for classification."
    }, {
      "heading" : "2.1 Multiple Attentions for Selection",
      "text" : "Each selection s+ is constructed by focusing on a different subset of words from the original text, as determined by a softmax weighted sum (Bahdanau et al., 2014). Given a piece of text with H words, we represent it as a bag-of-words feature matrixB ∈ IRH×d. Each row of the matrix corresponds to one word, which is represented by a d-dimensional vector, as provided by a learned word embedding table. The selection s+i for the i th view is the softmax weighted sum of features:\ns + i = H∑\nh=1\ndi,hB[h : h] (1)\nwhere the weight di,h is computed by:\ndi,h = exp(mi,h)∑H h=1 exp(mi,h)\n(2)\nmi,h = w s i tanh (W s i B[h : h]) (3)\nhere, wsi (a vector) and W s i (a matrix) are learned selection parameters. By varying the weights di,h, the selection for each view can focus on different words from B, as illustrated by different color curves connecting to s+ in Figure 1."
    }, {
      "heading" : "2.2 Aggregating Selections into Views",
      "text" : "Having built one s+ for each of our V views, the actual views are then created as follows:\nv1 =s + 1 ; vV = s + V (4) vi =tanh(W v i ([v1; v2; ...; vi−1 ; s + i ])) (5)\nfor i = 2 . . . V − 1\nwhere W vi are learned parameter matrices, and [. . . ; . . .] represents concatenation. The first and last views are formed by solely s+; however, they play very different roles in our network. vV is completely disconnected from the others, an independent attempt at good feature selection, intended to increase view diversity (Muslea et al., 2002; Guo and Viktor, 2006, 2008; Wang et al., 2015). Conversely, v1 forms the base of a structure similar to a multi-layer perceptron with shortcutting, as defined by the recurrence in Equation 5. Here, the concatenation of all previous views implements short-cutting, while the recursive definition of each view implements stacking, forming a deep network depicted by horizontal arrows in Figure 1. This structure makes each view aware of the information in those previous to it, allowing them to build upon each other. Note that the W v matrices are view-specific and grow with each view, making the overall parameter count quadratic in the number of views."
    }, {
      "heading" : "2.3 Classification with Views",
      "text" : "The final step is to transform our views into a classification of the input text. The MVN does so by concatenating its view vectors, which are then fed into a fully connected projection followed by a softmax function to produce a distribution over the possible classes. Dropout regularization (Hinton et al., 2012) can be applied at this softmax layer, as in (Kim, 2014)."
    }, {
      "heading" : "2.4 Beyond Bags of Words",
      "text" : "The MVN’s selection layer operates on a matrix of feature vectors B, which has thus far corresponded to a bag of word vectors. Each view’s selection makes intuitive sense when features correspond to words, as it is easy to imagine different readers of a text focusing on different words, with each reader arriving at a useful interpretation. However, there is a wealth of knowledge on how to construct powerful feature representations for text, such as those used by convolutional neural networks (CNNs). To demonstrate the utility of\nhaving views that weight arbitrary feature vectors, we augment our bag-of-words representation with vectors built by n-gram filters max-pooled over the entire text (Kim, 2014), with one feature vector for each n-gram order, n = 2 . . . 5. The augmentedB matrix hasH+4 rows. Unlike our word vectors, the 4 CNN vectors each provide representations of the entire text. Returning to our reader analogy, one could imagine these to correspond to quick (n = 2) or careful (n = 5) skims of the text. Regardless of whether a feature vector is built by embedding table or by max-pooled n-gram filters, we always back-propagate through all feature construction layers, so they become specialized to our end task."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Stanford Sentiment Treebank",
      "text" : "The Stanford Sentiment Treebank contains 11,855 sentences from movie reviews. We use the same splits for training, dev, and test data as in (Socher et al., 2013) to predict the fine-grained 5-class sentiment categories of the sentences. For comparison purposes, following (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), we train the models using both phrases and sentences, but only evaluate sentences at test time.\nWe initialized all of the word embeddings (Cherry and Guo, 2015; Cherry et al., 2015) using the publicly available 300 dimensional pre-trained vectors from GloVe (Pennington et al., 2014). We learned 8 views with 200 dimensions each, which requires us to project the 300 dimensional word vectors, which we implemented using a linear transformation, whose weight matrix and bias term are shared across all words, followed by a tanh activation. For optimization, we used Adadelta (Zeiler, 2012), with a starting learning rate of 0.0005 and a mini-batch of size 50. Also, we used dropout (with a rate of 0.2) to avoid overfitting. All of these MVN hyperparameters were determined through experiments measuring validation-set accuracy.\nThe test-set accuracies obtained by different learning methods, including the current state-ofthe-art results, are presented in Table 1. The results indicate that the bag-of-words MVN outperforms most methods, but obtains lower accuracy than the state-of-the-art results achieved by the tree-LSTM (Tai et al., 2015; Zhu et al., 2015) and the high-order CNN (Lei et al., 2015). However,\nwhen augmented with 4 convolutional features as described in Section 2.4, the MVN strategy surpasses both of these, establishing a new state-ofthe-art on this benchmark.\nIn Figure 2, we present the test-set accuracies obtained while varying the number of views in our MVN with convolutional features. These results indicate that better predictive accuracy can be achieved while increasing the number of views up to eight. After eight, the accuracy starts to drop. The number of MVN views should be tuned for each new application, but it is good to see that not too many views are required to achieve optimal performance on this task.\nTo better understand the benefits of the MVN method, we further analyzed the eight views constructed by our best model. After training, we obtained the view representation vectors for both the training and testing data, and then independently trained a very simple, but fast and stable Naı̈ve Bayes classifier (McCallum and Nigam, 1998) for each view. We report class-specific F-measures for\neach view in Figure 3. From this figure, we can observe that different views focus on different target classes. For example, the first two views perform poorly on the 0 (very negative) and 1 (negative) classes, but achieve the highest F-measures on the 2 (neutral) class. Meanwhile, the non-neutral classes each have a different view that achieves the highest F-measure. This suggests that some views have specialized in order to better separate subsets of the training data.\nWe provide an ablation study in Table 2. First, we construct a traditional ensemble model. We independently train eight MVN models, each with a single view, to serve as weak learners. We have them vote with equal weight for the final classification, obtaining a test-set accuracy of 50.2. Next, we restrict the views in the MVN to be unaware of each other. That is, we replace Equation 5 with vi = s + i , which removes all horizontal links in Figure 1. This drops performance to 49.0. Finally, we experiment with a variant of MVN, where each view is only connected to the most recent previous view, replacing Equation 5 with vi = tanh(W v i ([vi−1; s +\ni ])), leading to a version where the parameter count grows linearly in the number of views. This drops the test-set performance to 50.5. These experiments suggest that enabling the views to build upon each other is crucial for achieving the best performance."
    }, {
      "heading" : "3.2 AG’s English News Categorization",
      "text" : "The AG corpus (Zhang et al., 2015; Conneau et al., 2016) contains categorized news articles from more than 2,000 news outlets on the web. The task has four classes, and for each class there are 30,000 training documents and 1,900 test documents. A random sample of the training set was used for hyper-parameter tuning. The training and testing settings of this task are exactly the same as those presented for the Stanford Sentiment Treebank task in Section 3.1, except that the mini-batch size is reduced to 23, and each view has a dimension of 100.\nThe test errors obtained by various methods are presented in Table 3. These results show that the bag-of-words MVN outperforms the state-of-theart accuracy obtained by the non-neural n-gram TFIDF approach (Zhang et al., 2015), as well as several very deep CNNs (Conneau et al., 2016). Accuracy was further improved when the MVN was augmented with 4 convolutional features.\nIn Figure 4, we show how accuracy and loss evolve on the validation set during MVN training. These curves show that training is quite stable. The MVN achieves its best results in just a few thousand iterations."
    }, {
      "heading" : "4 Conclusion and Future Work",
      "text" : "We have presented a novel multi-view neural network for text classification, which creates multiple views of the input text, each represented as a weighted sum of a base set of feature vectors. These views work together to produce a discriminative feature representation for text classification. Unlike many neural approaches to classification,\nour architecture emphasizes network width in addition to depth, enhancing gradient flow during training. We have used the multi-view network architecture to establish new state-of-the-art results on two benchmark text classification tasks. In the future, we wish to better understand the benefits of generating multiple views, explore new sources of base features, and apply this technique to other NLP problems such as translation or tagging."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Bagging predictors",
      "author" : [ "Leo Breiman." ],
      "venue" : "Mach. Learn. 24(2).",
      "citeRegEx" : "Breiman.,? 1996",
      "shortCiteRegEx" : "Breiman.",
      "year" : 1996
    }, {
      "title" : "The unreasonable effectiveness of word representations for twitter named entity recognition",
      "author" : [ "Colin Cherry", "Hongyu Guo." ],
      "venue" : "HLT-NAACL. pages 735–745.",
      "citeRegEx" : "Cherry and Guo.,? 2015",
      "shortCiteRegEx" : "Cherry and Guo.",
      "year" : 2015
    }, {
      "title" : "Nrc: Infused phrase vectors for named entity recognition in twitter",
      "author" : [ "Colin Cherry", "Hongyu Guo", "Chengbi Dai." ],
      "venue" : "ACL-IJCNLP 2015:54–60.",
      "citeRegEx" : "Cherry et al\\.,? 2015",
      "shortCiteRegEx" : "Cherry et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for natural language processing",
      "author" : [ "Alexis Conneau", "Holger Schwenk", "Loı̈c Barrault", "Yann LeCun" ],
      "venue" : "CoRR abs/1606.01781",
      "citeRegEx" : "Conneau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2016
    }, {
      "title" : "Additive logistic regression: a statistical view of boosting",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani." ],
      "venue" : "Annals of Statistics 28:2000.",
      "citeRegEx" : "Friedman et al\\.,? 1998",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1998
    }, {
      "title" : "Mining relational data through correlation-based multiple view validation",
      "author" : [ "Hongyu Guo", "Herna L Viktor." ],
      "venue" : "KDD’06. ACM, pages 567–573.",
      "citeRegEx" : "Guo and Viktor.,? 2006",
      "shortCiteRegEx" : "Guo and Viktor.",
      "year" : 2006
    }, {
      "title" : "Convolutional neural networks",
      "author" : [ "Yoon Kim" ],
      "venue" : null,
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "NIPS’15. pages 649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory over recursive structures",
      "author" : [ "Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo." ],
      "venue" : "ICML. pages 1604–1612.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-view perceptron: a deep model for learning face identity and view representations",
      "author" : [ "Zhenyao Zhu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang." ],
      "venue" : "NIPS. pages 217–225.",
      "citeRegEx" : "Zhu et al\\.,? 2014",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "State-of-the-art deep neural networks leverage task-specific architectures to develop hierarchical representations of their input, with each layer building a refined abstraction of the layer that came before it (Conneau et al., 2016).",
      "startOffset" : 211,
      "endOffset" : 233
    }, {
      "referenceID" : 10,
      "context" : "Unlike existing multi-view neural network approaches for image processing (Zhu et al., 2014; Su et al., 2015), where multiple views are provided as part of the input, our MVN learns to automatically create views from its input text by focusing on different sets of words.",
      "startOffset" : 74,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "Compared to deep Convolutional Networks (CNN) for text (Zhang et al., 2015; Conneau et al., 2016), the MVN strategy emphasizes network width over depth.",
      "startOffset" : 55,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "Compared to deep Convolutional Networks (CNN) for text (Zhang et al., 2015; Conneau et al., 2016), the MVN strategy emphasizes network width over depth.",
      "startOffset" : 55,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "Our use of multiple views is similar in spirit to the weak learners used in ensemble methods (Breiman, 1996; Friedman et al., 1998; Wolpert, 1992), but our views produce vector-valued intermediate representations instead of classification scores, and all our views are trained jointly with feedback from the final classifier.",
      "startOffset" : 93,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "Our use of multiple views is similar in spirit to the weak learners used in ensemble methods (Breiman, 1996; Friedman et al., 1998; Wolpert, 1992), but our views produce vector-valued intermediate representations instead of classification scores, and all our views are trained jointly with feedback from the final classifier.",
      "startOffset" : 93,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : ", 2013) and the AG English news corpus (Zhang et al., 2015), show that 1) our method achieves very competitive accuracy, 2) some views distinguish themselves from others by better categorizing specific classes, and 3) when our base bag-of-words feature set is augmented with convolutional features, the method establishes a new state-of-the-art for both data sets.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "Each selection s is constructed by focusing on a different subset of words from the original text, as determined by a softmax weighted sum (Bahdanau et al., 2014).",
      "startOffset" : 139,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : ", 2012) can be applied at this softmax layer, as in (Kim, 2014).",
      "startOffset" : 52,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "having views that weight arbitrary feature vectors, we augment our bag-of-words representation with vectors built by n-gram filters max-pooled over the entire text (Kim, 2014), with one feature vector for each n-gram order, n = 2 .",
      "startOffset" : 164,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "For comparison purposes, following (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), we train the models using both phrases and sentences, but only evaluate sentences at test time.",
      "startOffset" : 35,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "We initialized all of the word embeddings (Cherry and Guo, 2015; Cherry et al., 2015) using the publicly available 300 dimensional pre-trained vectors from GloVe (Pennington et al.",
      "startOffset" : 42,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "We initialized all of the word embeddings (Cherry and Guo, 2015; Cherry et al., 2015) using the publicly available 300 dimensional pre-trained vectors from GloVe (Pennington et al.",
      "startOffset" : 42,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "The results indicate that the bag-of-words MVN outperforms most methods, but obtains lower accuracy than the state-of-the-art results achieved by the tree-LSTM (Tai et al., 2015; Zhu et al., 2015) and the high-order CNN (Lei et al.",
      "startOffset" : 160,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "All results except for the MVN are drawn from (Conneau et al., 2016)",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "The AG corpus (Zhang et al., 2015; Conneau et al., 2016) contains categorized news articles from more than 2,000 news outlets on the web.",
      "startOffset" : 14,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "The AG corpus (Zhang et al., 2015; Conneau et al., 2016) contains categorized news articles from more than 2,000 news outlets on the web.",
      "startOffset" : 14,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "These results show that the bag-of-words MVN outperforms the state-of-theart accuracy obtained by the non-neural n-gram TFIDF approach (Zhang et al., 2015), as well as several very deep CNNs (Conneau et al.",
      "startOffset" : 135,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : ", 2015), as well as several very deep CNNs (Conneau et al., 2016).",
      "startOffset" : 43,
      "endOffset" : 65
    } ],
    "year" : 2017,
    "abstractText" : "We propose a multi-view network for text classification. Our method automatically creates various views of its input text, each taking the form of soft attention weights that distribute the classifier’s focus among a set of base features. For a bag-of-words representation, each view focuses on a different subset of the text’s words. Aggregating many such views results in a more discriminative and robust representation. Through a novel architecture that both stacks and concatenates views, we produce a network that emphasizes both depth and width, allowing training to converge quickly. Using our multi-view architecture, we establish new state-of-theart accuracies on two benchmark tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}