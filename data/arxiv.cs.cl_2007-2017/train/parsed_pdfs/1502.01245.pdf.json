{
  "name" : "1502.01245.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Authorship recognition via fluctuation analysis of network topology and word intermittency",
    "authors" : [ "Diego R. Amancio" ],
    "emails" : [ "diego.raphael@gmail.com,", "diego@icmc.usp.br" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n01 24\n5v 1\n[ cs\n.C L\n] 4\nlanguage processing applications. More specifically, complex networks concepts and methods from dynamical systems theory have been successfully applied to recognize stylistic patterns in written texts. Despite the large amount of studies devoted to represent texts with physical models, only a few studies have assessed the relevance of attributes derived from the analysis of stylistic fluctuations. Because fluctuations represent a pivotal factor for characterizing a myriad of real systems, this study focused on the analysis of the properties of stylistic fluctuations in texts via topological analysis of complex networks and intermittency measurements. The results showed that different authors display distinct fluctuation patterns. In particular, it was found that it is possible to identify the authorship of books using the intermittency of specific words. Taken together, the results described here suggest that the patterns found in stylistic fluctuations could be used to analyze other related complex systems. Furthermore, the discovery of novel patterns related to textual stylistic fluctuations indicates that these patterns could be useful to improve the state of the art of many stylistic-based natural language processing tasks."
    }, {
      "heading" : "1. Introduction",
      "text" : "The application of concepts from Physics in textual analysis has increasingly become widespread [1–7]. The use of entropy concepts is perhaps one of the most known examples of adapting methods from Physics in language-based models [8]. In recent years, physicists have proposed novel approaches to tackle several natural language processing problems [9–19]. The emergence of fundamental principles of organization common to all languages has been studied in terms of the least-effort principle [20]. Other studies have been devoted to the analysis of word frequency distributions [21–25], which has led to the design of novel cutting-edge keyword detection methods [16,28–32]. Syntactical features have been employed to investigate the fundamental properties of the language from a physical standpoint [16, 50, 51]. In the semantic/pragmatic level, concepts from Physics have also been used to investigate the ubiquity of ambiguous structures in texts [10, 26, 27].\n2 In the field of stylometry, the use of complex networks (CN) in textual models\nhas become commonplace [12, 15, 16, 33–35]. More specifically, several studies have modeled texts as co-occurrence (word adjacency) networks, where nodes and edges are represented by words and adjacency relationships, respectively. It has been shown that networks modeling texts share the same statistical properties of many other real systems [36]. Specially, such networks display both small-world and scale-free properties, as a consequence of the Zipf’s law. Practical studies involving co-occurrence networks have devised algorithms to generate summaries [37], to assess text coherence and cohesion [38], and to evaluate the quality of manual and machine translations [39]. Even though word adjacency networks mostly grasp the syntactical factors of the language [16], it has been shown that they also convey semantic information [10,26,27].\nWhile co-occurrence networks focus mainly on short scales, other physical models\nhave been devised to capture long-range correlations. One of the most popular methods borrowed from the study of dynamical systems is the burstiness of word occurrences [29], which represents an attribute capable of capturing long-range textual features. Particularly, it has been shown that core words are unevenly distributed, while function words display distributions generated from random processes [31]. Such findings have motivated the proposition of algorithms aiming to detect keywords in single texts [30] using level statistics [33] and information theory [29]. The long-range textual structure has also been studied at the character unigram level [40, 41].\nMost of the research on textual pattern recognition has focused on the search for\nrecurring patterns in order to infer a specific class to unknown instances [42]. This approach has certainly worked well as many enlightening findings have been made this way. Despite the great number of studies on textual pattern recognition, the analysis of stylistic fluctuations along texts has received comparatively little attention. Empirical studies of some real systems have shown that fluctuations play a pivotal role on the unambiguous characterization of complex systems [43–47]. For example, when topology is a relevant network feature, the most informative patterns might be hidden in outlier fluctuations [48]. If one considers the distribution of word frequency, the fluctuations around the average might be useful to detect the most relevant concepts [29]. In biological systems, dynamical fluctuations of vital signals provide valuable information about the current state of the system [49].\nGiven the importance of the fluctuations in other real systems, the current paper\npresents a study on the properties of the stylistic variability along texts. Authors’ styles were characterized upon measuring the topological connectivity of networks modeling texts [33]. The stylistic evolution was quantified upon splitting the texts in shorter subtexts, which in turn were represented as smaller networks. From the topological analysis of these varying networks, several interesting finding could be found. First and foremost, it was possible to identify the correct authorship of texts from a multivariate analysis of the stylistic fluctuations along literary works. Interestingly, in this model, the variability of the average shortest path lengths along subtexts turned out be the most relevant feature for discriminating distinct authors. To identify the authorship\n3 of books, the proposed model also took advantage of the intermittency of time series representing the spatial distribution of words. Similarly to the CN-based model, a significant accuracy rate in discriminating authorship was found. Surprisingly, when the intermittency of 100 functional words was employed as features of the classifiers, the precise authorship could be found in 65% of the cases. As I shall show, the discovery of novel patterns related to the stylistic fluctuations in texts indicate that the proposed methodology can be extended to analyze other complex systems.\nThis paper is organized as follows. In Section 2, the methods employed to represent\ntexts as networks are presented. This section also swiftly presents the main topological measurements employed for the characterization of complex networks. In the same section, the intermittency concept is presented. In Section 3, the authorship recognition task is studied. In this case, the variability of complex network measurements along texts and the intermittency of specific function words were employed as attributes of the classifiers for the authorship recognition task. Finally, Section 4 presents perspectives for further research."
    }, {
      "heading" : "2. Methods",
      "text" : "In this paper, the style of written texts was quantified by measuring the topological properties of complex networks [60]. The representation of a text as a co-occurrence (word adjacency) network is detailed in Section 2.1. The topological features of complex networks employed to analyze the stylistic variation of texts are presented in Section 2.2. An alternative model based on the spatial distribution of words is presented in Section 2.3."
    }, {
      "heading" : "2.1. Modeling texts as complex networks",
      "text" : "There are several ways to model texts as networks [52]. While semantic networks capture the relationships between word meanings, co-occurrence networks are more suitable to grasp stylistic attributes of written texts. As a matter of fact, co-occurrence networks represent a simplified version of syntactic networks [16] because most of the syntactic connections occurs between neighboring words [16, 56].\nPrior to the creation of a co-occurrence network, some pre-processing steps are\nusually performed. Firstly, words conveying low semantic context (stopwords) are removed. Most of the words considered as stopwords are articles and prepositions (see the Supplementary Information). They are removed from the analysis because such words are mostly employed to connect other content words. After removing the stopwords, words with distinct spelling referring to the same concept are mapped to the same form. As a consequence, nouns and verbs are mapped to their singular and infinitive forms, respectively [53]. To perform such mapping, it is imperative to solve ambiguities at the word level because the mapped form might depend upon the sense assumed for a given word in a given context. To assist the disambiguation algorithm,\n4\nthe words are labeled with their respective parts-of-speech [53]. The labeling method employed is based on the maximum-entropy model proposed in [54].\nAfter the pre-processing step, each distinct word becomes a node. Therefore, the\ntotal number of nodes in the network is equal to the vocabulary size (M) of the preprocessed text. The words that appear separated by up to d − 1 intermediate words are connected in the network. In this paper, the value d = 1 was used. Therefore, only adjacent words were connected. Table 1 illustrates the pre-processing steps taken to form a small network from the poem “In the middle of the road”, by Carlos Drummond de Andrade. The network obtained from the pre-processed form is shown in Figure 1."
    }, {
      "heading" : "2.2. Topological characterization of complex networks",
      "text" : "There are a myriad of measurements currently employed to characterize the topology of complex networks [55]. Traditional measurements can be classified according to the amount of information needed for the computation. While local measurements only require information about the neighbors of a given node, global measurements require that the global network connectivity is known beforehand. There is also a third class: the quasi-local measurements. As the name suggests, quasi-local measurements require information about further neighbors (i.e. the nodes located two or more hops away from the node under analysis). The following list swiftly describes the measurements employed to analyze the topology of networks modeling texts.\n5\n6 In general terms, the accessibility has been proven useful to identify the borders of complex networks when self-avoiding random walks are performed [37]. In textual networks, this measurement has been employed to identify keywords and to generate informative extractive summaries [37]."
    }, {
      "heading" : "2.3. Intermittency",
      "text" : "In linguistic models, the effects of attraction and repulsion of words is an ever present phenomenon [31, 57]. Several studies have shown that the distribution of many words along documents is not regular [29–32]. Particularly, keywords are usually unevenly distributed along texts [16, 28–32]. This finding has motivated the design of keyword detection methods relying upon a single document [53]. To analyze the spatial distribution of words, each token is mapped to a element in a temporal series. The first word of the text represents the first element, the second word represents the second element and so forth. Given a word wi occurring fi times in the text, the recurrence times of wi generate the temporal series Ti = {t1, t2, t3, . . . , tfi−1}, where t1 is the distance (i.e. the number of intermediary words) between the first and second occurrence of wi, t2 is the distance between the second and third occurrence of wi and so on. Usually, two elements are added to the original temporal series Ti: the space t0 until the first occurrence of wi and the space tfi after the last occurrence of wi. The distribution of Ti might be characterized by the mean and standard deviation:\n〈T 〉i = 1\nfi + 1\nfi ∑\ni=0\nti = N + 1\nfi + 1 , (3)\n∆Ti =\n√ √ √ √ 1\nni\nni ∑\ni=0\n(ti − 〈T 〉), (4)\nwhere N = ∑\nfi. Given 〈T 〉 and ∆T , the irregularity of the distribution Ti is computed\nas\nIi = ∆T\n〈T 〉 =\nfi + 1 N + 1\n√ √ √ √ 1\nfi\nfi ∑\ni=0\n(ti − 〈T 〉). (5)\nThe measurement defined in eq. 5 is known as intermittency (or burstiness) of the distribution. It has been widely employed to detect keywords in texts as an alternative to the tf-idf technique [53]. In addition, the intermittency has proven relevant to detect keywords in genetic sequences [32].\nA qualitative comparison of words taking distinct values of intermittency is provided\nin Figure 2, which shows the distribution of the words “Carmylle” (fi = 54) and “feel” (fi = 54) along the book “Adventures of Sally”, by Pelham Grenville Wodehouse. Because the distribution of “Carmylle” is much more irregular than the distribution of “feel”, the former takes a much higher value of intermittency, as defined in eq. 5. The burstiness revealed by “Carmylle” also suggests that this word represents a relevant concept in the book [31]. An important property of the intermittency measurement\n7 is that it does not correlate with the frequency (see Figure 3). This means that the relevance assigned by the intermittency is not influenced by the word frequency. Taking advantage of this property, recent studies have combined both frequency and intermittency measurements to improve several keyword detection methods [16, 30]."
    }, {
      "heading" : "2.4. Pattern recognition methods",
      "text" : "The classification task aims at associating categories (or classes) to elements taking into account the attributes (or features) of these elements [65]. More specifically, an attribute is a measurable property of objects. To illustrate the concept, suppose that, in a given application, one desires to classify people according to their physical attributes. In this case, the height, the skin and hair color, the weight and others factors could\n8 be selected as attributes. In many cases, the choice of discriminative and informative attributes plays an essential role on the performance of classification systems. Most of the attributes employed in traditional applications assume either numerical (e.g. 1, −7 and 3.14) or categorical values (e.g. low and high). In the current study, one of the attributes employed to characterize texts is the intermittency of specific words. In this case, the intermittency of each word represents a numeric attribute.\nThe classification task is of paramount relevance for information retrieval\napplications. Particularly, in this paper, pattern recognition methods are used to capture the patterns emerging from the representation of texts as networks. Moreover, pattern recognition methods are used to quantify the discriminative ability provided by these patterns. Currently, there are several automatic classification methods. They are traditionally divided into the following groups:\n• Supervised classification: a binary relation mapping the input to the output is\ngenerated.\n• Unsupervised classification: a partition of the dataset is generated so that\nsimilar elements are clustered together.\n• Semi-supervised classification: the dataset available for automatic learning\ncomprises a small set of labeled instances. Most of the instances is not labeled, i.e the class associated to these instances is lacking. In this case, the objective is to map the unlabeled input to a labeled output.\nTypically, supervised classification methods process two datasets. The training\ndataset is the set of examples used as input. In other words, it represents the set of examples whose classes is known beforehand. In this paper, the training set is represented as Str = {β(tr,1), β(tr,2), β(tr,3) . . .}. The test dataset Sts = {β(ts,1), β(ts,2), β(ts,3) . . .} is the set used to evaluate the performance of the classifier. A given example β can be characterized by a set of M features: −→ β = (F1 = β (1), F2 = β(2), . . . , FM = β (M)), where F = {F1, F2, . . . , FM} is the set of attributes characterizing the example β. In other words, the k-th value taken by the attribute Fk in β is represented as β(k). In a supervised classification, a given example assumes a single class ci belonging to a finite set C = {c1, c2, . . .}.\nTo quantify the quality of the classification, the cross validation technique was\nemployed [69]. In this method, a fraction of the dataset is used to perform the training and another fraction is used to perform the evaluation. The implementation of this technique consists in splitting the training dataset in ten folders. Initially, nine folders are selected to train the classifiers and the remaining one is used for evaluation. This process is repeated ten times so that a different folder is used for the evaluation in each iteration. Finally, the accuracy rate is computed as the average accuracy obtained over the ten iterations. The cross validation is considered a reliable index since the evaluation is performed over unknown instances.\nIn the experiments, the analysis was performed on a dataset comprising books whose\nauthorship is known beforehand. As a consequence, supervised classification methods\n9 were employed to recognize patterns in the generated textual time series. The methods employed in this study were: Bayesian Networks (BNT), Complement Naive Bayes (CNB), Naive Bayes (NVB), RBF Networks (RBF), Multi Layer Perceptron (MLP), Support Vector Machines (SVM), k Nearest Neighbors (KNN), C4.5 (C45) and Random Forest (RFO). A short introduction to these methods is provided in Appendix A."
    }, {
      "heading" : "3. Results",
      "text" : "The stylistic properties of texts was studied in the context of the authorship recognition task. In this problem, one tries to recognize the identity of authors whose authorship is unknown. Owing to its central importance for stylometry, several contributions have been proposed [58]. Simple approaches include the analysis of word length and additional character features [67]. Mosteller and Wallace [68] proved that the frequency of function words (such as “and”, “any”, “ever”, “or”, “until” and “with”) can be employed to quantify the style of authors. More recently, many other approaches have been devised [58], including those relying upon topological analysis of complex networks [33, 70]. Here I use complex network and intermittency measurements to obtain potentially useful attributes for identifying the authors of books whose identity is lacking. Because this study focus on the analysis of stylistic fluctuations, the patterns displayed by the evolution of the statistical measurements along texts were studied.\nTwo types of temporal series representing the stylistic evolution in books are\nstudied. In Section 3.1, the relationship between the stylistic variation along books and the authorship recognition task is investigated. In Section 3.2, the intermittency profile of some words across different authors is employed to perform the authorship recognition task. The dataset employed in the experiments comprises books written by 8 authors, as shown in Table S1 of the Supplementary Information."
    }, {
      "heading" : "3.1. Stylistic variation along books",
      "text" : "In this section, I investigate whether the stylistic variation along texts provides useful attributes to the authorship recognition task. To quantify stylistic variations, the following methodology was taken. Each book in the dataset was split in subtexts comprising W tokens. Assuming that a book is formed by a sequence of tokens W = {w1, w2, . . .}, the j-th subtext Tj will comprise the sequence {wSj , wSj+1, . . . , wSj+W}, where Sj = W · j + 1 and j ∈ {0, 1, 2, . . .}. Each subtext Ti was modeled as a complex network (see Section 2.1) and the topological measurements of each subtext were extracted (see Section 2.2). Thus, each topological measurement X generates a temporal series X = {x1, x2, . . . xP }, where xi represents the value obtained for X in the subtext Ti and P is the total number of subtexts. An example of X for X = 〈l〉 and X = 〈C〉 is provided in Figure 4. The temporal series X of each book was then\n10\ndecomposed in terms of the Fourier transform:\nF (X)(j) = P ∑\nk=1\nxk exp\n(\n−2πijk\nN\n)\n, (6)\nwhere i2 = −1. It is worth noting that the first component, given by\nF (X)(0) = P ∑\nk=1\nxj = P 〈x〉,\nonly stores information concerning the average 〈x〉. Higher frequencies and, therefore, higher levels of variation in X are represented in F (X)({j∈N|j≥1}). As attributes of the classifiers, the first four components of F (X) were used.\nThe results obtained from the classification of authors are shown in Table 2. The\nlength of the subtexts considered were W = {500, 700, 900, 1, 100, 1, 300}. For each subtext length, the table lists the accuracy rate obtained by the best classifier. The lowest accuracy rate occurred for W = 500 and the highest discriminability was achieved with W = 1, 300. In all cases, the performance obtained by the classifiers was statistically significant, as revealed by low p-values. This result confirms that the stylistic variations of authors along texts (quantified via topological analysis of complex networks) can be employed to discriminate authors’ styles. Specially, the proposed method could be used as a complementary stylistic attribute, because the stylistic variation has been widely neglected as a relevant feature in current authorship attribution methods [58].\nTo verify the relative relevance of the features employed in the authorship\nrecognition task, the information gain of each attribute in the training dataset was computed. Mathematically, the relevance ascribed by the information gain (Ω) is\nΩ(Str, Fk) = H(Str)−H(Str|Fk), (7)\n11\nwhere H(Str) is the entropy of the training dataset Str and H(Str|Fk) is the entropy of Str when Fk is specified. H(Str|Fk) can be computed from training dataset as\nH(Str|Fk) = ∑\nv∈V (Fk)\n|β(k)(tr) = v| · |Str| −1 · H(β(k)(tr) = v), (8)\nwhere | · | is the cardinality of the set and V (Fk) represents the set of all values\ntaken by the attribute Fk in the training dataset, i.e.\nV (Fk) = |Str| ⋃\ni=1\nβ (k) (tr,i). (9)\nThe rank of the most informative measurements, according to eq. 7, is shown in\nTable 3. In this table, the rows indicate the ranking obtained by the attributes, for each subtext length (W ). All in all, the vocabulary size M turned out to be one of the most relevant attributes. The measurement displaying the highest relevance in higher components of the Fourier transform (j ≥ 1 in eq. 6) was the average shortest path length. More specifically, the third component (j = 3) displayed the highest relevance for large values of W , suggesting that the attribute related to the variation of 〈l〉 along the text becomes even more relevant when larger subtexts are analyzed. Interestingly, this result reinforces the importance of shortest paths for the authorship attribution task, since this measurement has been successfully employed to characterize authors’ styles in networks formed from full books [33]. The relevance of higher components of the Fourier transform can also be noted in the decision tree built with the C4.5 method [59] (see Figure 5). Note that F (〈l〉)(2) and F (M)(2) appear at superior levels of the tree, confirming thus their relevance. The relative importance of higher components becomes even more apparent if one observes that some traditional complex network measurements (i.e. F (X)(0)) correlates with the vocabulary size M [33]. This does not occur with F (〈l〉)(2), as revealed by the Pearson correlation coefficient displayed in Table 4. In fact, none of the higher components found in Table 4 correlates significantly with other relevant traditional attributes (F (X)(0)), thus confirming that higher components indeed provide novel information for characterizing styles in written texts.\nThe results concerning the evolution of styles revealed that distinct authors might\ndisplay distinct stylistic patterns along texts. This finding is similar to the results found\n12\nin [60], which showed that the temporal evolution of stylistic features of books published between 1590 and 1922 is able to identify the traditional literary movements. The main feature differentiating this work from previous studies is that the stylistic variation inside books is much more subtle than the corresponding variation over different literary styles [61]. The emergence of the described patterns suggests the applicability of other temporal models. Alternative models could probe, for example, the patterns present in the spatial distribution of character bigrams [40]. Particularly, this paper focus primarily on the evolution of stylistic patterns measured by the spatial distribution of words. For this reason, the next section investigates if the intermittency of specific words serves as authors’ fingerprints for the authorship recognition task."
    }, {
      "heading" : "3.2. Authorship recognition via word intermittency",
      "text" : "To verify if the uneven distribution of specific words along texts provides useful features for characterizing authors’ styles, the following experiment was carried out. Following the research on stylometry, this study focused on function words. The 100 most frequent\n13\n14\nwords in the corpus were considered as function words. As such, as attributes for the classifiers, the intermittency of these function words was used. The best classifier, the Multilayer Perceptron, yielded an accuracy rate of 65.0% (p-value = 1.3× 10−14). This result suggests that, besides the frequency, the intermittency of specific function words might be useful for characterizing authors’ styles in texts. Note that the discriminability obtained with intermittency features is not influenced by the frequency of function words, since there is no significant correlation between intermittency and frequency (see Section 2.3).\nA detailed analysis of the classification revealed that most of the errors occurred\nfor Arthur Conan Doyle, Wilkie Collins and Mark Twain (result not shown). If these authors are disregarded from the analyis, the use of intermittency features would provide an accuracy rate of 90% with the Multilayer Perceptron. Despite the large number of attributes employed for discriminating authors’ styles, the discriminative ability concentrated in a few function words. According to the information gain measurement, the words displaying the highest discriminative ability were “but” (H = 0.620), “and” (H = 0.604), “I ” (H = 0.530), “who” (H = 0.494) and “as” (H = 0.462). The high discriminability obtained with the intermittency of these five words can be noted in the principal component analysis shown in Figure 6.\nIn summary, one can conclude that the representation of specific words as temporal\nseries might be useful for the authorship recognition task. As commented in Section 3.1, the use of intermittency of specific words combined with traditional features might be useful to improve the performance of style-based real applications. In this case, an improved textual characterization would be provided, because the attributes generated from textual fluctuations do not correlate with traditional features. Moreover, distinct classifiers could be employed for each attribute type (e.g. frequency or intermittency), as\n15\nsome classifiers perform better for specific attributes. As such, the classification becomes more robust and accurate without the fine tuning required in single models [71]. The combination of attributes could be performed via ordinary voting of simple models [72]. Another possibility is to consider fuzzy methods as independent classifiers and then select the best weighting strategy for each classifier [26]. Furthermore, the successful application of intermittency measurements in characterizing authors’ styles suggests that complementary studies should be carried out in order to probe whether additional features of temporal series modeling the spatial distribution of words are able to reveal novel stylistic/topological patterns."
    }, {
      "heading" : "4. Conclusion",
      "text" : "In this study, I investigated if measurements characterizing temporal series from texts are useful to identify authors’ styles. In the light of the results, one can conclude that authors’ stylistic properties can be characterized upon analyzing the fluctuations of textual statistical measurements. The statistically significant accuracy rates obtained in the authorship attribution task confirmed that the features derived from the fluctuation of specific topological and intermittency measurements are able to discriminate distinct authors. Using a co-occurrence network model, it was shown that the relative importance of distinct attributes may depend on the subtext length. Nevertheless, in general, further components of the Fourier decomposition of topological measurements turned out to be relevant features for the task. An analysis of the spatial distribution of specific words revealed distinct patterns of distribution for different authors. Surprisingly, the intermittence of functional words correctly discriminate the authorship in 65% of the cases in a dataset comprising books written by 8 authors.\nThe focus of this investigation was on the evaluation of distinct attributes\nfor characterizing authors’ styles, rather than maximizing the accuracy rate of the classification. However, the dependence with stylistic attributes found for the proposed features suggests that attributes derived from the analysis of stylistic fluctuations can be combined in a hybrid way with traditional attributes, such as the frequency of function words [68]. As such, the findings reported in this paper shall potentially contribute to the improvement of current authorship recognition methods [67]. One could pursue this line of analysis further, identifying the combination of features yielding the best discriminability. Future investigations could probe the relevance of fluctuations in other related complex systems, such as DNA and other generic symbolic sequences, since the techniques described here can be extended in a straightforward fashion to such cases."
    }, {
      "heading" : "Acknowledgments",
      "text" : "DRA acknowledges financial support from São Paulo Research Foundation (FAPESPBrazil) (grant number 2014/20830-0).\n16"
    }, {
      "heading" : "Appendix A. Pattern Recognition Methods",
      "text" : "This appendix swiftly describes the main pattern recognition methods employed in this study. A complete reference to the field of pattern recognition can be found in [66].\nDecision trees\nDecision tree algorithms employ trees [62] to summarize the patterns recognized in the dataset (see Figure A1). Typically, a decision tree comprises internal and leaf nodes. While internal nodes store the tests performed on specific attributes, leaf nodes represent classes. The edges connect nodes according to the answers obtained from the tests. For example, the node representing the test F1 > θ1 has two outgoing edges, namely “YES” and “NO”. During the classification stage, one travels through the tree until a leaf node is reached. In this case, the class associated to the leaf node is assigned to the unknown instance. The classification process is illustrated in Figure A1.\nFigure A1. Example of decision tree. To classify a new instance, one starts the walk at the root node. The class assigned to the unknown instance is the class associated to the leaf node found at the end of the walk.\nTo construct a decision tree, at each step, one tries to find an attribute Fi and\na threshold θ so that the test Fi ≥ θ yields the best dataset partition. One assumes that the quality of a partition is proportional to the discriminability provided by that partition. At each division, the goal is to separate one or more classes in distinct groups. Several measurements have been proposed to quantify the quality of partitions. An wellknown measurement is the Kullback-Leibler divergence [63]. The process of choosing the attribute with the highest information gain is reiterated for the two subsets created at each internal node. The recursion is finalized when a subset contains instances belonging to a single class. In this case, a leaf node is created to store the corresponding class.\n17\nThe tree-based algorithms employed in this paper were the C4.5 and Random\nForest. Further details regarding these methods can be found in [69].\nBayesian decision\nTo classify a new instance, the Naive Bayes algorithm estimates the probability distribution of each class ci ∈ C. Given the likelihood profile of each class, the algorithm employs the maximum a posteriori strategy to infer the correct class. The probability of each ci ∈ C to be assigned to the instance β is\nP (ci| −→ β ) =\nP ( −→ β |ci)P (ci)\nP ( −→ β )\n= P (F1 = β\n(1), . . . , FM = β (M)|ci)P (ci)\nP (F1 = β(1), . . . , FM = β(M)) .\nNote that P (ci) can be estimated as N (ci)/ ∑ ci∈C N (ci), where N (ci) is the number of objects in Str belonging to class ci. For classification purposes, the quantity P ( −→ β ) can be disregarded from the analysis because P ( −→ β |ci) is constant for all ci ∈ C. Finally, in order to estimate P ( −→ β |ci), the traditional Naive Bayes classifier surmises independence between the features. Hence P ( −→ β |ci) is estimated as\nP ( −→ β |ci) = P (F1 = β (1), . . . , FM = β (M)|ci)\n= M ∏\nk=1\nP (Fk = β (k)|ci).\nUsing the value of P ( −→ β |ci), it is possible to replace it in the definition of P (ci| −→ β ). Therefore\nP (ci| −→ β ) =\nP (ci)\nP (F1 = β(1), . . .)\nM ∏\nk=1\nP (Fk = β (M)|ci).\nUpon using the maximum a posteriori rule, the class cs can be estimated as\ncβ = argmax ci∈C\nP (ci) M ∏\nk=1\nP (Fk = β (k)|ci).\nTo obtain cβ from the above equation, one must estimate the likelihood P (Fk|ci). Several methods have been proposed to perform the estimation [64]. The Parzen-Rosenblatt window algorithm has been widely employed as a non-parametric technique to estimate probability densities [64].\nIn addition to the Naive Bayes, the algorithms based on statistical paradigms\nemployed in this study were the Complement Naive Bayes and Bayesian Networks. More details concerning these methods can be found in [69].\nNeural Networks\nThe simplest artificial neural network (ANN) model is the Perceptron. In this model, each neuron stores activation and transfer functions. While the former sums (with\n18\nFigure A2. Example of a single neuron. We are given some input signals ai and expected outputs in a supervised classification. The learning algorithm aims at minimizing the error between the actual and expected output signals.\nweights) the input signals, the latter yields an output signal as a function of the input. Figure A2 illustrates a single neuron with input signals and weights represented as ai and wi, respectively. The output s is s = ∑ i aiwi + b. The transfer function φ may assume many distinct forms [65]. A very simple possibility is to consider that the neuron is activated whenever s surpasses a given threshold, i.e.\nφ(s) =\n{\n1 for s > 0, 0 otherwise. (A.1)\nThe correct choice of synaptic weights in neural networks allows the network to\neffectively process the input signals in order to generate the expected output. In general, the weights are assigned by learning algorithms [65]. Initially, the values wij of weights linking the i-th node of the input layer with the j-th node of the output layer assume random values. Given these initial weights, several input signals are presented to the neuron. Then, the obtained output is compared with the expected values. If the observed error exceeds a given threshold, the current weights are modified by the learning algorithm. In this case, the larger the error obtained, the greater is the change applied to the current weights. More specifically, weights are updated according to the rule w (t+1) ij = w (t) i + ηεjxi, where η is the learning rate and εj is the error obtained for the j-th neuron.\nThe ANN-based pattern recognition methods employed in this study were the\nMultilayer Perceptron and the RBF network. More details concerning these methods can be found in [65].\n19"
    } ],
    "references" : [ {
      "title" : "C4.5: Programs for Machine Learning",
      "author" : [ "R Quinlan J" ],
      "venue" : null,
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 1993
    }, {
      "title" : "Probing the topological properties of complex networks modeling short written texts. Manuscript under review",
      "author" : [ "R Amancio D" ],
      "venue" : null,
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2014
    }, {
      "title" : "Introduction to Algorithms",
      "author" : [ "H Cormen T", "E Leiserson C", "L Rivest R", "C Stein" ],
      "venue" : null,
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2001
    }, {
      "title" : "Towards a new evolutionary computation. Advances in estimation of distribution algorithms",
      "author" : [ "A Lozano J", "P Larranga", "I Inza", "E Bengoetxea" ],
      "venue" : null,
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2006
    }, {
      "title" : "Neural Networks for Pattern Recognition, Oxford: Oxford University",
      "author" : [ "M Bishop C" ],
      "venue" : null,
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 1995
    }, {
      "title" : "Pattern Recognition and Machine Learning, Springer-Verlag",
      "author" : [ "M Bishop C" ],
      "venue" : null,
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2007
    }, {
      "title" : "2001Applications of Computer Content Analysis, Westport, CT: Ablex Publishing",
      "author" : [ "J Tankard Jr. W" ],
      "venue" : null,
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2001
    }, {
      "title" : "Data Mining: Practical Machine Learning Tools and Techniques",
      "author" : [ "H Witten I", "E Frank" ],
      "venue" : null,
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The classification task aims at associating categories (or classes) to elements taking into account the attributes (or features) of these elements [65].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "To quantify the quality of the classification, the cross validation technique was employed [69].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Simple approaches include the analysis of word length and additional character features [67].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "5 method [59] (see Figure 5).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "The main feature differentiating this work from previous studies is that the stylistic variation inside books is much more subtle than the corresponding variation over different literary styles [61].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 6,
      "context" : "As such, the findings reported in this paper shall potentially contribute to the improvement of current authorship recognition methods [67].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "A complete reference to the field of pattern recognition can be found in [66].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "Decision tree algorithms employ trees [62] to summarize the patterns recognized in the dataset (see Figure A1).",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "Further details regarding these methods can be found in [69].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "Several methods have been proposed to perform the estimation [64].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "The Parzen-Rosenblatt window algorithm has been widely employed as a non-parametric technique to estimate probability densities [64].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "More details concerning these methods can be found in [69].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "The transfer function φ may assume many distinct forms [65].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "In general, the weights are assigned by learning algorithms [65].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "More details concerning these methods can be found in [65].",
      "startOffset" : 54,
      "endOffset" : 58
    } ],
    "year" : 2015,
    "abstractText" : "Statistical methods have been widely employed in many practical natural language processing applications. More specifically, complex networks concepts and methods from dynamical systems theory have been successfully applied to recognize stylistic patterns in written texts. Despite the large amount of studies devoted to represent texts with physical models, only a few studies have assessed the relevance of attributes derived from the analysis of stylistic fluctuations. Because fluctuations represent a pivotal factor for characterizing a myriad of real systems, this study focused on the analysis of the properties of stylistic fluctuations in texts via topological analysis of complex networks and intermittency measurements. The results showed that different authors display distinct fluctuation patterns. In particular, it was found that it is possible to identify the authorship of books using the intermittency of specific words. Taken together, the results described here suggest that the patterns found in stylistic fluctuations could be used to analyze other related complex systems. Furthermore, the discovery of novel patterns related to textual stylistic fluctuations indicates that these patterns could be useful to improve the state of the art of many stylistic-based natural language processing tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}