{
  "name" : "1508.06034.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "jng324@bloomberg.net", "vkanchakousk@bloomberg.net" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 8.\n06 03\n4v 1\n[ cs\n.C L\n] 2\n5 A\nug 2\n01 5"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automatic text summarization is a rich field of research. For example, shared task evaluation workshops for summarization were held for more than a decade in the Document Understanding Conference (DUC), and subsequently the Text Analysis Conference (TAC). An important element of these shared tasks is the evaluation of participating systems. Initially, manual evaluation was carried out, where human judges were tasked to assess the quality of automatically generated summaries. However in an effort to make evaluation more scaleable, the automatic ROUGE1 measure (Lin, 2004b) was introduced in DUC-2004. ROUGE determines the quality of an automatic summary through comparing overlapping units such as n-grams, word sequences, and word pairs with human written summaries.\n1Recall-Oriented Understudy of Gisting Evaluation\nROUGE is not perfect however. Two problems with ROUGE are that 1) it favors lexical similarities between generated summaries and model summaries, which makes it unsuitable to evaluate abstractive summarization, or summaries with a significant amount of paraphrasing, and 2) it does not make any provision to cater for the readability or fluency of the generated summaries.\nThere has been on-going efforts to improve on automatic summarization evaluation measures, such as the Automatically Evaluating Summaries of Peers (AESOP) task in TAC (Dang and Owczarzak, 2009; Owczarzak, 2010; Owczarzak and Dang, 2011). However, ROUGE remains as one of the most popular metric of choice, as it has repeatedly been shown to correlate very well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011).\nIn this work, we describe our efforts to tackle the first problem of ROUGE that we have identified above — its bias towards lexical similarities. We propose to do this by making use of word embeddings (Bengio et al., 2003). Word embeddings refer to the mapping of words into a multidimensional vector space. We can construct the mapping, such that the distance between two word projections in the vector space corresponds to the semantic similarity between the two words. By incorporating these word embeddings into ROUGE, we can overcome its bias towards lexical similarities and instead make comparisons based on the semantics of words sequences. We believe that this will result in better correlations with human assessments, and avoid situations where two word sequences share similar meanings, but get unfairly penalized by ROUGE due to differences in lexicographic representations.\nAs an example, consider these two phrases: 1) It is raining heavily, and 2) It is pouring. If we are performing a lexical string match, as ROUGE\ndoes, there is nothing in common between the terms “raining”, “heavily”, and “pouring”. However, these two phrases mean the same thing. If one of the phrases was part of a human written summary, while the other was output by an automatic summarization system, we want to be able to reward the automatic system accordingly.\nIn our experiments, we show that word embeddings indeed give us better correlations with human judgements when measured with the Spearman and Kendall rank coefficient. This is a significant and exciting result. Beyond just improving the evaluation prowess of ROUGE, it has the potential to expand the applicability of ROUGE to abstractive summmarization as well."
    }, {
      "heading" : "2 Related Work",
      "text" : "While ROUGE is widely-used, as we have noted earlier, there is a significant body of work studying the evaluation of automatic text summarization systems. A good survey of many of these measures has been written by Steinberger and Ježek (2012). We will thus not attempt to go through every measure here, but rather highlight the more significant efforts in this area.\nBesides ROUGE, Basic Elements (BE) (Hovy et al., 2005) has also been used in the DUC/TAC shared task evaluations. It is an automatic method which evaluates the content completeness of a generated summary by breaking up sentences into smaller, more granular units of information (referred to as “Basic Elements”).\nThe pyramid method originally proposed by Passonneau et al. (2005) is another staple in DUC/TAC. However it is a semi-automated method, where significant human intervention is required to identify units of information, called Summary Content Units (SCUs), and then to map content within generated summaries to these SCUs. Recently however, an automated variant of this method has been proposed (Passonneau et al., 2013). In this variant, word embeddings are used, as we are proposing in this paper, to map text content within generated summaries to SCUs. However the SCUs still need to be manually identified, limiting this variant’s scalability and applicability.\nMany systems have also been proposed in the AESOP task in TAC from 2009 to 2011. For example, the top system reported in Owczarzak and Dang (2011), AutoSum-\nmENG (Giannakopoulos and Karkaletsis, 2009), is a graph-based system which scores summaries based on the similarity between the graph structures of the generated summaries and model summaries."
    }, {
      "heading" : "3 Methodology",
      "text" : "Let us now describe our proposal to integrate word embeddings into ROUGE in greater detail.\nTo start off, we will first describe the word embeddings that we intend to adopt. A word embedding is really a function W , where W : w → Rn, and w is a word or word sequence. For our purpose, we want W to map two words w1 and w2 such that their respective projections are closer to each other if the words are semantically similar, and further apart if they are not. Mikolov et al. (2013) describe one such variant, called word2vec, which gives us this desired property2. We will thus be making use of word2vec.\nWe will now explain how word embeddings can be incorporated into ROUGE. There are several variants of ROUGE, of which ROUGE-1, ROUGE-2, and ROUGE-SU4 have often been used. This is because they have been found to correlate well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). ROUGE-1 measures the amount of unigram overlap between model summaries and automatic summaries, and ROUGE-2 measures the amount of bigram overlap. ROUGE-SU4 measures the amount of overlap of skip-bigrams, which are pairs of words in the same order as they appear in a sentence. In each of these variants, overlap is computed by matching the lexical form of the words within the target pieces of text. Formally, we can define this as a similarity function fR such that:\nfR(w1, w2) =\n{\n1, if w1 = w2 0, otherwise\n(1)\nwhere w1 and w2 are the words (could be unigrams or n-grams) being compared.\nIn our proposal3, which we will refer to as ROUGE-WE, we define a new similarity function\n2The effectiveness of the learnt mapping is such that we can now compute analogies such as king − man + woman = queen.\n3https://github.com/ng-j-p/rouge-we\nfWE such that:\nfWE(w1, w2) =\n{\n0, if v1or v2 are OOV v1 · v2, otherwise (2)\nwhere w1 and w2 are the words being compared, and vx = W (wx). OOV here means a situation where we encounter a word w that our word embedding function W returns no vector for. For the purpose of this work, we make use of a set of 3 million pre-trained vector mappings4 trained from part of Google’s news dataset (?) for W . Reducing OOV terms for n-grams. With our formulation for fWE , we are able to compute variants of ROUGE-WE that correspond to those of ROUGE, including ROUGE-WE-1, ROUGEWE-2, and ROUGE-WE-SU4. However, despite the large number of vector mappings that we have, there will still be a large number of OOV terms in the case of ROUGE-WE-2 and ROUGE-WE-SU4, where the basic units of comparison are bigrams.\nTo solve this problem, we can compose individual word embeddings together. We follow the simple multiplicative approach described by Mitchell and Lapata (2008), where individual vectors of constituent tokens are multiplied together to produce the vector for a n-gram, i.e.,\nW (w) = W (w1)× . . .×W (wn) (3)\nwhere w is a n-gram composed of individual word tokens, i.e., w = w1w2 . . . wn. Multiplication between two vectors W (wi) = {vi1, . . . , vik} and W (wj) = {vj1, . . . , vjk} in this case is defined as:\n{vi1 × vj1, . . . , vik × vjk} (4)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset and Metrics",
      "text" : "For our experiments, we make use of the dataset used in AESOP (Owczarzak and Dang, 2011), and the corresponding correlation measures.\nFor clarity, let us first describe the dataset used in the main TAC summarization task. The main summarization dataset consists of 44 topics, each of which is associated with a set of 10 documents. There are also four human-curated model summaries for each of these topics. Each of the 51 participating systems generated a summary for each of these topics. These automatically generated summaries, together with the human-curated\n4https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\nmodel summaries, then form the basis of the dataset for AESOP.\nTo assess how effective an automatic evaluation system is, the system is first tasked to assign a score for each of the summaries generated by all of the 51 participating systems. Each of these summaries would also have been assessed by human judges using these three key metrics: Pyramid. As reviewed in Section 2, this is a semi-automated measure described in Passonneau et al. (2005). Responsiveness. Human judges are tasked to evaluate how well a summary adheres to the information requested, as well as the linguistic quality of the generated summary. Readability. Human judges give their judgement on how fluent and readable a summary is.\nThe evaluation system’s scores are then tested to see how well they correlate with the human assessments. The correlation is evaluated with a set of three metrics, including 1) Pearson correlation (P), 2) Spearman rank coefficient (S), and 3) Kendall rank coefficient (K)."
    }, {
      "heading" : "4.2 Results",
      "text" : "We evaluate three different variants of our proposal, ROUGE-WE-1, ROUGE-WE-2, and ROUGE-WE-SU4, against their corresponding variants of ROUGE (i.e., ROUGE-1, ROUGE-2, ROUGE-SU4). It is worth noting here that in AESOP in 2011, ROUGE-SU4 was shown to correlate very well with human judgements, especially for pyramid and responsiveness, and out-performs most of the participating systems.\nTables 1, 2, and 3 show the correlation of the scores produced by each variant of ROUGE-WE with human assessed scores for pyramid, responsiveness, and readability respectively. The tables also show the correlations achieved by ROUGE-1, ROUGE-2, and ROUGE-SU4. The best result for each column has been bolded for readability.\nROUGE-WE-1 is observed to correlate very well with the pyramid, responsiveness, and readability scores when measured with the Spearman and Kendall rank correlation. However, ROUGE-SU4 correlates better with human assessments for the Pearson correlation. The key difference between the Pearson correlation and Spearman/Kendall rank correlation, is that the former assumes that the variables being tested are normally distributed. It also fur her assumes that the\nvariables are linearly related to each other. The latter two measures are however non-parametric and make no assumptions about the distribution of the variables being tested. We argue that the assumptions made by the Pearson correlation may be too constraining, given that any two independent evaluation systems may not exhibit linearity.\nLooking at the two bigram based variants, ROUGE-WE-2 and ROUGE-WE-SU4, we observe that ROUGE-WE-2 improves on ROUGE-2 most of the time, regardless of the correlation metric used. This lends further support to our proposal to use word embeddings with ROUGE.\nHowever ROUGE-WE-SU4 is only better than ROUGE-SU4 when evaluating readability. It does consistently worse than ROUGE-SU4 for pyramid and responsiveness. The reason for this is likely due to how we have chosen to compose unigram word vectors into bigram equivalents. The multiplicative approach that we have taken worked better for ROUGE-WE-2 which looks at contiguous bigrams. These are easier to interpret semantically than skip-bigrams (the target of ROUGEWE-SU4). The latter, by nature of their construction, loses some of the semantic meaning attached to each word, and thus may not be as amenable to the linear composition of word vectors.\nOwczarzak and Dang (2011) reports only the results of the top systems in AESOP in terms of Pearson’s correlation. To get a more complete picture of the usefulness of our proposal, it will be instructive to also compare it against the other top systems in AESOP, when measured with the Spearman/Kendall correlations. We show in Table 4 the top three systems which correlate best with the pyramid score when measured with the Spearman rank coefficient. C S IIITH3 (Kumar et al., 2011) is a graphbased system which assess summaries based on differences in word co-locations between generated summaries and model summaries. BE-HM (baseline by the organizers of the AESOP task) is the BE system (Hovy et al., 2005), where basic elements are identified using a head-modifier criterion on parse results from Minipar. Lastly, catolicasc1 (de Oliveira, 2011) is also a graph-based system which frames the summary evaluation problem as a maximum bipartite graph matching problem.\nWe see that ROUGE-WE-1 displays better correlations with pyramid scores than the top system in AESOP 2011 (i.e., C S IIITH3) when measured with the Spearman coefficient. The latter does slightly better however for the Kendall coefficient. This observation further validates that our\nproposal is an effective enhancement to ROUGE."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We proposed an enhancement to the popular ROUGE metric in this work, ROUGE-WE. ROUGE is biased towards identifying lexical similarity when assessing the quality of a generated summary. We improve on this by incorporating the use of word embeddings. This enhancement allows us to go beyond surface lexicographic matches, and capture instead the semantic similarities between words used in a generated summary and a human-written model summary. Experimenting on the TAC AESOP dataset, we show that this proposal exhibits very good correlations with human assessments, measured with the Spearman and Kendall rank coefficients. In particular, ROUGE-WE-1 outperforms leading state-of-theart systems consistently.\nLooking ahead, we want to continue building on this work. One area to improve on is the use of a more inclusive evaluation dataset. The AESOP summaries that we have used in our experiments are drawn from systems participating in the TAC summarization task, where there is a strong exhibited bias towards extractive summarizers. It will be helpful to enlarge this set of summaries to include output from summarizers which carry out substantial paraphrasing (Li et al., 2013; Ng et al., 2014; Liu et al., 2015).\nAnother immediate goal is to study the use of better compositional embedding models. The generalization of unigram word embeddings into bigrams (or phrases), is still an open problem (Yin and Schütze, 2014; Yu et al., 2014). A better compositional embedding model than the one that we adopted in this work should help us improve the results achieved by bigram variants of ROUGE-WE, especially ROUGE-WE-SU4. This is important because earlier works have demonstrated the value of using skip-bigrams for summarization evaluation.\nAn effective and accurate automatic evaluation measure will be a big boon to our quest for better text summarization systems. Word embeddings add a promising dimension to summarization evaluation, and we hope to expand on the work we have shared to further realize its potential."
    } ],
    "references" : [ {
      "title" : "Overview of the TAC 2009 Summarization Track",
      "author" : [ "Dang", "Owczarzak2009] Hoa Trang Dang", "Karolina Owczarzak" ],
      "venue" : "In Proceedings of the Text Analysis Conference (TAC)",
      "citeRegEx" : "Dang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dang et al\\.",
      "year" : 2009
    }, {
      "title" : "CatolicaSC at TAC",
      "author" : [ ],
      "venue" : "In Proceedings of the Text Analysis Conference (TAC)",
      "citeRegEx" : "Oliveira.,? \\Q2011\\E",
      "shortCiteRegEx" : "Oliveira.",
      "year" : 2011
    }, {
      "title" : "AutoSummENG and MeMoG in Evaluating Guided Summaries",
      "author" : [ "Giannakopoulos", "Vangelis Karkaletsis" ],
      "venue" : "In Proceedings of the Text Analysis Conference (TAC)",
      "citeRegEx" : "Giannakopoulos et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Giannakopoulos et al\\.",
      "year" : 2009
    }, {
      "title" : "Evaluating DUC 2005 using Basic Elements",
      "author" : [ "Hovy et al.2005] Eduard Hovy", "Chin-Yew Lin", "Liang Zhou" ],
      "venue" : "In Proceedings of the Document Understanding Conference (DUC)",
      "citeRegEx" : "Hovy et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2005
    }, {
      "title" : "Using Unsupervised System with Least Linguistic Features for TACAESOP Task",
      "author" : [ "Kumar et al.2011] Niraj Kumar", "Kannan Srinathan", "Vasudeva Varma" ],
      "venue" : "In Proceedings of the Text Analysis Conference (TAC)",
      "citeRegEx" : "Kumar et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2011
    }, {
      "title" : "Document Summarization via Guided Sentence Compression",
      "author" : [ "Chen Li", "Fei Liu", "Fuliang Weng", "Yang Liu" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Looking for a Few Good Metrics: ROUGE and its Evaluation",
      "author" : [ "Chin-Yew Lin" ],
      "venue" : "In Working Notes of the 4th NTCIR Workshop Meeting",
      "citeRegEx" : "Lin.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "ROUGE: A Package for Automatic Evaluation of Summaries",
      "author" : [ "Chin-Yew Lin" ],
      "venue" : "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop",
      "citeRegEx" : "Lin.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Toward Abstractive Summarization Using Semantic Representations",
      "author" : [ "Liu et al.2015] Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith" ],
      "venue" : "In Proceedings of the Conference of the North American Chapter of the Association",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Linguistic Regularities in Continuous Space Word Representations",
      "author" : [ "Wen-tau Yih", "Geoffrey Zweig" ],
      "venue" : "In Proceedings of the Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Vector-based Models of Semantic Composition",
      "author" : [ "Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata" ],
      "venue" : "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL),",
      "citeRegEx" : "Mitchell et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2008
    }, {
      "title" : "Exploiting Timelines to Enhance Multi-document Summarization",
      "author" : [ "Ng et al.2014] Jun-Ping Ng", "Yan Chen", "Min-Yen Kan", "Zhoujun Li" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association",
      "citeRegEx" : "Ng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2014
    }, {
      "title" : "An Introduction to DUC",
      "author" : [ "Over", "Yen2004] Paul Over", "James Yen" ],
      "venue" : "In Proceedings of the Document Understanding Conference (DUC)",
      "citeRegEx" : "Over et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Over et al\\.",
      "year" : 2004
    }, {
      "title" : "Overview of the TAC 2011 Summarization Track: Guided Task and AESOP Task",
      "author" : [ "Owczarzak", "Dang2011] Karolina Owczarzak", "Hoa Trang Dang" ],
      "venue" : "In Proceedings of the Text Analysis Conference (TAC)",
      "citeRegEx" : "Owczarzak et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Owczarzak et al\\.",
      "year" : 2011
    }, {
      "title" : "Overview of the TAC 2010 Summarization Track",
      "author" : [ "Karolina Owczarzak" ],
      "venue" : "In Proceedings of the Text Analysis Conference (TAC)",
      "citeRegEx" : "Owczarzak.,? \\Q2010\\E",
      "shortCiteRegEx" : "Owczarzak.",
      "year" : 2010
    }, {
      "title" : "Applying the Pyramid Method",
      "author" : [ "Ani Nenkova", "Kathleen McKeown", "Sergey Sigelman" ],
      "venue" : "DUC",
      "citeRegEx" : "Passonneau et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Passonneau et al\\.",
      "year" : 2005
    }, {
      "title" : "Automated Pyramid Scoring of Summaries using Distributional Semantics",
      "author" : [ "Emily Chen", "Weiwei Guo", "Dolores Perin" ],
      "venue" : "In Proceedings of the 51st Annual Meeting of the Association",
      "citeRegEx" : "Passonneau et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Passonneau et al\\.",
      "year" : 2013
    }, {
      "title" : "Evaluation Measures for Text Summarization",
      "author" : [ "Steinberger", "Ježek2012] Josef Steinberger", "Karel Ježek" ],
      "venue" : "Computing and Informatics,",
      "citeRegEx" : "Steinberger et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Steinberger et al\\.",
      "year" : 2012
    }, {
      "title" : "An Exploration of Embeddings for Generalized Phrases",
      "author" : [ "Yin", "Schütze2014] Wenpeng Yin", "Hinrich Schütze" ],
      "venue" : "In Proceedings of the ACL 2014 Student Research Workshop,",
      "citeRegEx" : "Yin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2014
    }, {
      "title" : "Factor-based Compositional Embedding Models",
      "author" : [ "Yu et al.2014] Mo Yu", "Matthew Gormley", "Mark Dredze" ],
      "venue" : "In Proceedings of the NIPS 2014 Deep Learning and Representation Learning Workshop",
      "citeRegEx" : "Yu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "There has been on-going efforts to improve on automatic summarization evaluation measures, such as the Automatically Evaluating Summaries of Peers (AESOP) task in TAC (Dang and Owczarzak, 2009; Owczarzak, 2010; Owczarzak and Dang, 2011).",
      "startOffset" : 167,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "Besides ROUGE, Basic Elements (BE) (Hovy et al., 2005) has also been used in the DUC/TAC shared task evaluations.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "The pyramid method originally proposed by Passonneau et al. (2005) is another staple in",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "posed (Passonneau et al., 2013).",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "ported in Owczarzak and Dang (2011), AutoSummENG (Giannakopoulos and Karkaletsis, 2009), is a graph-based system which scores summaries based on the similarity between the graph structures of the generated summaries and model summaries.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "Mikolov et al. (2013) describe one such variant, called word2vec, which gives us this desired property2.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "As reviewed in Section 2, this is a semi-automated measure described in Passonneau et al. (2005).",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "C S IIITH3 (Kumar et al., 2011) is a graphbased system which assess summaries based on differences in word co-locations between generated summaries and model summaries.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "BE-HM (baseline by the organizers of the AESOP task) is the BE system (Hovy et al., 2005), where basic elements are identified using a head-modifier criterion on parse results from Minipar.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "It will be helpful to enlarge this set of summaries to include output from summarizers which carry out substantial paraphrasing (Li et al., 2013; Ng et al., 2014; Liu et al., 2015).",
      "startOffset" : 128,
      "endOffset" : 180
    }, {
      "referenceID" : 11,
      "context" : "It will be helpful to enlarge this set of summaries to include output from summarizers which carry out substantial paraphrasing (Li et al., 2013; Ng et al., 2014; Liu et al., 2015).",
      "startOffset" : 128,
      "endOffset" : 180
    }, {
      "referenceID" : 8,
      "context" : "It will be helpful to enlarge this set of summaries to include output from summarizers which carry out substantial paraphrasing (Li et al., 2013; Ng et al., 2014; Liu et al., 2015).",
      "startOffset" : 128,
      "endOffset" : 180
    }, {
      "referenceID" : 19,
      "context" : "generalization of unigram word embeddings into bigrams (or phrases), is still an open problem (Yin and Schütze, 2014; Yu et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 134
    } ],
    "year" : 2015,
    "abstractText" : "ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefficients.",
    "creator" : "LaTeX with hyperref package"
  }
}