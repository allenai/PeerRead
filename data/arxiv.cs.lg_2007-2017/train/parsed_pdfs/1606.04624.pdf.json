{
  "name" : "1606.04624.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Finite-time Analysis for the Knowledge-Gradient Policy",
    "authors" : [ "Yingfei Wang", "Warren Powell" ],
    "emails" : [ "yingfei@cs.princeton.edu", "powell@princeton.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider sequential decision problems in which at each time step, we choose one of finitely many alternatives and observe a random reward. The rewards are independent of each other and follow some unknown probability distribution. One goal can be to identify the alternative with the best expected performance within a limited measurement budget, which is the objective of Bayesian ranking and selection problems. Ranking and selection problems are examples of sequential decision making problems with partial information that address the exploration-exploitation trade-off. Since the learner does not know the true distribution of each alternative, it needs to explore the\n∗Department of Computer Science, Princeton University, Princeton, NJ 08540, USA, yingfei@cs.princeton.edu †Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08540, USA, powell@princeton.edu\nar X\niv :1\n60 6.\n04 62\n4v 1\n[ cs\n.L G\n] 1\n5 Ju\nn 20\nchoices that might give good rewards in the future as well as exploit the alternatives that appear to be better based on previous observations.\nRanking and selection problems arise in many settings. We may have to choose a type of material that has the best performance, the features in a laptop or car that produce the highest sales, or the molecular combination that produces the most effective drug. Often, the cost of a measurement may be substantial. Laboratory or field experiments may take a day or several weeks. For this reason, we assume we have a limited budget for making measurements.\nRaiffa and Schlaifer established the Bayesian framework for R&S problems [33]. Several two-stage and sequential procedures exist for selecting the best alternative. Branke et. al made a thorough comparison of several fully sequential sampling procedures [6]. They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10]. Another single-step Bayesian look-ahead policy first introduced by [19] and then further studied by [15] is called the “knowledge-gradient policy” (KG). It chooses to measure the alternative that maximizes the single-period expected value of information. Whereas the above mentioned policies assumed an independent normal or one-dimensional Wiener process prior on the alternatives’ true means, Frazier et. al modified the knowledge-gradient policy to handle correlated multivariate normal belief on the mean values of these rewards [13].\nA similar field is the multi-armed bandit problem, which were originally studied under Bayesian assumptions [17]. A widely used class of policies for multi-armed bandit problems is called upper confidence bounding policies (UCB). Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7]. By contrast, knowledge gradient policies, which enjoy some nice theoretical properties, have never been characterized by the type of regret bounds for which UCB policies are famous.\nThis paper makes the following contributions: We first establish the connection between Bayesian ranking and selection problem and adaptive stochastic multi-set function maximization problems where each multi-set corresponds to a set of selected alternatives. The multi-set representation captures our ability to evaluate the same alternative more than once. This new perspective offers a new line of analysis for the properties of value-of-information policies. We derive the first finite-time bound for the knowledge gradient policy for R&S problems under the assumption that the utility function is adaptive submodular. However, pathwise adaptive submodularity can fail in offline learning settings when the utility function itself involves a maximum. To this end, instead of the pathwise behavior analyses of the utility function, we further study its average behavior by taking expectations over the observations given any fixed sample allocation, resulting in a well-known quantity: the value of information. As a result, we introduce the concept of the prior-value of a policy and analyze the prior-optimality of the KG policy to provide another insight into its performance based on the submodular assumption\nof the value of information that is weaker than adaptive submodularity. To accomplish this, we build on the general structure of the analysis of greedy algorithms given in [31] and [18]. We demonstrate submodularity for the two-alternative case and provide other conditions for more general problems, filling in a gap in the analysis of the knowledge gradient policy. Finally, we propose experiments to illustrate our theoretical analysis on the finite time behavior of the knowledge gradient policy. We further compare the KG policy with other policies with or without theoretical guarantees. Aside from the fact that the KG policy performs competitively with or significantly better than other policies especially in early iterations, we draw the conclusion that there is no universal best policy for all problem classes, which means that theoretical guarantees are not by themselves reliable indicators of which policy is best for a particular problem class and empirical experiments are needed to better understand their finite time performance.\nThis paper is organized as follows. In section 2, we lay out the mathematical models for Bayesian ranking and selection problems. In section 3, we describe the knowledge gradient policies. In section 4, we provide finite-time analyses of the knowledge gradient policy from two directions: the posterior optimality and the prior optimality. In section 5, we analyze the submodularity of the two-alternative case and provide other conditions for more general problems, bringing out the issue and importance of submodularity in leaning problems. Finally, in section 6 we present finite-time performance results and analyses of various policies for R&S problems."
    }, {
      "heading" : "2 Ranking and Selection Problems",
      "text" : "Suppose we have a collection X of M alternatives (where M might be quite large), each of which can be measured sequentially to estimate its unknown mean µx. We assume normally distributed measurement noise with known variance σ2W . We first introduce the model for independent normal beliefs. We begin with a normally distributed Bayesian prior belief on the sampling means that is independent across alternatives, µx ∼ N (θ0x, σ0x). At the nth iteration, we use some measurement policy π to choose one alternative xn and observe W n+1xn ∼ N (µxn , σW ).\nFor convenience, we introduce the σ-algebras Fn for any n = 0, 1, ..., N − 1 which is formed by the previous n measurement choices and outcomes, x0,W 1, ..., xn−1,W n. We define θnx = E[µx|Fn] and (σnx)2 = Var[µx|Fn]. Then conditionally on Fn, µx ∼ N (θnx , σnx). Let βnx = 1(σnx )2 be the conditional precision of µx and our state of knowledge be Sn = (θnx , β n x )x∈X . We will use Fn and Sn interchangeably. After the nth measurement we update our beliefs using Bayes’ rule:\nθn+1x =\n{ βnx θ n x+β WWn+1\nβn+βW if xn = x\nθnx otherwise, βn+1x = { βn + βW if xn = x βnx otherwise,\nwhere βW = 1/σ2W .\nWe may impose correlated beliefs between alternatives in order to strengthen the effect of each measurement. Starting from a prior distribution N (θ0,Σ0) and after measurement W n+1 of alternative x, a posterior distribution on the beliefs are calculated by:\nθn+1 = Σn+1 ( (Σn)−1 θn + βWW n+1ex ) , (1)\nΣn+1 = ( (Σn)−1 + βW exe T x )−1 , (2)\nwhere ex is the vector with 1 in the entry corresponding to alternative x and 0 elsewhere. Sn = (θn,Σn) is then our state of knowledge in this case.\nA decision function Xπ(Sn) is defined as a mapping from the knowledge state to X . We refer to the decision function Xπ and the policy π interchangeably.\nIf we are limited to N measurements, the objective is to maximize the expected reward of the final recommended alternative:\nmax π∈Π\nE [µxπ ] , (3)\nwhere xπ = arg maxx∈X θ N x and x n = Xπ(Sn) for 0 ≤ n < N ."
    }, {
      "heading" : "3 Knowledge Gradient",
      "text" : "For R&S problems, the knowledge gradient is a policy that at the nth iteration chooses its (n + 1)st measurement from X to maximize the single-period expected increase in value [15, 13]. To be more specific, the value of being in state Sn is maxx∈X θ n x . If we choose to measure xn = x right now, allowing us to observe W n+1x , then we transition to a new state of knowledge Sn+1 = (θn+1,Σn+1). At iteration n, θn+1x is a random variable since we do not yet know what W n+1 is going to be. We would like to choose x at iteration n which maximizes the expected value of maxx∈X θ n+1 x . We can think of this as choosing an alternative to maximize the incremental value, given by\nνKG,nx = E[max x′ θn+1x′ −max x′ θnx′|xn = x, Sn]. (4)\nThe knowledge gradient policy XKG(Sn) is defined by\nXKG(Sn) = arg max x∈X νKG,nx . (5)\nThe knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].\nThe knowledge gradient policy has some nice properties. For Bayesian ranking and selection problems, the knowledge gradient policy is optimal (by definition) if the measurement budget N = 1. The knowledge gradient is guaranteed to find the best alternative as the measurement budget N tends to infinity. If there are only two choices,\nthe knowledge gradient policy is optimal for any measurement budget. The knowledge gradient policy is the only stationary policy that is both myopically and asymptotically optimal. However, the KG has not enjoyed the finite-time bounds that have been popular in the UCB policies."
    }, {
      "heading" : "4 Finite-time Analysis of the Knowledge Gradient",
      "text" : "Policy\nWe follow the general structure of the analysis of greedy approximation [31] to develop the first finite-time bound for the knowledge gradient policy for R&S problems as follows. In Section 4.1, by interpreting the Bayesian R&S problems as the adaptive stochastic multi-set maximization problems, we show that the KG policy inherits precisely the performance guarantees of the greedy algorithm for classic submodular maximization problems if the utility function is adaptive submodular. We theoretically analyze the adaptive submodular assumption and point out that it can fail in the ranking and selection problems. In such cases, instead of the pathwise behavior analyses of the utility function, we study its average behavior by taking expectation over the observations in Section 4.2. In Section 4.3, we analyze the prior-optimality which provides another insight into the performance of the KG policy based on the submodularity of a wellunderstood quantity: value of information.\nIt is important to note that both the submodular maximization reduction and the theoretical analyses on the prior-optimality are not limited to the specific setup of Gaussian noise in observations and Gaussian prior structure. The theoretical guarantees are more generally applicable to any prior and measurement noise model as long as the adaptive submodular assumption or the submodular value of information assumption holds."
    }, {
      "heading" : "4.1 The Reduction of R&S to Adaptive Stochastic Multi-set",
      "text" : "Maximization\nWe first introduce the adaptive stochastic maximization problem. Let E be a finite set of items. Each item e ∈ E maps to a random outcome of a measurement Φ(e) in a set O of possible values. We define a realization as a function φ : E 7→ O representing the observation of each item in the ground set. Under Bayesian interpretation, we assume that there is a known prior probability distribution p(φ) := P(Φ = φ) over all possible realizations. The adaptive stochastic optimization problem consists of sequentially picking an item e ∈ E, revealing its outcome Φ(e) and picking the next item. After each pick, the observations so far can be represented as a partial realization ψ. A partial realization ψ is consistent with realization φ, denoted as φ ∼ ψ, if all the items selected in ψ have the same outcomes as in φ. We use dom(ψ) to refer to the items observed\nin ψ. We use the notation Zπ(φ) to denote the set of items chosen by policy π under realization φ.\nWe wish to maximize some utility function f : 2E × OE 7→ R that depends on which items we pick and which states they are in. The expected utility of a policy π is favg(π) := E [ f ( Zπ(Φ),Φ )] where the expectation is taken over the prior distribution p(φ). The goal of adaptive stochastic set maximization problem is to find an optimal policy π∗ that maximizes its expected utility under a cardinality constraint,\nπ∗ ∈ arg max π favg(π), subject to |Zπ(φ)| ≤ N,\nwhere N is the measurement budget. It is not obvious to treat the ranking and selection problem in an adaptive stochastic multi-set maximization way of thinking. To see this, define the ground set E = X . The outcomes are real numbers with O = R. Each alternative e = x can be selected multiple times. After each selection, its random outcome Φ(e) = Wx ∈ O is revealed.\nSince the true values µx are random variables, we can let ϕ be a sample realization of the truth with a (correlated) prior distribution p(ϕ) = N (θ0,Σ0). We use the notation φ ∈ Φ to denote an realization of the random observations in our problem. The prior probability distribution over the realizations is determined by p(ϕ) and the noise distribution N (0, σW ). For example, if in the ranking and selection problems each alternative can only be selected once, φ : E 7→ O. For multi-selections, one way of defining the realization is by first making replicas of each item to construct E ′ and then selecting each e′ ∈ E ′ at most once.\nConsider any sampling allocation z = (zx)x∈X , by which we measure alternative x for zx ∈ N times. We use Z to represent its corresponding multi-set. We use Zπ(φ) : Φ 7→ (X ×N) to refer to the alternatives selected by π under realization φ. Let θn be our vector of estimates of the means after n measurements according to allocation Z under realization φ, where |Z| = n. θn can be obtained according to the updating equation (1) and (2), and does not depend on the order of the allocations. It can thus be denoted as θn(Z, φ) : (X × N) × Φ 7→ RM . The next lemma states the equivalence of E[µxπ ] and E[maxx θNx ]. Hence, the utility function f : (X × N) × Φ 7→ R can be defined as maxx θ n x(Z, φ) and favg(π) := E [ maxx θ N x ( Zπ(Φ),Φ )] . The R&S objective (3) can then be re-written as π∗ ∈ arg max\nπ favg(π), subject to |Zπ(φ)| ≤ N.\nLemma 4.1 ([32]). Let π be a policy, and let xπ = arg maxx θ N x be the alternative selected by the policy. Then E[µxπ ] = E[max\nx θNx ].\nThe definition of the knowledge gradient νKG,nx coincides with the Conditional Expected Marginal Benefit ∆(e|ψ) defined by [18]:\n∆(e|ψ) := E [ f ( dom(ψ) ∪ {e},Φ ) − f ( dom(ψ),Φ ) |Φ ∼ ψ ] .\nThe knowledge gradient policy is thus in fact the adaptive greedy policy with uniform item costs, with a slight difference in the ability of selecting each item more than once. We generalize the definition of adaptive monotonicity and adaptive submodularity for set functions given by [18] to multi-set functions as follows.\nDefinition 4.2 (Adaptive Monotonicity). A function f : (X ×N)×Φ 7→ R is adaptive monotone with respect to distribution p(φ) if the conditional expected marginal benefit of any item is nonnegative: for all ψ and all x ∈ X .\n∆(x|ψ) ≥ 0.\nDefinition 4.3 (Adaptive Submodularity). A function f : (X×N)×Φ 7→ R is adaptive submodular with respect to distribution p(φ) if for all ψ and ψ′ such that dom(ψ) ⊆ dom(ψ′) and both ψ, ψ′ are consistent with some realization φ (i.e. ψ ⊆ ψ′), we have the conditional expected marginal benefit of any fixed item x ∈ X does not increase as more items are selected and observed,\n∆(x|ψ) ≥ ∆(x|ψ′).\nLet π∗ be the optimal policy to R&S problems. If f := maxx θ n x(Z, φ) is adaptive\nmonotone and adaptive submodular with respect to the prior distribution p(φ), then\nfavg(KG) > (1− e−1)favg(π∗).\nWe next show that the instances generated by ranking and selection problems are adaptive monotone.\nLemma 4.4. In ranking and selection problems, the utility function maxx θx is adaptive monotone with any Gaussian prior.\nProof. For any ψ, let n = |ψ|. Then for any item x ∈ X , ∆(x|ψ) can be rewritten as E[maxx′ θn+1x′ − maxx′ θnx′ |xn = x,Fn] = νKG,nx . Since for any x, θn+1x = θnx + σ̃(Σn, xn)Zn+1, where σ̃(Σ, x) = Σex√\n1/βW+Σxx and the random variable Zn+1 is standard\nnormal when conditioned on Fn [13]. Hence we have E[θn+1x′ |xn = x,Fn] = θnx′ for any x′. By Jensen’s inequality, we have ∆(x|ψ) = νKG,nx ≥ 0.\nEven though intuition suggests that the utility function should be adaptive submodular in the amount of information collected, as we collect more information it is natural to expect that the marginal value of this information should decrease, yet it is not always the case as shown in the next lemma. The proof can be found in Appendix A.1.\nLemma 4.5. For any independent normal prior distribution p(ϕ) and nondegenerated noise distribution (i.e. σW 6= 0), there exists ψ, ψ′ and x ∈ X such that ψ ⊆ ψ′ and ∆(x|ψ) < ∆(x|ψ′).\nIt can be seen that the adaptive submodular assumption can fail in the ranking and selection problems with the special utility function f = maxx θ n x(Z, φ) that involves maximization itself. Hence, instead of the above pathwise behavior analyses of the utility function, we would like to study its average behavior by taking the expectation over the observations given any fixed sample allocation Z in the next section."
    }, {
      "heading" : "4.2 The Value of Information",
      "text" : "We define the pathwise value of information v̂(Z, φ) as the incremental improvement over the best expected value that can be obtained without measurement, which is maxx∈X θ 0 x,\nv̂(Z, φ) := max x∈X θnx(Z, φ)−max x∈X θ0x.\nThe value of information v(Z) is then defined to be\nv(Z) := EΦ[v̂(Z,Φ)],\nwhere the expectation is taken over the prior distribution p(φ). The value of information has a long history spanning the literatures of several disciplines. Stigler considers the value of information in economics when buyers search for the best price [35]. Howard laid the groundwork for the value of information in a decision-theoretic context and spawned a great deal of work in this area [22]. Yokota and Thompson gives a first comprehensive review of value of information analyses related to health risk management [37]. Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].\nSince the value of information is a multi-set function, we first generalize the definitions and properties of submodular set functions described by [31] to submodular multi-set functions.\nDefinition 4.6. Given a finite set E, a real-valued function g on the set of multi-sets over E is called submodular if for all multi-sets S and T whose elements belong to E,\nρx(S) ≥ ρx(T ),∀S ⊆ T,∀x ∈ E,\nwhere ρx(S) , g(S ∪ {x}) − g(S) is the incremental value of adding element x to the multi-set S.\nProposition 1. Each of the following statements is equivalent and defines a submodular multi-set function (S pathwiseand T are multi-sets on E, x, y ∈ E):\n1. ρx(S) ≥ ρx(T ),∀S ⊆ T and ∀x.\n2. ρx(S) ≥ ρx(S ∪ {y}),∀S, x, y.\n3. g(T ) ≤ g(S) + ∑ x∈T−S ρx(S)− ∑ x∈S−T ρx(S ∪ T − {x}),∀S, T .\n4. g(T ) ≤ g(S) + ∑\nx∈T−S ρx(S),∀S ⊆ T .\nThis proposition follows from a similar proof of Proposition 2.1 in [31]. For completeness we provide the proof in Appendix A.2.\nIt is obvious that if θnx(Z, φ) is adaptive monotone or adaptive submodular with respect to p(φ), then so does v̂(Z, φ). It is also easy to show that if θnx(Z, φ) is adaptive monotone or adaptive submodular with respect to p(φ), then by the law of total expectation, i.e. E[E[U |V ]] = E[U ] for any random variables U and V , the value of information v(Z) is monotone or submodular. We close this section by showing the monotonicity of the multi-set function v and leave the analyses of submodularity in Section 5.\nLemma 4.7. (Monotonicity of the value of information) For any sampling allocation Z1 and Z2, if Z1 ⊆ Z2, then v(Z1) ≤ v(Z2).\nProof. We prove the monotonicity of v by showing v(Z) ≤ v(Z ∪ {xn+1}) for any allocation Z (with ∑ x∈X zx = n) and any additional measurement x\nn+1. By the tower property,\nv(Z ∪ {xn+1})− v(Z) = EΦ[v̂(Z ∪ {xn+1})]− E[v̂(Z)] = EΦ[max\nx∈X θn+1x (Z ∪ {xn+1})]− E[max x∈X θnx(Z)]\n= EΦ[E[max x θn+1x (Z ∪ {xn+1})−max x θnx(Z)|Φ ∼ ψZ ]] = EΦ[νKG,nx ],\nwhere ψZ is the partial realization with dom(ψZ) = Z. The lemma follows from the adaptive monotonicity, νKG,nx ≥ 0."
    }, {
      "heading" : "4.3 Guarantees on the Prior-optimality of the Knowledge Gra-",
      "text" : "dient Policy\nThere are two ways to evaluate the value of a policy. The first, which we call the posterior view, conditions on the allocation Z = Zπ(Φ) that would have occurred under policy π for each sample path φ ∈ Φ. This is the more conventional approach for evaluating policies. The second, which we call the prior view, starts by characterizing the value of an arbitrary allocation Z (before we have seen any sample realizations).\nMore formally, the classical way to estimate the value of a policy is to calculate the incremental improvement over what we could do before we collect any information, is given by\nf ′avg(π) = E[f(Zπ(Φ),Φ)]−max x θ0x.\nWe let P(π Z) be the probability that policy π produces allocation Z. Since with a fixed budget of N measurements, there are only finite choices of possible allocations, using the tower property, we can condition on the allocation Zπ = Z which gives us\nf ′avg(π) = ∑ Z∈ZN P(π Z) ( E[max x θnx(Z π(Φ),Φ)|Zπ = Z]−max x θ0x ) .\nWe note that in this method for evaluating a policy (which is the standard method), we only consider allocations Z that are actually produced by policy π for the outcomes in φ. This approach makes it much more difficult to understand the relationship between the allocation Z and the value of a policy.\nFor this reason, we adopt a different method of evaluating a policy which we term the prior view. Since this idea is new, we define it formally as follows\nDefinition 4.8 (The prior-value of a policy). Let Zn be the set of all possible allocations with a limited budget n. The value of a policy π with N measurements is defined as\nF π = ∑ Z∈ZN P(π Z) ( EΦ[max x θnx(Z,Φ)]−max x θ0x ) =\n∑ Z∈ZN P(π Z)v(Z).\nIn this view, we use the prior probability of an outcome p(φ) instead of the posterior p(φ|Zπ(φ) = Z) which is conditioned on an allocation Z. The value of this approach is that it writes the value of a policy directly as a function of v(Z), making it easier to study the effect of the properties of v(Z) on the value of a policy. Intuitively, since a policy could generate different allocations Z for different sample realizations, it is natural to define the value of a policy π as the weighted sum of the expected value of information based on all possible allocations Z and the weight should be the probability of occurrence of Z based on policy π.\nWe make the following assumption which is weaker than the adaptive submodularity assumption and will analyze it further in Section 5.\nAssumption 1. The value of information v is a submodular multi-set function on the set of alternatives X with respect to the prior distribution p(φ).\nLet π∗ be the optimal sequential policy under a budget of N measurements in the sense that the prior-value of π∗ is the largest. We call it prior-optimality. In what follows, we first bound KG’s sub-prior-optimality in Proposition 4.12:\nF π ∗ ≤ FKG[n]@π∗ ≤ FKG[n−1] +N(FKG[n] − FKG[n−1]), n = 1, 2, ..., N.\nThen we derive the worst-case bound for the KG policy in Theorem 4.14:\nFKG F π∗ ≥ 1− (N − 1 N )N ≥ e− 1 e ≈ 0.632.\nBesides the posterior optimality bound obtained from adaptive stochastic multi-set maximization, the prior-optimality provides another insight into the performance of the KG policy based on a well-understood quantity: value of information.\nDefinition 4.9 (Policy concatenation). [18] A concatenated policy π = π1@π2 is constructed by running π1 to completion, and then running policy π2 from a fresh start ignoring all the information collected while running π1.\nTo be more specific, suppose πi has a budget of ni, i = 1, 2, the first phase is to run π1 for n1 iterations starting from S\n0 and we get a sample realization including decisions and their corresponding measurements. The second phase is to run π2 for n2 measurements starting from S0 and we get another sample realization. Thus the sample realization of the concatenated process is all the decisions and their corresponding measurements collected in two phases. Note here, when running the second policy, we ignore all the information collected during running the first one, but when calculating the value of π1@π2, F π1@π2 , we use all the information collected in two phases.\nDefinition 4.10 (Policy truncation). [18] For a policy π, define the j-truncation π[j] of π as the policy that runs exactly (j + 1) steps under π’s decision rule and π{j} as the single step policy that randomly chooses an alternative according to the probability distribution of policy π’s decision for the (j + 1)-th step.\nWe now show that the value of π1 is no larger than the value of π1@π2.\nLemma 4.11. F π1 ≤ F π2@π1 for all policies π1 and π2 under any prior and probability distribution that describes a measurement.\nProof. We first show that F π1@π2 = F π2@π1 . In a concatenated policy, the two phases are independent since no information is shared among the two phases. Hence for a given allocation pair (Z1, Z2) where Z1 ∈ Zn1 , Z2 ∈ Zn2 , we have\nP(π1@π2 (Z1, Z2)) = P(π1 Z1)P(π2 Z2) = P(π2 Z2)P(π1 Z1) = P(π2@π1 (Z2, Z1)).\nF π1@π2 = F π2@π1 follows immediately from taking the sum over all possible pairs of (Z1, Z2)) such that Z2 ∪ Z1 = Z for any fixed allocation Z.\nTherefore F π1 ≤ F π1@π2 holds if and only if F π1 ≤ F π2@π1 . We then finish this proof by showing F π1 ≤ F π1@π2 . We write F π1@π2 − F π1 as a telescoping sequence\nF π1@π2 − F π1 = ∑\nZ∈Zn1+n2\nv(Z)P(π1@π2 Z)− ∑\nZ1∈Zn1 v(Z1)P(π1 Z1)\n= ∑\nZ∈Zn1+n2 ∑ Z1∪Z2=Z v(Z)P(π1 Z1)P(π2 Z2)\n− ∑\nZ1∈Zn1 ∑ Z2∈Zn2 v(Z1)P(π1 Z1)P(π2 Z2)\n= ∑\nZ1∈Zn1 ∑ Z2∈Zn2 [ v(Z1 ∪ Z2)− v(Z1) ] P(π1 Z1)P(π2 Z2)\n≥ 0,\nwhere the second equality holds due to the same reason as in the proof above for F π1@π2 = F π2@π1 and the third equality is just the same summation in different orders. The last inequality holds because of the monotonicity of multi-set function v.\nBased on the monotonicity of v and a similar argument as in Proposition 4.11, F is non-decreasing with respect to the number of measurements. Thus the more measurements, the better the policy. Hence π∗ has exactly N measurements. We have the following sub-optimality bound on KG’s prior-value. For a proof see Appendix A.3.\nProposition 4.12. Let ρKG,n = FKG [n] − FKG[n−1] , then\nF π ∗ ≤ F KG[n−1]@π∗ ≤ F KG[n−1] +NρKG,n\n= n−1∑ i=0 ρKG,i +NρKG,n, n = 0, 1, ..., N − 1. (6)\nWe now derive a bound for the adaptive greedy policy by applying linear programming to the problem of minimizing F KG\nFπ∗ subject to the inequalities (6), which is a worst-\ncase analysis. The following lemma states the linear program and its solution. We use it afterwards to establish the bounds.\nLemma 4.13. Given N ∈ Z+, consider the following linear program\nmin N−1∑ i=0 ai,\nt−1∑ i=0 ai +Nat ≥ 1, t = 0, 1, ..., N − 1.\nThen under these N constraints, min ∑N−1\ni=0 ai = 1− αN , where α = N−1 N .\nThe proof of this lemma can be found in [31]. We have the following results, which generalizes the classic result of the greedy algorithm that achieves (1 − 1/e)-approximation to prior-optimality for ranking and selection problems.\nTheorem 4.14. Assume we have a budget of N measurements. Let π∗ denote the optimal sequential policy for the ranking and selection problem, then we have\nFKG F π∗ ≥ 1− (N − 1 N )N .\nProof. By Proposition 4.12, we have F π ∗ ≤ ∑n−1 i=0 ρ\nKG,i + NρKG,n, n = 0, 1, ..., N − 1. Divide by F π ∗ on both sides of this inequality, we have\n1 ≤ n−1∑ i=0 ρKG,i F π∗ +N ρKG,n F π∗ , n = 0, 1, ..., N − 1.\nLet ai = ρKG,i\nFπ∗ , and then these inequalities are identical to the constraints in Lemma\n4.13. We notice that\nmin N−1∑ i=0 ai = min N−1∑ i=0 ρKG,i F π∗ ≤ N−1∑ i=0 ρKG,i F π∗ = FKG F π∗ .\nBy Lemma 4.13, we have min ∑N−1\ni=0 ai = 1−αN , so FKG Fπ∗ ≥ 1−αN = 1−(N−1 N )N ."
    }, {
      "heading" : "5 Analysis of Submodularity of the Value of Infor-",
      "text" : "mation\nThe finite-time bounds obtained in the previous sections assume that the value of information is submodular. In general, submodularity does not hold for arbitrary value functions. In this section, we analyze the submodularity of the two-alternative case for independent beliefs.\nWhile submodularity is a property for multi-set functions, we can extend it to any continuous function by making it possible for the increment to take any positive value. It could be easily extended to any continuous function. This allows us to use results from real analysis to study submodularity.\nDefinition 5.1. A function f : Rn 7→ R is submodular if for all x, y ∈ Rn, xi ≤ yi and δ ∈ Rn+,\nf(x+ δ)− f(x) ≥ f(y + δ)− f(y).\nWe show that submodularity of C2 functions is directly related to its second derivatives and cross-derivatives (the proof is given in Appendix A.4):\nTheorem 5.2. C2 function f: Rn → R is submodular if and only if every element of its Hessian is non-positive.\nThe concavity of the value of information has been studied extensively by [14]. In this section, we only study the cross-derivatives of the value of information.\nLet M = 2 and the measurement allocation z = (z1, z2). The value of information\nv(z) = s(z)f(− |θ 0 1−θ02 | s(z) ), where s(z) = √ σ̃21(z1) + σ̃ 2 2(z2), σ̃ 2 i (zi) =\nσ2,0i zi\nσ2W /σ 2,0 i +zi\n, f(a) =\naΦ(a) + φ(a), Φ and φ are the standard normal cumulative distribution and density respectively [14].\nAlthough the value of information is not concave in general in the two-alternative case, v is concave on the region where all zi’s are large enough (see Theorem 2 in [14]).\nWe directly calculate the first derivative and cross-derivative of v as\n∂v ∂z1 = σ̃1(z1)σ̃\n′ 1(z1)\ns(z)\n[ f(−|θ\n0 1 − θ02| s(z)\n) + |θ01 − θ02| Φ(− |θ 0 1−θ02 | s(z) )\ns(z)\n] ,\n∂2v\n∂z1∂z2 = σ̃1(z1)σ̃\n′ 1(z1)σ̃2(z2)σ̃ ′ 2(z2) s3(z) φ(−|θ 0 1 − θ02| s(z) )\n( |θ01 − θ02|2\nσ̃21(z1) + σ̃ 2 2(z2)\n− 1 ) .\nTheorem 5.3. The value of information is submodular when M = 2 and θ01 = θ 0 2.\nProof. Concavity of v(z) is proven in Remark 2 by [14]. Since θ01 = θ 0 2, |θ01− θ02| = 0 and thus ∂ 2v\n∂z1∂z2 ≤ 0. Therefore, v is submodular in this case.\n∂2v ∂z1∂z2 ≤ 0 is equivalent to |θ01 − θ02|2 ≤ σ̃21(z1) + σ̃22(z2). Rewriting this inequality, we get\n1 1\nσ2,01 + z1 σ2W\n+ 1\n1 σ2,02 + z2 σ2W\n≤ σ2,01 + σ 2,0 2 − |θ01 − θ02|2. (7)\nWe need σ2,01 +σ 2,0 2 − |θ01− θ02|2 ≥ 0, which can be achieved by setting our prior variance large enough or using a uniform prior over all alternatives. This is very reasonable when we have very little information about our problem domain.\nInequality equation (7) defines a region in the z1− z2 plane. Specifically, this region has the hyperbolic line 11\nσ 2,0 1 + z1 σ2 W + 11 σ 2,0 2 + z2 σ2 W = σ2,01 + σ 2,0 2 − |θ01 − θ02|2 as its boundary and\ncontains infinity. In particular, when z1 and z2 are large enough (or equivalently when our measurement is accurate enough), the value of information is submodular.\nSince there is no closed-form expression for the value of information under arbitrary allocations, we cannot verify submodularity in a simple way for problems with more than two alternatives and for correlated beliefs. Instead, it can be checked using numerical approximation and is easy to guarantee by running repeated experiments and averaging to reduce measurement noise. A necessary condition is the concavity of the value of information for measuring a fixed alternative x for n times, which can be checked exactly.\nIntuitively, we may expect that the marginal value of information should decline as we make more observations. But it is not always the case. It is shown that the value of information for measuring a single alternative may form an S-curve which is concave when there are many measurements, but may be convex at the beginning [14]. The S-curve behavior arises when the measurement noise is large and thus a single measurement simply contains too little information, leading to algorithmic difficulties and apparent paradoxes. This issue is not related to any specific policy, but rather is an inherent property of learning problems. Although the value of information is not necessarily concave, it can be made concave by measuring each alternative enough times or (equivalently) using sufficiently precise measurements."
    }, {
      "heading" : "6 Computational Experiments",
      "text" : "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3]. But none of these bounds are tight in finite time and different bounds can be based on different metrics. Hence, empirical experiments are needed to better understand the finite time performance of each policy. To this end, we propose experiments to illustrate the finite time behavior of both KG and other optimal learning policies. We consider the following learning settings that arise a lot in black box Bayesian optimization.\nEqual-prior: M = 100. The true values µx are uniformly distributed over [0, 60] and measurement noise σW = 100. θ 0 x = 30 and σ 0 x = 10 for every x.\nAsymmetric unimodular function (AUF): x is a controllable parameter ranging from 21 to 120. The objective function is F (x, ξ) = θ1 min(x, ξ) − θ2x, where θ1, θ2 and the distribution of the random variable ξ are all unknown. The aim is to solve maxx EF (x, ξ) while learning θ1, θ2 and the parameters that determine the distribution of ξ. The true distribution of ξ is taken as a normal distribution with mean 60 and standard deviation 18 (corresponding to a 30% noise ratio).\nGoldstein-Price’s function with additive noise:\nf(x, y, φ) = [1 + (x+ y + 1)2(19− 14x+ 3x2 − 14y + 6xy + 3y2)] · [30 + (2x− 3y)2(18− 32x+ 12x2 + 48y − 36xy + 27y2)] + φ,\nwhere −3 ≤ x ≤ 3, −3 ≤ y ≤ 3 and are uniformly discretized into 13 × 13 alternatives. In order to obtain the prior distribution, we follow [24] and [23] to use Latin hypercube designs for initial fit. For independent beliefs, we adopt a uniform prior with the same mean value θ0x and standard deviation σ 0 x for all alternatives. For correlated beliefs, we use a constant mean value θ0x for all alternatives and a prior covariance matrix of the form\nΣ0xx′ = σe −\n∑d i=1 λi(xi−x′i)2 ,\nwhere each arm x is a d-dimensional vector and σ, λi are constant. We adopt the rule of thumb by [24] for the default number (10 × p) of points, where p is the number of parameters to be estimated. In addition, as suggested by [23], to estimate the random errors, after the first 10 × p points are evaluated, we add one replicate at each of the locations where the best p responses are found. Maximum likelihood estimation is then used to estimate the parameters based on the points in the initial design.\nThe policies considered in this section is described as follows. EXPL: A pure exploration strategy that tests each alternative equally often. EXPT: A pure exploitation strategy, XEXPT,n(Sn) = arg maxx µ̂ n x. Interval Estimation (IE): [25]\nX IE,n(Sn) = arg max x θnx + zα/2σ n x .\nKriging: [23] Let x∗ = arg maxx(θ n x + σ n x), then\nXKriging,n(Sn) = arg max x (θnx − θnx∗)Φ( θnx − θnx∗ σnx ) + σnxφ( θnx − θnx∗ σnx ),\nwhere φ and Φ are the standard normal density and cumulative distribution functions. UCB-E: [2]\nXUCB-E,n(Sn) = arg max x µ̂nx +\n√ α\nNnx ,\nwhere µ̂nx, N n x are the sample mean of µx and number of times x has been measured up to time n. The quantity µ̂0x is initialized by measuring each alternative once. SR: [2] Let A1 = X , log(M) = 12 + ∑M i=2 1 i ,\nnm = ⌈ 1\nlog(M) n−M M + 1−m\n⌉ .\nFor each phase m = 1, ...,M − 1:\n1. For each x ∈ Am, select alternative x for nm − nm−1 rounds.\n2. Let Am+1 = Am \\ arg minx∈Am µ̂x."
    }, {
      "heading" : "6.1 Finite Time Performance of Different Policies",
      "text" : "Although the theoretical analysis in the previous section is to bound the performance of the knowledge gradient policy to the optimal policy (in theory), the optimal sequential policy is impossible to find in practice. To this end, we compare the value of KG to the expected value of the best alternative maxx µx. Define the opportunity cost (OC\nπ) of any policy π at any time step n as:\nOCπ = max x µx − µx̃n ,\nwhere x̃n = arg maxx θ n x . We illustrate the finite time behavior of the KG policy under Equal-prior and AUF with independent normal beliefs. We run KG and calculate the opportunity cost ratio = maxx µx−µx̃n\nmaxx µx in each iteration. We report the mean with 90%\nconfidence interval averaged over 1000 experiments in Figure 1. We next compare the performance of KG, IE with tuning, UCB-E with tuning, SR, EXPL and EXPT. Figure 2 shows the performance in problem classes AUF and Goldstein with independent beliefs under a measurement budget five times the number of alternatives. We run each policy for 1000 times. In each run, we pre-generate all the observations and share across different policies. We illustrate in the first column of Figure 2 the mean opportunity cost and the standard deviation of each policy over 1000 runs after the measurement budget is exhausted.\nIn order to give a comprehensive comparison based on different metrics, we also calculate the probability that the final recommendation of each policy is the optimal one and the probability that the opportunity cost of each policy is the lowest, as illustrated in the figures on the right hand side of Figure 2.\nThe three criteria characterize the behavior of policies from different perspectives. One observation is that there is no universal best policy for all problem classes or under all criteria, which means that theoretical guarantees are not by themselves reliable indicators of which policy is best for a particular problem class.\nWe also exploit correlated beliefs between alternatives in order to strengthen the effect of each measurement so that one measurement of some alternative can provide information for other alternatives.\nFirst, we present the OC of different policies after each iteration under AUF (θ2 = 0.5θ1) in Figure 3. We tune zα for IE and α for UCB for N = 400 measurements and the optimal values are zα = 0.969 and α = 6.657. Since UCB-E needs to measure each alternative once, we omit the OC for its first 100 (which is the number of alternatives) steps. KG uses independent beliefs while KGCB, IE and Kriging start from MLE fitted correlated beliefs. When incorporating correlated beliefs, a measurement of one alternative tells us something about other alternatives. As a result, KGCB learns faster\nthan KG."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we offer a new perspective of interpreting ranking and selection problems as adaptive stochastic multi-set maximization problems. We present the first finitetime bounds for the knowledge gradient on both the posterior optimality and the prior optimality. The prior view provides a cleaner relationship between the performance of the policy and the sample taken, making it possible to relate the value of information to the submodularity of the sample. We analyze the submodularity of the two-alternative case and provide other conditions for more general problems, bringing out the issue and importance of submodularity in leaning problems. We propose experiments to further illustrate the finite time behavior of the knowledge gradient policy as well as other policies with or without theoretical guarantees."
    }, {
      "heading" : "A Proofs",
      "text" : "A.1 Proof of Lemma 4.5\nFor any ψ with |ψ| = n, we consider the resulting knowledge state Sn = (θnx , βnx )x∈X . Since σW 6= 0, there exists such ψ that maxx θnx > maxx 6=x′ θnx with positive probability. Now consider another realization ψ′ with dom(ψ′) = dom(ψ) ∪ {x2}, where x2 is the second largest alternative of θnx . We denote the observation of x2 in ψ\n′ as W2 and the resulting Sn+1 as (θn+1x , β n+1 x )x∈X according to Bayes’ rule. The knowledge gradient ∆(x|ψ) = νKG,nx can be analytically expressed by\nνKG,nx = σ̃ n xf(ζ n x ),\nwhere σ̃nx = √ (βnx ) −1 − (βnx + βW )−1, ζnx = − ∣∣∣ θnx−maxx′ 6=x θnx′σ̃nx ∣∣∣ and f(ζ) = ζΦ(ζ) + φ(ζ). Φ(ζ) and φ(ζ) are, respectively, the cumulative standard normal distribution the standard normal density [15]. We first notice that f ′(ζ) = Φ(ζ) ≥ 0 for any ζ ∈ R so that f(ζ) is non-decreasing. We next compare νKG,nx1 and ν KG,n+1 x1 for x1 = arg maxx θ n x . According to Bayes’ rule, the precision β of x2 changes only when x2 is measured. So we have σ̃nx = σ̃ n+1 x . Similarly we have all the θ n+1 x unchanged except for alternative x2. By some algebra, it can be shown that for any W2 such that θ n x2 < W2 ≤ βnx2 βW (θnx1−θ n x2 )+θnx1 , we have νKG,nx1 < ν KG,n+1 x1 . Since θnx1 > θ n x2\nby construction, such W2 can be obtained with positive probability.\nA.2 Proof of Proposition 1\nIn this appendix, we prove the properties of submodular multi-set functions. We prove the equivalence by showing 2)⇒ 1)⇒ 3)⇒ 4)⇒ 2).\n• 2)⇒ 1). Take S ⊆ T and T −S = {x1, x2, ..., xr}. Then from 3) we have ρx(S) ≥ ρx(S∪{x1}), ρx(S∪{x1}) ≥ ρx(S∪{x1, x2}),..., ρx(S∪{x1, x2, ..., xr−1}) ≥ ρx(T ). Summing these r inequalities yields 1).\n• 1) ⇒ 3). For arbitrary S and T with T − S = {x1, x2, ..., xr} and S − T = {y1, y2, ..., yq}, from 1) we have\ng(S ∪ T )− g(S) = r∑ t=1 [g(S ∪ {x1, ..., xt})− g(S ∪ {x1, ..., xt−1})]\n= r∑ t=1 ρxt(S ∪ {x1, ..., xt−1})\n≤ r∑ t=1 ρxt(S) = ∑ x∈T−S ρx(S). (8)\nAnd\ng(S ∪ T )− g(T ) = q∑ t=1 [g(T ∪ {y1, ..., yt})− g(T ∪ {y1, ..., yt−1})]\n= q∑ t=1 ρyt(T ∪ {y1, ..., yt} − {yt}})\n≥ q∑ t=1 ρyt(T ∪ S − {yt}) = ∑ x∈S−T ρx(S ∪ T − {x}). (9)\nSubtracting equation (9) from equation (8) we get 3).\n• 3)⇒ 4). If S ⊆ T , S − T = ∅, and therefore the last term in 3) vanishes.\n• 4)⇒ 2). Substitute T = S ∪ {x, y} into 4) to obtain\ng(S ∪ {x, y}) ≤ g(S) + ρx(S) + ρy(S) = ρx(S) + g(S ∪ {y}).\nRearrange this inequality, we get\nρx(S ∪ {y}) = g(S ∪ {x, y})− g(S ∪ {y} ≤ ρx(S).\nA.3 Proof of Proposition 4.12\nLet z∗(Z, π,Φ) be the next adaptive greedy choice that maximizes the expected marginal increment given that policy π has generated Z. We first show that\nF π2@π1 ≤ F π2 + n1 ∑ Z∈Zn P(π2 Z) ( E [ v̂(Z ∪ {z∗(Z, π2,Φ)},Φ) ] − v(Z) ) for all policies π1 with a measurement budget n1 and π2 with a budget n2 under any prior and probability distribution that describes a measurement.\nProof. Let π[j] denote the first j measurement decisions under some policy π. First of all we break F π2@π1 − F π2 into n1 consecutive differences,\nF π2@π1 − F π2 = n1∑ j=1 ( F π2@π [j] 1 − F π2@π [j−1] 1 ) .\nSimilar to what we did in the last lemma, for each difference we have\nF π2@π [j] 1 − F π2@π [j−1] 1 = ∑\nZ1∈Zn2+j P(π2@π[j]1 Z1)v(Z1)− ∑ Z2∈Zn2+j−1 P(π2@π[j−1]1 Z2)v(Z2)\n= ∑\nZ1∈Zn2+j ∑ Z2∈Zn2+j−1,Z2∪Z3=Z1 P(π2@π[j−1]1 Z2)P(π {j} 1 Z3|π2@π [j−1] 1 Z2)v(Z1)\n− ∑\nZ2∈Zn2+j−1 ∑ Z3∈Z1 P(π2@π[j−1]1 Z2)P(π {j} 1 Z3|π2@π [j−1] 1 Z2)v(Z2)\n= ∑\nZ2∈Zn2+j−1 ∑ Z3∈Z1 P(π2@π[j−1]1 Z2)P(π {j} 1 Z3|π2@π [j−1] 1 Z2) ( v(Z2 ∪ Z3)− v(Z2) ) .\nNow we consider all possible pair (Z4, Z5) such that Z4 ∈ Zn2 , Z5 ∈ Zj−1 and Z4∪Z5 = Z2. Notice that the policy π2@π [j] 1 employs a fresh start at the time n2, therefore the events before and after time n2 are independent. Then we have∑ Z2∈Zn2+j−1 ∑ Z3∈Z1 P(π2@π[j−1]1 Z2)P(π {j} 1 Z3|π2@π [j−1] 1 Z2) ( v(Z2 ∪ Z3)− v(Z2)\n) =\n∑ Z2∈Zn2+j−1 ∑ Z4∪Z5=Z2 ∑ Z3∈Z1 P(π2 Z4)P(π[j−1]1 Z5)P(π {j} 1 Z3|π2@π [j−1] 1 Z2)\n× ( v(Z2 ∪ Z3)− v(Z2) ) .\nBased on the submodular property of function v, we have\nv(Z2 ∪ Z3)− v(Z2) ≤ v(Z4 ∪ Z3)− v(Z4).\nThen from the definition of z∗, we have\nv(Z4 ∪ Z3)− v(Z4) = E[v̂(Z4 ∪ Z3,Φ)− v̂(Z4,Φ)] = EΦ [ E[v̂(Z4 ∪ Z3,Φ)− v̂(Z4,Φ)|Zπ2(Φ) = Z4] ] ≤ EΦ [ E[v̂(Z4 ∪ {z∗(Z4, π2,Φ)},Φ)− v̂(Z4,Φ)|Zπ2(Φ) = Z4]\n] = EΦ[v̂(Z4 ∪ {z∗(Z4, π2,Φ)},Φ)]− v(Z4).\nCombining the last two inequalities, we have∑ Z2∈Zn2+j−1 ∑ Z4∪Z5=Z2 ∑ Z3∈Z1 P(π2 Z4)P(π[j−1]1 Z5)P(π {j} 1 Z3|π2@π [j−1] 1 Z2)\n× ( v(Z2 ∪ Z3)− v(Z2) ) ≤\n∑ Z2∈Zn2+j−1 ∑ Z4∪Z5=Z2 ∑ Z3∈Z1 P(π2 Z4)P(π[j−1]1 Z5)P(π {j} 1 Z3|π2@π [j−1] 1 Z2)\n× ( Ev̂(Z4 ∪ {z∗(Z4, π2,Φ)},Φ)− v(Z4) ) =\n∑ Z2∈Zn2+j−1 ∑ Z4∪Z5=Z2 P(π2 Z4)P(π[j−1]1 Z5) ( Ev̂(Z4 ∪ {z∗(Z4, π2,Φ)},Φ)− v(Z4) ) =\n∑ Z4∈Zn2 ∑ Z5∈Zj−1 P(π2 Z4)P(π[j−1]1 Z5) ( Ev̂(Z4 ∪ {z∗(Z4, π2,Φ)},Φ)− v(Z4) ) =\n∑ Z4∈Zn2 P(π2 Z4) ( Ev̂(Z4 ∪ {z∗(Z4, π2,Φ)},Φ)− v(Z4) ) ,\nand this ends the proof.\nSet π1 = π ∗ and π2 = KG [n−1] in Lemma 4.11 and the above proposition then what left to show is that\nFKG [n] − FKG[n−1] ≥ ∑ Z∈Zn P(π2 Z) ( Ev̂(Z ∪ {z∗(Z,KG[n−1],Φ)},Φ)− v(Z) ) .\nFrom the definition, the left hand side of the last equation:\nFKG [n] − FKG[n−1] = ∑ Z1∈Zn+1 P(KG Z1)v(Z1)− ∑ Z2∈Zn P(KG Z2)v(Z2)\n= ∑ Z2∈Zn ∑ Z3∈Z1 P(KG Z2)P(KG Z3|KG Z2)v(Z2 ∪ Z3)\n− ∑ Z2∈Zn P(KG Z2)v(Z2).\nNow it is enough to show that∑ Z3∈Z1 P(KG Z3|KG Z2)v(Z2 ∪ Z3)− v(Z2)\n≥ Ev̂(Z2 ∪ {z∗(Z2,KG[n−1],Φ)},Φ)− v(Z2).\nWe could group together the partial realizations ψ that lead to the same single step optimal decision z∗(Z2,KG\n[n−1],Φ), and then the last inequality follows from the adaptive greedy nature of the KG policy.\nA.4 Proof of Theorem 5.2\nFirst of all, we consider the case when f is a two dimensional function and the four points we pick form a rectangle. Assume f(x, y) is submodular. For any given point (x0, y0), we have f(x0+t+s, y0)−f(x0+t, y0) ≤ f(x0+s, y0)−f(x0, y0) and f(x0+t, y0)−f(x0, y0) ≤ f(x0 + t, y0 + s) − f(x0, y0 + s) for any s, t > 0. From the first inequality we get fxx(x0, y0) ≤ 0 directly. From the second inequality, we have fx(x0, y0) ≤ fx(x0, y0 + s), and finally fx,y(x0, y0) ≤ 0. On the other hand, if we have fxy ≤ 0, fxx ≤ 0, for any (x, y), then due to the fact that f(x0+t, y0+s)−f(x0+t, y0)− ( f(x0, y0+s)−f(x0, y0)\n) =∫ x0+t\nx0 ∫ y0+s y0 fxy(u, v)dudv ≤ 0, f(x0+t+s, y0)−f(x0+t, y0)− ( f(x0+s, y0)−f(x0, y0) ) = stfxx(x0 + ξ, y0) ≤ 0, we obtain the submodularity. We next consider the general case when f is n dimensional and the four points only form a parallelogram. Since the difference between the two marginal values can be decomposed into summation of several marginal value differences whose reference points form rectangles that parallel to coordinate planes, the result for the general case is straightforward from the two dimensional case."
    } ],
    "references" : [ {
      "title" : "Sample mean based index policies with o(logn) regret for the multiarmed bandit problem",
      "author" : [ "R. Agrawal" ],
      "venue" : "Adv. in Appl. Probab., ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "S",
      "author" : [ "J.-Y. Audibert" ],
      "venue" : "Bubeck, et al., Best arm identification in multi-armed bandits, COLT 2010-Proceedings, ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits",
      "author" : [ "J.-Y. Audibert", "R. Munos", "C. Szepesvári" ],
      "venue" : "Theor. Comput. Sci., 410 ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Mach. Learn., 47 ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Optimal learning for sequential sampling with non-parametric beliefs",
      "author" : [ "E. Barut", "W.B. Powell" ],
      "venue" : "J. Global Optim., ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Selecting a selection procedure",
      "author" : [ "J. Branke", "S.E. Chick", "C. Schmidt" ],
      "venue" : "Manag. Sci., 53 ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Bandits with heavy tail",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "arXiv preprint arXiv:1209.1727, ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "G",
      "author" : [ "O. Cappé", "A. Garivier", "O.-A. Maillard", "R. Munos" ],
      "venue" : "Stoltz, et al., Kullback–leibler upper confidence bounds for optimal sequential allocation, Ann. Statist., 41 ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A gradient approach for smartly allocating computing budget for discrete event simulation",
      "author" : [ "C.-H. Chen", "H.-C. Chen", "L. Dai" ],
      "venue" : "28th Proc. Winter Simul., IEEE Computer Society",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Efficient dynamic simulation allocation in ordinal optimization",
      "author" : [ "C.-H. Chen", "D. He", "M. Fu" ],
      "venue" : "IEEE Trans. Automat. Control, 51 ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Simulation budget allocation for further enhancing the efficiency of ordinal optimization",
      "author" : [ "C.-H. Chen", "J. Lin", "E. Yücesan", "S.E. Chick" ],
      "venue" : "Discrete Event Dyn. Syst., 10 ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "New two-stage and sequential procedures for selecting the best simulated system",
      "author" : [ "S.E. Chick" ],
      "venue" : "Oper. Res., 49 ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The knowledge-gradient policy for correlated normal beliefs",
      "author" : [ "P. Frazier", "W. Powell", "S. Dayanik" ],
      "venue" : "INFORMS J. Comput., 21 ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Paradoxes in learning and the marginal value of information",
      "author" : [ "P.I. Frazier", "W.B. Powell" ],
      "venue" : "Decis. Anal., 7 ",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A knowledge-gradient policy for sequential information collection",
      "author" : [ "P.I. Frazier", "W.B. Powell", "S. Dayanik" ],
      "venue" : "SIAM J. Control Optim., 47 ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Bandit processes and dynamic allocation indices",
      "author" : [ "J.C. Gittins" ],
      "venue" : "J. R. Stat. Soc. Ser. B. Stat. Methodol., ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Adaptive submodularity: A new approach to active learning and stochastic optimization",
      "author" : [ "D. Golovin", "A. Krause" ],
      "venue" : "COLT,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Bayesian look ahead one-stage sampling allocations for selection of the best population",
      "author" : [ "S.S. Gupta", "K.J. Miescke" ],
      "venue" : "J. Statist. Plann. Inference, 54 ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "G",
      "author" : [ "I. Guttman" ],
      "venue" : "C. Tiao, et al., A bayesian approach to some best population problems, Ann. Math. Statist., 35 ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Opportunity cost and OCBA selection procedures in ordinal optimization for a fixed number of alternative systems",
      "author" : [ "D. He", "S.E. Chick", "C.-H. Chen" ],
      "venue" : "IEEE Trans. Syst. , Man, and Cybern., Part C: Applications and Reviews, 37 ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Information value theory",
      "author" : [ "R. Howard" ],
      "venue" : "IEEE Trans Syst. Sci. and Cybern., 2 ",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "Global optimization of stochastic black-box systems via sequential kriging meta-models",
      "author" : [ "D. Huang", "T.T. Allen", "W.I. Notz", "N. Zeng" ],
      "venue" : "J. Global Optim., 34 ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Efficient global optimization of expensive black-box functions",
      "author" : [ "D.R. Jones", "M. Schonlau", "W.J. Welch" ],
      "venue" : "J. Global Optim., 13 ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learning and classifying under hard budgets",
      "author" : [ "A. Kapoor", "R. Greiner" ],
      "venue" : "Springer",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Regret bounds for sleeping experts and bandits",
      "author" : [ "R. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma" ],
      "venue" : "Mach. Learn., 80 ",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Adv. Appl. Math., 6 ",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Hierarchical knowledge gradient for sequential sampling",
      "author" : [ "M.R. Mes", "W.B. Powell", "P.I. Frazier" ],
      "venue" : "J. Mach. Learn. Res., 12 ",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The knowledge-gradient algorithm for sequencing experiments in drug discovery",
      "author" : [ "D.M. Negoescu", "P.I. Frazier", "W.B. Powell" ],
      "venue" : "INFORMS J. Comput., 23 ",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functions",
      "author" : [ "G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher" ],
      "venue" : "Math. Program., 14 ",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Optimal learning",
      "author" : [ "W.B. Powell", "I.O. Ryzhov" ],
      "venue" : "vol. 841",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Applied statistical decision theory",
      "author" : [ "H. Raiffa", "R. Schlaifer" ],
      "venue" : "Harvard Business School Publications, ",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "Gaussian process optimization in the bandit setting: No regret and experimental design",
      "author" : [ "N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger" ],
      "venue" : "arXiv preprint arXiv:0912.3995, ",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The economics of information",
      "author" : [ "G.J. Stigler" ],
      "venue" : "J. political Econ., ",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "The knowledge gradient for sequential decision making with stochastic binary feedbacks",
      "author" : [ "Y. Wang", "C. Wang", "W. Powell" ],
      "venue" : "33rd Proc. ICML",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Value of information literature analysis: a review of applications in health risk management",
      "author" : [ "F. Yokota", "K.M. Thompson" ],
      "venue" : "Med. Decis. Mak., 24 ",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Raiffa and Schlaifer established the Bayesian framework for R&S problems [33].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "al made a thorough comparison of several fully sequential sampling procedures [6].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].",
      "startOffset" : 66,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].",
      "startOffset" : 66,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].",
      "startOffset" : 66,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 17,
      "context" : "Another single-step Bayesian look-ahead policy first introduced by [19] and then further studied by [15] is called the “knowledge-gradient policy” (KG).",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "Another single-step Bayesian look-ahead policy first introduced by [19] and then further studied by [15] is called the “knowledge-gradient policy” (KG).",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "al modified the knowledge-gradient policy to handle correlated multivariate normal belief on the mean values of these rewards [13].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "A similar field is the multi-armed bandit problem, which were originally studied under Bayesian assumptions [17].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 25,
      "context" : "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].",
      "startOffset" : 131,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].",
      "startOffset" : 131,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].",
      "startOffset" : 131,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].",
      "startOffset" : 131,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].",
      "startOffset" : 131,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : "To accomplish this, we build on the general structure of the analysis of greedy algorithms given in [31] and [18].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "To accomplish this, we build on the general structure of the analysis of greedy algorithms given in [31] and [18].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "For R&S problems, the knowledge gradient is a policy that at the nth iteration chooses its (n + 1)st measurement from X to maximize the single-period expected increase in value [15, 13].",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 12,
      "context" : "For R&S problems, the knowledge gradient is a policy that at the nth iteration chooses its (n + 1)st measurement from X to maximize the single-period expected increase in value [15, 13].",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 27,
      "context" : "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 33,
      "context" : "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 28,
      "context" : "We follow the general structure of the analysis of greedy approximation [31] to develop the first finite-time bound for the knowledge gradient policy for R&S problems as follows.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "1 ([32]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "The definition of the knowledge gradient ν x coincides with the Conditional Expected Marginal Benefit ∆(e|ψ) defined by [18]: ∆(e|ψ) := E [ f ( dom(ψ) ∪ {e},Φ ) − f ( dom(ψ),Φ ) |Φ ∼ ψ ] .",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "We generalize the definition of adaptive monotonicity and adaptive submodularity for set functions given by [18] to multi-set functions as follows.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "Since for any x, θ x = θ x + σ̃(Σ, x)Z, where σ̃(Σ, x) = Σex √ 1/β+Σxx and the random variable Z is standard normal when conditioned on F [13].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 32,
      "context" : "Stigler considers the value of information in economics when buyers search for the best price [35].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 20,
      "context" : "Howard laid the groundwork for the value of information in a decision-theoretic context and spawned a great deal of work in this area [22].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "Yokota and Thompson gives a first comprehensive review of value of information analyses related to health risk management [37].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 30,
      "context" : "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].",
      "startOffset" : 241,
      "endOffset" : 260
    }, {
      "referenceID" : 23,
      "context" : "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].",
      "startOffset" : 241,
      "endOffset" : 260
    }, {
      "referenceID" : 8,
      "context" : "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].",
      "startOffset" : 241,
      "endOffset" : 260
    }, {
      "referenceID" : 11,
      "context" : "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].",
      "startOffset" : 241,
      "endOffset" : 260
    }, {
      "referenceID" : 14,
      "context" : "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].",
      "startOffset" : 241,
      "endOffset" : 260
    }, {
      "referenceID" : 28,
      "context" : "Since the value of information is a multi-set function, we first generalize the definitions and properties of submodular set functions described by [31] to submodular multi-set functions.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 28,
      "context" : "1 in [31].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 16,
      "context" : "[18] A concatenated policy π = π1@π2 is constructed by running π1 to completion, and then running policy π2 from a fresh start ignoring all the information collected while running π1.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] For a policy π, define the j-truncation π of π as the policy that runs exactly (j + 1) steps under π’s decision rule and π{j} as the single step policy that randomly chooses an alternative according to the probability distribution of policy π’s decision for the (j + 1)-th step.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "The proof of this lemma can be found in [31].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "The concavity of the value of information has been studied extensively by [14].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "The value of information v(z) = s(z)f(− |θ 0 1−θ 2 | s(z) ), where s(z) = √ σ̃ 1(z1) + σ̃ 2 2(z2), σ̃ 2 i (zi) = σ i zi σ2 W /σ 2,0 i +zi , f(a) = aΦ(a) + φ(a), Φ and φ are the standard normal cumulative distribution and density respectively [14].",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 13,
      "context" : "Although the value of information is not concave in general in the two-alternative case, v is concave on the region where all zi’s are large enough (see Theorem 2 in [14]).",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "Concavity of v(z) is proven in Remark 2 by [14].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "It is shown that the value of information for measuring a single alternative may form an S-curve which is concave when there are many measurements, but may be convex at the beginning [14].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 25,
      "context" : "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 31,
      "context" : "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 3,
      "context" : "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 2,
      "context" : "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 22,
      "context" : "In order to obtain the prior distribution, we follow [24] and [23] to use Latin hypercube designs for initial fit.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 21,
      "context" : "In order to obtain the prior distribution, we follow [24] and [23] to use Latin hypercube designs for initial fit.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "We adopt the rule of thumb by [24] for the default number (10 × p) of points, where p is the number of parameters to be estimated.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : "In addition, as suggested by [23], to estimate the random errors, after the first 10 × p points are evaluated, we add one replicate at each of the locations where the best p responses are found.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : "Kriging: [23] Let x∗ = arg maxx(θ n x + σ n x), then X(S) = arg max x (θ x − θ x∗)Φ( θ x − θ x∗ σn x ) + σ xφ( θ x − θ x∗ σn x ),",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "UCB-E: [2] X(S) = arg max x μ̂x + √ α Nn x ,",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "SR: [2] Let A1 = X , log(M) = 12 + ∑M i=2 1 i ,",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "Φ(ζ) and φ(ζ) are, respectively, the cumulative standard normal distribution the standard normal density [15].",
      "startOffset" : 105,
      "endOffset" : 109
    } ],
    "year" : 2016,
    "abstractText" : "We consider sequential decision problems in which we adaptively choose one of finitely many alternatives and observe a stochastic reward. We offer a new perspective of interpreting Bayesian ranking and selection problems as adaptive stochastic multi-set maximization problems and derive the first finite-time bound of the knowledge-gradient policy for adaptive submodular objective functions. In addition, we introduce the concept of prior-optimality and provide another insight into the performance of the knowledge gradient policy based on the submodular assumption on the value of information. We demonstrate submodularity for the two-alternative case and provide other conditions for more general problems, bringing out the issue and importance of submodularity in learning problems. Empirical experiments are conducted to further illustrate the finite time behavior of the knowledge gradient policy.",
    "creator" : "LaTeX with hyperref package"
  }
}