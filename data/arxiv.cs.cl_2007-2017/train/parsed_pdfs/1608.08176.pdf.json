{
  "name" : "1608.08176.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based SE)",
    "authors" : [ "Amritanshu Agrawal", "Wei Fu", "Tim Menzies" ],
    "emails" : [ "aagrawa8@ncsu.edu", "wfu@ncsu.edu", "tim.menzies@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "When run on SE data, LDA suffers from “order effects” i.e. different topics be generated if the training data was shuffled into a different order. Such order effects introduce a systematic error for any study that uses topics to make conclusions.\nThis paper introduces LDADE, a Search-Based SE tool that tunes LDA’s parameters using DE (Differential Evolution). LDADE has been tested on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of SE papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (the traditional VEM method, or using Gibbs sampling). In all tests, the pattern was the same: LDADE’s tunings dramatically reduces topic instability.\nThe implications of this study for other software analytics tasks is now an open and pressing issue. In how many domains can search-based SE dramatically improve software analytics?\nKeywords—Topic modeling, Stability, LDA, tuning, differential evolution.\nI. INTRODUCTION\nThe current great challenge in software analytics is understanding unstructured data. As shown in Figure 1, most of the planet’s 1600 Exabytes of data does not appear in structured sources (databases, etc) [45]. Rather it is unstrucutred data, often in free text, and found in word processing files, slide presentations, comments, etc etc.\nSuch unstructured data does not have a pre-defined data model and is typically text-heavy. Finding insights among unstructured text is difficult unless we can search, characterize, and classify the textual data in a meaningful way. One of the common techniques for finding related topics within unstructured text (an area called topic modeling) is Latent Dirichlet Allocation (LDA) [6]. Topic modeling is widely used in SE (see Table I) and many papers in recent years have reported topic modeling results at numerous SE venues (see Figure 2 and Table II).\nResearchers can use topic models in one of two ways. Firstly, topics may be used as feature selector that finds useful inputs which are then used by, say, a classifier to characterize different kinds of software (e.g. buggy or not [8]). In this mode, no\nhuman need ever read the generated topics and instabilities in generated topics is less of an issue.\nSecondly, researchers may present and reflect on the generated topics in order to offer some insight into the structure of the data. In this second mode, researchers present and discuss the specifics of the generated topics in order to defend particular conclusions.\nThis paper is about a systematic error relating to this second mode. Specifically, we show that the generated topics from Latent Dirichlet Allocation (LDA), a widely-used topic modeling algorithm in SE, are subject to “order effects”. That is, if the order of the input examples is changed, the generated topics will also change– often dramatically so. Hence, any conclusion based on an order effect is unstable since that conclusion is the result of a (randomly selected) input ordering.\nTo fix this problem, we proposes LDADE: a combination of LDA and a search-based optimizer (differential evolution, or DE) [57]) that automatically tunes LDA’s < k, α, β > parameters. This paper tests LDADE on:\n• Data from a programmer information exchange site (Stackoverflow); • Title and abstract text of 9291 SE papers; • Software defect reports from NASA.\nUsing these datasets, we explore these research questions:\n• RQ1: Are the default settings of LDA incorrect? We will show that using the default settings of LDA for SE data can lead to systematic errors since stability scores start to drop after n = 5 terms per topic.\nar X\niv :1\n60 8.\n08 17\n6v 1\n[ cs\n.S E\n] 2\n9 A\nug 2\n01 6\n• RQ2: Does LDADE improve the stability scores? In our work, we will show dramatic improvement in the stability scores with the parameters found automatically by DE. We would strongly recommend tuning for future LDA studies. • RQ3: Do different data sets need different configurations to make LDA stable? We will show that DE finds different “best” parameter settings for different data sets. Hence reusing tunings suggested by other researchers from other data sets. Instead, use automatic tuning methods to find the best tuning parameters for the current data set. • RQ4: Is tuning easy? We show that, measured in the terms of the internal search space of the optimizer, tuning LDA is much simpler than standard optimization methods. • RQ5: What is the runtime cost of tuning? The advantages of LDADE come at some cost: tuning with DE makes LDA three to seven times slower. While this is definitely more than not using LDA, this may not be an arduous increase given modern cloud computing environments. • RQ6: Should topic modeling be used “off-the-shelf” with their default tunings? Based on these findings, our answer to this question is an emphatic “no”. We can see little reason to use “off-the-shelf” LDA by those who are making conclusions out of these topics.\nBefore proceeding, we offer three caveats. Firstly, having mitigated for unstable topics in LDA, the reader may be asking “ok, but what useful conclusions can you draw from this new kind of stabler LDA?”. In a companion paper submitted to ICSE’17 [37] we apply LDADE to 9291 SE papers to find trends in topics between 1993 to 2013. That paper reports (a) what topics that are becoming more/less prominent; (b) what topics are most frequently accepted to what conferences; (c) which conferences mostly overlap with other conferences; (d)\nwhat topics most distinguish different conferences. Finally, that study can assist in teaching via (e) a list of the 21 most cited papers within the seven top topics of SE.\nSecondly, as witnessed by the central columns of Table I, many prior papers have commented that the results of a topic modeling analysis can be effected by tuning the control parameters of LDA. Yet a repeated pattern in the literature is that, despite these stated concerns, researchers rarely take the next step to find ways to better control tunings. To the best of our knowledge, apart from this paper, there are no reports in the SE literature of simple reproducible automatic methods that can resolve the problem of unstable topics.\nThirdly, while the specifics of this paper are about taming instability in topic modeling, we also think that this work makes a more general point. This is the second time we have applied DE to automatically adjust the control parameters of an algorithm used in software analytics. Previously [14], we showed that (a) tuning software defect predictors can lead to large improvements of the performance of those predictors; and that (b) those tunings are different in different data sets; so (c) it is important to apply automatic tuning as part of the analysis of any new data set. Note that those old conclusions are the same as for this paper. These two results suggests that it is time to revisit past results to check:\n• In the past, just how poorly have we been using software analytics in the past? • And how much better can we improve future results using methods taken from search-based software engineering?"
    }, {
      "heading" : "II. RELATED WORK",
      "text" : ""
    }, {
      "heading" : "A. About Tuning: Important and Ignored",
      "text" : "The impact of tuning are well understood in the theoretical machine learning literature [4]. When we tune a data miner, what we are really doing is changing how a learner applies its heuristics. This means tuned data miners use different heuristics, which means they ignore different possible models, which means they return different models; i.e. how we learn changes what we learn.\nYet issues relating to tuning are poorly addressed in the software analytics literature. Wei et al. [14] surveyed hundreds of recent SE papers in the area of software defect prediction from static code attributes. They found that most SE authors do not take steps to explore tunings (rare exception: [59]). For example, Elish et al. [11] compared support vector machines to other data miners for the purposes of defect prediction. That paper tested different “off-the-shelf” data miners on the same data set, without adjusting the parameters of each individual learner. Yet Wei et al. showed that (a) finding useful tunings for software defect prediction is remarkably easy using DE; (b) different data sets require different tunings; hence, for software defect prediction, (c) tuning is essential for all new data sets.\nAccordingly, we are now engaged in an on-going research project that explores tuning for software analytics.\n• Find some data mining widely used in SE; • Check the conclusions of that data miner under different\nparameter tunings;\n• See if those different tunings significantly change the results of that learner; • Look for ways to better manage the exploration of those tunings.\nThis paper explores these four steps in the context of topic modeling."
    }, {
      "heading" : "B. About Topic Modeling",
      "text" : "Latent Dirichlet Allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. It learns the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document). What makes topic modeling interesting is that these algorithms scale to very large text corpuses. For example, in this paper, we apply to whole of Stackoverflow, as well as to two other large text corpuses in SE.\nInitially, α, β may be set randomly as follows: each word in a document was generated by first randomly picking a topic (from the document’s distribution of topics) and then randomly picking a word (from the topic’s distribution of words). Successive iterations of the algorithm count the implications of prior sampling which, in turn, incrementally updates α, β.\nApart from α, β, the other parameters that define LDA are:\n• k = number of topics."
    }, {
      "heading" : "C. About Order Effects",
      "text" : "This paper uses tuning to fix “order effects” in topic modeling. Langley [18] defines such effects as follows:"
    }, {
      "heading" : "A learner L exhibits an order effect on a training set",
      "text" : ""
    }, {
      "heading" : "T if there exist two or more orders of T for which",
      "text" : "L produces different knowledge structures.\nMany learners exhibit order effects: e.g. certain incremental clustering algorithms generate different clusters, depending on the order with which they explore the data [18]. Hence, some algorithms survey the space of possible models across numerous random divisions of the data (e.g. Random Forests [7]).\nFrom the description offered above in §II-B, we can see (a) how topic modeling might be susceptible to order effects and (b) how such order effects might be tamed:\n• In the above description, k, α, β are initialized at random then updated via an incremental re-sampling process. Such incremental updates are prone to order effects. • One technique to reduce the effect of different data orderings is to initialize, k, α, β to some large value. The trick when applying this technique is that different data sets will requiring different initializations; i.e. the tuning process will have to be repeated for each new data set."
    }, {
      "heading" : "D. WorkArounds for LDA Instability Problem",
      "text" : "In order to understand how other researchers have explored LDA instability, in April 2016, we searched scholar.google.com for the conjunction of “lda” and “topics” or “stable” or “unstable” or “coherence”. Since 2012, there are 189 such papers, 57 of which are related to software engineering results. Of those papers:\n• 28 mention instability in LDA. • Of those 28, despite mentioning stability problems, 10\npapers used LDA’s “off-the-shelf” parameters; • The other papers used some combination of manual\nadjustment or some under-explained limited exploration of tunings based on “engineering judgment” (i.e. some settings guided by the insights of the researchers). • Only 3 of the authors acknowledge that tuning might have a large impact on the results.\nApart from tuning, there are several other workarounds explored in the literature in order to handle LDA instability. Overall, there was little systematic exploration of tuning and LDA in the SE literature. Instead, researchers relied on other methods that are less suited to automatic reproduction of prior results.\nIn the literature, researchers [23], [35], [36] manually accessed the topics and then used for further experiments. Some made use of Amazon Mechanical Turk to create goldstandard coherence judgements [28]. All these solutions are related to results stability rather than model stability. Note that this workaround takes extensive manual effort and time.\nAnother approach to taming LDA instability is to incorporate user knowledge into the corpus. For example, SCLDA [68] can handle different kinds of knowledge such as word correlation, document correlation, document label and so on. Using such user knowledge, while certainly valuable, is somewhat subjective. Hence, for reasons of reproducibility, we prefer fully automated methods.\nSome researchers [33], [52], [58] used Genetic Algorithm (GA) to tune the parameters– but for different purposes than LDADE. Annibale et al. [52] used GAs to increase the precision of predictions made by a classifier; i.e. they are exploring supervised classification while the task explored in this paper is unsupervised clustering.\nFinally, other researchers explore some limited manual parameter tuning for LDA (e.g. experiment with one parameter: cluster size) [16], [63] achieved higher stability by just increasing the number of cluster size. Note that the automatic\ntuning methods explored by this paper can explore multiple parameters. Further, our analysis is repeatable."
    }, {
      "heading" : "III. METHODS",
      "text" : "This section describes our evaluation methods for measuring instability as well as the optimization methods used to reduce that instability."
    }, {
      "heading" : "A. Data Sets",
      "text" : "To answer our research questions, and to enable reproducibility of our results, we use three open source datasets summarized in Table III and described below.\nPITS is a text mining data set generated from NASA software project and issue tracking system (PITS) reports [38], [41]. This text discusses bugs and changes found in big reports and review patches. Such issues are used to manage quality assurance, to support communication between developers. Topic modeling in PITS can be used to identify the top topics which can identify each severity separately. The dataset can be downloaded from the PROMISE repository [40]. Note that, this data comes from six different NASA projects, which we label as PitsA, PitsB, etc.\nStackoverflow is the flagship site of the Stack Exchange Network which features questions and answers on a wide range of topics in computer programming. Topic modeling on Stackoverflow is useful for finding patterns in programmer knowledge. This data can be downloaded online1.\nCitemap contains titles and abstracts of 9291 papers from a database of 11 senior software engineering conferences from 1993-2013. This data was obtained in the form of an SQL dump from the work of Vasilescu et al. [64]. This dataset is available on-line2.\nFor this study, all datasets were preprocessed using the usual text mining filters [13]:\n• Stop words removal using NLTK toolkit3 [5] : ignore very common short words such as “and” or “the”. • Porter’s stemming filter [54]: delete uninformative word endings; e.g. after stemming, all the following words would be rewritten to “connect”: “connection”, “connections”, “connective”, “connected”, “connecting”.\n1http://tiny.cc/SOProcess 2https://github.com/ai-se/citemap/blob/master/citemap.csv 3http://www.nltk.org/book/ch02.html\n• PITS and Citemap is small enough to process on a single (four core) desktop machine using Scikit-Learn [53] and Python. • StackOverflow is so large (7GB) that its processing requires extra hardware support. This study used a Spark and Mllib on a cluster of 45 nodes to reduce the runtime."
    }, {
      "heading" : "B. Similarity Scoring",
      "text" : "To evaluate topics coherence in LDA, there is a direct approach, by asking people about topics, and an indirect approach by evaluating pointwise mutual information (PMI) [28], [50] between the topic words. We could not use any of these criteria, as it requires experts to have domain knowledge. Perplexity is the inverse of the geometric mean per-word likelihood. The smaller the perplexity, the better (less uniform) is the LDA model. The usual trend is that as the value of perplexity drops, the number of topics should grow [26]. Researchers caution that the value of perplexity doesn’t remain constant with different topic size and with dictionary sizes [26], [69]. A lot depend on the code implementation of perplexity\nand the type of datasets used. Since, we are using different implementations of LDA across different platforms on various datasets, we are not using perplexity as evaluation measure.\nAnother approach researchers have used is Jaccard Similarity [16], [50]. We define our measure similar to it. For this work, we assess topic model stability via the median number overlaps of size n words (size of topic), which we denote <n.\nFor this measurement, we first determine the maximum size of topics we will study. For that purpose, we will study the case of n ≤ 9 (we use 9 as our maximum size since the cognitive science literature tells us that 7±2 is a useful upper size for artifacts to be browsed by humans [42]). .\nNext, for 1 ≤ n ≤ 9, we will calculate the median size of the overlap, computed as follows:\n• Let one run of our rig shuffle the order of the training data, then builds topic models using the data; • m runs of our rig executes m copies of one run, each time using a different random number seed, • We say topics are stable, when there are x occurrences of n terms appearing in all the topics seen in the m runs.\nFor example, consider the topics shown in Figure 4. These are generated via four runs of our system. In this hypothetical example, we will assume that the runs of Figure 4 were generated by an LDA suffering from topic instability. For n = 5, we note that Topic 0 of run1 scores 24 = 0.5 since it shares 5 words with topics in only two out of four runs. Repeating that calculation for the other run1 topics shows that:\n• Topic 1 of run1 scores 34 = 0.75;\n• Topic 2 or run1 scores 14 = 0.25; • Topic 3 of run1 scores 44 = 1.\nFrom this information, we can calculate <5 (the median number overlaps of size n = 5 words) as:\nmedian(0.5, 0.75, 0.25, 1) = 0.625\nFigure 5 shows the <n scores of Figure 4 for 1 ≤ n ≤ 9. From this figure, we can see LDA topic instability since any report of the contents of a topic that uses more than three words per topic would be unreliable.\nFigure 5: <n scores of Figure 4 for 1 ≤ n ≤ 9\nFor the following analysis, we distinguish between the Raw score and the Delta score:\n• The two Raw scores are the <n median similarity scores seen before and after tuning LDA; • The Delta score is the difference between the two Raw scores.\nThe pseudocode for these calculations is shown in Algorithm 1 with the default set of parameters. In the following description, superscript numbers denote lines in the pseudocode. The data ordering is shuffled every time LDA is run5. Data is in the form of term frequency scores of each word per document. Shuffling is done in order to induce maximum variance among the ordering of data with different runs of LDA. Topics6 are a list of list which contains topics from all the different runs. A stability score is evaluated on every 10 runs (Fixed), and this process is continued 10 (Fixed) times. At the end, the median score is selected as the untuned raw score (<n ) 3−11. Hence, the runtimes comes from 10∗10 evaluations of untuned experiment."
    }, {
      "heading" : "C. Tuning Topic Modeling with LDADE",
      "text" : "LDADE is a combination of topic modeling (with LDA) and an optimizer (differential evolution, or DE) that adjusts the parameters of LDA in order to optimize (i.e. maximize) similarity scores.\nWe choose to use DE after a literature search on searchbased SE methods. The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46];\nAlgorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score\n1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.add(lda(k, α, β )) 7: end for 8: Score.add(Overlap(Topics, n, l[0])) 9: end for\n10: Raw Score← median(Score) 11: return Raw Score 12: end function\nparticle swarm optimization [51]; numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods [27], [70]. Of these, we use DE for two reasons. Firstly, it has proved useful in prior SE tuning studies [67]. Secondly, our reading of the current literature is that there are many advocates for differential evolution.\nLDADE adjusts the parameters of Table IV. Most of these parameters were explained above. As to the rest:\n• VEM is the deterministic variational EM method that computes α, β via expectation maximization [43]. • Gibbs sampling [22], [67] is a Markov Chain Monte Carlo algorithm, which is an approximate stochastic process for computing and updating α, β. Topic modeling researchers in SE have argued that Gibbs leads to stabler models [29], [30] (a claim which we test, below).\nWe manually adjust these inference techniques according to different implementations across different platforms.\nAlgorithm 2 shows LDADE’s version of DE. DE evolves a NewGeneration of candidates from a current Population. Each candidate solution in the Population is a pair of (Tunings, Scores). Tunings are selected from Table IV and Scores come similarly from Algorithm 13−11. Note that there is not any outer loop3: in Algorithm 2, LDA is only run as 1 rig11&12. Here, the runtimes comes from iter ∗ np ∗ 10 evaluations of tuned experiment.\nThe main loop of DE9 runs over the Population, replacing old items with new Candidates (if new candidate is better). DE generates new Candidates via extrapolating23 between current solutions in the frontier. Three solutions a, b, c are selected at random. For each tuning parameter i, at some probability cr, we replace the old tuning xi with yi. For booleans, we use yi = xi (see line 31). For numerics, yi = ai+ f × (bi− ci) where f\nis a parameter controlling crossover. The trim function33 limits the new value to the legal range min..max of that parameter.\nAlgorithm 2 Pseudocode for DE with a constant number of iterations Input: np= 10, f= 0.7, cr= 0.3, iter= 3, Goal ∈ Finding maximum score Output: Raw Score, final generation\n1: function DE(np, f, cr, iter,Goal) 2: Cur Gen← ∅ 3: Population←InitializePopulation(np) 4: for i = 0 to np− 1 do 5: Cur Gen.add(Population[i],ldascore(Population[i],n,Data) 6: end for 7: for i = 0 to iter do 8: NewGeneration← ∅ 9: for j = 0 to np− 1 do\n10: Si ←Extrapolate(Population[j],Population,cr,f,np) 11: if ldascore(Si) ≥ Cur Gen[j][1] then 12: NewGeneration.add(Si,ldascore(Si,n, Data)) 13: else 14: NewGeneration.add(Cur Gen[j]) 15: end if 16: Cur Gen← NewGeneration 17: end for 18: end for 19: Raw Score← GetBestSolution(Cur Gen) 20: final generation← Cur Gen 21: return Raw Score, final generation 22: end function 23: function EXTRAPOLATE(old, pop, cr, f, np) 24: a, b, c← threeOthers(pop, old) 25: newf ← ∅ 26: for i = 0 to np− 1 do 27: if cr ≤ random() then 28: newf .add(old[i]) 29: else 30: if typeof(old[i])== bool then then 31: newf .add(not old[i]) 32: else 33: newf .add(trim(i,(a[i]+f∗(b[i] − c[i])))) 34: end if 35: end if 36: end for 37: return newf 38: end function\nThe loop invariant of DE is that, after the zero-th iteration7, the Population contains examples that are better than at least one other candidate. As the looping progresses, the Population is full of increasingly more valuable solutions which, in turn, also improves the candidates, which are Extrapolated from the Population. Hence, Vesterstrom and Thomsen [65] found DE to be competitive with particle swarm optimization and other GAs.\nNote that DEs have been applied before for parameter tuning (e.g. see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability."
    }, {
      "heading" : "IV. EXPERIMENTATION",
      "text" : "In this section, any result from the smaller data sets (Pits and Citemap) come from Python implementation based on Scikit-Learn running on a single 4-core machine. Also, any results from the larger data (Stackoverflow) comes from a Scala implementation based on Mllib running on a 45 node Spark system (8 cores per node).\nNote that, for the most part, we defer the Stackoverflow results to the §V Threats to Validity where we will show that\n(a) topic modeling instability exists across multiple platforms and implementation languages; (b) tuning can improve stability for different platforms and implementations for LDA."
    }, {
      "heading" : "A. RQ1: Are the default settings of LDA correct?",
      "text" : "This first research question checks the core premise of this article– that changes in the order of training data dramatically effects the topics learned via LDA. Note that if this is not true, then there would be no value-added to this paper.\nFigure 6 plots n vs <n for untuned LDA. Note that the stability collapses the most after n = 5 words. This means that any report of LDA topics that uses more than five words per topic will be changed, just by changing the order of the inputs. This is a significant result since the standard advice in the LDA papers [34], [52] is to report the top 10 words per topic. As shown in Figure 7a, it would be rare that any such 10 word topic would be found across multiple runs.\nResult 1 Using the default settings of LDA for SE data can lead to systematic errors due to topic modeling instability.\nFigure 6: Before tuning: uses LDA’s default parameters"
    }, {
      "heading" : "B. RQ2: Does tuning improve the stability scores of LDA?",
      "text" : "Figure 7a and Figure 7b shows the stability improvement generated by tuning. Tuning never has any negative effect (reduces stability) and often has a large positive effect– particular after 5 terms overlap. The largest improvement we was in PitsD dataset which for up to 8 terms overlap was 100% (i.e. was always found in all runs). Overall, after reporting topics of up to 7 words, in the majority case (66%), those topics can be found in models generated using different input orderings. Accordingly, our answer to RQ2 is:\nResult 2 Based on Figure 7, we strongly recommend tuning for future LDA studies.\nFigure 7a: After tuning: uses parameters learned by DE. Figure 7b: Delta = After - Before.\nFigure 7: RQ1, RQ2 stability results over ten repeated runs. In these figures, larger numbers are better.\nFigure 8: Datasets vs Parameter (k) variation\nFigure 9 Datasets vs Parameter (α) variation\nFigure 10 Datasets vs Parameter (β) variation"
    }, {
      "heading" : "C. RQ3: Do different data sets need different configurations to make LDA stable?",
      "text" : "Figures 8, 9, and 10 show the results of tuning. On display in each set of vertical bars are the median values generated across 10 tunings. Also, shown are the inter-quartile range (IQR) of those tunings (the IQR is the 75th-25th percentile values and is a non-parametric measure of variation around the median value). Note that in Figure 8, IQR=0 for PitsB dataset where tuning always converged on the same final value.\nThese figures show how tuning selects the different ranges of parameters. Some of the above numbers are far from the standard values; e.g. Garousi et al. [17] recommend using k = 67 topics yet in our data sets, best results were seen using k ≤ 24. Clearly:\nResult 3 Do not reuse tunings suggested by other researchers from other data sets. Instead, always retune for all new data."
    }, {
      "heading" : "D. RQ4: Is tuning easy?",
      "text" : "The DE literature recommends using a population size np that is ten times larger than the number of parameters being optimized [57]. For example, when tuning k, α, β, the DE literature is recommending np = 30. Figure 11 explores np = 30 vs the np = 10 we use in Algorithm 2 (as well as some other variants of DE’s F and CR parameters). That figure\nshows results jsut for Citemap and, for space reasons, results relating to other data sets are shown at https://goo.gl/HQNASF. After reviewing the results from all the datasets, we can say that there isn’t much of an improvement by using different F, CR, and Population size. So our all other experiments used F = 0.7, CR = 0.3 and np = 10. Also:\nResult 4 Finding stable parameters for topic models is easier than standard optimization tasks."
    }, {
      "heading" : "E. RQ5: What is the runtime cost of tuning?",
      "text" : "Search-based SE methods can be very slow. Harman et al. once needed 15 years of CPU time to find and verify the tunings required for software clone detectors [66]. Sayyad et al. routinely used 106 evaluations (or more) of their models in order to extract products from highly constrained product lines [56]. Hence, before recommending any search-based method, it is wise to consider the runtime cost of that recommendation.\nTo understand our timing results, recall that untuned, tuned LDA use Algorithm 1, Algorithm 2 respectively. Based on the psuedocode shown above, our pre-experimental expectation is that tuning will be three times slower than not tuning.\nWhile this is definitely more than not using LDA, this may not be an arduous increase given modern cloud computing environments."
    }, {
      "heading" : "F. RQ6: Should data miners be used “off-the-shelf” with their default tunings?",
      "text" : "Figure 7 shows that there is much benefit in tuning. Figures 8, 9, and 10 show that the range of “best” tunings is very dataset-specific. Hence, for a new dataset, the off-the-shelf tunings may often fall far from the useful range. Figures 12 and 13 show that tuning is definitely slower than otherwise, but the overall cost is not prohibitive. Hence:\nResult 6 If the goal is to let humans browse the learned topics, then we cannot recommend using “off-the-shelf” LDA.\nNote that, as said above, this conclusion holds for unsupervised clustering applications. For other kinds of supervised classification applications, off-the-shelf LDA may suffice."
    }, {
      "heading" : "V. THREATS TO VALIDITY",
      "text" : "The usual software analytics threats to validity hold for this paper:\n• Our conclusions assume that the goal of topic modeling is to produce stable topics that humans will browse and reflect on. As mentioned in the introduction, there are class of papers that do not do that. Rather, they use LDA as part of some internal process where the topics are intermediaries used to generate some other goal [8]. As we said at the start of this paper, our results do not effect that kind of paper. • The conclusions of this paper are based on a finite number of data sets and it is possible that other data might invalidate our conclusions. As with all analytics papers, any researcher can do is to make their conclusions and materials public, then encourage other researchers to repeat/ refute/ improve their conclusions. For example, the footnotes of this paper contain all the urls where other researchers can download our materials and explore the conclusions of this paper.\nAs to more specific threats to validity, one issue might be that our conclusions on “LDA” are really quirks of a specific implementation. To check this, it is insightful to compare our results with:\n• The Pits and Citemap results, executed in Scikit-Learn and Python running on a desktop machine. • The StackOverflow data set executed in Scala using Mllib running on a Spark cluster.\nAnother useful comparison is to change the internal of the LDA:\n• Sometimes use VEM sampling; • Sometimes use Gibbs sampling;\nThat said, there are some deltas between VEM and Gibbs where it seems tuning is more important for VEM than Gibbs (evidence: the improvements seen after tuning are largest for the VEM results of Figure 14 and at https://goo.gl/faYAcg).\nAnother threat to validity of this work is that it is a quirk of the control parameters used within our DE optimizer. We have some evidence that this is not the case. Figure 11 and https://goo.gl/HQNASF explored a range of DE tunings and found little difference across that range. Also, Table V explores another choice within DE – how many evaluations to execute before terminating DEs. All the results in this paper use an evaluation budget of 30 evaluations. Table V compares results across different numbers of evaluations. While clearly, the\nmore evaluations the better, there is little improvement after the 30 evaluations used in this paper."
    }, {
      "heading" : "VI. CONCLUSION AND FUTURE WORK",
      "text" : "Based on the above, we offer a specific and general conclusion. Most specifically, we recommend\n• Any study that shows the topics learned from LDA, then uses that display to make a particular conclusion, needs to first tune LDA. We say this since the topics learned from untuned LDA are unstable, i.e. different input orderings will lead to different conclusions. However, after tuning, stability can be greatly increased. • Unlike the advise of Stacy et al. [34], LDA topics should not be reported as the top ten words. Due to order effects, such a report can be highly incorrect. Our results show that up to eight words can be reliably reported, but only after tuning for stability using tools like LDADE. • Do not download someone else’s pre-tuned LDA since, as shown here, the best LDA tunings vary from data set to data set.\nOur experience is that this recommendation is not an arduous demand since tuning adds less than a factor of ten to the total run times of an LDA study.\nMore generally, we comment that the field of software analytics needs to make far more use of search-based software engineering in order to tune their learners. In other work, we have shown that tuning significantly helps defect prediction [14] (a result that has also been reported by other researchers [52]). In this work, we have shown that tuning significantly helps topic modeling by mitigating a systematic error in LDA (order effects that lead to unstable topics). The implications of this study for other software analytics tasks is now an open and pressing issue. In how many domains can search-based SE dramatically improve software analytics?"
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "The work is partially funded by NSF awards #1506586."
    } ],
    "references" : [ {
      "title" : "Mining search topics from a code search engine usage log",
      "author" : [ "S.K. Bajracharya", "C.V. Lopes" ],
      "venue" : "In MSR,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "What are developers talking about? an analysis of topics and trends in stack overflow",
      "author" : [ "A. Barua", "S.W. Thomas", "A.E. Hassan" ],
      "venue" : "Empirical Software Engineering,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "MOSS” multiobjective scatter search applied to nonlinear multiple criteria optimization",
      "author" : [ "R.P. Beausoleil" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "J. Bergstra", "Y. Bengio" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Nltk: the natural language toolkit",
      "author" : [ "S. Bird" ],
      "venue" : "In Proceedings of the COLING/ACL on Interactive presentation sessions,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "In the Journal of machine Learning research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Topic-based software defect explanation",
      "author" : [ "T.-H. Chen", "W. Shang", "M. Nagappan", "A.E. Hassan", "S.W. Thomas" ],
      "venue" : "Journal of Systems and Software,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Explaining software defects using topic models",
      "author" : [ "T.-H. Chen", "S.W. Thomas", "M. Nagappan", "A.E. Hassan" ],
      "venue" : "In Mining Software Repositories (MSR),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Tuning pid controller with multiobjective differential evolution",
      "author" : [ "I. Chiha", "J. Ghabi", "N. Liouane" ],
      "venue" : "In Communications Control and Signal Processing (ISCCSP),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Predicting defect-prone software modules using support vector machines",
      "author" : [ "K.O. Elish", "M.O. Elish" ],
      "venue" : "Journal of Systems and Software,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Converging on the optimal attainment of requirements",
      "author" : [ "M.S. Feather", "T. Menzies" ],
      "venue" : "In Requirements Engineering,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "The Text Mining Handbook",
      "author" : [ "J R.-S. Feldman" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Tuning for software analytics: Is it really necessary",
      "author" : [ "W. Fu", "T. Menzies", "X. Shen" ],
      "venue" : "Information and Software Technology,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Automated classification of software change messages by semi-supervised latent dirichlet allocation",
      "author" : [ "Y. Fu", "M. Yan", "X. Zhang", "L. Xu", "D. Yang", "J.D. Kymer" ],
      "venue" : "Information and Software Technology,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Analysis of user comments: an approach for software requirements evolution",
      "author" : [ "L.V. Galvis Carreño", "K. Winbladh" ],
      "venue" : "In Proceedings of the 2013 International Conference on Software Engineering,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Citations, research topics and active countries in software engineering: A bibliometrics study",
      "author" : [ "V. Garousi", "M.V. Mäntylä" ],
      "venue" : "Computer Science Review,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Models of incremental concept formation",
      "author" : [ "J.H. Gennari", "P. Langley", "D. Fisher" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1989
    }, {
      "title" : "The general employee scheduling problem. an integration of ms and ai",
      "author" : [ "F. Glover", "C. McMillan" ],
      "venue" : "Computers & operations research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1986
    }, {
      "title" : "On the complexity of the satisfiability problem",
      "author" : [ "A.T. Goldberg" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1979
    }, {
      "title" : "Using heuristics to estimate an appropriate number of latent topics in source code analysis",
      "author" : [ "S. Grant", "J.R. Cordy", "D.B. Skillicorn" ],
      "venue" : "Science of Computer Programming,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T.L. Griffiths", "M. Steyvers" ],
      "venue" : "Proceedings of the National academy of Sciences,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2004
    }, {
      "title" : "How do users like this feature? a fine grained sentiment analysis of app reviews",
      "author" : [ "E. Guzman", "W. Maalej" ],
      "venue" : "In Requirements Engineering Conference (RE),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Relating requirements to implementation via topic analysis: Do topics extracted from requirements make sense to managers and developers",
      "author" : [ "A. Hindle", "C. Bird", "T. Zimmermann", "N. Nagappan" ],
      "venue" : "In Software Maintenance (ICSM),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Automated topic naming to support cross-project analysis of software maintenance activities",
      "author" : [ "A. Hindle", "N.A. Ernst", "M.W. Godfrey", "J. Mylopoulos" ],
      "venue" : "In Proceedings of the 8th Working Conference on Mining Software Repositories,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Latent dirichlet allocation: stability and applications to studies of user-generated content",
      "author" : [ "S. Koltcov", "O. Koltsova", "S. Nikolenko" ],
      "venue" : "In Proceedings of the 2014 ACM conference on Web science,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Gale: Geometric active learning for search-based software engineering",
      "author" : [ "J. Krall", "T. Menzies", "M. Davies" ],
      "venue" : "Software Engineering, IEEE Transactions on,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality",
      "author" : [ "J.H. Lau", "D. Newman", "T. Baldwin" ],
      "venue" : "In EACL,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "Topic modeling of nasa space system problem reports: research in practice",
      "author" : [ "L. Layman", "A.P. Nikora", "J. Meek", "T. Menzies" ],
      "venue" : "In Proceedings of the 13th International Workshop on Mining Software Repositories,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Predicting effectiveness of irbased bug localization techniques",
      "author" : [ "T.-D.B. Le", "F. Thung", "D. Lo" ],
      "venue" : "In 2014 IEEE 25th International Symposium on Software Reliability Engineering,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "An exploratory analysis of mobile development issues using stack overflow",
      "author" : [ "M. Linares-Vásquez", "B. Dit", "D. Poshyvanyk" ],
      "venue" : "In Proceedings of the 10th Working Conference on Mining Software Repositories,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "Improving trace accuracy through data-driven configuration and composition of tracing features",
      "author" : [ "S. Lohar", "S. Amornborvornwong", "A. Zisman", "J. Cleland-Huang" ],
      "venue" : "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2013
    }, {
      "title" : "Bug localization using latent dirichlet allocation",
      "author" : [ "S.K. Lukins", "N.A. Kraft", "L.H. Etzkorn" ],
      "venue" : "Information and Software Technology,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2010
    }, {
      "title" : "The app sampling problem for app store mining",
      "author" : [ "W. Martin", "M. Harman", "Y. Jia", "F. Sarro", "Y. Zhang" ],
      "venue" : "In Mining Software Repositories (MSR),",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2015
    }, {
      "title" : "Mining business topics in source code using latent dirichlet allocation",
      "author" : [ "G. Maskeri", "S. Sarkar", "K. Heafield" ],
      "venue" : "In Proceedings of the 1st India software engineering conference,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2008
    }, {
      "title" : "Trends in topics at SE conferences (1993-2013)",
      "author" : [ "G. Mathew", "A. Agrawal", "T. Menzies" ],
      "venue" : "Submitted to ICSE’17. Available from tiny.cc/trendsSE,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2017
    }, {
      "title" : "Improving iv&v techniques through the analysis of project anomalies: Text mining pits issue reports-final report",
      "author" : [ "T. Menzies" ],
      "venue" : null,
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2008
    }, {
      "title" : "The business case for automated software engineering",
      "author" : [ "T. Menzies", "O. Elrawas", "J. Hihn", "M. Feather", "R. Madachy", "B. Boehm" ],
      "venue" : "In Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2007
    }, {
      "title" : "The Promise Repository of Empirical Software Engineering Data",
      "author" : [ "T. Menzies", "R. Krishna", "D. Pryor" ],
      "venue" : "http://openscience.us/repo,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2015
    }, {
      "title" : "Automated severity assessment of software defect reports",
      "author" : [ "T. Menzies", "A. Marcus" ],
      "venue" : "In Software Maintenance,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2008
    }, {
      "title" : "The magical number seven, plus or minus two: Some limits on our capacity for processing information",
      "author" : [ "G.A. Miller" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1956
    }, {
      "title" : "Expectation-propagation for the generative aspect model",
      "author" : [ "T. Minka", "J. Lafferty" ],
      "venue" : "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2002
    }, {
      "title" : "Sspmo: A scatter tabu search procedure for non-linear multiobjective optimization",
      "author" : [ "J. Molina", "M. Laguna", "R. Martı", "R. Caballero" ],
      "venue" : "INFORMS Journal on Computing,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2007
    }, {
      "title" : "Structured versus unstructured data: The balance of power continues to shift. IDC (Industry Development and Models) Mar",
      "author" : [ "A. Nadkarni", "N. Yezhkova" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2014
    }, {
      "title" : "Abyss: Adapting scatter search to multiobjective optimization",
      "author" : [ "A.J. Nebro", "F. Luna", "E. Alba", "B. Dorronsoro", "J.J. Durillo", "A. Beham" ],
      "venue" : "Evolutionary Computation, IEEE Transactions on,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2008
    }, {
      "title" : "Topic modelling for qualitative studies",
      "author" : [ "S.I. Nikolenko", "S. Koltcov", "O. Koltsova" ],
      "venue" : "Journal of Information Science, page 0165551515617393,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2015
    }, {
      "title" : "On the equivalence of information retrieval methods for automated traceability link recovery",
      "author" : [ "R. Oliveto", "M. Gethers", "D. Poshyvanyk", "A. De Lucia" ],
      "venue" : "In Program Comprehension (ICPC),",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2010
    }, {
      "title" : "Differential evolution methods for unsupervised image classification",
      "author" : [ "M.G. Omran", "A.P. Engelbrecht", "A. Salman" ],
      "venue" : "In Evolutionary Computation,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2005
    }, {
      "title" : "An analysis of the coherence of descriptors in topic modeling",
      "author" : [ "D. O’Callaghan", "D. Greene", "J. Carthy", "P. Cunningham" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2015
    }, {
      "title" : "Particle swarm-simulated annealing fusion algorithm and its application in function optimization",
      "author" : [ "H. Pan", "M. Zheng", "X. Han" ],
      "venue" : "In Computer Science and Software Engineering,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2008
    }, {
      "title" : "How to effectively use topic models for software engineering tasks? an approach based on genetic algorithms",
      "author" : [ "A. Panichella", "B. Dit", "R. Oliveto", "M. Di Penta", "D. Poshyvanyk", "A. De Lucia" ],
      "venue" : "In Proceedings of the 2013 International Conference on Software Engineering,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2013
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2011
    }, {
      "title" : "Retrieval from software libraries for bug localization: a comparative study of generic and composite text models",
      "author" : [ "S. Rao", "A. Kak" ],
      "venue" : "In Proceedings of the 8th Working Conference on Mining Software Repositories,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2011
    }, {
      "title" : "Scalable product line configuration: A straw to break the camel’s back",
      "author" : [ "A.S. Sayyad", "J. Ingram", "T. Menzies", "H. Ammar" ],
      "venue" : "In Automated Software Engineering (ASE),",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2013
    }, {
      "title" : "Differential evolution–a simple and efficient heuristic for global optimization over continuous spaces",
      "author" : [ "R. Storn", "K. Price" ],
      "venue" : "Journal of global optimization,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 1997
    }, {
      "title" : "Msr4sm: Using topic models to effectively mining software repositories for software maintenance tasks",
      "author" : [ "X. Sun", "B. Li", "H. Leung", "Y. Li" ],
      "venue" : "Information and Software Technology,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2015
    }, {
      "title" : "Automated Parameter Optimization of Classification techniques for Defect Prediction Models",
      "author" : [ "C. Tantithamthavorn", "S. McIntosh", "A.E. Hassan", "K. Matsumoto" ],
      "venue" : "In Proc. of the International Conference on Software Engineering (ICSE),",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2016
    }, {
      "title" : "Mining software repositories using topic models",
      "author" : [ "S.W. Thomas" ],
      "venue" : "In Proceedings of the 33rd International Conference on Software Engineering,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2011
    }, {
      "title" : "Studying software evolution using topic models",
      "author" : [ "S.W. Thomas", "B. Adams", "A.E. Hassan", "D. Blostein" ],
      "venue" : "Science of Computer Programming,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2014
    }, {
      "title" : "Static test case prioritization using topic models",
      "author" : [ "S.W. Thomas", "H. Hemmati", "A.E. Hassan", "D. Blostein" ],
      "venue" : "Empirical Software Engineering,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2014
    }, {
      "title" : "Using latent dirichlet allocation for automatic categorization of software",
      "author" : [ "K. Tian", "M. Revelle", "D. Poshyvanyk" ],
      "venue" : "In Mining Software Repositories,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2009
    }, {
      "title" : "A historical dataset of software engineering conferences",
      "author" : [ "B. Vasilescu", "A. Serebrenik", "T. Mens" ],
      "venue" : "In Proceedings of the 10th Working Conference on Mining Software Repositories,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2013
    }, {
      "title" : "A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems",
      "author" : [ "J. Vesterstrøm", "R. Thomsen" ],
      "venue" : "In Evolutionary Computation,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2004
    }, {
      "title" : "Searching for better configurations: a rigorous approach to clone evaluation",
      "author" : [ "T. Wang", "M. Harman", "Y. Jia", "J. Krinke" ],
      "venue" : "In Proceedings  of the 2013 9th Joint Meeting on Foundations of Software Engineering,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2013
    }, {
      "title" : "Lda-based document models for ad-hoc retrieval",
      "author" : [ "X. Wei", "W.B. Croft" ],
      "venue" : "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2006
    }, {
      "title" : "Improving the Usability of Topic Models",
      "author" : [ "Y. Yang" ],
      "venue" : "PhD thesis, NORTHWESTERN UNIVERSITY,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 2015
    }, {
      "title" : "A heuristic approach to determine an appropriate number of topics in topic modeling",
      "author" : [ "W. Zhao", "J.J. Chen", "R. Perkins", "Z. Liu", "W. Ge", "Y. Ding", "W. Zou" ],
      "venue" : "BMC bioinformatics,",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2015
    }, {
      "title" : "Active learning for multi-objective optimization",
      "author" : [ "M. Zuluaga", "G. Sergent", "A. Krause", "M. Püschel" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 42,
      "context" : "As shown in Figure 1, most of the planet’s 1600 Exabytes of data does not appear in structured sources (databases, etc) [45].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "One of the common techniques for finding related topics within unstructured text (an area called topic modeling) is Latent Dirichlet Allocation (LDA) [6].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "buggy or not [8]).",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 42,
      "context" : "From [45].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 53,
      "context" : "To fix this problem, we proposes LDADE: a combination of LDA and a search-based optimizer (differential evolution, or DE) [57]) that automatically tunes LDA’s < k, α, β > parameters.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 51,
      "context" : "[55] 2011 112 WCRE Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "[48] 2010 108 MSR Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[2] 2014 96 ESE Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 49,
      "context" : "[52] 2013 75 ICSE Y Y Y Uses GA to tune parameters.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] 2013 61 ICSE Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] 2011 45 MSR Y Y N They validated the topic labelling techniques using multiple experiments.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] 2014 44 RE Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 56,
      "context" : "[60] 2011 44 ICSE Y Y N Open issue to choose optimal parameters.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[9] 2012 35 MSR Y Y N Choosing the optimal number of topics is difficult.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 57,
      "context" : "[61] 2014 35 SCP Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 58,
      "context" : "[62] 2014 31 ESE Y Y N Choosing right set of parameters is a difficult task.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[1] 2009 29 MSR Y Y N Explored Configurations without any explanation and accepted to the fact their results were better because of the corpus they used.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 30,
      "context" : "[33] 2013 27 ESEC/FSE Y Y Y Explored Configurations using LDA-GA.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[32] 2013 20 MSR Y Y N In Future, they planned to use LDA-GA.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] 2014 15 WebSci Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] 2012 13 ICSM Y Y N Explored Configurations without any explanation (Just with no.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] 2013 13 SCP Y Y N Their work focused on optimizing LDA’s topic count parameter.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] 2015 6 IST Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] 2016 5 CS Review Y Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[31] 2014 5 ISSRE N Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : "[47] 2015 3 JIS Y Y N They improvised LDA into ISLDA which gave stability across different runs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 54,
      "context" : "[58] 2015 2 IST Y Y Y Explored Configurations using LDA-GA.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[8] 2016 0 JSS N Y N Explored Configurations without any explanation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 34,
      "context" : "In a companion paper submitted to ICSE’17 [37] we apply LDADE to 9291 SE papers to find trends in topics between 1993 to 2013.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "Previously [14], we showed that (a) tuning software defect predictors can lead to large improvements of the performance of those predictors; and that (b) those tunings are different in different data sets; so (c) it is important to apply automatic tuning as part of the analysis of any new data set.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "The impact of tuning are well understood in the theoretical machine learning literature [4].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "[14] surveyed hundreds of recent SE papers in the area of software defect prediction from static code attributes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 55,
      "context" : "They found that most SE authors do not take steps to explore tunings (rare exception: [59]).",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "[11] compared support vector machines to other data miners for the purposes of defect prediction.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "Figure 3: Example (from [48]) of generating topics from Stackoverflow.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "Langley [18] defines such effects as follows:",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 16,
      "context" : "certain incremental clustering algorithms generate different clusters, depending on the order with which they explore the data [18].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "In the literature, researchers [23], [35], [36] manually accessed the topics and then used for further experiments.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 32,
      "context" : "In the literature, researchers [23], [35], [36] manually accessed the topics and then used for further experiments.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 33,
      "context" : "In the literature, researchers [23], [35], [36] manually accessed the topics and then used for further experiments.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "Some made use of Amazon Mechanical Turk to create goldstandard coherence judgements [28].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 64,
      "context" : "For example, SCLDA [68] can handle different kinds of knowledge such as word correlation, document correlation, document label and so on.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 30,
      "context" : "Some researchers [33], [52], [58] used Genetic Algorithm (GA) to tune the parameters– but for different purposes than LDADE.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 49,
      "context" : "Some researchers [33], [52], [58] used Genetic Algorithm (GA) to tune the parameters– but for different purposes than LDADE.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 54,
      "context" : "Some researchers [33], [52], [58] used Genetic Algorithm (GA) to tune the parameters– but for different purposes than LDADE.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 49,
      "context" : "[52] used GAs to increase the precision of predictions made by a classifier; i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "experiment with one parameter: cluster size) [16], [63] achieved higher stability by just increasing the number of cluster size.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 59,
      "context" : "experiment with one parameter: cluster size) [16], [63] achieved higher stability by just increasing the number of cluster size.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 35,
      "context" : "PITS is a text mining data set generated from NASA software project and issue tracking system (PITS) reports [38], [41].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 38,
      "context" : "PITS is a text mining data set generated from NASA software project and issue tracking system (PITS) reports [38], [41].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 37,
      "context" : "The dataset can be downloaded from the PROMISE repository [40].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 60,
      "context" : "[64].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "For this study, all datasets were preprocessed using the usual text mining filters [13]:",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "• Stop words removal using NLTK toolkit3 [5] : ignore very common short words such as “and” or “the”.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 50,
      "context" : "• PITS and Citemap is small enough to process on a single (four core) desktop machine using Scikit-Learn [53] and Python.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 26,
      "context" : "To evaluate topics coherence in LDA, there is a direct approach, by asking people about topics, and an indirect approach by evaluating pointwise mutual information (PMI) [28], [50] between the topic words.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 47,
      "context" : "To evaluate topics coherence in LDA, there is a direct approach, by asking people about topics, and an indirect approach by evaluating pointwise mutual information (PMI) [28], [50] between the topic words.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "The usual trend is that as the value of perplexity drops, the number of topics should grow [26].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 24,
      "context" : "Researchers caution that the value of perplexity doesn’t remain constant with different topic size and with dictionary sizes [26], [69].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 65,
      "context" : "Researchers caution that the value of perplexity doesn’t remain constant with different topic size and with dictionary sizes [26], [69].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "Another approach researchers have used is Jaccard Similarity [16], [50].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 47,
      "context" : "Another approach researchers have used is Jaccard Similarity [16], [50].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 39,
      "context" : "For that purpose, we will study the case of n ≤ 9 (we use 9 as our maximum size since the cognitive science literature tells us that 7±2 is a useful upper size for artifacts to be browsed by humans [42]).",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 36,
      "context" : "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 53,
      "context" : "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 41,
      "context" : "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 43,
      "context" : "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, α, β (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score← ∅ 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data← shuffle(Data) 6: Topics.",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 48,
      "context" : "particle swarm optimization [51]; numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods [27], [70].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "particle swarm optimization [51]; numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods [27], [70].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 66,
      "context" : "particle swarm optimization [51]; numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods [27], [70].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 63,
      "context" : "Firstly, it has proved useful in prior SE tuning studies [67].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 40,
      "context" : "• VEM is the deterministic variational EM method that computes α, β via expectation maximization [43].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "• Gibbs sampling [22], [67] is a Markov Chain Monte Carlo algorithm, which is an approximate stochastic process for computing and updating α, β.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 63,
      "context" : "• Gibbs sampling [22], [67] is a Markov Chain Monte Carlo algorithm, which is an approximate stochastic process for computing and updating α, β.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "Topic modeling researchers in SE have argued that Gibbs leads to stabler models [29], [30] (a claim which we test, below).",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "k 10 [10,100] Number of topics or cluster size",
      "startOffset" : 5,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "α None [0,1] Prior of document topic distribution.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "β None [0,1] Prior of topic word distribution.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "add(Population[i],ldascore(Population[i],n,Data) 6: end for 7: for i = 0 to iter do 8: NewGeneration← ∅ 9: for j = 0 to np− 1 do 10: Si ←Extrapolate(Population[j],Population,cr,f,np) 11: if ldascore(Si) ≥ Cur Gen[j][1] then 12: NewGeneration.",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 61,
      "context" : "Hence, Vesterstrom and Thomsen [65] found DE to be competitive with particle swarm optimization and other GAs.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 30,
      "context" : "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 46,
      "context" : "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 49,
      "context" : "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 54,
      "context" : "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 31,
      "context" : "This is a significant result since the standard advice in the LDA papers [34], [52] is to report the top 10 words per topic.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 49,
      "context" : "This is a significant result since the standard advice in the LDA papers [34], [52] is to report the top 10 words per topic.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "[17] recommend using k = 67 topics yet in our data sets, best results were seen using k ≤ 24.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 53,
      "context" : "The DE literature recommends using a population size np that is ten times larger than the number of parameters being optimized [57].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 62,
      "context" : "once needed 15 years of CPU time to find and verify the tunings required for software clone detectors [66].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 52,
      "context" : "routinely used 10 evaluations (or more) of their models in order to extract products from highly constrained product lines [56].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "Rather, they use LDA as part of some internal process where the topics are intermediaries used to generate some other goal [8].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 31,
      "context" : "[34], LDA topics should not be reported as the top ten words.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "In other work, we have shown that tuning significantly helps defect prediction [14] (a result that has also been reported by other researchers [52]).",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 49,
      "context" : "In other work, we have shown that tuning significantly helps defect prediction [14] (a result that has also been reported by other researchers [52]).",
      "startOffset" : 143,
      "endOffset" : 147
    } ],
    "year" : 2016,
    "abstractText" : "Topic Modeling finds human-readable structures in large sets of unstructured SE data. A widely used topic modeler is Latent Dirichlet Allocation. When run on SE data, LDA suffers from “order effects” i.e. different topics be generated if the training data was shuffled into a different order. Such order effects introduce a systematic error for any study that uses topics to make conclusions. This paper introduces LDADE, a Search-Based SE tool that tunes LDA’s parameters using DE (Differential Evolution). LDADE has been tested on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of SE papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (the traditional VEM method, or using Gibbs sampling). In all tests, the pattern was the same: LDADE’s tunings dramatically reduces topic instability. The implications of this study for other software analytics tasks is now an open and pressing issue. In how many domains can search-based SE dramatically improve software analytics? Keywords—Topic modeling, Stability, LDA, tuning, differential evolution.",
    "creator" : "LaTeX with hyperref package"
  }
}