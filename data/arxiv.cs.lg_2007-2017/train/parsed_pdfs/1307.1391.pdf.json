{
  "name" : "1307.1391.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Quiet in Class : Classification, Noise and the Dendritic Cell Algorithm",
    "authors" : [ "Feng Gu", "Jan Feyereisl", "Robert Oates", "Jenna Reps", "Julie Greensmith", "Uwe Aickelin" ],
    "emails" : [ "fxg@cs.nott.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The Dendritic Cell Algorithm (DCA) is an immune-inspired algorithm developed as part of the Danger Project [1]. Despite being applied to a number of applications, it was originally designed and used as an anomaly detection and attribution algorithm [9]. For the duration of this work, the anomaly detection problem is defined as a binary classification problem, performed on (potentially noisy) discrete time series data. The authors make no assumptions about the relative persistence of anomalous states and normal states, though the persistence of both states is assumed to be sufficiently long to differentiate them from noise. It is also assumed that examples of a system’s anomalous behaviour are available for use as training data. This is in contrast to the many alternate definitions of the anomaly detection problem, where there can be the implicit assumption that anomalies are transient or the assumption that only normal behaviour can be studied a priori, reducing the problem to a single class classification. For this investigation a separate, related problem is also defined, termed ar X\niv :1\n30 7.\n13 91\nv1 [\ncs .L\nG ]\n4 J\nul 2\n“the anomaly attribution problem”. This is the problem of attributing causal relationships between the presence of elements in the environment and the occurrence of identified anomalies.\nSince its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22]. This body of work has identified several interesting properties of the DCA. For example, it has been shown that the structure of a single dendritic cell within the DCA is similar in function to the operation of a filter with a dynamically changing transfer function [18,19]. This property could be potentially useful as it allows the algorithm to both exploit the temporal ordering of the input data and remove noisy artefacts from the environmental measurements. However, the effects of the dynamic filter within the DCA to the anomaly detection problem, beneficial or otherwise, have never been demonstrated.\nOther theoretical work identifies properties of the DCA that are clearly detrimental to its application to certain problems. One such property is that its classification stage is functionally equivalent to a statically weighted, linear classifier [22]. Such a classifier is neither able to adapt to training data nor meaningfully act on problems which are not linearly separable. Such a criticism is a severe blow to the utility of the DCA in its current form but only strikes at one aspect of a multifaceted algorithm. Within the literature, it has been suggested that replacing the classification stage of the DCA with a trainable, nonlinear, machine learning algorithm would negate much of the criticism made of the DCA while preserving its novel properties [13,20,22].\nModifying the DCA to compensate for the weaknesses identified within the literature, while retaining its original properties, is only a valid course of action if those properties are clearly beneficial. In summary, it is important to identify if the overhead of “fixing” the DCA carries sufficient benefit over creating a new technique for solving the anomaly detection problem. This work is a step towards validating the usefulness of the DCA’s novel properties by separating the algorithm into its component parts and assessing their individual contributions. The structure of the paper is as follows, Section 2 provides an outline of the related work; Section 3 gives the research aims in the form of hypotheses; Section 4 presents algorithmic details as mathematical functions; Section 5 details the experimental design; Section 6 shows the results of conducted experiments and the corresponding analysis; finally Section 7 is a discussion of the findings and highlights the future steps for this work."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 The Dendritic Cell Algorithm",
      "text" : "Several different versions of the DCA exist within the literature. The deterministic DCA (dDCA) that was developed for ease of analysis, will be the version considered in this work. The algorithmic details can be found in [10].\nThe first stage of the DCA is an anomaly detection phase, where the population’s classification decisions are monitored in order to identify anomalies within\na given dataset. The second phase attempts to correlate the antigen sampled by the cells with the occurrence of detected anomalies.\nThe DCA receives two types of input, namely signal and antigen. Signals are represented as vectors of real-valued numbers and are periodic measurements of features within the problem environment. An assumption made by the algorithm is that the presence or absence of an anomaly can be detected by observing these features. Antigen are symbols (typically represented as an enumerated type), which represent items of interest within the environment. It is assumed that some of the antigen have a causal relationship with observed anomalies.\nThe DCA is a population-based algorithm, where several heterogenous agents (cells) monitor the same inputs in parallel. Each cell stores a history of the received input signals, while maintaining a sum of their magnitudes. Upon the sum of the input signal magnitudes reaching a predefined decision threshold, the cell performs a classification based on the signal history. When the decision has been recorded, the cell is reset and instantaneously returned to the population. Each cell is assigned a different decision threshold generated from a uniform distribution, ensuring that cells observe the data over different time scales.\nIt is demonstrated in [22] that both the classification boundary and the position of the decision boundary can be expressed as hyperplanes, akin to those found in linear classifiers. This premise is used as a foundation for this investigation, so the pertinent machine learning concepts are presented in Section 2.2. As the classification performed by a cell is performed using the history of the sampled signals rather than an instantaneous sample of the environmental features, it can be shown that the DCA exhibits a filtering property which allows it to remove high frequency noise from the input signals [18]. This process relies on the underlying state of the system (normal or anomalous) being persistent for a long enough period of time to distinguish it from the noise. This filtering property is also a key premise of this work and shall be discussed in greater depth in Section 2.3."
    }, {
      "heading" : "2.2 Machine Learning Concepts",
      "text" : "In our investigation, the classification stage of the DCA is replaced by a trainable classifier, which is based on the operation of Support Vector Machines (SVM) [3]. Here we present an introduction to this algorithm and the relevant machine learning concepts. SVM models can be described using linear discriminant functions [6], quadratic optimisation [7], and kernel methods [21].\nLet (x1, y1), ..., (xn, yn) ∈ X×Y be a given training set with n data instances, where X ⊆ Rd is a d-dimensional input feature space and Y = {±1} is a set of truths or class labels. For each data instance xi ∈ X where i ∈ [1, n]∩N, a linear discriminant function is defined as f : Rd → R,\nf(xi) = 〈w,xi〉+ b (1)\nwhere 〈·〉 denotes the inner product of two vectors, w is the weight vector and b is the bias. The decision boundary of classification is given by 〈w,x〉+ b = 0,\nwhich corresponds to a (d − 1)-dimensional hyperplane within a d-dimensional feature space. A signed measure of the perpendicular distance r from the decision surface to a data point x can be calculated as,\nr = f(x)\n‖w‖ (2)\nwhere ‖ · ‖ is the norm operator of a vector. The linear discriminant functions of SVM models are based on the maximal margin classifier, defined as follows:\n〈w,xi〉+ b ≥ +1 if yi = +1 (3)\n〈w,xi〉+ b ≤ −1 if yi = −1 (4)\nData points lying on the hyperplane H1 : 〈w,x〉 + b = 1 have a perpendicular distance from the origin |1 − b|/‖w‖. Similarly, data points lying on the hyperplane H2 : 〈w,x〉+ b = −1 have a perpendicular distance from the origin | − 1 − b|/‖w‖. The margin between the two hyperplanes H1 and H2 is equal to 2/‖w‖. An optimal decision boundary, defined by 〈w,x〉+ b = 0, is found by maximising this margin. It is equidistant and parallel to H1 and H2.\nThe learning task of SVM can be defined as an optimisation problem,{ minimisew,b ‖w‖2 subject to yi(〈w,xi〉+ b)− 1 ≥ 0 ∀i (5)\nwhere the constraints are derived from combining Equation 3 and Equation 4. Such an optimisation problem becomes much easier to solve if we introduce Lagrangian multipliers. Let αi ≥ 0 be the Lagrangian multipliers, which correspond to the constraints in Equation 5. A primal Lagrangian of the above optimisation problem is defined as\nLP = 1\n2 ‖w‖2 − n∑ i=1 αi[yi(〈w,xi〉+ b)− 1] (6)\nThe primal form LP is differentiable with respect to w and b, and an equivalent dual form, known as the Wolfe dual [7], can be derived. The optimisation becomes a convex quadratic programming problem, and all data points that satisfy the constraints also form a convex set [3]. This dual form is defined as\nLD = n∑ i=1 αi − 1 2 n∑ i,j=1 αiαjyiyj〈xi,xj〉 (7)\nDuring the training phase of SVM, LD is maximised with respect to all αi. The solution of 7 contains feature vectors xi such that their corresponding αi 6= 0. These vectors are called support vectors, and they lie on either H1 or H2. For non-separable cases, additional constraints are required to allow for outliers. These constraints are ∑ yiαi = 0 and 0 ≤ αi ≤ C, where C is a parameter\nthat controls the regularisation term. In addition, 〈xi,xj〉 can be replaced by 〈Φ(xi), Φ(xj)〉 through kernel methods. A kernel function is defined as,\nk(xi,xj) = 〈Φ(xi), Φ(xj)〉 (8)\nwhere Φ is a mapping from the original input feature space X to a higher dimensional (inner product) feature space F , where nonlinearly separable problems become more separable [21].\nDepending on the applications, a number of kernel functions are available, including linear kernels, polynomial kernels, and Gaussian kernels [21]. A linear kernel only involves performing inner product operations with the input data. Therefore a linear SVM that uses such a kernel is usually simple to train and use. It is more computationally efficient than other SVM models that use more complicated kernel functions [8]. The linear SVM is chosen in this work due to its algorithmic and computational simplicity."
    }, {
      "heading" : "2.3 Signal Processing Concepts",
      "text" : "Filters can be viewed as algorithms or structures which apply a gain (ratio of output to input), to their input signal to produce a new output signal. Where filters differ from a simple amplifier, is that the gain applied is a function of the frequency of the input. The mathematical function relating gain to frequency is referred to as the “transfer function” of the filter. In the field of signal processing it is common practice to express filters by providing their transfer functions. For completeness the filters being used for this work will be given here.\nThe filter with the most analogous behaviour to the DCA is the sliding window filter [18]. A sliding window filter is so called as it can be viewed as a bounding box being translated along the input data. At each step t, the output of the sliding window filter is the average sample size contained within the window. This is expressed in Equation 9,\not = 1\nW t∑ a=(t−W ) ia (9)\nwhere ot is the output of the filter at step t, ia is the input sample at time index a and W is the width of the window in steps.\nThe transfer function of the sliding window filter is given in Equation 10 [14],\nGS(ω) = 1\nW W−1∑ g=0 e−jgω (10)\nwhere GS(ω) is the transfer function of the sliding window filter, j is the complex number constant and ω is the frequency of the input signal.\nA dendritic cell acts like a sliding window filter which only reports its output every W steps [18]. The transfer function for such a filter is given in Equation 11,\nGD(ω) = 1\nW 2 W−1∑ g=0 W−1∑ b=0 e−jb((ω+(2gπ))) (11)\nwhere GD(ω) is the transfer function of the dendritic cell. However, this transfer function assumes a constant window size W . For a dendritic cell the window size is a function of the magnitude of the input signal being filtered and the decision boundary assigned to the cell. This makes expressing a cell’s transfer function extremely difficult as the magnitude of the signal cannot be known a priori. With a given training set, a suitable value to use for the decision boundary could be found by minimising the classification error. However, it is not known if this dynamically changing window size is of any benefit to the algorithm."
    }, {
      "heading" : "3 Research Aims",
      "text" : "To justify future work on the DCA it is necessary to assess the importance of its novel features. In the literature, three novel properties of the DCA remain unvalidated: the effect of antigen; the effect of the dynamic filtering; and the effect of having a population of classifiers. Of these, it is arguable that the effect of the dynamic filtering is the most important. This is because the antigen effect is unlikely to yield positive results if the anomaly detection phase is insufficient and the classifier population is unlikely to yield positive results if the dynamic filters used by that population prove to be insufficient.\nIn order to verify the need for a filter of any kind, it is important to determine if filtering the output from a classifier improves the results of classification when using a time-ordered, noisy dataset. The following null hypothesis will be the first step in this investigation.\nH 1 Filtering the results of a linear classifier presented with time ordered, noisy input data will not result in significant difference of the classification performance.\nThis is obviously dependent on designing an appropriate filter as part of the experimental setup.\nIn order to justify the additional implementation complexity, the dynamic filters should outperform a suitably tuned static counterpart. This yields the following testable null hypothesis.\nH 2 The results from a linear classifier filtered by a dynamic moving window function will have no significant difference to the results from the same classifier using a static moving window function.\nWhile this investigation is not primarily focussed on the other novel features of the DCA it is of interest to compare the output from the original DCA to that of a filtered and an unfiltered classifier. A trained classifier may have the advantage of being able to adapt to the input data, but the DCA has the additional antigen and multiple perspectives properties, so it will be difficult to definitively identify the reasons for relative performance. However, should the DCA outperform a filtered classifier, it shows that the other properties of the DCA add some information to the decision making process. If on the other hand the DCA is\noutperformed by a filtered classifier, it would suggest that the benefits of adding a training phase, at the very least, outweigh the possible benefits of the other novel aspects of the algorithm. In either case more experiments would need to be done to assess the merits of the other algorithmic properties. The testable hypothesis from this investigation’s perspective is as follows.\nH 3 The classification performance of the DCA will not be significantly different to that of a linear classifier, filtered or otherwise on a time-ordered, noisy dataset.\nIf it is possible to reject all of these null hypotheses, then a second set of statistical tests can be performed, assessing the relative benefit of using one technique over the other for the dataset used."
    }, {
      "heading" : "4 Algorithmic Details",
      "text" : "To investigate the merits of the sliding window effect of the DCA, it is necessary to separate it from the rest of the algorithm, and use it in conjunction with a better understood classifier. For this investigation, two moving window functions are used as filters for processing the decisions made by a linear SVM. For a given training set, the linear SVM finds an optimal decision boundary and returns the signed orthogonal distance from the decision boundary to each data point, as defined in Equation 2. The moving window functions initialise either a set of window sizes or a set of decision thresholds, and label the data instances within every moving window created. An error function is used to find the optimal window size or decision threshold. The knowledge obtained through training is then applied to classify data instances within the testing set.\nFor clarity, the algorithmic combinations of a linear SVM with a static and dynamic moving window function that are used in the experiments are defined in the subsequent sections. As the dynamic moving window function cannot be easily defined in a continuous frequency domain, we define both moving window functions in a discrete time domain. For this section time is indexed by i ∈ [1, n] ∩ N i.e. the index of a data instance in the feature space."
    }, {
      "heading" : "4.1 Static Moving Window Function",
      "text" : "Let A = {αl | αl ∈ N} be a set of m initial window sizes where l ∈ [1,m]∩N, and k ∈ [1, d nαl e] ∩ N be the index of a moving window depending on αl, where d·e denotes the ceiling function. Let Sk = [1 + (k− 1)αl, kαl]∩N be a set of indexes of the data instances contained within a static moving window. This divides the entire interval [1, n] ∩ N into d nαl e partitions. The function for determining the class label of each data instance with respect to a window size αl is defined as c : Rn × N× N→ {±1},\nc(f(x), αl, i) = d nαl e∑ k=1 1Sk(i) sgn ( n∑ s=1 f(xs) ‖w‖ 1Sk(s) ) (12)\nwhere 1X(x) defines an indicator function that returns one if x ∈ X holds and zero otherwise, and sgn(·) denotes a sign function of real numbers defined as,\nsgn(x) = { +1 if x ≥ 0 −1 otherwise (13)\nwhere x ∈ R. For each window size αl, the function c firstly calculates the cumulative distance of all data points, within a generated window, with respect to the decision boundary. It then labels each data instance within such window according to the sign of the calculated cumulative distance. This process is iterative for all the windows generated with respect to a window size.\nA mean square error based function is used for evaluating the effectiveness of each window size with respect to the class label, defined as e : N→ R.\ne(αl) = 1\nn n∑ i=1 ‖c(f(x), αl, i)− yi‖2 (14)\nThe static moving window function returns an optimal window size αopt ∈ A that minimises the resulting classification error, defined as\nαopt = arg min αl∈A {e(αl)} (15)"
    }, {
      "heading" : "4.2 Dynamic Moving Window Function",
      "text" : "Let B = {βl | βl ∈ R} where l ∈ [1,m]∩N be a set of m initial decision thresholds (lifespans), and k ∈ [1, b ∑ f(xi) βl c]∩N be the index of a moving window depending on the decision threshold βl, where b·c denotes the floor function. For each decision threshold βl, the moving windows are found by the following inequality,\nbk+1l = arg maxa∈N a ∈ (bkl , n] | a∑\ni=bkl\n∣∣∣∣f(xi)‖w‖ ∣∣∣∣ ≤ βl  ∀k (16) where | · | is the absolute operator, and each dynamic moving window is bounded by [bkl , b k+1 l ]∩N ⊆ [1, n]∩N, where bkl and b k+1 l are the beginning and end points of the kth moving window and b1l = 1. The dynamic window size of a decision threshold βl is bounded by the cumulative absolute distances |ri| = |f(xi)/‖w‖| from the optimal decision boundary to all the points within it. This is due to the magnitude of |ri| being closely related to the degree of confidence (sufficient information) for making a decision regarding classification. Let S̃k = [b k l , b k+1 l ]∩N be a set of indexes of the data instances contained within a dynamic moving\nwindow. This divides the entire interval [1, n] ∩ N into d ∑ f(xi) βl e partitions. A similar function to Equation 12 for labelling each data instance with respect to a decision threshold βl is defined as c̃ : Rn × R× N→ {±1}.\nc̃(f(x), βl, i) =\nb ∑ f(xi) βl\nc∑ k=1 1S̃k (i) sgn ( n∑ s=1 f(xs) ‖w‖ 1S̃k (s) ) (17)\nSimilar to Equation 14, a mean square error based function with respect to the class label is used for assessing the effectiveness of each decision threshold, defined as ẽ : R→ R.\nẽ(βl) = 1\nn n∑ i=1 ‖c̃(f(x), βl, i)− yi‖2 (18)\nThe dynamic moving window function returns an optimal decision threshold βopt ∈ B that minimises the resulting classification error, defined as\nβopt = arg min βl∈B {ẽ(βl)} (19)"
    }, {
      "heading" : "5 Experimental Design",
      "text" : "This section details the techniques used to implement the algorithms of interest and the synthetic data required to test the null hypotheses outlined in Section 3. Details of the raw datasets, experimental results and statistical analyses involved in this paper can be found in [11]."
    }, {
      "heading" : "5.1 Synthetic Datasets",
      "text" : "Synthetic datasets based on two Gaussian distributions are common practice in machine learning, as shown in [22]. This is due to the fact that varying the distance between the distributions allows for control over the separability of the data. For the experiments in Musselle’s work [16], where the temporal nature of the data is important, the author uses a Markov chain to generate synthetic datasets, where the probability of state change dictates the relative concentrations of the normal and anomalous behaviour.\nFor this investigation, both separability and temporal ordering are important. Therefore it was decided to use a dataset based on two Gaussian distributions, then introduce to it an artificial temporal ordering. This is achieved by creating time varying signals representing the class features. Each dataset is divided into quarters, where the first and third quarters are of class I and the second and fourth quarters are of class II. This ordering provides a low frequency underlying change of class, and provides examples of class transitions in both directions. As a consequence, by varying the separability of the classes, one also changes the signal to noise ratio of the time-ordered data, effectively maintaining the same level of noise, but increasing the magnitude of the underlying signal as the separability increases, as illustrated by Fig. 1.\nFor the generated datasets, class I’s mean is fixed at 0.2, and 100 datasets are generated by varying class II’s mean from 0.2 (total overlap), to 0.8 (linearly separable) at a regular interval. Both distributions use a standard deviation of 0.1. As the mean of class II increases, the Euclidean distance between the centroids of the two classes increases accordingly. This corresponds to the increment in separability of the two classes. Each dataset contains 2,000 instances, 1,000 for training and 1,000 for testing. By using large numbers of samples, it is intended to reduce artefacts caused by bias in the random number generator."
    }, {
      "heading" : "5.2 Algorithm Setup",
      "text" : "Parameters used in the linear SVM are the default values of the R package kernlab [15], and kept the same for both moving window functions. For the static moving window function, the cardinality of the set of initial window sizes |A| is 100, and a window size αl ∈ [1, 100]∩N. For the dynamic moving window function, the cardinality of the set of initial decision thresholds |B| is also 100, and a decision threshold βl is calculated as,\nβl = arg max xi∈X { |f(xi)| ‖w‖ } l |B| λ (20)\nwhere λ is a scaling factor that controls the window sizes considered by the parameter tuning. If λ = 1, windows are constrained to values typically used within the DCA literature. With λ = 100, parameters which are equivalent to those used by the static and dynamic moving window functions can also be considered by the tuning process.\nThe DCA often requires a preprocessing phase that is analogous to the training phase of the linear classifier algorithm, thus only testing sets are used by the DCA. Firstly the two input features are normalised into a range [0, 1] through min-max normalisation. The correlation coefficient between each feature and the class label is then calculated and used to map either of the features to the appropriate signal category. The remaining parameters are chosen according to the values suggested in [10]. The initialisation of lifespans in the DCA uses a similar principle as Equation 20, however the maximisation term is replaced by the signal transformation function of the algorithm and the entire set of lifespans are used for the DC population."
    }, {
      "heading" : "5.3 Statistical Tests",
      "text" : "All results will be tested using the Shapiro-Wilk normality test to verify if parametric or nonparametric statistical tests are suitable [5]. All of the null hypotheses in Section 3 are phrased as the absence of a detectable significant difference between pairs of results. The two-sided student t-test will be used for normally distributed samples, and the two-sided Wilcoxon signed rank test will be used for non-normally distributed ones [5].\nIf differences are detected, the one-sided versions of the relevant difference test will be used to ascertain the relative performance of the results. For all statistical tests a significance level of α = 0.05 will be considered sufficient."
    }, {
      "heading" : "6 Results and Analysis",
      "text" : "Results from the experiments are presented in terms of the error rates, which are equal to the number of misclassified data instances divided by the total number of instances in the tested dataset. The error rates of the six tested methods across all of the datasets are plotted against the Euclidean distance between the two class\ncentroids in Fig. 2. For non-separable cases, the classification performance differs from one method to another. In order to determine whether these differences are statistically significant, statistical tests are performed as follows.\nThe Shapiro-Wilk tests confirm that the data are not normally distributed (p-values are less than 0.05) and therefore the Wilcoxon tests are used to assess the statistical significance for both the two-sided and one-sided comparisons described previously. As all the p-values are less than 0.05, we reject the null hypotheses of all the two-sided Wilcoxon tests with a 95% confidence and conclude that significant differences exist between the results of the different methods. As a result, all of the three null hypotheses presented in Section 3 are rejected.\nFor completeness Fig. 2 shows results for λ = 1 (the original DCA parameter range) and the extended search space of λ = 100. However for analysis, we will only consider the best performing parameterisations of each unique method. From inspection of Fig. 2, it is argued that the order of the methods, in terms of ascending classification performance, is as follows: the linear SVM; the DCA (DCA1); the dynamic moving window function (DMOV2); and the static moving window function. As all the p-values of the one-sided Wilcoxon tests are less than 0.05, this inspection is statistically verified."
    }, {
      "heading" : "7 Discussion and Future Work",
      "text" : "The experimental results demonstrate that filtering the decisions of a linear classifier presented with time-ordered and noisy input data significantly changes and improves its classification performance. This was expected to be the case, as even when the datasets are non-separable in the feature space, the temporal ordering means that so long as the hyperplane has a greater than 50% accuracy, it is likely that the average of several instances from the same class, will tend towards the correct class label. This can also be viewed from the frequency domain as non-separability introducing a high frequency noise component into the signal, which can be removed by filtering.\nThe classification performance of the DCA is significantly different from a linear classifier, filtered or otherwise, on a time-ordered and noisy dataset. In fact, the DCA produces significantly better classification performance than a standard linear classifier, but significantly worse classification performance than the filtered linear classifiers. This implies that the filtering property of the DCA is an important factor of its performance, but that the addition of a training phase to the DCA can add further, substantial improvements.\nIt is also shown that the classification performance of a linear classifier with a static moving window function is significantly different and better, in comparison to that of a linear classifier with a dynamic moving window function. This is only a valid statement for the datasets used, but infers that the heuristic used by the DCA to alter the transfer function of its filtering component, (i.e. the magnitude of the input signal) is not as good as a simple, static filter.\nThese results suggest that the problems with the DCA are more deep-rooted than having linear decision boundaries. The DCA’s main advantage over the SVM seems to have been its novel filtering technique. However, by substituting the individual components of the DCA with traditional techniques from the domains of signal processing and machine learning, it is clear that it is outperformed. Finding equivalence between the DCA’s properties and standard techniques does not necessarily signal an end for the algorithm. However, if those standard techniques can be combined in such a way that their overall structure is the same as the DCA, but their overall performance is better, then there is a danger that “fixing” the DCA will eradicate it entirely. Before clear guidance can be formulated on when, if ever, the DCA is an appropriate choice for a given application, it is important to explore all of its algorithmically unique components. With the classification and filtering properties investigated the next properties that should come under scrutiny are the use of multiple timescales across the cell population and the sampling of antigen."
    } ],
    "references" : [ {
      "title" : "The Danger Project",
      "author" : [ "U. Aickelin", "S. Cayzer", "P. Bentley", "J. Greensmith", "J. Kim", "G. Tedesco", "J. Twycross" ],
      "venue" : "In http://ima.ac.uk/danger,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Behavioural Correlation for Malicious Bot Detection",
      "author" : [ "Y. Al-Hammadi" ],
      "venue" : "PhD thesis, School of Computer Science, University of Nottingham,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A Tutorial on Support Vector Mahinces for Pattern Recognition",
      "author" : [ "C.J.C. Buerges" ],
      "venue" : "Data Mining and Knowledge Discovery, 2:121–167,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "FDCM: A Fuzzy Dendritic Cell Algorithm",
      "author" : [ "Z. Chelly", "Z. Elouedi" ],
      "venue" : "In Proceedings of the 9th International Conference on Artificial Immune Systems (ICARIS), pages 102–115,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Statistics: An Introduction Using R",
      "author" : [ "M.J. Crawley" ],
      "venue" : "Wiley Blackwell,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Pattern Classification",
      "author" : [ "R.O. Duda", "P.E. Hart", "D.G. Stork" ],
      "venue" : "Wiley-Blackwell, 2nd edition,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Practical Methods of Optimization",
      "author" : [ "R. Fletcher" ],
      "venue" : "John Wiley and Sons,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Mixing Linear SVMs for Nonlinear Classification",
      "author" : [ "Z.Y. Fu", "A. Robles-Kelly", "J. Zhou" ],
      "venue" : "IEEE Transactions on Neural Networks, 21(12):1963 – 1975,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The Dendritic Cell Algorithm",
      "author" : [ "J. Greensmith" ],
      "venue" : "PhD thesis, School of Computer Science, University of Nottingham,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The Deterministic Dendritic Cell Algorithm",
      "author" : [ "J. Greensmith", "U. Aickelin" ],
      "venue" : "In Proceedings of the 7th International Conference on Artificial Immune Systems (ICARIS), pages 291–303,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Documentation of ICARIS 2011 paper (raw data, experimental results and statistical analysis)",
      "author" : [ "F. Gu", "J. Feyereisl", "R. Oates", "J. Reps", "J. Greensmith", "U. Aickelin" ],
      "venue" : "http://www.cs.nott.ac.uk/~fxg/icaris_paper2011.html,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Exploration of the Dendritic Cell Algorithm with the Duration Calculus",
      "author" : [ "F. Gu", "J. Greensmith", "U. Aickelin" ],
      "venue" : "In Proceedings of the 8th International Conference on Artificial Immune Systems (ICARIS), pages 54–66,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Artificial immune systems and kernel methods",
      "author" : [ "T.S. Guzella", "T.A. Mota-Santos", "W.M. Caminhas" ],
      "venue" : "In Proceedings of the 7th International Conference on Artificial Immune Systems (ICARIS), pages 303–315,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Digital Signal Processing: A Practical Approach",
      "author" : [ "E. Ifeachor", "P.B. Jervis" ],
      "venue" : "Prentice Hall, 2nd edition,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Kernel-based machine learning lab",
      "author" : [ "A. Karatzoglou", "A. Smola", "K. Hornik" ],
      "venue" : "Technical report, Department of Statistics and Probability Theory, Vienna University of Technology,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Insight into the Antigen Sampling Component of the Dendritic Cell Algorithm",
      "author" : [ "C. Musselle" ],
      "venue" : "In Proceedings of the 9th International Conference on Artificial Immune Systems (ICARIS), pages 88–101,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The Suitability of the Dendritic Cell Algorithm for Robotic Security Applications",
      "author" : [ "R. Oates" ],
      "venue" : "PhD thesis, School of Computer Science, University of Nottingham,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Frequency Analysis for Dendritic Cell Population Tuning: Decimating the Dendritic Cell",
      "author" : [ "R. Oates", "G. Kendall", "J. Garibaldi" ],
      "venue" : "Evolutionary Intelligence, 1(2):145– 157,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The Limitations of Frequency Analysis for Dendritic Cell Population Modelling",
      "author" : [ "R. Oates", "G. Kendall", "J. Garibaldi" ],
      "venue" : "In Proceedings of the 7th International Conference on Artificial Immune Systems (ICARIS), pages 328–339,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Classifying in the presence of uncertainty: a DCA persepctive",
      "author" : [ "R. Oates", "G. Kendall", "J. Garibaldi" ],
      "venue" : "In Proceedings of the 9th International Conference on Artificial Immune Systems (ICARIS), pages 75–87,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning with Kernels : Support Vector Machines, Regularization, Optimization, and Beyond",
      "author" : [ "B. Scholkopf", "A.J. Smola" ],
      "venue" : "The MIT Press,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Geometrical insights into the dendritic cell algorithm",
      "author" : [ "T. Stibor", "R. Oates", "G. Kendall", "J. Garibaldi" ],
      "venue" : "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 1275–1282,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The Dendritic Cell Algorithm (DCA) is an immune-inspired algorithm developed as part of the Danger Project [1].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "Despite being applied to a number of applications, it was originally designed and used as an anomaly detection and attribution algorithm [9].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 17,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 17,
      "context" : "For example, it has been shown that the structure of a single dendritic cell within the DCA is similar in function to the operation of a filter with a dynamically changing transfer function [18,19].",
      "startOffset" : 190,
      "endOffset" : 197
    }, {
      "referenceID" : 18,
      "context" : "For example, it has been shown that the structure of a single dendritic cell within the DCA is similar in function to the operation of a filter with a dynamically changing transfer function [18,19].",
      "startOffset" : 190,
      "endOffset" : 197
    }, {
      "referenceID" : 21,
      "context" : "One such property is that its classification stage is functionally equivalent to a statically weighted, linear classifier [22].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "Within the literature, it has been suggested that replacing the classification stage of the DCA with a trainable, nonlinear, machine learning algorithm would negate much of the criticism made of the DCA while preserving its novel properties [13,20,22].",
      "startOffset" : 241,
      "endOffset" : 251
    }, {
      "referenceID" : 19,
      "context" : "Within the literature, it has been suggested that replacing the classification stage of the DCA with a trainable, nonlinear, machine learning algorithm would negate much of the criticism made of the DCA while preserving its novel properties [13,20,22].",
      "startOffset" : 241,
      "endOffset" : 251
    }, {
      "referenceID" : 21,
      "context" : "Within the literature, it has been suggested that replacing the classification stage of the DCA with a trainable, nonlinear, machine learning algorithm would negate much of the criticism made of the DCA while preserving its novel properties [13,20,22].",
      "startOffset" : 241,
      "endOffset" : 251
    }, {
      "referenceID" : 9,
      "context" : "The algorithmic details can be found in [10].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 21,
      "context" : "It is demonstrated in [22] that both the classification boundary and the position of the decision boundary can be expressed as hyperplanes, akin to those found in linear classifiers.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "As the classification performed by a cell is performed using the history of the sampled signals rather than an instantaneous sample of the environmental features, it can be shown that the DCA exhibits a filtering property which allows it to remove high frequency noise from the input signals [18].",
      "startOffset" : 292,
      "endOffset" : 296
    }, {
      "referenceID" : 2,
      "context" : "In our investigation, the classification stage of the DCA is replaced by a trainable classifier, which is based on the operation of Support Vector Machines (SVM) [3].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "SVM models can be described using linear discriminant functions [6], quadratic optimisation [7], and kernel methods [21].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "SVM models can be described using linear discriminant functions [6], quadratic optimisation [7], and kernel methods [21].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 20,
      "context" : "SVM models can be described using linear discriminant functions [6], quadratic optimisation [7], and kernel methods [21].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "The primal form LP is differentiable with respect to w and b, and an equivalent dual form, known as the Wolfe dual [7], can be derived.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "The optimisation becomes a convex quadratic programming problem, and all data points that satisfy the constraints also form a convex set [3].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 20,
      "context" : "where Φ is a mapping from the original input feature space X to a higher dimensional (inner product) feature space F , where nonlinearly separable problems become more separable [21].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 20,
      "context" : "Depending on the applications, a number of kernel functions are available, including linear kernels, polynomial kernels, and Gaussian kernels [21].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "It is more computationally efficient than other SVM models that use more complicated kernel functions [8].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "The filter with the most analogous behaviour to the DCA is the sliding window filter [18].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "The transfer function of the sliding window filter is given in Equation 10 [14],",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "A dendritic cell acts like a sliding window filter which only reports its output every W steps [18].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "Details of the raw datasets, experimental results and statistical analyses involved in this paper can be found in [11].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "Synthetic datasets based on two Gaussian distributions are common practice in machine learning, as shown in [22].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "For the experiments in Musselle’s work [16], where the temporal nature of the data is important, the author uses a Markov chain to generate synthetic datasets, where the probability of state change dictates the relative concentrations of the normal and anomalous behaviour.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "Parameters used in the linear SVM are the default values of the R package kernlab [15], and kept the same for both moving window functions.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "For the static moving window function, the cardinality of the set of initial window sizes |A| is 100, and a window size αl ∈ [1, 100]∩N.",
      "startOffset" : 125,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "Firstly the two input features are normalised into a range [0, 1] through min-max normalisation.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "The remaining parameters are chosen according to the values suggested in [10].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "All results will be tested using the Shapiro-Wilk normality test to verify if parametric or nonparametric statistical tests are suitable [5].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "The two-sided student t-test will be used for normally distributed samples, and the two-sided Wilcoxon signed rank test will be used for non-normally distributed ones [5].",
      "startOffset" : 167,
      "endOffset" : 170
    } ],
    "year" : 2013,
    "abstractText" : "Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yielded several criticisms about its underlying structure and operation. As a result, several alterations and fixes have been suggested in the literature to correct for these findings. A contribution of this work is to investigate the effects of replacing the classification stage of the DCA (which is known to be flawed) with a traditional machine learning technique. This work goes on to question the merits of those unique properties of the DCA that are yet to be thoroughly analysed. If none of these properties can be found to have a benefit over traditional approaches, then “fixing” the DCA is arguably less efficient than simply creating a new algorithm. This work examines the dynamic filtering property of the DCA and questions the utility of this unique feature for the anomaly detection problem. It is found that this feature, while advantageous for noisy, time-ordered classification, is not as useful as a traditional static filter for processing a synthetic dataset. It is concluded that there are still unique features of the DCA left to investigate. Areas that may be of benefit to the Artificial Immune Systems community are suggested.",
    "creator" : "LaTeX with hyperref package"
  }
}