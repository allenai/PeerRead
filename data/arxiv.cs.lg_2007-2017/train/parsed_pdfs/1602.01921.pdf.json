{
  "name" : "1602.01921.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Characteristics of Visual Categorization of Long-Concatenated and Object-Directed Human Actions by a Multiple Spatio-Temporal Scales Recurrent Neural Network Model",
    "authors" : [ "Haanvid Lee", "Minju Jung", "Jun Tani" ],
    "emails" : [ "tani1216jp@gmail.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Recent studies on exploring the visual recognition mechanisms both in artificial systems and human brains have attracted large attentions. Especially, computational approaches using deep learning scheme have achieved significant advancement in recognition of static visual images of natural objects. The essential idea of machine vision based on deep learning is that all necessary information processing functions and structures for recognizing visual image patterns are developed in hierarchical neural network models via iterative learning of exemplar visual image patterns. Especially, a convolutional neural network (CNN) [2], which has been developed as inspired by the mammalian visual cortex for its spatial hierarchical processing of visual features, has shown remarkably better recognition performance for static natural visual images compared to conventional vision recognition schemes which used elaborately hand-coded visual features. Actually, recent CNN trained with 1 million of visual image in ImageNet [3] can classify hundreds of object image with error rate of 0.0665 [4]. It is considered that the CNN’s performance in this task is close to that of human [5]. Despite the success of CNN in recognition of static visual images, the research outcomes\nfor recognition of dynamic visual image have not been confirmative yet. This is because the network models have to cope with massive amount of information extended not only in spatial dimensions but also in temporal dimension. A typical approach to the problem has been to use a 3D convolutional neural network (3D CNN) [6] in which dynamic visual image patterns can be recognized by simply transforming a sequence of 2-dimensional visual spatial patterns within a fixed temporal window into a large 3-dimensional pattern. Actually, 3D CNN showed good performances on many challenging video recognition public datasets related to human action categories such as UCF-101 [7] and HMDB-51 [8] by extracting short-range temporal correlations in the temporal window. However, the benchmark studies using such datasets may not prove always the recognition capability of the model for dynamic visual image of human movement, because it was shown that even CNN without any temporal processing capability can achieve recognition rate near 60% for those video images. Video images such as a person riding a horse or a person playing cello in those dataset can be mostly recognized only with information on static visual images of objects such as horse and cello without using temporal information latent in human movement patterns. Baccouche et al. [9] has proposed a two-stage model to maintain temporal information in the entire sequence by adding a long short-term memory (LSTM) network [10] as a second stage of the 3D CNN. Similarly, Venugopalan and colleagues [11] proposed an architecture composed of a CNN for video processing and an LSTM concatenated downstream of the CNN for the generation of corresponding word sequences that was trained by using nearly a hundred thousand video clips with annotated descriptive sentences. Although in the first glance this research outcome seems impressive, the same question arises, when we see the test example videos used in this task, such as a cat playing with a toy or man playing with a soccer ball. Does recognition of such video images require the network model to extract any deep temporal structures hidden in human movement patterns? If machine vision systems are required to recognize human actions with semantics, they have to perceive continuous visual stream by extracting the underlying spatio-temporal structures. Such spatio-temporal structures should be involved with compositionality in human action generation [12, 13]. Compositionality means that the whole can be composed/decomposed by/into reusable parts. Complex human actions can be characterized by compositionality both in spatial and temporal dimensions. The temporal compositionality can be accounted by the fact that most of goal-directed human actions are composed by sequential combinations of commonly used behavior primitives [12]. The spatial compositionality can be accounted by combinations between transitive actions and objects in object-directed actions or coordinated combination of movement patterns in different limbs. The challenge in visual understanding of human action is to extract such compositional structure via iterative learning of visual experiences in observing various actions. And this job has to be done under the condition that each trajectory of perceived visual stream could be diverse even for the same category of action. This is because profiles of behavior primitives are quite deformational depending on each individual. Additionally, visual appearance of actions are totally dependent on the view of the observer. Upon this thought, Jung et al. proposed the multiple spatio-temporal scale neural network (MSTNN) model. The MSTNN imposes both spatial and temporal constraints on dynamic neural activity in the network. More specifically, the subnetworks in the lower level employ smaller size kernels allowing only local connections, and leaky integrator units employing a smaller time constant, while those in the higher level employ larger size kernels with greater global connectivity, and leaky integrator units with a larger time constant. It was shown that MSTNN becomes able to categorize different sequential combinations of behavior primitives\nacross different subjects by extracting long-term correlation via learning of exemplar patterns. However, MSTNN has shown its limitation in learning longer sequences of behavior primitive patterns. This is due to the network architecture of the MSTNN that utilizes slow damping dynamics in leaky integrator neurons allocated in the higher level network for memorizing episode perceived in the past. Using just leaky integrator neurons with different timescales are not enough for learning more complicated and context sensitive sequences. In addressing this problem, the current paper propose to add leaky integrator neural units with recurrent connections at each level to MSTNN. This newly proposed model, multiple spatiotemporal scale recurrent neural network (MSTRNN) contains both of the visual feature leaky integrator neural units without recurrent connectivity and the contextual leaky integrator neural units with recurrent connectivity with the same time constant at each level. The current paper investigates how such addition of recurrent neural units can contribute to enhancement of learning of visual sequences with long time dependency. Another focus in the current study is to investigate how structural relationship between objects and transitive actions can be learned with generalization in the proposed network model. For this purpose, a video data set of containing a set of object-directed actions performed by multiple subjects was prepared. This set of object-directed actions was designed such that the category of object-directed action cannot be inferred in a trivial manner. In this designing, Book, Laptop, Bottle, and Cup were considered for the set of objects whereas Sweep, Open, Close, Drink, Change-page, Type, Shake, Stir and Blow were considered for the set of transitive actions. An object can appear with multiple transitive actions such as Drink-Cup, Strir-Cup, and Blow-Cup. On the other hand, a transitive action can appear with multiple objects such as Open-Book, Open-Laptop, and Open-Bottle. For preparing learning dataset, 10 subjects were asked to generate these many-to-many combinations of object-directed actions freely when recorded in video. After the model network was trained with those multiple subjects video dataset, its categorization capability was examined by employing leave-one-subject-out crossvalidation (LOSOCV) scheme. The current study will examine the capability of generalization in learning of the proposed model by conducting various test analysis. Those include comparative examinations of the model performance especially focusing on its miscategorization characteristics observed in test categorization for a set of object-directed actions and pantomime actions without accessing physical objects. Such comparative analysis can gain understanding of how categorical memories for a set of different object-directed actions can be developed in their relational structure in distributed activities of neural units both in artificial systems and in humans. The next section describe our newly proposed model, MSTRNN followed by descriptions of two different experiment tasks, the one for examination of categorization performance in longer sequences of movement primitives compared to our prior study using MSTNN and the other for testing by using the aforementioned datasets for object-directed actions."
    }, {
      "heading" : "2. Model",
      "text" : "The Multiple Spatio-Temporal Recurrent Neural Network (MSTRNN) is a hierarchical neural network model that has the capacity of extracting spatio-temporal features in the dynamic images. In the prior study, a dynamic neural network model, referred to Multiple SpatioTemporal Neural Network (MSTNN) [1] was developed for the purpose of automatic categorization of video image patterns based on learning. MSTNN was developed by combining two prior existing models of Convolutional Neural Network (CNN) [2] and Multiple Timescales Recurrent Neural Network (MTRNN) [14]. The basic idea of the MSTNN is that both spatial and temporal constraints imposed on neural activity change\ndepending on the layer. The model used feature maps made of leaky integrator neurons with time constants that have same values in the same layer. The time constant is set to be smaller in the lower layer and larger in the higher level. At the same time, the connectivity is set with more local with smaller kernel size in the lower level and more global with larger kernel size in the higher level. It was shown that MSTNN can extract spatio-temporal feature hierarchy by adopting such constraints in the model architecture. However, the MSTNN did not fully exploit the core mechanism for temporal processing in RNN or MTRNN that is recurrent context processing using recurrent connectivity in the internal units or so-called the context units. It has been shown that context units with recurrent connectivity play important roles in extracting latent temporal structures from exemplar temporal sequences [15, 16]. Therefore, the current model MSTRNN has been developed to contain the context maps with recurrent connectivity in addition to the feature maps without recurrent connectivity that has been used in MSTNN. The context units are connected to the feature units that are in the same context layer to enhance the model’s capability of extracting spatio-temporal features from exemplar. In the following section 2.1, the structure of the proposed model will be explained in more detail. And in section 2.2 and 2.3, the forward dynamics of the proposed model and training method will be discussed."
    }, {
      "heading" : "2.1 Model Architecture",
      "text" : "The MSTRNN model consists of the following layers: the input layer, a series of the context layers, the fully-connected layers, and the output layer. The overall structure of the model is shown in the Figure 1 (a). At the bottom of the model, there is an input layer. On top of the input layer, several context layers, which will be introduced later on, are sequentially connected. After series of the context layers, two fully-connected layers are used. And then the output layer is connected to the top of the fully-connected layers. In the MSTRNN model, the context layer is considered as the core building block. A context layer consists of feature units used in the MSTNN model, and our newly designed context units, and pooling units [17] if the layer does the pooling operation (see Figure 1 (b), (c)). The context layer was built by adding context units that have recurrent weights to the sequentially connected convolutional layer and the max-pooling layer. By addition the context units, the proposed MSTRNN is expected to enhance its capability of extracting spatiotemporal structures latent in exemplar visual stream. The context layer is made of maps of leaky integrator neurons of which decaying rate of the internal states is determined by their time constant. The context units have recurrent weights that connect the units themselves. The weights are connected between the maps of the context units from the previous time step to the current time step in the same retinotopic positions. The connection is made on the same retinotopic positions since the model uses convolutional kernels that compress spatial features of its input by certain ratio according to its size. The recurrent connections are established in order to make the previous time step’s internal states of the context units to have an effect on the current time step’s internal states of the units. The context units manipulate contextual information of its input sequences by means of nonlinear mapping in similar way with the context layers in the MTRNN. Besides the input received from the recurrent connections, the context units also receive input from the max-pooling units via convolutional kernel as shown on the Figure 1. (b). If the map size of the feature units is 1x1, then the pooling operation cannot be carried out on the feature units. In the case of the context layer without max-pooling units, the context units receives input from the context units as shown on the Figure 1. (c).\nThe feature units in the context layer are also made of leaky integrator neurons that have\nsame time constants to that of the context units. Having the same time constants makes the internal states of the feature units and that of context units to share the same decaying rate. This makes the feature units and the context units in the same context layer to have same temporal dynamics. Besides the decayed internal states, the feature units of the context layer receives input from two sources. The first source is the previous context layer’s pooling units. If the previous context layer does not have pooling operation, the source becomes the feature units of the previous layer. In the case where the current context layer is the first one, then the second source is the feature maps of the input layer. The connection between the feature units of the current context layer and its first source of input is made by the convolutional kernels. Second source is the context units in the previous context layer. If the current context layer is the first one, then since the previous layer is the input layer, it does not have the second source. The feature units receive input from the context units in the previous context layer via weights that connect between the two units in the same retinotopic positions of their mapping. Since the model was designed to have same map size for both of the feature units in the current context layer, and the context units in the previous context layer, the weights connect on the same retinotopic positions. If the context layer has max-pooling operation, the feature units of the layer provide inputs to the max-pooling units via max-pooling patches. There are the same number of maps for the feature units and the max-pooling units. This is because the maps of both the feature units and the max-pooling units are coupled 1:1 by the max-pooling operation. The max-pooling units, unlike the feature units and context units, are not made of the leaky integrator neurons, but they are consisted of normal static neurons that do not keep the internal states in the previous time step. This is because the functionality of the max-pooling units lies not in capturing temporal structures, but in capturing the translation invariant features of the images and reducing the feature size [17]. The time constant of a context layer becomes larger as the context layer goes up. Larger time constant makes the internal states of leaky integrator type neurons used in feature units and the context units of the context layer change more slowly at each time step. With the smaller time constant, the internal states of the leaky integrator neurons change faster than the larger one. The proposed MSTRNN model is assigned with larger time constants in the higher context layers and the smaller one in the lower layers so that the whole network can develop spatiotemporal hierarchy, as analogous to MTRNN as well as MSTNN. The input layer of the MSTRNN consists of one greyscale feature map of monochrome video or three feature maps for three RGB channels of color videos depending on experiments described later. The size of the feature may vary depending on the input videos frame size. The output layer of the model consists of one or more softmax vectors depending on the number of classification category groups the task requires. In the first experiment of classifying the action category of the input video, the model uses one softmax vector for the output layer. In the second experiment, of classifying two types of categories, namely object category and objectdirected action category of the input video, the model uses two softmax vectors for representing these two types of categories."
    }, {
      "heading" : "2.2 Forward Dynamics",
      "text" : "The forward dynamics of the context units are shown in the Equation 1, 2. The internal states and the activation values of the ath context map at the lth map of context units at time step t and at the retinotopic coordinate of (x, y) are represented as \uD835̂\uDC50\uD835\uDC50\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 and \uD835\uDC50\uD835\uDC50\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 respectively.\n\uD835̂\uDC50\uD835\uDC50\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 = �1 − 1 \uD835\uDF0F\uD835\uDF0F\uD835\uDC59\uD835\uDC59 � \uD835̂\uDC50\uD835\uDC50\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 (\uD835\uDC61\uD835\uDC61−1)\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 + 1 \uD835\uDF0F\uD835\uDF0F\uD835\uDC59\uD835\uDC59 ��(\uD835\uDC58\uD835\uDC58�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 ∗ \uD835\uDC53\uD835\uDC53\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61 )\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\n\uD835\uDC41\uD835\uDC41\uD835\uDC59\uD835\uDC59\n\uD835\uDC59\uD835\uDC59=1\n+ \uD835\uDC4F\uD835\uDC4F�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59� + 1 \uD835\uDF0F\uD835\uDF0F\uD835\uDC59\uD835\uDC59 ��\uD835\uDC64\uD835\uDC64�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 \uD835\uDC50\uD835\uDC50\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 (\uD835\uDC61\uD835\uDC61−1)\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\n\uD835\uDC35\uD835\uDC35\uD835\uDC59\uD835\uDC59\n\uD835\uDC59\uD835\uDC59=1\n� (1)\n\uD835\uDC50\uD835\uDC50\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 = 1.7159 tanh � 2 3 \uD835̂\uDC50\uD835\uDC50\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61� (2)\nWhere τ\uD835\uDC59\uD835\uDC59 represents the time constant of lth context layer, \uD835\uDC58\uD835\uDC58�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 is the convolutional\nkernel for \uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61ℎ context layer that connects the lth layer, mth map of the feature units to the lth layer, ath map of the context units, \uD835\uDC4F\uD835\uDC4F�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 is the bias for the convolution operation for calculating ath map of context units at lth context layer, ∗ is the convolution operator, \uD835\uDC41\uD835\uDC41\uD835\uDC59\uD835\uDC59 is the total number of maps in the previous units in lth context layer, \uD835\uDC35\uD835\uDC35\uD835\uDC59\uD835\uDC59 is the total number of maps in the context units of the lth context layer, \uD835\uDC64\uD835\uDC64�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\n\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 is the recurrent weight that connects between the bth map of the context units in the lth context layer of the previous time step and the ath map of the context units in the lth context layer of the current time step on the same retinotopic position (x, y). The first two terms on the right hand side (RHS) of the Equation 1 are same with the forward dynamics of convolutional layer of the MSTNN. The first term represents previous time step (t-1)’s internal states of the context units decayed by the term (1-1/\uD835\uDF0F\uD835\uDF0F\uD835\uDC59\uD835\uDC59). The second\nterm represents activation values of previous units convoluted by the corresponding context units’ convolutional kernels \uD835\uDC58\uD835\uDC58�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 and the bias \uD835\uDC4F\uD835\uDC4F�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59. The difference between the context units of the MSTRNN and the convolutional layer of the MSTNN is made by the 3rd term on the RHS of the Equation 1. The third term represents the dynamics made by the recurrent weight \uD835\uDC64\uD835\uDC64�\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 . The recurrent weight is multiplied to the internal state of the context units in the previous time step \uD835\uDC50\uD835\uDC50\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 (\uD835\uDC61\uD835\uDC61−1)\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61. To calculate activation values of the context units from its internal states, hyperbolic tangent function was used as shown on Equation 2. The forward dynamics of the feature units are shown in the Equation 3, 4. The internal states and the activation values of the mth map of feature units at the lth context layer at time step t and at the retinotopic coordinate of (x, y) are represented as \uD835\uDC53\uD835\uDC53\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 and \uD835\uDC53\uD835\uDC53\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 respectively.\n\uD835\uDC53\uD835\uDC53\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 = �1 − 1 \uD835\uDF0F\uD835\uDF0F\uD835\uDC59\uD835\uDC59 � \uD835\uDC53\uD835\uDC53\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 (\uD835\uDC61\uD835\uDC61−1)\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 + 1 \uD835\uDF0F\uD835\uDF0F\uD835\uDC59\uD835\uDC59 ���\uD835\uDC58\uD835\uDC58\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 ∗ \uD835\uDC53\uD835\uDC53(\uD835\uDC59\uD835\uDC59−1)\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61 � \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\n\uD835\uDC41\uD835\uDC41\uD835\uDC59\uD835\uDC59−1\n\uD835\uDC59\uD835\uDC59=1\n+ \uD835\uDC4F\uD835\uDC4F\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59� + 1 \uD835\uDF0F\uD835\uDF0F\uD835\uDC59\uD835\uDC59 �� \uD835\uDC64\uD835\uDC64\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 \uD835\uDC50\uD835\uDC50(\uD835\uDC59\uD835\uDC59−1)\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\n\uD835\uDC34\uD835\uDC34\uD835\uDC59\uD835\uDC59−1\n\uD835\uDC59\uD835\uDC59=1\n� (3)\n\uD835\uDC53\uD835\uDC53\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 = 1.7159 tanh � 2 3 \uD835\uDC53\uD835\uDC53\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61� (4)\nWhere τ\uD835\uDC59\uD835\uDC59 represents the time constant of the lth context layer, \uD835\uDC58\uD835\uDC58\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 is the convolutional\nkernel for the lth context layer that connects nth map of feature units in the previous context layer and mth map of feature units in the current context layer, \uD835\uDC4F\uD835\uDC4F\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 is the bias used in the convolution operation for calculating the mth map of the feature units in the lth context layer, ∗ is the convolution operator, \uD835\uDC41\uD835\uDC41\uD835\uDC59\uD835\uDC59 is the total number of maps of the feature units in the lth context layer (\uD835\uDC41\uD835\uDC410 is the total number of feature maps in the input layer), \uD835\uDC34\uD835\uDC34\uD835\uDC59\uD835\uDC59 is the total number of maps of the context units in the lth context layer (The total number of maps of context units in the input layer’s is “0” or \uD835\uDC34\uD835\uDC340= 0). First two terms on the RHS of the Equation 3 are adopted from the forward dynamics of the convolutional layer of the MSTNN. The third RHS term of the Equation 3 represents the values calculated by multiplying the activation values of the context units in the previous context layer \uD835\uDC50\uD835\uDC50(\uD835\uDC59\uD835\uDC59−1)\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 with the corresponding weight \uD835\uDC64\uD835\uDC64\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 \uD835\uDC61\uD835\uDC61\uD835\uDC61\uD835\uDC61 and adding it to the internal states of the feature units in the current context layer in the same retinotopic positions (x, y). For the activation function, as shown on Equation 4, the model used hyperbolic tangent function. The fully-connected layer that is connected to the last context layer can be considered as a context layer that has time constant of “1” with the map size of 1x1. By regarding as so, the forward dynamics of the first fully-connected layer can be explained also by the Equation 3. Due to the setting of the time constant of the layer as 1.0, the first RHS term of the equation 3 vanishes. And the second term on the RHS of the Equation 3, which is the convolution term for the feature units, becomes a term for the fully-connected layer by setting the map size of the units as 1x1. The third term in the RHS of the equation is used, because the previous layer is the context layer that has context units of which map size is 1x1. For the calculation of the activation values, the neurons in the fully-connected layers also use the same activation function that was used by the feature units in the context layer. Therefore, the Equation 4 can explain how the activation values of the neurons in the fully-connected layer are obtained from their internal states. The forward dynamics of the second fully-connected layer can be also explained in the same way. The internal states of neurons in the output layer can be computed in the same manner as computed for the fully-connected layers by the Equation 3. But the calculation for obtaining the activation values of the neurons are different from that of the fully-connected layer since the output layer consists of softmax vector of multiple dimensions. The exact computation of\nactivation of softmax vector is shown in Equation 5. The internal states and the activation values of the mth neuron of the sth softmax vector at time step t are represented by \uD835\uDC5C\uD835\uDC5C�\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61 and \uD835\uDC5C\uD835\uDC5C\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61 respectively.\n\uD835\uDC5C\uD835\uDC5C\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61 = exp (\uD835\uDC5C\uD835\uDC5C�\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61 )\n∑ exp (\uD835\uDC5C\uD835\uDC5C�\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61 ) \uD835\uDC41\uD835\uDC41\uD835\uDC60\uD835\uDC60 \uD835\uDC59\uD835\uDC59=1\n(5)\nWhere \uD835\uDC41\uD835\uDC41\uD835\uDC60\uD835\uDC60 is the number of neurons in the sth softmax vector. Each of softmax vectors\nrepresent a group of categories. For example, in the experiment that will be introduced in the section 3.2, the task for the MSTRNN model is to classify the action category and the directedobject category separately. And therefore the model used in the experiment has two softmax vectors in the output layer. Each neuron in a softmax vector represents a category. For example, if the vector represents categorical output for the action class with 9 categories, there are 9 neurons in the vector that represent each of action categories. Activation values of each softmax vectors is obtained by normalizing the internal states of the neurons separately in each vectors. Therefore when each vector’s activation values of the softmax neurons are summed, the summed value becomes “1”. Categorization in the model was performed in the delay response manner [1]. The model was trained to generate the categorization outputs after the end of input video stream followed by black frames. We set delay response period with black frame as 15 time steps for all of the experiments. The categorized output per frame for each of the groups of categories were made by selecting the categories that are represented by the neurons that had the largest activation values among the neurons in each of the softmax vectors. The categorized outputs per frame during the delay response period were counted. And the most confident categories were chosen as the categorized outputs for the input video image."
    }, {
      "heading" : "2.3 Training",
      "text" : "The training was conducted by a supervised manner using the delay response scheme [1]. During the teaching period, errors were calculated during the delay response period for each of time steps by comparing the true outputs and the classified outputs. The comparison was made by the Kullback-Leibler divergence. The cost function used for training of the MSTRNN model is shown in the Equation 6. The error calculated for an input video is represented as E.\nE = � ��\uD835\uDC5C\uD835\uDC5C�\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59\uD835\uDC59 � \uD835\uDC5C\uD835\uDC5C�\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59 \uD835\uDC5C\uD835\uDC5C\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61\n� \uD835\uDC46\uD835\uDC46\n\uD835\uDC60\uD835\uDC60=1\n\uD835\uDC41\uD835\uDC41(\uD835\uDC60\uD835\uDC60)\n\uD835\uDC59\uD835\uDC59=1\n\uD835\uDC47\uD835\uDC47\n\uD835\uDC61\uD835\uDC61=\uD835\uDC47\uD835\uDC47−\uD835\uDC51\uD835\uDC51+1\n(6)\nWhere d is the delay response period, T is the input video’s duration (length of frames),\nS is the total number of softmax vectors in the output layer. N(s) is the total number of neurons in the sth softmax vector of the output layer, n is the index for the neurons in the sth softmax vector in the output layer, \uD835\uDC5C\uD835\uDC5C�\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59 is the true output at nth neuron in the sth softmax vector, \uD835\uDC5C\uD835\uDC5C\uD835\uDC60\uD835\uDC60\uD835\uDC59\uD835\uDC59\uD835\uDC61\uD835\uDC61 is the classified output made on the nth neuron in sth softmax vector at time step t. The true output was given as the one-hot-vector for each of the softmax vectors used in the output layer. True outputs for the neurons that represent correct categories in each of the softmax vectors were given as “1”. And rest of the softmax vectors’ neurons were given “0” for the true output. The error obtained by the Equation 6 for an input video was used for optimizing a set of learnable parameters by using back propagated through time (BPTT) [18]. And the stochastic gradient descent algorithm was used to update the learnable parameters in the model. The\nlearnable parameters except the biases were learned using weight decay of 0.0005 and fullyconnected layer was trained with 50% dropout rate [19] to prevent the overfitting."
    }, {
      "heading" : "3. Experiments",
      "text" : "The current study examined the characteristics of the MSTRNN model by conducting two types of experimental tasks for learning to categorize human actions. The first experiment task examined the capability of MSTRNN in learning longer sequences of human movement patterns as compared to the capability of MSTNN [1] in the same task. The second experiment task examined how the MSTRNN can learn to categorize a set of object directed actions with generalization by conducting various cognitive analysis."
    }, {
      "heading" : "3.1 Learning to categorize longer sequences of movement patterns",
      "text" : ""
    }, {
      "heading" : "A. Task description",
      "text" : "The experiment was conducted for comparing the capability of learning to categorize a set of compositional long visual sequences between MSTRNN and MSTNN. For this purpose, a set of exemplar video data was prepared by concatenating 3 different human action videos from the Weizmann dataset. Those 3 human actions from the Weizmann dataset are, jump-in-place (JP), one-hand-wave (OH), two-hand-wave (TH) as shown on Figure 2. The dataset is consisted of 9 subjects, 27 categories, and one trial of action sequence for each category. And the foreground silhouettes of the Weizmann dataset was used by background subtraction utilizing background sequences. The silhouette frames were resized to 48x54. For the structure of the MSTRNN and MSTNN models used in the experiment, both have input layer that has one feature map for the input of grayscale video images. The size of the feature map is 48x54. Both models have the identical output layer structure. Output layer consists of a softmax vector that has 27 neurons. Each of the softmax neurons in the vector represents one of 27 movement concatenation categories in the dataset. The details of the MSTRNN’s structure and the MSTNN’s structure, except the input and output layers, are specified in the Table 1 and Table 2 respectively. The only difference between the architectures of the MSTNN and the MSTRNN used in the experiment is either using or not using the context units in those models. From the Figure 1. (b), (c), it can be seen that the MSTRNN’s structure can be converted to the MSTNN model by deleting the context units in the MSTRNN model.\nBoth the MSTRNN and the MSTNN models were trained according to the training method described in section 2.3 and using LOSOCV scheme. The learning rate of both the MSTNN model and the MSTRNN model were set to the same value of 0.1. And both models were trained for 50 epochs and tested with the test data in every epoch. By using LOSOCV method, each of the 9 subjects’ dataset were left out from the train data to be used as the test data. Therefore, 9 accuracies were obtained from 9 different cases using each of the 9\nsubjects’ data as the test data for 50 epochs. For the evaluation measure, the accuracies were averaged across the 9 test subjects and the maximum categorization accuracy was used. The accuracy was rounded off to the first place of decimal point."
    }, {
      "heading" : "B. Experiment results",
      "text" : "The simulation result turned out that the categorization accuracy of the MSTRNN almost doubled the one by the MSTNN as shown in Figure 3 (The result video is available at https://sites.google.com/site/haanvidlee/mstrnn-experiment-1-demo-video). The categorization accuracy of the MSTRNN was 83.5% while the one by MSTNN was just 43.2%. This result implies that the context units with recurrent weights contributed in better categorization for long-ranged video image of compositional human action sequences."
    }, {
      "heading" : "3.2 Learning to categorize object-directed actions",
      "text" : ""
    }, {
      "heading" : "A. Description of the task",
      "text" : "In the current task, the video dataset was prepared by using 9 actions directed to 4 objects which were video-shoot with 10 subjects. In the dataset, 15 object-directed action categories were prepared as the total, as shown in Figure 4 and in Table 3. The video dataset was made such that generalization can be preserved in some extents. We allocated non-directed objects as distractors along with action directed objects in the video scene. For each object directed action category, 6 videos were shot for each subject with appearance of three different nondirected objects repeated twice for each. This setting was introduced for the purpose of later analysis on the model capability to capture possible links between dynamic visual patterns of actions and directed objects. The subjects generated each action naturally without the experimenter's specification about movement trajectory or speed. For manipulating objects, left-handed subjects were free to use their left hands. Objects were located in various positions in the task space. It is, however, noted that the camera view angle was fixed because the problem of view invariance is the out of the scope in the current study. We conducted a set of simulation experiments with using the aforementioned video dataset. The first experiment examined the overall performance in categorization via learning with using the LOSOCV scheme by using the whole dataset. The second experiment examined categorization performance for pantomime actions for examining structural links acquired between action and object through learning. It is expected that such examination using slightly distorted visual patterns associated with pantomime actions can gain understanding of how much categorization can be performed with generalization among learned categories. In the current experiment, the MSTRNN model was designed to contain two types of softmax vector outputs, one for the action category (9 dimension) and the other for the directed object category (4 dimension) so that the model network can categorize both action and directed object in video scene perceived. For the input layer, it consists of three feature units (maps) for three RGB channels of the input image sequences. The map size of the feature units in the input layer was set to be 115x165. The exact parameters used in the model, especially in sequentially connected context layers, are described in the Table 4. In this table, the numbering of the context layers was taken from bottom (input layer) to top (output layer). For the first experiment using the LOSOCV scheme for examining the overall categorization accuracy, the model network was trained for 50 epochs. The learning rate of the model started from 0.01 and decayed by 2% for every epoch after the 1st epoch. The model was trained on the data of 9 subjects, and was tested with the data that was left-out from the training. Then, an average accuracy for all 15 object-directed action categories among all 10 subject cases was computed for every epoch during all training processes. From this computation, we could obtain the best epoch that provides the best average accuracy for all object-directed action categories. We also computed accuracy for each object-directed action category at the best epoch. The obtained categorization accuracies were rounded off to the first decimal place.\nTable 3. The human-object interaction dataset. The number indicates ID of each object-directed action category.\nAction Object Sweep Open Close Drink\nChange Page Type Shake Stir Blow\nBook 1 2 3 - 4 - - -\nLaptop 5 6 7 - - 8 - - -\nBottle - 9 10 11 - - 12 - -\nCup - - - 13 - - - 14 15\nFigure 4. The 15 categories of directed-object and action combination in the human-object interaction dataset."
    }, {
      "heading" : "B. Experimental results (1) Leave-one-subject-out classification results of all subjects.",
      "text" : "The overall categorization accuracy by MSTRNN for directed-object and action were 74.3%, 55.9%, respectively. The overall accuracy for the joint categorization between object category and action category was 47.9%. Although the MSTRNN model used in the experiment can categorize object-directed action patterns in the test data with generalization in some degree, the model exhibited certain amount of miscategorization (The result video is available at https://sites.google.com/site/haanvidlee/mstrnn-experiment-2-demo-video). Next, we examine what sorts of miscategorization took place. In order to examine the structure of the miscategorization among the joint categories between object and action, a matrix table, referred to the confusion matrix was built as shown in Table 5. In Table 5, names of objects and object-directed actions are abbreviated. Names of objects such as book, laptop, bottle, cup are abbreviated to BK, LT, BT, CP, respectively. Also, names of actions such as sweep, open, close, drink, change page, type, shake, stir, blow are abbreviated to SW, OP, CL, DR, CP, TY, SH, ST, BL, respectively. The confusion matrix shows probability distribution in all possible joint categorization outputs between objects and objectdirected actions. For example, if (actionX-objectY, actionX-objectY) in the matrix is 100%, this means that there is no miscategorization for the joint category for actionX-objectY. On the other hand, if probability of (actionX-objectY, actionX-objectY) is 60%, (actionX-objectZ, actionX-objectY) is 40%, and others are 0%, there is 40% probability for miscategorization of test image of actionX-objectY with joint category of actionX-objectZ. At the first glance, miscategorization can be observed most often among the joint categories those share same objects. For example, Open-Bottle is categorized correctly with 50% probability, miscategorized with Close-Bottle with 23% as the most possible miscategorization, and miscategorized with Shake-Bottle with 13% as the next most possible one. It is also frequently observed that miscategorization takes place among joint categories composed of same actions but different directed objects. For example, Drink-Cup is categorized correctly with 47% probability and miscategorized with Drink-Bottle with 22% as the largest miscategorization possibility. The aforementioned observation could suggest a hypothesis that a set of categorical memories for object-directed actions are organized in relational structure in a certain metric space where miscategorization may take more frequently between different categories of which distance is closer. Here, the problem is how to define distance between those categories. Here, we just define the distance hypothetically for the moment. First, distance between two joint categories is calculated by simply taking 2-dimensional bit hamming distance [20] for action category and object category. If either action category or object category is different between particular two joint categories, the distance becomes 1. However, when both of action category and object category are different between two joint categories, some consideration is required. In this situation, we may consider Bellman-Ford [21, 22] algorithm that defines distance between nodes in graph structure in terms of the minimum path length between them. For example, let us consider two joint categories of Sweep-Laptop and Open-Book. Although these two share neither action category nor object category, these two joint categories can be connected within two connection links through another joint category of Open-Laptop. Thus, it can be said that the distance between Sweep-Laptop and Open-Book is 2 because the minimum path length of connecting these two is 2. For another example, distance between Type-Laptop and Shake-Bottle can be computed as 3 because these two can be connected with 3 links in the minimum path, namely Type-Laptop Open-LaptopOpen-BottleShakeBottle. The maximum distance among the learned joint categories turned out to be 5. We\ndefined any distance between learned joint categories and unlearned irrational jointed categories, such as Drink-Book as 6. This comes from an assumption that those unlearned categories might be allocated in far distance from learned one in the memory structure of the trained model network. After defining distance between different joint categories in the aforementioned manner, we examined how probability of miscategorization with a particular category correlates with the distance between the true category and the miscategorized one. The result of computation with using all test data is plotted in a graph in Figure 5 in which x-axis represents the distance between true joint category and miscategorized one and y-axis represents its average probability for all test cases. It can be seen that there is a negative correlation between miscategorization probability and distance between true category and miscategorized one. The maximum distance among the learned joint categories was 5. We defined the distance between true joint categories and irrationally jointed categories, such as Drink-Book as 6. This is because those categories irrationally combined between unrelated actions and objects should have the lowest correlation. The probability for correct categorization with zero distance is the highest with 47.9%. And the probability of miscategorization with a category of which distance from the true category is more than 6 is the lowest with 0.2%. It can be said that the probability of miscategorization with a particular output category drops as its distance from the true category becomes larger. Next, we examine how structural links between actions and directed objects were developed in the model network through the learning. For this purpose, probability distribution of categorization outputs among different objects for testing with each combination of true action-directed objects and distractor objects averaged over all object-directed actions was calculated. Table 6 shows the results with the column for each condition of testing with true object and distractor object used and the row for the probability distribution among object categorization outputs. From this table, it can be seen that the probability for true object categorization output is the largest for all pair conditions with true objects and distractor objects used in the tests. It can be seen also that the probability for miscategorization with distractor objects is the second largest by following the one with true object in 7 out of 12 test conditions (the third largest in 5 out of 12 conditions.). These results indicate that the model network was successful in capturing the structural links between objects and actions directed on them, although there is a tendency that categorization outputs are distracted by distractor objects appeared along with the true action-directed objects in the tests in some degree.\n(2) Categorization of pantomime actions Finally, the results on test categorization for pantomime actions are described. The videos were shot with 6 trials for two categories of pantomime actions, Shake-Bottle and Drink-Cup without using the physical objects for all 10 subjects (see Figure 6). The categorization test was conducted by using the model network trained in the previous experiment. Table 7 shows the probability distribution among joint output categories for two test pantomime actions of Shake-Bottle and Drink-Cup (note that only joint categories of which probability of categorized is more than 0% are shown in the columns.) The table shows that the model categorized the joint categories of Shake-Bottle as Shake-Bottle correctly with probability of 72% and incorrectly as Drink-Bottle with probability of 13%. However, DrinkCup was often mis-categorized with Drink-Bottle with probability of 53% and correctly with probability of 30%. Although this accuracy seems relatively low at the first glance, It can be considered that this miscategorization took place because visual profiles of pantomime actions for Drink-Cup and Drink-Bottle are quite similar for which even human subjects can miscategorize. At the least, the model network could categorize the pantomime actions of Drink-Cup as an action category for Drink.\nThe test result was also analyzed to examine if the probability of miscategorization\ncorrelates with its distance from the true category in the same way with the previous experiment. The plot in Figure 7 shows correlation between probability for miscategorization and distance between the true category and the miscategorized one averaged over all pantomime test cases as similar to Figure 5. It can be seen that the miscategorization probability drops as the distance increases in similar to what was observed in the previous experiment. This result indicates that miscategorization of pantomime actions more likely takes place among similar or related joint categories."
    }, {
      "heading" : "4. Discussion and conclusion",
      "text" : "A newly proposed dynamic neural network model, MSTRNN has been built by modifying the MSTNN [1] model to include leaky integrator units that have recurrent connections in each level. In the first experiment in categorizing long-ranged video image of compositional human action sequences, the proposed model exhibited higher performance than the MSTNN model. It was speculated that the recurrent connectivity in the context units enhanced the capability of the model in extracting long-term correlation latent in the trained data. In the second experiment, MSTRNN was tested with a task of learning to categorize objectdirected action for the purpose of examining the cognitive capability of the model for capturing underlying spatio-temporal structures in linking visual image of objects and that of human actions directed to those objects. The overall categorization accuracy by MSTRNN for 4 object categories and 9 object-directed action categories were 74.3%, 55.9%, respectively. For investigating possible relational structures developed among different categories of objectdirected action of learned, characteristics of miscategorization during the test trials was examined. For this purpose, so-called the confusion matrix was built to obtain the probability distribution of miscategorization of test video image among all possible joint categories between action and objects. The result of the analysis showed that miscategorization was made most frequently among the joint categories those share same directed-objects but different actions. Miscategorization among the joint categories those share same actions but different objects appeared next most frequently. It was also observed that miscategorization with unlearned or irrational joint categories was rarely observed. By defining distance between two joint categories with using path length based on bit difference in action labels and object labels, it was shown that the probability distribution of miscategorization is negatively correlated with distance between the true category and the miscategorized one. This means that miscategorization is made more frequently between joint categories which are similar in terms of their labels in action category and object category. Our study examined also the effect of distractor objects which appeared along with actiondirected objects in the test trials. Our analysis showed that the probability of categorizing with true action-directed objects is the highest among other distractor objects appeared. It was, however shown that there is a tendency that categorization outputs are distracted to distractor objects used in the test trial in some extents. Furthermore, the current study investigated if the model can use the learned structural relationship between actions and their directed objects for categorizing pantomime actions without using physical objects. It was shown that the model can categorize pantomime actions correctly in some extents (Shake-Bottle was categorized correctly with 72% and Drink-Cup was miscategorized with Drink-Bottle with 53%.). The analysis on the probability distribution of miscategorization of pantomime actions showed similar negative correlation between the probability of miscategorization and distance with the one obtained in the test trials with object-directed actions using actual objects. These analytical results obtained so far confirm that the model network developed categorical memories for a set of trained object-directed actions with organizing relational structure among them as embedded in a certain metric space. Development of such relational structure in metric space is considered to be advantageous because it can allow generalization in categorization in some extents.. An essential question remained is what sort of metric space was developed in the categorical memories in the trained model network. Although we defined the distance between categories of different object-directed actions using labels associated with actions and objects by which it turned out that the miscategorization probability could negatively correlate with this defined distance in the first approximation, the adopted way of defining the distance cannot\nexclude its arbitrariness completely. It is highly desired that the distance measure could be attained from spatio-temporal analysis of neural activity associated with perception of each categorical patterns. Although our preliminary study examined various trials using analytical schemes such as principle component analysis (PCA) [23] or auto-encoder [24] scheme for this purpose, any reportable results have not been obtained yet. The future study should develop effective methods to deliberate the distance measure among spatio-temporal patterns in massive number of neural units for different categories. Currently, the best rate for the joint categorization in the second experiment is 47% which should be improved further in future study. One of the main reasons for the substantial amount of miscategorization might be due to overfitting. In future study, we may alleviate this problem by increasing the size of the dataset by perturbing existing training dataset. Increasing the size of the dataset can be done by randomly cropping certain size of frames of the existing dataset enough to recognize the action and both a directed-object and a distractor in the scene. Also, method of flipping the images of the dataset horizontally by 50% could be used to increase the size of the dataset [25]. It is also true that good performance depends on setting of parameters in the model. In the current study, it was found that the performance of the model depends crucially on setting of the time constant at each context layer. Since there is no analytical way to determine the optimal values for time constants, their adequate values had to be explored heuristically. Future study should investigate a scheme for self-adapting the time constants of the model. Finally, future study should investigate the possibility of gaining the categorization performance by implementing also the top-down prediction and attentional pathway added to the current bottom-up pathway, because it is generally assumed that the introduction of the topdown process can gain the robustness in categorization by providing the prior for future perceptual events."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2014R1A2A2A01005491)."
    } ],
    "references" : [ {
      "title" : "Self-Organization of Spatio-Temporal Hierarchy via Learning of Dynamic Visual Image Patterns on Action Sequences",
      "author" : [ "M Jung", "J Hwang", "J. Tani" ],
      "venue" : "PloS one. 2015 Jul 6;10(7):e0131214",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y LeCun", "L Bottou", "Y Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1998
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J Deng", "W Dong", "R Socher", "LJ Li", "K Li", "L. Fei-Fei" ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition;",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C Szegedy", "W Liu", "Y Jia", "P Sermanet", "S Reed", "D Anguelov", "D Erhan", "V Vanhoucke", "A. Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "O Russakovsky", "J Deng", "H Su", "J Krause", "S Satheesh", "S Ma", "Z Huang", "A Karpathy", "A Khosla", "M Bernstein", "AC Berg", "L. Fei-Fei" ],
      "venue" : "International Journal of Computer Vision",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "3D convolutional neural networks for human action recognition",
      "author" : [ "S Ji", "W Xu", "M Yang", "K. Yu" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild",
      "author" : [ "K Soomro", "AR Zamir", "M. Shah" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "HMDB: a large video database for human motion recognition",
      "author" : [ "Kuehne H", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre" ],
      "venue" : "In Proceedings of the International Conference on Computer Vision (ICCV)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Sequential Deep Learning for Human Action Recognition",
      "author" : [ "M Baccouche", "F Mamalet", "C Wolf", "C Garcia", "A. Baskurt" ],
      "venue" : "Human Behavior Understanding",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Learning precise timing with LSTM recurrent networks",
      "author" : [ "FA Gers", "NN Schraudolph", "J. Schmidhuber" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Sequence to Sequence - Video to Text",
      "author" : [ "S Venugopalan", "M Rohrbach", "J Donahue", "R Mooney", "T Darrell", "K. Saenko" ],
      "venue" : "CoRR. 2015;abs/1505.00487",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Perceptual structures and distributed motor control: Bethesda, MD: American Physiological Society.",
      "author" : [ "Arbib MA" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1981
    }, {
      "title" : "Self-Organization and Compositionality in Cognitive Brains: A Neurorobotics Study",
      "author" : [ "J. Tani" ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment",
      "author" : [ "Y Yamashita", "J. Tani" ],
      "venue" : "PLoS Comput Biol",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Elman JL" ],
      "venue" : "Cognitive Science",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1990
    }, {
      "title" : "Attractor dynamics and parallelism in a connectionist sequential machine",
      "author" : [ "Jordan MI" ],
      "venue" : "Artificial neural networks: IEEE Press;",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1990
    }, {
      "title" : "Flexible, high performance convolutional neural networks for image classification",
      "author" : [ "DC Ciresan", "U Meier", "J Masci", "LM Gambardella", "J. Schmidhuber" ],
      "venue" : "Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Volume Two;",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Parallel distributed processing: MIT",
      "author" : [ "DE Rumelhart", "JL McClelland", "PR. Group" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1986
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N Srivastava", "G Hinton", "A Krizhevsky", "I Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1929
    }, {
      "title" : "Error detecting and error correcting codes",
      "author" : [ "Hamming RW" ],
      "venue" : "Bell System Tech J. 1950;29:147--60",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1950
    }, {
      "title" : "On a routing problem",
      "author" : [ "R. Bellman" ],
      "venue" : "Quarterly of Applied Mathematics",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1958
    }, {
      "title" : "Analysis of a Complex of Statistical Variables into Principal Components",
      "author" : [ "H. Hotelling" ],
      "venue" : "Journal of Educational Psychology",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1933
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Hinton GE", "Salakhutdinov RR" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2006
    }, {
      "title" : "Large-scale video classification with convolutional neural networks",
      "author" : [ "A Karpathy", "G Toderici", "S Shetty", "T Leung", "R Sukthankar", "L. Fei-Fei" ],
      "venue" : "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The MSTRNN has been developed by newly introducing recurrent connectivity to a priorproposed model, multiple spatio-temporal scales neural network (MSTNN) [1] such that the model can learn to extract latent spatio-temporal structures more effectively by developing adequate recurrent contextual dynamics.",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "Especially, a convolutional neural network (CNN) [2], which has been developed as inspired by the mammalian visual cortex for its spatial hierarchical processing of visual features, has shown remarkably better recognition performance for static natural visual images compared to conventional vision recognition schemes which used elaborately hand-coded visual features.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Actually, recent CNN trained with 1 million of visual image in ImageNet [3] can classify hundreds of object image with error rate of 0.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "0665 [4].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : "It is considered that the CNN’s performance in this task is close to that of human [5].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "A typical approach to the problem has been to use a 3D convolutional neural network (3D CNN) [6] in which dynamic visual image patterns can be recognized by simply transforming a sequence of 2-dimensional visual spatial patterns within a fixed temporal window into a large 3-dimensional pattern.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "Actually, 3D CNN showed good performances on many challenging video recognition public datasets related to human action categories such as UCF-101 [7] and HMDB-51 [8] by extracting short-range temporal correlations in the temporal window.",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "Actually, 3D CNN showed good performances on many challenging video recognition public datasets related to human action categories such as UCF-101 [7] and HMDB-51 [8] by extracting short-range temporal correlations in the temporal window.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : "[9] has proposed a two-stage model to maintain temporal information in the entire sequence by adding a long short-term memory (LSTM) network [10] as a second stage of the 3D CNN.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[9] has proposed a two-stage model to maintain temporal information in the entire sequence by adding a long short-term memory (LSTM) network [10] as a second stage of the 3D CNN.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : "Similarly, Venugopalan and colleagues [11] proposed an architecture composed of a CNN for video processing and an LSTM concatenated downstream of the CNN for the generation of corresponding word sequences that was trained by using nearly a hundred thousand video clips with annotated descriptive sentences.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "Such spatio-temporal structures should be involved with compositionality in human action generation [12, 13].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "Such spatio-temporal structures should be involved with compositionality in human action generation [12, 13].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "The temporal compositionality can be accounted by the fact that most of goal-directed human actions are composed by sequential combinations of commonly used behavior primitives [12].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 0,
      "context" : "In the prior study, a dynamic neural network model, referred to Multiple SpatioTemporal Neural Network (MSTNN) [1] was developed for the purpose of automatic categorization of video image patterns based on learning.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "MSTNN was developed by combining two prior existing models of Convolutional Neural Network (CNN) [2] and Multiple Timescales Recurrent Neural Network (MTRNN) [14].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "MSTNN was developed by combining two prior existing models of Convolutional Neural Network (CNN) [2] and Multiple Timescales Recurrent Neural Network (MTRNN) [14].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 14,
      "context" : "It has been shown that context units with recurrent connectivity play important roles in extracting latent temporal structures from exemplar temporal sequences [15, 16].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 15,
      "context" : "It has been shown that context units with recurrent connectivity play important roles in extracting latent temporal structures from exemplar temporal sequences [15, 16].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 16,
      "context" : "A context layer consists of feature units used in the MSTNN model, and our newly designed context units, and pooling units [17] if the layer does the pooling operation (see Figure 1 (b), (c)).",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "This is because the functionality of the max-pooling units lies not in capturing temporal structures, but in capturing the translation invariant features of the images and reducing the feature size [17].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 0,
      "context" : "Categorization in the model was performed in the delay response manner [1].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "3 Training The training was conducted by a supervised manner using the delay response scheme [1].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "The error obtained by the Equation 6 for an input video was used for optimizing a set of learnable parameters by using back propagated through time (BPTT) [18].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 18,
      "context" : "0005 and fullyconnected layer was trained with 50% dropout rate [19] to prevent the overfitting.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "The first experiment task examined the capability of MSTRNN in learning longer sequences of human movement patterns as compared to the capability of MSTNN [1] in the same task.",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "First, distance between two joint categories is calculated by simply taking 2-dimensional bit hamming distance [20] for action category and object category.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "In this situation, we may consider Bellman-Ford [21, 22] algorithm that defines distance between nodes in graph structure in terms of the minimum path length between them.",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "Discussion and conclusion A newly proposed dynamic neural network model, MSTRNN has been built by modifying the MSTNN [1] model to include leaky integrator units that have recurrent connections in each level.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "Although our preliminary study examined various trials using analytical schemes such as principle component analysis (PCA) [23] or auto-encoder [24] scheme for this purpose, any reportable results have not been obtained yet.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "Although our preliminary study examined various trials using analytical schemes such as principle component analysis (PCA) [23] or auto-encoder [24] scheme for this purpose, any reportable results have not been obtained yet.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 23,
      "context" : "Also, method of flipping the images of the dataset horizontally by 50% could be used to increase the size of the dataset [25].",
      "startOffset" : 121,
      "endOffset" : 125
    } ],
    "year" : 2016,
    "abstractText" : "The current paper proposes a novel dynamic neural network model, multiple spatio-temporal scales recurrent neural network (MSTRNN) used for categorization of complex human action pattern in video image. The MSTRNN has been developed by newly introducing recurrent connectivity to a priorproposed model, multiple spatio-temporal scales neural network (MSTNN) [1] such that the model can learn to extract latent spatio-temporal structures more effectively by developing adequate recurrent contextual dynamics. The MSTRNN was evaluated by conducting a set of simulation experiments on learning to categorize human action visual patterns. The first experiment on categorizing a set of long-concatenated human movement patterns showed that MSTRNN outperforms MSTNN in the capability of learning to extract long-ranged correlation in video image. The second experiment on categorizing a set of objectdirected actions showed that the MSTRNN can learn to extract structural relationship between actions and directed-objects. Our analysis on the characteristics of miscategorization in both cases of object-directed action and pantomime actions indicated that the model network developed the categorical memories by organizing relational structure among them. Development of such relational structure is considered to be beneficial for gaining generalization in categorization.",
    "creator" : "Acrobat PDFMaker 11 for Word"
  }
}