{
  "name" : "1308.6342.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Linear and Parallel Learning of Markov Random Fields",
    "authors" : [ "Yariv Dror Mizrahi", "Misha Denil", "Nando de Freitas" ],
    "emails" : [ "yariv@math.ubc.ca", "misha.denil@cs.ox.ac.uk", "nando@cs.ox.ac.uk" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Markov Random Fields (MRFs), also known as undirected probabilistic graphical models, are ubiquitous structured probability models that have significantly impacted a large number of fields, including computer vision (Li, 2001; Szeliski et al., 2008), computational photography and graphics (Agarwala et al., 2004), computational neuroscience (Ackley et al., 1985), bioinformatics (Yanover et al., 2007), sensor networks (Liu & Ihler, 2012), social networks (Strauss & Ikeda, 1990), Markov logic (Richardson & Domingos, 2006), natural language processing (Lafferty et al., 2001; Sutton & McCallum, 2012) and statistical physics (Kindermann & Snell, 1980). As pointed out in Wainwright & Jordan (2008) there are also many applications in statistics, constraint satisfaction and combinatorial optimization, error-correcting codes and epidemiology. Not surprisingly, many comprehensive treatments of this important topic have appeared in the last four decades (Kindermann & Snell, 1980; Lauritzen, 1996;\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nBremaud, 2001; Koller & Friedman, 2009; Murphy, 2012).\nDespite the great success and impact of these models, fitting them to data remains a formidable challenge. Although the log-likelihood is typically convex in the parameters, the gradient of these models is intractable.\nIn many cases, maximum likelihood in these models is data efficient in the sense that the data term in the gradient can be easily precomputed, making its evaluation trivial during optimization. The main difficulty with maximum likelihood is that it is not model efficient since evaluating the gradient involves computing expectations over the model distribution. This requires evaluating a sum with exponentially many terms, which is intractable for even moderately sized models. The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyvärinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).\nAn important class of approximate method for this problem are stochastic approximation methods, which approximate the model term by drawing samples from the model distribution, typically via MCMC. This simulation is costly and often many samples are required for accurate estimation. Moreover, in settings where the parameters or data must be distributed across many machines such simulation poses additional difficulties.\nAnother approach is to approximate the maximum likelihood objective with a factored alternative. The leading method in this area is pseudo-likelihood. In this approach the joint distribution over all variables in the MRF is replaced by a product of conditional distributions for each variable. Replacing the joint distribution with a product of conditionals eliminates the model term from the gradient of the pseudo-likelihood\nar X\niv :1\n30 8.\n63 42\nv4 [\nst at\n.M L\n] 5\nF eb\n2 01\nobjective, which circumvents the model inefficiency of maximum likelihood estimation. However, pseudolikelihood is not data efficient, since the conditional distributions often depend on the actual data and the current value of the parameters. We return to this issue in more detail in Section 2.3.\nApplying pseudo likelihood in a distributed setting is also difficult, because the conditional distributions share parameters. Several researchers have addressed this issue by proposing to approximate pseudolikelihood by disjointly optimizing each conditional and combining the parameters using some form of averaging (Ravikumar et al., 2010; Wiesel & Hero III, 2012; Liu & Ihler, 2012).\nIn this paper we introduce a new approach to parameter estimation in MRFs with untied parameters, which avoids the model inefficiency of maximum likelihood for an important class of models while preserving its data efficiency. Moreover, our algorithm is embarrassingly parallel and can be implemented in a distributed setting without modification. Our algorithm replaces the joint maximum likelihood problem with a collection of much smaller auxiliary maximum likelihood problems which can be solved independently.\nWe prove that if the auxiliary problems satisfy certain conditions, the relevant parameters in the auxiliary problems converge to the values of the true parameters in the joint model. Our experiments show that good performance is achieved in this case and that good performance is still achieved when these conditions are not satisfied. Violating the conditions for convergence sacrifices theoretical guarantees in exchange for even further computational savings while maintaining good empirical performance.\nUnder a strong assumption (which is generally not satisfied in practice) we prove that our algorithm is exactly equal to maximum likelihood on the full joint distribution. While not directly applicable, this result provides additional insight into why our approach is effective.\nA similar method was recently, and independently, introduced in the context of Gaussian graphical models by Meng et al. (2013). In that paper, the authors consider local neighborhoods of nodes, whereas we consider neighborhoods of cliques, and they rely on a convex relaxation via the Schur complement to derive their algorithm for inverse covariance estimation. At the time of revising this paper, the same authors have shown that the convergence rate to the true parameters with their method is comparable to centralized maximum likelihood estimation (Meng et al., 2014).\nAlthough our work and that of Meng arrive at distributed learning algorithms via different paths, and while Meng et al. consider only Gaussian graphical models, it is clear that both works show that it is possible to capitalize on graph structures beyond low tree width to design algorithms that are both data and model efficient and exhibit good empirical performance."
    }, {
      "heading" : "2. Model Specification and Learning Objectives",
      "text" : "We are interested in estimating the parameter vector θ of a positive distribution p(x |θ) > 0 that satisfies the Markov properties of an undirected graph G. That is, a distribution that can be represented as a product of factors, one per maximal clique,\np(x |θ) = 1 Z(θ) ∏ c∈C ψc(xc |θc), (1)\nwhere C is the set of maximal cliques of G, ψc(xc |θc) ≥ 0 is the potential function or factor associated with the variables in clique c, and Z(θ) is the partition function given by\nZ(θ) = ∑ x ∏ c∈C ψc(xc |θc). (2)\nIn such models we often use exponential functions to represent the potentials,\nψc(xc |θc) = exp(−E(xc |θc)), (3)\nwhere E(xc |θc) ∈ R is called the energy , which we will assume is chosen so that the parameters are identifiable. The resulting joint distribution can then be written as a Gibbs distribution\np(x |θ) = 1 Z(θ) exp(− ∑ c E(xc |θc)).\nWhen the energy is a linear function of the parameters, i.e. E(xc |θc) = −θTc φc(xc) where φc(xc) is a feature vector derived from the values of the variables xc, we have a maximum entropy or log-linear model (Wasserman, 2004; Buchman et al., 2012; Murphy, 2012). The features in these models are also referred to as local sufficient statistics."
    }, {
      "heading" : "2.1. Maximum Likelihood",
      "text" : "There is (in general) no closed form solution for the ML estimate of the parameters of an MRF, so gradientbased optimizers are needed.\nConsider the fully-observed maximum entropy model\np(x |θ) = 1 Z(θ) exp( ∑ c θTc φc(x)) (4)\nwhere c indexes the maximal cliques. The scaled loglikelihood is given by\n`(θ) = 1\nN N∑ n=1 log p(xn |θ)\n= 1\nN N∑ n=1 [∑ c θTc φc(xn)− logZ(θ) ]\nwhich is a convex function of θ.\nThe derivative for the parameters of a particular clique, q, is given by\n∂`\n∂θq =\n1\nN N∑ n=1 [ φq(xn)− ∂ ∂θq logZ(θ) ] , (5)\nwhere\n∂\n∂θq logZ(θ) = E\n[ φq(x) |θ ] = ∑ x φq(x)p(x |θ) .\n(6)\nEquation 6 is the expectation of the feature φq(x) over the model distribution. For many models of interest this quantity is intractable.\nThe full derivative of the log-likelihood contrasts the model expectation against the expected value of the feature over the data,\n∂`\n∂θq =\n1\nN N∑ n=1 φq(xn)− E [ φq(x) |θ ] . (7)\nAt the optimum these two terms will be equal and the empirical distribution of the features will match the model predictions."
    }, {
      "heading" : "2.2. Maximum Pseudo-Likelihood",
      "text" : "To surmount the intractable problem of computing expectations over the model distribution, pseudolikelihood considers a simpler factorised objective function,\n`PL(θ) = 1\nN N∑ n=1 M∑ m=1 log p(xmn |x−mn,θ) (8)\nwhere x−mn denotes all the components of the n-th data vector, except for component m. (For models with sparse connectivity, we only need to condition on\nthe neighbors of node m.) In the binary, log-linear case, the gradient of this objective can be expressed in contrastive form,\n∂`PL\n∂θq =\n1\nN ∑ n,m p(x̄mmn |x−mn,θ) [ φq(xn)− φq(x̄mn ) ] ,\nwhere x̄mn is the data vector x̄n with the m-th bit flipped. That is, x̄imn = 1 − xmn if i = m and xmn otherwise (Marlin et al., 2010)."
    }, {
      "heading" : "2.3. Model and Data Efficiency",
      "text" : "There are two terms in the gradient of Equation 7. The first term is an empirical expectation, 1N ∑N n=1 φq(xn), and depends only on the data. The value of this term for each clique can be pre-computed before parameter optimization begins, making this term of the gradient extremely cheap to evaluate during optimization.\nThe data term in the maximum likelihood gradient is contrasted with an expectation over the model distribution, E [ φq(x) |θ ] , which is a sum over exponentially many configurations. For large models this term is intractable.\nWe describe this situation by saying that maximum likelihood estimation is data efficient, since the terms involving only the data can be computed efficiently. However, maximum likelihood is not model efficient, since the model term in the gradient is intractable, and the difficulty in evaluating it is the primary motivation for the development of alternative objectives like pseudo-likelihood.\nPseudo-likelihood addresses the model inefficiency of maximum likelihood by eliminating the model term from the gradient, which makes pseudo-likelihood model efficient. However, pseudo-likelihood is not data efficient, since computing the gradient requires access to the full conditional distributions p(x̄mmn |x−mn,θ). Because of this the outer sum over data examples must be computed for each gradient evaluation. (Note that for binary models the full conditionals correspond to logistic regressions, so any advances in scaling logistic regression to massive models and datasets would be of use here.)\nIn the following section we introduce the LAP algorithm (which stands for Linear and Parallel), which uses a particular decomposition of the graph to avoid the exponential cost in maximum likelihood, but unlike pseudo-likelihood our algorithm is fully parallel and maintains the data efficiency of maximum likelihood estimation. Our algorithm divides the full parameter estimation process into several fully independent sub-problems which can be solved in parallel.\nAlgorithm 1 LAP\nInput: MRF with maximum cliques C for q ∈ C do\nConstruct auxiliary MRF Mq on variables in Aq Estimate parameters θMq in Mq using ML Set θq ← θMqq\nend for\nOnce each sub-problem has been solved the solutions of the sub-problems are combined to give a solution to the full problem."
    }, {
      "heading" : "3. Algorithm Description",
      "text" : "The LAP algorithm operates by splitting the joint parameter estimation problem into several independent sub-problems which can be solved in parallel. Once the sub-problems have been solved, it combines the solutions to each sub-problem together into a solution to the full problem.\nFor a fixed clique q we define its 1-neighborhood Aq = ⋃\nc∩q 6=∅\nc\nto contain all of the variables of q itself as well as the variables with at least one neighbor in q.\nLAP creates one sub-problem for each maximal clique in the original problem by defining an auxiliary MRF, Mq, over the variables in Aq. Details on how to construct the auxiliary MRF will be discussed later, for now we assume we have an auxiliary MRF on Aq and that it contains a clique over the variables in q that is parametrized the same way as q in the original problem.\nLAP derives the parameter vector θq for the full problem by estimating parameters in the auxiliary MRF on Aq using maximum likelihood and reading off the parameters for the clique q directly. The steps of the algorithm are summarized in Algorithm 1.\nIn a log-linear model, when estimating the vector of parameters θMq of the auxiliary MRF by maximum likelihood, the relevant derivative is\n∂`Mq ∂θMqq = 1 N N∑ n=1 φq(xAqn)− E [ φq(xAq )|θ Mq ] .\nThis approach is data efficient, since the sufficient statistics 1N ∑N n=1 φq(xAqn) can be easily precomputed. Moreover, the data vector xn can be stored in a distributed fashion, with the node estimating the MRFMq only needing access to the sub-vector xAqn.\n(a)\nIn addition, LAP is model efficient since the expectation E [ φq(xAq )|θ Mq ] can be easily computed when the number of variables in Aq is small. To illustrate this point, consider the models shown in Figure 1. For dense graphs, such as the restricted Boltzmann machine, the exponential cost of enumerating over all the variables in Aq is prohibitive. However, for other practical MRFs of interest, including lattices and Chimeras\n(Denil & de Freitas, 2011), this cost is perfectly acceptable."
    }, {
      "heading" : "3.1. Construction of the Auxiliary MRF",
      "text" : "The effectiveness of LAP comes from proper construction of the auxiliary MRF, Mq. As already mentioned, Mq must contain the clique q, which must be parametrized in the same way as in the joint model. This requirement is clear from the previous section, otherwise the final step in Algorithm 1 would be invalid.\nWe will see in the analysis section that it is desirable for Mq to be as close to the marginal distribution on xAq as possible. This means we must include all cliques from the original MRF which are subsets of Aq. Additionally, marginalization may introduce additional cliques not present in the original joint distribution. It is clear that these cliques can only involve variables in Aq \\ q, but determining their exact structure in general can be difficult.\nWe consider three strategies for constructing auxiliary MRFs, which are distinguished by how they induce clique structures on Aq \\ q. The three strategies are as follows.\nExact Here we compute the exact structure of the marginal distribution over Aq from the original problem. We have chosen our test models to be ones where the marginal structure is readily computed.\nDense For many classes of model the marginal over Aq involves a fully parametrized clique over Aq \\ q for nearly every choice of q (for example, this is the case in lattice models). The dense variant assumes that the marginal always has this structure. Making this choice will sometimes over-parametrize the marginal, but avoids the requirement of explicitly computing its structure.\nPairwise Both the exact and dense strategies create high order terms in the auxiliary MRF. While high order terms do exist in the marginals of discrete MRFs, it is computationally inconvenient to include them, since the add many parameters to each sub-problem. In the pairwise variant we use the same graph structure as in dense, but here we introduce only unary and binary potentials over Aq \\ q. This results in a significant computational savings for each sub-problem in LAP, but fails to capture the true marginal distribution in many cases (including all of the example problems we consider)."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section we describe some experiments designed to show that the LAP estimator has good empirical performance. We focus on small models where exact maximum likelihood is tractable in order to allow performance to be measured. We chose to focus our experiments on demonstrating accuracy rather than scalability since the scaling and data efficiency properties of LAP are obvious.\nThe purpose of the experiments in this section is to show two things:\n1. The accuracy of LAP estimates is not worse than its main competitor, pseudo-likelihood; and\n2. LAP achieves good performance even when the exact marginal structure is not used.\nIn all of our experiments we compare pseudo-likelihood estimation against LAP using the three different strategies for constructing the auxiliary MRF discussed in the previous section. In each plot, lines labeled PL correspond to pseudo-likelihood and ML corresponds to maximum likelihood. LAP E, LAP D and LAP P refer respectively to LAP with the exact, dense and pairwise strategies for constructing the auxiliary MRF.\nWe compare LAP and pseudo-likelihood to maximum likelihood estimation on three different model classes. The first is a 4 × 4 Ising grids with 4-neighborhoods, and the results are shown in Figure 2. The second is a 4× 4× 4 Ising lattice with 6-neighborhoods, which is shown in Figure 3. Finally, we also consider a Chimera 3× 3× 3 model, with results shown in Figure 4.\nThe procedure for all models is the same: we choose the generating parameters uniformly at random from the interval [−1, 1] and draw samples approximately\nfrom the model. We then fit exact maximum likelihood parameters based on these samples, and compare the parameters obtained by pseudo-likelihood and LAP to the maximum likelihood estimates. The left plot in each figure shows the mean relative error of the parameter estimates using the maximum likelihood estimates as ground truth. Specifically, we measure\nerr(θ) = ‖θML‖−1 · ‖θ − θML‖\nfor each estimate on each set of samples and average over several runs.\nWe also measure the variance of the estimates produced by each algorithm over several runs. In this case we measure the variance of the estimates of each parameter separately and average these variances over all parameters in the model. These measurements are shown in the right plot in each figure. For reference we also show the variance of the maximum likelihood estimates in these plots.\nIn all of the experiments we see that the performance of all of the LAP variants is basically indistinguishable from pseudo-likelihood, except for small numbers of samples. Interestingly, LAP P does not perform noticeably worse than the other LAP variants on any of the problems we considered here. This is interesting because LAP P approximates the marginal with a pairwise MRF, which is not sufficient to capture the true marginal structure in any of our examples. LAP P is also the most efficient LAP variant we tested, since the auxiliary MRFs it uses have the fewest number of parameters."
    }, {
      "heading" : "5. Theory",
      "text" : "In this section show that matching parameters in the joint and the marginal distributions is valid, provided the parameterizations are chosen correctly. We then prove consistency of the LAP algorithm and illustrate\nits connection to maximum likelihood.\nUndirected probabilistic graphical models can be specified, locally, in terms of Markov properties and conditional independence and, globally, in terms of an energy function ∑ cE(xc|θc). The Hammersley-Clifford theorem (Hammersley & Clifford, 1971) establishes the equivalence of these two representations.\nOne important fact that is often omitted is that the energy function and the partition function are not unique. It is however possible to obtain uniqueness, for both of these functions, by imposing normalization with respect to a setting of the random variables of the potential. This gives rise to the concept of normalized potential (Bremaud, 2001):\nDefinition 1. A Gibbs potential E(xc|θc) is said to be normalized with respect to zero if E(xc|θc) = 0 whenever there exists t ∈ c such that xt = 0.\n(In this section, we use the term Gibbs potential, or simply potential, to refer to the energy so as to match the nomenclature of (Bremaud, 2001).) The following theorem plays a central role in understanding the LAP algorithm. The proof can be found in (Griffeath, 1976; Bremaud, 2001):\nTheorem 2. [Existence and Uniqueness of the normalized potential] There exists one and only one (Gibbs) potential normalized with respect to zero corresponding to a Gibbs distribution.\nEarlier in this paper we used x with no subscript to refer to the vector of all variables in the MRF since there was no risk of confusion. In this section we increase the precision in our notation by using S to denote the set of all variables and use xS instead of x for the vector of all variables in the MRF."
    }, {
      "heading" : "5.1. The LAP Argument",
      "text" : "Suppose we have a Gibbs distribution p(xS |θ) which factors according to the clique system C, and let q ∈ C be a particular clique of interest. Let\np(xAq |φ) = 1\nZ(φ) exp(− ∑ c∈Cq E(xc |φc))\nbe the marginal distribution on Aq (with clique system Cq) parametrized so that the potentials are normalized with respect to zero.\nWe can also write the marginal in the following way p(xAq |θ) = ∑ S\\Aq p(xS |θ)\n= 1\nZ(θ) ∑ S\\Aq exp(− ∑ c∈C E(xc |θc))\n= 1\nZ(θ) exp(−E(xq |θq)− ∑ c∈Cq\\{q} E(xc |θS\\q))\nIf the parametrization of p(xS |θ) is also chosen to be normalized with respect to zero then the potentials of these two representations of the marginal must be equal. Theorem 2 also tells us that the partition functions must be equal. In particular we see that\nE(xq |φq) = E(xq |θq)\nwhich implies that θq = φq if the parameters are identifiable."
    }, {
      "heading" : "5.2. Consistency of LAP",
      "text" : "Let θ be the true vector of parameters taken from the unknown generating distribution p(xS |θ) parametrized such that the potentials are normalized with respect to zero. Suppose we have N samples drawn iid from this distribution. Let θ̂ ML\nbe the ML parameters for θ given the data and let θ̂ LAP be the corresponding LAP estimate. We claim that θ̂ LAP\n→ θ as N →∞, provided the true marginal distributions are contained in the class of auxiliary MRFs.\nProof. Let q ∈ C be an arbitrary clique of interest. It is sufficient to show that θ̂ LAP\nq → θq. If φ is the true parameter of the marginal over xAq in normalized form, i.e.\np(xAq |φ) = ∑ S\\Aq p(xS |θ) ,\nthen it is known that\nφ̂ ML → φ\nsince maximum likelihood in consistent under smoothness and identifiability assumptions (for example, see Fienberg & Rinaldo (2012)). From the LAP argument we see that φq = θq so φ̂ ML q → θq."
    }, {
      "heading" : "5.3. Relationship to maximum likelihood",
      "text" : "Here we prove that, under certain (strong) assumptions, LAP is exactly equal to maximum likelihood. The main result here will be that under the required assumptions estimation by maximum likelihood and marginalization commute.\nSuppose we have a discrete MRF on xS which factorizes according to the cliques C, and let q ∈ C be a particular clique of interest.\nWe will make use of the following characterization of maximum likelihood estimates, which is proved in (Jordan, 2002).\nLemma 3. If a distribution p̂(xS) satisfies that for each c ∈ C\np̂(xc) = p̃(xc)\nthen p̂(xS) is a maximum likelihood estimate for the empirical distribution p̃(xS).\nThis characterization allows us to derive an explicit expression for a maximum likelihood estimate of p̂(xS).\nProposition 4. The distribution\np̂(xS) = p̃(xAq )p̃(xS\\q)\np̃(xAq\\q)\nis a maximum likelihood estimate for p̃(xS).\nProof. To see this we compute∑ q p̂(xS) = ∑ q p̃(xAq )p̃(xS\\q) p̃(xAq\\q) = p̃(xS\\q)\nand ∑ S\\Aq p̂(xS) = ∑ S\\Aq p̃(xAq )p̃(xS\\q) p̃(xAq\\q) = p̃(xAq )\nFor an arbitrary clique c ∈ C, either c ⊂ S \\ q or c ⊂ Aq, and we see that f̂(xc) = f̃(xc) by further marginalizing one of the above expressions. This shows that the expression we have given for p̂(xS) satisfies the criteria of Lemma 3, and is therefore a maximum likelihood estimate for p̃(xS).\nSuppose we have a family of distributions F on xS which satisfy the Markov properties of the MRF, and\nsuppose that p̂(xS) ∈ F where p̂(xS) is defined as in Proposition 4.\nDefine the auxiliary family Fq associated with the clique q as follows.\nFq = { ∑ S\\Aq p(xS) | p(xS) ∈ F}\nThat is, Fq is the family of distributions obtained by marginalizing the family F over S \\Aq. Proposition 5. The auxiliary family Fq contains the marginal empirical distribution p̃(xAq ). Moreover p̂(xAq ) = p̃(xAq ) is a maximum likelihood estimate for p̃(xAq ) in Fq.\nProof. Recall that p̂(xS) from Proposition 4 is in F by assumption. Thus,∑\nS\\Aq\np̂(xS) = p̃(xAq )\nis in Fq by definition. That p̂(xAq ) ∈ Fq is a maximum likelihood estimate follows since the log likelihood gradient in Equation 7 is zero when the model and empirical distributions are equal.\nSuppose we can represent the family F as a Gibbs family, i.e.\nF = F(Θ) = {p(xS |θ) |θ ∈ Θ}\nfor some domain of parameters Θ, where\np(xS |θ) = 1\nZ(θ) exp(− ∑ c∈C E(xc |θc)) .\nMoreover, suppose we have chosen this parameterization so that the potential functions are normalized with respect to zero.\nSince F is representable as a Gibbs family then the auxiliary family Fq is also representable as a Gibbs family with\nFq = Fq(Φ) = {p(xAq |φ) |φ ∈ Φ}\nfor some domain of parameters Φ. We will again suppose that this parameterization is chosen so that the potential functions are normalized with respect to zero.\nWe have already shown that maximum likelihood estimates for p̃(xS) and p̃(xAq ) exist in the families F and Fq, respectively. Since we have chosen the parameterizations of these families to be normalized we also have unique maximum likelihood parameters θ̂ ∈ Θ\nand φ̂ ∈ Φ such that p(xS | θ̂) ∈ F(Θ) is a maximum likelihood estimate for p̃(xS) and p(xAq | φ̂) ∈ F(Φ) is a maximum likelihood estimate for p̃(xAq ).\nWe can now prove the main result of this section.\nTheorem 6. Under the assumptions used in this section, estimating the joint parameters by maximum likelihood and integrating the resulting maximum likelihood distribution gives the same result as integrating the joint family of distributions and performing maximum likelihood estimation in the marginal family. Concisely,∑\nS\\Aq\np(xS | θ̂) = p(xAq | φ̂)\nProof. We have the following sequence of equalities:\np(xS | θ̂) (1) = p̂(xS) (2) = p̃(xAq )p̃(xS\\q)\np̃(xAq\\q)\n(3) = p̂(xAq )p̃(xS\\q)\np̃(xAq\\q)\n(4) = p(xAq | φ̂)p̃(xS\\q)\np̃(xAq\\q)\nThe first equality follows from the parameterization of F , the second follows from Proposition 4, the third from Proposition 5 and the fourth follows from the parameterization of Fq. The theorem is proved by summing both sides of the equality over S \\Aq.\nApplying the LAP argument to Theorem 6 we see that θ̂q = φ̂q.\nRemark The assumption that p̂(xS) ∈ F amounts to assuming that the empirical distribution of the data factors according to the MRF. This is very unlikely to hold in practice for finite data. However, if the true model structure is known then this property does hold in the limit of infinite data."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have presented a distributed learning algorithm for practical MRFs, where the parameters of each clique can be estimated in different machines. The algorithm is also data efficient in log-linear models, since the estimation of each clique parameter only requires access to local sufficient statistics of the data. Not only are the statistics local to the 1-neighborhoods of each clique, but they can also be precomputed.\nOur experiments indicate that the LAP estimators behave similarly to pseudo-likelihood and maximum likelihood for large sample sizes. However, these alternative estimators do not enjoy the same data and model\nefficiencies as LAP. Finally, we proved that the proposed estimator is consistent.\nThe work of Meng and colleagues only considered Gaussian graphical models probably as a result of their linear algebra relaxation techniques. However, it seems feasible to apply their algorithm to the discrete case. A comparison of both techniques is an immediate direction for future work. Combining the different proof techniques of Meng and ours is also of interest. A further addition to the theory would be the derivation of PAC bounds to improve our understanding of the sampling complexity of these estimators.\nThis works opens up many directions for future work, including model selection, latent variables, and tied parameters. A distributed implementation on Apache Spark/Hadoop is one of our near goals."
    } ],
    "references" : [ {
      "title" : "A learning algorithm for Boltzmann machines",
      "author" : [ "D.H. Ackley", "G. Hinton", "Sejnowski" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "Ackley et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Ackley et al\\.",
      "year" : 1985
    }, {
      "title" : "Interactive digital photomontage",
      "author" : [ "A. Agarwala", "M. Dontcheva", "M. Agrawala", "S. Drucker", "A. Colburn", "B. Curless", "D. Salesin", "M. Cohen" ],
      "venue" : "In ACM SIGGRAPH,",
      "citeRegEx" : "Agarwala et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Agarwala et al\\.",
      "year" : 2004
    }, {
      "title" : "Statistical analysis of non-lattice data",
      "author" : [ "J. Besag" ],
      "venue" : "Journal of the Royal Statistical Society. Series D,",
      "citeRegEx" : "Besag,? \\Q1975\\E",
      "shortCiteRegEx" : "Besag",
      "year" : 1975
    }, {
      "title" : "On sparse, spectral and other parameterizations of binary probabilistic models",
      "author" : [ "D. Buchman", "M.W. Schmidt", "S. Mohamed", "D. Poole", "N. de Freitas" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Buchman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Buchman et al\\.",
      "year" : 2012
    }, {
      "title" : "Toward the implementation of a quantum RBM",
      "author" : [ "M. Denil", "N. de Freitas" ],
      "venue" : "In NIPS Deep Learning and Unsupervised Feature Learning Workshop,",
      "citeRegEx" : "Denil and Freitas,? \\Q2011\\E",
      "shortCiteRegEx" : "Denil and Freitas",
      "year" : 2011
    }, {
      "title" : "Maximum likelihood estimation in log-linear models",
      "author" : [ "S.E. Fienberg", "A. Rinaldo" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Fienberg and Rinaldo,? \\Q2012\\E",
      "shortCiteRegEx" : "Fienberg and Rinaldo",
      "year" : 2012
    }, {
      "title" : "Introduction to random fields",
      "author" : [ "D. Griffeath" ],
      "venue" : "In Denumerable Markov Chains,",
      "citeRegEx" : "Griffeath,? \\Q1976\\E",
      "shortCiteRegEx" : "Griffeath",
      "year" : 1976
    }, {
      "title" : "Markov fields on finite graphs and lattices",
      "author" : [ "J.M. Hammersley", "P. Clifford" ],
      "venue" : "Unpublished manuscript,",
      "citeRegEx" : "Hammersley and Clifford,? \\Q1971\\E",
      "shortCiteRegEx" : "Hammersley and Clifford",
      "year" : 1971
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton,? \\Q2000\\E",
      "shortCiteRegEx" : "Hinton",
      "year" : 2000
    }, {
      "title" : "Estimation of non-normalized statistical models using score",
      "author" : [ "A. Hyvärinen" ],
      "venue" : "matching. JMLR,",
      "citeRegEx" : "Hyvärinen,? \\Q2005\\E",
      "shortCiteRegEx" : "Hyvärinen",
      "year" : 2005
    }, {
      "title" : "An introduction to probabilistic graphical models",
      "author" : [ "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Jordan,? \\Q2002\\E",
      "shortCiteRegEx" : "Jordan",
      "year" : 2002
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedman,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J.D. Lafferty", "A. McCallum", "F.C.N. Pereira" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Graphical models",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "Lauritzen,? \\Q1996\\E",
      "shortCiteRegEx" : "Lauritzen",
      "year" : 1996
    }, {
      "title" : "Markov random field modeling in image analysis",
      "author" : [ "S.Z. Li" ],
      "venue" : null,
      "citeRegEx" : "Li,? \\Q2001\\E",
      "shortCiteRegEx" : "Li",
      "year" : 2001
    }, {
      "title" : "Distributed parameter estimation via pseudo-likelihood",
      "author" : [ "Q. Liu", "A. Ihler" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Liu and Ihler,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu and Ihler",
      "year" : 2012
    }, {
      "title" : "Asymptotic efficiency of deterministic estimators for discrete energy-based models: Ratio matching and pseudolikelihood",
      "author" : [ "B. Marlin", "N. de Freitas" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Marlin and Freitas,? \\Q2011\\E",
      "shortCiteRegEx" : "Marlin and Freitas",
      "year" : 2011
    }, {
      "title" : "Inductive principles for restricted Boltzmann machine learning",
      "author" : [ "B. Marlin", "K. Swersky", "B. Chen", "N. de Freitas" ],
      "venue" : "In AIStats,",
      "citeRegEx" : "Marlin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Marlin et al\\.",
      "year" : 2010
    }, {
      "title" : "Distributed learning of Gaussian graphical models via marginal likelihoods",
      "author" : [ "Z. Meng", "D. Wei", "A. Wiesel", "A.O. Hero III" ],
      "venue" : "In AIStats,",
      "citeRegEx" : "Meng et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2013
    }, {
      "title" : "Marginal likelihoods for distributed parameter estimation of Gaussian graphical models",
      "author" : [ "Z. Meng", "D. Wei", "A. Wiesel", "A.O. Hero III" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Meng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2014
    }, {
      "title" : "Machine Learning: A Probabilistic Perspective",
      "author" : [ "K.P. Murphy" ],
      "venue" : null,
      "citeRegEx" : "Murphy,? \\Q2012\\E",
      "shortCiteRegEx" : "Murphy",
      "year" : 2012
    }, {
      "title" : "High-dimensional Ising model selection using `1regularized logistic regression",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2010
    }, {
      "title" : "Pseudolikelihood estimation for social networks",
      "author" : [ "D. Strauss", "M. Ikeda" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Strauss and Ikeda,? \\Q1990\\E",
      "shortCiteRegEx" : "Strauss and Ikeda",
      "year" : 1990
    }, {
      "title" : "An introduction to conditional random fields",
      "author" : [ "C. Sutton", "A. McCallum" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Sutton and McCallum,? \\Q2012\\E",
      "shortCiteRegEx" : "Sutton and McCallum",
      "year" : 2012
    }, {
      "title" : "On autoencoders and score matching for energy based models",
      "author" : [ "K. Swersky", "M.A. Ranzato", "D. Buchman", "B. Marlin", "N. Freitas" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Swersky et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Swersky et al\\.",
      "year" : 2011
    }, {
      "title" : "An overview of composite likelihood methods",
      "author" : [ "C. Varin", "N. Reid", "D. Firth" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Varin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Varin et al\\.",
      "year" : 2011
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Wainwright and Jordan,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright and Jordan",
      "year" : 2008
    }, {
      "title" : "Distributed covariance estimation in Gaussian graphical models",
      "author" : [ "A. Wiesel", "A.O. Hero III" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Wiesel and III,? \\Q2012\\E",
      "shortCiteRegEx" : "Wiesel and III",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Markov Random Fields (MRFs), also known as undirected probabilistic graphical models, are ubiquitous structured probability models that have significantly impacted a large number of fields, including computer vision (Li, 2001; Szeliski et al., 2008), computational photography and graphics (Agarwala et al.",
      "startOffset" : 216,
      "endOffset" : 249
    }, {
      "referenceID" : 1,
      "context" : ", 2008), computational photography and graphics (Agarwala et al., 2004), computational neuroscience (Ackley et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : ", 2004), computational neuroscience (Ackley et al., 1985), bioinformatics (Yanover et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : ", 2007), sensor networks (Liu & Ihler, 2012), social networks (Strauss & Ikeda, 1990), Markov logic (Richardson & Domingos, 2006), natural language processing (Lafferty et al., 2001; Sutton & McCallum, 2012) and statistical physics (Kindermann & Snell, 1980).",
      "startOffset" : 159,
      "endOffset" : 207
    }, {
      "referenceID" : 0,
      "context" : ", 2004), computational neuroscience (Ackley et al., 1985), bioinformatics (Yanover et al., 2007), sensor networks (Liu & Ihler, 2012), social networks (Strauss & Ikeda, 1990), Markov logic (Richardson & Domingos, 2006), natural language processing (Lafferty et al., 2001; Sutton & McCallum, 2012) and statistical physics (Kindermann & Snell, 1980). As pointed out in Wainwright & Jordan (2008) there are also many applications in statistics, constraint satisfaction and combinatorial optimization, error-correcting codes and epidemiology.",
      "startOffset" : 37,
      "endOffset" : 394
    }, {
      "referenceID" : 2,
      "context" : "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyvärinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).",
      "startOffset" : 129,
      "endOffset" : 263
    }, {
      "referenceID" : 8,
      "context" : "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyvärinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).",
      "startOffset" : 129,
      "endOffset" : 263
    }, {
      "referenceID" : 9,
      "context" : "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyvärinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).",
      "startOffset" : 129,
      "endOffset" : 263
    }, {
      "referenceID" : 17,
      "context" : "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyvärinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).",
      "startOffset" : 129,
      "endOffset" : 263
    }, {
      "referenceID" : 25,
      "context" : "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyvärinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).",
      "startOffset" : 129,
      "endOffset" : 263
    }, {
      "referenceID" : 24,
      "context" : "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyvärinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).",
      "startOffset" : 129,
      "endOffset" : 263
    }, {
      "referenceID" : 21,
      "context" : "Several researchers have addressed this issue by proposing to approximate pseudolikelihood by disjointly optimizing each conditional and combining the parameters using some form of averaging (Ravikumar et al., 2010; Wiesel & Hero III, 2012; Liu & Ihler, 2012).",
      "startOffset" : 191,
      "endOffset" : 259
    }, {
      "referenceID" : 19,
      "context" : "At the time of revising this paper, the same authors have shown that the convergence rate to the true parameters with their method is comparable to centralized maximum likelihood estimation (Meng et al., 2014).",
      "startOffset" : 190,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : ", 2010; Wiesel & Hero III, 2012; Liu & Ihler, 2012). In this paper we introduce a new approach to parameter estimation in MRFs with untied parameters, which avoids the model inefficiency of maximum likelihood for an important class of models while preserving its data efficiency. Moreover, our algorithm is embarrassingly parallel and can be implemented in a distributed setting without modification. Our algorithm replaces the joint maximum likelihood problem with a collection of much smaller auxiliary maximum likelihood problems which can be solved independently. We prove that if the auxiliary problems satisfy certain conditions, the relevant parameters in the auxiliary problems converge to the values of the true parameters in the joint model. Our experiments show that good performance is achieved in this case and that good performance is still achieved when these conditions are not satisfied. Violating the conditions for convergence sacrifices theoretical guarantees in exchange for even further computational savings while maintaining good empirical performance. Under a strong assumption (which is generally not satisfied in practice) we prove that our algorithm is exactly equal to maximum likelihood on the full joint distribution. While not directly applicable, this result provides additional insight into why our approach is effective. A similar method was recently, and independently, introduced in the context of Gaussian graphical models by Meng et al. (2013). In that paper, the authors consider local neighborhoods of nodes, whereas we consider neighborhoods of cliques, and they rely on a convex relaxation via the Schur complement to derive their algorithm for inverse covariance estimation.",
      "startOffset" : 33,
      "endOffset" : 1483
    }, {
      "referenceID" : 3,
      "context" : "E(xc |θc) = −θc φc(xc) where φc(xc) is a feature vector derived from the values of the variables xc, we have a maximum entropy or log-linear model (Wasserman, 2004; Buchman et al., 2012; Murphy, 2012).",
      "startOffset" : 147,
      "endOffset" : 200
    }, {
      "referenceID" : 20,
      "context" : "E(xc |θc) = −θc φc(xc) where φc(xc) is a feature vector derived from the values of the variables xc, we have a maximum entropy or log-linear model (Wasserman, 2004; Buchman et al., 2012; Murphy, 2012).",
      "startOffset" : 147,
      "endOffset" : 200
    }, {
      "referenceID" : 17,
      "context" : "That is, x̄mn = 1 − xmn if i = m and xmn otherwise (Marlin et al., 2010).",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "The proof can be found in (Griffeath, 1976; Bremaud, 2001):",
      "startOffset" : 26,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "We will make use of the following characterization of maximum likelihood estimates, which is proved in (Jordan, 2002).",
      "startOffset" : 103,
      "endOffset" : 117
    } ],
    "year" : 2014,
    "abstractText" : "We introduce a new embarrassingly parallel parameter learning algorithm for Markov random fields with untied parameters which is efficient for a large class of practical models. Our algorithm parallelizes naturally over cliques and, for graphs of bounded degree, its complexity is linear in the number of cliques. Unlike its competitors, our algorithm is fully parallel and for log-linear models it is also data efficient, requiring only the local sufficient statistics of the data to estimate parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}