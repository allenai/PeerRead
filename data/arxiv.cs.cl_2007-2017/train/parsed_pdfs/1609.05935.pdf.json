{
  "name" : "1609.05935.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 9.\n05 93\n5v 1\n[ cs\n.C L\n] 1\n9 Se\np 20\n16\nIndex Terms— recurrent neural network, CTC, speech recognition, end-to-end training."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system. They have also been used for featureaugmentation in a “tandem” GMM-HMM system [8], which again relied on a standard HMM backbone. The state-of-the-art today is a hybrid HMM/neural-net system, which uses the classical decoding strategy\nw∗ = argmax w P (w)P (a|w) (1)\nwhere the prior on words P (w) is estimated with a language model trained on text only, and the probability of the acoustics P (a|w) is estimated with a neural network acoustic model. The acoustic model still uses a decision tree [9] to further decompose the word sequence into context-dependent triphone states, and a decoder to perform a complex discrete search for the likeliest word sequence.\nRecently, several research groups have begun to study whether a neural network itself can subsume the functions of the decision tree, and decoding search. We refer to these approaches as “all-neural,” and they are also commonly referred to as “end-to-end.” Two main approaches have been used, both of which attempt to leverage a recurrent neural network’s potential ability to “remember” information for a long period of time1 and then act on it [12, 13, 14]. The first of these approaches uses an RNN trained in the “connectionist temporal classification” (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20]. At its base, this approach has no decoder at all, with all logic related to language modeling and decoding being implicitly done by the RNN. The output of a CTC system can of course be consumed by a subsequent decoding process, but it is not necessary. The second approach uses an RNN with\n1The vanishing gradient problem has been sufficiently overcome by longshort-term memory (LSTM) [10] and recurrent neural networks with rectified linear units (ReLU-RNNs) [11] that many problems in machine translation and language processing can be handled regardless.\nan attention mechanism [21, 22, 23]. The attention mechanism provides a weighted sum of the hidden activations in a learned encoding of the input frames as an additional input to the RNN at each time frame. When the network learns an attention function that happens to be unimodal with a peak that moves from left to right monotonically, it is similar to a Viterbi alignment. Attention-based models use a beam search to decode sequentially, symbol by symbol.\nThis paper is motivated by the desire to see to what extent the decoding process of a standard system can be modeled by a neural net itself, and to gauge the necessity of a pronunciation dictionary and decision tree. We describe a CTC based system that advances the state-of-the-art in all-neural modeling for conversational speech recognition. We propose a two-stage CTC process, in which we first train a system that consumes speech features and hypothesizes letter sequences. Second, we re-use the CTC apparatus to train a system that consumes this noisy letter sequence and produces a cleaner version. Additionally, we present a careful exploration of the unitvocabulary, and of the training process. We find that a symbol inventory that uses special word-initial characters (capital letters) rather that spaces performs well. Finally, we explore the addition of both character and word language models. We advance the state of the art at every level from pure all-neural ASR through to the addition of a word-based decoding process.\nThe remainder of this paper is organized as follows. Section 2 places our work in the context of previous efforts. Section 3 describe the model we use, and 4 our standard decoding process, and extensions. Section 5 proposes the novel technique of iterated CTC. In Section 6 we describe the details of training the models. Section 7 presents experimental results, followed by conclusions in Section 8."
    }, {
      "heading" : "2. RELATION TO PRIOR WORK",
      "text" : "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations. Past work was motivated by the need to quickly build systems in new languages without a dictionary, and kept the rest of a standard HMM system, in particular the use of a decision tree. In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.\nOur approach is most similar to the CTC methods of [20, 28, 19, 18, 17]. In contrast to [20, 28, 17], we use a ReLU-RNN rather than an LSTM, and find it to be effective and much faster. In contrast to [19], we use recurrent networks at every level as opposed to deep neural nets (DNNs) in the lower levels, and an RNN at the top level only. Also in contrast to [19], we study performance in the absence of an external language model as well as with one.\nWe extend past CTC work by the use of what we term iterated CTC, first operating on acoustic features, and then on letter sequences. This use of CTC on symbolic input is a novel alternative to encoder-decoder models, and is described in Section 5. Interestingly, the attention-based approach of [22] also introduces an extra\nRNN layer with the motivation of modeling symbol/language level phenomena."
    }, {
      "heading" : "3. THE MODEL",
      "text" : "We adopt a multi-layer RNN trained with CTC [15]. Central to the CTC process is the use of a “don’t care” or blank symbol, which is allowed to optionally occur between regular symbols. The standard alpha-beta recursions are used to compute the posterior state occupancy probabilities.\nLet the input consist of t acoustic frames along with a symbol sequence S. Denote an alignment of the t audio frames to the sequence S by π, and the product of the state-level neural net probabilities for the alignment as P (S|π). Let ptq be the probability the neural net assigns to symbol q at time t, i.e., the output after the softmax function. The CTC objective function is given by\nL = ∑\nπ\nP (S|π)P (π) = ∑\nπ\nP (π) ∏\nt\np t S π(t) .\nP (π) is determined by the HMM transition probabilities. The key input to CTC is the probability as determined by the neural network of a particular symbol St at time t.\nConsistent with standard notation, denote the posterior probability of being in state/symbol q at time t by γtq . Note that this is derived from the alpha-beta computations, and is distinct from the probability ptq that the neural network assigns to symbol q at time t. The derivative of the CTC objective function with respect to the activation atq for output q at time t before the softmax is\ndL datq = γtq − p t q\nThis is the error signal for backpropagation into the RNN."
    }, {
      "heading" : "4. INTERPRETING THE OUTPUT",
      "text" : ""
    }, {
      "heading" : "4.1. Raw CTC Output",
      "text" : "After training, the output of the RNN can be directly converted into a readable character sequence. There are two problems to solve:\n1. Where to put spaces between words.\n2. How to distinguish instances of repeated characters, for example the ll in hello, from a sequence of frames each labeled with the same letter, e.g. the l in help. Recall that the CTC blank symbol is optional, so a sequence of frames labeled by a single letter cannot be immediately distinguished from multiple occurrences of that letter.\nWhen the symbol inventory includes a space symbol (distinct from the blank symbol), the first problem is easily solved. Past work, e.g. [18], solve the second problem with a search over alternatives, or requiring a blank between letters. Instead, we propose a new symbol inventory as described below."
    }, {
      "heading" : "4.2. Symbol Inventory",
      "text" : "Past work [18, 19, 22] has explicitly modeled the spaces between words in the acoustic model, e.g., with a special space symbol “ ” distinct from the CTC blank symbol. Since words are frequently run together, we propose an alternative representation where word-initial characters are considered distinct from non-initial characters. A convenient representation of this is to use capital letters in the wordinitial position. Since the forward-backward computation in CTC\nrequires that the input sequence be longer than the output sequence, this also increases the set of utterances that can be aligned. This is also consistent with speech recognition systems that use positiondependent phonetic variants. To identify repeated letters, we use special double-letter units to represent repeated characters like ll. Finally, to improve the readability of the output without any further processing, we attach apostrophes to the following letters, creating units like the “’d” in “we’d.” Altogether the unit inventory size is 79.\nAs an example of this, the sentence\n“yes he has one”\nwould be rendered for training as\n“YesHeHasOne”.\nWith this encoding, a readable decoding can then be produced very simply:\n1. Select the most likely symbol at each frame\n2. Discard all occurrences of the “don’t care” symbol\n3. Compress consecutive occurrences of the same letter into one occurrence\n4. Add a space in front of each capitalized letter and show the output"
    }, {
      "heading" : "4.3. Character Beam Search",
      "text" : "Previous work [19, 18, 23] has used a character-level language model to improve the output of a neural system. This implements the classical decoding paradigm of Eqn. 1 at the character level, with the character language model providing P (w) or in this case P (c). The neural network provides P (c|a), and beam search is used to find the likeliest character sequence. We present results for this approach in Section 7.4."
    }, {
      "heading" : "4.4. Word Beam Search",
      "text" : "To provide a complete set of results comparable to [19], we have also used a word-based decoder that uses a graphemic dictionary, and uses the frame-level likelihoods in the standard way. The decoder is the dynamic decoder as described in [29]. We used the CUED-RNNLM toolkit [30] to train two forward- and two backward-running RNN language models. These are interpolated with a standard 4-gram model and used to rescore N-best lists produced by the N-gram decoder. Details can be found in a companion paper [31].\n5. ITERATED CTC (CTC2)\nThe output described in the previous section is of course noisy. For example, one of the Switchboard utterances is “no white collar crime does not exactly fall into it”, but the raw network output is “and now whi coler crime doen exsitally fall into it”. The classical approach to improving this is the incorporation of a lexicon of legal word units and a language model, as described in Section 4.\nTo improve the output with a purely neural network based approach, we propose using iterated CTC. Specifically, the initial output is represented with one-hot feature vectors analogous to the acoustic feature vectors, and the RNN/CTC training process is\nrepeated. The result is a model that transforms a noisy symbol sequence into a less noisy sequence. We have found that this process, while producing less dramatic improvements than the incorporation of a full fledged decoder, consistently improves the output, while staying in the all-neural paradigm. This is the case even when the original network is optimally deep."
    }, {
      "heading" : "6. TRAINING PROCESS",
      "text" : "Our models are trained using stochastic gradient descent with momentum and L2 regularization. For all but our largest networks, minibatches of 32 utterances are processed at once, resulting in updates after several thousand speech frames. For networks of width 1024 and depth 7 or greater, we have found it necessary to process 64 utterances simultaneously to achieve accurate gradient estimates and stable convergence. We use frame-skipping [17], where we stack three consecutive frames into a single vector to produce an input sequence one-third as long and three times as wide as the original input. When we train on the 300-hour Switchboard set, we use a per-frame learning rate of 0.5, and decrease it by a factor of 4 if 3 iterations over the data fail to produce an improvement on development data. When the Fisher data is added, resulting in about 2000 hours of training data, the rate is reduced if a single iteration passes without increasing dev set likelihood.\nWe implement the model with direct calls to the CUDNN v5.0 RNN library. Training with 40-dimensional input feature vectors (prior to skipping), a 1024-dimension bi-directional ReLU-RNN with nine hidden layers (our best configuration) is about 0.0058 times real time, i.e., 170 times faster than real time, running on a single NVIDIA Titan-X GPU."
    }, {
      "heading" : "6.1. Stabilization Methods",
      "text" : "In initial experiments, we found it useful to introduce several stabilization techniques. Most importantly, we use gradient clipping to prevent “exploding gradients” during the RNN training. The magnitude of the gradients are clipped at 1 prior to the momentum update. Secondarily, we have noticed that when rare units are present (e.g. the “ii” in “Hawaii,” the training process tends to push their probabilities close to 0 between occurrences, which leads to poor performance and sometimes instability when the unit is eventually seen. To avoid this, we interpolate the gradient with a small gradient tending towards the uniform distribution. This is implemented by interpolating the γ values from the alpha-beta computation with a uniform distribution. We reserve 1% of the total probability mass\nfor this uniform distribution. Finally, we have also found it important to compute the gradient over a large number of utterances (32 or 64) before doing a parameter update."
    }, {
      "heading" : "6.2. Model Initialization",
      "text" : "Weight matrices are initialized with small random weights uniformly distributed and inversely proportional to the square root of the fanin, with one exception. In the output layer, which maps from the RNN hidden dimension (typically 1024) down to the size of the symbol inventory (79 in our case), we assign explicit responsibility for each output symbol to a specific RNN activation. This is done by using an identity matrix for the first 79 dimensions, and zeros elsewhere. In the case of bidirectional networks, this is done symmetrically so both forward and backward portions of the network contribute equally. While we have not performed an exhaustive evaluation of this scheme, in initial experiments we observed consisent small gains. Concurrent with this work, a similar scheme was very recently proposed in [32] for standard neural net systems."
    }, {
      "heading" : "6.3. Polishing with In-domain Data",
      "text" : "Our training process begins and ends with a focus on the in-domain 300 hour switchboard-only dataset. We start by training on the 300 hour set mainly for convenience in showing results with both the 300 hour (Switchboard) and 2000 hour (Switchboard + Fisher) datasets. The 2000 hour models presented here were initialized with the output of 300 hour training. While training from scratch with 2000 hours of data works about as well, we have found it consistently useful to finish all training runs by executing a few more iterations of training on the in-domain Switchboard-only data. We do this starting from a very low learning rate (one-tenth the normal rate), and most of the gain is observed in the first iteration of training."
    }, {
      "heading" : "7. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "7.1. Corpus",
      "text" : "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.\nModel selection uses the RT-02 CTS evaluation set for development. The input features are 40-dimensional log-mel-filterbank energies, extracted every 10 milliseconds. The feature vectors are normalized to zero mean on a per-utterance level. Since the logarithmic compression already limits the dynamic range to reasonable levels, we do not perform variance normalization."
    }, {
      "heading" : "7.2. Network Architecture and Symbol Inventory",
      "text" : "For computational efficiency, we restricted ourselves to the models directly supported by the CUDNN v5.0 library. This encompasses multi-layer uni- and bidirectional RNNs. Support is provided for LSTMs, Gated Recurrent Units, standard sigmoid RNNs, and ReLU-RNNs. In initial experiments, we found that ReLU-RNNs are as good as LSTMs for this task, and many times faster. Therefore we use them exclusively in the experiments. We further focus on bi-directional networks for improved performance.\nTable 1 shows the effect of our choice of symbol inventory. We see that the use of special word-initial characters improves performance over the use of explicit blanks. Redundantly modeling wordfinal characters does not provide a further improvement. For ease of interpretability, all models use explicit double-character symbols.\nIn Table 2, we show the effect of network width, keeping the depth constant at 5. While the widest network is the best, for computational reasons we restrict our further experiments to networks of width 1024 or less.\nIn Table 3, we present the effect of network depth, for hidden layer sizes of 512 and 1024. Note that since we use a bidirectional network, the total number of activations in a layer is double this. We find that relatively deep networks perform well. Based on these results, the remainder of the experiments use a 1024 width 9 layer bidirectional network. Including weight matrices and biases, the total number of parameters in the 9 layer 1024 wise network is about 53 million parammeters."
    }, {
      "heading" : "7.3. Iterated CTC and Beam Search",
      "text" : "We evaluate the post-processing methods in Table 4. Clearly, the RNN is not yet learning all the logic of a beam-search decoder, and the effectiveness of a character-based beam search is midway between using the raw output, and a full word-based search. We see\nthat iterated CTC can produce a significant improvement, though not as much as a complete beam search, while remaining in the all-neural framework. An examination of the iterated CTC errors indicates that it mostly reduces the substitution rates, as the global shifts created by insertions and deletions seem difficult for the RNN to compensate."
    }, {
      "heading" : "7.4. Comparison to Previously Published Results",
      "text" : "We summarize our results on the NIST 2000 CTS test set and compare them with past work in Tables 5 and 6. The systems are categorized according to whether they use a lexicon to enforce the output of legal words, and in their use of a language model. We see an improvement over previous results with our RNN based system. In [28], a LSTM-CTC system using phonemic rather than grapemic targets is presented, and achieves an error rate of 15% on the Switchboard portion of eval 2000; that system still uses a phonetic dictionary. Compared to a standard system, such as [33], which achieves 9.6% and 13% on Switchboard and CallHome respectively with conventional 300-hour training, we see that current neural-only systems cannot yet mimic all the logic in a conventional system. However, our Relu-RNN system does set a new state of the art for an all-neural system, and we see that performance of such systems is rapidly improving."
    }, {
      "heading" : "8. CONCLUSIONS",
      "text" : "We advance the state of the art with an all-neural speech recognizer, principally by employing a novel symbol encoding, and optimized training process. We further present an iterated CTC approach for use without any decoding process. In this framework, a network first maps from audio to symbols, followed by a second symbolto-symbol mapping network. Both using raw network output and search-based post-processing, we systematically improve on previously published results in the end-to-end neural speech recognition paradigm."
    }, {
      "heading" : "9. REFERENCES",
      "text" : "[1] George E Dahl, Dong Yu, Li Deng, and Alex Acero, “Large vocabulary continuous speech recognition with contextdependent DBN-HMMs,” in 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2011, pp. 4688–4691.\n[2] Abdel-rahman Mohamed, George Dahl, and Geoffrey Hinton, “Deep belief networks for phone recognition,” in NIPS workshop on deep learning for speech recognition and related applications, 2009, vol. 1, p. 39.\n[3] Frank Seide, Gang Li, and Dong Yu, “Conversational speech transcription using context-dependent deep neural networks.,” in Interspeech, 2011, pp. 437–440.\n[4] Hasim Sak, Andrew W Senior, and Françoise Beaufays, “Long short-term memory recurrent neural network architectures for large scale acoustic modeling.,” in INTERSPEECH, 2014, pp. 338–342.\n[5] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.\n[6] Herve A Bourlard and Nelson Morgan, Connectionist speech recognition: a hybrid approach, Springer Science & Business Media, 1993.\n[7] Tony Robinson and Frank Fallside, “A recurrent error propagation network speech recognition system,” Computer Speech & Language, vol. 5, no. 3, pp. 259–274, 1991.\n[8] Hynek Hermansky, Daniel PW Ellis, and Sangita Sharma, “Tandem connectionist feature extraction for conventional HMM systems,” in ICASSP 2000. IEEE, 2000, vol. 3, pp. 1635–1638.\n[9] Steve J Young, Julian J Odell, and Philip C Woodland, “Treebased state tying for high accuracy acoustic modelling,” in Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994, pp. 307–312.\n[10] Sepp Hochreiter and Jürgen Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[11] Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton, “A simple way to initialize recurrent networks of rectified linear units,” arXiv preprint arXiv:1504.00941, 2015.\n[12] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to sequence learning with neural networks,” in Advances in neural information processing systems, 2014, pp. 3104–3112.\n[13] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.\n[14] Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton, “Grammar as a foreign language,” in Advances in Neural Information Processing Systems, 2015, pp. 2773–2781.\n[15] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber, “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.\n[16] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech recognition with recurrent neural networks.,” in ICML, 2014, vol. 14, pp. 1764–1772.\n[17] Haşim Sak, Andrew Senior, Kanishka Rao, and Françoise Beaufays, “Fast and accurate recurrent neural network acoustic models for speech recognition,” arXiv preprint arXiv:1507.06947, 2015.\n[18] Andrew L Maas, Ziang Xie, Dan Jurafsky, and Andrew Y Ng, “Lexicon-free conversational speech recognition with neural networks,” in Proc. NAACL, 2015.\n[19] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., “Deep speech: Scaling up endto-end speech recognition,” arXiv preprint arXiv:1412.5567, 2014.\n[20] Yajie Miao, Mohammad Gowayyed, and Florian Metze, “Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,” in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167–174.\n[21] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Yoshua Bengio, et al., “End-to-end attention-based large vocabulary speech recognition,” in ICASSP 2016. IEEE, 2016, pp. 4945– 4949.\n[22] Liang Lu, Xingxing Zhang, and Steve Renals, “On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition,” in ICASSP 2016. IEEE, 2016, pp. 5060–5064.\n[23] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in ICASSP 2016. IEEE, 2016, pp. 4960–4964.\n[24] Ernst Günter Schukat-Talamazzini, Heinrich Niemann, Wieland Eckert, Thomas Kuhn, and S Rieck, “Automatic speech recognition without phonemes.,” in Eurospeech, 1993.\n[25] Christoph Schillo, Gernot A Fink, and Franz Kummert, “Grapheme based speech recognition for large vocabularies.,” in INTERSPEECH, 2000, pp. 584–587.\n[26] Stephan Kanthak and Hermann Ney, “Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition,” in ICASSP, 2002, vol. 2, pp. 845–848.\n[27] Mirjam Killer, Sebastian Stüker, and Tanja Schultz, “Grapheme based speech recognition.,” in INTERSPEECH, 2003.\n[28] Yajie Miao, Mohammad Gowayyed, Xingyu Na, Tom Ko, Florian Metze, and Alexander Waibel, “An empirical exploration of ctc acoustic models,” in ICASSP. IEEE, 2016, pp. 2623– 2627.\n[29] Charith Mendis, Jasha Droppo, Saeed Maleki, Madanlal Musuvathi, Todd Mytkowicz, and Geoffrey Zweig, “Parallelizing wfst speech decoders,” in ICASSP 2016. IEEE, 2016, pp. 5325–5329.\n[30] Xie Chen, Xunying Liu, Yanmin Qian, MJF Gales, and PC Woodland, “CUED-RNNLM: An open-source toolkit for efficient training and evaluation of recurrent neural network language models,” in ICASSP 2016. IEEE, 2016, pp. 6000– 6004.\n[31] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “The Microsoft 2016 conversational speech recognition system,” submitted to ICASSP, 2017.\n[32] “Improved neural network initialization by grouping contextdependent targets for acoustic modeling,” in Interspeech, 2016.\n[33] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahrmani, Vimal Manohar, Xingyu Na, Yiming Wang, and Sanjeev Khudanpur, “Purely sequence-trained neural networks for asr based on lattice-free MMI,” Interspeech, 2016."
    } ],
    "references" : [ {
      "title" : "Large vocabulary continuous speech recognition with contextdependent DBN-HMMs",
      "author" : [ "George E Dahl", "Dong Yu", "Li Deng", "Alex Acero" ],
      "venue" : "2011 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2011, pp. 4688–4691.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep belief networks for phone recognition",
      "author" : [ "Abdel-rahman Mohamed", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "NIPS workshop on deep learning for speech recognition and related applications, 2009, vol. 1, p. 39.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Conversational speech transcription using context-dependent deep neural networks",
      "author" : [ "Frank Seide", "Gang Li", "Dong Yu" ],
      "venue" : "Interspeech, 2011, pp. 437–440.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
      "author" : [ "Hasim Sak", "Andrew W Senior", "Françoise Beaufays" ],
      "venue" : "INTERSPEECH, 2014, pp. 338–342.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Connectionist speech recognition: a hybrid approach, Springer",
      "author" : [ "Herve A Bourlard", "Nelson Morgan" ],
      "venue" : "Science & Business Media,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1993
    }, {
      "title" : "A recurrent error propagation network speech recognition system",
      "author" : [ "Tony Robinson", "Frank Fallside" ],
      "venue" : "Computer Speech & Language, vol. 5, no. 3, pp. 259–274, 1991.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Tandem connectionist feature extraction for conventional HMM systems",
      "author" : [ "Hynek Hermansky", "Daniel PW Ellis", "Sangita Sharma" ],
      "venue" : "ICASSP 2000. IEEE, 2000, vol. 3, pp. 1635–1638.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Treebased state tying for high accuracy acoustic modelling",
      "author" : [ "Steve J Young", "Julian J Odell", "Philip C Woodland" ],
      "venue" : "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994, pp. 307–312.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton" ],
      "venue" : "arXiv preprint arXiv:1504.00941, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "Advances in neural information processing systems, 2014, pp. 3104–3112.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015, pp. 2773–2781.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Towards end-to-end speech recognition with recurrent neural networks",
      "author" : [ "Alex Graves", "Navdeep Jaitly" ],
      "venue" : "ICML, 2014, vol. 14, pp. 1764–1772.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fast and accurate recurrent neural network acoustic models for speech recognition",
      "author" : [ "Haşim Sak", "Andrew Senior", "Kanishka Rao", "Françoise Beaufays" ],
      "venue" : "arXiv preprint arXiv:1507.06947, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Lexicon-free conversational speech recognition with neural networks",
      "author" : [ "Andrew L Maas", "Ziang Xie", "Dan Jurafsky", "Andrew Y Ng" ],
      "venue" : "Proc. NAACL, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep speech: Scaling up endto-end speech recognition",
      "author" : [ "Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates" ],
      "venue" : "arXiv preprint arXiv:1412.5567, 2014.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding",
      "author" : [ "Yajie Miao", "Mohammad Gowayyed", "Florian Metze" ],
      "venue" : "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167–174.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "End-to-end attention-based large vocabulary speech recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio" ],
      "venue" : "ICASSP 2016. IEEE, 2016, pp. 4945– 4949.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition",
      "author" : [ "Liang Lu", "Xingxing Zhang", "Steve Renals" ],
      "venue" : "ICASSP 2016. IEEE, 2016, pp. 5060–5064.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals" ],
      "venue" : "ICASSP 2016. IEEE, 2016, pp. 4960–4964.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Automatic speech recognition without phonemes",
      "author" : [ "Ernst Günter Schukat-Talamazzini", "Heinrich Niemann", "Wieland Eckert", "Thomas Kuhn", "S Rieck" ],
      "venue" : "Eurospeech, 1993.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Grapheme based speech recognition for large vocabularies",
      "author" : [ "Christoph Schillo", "Gernot A Fink", "Franz Kummert" ],
      "venue" : "INTERSPEECH, 2000, pp. 584–587.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition",
      "author" : [ "Stephan Kanthak", "Hermann Ney" ],
      "venue" : "ICASSP, 2002, vol. 2, pp. 845–848.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Grapheme based speech recognition",
      "author" : [ "Mirjam Killer", "Sebastian Stüker", "Tanja Schultz" ],
      "venue" : "INTERSPEECH, 2003.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "An empirical exploration of ctc acoustic models",
      "author" : [ "Yajie Miao", "Mohammad Gowayyed", "Xingyu Na", "Tom Ko", "Florian Metze", "Alexander Waibel" ],
      "venue" : "ICASSP. IEEE, 2016, pp. 2623– 2627.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Parallelizing wfst speech decoders",
      "author" : [ "Charith Mendis", "Jasha Droppo", "Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Geoffrey Zweig" ],
      "venue" : "ICASSP 2016. IEEE, 2016, pp. 5325–5329.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "CUED-RNNLM: An open-source toolkit for efficient training and evaluation of recurrent neural network language models",
      "author" : [ "Xie Chen", "Xunying Liu", "Yanmin Qian", "MJF Gales", "PC Woodland" ],
      "venue" : "ICASSP 2016. IEEE, 2016, pp. 6000– 6004.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The Microsoft 2016 conversational speech recognition system",
      "author" : [ "W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig" ],
      "venue" : "submitted to ICASSP, 2017.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Purely sequence-trained neural networks for asr based on lattice-free MMI",
      "author" : [ "Daniel Povey", "Vijayaditya Peddinti", "Daniel Galvez", "Pegah Ghahrmani", "Vimal Manohar", "Xingyu Na", "Yiming Wang", "Sanjeev Khudanpur" ],
      "venue" : "Interspeech, 2016.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.",
      "startOffset" : 63,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.",
      "startOffset" : 63,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.",
      "startOffset" : 63,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.",
      "startOffset" : 63,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.",
      "startOffset" : 63,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "They have also been used for featureaugmentation in a “tandem” GMM-HMM system [8], which again relied on a standard HMM backbone.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "The acoustic model still uses a decision tree [9] to further decompose the word sequence into context-dependent triphone states, and a decoder to perform a complex discrete search for the likeliest word sequence.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "” Two main approaches have been used, both of which attempt to leverage a recurrent neural network’s potential ability to “remember” information for a long period of time and then act on it [12, 13, 14].",
      "startOffset" : 190,
      "endOffset" : 202
    }, {
      "referenceID" : 12,
      "context" : "” Two main approaches have been used, both of which attempt to leverage a recurrent neural network’s potential ability to “remember” information for a long period of time and then act on it [12, 13, 14].",
      "startOffset" : 190,
      "endOffset" : 202
    }, {
      "referenceID" : 13,
      "context" : "” Two main approaches have been used, both of which attempt to leverage a recurrent neural network’s potential ability to “remember” information for a long period of time and then act on it [12, 13, 14].",
      "startOffset" : 190,
      "endOffset" : 202
    }, {
      "referenceID" : 14,
      "context" : "The first of these approaches uses an RNN trained in the “connectionist temporal classification” (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "The first of these approaches uses an RNN trained in the “connectionist temporal classification” (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 16,
      "context" : "The first of these approaches uses an RNN trained in the “connectionist temporal classification” (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 17,
      "context" : "The first of these approaches uses an RNN trained in the “connectionist temporal classification” (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 18,
      "context" : "The first of these approaches uses an RNN trained in the “connectionist temporal classification” (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : "The first of these approaches uses an RNN trained in the “connectionist temporal classification” (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 9,
      "context" : "1The vanishing gradient problem has been sufficiently overcome by longshort-term memory (LSTM) [10] and recurrent neural networks with rectified linear units (ReLU-RNNs) [11] that many problems in machine translation and language processing can be handled regardless.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "1The vanishing gradient problem has been sufficiently overcome by longshort-term memory (LSTM) [10] and recurrent neural networks with rectified linear units (ReLU-RNNs) [11] that many problems in machine translation and language processing can be handled regardless.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 20,
      "context" : "an attention mechanism [21, 22, 23].",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : "an attention mechanism [21, 22, 23].",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "an attention mechanism [21, 22, 23].",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations.",
      "startOffset" : 57,
      "endOffset" : 73
    }, {
      "referenceID" : 24,
      "context" : "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations.",
      "startOffset" : 57,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations.",
      "startOffset" : 57,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations.",
      "startOffset" : 57,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.",
      "startOffset" : 43,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.",
      "startOffset" : 43,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.",
      "startOffset" : 43,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.",
      "startOffset" : 43,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 27,
      "context" : "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "In contrast to [20, 28, 17], we use a ReLU-RNN rather than an LSTM, and find it to be effective and much faster.",
      "startOffset" : 15,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "In contrast to [20, 28, 17], we use a ReLU-RNN rather than an LSTM, and find it to be effective and much faster.",
      "startOffset" : 15,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "In contrast to [20, 28, 17], we use a ReLU-RNN rather than an LSTM, and find it to be effective and much faster.",
      "startOffset" : 15,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "In contrast to [19], we use recurrent networks at every level as opposed to deep neural nets (DNNs) in the lower levels, and an RNN at the top level only.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 18,
      "context" : "Also in contrast to [19], we study performance in the absence of an external language model as well as with one.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 21,
      "context" : "Interestingly, the attention-based approach of [22] also introduces an extra",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "We adopt a multi-layer RNN trained with CTC [15].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "[18], solve the second problem with a search over alternatives, or requiring a blank between letters.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Past work [18, 19, 22] has explicitly modeled the spaces between words in the acoustic model, e.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "Past work [18, 19, 22] has explicitly modeled the spaces between words in the acoustic model, e.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 21,
      "context" : "Past work [18, 19, 22] has explicitly modeled the spaces between words in the acoustic model, e.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "Previous work [19, 18, 23] has used a character-level language model to improve the output of a neural system.",
      "startOffset" : 14,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "Previous work [19, 18, 23] has used a character-level language model to improve the output of a neural system.",
      "startOffset" : 14,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : "Previous work [19, 18, 23] has used a character-level language model to improve the output of a neural system.",
      "startOffset" : 14,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "To provide a complete set of results comparable to [19], we have also used a word-based decoder that uses a graphemic dictionary, and uses the frame-level likelihoods in the standard way.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 28,
      "context" : "The decoder is the dynamic decoder as described in [29].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 29,
      "context" : "We used the CUED-RNNLM toolkit [30] to train two forward- and two backward-running RNN language models.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 30,
      "context" : "Details can be found in a companion paper [31].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "We use frame-skipping [17], where we stack three consecutive frames into a single vector to produce an input sequence one-third as long and three times as wide as the original input.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "[18] N N 56.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "0 [22] N N 48.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 17,
      "context" : "[18] N Char NG 43.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "8 [18] N Char RNN 40.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 21,
      "context" : "8 [22] Y Word NG 46.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 18,
      "context" : "8 [19] Y Word NG 31.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 18,
      "context" : "[19] (ensemble) Y Word NG 19.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "In [28], a LSTM-CTC system using phonemic rather than grapemic targets is presented, and achieves an error rate of 15% on the Switchboard portion of eval 2000; that system still uses a phonetic dictionary.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 31,
      "context" : "Compared to a standard system, such as [33], which achieves 9.",
      "startOffset" : 39,
      "endOffset" : 43
    } ],
    "year" : 2016,
    "abstractText" : "This paper advances the design of CTC-based all-neural (or end-toend) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.",
    "creator" : "LaTeX with hyperref package"
  }
}