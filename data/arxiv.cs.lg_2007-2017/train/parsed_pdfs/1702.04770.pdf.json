{
  "name" : "1702.04770.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Training Language Models Using Target-Propagation",
    "authors" : [ "Sam Wiseman", "Sumit Chopra", "Arthur Szlam", "Ruoyu Sun", "Nicolas Vasilache" ],
    "emails" : [ "swiseman@seas.harvard.edu", "spchopra@fb.com", "ranzato@fb.com", "aszlam@fb.com", "ruoyu@fb.com", "soumith@fb.com", "ntv@fb.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Modern RNNs are trained almost exclusively using truncated Back-Propagation Through Time (BPTT) (Elman, 1990; Werbos, 1990; Mikolov et al., 2010). Despite its popularity, BPTT training has two major drawbacks, namely, that it is inherently sequential (and thus difficult to parallelize), and that it truncates the number of time-steps over which gradient information can propagate.\nInspired by the recent reports of success of Target-Propagation (TPROP) in training nonrecurrent deep networks (Carreira-Perpiñán and Wang, 2014; Lee et al., 2015; Taylor et al., 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009). TPROP can be understood as a generalization of backpropagation, where neural networks are trained by providing local targets for each hidden unit. Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986;\nLe Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpiñán and Wang, 2014; Lee et al., 2015).\nConcretely, we treat the hidden states of an RNN as free variables, which are optimized independently, but encouraged to be predictable from previous hidden states. Formulating the model in this way offers an approach to avoiding the sequential nature of BPTT training, by allowing for optimization over the parameters and all hidden states simultaneously for the entire data set, which is easily parallelized.\nWe extensively evaluate applying TPROP to train language models, and we find: (1) that batch TPROP is as effective as Batch BPTT in minimizing training loss, but they both fail to generalize well; (2) that mini-batch TPROP achieves comparable generalization performance to BPTT only in the limit when TPROP reduces to mere BPTT; (3) that the relatively unconstrained nature of the hidden state optimization appears to be responsible for the performance degradation."
    }, {
      "heading" : "1.1 Review: Truncated BPTT",
      "text" : "Letting xt represent the input at time t, an RNN produces hidden states ht and predictions ŷt at each time-step using the following recurrence\nht = g(xt, ht−1), (1)\nŷt = f(ht), (2)\nwhere g and f are non-linear, parametric functions of their inputs. For instance, an Elman RNN for language modeling might be specified by defining g(xt, ht−1) = σ(Wxxt + Whht−1), and f(ht) = softmax(Wyht), where σ denotes the logistic sigmoid function.\nIt is typical to apply RNNs to sequenceprediction problems that have a well-defined loss `(ŷt, yt) at every time-step, where yt is the desired\nar X\niv :1\n70 2.\n04 77\n0v 1\n[ cs\n.C L\n] 1\n5 Fe\nb 20\n17\noutput at the t’th step. In the case of language modeling, ` is the cross-entropy loss, where the number of classes is equal to the number of words in the vocabulary, and yt = xt+1.\nGiven `, as well as a dataset consisting of an input sequence x1, . . . , xT and desired output sequence y1, . . . , yT , it is possible to obtain a loss for the entire dataset in terms of any parameters θ (and an initial hidden state h0) by unrolling the RNN in time, as depicted in Figure 1. In particular, we have\nL(θ) = T∑ t=1 `(f(ht), xt+1)\n= `(f(g(x1, h0)), x2) (3)\n+`(f(g(x2, g(x1, h0)), x3) + . . .\nTypically the loss in Equation 3 is not optimized in batch. Rather, the sequence is unrolled for a window of K steps, and a gradient step is taken to minimize the loss on those K steps. After updating parameters, the minimization continues by unrolling the next (non-overlapping) window ofK steps, initializing h0 to the value of the K’th hidden state in the previous window, minimizing the loss on this new window, and continuing through the dataset in this way. Crucially, each window is considered to be independent of all previous ones, and so no gradients are calculated with respect to time-steps in previous windows. Going forward, we will refer to the loss induced by this windowing and truncation as the “Truncated BPTT Loss.”\nIssues with Truncated BPTT The Truncated BPTT Loss and associated minimization procedure suffers from two problems. First, the training algorithm is inherently sequential: one cannot process two consecutive spans of size K in parallel, since the processing of a K-window depends on having processed the previous K-window, which makes parallelization impossible. Second, the hard truncation of the gradients beyond K steps\nmakes it difficult for the network to capture longterm dependencies in the data."
    }, {
      "heading" : "2 Target Propagation",
      "text" : "In order to address the issues associated with the truncated BPTT algorithm, we propose to train RNNs with a slightly modified loss. In particular, rather than unrolling the losses ` over time-steps (as in Equation 3), which has the effect of instantiating the ht only implicitly, we instead treat the ht as explicit variables to be optimized over. In order to maintain the recurrent property of the model, however, we add additional constraint terms to the loss, which encourage adjacent hidden states to approximately obey the parametric recurrence.\nMore concretely, we define ĥt to be the predicted hidden state at time t, as follows:\nĥt = g(xt, ht−1) (4)\nŷt = f(ĥt). (5)\nNote that Equation (4) uses the independent variable ht−1 on the right-hand-side; otherwise it is equivalent to Equation (1). UsingH to refer to the set of all ht, we then modify the loss to be:\nL(H, θ) = T∑ t=1 `(f(ĥt), xt+1) + λC(ĥt, ht), (6)\nwhere C is a penalty or constraint function (e.g., an L2 penalty) introduced to force the predicted hidden state ĥt at time step t to be close to the free hidden state ht, and λ is a coefficient whose value governs how strictly we wish to enforce the constraint. This approach is referred to as TargetPropagation because the ht and the ĥt serve as targets for each other during optimization. See Figure 2 for reference."
    }, {
      "heading" : "2.1 The Blocked Target-Propagation Algorithm (BTPROP)",
      "text" : "We also consider a generalization of the TPROP loss, where instead of making each ht a free variable, we only have a free variable every B time steps. The remaining hidden states are defined implicitly through the recurrence (1), which further constrains the model during training. We refer to the sub-sequence of length B consisting of timesteps beginning with a free variable and ending before the next free variable as a “block.” Note that with block size B = 1 we recover the TPROP formulation in the previous subsection.\nBenefits of BTPROP We emphasize that the BTPROP loss offers an approach to addressing the issues with the Truncated BPTT Loss identified in Section 1.1. First, the independence of nonboundary time steps in different blocks suggests that training may be efficiently parallelized by distributing a large number of (contiguous) blocks to each machine in a cluster, which would necessitate inter-machine communication only for the small number of ht that border blocks on a different machine. We note, however, that since parameters θ are shared across time-steps (and must therefore be synchronized between machines), for such a scheme to offer a speed-up it must also be the case thatH and θ can be optimized independently, and that the H-optimization results in faster convergence of the θ-optimization.\nTo address the second issue with BPTT identified in Section 1.1, we note that the penalty terms C encourage the final hidden state of a block to be close to the initial hidden state of the subsequent block, which should allow for the capturing of dependencies between multiple blocks during training. Also note that by restricting the block size we can restrict the temporal dependence of the loss on any given ht, thereby mitigating the vanishing or exploding gradient problem.\nFinally, we note that we expect BTPROP to offer an advantage over TPROP (withB = 1), since it leads to a more constrained optimization problem (with fewer variables), and because it allows for intra-block BPTT, which has become a relatively mature technology."
    }, {
      "heading" : "3 Training",
      "text" : "The loss in Equation 6 can be seen as deriving from an equality-constrained formulation of the\ncross-entropy loss. In particular, if we introduce equality constraints between each ht and ĥt, as well as a dual variable ut for each constraint, the Lagrangian can be written (up to a constant) as\nLaug(H, θ,U) = T∑ t=1 `(f(ĥt), xi+1) + λ 2 ||ht − ĥt + ut||2, (7)\nwhich is in the form of Equation 6 with C defined as C(ĥt, ht) = 12 ||ht − ĥt + ut||\n2. It is natural to minimize the now-unconstrained loss (7) using an Alternating Direction Method of Multipliers (ADMM) style approach (Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011), which results in the following meta-algorithm:\n1. Minimize Laug with respect toH\n2. Minimize Laug with respect to θ\n3. Update duals: ut ← ut + αu∇utLaug.\nWe note that for many choices of RNN architecture it will be impossible to perform either the θminimization or theH-minimization analytically.1 As such, we simply use gradient-based algorithms for a fixed number of steps, and we find that it is not necessary (and generally not advisable) to minimize until convergence.\nWe also consider two alternatives to the above algorithm. The first, which we refer to as the Penalty Method (PM) (Courant et al., 1943; Nocedal and Wright, 2006), is identical to ADMM, except that it avoids the use of dual variables entirely, and so skips step 3. The second, the Augmented Langrangian Method (ALM) (Hestenes, 1969; Powell, 1967; Nocedal and Wright, 2006), minimizes jointly over H, θ before updating the dual variables, effectively merging steps 1. and 2. We found that ADMM outperforms PM, and very minimally outperforms ALM as well, and so we report ADMM results in what follows."
    }, {
      "heading" : "4 Experiments",
      "text" : "We run word-level language modeling experiments on the Penn Tree Bank (PTB) (Marcus et al.,\n1Exceptions to this include various forms of multiplicative RNNs (Wu et al., 2016) with no non-linearity. In this case, alternating minimizations can be carried out with least squares. We experimented along these lines, but found such methods to underperform gradient based approaches, which are also much more flexible.\n1993) and Text8 datasets.2 For all experiments we use single-layer Gated Recurrent Unit (GRU) RNNs (Cho et al., 2014) (without Dropout (Srivastava et al., 2014)), and we use Adagrad (Duchi et al., 2011) for optimization.\nWe report the perplexity obtained on the validation dataset after freezing the final parameters θ obtained during the alternating optimization process; no optimization is done at test time. (Preliminary experiments suggested that slightly better results can be obtained by optimizing over past ht variables at test-time, though we did not pursue this direction due to its inefficiency.)"
    }, {
      "heading" : "4.1 Results",
      "text" : "We report our main results in Table 1, where we compare BTPROP validation perplexity performance for various block-sizes and various numbers of H steps with BPTT. We report only validation numbers because training performance between BTPROP and BPTT was generally comparable.\nStarting from the left portion of Table 1, we see that BTPROP is roughly comparable to BPTT for bigger B. Importantly, however, BTPROP performance does not tend to improve with additional H-steps. This is disappointing because it can be shown that BTPROP with a single H-step is essentially equivalent to BPTT; see the Supplemental Material for a more rigorous formulation and proof. Furthermore, it is clear from the table that both Batch BTPROP and Batch BPTT are significantly outperformed by their minibatch counterparts, presumably due to the regularization effect of updating parameters after seeing only a subset of the data (see Keskar et al. (2016) for a discussion of this phenomenon). This suggests that even if Batch BTPROP were more easily parallelized, it would not be worth the decreased generalization.\n2http://mattmahoney.net/dc/textdata\nWe now consider the right two portions of Table 1, which involve Minibatch BTPROP. We note that applying BTPROP in a minibatch setting is not straightforward, since the penalty term C(ĥt, ht) will now involve hidden states ht that have not been updated since the previous epoch, and which therefore may not provide reasonable targets. We address this by initializing the hidden states ht in the current minibatch to the ĥt given by the current parameters before starting the H-optimization. We also emphasize that in the minibatch setting there is little hope for a parallelization gain; it is possible, however, that the BTPROP loss will still allow the model to benefit from training with longer-range dependencies. Unfortunately, the results again suggest that BTPROP gains little over BPTT, and that moreover additionalH-steps are not in general beneficial."
    }, {
      "heading" : "4.2 BTPROP as L2 Regularizer",
      "text" : "Additional experiments summarized in Table 2 suggest that the small BTPROP performance gains in the Minibatch setting (see Table 1) are attributable to the regularization effect of the L2 penalty in (7), rather than training with longer dependencies. There, we compare the Validation PPL obtained on Minibatch PTB withH-steps = 1 and B = 20 with doing no H-optimization (which improves (i.e., lowers)) perplexity, and with doing no H-optimization but also setting λ = 0 (which significantly hurts (i.e., increases) perplexity)."
    }, {
      "heading" : "4.3 A Possible Explanation and Future Work",
      "text" : "One explanation for the H-optimization hurting performance is that its relatively unconstrained nature may allow for finding hidden states that decrease training loss without leading to parameters that generalize. Indeed, many of the reported successes of TPROP-style training have involved very constrained problems, such as those with binary hidden-state constraints (CarreiraPerpiñán and Raziperchikolaei, 2015) or sparsity constraints (Kavukcuoglu et al., 2009). To test this hypothesis, in Figure 3 we plot the perplexity obtained from BTPROP with B = 10 (for both 2 and 5 H-optimization steps) as well as the perplexity obtained from BPTT with K = 10, as the dimensionality of the hidden state increases. We see that BPTT actually underperforms BTPROP for small hidden states, but improves relatively as the hidden state size increases, lending support to our hypothesis. A similar pattern emerges when comparing BTPROP with B = 5 and BPTT with K = 5. This suggests that getting BTPROP to work with larger hidden states may involve finding additional approaches to constraining the Hoptimization, such as by, for instance, penalizing the KL divergence between the distributions softmax(Wyĥt) and softmax(Wyht), which we leave to future work."
    }, {
      "heading" : "A Supplemental Material",
      "text" : "A.1 Connection between BPTT and TPROP Here we show that when using the Penalty Method loss (or, equivalently, the ADMM loss while keeping the dual variables ut set to 0), doing BTPROP with H-steps = 1 and θ-steps = 1 results in gradients with respect to θ that are equal to the gradients with respect to θ obtained from BPTT (up to a constant factor), if: (1) theH are initialized such that ht = gθ(xt, ht−1); (2) the ht are updated with vanilla gradient descent.\nMore formally, let `(gθ(xt, ht−1), yt) be a pertime-step loss, where θ are the parameters of g. Also define\n`pm = `(ht, yt) + λ\n2 ||gθ(xt, ht−1)− ht||2. (8)\nWe show that if we initialize ht = gθ(xt, ht−1) then we have\n∂ `pm(h̃t, yt)\n∂ θ = ηλ\n∂ `(gθ(xt, ht−1), yt)\n∂ θ , (9)\nwhere we have defined\nh̃t = ht − η ∂ `pm(ht, yt)\n∂ ht . (10)\nFrom the definition of `pm, we have\n∂ `pm(ht, yt) ∂ ht = ∂ `(ht, yt) ∂ ht + λ(ht − gθ(xt, ht−1))\n= ∂ l(ht, yt)\n∂ ht ,\nwhere the last line follows from the fact that ht = gθ(xt, ht−1). Substituting into (10) then gives\nh̃t = ht − η ∂ `(ht, yt)\n∂ ht .\nNow, from the definition of `pm and the chain rule, we can write the left hand side of (9) as\n∂ `pm(h̃t, yt)\n∂ θ\n= ∂ gθ(xt, ht−1) ∂ θ λ ( gθ(xt, ht−1)− h̃t ) = ∂ gθ(xt, ht−1) ∂ θ λ ( gθ(xt, ht−1)−\nht + η ∂ `(ht, yt)\n∂ ht ) = ∂ gθ(xt, ht−1)\n∂ θ λ\n( η ∂ `(ht, yt)\n∂ ht\n) , (11)\nwhere the last line again follows from the fact that ht = gθ(xt, ht−1). Since we can also write the right hand side of (9) as\n∂ `(gθ(xt, ht−1), yt) ∂ θ = ∂ gθ(xt, ht−1) ∂ θ ∂ `(ht, yt)\n∂ gθ(xt, ht−1)\n= ∂ gθ(xt, ht−1)\n∂ θ\n∂ `(ht, yt)\n∂ ht ,\nwe obtain the equality as desired.\nA.2 Hyperparameter Grid Used in Experiments\nSee Table 3.\nA.3 Increasing Mini-Batch Size with BPTT\nAn obvious approach to speed up training consists of increasing the mini-batch size of SGD. Unfortunately, the results in table 4 suggest that performance deteriorates on both training and test sets whenever the minibatch size is big enough to leverage parallel computation (mini-batch size of size 4096 and above). These findings are in line with the finding in Section 4.1 that both batch BPTT and batch BTPROP are significantly inferior to their mini-batch counterparts.\nA.4 Code Code implementing our experiments can be found at https://github.com/ facebookresearch/TPRNN"
    } ],
    "references" : [ {
      "title" : "How auto-encoders could provide credit assignment in deep networks via target propagation",
      "author" : [ "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1407.7906 .",
      "citeRegEx" : "Bengio.,? 2014",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2014
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "Stephen P. Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein." ],
      "venue" : "Foundations and Trends in Machine Learning 3(1):1–122.",
      "citeRegEx" : "Boyd et al\\.,? 2011",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2011
    }, {
      "title" : "Hashing with binary autoencoders",
      "author" : [ "Miguel Á. Carreira-Perpiñán", "Ramin Raziperchikolaei." ],
      "venue" : "CVPR. pages 557–566.",
      "citeRegEx" : "Carreira.Perpiñán and Raziperchikolaei.,? 2015",
      "shortCiteRegEx" : "Carreira.Perpiñán and Raziperchikolaei.",
      "year" : 2015
    }, {
      "title" : "Distributed optimization of deeply nested systems",
      "author" : [ "Miguel Á. Carreira-Perpiñán", "Weiran Wang." ],
      "venue" : "Proceedings of AISTATS. pages 10–19.",
      "citeRegEx" : "Carreira.Perpiñán and Wang.,? 2014",
      "shortCiteRegEx" : "Carreira.Perpiñán and Wang.",
      "year" : 2014
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Variational methods for the solution of problems of equilibrium and vibrations",
      "author" : [ "Richard Courant" ],
      "venue" : "Bull. Amer. Math. Soc 49(1):1–23.",
      "citeRegEx" : "Courant,? 1943",
      "shortCiteRegEx" : "Courant",
      "year" : 1943
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research 12(Jul):2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L. Elman." ],
      "venue" : "Cognitive Science 14(2):179–211.",
      "citeRegEx" : "Elman.,? 1990",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "A dual algorithm for the solution of nonlinear variational problems via finite element approximation",
      "author" : [ "Daniel Gabay", "Bertrand Mercier." ],
      "venue" : "Computers & Mathematics with Applications 2(1):17–",
      "citeRegEx" : "Gabay and Mercier.,? 1976",
      "shortCiteRegEx" : "Gabay and Mercier.",
      "year" : 1976
    }, {
      "title" : "Sur l’approximation, par éléments finis d’ordre un, et la résolution, par pénalisation-dualité d’une classe de problèmes de dirichlet non linéaires",
      "author" : [ "Roland Glowinski", "A Marroco." ],
      "venue" : "Revue française d’automatique, informatique, recherche",
      "citeRegEx" : "Glowinski and Marroco.,? 1975",
      "shortCiteRegEx" : "Glowinski and Marroco.",
      "year" : 1975
    }, {
      "title" : "Multiplier and gradient methods",
      "author" : [ "Magnus R Hestenes." ],
      "venue" : "Journal of optimization theory and applications 4(5):303–320.",
      "citeRegEx" : "Hestenes.,? 1969",
      "shortCiteRegEx" : "Hestenes.",
      "year" : 1969
    }, {
      "title" : "Learning invariant features through topographic filter maps",
      "author" : [ "Koray Kavukcuoglu", "Marc’Aurelio Ranzato", "Rob Fergus", "Yann LeCun" ],
      "venue" : "In CVPR. IEEE",
      "citeRegEx" : "Kavukcuoglu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kavukcuoglu et al\\.",
      "year" : 2009
    }, {
      "title" : "On large-batch training for deep learning: Generalization gap and sharp minima",
      "author" : [ "Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang." ],
      "venue" : "arXiv:1609.04836 .",
      "citeRegEx" : "Keskar et al\\.,? 2016",
      "shortCiteRegEx" : "Keskar et al\\.",
      "year" : 2016
    }, {
      "title" : "A cost function for internal representations",
      "author" : [ "Anders Krogh", "CI Thorbergsson", "John A Hertz." ],
      "venue" : "NIPS. pages 733–740.",
      "citeRegEx" : "Krogh et al\\.,? 1989",
      "shortCiteRegEx" : "Krogh et al\\.",
      "year" : 1989
    }, {
      "title" : "Modèles connexionnistes de l’apprentissage",
      "author" : [ "Yann Le Cun." ],
      "venue" : "Ph.D. thesis, Paris 6.",
      "citeRegEx" : "Cun.,? 1987",
      "shortCiteRegEx" : "Cun.",
      "year" : 1987
    }, {
      "title" : "Learning processes in an asymmetric threshold network",
      "author" : [ "Yann LeCun." ],
      "venue" : "Springer-Verlag, pages 233– 240.",
      "citeRegEx" : "LeCun.,? 1986",
      "shortCiteRegEx" : "LeCun.",
      "year" : 1986
    }, {
      "title" : "Theoretical framework for backpropagation",
      "author" : [ "Yann LeCun." ],
      "venue" : "Proceedings of the 1988 Connectionist Models Summer School. Morgan Kaufmann, CMU, Pittsburgh, Pa, pages 21–28.",
      "citeRegEx" : "LeCun.,? 1988",
      "shortCiteRegEx" : "LeCun.",
      "year" : 1988
    }, {
      "title" : "Difference target propagation",
      "author" : [ "Dong-Hyun Lee", "Saizheng Zhang", "Asja Fischer", "Yoshua Bengio." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 498–515.",
      "citeRegEx" : "Lee et al\\.,? 2015",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2015
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini." ],
      "venue" : "Computational linguistics 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafit", "L. Burget", "J. Cernock", "S. Khudanpur." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Dynamic factor graphs for time series modeling",
      "author" : [ "Piotr Mirowski", "Yann LeCun." ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases (ECML/PKDD). Springer, ISBN:978-3-642-041730, volume 5782, pages 128–143.",
      "citeRegEx" : "Mirowski and LeCun.,? 2009",
      "shortCiteRegEx" : "Mirowski and LeCun.",
      "year" : 2009
    }, {
      "title" : "Numerical optimization, second edition",
      "author" : [ "Jorge Nocedal", "Stephen J Wright." ],
      "venue" : "Numerical optimization pages 497–528.",
      "citeRegEx" : "Nocedal and Wright.,? 2006",
      "shortCiteRegEx" : "Nocedal and Wright.",
      "year" : 2006
    }, {
      "title" : "A method for non-linear constraints in minimization problems”",
      "author" : [ "Michael JD Powell" ],
      "venue" : null,
      "citeRegEx" : "Powell.,? \\Q1967\\E",
      "shortCiteRegEx" : "Powell.",
      "year" : 1967
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Training neural networks without gradients: A scalable ADMM approach",
      "author" : [ "Gavin Taylor", "Ryan Burmeister", "Zheng Xu", "Bharat Singh", "Ankit Patel", "Tom Goldstein." ],
      "venue" : "Proceedings of ICML. pages 2722–2731.",
      "citeRegEx" : "Taylor et al\\.,? 2016",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 2016
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "Paul J Werbos." ],
      "venue" : "Proceedings of the IEEE 78(10):1550–1560.",
      "citeRegEx" : "Werbos.,? 1990",
      "shortCiteRegEx" : "Werbos.",
      "year" : 1990
    }, {
      "title" : "On multiplicative integration with recurrent neural networks",
      "author" : [ "Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov." ],
      "venue" : "NIPS. pages 2856–2864.",
      "citeRegEx" : "Wu et al\\.,? 2016",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Modern RNNs are trained almost exclusively using truncated Back-Propagation Through Time (BPTT) (Elman, 1990; Werbos, 1990; Mikolov et al., 2010).",
      "startOffset" : 96,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "Modern RNNs are trained almost exclusively using truncated Back-Propagation Through Time (BPTT) (Elman, 1990; Werbos, 1990; Mikolov et al., 2010).",
      "startOffset" : 96,
      "endOffset" : 145
    }, {
      "referenceID" : 19,
      "context" : "Modern RNNs are trained almost exclusively using truncated Back-Propagation Through Time (BPTT) (Elman, 1990; Werbos, 1990; Mikolov et al., 2010).",
      "startOffset" : 96,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "Inspired by the recent reports of success of Target-Propagation (TPROP) in training nonrecurrent deep networks (Carreira-Perpiñán and Wang, 2014; Lee et al., 2015; Taylor et al., 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009).",
      "startOffset" : 111,
      "endOffset" : 184
    }, {
      "referenceID" : 17,
      "context" : "Inspired by the recent reports of success of Target-Propagation (TPROP) in training nonrecurrent deep networks (Carreira-Perpiñán and Wang, 2014; Lee et al., 2015; Taylor et al., 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009).",
      "startOffset" : 111,
      "endOffset" : 184
    }, {
      "referenceID" : 24,
      "context" : "Inspired by the recent reports of success of Target-Propagation (TPROP) in training nonrecurrent deep networks (Carreira-Perpiñán and Wang, 2014; Lee et al., 2015; Taylor et al., 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009).",
      "startOffset" : 111,
      "endOffset" : 184
    }, {
      "referenceID" : 20,
      "context" : ", 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009).",
      "startOffset" : 107,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpiñán and Wang, 2014; Lee et al., 2015).",
      "startOffset" : 177,
      "endOffset" : 303
    }, {
      "referenceID" : 16,
      "context" : "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpiñán and Wang, 2014; Lee et al., 2015).",
      "startOffset" : 177,
      "endOffset" : 303
    }, {
      "referenceID" : 13,
      "context" : "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpiñán and Wang, 2014; Lee et al., 2015).",
      "startOffset" : 177,
      "endOffset" : 303
    }, {
      "referenceID" : 0,
      "context" : "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpiñán and Wang, 2014; Lee et al., 2015).",
      "startOffset" : 177,
      "endOffset" : 303
    }, {
      "referenceID" : 3,
      "context" : "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpiñán and Wang, 2014; Lee et al., 2015).",
      "startOffset" : 177,
      "endOffset" : 303
    }, {
      "referenceID" : 17,
      "context" : "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpiñán and Wang, 2014; Lee et al., 2015).",
      "startOffset" : 177,
      "endOffset" : 303
    }, {
      "referenceID" : 9,
      "context" : "It is natural to minimize the now-unconstrained loss (7) using an Alternating Direction Method of Multipliers (ADMM) style approach (Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011), which results in the following meta-algorithm:",
      "startOffset" : 132,
      "endOffset" : 205
    }, {
      "referenceID" : 8,
      "context" : "It is natural to minimize the now-unconstrained loss (7) using an Alternating Direction Method of Multipliers (ADMM) style approach (Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011), which results in the following meta-algorithm:",
      "startOffset" : 132,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : "It is natural to minimize the now-unconstrained loss (7) using an Alternating Direction Method of Multipliers (ADMM) style approach (Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011), which results in the following meta-algorithm:",
      "startOffset" : 132,
      "endOffset" : 205
    }, {
      "referenceID" : 21,
      "context" : "The first, which we refer to as the Penalty Method (PM) (Courant et al., 1943; Nocedal and Wright, 2006), is identical to ADMM, except that it avoids the use of dual variables entirely, and so skips step 3.",
      "startOffset" : 56,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "The second, the Augmented Langrangian Method (ALM) (Hestenes, 1969; Powell, 1967; Nocedal and Wright, 2006), minimizes jointly over H, θ before updating the dual variables, effectively merging steps 1.",
      "startOffset" : 51,
      "endOffset" : 107
    }, {
      "referenceID" : 22,
      "context" : "The second, the Augmented Langrangian Method (ALM) (Hestenes, 1969; Powell, 1967; Nocedal and Wright, 2006), minimizes jointly over H, θ before updating the dual variables, effectively merging steps 1.",
      "startOffset" : 51,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "The second, the Augmented Langrangian Method (ALM) (Hestenes, 1969; Powell, 1967; Nocedal and Wright, 2006), minimizes jointly over H, θ before updating the dual variables, effectively merging steps 1.",
      "startOffset" : 51,
      "endOffset" : 107
    }, {
      "referenceID" : 26,
      "context" : "Exceptions to this include various forms of multiplicative RNNs (Wu et al., 2016) with no non-linearity.",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "2 For all experiments we use single-layer Gated Recurrent Unit (GRU) RNNs (Cho et al., 2014) (without Dropout (Srivastava et al.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 23,
      "context" : ", 2014) (without Dropout (Srivastava et al., 2014)), and we use Adagrad (Duchi et al.",
      "startOffset" : 25,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : ", 2014)), and we use Adagrad (Duchi et al., 2011) for optimization.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, it is clear from the table that both Batch BTPROP and Batch BPTT are significantly outperformed by their minibatch counterparts, presumably due to the regularization effect of updating parameters after seeing only a subset of the data (see Keskar et al. (2016) for a discussion of this phenomenon).",
      "startOffset" : 253,
      "endOffset" : 274
    }, {
      "referenceID" : 11,
      "context" : "Indeed, many of the reported successes of TPROP-style training have involved very constrained problems, such as those with binary hidden-state constraints (CarreiraPerpiñán and Raziperchikolaei, 2015) or sparsity constraints (Kavukcuoglu et al., 2009).",
      "startOffset" : 225,
      "endOffset" : 251
    } ],
    "year" : 2017,
    "abstractText" : "While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.",
    "creator" : "LaTeX with hyperref package"
  }
}