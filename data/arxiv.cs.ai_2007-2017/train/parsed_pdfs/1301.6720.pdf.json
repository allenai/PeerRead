{
  "name" : "1301.6720.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Solving POMDPs by Searching the Space of Finite Policies",
    "authors" : [ "Nicolas Meuleau", "Kee-Eung Kim", "Leslie Pack Kaelbling" ],
    "emails" : [ "@cs.brown.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Solving partially observable Markov decision processes (POMDPs) is highly intractable in gen eral, at least in part because the optimal policy may be infinitely large. In this paper, we ex plore the problem of finding the optimal policy from a restricted set of policies, represented as finite state automata of a given size. This prob lem is also intractable, but we show that the com plexity can be greatly reduced when the POMDP and/or policy are further constrained. We demon strate good empirical results with a branch-and bound method for finding globally optimal deter ministic policies, and a gradient-ascent method for finding locally optimal stochastic policies.\n1 INTRODUCTION\nIn many application domains, the partially observable Markov decision process (POMDP) [1, 22, 23, 5, 9, 4, 13] is a much more realistic model than its completely observable counterpart, the classic MDP [11, 20]. However, the com plexity resulting from the lack of observability limits the application of POMDPS to dramatically small decision prob lems. One of the difficulties of the optimal-Bayesian solution technique is that the policy it produces may use the complete previous history of the system to determine the next action to perform. Therefore, the optimal policy may be infinite and we have to approximate it at some level to be able to implement it in a finite machine. Another problem is that the calculation requires reformulating the problem in the continuous space of belief functions, and hence it is much harder than the simple finite computation that is sufficient to optimize completely observable MDPs.\nWhat can we do if we have to solve a huge POMDP? Since it may be impossible just to represent the optimal pol icy in memory, it makes sense to restrict our search to policies that are reasonable in some way (calculable and\nstoreable in a finite machine). Knowing that any policy is representable as a (possibly infinite) state automaton, the first constraint we would want to impose on the pol icy is to be representable by a finite state automaton, or, as we will call it, a finite \"policy graph\". Many previous approaches implicitly rely on a similar· hypothesis: Some authors [ 14, 12, 2, 27] search for optimal reactive (or mem ory less) policies, McCallum [15, 16] searches the space of policies using a finite-horizon memory, Wiering and Schmidhuber's HQL [26] learns finite sequences of reactive policies, and Peshkin et al. [19] look for optimal fi�ite extemal-memory policies. All these examples are particu lar cases of finite policy graphs, with a set of extra structural constraints in each case (i.e., not every node-transition and action choice is possible in the graph). Note that in general, finite policy graphs can remember events arbitrarily far in the past. They are just limited in the number of events they can memorize.\nIn this paper we study the problem of finding the best policy representable as a finite policy graph of a given size, pos sibly with simple constraints on the structure of the graph. The idea of searching explicitly for an optimal finite pol icy graph for a given POMDP is not new. In the early 70s, Satia and Lave [21] proposed a heuristic approach for find ing <-optimal decision trees. Hansen [6, 7, 8] proposed a policy iteration algorithm that outputs an <-optimal con troller. These solution techniques work explicitly in the belief space used in the classical-Bayesian-optimal so lution of POMDPs, and they output policy-graphs which are not more than c from this optimal solution. Another ap proach uses EM to find controllers that are optimal over a finite horizon [10].\nA characteristic property of our algorithms is that they scale up well with respect to the size of the problem. Their draw back is that their execution time increases quickly with the size of the policy graph, i.e., with the complexity of the policy we are looking for. In general, they will be adapted to large POMDPs where relatively simple policies perform near optimally. Another characteristic of our approach is that we do not refer to the value of the optimal Bayesian\n418 Meuleau, Kim, Kaelbling, and Cassandra\nsolution anymore, we just want the best graph given the constraint imposed on the number of nodes. Note that the optimality criterion used is the same as in the Bayesian ap proach, i.e., the expected discounted cumulative rewards (the expectation being relative to the prior belief on the states). However, since we do not evaluate the solution produced relative to the optimal peiformance, the problem may be solved without using the notion of belief-space.\nOur development relies on a basic property of finite-state controllers that has already been stressed by Hansen [6], and that is also very close to Parr and Russell's HAM theo rem [18]. Namely, given a PO MOP and a finite policy graph, the sequence of pairs (state of the PO MOP, node of the pol icy graph) constitutes a Markov chain. Going farther, we define a new MOP on the cross product of the state-space and the set of nodes, where a decision is the choice of an action and of a next node (conditioned on the last obser vation). Working in this cross-product MOP presents many advantages: it allows us to calculate and then differentiate the value of a fixed policy, to calculate upper and lower bounds on the value attainable by completing a given par tial policy, and also to establish some complexity results. We use these properties to develop implementations of two classic search procedures (one global and one local) where the majority of the computation consists of solving some Bellman equations in the cross-product MOP. An important point is that the structure of both the POMOP and the pol icy graph (if there is one) is reflected in the cross-product MOP. It can be used to accelerate the solution of Bellman equations [3], and hence the execution of the solution tech niques. In other words, the algorithms we propose can ex ploit the structure of the PO MOP to find relatively quickly the best general finite policy graph of a given size. If this leverage is not sufficient, we may limit further the search space by imposing some structure on the policy graph, and then using this structure to speed up the solution of the cross-product MOP (for instance, we can limit ourselves to one of the finite-memory architectures mentioned above).\nThe paper is organized as follows. First we give a quick introduction to POMOPs and policy graphs, and define the cross-product MOP. Then we show that finding the best deterministic finite policy graph is an NP-hard problem. There is then no really easy way to solve our problem. In this paper, we propose two possible approaches: a global branch and bound search for finding the best determinis tic policy graph, and a local gradient descent search for finding the best stochastic policy graph. These two algo rithms are based on solving some Bellman equations in the cross-product MOP. Therefore, they can take full advan tage of any preexisting structure in the POMOP or in the policy graph. 'JYpically, these algorithms will be adapted to very structured POMOPs with a large number of states, a small number of observations, and such that some sim ple policies perform well. In the end of the paper, we give\nempirical evidence that our approach allows the solution of some POMDPs whose size is far beyond the limits of clas sical solutions.\n2 POMDPS AND FINITE POLICY GRAPHS\n2.1 POMDPS\nA partially observable Markov decision process (POMOP) is defined as a tuple ( S, 0, A, B, T, R) where:\n• Sis the (finite) set of states;\n• 0 is the (finite) set of observations;\n• A is the (finite) set of actions;\n• B(s, o) = Pr(ot = o I st = s) for all t;\n• T(s, a, s') = Pr(st+1 = s' I st = s, at =a) for all t;\n• rt = R(s, a, s') if st = s, at = a and st+l = s', for all t.\nThe underlying Markov decision process (MOP) ( S, A, T, R) is optimized is the following way [ 11, 20]: given an initial state s0, the aim is to maximize the expected discounted cumulative reward\nwhere 'Y E [0, 1) is the discount factor. The optimal so lution is a mapping p* : S -+ A that specifies the action to perform in each possible state. The optimal expected discounted reward, or \"value function\", is defined as the unique solution of the set of Bellman equations:\nV*(s) = m ax [2::: T(s, a, s') (R(s, a, s') + -yV*(s'))] , aEA •'ES\nfor all s. It is a remarkable property of MOPs that there exists an optimal policy that always executes the same ac tion in the same state. Unfortunately, this policy cannot be used in the partially observable framework, because of the residual uncertainty on the current state of the process.\nIn the PO MOP framework, a policy is in general a rule spec ifying the action to perform at each time step as a function of the whole previous history, i.e., the complete sequence of observation-action pairs since time 0. A particular kind of policy, the so-called reactive policies (RPs ), condition the choice of the next action only on the previous observation. Thus, they can be represented as mappings p : 0 -+ A. Given a probability distribution 1r0 over the starting state,\nSolving POMDPs by Searching the Space of Finite Policies 419\n�igure � : . structure of the policy graphs representing reac tive policies (101 = 3). The only degrees of freedom are the choices of the action in the 101 nodes.\neach policy p. (reactive or not) realizes an expected cumu lated reward:\n(1)\nThe classical-Bayesian-approach allows us to determine the policy that maximizes this value. It is based on updating �he state distribution (or belief) at each time step, depend Ing on the most recent observations [5, 9, 4, 13]. The prob lem is re-formulated as a new MDP using belief-states in stead of the original states. Generally, the optimal solution is not a reactive policy. It is a sophisticated behavior with optimal balance between exploration and exploitatio�. Un fortunately, the Bayesian calculation is highly intractable as it searches into the continuous space of beliefs and con siders every possible sequence of observations.\n2.2 FINITE POLICY GRAPHS\nA policy graph for a given POMDP is a graph where the nodes are labeled with actions a E A, the arcs are labeled with observations o E 0, and there is one and only one arc emanating from each node for each possible observa tion. W hen the system is in a certain node, it executes the action associated with this node. This implies a state transi tion in the POMDP and eventually a new observation (which depends on the arrival state of the underlying MDP). This observatio� its�lf conditions a transition in the policy graph to the destmatlon node of the arc associated with the new observation. Every policy has a representation as a possi bly (countably) infinite policy graph. A policy that chooses a different action for each possible previous history will be repre�ented by an infinite tree with a branch for each possi ble history. Reactive policies correspond to a special kind of finite policy graph with as many nodes as there are ob servations in the POMDP, and where all arcs labeled with the same observation go to the same node (figure 1).\nI� a stochastic policy graph there is a probability distribu tlo� over actions attached to each node instead of a single actlon, and transitions from one node to another are ran dom, the arrival node depending only on the starting node\n�d the last observation. We will use the following nota tion:\n• N is the set of nodes of the graphs,\n• n1 E N is the current node at timet,\n• .,P(n, a) is the probability of choosing action a in node nEN:\n.,P(n, a) �r Pr(a1 =a I n1 = n), for all t,\n• TJ( n, o, n') is the probability of moving from node n E N to node n' E N, after observation o E 0:\nTJ(n, o, n') �r Pr(n1+1 = n' I n1 = nl\\o1+1 = o), for all t.\n• 7]0 is the probability distribution of the initial node n° conditioned on the first observation o0:\nTJ0(o, n) �r Pr(n° = n I o0 = o).\nIn some cases, we will want to impose extra constraints on the policy graph. In most of this paper, we will limit our selves to \"restriction constraints\" which consist in restrict ing the set of possible actions executable in some nodes, and/or restricting the set of possible successors of some nodes under some observations. Note that forcing the graph to implement an RP represents a set of restriction constraint as defined here. We consider more sophisticated sets of constraints in section 4.\n2.3 THE CROSS-PRODUCT MDP\nOne advantage of representing the policy as a policy graph is that the cross-product of the POMDP and the policy graph is itself a finite MDP. Another interesting point is that all the structure of both the POMDP and the policy graph (if there is some) is represented in this cross-product MDP. It will allow us to develop relatively fast implementations of some classical techniques to solve our problem.\nCalculating the value of a policy graph The following theorem has been used by Hansen [6, 8, 7], and his closely related to Parr and Russell's HAM theorem [18].\nTheorem 1 Given a policy graph p. = ( .,P, TJ) and a POMDP (S, 0, A, B, T, R), the sequence of node-state pairs ( n1, st) generated constitutes a Markov chain.\nThe influence diagram of figure 2 proves this property: (n1+�, s1+1) depends only on (n1, s1). The transition ma trix TJJ. of this Markov chain is given by\nTP.((n, s) , (n', s')) = L 1/l(n, a)T(s, a, s') L B(s', o)7J(n, o, n'). (2)\naEA oEO\n420 Meuleau, Kim, Kaelbling, and Cassandra\nThen the value function of the policy J.l is found by solving the fundamental equation (in matrix form):\n(4)\nSince T�-' is a stochastic matrix and 1 < 1, the matrix (I - 1T�-') is invertible and we have:\n(5)\nFinally, the value of the policy, independent of the starting node and state, is\n(6)\nwhere 7r0 is the joint probability distribution on n° and s0:\nir0(n, s) = 1r0(s) L B(s, o)1J0(o, n). oeo\nDifferentiating this value with respect to the parameters of the graph will enable us to climb its gradient.\nSolving the cross-product MDP. A complete MOP is de fined on S = N x S. In each pair (n, s), we have to choose an action a, wait for the new observation, and then chose the next node. It is equivalent to choosing an action and a mapping T)n : 0 -+ N which determines the next node as a function of the next observation. Therefore, the action space of the cross-product MOP is A = A x N°. Theorem 2 The tuple (S, A, T, R) is a finite stationary Markov decision process, where S = N x S, A = Ax N°, T((n, s), (a, 1Jn), (n', s)) = T(s, a, s')\noEO s.t. qn(o)=n'\nand R( (n, s), (a, 1)\" ) , (n', s')) = R(s, a, s').\nB(s', o)\nThe fundamental equation of the MDP is, in matrix form:\nv· =max max [c + 1f'v•], aEA qneNo\nwhere the maximization is applied row by row. When we expand this equation, the maximization over 1Jn E N° can be replaced by a maximization over n' E N and moved to the end of the equation:\nV*(n, s) = Tea.f[L T(s, a, s') ( R(s, a, s') 11ES\n+ 1 � B(s', o) max V*(s', n'l) ] . L....ti n'EN oEO (7)\nThe expected optimal reward, independent of the starting state and node, is E* = 7r0 v•. The stationary optimal policy of the cross-product MOP is a mapping [I* : S -+ A. Note that this optimal policy is generally not imple mentable in the policy graph, since it may associate two different actions with the same node, depending on the state with which the node is coupled. In other words, we need to know the current state to use this policy. The agent using a policy graph is basically embedded in the cross-product MOP, but it has only partial observability of its product state (nt, st): it sees nt but not st. The cross-product MDP is in fact a PO MOP (S, 0, A, B, T, R) where 0 = N and B is the projection of N x S on N. However, the solution of the fundamental equation (7) is useful in some algorithms, be cause it represents an upper-bound of the performance at tainable by any implementable policy. We will use this in a branch-and-bound algorithm for finding the optimal deter ministic policy graph. Note that the addition of restriction constraints on the policy, as defined in section 2.2, does not invalidate theorem 2. It just limits the set of possible ac tions in some states of the cross-product MOP, and then it reduces the complexity of its solution. As a consequence, the branch-and-bound algorithm will also be able to find the best graph under some restriction constraints.\nComputational leverage. It is a very important point that most of the computation performed by the algorithms that will follow consists of solving a Bellman fundamen tal equation with a fixed policy as in ( 4 ), or for the sake of finding the optimal deterministic policy as in (7). This can be done by successive approximations, the algorithm being called \"value iteration\" in the case of (7). The complexity of the algorithm is O(ISI2IOIIAI) = O(INI2ISI2IOIIAI) (times the number of iterations, which can be O(ISI)). The important point is that any structure in the transition matrix T can be exploited while executing these back-ups. The structure ofT has two components:\nthe structure of the POMOP: A sparse transition matrix T of the POMOP provides leverage that allows the speed-up of successive-approximation iterations [3]. If K < lSI is the branching factor of the PO MOP (i.e.,\nSolving POMDPs by Searching the Space of Finite Policies 421\nthe average number of possible successors of a state) then the complexity can be reduced to O(INI2 KISI). For instance, deterministic transitions reduce the com plexity by a factor of lSI. In the same way, a sparse observation matrix is exploitable. For instance, deter ministic observations reduce the complexity by a fac tor of 101.\nthe structure of the policy graph: If the leverage gained from the structure of the POMDP is not sufficient, then one can choose to restrict further the search space by imposing structural constraints on the graph, and using this structure to speed up the calculation. An extreme, but often adopted, solution is to look for the best RP. In this case, the gain is a factor of 101 (the complexity is O(IOI2ISI2IAI) instead of O(INI2ISI2IOIIAI) = O(IOI3ISI2IAI)). We will say more about constraining the policy graph in section 4.\nWhen both the problem and the policy are structured, the leverage gained can be bigger than just the addition of the effect of the two structures. For instance, evaluating a RP in a completely deterministic problem can be done in O(ISIIAI) instead of O(INIISIIAI) = O(IOIISIIAI).\n3 FINDING THE OPTIMAL POLICY GRAPH\nIn this paper, we consider the problem of finding the best policy graph of a given size for a POMDP. Littman [14) showed that finding the best RP for a given POMDP is an NP-hard problem. First, we generalize this result to any finite policy graph with a given number of nodes and any set of restriction constraints.\nTheorem 3 Given a POMDP and a set of restriction con straints, the problem of finding the optimal deterministic policy graph satisfying the constraints is NP-hard.\nThe proof is straightforward: Finding the best deterministic policy graph is equivalent to finding the best deterministic RP of the cross-product POMDP. Then the result follows from Littman's theorem.\nThe techniques for solving NP-hard problems may be clas sified into three groups: global search, local search and approximation algorithms. In this paper, we will use two classic techniques, a global search (section 3.1) and a local search (section 3.2). We will consider a possible approxi mation algorithm in section 3.3.\n3.1 GLOBAL SEARCH\nA heuristically guided search is used to find the best deter ministic policy graph of a given size, whatever the restric tion constraints imposed on 1/J (actions) and 7J (structure).\nIt is a branch-and-bound algorithm; i.e., it systematically enumerates the set of all possible solutions using bounds on the quality of partial solutions to exclude entire regions of the search space. If the lower bound of one partial policy is greater than the upper bounds of others, then it is useless explore these partial policies. Otherwise, each possible ex tension of them will considered in time. Therefore, the al gorithm is guaranteed to find the optimal solution in finite time. Note that this approach is a generalization of a pre vious algorithm proposed by Littman [14]. His algorithm is limited to policy graphs representing RPs and to POMDPs with a very particular structure: state-transitions and obser vations are deterministic, and the problem is an achieve ment task (i.e., there is a given goal state that must be reached as soon as possible). The formalism proposed here handles any kind of POMDP and any kind of policy graph with restriction constraints. However, Littman gives more details on some aspects of the algorithm, and the reader can refer to his paper to complete the brief description that we give here."
    }, {
      "heading" : "Ordering of free parameters. The tree of all possible",
      "text" : "policies is expanded (in depth-first, breadth-first, or in a best-first way) by picking the free parameters of the pol icy one after the other, and considering all possible assign ment values for each of them. The game of pruning some branches based on upper bound/lower bounds comparison is added onto that. The size of the tree that is actually ex panded in this process strongly depends on the order in which the free parameters are picked. In our case, it is important that the free values of 1/; come before the free values of 1). In other words, when building a solution, we assign actions to the nodes first, and then we fix the struc ture of the graph. Otherwise, no pruning is possible before all possible structures have been expanded (this is due to the nature of the upper-bound that we use, see below). In our implementation, the parameter 7]0 is expanded after 1/J but before 1). There is also an issue with the symmetry of the policy-graph space. For instance, in the absence of re striction constraints, we can permute the role played by the different nodes without changing the policy. Each policy graph is then represented by IN I! leaves of the tree. We can avoid enumerating equivalent graphs by imposing some ar bitrary rule when expanding the tree. For instance we can impose that the index of the action attached to a node i al ways be greater or equal to the index of the action of node i + 1, for all i. This simple trick can improve greatly the performance of the heuristic search, merely dividing the execution time by IN I!.\nUpper bounds. A partial solution is a general finite pol icy graph with more restriction constraints than initially (each time we specify an action or a node-transition, we add a constraint). Then we can get an upper bound by solv ing the cross-product MDP, as explained in the second part\n422 Meuleau, Kim, Kaelbling, and Cassandra\nof section 2.3, and taking the product ir0V*. A completely specified policy graph corresponds to an RP of the cross product (PO )MDP, so no policy graph can do better than the optimal solution of the cross-product MDP. Note that this upper bound has a nice monotonicity property: it does not increase when we fix a free parameter, and it is equal to the true value of the policy graph when the graph is com pletely specified. On the other hand, as long as no value of 1j; is specified, the optimal policy found by solving the cross-product MDP is equivalent to the optimal policy of the original MDP (S, A, T, R): the choice of the action in each (n, s) is independent of nand depends only on s. Hence, the calculated upper bound is always equal to the value of the optimal policy of the cross-product MDP. This is why no pruning can be done as long as no value of 1j; has been specified and this parameter must be the considered first when expending the tree.\nLower bounds. If the algorithm searches in depth-first order, then we can use the values of the complete poli cies already expanded to determine lower bounds on the best performance attainable by extending each partial pol icy. Otherwise, we can find a lower bound for a given par tial policy by completing it at random and calculating the value of the resulting complete policy. An improvement consists of performing a simple local optimization after having completed the policy [14]. In our implementation, we also used a heuristic technique based on the solution of the cross-product MOP to complete the partial policy. We calculate the performance of a complete policy by solving equation (4) in the cross-product MDP.\nComplexity. The calculation of the upper and lower bounds of each node of the expanded tree requires solving some Bellman equations in the cross-product MDP. Hence it can be done in O(INI2ISI2), or less if there is some structure in the POMDP or the policy. To reduce the num ber of iterations of successive approximation executed dur ing this calculation, one can store, with each partial policy, the value function found when calculating its upper bound. Then we can start the computation of the upper bound of a child partial policy starting from the value of its parent. Since they are often not very different, we can gain a lot of time with this trick. However, the memory space needed in creases dramatically. Even if we can calculate the bounds relatively quickly, the real problem is how many nodes it will be necessary to expand before reaching the optimum. In the worst case, the complete tree of all possible solu tions will be expanded, which represents a complexity ex ponential in the number of degrees of freedom of the policy graph. In practice, our simulations showed that many fewer nodes are actually expanded. Note that adding simple con straints on the policy reduces not only the complexity of the solution of the cross-product MDP, but also the size of the search space and hence the number of nodes expanded.\n3.2 LOCAL SEARCH\nIn this approach, we try to find the best stochastic policy graph by treating this problem as a classical non-linear nu merical optimization problem. Since the value of a policy graph is continuous and differentiable with respect to the policy parameters, we can calculate its gradient and climb it in many different ways. We will not develop all the pos sibilities for climbing the gradient here, but we will rather focus on the calculation of the gradient, and then just de pict a simple implementation of gradient ascent. Note that since the gradient may be calculated exactly, this approach is guaranteed to converge to a local optimum. The topology of the search space, and hence the number of local optima, depends on two things: the structure of the POMDP at han<i, and the constraints imposed on the policy. By introducing constraints on the policy, we can hope not only to reduce the execution time of the algorithm, but also to change the \"landscape\" for a less multimodal one.\nCalculating the gradient. The value E�' of a policy graph I' is given by equation (6). For each policy parameter x we have oE�' fox= ir08V�' fox. The value function V�' is given by (5). Hence we have\nWe are interested in the gradient with respect to the pol icy parameters, i.e., we will consider x = 1/;(n, a) and x = I)( n, o, n'). The partial derivative ofT�' and C�' with respect to these variables can be calculated easily starting from (2) and (3). The main difficulty in the calculation of the gradient is inverting the matrix (I- 7T�'). If we want to exploit the structure of T�', we can do it by successive approximation, the basic update rule being:\n(8)\nwhere W is an IN I lSI x IN I lSI matrix. Without any useful structure in 'f'J.I, the complexity of a complete back up is then in O(ISI3IOIIAI). Once the matrix is inverted, the inverse can be used to calculate the gradient with respect to any parameter x. A minor acceleration can be obtained by using the value of (I -7T�') -1 at the previous point to start the iterative computation of this value at the new point. It can reduce the number of iterations of (8) at each step, but it is still a matrix-wise DP with a complexity in O(ISI3), and hence in O(ISI3).\nThere is another way of computing the gradient with a com plexity only in O(ISI2). Instead of performing the matrix wise DP to calculate (I - 7T�')-1 explicitly, we perform several (classical) vector-wise DPs for which complexity is in O(ISI2), or less if there is some structure in N or S. First we compute V�' by solving (4), which implies a vector-wise DP with complexity in O(ISI2IOIIAI). Then\n--;\nSolving POMDPs by Searching the Space of Finite Policies 423\nwe calculate\nv �r 8C�' 81'�' V�'\n1- 8x +i 8x ·\nAt last, we get 8V JJ I 8x = (I - ;TJJ) -1 v1 by iterating\nv2 r- v1 + ,t�' v2,\nwhich is also a vector-wise, square-complexity DP. The total complexity of this calculation is 0(2ISI2IOIIAI) (ne glecting the calculation of V2). Unfortunately, the calcula tion must be re-done for each policy parameter X, since V1 and V2 depend On X. Thus, we have divided the complex ity by a factor of lSI, but multiplied it by the number of free variables of the graph. However, this approach will be useful in most cases, since there are often many fewer free variables in the policy graph than \"cross-product states\". For instance, if we are looking for the best reactive policy, then the indirect calculation allows us to gain a factor of ISIIIAI-\nClimbing tbe gradient. Climbing the gradient consists of updating each free value x with the rule x f-- x + {38 E I 8x, where f3 is the step-size parameter. In our case the problem is somewhat more complicated since all the parameters that we optimize are probabilities and we have to ensure that they stay valid (i.e., inside of the simplex) after each update. There are numerous ways for doing that, including renormalizing and using the soft-max function. In our implementation, we chose to project the calculated gradient on the simplex, and then apply it until we reach an edge of the simplex. If we reach an edge and the gradient points outside of the simplex, then we project the gradient on the edge before applying it.\nRelated work. The idea of using a gradient algorithm for solving POMDPs has already been pursued by several au thors [2, 12, 27]. The main difference between this work and ours is that these authors use a Monte-Carlo estima tion of the gradient instead of an exact calculation, and that they limit themselves to RPs, which is much less general than our approach. Moreover, Jaakkola et al. do not use the exponentially discounted criterion (I), but the average re ward per time step. In a companion paper [ 17], we propose a stochastic gradient descent approach for learning finite policy graph during a trial-based interaction with the pro cess.\n3.3 OI'HER APPROACHES\nA Monte-Carlo approach based on Watkins'Q-learning [25, 24] is also applicable to our problem. For instance, we can an use Q-learning based on observation-action pairs to find (with no guarantee of convergence) the optimal RP for a POMDP [14]. Another instance is Wiering and Schmid huber's HQL [26], which learns finite sequences of RPs.\nlui\nFigure 3: The load/unload problem with 8 locations: the agent starts in the \"Unload\" location (U) and receives a re ward each time it returns to this place after passing through the \"Load\" location (L). The problem is partially observ able because the agent cannot distinguish the different lo cations in between Load and Unload, and because it can not perceive if it is loaded or not (lSI = 14, 101 = 3 and IAI = 2).\nThis Monte-Carlo approach works only if there are strong structural constraints on the graph, and thus cannot be ap plied for finding general finite policy graphs. Note also that Littman reported observing a great superiority (in terms of execution time) of the global branch-and-bound search over the Monte-Carlo approach, in the case where the graph is constrained to encode a simple RP. Our simulations with other architectures (sets of structural constraints) showed similar results: in general, the Monte-Carlo approach can not compete with the two others.\n4 INTRODUCING STRUCTURAL CONSTRAINTS\nBecause the majority of their computation is to perform Bellman back-ups in the cross-product MDP, the algorithms outlined above can take advantage of any preexisting struc ture in the POMDP. However, this leverage can be insuffi cient if the problem is too big or too difficult for the two techniques. In this case, one may whish to restrict further the search space by imposing structural constraints on the policy graph. For instance, a simple solution consists of defining a neighborhood for the nodes of the graph, and allowing transitions only to a neighboring node. This cor responds to a set of restriction constraints ( 7J is forced to take the value zero in many points), and hence the algo rithm above can still be applied. A somehow extreme so lution consists of limiting the search to reactive policies (then 7J is completely fixed in advance). More complex sets of constraints can also be used, for instance, we can limit the search to policy representable as a finite sequence of RPs (with particular rules governing the transition from one RP to another), as in Wiering and Schmidhuber's HQL [26]. Other instances include the finite-horizon memory policies used by McCallum [15, 16], or the external-memory poli cies used by Peshkin et al. [19]. Although these architec tures cannot be described only in terms of restriction con straints (there are also equality constraints between differ ent parameters of the graph), the previous results and al gorithm can be extended to each of them in particular. In other words, we can use the previous algorithm to find\n• the best RP-sequence of a given length,\n424 Meuleau, Kim, Kaelbling, and Cassandra\n• the best policy using a given finite-horizon memory,\n• the best policy using an external-memory of a given size.\n(we can also show that it is NP-hard to solve these prob lems).\nWhat do we gain and what do we lose when we impose a structure on the policy graph? In general, imposing structural constraints reduces the number of parameters per node (which should help both techniques), and modifies the topology of the search space (which influences the gradi ent descent approach). Another point is that the best graph without the constraints can be better than the best graph with the constraints, i.e., the constraints can decrease the value of the best solution. Even if this does not happen, more nodes may be required to reach the best performance with the constraints than without. Consider, for instance, the load/unload problem represented in figure 3. This sim ple problem is solved optimally with a two-node policy graph, or with a sequence of two RPs as used in HQL. As an RP is encoded with an IOI-node graph, any sequence of two RPs will be encoded by at least 2101 = 6 nodes. However, the number of parameters per node will be smaller than in the unconstrained case. In general, adding structure will be interesting if we choose an architecture that fits the prob lem at hand. Hence, it is a question of previous knowledge about the problem at hand and its optimal solution.\n5 SIMULATION RESULTS\nIn our first experiments, we used the simple load/unload problem of figure 3 with an increasing number of loca tions, to see how both algorithms scale up to increasing problem-size, and how they compare. Since it is a very\neasy POMDP, the results obtained represent a kind of upper bound on the performance of the algorithms. It is unlikely that they will perform better on another (harder) problem. During this experiment IN I was set to its optimal value of 2 and the gradient algorithm always started from the center of the simplex (i.e., the policy graph is initialized with uni form distributions).1 We measured the time of execution of each algorithm, as a function of lSI. In the case of gradient ascent, we stopped when we reached 99% of the optimal. When the heuristic search uses a stochastic calculation of upper bounds, we average the measure over 50 runs. -y was set to 0.996 (a big value is necessary to accommodate big state-spaces), and the learning rate of gradient descent was optimized. The results are given in figure 4. They show that the heuristic search clearly outperforms the gradient algorithm, which becomes numerically unstable when the number of states increases in this kind of geometrically discounted absorbing problem.\nIn the second set of experiments, we wanted to measure how far our algorithms can go in terms of problem size, in a problem more difficult than the simple load/unload. We used the a set of partially observable mazes with the regular structure represented in figure 5, and whose size varies from 9 to 989 states (101 = 9 and lA I = 4). These mazes are not particularly easy, since they have only two different optimal paths. The minimal number of nodes for solving them is 4, one per action (although the policy is not reactive). The time required for the (depth first) branch and-bound algorithm to find the optimal solution with this optimal number of nodes is shown in figure 6. We see that\n1We used a simpler version of the algorithms where the start ing node is fixed. Otherwise, the policy using only uniform dis tributions is a (very unstable) local optimum.\nSolving POMDPs by Searching the Space of Finite Policies 425\nMaze Problem 3�>,------------------------------.\n200 400 600 Number of states\n800\nFigure 6: Performance of the branch-and-bound algorithm on the maze problem: execution time as a function of the number of states.\nwe can solve a partially observable maze with almost 1000 states in a less than 6 hours. It represents a performance far above the capacities of classic approaches for solving POMDPs. Note also that, as the number of states grows, the measured complexity is almost linear in the number of states.\n6 CONCLUSION\nWe studied the problem of finding the optimal policy rep resentable as a finite state automaton of a given size, pos sibly with some simple structural constraints. This ap proach by-passes the continuous and intractable belief-state space. However, we showed that we end up with a NP hard problem anyway. Then we proposed to use two classic search techniques, and developed efficient implementations of them that allow using the structure of the problem to ac celerate the computation. If this is not sufficient, bigger leverage can be gained by imposing structure on the policy. However, our algorithms are limited by necessity to enu merate at least once per iteration, the complete state space of the POMDP. In a companion paper [17], we propose an indirect learning algorithm that avoids this bottleneck.\nReferences\n[ 1] K.J. Astrom. Optimal control of Markov decision pro cesses with incomplete state estimation. J. Math. Ani. Appl., 10, 1965.\n[2] L.C. Baird and A.W. Moore. Gradient descent for general reinforcement learning. In Advances in Neu ral Information Processing Systems, 12. MIT Press, Cambridge, MA, 1999.\n[3] C. Boutillier, T.L. Dean, and S. Hanks. Decision the oretic planning: structural assumptions and computa-\ntional leverage. Journal of AI Research, To appear, 1999.\n[4] A.R. Cassandra. Exact and Approximate Algorithms for Partially Observable Markov Decision Processes. PhD thesis, Brown University, 1998.\n[5] A.R. Cassandra, L.P. Kaelbling, and M.L. Littman. Acting optimally in partially observable stochastic domains. In Proceedings of the Twelfth National Con ference on Artificial Intelligence, 1994.\n[6] E.A. Hansen. An improved policy iteration algorithm for partially observable MDPs. In Advances in Neu ral Information Processing Systems, IO. MIT Press, Cambridge, MA, 1997.\n[7] E.A. Hansen. Finite-Memory Control of Partially Observable Systems. PhD thesis, Department of Computer Science, University of Massachusetts at Amherst, 1998.\n[8] E.A. Hansen. Solving POMDPs by searching in pol icy space. In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, pages 211-219, Madison, WI, 1998.\n[9] M. Hauskrecht. Planning and Control in Stochas tic Domains with Imperfect Information. PhD thesis, MIT, Cambridge, MA, 1997.\n[10] 0. Higelin. Optimal Control of Complex Structured Processes. PhD thesis, University of Caen, France, 1999.\n[11] R.A. Howard. Dynamic Programming and Markov Processes. MIT Press, Cambridge, 1960.\n[12] T. Jaakkola, S. Singh, and M.R. Jordan. Rein forcement learning algorithm for partially observable Markov problems. In Advances in Neural Informa tion Processing Systems, 7. MIT Press, Cambridge, MA, 1994.\n[13] L.P. Kaelbling, M.L. Littman, and A.R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101, 1998.\n[14] M.L. Littman. Memory less policies: Theoretical lim itations and practical results. In From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior. MIT Press, Cambridge, MA, 1994.\n[15] R.A. McCallum. Overcoming incomplete perception with utile distinction memory. In The Proceedings of the Tenth International Machine Learning Confer ence, Amherst, MA, 1993.\n426 Meu1eau, Kim, Kae1bling, and Cassandra\n[16] R.A. McCallum. Reinforcement Learning with Selec tive Perception and Hidden State. PhD thesis, Univer sity of Rochester, Rochester, NY, 1995.\n[17] N. Meuleau, L. Peshkin, K.E. Kim, and L.P. Kael bling. Learning finite-state controllers for partially observable environments. Proceedings of the Fif teenth Conference on Uncertainty in Artificial Intel ligence, To appear, 1999.\n[ 18] R. Parr and S. Russell. Reinforcement learning with hierarchies of machines. In Advances in Neural In formation Processing Systems I I . MIT Press, Cam bridge, MA, 1998.\n[19] L. Peshkin, N. Meuleau, and L.P. Kaelbling. Learn ing policies with external memory. Proceedings of the Sixteenth International Conference on Machine Learning, To appear, 1999.\n[20] M.L. Puterman. Marlwv Decision Processes: Dis crete Stochastic Dynamic Programming. Wiley, New York, NY, 1994.\n[21] J.K. Satia and R.E. Lave. Markov decision processes with probabilistic observation of states. Management Science, 20(1):1-13, 1973.\n[22] R.D. Smallwood and E.J. Sondik. The optimal con trol of partially observable Markov decision processes over a finite horizon. Operations Research, 21:1071- 1098, 1973.\n[23] E.J. Sondik. The optimal control of partially observ able Markov decision processes over the infinite hori zon: Discounted costs. Operations Research, 26, 1978.\n[24] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.\n[25] C. Watkins. Learning from Delayed Rewards. PhD thesis, King's College, Cambridge, 1989.\n[26] M. Wiering and J. Schmidhuber. HQ-Leaming. Adap tive Behavior, 6(2):219-246, 1997.\n[27] R.J. Williams. Towards a theory of reinforcement learning connectionist systems. Technical Re port NU-CCS-88-3, Northeastern University, Boston, MA,1988."
    } ],
    "references" : [ {
      "title" : "Optimal control of Markov decision pro­ cesses with incomplete state estimation",
      "author" : [ "K.J. Astrom" ],
      "venue" : "J. Math. Ani. Appl.,",
      "citeRegEx" : "Astrom.,? \\Q1965\\E",
      "shortCiteRegEx" : "Astrom.",
      "year" : 1965
    }, {
      "title" : "Gradient descent for general reinforcement learning",
      "author" : [ "L.C. Baird", "A.W. Moore" ],
      "venue" : "In Advances in Neu­ ral Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1999
    }, {
      "title" : "Exact and Approximate Algorithms for Partially Observable Markov Decision Processes",
      "author" : [ "A.R. Cassandra" ],
      "venue" : "PhD thesis, Brown University,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1998
    }, {
      "title" : "Acting optimally in partially observable stochastic domains",
      "author" : [ "A.R. Cassandra", "L.P. Kaelbling", "M.L. Littman" ],
      "venue" : "In Proceedings of the Twelfth National Con­ ference on Artificial Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1994
    }, {
      "title" : "An improved policy iteration algorithm for partially observable MDPs. In Advances in Neu­ ral Information Processing Systems, IO",
      "author" : [ "E.A. Hansen" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Finite-Memory Control of Partially Observable Systems",
      "author" : [ "E.A. Hansen" ],
      "venue" : "PhD thesis, Department of Computer Science, University of Massachusetts at Amherst,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "Solving POMDPs by searching in pol­ icy space",
      "author" : [ "E.A. Hansen" ],
      "venue" : "In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Planning and Control in Stochas­ tic Domains with Imperfect Information",
      "author" : [ "M. Hauskrecht" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Optimal Control of Complex Structured Processes",
      "author" : [ "0. Higelin" ],
      "venue" : "PhD thesis, University of Caen, France,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Dynamic Programming and Markov Processes",
      "author" : [ "R.A. Howard" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1960
    }, {
      "title" : "Rein­ forcement learning algorithm for partially observable Markov problems",
      "author" : [ "T. Jaakkola", "S. Singh", "M.R. Jordan" ],
      "venue" : "In Advances in Neural Informa­ tion Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1994
    }, {
      "title" : "Planning and acting in partially observable stochastic domains",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Overcoming incomplete perception with utile distinction memory",
      "author" : [ "R.A. McCallum" ],
      "venue" : "In The Proceedings of the Tenth International Machine Learning Confer­ ence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1993
    }, {
      "title" : "Reinforcement Learning with Selec­ tive Perception and Hidden State",
      "author" : [ "R.A. McCallum" ],
      "venue" : "PhD thesis, Univer­ sity of Rochester,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1995
    }, {
      "title" : "Kael­ bling. Learning finite-state controllers for partially observable environments",
      "author" : [ "N. Meuleau", "L. Peshkin", "K.E. Kim", "L.P" ],
      "venue" : "Proceedings of the Fif­ teenth Conference on Uncertainty in Artificial Intel­ ligence, To appear,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1999
    }, {
      "title" : "Reinforcement learning with hierarchies of machines. In Advances in Neural In­ formation Processing Systems I I",
      "author" : [ "R. Parr", "S. Russell" ],
      "venue" : null,
      "citeRegEx" : "Parr and Russell.,? \\Q1998\\E",
      "shortCiteRegEx" : "Parr and Russell.",
      "year" : 1998
    }, {
      "title" : "Learn­ ing policies with external memory",
      "author" : [ "L. Peshkin", "N. Meuleau", "L.P. Kaelbling" ],
      "venue" : "Proceedings of the Sixteenth International Conference on Machine Learning, To appear,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1999
    }, {
      "title" : "Lave. Markov decision processes with probabilistic observation of states",
      "author" : [ "R.E.J.K. Satia" ],
      "venue" : "Management Science,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1973
    }, {
      "title" : "Sondik. The optimal con­ trol of partially observable Markov decision processes over a finite horizon",
      "author" : [ "E.J.R.D. Smallwood" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1973
    }, {
      "title" : "The optimal control of partially observ­ able Markov decision processes over the infinite hori­ zon: Discounted costs",
      "author" : [ "E.J. Sondik" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1978
    }, {
      "title" : "Learning from Delayed Rewards",
      "author" : [ "C. Watkins" ],
      "venue" : "PhD thesis, King's College,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1989
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "Solving partially observable Markov decision processes (POMDPs) is highly intractable in gen­ eral, at least in part because the optimal policy may be infinitely large. In this paper, we ex­ plore the problem of finding the optimal policy from a restricted set of policies, represented as finite state automata of a given size. This prob­ lem is also intractable, but we show that the com­ plexity can be greatly reduced when the POMDP and/or policy are further constrained. We demon­ strate good empirical results with a branch-and­ bound method for finding globally optimal deter­ ministic policies, and a gradient-ascent method for finding locally optimal stochastic policies.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}