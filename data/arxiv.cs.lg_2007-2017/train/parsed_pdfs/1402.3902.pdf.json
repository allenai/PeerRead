{
  "name" : "1402.3902.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Smoothed Analysis for Learning Sparse Polynomials",
    "authors" : [ "Alexandros G. Dimakis", "Murat Kocaoglu", "Karthikeyan Shanmugam", "Csaba Szepesvári", "K. Shanmugam", "G. DIMAKIS", "KLIVANS KOCAOGLU SHANMUGAM" ],
    "emails" : [ "DIMAKIS@AUSTIN.UTEXAS.EDU", "KLIVANS@CS.UTEXAS.EDU", "MKOCAOGLU@UTEXAS.EDU", "KARTHIKSH@UTEXAS.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 2.\n39 02\nv1 [\ncs .L\nG ]\n1 7\nOur proof combines a method for identifying unique sign patterns induced by the underlying monomials of f with recent work in compressed sensing. We identify other natural conditions on f for which our techniques will succeed. Keywords: Smoothed Analysis, Learning Sparse Polynomials, Compressed Sensing"
    }, {
      "heading" : "1. Introduction",
      "text" : "Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al. (2002); Gopalan et al. (2008); Akavia (2010). In almost all cases, known algorithms for learning or interpolating sparse polynomials require query access to the unknown polynomial. An outstanding open problem is to find an algorithm for learning s-sparse polynomials with respect to the uniform distribution on {−1, 1}n that runs in time polynomial in n and g(s) (where g is any fixed function independent of n) and requires only randomly chosen examples to succeed. In particular, such an algorithm would imply a breakthrough result for the problem of learning k-juntas (functions that depend on only k << n input variables; it is not known how to learn ω(1)-juntas in polynomial time).\nWe present an algorithm and a set of natural conditions such that any sparse polynomial f satisfying these conditions can be learned from random examples only in time polynomial in n and 2s. In particular, any f whose coefficients have been subjected to a small perturbation satisfies these conditions (for example, if a Gaussian with arbitrarily small variance has been added independently to each coefficient, f will satisfy these conditions with probability 1). As such, we obtain an algorithm that succeeds in the setting of smoothed-analysis:\nc© 2014 A. G. Dimakis, A. Klivans, M. Kocaoglu & K. Shanmugam.\nTheorem 1 Let f : {−1, 1}n → R be a polynomial with at most s non-zero real coefficients. There exists an algorithm that, with high probability over a perturbation of f ’s coefficients, exactly reconstructs f in time polynomial in n and 2s.\nSmoothed-analysis, pioneered in Spielman and Teng (2004), has now become a common alternative for problems that seem intractable in the worst-case. In Section 4, we describe a list of technical conditions such that any f satisfying one of these conditions can be exactly reconstructed by our algorithm in time polynomial in n and 2s. It is easy to see that functions generated as described above in the smoothed-analysis setting will satisfy one of the conditions.\nOur algorithm also succeeds in the presence of noise:\nTheorem 2 Let f = f1+ f2 be a polynomial such that f1 and f2 depend on mutually disjoint set of parity functions. f1 is s-sparse and the values of f1 are ‘well separated’. ‖f2‖1 ≤ ν. If observations are corrupted by additive noise bounded by ǫ, then there exists an algorithm, that knows ǫ+ ν, that gives g in time polynomial in n and 2s such that ‖f − g‖2 ≤ O(ν + ǫ) with high probability.\nFor a more formal statement of the theorem see Section 5. Additionally, we obtain the following for the well-known problem of learning k-juntas:\nTheorem 3 Given an s-sparse k-junta f : {−1, 1}n → {−1, 1} for which rank ( f−1(1) )\n< n or rank ( f−1(−1) )\n< n, there is an algorithm that runs in time poly (n, 2s) and identifies at least one relevant variable.\nIn the above theorem, rank is computed with respect to the binary field. Typically in order to solve the problem of learning juntas, it suffices to be able to identify a single relevant variable. For technical reasons, this is not the case here (see Section 6 for a more complete discussion)."
    }, {
      "heading" : "1.1. Approach and Related Work",
      "text" : "The problem of recovering the sparsest solution of a set of underdetermined linear equations has received significant recent attention in the context of compressed sensing Candès et al. (2006); Candès and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support.\nThe recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing. The authors show that the problem of learning a sparse polynomial is equivalent to recovering the unknown sparse coefficient vector using linear measurements. The key difference from standard compressed sensing is that the unknown vector has exponential length, having one coordinate for each Fourier coefficient. By applying techniques from compressed sensing theory, namely Restricted Isometry Property (see Stobbe and Krause (2012)) and incoherence (see Negahban and Shah (2012)), the authors independently established results for reconstructing sparse polynomials using convex optimization. The results have near-optimal sample complexity. However, the running time of these algorithms is exponential in the underlying dimension, n. This is because the measurement matrix of the equivalent compressed sensing problem requires one column for every possible non-zero monomial.\nIn this paper, we show how to solve this problem in time polynomial in n and 2s under some natural technical conditions on the sparse polynomial. Our key contribution is a novel identification procedure that can reduce the list of potentially non-zero coefficients from the naive bound of 2n to 2s. Having determined a set of 2s candidate non-zero coefficients, we can apply the previous compressed sensing framework to the reduced problem and exactly reconstruct the unknown polynomial using convex optimization. The convex optimization framework for sparse recovery has several desirable properties: there is no need for an upper bound on the magnitude of f and we can exactly recover the unknown coefficients1. Further, we show that our framework allows us to reconstruct approximately sparse polynomials even in the presence of additive bounded noise (for a definition of the noise model see Section 5).\nWe remark here on the interesting recent work of Andoni et al. (2014) that approximately learns sparse polynomial functions when the underlying domain is Gaussian. Their results do not seem to translate to the Boolean domain. We also note the work of Kalai et al. (2009) that gives an algorithm for learning sparse Boolean functions with respect to a randomly chosen product distribution on {−1, 1}n. Their work does not apply to the uniform distribution on {−1, 1}n.\nThe core assumption in our algorithms is that the underlying function should satisfy a notion that we call the unique sign pattern property, which means that there is at least one function value that is produced only when the s parity functions of interest assume a unique set of signs. This condition is satisfied under various natural settings. Smoothed analysis is an important setting in which Fourier coefficients, i.e., ci’s are perturbed with Gaussian random variables of standard deviation σ > 0 or by random variables drawn from any set of reasonable continuous distributions. This model of perturbation induces polynomials that naturally satisfy the unique sign pattern property. Another setting is the random parity function, i.e., when the set of s parity functions are drawn uniformly randomly from the set 2[n] of all possible parities. The details on the unique sign pattern property and more general conditions under which it is satisfied are discussed in detail in Section 4."
    }, {
      "heading" : "2. Definitions",
      "text" : "Consider a function over the boolean hypercube f : {−1, 1}n → R. Given a sequence of labeled samples of the form 〈f(x),x〉, where x is sampled from the uniform distribution U over the hypercube {−1, 1}n, we are interested in an efficient algorithm that learns the function f with high probability. Through Fourier expansion, f can be written as a linear combination of monomials :\nf (x) = ∑\nS⊆[n]\ncSχS(x), ∀ x ∈ {−1, 1} n (1)\nwhere [n] is the set of integers from 1 to n, χS(x) = ∏\ni∈S\nxi and cS ∈ R. Let c be the vector of\ncoefficients cS . A monomial χS (x) is also called a parity function. In this work, we restrict ourselves to sparse polynomials f with sparsity s in the Fourier domain, i.e., f is a linear combination of some unknown parity functions χS1(x), χS2(x), . . . χSs (x) and unknown s real coefficients given by {cSi} s i=1 such that cSi 6= 0, ∀1 ≤ i ≤ s and all other coefficients are 0. Let the subsets corresponding to the s parity functions form a family of sets I = {Si} s i=1. Finding I is equivalent to finding the s parity functions.\n1. This assumes infinite precision arithmetic. If we wish to obtain numerical results with precision τ we incur a log(1/τ ) factor in all of our bounds.\nNote: In certain places, where the context will make it clear, we slightly abuse the notation such that the set Si identifying a specific parity function is replaced by just the index i. The coefficients may be denoted simply by ci and the parity functions by χi (·).\nLet F2 denote the binary field. Every parity function χi(·) can be represented by a vector pi ∈ F n×1 2 . The j-th entry pi(j) in the vector pi is given by:\npi(j) =\n{\n1, if j ∈ Si 0, otherwise\n(2)\nDefinition 4 A set of s parity functions {χi(·)}si=1 are said to be linearly independent if the corresponding set of vectors {pi}si=1 are linearly independent over F2.\nSimilarly, they are said to have rank r if the dimension of the subspace spanned by the set {pi}si=1 is r.\nDefinition 5 The coefficients {ci}si=1 are said to be in general position if for all possible set of values bi ∈ {0, 1,−1}, ∀ 1 ≤ i ≤ s with at least one nonzero bi, the following holds:\ns ∑\ni=1\ncibi 6= 0. (3)\nDefinition 6 The coefficients {ci}si=1 are said to be µ-separated if for all possible set of values bi ∈ {0, 1,−1}, ∀ 1 ≤ i ≤ s with at least one nonzero bi, the following holds:\n∣ ∣ ∣ ∣ ∣ s ∑\ni=1\ncibi\n∣ ∣ ∣ ∣ ∣ > µ. (4)\nDefinition 7 A sign pattern is a distinct vector of signs a = [χ1 (·) , χ2 (·) , . . . χs (·))] ∈ {−1, 1}1×s assumed by the set of s parity functions.\nSince this work involves switching representations between the real and the binary field, we define a function q that does the switch.\nDefinition 8 q : {−1, 1}a×b → Fa×b2 is a function that converts a sign matrix X to a matrix Y over F2 such that:\nYij = q(Xij) =\n{\n1 ∈ F2, if Xij = −1 0 ∈ F2, if Xij = 1\n}\n(5)\nClearly, it has an inverse function q−1 such that q−1(Y) = X.\nWe also present some definitions to deal with the case when the polynomial f is not exactly s-sparse and and observations are noisy. Let 2[n] denote the power set of [n].\nDefinition 9 A polynomial f : {−1, 1}n → R is called approximately (s, ν)-sparse if there exists I ⊂ 2[n] with |I| = s such that ∑\nS∈Ic |cS | < ν, where {cS} are the Fourier coefficients as in (1).\nIn other words, the sum of the absolute values of all the coefficients except the ones corresponding to I are rather small.\nWe are interested in algorithms that learn the unknown coefficients {cS}S∈I and the monomials {χS(·)}S∈I of an s-sparse function f exactly from a set of labeled samples which are drawn independently from the uniform distribution U . When the values of an approximately (s, ν)-sparse polynomial are observed under additive noise, we are interested in learning it up to some error."
    }, {
      "heading" : "3. Problem Setting",
      "text" : "Suppose m labeled samples 〈f (x) ,x〉mi=1 are drawn from the uniform distribution U on the boolean hypercube. For any B ⊆ 2[n], let cB ∈ R2\nn×1 be the vector of real coefficients such that cB(S) = cS , ∀S ∈ B and cB(S) = 0, ∀S /∈ B. Let A ∈ Rm×2 n\nsuch that every row of A corresponds to one random input sample x ∼ U . Let x also denote the row index and S ⊆ [n] denote the column index of A. A(x, S) = χS (x). Let AS denote the sub matrix formed by the columns corresponding to the subsets in S . Let I be the set consisting of the s parity functions of interest in both the sparse and the approximately sparse case. A sparse representation of an approximately (s, ν)-sparse function f is fI = A(x) cI , where cI is defined above.\nWe use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012). Specifically, for the remainder of this paper, we rely on Negahban and Shah (2012) as a point of reference. We review their framework and explain how we use it to obtain our results.\nTheorem 10 (Negahban and Shah (2012)) Any two columns of A satisfy the incoherence property, (AS1) T AS2 ≤ 4 √ n m with probability at least 1− 2 256n for S1 6= S2.\nGiven any sub matrix, it follows from the union bound that any non-diagonal entry of (AS) T AS is at most 4 √\nn m (small) with probability at least 1−O\n(\n1 4n\n)\nfor any subset of columns S . Let y ∈ Rm and βS ∈ R2 n\n, such that βS = 0, ∀S ⊆ Sc. Now, consider the following convex program for noisy compressive sensing in this setting:\nmin‖βS‖1 such that\n√\n1\nm ‖AβS − y‖2 ≤ ǫ. (6)\nLet βoptS be an optimum for the program (6). Note that only the columns of A in S are used in the program. The convex program runs in time poly (m, |S|).\nThe compressive sensing framework in Negahban and Shah (2012) along with the incoherence property has the following intuition: In a problem where an approximately sparse (βS ) feasible solution (coefficients in a sparse set I ⊆ S are ’big’ and others ’small’) is known to exist, the optimum (βoptS ) approximates the sparse solution well and the error is bounded by the small coefficients and noise in the observation. We state a slightly general result, that is implied by the results in Negahban and Shah (2012).\nTheorem 11 (Negahban and Shah (2012)) For any family of subsets I ∈ 2[n] such that |I| = s, m = 4096ns2 and c1 = 4, c2 = 8, for any feasible solution βS of program 6, we have:\n‖βS − β opt S ‖2 ≤ c1ǫ+ c2\n( n\nm\n)1/4 ‖βIc ⋂ S‖1 (7)\nwith probability at least 1−O ( 1 4n )\nWhen S is set to the power set 2[n], ǫ = 0 and y is the vector of observed values for an s-sparse polynomial, the s-sparse vector cI is a feasible solution to program (6). By Theorem 11, the optimum recovers the sparse vector cI and hence learns the function. The only caveat is that the complexity is exponential in n.\nThe main idea behind our algorithms for noiseless and noisy sparse function learning is to ‘capture’ the actual s-sparse set I of interest in a small set S = O (2s) of coefficients by a separate\nalgorithm that runs in time poly(n, 2s). Using the restricted set of coefficients S , we search for the sparse solution under the noisy and noiseless cases using program (6). We capture this in the following lemma:\nLemma 12 Given an algorithm that runs in time poly(n, 2s) and generates a set of parities S such that |S| = O (2s) ,I ⊆ S with |I| = s, program (6) with S and m = 4096ns2 random samples as inputs runs in time poly(n, 2s) and learns the correct function with probability 1−O (\n1 4n\n)\n.\nFor the noiseless case, after finding a small enough S , instead of using the convex program, one can either exhaustively search over the all possible s parities or estimate the coefficient for each parity function χ ∈ S by averaging f ×χ(·) over the distribution U . The first method will have a run time dependence on s, which is poly ( 2s 2 ) . The averaging method will produce only an estimate and\nneeds an upper bound on the ℓ2 norm of f to quantify dependence of run time on accuracy. The use of the compressed sensing framework ensures exact recovery and also a better running time having a poly (2s) dependence on s.\nUnique Sign Pattern Property: The key property that lets us find a small S efficiently is the unique sign pattern property. Observe that an s-sparse function can produce at most 2s different real values. If the maximum value obtained always corresponds to a unique pattern of signs, we collect random samples x corresponding to the subsequent O(n) occurrences of this maximum value into a matrix X . Unique sign pattern assures that χi (x) is the same for a given i, for all such x. Hence, it can be either −1 or 1.\nObserve that q (χi(x)) = pTi w where w = q (x). Therefore, there are s linear constraints on w each corresponding to an equation pTi w. Clearly, this is a hyperplane of affine dimension at least n − s. Hence, the dimension of W (binary field equivalent of the input matrix X, i.e., q (X)) is at least n− s with high probability when enough samples are drawn.\nIf the function satisfies the unique sign pattern for the maximum value, any parity function {pi} has to satisfy either 1 = Wpi or 0 = Wpi. Since, W has rank at least n−s, there are at most 2s+1 possible parity functions. Clearly, all parity functions needed to learn f are captured in this set. The unique sign property again plays an important role, along with Theorem 11 with more technicalities added, in the noisy case which we visit in Section 5.\nHowever, an important class of boolean polynomials that does not satisfy the unique sign pattern property is a k-junta. An s-sparse k-junta is a function f : {−1, 1}n → {−1, 1} which is s-sparse in the Fourier domain and depends only on k out of n variables. It is also known that learning a single variable the junta depends on is as hard as learning the entire junta except for a poly ( 2k ) slowdown (see Mossel et al. (2003)). As far as we are aware of, the best running time for learning a k-junta is n0.6kpoly(n) due to Valiant (2012).\nIn Section 6, using algebraic insights from the previous cases, we show that if rank ( f−1(1) )\n< n or rank ( f−1(−1) )\n< n, then we can identify at least one variable the function f depends on in time poly (n, 2s).\nIn the next section, we provide an algorithm to generate the bounded set S for the noiseless case for an s-sparse function f and provide guarantees for the algorithm formally."
    }, {
      "heading" : "4. Algorithm and Guarantees: Noiseless case",
      "text" : "Let I be the family of s subsets {Si}si=1 each corresponding to the s parity functions χSi (·) in an s-sparse function f . In this section, we provide an algorithm, named LearnBool, that finds a small\nset S containing I first and then uses program (6) with S . We show that the algorithm learns f in time poly (n, 2s) from randomly uniformly drawn labeled samples from the boolean hypercube with high probability under some natural conditions.\nTheorem 13 Let f be an s-sparse function that satisfies at least one of the following properties:\n(a) The coefficients {ci}si=1 are in general position.\n(b) The s parity functions are linearly independent.\n(c) All the coefficients are positive.\nGiven labeled samples, the Algorithm 1 learns f exactly (or vopt = c) in time poly (n, 2s) with probability 1−O (\n1 n\n)\n.\nSmoothed Analysis Setting: Perturbing ci’s with Gaussian random variables of standard deviation σ > 0 or by random variables drawn from any set of reasonable continuous distributions ensures that the perturbed function satisfies property (a) with probability 1.\nRandom Parity Functions: When ci’s are arbitrary and the set of s parity functions are drawn uniformly randomly from 2[n], then property (b) holds with high probability if s is a constant. If the set of s parity functions for f are chosen uniformly from a set of all possible sets of s parity functions, then the probability that they are linearly independent is given by:\nPr ( set of parity functions are linearly independent) ≥ s ∏\ni=1\n( 2n − 2i−1 )\n2ns ≥\n(\n1− 2s−1\n2n\n)s\n(8)\n≥\n(\n1−O\n(\n2s−1\n2n s\n))\n(9)\nFrom the viewpoint of the above model for choosing the set of parity functions, only a vanishingly small fraction of the set of cases are left uncovered by the algorithm. We also point out that the run time complexity for learning f with sparsity s = log n is polynomial in n in this case.\nWe prove Theorem 13 at the end of this section. Next, we state a technical lemma about Algorithm 1 that will be used in the proof. Since the function f is s-sparse, it takes at most 2s distinct real values. Recall that if the function is such that f(x) attains its maximum value only if [χ1(x), χ2 (x) . . . χs (x)] = amax ∈ {−1, 1}\ns for some unique sign pattern amax, then the function is said to possess the unique sign property.\nLemma 14 If an s-sparse function f has the unique sign property then, in Algorithm 1, S is such that I ⊆ S, |S| ≤ 2s+1 with probability 1−O (\n1 n\n)\nand runs in time poly(n, 2s).\nThe proof of the above lemma involves showing that the random matrix Ymax (see Algorithm 1) has rank at least n− s. The full details of the proof is relegated to Appendix A. Now, we relate the unique sign property to the conditions mentioned in Theorem 13 for its proof. Proof [of Theorem 13] Due to Lemmas 14 and 12, we just need to show that each of the conditions in the theorem implies the unique sign property, i.e., the maximum value of the function f is attained when the set of parity functions takes a unique sign pattern.\nCase 1: If the coefficients are in general position (Definition 5), all values taken by the function correspond to distinct sign patterns. This implies the unique sign property for the maximum value.\nAlgorithm 1: LearnBool Input: Sparsity parameter s, m1 = 2n2s random labeled samples {〈f (xi) ,xi〉} m1 i=1. Pick samples {xij} nmax j=1 corresponding to the maximum value of f observed in all the m samples. Stack all xij row wise into a matrix Xmax of dimensions nmax × n. Initialise S = ∅. Let Ymax = q (Xmax). Find all feasible solutions p ∈ Fn×12 such that:\n1nmax×1 = Ymaxp or 0nmax×1 = Ymaxp (10)\nCollect all feasible solutions p to either of the above equations in the set P ⊆ Fn×12 . S = {{j ∈ [n] : p(j) = 1}|p ∈ P}. Using m = 4096ns2 more samples (number of rows of A is m corresponding to these new samples), solve:\nβoptS = min‖βS‖1 such that AβS = y, (11)\nwhere y is the vector of m observed values. Set vopt = βoptS . Output: vopt.\nCase 2: If all the parity functions are linearly independent, any sign pattern can be realized. Then, the sign pattern [sign (c1) , sign (c2) . . . sign (cs)] can be realized by the set of parity functions and this produces the value s ∑\ni=1 |ci|. And any other sign pattern will produce a strictly lesser value as\nall ci are nonzero. Hence, the maximum value is unique in this case. Case 3: Let us consider the case when all the coefficients are positive. Even if the parity functions are linearly dependent, the sign pattern with all +1’s can be produced and this attains the unique maximum value s ∑\ni=1 |ci|. This implies the unique sign property."
    }, {
      "heading" : "5. Algorithms and Guarantees: Noisy Case",
      "text" : "In this section, we provide our algorithm for learning an approximately (s, ν)-sparse function with noisy samples, and prove guarantees regarding the error between the function learnt and the actual function. When m random samples are observed, the noisy output model for an approximately (s, ν)-sparse function f is given by:\ny = Ac+ ε (12)\nwhere A is the m by 2n matrix where each row corresponds to a sample x and each column corresponds to a parity function and c is the set of Fourier coefficients for f and the noise |εi| ≤ ǫ, 1 ≤ i ≤ m. We recall that ∑\nS⊆Ic |cS | < ν for an approximately sparse f . We assume that ǫ+ ν is known.\nAlthough the observations are noisy, the set of inputs for which the sparse representation of the function f , i.e., fI (this depends on only Fourier coefficients in I ) attains its maximum, can still be perfectly identified under certain conditions given in the Lemma below. Algorithm 2 identifies those inputs.\nAlgorithm 2: MaxCluster Input: The sequence of labeled samples 〈f(xi) + εi,xi〉mi=1 Initialise Xmax = ∅. Let η be the maximum value observed. Stack all the inputs xi, such that f (xi) + εi is in the neighborhood of radius 2 (ǫ+ ν) around η, into Xmax. Output: Xmax.\nLemma 15 If the function f is approximately (s, ν)-sparse, observations follow the noise model in (12), and if the values of fI are separated by at least 4(ǫ + ν), then the output matrix Xmax in Algorithm 2 will contain exactly those inputs for which fI attains the maximum value among the drawn samples.\nProof Consider a sample x. Clearly, from the noise model and the definition of approximate sparsity, |f (xi + εi)− fI (xi)| ≤ ν + ǫ. Hence, when using a radius of 2(ν + ǫ) for clustering, clearly no two samples with different fI will be included in Xmax and definitely one sample belonging to the maximum fI among the observed samples will be included.\nAlgorithm 3: LearnBoolNoisy Input: Sparsity parameter s, ǫ+ ν, m1 = 2n2s random labeled samples {〈f (xi) ,xi〉} m1 i=1. Run MaxCluster algorithm to obtain Xmax. Initialise P = ∅. Find all feasible solutions p such that: 1 = q(Xmax)p or 0 = q(Xmax)p. Collect all feasible p in the set P ⊆ Fn2 . S = {{j ∈ [n] : pi(j) = 1}|pi ∈ P}. Using m = 4096ns2 more samples (number of rows of A is m corresponding to these new samples), solve:\nβoptS = min‖βS‖1 such that\n√\n1\nm ‖AβS − y‖2 ≤ ǫ+ ν, (13)\nwhere y is the vector of m noisy observed values (as in (11)) Set vopt = βoptS . Output: vopt.\nNow we state and prove our main thoerem for learning a sparse function from noisy observations. Consider the model given in (12). Let f be the function f : {−1, 1}n → R, and c ∈ Rn×1 be the set of Fourier coefficients of f and vopt be the output of Algorithm 3.\nTheorem 16 Assume f is an approximately (s, ν)-sparse function as given in Definition 9 and observed samples satisfy the noise model in (12). Then, Algorithm 3 outputs vopt in time poly(n, 2s) with probability 1 − O (\n1 n\n)\nsatisfying ‖c − vopt‖2 ≤ α1ǫ + α2ν, if f satisfies at least one of the following properties:\n(a) The coefficients {cS}S∈I are 4(ν + ǫ)-separated.\n(b) The set of parity functions χi(·) are linearly independent, and min S∈I cS > 4(ǫ+ ν).\n(c) All the coefficients are positive, and min S∈I cS > 4(ǫ+ ν).\nHere, α1 and α2 are some constants.\nProof The three properties imply that fI has the unique sign property for the maximum value due to the same arguments in the proof of Theorem 13. Further, they also imply that the values of fI are separated by 4 (ǫ+ ν) in each of the cases. By Lemma 15, rows of Xmax contain only the inputs at which fI attains its maximum among the observed values.\nUsing Lemma 14 on fI , which is exactly s-sparse, it can be seen that |S| ≤ 2s+1 and contains all the parity functions in fI with probability 1 − O ( 1 n )\nas in Algorithm 1. This is because P is formed using inputs in Xmax that give the maximum fI among the observed samples in an identical fashion as in Algorithm 1. Now, we have the following chain of inequalities:\n‖c− vopt‖2 ≤ ‖cS − β opt S ‖2 + ‖cSc‖2 (triangle inequality)\n≤ ‖cS − β opt S ‖2 + ‖cSc‖1 (‖·‖2 ≤ ‖·‖1) a ≤ c1(ν + ǫ) + c2 ( n\nm\n)1/4 ‖cIc ⋂ S‖1 + ‖cSc‖1\n≤ c1(ν + ǫ) + c2(ν) + ν (n < m) (14)\nFor inequality (a), it is easy to see that cS is a feasible solution to program 13 and therefore Theorem 11 can be applied with βS = cS with noise threshold ν + ǫ. Further, ‖cIc‖1 < ν.\nSince |S| ≤ 2s+1, the optimization program 13 runs in time poly (n, 2s).\n6. Tools for learning one variable dependency for a k-junta\nIn the previous sections, we provided algorithms for learning s-sparse polynomials and approximately sparse polynomials in noise that take real values. This algorithm works only if the polynomial f (or fI) satisfies the unique sign pattern property. We characterized functions for which the maximum value is produced by a unique sign pattern. It is possible to extend the above algorithms to the case when at least one of the observed values corresponds to a unique sign pattern.\nAn important class of boolean polynomials that does not satisfy the unique sign pattern property is a k-junta. An s-sparse k-junta is a function f : {−1, 1}n → {−1, 1} which is s-sparse in the Fourier domain and depends only on k out of n variables. Observe that s ≤ 2k. The function is often defined without the sparsity constraint. But we will have that explicitly to denote the situation where the function has only s non-zero coefficients in the Fourier expansion. Except few simple cases, a junta does not in general satisfy the unique pattern property.\nIn this section, we show that when the s-sparse k junta satisfies a condition, then it is possible to find out at least one variable the function depends on in time poly (n, 2s). With s ≤ 2k, this gives a running time of poly(n)22 k\n. Observe that, having identified one variable and fixing it, it is not possible to apply the algorithm recursively to find the subsequent dependent variables of the\nfunction on the remaining variables. This is because after restricting the first identified dependent variable, it is not clear if the function on the remaining variables will satisfy this condition. First, we state the algorithm below and then a formal statement of the condition under which the algorithm succeeds in finding at least one variable.\nAlgorithm 4: Learnval Input: sparsity parameter s. m = 2n2s random labeled samples {〈f (xi) ,xi〉}mi=1. Pick samples {xij} n1 j=1 corresponding to the value 1 observed in all the m samples. Stack all q (\nxij )\nrow wise into a matrix Z1 of dimensions n1 × n. Pick samples {zij} n2 j=1 corresponding to the value −1 observed in all the m samples. Stack all q (\nxij )\nrow wise into a matrix Z2 of dimensions n2 × n. if rank (Z1) < n then\nFind one feasible w : Z1w = 0. else\nif rank (Z2) < n then Find one feasible w : Z2w = 0. else Output: Report an Error and Exit.\nend end Let C be the set of non zero coordinates of w. Output: C .\nLet f−1(−1) and f−1(1) be the set of inputs on which f produces the values −1 and 1 respectively. Now we state the main result of this section.\nTheorem 17 Given an s-sparse k-junta f for which rankF2 ( f−1(1) ) < n or rankF2 ( f−1(−1) )\n< n, the output C by Algorithm 4 contains indices of variables on which the function f depends, where |C| ≥ 1. Furthermore, the algorithm runs in time poly (n, 2s).\nWe have seen that an s-sparse k-junta does not satisfy the unique sign pattern property in general. This means that value 1 is produced by a set of sign patterns {a1 a2 . . . am} and −1 is produced by a number of sign patterns {am+1 . . . a2r} where r is the rank of s-parity functions. Let bi = q (ai) be the binary equivalent of the ith sign pattern.\nLet the parity functions pi be stacked in a matrix P ∈ F s×n 2 . The vectors z satisfying Pz = bi, form a hyperplane Hi = {ui + N (P)} where N (P) is the null space of the matrix P and ui /∈ N (P) , ∀i. Clearly, dim (N (P)) = n − r. Therefore, we have the following structural lemma (without proof):\nLemma 18 For an s-sparse k-junta, f−1(−1) and f−1(1) are unions of disjoint hyperplanes , i.e., f−1(1) = m ⋃\ni=1 Hi and f−1(−1) =\n2r ⋃\ni=m+1 Hi.\nAnother observation is that although ‘seeing’ every input in f−1(1) takes exponential number of samples, it only needs poly (n, 2s) samples to span the vector space span ( f−1(1) )\n. This is captured by the following lemma. The proof is relegated to the appendix.\nLemma 19 In Algorithm 4, rank (Z1) = rank ( f−1(1) ) , rank (Z2) = rank ( f−1(−1) )\nwith probability 1−O (\n1 n\n)\n.\nProof [Proof of Theorem 17] From Lemma 18, span ( f−1(1) )\n= span (N (P) ,u1 . . .um). Without loss of generality let us assume that rank ( f−1(1) )\n< n. Then clearly, there is a non-zero w in the dual space of span ( f−1(1) ) . Since, N (P) ⊂ span ( f−1(1) )\n, w ∈ rowspan (P). Hence, the indices corresponding to the non-zero entries of w indicate variables on which f depends.\nThis with Lemma 19 establishes the result since the time complexity of algorithm 4 is easily seen to be poly (n, 2s)."
    }, {
      "heading" : "Appendix A. Proof of Lemma 14",
      "text" : "Proof Let E1 be the event that the maximum value observed among m1 samples in the algorithm 1 is the maximum value attained by f . Note that, the probability that the function attains the maximum value is at least 12s . To see this, if the parity functions have rank r, then the set of r linearly independent parity functions take values uniformly in the hypercube {−1, 1}r and other are determined by these r signs. Hence, the probability of finding the maximum value is 12r ≥ 1 2s . If the functions satisfies the unique sign property for the maximum value and if E1 is true, it is easily seen that the actual party functions pi are in the set P in the algorithm 1.\nConsider the algorithm 1. Let E3 be the event that the matrix Ymax has at least rank n− s. E3 implies that |P | = |S| ≤ 2s+1, ∀1 ≤ i ≤ s. Let E2 be the event that nmax > 2n. Conditioned on E2 and E1 being true, we first argue that the rank of Ymax is at least n − s with high probability. Let the rank of the actual set of parity functions [p1,p2 . . .ps] be k ≤ s.\nIf E1 and E2 are true, then Ymax contains 2n random samples such that they all produce the same sign pattern amax because the actual function f satisfies the unique sign pattern property for the maximum value. Let zmax = q (amax). Observe that rows of Ymax are random samples uniformly drawn from the hyperplane H = {x ∈ Fn×12 : x\nT [pi] = zmax(i), ∀1 ≤ i ≤ s}. Since the rank of the parity functions is k, the dimension of H is n−k. Now, the rank of space spanned by 2n samples drawn randomly uniformly from H is at least the rank of space spanned by 2n samples drawn randomly uniformly from F1×n−k2 . The probability that a random 2n× n− k binary matrix is full rank is given by:\nPr (a random 2n × n− k binary matrix is full rank) = n−k−1 ∏\ni=0\n(\n1− 1\n22n−i\n)\n≥\n(\n1− 1\n2n\n)n−k\n≥\n(\n1− 1\n2n\n)n\n≥ 1−O\n(\n1\nn\n)\n(15)\n(16)\nHence, Pr (E3|E2 ⋂ E1) ≥ 1−O ( 1 n ) . Pr (E1 ⋂\nE2) is the probability that there are at least nmax samples corresponding to the maximum value of the actual function in the 2n2s samples drawn.\nTherefore, Pr (E1 ⋂ E2) ≥ 1 − ( 1− 12s )2n2s−2n\nbecause the maximum value of f is seen with probability at least 12s . Using this in the following chain, we have:\nPr ( |S| ≤ 2s+1, I ⊆ S ) ≥ Pr ( E1 ⋂ E2 ⋂ E3 ) ≥ Pr ( E1 ⋂ E2 ) Pr ( E3|E2 ⋂ E1 )\n≥ Pr ( E1 ⋂ E2 )\n(\n1−O\n(\n1\nn\n))\n(by (15))\n≥\n(\n1−\n(\n1− 1\n2s\n)2n2s−2n ) (\n1−O\n(\n1\nn\n))\n≥\n(\n1− exp\n(\n−2n\n(\n1− 1\n2s\n)))(\n1−O\n(\n1\nn\n))\n≥\n(\n1−O\n(\n1\nn\n))(\n1−O\n(\n1\nn\n))\n≥ 1−O\n(\n1\nn\n)\n. (17)"
    }, {
      "heading" : "Appendix B. Proof of Lemma 19",
      "text" : "Proof Let rank (P) = r. It is enough to show that rank ( f−1(1) )\n= rank (Z1) with probability 1−O (\n1 n\n)\n. Let E1 be the event that H1 is sampled at least 2n times. Let Ei be the event that Hi is samples at least once for all 2 ≤ i ≤ m. Let E be the event that the row space of Z1 includes the vector space span (N (P) ,u1).\nLet z1, z2 . . . 2n+ q (q ≥ 0) be the row entries from H1. Then zj = u1 + vj where vj ∈ N (P). Let E′ be the event that {vj}2nj=1 has rank n − r. Consider the event E\n′ conditional on E1. Clearly, after 2n random draws, the probability that {vj}2nj=1 will have rank n − r is equal to probability that a binary 2n×n− r matrix will have full rank is at least 1−O (1/n) (by (15) in the proof of Lemma 14).\nConditional on both E1 and E′, the event E is true when ui is recoverable from zj’s. Since, adding odd number of 1’s results in 1 over F2, ui is recoverable if there exists a vi which is a combination of odd number (odd number ≥ 3) of linearly independent vectors from the set vj . The probability that no vector (distinct form the linearly independent set) is an odd combination of n−r linearly independent vectors is at most (\n1 2 + n−r 2n−r )2n−(n−r) = O ( 1 n )\n. Using all these conditional probabilities, we have the following chain of inequalities:\nPr (E) ≥ Pr (E1) Pr ( E′|E1 ) Pr ( E|E1, E ′ )\n≥\n(\n1−\n(\n1− 1\n2s\n)2n2s−2n+1 ) (\n1−O\n(\n1\nn\n))(\n1−O\n(\n1\nn\n))\n≥ 1−O\n(\n1\nn\n)\n(18)\nAlso, from discussions above, we have the following chain:\nPr ( rank ( f−1(1) ) 6= rank (Z1) ) ≤ Pr (Ec) + m ∑\ni=2\nPr (Eci )\n≤ O\n(\n1\nn\n) + 2s ( 1− 1\n2s\n)2n2s\n≤ O\n(\n1\nn\n)\n+ 2sexp (−2n)\n= O\n(\n1\nn\n)\n(19)"
    } ],
    "references" : [ {
      "title" : "Deterministic sparse fourier approximation via fooling arithmetic progressions",
      "author" : [ "Adi Akavia" ],
      "venue" : "In COLT, pages 381–393,",
      "citeRegEx" : "Akavia.,? \\Q2010\\E",
      "shortCiteRegEx" : "Akavia.",
      "year" : 2010
    }, {
      "title" : "Learning sparse polynomial functions",
      "author" : [ "Alexandr Andoni", "Rina Panigrahy", "Gregory Valiant", "Li Zhang" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Andoni et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Andoni et al\\.",
      "year" : 2014
    }, {
      "title" : "Decoding by linear programming",
      "author" : [ "Emmanuel J Candès", "Terence Tao" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candès and Tao.,? \\Q2005\\E",
      "shortCiteRegEx" : "Candès and Tao.",
      "year" : 2005
    }, {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "Emmanuel J Candès", "Justin Romberg", "Terence Tao" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candès et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2006
    }, {
      "title" : "Compressed sensing",
      "author" : [ "David L Donoho" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Donoho.,? \\Q2006\\E",
      "shortCiteRegEx" : "Donoho.",
      "year" : 2006
    }, {
      "title" : "Near-optimal sparse fourier representations via sampling",
      "author" : [ "Anna C. Gilbert", "Sudipto Guha", "Piotr Indyk", "S. Muthukrishnan", "Martin Strauss" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Gilbert et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gilbert et al\\.",
      "year" : 2002
    }, {
      "title" : "Agnostically learning decision trees",
      "author" : [ "P. Gopalan", "A. Kalai", "A. Klivans" ],
      "venue" : "In Proceedings of STOC,",
      "citeRegEx" : "Gopalan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Gopalan et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning and smoothed analysis",
      "author" : [ "Adam Tauman Kalai", "Alex Samorodnitsky", "Shang-Hua Teng" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Kalai et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning decision trees using the fourier spectrum",
      "author" : [ "Eyal Kushilevitz", "Yishay Mansour" ],
      "venue" : null,
      "citeRegEx" : "Kushilevitz and Mansour.,? \\Q1993\\E",
      "shortCiteRegEx" : "Kushilevitz and Mansour.",
      "year" : 1993
    }, {
      "title" : "Randomized interpolation and approximation of sparse polynomials",
      "author" : [ "Yishay Mansour" ],
      "venue" : null,
      "citeRegEx" : "Mansour.,? \\Q1995\\E",
      "shortCiteRegEx" : "Mansour.",
      "year" : 1995
    }, {
      "title" : "Learning juntas",
      "author" : [ "Elchanan Mossel", "Ryan O’Donnell", "Rocco P Servedio" ],
      "venue" : "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Mossel et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Mossel et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning sparse boolean polynomials",
      "author" : [ "Sahand Negahban", "Devavrat Shah" ],
      "venue" : "In Communication, Control, and Computing (Allerton),",
      "citeRegEx" : "Negahban and Shah.,? \\Q2012\\E",
      "shortCiteRegEx" : "Negahban and Shah.",
      "year" : 2012
    }, {
      "title" : "Learning sparse multivariate polynomials over a field with queries and counterexamples",
      "author" : [ "R. Schapire", "R. Sellie" ],
      "venue" : null,
      "citeRegEx" : "Schapire and Sellie.,? \\Q1996\\E",
      "shortCiteRegEx" : "Schapire and Sellie.",
      "year" : 1996
    }, {
      "title" : "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time",
      "author" : [ "D. Spielman", "S. Teng" ],
      "venue" : null,
      "citeRegEx" : "Spielman and Teng.,? \\Q2004\\E",
      "shortCiteRegEx" : "Spielman and Teng.",
      "year" : 2004
    }, {
      "title" : "Learning fourier sparse set functions",
      "author" : [ "Peter Stobbe", "Andreas Krause" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Stobbe and Krause.,? \\Q2012\\E",
      "shortCiteRegEx" : "Stobbe and Krause.",
      "year" : 2012
    }, {
      "title" : "Finding correlations in subquadratic time, with applications to learning parities and juntas",
      "author" : [ "Gregory Valiant" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Valiant.,? \\Q2012\\E",
      "shortCiteRegEx" : "Valiant.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al.",
      "startOffset" : 200,
      "endOffset" : 231
    }, {
      "referenceID" : 5,
      "context" : "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al.",
      "startOffset" : 200,
      "endOffset" : 247
    }, {
      "referenceID" : 5,
      "context" : "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al.",
      "startOffset" : 200,
      "endOffset" : 275
    }, {
      "referenceID" : 4,
      "context" : "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al. (2002); Gopalan et al.",
      "startOffset" : 276,
      "endOffset" : 298
    }, {
      "referenceID" : 4,
      "context" : "Introduction Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years Kushilevitz and Mansour (1993); Mansour (1995); Schapire and Sellie (1996); Gilbert et al. (2002); Gopalan et al. (2008); Akavia (2010).",
      "startOffset" : 276,
      "endOffset" : 321
    }, {
      "referenceID" : 0,
      "context" : "(2008); Akavia (2010). In almost all cases, known algorithms for learning or interpolating sparse polynomials require query access to the unknown polynomial.",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "Smoothed-analysis, pioneered in Spielman and Teng (2004), has now become a common alternative for problems that seem intractable in the worst-case.",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "Approach and Related Work The problem of recovering the sparsest solution of a set of underdetermined linear equations has received significant recent attention in the context of compressed sensing Candès et al. (2006); Candès and Tao (2005); Donoho (2006).",
      "startOffset" : 198,
      "endOffset" : 219
    }, {
      "referenceID" : 2,
      "context" : "(2006); Candès and Tao (2005); Donoho (2006).",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "(2006); Candès and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise.",
      "startOffset" : 8,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "(2006); Candès and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support. The recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing.",
      "startOffset" : 8,
      "endOffset" : 383
    }, {
      "referenceID" : 2,
      "context" : "(2006); Candès and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support. The recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing.",
      "startOffset" : 8,
      "endOffset" : 409
    }, {
      "referenceID" : 2,
      "context" : "(2006); Candès and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support. The recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing. The authors show that the problem of learning a sparse polynomial is equivalent to recovering the unknown sparse coefficient vector using linear measurements. The key difference from standard compressed sensing is that the unknown vector has exponential length, having one coordinate for each Fourier coefficient. By applying techniques from compressed sensing theory, namely Restricted Isometry Property (see Stobbe and Krause (2012)) and incoherence (see Negahban and Shah (2012)), the authors independently established results for reconstructing sparse polynomials using convex optimization.",
      "startOffset" : 8,
      "endOffset" : 972
    }, {
      "referenceID" : 2,
      "context" : "(2006); Candès and Tao (2005); Donoho (2006). In compressed sensing, one tries to recover an unknown sparse vector using few linear observations (measurements), possibly in the presence of noise. A large body of recent results has established conditions under which convex relaxations can exactly recover the unknown vector or its support. The recent papers Stobbe and Krause (2012); Negahban and Shah (2012) are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing. The authors show that the problem of learning a sparse polynomial is equivalent to recovering the unknown sparse coefficient vector using linear measurements. The key difference from standard compressed sensing is that the unknown vector has exponential length, having one coordinate for each Fourier coefficient. By applying techniques from compressed sensing theory, namely Restricted Isometry Property (see Stobbe and Krause (2012)) and incoherence (see Negahban and Shah (2012)), the authors independently established results for reconstructing sparse polynomials using convex optimization.",
      "startOffset" : 8,
      "endOffset" : 1019
    }, {
      "referenceID" : 1,
      "context" : "We remark here on the interesting recent work of Andoni et al. (2014) that approximately learns sparse polynomial functions when the underlying domain is Gaussian.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "We remark here on the interesting recent work of Andoni et al. (2014) that approximately learns sparse polynomial functions when the underlying domain is Gaussian. Their results do not seem to translate to the Boolean domain. We also note the work of Kalai et al. (2009) that gives an algorithm for learning sparse Boolean functions with respect to a randomly chosen product distribution on {−1, 1}n.",
      "startOffset" : 49,
      "endOffset" : 271
    }, {
      "referenceID" : 11,
      "context" : "We use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012).",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "We use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012). Specifically, for the remainder of this paper, we rely on Negahban and Shah (2012) as a point of reference.",
      "startOffset" : 48,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "We use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012). Specifically, for the remainder of this paper, we rely on Negahban and Shah (2012) as a point of reference.",
      "startOffset" : 48,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : "We use the compressed sensing framework used in Negahban and Shah (2012) and Stobbe and Krause (2012). Specifically, for the remainder of this paper, we rely on Negahban and Shah (2012) as a point of reference. We review their framework and explain how we use it to obtain our results. Theorem 10 (Negahban and Shah (2012)) Any two columns of A satisfy the incoherence property, (AS1) T AS2 ≤ 4 √ n m with probability at least 1− 2 256n for S1 6= S2.",
      "startOffset" : 48,
      "endOffset" : 323
    }, {
      "referenceID" : 11,
      "context" : "The compressive sensing framework in Negahban and Shah (2012) along with the incoherence property has the following intuition: In a problem where an approximately sparse (βS ) feasible solution (coefficients in a sparse set I ⊆ S are ’big’ and others ’small’) is known to exist, the optimum (β S ) approximates the sparse solution well and the error is bounded by the small coefficients and noise in the observation.",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "The compressive sensing framework in Negahban and Shah (2012) along with the incoherence property has the following intuition: In a problem where an approximately sparse (βS ) feasible solution (coefficients in a sparse set I ⊆ S are ’big’ and others ’small’) is known to exist, the optimum (β S ) approximates the sparse solution well and the error is bounded by the small coefficients and noise in the observation. We state a slightly general result, that is implied by the results in Negahban and Shah (2012). Theorem 11 (Negahban and Shah (2012)) For any family of subsets I ∈ 2[n] such that |I| = s, m = 4096ns2 and c1 = 4, c2 = 8, for any feasible solution βS of program 6, we have: ‖βS − β opt S ‖2 ≤ c1ǫ+ c2 ( n m )1/4 ‖βIc ⋂ S‖1 (7) with probability at least 1−O ( 1 4n )",
      "startOffset" : 37,
      "endOffset" : 512
    }, {
      "referenceID" : 11,
      "context" : "The compressive sensing framework in Negahban and Shah (2012) along with the incoherence property has the following intuition: In a problem where an approximately sparse (βS ) feasible solution (coefficients in a sparse set I ⊆ S are ’big’ and others ’small’) is known to exist, the optimum (β S ) approximates the sparse solution well and the error is bounded by the small coefficients and noise in the observation. We state a slightly general result, that is implied by the results in Negahban and Shah (2012). Theorem 11 (Negahban and Shah (2012)) For any family of subsets I ∈ 2[n] such that |I| = s, m = 4096ns2 and c1 = 4, c2 = 8, for any feasible solution βS of program 6, we have: ‖βS − β opt S ‖2 ≤ c1ǫ+ c2 ( n m )1/4 ‖βIc ⋂ S‖1 (7) with probability at least 1−O ( 1 4n )",
      "startOffset" : 37,
      "endOffset" : 550
    }, {
      "referenceID" : 10,
      "context" : "slowdown (see Mossel et al. (2003)).",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "slowdown (see Mossel et al. (2003)). As far as we are aware of, the best running time for learning a k-junta is n0.6kpoly(n) due to Valiant (2012). In Section 6, using algebraic insights from the previous cases, we show that if rank ( f−1(1) )",
      "startOffset" : 14,
      "endOffset" : 147
    } ],
    "year" : 2017,
    "abstractText" : "Let f : {−1, 1}n → R be a polynomial with at most s non-zero real coefficients. We give an algorithm for exactly reconstructing f given random examples only from the uniform distribution on {−1, 1}n that runs in time polynomial in n and 2s and succeeds if each coefficient of f has been perturbed by a small Gaussian (or any other reasonable distribution on the reals). Learning sparse polynomials over the Boolean domain in time polynomial in n and 2s is considered a notoriously hard problem in the worst-case. Our result shows that the problem is tractable in the smoothedanalysis setting. Our proof combines a method for identifying unique sign patterns induced by the underlying monomials of f with recent work in compressed sensing. We identify other natural conditions on f for which our techniques will succeed.",
    "creator" : "LaTeX with hyperref package"
  }
}