{
  "name" : "1405.5358.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Off-Policy Shaping Ensembles in Reinforcement Learning",
    "authors" : [ "Anna Harutyunyan", "Peter Vrancx", "Ann Nowé" ],
    "emails" : [ "anowe}@vub.ac.be" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning (RL) is a framework [24], where an agent learns from interacting with its (typically Markovian) environment. The bulk of RL algorithms focus on the on-policy setup, in which the agent learns only about the policy it is executing. While the offpolicy setup, in which the agent’s behavior and target policies are allowed to differ is arguably more versatile, its use in practice has been hindered by the convergence issues arising when combined with function approximation (a likely scenario, given any reasonable problem); e.g. the popular Q-learning potentially diverges [1]. This issue was recently resolved by the advancement of the family of gradient temporal-difference methods, such as Greedy-GQ [18]. An interesting implication of this is the possibility to learn multiple tasks in parallel from a shared experience stream in a sound framework, an architecture dubbed Horde by Sutton et al [26]. In the spirit of ensemble methods [31], we use this idea in the context of learning a single task faster. Our larger aim is to devise ensembles of policies that improve the (off-policy) learning speed of a task online in real time, without incurring extra sample or computational costs.\nWe choose the policies in our ensemble to be related through potential-based reward shaping. Reward shaping is a well-known technique to speed up the learning process by injecting domain knowledge into the reward function. The idea of considering multiple shaping signals instead of a single one, is relatively recent: Devlin et al. observe that it improves performance in the multi-agent context [6], and Brys et al. using a multi-objectivization formalism demonstrate its usefullness while treating different shapings as correlated objectives [4].\nThe scenario we consider in this paper is that of off-policy learning under fixed behavior, a scenario Maei et al. [18] refer to as latent learning. This is often the setup in applications where the environment samples are costly and a failure is highly penalized, making the usual trial and error tactic implausible, e.g. robotic applications.\n1 AI Lab, Vrije Universiteit Brussel, Belgium, email: {anna.harutyunyan, timbrys, pvrancx, anowe}@vub.ac.be\nOne can imagine an agent executing a safe exploratory policy, while learning control policies for a variety of tasks.\nWe note that even though the effects of reward shaping in this latent learning context are bound to be limited, since a large part of its benefits lie in guiding exploration during learning, we witness a significant rise in performance, making this a validation of the effectiveness of reward shaping purely as a means of faster knowledge propagation. See Section 3 for a discussion.\nUnlike the existing ensembles in RL, this is the first policy ensemble architecture capable of learning online in real-time and sound w.r.t. convergence in realistic setups – guarantees provided by Horde [26]. The limitation (as with Horde in general) is that it can only be applied in the latent learning setup, to ensure convergence.\nOutline In the following section, we give a brief overview of definitions and notation. Section 3 further motivates the use of Horde and multiple shaping signals to form our ensemble. Section 4 summarizes our architecture, and describes the rank voting mechanism used for combining policies. Section 5 gives experimental results in the mountain car domain, and Section 6 concludes and discusses future work directions."
    }, {
      "heading" : "2 Background",
      "text" : "The environment of a RL agent is usually modeled as a Markov Decision Process (MDP) [23] given by a 4-tuple 〈S,A, T,R〉, where S is the set of states, A is the set of actions available to the agent, T : S × A × S → R is the transition function with T (s, a, s′) denoting the probability of ending up in state s′ upon taking action a in state s, and R : S × A × S → R is the reward function with R(s, a, s′) denoting the expected reward on the transition from s to s′ upon taking action a. The Markovian assumption is that st+1 and the reward rt+1 only depend on st and at, where t denotes the discrete time step. A stochastic policy π : S ×A→ R defines a probability distribution for actions in each state:\nπ(a, s) = Pr(at = a|st = s) (1) Value functions estimate the utility of policies via their expected cumulative reward. In the discounted setting, the state-action value function Qπ : S ×A→ R is given by:\nQπ(s, a) = Eπ[γ trt+1 + γ t+1rt+2 + . . . |st = s, at = a] (2)\nwhere γ ∈ (0, 1] is the discounting factor, and Q is stored as a table with an entry for each state-action pair.\nA policy is optimal if its value is maximized for all state-action pairs. Solving an MDP implies finding the optimal policy. When the\nar X\niv :1\n40 5.\n53 58\nv1 [\ncs .A\nI] 2\n1 M\nay 2\n01 4\nenvironment dynamics (given by T and R) are unknown, one can solve the MDP by applying the family of temporal difference (TD) algorithms [24] to iteratively estimate the value functions. The following is the update rule of the popular Q-learning method in its simplest form [30]:\nQπ(st, at)← Qπ(st, at) + αδt (3)\nδt = (rt+1 + γ max a∗∈A\nQπ(st+1, a ∗)−Qπ(st, at)) (4)\nwhere rt+1 is the reward received at the transition (st, at, st+1), α is the learning rate or step size, δt is the TD error and st+1 is drawn according to T given at. Eligibility traces controlled by a trace decay parameter λ can be used as a way to speed up knowledge propagation [24].\nJaakkola et al. [13] show that in the tabular case this process converges to the optimal solution, under standard stochastic approximation assumptions.\nWhen the state or action spaces are too large, or continuous, tabular representations do not suffice and one needs to use function approximation (FA). The state (or state-action) space is then represented through a set of features φ, and the algorithms learn the value of a parameter vector θ. In the (common) linear case:\nQ(st, at) = θ Tφst,at (5)\nand (3) becomes:\nθt+1 ← θt + αδtφt, (6)\nwhere we slightly abuse notation by letting φt denote the stateaction features φst,at , and δt is still computed according to (4)."
    }, {
      "heading" : "2.1 Horde",
      "text" : "Unfortunately, FA can cause off-policy bootstrapping methods, such as Q-learning, to diverge even on simple problems [1, 27]. The family of gradient temporal-difference (GTD) algorithms resolve this issue for the first time, while keeping the constant per-step complexity, provided a fixed (or slowly changing) behavior [25, 17]. They accomplish this2 by performing gradient descent on a reformulated objective function, which ensures convergence to the TD fixpoint by introducing a gradient bias into the TD update. Mechanistically, it requires maintaining and learning a second set of weights w, along with θ, with the following update rules:3\nθt+1 ← θt + αtδtφt − αγφ′t(φTt wt) (7) wt+1 ← wt + βt(δt − φTt wt)φt (8)\nOff-policy learning allows one to learn about any policy, regardless of the behavior policy being followed. One then does not need to limit themselves to a single policy, and may learn about an arbitrary number of policies from a single stream of environment interactions (or experience), with computational considerations being the bottleneck. GTD methods not only reliably converge in realistic setups (with FA), but unlike second order algorithms with similar guarantees (e.g. LSTD [2]), run in constant time and memory per-step, and\n2 Please refer to Maei’s dissertation for the full details [16]. 3 This is the simplest form of the update rules for gradient temporal-\ndifference algorithms, namely that of TDC [25]. GQ(λ) augments this update with eligibility traces.\nare hence scalable. Sutton et al. [26] formalize a framework of parallel real-time off-policy learning, naming it Horde. They demonstrate Horde being able to learn a set of predictive and goal-oriented value functions4 in real-time from a single unsupervised stream of sensorimotor experience. There have been further successful applications of Horde in realistic robotic setups [22]. We take a different angle to the existing literature in an attempt to use the power of Horde for learning about a single task from multiple viewpoints."
    }, {
      "heading" : "2.2 Reward shaping",
      "text" : "Reward shaping augments the true reward signal with an additional heuristic reward, provided by the designer. It was originally thought of as a way of scaling up RL methods to handle difficult problems [7], as RL generally suffers from infeasibly long learning times. If applied carelessly, however, shaping can slow down or even prevent finding the optimal policy [28]. Ng et al. [21] show that grounding the shaping rewards in state potentials is both necessary and sufficient for ensuring preservation of the (optimal) policies of the original MDP. Potential-based reward shaping maintains a potential function Φ : S → R, and defines the auxiliary reward function F as:\nF (s, a, s′) = γΦ(s′)− Φ(s) (9)\nwhere γ is the main discounting factor. Intuitively, potentials are a way to encode the desirability of a state, and the shaping reward on a transition signals positive or negative progress towards desirable states. Potential-based shaping has been repeatedly validated as a way to speed up learning in problems with uninformative rewards [11].\nWe refer to the rewards augmented with shaping signals as shaped rewards, the value functions w.r.t. them as shaped value functions, and the greedy policies induced by the shaped value functions as shaped policies. Shaped policies converge to the same (optimal) policy as the base policy, but differ during the learning process."
    }, {
      "heading" : "3 Ensembles of Shapings",
      "text" : "In this section we further motivate why we find Horde to be a well-suited framework for ensemble learning by surveying ensemble methods in reinforcement learning, and argue why policies obtained by potential-based reward shaping are good candidates for such an ensemble.\nEnsemble techniques such as boosting [9] and bagging [3] are widely used in supervised learning as effective methods to reduce bias and variance of solutions. The use of ensembles in RL has been extremely sparse thus far. Most previous uses of ensembles of policies involved independent runs for each policy, with the combination happening post-factum [8]. This is limited in practical usage, since it requires a large computational and sample overhead, assumes a repeatable setup, and does not improve learning speed. Others, in general, lack convergence guarantees,5 either using mixed on- and offpolicy learners [31], or Q-learners under function approximation [4]. In general, an off-policy setup seems inevitable when considering ensembles of policies; it is surely only interesting if the policies reflect information different from the behavior, since the strength of\n4 Sutton et al. [26] give Horde in terms of general value functions, each with 4 auxilary inputs: π, γ, r, z. In this paper we always assume π to be the greedy policy w.r.t. to Q, γ and z shared between all demons, and r to be related to the base reward via a shaping reward. 5 See the discussion on convergence in Section 6.1.2 of van Hasselt’s dissertation [29].\nensemble learning lies in the diversity of information its components contribute [14]. Q-learning in this setup is not reliable in the presence of FA. While the unofficial mantra is that in practice under a sufficiently similar (e.g. -greedy) policy, Q-learning used with FA does not diverge, even despite the famous counterexamples [1, 27], ensembles of diverse Q-learners are bound to have larger disagreement amongst themselves and with the behavior policy, and have a much larger potential of becoming unstable.6\nThe ability to learn multiple policies reliably in parallel in a realistic setup is provided by the Horde architecture. For this reason, we believe Horde to be an ideally suited framework for ensemble learning in RL.\nNow we turn to the question of the choice of components of our ensemble. Recall that our larger aim is to use ensembles to speed up learning of a single task in real time. Krogh and Vedelsby [14] show in the context of neural networks that effective ensembles have accurate and diverse components, namely that they make their errors at different parts of the space. In the RL context this diversity can be expressed through several aspects, related to dimensions of the learning process: (1) diversity of experience, (2) diversity of algorithms and (3) diversity of reward signals. Diversity of experience naturally implies high sample complexity, and assumes either a multi-agent setup, or learning in stages. Diversity of algorithms may run into convergence issues, unless all algorithms are sound off-policy, by the argument above. Marivate and Littman [19] consider diversity of MDPs, by improving performance in a generalized MDP through an ensemble trained on sample MDPs, which also requires a two-stage learning process. In the context of our aim of improving learning speed, we focus on the latter aspect of diversity: diversity of reward signals.\nAs discussed in Section 2.2, potential-based reward shaping provides a framework for enriching the base reward by incorporating heuristics that express the desirability of states. One can usually think of multiple such heuristics for a single problem, each effective in different situations. Combining them naı̈vely, e.g. with linear scalarization on the potentials, may be uninformative since the heuristics may counterweigh each other at some parts of the space, and “cancel out”. On the other hand, it is typically infeasible for the designer to handcode all tradeoffs without executing each shaping separately. Horde provides a sound framework to learn and maintain all of the shapings in parallel, enabling the possibility of using any (scale free) ensemble methods for combination.\nShaping off-policy We note that we are straying from convention in using reward shaping in an off-policy latent learning setup. The effects of reward shaping on the learning process are usually considered to lie in the guidance of exploration during learning [10, 20, 21]. Laud and DeJong [15] formalize this by showing that the difficulty of learning is most dependent on the reward horizon, a measure of the number of decisions a learning agent must make before experiencing accurate feedback, and that reward shaping artificially reduces this horizon. In our setting we assume no control over the agent’s behavior, and the performance benefits in Section 5 must be explained by a different effect. Namely, shaping rewards in the TD updates aid faster knowledge propagation, which we now observe decoupled from guidance of exploration due to the off-policy latent learning setup.\n6 See the discussion in Section 8.5 of Sutton and Barto [24] relating potential to diverge to the proximity of behavior and target policies. To the best of our knowledge, there have been no formal results on this topic.\nIn the next section we describe the exact architecture used for this paper, and the combination method we chose."
    }, {
      "heading" : "4 Architecture",
      "text" : "We maintain our Horde of shapings as a set D of Greedy-GQ(λ)learners. The reward function is a vector: R = 〈R + F0, R + F1, . . . , R + F|D|−1〉, where F0 = 0 (d0 always learns on the base reward alone), andFi, i = 1, . . . , |D|−1 are potential-based rewards given by (9) on potentials Φ1,Φ2, . . . provided by the designer. We adopt the terminology of Sutton et al. [26], and refer to individual agents within Horde as demons. Each demon learns a greedy policy πi w.r.t. its reward Ri. We refer to the demons learning on shaped rewards as shaped demons.\nAt any point of learning, we can devise a combination policy by collecting votes on action preferences from all shaped demons (d1, d2, . . .). Wiering et al. [31] discuss several intuitive ways to do so, e.g. majority voting, rank voting, Boltzman multiplication, etc. We describe rank voting used in this paper, but in general the choice of ensemble combination is up to the designer, and may depend on the specifics of the problem and architecture. Even though the base demon d0 does not contribute a vote, we maintain it as a part of the ensemble.\nRank voting Each demon (except for d0) ranks its n actions according to its greedy policy, casting a vote of n− 1 for its most, and a vote of 0 for its least preferred actions. The voting schema then is defined for policies, rather than value functions, which mitigates the\nmagnitude bias.7 We slightly modify the formulation from [31], by ranking Q-values, instead of policy probabilities, i.e. let r : D×A→ N be the ranking map of a demon. Then rd(a) > rd(a′), if and only if Qd(s, a) > Qd(s, a′). The combination or ensemble policy acts greedily w.r.t. the cumulative preference values P :\nP (st, a) = |D|−1∑ d=1 rd(a), ∀a ∈ A (10)\nIn the next section we validate our approach on the typical mountain car benchmark and interpret the results."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section we give comparison results between the individuals in our ensemble, and the combination policy. We remind the reader that while all policies eventually arrive at the same (optimal) solution, our focus is the time it takes them to get there.\nWe focus our attention to a classical benchmark domain of mountain car [24]. The task is to drive an underpowered car up a hill (Fig. 2). The (continuous) state of the system is composed of the current position (in [−1.2, 0.6]) and the current velocity (in [−0.07, 0.07]) of the car. Actions are discrete, a throttle of {−1, 0, 1}. The agent starts at the position −0.5 and a velocity of 0, and the goal is at the position 0.6. The rewards are −1 for every time step. An episode ends when the goal is reached, or when 2000 steps8 have elapsed. The state space is approximated with the standard tile-coding technique [24], using ten tilings of 10 × 10, with a parameter vector learnt for each action. The behavior policy is a uniform distribution over all actions at each time step.\nIn this domain we define three intuitive shaping potentials. Each is normalized into the range [0, 1].\nRight shaping. Encourage progress to the right (in the direction of the goal). This potential is flawed by design, since in order to get to the goal, one needs to first move away from it.\nΦ1(x) = cr × x (11)\n7 Note that even though the shaped policies are the same upon convergence – the value functions are not. 8 Note the significantly shorter lifetime of an episode here, as compared to results in Degris et al. [5]; since the shaped rewards are more informative, they can get by with very rarely reaching the goal.\nHeight shaping. Encourage higher positions (potential energy), where height h is computed according to the formula in Fig. 2.\nΦ2(x) = ch × h (12)\nSpeed shaping. Encourage higher speeds (kinetic energy).\nΦ3(x) = cs × |ẋ|2 (13)\nHere x = 〈x, ẋ〉 is the state (position and velocity), and c = 〈cr, ch, cs〉 is a vector of tuned scaling constants.9\nThus our architecture has 4 demons:< d0, d1, d2, d3 >, where d0 learns on the base reward, and the others on their respective shaping rewards. The combination policy is formed via rank voting, which we found to outperform majority voting, and a variant of Q-value voting on this problem.\nThe third (speed) shaping turns out to be the most helpful universally. If this is the case one would likely prefer to just use that single shaping on its own, but we assume such information is not available a priori, which is a more realistic (and challenging) situation. To make our experiment more interesting we consider two scenarios: with and without this best shaping. Ideally we would like our combination method to be able to outperform the two comparable shapings in the first scenario, and pick out the best shaping in the second scenario.\nWe used γ = 0.99. The learning parameters were tuned and selected to be λ = 0.4, β = 0.0001, α = 〈0.1, 0.05, 0.1, 0.1〉, where λ is the trace decay parameter, β the step size for the second set of weights in Greedy-GQ, and α the vector of step sizes for the value functions of our demons.10 We ran 1000 independent runs of 100 episodes each. The evaluation was done by interrupting the off-policy learner every 5 episodes, and executing each demon’s greedy policy once. No learning was allowed during evaluation. The graphs reflect the average base reward. The initial and final performance refer to the first and last 20% of a run.\nThe results in Fig. 3, and Tables 1 and 2 show that individual shapings alone aid learning speed significantly. The combination method meets our desiderata: it either statistically matches or is better than the best shaping at any stage, overall outperforming all single shapings. The exception is the final performance of the run in Scenario 2, where the performance of the best shaping is significantly different\n9 The scaling of potentials is in general a challenging problem in reward shaping research. Finding the right scaling factor requires a lot of a priori tuning, and the factor is generally assumed constant over the state space. The scalable nature of Horde could be used to lift this problem, by learning multiple preset scales for each potential, and combining them via either a voting method like the one described here, or a meta-learner. See Section 6. 10 These were tuned individually, as the value functions differ in magnitude.\nfrom the combination. The difference in actual averaged performance however is relatively small, and arguably negligible.\nWe note that even the best performances in these tables do not reach the maximum attainable, if behaving online.11"
    }, {
      "heading" : "6 Conclusions and future work",
      "text" : "We gave the first policy ensemble that is both sound and capable of learning in real time, by exploiting the power of Horde architecture to learn a single policy well. The value functions in our ensemble\n11 An artefact of value-function methods learnt off-policy under a behavior policy rarely reaching the goal. Given longer learning periods, they will get closer and closer to the attainable optimum, but we choose not to concern ourselves with this in the context of this paper, as our main focus lies in improving on the learning time within the off-policy framework.\nlearned on shaped rewards, and we used a voting method to combine them. We validated the approach on the classical mountain car domain, considering two scenarios: with and without a clearly best shaping signal. In the former scenario, the combination outperformed single shapings, and in the latter was able to match the performance of that best shaping. In general, we expect to see larger benefits on larger problems; a more extensive suite of experiments is subject to future work.\nThe primary limitation of Horde is the requirement to keep the behavior policy fixed (or change it slowly). While this is an important case, relaxing this constraint would further expand the effectiveness of the architecture. This is a topic of ongoing research in the GTD community.\nFuture work In this work, we considered an ad-hoc voting approach to combining shapings. One of the possible future directions would be to learn optimal combination ways via predicting some shared fitness value w.r.t. the policies induced by the learnt value functions. The challenge with this is that the meta-learning has to happen at a much faster pace for it to be useful in speeding up the main learning process. In the case of shapings, this is doubly the case, since they all eventually converge to the same (optimal) policy. The size of this window of opportunity is related to the size of the problem.\nThe scalability of Horde allows for learning potentially thousands of value functions efficiently in parallel. While in the context of shaping it will rarely be sensible to actually define thousands of distinct shapings, one could imagine defining shaping potentials with many different scaling factors each, and having a demon combining the shapings from each group. This would not only mitigate the scaling problem, but potentially make the representation more flexible by having non-static scaling factors throughout the state space. This has a roughly similar flavor to the approach of Marivate and Littman [19], who learn to solve many variants of a problem for the best parameter settings in a generalized MDP.\nOne could go further and attempt to learn the best potential functions [20, 12]. As before, one needs to be realistic about attainability of learning this in time, since as argued by Ng et al. [21], the best potential function correlates with the optimal value function V ∗, learning which would solve the base problem itself and render the potentials pointless."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "Anna Harutyunyan is supported by the IWT-SBO project MIRAD (grant nr. 120057). Tim Brys is funded by a Ph.D grant of the Research Foundation-Flanders (FWO)."
    } ],
    "references" : [ {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L. Baird" ],
      "venue" : "In Proceedings of the Twelfth International Conference on Machine Learning, pp. 30–37. Morgan Kaufmann, ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning",
      "author" : [ "S.J. Bradtke", "A.G. Barto", "P. Kaelbling" ],
      "venue" : "Machine Learning, pp. 22–33, ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Bagging predictors",
      "author" : [ "L. Breiman" ],
      "venue" : "Mach. Learn., 24(2), 123–140, ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Multi-objectivization in reinforcement learning",
      "author" : [ "T. Brys", "A. Harutyunyan", "P. Vrancx", "M.E. Taylor", "D. Kudenko", "A. Nowé" ],
      "venue" : "Technical Report AI-TR-13-354, AI Lab, Vrije Universiteit Brussel, ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Off-policy actor-critic",
      "author" : [ "T. Degris", "M. White", "R.S. Sutton" ],
      "venue" : "Proceedings of the Twenty-Ninth International Conference on Machine Learning (ICML), ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An empirical study of potentialbased reward shaping and advice in complex",
      "author" : [ "S. Devlin", "D. Kudenko", "M. Grzes" ],
      "venue" : "multi-agent systems’, Advances in Complex Systems (ACS), 14(02), 251–278, ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Robot shaping: Experiment in behavior engineering",
      "author" : [ "M. Dorigo", "M. Colombetti" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "Ensemble methods for reinforcement learning with function approximation.",
      "author" : [ "S. Fauer", "F. Schwenker" ],
      "venue" : "in MCS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Experiments with a New Boosting Algorithm",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "International Conference on Machine Learning, pp. 148–156, ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Improving Exploration in Reinforcement Learning through Domain Knowledge and Parameter Analysis",
      "author" : [ "M. Grzes" ],
      "venue" : "Ph.D. dissertation, University of York",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Theoretical and empirical analysis of reward shaping in reinforcement learning",
      "author" : [ "M. Grzes", "D. Kudenko" ],
      "venue" : "Machine Learning and Applications, Fourth International Conference on, 0, 337–344, ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Online learning of shaping rewards in reinforcement learning",
      "author" : [ "M. Grzes", "D. Kudenko" ],
      "venue" : "Neural Networks, 23(4), 541 – 550, ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convergence of stochastic iterative dynamic programming algorithms",
      "author" : [ "T. Jaakkola", "M.I. Jordan", "S.P. Singh" ],
      "venue" : "Neural Computation, 6, 1185–1201, ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Neural network ensembles",
      "author" : [ "A. Krogh", "J. Vedelsby" ],
      "venue" : "cross validation, and active learning’, in Advances in Neural Information Processing Systems, pp. 231–238. MIT Press, ",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "The influence of reward on the speed of reinforcement learning: An analysis of shaping",
      "author" : [ "A. Laud", "G. DeJong" ],
      "venue" : "In Proc. 20th International Conference on Machine Learning. AAAI Press, ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Gradient Temporal-Difference Learning Algorithms",
      "author" : [ "H.R. Maei" ],
      "venue" : "Ph.D. dissertation, University of Alberta",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "gq(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
      "author" : [ "H.R. Maei", "R.S. Sutton" ],
      "venue" : "Proceedings of the Third Conf. on Artificial General Intelligence., ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Toward offpolicy learning control with function approximation",
      "author" : [ "H.R. Maei", "C. Szepesvári", "S. Bhatnagar", "R.S. Sutton" ],
      "venue" : "Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML 2010), eds., Johannes Fürnkranz and Thorsten Joachims, pp. 719–726. Omnipress, ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An ensemble of linearly combined reinforcement-learning agents",
      "author" : [ "V. Marivate", "M. Littman" ],
      "venue" : "AAAI Workshops",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Automatic shaping and decomposition of reward functions",
      "author" : [ "B. Marthi" ],
      "venue" : "Proceedings of the 24th International Conference on Machine Learning, ICML ’07, pp. 601–608, New York, NY, USA, ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Policy invariance under reward transformations: Theory and application to reward shaping",
      "author" : [ "A.Y. Ng", "D. Harada", "S. Russell" ],
      "venue" : "In Proceedings of the Sixteenth International Conference on Machine Learning, pp. 278–287. Morgan Kaufmann, ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Adaptive artificial limbs: a real-time approach to prediction and anticipation",
      "author" : [ "P.M. Pilarski", "M.R. Dawson", "T. Degris", "J.P. Carey", "K.M. Chan", "J.S. Hebert", "R.S. Sutton" ],
      "venue" : "Robotics Automation Magazine, IEEE, 20(1), 53–64, ",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : "John Wiley & Sons, Inc., New York, NY, USA, 1st edn.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "volume 116, Cambridge Univ Press",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvri", "E. Wiewiora" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning, ",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup" ],
      "venue" : "The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 2, AAMAS ’11, pp. 761–768, Richland, SC, ",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "J.N. Tsitsiklis", "B. Van Roy" ],
      "venue" : "Technical report, IEEE Transactions on Automatic Control, ",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning to drive a bicycle using reinforcement learning and shaping",
      "author" : [ "J. Randløv", "P. Alstrøm" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1998
    }, {
      "title" : "Insights in reinforcement learning : formal analysis and empirical evaluation of temporal-difference learning algorithms, Ph.D",
      "author" : [ "H. van Hasselt" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    }, {
      "title" : "Q-learning",
      "author" : [ "C.J.C.H. Watkins", "P. Dayan" ],
      "venue" : "Machine Learning, 8(3), 272–292, ",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Ensemble algorithms in reinforcement learning’, Systems, Man, and Cybernetics, Part B: Cybernetics",
      "author" : [ "M.A. Wiering", "H. van Hasselt" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Reinforcement learning (RL) is a framework [24], where an agent learns from interacting with its (typically Markovian) environment.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "the popular Q-learning potentially diverges [1].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "This issue was recently resolved by the advancement of the family of gradient temporal-difference methods, such as Greedy-GQ [18].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 25,
      "context" : "An interesting implication of this is the possibility to learn multiple tasks in parallel from a shared experience stream in a sound framework, an architecture dubbed Horde by Sutton et al [26].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 30,
      "context" : "In the spirit of ensemble methods [31], we use this idea in the context of learning a single task faster.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "observe that it improves performance in the multi-agent context [6], and Brys et al.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "using a multi-objectivization formalism demonstrate its usefullness while treating different shapings as correlated objectives [4].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "[18] refer to as latent learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "convergence in realistic setups – guarantees provided by Horde [26].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "The environment of a RL agent is usually modeled as a Markov Decision Process (MDP) [23] given by a 4-tuple 〈S,A, T,R〉, where S is the set of states, A is the set of actions available to the agent, T : S × A × S → R is the transition function with T (s, a, s′) denoting the probability of ending up in state s′ upon taking action a in state s, and R : S × A × S → R is the reward function with R(s, a, s′) denoting the expected reward on the transition from s to s′ upon taking action a.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "environment dynamics (given by T and R) are unknown, one can solve the MDP by applying the family of temporal difference (TD) algorithms [24] to iteratively estimate the value functions.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 29,
      "context" : "The following is the update rule of the popular Q-learning method in its simplest form [30]:",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "Eligibility traces controlled by a trace decay parameter λ can be used as a way to speed up knowledge propagation [24].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "[13] show that in the tabular case this process converges to the optimal solution, under standard stochastic approximation assumptions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "as Q-learning, to diverge even on simple problems [1, 27].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 26,
      "context" : "as Q-learning, to diverge even on simple problems [1, 27].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 24,
      "context" : "The family of gradient temporal-difference (GTD) algorithms resolve this issue for the first time, while keeping the constant per-step complexity, provided a fixed (or slowly changing) behavior [25, 17].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 16,
      "context" : "The family of gradient temporal-difference (GTD) algorithms resolve this issue for the first time, while keeping the constant per-step complexity, provided a fixed (or slowly changing) behavior [25, 17].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 1,
      "context" : "LSTD [2]), run in constant time and memory per-step, and",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 15,
      "context" : "2 Please refer to Maei’s dissertation for the full details [16].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "3 This is the simplest form of the update rules for gradient temporaldifference algorithms, namely that of TDC [25].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "[26] formalize a framework of parallel real-time off-policy learning, naming it Horde.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "There have been further successful applications of Horde in realistic robotic setups [22].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "It was originally thought of as a way of scaling up RL methods to handle difficult problems [7], as RL generally suffers from infeasibly long learning times.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 27,
      "context" : "If applied carelessly, however, shaping can slow down or even prevent finding the optimal policy [28].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "[21] show that grounding the shaping rewards in state potentials is both necessary and sufficient for ensuring preservation of the (optimal) policies of the original MDP.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "Potential-based shaping has been repeatedly validated as a way to speed up learning in problems with uninformative rewards [11].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "Ensemble techniques such as boosting [9] and bagging [3] are widely used in supervised learning as effective methods to reduce bias and variance of solutions.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "Ensemble techniques such as boosting [9] and bagging [3] are widely used in supervised learning as effective methods to reduce bias and variance of solutions.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "Most previous uses of ensembles of policies involved independent runs for each policy, with the combination happening post-factum [8].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 30,
      "context" : "Others, in general, lack convergence guarantees, either using mixed on- and offpolicy learners [31], or Q-learners under function approximation [4].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "Others, in general, lack convergence guarantees, either using mixed on- and offpolicy learners [31], or Q-learners under function approximation [4].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 25,
      "context" : "[26] give Horde in terms of general value functions, each with 4 auxilary inputs: π, γ, r, z.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "2 of van Hasselt’s dissertation [29].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "ensemble learning lies in the diversity of information its components contribute [14].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "-greedy) policy, Q-learning used with FA does not diverge, even despite the famous counterexamples [1, 27], ensembles of diverse Q-learners are bound to have larger disagreement amongst themselves and with the behavior policy, and have a much larger potential of becoming unstable.",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 26,
      "context" : "-greedy) policy, Q-learning used with FA does not diverge, even despite the famous counterexamples [1, 27], ensembles of diverse Q-learners are bound to have larger disagreement amongst themselves and with the behavior policy, and have a much larger potential of becoming unstable.",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "Krogh and Vedelsby [14] show in the context of neural networks that effective ensembles have accurate and diverse components, namely that they make their errors at different parts of the space.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "Marivate and Littman [19] consider diversity of MDPs, by improving performance in a generalized MDP through an ensemble trained on sample MDPs, which also requires a two-stage learning process.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "The effects of reward shaping on the learning process are usually considered to lie in the guidance of exploration during learning [10, 20, 21].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 19,
      "context" : "The effects of reward shaping on the learning process are usually considered to lie in the guidance of exploration during learning [10, 20, 21].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 20,
      "context" : "The effects of reward shaping on the learning process are usually considered to lie in the guidance of exploration during learning [10, 20, 21].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "Laud and DeJong [15] formalize this by showing that the difficulty of learning is most dependent on the reward horizon, a measure of the number of decisions a learning agent must make before experiencing accurate feedback, and that reward shaping artificially reduces this horizon.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "5 of Sutton and Barto [24] relating potential to diverge to the proximity of behavior and target policies.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 25,
      "context" : "[26], and refer to individual agents within Horde as demons.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31] discuss several intuitive ways to do so, e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "We slightly modify the formulation from [31], by ranking Q-values, instead of policy probabilities, i.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "We focus our attention to a classical benchmark domain of mountain car [24].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "The state space is approximated with the standard tile-coding technique [24], using ten tilings of 10 × 10, with a parameter vector learnt for each action.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Each is normalized into the range [0, 1].",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "[5]; since the shaped rewards are more informative, they can get by with very rarely reaching the goal.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "This has a roughly similar flavor to the approach of Marivate and Littman [19], who learn to solve many variants of a problem for the best parameter settings in a generalized MDP.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "One could go further and attempt to learn the best potential functions [20, 12].",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "One could go further and attempt to learn the best potential functions [20, 12].",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 20,
      "context" : "[21], the best potential function correlates with the optimal value function V ∗, learning which would solve the base problem itself and render the potentials pointless.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2014,
    "abstractText" : "Recent advances of gradient temporal-difference methods allow to learn off-policy multiple value functions in parallel without sacrificing convergence guarantees or computational efficiency. This opens up new possibilities for sound ensemble techniques in reinforcement learning. In this work we propose learning an ensemble of policies related through potential-based shaping rewards. The ensemble induces a combination policy by using a voting mechanism on its components. Learning happens in real time, and we empirically show the combination policy to outperform the individual policies of the ensemble.",
    "creator" : "LaTeX with hyperref package"
  }
}