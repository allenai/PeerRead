{"conference": "ICLR 2017 conference submission", "title": "Variational Recurrent Adversarial Deep Domain Adaptation", "abstract": "We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model's ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches.", "reviews": [{"is_meta_review": true, "comments": "This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.\n\nExperiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.\n\nI don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:\n\n- As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.\n\n- One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The paper offers a contribution to domain adaptation. The novelty with respect to methodology is modest, utilizing an existing variational RNN formulation and adversarial training method in this setting. But the application is important and results are strong. Improving the analysis of the results, and studying variants of the approach to understand the contribution of each component, will make this paper considerably stronnger. We encourage the authors to revise accordingly.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Response to all reviewers", "OTHER_KEYS": "Sanjay Purushotham", "comments": "Thank you all for your time and thoughtful comments! We have incorporated your suggestions to improve our paper.\n\nFirst, we would like to clarify the key contributions of our paper. Motivated by challenges within the healthcare domain, we are proposing a novel unsupervised deep domain adaptation framework for knowledge transfer within healthcare time series data. The framework 1) captures temporal dependencies present in the time series data, and 2) transfers these dependencies from  the source to target domain via adversarial training, and 3) is able to obtain state-of-the-art results, outperforming previous domain adaptation approaches. To the best of our knowledge, this is the first time that complex temporal latent dependencies have been used to transfer knowledge in healthcare multivariate time series data. It is worth noting that our domain adaptation framework is general and is suitable for applications where dependencies are present in the multivariate time series data. \n\nThe other major contribution of our work is moreso from an applications perspective. Healthcare data is a new frontier for machine learning. The data we worked with is complex: it\u2019s temporal, it's multivariate, and it\u2019s noisy,. Healthcare data is also quite different from the data that machine learning scientists are accustomed to working with. The dataset we used (which is the largest publicly available medical data healthset) only have a few thousand training examples (which we then further subdivided into multiple domains). This is considered a small dataset. Further, domain adaptation is typically done when there are ample unlabeled target examples which can be used to learn some mutual representation. We did not have this luxury. Despite, our model outperformed all other models.\n\nWe have uploaded a revised draft to address the questions and comments of the reviewers. In the revised draft, we have added an appendix section with the following new results:\n- Discussion on different training methods for our VRADA model and their results\n- Results on variations of VRADA model including\n  --- Adversarial training at each time step\n  --- Impact of adversarial training on the memory cell state visualizations\n  --- Study on the effect of reconstruction loss on the prediction performance\n- R-DANN model architecture block diagram\n", "IS_META_REVIEW": false, "DATE": "20 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting applications but with unconvincing results", "is_meta_review": false, "comments": "The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data. The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings.\n\nThe new contribution of the work is relatively small. It extends VRNN with adversarial training for learning domain agnostic representations. From the experimental results, the proposed method clearly out-performs competing algorithms. However, it is not clear where the advantage is coming from. The only difference between the proposed method and R-DANN is using variational RNN vs RNN. Little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in Figure 4.  \n\nDetailed comments:\n1. Please provide more details on what is plotted in Figure 1. Is 1 (b) is the t-sne projection of representations learned by DANN or R-DANN? The text in section 4.4 suggests it\u2019s the later case. It is surprising to see such a regular plot for VRADA.  What do you think are the two dominant latent factors encoded in figure 1 (c)? \n\n2. In Table 2, the two baselines have quite significant difference in performance testing on the entire target (including validation set) vs on the test set only. VRADA, on the other hand, performs almost identical in these two settings. Could you please offer some explanation on this?\n\n3. Please explain figure 3 and 4 in more details. how to interpret the x-axis of figure 3, and the x and y axes of figure 4. Again the right two plots in figure 4 are extremely regular comparing to the ones on the left. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Final review.", "is_meta_review": false, "comments": "Update: I thank the authors for their comments! After reading them, I still think the paper is not novel enough so I'm leaving the rating untouched.\n\nThis paper proposes a domain adaptation technique for time series. The core of the approach is a combination of variational recurrent neural networks and adversarial domain adaptation (at the last time step).\n\nPros:\n\n1. The authors consider a very important application of domain adaptation.\n\n2. The paper is well-written and relatively easy to read.\n\n3. Solid empirical evaluation. The authors compare their method against several recent domain adaptation techniques on a number of datasets.\n\nCons:\n\n1. The novelty of the approach is relatively low: it\u2019s just a straightforward fusion of the existing techniques.\n\n2. The paper lacks any motivation for use of the particular combination (VRNN and RevGrad). I still believe comparable results can be obtained by polishing R-DANN (e.g. carefully penalizing domain discrepancy at every step)\n\nAdditional comments:\n\n1. I\u2019m not convinced by the discussion presented in Section 4.4. I don\u2019t think the visualization of firing patterns can be used to support the efficiency of the proposed method.\n\n2. Figure 1(c) looks very suspicious. I can hardly believe t-SNE could produce this _very_ regular structure for non-degenerate (non-synthetic, real-world) data.\n\nOverall, it\u2019s a solid paper but I\u2019m not sure if it is up to the ICLR standard.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "A combination of variational RNN and domain adversarial networks", "is_meta_review": false, "comments": "This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.\n\nExperiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.\n\nI don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:\n\n- As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.\n\n- One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Clarification", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "09 Dec 2016", "is_meta_review": false}, {"TITLE": "R-DANN baseline", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "07 Dec 2016", "is_meta_review": false}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Sanjay Purushotham, Wilka Carvalho, Tanachat Nilanon, Yan Liu", "KEYWORDS": "We propose Variational Recurrent Adversarial Deep Domain Adaptation approach to capture and transfer temporal latent dependencies in multivariate time-series data", "accepted": true, "id": ""}
