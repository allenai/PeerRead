{
  "name" : "1606.09636.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mesoscopic representation of texts as complex networks",
    "authors" : [ "Henrique Ferraz de Arruda", "Vanessa Queiroz Marinho", "Diego Raphael Amancio", "Filipi Nascimento Silva", "Luciano da Fontoura Costa" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Mesoscopic representation of texts as complex networks\nHenrique Ferraz de Arruda, Vanessa Queiroz Marinho, and Diego Raphael Amancio∗ Institute of Mathematics and Computer Science,\nUniversity of São Paulo, São Carlos, SP, Brazil.\nFilipi Nascimento Silva and Luciano da Fontoura Costa São Carlos Institute of Physics, University of São Paulo, São Carlos, SP, Brazil\nTexts are complex structures emerging from an intricate system consisting of syntactical constraints and semantical relationships. While the complete modeling of such structures is impractical owing to the high level of complexity inherent to linguistic constructions, under a limited domain, certain tasks can still be performed. Recently, statistical techniques aiming at analysis of texts, referred to as text analytics, have departed from the use of simple word count statistics towards a new paradigm. Text mining now hinges on a more sophisticate set of methods, including the representation of texts as complex networks. In this perspective, networks represent a set of textual elements, typically words; and links are established via adjacency relationships. While current word-adjacency (co-occurrence) methods successfully grasp syntactical and stylistic features of written texts, they are unable to represent important aspects of textual data, such as its topical structure. As a consequence, the mesoscopic structure of texts is often overlooked by current methodologies. In order to grasp mesoscopic characteristics of semantical content in written texts, we devised a network approach which is able to analyze documents in a multi-scale, mesoscopic fashion. In the proposed model, a limited amount of adjacent paragraphs are represented as nodes, which are connected whenever they share a minimum semantical content. To illustrate the capabilities of our model, we present, as a use case, a qualitative analysis of “Alice’s Adventures in Wonderland”, a novel by Lewis Carroll. We show that the mesoscopic structure of documents modeled as networks reveals many semantic traits of texts, a feature that could be explored in a myriad of semantic-based applications.\nI. INTRODUCTION\nThe availability of an ever growing amount of complex data brought up by the age of information has strongly impacted science, giving rise to a novel perspective on data analysis. The use and development of systematic approaches to analyze complex data has already become mandatory for many problems in a wide range of knowledge areas, such as physics [1], biology [2, 3], medicine [4] and even humanities [5, 6]. This also includes techniques devoted to the systematic analysis of texts, known as text mining [7]. Traditionally, processes involving text analytics were solely based on simple statistics considering mostly the frequency of words occurrence [8, 9], which are, in general, suitable for the task of text classification [10]. However, more sophisticated methods have been devised for more complex tasks, such as to quantify the words relevance [11, 12] in a document. These techniques can be employed to detect, for instance, important topics in a given text [13, 14]. Even more complex are the methods used to study the relationships among words or topics in a document or a set of documents. Such a kind of analysis can be undertaken by considering semantic similarities [15] or linguistic characteristics [16]. By using these new techniques, many other applications could be performed, e.g., automatic summarization [17], sentiment analysis [18] or authorship detection [19]. Apart\n∗Electronic address: diego.raphael@gmail.com\nfrom the mentioned approaches, texts datasets can be analyzed in terms of the relationships among their elements, such as words and paragraphs. In this context, texts can be regarded as a complex structure, rendering complex networks a suitable representation for such structures.\nComplex networks are a discrete representation of interactions among elements in a complex system. In such a representation, elements are referred to as nodes and pairwise interactions are represented via edges. A wellknown approach to construct complex networks from a text dataset is the word-adjacency (or co-occurrence) technique [20, 21], which is based on connecting pairs of words that are immediately adjacent. Such a strategy of mapping texts into networks is a simplification of networks formed by syntactical links [22]. Despite this seeming limitation, word adjacency networks have been employed successfully to address a great variety of natural language processing problems. This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.\nPrevious works employing the co-occurrence network representation of texts have pointed out some of its limitations. Perhaps the most critical disadvantage associated with such a representation is its inability to portray the topical structure present in many kinds of texts. At a glance, the topical structure of a text is expected to naturally emerge from its network representation through a pronounced community structure, however, this hardly ever happens on typical co-occurrence networks, which\nar X\niv :1\n60 6.\n09 63\n6v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n16\n2 present no community structure at all [36]. This suggests that this representation does not effectively capture the information lying in the mesoscopic structure of the text, such as topics and subtopics. Besides, in co-occurrence networks, the information regarding the temporal evolution along a text is also overlooked, as the emphasis of such models is to analyze language/stylistic dependent features.\nThe rest of the paper is organized as follows: Section II describes in details our approach to create the mesoscopic network from a given document. Section III provides a case study of our approach with the analysis of a book. Finally, Section IV shows a synthesis of our paper and the perspectives for further studies."
    }, {
      "heading" : "II. METHODOLOGY",
      "text" : "This section describes, in details, the procedure to obtain such mesoscopic complex networks from sets of organized text, which includes books and other classes of text with paragraph structure. Here, we also briefly present the technique employed to construct visualizations for such networks.\nIn recent years, a new set of techniques has been introduced to form networks from unstructured documents, which takes into consideration its mesoscopic structure [36]. Such networks are generated by connecting words existing in the same context, which is defined in terms of fixed window length. This approach was found to be able to produce networks with strong community structures, with each community corresponding to the main contextual topics or subtopics of the text [36]. Even though the semantical organization of texts is captured in such models, they are unable to represent the temporal evolution of the story being told.\nHere, we extend the concepts introduced in [36] to derive a new technique to construct networks from organized texts. Our methodology accounts for the two main characteristics overlooked by the traditional approaches: (a) the mesoscopic structure and (b) its evolution along a text. To consider (a), unlike traditional models linking adjacent words, we use larger pieces of texts as the simplest text unit, which are connected according to the similarity among themselves. The temporal evolution of ideas and concepts in (b) is recovered from our model because, by construction, nodes (i.e. adjacent pieces of texts) are always connected.\nIn the following, we consider an organized text as a sequence of words segmented by paragraphs. In our analysis, paragraphs can be innate from the source material, such as the paragraphs in a book, or can be inferred from the text own structure, for instance, by considering each paragraph as a sequence containing a fixed number of words.\nOur approach starts with the typical pre-processing step employed for semantical-based text analysis. First, punctuation marks and numbers are removed. We also\nremove words conveying low contextual meaning, i.e. the stopwords. Examples of stopwords are articles and prepositions. If a lemmatization technique [7] is available for the language being considered, it is used to normalize concepts. In this step, words are reduced to their canonical forms, so that inflections in verbal tense, number, case or gender are disregarded. For example, the sentence “ ‘Oh, I’ve had such a curious dream!’ said Alice” becomes “curious dream say alice”, after being preprocessed. Next, we employ the tf-idf (term frequencyinverse document frequency) technique [7], which defines a map tf-idf(w, d,D) quantifying the importance of each word w in a given document d from a set of documents D. The tf-idf(w, d,D) map is computed as\ntf-idf(w, d,D) = tf(w, d)× idf(w,D), (1)\nwhere tf(w, d), the term-frequency component, accounts for the relevance of w ∈ d and idf(w,D), the inverse document frequency, quantifies the frequency of w in all d ∈ D. Many variations of both tf and idf terms have been proposed [7]. In this paper, we consider tf(w, d) as the raw frequency of a given word w in a document d. The idf(w,D) was calculated as\nidf(w,D) = 1 + log ( |D| fw ) , (2)\nwhere |D| is the total number of documents in D and fw is the frequency of occurrence for a given word w in D, i.e. fw is the number of documents in which w occurs at least once. Note that, when fw = |D|, log(|D|/fw) = 0. If the additional term were not used in equation 2, the value of tf-idf would always be zero, despite of any rawfrequency observed in the current document (tf term).\nThe mesoscopic network is generated from the preprocessed text, hereafter referred to as organized text O. The organized textO consists of a sequence of paragraphs O = (p0, p1, p2 . . . ) with each paragraph pi comprising a sequence of words pi = (wi0, wi1, wi2 . . . ). Differently from the co-occurrence model, where nodes represent words, here, we model entire paragraphs or sequences of consecutive paragraphs by nodes. In particular, for a choice of window size ∆, each possible subsequence comprising ∆ paragraphs in O, P∆k = (pk, pk+1, . . . pk+∆−1), is given a node representations in the devised mesoscopic network. Fig. 1(a) illustrates the process to obtain the nodes of the mesoscopic network.\nThe edges of the mesoscopic network are devised considering a contextual similarity measurement obtained among all pairs of the sequences of paragraphs P∆k in the investigated document. Here, we employed the traditional bag of words combined with the cosine similarity measurement [7]. Bearing in mind that the number of words in paragraphs can vary significantly along the investigated document, the choice of cosine similarity was due it not depending on the length of the chunks of text being compared [? ]. First, for each considered sequence of paragraphs P , a vector WP , spanning the\n3 P0 3\nP1 3 P2 3\nP3 3\nP4 3\nP0 3\nP1 3 P2\n3\nP3 3\nP4 3\nP0 3\np0\np1\np2\np3\np4\np5\np6\nOrganized text (O)\n(a) (c)\n(b)\nParagraph windows (Pk3)\nP1 3\nP2 3\nP3 3\nP4 3\nFIG. 1: Illustration of the presented methodology. Initially, the text is organized in sets of subsequent and overlapping windows P 3k , each containing 3 structural paragraphs, as shown in (a). Next, the cosine similarity is calculated among all pairs of text windows (illustrated by the width of the lines in b). The mesoscopic network is obtained by maintaining only connections among pairs of windows with similarity higher than a threshold value T . This is illustrated by the network visualization shown in (c).\nsame number of words present in O, is obtained from the tf-idf map tf-idf(w,P,O) applied to each word w in O. Note that, when a certain word w is not present in P ,\ntf-idf(w,P,O) = 0. The content similarity measurement S(PA, PB) between two paragraph windows PA and PB is obtained using\nS(PA, PB) =\n∑ w∈O\ntf-idf(w,PA, O)× tf-idf(w,PB , O)√ ∑ w∈O tf-idf(w,PA, O)2 √ ∑ w∈O tf-idf(w,PB , O)2 . (3)\nAs a result, a fully connected network is created (see Fig. 1(b)), where edge weights correspond to the similarity S(PA, PB) among each pair of nodes. The final mesoscopic network is obtained by pruning the weakest connections, i.e. the links whose weight takes a value be-\nlow a given threshold T . After such a procedure, edge weights are ignored, resulting in a unweighted network (see Fig. 1(c)).\nTo better understand the overall structure of mesoscopic networks, we visualized the network structure\n4 using a technique based on force-directed nodes placement. In particular, we use a technique based on the Fruchterman-Reingold (FR) [37] algorithm, in which the network is regarded as a system of nodes behaving like particles that interact among themselves by the action of two types of forces: attractive forces, existing only between connected nodes, and repulsive forces, that exist between all pairs of nodes. By minimizing the energy of such a system, the network organizes itself in a graphically appealing layout. Such a visualization technique naturally hisghlights many aspects of the topological structure of networks [37].\nTo illustrate the proposed methodology, we investigated the obtained properties from the mesoscopic network modelling a well-known book: “Alice’s Adventures in Wonderland”. This story revolves around the adventures of a little girl, called Alice, after she falls in a hole and arrives in a unknown and fantasy world. The book was written in 1865 by Charles Lutwidge Dodgson under the pseudonym Lewis Carroll. It is divided into the following twelve chapters:\n1. Down the Rabbit-Hole\n2. The Pool of Tears\n3. A Caucus-Race and a Long Tale\n4. The Rabbit Sends in a Little Bill\n5. Advice from a Caterpillar\n6. Pig and Pepper\n7. A Mad Tea-Party\n8. The Queen’s Croquet-Ground\n9. The Mock Turtle’s Story\n10. The Lobster Quadrille\n11. Who Stole the Tarts?\n12. Alice’s Evidence"
    }, {
      "heading" : "III. RESULTS AND DISCUSSION",
      "text" : "Two mesoscopic network, G1 and G2, were constructed from the book under investigation by applying the proposed methodology, for which two distinct thresholds, respectively, T1 = 0.46 and T2 = 0.24 were considered. For such a process, we have chosen a fixed window size ∆ = 20.\nWe start the analysis of the obtained mesoscopic structures by initially investigating the properties of the G1 network, which is simpler. In the following, the analysis is undertaken considering a 2D visualization of the G1 network, which is shown in Figure 2 and was obtained by employing the FR algorithm. Since nodes sharing the same paragraphs become strongly connected among\nthemselves, a pronounced chain-like structure naturally emerges on the devised mesoscopic network. In addition, such a structure is aligned to the ordering of the nodes along the book. This property is better observed in Figure 2(a), where the color of each node indicates its position along the text. Nevertheless, the existence of connections among distant nodes along the text indicates regions of high contextual similarity that are not a result of overlapping sequences of paragraphs. In the obtained visualization, such a structure creates bridges of connections among contextually similar regions of nodes, which in turn brings them more closely together along the chain-like structure of the network.\nTo better understand the relationship between the contextual information of the book and its mesoscopic structure, we used the chapter organization of the book to segment the obtained network. Such aspect is visualized in Figure 2(b), where the chapter of each node is indicated by a color according to the legend. By taking into consideration the connectivity among the chapters of the book, derived the following observations:\n• In chapter 1, we note that there is no strong connection among its paragraphs and those from other chapters, except for chapter 2, which is explained by the aforementioned overlap between subsequent paragraphs. The lack of long range connections among the nodes of chapter 1 may happens because the main subject of this chapter is substantially different from every other in the book. In this chapter, the story unfolds in a realistic scenario and has no descriptions of the fantasy locations and creatures found in the rest of the book, except for the Rabbit ;\n• Chapters 2, 3, and the beginning of chapter 4 are connected among themselves. This may be a consequence of the fact that all these chapters describe the period of the story when Alice, the protagonist, was very frightened about the world she just jumped in. In addition, all these chapters mentioned when she cried and formed a pool of tears;\n• In chapter 5, there are strong connections between regions lying in the same chapter. This probably happens because there is an long conversation between Alice and the Caterpillar, in which they discuss the many sizes she had during the previous chapters;\n• The connection between chapter 7 and chapter 11 can be related to the character The Hatter, who is drinking tea in both chapters. Furthermore, in both situations, he talked about some specific kinds of food, which are related to the tea party, eg. bread and butter;\n• There is a group of highly connected nodes in the end of chapter 9 and in the beginning of chapter 10. This probably happens because in the final paragraphs of chapter 9, Alice met The Mock Turtle and their talk ended only in chapter 10.\n5 Similarly to Figure 2, Figure 3 displays a visualization of the G2 network, which was constructed using a lower threshold value T2 = 0.24. By varying this threshold, we illustrate the potential of our method in describing the characteristics of the network in a multi-scale fashion. From Figure 3(a), we can observe that the network maintains a chain-like structure similar to that found in the G1 network. However, such a network presents much more connections among nodes representing paragraphs belonging to different parts of the book. This is a consequence of the G2 network capturing more fine-grained details about the relationships among the paragraphs. By a closer look at the differences between G1 and G2, we note, for instance, that, while chapter 1 is connected only to chapter 2 in G1, in G2 it becomes connected with many other parts of the book, in particular, with chapters 2, 3, 4, 6, 7, and 8. The analysis of more fine-grained networks, however, may present some disadvantages. Since it tends to incorporate much more local characteristics, it may include noise and relationships not driven by a strong contextual content."
    }, {
      "heading" : "IV. CONCLUSION",
      "text" : "In order to investigate the mesoscopic properties of texts modelled as networks, we proposed a novel approach that considers the semantical similarity between textual segments. Distinctly from previous studies, we modeled sequences of adjacent paragraphs as nodes in a network, whose links are established by content similarity. By doing so, we could capture two important features present in written texts: long-range correlations and the temporal evolution of the story. The main feature of our approach concerns its ability to analyze documents in a multi-scale fashion. Specifically, two parameters control the scale: (i) ∆: the number of paragraphs in each window, and (ii) T : the threshold used to prune connections among nodes with low contextual similarity.\nWe tested our approach in a well-known book, “Alice’s Adventures in Wonderland”, by employing network visualization techniques on the generated mesoscopic networks. Many insights can be drawn from the visualizations by tracing a parallel between its structure and the story presented in the book. In particular, we investigated the correspondence between the content of each chapter and underlying network structure arising from the proposed model.\nOur model uncovered many relationships among different contexts sharing the same topics, such as similar characters or places appearing throughout the story. For example, the high contextual similarity found between chapters 7 and 11 can be explained by the fact that both chapters share a recurrent subject revolving around the character “The Hatter” approaching the tea party thematic. Note that similar textual inferences could not be drawn from models solely based on local features, as it is the case of traditional word-adjacency or syntactical net-\nworks, as they emphasize only stylistic textual subtleties. The proposed network representation paves the way for developing new techniques that could be applied to automatically analyze the mesoscopic structure of documents. These techniques could be used to improve traditional approaches used to tackle typical text mining problems under a new perspective. This capability should be further explored in future works, for instance, by measuring the efficiency of our model in text classification, summarization and similar applications where an accurate semantic analysis plays a prominent role in the characterization of written texts."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors acknowledge financial support from Capes-Brazil, São Paulo Research Foundation (FAPESP) (grant no. 2015/08003-4, 2015/05676-8, 2014/20830-0 and 2011/50761-2), CNPq-Brazil (grant no. 307333/2013-2) and NAP-PRP-USP.\n6 (a) (b)\nFIG. 2: Visualization of the network G1, representing the book Alice’s Adventures in Wonderland with a threshold T1 = 0.41. Each node indicates a sequence of paragraphs. The order of the network nodes according to the story are shown in (a). Note that the first nodes of the story appears in blue, while the last nodes are represented in a reddish color. In (b), the chapters to which paragraphs belong are represented with distinct colors.\n(a) (b)\nFIG. 3: Visualization of the network G1, representing the book Alice’s Adventures in Wonderland with a threshold T1 = 0.24. Each node indicates a sequence of paragraphs. The order of the network nodes according to the story are shown in (a). Note that the first nodes of the story appears in blue, while the last nodes are represented in a reddish color. In (b), the chapters to which paragraphs belong are represented with distinct colors.\n7 [1] S. Boccaletti, V. Latora, Y. Moreno, M. Chavez, and D.U. Hwang, Physics reports 424, 175 (2006). [2] A.-L. Barabasi and Z. N. Oltvai, Nature reviews genetics 5, 101 (2004). [3] H. F. de Arruda, C. H. Comin, M. Miazaki, M. P. Viana, and L. da Fontoura Costa, Journal of neuroscience methods 245, 1 (2015). [4] A.-L. Barabási, N. Gulbahce, and J. Loscalzo, Nature Reviews Genetics 12, 56 (2011). [5] Y. Moreno, M. Nekovee, and A. F. Pacheco, Physical Review E 69, 066130 (2004). [6] M. Kalimeri, V. Constantoudis, C. Papadimitriou, K. Karamanos, F. K. Diakonos, and H. Papageorgiou, Journal of Quantitative Linguistics 22, 101 (2015). [7] C. D. Manning and H. Schütze, Foundations of Statistical Natural Language Processing (MIT Press, Cambridge, MA, USA, 1999), ISBN 0-262-13360-1. [8] U. Y. Nahm and R. J. Mooney, in AAAI 2002 Spring Symposium on Mining Answers from Texts and Knowledge Bases (2002), vol. 1. [9] E. G. Altmann, J. B. Pierrehumbert, and A. E. Motter, PLoS One 4, e7678 (2009).\n[10] T. Joachims, in Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval (ACM, 2001), pp. 128–136. [11] J. Ramos, in Proceedings of the first instructional conference on machine learning (2003). [12] A. Hotho, A. Nürnberger, and G. Paaß, in Ldv Forum (2005), vol. 20, pp. 19–62. [13] D. M. Blei, A. Y. Ng, and M. I. Jordan, the Journal of machine Learning research 3, 993 (2003). [14] L. AlSumait, D. Barbará, and C. Domeniconi, in Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on (IEEE, 2008), pp. 3–12. [15] T. K. Landauer, P. W. Foltz, and D. Laham, Discourse processes 25, 259 (1998). [16] V. Hatzivassiloglou, L. Gravano, and A. Maganti, in Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval (ACM, 2000), pp. 224–231. [17] Y.-L. Chang and J.-T. Chien, in Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on (IEEE, 2009), pp. 1689–1692. [18] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,\nand C. Potts, in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1 (Association for Computational Linguistics, 2011), pp. 142–150. [19] X. Chen, P. Hao, R. Chandramouli, and K. Subbalakshmi, in Machine Learning and Data Mining in Pattern Recognition (Springer, 2011), pp. 375–386. [20] D. R. Amancio, O. N. Oliveira Jr., and L. F. Costa, Physica A 391, 4406 (2012). [21] A. Kulig, S. Drożdż, J. Kwapień, and P. Oświȩcimka, Phys. Rev. E 91, 032810 (2015). [22] R. Ferrer i Cancho, R. V. Solé, and R. Köhler, Phys. Rev. E 69, 051915 (2004). [23] R. Feldman, Commun. ACM 56, 82 (2013). [24] A. Mehri, A. H. Darooneh, and A. Shariati, Physica A\n391, 2429 (2012). [25] S. Segarra, M. Eisen, and A. Ribeiro, IEEE Transactions\non Signal Processing 63, 5464 (2015). [26] D. R. Amancio, Journal of Statistical Mechanics: Theory\nand Experiment 2015, P03005 (2015). [27] D. R. Amancio, PLoS ONE 10, e0136076 (2015). [28] H. F. de Arruda, L. d. F. Costa, and D. R. Amancio,\nEPL (Europhysics Letters) 113, 28007 (2016). [29] R. Mihalcea, P. Tarau, and E. Figa, in Proceedings\nof the 20th International Conference on Computational Linguistics (Association for Computational Linguistics, Stroudsburg, PA, USA, 2004), COLING ’04. [30] T. C. Silva and D. R. Amancio, EPL (Europhysics Letters) 98, 58001 (2012). [31] D. R. Amancio, O. N. Oliveira Jr., and L. da F. Costa, EPL (Europhysics Letters) 98, 18002 (2012). [32] L. Antiqueira, O. N. Oliveira Jr., L. F. Costa, and M. G. V. Nunes, Information Sciences 179, 584 (2009). [33] D. R. Amancio, M. G. V. Nunes, O. N. Oliveira Jr., and L. F. Costa, Physica A 391, 1855 (2012). [34] Q. Xuan and T.-J. Wu, Phys. Rev. E 80, 026103 (2009). [35] D. R. Amancio, L. Antiqueira, T. A. S. Pardo,\nL. F. Costa, O. N. Oliveira Jr., and M. G. V. Nunes, International Journal of Modern Physics C 19, 583 (2008). [36] H. F. de Arruda, L. d. F. Costa, and D. R. Amancio, Chaos 26, 063120 (2016). [37] T. Fruchterman and E. Reingold, Software-Practice & Experience 21, 1129 (1991)."
    } ],
    "references" : [ {
      "title" : "M",
      "author" : [ "S. Boccaletti", "V. Latora", "Y. Moreno" ],
      "venue" : "Chavez, and D.- U. Hwang, Physics reports 424, 175 ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Nature reviews genetics 5",
      "author" : [ "A.-L. Barabasi", "Z.N. Oltvai" ],
      "venue" : "101 ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Nature Reviews Genetics 12",
      "author" : [ "A.-L. Barabási", "N. Gulbahce", "J. Loscalzo" ],
      "venue" : "56 ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Physical Review E 69",
      "author" : [ "Y. Moreno", "M. Nekovee", "A.F. Pacheco" ],
      "venue" : "066130 ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Journal of Quantitative Linguistics 22",
      "author" : [ "M. Kalimeri", "V. Constantoudis", "C. Papadimitriou", "K. Karamanos", "F.K. Diakonos", "H. Papageorgiou" ],
      "venue" : "101 ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Foundations of Statistical Natural Language Processing",
      "author" : [ "C.D. Manning", "H. Schütze" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1999
    }, {
      "title" : "PLoS One 4",
      "author" : [ "E.G. Altmann", "J.B. Pierrehumbert", "A.E. Motter" ],
      "venue" : "e7678 ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and G",
      "author" : [ "A. Hotho", "A. Nürnberger" ],
      "venue" : "Paaß, in Ldv Forum ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "the Journal of machine Learning research 3",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "993 ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "in Data Mining",
      "author" : [ "L. AlSumait", "D. Barbará", "C. Domeniconi" ],
      "venue" : "2008. ICDM’08. Eighth IEEE International Conference on ",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Discourse processes 25",
      "author" : [ "T.K. Landauer", "P.W. Foltz", "D. Laham" ],
      "venue" : "259 ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "and A",
      "author" : [ "V. Hatzivassiloglou", "L. Gravano" ],
      "venue" : "Maganti, in Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "in Acoustics",
      "author" : [ "Y.-L. Chang", "J.-T. Chien" ],
      "venue" : "Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and C",
      "author" : [ "A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng" ],
      "venue" : "Potts, in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1 ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and K",
      "author" : [ "X. Chen", "P. Hao", "R. Chandramouli" ],
      "venue" : "Subbalakshmi, in Machine Learning and Data Mining in Pattern Recognition ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "O",
      "author" : [ "D.R. Amancio" ],
      "venue" : "N. Oliveira Jr., and L. F. Costa, Physica A 391, 4406 ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Phys",
      "author" : [ "A. Kulig", "S. Drożdż", "J. Kwapień", "P. Oświȩcimka" ],
      "venue" : "Rev. E 91, 032810 ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Commun",
      "author" : [ "R. Feldman" ],
      "venue" : "ACM 56, 82 ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Physica A 391",
      "author" : [ "A. Mehri", "A.H. Darooneh", "A. Shariati" ],
      "venue" : "2429 ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "IEEE Transactions on Signal Processing 63",
      "author" : [ "S. Segarra", "M. Eisen", "A. Ribeiro" ],
      "venue" : "5464 ",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Journal of Statistical Mechanics: Theory and Experiment 2015",
      "author" : [ "D.R. Amancio" ],
      "venue" : "P03005 ",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "PLoS ONE 10",
      "author" : [ "D.R. Amancio" ],
      "venue" : "e0136076 ",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and E",
      "author" : [ "R. Mihalcea", "P. Tarau" ],
      "venue" : "Figa, in Proceedings of the 20th International Conference on Computational Linguistics ",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "EPL (Europhysics Letters) 98",
      "author" : [ "T.C. Silva", "D.R. Amancio" ],
      "venue" : "58001 ",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "O",
      "author" : [ "D.R. Amancio" ],
      "venue" : "N. Oliveira Jr., and L. da F. Costa, EPL (Europhysics Letters) 98, 18002 ",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "O",
      "author" : [ "L. Antiqueira" ],
      "venue" : "N. Oliveira Jr., L. F. Costa, and M. G. V. Nunes, Information Sciences 179, 584 ",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "O",
      "author" : [ "D.R. Amancio", "M.G.V. Nunes" ],
      "venue" : "N. Oliveira Jr., and L. F. Costa, Physica A 391, 1855 ",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Phys",
      "author" : [ "Q. Xuan", "T.-J. Wu" ],
      "venue" : "Rev. E 80, 026103 ",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "O",
      "author" : [ "D.R. Amancio", "L. Antiqueira", "T.A.S. Pardo", "L.F. Costa" ],
      "venue" : "N. Oliveira Jr., and M. G. V. Nunes, International Journal of Modern Physics C 19, 583 ",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Software-Practice & Experience 21",
      "author" : [ "T. Fruchterman", "E. Reingold" ],
      "venue" : "1129 ",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1991
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The use and development of systematic approaches to analyze complex data has already become mandatory for many problems in a wide range of knowledge areas, such as physics [1], biology [2, 3], medicine [4] and even humanities [5, 6].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "The use and development of systematic approaches to analyze complex data has already become mandatory for many problems in a wide range of knowledge areas, such as physics [1], biology [2, 3], medicine [4] and even humanities [5, 6].",
      "startOffset" : 185,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "The use and development of systematic approaches to analyze complex data has already become mandatory for many problems in a wide range of knowledge areas, such as physics [1], biology [2, 3], medicine [4] and even humanities [5, 6].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "The use and development of systematic approaches to analyze complex data has already become mandatory for many problems in a wide range of knowledge areas, such as physics [1], biology [2, 3], medicine [4] and even humanities [5, 6].",
      "startOffset" : 226,
      "endOffset" : 232
    }, {
      "referenceID" : 4,
      "context" : "The use and development of systematic approaches to analyze complex data has already become mandatory for many problems in a wide range of knowledge areas, such as physics [1], biology [2, 3], medicine [4] and even humanities [5, 6].",
      "startOffset" : 226,
      "endOffset" : 232
    }, {
      "referenceID" : 5,
      "context" : "This also includes techniques devoted to the systematic analysis of texts, known as text mining [7].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "Traditionally, processes involving text analytics were solely based on simple statistics considering mostly the frequency of words occurrence [8, 9], which are, in general, suitable for the task of text classification [10].",
      "startOffset" : 142,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : "However, more sophisticated methods have been devised for more complex tasks, such as to quantify the words relevance [11, 12] in a document.",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "These techniques can be employed to detect, for instance, important topics in a given text [13, 14].",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "These techniques can be employed to detect, for instance, important topics in a given text [13, 14].",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "Such a kind of analysis can be undertaken by considering semantic similarities [15] or linguistic characteristics [16].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "Such a kind of analysis can be undertaken by considering semantic similarities [15] or linguistic characteristics [16].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : ", automatic summarization [17], sentiment analysis [18] or authorship detection [19].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : ", automatic summarization [17], sentiment analysis [18] or authorship detection [19].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : ", automatic summarization [17], sentiment analysis [18] or authorship detection [19].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "A wellknown approach to construct complex networks from a text dataset is the word-adjacency (or co-occurrence) technique [20, 21], which is based on connecting pairs of words that are immediately adjacent.",
      "startOffset" : 122,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "A wellknown approach to construct complex networks from a text dataset is the word-adjacency (or co-occurrence) technique [20, 21], which is based on connecting pairs of words that are immediately adjacent.",
      "startOffset" : 122,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 18,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 138,
      "endOffset" : 145
    }, {
      "referenceID" : 23,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 138,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 138,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 26,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 27,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 28,
      "context" : "This includes sentiment analysis [23], authorship detection [24–26], stylometry [27], text classification [28], word sense disambiguation [29–31], text summarization [32, 33], machine translation [34, 35] and others.",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "If a lemmatization technique [7] is available for the language being considered, it is used to normalize concepts.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "Next, we employ the tf-idf (term frequencyinverse document frequency) technique [7], which defines a map tf-idf(w, d,D) quantifying the importance of each word w in a given document d from a set of documents D.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "Many variations of both tf and idf terms have been proposed [7].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Here, we employed the traditional bag of words combined with the cosine similarity measurement [7].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : "In particular, we use a technique based on the Fruchterman-Reingold (FR) [37] algorithm, in which the network is regarded as a system of nodes behaving like particles that interact among themselves by the action of two types of forces: attractive forces, existing only between connected nodes, and repulsive forces, that exist between all pairs of nodes.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 29,
      "context" : "Such a visualization technique naturally hisghlights many aspects of the topological structure of networks [37].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "[1] S.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[4] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] Y.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7] C.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[9] E.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[12] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[13] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[14] L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[15] T.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[16] V.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[17] Y.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[18] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[19] X.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[20] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[21] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[23] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[24] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[25] S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[26] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[27] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[29] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[30] T.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[31] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[32] L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[33] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[34] Q.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[35] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[37] T.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "Texts are complex structures emerging from an intricate system consisting of syntactical constraints and semantical relationships. While the complete modeling of such structures is impractical owing to the high level of complexity inherent to linguistic constructions, under a limited domain, certain tasks can still be performed. Recently, statistical techniques aiming at analysis of texts, referred to as text analytics, have departed from the use of simple word count statistics towards a new paradigm. Text mining now hinges on a more sophisticate set of methods, including the representation of texts as complex networks. In this perspective, networks represent a set of textual elements, typically words; and links are established via adjacency relationships. While current word-adjacency (co-occurrence) methods successfully grasp syntactical and stylistic features of written texts, they are unable to represent important aspects of textual data, such as its topical structure. As a consequence, the mesoscopic structure of texts is often overlooked by current methodologies. In order to grasp mesoscopic characteristics of semantical content in written texts, we devised a network approach which is able to analyze documents in a multi-scale, mesoscopic fashion. In the proposed model, a limited amount of adjacent paragraphs are represented as nodes, which are connected whenever they share a minimum semantical content. To illustrate the capabilities of our model, we present, as a use case, a qualitative analysis of “Alice’s Adventures in Wonderland”, a novel by Lewis Carroll. We show that the mesoscopic structure of documents modeled as networks reveals many semantic traits of texts, a feature that could be explored in a myriad of semantic-based applications.",
    "creator" : "LaTeX with hyperref package"
  }
}