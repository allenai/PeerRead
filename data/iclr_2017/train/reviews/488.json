{"conference": "ICLR 2017 conference submission", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.", "reviews": [{"is_meta_review": true, "comments": "*** Paper Summary ***\n\nThis paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.\n\n*** Review Summary ***\n\nThe paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.\n\n*** Detailed Review ***\n\nThe paper reads well. I have only a few comments regarding experiments and link to prior resarch:\n\nExperiments:\n\n- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\n- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?\n- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?\n\nRelated Work:\n\nI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.\n\nAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.\n\n*** References ***\n\nMarginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.\nStacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.\nLearning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.\nLearning Deep Architectures for AI, Yoshua Bengio 2009\nLarge Scale Transductive SVMs. Ronan Collobert et al 2006\nOptimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper is concerned with extending adversarial and virtual adversarial training to text classification tasks. The main technical contribution is to apply perturbations to word embeddings rather than discrete input symbols. Excellent empirical performance is reported across a variety of tasks. \n \n The reviewers were consensual in acknowledging the clarity and significance of the contribution, highlighting the quality of the numerical experiments. Moreover, the authors were responsive in the rebuttal phase and updated their paper with reviewers suggestions (such as the svm-related comparisons). \n \n The AC thus recommends accepting this work as a poster.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Reads well, missing theoretical differences with past techniques.", "is_meta_review": false, "comments": "The authors propose to apply virtual adversarial training to semi-supervised classification.\n\nIt is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.\n\nIn terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP).\n\nConcerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup).\n\nOverall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice. Theoretical and experimental comparison with past work is missing.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Good paper. The idea is simple but its result contributes new knowledge to the litterature", "is_meta_review": false, "comments": "This paper applies the idea of the adversarial training and virtual adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Extending the idea of the adversarial training to the text tasks is simple but non-trivial. Overall the paper is worth to publish. \n\nI only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "balancing constraint", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "16 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.", "is_meta_review": false, "comments": "*** Paper Summary ***\n\nThis paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.\n\n*** Review Summary ***\n\nThe paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.\n\n*** Detailed Review ***\n\nThe paper reads well. I have only a few comments regarding experiments and link to prior resarch:\n\nExperiments:\n\n- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\n- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?\n- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?\n\nRelated Work:\n\nI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.\n\nAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.\n\n*** References ***\n\nMarginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.\nStacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.\nLearning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.\nLearning Deep Architectures for AI, Yoshua Bengio 2009\nLarge Scale Transductive SVMs. Ronan Collobert et al 2006\nOptimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Experiment settings", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Takeru Miyato, Andrew M. Dai, Ian Goodfellow", "accepted": true, "id": ""}
