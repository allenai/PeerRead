{
  "name" : "1106.0566.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords. Evolutionary algorithm, Mutation rate, Adaptation, Dynamic optimization."
    }, {
      "heading" : "1 Introduction",
      "text" : "Evolutionary Algorithms (EAs) are stochastic search algorithms, which have been used to solve many optimization problems in real-world applications [34, 47, 51]. As one of the primary operators in the framework of EAs, the mutation operator has significant influence on the performance of an EA. During the past decades, various strategies\n∗Corresponding author: Ke Tang (E-mail: ketang@ustc.edu.cn. Telephone: +86 551 360 0754)\nof controlling the parameters of the mutation operator have been developed to promote the performance of EAs. Some of them were concerned with the mutation operators for binary search space (e.g., [3, 20, 21, 29, 43, 45, 48]), while some were dedicated to continuous search space [1, 23, 42, 50]. In this paper, we restrict our investigation to the former.\nAs the most commonly used mutation operator for binary search space, the so-called bitwise mutation operator flips each bit of an individual (solution) with a uniform probability Pm, where Pm is called the mutation rate. Early investigations often employed a fixed mutation rate over the whole optimization process [20], but further studies have revealed that a time-variable mutation rate scheme might be better than a fixed mutation rate. According to Bäck [5], Hinterding et al. [27] and Thierens [48], there are three classes of time-variable mutation rate schemes : dynamic mutation rate schemes, adaptive mutation rate schemes and self-adaptive mutation schemes. During the past decades, a large number of empirical investigations have been dedicated to show the advantages of various time-variable mutation rate schemes. Holland [28] proposed a time-variable mutation rate scheme for Genetic Algorithm (GA). After fourteen years, Fogarty [21] designed a number of dynamic mutation rate schemes with which the performance of GAs was significantly improved on some static optimization problems. Inspired by Evolution Strategies (ES), Bäck [2] proposed a self-adaptive mutation scheme for GAs for the continuous search space. Later he carried out both theoretical analysis and empirical study to show that for multimodal static optimization problems there might exist some “Optimal Mutation Rate Schedule” that can accelerate the search of EAs [3]. Srinivas and Patnaik proposed a GA with adaptive mutation rate scheme and adaptive crossover scheme (Adaptive Genetic Algorithm, AGA) in [44]. They empirically showed that AGA outperformed Simple GA (SGA) on a number of static benchmark optimization problems. Smith and Fogarty [43] carried out empirical comparisons between an EA with self-adaptive mutation rate scheme and a number of EAs with different fixed mutation rates, and they found that the former outperforms the latter on a number of static optimization problems. Thierens [48] designed two adaptive mutation rate schemes, whose advantages over a fixed mutation rate were demonstrated by experimental studies.\nIn comparison with empirical studies, the theoretical investigations on time-variable mutation rate schemes are much fewer. Droste, Jansen and Wegener [18, 29] analyzed a (1 + 1) EA with a dynamic mutation rate scheme, and they proved that the expected number of generations spent by this (1 + 1) EA on the so-called PathToJump problem is O(n2 logn). In contrast, with a probability that converges to 1 (with respect to the problem size n), the (1 + 1) EA with the fixed mutation rate 1/n will spend a super-polynomial number of generations to optimize the above PathToJump problem. On the basis of [18] and [29], Jansen and Wegener [30] carried out further theoretical investigations for the above (1 + 1) EA (with a dynamic mutation rate), and they presented a number of time complexity results on some well-known benchmark problems, such as OneMax [3, 12, 19] and LeadingOnes [12, 41]. The investigations in [30] show us that the specific dynamic mutation rate scheme outperforms (in terms of the runtime of (1 + 1) EA) the fixed mutation rate 1/n significantly on PathToJump, while is slightly inferior on OneMax and LeadingOnes. As we might expect, there have also been a few studies demonstrating the potential weakness of some specific time-variable mutation rate schemes (e.g., [40]), but it is a drop in the bucket compared with the positive results. Besides, these negative results only showed that there is no time-variable mutation rate scheme is universally good for an EA over a large variety of problems, which did not exclude the usefulness of timevariable mutation rate schemes. In other words, no general result is available so far demonstrating the complete failure of the time-variable mutation rate schemes under certain conditions.\nIn this paper, we prove for both individual-based and population-based EAs that time-variable mutation rate schemes (including any dynamic, adaptive and self-adaptive mutation schemes) cannot significantly outperform a well-chosen fixed mutation rate on some Dynamic Optimization Problems (DOPs) [9, 33]. Unlike static optimization problems that maintain problem instances, stationary objective functions and constraints consistently, for DOPs, “the objective function, the problem instance, or constraints may change over time” [33]. In most investigated DOPs, the above changes of DOPs will directly result in the movements of global optima [7, 37, 49]. However, such movements of different DOPs can follow distinctive manners. To representatively characterize moving global optima in DOPs, we employ the so-called BDOP class in our theoretical investigations, which models the movements of global optima as random walks of binary strings in the solution space. The BDOP class is a representative DOP model for theoretical investigations, which consist of DOPs with different dynamic degrees. Concretely, the dynamic degree of a DOP belonging to the BDOP class (named BDOP in the paper) mainly depends on the value of the so-called shifting rate σ ∈ (0, 1/2], which is a parameter controlling the random walks of BDOPs’ global optima. A larger σ tends to lead to larger frequency and size of change for the moving global optimum.\nIntuitively, the simplest way to cope with a DOP is to regard the DOP as a new static optimization problem after every change. In this way, a DOP can be solved by traditional EAs or other problem-specific algorithms designed for static optimization problems. However, this kind of approaches require vast computational resources (e.g., computation time), which is not practical for many real-world DOPs (e.g., dynamic vehicle routing problem\n[10] and online scheduling problem [15]). Under this circumstance, intensive investigations have been carried out to understand the behaviors of EAs on DOPs or to design better EAs for DOPs, among which the theoretical investigations are often restricted to the (1 + 1) EA. From a theoretical perspective, Rohlfshagen et al. studied how the frequency and magnitude of the optimum’s movement influence the performance of the (1 + 1) EA for dynamic optimization [39]. Surprisingly, on two specific problems called Magnitude and Balance, they showed that larger frequency and magnitude do not necessarily lead to a worse performance of the EA. Stanhope and Daida [46] studied the fitness dynamics of the (1 + 1) EA on the so-called dynamic BitMatching problem that was frequently used in previous empirical investigations [17, 49, 52]. By a similar approach, Branke and Wang [11] compared the (1+1) and (1, 2) EAs on the dynamic BitMatching problem. Droste [16, 17] studied the time complexity of the (1 + 1) EA with both one-bit mutation and bitwise mutation (with mutation rate 1/n) on the dynamic BitMatching problem. These results established the theoretical foundations of the field. However, theoretical investigation that aims at revealing the relationship between the adaptation of mutation rate and the performance of EAs on DOPs still lacks, though there has been some related empirical studies [38, 45].\nIn this paper, we demonstrate theoretically the relationship between time-variable mutation schemes and the performances of both individual-based and population-based EAs on the BDOP class. In our study, we still adopt the classical time complexity criterion, the first hitting time [24, 25, 26], to measure the performances of EAs theoretically. Based on the measure, an EA is said to be efficient on a DOP, if and only if the corresponding first hitting time is polynomial in the problem size with a probability that is at least the reciprocal of a polynomial function of the problem size n. Otherwise, the EA is said to be inefficient (on the DOP). According to the popular perspective that adaptation of mutation rate might be helpful, one might expect that there are some time-variable mutation scheme that can improve the performances of EAs so that they can cope with BDOPs efficiently. However, in this paper, we find that once the asymptotic order of the shifting rate σ is larger than logn/n2 (i.e., σ = ω(logn/n2)), the BDOPs will become essentially hard for (1 + 1) EA with any time-variable mutation scheme in which the mutation rate at every generation is upper bounded by 1− 1/ logn; When the asymptotic order of the shifting rate σ is larger than logn/n (i.e., σ = ω(logn/n)), the BDOPs will become essentially hard for the (1 + λ) EA1 (λ can be any polynomial function of n) with any time-variable mutation scheme in which the mutation rate at every generation is upper bounded by 1− 1/ logn. Case studies on an instance of the BDOP class called the BitMatchingD problem further demonstrate the limitations of any time-variable mutation scheme via concrete time complexity results on both (1 + 1) and (1 + λ) EAs, and also show the positive impact of population on the performances of EAs.\nThe main contributions and their significance of this paper are summarized as follows: First, our investigation provides theoretical evidence for the view that, compared with a fixed mutation rate, adopting some time-variable mutation rate scheme is not always a significantly better choice. It is the first time that some general results for any time-variable mutation rate scheme are given and proven rigorously. Second, this paper substantially increases the understanding of the role of mutation in the context of DOP, and the relationship between time-variable mutation rate schemes and time complexities of EAs are investigated theoretically in depth. Third, by comparing individual-based and population-based EAs on BDOPs, our investigations revealed theoretically for the first time that population may have positive impact on the performances of EAs for solving DOPs.\nThe rest of the paper is organized as follows: Section 2 introduces the DOPs and algorithms analyzed in this paper. Section 3 studies the impact of time-variable mutation rate schemes on performance of the (1 + 1) EA over the BDOP class. Section 4 further studies the impact of time-variable mutation rate schemes on performance of the (1 + λ) EA over the BDOP class. Section 5 offers some discussions, interpretations and generalization for the theoretical results obtained in this paper. Section 6 concludes the whole paper."
    }, {
      "heading" : "2 Problem and Algorithm",
      "text" : "In this section, we introduce some preliminaries for this paper, including the notations of asymptotic orders, the BDOP class and EAs discussed in this paper."
    }, {
      "heading" : "2.1 Preliminaries",
      "text" : "To facilitate our analysis, we first introduce some notations that are used in comparing the asymptotic growth order of functions. Let g1 = g1(n) and g2 = g2(n) be two positive functions of n, then [32]:\n• g1 = O(g2), iff ∃n0 ∈ N, c ∈ R+: ∀n > n0, g1(n) ≤ cg2(n) (g1 is asymptotically bounded above by g2 up to a constant factor, and the asymptotic order of g1 is no larger than that of g2);\n1The (1 + λ) EA is a population-based EA which maintains a unique parent individual and generates λ offspring individuals at each generation.\n• g1 = Ω(g2), iff g2 = O(g1) (The asymptotic order of g1 is no smaller than that of g2);\n• g1 = Θ(g2), iff g1 = O(g2) and g1 = Ω(g2) both hold (The asymptotic order of g1 is the same to that of g2);\n• g1 = o(g2), iff limn→∞ g1(n)/g2(n) = 0 (g1/g2 approximates 0 when n → ∞, and the asymptotic order of g1 is smaller than that of g2);\n• g1 = ω(g2), iff g2 = o(g1) (The asymptotic order of g1 is larger than that of g2).\nMoreover, to distinguish the polynomial functions from those super-polynomial functions of n, we utilize the following notations:\n• g1 ≺ Poly(n) and 1 g1 ≻ 1Poly(n) both hold iff ∃c ∈ R + 0 : g1(n) = O(n c);\n• g1 ≻ SuperPoly(n) and 1 g1 ≺ 1SuperPoly(n) both hold iff ∀c ∈ R + 0 : g1(n) = ω(n c).\nIn this paper, the above notations are further utilized in representing the asymptotic orders related to probabilities:\nDefinition 1 (Overwhelming Probability and Super-polynomially Small Probability). A probability P1 is regarded as “an overwhelming probability” if and only if there exists some super-polynomial function g(n) of the problem size n (g(n) ≻ SuperPoly(n)) and a positive integer n0 such that ∀n ≥ n0 : P1 ≥ 1 − 1/g(n) holds. A probability P2 is regarded as “a super-polynomially small probability” (or “a probability that is super-polynomially close to 0”) if and only if there exists some super-polynomial function g(n) of the problem size n (g(n) ≻ SuperPoly(n)) and a positive integer n0 such that ∀n ≥ n0 : P2 ≤ 1/g(n) holds.\nIn the following parts of the section, we will first describe the general DOP model for our theoretical analysis. After that, we will present the concrete DOP analyzed in this paper. Finally, we introduce the (1 + 1) EA and time-variable mutation rate scheme."
    }, {
      "heading" : "2.2 A Theoretical DOP Model",
      "text" : "In this subsection, we define a theoretical model for dynamical optimization problems on binary search space. Briefly, our DOP model characterizes a common feature of most DOPs investigated by the evolutionary computation community [33, 39, 45, 49, 52], that is, the global optimum of a DOP is probabilistically changing over time. Like those DOPs intensively studied by the community, the DOP model allows uncertain events to occur at discrete time points only and accomplish without delay. In this way, the shortest duration of a stationary objective function can be guaranteed, which greatly facilitates optimization algorithms, and, in the meantime, reflects reasonable simplifications that are widely employed when solving sophisticated DOPs in practice. Taking the dynamic vehicle routing problem as an example, it is difficult to always take any real-time factor into account when searching for the optimal routing. Instead, such factors are often temporarily collected, and contribute to the modification of objective function only at some discrete time points.\nConcretely, we define the DOP phase to be the time interval in which the objective function of a DOP remains stationary. For the sake of simplicity, we assume the duration of each DOP phase is 1 such that the time point index t (t ∈ N, t = 0 is the starting time point) can be utilized to distinguish one DOP phase from another. In the tth DOP phase, the change with respect to the objective function occurs and finishes at time point t. Within the DOP phase, the objective function, denoted by ft : {0, 1}\nn → R, remains stationary and only has a unique global optimum in the search space. The overall goal of solving the DOP is to maximize the objective function in the presence of movements of the global optimum. Besides, we do not consider any constraint-based stationary objective function in our investigations, and models involving such settings will be left as our future work. By summarizing the above descriptions, we present the following definition:\nDefinition 2 (DOP Model). A DOP is a maximization problem whose stationary objective function may change at any time point t ∈ N. At the tth DOP phase, the fitness (value of objective function) with respect to a given solution x ∈ {0, 1}n is given by ft(x), where ft : {0, 1} n → R is the stationary objective function at the tth DOP phase.\nAs stated, a notable characteristic of most DOPs is that their global optima may change over time. In this paper, we characterize the movements of global optima as a kind of pure random walks in the binary solution space, which is a simple and natural way of modeling stochastic movements in theory:\nDefinition 3 (Bitwise Shifting Global Optimum (BSGO)). The global optimum of a DOP is called a BSGO, if it is shifting following the rule ∀t ∈ N : x∗t+1 = Bn(x ∗ t ), where x ∗ t is the global optimum at the t\nth DOP phase, Bn : {0, 1}\nn → {0, 1}n flips every bit of the input binary string with a probability of σ ∈ (0, 1/2], and σ is called the shifting rate.\nThe DOPs whose unique global optima are BSGOs form the BDOP class:\nDefinition 4 (BDOP Class). For a DOP following the model defined in Definition 2, if it only has a unique global optimum and the optimum is a BSGO, then the DOP is a BDOP, and we also say that the DOP belongs to the BDOP class.\nIn the rest of the paper, theoretical investigations will be carried out on the BDOP class, where the algorithms for solving such optimization problems will be introduced in the next subsection."
    }, {
      "heading" : "2.3 Time-variable Mutation Rate Schemes and Evolutionary Algorithms",
      "text" : "In this paper, both individual-based and population-based EAs will be employed in our theoretical analysis, and the aim is to demonstrate the impact of time-variable mutation rate schemes on different EAs when solving DOPs. Concretely, the individual-based EA studied in this paper is called the (1 + 1) EA. At each generation, the EA maintains a unique parent individual, and the parent individual can only generate a unique offspring individual via mutation; The selection operator preserves the one with better fitness between the parent and offspring individuals (i.e., 1 parent + 1 offspring). A concrete description of the (1 + 1) EA studied in this paper is given below:\nAlgorithm 1 ((1 + 1) EA). Choose the initial individual x (P ) 0 randomly by the uniform distribution over the whole search space. Set the initial generation index t = 0. The tth generation of the EA consists of the following steps:\n• Mutation: Each bit of the parent individual x (P ) t is flipped with the probability of Pm(n, t) ∈ [0, 1], where n\nis the problem size (i.e., length of the binary string). After that, an offspring individual x (O) t is obtained.\n• Fitness Evaluation: Evaluate the fitness of x (P ) t and x (O) t based on the stationary fitness function ft at the\ntth generation (tth DOP phase).\n• Selection: If ft(x (O) t ) ≥ ft(x (P ) t ), then set x (P ) t+1 = x (O) t ; Otherwise, set x (P ) t+1 = x (P ) t .\nIf the given stopping criterion is met after the tth generation, then the EA stops; Otherwise, set t = t+ 1 and a new generation begins.\nA significant difference between the above algorithm and the (1 + 1) EA for static problems [19] is that, at every generation the former evaluates not only the fitness of the offspring but also that of the parent, while the latter only evaluates the fitness of the offspring. The reason of employing two fitness evaluations in one generation of our (1 + 1) EA is that the fitness of the parent individual may change in response to the change of objective function. Another difference between the above EA and the traditional (1 + 1) EA is that the former allows the mutation rate to vary over generation (i.e., the (1 + 1) EA adopts some time-variable mutation rate scheme), while the latter only adopts the fixed mutation rate Pm = 1/n, where n is the problem size. To make our description formal, the concrete definition of time-variable mutation rate scheme for our (1 + 1) EA is given below:\nDefinition 5 (Time-variable mutation rate scheme for (1 + 1) EA). The time-variable mutation rate scheme of the (1+1) EA is a mapping Pm : N×N → [0, 1]. Such a scheme sets the mutation rate at the tth generation be Pm(n, t), where n is the problem size.\nIn addition to studying the time-variable mutation scheme used in the above (1 + 1) EA, we also study the time-variable mutation schemes in the context of the following (1 + λ) EA (λ is polynomial in n):\nAlgorithm 2 ((1 + λ) EA). Choose the initial individual x (P ) 0 randomly by the uniform distribution over the whole search space. Set the initial generation index t = 0. The tth generation of the EA consists of the above steps:\n• Mutation: The parent individual x (P ) t generates λ (λ ≺ Poly(n)) offspring individuals x (1) t , . . . , x (λ) t in-\ndependently. When generating the χth offspring individual x (χ) t (χ ∈ {1, . . . , λ}), each bit of the parent individual x (P ) t is flipped with the probability of Pm(n, t, χ) ∈ [0, 1].\n• Fitness Evaluation: Evaluate the fitness of x (P ) t , x (1) t , . . . , and x (λ) t based on the stationary fitness function\nft at the t th generation (tth DOP phase).\n• Selection: If max {\nft(x (1) t ), . . . , ft(x (λ) t )\n}\n≥ ft(x (P ) t ), then set x (P ) t+1 = argmaxχ∈{1,...,λ} ft(x (χ) t ); Otherwise,\nset x (P ) t+1 = x (P ) t .\nIf the given stopping criterion is met after the tth generation, then the EA stops; Otherwise, set t = t+ 1 and a new generation begins.\nThe (1+λ) EA is a population-based EA adopting the offspring-population strategy. To be specific, unlike those population-based EAs which maintain multiple parents and offsprings at each generation, the (1 + λ) EA maintains a unique parent individual and a population of offspring individuals generated from the same parent. To guarantee that there is a unique parent at each generation, the selection operator imposes extremely high selection pressure, and preserves the one with the best fitness among the total 1 + λ individuals (i.e., 1 parent + λ offsprings). Also, the (1 + λ) EA can be considered as a special case of the (λ+ λ) EA, where the selection operator always copies the selected individual for λ times to construct the population of the next generation. In this paper, it is worth noting that the (1 + λ) EA introduced above allows different offsprings at the same generation to be generated via distinct mutation rates, which offers larger freedom for the adaptation of mutation rates than using the same mutation rate in generating all offspring individuals. Formally, the time-variable mutation rate scheme utilized by our (1 + λ) EA is defined as follows:\nDefinition 6 (Time-variable mutation rate scheme for (1 + λ) EA). The time-variable mutation rate scheme of the (1 + λ) EA is a mapping Pm : N × N × {1, . . . , λ} → [0, 1]. Such a scheme sets the mutation rate for obtaining the χth offspring individual at the tth generation to be Pm(n, t, χ), where n is the problem size.\nApparently, when λ = 1, the (1 + λ) EA is equivalent to the (1 + 1) EA, and Definitions 5 and 6 are equivalent to each other.\nIn our theoretical investigations, we assume that there are always enough parallel computational resources available to support a polynomial number of simultaneous fitness evaluations, which is critical to make the (1 + λ) EA valid for solving DOPs. Moreover, we also assume that the tth (t ∈ N) generation of every EA starts at the beginning of the tth DOP phase, and finish at the end of the tth DOP phase, which is important for theoretical analysis since it can avoid the degenerate cases where the objective function changes when an EA is carrying out fitness evaluations."
    }, {
      "heading" : "2.4 Measure of Time Complexity",
      "text" : "So far we have introduced the problem and algorithm investigated in this paper. In this subsection, we present the measure of performances of EAs, which is indispensable to our theoretical studies. Traditionally, the performance of an EA on a static optimization problem can be measured by the first hitting time [24, 25, 26, 54, 53]. This concept measures the number of generations needed by an EA to find the optimum of a static optimization problem, which can be generalized to facilitate theoretical analysis evolutionary dynamic optimization. For (1 + 1) and (1 + λ) EAs on DOPs, we formally define the first hitting time as follows:\nDefinition 7 (First hitting time). On a DOP {ft : t ∈ N}, the first hitting time of a (1 + λ) EA (λ ∈ N+ is polynomial in n), denoted by τ , is defined as follows:\nτ := min { t ≥ 0; (\nx (P ) t = x ∗ t\n) ∨ (\nx (1) t = x ∗ t\n) ∨ · · · ∨ (\nx (λ) t = x ∗ t\n)}\n, (1)\nwhere x (P ) t is the parent individual at the t th generation, and x (1) t , . . . , x (λ) t are the λ offspring individuals generated by x (P ) t . Setting λ = 1 in the above definition and replacing the notation x (1) t with x (O) t yield the first hitting time of the (1 + 1) EA.\nBased on the problem and algorithms introduced in this section, in the rest of the paper we study time-variable mutation rate schemes in terms of first hitting times of EAs."
    }, {
      "heading" : "3 (1 + 1) EA with Time-Variable Mutation Schemes",
      "text" : "During the past decade, a number of studies have been dedicated to prove or validate that some specific time-variable mutation rate schemes are helpful to improve performances of EAs, although it is unclear whether this is generally\ntrue. In this section, we present several theoretical results concerning the performance of the (1+1) EA with different time-variable mutation rate schemes on BDOPs. In the first subsection, we offer a general result, demonstrating that the BDOPs with shifting rate σ = ω(logn/n2) cannot be solved efficiently by the (1 + 1) EA with any time-variable mutation rate scheme satisfying ∀t ∈ N : Pm(n, t) ∈ [0, 1−1/ logn]. In the second subsection, we generalize the above result to a specific BDOP called the BitMatchingD problem, and show that the (1+1) EA with any time-variable mutation rate scheme (i.e., ∀t ∈ N : Pm(n, t) ∈ [0, 1]) fails to optimize the BitMatchingD problem efficiently when the shifting rate is σ = ω(logn/n2)."
    }, {
      "heading" : "3.1 A General Result",
      "text" : "The BDOPs studied in this subsection are with shifting rate σ = ω(logn/n2), which implies that the shifting rate of the BDOPs satisfies that limn→∞(log n/n\n2)/σ = 0. The average movement of global opttimum at each DOP phase (which is equivalent to a generation of EA, as mentioned), measured by the Hamming distance, is larger than Θ(logn/n). This includes the case that at each DOP phase the global optimum changes by less than a bit on average. Intuitively, such a small movement speed of global optimum seems not to seriously affect the optimization process, and the (1 + 1) EA may probably cope with such situations by switching to appropriate mutation rates. In this subsection it is discovered by Theorem 1 that even such small movements will have significant influence on the first hitting time of the (1 + 1) EA with different time-variable mutation schemes. The main result in this subsection is formally presented as follows:\nTheorem 1. Given any BDOP with shifting rate σ = ω(logn/n2) and any time-variable mutation rate scheme {Pm(n, t) ∈ [0, 1− 1/ logn] : t ∈ N}, the first hitting time of the (1+1) EA is super-polynomial with an overwhelming probability.\nThe above theorem holds when ∀t ∈ N : Pm(n, t) ∈ [0, 1 − 1/ logn], which means that the largest mutation rate that the EA can switch to is 1 − 1/ logn. Within the interval [0, 1 − 1/ logn], the (1 + 1) EA can adjust its time-variable mutation rate freely following an oracle, i.e., at each generation the EA can even choose the mutation rate best suiting the current situation. The theorem can be proven given the following lemma first:\nLemma 1. Given any BDOP with shifting rate σ = ω(logn/n) and any time-variable mutation rate scheme {Pm(n, t) ∈ [0, 1− 1/ logn] : t ∈ N}, the first hitting time of the (1+1) EA is super-polynomial with an overwhelming probability.\nProof Idea of Lemma 1. Generally speaking, when proving Lemma 1, we should keep in mind that the (1+1) EA can adjust its time-variable mutation rate following an oracle. To be specific, we must carry out a best-case analysis so as to bound all potential behaviors of catching the global optimum of a BDOP. Given a solution x, define the number of matching bits of the solution (to the global optimum) to be the problem size n minus the Hamming distance between the solution and the current global optimum x∗t (i.e., n−H(x, x ∗ t )). A formal definition of Hamming distance H(·, ·) mentioned above is given as below:\nDefinition 8 (Hamming distance). The Hamming distance between two solutions x = (s1, . . . , sn) and y = (s ′ 1, . . . , s ′ n) (x, y ∈ {0, 1}n) is given by H(x, y) := ∑n\ni=1 |si − s ′ i|.\nLet x (P ) t and x (O) t be the parent and offspring individuals of t th generation (t ∈ N) of the (1+1) EA, respectively.\nLet xt be the one with higher fitness between x (P ) t and x (O) t (at the t th generation):\n∀t ∈ N : xt :=\n{\nx (P ) t , if ft(x (P ) t ) > ft(x (O) t ); x (O) t , if ft(x (P ) t ) ≤ ft(x (O) t ).\nLet N (P ) t := n−H\n(\nx (P ) t , x ∗ t\n)\n, N (O) t := n−H\n(\nx (O) t , x ∗ t\n)\n, and\nNt := n−H(xt, x ∗ t ). (2)\nIt follows from the above definitions that\nmax {\nN (P ) t , N (O) t\n} ≥ Nt ≥ min { N (P ) t , N (O) t } . (3)\nThe definition of Nt (t ∈ N) also implies the overall mapping that maps Nt to Nt+1, is determined by not only the EA, but also the optimum-shifting in BDOP. The reason is that, the (1 + 1) EA maps one solution to another only,\nwhile the optimum-shifting in BDOP can be considered as a mapping that describes the movements of the global optimum. The above two factors, together with Nt, determine the Hamming distance between a solution and the current global optimum at the (t+ 1)th generation.\nTo explain the proof idea, we define a number axis (real number line) with respect to the number of matching bits (to the current global optimum), ranging from 0 to n. As illustrated in Fig. 1, we further define several intervals on the axis:\nDefinition 9. The First Forbidden Interval and First LongJump Interval are defined as follows:\n1. First Forbidden Interval: The First Forbidden Interval is the interval F1 := [n − n/ log 3 n, n], where n is\nthe problem size.\n2. First LongJump Interval: The First LongJump Interval is the interval L1 := [0, n/ log 2 n].\nIn addition, we let O1 := (n/ log 2 n, n − n/ log3 n) such that L1 ∪ O1 ∪ F1 = [0, n]. The above decomposition is illustrated in Fig. 1. The purpose of defining the three intervals is to quantitatively characterize three general states of the (1+1) EA respectively. Intuitively speaking, for the First Forbidden Interval, Nt ∈ F1 demonstrates the situation that the better one between the parent and offspring at the tth generation is very close to the moving global optimum of a BDOP. Similarly, belonging to the First LongJump interval (Nt ∈ L1) indicates the situation that the solution found by the EA is extremely far away from the global optimum, but it can reach the First Forbidden Interval F1 by an extremely long jump resulted from a very large mutation rate (i.e., 1− 1/ logn). Finally, belonging to the interval O1 represents the situation that the EA is still far from finding the optimum, no matter what concrete mutation rate the EA adopts.\nIn our best-case analysis for proving Lemma 1, as long as the EA has found solutions belonging to the intervals F1 and L1, we optimistically consider that the EA has reached the global optimum. By noting an important fact in Definition 1 that the EA starts in O1 (i.e., the number of matching bits of the initial solution belongs to O1) with an overwhelming probability, to prove Lemma 1, we only need to prove that the transition from O1 to F1 and L1 is very unlikely to happen.\nFormally, we need to prove the following propositions when proving Lemma 1:\nProposition A1.1: P(N0 ∈ F1 ∪ L1) ≺ 1/SuperPoly(n).\nProposition A1.2: ∀t ∈ N+ : P(N (P )t ∈ F1 ∪ L1 | Nt−1 ∈ O1) ≺ 1/SuperPoly(n).\nProposition A1.3: ∀t ∈ N+ : P(N (P )t ∈ O1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1) ≺ 1/SuperPoly(n).\nFor the detailed proof following the above sketch, interested readers can refer to the appendix of the paper. From Lemma 1, when proving Theorem 1 we only need to cope with smaller shifting rates satisfying the conditions σ = O(log n/n) and σ = ω(logn/n2). Given the condition σ = O(log n/n), we let σ ≤ δ logn/n in the proof of Lemma 1, where δ is an arbitrary positive constant. Having identified the above conditions, we define γ = γ(n, σ) as follows:\nγ = γ(n, σ) := min\n{\nn\nlogn , σ ·\nn2\nlogn\n}\n.\nFurther, let G = G(n, σ) be defined by\nG = G(n, σ) := γ4/7 logn. (4)\nThe purpose of introducing the notations γ and G is to further define subintervals for the interval [0, n] with respect to the number of matching bits of a solution to the current optimum, in addition to F1, L1 and O1. Concretely, we consider the following new intervals:\nDefinition 10. Some intervals are introduced below:\n1. Second Forbidden Interval: The Second Forbidden Interval is the interval F2 := [n−G,n], where n is the problem size.\n2. Primary Adjacent Intermediate Interval: The Primary Adjacent Intermediate Interval is the interval A1 := [n− 2G,n−G).\n3. Secondary Adjacent Intermediate Interval: The Secondary Adjacent Intermediate Interval is the interval A2 := [n− 3G,n− 2G).\n4. Second LongJump Interval: The Second LongJump Interval is the interval L2 := [0, 4G], where n is the problem size.\n5. Primary Remote Intermediate Interval: The Primary Remote Intermediate Interval is the interval B1 := (4G, 5G].\n6. Secondary Remote Intermediate Interval: The Secondary Remote Intermediate Interval is the interval B2 := (5G, 6G].\nThe above intervals, along with F1 and L1, are illustrated in Fig. 2. Generally speaking, the above interval decomposition inherits and generalizes the intuitive idea utilized in the proof of Lemma 1. Similar to F1 and L1 in the proof of Lemma 1, the Second Forbidden Interval F2 is used to characterize the state that a solution found by the EA is very close to the current global optimum; The Second LongJump Interval L2 is used to characterize the state that a solution found by the EA is extremely far away from the current global optimum, which is very likely to further reach the optimum by employing an extremely large mutation rate (e.g., 1 − 1/ logn). However, due to different values of shifting rates (ω(logn/n) holds in Lemma 1 while ω(logn/n2) and O(log n/n) hold in Theorem 1), the concrete sizes of F2 and L2 are different from those of F1 and L1 respectively. In addition to the configurations of “forbidden” and “long-jump” intervals, here we employ some extra intervals which serve as intermediate intervals for reaching the “forbidden” and “long-jump” intervals, F2 and L2. Concretely, at the very beginning of optimization, the solution found by the EA belongs to the interval O1 defined in the last subsection. To find a solution in F2 by employing a small mutation rate, the EA must find some solution in the Secondary Adjacent Intermediate Interval A2 first. Afterwards, the EA needs to travel through the Primary Adjacent Intermediate Interval A1, i.e., try to find solutions that exceeds A1 and finally reach F2. On the other hand, to find a solution in F2 by employing an extremely large mutation rate, the EA has to find some solution in the “long-jump” interval L2 first. Nevertheless, to reach L2, the EA has to find some solution in the Secondary Remote Intermediate Interval B2 first, and afterwards travel through the Primary Remote Intermediate Interval B1 so as to reach L2. In our best-case analysis, once the EA has found solutions belonging to the intervals F2 and L2, we optimistically consider that the EA has reached the global optimum.\nSo far we have briefly introduced the interval decomposition utilized in the proof of Theorem 1. Next, we provide the detailed sketch for proving Theorem 1.\nProposition B1.1: P(N0 ∈ A2 ∪ A1 ∪ F2 ∪ B2 ∪ B1 ∪ L2) ≺ 1/SuperPoly(n).\nProposition B1.2: ∀t that satisfies t ∈ N+ and Pm(n, t) < γ1/14 logn/n,\nP\n(\n|Nt −Nt−1| < δγ 1/7 logn | Pm(n, t) <\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≻ 1− 1\nSuperPoly(n)\nholds.\nProposition B1.3:\n(B1.3a) To reach F2 within a polynomial number of generations, with an overwhelming probability the EA must reach A2 or L2 first.\n(B1.3b) To reach L2 within a polynomial number of generations, with an overwhelming probability the EA must reach B2 first.\nProposition B1.4:\n(B1.4a) ∀t that satisfies t ∈ N+ and Nt−1 ∈ A2 ∪A1 ∪ F2,\nP\n(\nNt −Nt−1 < γ 1/7 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <\nδ logn\nn\n)\n≻ 1− 1\nSuperPoly(n)\nholds. (B1.4b) ∀t that satisfies t ∈ N+ and Nt−1 ∈ B2 ∪ B1 ∪ L2,\nP\n(\nNt−1 −Nt < γ 1/7 | Nt−1 ∈ B2 ∪ B1 ∪ L2, σ <\nδ logn\nn\n)\n≻ 1− 1\nSuperPoly(n)\nholds.\nProposition B1.5: To travel through A1, the EA will spend a super-polynomial number of generations with an overwhelming probability.\nOne may note that Propositions B1.1, B1.3 and B1.5 are in response to our discussions in the previous paragraph. In addition, Propositions B1.2 and B1.4 are both formal propositions concerning the gain of the better solution (between the parent and offspring at the tth generation) found by the EA, compared with that of the previous generation. However, the two propositions tackle different conditions concerning the shifting rate of BDOP (i.e., σ), mutation rate (Pm(n, t)) and so on, which are crucial for the formal proof of Propositions B1.3 and B1.5. A detailed proof of Theorem 1 is in the appendix of the paper.\nTheorem 1 presents a general result showing that any BDOP with shifting rate ω(logn/n2) is essentially hard to the (1 + 1) EA with any time-variable mutation rate scheme {Pm(n, t) : t ∈ N} satisfying ∀t ∈ N : Pm(n, t) ∈ [0, 1 − 1/ logn]. These essentially hard BDOPs, which characterize the movements of global optimum as random walks in the binary solution space, demonstrate the potential weakness of time-variable mutation rate scheme on DOPs with moving optimum. However, it is worth noting that the results obtained in this section focus on those time-variable mutation rate schemes that satisfy ∀t ∈ N : Pm(n, t) ∈ [0, 1 − 1/ logn], i.e., the largest value that the mutation rate can take is 1 − 1/ logn. It is unclear whether an even larger mutation rate that exceeds 1 − 1/ logn would help, though knowing when to apply such an extreme mutation rate relies on some ideal oracle (i.e., we know the consequence of such decisions in advance). To further validate the potential failure of all time-variable mutation rate schemes on some specific BDOP, we will extend Theorem 1 for the BDOP class to a concrete example, named the BitMatchingD problem.\n3.2 A More Precise Result on BitMatchingD\nIn this subsection, we draw our attentions to a specific example of BDOPs class called BitMatchingD. Concretely, the BitMatchingD problem is a BDOP using the following stationary function at the t th DOP phase:\nBitMatchingD(x, t) := n−H(x, x ∗ t ), (5)\nwhere x∗t is the global optimum of BitMatchingD at the t th DOP phase.\nTo have a more comprehensive understanding of the performances of time-variable mutation rate schemes on BitMatchingD, here we consider the time complexity of finding approximate solutions with some certain quality, instead of considering only the time complexity of finding the moving global optimum. Here, the following specific characteristic for approximate solutions of DOPs, named the best-case DOP approximation ratio, is taken into account:\nDefinition 11 (Best-case DOP approximation ratio). Suppose we have a maximization DOP I and an optimization algorithm A. Let ft(x ∗ t ) be the optimal value of the objective function of the DOP I at the t\nth generation (t ∈ N), xt be the best solution found by the algorithm A at the tth generation, ft(xt) be the value of the objective function with respect to the solution xt at the t th generation. For the algorithm A, the ratio supt̂≤t ft̂(xt̂) ft̂(x ∗ t̂ ) is called the best-case DOP approximation ratio at the tth generation.\nGiven the above definition, The definition of first hitting time in Eq. 1 can be generalized to the so-called (1− ǫ)first hitting time concerning the time for finding the solutions that reach a certain best-case DOP approximation ratio, say, 1− ǫ:\nDefinition 12 ((1− ǫ)-first hitting time for (1 + 1) EA). On a DOP {ft : t = 1, 2 . . .}, the (1− ǫ)-first hitting time of an EA, denoted by τǫ, is defined as follows:\nτǫ := min { t ≥ 0; ( x (P ) t ∈ Jt ) ∨ ( x (O) t ∈ Jt )} , (6)\nwhere Jt := {\nxt : ft(xt) ft(x∗t )\n≥ 1− ǫ } and ǫ ∈ [0, 1).\nThe BitMatchingD problem is a special case of the BDOP class. By generalizing the proof ideas mentioned in the last subsection, we are able to obtain a number of new theoretical results. The following lemma can be derived from Lemma 1:\nLemma 2. Given the BitMatchingD problem with shifting rate σ = ω(logn/n) and any time-variable mutation rate scheme {Pm(n, t) ∈ [0, 1] : t ∈ N} of the (1 + 1) EA, the (1 − 1/ log\n3 n)-first hitting time of the (1 + 1) EA is super-polynomial with an overwhelming probability.\nBased on Lemma 2 and the proof idea of Theorem 1, we can obtain the following theorem:\nTheorem 2. Given the BitMatchingD problem with shifting rate σ = ω(logn/n 2), and any time-variable mutation rate scheme {Pm(n, t) ∈ [0, 1] : t ∈ N} of the (1 + 1) EA, the (1 − γ4/7 logn/n)-first hitting time of (1 + 1) EA is super-polynomial with an overwhelming probability, where γ is defined by\nγ = γ(n, σ) := min\n{\nn\nlogn , σ ·\nn2\nlogn\n}\n. (7)\nThe proofs of Lemma 2 and Theorem 2 are similar to those of Lemma 1 and Theorem 1 respectively. Interested readers can refer to the appendix for details. Lemma 2 and Theorem 2 will lead to the following corollary about the most commonly used fixed mutation rate 1/n directly, which was proven by Dorste [17]:\nCorollary 1. The first hitting time of the (1 + 1) EA with the fixed mutation rate 1/n on BitMatchingD problem with σ = ω(logn/n2) is super-polynomial with an overwhelming probability.\nCombining the above corollary with Theorem 2, we obtain an interesting result:\nCorollary 2. Given the BitMatchingD problem with σ = ω(logn/n 2), both the (1+ 1) EA with any time-variable mutation scheme and the (1 + 1) EA with the most commonly used fixed mutation rate 1/n performs inefficiently.\nClearly, the corollary indicates that no time-variable mutation rate schemes significantly outperforms the most commonly used fixed mutation rate 1/n when the (1 + 1) EA is employed as the optimizer of the BitMatchingD problem with shifting rate σ = ω(logn/n2). Moreover, Droste [17] proved that the (1+1) EA with the fixed mutation rate 1/n can reach the global optimum of BitMatchingD with shifting rate σ = O(log n/n\n2) with a polynomial average first hitting time:\nTheorem 3 (Droste [17]). The mean first hitting time of the (1 + 1) EA with the fixed mutation rate Pm = 1/n on BitMatchingD with σ = O(log n/n 2) is polynomial in the problem size n.\nGiven that fixed mutation rates are only special cases of time-variable mutation schemes, the above result can also be interpreted as that there exists some time-variable mutation scheme with which the (1 + 1) EA can solve BitMatchingD with σ = O(log n/n\n2) with a polynomial mean first hitting time. It follows from Theorem 2 that the BitMatchingD problem with σ = Θ(logn/n\n2) is the hardest BitMatchingD on which a (1 + 1) EA can guarantee efficient performance. In the next section, we show that by adopting a population, a (1+λ) EA with some time-variable mutation schemes can break the above limitation. However, when the shifting rate σ = Ω(logn/n2), a (1 + λ) EA with different time-variable mutation schemes will still encounter bottleneck when optimizing BDOPs."
    }, {
      "heading" : "4 (1 + λ) EA with Time-Variable Mutation Schemes",
      "text" : "So far we have analyzed the effectiveness of time-variable mutation schemes in the context of the (1 + 1) EA. In this section, our analysis will be carried out in the context of a population-based EA called (1 + λ) EA. A case study on the BitMatchingD problem will be given to show the overall impact of population and time-variable mutation schemes."
    }, {
      "heading" : "4.1 A General Result",
      "text" : "The (1 + λ) EA studied in this paper follows the framework presented in Algorithm 2. The time variable mutation schemes for the (1 + λ) EA, defined in Definition 6, allows the EA to utilize distinct mutation rates in generating different offsprings in the same generation. However, when solving BDOPs, such an EA may still be inefficient when the shifting rate of a BDOP exceeds Θ(logn/n):\nTheorem 4. Given any BDOP with shifting rate σ = ω(logn/n) and any time-variable mutation rate scheme {Pm(n, t, χ) ∈ [0, 1− 1/ logn] : t ∈ N, χ ∈ {1, . . . , λ}}, the first hitting time of the (1 + λ) EA is super-polynomial with an overwhelming probability, where the offspring size λ is a polynomial function of n.\nThe proof of Theorem 4 is a direct generalization of the proof idea of Lemma 1. Interested readers can refer to the appendix for details.\nApparently, over the BDOP class, Theorem 4 shows a theoretical limitation for time-variable mutation schemes associated with the (1 + λ) EA. Nevertheless, the theorem sheds some light on potential efficient performances of the (1 + λ) EA with time-variable mutation schemes on those BDOPs whose shifting rate is between Θ(logn/n) and Θ(logn/n2) (note that Theorem 1 tells us that the (1 + 1) EA performs inefficiently on these BDOPs). In fact, compared with the (1 + 1) EA, the (1 + λ) EA has indeed been strengthened by two factors. First, the offspringpopulation strategy, which is a specific way of utilizing population, offers larger selection pressure for the (1+λ) EA. When optimizing DOPs, this feature is very helpful for tracking the movement of global optimum. Second, in one generation the (1 + λ) EA is capable of exploring different subsets of the search space via distinct step sizes. Owing to these two factors, it can be expected that the (1 + λ) EA can significantly outperform the (1 + 1) EA on some BDOPs. In the next subsection, we present such a theoretical example.\n4.2 Case Studies on BitMatchingD\nAs in Section 3.2, we still employ the BitMatchingD problem as an example of the BDOP class. We show that the (1 + λ) EA with time-variable mutation schemes can improve the performance of the (1 + 1) EA. Meanwhile, by the same theoretical result we are able to demonstrate that the general limitation of (1 + λ) EA over the BDOP class, predicted by Theorem 4, is almost tight. The main result of this subsection is as follows:\nTheorem 5. Given the BitMatchingD problem with shifting rate σ ≤ 1/(5n), if\nCond. 1 the time-variable mutation rate scheme {Pm(n, t, χ) ∈ [0, 1] : t ∈ N, χ ∈ {1, . . . , λ}} satisfies\n∀t ∈ N : sup t∈N,χ∈{1,...,λ} Pm(n, t, χ) = O\n(\nlogn\nn\n)\n, inf t∈N,χ∈{1,...,λ}\nPm(n, t, χ) ≻ 1\nPoly(n) ;\nCond. 2 the polynomial offspring size λ of the EA satisfies\nλ = ω\n\n\n(\n1− sup t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)−n (\ninf t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)−1 \n . (8)\nthen the mean first hitting time of the (1+λ) EA with the above time-variable mutation rate scheme is bounded from above by 8n/5.\nTheorem 5 demonstrates that, when a time-variable mutation rate scheme satisfies certain conditions, then the (1 + λ) EA adopts such a mutation scheme will perform efficiently on the BitMatchingD problem whose shifting rate is no larger than 1/(5n). The proof of the theorem utilizes the drift analysis technique proposed by He and Yao [24]:\nLemma 3 (Drift Analysis [24]). Let ξt (t ∈ N) be the population at the tth generation of the EA, D(ξt, t) be the distance metric measuring the distance between the population ξt and the global optimum at the t\nth generation, and {D(ξt, t) : t = 0, 1, . . . } be a super-martingale describes an EA, if for any time t = 1, 2, . . . , if D(ξt, t) > 0 and\nE [D (ξt, t)−D (ξt+1, t+ 1) | ξt] ≥ cl > 0,\nthen the mean first hitting time satisfies\nE[τ | ξ0] ≤ D(ξ0, 0)\ncl ≤\nsupX∈P,t∈ND(X, t)\ncl ,\nwhere ξ0 is the initial population of the EA, P is the set of all populations.\nBefore providing the formal proof for Theorem 5, we introduce some notations. Let x (P ) t be the parent individual\nof tth generation (t ∈ N) of the (1 + λ) EA, and x(1)t , . . . , x (λ) t be the λ offspring individuals generated by x (P ) t . Let x (O) t be the one with highest fitness among x (1) t , . . . , x (λ) t :\n∀t ∈ N : x(O)t := arg max χ∈{1,...,λ} ft(x (χ) t ).\nLet xt be the one with higher fitness between x (P ) t and x (O) t :\n∀t ∈ N : xt :=\n{\nx (P ) t , if ft(x (P ) t ) > ft(x (O) t ); x (O) t , if ft(x (P ) t ) ≤ ft(x (O) t ).\nLet N (P ) t := n − H\n(\nx (P ) t , x ∗ t\n)\n, N (O) t := n − H\n(\nx (O) t , x ∗ t\n)\n, and Nt := n − H(xt, x ∗ t ). Specifically, for the Bit-\nMatchingD problem, it is clear that Nt is the larger one between N (P ) t and N (O) t , i.e., Nt = max\n{\nN (P ) t , N (O) t\n}\n.\nFurthermore, define πi,j (i, j ∈ {0, 1, . . . , n}) and the corresponding generalized notation as follows:\nπi,j := P ( N (P ) t = j | Nt−1 = i ) ;\nπi,⊕j := P ( N (P ) t ⊕ j | Nt−1 = i ) ,\nwhere the “⊕” at both sides can be replaced simultaneously by “>”,“<”,“≥” or“≤”. πi,j is independent of the generation index t since at each DOP phase the global optimum moves with the same shifting rate, as defined in Definition 3. Define p̃i,j(t) (i, j ∈ {0, 1, . . . , n}) and the corresponding generalized notation as follows:\np̃i,j(t) := P ( N (O) t = j | N (P ) t = i ) ;\np̃i,⊕j(t) := P ( N (O) t ⊕ j | N (P ) t = i ) ,\nwhere the “⊕” at both sides can be replaced simultaneously by “>”,“<”,“≥” or“≤”. Based upon the above lemma and notations, we provide the proof of Theorem 5.\nProof of Theorem 5. To apply Lemma 3, we need to define a suitable distance function, and estimate the corresponding one step mean drift at the tth (t ∈ N+) generation of the (1 + λ) EA. Given the union of the parent and its λ offsprings as a whole population X , the distance function D(X, t) (t ∈ N), which measures the distance between X and the moving global optimum at the tth generation, is formally defined by\nD(X, t) := min {H(x, x∗t );x ∈ X} ,\nwhere x∗t is the global optimum at the t th generation (t ∈ N+). Then denote by ∆Di(t) the one step mean drift at the tth generation, conditional on that the largest number of matching bits found at the (t − 1)th generation is Nt−1 = i:\n∆Di(t) := E [D(Xt−1, t− 1)−D(Xt, t)|Nt−1 = i] = ∞ ∑\nj=1\nj · P (D(Xt−1, t− 1)−D(Xt, t) = j|Nt−1 = i) ,\nwhich is the sum of the following four components:\n∆i\n[\nN (P ) t ≥ i, N (O) t ≤ N (P ) t\n]\n:= E [\nD(Xt−1, t− 1)−D(Xt, t);N (P ) t ≥ i, N (O) t ≤ N (P ) t\n∣ ∣Nt−1 = i ] ;\n∆i\n[\nN (P ) t ≥ i, N (O) t > N (P ) t\n]\n:= E [\nD(Xt−1, t− 1)−D(Xt, t);N (P ) t ≥ i, N (O) t > N (P ) t\n∣ ∣Nt−1 = i ] ;\n∆i\n[\nN (P ) t < i,N (O) t ≤ i\n]\n:= E [\nD(Xt−1, t− 1)−D(Xt, t);N (P ) t < i,N (O) t ≤ i\n∣ ∣Nt−1 = i ] ;\n∆i\n[\nN (P ) t < i,N (O) t > i\n]\n:= E [\nD(Xt−1, t− 1)−D(Xt, t);N (P ) t < i,N (O) t > i\n∣ ∣Nt−1 = i ] .\nFormally, based the above notations the one step mean drift can be rewritten as\n∆Di(t) = ∆i\n[\nN (P ) t ≥ i, N (O) t ≤ N (P ) t\n]\n+∆i\n[\nN (P ) t ≥ i, N (O) t > N (P ) t\n]\n+∆i\n[\nN (P ) t < i,N (O) t ≤ i\n]\n+∆i\n[\nN (P ) t < i,N (O) t > i\n]\n.\nNext we estimate the four components one after another. By dividing the event “N (P ) t ≥ i, N (O) t ≤ N (P ) t ” into a number of sub-events “N (P ) t = i, N (O) t ≤ N (P ) t ”, “N (P ) t = i+1, N (O) t ≤ N (P ) t ”, . . . , “N (P ) t = n,N (O) t ≤ N (P ) t ” whose probabilities are all positive, we know the following fact about the first component of the one step mean drift:\n∆i [ N (P ) t ≥ i,N (O) t ≤ N (P ) t ] = n ∑\nj=i\n(j − i)πi,j · P ( N (P ) t ≥ N (O) t | N (P ) t = j ) > 0,\nSimilarly, we can divide the event N (P ) t ≥ i, N (O) t > N (P ) t into several sub-events, and then estimate the following\ncomponent of the one step mean drift:\n∆i [ N (P ) t ≥ i,N (O) t > N (P ) t ] =\nn ∑\nj=i\nn−j ∑\nk=1\n(j − i+ k)πi,j · P ( N (P ) t + k = N (O) t | N (P ) t = j )\n>\nn ∑\nj=i\n(j − i+ 1)πi,j · P ( N (O) t > N (P ) t | N (P ) t = j )\n> inf 0≤k<n,t∈N\nP (\nN (O) t > N (P ) t | N (P ) t = k\n)\n·\nn ∑\nj=i\n(j − i+ 1)πi,j\n= inf 0≤k<n,t′∈N\np̃k,>k(t ′) ·\n{(\nn ∑\nj=i\nπi,j\n)\n+\n(\nn ∑\nj=i\n(j − i)\n(\nn− i j − i\n)\nσ j−i(1− σ)n−j+i\n)}\n> inf 0≤k<n,t′∈N\np̃k,>k(t ′) ·\n{\nπi,≥i +\n(\n(1− σ)i n−i ∑\nj=0\nj\n(\nn− i\nj\n)\nσ j(1− σ)n−i−j\n)}\n> inf 0≤k<n,t′∈N\np̃k,>k(t ′) · { πi,≥i + (1− σ) i(n− i)σ }\n> inf 0≤k<n,t′∈N\np̃k,k+1(t ′) · { πi,≥i + (1− σ) n σ } ,\nwhere we utilize the fact that πi,j > ( n−i j−i ) σj−i(1− σ)n−(j−i) holds for all j ≥ i. By similar calculations, we further\nobtain the following component of the one step mean drift:\n∆i [ N (P ) t < i,N (O) t ≤ i ] = i−1 ∑\nj=0\ni−j ∑\nk=1\n(j − i+ k)πi,j · P ( N (P ) t + k = N (O) t | N (P ) t = j )\n+ i−1 ∑\nj=0\n(j − i)πi,j · P ( N (O) t ≤ N (P ) t | N (P ) t = j )\n>\ni−1 ∑\nj=0\nπi,j ·\ni−j ∑\nk=1\nP (\nN (P ) t + k = N (O) t | N (P ) t = j\n)\n−\ni−1 ∑\nj=0\n(i− j)πi,j ·\ni−j ∑\nk=1\nP (\nN (P ) t + k = N (O) t | N (P ) t = j\n)\n− i−1 ∑\nj=0\n(i− j)πi,j · P ( N (O) t ≤ N (P ) t | N (P ) t = j )\n≥\ni−1 ∑\nj=0\nπi,j ·\ni−j ∑\nk=1\nP (\nN (P ) t + k = N (O) t | N (P ) t = j\n)\n−\ni−1 ∑\nj=0\n(i− j)πi,j\n> inf 0≤k<n,t′∈N\np̃k,k+1(t ′) ·\ni−1 ∑\nj=0\nπi,j −\ni−1 ∑\nj=0\n(i− j)\n(\ni\ni− j\n)\nσ i−j\n> inf 0≤k<n,t′∈N\np̃k,k+1(t ′) ·\ni−1 ∑\nj=0\nπi,j −\ni ∑\nk=1\nk(iσ)k\n= inf 0≤k<n,t′∈N\np̃k,k+1(t ′) ·\ni−1 ∑\nj=0\nπi,j − (iσ) · i(iσ)i+1 − (i+ 1)(iσ)i + 1\n(1− iσ)2\n> inf 0≤k<n,t′∈N\np̃k,k+1(t ′) · πi,<i − (nσ) ·\nn(nσ)n+1 − (n+ 1)(nσ)n + 1\n(1− nσ)2\nFinally, the fourth component of the one step mean drift ∆Di(t), as the first component, is positive:\n∆i [ N (P ) t < i,N (O) t > i ] >\nn ∑\nj=i\nπi,j · P ( N (O) t > i | N (P ) t = j ) > 0.\nThe lower bounds of the four components yield the lower bound of ∆Di(t):\n∆Di(t) = ∆i [ N (P ) t ≥ i,N (O) t ≤ N (P ) t ] +∆i [ N (P ) t ≥ i,N (O) t > N (P ) t ] +∆i [ N (P ) t < i,N (O) t ≤ i ] +∆i [ N (P ) t < i,N (O) t > i ]\n> inf 0≤k<n,t′∈N\np̃k,k+1(t ′) · { πi,≥i + (1− σ) n σ }\n+ inf 0≤k<n,t′∈N\np̃k,k+1(t ′) · πi,<i −\ni ∑\nk=1\nk(iσ)k\n> inf 0≤k<n,t′∈N\np̃k,k+1(t ′) · πi,≥i + inf\n0≤k<n,t′∈N p̃k,k+1(t\n′) · πi,<i −\ni ∑\nk=1\nk(iσ)k\n= inf 0≤k<n,t′∈N\np̃k,k+1(t ′)− ·\ni(iσ)i+2 − (i+ 1)(iσ)i+1 + (iσ)\n(1− iσ)2 ,\nwhere ∑i k=1 k(iσ) k is monotonically increasing with positive i and σ. Given the fact i < n and the condition\nσ ≤ 1/(5n), we further have:\n∆Di(t) > inf 0≤k<n,t′∈N\np̃k,k+1(t ′)−\nn 5n+2 − n+1 5n+1 + 1 5\n16 25\n> inf 0≤k<n,t′∈N\np̃k,k+1(t ′)−\n5\n16\n> 1−\n(\n1−\n(\n1− sup t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)n−1\ninf t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)λ\n− 5\n16\n> 11\n16 −\n(\n1−\n(\n1− sup t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)n\ninf t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)λ\n.\nUnder the conditions supt∈N,χ∈{1,...,λ} Pm(n, t, χ) = O(log n/n) and inft∈N,χ∈{1,...,λ} Pm(n, t, χ) ≻ 1/Poly(n),\n(\n1− sup t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)n\ninf t∈N,χ∈{1,...,λ}\nPm(n, t, χ) ≻ 1\nPoly(n) ,\nand the above item is strictly less than 1. Hence, there exists a polynomial offspring size\nλ = ω\n\n\n(\n1− sup t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)−n (\ninf t∈N,χ∈{1,...,λ} Pm(n, t, χ)\n)−1 \n ,\nsuch that the one step mean drift at the tth generation (∀t < τ, t ∈ N+) has a common lower bound\n∀t < τ, t ∈ N+ : ∆i(t) > 5\n8 .\nAccording to Lemma 3, given the offspring size λ specified by Eq. 8, the mean first hitting time of the EA under two conditions of Theorem 5 satisfies\nE [τ |Cond. 1,Cond. 2] < 8n\n5 .\nUnder this circumstance, the number of function evaluations of finding the global optimum for the first time, denoted by T , satisfies\nE [T |Cond. 1,Cond. 2] < 8λn\n5 .\nThe above theorem offers a sufficient condition for the (1 + λ) EA to achieve efficient performance on the BitMatchingD problem with shifting rate σ ≤ 1/(5n). As a direct corollary of Theorem 5, we present the following time complexity result of the (1+n2 logn) EA whose time-variable mutation scheme {Pm(n, t, χ) ∈ [0, 1] : t ∈ N, χ ∈ {1, . . . , λ}} satisfying supt∈N,χ∈{1,...,λ} Pm(n, t, χ) = logn/n, inft∈N,χ∈{1,...,λ} Pm(n, t, χ) = 1/n (note that the fixed mutation rate 1/n is a special case of such time-variable schemes):\nCorollary 3. Given the BitMatchingD problem with shifting rate σ ≤ 1/(5n), if the time-variable mutation rate scheme {Pm(n, t, χ) ∈ [0, 1] : t ∈ N, χ ∈ {1, . . . , λ}} of the (1 + n2 logn) EA satisfies\n∀t ∈ N : sup t∈N,χ∈{1,...,λ} Pm(n, t, χ) = log n/n, inf t∈N,χ∈{1,...,λ}\nPm(n, t, χ) = 1\nn ,\nthen the mean number of function evaluations of finding the global optimum for the first time is bounded from above by 8n3 logn/5.\nTheorem 5 tells us, when the shifting rate of the global optimum is smaller than 1/(5n) (each movement only changes no more than 1/5 bit of the global optimum on average), the (1 + λ) EA is able to compensate the negative influence brought by the movement of the global optimum via generating a number of offspring individuals with different mutation rates (in each generation), and the EA can achieve efficient performance on the BitMatchingD problem. However, by the following theorem we will know that, when the shifting rate σ further grows to ω(logn/n) (each movement changes at least logn bits of the global optimum on average), even the multiple-offspring strategy and time-variable mutation rate schemes cannot help to achieve efficient performance on the BitMatchingD problem.\nTheorem 6. Given the BitMatchingD problem with shifting rate σ = ω(logn/n), and any time-variable mutation rate scheme {Pm(n, t, χ) ∈ [0, 1] : t ∈ N, χ ∈ {1, . . . , λ}}, the first hitting time of the (1 + λ) EA is super-polynomial with an overwhelming probability.\nThe proof of Theorem 6 follows the proof idea of Lemma 1 and Theorem 4. Interested readers can refer to the appendix for details."
    }, {
      "heading" : "5 Discussions",
      "text" : "In this section, we discuss some issues related to the theoretical results presented in previous sections."
    }, {
      "heading" : "5.1 Generalizations of Theoretical Results",
      "text" : "Here we discuss potential ways of generalizing our theoretical results from different perspectives."
    }, {
      "heading" : "5.1.1 What if the shifting rate of a DOP is also time-variable?",
      "text" : "Technically, the theoretical results presented so far can further be generalized to a broader class of DOPs. In particular, we can modify the definition of Bitwise Shifting Global Optimum (BSGO) (Definition 3) by allowing the global optimum to move with different shifting rates at different DOP phases:\nDefinition 13 (Bitwise Shifting Global Optimum with Time-variable Shifting Rate (BSGO-TSR)). The global optimum of a DOP is called a BSGO-TSR, if it is shifting following the rule ∀t ∈ N : x∗t+1 = Bn,t(x ∗ t ), where Bn,t : {0, 1} n → {0, 1}n flips every bit of the input binary string with a probability of σ(t) ∈ (0, 1/2], and σ(t) is called the time-variable shifting rate.\nThe theoretical results obtained in this paper can be generalized by replacing the time-invariable shifting rate of the BDOP with the above time-variable shifting rate, and the corresponding proofs will not be significantly different from existing ones. As an example, we present a generalized version of Theorem 2 as an example:\nProposition 1. Given the BitMatchingD problem with shifting rate {σ(t) = ω(logn/n 2) : t ∈ N+}, and any timevariable mutation rate scheme {Pm(n, t) ∈ [0, 1] : t ∈ N} of EA, the (1− γ4/7 logn/n)-first hitting time of (1+ 1) EA is super-polynomial with an overwhelming probability, where γ is defined by\nγ = γ(n, σ) := min\n{\nn\nlogn , σ ·\nn2\nlogn\n}\n. (9)\nThe above proposition replaces the fixed shifting rate σ = ω(logn/n2) in Theorem 2 with the time-variable shifting rate {σ(t) = ω(logn/n2) : t ∈ N+}. Similarly, we can generalize other theorems in the paper. Such generalizations are correct since our theoretical analysis does not utilize concrete values of the shifting rate σ, but only relies on the upper bound, lower bound or asymptotic order of σ. Hence, the original proofs of our theorems can easily be relaxed and utilized as proofs of such generalizations. For the sake of brevity, we do not provide the detailed analysis in this paper."
    }, {
      "heading" : "5.1.2 Characterizing all forms of adaptations by condition-variable mutation rate schemes",
      "text" : "The time-variable mutation rate schemes studied in this paper are defined in Definitions 5 and 6. In our theoretical analysis showing theoretical limitations of time-variable mutation rate schemes, we avoid to utilize the concrete values of mutation rates. Instead, we optimistically considered that, by the help of an oracle, an EA can always choose the most promising mutation rates in every generation. Such a notion can be alternatively characterized by explicitly involving sophisticated information (e.g, fitness of current individuals) as conditions for specifying mutation rates, which yields the definition of condition-variable mutation rate schemes for (1 + λ) EA2:\nDefinition 14 (Condition-variable mutation rate scheme for (1 + λ) EA). The condition-variable mutation rate scheme of the (1 + λ) EA is a mapping Pm : N × N × {1, . . . , λ} × CS → [0, 1], where CS is the condition space consisting of all potential conditions that may contribute to the decision of mutation rates. Such a scheme sets the mutation rate for obtaining the χth offspring individual at the tth generation be Pm(n, t, χ,Ct) in the presence of Ct.\nThe above definition is general enough to characterize all forms of adaptations. Owing to the oracle notion utilized in our paper, all theoretical results proven for time-variable mutation schemes also hold accordingly when condition-variable schemes replace time-variable schemes. For example, the generalized versions of Theorems 1 and 4 with respect to condition-variable schemes are\nProposition 2. Given any BDOP with shifting rate σ = ω(logn/n2) and any condition-variable mutation rate scheme {Pm(n, t,Ct) ∈ [0, 1− 1/ logn] : t ∈ N,Ct ∈ CS}, the first hitting time of the (1 + 1) EA is super-polynomial with an overwhelming probability.\nProposition 3. Given any BDOP with shifting rate σ = ω(logn/n) and any condition-variable mutation rate scheme {Pm(n, t, χ,Ct) ∈ [0, 1− 1/ logn] : t ∈ N, χ ∈ {1, . . . , λ},Ct ∈ CS}, the first hitting time of the (1 + λ) EA is super-polynomial with an overwhelming probability, where the offspring size λ is a polynomial function of n.\nFor the correctness of the propositions, a straightforward and intuitive explanation is that no matter which condition-variable mutation rate scheme an EA adopts, in the optimization process it has to follow a time-dependent configuration of concrete mutation rates that can be viewed as a time-variable mutation rate scheme specified by an oracle. Since we have proven the high failure probability of each time-variable mutation rate scheme, we cannot expect that such a concrete setting can be effective if it is online-specified by some condition-variable mutation scheme. From a technical perspective, by looking at the details of the proofs of the theorems, it is easy to find that they have considered in detail all possible random transitions between different subsets decomposed from the solution space, and the decisions of any condition-variable mutation rate scheme under different conditions in every generation have been included in the analysis. To sum up, the theoretical results obtained in this paper are general enough to show theoretical limitations of adaptations of mutation rates in evolutionary algorithms."
    }, {
      "heading" : "5.2 Conjectures about (µ+ λ) EA",
      "text" : "In the evolutionary computation community, the (µ + λ) EAs, which maintain µ parents and generate λ offsprings in each generation, have received extensive investigations over the past decades. Apparently, the (1 + 1) and (1 + λ) EAs studies in this paper are special cases of (µ+ λ) EAs. After showing the theoretical limitations of time-variable mutation rate schemes for both EAs, a natural question is, whether such theoretical results can be generalized to other (µ+ λ) EAs’ cases?\nAssume that each time-variable mutation rate scheme of a (µ + λ) EA allows the algorithm to adopt λ (not necessarily different) mutation rates when generating λ offsprings at each generation. For any of such EAs, we conjecture that the time-variable mutation rate schemes fail to help them to perform efficiently when the shifting rate of a BDOP exceeds some threshold. However, for different settings of µ and λ, the concrete thresholds might be different (as shown by our results). Intuitively, we conjecture that when the ratio λ/µ becomes larger, the threshold of shifting rate will become higher. Nevertheless, this does not mean that we can excessively enhance the threshold by increasing the ratio λ/µ, and the maximal threshold of shifting rate for any (µ+λ) EA might converge to Θ(logn/n). When the shifting rate grows to ω(logn/n), from one DOP phase to the next phase the global optimum of a BDOP will change more than logn of its bits on average, which is too drastic for an EA to track. The rigorous proofs for the above conjectures will be left as our future work.\n2As stated, when λ = 1, the (1 + λ) EA is equivalent to the (1 + 1) EA"
    }, {
      "heading" : "5.3 Impact of Population on Evolutionary Dynamic Optimization",
      "text" : "We study both the (1+1) and (1+λ) EAs in this paper such that the impact of population can be demonstrated. The former is an individual-based EA, and the latter can be regarded as a population-based EA adopting the multipleoffspring strategy (a concrete way of utilizing population). Our theoretical results clearly demonstrate the positive impact of population on the performance of EA in terms of BDOPs with distinct shifting rates. To be specific, in the absence of the multiple-offspring strategy, the largest shifting rate of the BDOP class that a (1 + 1) EA can deal with efficiently is Θ(logn/n2) (Theorems 1 and 3). After adopting the multiple-offspring strategy, the (1 + λ) EA can solve efficiently the BitMatchingD problem with a shifting rate growing to σ ≤ 1/(5n). In the evolutionary computation field, this is the first time that the positive impact is validated in the context of evolutionary dynamic optimization."
    }, {
      "heading" : "5.4 Adaptation of Mutation Rate is not a Panacea",
      "text" : "In this paper, the effectiveness of time-variable mutation rate schemes is investigated on two testbeds, that is, the (1 + 1) and (1 + λ) EAs. On a BDOP whose global optimum is consistently shifting, one might expect that there is some time-variable mutation scheme which can assist the EA to track the optimum by “cleverly” and dynamically choosing appropriate mutation rate, such that the mutation rates of an EA can “fit” the stochastic movement of the global optimum. However, for both the (1 + 1) and (1 + λ) EAs we show that there are classes of BDOPs on which various time-variable mutation rate schemes fail to help EAs to perform efficiently. Moreover, our theoretical analysis has further been generalized to a concrete instance of the BDOP class called BitMatchingD. When optimizing the BitMatchingD problem whose shifting rate exceeds the theoretical threshold Θ(logn/n\n2), no timevariable mutation rate scheme can assist the (1 + 1) EA to optimize efficiently the problem. When optimizing the BitMatchingD problem whose shifting rate exceeds the theoretical bound Θ(logn/n), no time-variable mutation rate scheme can assist the (1 + λ) EA (λ is polynomial in n) to optimize efficiently the problem. Given the fact that the static BitMatching problem can be solved by the (1+1) EA with O(n lnn) generations [19], and by the (1+λ) EA with O(n lnn) function evaluations given an appropriate λ [32], for both EAs the hardness of BitMatchingD mainly comes from the movement of the global optimum. For real-world DOPs with not-too-simple stationary objective functions and a moving global optimum, it is highly likely that even a well-designed time-variable mutation rate scheme is insufficient to improve the performance of an EA, or even the “promising” time-variable mutation rate scheme does not exist. Meanwhile, noting that designing a delicate time-variable scheme could be rather timeconsuming, it might be better to follow the well-known Occam’s razor and use some fixed mutation rate, unless one can ensure that the optima of DOPs are not moving too fast."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this paper, we theoretically study the relationship between time-variable mutation rate scheme and time complexity of EAs on a class of DOPs. The analytical results are given in terms of the first hitting time of finding the moving global optimum. By decomposing the search space and estimating transitions among the resultant subspaces (intervals), our analysis shows that, when optimizing a class of DOPs, theoretical limitations do exist for both (1+1) and (1+λ) EAs with any time-variable mutation rate. Such theoretical results may lead to new understanding of the role of mutation in solving DOPs: although some specific time-variable mutation schemes have proven or validated to be helpful on some static optimization problems, it might be not be beneficial to seek for some sophisticated time-variable mutation rate scheme to improve the performances of EAs on many DOPs with moving global optima.\nIt is worth noting that we have not taken the interactions among the adaptations of parameters in different operators (e.g., mutation and crossover) or strategies (e.g., population) of EAs into account. It is possible that the combinations of different strategies can improve the performances of EAs on DOPs. Nevertheless, it seems likely that the EA will still meet some new theoretical limitation when optimizing BDOPs. In the future, we will try to carry out such theoretical studies following the methodology utilized in this paper, so as to gain more insight into the adaptations of EAs’ operators and strengthen the theoretical foundations of EAs."
    }, {
      "heading" : "A Analytical Tools",
      "text" : "Before providing the proof of Lemma 1 and Theorem 1, we need to introduce a number of lemmas. First, three mathematical tools from previous literatures are presented directly without proofs.\nLemma 4 (Chernoff bounds [35]). Let a1, a2, . . . , ak ∈ {0, 1} be k independent random variables with the same distribution:\n∀i 6= j : P(ai = 1) = P(aj = 1),\nwhere i, j ∈ {1, . . . , k}. Let a = ∑k\ni=1 ai.\n• ∀0 < δ < 1:\nP ( a < (1− δ)E[a] ) < e−E[a]δ 2/2. (10)\n• ∀δ ≤ 2e− 1:\nP ( a > (1 + δ)E[a] ) < e−E[a]δ 2/4. (11)\n• ∀δ > 0:\nP ( a > (1 + δ)E[a] ) <\n(\neδ\n(1 + δ)1+δ\n)E[a]\n. (12)\nChernoff bounds are widely used in theoretical analysis of EAs [19, 13, 14], and play important role in proving the theoretical result presented in this paper. Moreover, we present the following lemma proven by Droste [17].\nLemma 5 (Droste [17]). Let w1, . . . , wi, v1, . . . , vj ∈ {0, 1} be i + j independent random variables with the same distribution, then\nP\n(\ni ∑\nk=1\nwk >\nj ∑\nk=1\nvk\n)\n≤ i\nj .\nLemma 6 ([22]). Given integers c and d,\n∑\nk\n(\nr\nc+ k\n)(\ns\nd− k\n)\n=\n(\nr + s\nc+ d\n)\nholds."
    }, {
      "heading" : "B Transition Lemmas with Proofs",
      "text" : "B.1 Transition Lemmas\nBased on the three basic lemmas, the definitions and notations introduced in previous sections, we will present several “transition lemmas”, concerning the transition probabilities between different subintervals of [0, n], in responses to the DOP change and the mutation operator of EA. The purpose of employing these lemmas is to pack the propositions concerning the transitions between different intervals defined in Section 3, so that they can be directly utilized in the proofs of Lemmas 1 and 2, Theorem 1, and 2. As a result, the above proofs can be significantly simplified. Given the notations N (P ) t , N (O) t and Nt defined in Section 4.2, the transition lemmas can be presented as follows:\nLemma 7. Given any BDOP with σ = ω(logn/n) and σ ∈ (0, 1/2], for the tth generation (t ∈ N) of the (1 + λ) EA with the mutation rate configuration {Pm(n, t, χ) ∈ [0, 1] : χ ∈ {1, . . . , λ}},\n1. if Nt−1 = i = o(n) and Nt−1 = i > n/ log 2 n (t ≥ 1), then the probability of N (P ) t = j ∈ [n − n log3 n , n] is\nsuper-polynomially small.\n2. if ∃ constants ǫ1, ǫ2 ∈ (0, 1) such that ǫ2 ≤ ǫ1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n, then the probability of N (P ) t = j ∈\n[n− n log3 n , n] is super-polynomially small.\n3. if Nt−1 = i = n − o(n) (t ≥ 1), then the probability of N (P ) t = j > Nt−1 − iσ/4 is super-polynomially small,\nwhere iσ/4 = ω(logn).\n4. the above three propositions also hold if N (P ) t is replaced by N (χ) t , and iσ is replaced by i(Pm(n, t, χ) + σ −\n2Pm(n, t, χ)σ) (in the 3 rd proposition).\nLemma 8. Given any BDOP with σ ∈ (0, 1/2], for the tth generation (t ∈ N) of the (1 + λ) EA with the mutation rate configuration {Pm(n, t, χ) ∈ [0, 1] : χ ∈ {1, . . . , λ}},\n1. if N (P ) t = i = o(n), N (P ) t = i > n/ log 2 n and Pm(n, t, χ) = ω(logn/n), then the probability of N (χ) t = j ∈\n[n− n log3 n , n] is super-polynomially small.\n2. if ∃ constants ǫ1, ǫ2 ∈ (0, 1) such that ǫ2 ≤ ǫ1, ǫ2n ≤ N (P ) t = i ≤ ǫ1n and Pm(n, t, χ) = ω(logn/n), then the\nprobability of N (χ) t = j ∈ [n− n log3 n , n] is super-polynomially small.\n3. if N (P ) t = i = n− o(n) and Pm(n, t, χ) = ω(logn/n), then the probability of N (χ) t = j > N (P ) t − iPm(n, t, χ)/4\nin one generation is super-polynomially small, where iPm(n, t, χ)/4 = ω(logn).\n4. the above three propositions also hold if “t ∈ N” is replaced by “t ∈ N+”, N (P )t is replaced by Nt−1, and iPm(n, t, χ) is replaced by i(Pm(n, t, χ) + σ − 2Pm(n, t, χ)σ) (in the 3 rd proposition).\nLemma 9. Given the BitMatchingD problem with σ ∈ (0, 1/2], for the t th generation (t ∈ N) of the (1 + λ) EA with the mutation rate configuration {Pm(n, t, χ) ∈ [0, 1] : χ ∈ {1, . . . , λ}},\n1. if Pm(n, t, χ) = ω(logn/n), and ∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1, ǫ2n ≤ N (P ) t = i ≤ ǫ1n, then\nthe probability of N (χ) t = j ∈ [0, n log2 n ] is super-polynomially small.\n2. if N (P ) t = i = o(n) and Pm(n, t, χ) = ω(logn/n), then the probability of N (χ) t = j < N (P ) t +(n− i)Pm(n, t, χ)/4\nis super-polynomially small, where (n− i)Pm(n, t, χ)/4 = ω(logn).\nLemma 10. Given any BDOP with σ = ω(logn/n) and σ ∈ (0, 1/2], for the tth generation (t ∈ N) of the (1 + λ) EA with the mutation rate configuration {Pm(n, t, χ) ∈ [0, 1− 1/ logn] : χ ∈ {1, . . . , λ}} (Specifically, if the BDOP is BitMatchingD, then the condition {Pm(n, t, χ) ∈ [0, 1 − 1/ logn] : χ ∈ {1, . . . , λ}} can be further relaxed to {Pm(n, t, χ) ∈ [0, 1] : χ ∈ {1, . . . , λ}}),\n1. if Nt−1 = i = n − o(n) and Nt−1 = i < n − n/ log 3 n, then the probability of N (P ) t = j ∈ [0, n log2 n ] is\nsuper-polynomially small.\n2. if ∃ constants ǫ1, ǫ2 ∈ (0, 1) such that ǫ2 ≤ ǫ1, and ǫ2n ≤ Nt−1 = i ≤ ǫ1n, then the probability of N (P ) t = j ∈\n[0, n log2 n ] is super-polynomially small.\n3. if Nt−1 = i = o(n), then the probability of N (P ) t = j < Nt−1 + (n − i)σ/4 is super-polynomially small, where\n(n− i)σ/4 = ω(logn).\n4. the above three propositions also hold if N (P ) t is replaced by N (χ) t , and (n−i)σ is replaced by (n−i)(Pm(n, t, χ)+\nσ − 2Pm(n, t, χ)σ) (in the 3 rd proposition).\nWhen λ = 1, the above lemmas hold for the (1 + 1) EA. The proofs of the above lemmas are very similar to each other. For the sake of brevity, next we only provide the detailed proofs of Lemmas 7 and 10.1.\nB.2 Proof of Lemma 7\nTo prove the transition lemmas, we need the following lemma:\nLemma 11. Let r(n, t, χ) = Pm(n, t, χ)(1− σ)+ σ(1−Pm(n, t, χ)) be the composite bitwise mapping rate for for the χth offspring generated at the tth generation (t ∈ N+). It satisfies that\n∀t ∈ N+ : r(n, t, χ) ∈ ( min\n{\n1 2 ,max { σ, Pm(n, t, χ) }\n}\n,max {1\n2 , Pm(n, t, χ)\n}]\n. (13)\nNoting that σ ∈ (0, 1/2], we have:\n1. If Pm(n, t, χ) ∈ (0, 1 2 ],\nr(n, t, χ) = σ + (1− 2σ)Pm(n, t, χ) ≤ σ + 1\n2 (1− 2σ) =\n1 2 ,\nr(n, t, χ) = Pm(n, t, χ) + (1− 2Pm(n, t, χ))σ > Pm(n, t, χ),\nr(n, t, χ) = σ + (1− 2σ)Pm(n, t, χ) > σ;\n2. If Pm(n, t, χ) ∈ ( 1 2 , 1),\nr(n, t, χ) = Pm(n, t, χ) + (1 − 2Pm(n, t, χ))σ < Pm(n, t, χ) r(n, t, χ) = σ + (1 − 2σ)Pm(n, t, χ) > σ + 1\n2 (1 − 2σ) =\n1 2 .\nBy summarizing the above inequalities, we have\nr(n, t, χ) ∈ ( min\n{\n1 2 ,max { σ, Pm(n, t, χ) }\n}\n,max {1\n2 , Pm(n, t, χ)\n}]\n.\nB.2.1 Lemma 7.1 — Lemma 7.3\nWe prove the three propositions of the lemma respectively. Proof of Lemma 7.1 Noting that σ ≤ 1/2 holds, we estimate the probability that in one generation the EA finds number of matching bits N (P ) t = j ∈ [n− n/ log 3 n, n]:\nP\n(\nN (P ) t = j | Nt−1 = i = o(n), Nt−1 = i >\nn\nlog2 n , j ∈\n[ n− n\nlog3 n , n\n] , σ = ω ( logn\nn\n)\n)\n=\nmin{i,n−j} ∑\nk=0\n(\nn− i\nj − i+ k\n)(\ni\nk\n)\nσj−i+2k(1 − σ)n−(j−i+2k)\n< σj−i min{i,n−j} ∑\nk=0\n(\nn− i\nn− j − k\n)(\ni\nk\n)\n=\n(\nn\nn− j\n)\nσj−i ≤ nn−j (1\n2\n)j−i\n(by Lemma 6)\n= no ( n log n ) (1\n2\n)n−o(n)\n= 2o(n) (1\n2\n)n−o(n)\n≺ 1\nSuperPoly(n) ,\nwhich is a super-polynomially small probability. Proof of Lemma 7.2 The following probability can be estimated similarly as in the first case of the proof of Lemma 7.1. For the sake of brevity, we provide the result directly:\nP\n(\nNt = j | ǫ2n ≤ Nt−1 = i ≤ ǫ1n, j ∈ [ n− n\nlog3 n , n\n] , σ = ω ( logn\nn\n)\n)\n≺ 1\nSuperPoly(n) ,\nwhich is a super-polynomially small probability. Proof of Lemma 7.3 We will prove this result by applying Chernoff bounds to the matching bits and non-matching bits respectively. After a DOP change, if the number of flipped matching bits is no smaller than that of the flipped non-matching bits, then the number of matching cannot increase after the DOP change.\nAccording to the condition of Lemma 7.3, Nt−1 = i = n − o(n) holds, thus the number of non-matching bits is n− i = o(n). Let D+t and D − t be the numbers of flipped non-matching and matching bits after the DOP change at the beginning of the tth generation, respectively. According to Chernoff bounds, we have\nP\n(\nD+t > iσ\n4 | Nt−1 = i = n− o(n), σ = ω\n( logn\nn\n)\n)\n<\n(\ne\nc(n)\n)Θ(nσ)\n=\n(\ne\nω(logn)\n)ω(logn)\n≺ 1\nSuperPoly(n) ,\nwhere c(n) is a polynomial function of the problem size n that satisfies c(n) = o(nσ) and c(n) = ω(1). On the other hand, for the number of flipped matching bits, we can also use Chernoff bounds:\nP\n(\nD−t < iσ\n2 | Nt−1 = i = n− o(n), σ = ω\n( logn\nn\n)\n)\n< e− (n−o(n))σ 8 = e−ω(logn) ≺ 1\nSuperPoly(n) .\nThus, given the condition Nt−1 = i = i(n) = n− o(n) (where i(n) is a function of the problem size n), the following probability is super-polynomially small:\nP\n(\nD + t +\niσ\n4 > D\n− t | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\n<\nn−1 ∑\nk=0\nP\n(\nD − t = k | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\nP\n(\nD + t +\niσ\n4 > k | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\n=\ni(n)σ 2\n−1 ∑\nk=0\nP\n(\nD − t = k | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\nP\n(\nD + t +\niσ\n4 > k | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\n+\nn−1 ∑\nk= i(n)σ\n2\nP\n(\nD − t = k | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\nP\n(\nD + t +\niσ\n4 > k | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\n< 1\nSuperPoly(n)\ni(n)σ 2\n−1 ∑\nk=0\nP\n(\nD + t +\niσ\n4 > k | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\n+ 1\nSuperPoly(n)\nn−1 ∑\nk= i(n)σ\n2\nP\n(\nD − t = k | Nt−1 = n− o(n), σ = ω\n( log n\nn\n)\n)\n≺ 1\nSuperPoly(n)\nAs a consequence,\nP\n(\nD+t −D − t > −\niσ\n4 | Nt−1 = n− o(n), σ = ω\n( logn\nn\n)\n)\n≺ 1\nSuperPoly(n)\nholds. Combining the above inequality with the fact\nN (P ) t = Nt−1 +D + t −D − t ,\nwe have proven Lemma 7.3.\nB.2.2 Lemma 7.4\nIn context of the details redefined in Lemma 7.4, we prove the three propositions of the lemma respectively. In this proof, the number of matching bits of the χth offspring individual generated at the tth generation (i.e., N (χ) t ) will be considered. Proof of Lemma 7.1 We need to consider two different cases: Pm(n, t, χ) ≤ 1− h and Pm(n, t, χ) > 1− h, where h ∈ (0, 1) is a constant. − First Case: Pm(n, t, χ) ≤ 1 − h holds. Given the conditions that σ = ω(logn/n) and Pm(n, t, χ) ≤ 1 − h, by applying Lemma 11 we obtain:\nr(n, t, χ) = ω(logn/n),\nr(n, t, χ) ≤ max {1\n2 , 1− h\n}\n.\nBy above inequalities, we estimate the probability that in one generation the EA finds number of matching bits\nN (χ) t = j ∈ [n− n/ log 3 n, n]:\nP\n(\nN (χ) t = j | Nt−1 = i = o(n), Nt−1 = i >\nn\nlog2 n , j ∈\n[\nn− n log3 n , n ] , Pm(n, t, χ) ≤ 1− h, σ = ω ( log n n )\n)\n=\nmin{i,n−j} ∑\nk=0\n(\nn− i\nj − i+ k\n)(\ni\nk\n)\nr(n, t, χ)j−i+2k(1− r(n, t, χ))n−(j−i+2k)\n< r(n, t, χ)j−i min{i,n−j} ∑\nk=0\n(\nn− i\nn− j − k\n)(\ni\nk\n)\n=\n(\nn\nn− j\n)\nr(n, t, χ)j−i < nn−j max {1\n2 , 1− h\n}j−i\n(by Lemma 6)\n= n o ( n log n ) max {1\n2 , 1− h\n}n−o(n) = 2o(n) max {1\n2 , 1− h\n}n−o(n) ≺ 1\nSuperPoly(n) ,\nwhich is a super-polynomially small probability. − Second Case: Pm(n, t, χ) > 1− h holds. Given the condition that σ = ω(logn/n) and Pm(n, t, χ) > 1 − h, by\napplying Lemma 11 we obtain:\nr(n, t, χ) > min\n{\n1 2 ,max { σ, Pm }\n}\n≥ min\n{\n1 2 ,max { σ, 1 − h }\n}\n≥ min {1\n2 , 1− h\n}\n.\nOn the other hand, we must note the fact called symmetrical bitwise mapping: given the condition that the number of matching bits Nt−1 equals i and the composite bitwise mapping rate r(n, t, χ), the consequence of the bitwise mapping is equivalent to that of the case in which Nt−1 equals n− i and the composite bitwise mapping rate equals 1− r(n, t, χ).\nFormally, we have\n1− r(n, t, χ) < 1−min {1\n2 , 1− h\n} = max {1\n2 , h\n}\n.\nNoting the fact described above, we know ∀i that satisfies i = o(n) and i > n/ log2 n, the following equation holds in response to the symmetrical bitwise mapping:\nP\n(\nN (χ) t = j | Nt−1 = i, i = o(n), i >\nn\nlog2 n , j ∈\n[ n− n\nlog3 n , n\n]\n, r(n, t, χ)\n)\n= P\n(\nN (χ) t = j | N ∗ t−1 = n− i, i = o(n), i >\nn\nlog2 n , j ∈\n[ n− n\nlog3 n , n\n]\n, r∗(n, t, χ) = 1− r(n, t, χ)\n)\n,\nwhere we use r∗(n, t, χ) to represent the notional composite bitwise mapping rate with the value of 1− r(n, t, χ), and N∗t−1 to represent the notional number of matching bits (found by the EA) at the end of the (t− 1)\nth generation. According to the value of r∗(n, t, χ), there are further two subcases for us to consider. In the first situation, r∗(n, t, χ) = O(log n/n) holds. By Chernoff bounds, we know that with an overwhelming probability there are at most log2 n flipped bits among the total n bits:\nP\n(\n|N (χ) t −N ∗ t−1| < log\n2 n | r∗(n, t, χ) = O ( logn\nn\n)\n)\n> 1−\n(\ne\nΩ(logn)\n)Ω(log2 n)\n≻ 1− 1\nSuperPoly(n) .\nConsequently, with an overwhelming probability the number of matching bits will decrease or increase by at most log2 n after the overall bitwise mapping. It follows from N∗t−1 = n−i, i = o(n) and i > n/ log 2 n that the above upper bound implies that N (χ) t < n − n/ log 2 n + log2 n < n − n/ log3 n and N (χ) t > n/ log\n2 n hold with an overwhelming probability. In other words,\nP\n(\nN (χ) t = j | N ∗ t−1 = n− i, i = o(n), i >\nn\nlog2 n , j ∈\n[ n− n\nlog3 n , n\n] , r∗(n, t, χ) = O ( logn\nn\n)\n)\n≺ 1\nSuperPoly(n) .\nIn the second subcase, r∗(n, t, χ) = ω(logn/n) holds. In the proof of Lemma 7.3, we will consider the case r(n, t, χ) = ω(logn/n). Since there is no essential difference between the proofs related to r∗(n, t, χ) and r(n, t, χ), we will not provide the proof here for the sake of brevity. For details, one can refer to the proof of Lemma 7.3 below.\nCombining the above two subcases together, we obtain that:\nP\n(\nN (χ) t = j | Nt−1 = i, i = o(n), i >\nn\nlog2 n , j ∈\n[ n− n\nlog3 n , n\n] , r(n, t, χ) > max {1\n2 , 1− h\n}\n)\n= P\n(\nN (χ) t = j | N ∗ t−1 = n− i, i = o(n), i >\nn\nlog2 n , j ∈\n[ n− n\nlog3 n , n\n] , r∗(n, t, χ) < max {1\n2 , h\n}\n)\n≺ 1\nSuperPoly(n) .\nThus we have finished the proof of the second case. Combining the first and second cases together, we have proven Lemma 7.1.\nProof of Lemma 7.2 For convenience, we omit it the index “(n, t)” in the proof, since the rest part of the proof are restricted in the tth generation only. The following probability can be estimated similarly as in the first case of the proof of Lemma 7.1. For the sake of brevity, we provide the result directly:\nP\n(\nNt+1 = j | ǫ2n ≤ Nt = i ≤ ǫ1n, j ∈ [ n− n\nlog3 n , n\n] , r = ω ( logn\nn\n)\n)\n=\nmin{i,n−j} ∑\nk=0\n(\nn− i\nj − i+ k\n)(\ni\nk\n)\nrj−i+2k(1− r)n−(j−i+2k)\n< rj−i(1 − r)n−(j−i+2n/ log 3 n)\nmin{i,n−j} ∑\nk=0\n(\nn− i\nn− j − k\n)(\ni\nk\n)\n=\n(\nn\nn− j\n)\nrj−i(1− r)n−(j−i+2n/ log 3 n) < nn−jrj−i(1− r)n−(j−i+2n/ log 3 n) (by Lemma 6)\n= no ( n log n ) ω ( logn\nn\n)Θ(n) (\n1− ω ( logn\nn\n)\n)Θ(n)\n≺ 1\nSuperPoly(n) ,\nwhich is a super-polynomially small probability. Thus we have proven Lemma 7.2. Proof of Lemma 7.3 We will prove this result by applying Chernoff bounds to the matching bits and non-matching bits respectively. In one generation, if the number of flipped matching bits is no smaller than that of the flipped non-matching bits, then the number of matching cannot increase.\nAccording to the condition of Lemma 7.3, the number of matching bits at the (t − 1)th generation satisfies Nt−1 = i = n − o(n), thus the number of non-matching bits is n − i = o(n). Let L + t and L − t be the numbers of flipped non-matching and matching bits after the DOP change and the mutation at the tth generation, respectively. By Chernoff bounds, we have\nP\n(\nL+t > ir\n4 | Nt−1 = i = n− o(n), r = ω\n( logn\nn\n)\n)\n<\n(\ne\nc(n)\n)Θ(nr)\n=\n(\ne\nω(logn)\n)ω(logn)\n≺ 1\nSuperPoly(n) ,\nwhere c(n) is a polynomial function of the problem size n that satisfies c(n) = o(npr) and c(n) = ω(1). On the other hand, for the number of flipped matching bits, we also apply Chernoff bounds:\nP\n(\nL−t < ir\n2 | Nt−1 = i = n− o(n), r = ω\n( logn\nn\n)\n)\n< e− (n−o(n))r 8 = e−ω(logn) ≺ 1\nSuperPoly(n) .\nThus we know that, given the condition Nt = i = i(n) = n− o(n) (where i(n) is a function of the problem size n),\nthe following probability is super-polynomially close to 0:\nP\n(\nL + t +\nir 4 > L − t | Nt−1 = n− o(n), r = ω ( log n n )\n)\n<\nn−1 ∑\nk=0\nP\n(\nL − t = k | Nt−1 = n− o(n), r = ω\n( log n\nn\n)\n)\nP\n(\nL + t +\nir 4 > k | Nt−1 = n− o(n), r = ω ( log n n )\n)\n=\ni(n)r 2\n−1 ∑\nk=0\nP\n(\nL − t = k | Nt−1 = n− o(n), r = ω\n( log n\nn\n)\n)\nP\n(\nL + t+1 +\nir 4 > k | Nt−1 = n− o(n), r = ω ( log n n )\n)\n+\nn−1 ∑\nk= i(n)r\n2\nP\n(\nL − t = k | Nt−1 = n− o(n), r = ω\n( log n\nn\n)\n)\nP\n(\nL + t +\nir 4 > k | Nt−1 = n− o(n), r = ω ( log n n )\n)\n< 1\nSuperPoly(n)\ni(n)r 2\n−1 ∑\nk=0\nP\n(\nL + t +\nir 4 > k | Nt−1 = n− o(n), r = ω ( log n n )\n)\n+ 1\nSuperPoly(n)\nn−1 ∑\nk= i(n)r\n2\nP\n(\nL − t = k | Nt−1 = n− o(n), r = ω\n( log n\nn\n)\n)\n≺ 1\nSuperPoly(n)\nNoting that\nN (χ) t = Nt−1 + L + t − L − t ,\nwe have proven Lemma 7.3.\nB.3 Proof of Lemma 10.1\nLemma 10.1 has two versions. The first version presents a general result for any BDOP whose shifting rate satisfies σ = ω(logn/n) and σ ∈ (0, 1/2], where the time-variable mutation rate should be smaller than 1 − 1/ logn. The second version is for the BitMatchingD problem whose shifting rate satisfies σ = ω(logn/n) and σ ∈ (0, 1/2], where the time-variable mutation rate can take any value between 0 and 1.\nB.3.1 General result for BDOP Class\nLet us first study N (P ) t under the conditions Nt−1 = n− o(n) and Nt−1 < n− n log3 n . According to Chernoff bounds, we know that with an overwhelming probability there are at most 3n/4 flipped bits among the total n bits after the DOP change at the beginning of the tth generation:\nP\n(\n|N (P ) t −Nt−1| <\n3 4 n | σ ∈ ( 0, 1 2 ]\n)\n> 1− e−n/24 ≻ 1− 1\nSuperPoly(n) .\nNoting that the range between n/ log2 n and n − o(n) is much larger than 3n/4 (i.e., n− o(n) − n/ log2 n > 3n/4), we know that the probability of N (P ) t ∈ L1, conditional on Nt−1 = n − o(n) and Nt−1 < n − n/ log\n3 n, is superpolynomially close to 0. Formally,\nP\n(\nN (P ) t ≤\nn\nlog2 n | Nt−1 = n− o(n), Nt−1 < n−\nn\nlog3 n\n)\n≺ 1\nSuperPoly(n) . (14)\nThus the original version of Lemma 10.1 for the general BDOP class is proven.\nLet us further consider the proof when N (P ) t is replaced by N (χ) t in Lemma 10.1. It follows from the two conditions σ = ω(logn/n) and Pm(n, t, χ) ≤ 1− 1/ logn that r(n, t, χ) ≤ 1− 1/ logn. According to Lemma 11, we estimate the probability that N (χ) t ∈ L1, conditional on Nt−1 ∈ A2 ∪ A1 and r(n, t, χ) ≤ 1 − 1/ logn. Let L − t be the number of\nflipped matching bits after the DOP change and the mutation at the tth generation, we have\nP\n(\nN (χ) t ∈ L1 | Nt−1 = n− o(n), Nt−1 < n−\nn\nlog3 n , r(n, t, χ) ≤ 1−\n1\nlogn\n)\n= P\n(\nN (χ) t ≤\nn\nlog2 n | Nt−1 = n− o(n), Nt−1 < n−\nn\nlog3 n , r(n, t, χ) ≤ 1−\n1\nlogn\n)\n≤ P\n(\nn− ( L−t + (n−Nt−1) ) ≤ n\nlog2 n | Nt−1 = n− o(n), Nt−1 < n−\nn\nlog3 n , r(n, t, χ) ≤ 1−\n1\nlogn\n)\n= P\n(\nL−t ≥ Nt−1 − n\nlog2 n | Nt−1 = n− o(n), Nt−1 < n−\nn\nlog3 n , r(n, t, χ) ≤ 1−\n1\nlog n\n)\n.\nOn one hand, if r(n, t, χ) > 14e , then by Chernoff bound we further have\nP\n(\nN (χ) t ∈ L1 | Nt−1 = n− o(n), Nt−1 < n−\nn log3 n , 1 4e < r(n, t, χ) ≤ 1− 1 log n\n)\n≤ P\n(\nL−t ≥ Nt−1 − n\nlog2 n = (1 + ρ1)Ê[L\n− t ] | Nt−1 = n− o(n), Nt−1 < n−\nn log3 n , 1 4e < r(n, t, χ) ≤ 1− 1 logn\n)\n= P\n(\nL−t ≥ (1 + ρ1)Ê[L − t ] | Nt−1 = n− o(n), Nt−1 < n−\nn log3 n , 1 4e < r(n, t, χ) ≤ 1− 1 logn\n)\n< e−Ê[L − t ]ρ 2 1/4 ≺\n1\nSuperPoly(n) , (15)\nwhere ρ1 = (Nt−1 − n/ log 2 n)/(Nt−1 −Nt−1/ logn)− 1 = Θ(1) (by Nt−1 = n− o(n)), and Ê[L − t ] = E[L − t | Nt−1 = n − o(n), Nt−1 < n − n/ log 3 n, 1/4e < r(n, t, χ) ≤ 1 − 1/ logn] = Nt−1r(n, t, χ) = Θ(n) (by Nt−1 = n − o(n) and r(n, t, χ) > 1/4e). On the other hand, let us consider the case r(n, t, χ) ≤ 14e . By Chernoff bound, we have\nP\n(\nN (χ) t ∈ L1 | Nt−1 = n− o(n), Nt−1 < n−\nn\nlog3 n , r(n, t, χ) ≤\n1\n4e\n)\n= P\n(\nL−t ≥ Nt−1 − n\nlog2 n = (1 + ρ2)Ē[L\n− t ] | Nt−1 = n− o(n), Nt−1 < n−\nn\nlog3 n , r(n, t, χ) ≤\n1\n4e\n)\n< ( e\n1 + ρ2\n)(1+ρ2)Ē[L − t ]\n= ( e\n1 + ρ2\n)Nt−1− n\nlog2 n ≺\n1\nSuperPoly(n) , (16)\nwhere Ē[L−t ] = E[L − t | Nt−1 = n − o(n), Nt−1 < n − n/ log 3 n, r(n, t, χ) ≤ 1/4e] ≤ n/4e and ρ2 = (Nt−1 − n/ log2 n)/Ē[L−t ]− 1 > (n/2)/(n/4e)− 1 ≥ 2e− 1. Thus we have proven Lemmas 10.1.\nB.3.2 Specific result for BitMatchingD\nBy Chernoff bounds, we know that with an overwhelming probability there are at most 3n/4 flipped bits among the total n bits after the DOP change at the beginning of the tth generation:\nP\n(\n|N (P ) t −Nt−1| <\n3 4 n | σ ∈ ( 0, 1 2 ]\n)\n> 1− e−n/24 ≻ 1− 1\nSuperPoly(n) . (17)\nConsequently, with an overwhelming probability, the number of matching bits will decrease or increase by at most 3n/4 after the DOP change. Recall that Nt−1 = i = n− o(n) is one of the conditions of Lemma 10.1, we know that N (P ) t > n/ log 2 n holds with an overwhelming probability. Thus we have proven the original version of Lemma 10.1.\nMeanwhile, let us consider the alternative version of Lemma 10.1, where N (P ) t is replaced by N (χ) t . Since the EA always preserves the one with better fitness between the parent and offspring individuals, we know that Nt ≥ N (P ) t always holds given the BitMatchingD problem. Combining the above fact with Eq. 17, we obtain the alternative version of Lemma 10.1."
    }, {
      "heading" : "C Proofs of Lemmas 1 and 2, and Theorems 4 and 6",
      "text" : "C.1 Lemmas 1 and 2\nThe only difference between the proofs of Lemmas 1 and 2 is that the former utilizes the original version of Lemma 10 for the general BDOP class, while the latter utilizes the specific version of Lemma 10 for the BitMatchingD problem. Hence, we only provide a unified proof for the sake of brevity. As mentioned in Section 3.1, the proof contains the analysis related to Propositions A1.1, A1.2, and A1.3.\nHere we study the above propositions one after another. Analysis of Proposition A1.1. As the first step, we prove that the initial number of matching bits satisfies that N (P ) 0 ∈ O1 holds with an overwhelming probability. Since the initial individual is generated randomly by the uniform distribution, we estimate the following two probabilities by Chernoff bounds (Lemma 4):\nP (\nN (P ) 0 < 1 4n\n) < e−n/16 ≺ 1\nSuperPoly(n) ,\nP (\nN (P ) 0 > 3 4n\n) < (8e\n27\n)n/4\n≺ 1\nSuperPoly(n) ,\nIn other words, N (P ) 0 ∈ [n/4, 3n/4] (where [n/4, 3n/4] ⊂ O1) holds with an overwhelming probability.\nGiven the conditionN (P ) 0 ∈ [n/4, 3n/4], we now prove that the probability of N (O) 0 ∈ F1∪L1 is super-polynomially\nclose to 0. According to the mutation rate at the 0th generation Pm(n, 0), there are two cases: − First case (Pm(n, 0) = ω(logn/n)): According to Lemmas 8.2 and 9.1, we know that:\nP\n(\nN (O) 0 ∈ F1 ∪ L1 | N (P ) 0 ∈\n[n 4 , 3n 4 ] , Pm(n, 0) = ω ( log n n )\n)\n≺ 1\nSuperPoly(n) .\n− Second case (Pm(n, 0) = O(log n/n)): By Chernoff bounds, we know that with an overwhelming probability there are at most log2 n flipped bits among the total n bits after mutation (which implies that the number of matching bits can decrease or increase by at most log2 n after mutation):\nP\n(\n|N (O) 0 −N (P ) 0 | < log 2 n | N (P ) 0 ∈\n[n 4 , 3n 4 ] , Pm(n, 0) = O ( logn n )\n)\n> 1−\n(\ne\nΩ(logn)\n)Ω(log2 n)\n≻ 1− 1\nSuperPoly(n) .\nIn other words, N (O) 0 ∈ [log 2 n + n/4, log2 n + 3n/4] holds with an overwhelming probability (given the condition that N (P ) 0 ∈ [n/4, 3n/4], Pm(n, 0) = O(log n/n)). It then follows from N (O) 0 ∈ [log\n2 n+ n/4, log2 n+ 3n/4] * F1 ∪ L1 that N\n(O) 0 /∈ F1 ∪ L1 holds with an overwhelming probability (given the conditions that N (P ) 0 ∈ [n/4, 3n/4] and\nPm(n, 0) = O(log n/n)).\nCombining the above facts for N (P ) 0 and N (O) 0 , we obtain\nP(N0 ∈ F1 ∪ L1) ≺ 1\nSuperPoly(n) . (18)\nAnalysis of Proposition A1.2. Given the condition Nt−1 ∈ O1 (t ∈ N+), we now prove that the probability of the event O1 Ct−→ F1 ∪ L1 is super-polynomially close to 0. The condition Nt−1 ∈ O1 leads to one of the following cases, and Lemmas 7 and 10 provide the corresponding probability results:\nCase i [Nt−1 = o(n) and Nt−1 > n/ log 2 n both hold ]: By Lemmas 7.1 and 10.3, the probability of the event N (P ) t ∈ F1 ∪ L1, conditional on Nt−1 = o(n) and Nt−1 > n/ log 2 n, is super-polynomially close to 0;\nCase ii [∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n]: By Lemmas 7.2 and 10.2, the probability of the event N (P ) t ∈ F1 ∪L1, conditional on the event that ∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n, is super-polynomially close to 0;\nCase iii [Nt−1 = n − o(n) and Nt−1 < n − n/ log 3 n both hold ]: By Lemma 7.3 and 10.1, the probability of the event N (P ) t ∈ F1 ∪ L1, conditional on Nt−1 = n− o(n) and Nt−1 < n− n/ log\n3 n, is super-polynomially close to 0.\nBy summarizing the above results, we have:\nP (\nN (P ) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n) ≺ 1\nSuperPoly(n) . (19)\nAnalysis of Proposition A1.3. Next we aim at proving that the joint probability of the events N (P ) t ∈ O1 and N (O) t ∈ F1 ∪ L1, conditional on Nt−1 ∈ O1, is super-polynomially close to 0. The above proposition is a direct corollary of the following inequality:\nP (\nN (P ) t ∈ F1 ∪ L1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n) + P (\nN (P ) t ∈ O1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n) ≺ 1\nSuperPoly(n) .\nMeanwhile, the above inequality is equivalent to the following inequality:\nP (\nN (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n) ≺ 1\nSuperPoly(n) . (20)\nHence, to prove that Proposition A1.3, we only need to prove Eq. 20. As we know, the condition Nt−1 ∈ O1 always implies three potential cases, and Lemma 7 provides the corresponding probability results:\nCase i [Nt−1 = o(n) and Nt−1 > n/ log 2 n both hold ]: By Lemmas 7.1 and 7.4 3, the probability ofN (O) t ∈ F1, conditional on Nt−1 = o(n) and Nt−1 > n/ log 2 n, is super-polynomially close to 0; By Lemmas 10.3 and 10.4, the probability of N (O) t ∈ L1, conditional on Nt−1 = o(n), is super-polynomially close to 0.\nCase ii [∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n]: By Lemmas 7.2 and 7.4, the probability of N (O) t ∈ F1, conditional on the event that ∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n, is super-polynomially close to 0; By Lemmas 10.2 and 10.4, the probability of the event N (O) t ∈ L1, conditional on the event that ∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n, is super-polynomially close to 0.\nCase iii [Nt−1 = n − o(n) and Nt−1 < n − n/ log 3 n hold ]: By Lemmas 7.3 and 7.4, the probability of the event N (O) t ∈ F1, conditional on Nt−1 = n − o(n) and Nt−1 < n − n/ log 3 n, is super-polynomially close to 0; By Lemmas 10.1 and 10.4, the probability of the event N (O) t ∈ L1, conditional on Nt−1 = n − o(n) and Nt−1 < n− n/ log 3 n, is super-polynomially close to 0.\nBy summarizing the above results, we can obtain Eq. 20. Hence we have proven that:\nP (\nN (P ) t ∈ O1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n) ≺ 1\nSuperPoly(n) . (21)\nConclusion. Combining the probabilities described in Eqs. 19, and 21 together, we have\nP ( Nt ∈ F1 ∪ L1 | Nt−1 ∈ O1 )\n≤ P (\nN (P ) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n)\n+ P(N (P )t ∈ O1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1) ≺\n1\nSuperPoly(n)\nholds ∀t ∈ N+. Let τF1 be defined as the first hitting time to the interval F1, formally,\nτF1 = min { t ≥ 0; (N (P ) t ∈ F1) ∨ (N (O) t ∈ F1) } . (22)\nMeanwhile, ∀t ∈ N+ we have:\nP(τF1 = t) < P(Nt ∈ F1 ∪ L1, Nt−1 ∈ O1, . . . , N0 ∈ O1)\n= P(Nt ∈ F1 ∪ L1 | Nt−1 ∈ O1, . . . , N0 ∈ O1) · P(Nt−1 ∈ O1, . . . , N0 ∈ O1)\n= P(Nt ∈ F1 ∪ L1 | Nt−1 ∈ O1) · P(Nt−1 ∈ O1, . . . , N0 ∈ O1) ≤ P(Nt ∈ F1 ∪ L1 | Nt−1 ∈ O1) ≺ 1\nSuperPoly(n) .\n3For the (1 + 1) EA, there is only one offspring individual at each generation, thus N (O) t = N (1) t .\nMoreover, Eq. 18 implies P(τF1 = 0) ≺ 1/SuperPoly(n). Consequently, ∀g(n) ≺ Poly(n), the first hitting time to the target interval F1, which is denoted by τF1 , satisfies that\nP ( τF1 ≤ g(n) ) =\ng(n) ∑\nt=0\nP(τF1 = t) ≤ ( g(n) + 1 )\nmax t≤g(n)\n{ P(τF1 = t) }\n≺ Poly(n)\nSuperPoly(n) ∼\n1\nSuperPoly(n) .\nIn other words, the probability of τF1 ≺ Poly(n) is super-polynomially close to 0. Consequently, we have proven Lemmas 1 and 2.\nC.2 Theorems 4 and 6\nThe proof idea of Theorem 4 is quite similar to that of Lemma 1. After updating the definition of N (O) t by max{N (1) t , . . . , N (λ) t } in response to the multiple-offspring strategy employed by the (1 + λ) EA, the main difference between the two proofs is that the transition events with respect to every generation of the (1 + λ) EA must involve λ mutations generating λ different offspring individuals, while those of the (1 + 1) EA only consider a single mutation leading to a unique offspring individual. Meanwhile, the definition of τF1 must be updated accordingly\nτF1 = min { t ≥ 0; ( N (P ) t ∈ F1 ) ∨ ( N (1) t ∈ F1 ) ∨ · · · ∨ ( N (λ) t ∈ F1 )} , (23)\nwhere N (1) t , . . . , N (λ) t are number of matching bits with respect to the λ offspring individuals at the t th generation respectively.\nThe proof of Theorem 4 can be obtained by replacing the probability propositions related to a unique mutation with probability propositions related to λ mutations. The probability propositions, demonstrated by Propositions A1.2, A1.2, and A1.3, are all about probabilities that are super-polynomially close to 0, and similar arguments will also hold for the (1 + λ) EA, since the offspring size λ is polynomial of n and will not increase a super-polynomially small probability (i.e., 1/SuperPoly(n)) to a polynomially large probability (i.e., 1/Poly(n)).\nFor the sake of brevity, we do not provide in detail the proof of Theorem 4 here. Instead, to illustrate the above proof idea, we only prove a probability proposition for (1+λ) EA as an instance. The probability proposition, which is an important step towards proving Theorem 4, is the same to Proposition A1.3 except that N (O) t is redefined as max{N (1) t , . . . , N (λ) t } in response to the multiple-offspring strategy:\n∀t ∈ N+ : P(N (P )t ∈ O1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1) ≺ 1/SuperPoly(n).\nTo prove the above proposition, we only need to prove the following inequality:\nP (\nN (P ) t ∈ F1 ∪ L1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n) + P (\nN (P ) t ∈ O1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n) ≺ 1\nSuperPoly(n) ,\nwhich is equivalent to the following inequality:\nP (\nN (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1\n) ≺ 1\nSuperPoly(n) .\nAs we have done in “Analysis of Proposition A1.3”, the condition Nt−1 ∈ O1 can be divided into three potential cases, and Lemma 7 provides the corresponding probability results:\nCase i [Nt−1 = o(n) and Nt−1 > n/ log 2 n both hold ]: By Lemmas 7.1 and 7.4, the probability of N (χ) t ∈ F1 (∀χ ∈ {1, . . . , λ}), conditional on Nt−1 = o(n) and Nt−1 > n/ log 2 n, is super-polynomially close to 0. Adding up such probabilities with respect to λ (λ is polynomial in n) different offspring individuals (i.e., different χ) yields an upper bound for the probability of N (O) t ∈ F1 conditional on Nt−1 = o(n) and Nt−1 > n/ log\n2 n, which is still super-polynomially close to 0.\nBy Lemmas 10.3 and 10.4, the probability of N (χ) t ∈ L1, conditional on Nt−1 = o(n), is super-polynomially close to 0. Adding up such probabilities with respect to λ (λ is polynomial in n) different offspring individuals yields an upper bound for the probability of N (O) t ∈ L1 conditional on Nt−1 = o(n) and Nt−1 > n/ log\n2 n, which is still super-polynomially close to 0.\nCase ii [∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n]: By Lemmas 7.2 and 7.4, the probability of N (χ) t ∈ F1 (∀χ ∈ {1, . . . , λ}), conditional on the event that ∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n, is super-polynomially close to 0. Adding up such probabilities with respect to λ (λ is polynomial in n) different offspring individuals yields an upper bound for the probability of N (O) t ∈ F1 conditional on the same event, which is still super-polynomially close to 0. By Lemmas 10.2 and 10.4, the probability of the event N (χ) t ∈ L1, conditional on the event that ∃ constants ǫ1 and ǫ2 such that 0 < ǫ2 ≤ ǫ1 < 1 and ǫ2n ≤ Nt−1 = i ≤ ǫ1n, is super-polynomially close to 0. Adding up such probabilities with respect to λ (λ is polynomial in n) different offspring individuals yields an upper bound for the probability of N (O) t ∈ L1 conditional on the same event, which is still super-polynomially close to 0.\nCase iii [Nt−1 = n − o(n) and Nt−1 < n − n/ log 3 n hold ]: By Lemmas 7.3 and 7.4, the probability of the event N (χ) t ∈ F1 (∀χ ∈ {1, . . . , λ}), conditional on Nt−1 = n − o(n) and Nt−1 < n − n/ log\n3 n, is superpolynomially close to 0. Adding up such probabilities with respect to λ (λ is polynomial in n) different offspring individuals yields an upper bound for the probability of N (O) t ∈ F1 conditional on the same event, which is still super-polynomially close to 0.\nBy Lemmas 10.1 and 10.4, the probability of the event N (χ) t ∈ L1, conditional on Nt−1 = n − o(n) and Nt−1 < n − n/ log 3 n, is super-polynomially close to 0. Adding up such probabilities with respect to λ (λ is polynomial in n) different offspring individuals yields the upper bound for the probability of N (O) t ∈ L1 conditional on the same event, which is still super-polynomially close to 0.\nBy summarizing the above results, we have proven\n∀t ∈ N+ : P(N (P )t ∈ O1, N (O) t ∈ F1 ∪ L1 | Nt−1 ∈ O1) ≺ 1/SuperPoly(n).\nThe proof of Theorem 6 is almost the same to that of Theorem 4. The only difference between them is that the former utilizes the original version of Lemma 10 for the general BDOP class, while the latter utilizes the specific version of Lemma 10 for the BitMatchingD problem. For the sake of brevity, we do not provide the details here."
    }, {
      "heading" : "D Proof of Theorems 1 and 2",
      "text" : "The main difference between the proofs of Theorems 1 and 2 is that the former utilizes the result of Lemma 1 so as to restrict the analysis of the general BDOP class to the case σ ≤ δ logn/n, while the latter utilizes Lemma 2 to restrict the analysis of BitMatchingD problem to the case σ ≤ δ logn/n, where δ is an arbitrary positive constant. Here we only provide a unified proof for the sake of brevity. As mentioned in Section 3.1, the proof contains the analysis related to Propositions B1.1, B1.2, B1.3, B1.4, and B1.5. Here we study the above propositions one after another.\nTheorems 1 and 2 are about the (1 + 1) EA which generates a unique offspring individual at each generation. For the sake of simplicity, in the proof, Pm(n, t, 1), which represents the concrete mutation rate employed by the mutation of the parent individual at the tth generation, is written as Pm(n, t) for short, where we omit the offspring index. Similarly, the offspring index will also be omitted when applying the transition lemmas.\nAnalysis of Proposition B1.1. Since (A2 ∪A1 ∪ F2 ∪B2 ∪ B1 ∪L2) ⊂ (F1 ∪L1), the proof of this sketch is the same to “Analysis of Proposition A1.1” part in the proof of Lemmas 1 and 2.\nAnalysis of Proposition B1.2. Let U1 = U1(n, σ) = δγ 1/7 logn. According to Chernoff bounds, we know that with an overwhelming probability there are at most U1 flipped bits among the total n bits after the DOP change (which implies that the number of matching bits can decrease or increase by at most U1 after DOP change):\nP\n(\n|N (P ) t −Nt−1| ≥ U1 | Pm(n, t) <\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n<\n(\ne\nγ1/7\n)U1\n=\n(\ne\nω(1)\n)ω(logn)\n≺ 1\nSuperPoly(n) , (24)\nwhere t ∈ N+ is the generation index. On the other hand, concerning the number of matching bits of the offspring\nat the tth generation, we obtain another inequality by Chernoff bounds:\nP\n(\n|N (O) t −Nt−1| ≥ U1 | Pm(n, t) <\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n<\n(\ne\nΘ(γ1/14)\n)U1\n=\n(\ne\nω(1)\n)ω(logn)\n≺ 1\nSuperPoly(n) ,\nwhere we utilize the fact that the composite bitwise mapping rate (including both DOP change and mutation) within the tth generation, denoted by r(n, t), satisfies that r(n, t) = (1 − σ)Pm(n, t) + (1 − Pm(n, t))σ = Pm(n, t) + σ − 2Pm(n, t)σ < 2γ 1/14 logn/n (since Pm(n, t) < γ 1/14 logn/n and σ < δ log n/n < γ1/14 logn/n holds).\nNoting that Nt ∈ {N (P ) t , N (O) t }, we obtain the following result by combining the above two inequalities together:\nP\n(\n|Nt −Nt−1| ≥ U1 | Pm(n, t) < γ1/14 logn\nn , σ <\nδ logn\nn\n)\n< P\n(\n|N (P ) t −Nt−1| ≥ U1 | Pm(n, t) <\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n+P\n(\n|N (O) t −Nt−1| ≥ U1 | Pm(n, t) <\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≺ 1\nSuperPoly(n) .\nConsequently, we have\nP\n(\n|Nt −Nt−1| < U1 | Pm(n, t) < γ1/14 log n\nn , σ <\nδ logn\nn\n)\n≻ 1− 1\nSuperPoly(n) ,\nthus we have proven Proposition B1.2. Analysis of Proposition B1.3. Let us consider Proposition B1.3a first. Proposition B1.1 tells us that N0 /∈ A2 ∪ A1 ∪ F2 ∪ B2 ∪ B1 ∪ L2 holds with an overwhelming probability. To arrive at A2 ∪ A1 ∪ F2 at some generation (e.g., the tth generation), the EA has two choices when deciding the mutation rate of the tth generation:\n1. Small Mutation Rate (SMR): Pm(n, t) < γ 1/14 logn/n;\n2. Large Mutation Rate (LMR): Pm(n, t) ≥ γ 1/14 logn/n.\nLet us investigate the case of reaching A2∪A1∪F2 by adopting SMR from F1\\(A2∪A1∪F2), O1 and L1\\L2. Since in Proposition B1.2 we have proven that SMR can only provide relatively smaller increment U1 for the number of\nmatching bits with an overwhelming probability, we know that the only region that is possible4 to reach A2∪A1∪F2 by adopting SMR, is a subset of F1 \\ (A2 ∪ A1 ∪ F2).\nAccording to Proposition B1.2, once SMR is used, the number of matching bits can increase by at most U1 = δγ1/7 log n with an overwhelming probability. Given the condition that Nt−1 ∈ F1\\(A2∪A1∪F2), the fact G = ω(U1) (with respect to the problem size n) implies that even the maximal one-generation increment U1 is not valid to jump over the interval A2 with an overwhelming probability. Thus, in this case, the EA must reach some intermediate point belonging to A2 first (otherwise the first hitting time to F2 has already been proven to be super-polynomial, which is the final conclusion of the theorem). In other words, we have proven that one way to reach F2 is to reach A2 first.\nNow we consider the case in which the EA reaches A2 ∪ A1 ∪ F2 by adopting LMR. Here two subcases must be considered further. In the first subcases, the offspring resulted from the LMR is not preserved by the selection operator of the EA, instead, the parent is preserved by the selection operator. This subcase can be viewed as adopting SMR with the value of 0 (which will not mutate the parent at all), and thus can be included in the analysis of the last two paragraphs.\nIn the second subcases, the offspring resulted from the LMR is preserved by the selection operator of the EA. As we have shown in the third sketch in Fig. 3, we want to prove that L2 is the only region that is possible to reach A2 ∪ A1 ∪ F2 by one-generation transition with LMR (and thus L2 is the only region that can reach F2 in one generation adopting LMR). As it is shown in the third sketch in Fig. 3, to prove the above proposition, there are three intervals for us to exclude (prove to be “unlikely”): L1 \\ L2,O1 and F1 \\ (A2 ∪ A1 ∪ F2). According to Lemma 8, if Nt−1 ∈ O1 ∪ (F1 \\ (A2 ∪A1 ∪ F2)) = (n/ log 2 n, n− 3G) and Pm(n, t) ≥ γ 1/14 logn/n, the probability of N (O) t ∈ A2 ∪ A1 ∪ F2 is super-polynomially close to 0. In other words, to prove Proposition B1.3a, we only need to concern the case in which Nt−1 ∈ L1 \\ L2 = (4G,n/ log2 n] and Pm(n, t) ≥ γ 1/14 logn/n hold. Given the above two conditions, below we prove that the probability of N (O) t ∈ A2 ∪ A1 ∪ F2 = [n− 3G,n] is super-polynomially close to 0. Given an arbitrary constant h ∈ (0, 1), − if γ1/14 log n/n ≤ Pm(n, t) ≤ 1 − h holds. Given the conditions that σ < δ logn/n and γ\n1/14 logn/n ≤ Pm(n, t) ≤ 1− h, by applying Lemma 11 we obtain:\nr(n, t) = ω(logn/n),\nr(n, t) ≤ max {1\n2 , 1− h\n}\n.\nBy above inequalities, we estimate the probability that in one generation the EA finds number of matching bits N (O) t = j ∈ A2 ∪ A1 ∪ F2:\nP\n(\nN (O) t = j | Nt−1 = i ∈ L1 \\ L2, j ∈ A2 ∪ A1 ∪ F2,\nγ1/14 log n\nn ≤ Pm(n, t) ≤ 1− h\n)\n=\nmin{i,n−j} ∑\nk=0\n(\nn− i\nj − i+ k\n)(\ni\nk\n)\nr(n, t)j−i+2k(1− r(n, t))n−(j−i+2k)\n< r(n, t)j−i min{i,n−j} ∑\nk=0\n(\nn− i\nn− j − k\n)(\ni\nk\n)\n=\n(\nn\nn− j\n)\nr(n, t)j−i < nn−j max {1\n2 , 1− h\n}j−i\n(by Lemma 6 in Appendix)\n= no ( n log n ) max {1\n2 , 1− h\n}n−o(n) = 2o(n) max {1\n2 , 1− h\n}n−o(n) ≺ 1\nSuperPoly(n) ,\nwhich is a super-polynomially small probability. − if Pm(n, t) > 1− h holds (i.e., P(Pm(n, t) > 1− h) = 1). On one hand, given the condition that σ = ω(logn/n)\nand Pm(n, t) > 1− h, by applying Lemma 11 we obtain:\nr(n, t) > min\n{\n1 2 ,max { σ, Pm }\n}\n> min\n{\n1 2 ,max { σ, 1− h }\n}\n≥ min {1\n2 , 1− h\n}\n.\nOn the other hand, we must note the fact called symmetrical bitwise mapping (Fig. 4): given the condition that the number of matching bits Nt−1 equals i and the composite bitwise mapping rate r(n, t), the consequence of the bitwise mapping is equivalent to that of the case in which Nt−1 equals n− i and the composite bitwise mapping rate\n4Here “possible” means that the event is with at least a polynomially large probability (There exists a positive polynomial function of the problem size n, such that the probability is no smaller than the reciprocal of the polynomial function).\nequals 1− r(n, t). Formally, we have\n1− r(n, t) < 1−min {1\n2 , 1− h\n} = max {1\n2 , h\n}\n.\nNoting the fact described above, the following equation holds in response to the so-called symmetrical bitwise mapping:\nP\n(\nN (O) t = j | Nt−1 = i, i ∈ L1 \\ L2, j ∈ A2 ∪ A1 ∪ F2, r(n, t)\n)\n= P\n(\nN (O) t = j | N ∗ t−1 = n− i, i ∈ L1 \\ L2, j ∈ A2 ∪ A1 ∪ F2, r ∗(n, t) < 1− r(n, t)\n)\n,\nwhere we use r∗(n, t) to represent the notional composite bitwise mapping rate with the value of 1 − r(n, t), N∗t−1 represent the notional number of matching bits (found by the EA) at the end of the (t− 1)th generation.\nAs shown in Fig. 4, we only need to prove that the probability of reaching A2∪A1∪F2 is super-polynomially close to 0, given the conditions that the notional number of matching bitsN∗t−1 belongs to (L1\\L2)\n∗ = (n−n/ log2 n, n−4G) and the notional composite bitwise mapping rate r∗(n, t) < max{1/2, h}. According to the value of r∗(n, t), there are two situations.\nIn the first situation, r∗(n, t) = O(log n/n) holds. According to Chernoff bounds, we know that with an overwhelming probability there are at most γ1/14 logn flipped bits among the total n bits:\nP\n(\n|N (O) t −N ∗ t−1| < γ\n1/14 logn | r∗(n, t) = O ( log n\nn\n)\n)\n> 1−\n(\ne\nΩ(γ1/14)\n)γ1/14 logn\n≻ 1− 1\nSuperPoly(n) .\nConsequently, with an overwhelming probability, the number of matching bits will decrease or increase by at most γ1/14 logn after the overall bitwise mapping (including DOP change and mutation). Given the conditions that N∗t−1 = n − i and i ∈ L1 \\ L2, the above upper bound implies that N (O) t < n − 4G + γ 1/14 logn < n − 3G and N (O) t /∈ L2 hold with an overwhelming probability. In other words,\nP\n(\nN (O) t = j | N ∗ t−1 = n− i, i ∈ L1 \\ L2, j ∈ A2 ∪ A1 ∪ F2, r\n∗(n, t) = O ( logn\nn\n)\n)\n≺ 1\nSuperPoly(n) .\nIn the second situation, r∗(n, t) = ω(logn/n) holds. This case can be viewed as imposing a mutation with LMR r∗(n, t) = ω(logn/n) to an individual with its number of matching bits belonging to [n−n/ log2 n, n−4G). According to Lemma 8.3, we have:\nP\n(\nN (O) t = j | N ∗ t−1 = n− i, i ∈ L1 \\ L2, j ∈ A2 ∪A1 ∪ F2, r\n∗(n, t) = ω ( logn\nn\n)\n)\n≺ 1\nSuperPoly(n) .\nSince there is no essential difference between the proofs related to r∗(n, t) and r(n, t), we will not provide the proof here for the sake of brevity. For details, one can refer to the proof of Lemma 7.3 in the appendix.\nCombining the above two situations of r∗(n, t) together, we obtain that:\nP\n(\nN (O) t = j | Nt−1 = i, i ∈ L1 \\ L2, j ∈ A2 ∪ A1 ∪ F2, Pm(n, t) > 1− h, σ <\nδ logn\nn\n)\n= P\n(\nN (O) t = j | Nt−1 = i, i ∈ L1 \\ L2, j ∈ A2 ∪ A1 ∪ F2, r(n, t) > max\n{1\n2 , 1− h\n}\n)\n(25)\n= P\n(\nN (O) t = j | N ∗ t−1 = n− i, i ∈ L1 \\ L2, j ∈ A2 ∪ A1 ∪ F2, r\n∗(n, t) < max {1\n2 , h\n}\n)\n≺ 1\nSuperPoly(n) ,\nwhere we utilize P(Pm(n, t) > 1− h, σ < δ logn/n) = 1 (Pm(n, t) > 1− h and σ < δ logn/n are preconditions of the analysis here) to obtain Eq. 25.\nBy applying total probability theorem [36], we further combine the cases γ1/14 logn/n ≤ Pm(n, t) ≤ 1 − h and Pm(n, t) > 1− h together. As a result, we obtain\nP\n(\nN (O) t = j | Nt−1 ∈ L1 \\ L2, j ∈ A2 ∪ A1 ∪ F2, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≺ 1\nSuperPoly(n) .\nTill now, the cases of SMR and LMR have been analyzed rigorously, and we know that the probability of reaching F2 from (4G,n− 3G) by any one-generation transition is super-polynomially small. Recall that we have also proven that the EA must reach A2 before reaching A1 ∪ F2 if SMR is used in the one-generation transition that reaches A2 ∪ A1 ∪ F2, we have proven Proposition B1.3a.\nThe proof of Proposition B1.3b is similar to that of Proposition B1.3a. The major difference between the ideas of the two proofs is that, in Proposition B1.3a the probability of reaching A2 ∪A1 ∪F2 from L2 (given the condition5 that σ < δ logn/n) by one-generation transition is not super-polynomially close to 0, while in Proposition B1.3b the probability of reaching L2 from A2∪A1 (given the condition that σ < δ logn/n) by one-generation transition (including the shift of optimum of BDOP and the mutation for the solution) is super-polynomially close to 0. The brief proofs of the latter proposition, conditional on the general BDOP class (for proving Theorem 1) and the BitMatchingD problem (for proving Theorem 2), are presented respectively.\n(For general BDOP class) Noting that the shifting rate σ ≤ δ logn/n, we estimate the following probability by Chernoff bound:\nP\n(\n|N (P ) t −Nt−1| ≥ U1 | σ <\nδ logn\nn\n)\n<\n(\ne\nγ1/7\n)U1\n=\n(\ne\nω(1)\n)ω(logn)\n≺ 1\nSuperPoly(n) .\nNoting that the range between L2 and A2 ∪ A1 is much larger than U1 (i.e., n − 7G > U1), we know that, given the condition that σ < δ logn/n, the probability of reaching L2 from A2 ∪ A1 by the parent of the tth generation is super-polynomially close to 0. Formally,\nP\n(\nN (P ) t ∈ L2 | Nt−1 ∈ A2 ∪ A1, σ <\nδ logn\nn\n)\n≺ 1\nSuperPoly(n) . (26)\nMeanwhile, we estimate the probability that N (O) t ∈ L2, conditional on Nt−1 ∈ A2∪A1 and r(n, t) ≤ 1− 1/ logn, where r(n, t) ≤ 1 − 1/ logn is derived from σ ≤ δ logn/n and Pm(n, t) ≤ 1 − 1/ logn (a condition of Theorem 1) by Lemma 11. Let L−t be the number of flipped matching bits after the DOP change and the mutation at the t th\n5Since Lemma 2 has already discuss the case of σ = ω(log n/n), we only need to consider the case of σ < δ logn/n here.\ngeneration, we have\nP\n(\nN (O) t ∈ L2 | Nt−1 ∈ A2 ∪ A1, r(n, t) ≤ 1−\n1\nlogn\n)\n= P\n(\nN (O) t ≤ 4G | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤ 1−\n1\nlog n\n)\n≤ P\n(\nn− ( L−t + (n−Nt−1) ) ≤ 4G | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤ 1− 1\nlogn\n)\n= P\n(\nL−t ≥ Nt−1 − 4G | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤ 1− 1\nlogn\n)\n≤ P\n(\nL−t ≥ n− 7G | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤ 1− 1\nlogn\n)\nOn one hand, if r(n, t) > 14e , then we further have\nP\n(\nN (O) t ∈ L2 | Nt−1 ∈ A2 ∪ A1,\n1\n4e < r(n, t) ≤ 1−\n1\nlog n\n)\n≤ P\n(\nL−t ≥ (1 + ρ1)n ( 1− 1\nlogn\n)\n> (1 + ρ1)Ê[L − t ] | Nt−1 ∈ [n− 3G,n−G),\n1\n4e < r(n, t) ≤ 1−\n1\nlogn\n)\n< P\n(\nL−t > (1 + ρ1)Ê[L − t ] | Nt−1 ∈ [n− 3G,n−G),\n1\n4e < r(n, t) ≤ 1−\n1\nlogn\n)\n< e−Ê[L − t ]ρ 2 1/4 ≺\n1\nSuperPoly(n) , (27)\nwhere ρ1 = (n− 7G)/(n−n/ logn)− 1 = Θ(1/ logn) (since G = γ 4/7 logn and γ ≤ n/ logn), Ê[L−t ] = E[L − t | Nt−1 ∈ [n− 3G,n−G), 1/4e < r(n, t) ≤ 1− 1/ logn] = Θ(n), and the last two inequalities is obtained by Chernoff bound. On the other hand, let us consider the case r(n, t) ≤ 14e . By Chernoff bound, we have\nP\n(\nN (O) t ∈ L2 | Nt−1 ∈ A2 ∪A1, r(n, t) ≤\n1\n4e\n)\n= P\n(\nL−t ≥ n− 7G = (1 + ρ2)Ē[L − t ] | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤\n1\n4e\n)\n< ( e\n1 + ρ2\n)(1+ρ2)Ē[L − t ]\n= ( e\n1 + ρ2\n)n−7G\n≺ 1\nSuperPoly(n) , (28)\nwhere Ē[L−t ] = E[L − t | Nt−1 ∈ [n−3G,n−G), r(n, t) ≤ 1/4e] ≤ n/4e and ρ2 = (n−7G)/Ē[L − t ]−1 > (n/2)/(n/4e)− 1 ≥ 2e− 1 (since G = γ4/7 logn and γ ≤ n/ logn). Combining Eqs. 27 and 28 together, we obtain\nP\n(\nN (O) t ∈ L2 | Nt−1 ∈ A2 ∪ A1, r(n, t) ≤ 1−\n1\nlogn\n)\n≺ 1\nSuperPoly(n) .\nCombining the above inequality with Eq. 26, we know that the probability of reaching L2 from A2 ∪ A1 by onegeneration transition is super-polynomially close to 0. The rest part of the proof of Proposition B1.3b is similar to that of Proposition B1.3a.\n(For BitMatchingD) According to Chernoff bounds, with an overwhelming probability there are at most U1 flipped bits among the total n bits after the DOP change (which implies that the number of matching bits can decrease or increase by at most U1 after DOP change):\nP\n(\n|N (P ) t −Nt−1| ≥ U1 | σ <\nδ logn\nn\n)\n<\n(\ne\nγ1/7\n)U1\n=\n(\ne\nω(1)\n)ω(logn)\n≺ 1\nSuperPoly(n) .\nNoting that the range between L2 and A2 ∪ A1 is much larger than U1 (n − 7G > U1), we know that, given the condition that σ < δ logn/n, the probability of reaching L2 from A2 ∪ A1 by the help of the DOP change at the beginning of the tth generation is super-polynomially close to 0.\nMoreover, since the selection operator of the EA always preserves the better individual between the parent\nand offspring (For BitMatchingD , Nt = max{N (P ) t , N (O) t } ≥ N (P ) t holds according to Eqs. 2 and 3), the above\ninequality also implies that Nt /∈ L2 holds with an overwhelming probability. In other words, the probability of reaching L2 from A2 ∪ A1 by one-generation transition is super-polynomially close to 0. The rest part of the proof of Step 2.3b is similar to that of Step 2.3a.\nAnalysis of Proposition B1.4. Let U2 = γ 1/7. According to the condition Nt−1 ∈ A2 ∪ A1 ∪ F2, there are at most 3G non-matching bits at the end of the (t − 1)th generation. On the other hand, since the number of flipped non-matching bits is always no smaller than the final increment of the number of matching bits (after DOP change or/and mutation), we know that, to increase the number of matching bits by at least U2, the number of flipped non-matching bits must be larger than U2. Hence, concerning the number of matching bits of the parent after the DOP change at the tth generation, we obtain\nP\n(\nN (P ) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <\nδ logn\nn\n)\n<\n(\n3G\nU2\n)\nσU2(1 − σ)n−U2 <\n(\n3G\nγ1/7\n)\nσγ 1/7\n< (3G)γ 1/7 σγ 1/7 < (3δG logn\nn\n)γ1/7 = (3δγ4/7 log2 n\nn\n)γ1/7\n(29)\n< (3δn4/7 log2 n\nn\n)γ1/7\n≺ 1\nSuperPoly(n) , (30)\nwhere we obtain Eqs. 29 and 30 by applying Eqs. 9 and 4 respectively. Next, concerning the number of matching bits of the offspring at the tth generation, we must consider two different cases. The EA adopts SMR at the tth generation in the first case while it adopts LMR at the tth generation in the second case.\nFor the former case, we estimate the following inequality by noting the condition Nt−1 ∈ A2 ∪ A1 ∪ F2:\nP\n(\nN (O) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, Pm(n, t) <\nγ1/14 logn\nn , σ <\nδ log n\nn\n)\n<\n(\n3G\nU2\n)\nr(n, t)U2 ( 1− r(n, t) )n−U2 <\n(\n3G\nγ1/7\n)\nr(n, t)γ 1/7\n< (3G)γ 1/7 r(n, t)γ 1/7 < (6Gγ1/14 logn\nn\n)γ1/7 = (6γ9/14 log2 n\nn\n)γ1/7\n< (6n9/14 log2 n\nn\n)γ1/7\n≺ 1\nSuperPoly(n) , (31)\nwhere we utilize the fact that the composite bitwise mapping rate r(n, t) satisfies that r(n, t) = (1 − σ)Pm(n, t) + (1 − Pm(n, t))σ = Pm(n, t) + σ − 2Pm(n, t)σ < 2γ 1/14 logn/n (since Pm(n, t) < γ 1/14 logn/n and σ < δ logn/n < γ1/14 logn/n holds). On the other hand, let us investigate the latter case in which the EA adopts LMR at the tth generation. According to the condition Nt−1 ∈ A2 ∪A1 ∪ F2, Nt−1 = n− o(n) holds. By applying Lemma 8.3, we obtain\nP\n(\nN (O) t > Nt−1 | Nt−1 ∈ A2 ∪ A1 ∪ F2, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≺ 1\nSuperPoly(n) . (32)\nCombining Eqs. 31 and 32 together we have:\nP\n(\nN (O) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <\nδ logn\nn\n)\n≺ 1\nSuperPoly(n) , (33)\nNoting that Nt ∈ {N (P ) t , N (O) t }, we obtain the following result by combining Eqs. 30 with the above inequality:\nP\n(\nNt −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ < δ log n\nn\n)\n< P\n(\nN (P ) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <\nδ logn\nn\n)\n+P\n(\nN (O) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <\nδ logn\nn\n)\n≺ 1\nSuperPoly(n) .\nConsequently,\nP\n(\nNt −Nt−1 < U2 | Nt−1 ∈ A2 ∪A1 ∪ F2, σ < δ logn\nn\n)\n≻ 1− 1\nSuperPoly(n) . (34)\nHence, we know that once the EA is in A2 ∪ A1 ∪ F2, the number of matching bits can only increase by at most U2 in one generation with an overwhelming probability no matter which mutation rate the EA adopts.\nAnalysis of Proposition B1.5. As we have shown in Proposition B1.3, so far we have not excluded the possibility6 of the following two events:\n1. The EA reaches F2 via B2 and then via L2 by multiple-generation transition (it is possible that the EA reaches F2 from L2 by large enough LMR, e.g., the mutation rate 1).\n2. The EA reaches F2 via A2 (without reaching some intermediate points in L2) by multiple-generation transition.\nHence, the proof of Proposition B1.5 contains two parts. First, we need to prove that, if the EA has already reached B2, then it cannot travel through B1 and reach L2 with an overwhelming probability. Second, we need to prove that, if the EA has already reached A2, then it cannot travel through A1 and reach F2 with an overwhelming probability. If the above results have been proven, then we can only hope some events with super-polynomially small probability (e.g., the EA reaches F2 from A2 by one-generation transition) to happen, which will lead to a super-polynomial first hitting time with an overwhelming probability.\nSince the ideas of two parts are quite similar, we only provide the details of the second part (which will lead to the final conclusion of the theorem) for the sake of brevity. Assume that we have proven the first part, that is, if the EA has already reached B2, then it cannot travel through B1 and finally reach L2 with an overwhelming probability. According to Proposition B1.4, to reach F2, the EA must reach L2 or A2 first, we know that the only choice to reach F2 is via A2. To reach F2 via A2, the EA must travel through A1. The reason is given by Proposition B1.4 and the fact that A1 is with the length of G > U2.\nNext, we will provide the proof of for the aforementioned proposition: if the EA has already reached A2, then it cannot travel through A1 and reach F2 with an overwhelming probability. (as we have mentioned, by the same technique we can prove the similar result for L2). For ∀t ∈ N+, given the conditions that Nt ∈ A2 ∪ A1 and Nt−1 = i, we let the probabilities of decreasing and increasing the number of matching bits be p\n−(n, i, t) and p+(n, i, t), respectively, i.e.,\np−(n, i, t) = P\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ < δ logn\nn\n)\n,\np+(n, i, t) = P\n(\nNt > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ < δ logn\nn\n)\n.\nNext we prove that ∀t ∈ N+ that satisfies Nt−1 = i ∈ A2 ∪A1, the following two inequalities holds:\np−(n, i, t) > p−i (n) = Θ (γ logn\nn\n)\n, (35)\np+(n, i, t) < p+i (n) = Θ (γ4/7 logn\nn\n)\n, (36)\nwhere p−i = p − i (n) is a general lower bound of p −(n, i, t), and p+i = p + i (n) is a general upper bound of p −(n, i, t). To prove the bound for p−(n, i, t), we need to consider two cases. In the first case, the EA adopts SMR at the tth generation; In the second case, the EA adopts LMR at the tth generation. Concerning the first case, we estimate the following probability:\nP\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 log n\nn , σ <\nδ logn\nn\n)\n≥\n(\ni ∑\nk=1\n(\ni\nk\n)\nσk(1− σ)i−k ( 1− Pm(n, t) k )\n)\n(1 − σ)n−i ( 1− Pm(n, t) )n−i\n>\n(\n( 1− Pm(n, t) )\ni ∑\nk=1\n(\ni\nk\n)\nσk(1− σ)i−k\n)\n(1− σ)n−i ( 1− Pm(n, t) )n−i\n= ( 1− (1 − σ)i ) (1− σ)n−i ( 1− Pm(n, t) )n−i+1 ,\n6Here “excluding the possibility” of an event is referred to proving that the probability of the event is super-polynomially close to 0.\nwhere we consider the case in which all the non-matching bits are not flipped during the DOP change and mutation (this event is with the probability of (1− σ)n−i(1−Pm(n, t))\nn−i) while some of the matching bits are flipped by the DOP change and at least one of these flipped matching bits is not flipped again by mutation (this event is with the probability of ∑i\nk=1\n(\ni k\n) σk(1− σ)i−k ( 1− Pm(n, t) k ) . According to the value of σ, let us consider two subcases:\n1. If σ ≤ 1n , according to Eq. 9, we know γ = σn 2/ logn. Since n− i+ 1 < 3G+ 1 < n/(γ1/7 logn) holds, we have\nP\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 logn\nn , σ ≤\n1\nn\n)\n≥ ( 1− (1− σ)i ) (1 − σ)n ( 1− Pm(n, t) )n−i+1 ≥ c1 · iσ ( 1− γ1/14 logn\nn\n) n\nγ1/7 log n ≥ c1 2 · γ logn n ,\nwhere c1 is a positive constant.\n2. If 1n < σ < δ logn/n, according to Eq. 9, we know γ = n/ logn. Since 3G < n/(γ 1/7 log n) and i > n− 3G > n/2\nhold, we have\nP\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 logn n , 1 n < σ < δ logn n\n)\n>\n(\n1− ( 1− 1\nn\n)i )\n(1− σ)3G ( 1− Pm(n, t) )3G > c2 · (1− σ) 3G ( 1− Pm(n, t) )3G\n> c2 · ( 1− δ logn\nn\n) n\nγ1/7 log n\n( 1− γ1/14 log n\nn\n) n\nγ1/7 log n = c3 · γ logn\nn ,\nwhere c2 and c3 are positive constants.\nCombining the above two cases together, we know that\nP\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 logn\nn , σ <\nδ logn\nn\n)\n> max {c1 2 , c3 } · γ logn n (37)\nholds. We now consider the second case in which the EA adopts LMR at the tth generation. We estimate the following probability:\nP\n(\nN (P ) t < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <\nδ logn\nn\n)\n≥\ni ∑\nk=1\n(\ni\nk\n)\nσk(1− σ)n−k > ( 1− (1 − σ)i ) (1− σ)n−i.\nMoreover, by applying Lemma 8.3, we have\nP\n(\nN (O) t ≥ Nt−1 | Nt−1 ∈ A2 ∪ A1, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≺ 1\nSuperPoly(n) .\nCombining the above two inequalities together and noting the fact that the selection operator always preserve the better one between the parent and offspring, we have\nP\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn\nn , σ <\nδ logn\nn\n)\n≥ P\n(\nN (P ) t < Nt−1, N (O) t < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n= P\n(\nN (P ) t < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n−P\n(\nN (P ) t < Nt−1, N (O) t ≥ Nt−1 | Nt−1 = i ∈ A2 ∪A1, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n> P\n(\nN (P ) t < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <\nδ logn\nn\n)\n−P\n(\nN (O) t ≥ Nt−1 | Nt−1 ∈ A2 ∪A1, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ log n\nn\n)\n> ( 1− (1 − σ)i ) (1− σ)n−i − 1\nSuperPoly(n) >\n1 2 · ( 1− (1− σ)i ) (1− σ)n−i.\nAccording to the value of σ, let us further consider two subcases:\n1. If σ ≤ 1n , according to Eq. 9, we know γ = σn 2/ logn. We have\nP\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn\nn , σ ≤\n1\nn\n)\n> 1\n2 · ( 1− (1 − σ)i )\n(1− σ)n−i > 1\n2 · ( 1− (1− σ)i ) (1 − σ)n ≥ c4 · iσ > c4 2 · γ logn n ,\nwhere c4 is a positive constant.\n2. If 1n < σ < δ logn/n, according to Eq. 9, we know γ = n/ logn. Since 3G < n/(δ logn) and i > n − 3G > n/2 hold, we have\nP\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn n , 1 n < σ < δ logn n\n)\n> 1\n2 ·\n(\n1− ( 1− 1\nn\n)i )\n(1− σ)3G > c5 · (1 − σ) 3G > c5 ·\n( 1− δ logn\nn\n) n\nδ log n = c6 · γ logn\nn ,\nwhere c5 and c6 are positive constants.\nCombining the above two cases together, we know that\nP\n(\nNt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn\nn , σ <\nδ logn\nn\n)\n> max {c4 2 , c6 } · γ logn n\nholds. Combining Eq. 37 with the above inequality, we have proven that\np−(n, i, t) > p−i (n) = Θ (γ logn\nn\n)\n.\nNow let us prove the upper bound of p+(n, i, t). To increase the number of matching bits by DOP change and/or mutation, the number of flipped non-matching bits must be larger than the number of flipped matching bits after DOP change and/or mutation. According to Lemma 5, concerning the relation between N (P ) t and Nt−1, we have\nP\n(\nN (P ) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <\nδ logn\nn\n)\n≤ 3G\nn− 3G ≤\n3G n/2 ≤\n6γ4/7 logn\nn . (38)\nFor the offspring at the tth generation, we still need to consider two cases. In the first case, the EA adopts SMR at the tth generation; In the second case, the EA adopts LMR at the tth generation.\nWe now consider the first case in which the EA adopts SMR. By applying Lemma 5 we have\nP\n(\nN (O) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) <\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≤ 3G\nn− 3G ≤\n3G n/2 ≤\n6γ4/7 logn\nn ,(39)\nwhere we utilize the fact that the overall impact of DOP change and mutation to the offspring individual at the tth generation can be represented by a overall mapping with bitwise mapping rate r(n, t) = (1 − σ)Pm(n, t) + (1 − Pm(n, t))σ = Pm(n, t) + σ − 2Pm(n, t)σ.\nNoting that Nt ∈ {N (P ) t , N (O) t }, we obtain the following result by combining Eqs. 38 and 39 together:\nP\n(\nNt > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 logn\nn , σ <\nδ logn\nn\n)\n< P\n(\nN (P ) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <\nδ logn\nn\n)\n+P\n(\nN (O) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) <\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≤ 12γ4/7 logn\nn . (40)\nOn the other hand, we consider the case in which LMR is adopted by the EA. By applying Lemma 8.3, we know\nP\n(\nN (O) t > Nt−1 | Nt−1 ∈ A2 ∪ A1 ∪ F2, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≺ 1\nSuperPoly(n) .\nIn other words, if the LMR is used, the number of matching bits found by the offspring at the tth generation will not be larger than Nt−1. Similar to Eq. 40, we obtain\nP\n(\nNt > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn\nn , σ <\nδ logn\nn\n)\n< P\n(\nN (P ) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <\nδ logn\nn\n)\n+P\n(\nN (O) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥\nγ1/14 logn\nn , σ <\nδ logn\nn\n)\n≤ 6γ4/7 logn\nn +\n1\nSuperPoly(n) <\n12γ4/7 logn\nn .\nCombining Eq. 40 with the above inequality, we obtain\nP\n(\nNt > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ < δ logn\nn\n)\n< 12γ4/7 logn\nn .\nIn other words, we have proven Eq. 36. Droste utilized the idea of “effective mutation” to prove Theorem 2 of [17]. Intuitively speaking, this technique estimates the lower bound for the number of effective mutation (which can be interpreted as the number of generations in which the number of matching bits changes) for reaching the target, and it also estimates the upper bound for effective mutation that can be provided by the EA. If the former one is significantly smaller than the later one, then the EA cannot reach the target. Following this intuitive idea, we then provide the formal proof of the final conclusion of the theorem. By Eqs. 35 and 36, we obtain the upper bound of the probability of Nt > Nt−1, conditional on the event that Nt 6= Nt−1:\nP\n(\nNt > Nt−1 | Nt 6= Nt−1 = i ∈ A2 ∪ A1, σ < δ logn\nn\n)\n= p+(n, i, t)\np+(n, i, t) + p−(n, i, t) < p+i p+i + p − i = Θ ( γ4/7 logn/n ) Θ ( γ4/7 logn/n ) +Θ ( γ logn/n ) = Θ ( γ−3/7 ) , (41)\nwhere Θ is referred to the asymptotic order of the problem size n. We now prove that the EA will spend super-polynomial number of generations to travel through A1 and reach F2. Let T be defined formally as follows:\nT = ∣ ∣\n∣\n{ t ∈ N+ | Nt−1 6= Nt, Nt−1 ∈ A2 ∪ A1, σ < δ logn\nn\n}∣\n∣ ∣. (42)\nRecall that in Proposition B1.3, we have proven that the EA has to travel through the whole A1 with length of G; In Proposition B1.4, we have proven that in one generation the number of matching bits cannot increase by more than U2 with an overwhelming probability. According to Propositions B1.3 and B1.4, to travel through A1 and reach F2, T is lower bounded by G/U2 = γ 3/7 logn with an overwhelming probability. Formally we have\nP ( T ≥ γ3/7 logn ) ≻ 1− 1\nSuperPoly(n) . (43)\nFurther, let T+ and T− be defined as follows:\nT+ = ∣ ∣\n∣\n{ t ∈ N+ | Nt−1 < Nt, Nt−1 ∈ A2 ∪ A1, σ < δ logn\nn\n}∣\n∣ ∣, (44)\nT− = ∣ ∣\n∣\n{ t ∈ N+ | Nt−1 > Nt, Nt−1 ∈ A2 ∪A1, σ < δ logn\nn\n}∣\n∣ ∣, (45)\nand the above definitions imply that T = T+ + T−.\nMoreover, by Proposition B1.4, given the condition that the EA is in A2 ∪ A1, in one generation the number of matching bits cannot increase by more than U2 with an overwhelming probability. Hence, among the T generations, the number of matching bits can increase by T+U2−T\n− at most with an overwhelming probability. To travel through A1, the following inequality must hold (a necessary condition):\nT+U2 − T − ≥ G.\nNoting that T = T+ + T−, it follows that\nT+ ≥ G+ T\nU2 + 1 .\nRecall the definitions of G, T and U2, we have\nG+ T U2 + 1 > d · T γ1/7\nwhere d is a positive constant. Combining the above two inequalities together, we know that the following condition must be satisfied so as to travel through A1:\nT+ > d · T\nγ1/7 . (46)\nNext, we only need to prove that the above condition cannot be satisfied with an overwhelming probability (thus the EA cannot travel through A1 with an overwhelming probability). It follows from Eqs. 41 and 42 that\nE[T+ | T ] = O\n(\nT\nγ3/7\n)\n,\nwhere O is referred to the asymptotic order of the problem size n. By Chernoff bounds, we estimate the probability of Eq. 46:\nP\n(\nT+ > d · T\nγ1/7 | T\n)\n<\n(\neγ1/7E[T+ | T ] dT\n)dT/γ1/7\n= O\n(\n1\nγ2/7\n)Θ(T/γ1/7)\n,\nwhere d is a positive constant. Meanwhile, Eq. 43 implies that\nP ( T ≥ γ3/7 logn ) ≻ 1− 1\nSuperPoly(n) .\nBy the total probability theorem [36], we obtain\nP\n(\nT+ > d · T\nγ1/7\n)\n≺ 1\nSuperPoly(n) .\nIn other words, the condition T+U2 − T − ≥ G does not hold with an overwhelming probability, which implies that the EA cannot travel through A1 and reach F2 with an overwhelming probability, given the condition that it has already reached A2. Consequently, we have proven that the EA cannot reach F2 by a polynomial first hitting time with an overwhelming probability. Formally, let τF2 be defined as follows\nτF1 = min { t ≥ 0; (N (P ) t ∈ F1) ∨ (N (O) t ∈ F1) } . (47)\nWe have proven that\nP(τF2 ≺ Poly(n)) ≺ 1\nSuperPoly(n) ,\nwhich leads to Theorems 1 and 2 according to the definition of F2 in Definition 10."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank Dr. Yang Yu for his constructive comments to this paper. This work is partially supported by Natural Science Foundation of China grants (No. 61033009, No. 61028009, No. 61003064, and No. U0835002), National S&T Major Project (Grant 2010ZX01036-001-002), and an Engineering and Physical Science Research Council grant in UK (No. EP/I010297/1)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "Mutation has traditionally been regarded as an important operator in evolutionary algorithms. In particular, there have been many experimental studies which showed the effectiveness of adapting mutation rates for various static optimization problems. Given the perceived effectiveness of adaptive and self-adaptive mutation for static optimization problems, there have been speculations that adaptive and self-adaptive mutation can benefit dynamic optimization problems even more since adaptation and self-adaptation are capable of following a dynamic environment. However, few theoretical results are available in analyzing rigorously evolutionary algorithms for dynamic optimization problems. It is unclear when adaptive and self-adaptive mutation rates are likely to be useful for evolutionary algorithms in solving dynamic optimization problems. This paper provides the first rigorous analysis of adaptive mutation and its impact on the computation times of evolutionary algorithms in solving certain dynamic optimization problems. More specifically, for both individual-based and population-based EAs, we have shown that any time-variable mutation rate scheme will not significantly outperform a fixed mutation rate on some dynamic optimization problem instances. The proofs also offer some insights into conditions under which any time-variable mutation scheme is unlikely to be useful and into the relationships between the problem characteristics and algorithmic features (e.g., different mutation schemes).",
    "creator" : "LaTeX with hyperref package"
  }
}