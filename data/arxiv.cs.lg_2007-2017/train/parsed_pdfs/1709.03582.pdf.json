{
  "name" : "1709.03582.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Art of singular vectors and universal adversarial perturbations",
    "authors" : [ "Valentin Khrulkov", "Ivan Oseledets" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "It has been shown recently (Moosavi-Dezfooli et al. 2016), that for many state-of-the-art deep neural networks (DNN) there exist universal adversarial perturbations - single vectors of a small norm which being added to images in a dataset with high probability lead to misclassifying these images. In this work we present an algorithm for constructing the universal adversarial perturbations as the so-called (p, q)-singular vectors of the Jacobian matrices of feature maps of a DNN. The (p, q)-singular vector is defined as\ny = argmax ‖Ay‖q ‖y‖p .\nFor p = q = 2 we obtain the standard singular vector. The universal adversarial perturbations are typically generated with a bound in the∞-norm, which motivates the usage of such singular vectors. To obtain the (p, q)-singular vectors we use a modification of the standard power method, which is adapted to arbitrary p-norms. The main contributions of our paper are\n• We propose an algorithm for generating the universal adversarial perturbation, using the generalized power method for computing the (p, q)-singular vectors of the Jacobian matrices of the features maps.\n• Our method is able to produce relatively good universal adversarial examples from a relatively small number of images from a dataset\nCopyright c© 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n• We investigate a correlation between the largest (p, q)singular value and the fooling rate of the generated adversarial examples; this suggests that this singular value can be used as a quantitative measure of the robustness of a given neural network and can be in principle incorporated as the regularizer for the DNN.\n• We analyze various properties of the computed adversarial perturbations such as generalization across networks and dependence of the fooling rate on the batch size used for construction of the perturbation."
    }, {
      "heading" : "2 Problem statement",
      "text" : "Suppose that we have a standard feed-forward DNN which takes a vector x as the input, and outputs a vector of probabilities p(x) for class labels. Our goal is given parameters q ≥ 1 and L > 0 produce a vector ε such that\nargmax p(x) 6= argmax p(x+ ε), ‖ε‖q = L, for as many x in a dataset as possible. Efficiency of the given universal adversarial perturbation ε for the dataset X of the size N is called the fooling rate and is defined as\n|{x ∈ X : argmax p(x) 6= argmax p(x+ ε)}| N .\nLet us denote the outputs of the i-th hidden layer of the network by fi(x). Then we have\nfi(x+ ε)− fi(x) ≈ Ji(x)ε, where\nJi(x) = ∂fi ∂x ∣∣∣∣ x ,\nThus, for any q\n‖fi(x+ ε)− fi(x)‖q ≈ ‖Ji(x)ε‖q, (1) and it seems reasonable that if we sufficiently perturb the output of some hidden layer, it will also change the predicted class label. Thus to construct the adversarial perturbation for an individual image x we need to solve\n‖Ji(x)ε‖q → max, ‖ε‖p = L, (2) and due to linearity of eq. (2) it is sufficient to solve it for ‖ε‖p = 1. A solution of eq. (2) is called the (p, q)-singular\nar X\niv :1\n70 9.\n03 58\n2v 1\n[ cs\n.C V\n] 1\n1 Se\np 20\n17\nvector of Ji(x) and its computation is a well-known problem (Boyd 1974; Bhaskara and Vijayaraghavan 2011). Even though in some cases e.g. p =∞, q =∞ and p = 1, q = 1, the exact solution of the problem eq. (2) is known (Trefethen and Bau III 1997), it is impossible to construct it in practice, due to extremely large sizes of matrices involved (e.g. 831744× 150528). However, iterative methods for approximating the solution exist and in the next section we present the Power Method algorithm originally developed by Boyd (Boyd 1974), and explain how we use it to construct the universal adversarial perturbations."
    }, {
      "heading" : "3 Generalized power method",
      "text" : "Suppose that for some linear mapAwe are given the matrixby-vector products of A and A>. Given parameter r ≥ 1 we also define a function ψr(x) = signx|x|r−1, which applies to vectors element-wise. As usual for r ≥ 1 we also define r′ such that 1r + 1 r′ = 1. Then, given some initial condition x, one can apply the following algorithm 1 to obtain the solution of eq. (2). In the case p = q = 2 it\nAlgorithm 1 Power method for generating (p, q)-singular vectors of a linear map A\n1: Inputs: initial condition x, matrix-by-vector product functions of A and A> 2: x← x‖x‖p . (p, q)-singular vector 3: s← ‖Ax‖q . (p, q)-singular value 4: while not converged do 5: Sx← ψp′(A>ψq(Ax)) 6: x← Sx‖Sx‖p 7: s← ‖Ax‖q 8: return x, s\nbecomes the familiar power method for obtaining the largest eigenvalue and eigenvector, applied to the matrix A>A. We seek universal adversarial perturbations. Let us introduce a new optimization problem∑\nxj∈X ‖Ji(xj)ε‖qq → max, ‖ε‖p = L, (3)\nwhere X denotes the dataset from which we sample images. A solution of eq. (3) uniformly perturbs the output of the i-th layer of the DNN, and thus can serve as the universal adversarial perturbation. Note that the problem eq. (3) is exactly equivalent to\n‖Jiε‖q → max, ‖ε‖p = L, where Ji is the matrix obtained via stacking Ji(xj) vertically for each xj ∈ X . To make this optimization problem tractable, we apply the same procedure to some randomly chosen subset of images Xb ⊂ X , obtaining∑\nxj∈Xb\n‖Ji(xj)ε‖qq → max, ‖ε‖p = L, (4)\nand hypothesize that the obtained solution will be a good approximate to the exact solution of eq. (3). We present this approach in more detail in the next section.\nStochastic power method Let us choose a batch of images Xb = {x1, x2 . . . xb} from the dataset and fix some layer of the DNN, giving the map fi(x). Denote sizes of x, fi(x) by n andm correspondingly. Then, using the notation from section 2 we can compute Ji(xj) ∈ Rm×n for each xj ∈ Xb. Let us now stack this Jacobian matrices vertically thus obtaining the matrix Ji(Xb) of size bm× n:\nJi(Xb) = Ji(x1)Ji(x2). . . Ji(xb)  . (5) Note that to compute the matrix-by-vector products of Ji(Xb) and J>i (Xb) it suffices to be able to efficiently compute the individual matrix-by-vector product of Ji(x) and J>i (x). We will present an algorithm for that in the next section and for now let us assume that these matrix-by-vector products are given. We can now apply algorithm 1 to the matrix Ji(Xb) obtaining Stochastic Power Method (SPM). Note that in algorithm 2 we could in principle change the\nAlgorithm 2 Stochastic Power Method for generating universal adversarial perturbations\n1: Inputs: batch of images Xb = {x1, x2, . . . xb}, fi(x) - fixed hidden layer of the DNN 2: for xj ∈ X do 3: Construct matrix-by-vector product functions of Ji(xj) and J>i (xj) 4: Construct matrix-by-vector product functions of Ji(Xb) and J>i (Xb) defined in eq. (5) 5: Run algorithm 1 with desired p and q 6: return ε . Universal Adversarial Perturbation\nbatch of images Xb between iterations of the power method to compute ”more general” singular vectors. However in our experiments we discovered that it almost does not affect fooling rate of the generated universal adversarial perturbation."
    }, {
      "heading" : "4 Efficient implementation of matrix-by-vector products",
      "text" : "Matrices involved in algorithm 2 for typical DNNs are too large to be formed explicitly. However, using automatic differentiation available in most deep learning packages it is possible to construct matrix-by-vector products which then are evaluated in a fraction of a second. Suppose that we are given an operation gradx[f ](x0) which computes the gradient of a scalar function f(x) with respect to the vector variable x at the point x0. Let fi(x) be some fixed layer of the DNN, such that x ∈ Rn and fi(x) ∈ Rm, thus Ji(x) ∈ Rm×n and for vectors v1 ∈ Rn, v2 ∈ Rm we would like to compute Ji(x)v1, J>i (x)v2 at some fixed point x. Our approach is based on the trick presented in (Townsend 2017) and is given in algorithm 3. For a given batch of images this algorithm has to be run only once and thus has O(1) complexity.\nAlgorithm 3 Constructing matrix-by-vector product functions\n1: Inputs :v1 ∈ Rn, v2 ∈ Rm - vectors to compute matrixby-vector products with, fi(x) - fixed hidden layer of the DNN 2: J>i (x)v2 ← gradv[〈v, fi(x)〉](v2) 3: g(v2)← 〈J>i (x)v2, v1〉 4: Ji(x)v1 ← gradv2 [g](0m) 5: return Ji(x)v1, J>i (x)v2"
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section we analyze various adversarial perturbations constructed using algorithm 2. For testing we used the ILSVRC 2012 validation dataset (Russakovsky et al. 2015) (50000 images). In our experiments we chose p = ∞, q = 10 and computed the (p, q) - singular vectors for various layers of VGG-16 and VGG-19 (Simonyan and Zisserman 2014) and ResNet50 (He et al. 2016). q = 10 was chosen to smoothen optimization problem and effectively serves as the replacement for q = ∞, for which we discovered that the highest fooling rates are achieved. Batch size in algorithm 2 was chosen to be 64 and we used the same 64 images to construct all the adversarial perturbations. Some of the computed singular vectors are presented in figs. 1a to 1c. We observe that computed singular vectors look visually appealing and contain interesting patterns. We also compute the (∞, 10)-singular values for all the layers of VGG-16, VGG-19 and ResNet50. Results are given in fig. 2. We can note that in general singular values of the layers of ResNet50 are much smaller in magnitude than those\nof the VGG nets. Convergence of algorithm 2 is analyzed in fig. 3. We see that relatively low number of iterations is required, and iterative method is much more efficient than explicit computation of the singular vector, which in the case of (∞,∞)-singular vectors takes O(nm) operations and is intractable for typical sizes of the input and of the hidden layers of the DNN."
    }, {
      "heading" : "6 Fooling rate and singular values",
      "text" : "As a next experiment we computed and compared the fooling rate of the perturbation by various computed singular vectors. We choose 1‖ε‖∞ = 10 – recall that this can be achieved just by multiplying the computed singular vector by the factor of 10. Results are given in tables 1 to 3. We see that using just 64 images allowed us to achieve more than 40% fooling rate on the dataset containing 50000 images for all the investigated networks. Similar fooling rates reported by (Moosavi-Dezfooli et al. 2016, Figure 6) required roughly 1000 images. As a next experiment we investigate dependence of the achieved fooling rate on the batch size used in algorithm 2. Some of the results are given in fig. 4. Surprisingly, increasing batch size does not significantly affect the fooling rate and by using as few as 16 images it is possible to construct the adversarial perturbations with 56% fooling rate. This might indicate that the singular vector constructed using Stochastic Power Method reasonably well approximates solution of the general optimization problem eq. (3). It appears that higher singular value of the layer does not\n1Pixels in the images from the dataset are normalized to be in [0, 255] range, so by choosing ‖ε‖∞ = 10 we make the adversarial perturbations quasi-imperceptible to human eye\nnecessarily indicate higher fooling rate of the corresponding singular vector. However as shown on fig. 2 singular values of various layers VGG-19 are in general larger than those of VGG-16, and of VGG-16 are in general larger than singular values of ResNet50, which is roughly in correspondence between the maximal fooling rates we obtained for these networks. Moreover, layers closer to the input of the DNN seem to produce better adversarial perturbations, than those closer to the end. Based on this observation we hypothesize that to defend the DNN against this kind of adversarial attack one can choose some subset I of the layers (preferably closer to the input) of the DNN and include the term\n∑ i∈I ‖Ji(X)‖qq,\nin the regularizer, where X indicates the current learning batch. We plan to analyze this approach in future work. Finally, we investigate generalization of the perturbations across different networks. For each DNN we have chosen the adversarial perturbation with the highest fooling rate from tables 1 to 3 and tested it against other networks. Results are given in table 4. Surprisingly, in some cases the fooling rate of the adversarial perturbation constructed using other network was higher than that of the network’s own adversarial perturbation."
    }, {
      "heading" : "7 Analysis of perturbations",
      "text" : "In this section we analyze some of the misclassified images. As the adversarial perturbation for each network we chose the one with the highest fooling rate (as before, selected from the ones given in tables 1 to 3). For each perturbation ε we also set ‖ε‖∞ = 10. For analysis we picked some of those images which were misclassified after adversarial attack by all the networks (see fig. 5). Predicted classes for the images and their perturbations are\ngiven in tables 5 to 7. We note that the top-1 class probability for images after adversarial attack is relatively low in the most cases. We test this behavior by computing the top-5 probabilities for several values of the ∞-norm of the adversarial perturbation. Results are given in figs. 6a and 6b. In both experiments the top-1 probability decreases significantly and becomes roughly equal to the top-2 probability, and in second case to the top-3. Similar behavior was noticed in some of\nthe cases when the adversarial example failed to fool the DNN — top-1 probability still has decreased significantly. We hypothesize that this might indicate that adversarial perturbations constructed as the (p, q)-singular vectors push an image away from the decision boundary.\nConclusions and related work We proposed a new algorithm for generating the universal adversarial perturbations, and tested it on several state-ofthe-art deep neural networks. We showed that using even small number of images results in the adversarial perturbations with high fooling rate. We investigated several properties of the constructed perturbations, such as dependence of its efficiency on the corresponding singular value and on the batch size. We also showed that constructed perturbations generalize well to other networks. Theoretical analysis of constructed perturbations and defense against this kind of adversarial attack is a problem of future research. Concept of adversarial examples was discussed in (Goodfellow, Shlens, and Szegedy 2014). Algorithm proposed there allows one to construct adversarial perturbations for individual images. Another approach was presented in (MoosaviDezfooli, Fawzi, and Frossard 2016). An algorithm for constructing universal adversarial perturbations was introduced in (Moosavi-Dezfooli et al. 2016). Efficiency of various perturbations is analyzed in (Fawzi, Moosavi-Dezfooli, and Frossard 2016). Various approaches to defense against adversarial attacks has been proposed. First attempts has been made in the original paper (Goodfellow, Shlens, and Szegedy 2014). Recently, approach based on controlling the Lipschitz constant of the network was suggested (Cisse et al. 2017). For testing various kinds of adversarial attacks Python library was introduced in (Papernot et al. 2016)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This study was supported by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001), by RFBR grants 16-31-60095-mol-a-dk, 16-31-00372-mola and by Skoltech NGP program."
    } ],
    "references" : [ {
      "title" : "and Vijayaraghavan",
      "author" : [ "A. Bhaskara" ],
      "venue" : "A.",
      "citeRegEx" : "Bhaskara and Vijayaraghavan 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "D",
      "author" : [ "Boyd" ],
      "venue" : "W.",
      "citeRegEx" : "Boyd 1974",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Parseval networks: Improving robustness to adversarial examples",
      "author" : [ "Cisse" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Cisse,? \\Q2017\\E",
      "shortCiteRegEx" : "Cisse",
      "year" : 2017
    }, {
      "title" : "Robustness of classifiers: from adversarial to random noise",
      "author" : [ "Moosavi-Dezfooli Fawzi", "A. Frossard 2016] Fawzi", "S.-M. Moosavi-Dezfooli", "P. Frossard" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Fawzi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fawzi et al\\.",
      "year" : 2016
    }, {
      "title" : "I",
      "author" : [ "Goodfellow" ],
      "venue" : "J.; Shlens, J.; and Szegedy, C.",
      "citeRegEx" : "Goodfellow. Shlens. and Szegedy 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He" ],
      "venue" : "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "citeRegEx" : "He,? \\Q2016\\E",
      "shortCiteRegEx" : "He",
      "year" : 2016
    }, {
      "title" : "Universal adversarial perturbations",
      "author" : [ "Moosavi-Dezfooli" ],
      "venue" : "arXiv preprint arXiv:1610.08401",
      "citeRegEx" : "Moosavi.Dezfooli,? \\Q2016\\E",
      "shortCiteRegEx" : "Moosavi.Dezfooli",
      "year" : 2016
    }, {
      "title" : "Deepfool: a simple and accurate method to fool deep neural networks",
      "author" : [ "Fawzi Moosavi-Dezfooli", "A. Fawzi", "P. Frossard" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Moosavi.Dezfooli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Moosavi.Dezfooli et al\\.",
      "year" : 2016
    }, {
      "title" : "2016. cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768",
      "author" : [ "Papernot" ],
      "venue" : null,
      "citeRegEx" : "Papernot,? \\Q2016\\E",
      "shortCiteRegEx" : "Papernot",
      "year" : 2016
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Russakovsky" ],
      "venue" : null,
      "citeRegEx" : "Russakovsky,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for largescale image recognition",
      "author" : [ "A. man" ],
      "venue" : "arXiv preprint arXiv:1409.1556",
      "citeRegEx" : "man,? \\Q2014\\E",
      "shortCiteRegEx" : "man",
      "year" : 2014
    }, {
      "title" : "and Bau III",
      "author" : [ "L.N. Trefethen" ],
      "venue" : "D.",
      "citeRegEx" : "Trefethen and Bau III 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Vulnerability of state-of-the-art deep neural networks to adversarial attacks has been attracting a lot of attention recently. In this work we propose a new algorithm for constructing universal adversarial perturbations. Our approach is based on computing the so called (p, q)-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns and by using a batch of just 64 images we can construct adversarial perturbations with relatively high fooling rate. We also investigate a correlation between the singular values of the Jacobian matrices and the fooling rate of a corresponding singular vector.",
    "creator" : "LaTeX with hyperref package"
  }
}