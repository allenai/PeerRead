{
  "name" : "1503.02009.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards an intelligent VNS heuristic for the k-labelled spanning forest problem",
    "authors" : [ "Sergio Consoli", "José Andrés Moreno Pérez", "Nenad Mladenović" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n02 00\n9v 1\n[ cs\n.O H\n] 5\nM ar\n2 01\n5\nTowards an intelligent VNS heuristic for the\nk-labelled spanning forest problem\nSergio Consoli1, José Andrés Moreno Pérez2, and Nenad Mladenović3"
    }, {
      "heading" : "1 ISTC/STLab, National Research Council (CNR), Catania, Italy",
      "text" : ""
    }, {
      "heading" : "2 Department of Computing Engineering, Universidad de La Laguna, Tenerife, Spain",
      "text" : ""
    }, {
      "heading" : "3 School of Computing & Mathematics, Brunel University, London, United Kingdom",
      "text" : "{sergio.consoli@jrc.it,jamoreno@ull.es,nenad.mladenovic@brunel.ac.uk}\nIn a currently ongoing project, we investigate a new possibility for solving the k -labelled spanning forest (kLSF) problem by an intelligent Variable Neighbourhood Search (Int-VNS) metaheuristic. In the kLSF problem we are given an undirected input graph G = (V,E, L) where V is the set of nodes, E the set of edges, that are labelled on the set L of labels, and an integer positive value k̄, and the aim is to find a spanning forest G∗ = (V,E∗, L∗) of the input graph having the minimum number of connected components, i.e. min|Comp(G∗)|, and the upper bound k̄ on the number of labels to use, |L∗| ≤ k̄. The problem is related to the minimum labelling spanning tree (MLST) problem [1], whose goal is to get the spanning tree of the input graph with the minimum number of labels, and has several applications in the real-world, where one aims to ensure connectivity by means of homogeneous connections. The kLSF problem was recently introduced in [2] along with the proof of its NP-hardness; therefore any practical solution approach requires heuristics [2, 3]. In particular our aim is to present an intelligent VNS which is aimed to achieve further improvements for the kLSF problem. This approach is derived from the promising strategy recently proposed in [4] for the MLST problem, and integrates the basic VNS for the kLSF problem in [3] with other complementary approaches from machine learning, statistics and experimental algorithmics, in order to produce high-quality performance and to completely automate the resulting strategy.\nThe first extension that we introduce is a local search mechanism that is inserted at top of the basic VNS. The resulting local search method is referred to as Complementary Variable Neighbourhood Search (Co-VNS) [4]. Given a labelled graph G = (V,E, L) with n vertices, m edges, and ℓ labels, CoVNS replaces iteratively each incumbent solution L∗ = (l1, l2, ..., lℓ), where, ∀i = 1, . . . , |L|, li = 1 if label i ∈ L\n∗, li = 0 otherwise, with another solution selected from the complementary space of L∗, referred to as Cospace, defined as the set of all the labels that are not contained in L∗, that is L∆L∗. The iterative process of extraction of a new solution from the complementary space of the current solution helps to escape the algorithm from possible traps in local minima, since the complementary solution lies in a very different zone of the search space with respect to the incumbent solution. Successively, the basic VNS [3] is applied in order to improve the resulting solution. At the starting point of VNS, it is required to define a suitable neighbourhood structure of size qmax. The simplest and most common choice is a structure in which the neigh-"
    }, {
      "heading" : "2 Consoli et al.",
      "text" : "bourhoods have increasing cardinality: |N1(·)| < |N2(·)| < ... < |Nqmax(·)|. In order to impose a neighbourhood structure on the solution space S, comprising all possible solutions, we define the distance between any two such solutions L1, L2 ∈ S, as the Hamming distance: ρ(L1, L2) = |L1∆L2| = ∑ℓ i=1 λi, where λi = 1 if label i is included in one of the solutions but not in the other, and 0 otherwise, ∀i = 1, ..., ℓ. In order to construct the neighbourhood of a solution L∗, the algorithm first proceeds with the deletion of labels from L∗. In other words, given a solution L∗, its qth neighbourhood, Nq(C), consists of all the different sets obtained from L∗ by removing q labels, where q ← 1, 2, ..., qmax. In order to seek further improvements and to automate on-line the search process, Co-VNS has been modified by including a probability-based local search with a self-tuning parameters setting, resulting in the intelligent VNS metaheuristic that we propose.\nThe probability-based local search is obtained by introducing a probabilistic choice on the next label to be added into incomplete solutions. By allowing worse components to be added to incomplete solutions, this probabilistic constructive heuristic produces a further increase on the diversification of the optimization process. The construction criterion is as follows. The procedure starts from an initial solution and iteratively selects at random a candidate move. If this move leads to a solution having a better objective function value than the current solution, then this move is accepted unconditionally; otherwise the move is accepted with a probability that depends on the deterioration,∆, of the objective function value. This construction criterion takes inspiration from Simulated Annealing: the acceptance probability of a worse component into a partial solution is evaluated according to the Boltzmann function exp(−∆/T ), where the parameter T , referred to as temperature, controls the dynamics of the function. Initially the value of T is large, so allowing many worse moves to be accepted, and is gradually reduced by the following geometric cooling law: Tj+1 = α · Tj, where T0 = |BestL| and α = 1/|BestL| ∈ [0, 1], with BestL being the current best solution and |BestL| its number of labels. This cooling schedule does not requires any intervention from the user regarding the setting of its parameters, as it is guided automatically by the best solution BestL. Therefore the whole process is able to react in response to the search algorithm’s behavior and to adapt its setting on-line according to the instance of the problem under evaluation.\nThe achieved optimization strategy seems to be highly promising for the kLSF problem. Ongoing investigation will consist in a statistical comparison of the resulting strategy against the best kLSF algorithms in the literature, in order to quantify and qualify the improvements obtained by the proposed Int-VNS."
    } ],
    "references" : [ {
      "title" : "The minimum labelling spanning trees",
      "author" : [ "R.S. Chang", "S.J. Leu" ],
      "venue" : "Inf Proc Let",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1997
    }, {
      "title" : "The k-labeled spanning forest problem",
      "author" : [ "R. Cerulli", "A. Fink", "M. Gentili", "A. Raiconi" ],
      "venue" : "Procedia - Social and Behavioral Sciences",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Variable neighbourhood search for the k-labelled spanning forest problem. ENDM",
      "author" : [ "S. Consoli", "J.A. Moreno-Pérez" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Intelligent variable neighbourhood search for the minimum labelling spanning tree problem",
      "author" : [ "S. Consoli", "J.A. Moreno-Pérez", "N. Mladenović" ],
      "venue" : "ENDM",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The problem is related to the minimum labelling spanning tree (MLST) problem [1], whose goal is to get the spanning tree of the input graph with the minimum number of labels, and has several applications in the real-world, where one aims to ensure connectivity by means of homogeneous connections.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "The kLSF problem was recently introduced in [2] along with the proof of its NP-hardness; therefore any practical solution approach requires heuristics [2, 3].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "The kLSF problem was recently introduced in [2] along with the proof of its NP-hardness; therefore any practical solution approach requires heuristics [2, 3].",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "The kLSF problem was recently introduced in [2] along with the proof of its NP-hardness; therefore any practical solution approach requires heuristics [2, 3].",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "This approach is derived from the promising strategy recently proposed in [4] for the MLST problem, and integrates the basic VNS for the kLSF problem in [3] with other complementary approaches from machine learning, statistics and experimental algorithmics, in order to produce high-quality performance and to completely automate the resulting strategy.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "This approach is derived from the promising strategy recently proposed in [4] for the MLST problem, and integrates the basic VNS for the kLSF problem in [3] with other complementary approaches from machine learning, statistics and experimental algorithmics, in order to produce high-quality performance and to completely automate the resulting strategy.",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "The resulting local search method is referred to as Complementary Variable Neighbourhood Search (Co-VNS) [4].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "Successively, the basic VNS [3] is applied in order to improve the resulting solution.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Initially the value of T is large, so allowing many worse moves to be accepted, and is gradually reduced by the following geometric cooling law: Tj+1 = α · Tj, where T0 = |BestL| and α = 1/|BestL| ∈ [0, 1], with BestL being the current best solution and |BestL| its number of labels.",
      "startOffset" : 199,
      "endOffset" : 205
    } ],
    "year" : 2017,
    "abstractText" : null,
    "creator" : "LaTeX with hyperref package"
  }
}