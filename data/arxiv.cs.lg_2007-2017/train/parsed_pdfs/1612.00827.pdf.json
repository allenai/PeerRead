{
  "name" : "1612.00827.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Operations on a Stack with Neural Turing Machines",
    "authors" : [ "Tristan Deleu" ],
    "emails" : [ "tristan.deleu@snips.ai", "joseph.dureau@snips.ai", "@NIPS" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 2.\n00 82\n7v 1\n[ cs\n.L G\n] 2\nD ec"
    }, {
      "heading" : "1 Introduction",
      "text" : "Although neural networks shine at finding meaningful representations of the data, they are still limited in their capacity to plan ahead, reason and store information over long time periods. Keeping track of nested parentheses in a language model, for example, is a particularly challenging problem for RNNs [9]. It requires the network to somehow memorize the number of unmatched open parentheses. In this paper, we analyze the ability of Neural Turing Machines (NTMs) to recognize well-balanced strings of parentheses. We show that even though the NTM architecture does not explicitely operate on a stack, it is able to emulate this data structure with its heads. Such a behaviour was unobserved on other simple algorithmic tasks [4].\nAfter a brief recall of the Neural Turing Machine architecture in Section 3, we show in Section 4 how the NTM is able to learn an algorithm to recognize strings of well-balanced parentheses, called Dyck words. We also show how this model is capable to strongly generalize to longer sequences."
    }, {
      "heading" : "2 Related Work",
      "text" : "Grammar induction Deep learning models are often trained on large datasets, generally extracted from real-world data at the cost of an expensive labeling step by some expert. In the context of Natural Language Processing, an alternative is to generate data from an artificial language, based on a predefined grammar. Historically, these formal languages have been used to evaluate the theoretical foundations of RNNs [14].\nHochreiter and Schmidhuber [7] tested their new Long Short-Term Memory (LSTM) on the embedded Reber language, to show how their output gates can be beneficial. This behaviour was later extended to a variety of context-free and context-sensitive languages [3]. However, as opposed to these previous works focused on character-level language modeling, here our task of interest is the membership problem. This is a classification problem, where positive examples are generated by a given grammar, and negative examples are randomly generated with the same alphabet.\nDifferentiable memory To enhance their capacity to retain information, RNNs can be augmented with an explicit and differentiable memory module. Memory Networks and Dynamic Memory Net-\n1st Workshop on Neural Abstract Machines & Program Induction (NAMPI), @NIPS 2016, Barcelona, Spain.\nworks [16, 11, 17] use a hierarchical attention mechanism on an associative array to solve text QA tasks involving reasoning. Closely related our work, Stack-augmented RNNs [8] are capable of inferring algorithmic patterns on some context-free and context-sensitive languages, including anbn, anbncn, and anbmcn+m."
    }, {
      "heading" : "3 Neural Turing Machines",
      "text" : "The Neural Turing Machine (NTM) [4] is an instance of memory-augmented neural networks, consisting of a neural network controller which interacts with a large (though bounded, unlike a Turing machine) memory tape. The NTM uses soft read and write heads to retrieve information from the memory and store information in memory. The dynamics of these heads are governed by one or multiple sets of weights wrt for the read head(s) and w w t for the write head(s). These are controlled by the controller (either a Feed-forward network, or an LSTM), and maintain the overall architecture differentiable. The read head returns a read vector rt as a weighted sum over the rows of the memory bank Mt:\nrt = ∑\ni\nw r t (i)Mt(i) (1)\nSimilarly, the write head modifies the memory Mt by first erasing a weighted version of some erase vector et from each row in the memory (Equation 2), then adding a weighted version of an add vector at (Equation 3). Both vectors et and at are generated by the controller.\nM̃t+1(i) ←− Mt(i) · (1−w w t (i) et) (2)\nMt+1(i) ←− M̃t+1(i) +w w t (i)at (3)\nThe weights wrt and w r t are produced through a series of differentiable operations, called the addressing mechanisms. These fall into two categories: a content-based addressing comparing each memory locations with some key kt, and a location-based addressing responsible for shifting the heads (similar to a Turing machine). Even though recent works [13, 6] tend to drop the locationaddressing, we chose to use the original formulation of the NTM and keep both addressing mechanisms."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dyck words",
      "text" : "A Dyck word is a balanced string of opening and closing parentheses. Besides the important role they play in parsing, they have multiple connections with other combinatorial objects [15, 2]. In particular, one convenient and visual way representation of a Dyck word is a path on the integer line (see Figure 2).\nTo avoid ambiguities, we will consider strings of parentheses as words w ∈ {u, d}∗ = A∗, where each character u corresponds to an opening parenthesis and d to a closing parenthesis. The subset of A∗ containing the Dyck words of length < 2n is called the Dyck language and is denotedD<2n."
    }, {
      "heading" : "4.2 Experimental setup",
      "text" : "We are interested here in the membership problem over the Dyck language. We trained a NTM for a classification task, where positive examples are uniformly sampled [2] from the Dyck language D<12, and negative examples are non-Dyck words w ∈ A∗ of length < 12 with the same number of characters u and d. We use the same experimental setup as described in [4], with a 1-layer feedforward controller with 100 hidden units, 1 read head, 1 write head, and a memory bank containing 128 memory locations, each of dimension 20. We used a ReLU nonlinearity for the key kt and add vector at and a hard sigmoid for the erase vector et. We trained the model using the Adam optimizer [10] with a learning rate of 0.001 and batch size 16."
    }, {
      "heading" : "4.3 Stack emulation",
      "text" : "The Dyck language is a context-free language that can be recognized by a pushdown automaton [1]. Here, we are interested in the nature of the algorithm the NTM is able to infer only from examples on this task. More specifically, we want to know if, and how, the NTM uses its memory to act as a stack, without specifying the push and pop operations explicitely [8, 5]. In Figure 3, we show the behaviour of the read and write heads on a Dyck word and a non-Dyck word, along with the probability returned by the model of each prefix to be a Dyck word.\nWe observe that the model is actually emulating a stack with its read head. Each time the NTM reads an opening parenthesis u, the read head is moved upward and conversely when reading a closing parenthesis d. This behaviour is different from what was previously reported on other algorithmic tasks [4], where the content of the memory played a central role. Here, the NTM barely writes anything in memory, but uses its read head for computation purposes, following closely the graphical representation of the words (on the right).\nThe NTM uses its read head similarly for non-Dyck words, up until it reads a closing parenthesis with no matching opening parenthesis (illustrated by the red line in the graphical representation of the word), where the model correctly predicts the word is no longer a Dyck word. Beyond simply counting opening and closing parentheses, the NTM was also able to remember that violation point, despite the lack of recurrent controller."
    }, {
      "heading" : "4.4 Strong generalization",
      "text" : "When testing a model, it is often assumed that the training and test data are sampled from the same (unknown) distribution. However, here we are not only interested in the capacity of the NTM to recognize Dyck words of similar length, but also its capacity to learn an algorithm and generalize to longer sequences. This is called strong generalization [12].\nIn Figure 4, we compare the generalization performance of the NTM against an LSTM. This LSTM was selected as the model yielding the best AUC on sequences in D<200, with the number of hidden units selected in [2, 5, 10, 20, 50, 100, 200, 500] (best: 10). While the LSTM shows signs of strong generalization on sequences twice as long as what it was given during training, the AUC starts dropping for much longer sequences. On the other hand, the NTM generalizes perfectly even for much longer sequences (up to 20 times longer than the training regime). Beyond n ≈ 120, the AUC starts to slightly decrease, most likely due to overflow issues: the stack emulated by the read head is limited by the number of memory locations, here 128."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Through an experiment on an artificial language called the Dyck language, we have shown that Neural Turing Machines are not only able to use their memory for storage, but can also use their heads for computational purposes. This allows the NTM to strongly generalize to inputs much longer, effectively learning an algorithm (contrary to only learning patterns in the data). The size of the memory allocated for the NTM being the only constraint. An interesting line of research could then be to run a similar experiment on a model trained under a memory-restricted regime, like a single memory location, and see how the NTM can emulate a stack under this stronger constraint."
    } ],
    "references" : [ {
      "title" : "Context-Free Languages and Push- Down Automata",
      "author" : [ "Jean-Michel Autebert", "Jean Berstel", "Luc Boasson" ],
      "venue" : "In Handbook of Formal Languages,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1997
    }, {
      "title" : "Analytic Combinatorics",
      "author" : [ "Philippe Flajolet", "Robert Sedgewick" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "LSTM recurrent networks learn simple context-free and context-sensitive languages",
      "author" : [ "Felix A Gers", "Jürgen Schmidhuber" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Learning to Transduce with Unbounded Memory",
      "author" : [ "Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "CoRR, abs/1506.02516,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Dynamic Neural Turing Machine with Soft and Hard Addressing",
      "author" : [ "Çaglar Gülçehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "Schemes. CoRR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "Inferring Algorithmic Patterns with Stack-Augmented",
      "author" : [ "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "Recurrent Nets. CoRR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Visualizing and understanding recurrent networks",
      "author" : [ "Andrej Karpathy", "Justin Johnson", "Fei-Fei Li" ],
      "venue" : "arXiv preprint arXiv:1506.02078,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Key-Value Memory Networks for Directly Reading Documents",
      "author" : [ "Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "One-shot Learning with Memory-Augmented",
      "author" : [ "Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy P. Lillicrap" ],
      "venue" : "Neural Networks. CoRR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Enumerative combinatorics. Volume 2. Cambridge studies in advanced mathematics",
      "author" : [ "Richard P. Stanley" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1999
    }, {
      "title" : "Dynamic Memory Networks for Visual and Textual Question Answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher" ],
      "venue" : "CoRR, abs/1603.01417,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Keeping track of nested parentheses in a language model, for example, is a particularly challenging problem for RNNs [9].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "Hochreiter and Schmidhuber [7] tested their new Long Short-Term Memory (LSTM) on the embedded Reber language, to show how their output gates can be beneficial.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "This behaviour was later extended to a variety of context-free and context-sensitive languages [3].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "works [16, 11, 17] use a hierarchical attention mechanism on an associative array to solve text QA tasks involving reasoning.",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "works [16, 11, 17] use a hierarchical attention mechanism on an associative array to solve text QA tasks involving reasoning.",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "Closely related our work, Stack-augmented RNNs [8] are capable of inferring algorithmic patterns on some context-free and context-sensitive languages, including ab, abc, and abc.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "Even though recent works [13, 6] tend to drop the locationaddressing, we chose to use the original formulation of the NTM and keep both addressing mechanisms.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "Even though recent works [13, 6] tend to drop the locationaddressing, we chose to use the original formulation of the NTM and keep both addressing mechanisms.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 11,
      "context" : "Besides the important role they play in parsing, they have multiple connections with other combinatorial objects [15, 2].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "Besides the important role they play in parsing, they have multiple connections with other combinatorial objects [15, 2].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "We trained a NTM for a classification task, where positive examples are uniformly sampled [2] from the Dyck language D<12, and negative examples are non-Dyck words w ∈ A∗ of length < 12 with the same number of characters u and d.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "We trained the model using the Adam optimizer [10] with a learning rate of 0.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "The Dyck language is a context-free language that can be recognized by a pushdown automaton [1].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "More specifically, we want to know if, and how, the NTM uses its memory to act as a stack, without specifying the push and pop operations explicitely [8, 5].",
      "startOffset" : 150,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "More specifically, we want to know if, and how, the NTM uses its memory to act as a stack, without specifying the push and pop operations explicitely [8, 5].",
      "startOffset" : 150,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "This LSTM was selected as the model yielding the best AUC on sequences in D<200, with the number of hidden units selected in [2, 5, 10, 20, 50, 100, 200, 500] (best: 10).",
      "startOffset" : 125,
      "endOffset" : 158
    }, {
      "referenceID" : 3,
      "context" : "This LSTM was selected as the model yielding the best AUC on sequences in D<200, with the number of hidden units selected in [2, 5, 10, 20, 50, 100, 200, 500] (best: 10).",
      "startOffset" : 125,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "This LSTM was selected as the model yielding the best AUC on sequences in D<200, with the number of hidden units selected in [2, 5, 10, 20, 50, 100, 200, 500] (best: 10).",
      "startOffset" : 125,
      "endOffset" : 158
    } ],
    "year" : 2016,
    "abstractText" : "Multiple extensions of Recurrent Neural Networks (RNNs) have been proposed recently to address the difficulty of storing information over long time periods. In this paper, we experiment with the capacity of Neural Turing Machines (NTMs) to deal with these long-term dependencies on well-balanced strings of parentheses. We show that not only does the NTM emulate a stack with its heads and learn an algorithm to recognize such words, but it is also capable of strongly generalizing to much longer sequences.",
    "creator" : "LaTeX with hyperref package"
  }
}