{
  "name" : "1608.05745.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time Attention Mechanism",
    "authors" : [ "Edward Choi", "Mohammad Taha Bahadori", "Andy Schuetz", "Walter F. Stewart", "Jimeng Sun" ],
    "emails" : [ "mp2893@gatech.edu},", "bahadori@gatech.edu},", "schueta1@sutterhealth.org,", "stewarwf@sutterhealth.org,", "jsun@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "have to suffer the tradeoff between the two by either picking complex black box models such as recurrent neural networks (RNN) or relying on less accurate traditional models with better interpretation such as logistic regression. To address this dilemma, we present REverse Time AttentIoN model (RETAIN) for analyzing EHR data that achieves high accuracy while remaining clinically interpretable. RETAIN is a two-level neural attention model that can find influential past visits and significant clinical variables within those visits (e.g,. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that more recent clinical visits will likely get higher attention. Experiments on a large real EHR dataset of 14 million visits from 263K patients over 8 years confirmed the comparable predictive accuracy and computational scalability to the state-of-the-art methods such as RNN. Finally, we demonstrate the clinical interpretation with concrete examples from RETAIN."
    }, {
      "heading" : "1 Introduction",
      "text" : "The broad adoption of Electronic Health Record (EHR) systems has opened the possibility of applying clinical predictive models to improve the quality of clinical services. Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20]. EHR data are represented as temporal sequences of high-dimensional clinical variables (e.g., diagnoses, medications and procedures), where each sequence corresponds to all medical visits from a single patient. Abundance of such EHR data provide great machine learning opportunities for developing accurate while interpretable data driven models. However, the key challenge lies in how to model the temporality and high-dimensionality of these EHR sequences.\nAccuracy and interpretation are two goals of any successful predictive models. There is a common belief that one has to trade accuracy for interpretation in favor to simpler models [6]. Traditional methods with great clinical interpretation can be divided into three groups: 1) identifying a set of rules (e.g. via decision trees [27]), 2) case-based reasoning by finding similar patients (e.g. via k-nearest neighbors [18] and distance metric learning [35]), and 3) identifying a list of risk factors (e.g. via LASSO coefficients [15]). While interpretable, all above models ignored the temporality of the EHR data, hence led to sub-optimal models. Latent-variable time-series models, such as [33, 34], do capture the temporality, but often have limited interpretation due to abstract state variables.\nRecently, recurrent neural networks (RNN) have been successfully applied in modeling sequential EHR data to perform various predictive analysis such as learning to diagnose [30] and disease progression modeling [11, 14]. Despite the promising gain in accuracy, RNNs are notoriously difficult to interpret. While there have been several attempts at directly interpreting RNNs [19, 26, 8], none of them provide the level of interpretation that can serve the healthcare applications.\nIn this work, we propose RETAIN, a two-level neural attention model for sequential data that provides detailed interpretation of the prediction results while demonstrating the prediction accuracy comparable to RNN. This is made possible by our attention mechanism inspired by the behavior of human physicians. The key idea of RETAIN (see Figure 1) is to have a temporal attention generation mechanism, while keeping the\nar X\niv :1\n60 8.\n05 74\n5v 1\n[ cs\n.L G\n] 1\n9 A\nug 2\n01 6\nrepresentation learning part simple and interpretable. Similar to human doctors, RETAIN makes a prediction by looking at the patient’s past visits in reverse time order, which computationally makes the attention generation mechanism more stable too. RETAIN not only considers which past visit is worth paying attention to, but also considers how much individual variables in each visit contribute to the prediction.\nUsing a large EHR data with 263K patients over 14m visits across 8 years, we compare RETAIN against several baseline methods including traditional machine learning methods and RNN variants on heart failure onset prediction. RETAIN achieves comparable performance to RNN in both accuracy and speed and outperforms significantly all the other baselines. Finally, we demonstrate RETAIN to provide intuitive interpretation through a concrete case study and visualization."
    }, {
      "heading" : "2 Methodology",
      "text" : "In this section, we first describe the structure of sequential EHR data, our notation, and a general form of predictive analysis in healthcare using EHR. Then, we describe the details of RETAIN.\nEHR Structure and our Notation. EHR data of each patient is modeled as a time-labeled sequence of multivariate observations. Assuming we use r different variables, the n-th patient of total N patients can be represented by a sequence of T (n) tuples (t(n)i ,x (n) i ) ∈ R×Rr, i = 1, . . . , T (n). The timestamps t (n) i denotes the time of the i-th visit of the n-th patient and T (n) the sequence length of the n-th patient. To avoid cluttered notation, we describe the algorithms for a single patient and drop the superscript (n) whenever it is unambiguous. The goal of predictive modeling is to predict labels for each time step yi ∈ {0, 1}s or at the end of the sequence y ∈ {0, 1}s (without time index). The number of labels s can be more than 1.\nFor example, in disease progression modeling (DPM), we are given a sequence of visits where each visit is a set of varying number of medical codes {c1, c2, . . . , cn}. The code cj is the j-th code from the vocabulary C. Therefore, in DPM, the number of variables r = |C| and input vector xi ∈ {0, 1}|C| is a binary vector where the value one in the j-th coordinate indicates code cj is included in i-th visit. Given a sequence of visits x1, . . . ,xT , the goal of DPM is, for each time step i, to predict the codes occurring at the next visit x2, . . . ,xT+1, making the number of labels s = |C|.\nIn the case of learning to diagnose (L2D) [30], the input vector xi consists of measurements (possibly continuous) collected by monitoring devices. If there are r different measurements, then xi ∈ Rr. The goal of L2D is, given input sequence x1, . . . ,xT , to predict the occurrence of a specific disease (s = 1) or multiple diseases (s > 1). Without loss of generality, we will describe the algorithm for DPM, as L2D can be seen as a\nspecial case of DPM where we make a single prediction at the end of the visit sequence. In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers)."
    }, {
      "heading" : "2.1 Preliminaries on Neural Attention Models",
      "text" : "Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12]. The need for attention mechanism can be seen in the language translation task [2]: Representing the entire sentence with one fixed-size vector is inefficient; the neural translation machine usually finds it difficult to translate the given sentence represented by a single vector.\nIntuitively, the attention mechanism for language translation works as follows: given a sentence of length S in the original language, we generate h1, . . . ,hS , the representation of the words in the sentence, e.g. using an RNN. To find the j-th word in the target language, we generate attentions αji for i = 1, . . . , S for each word in the original sentence. Then, we compute the context vector cj = ∑ i α j ihi and use it to predict the j-th word in the target language. In general, the attention mechanism allows the model to focus on a specific word (or words) in the given sentence when generating each word in the target language.\nIn this work, we define a temporal attention mechanism to provide interpretable prediction models in healthcare. Doctors generally pay attention to specific clinical information (e.g., key risk factors) and their timing when reviewing EHR data. We exploit this insight to develop a temporal attention model that mimics doctors’ practice, which will be introduced next.\n2.2 Reverse Time Attention Model RETAIN Figure 2 shows the high-level overview of our model. One key idea is to delegate a considerable portion of the prediction responsibility to the attention weights generation process. RNNs become difficult to interpret due to the recurrent weights feeding past information to the hidden layer. Therefore, to consider both the visit-level and the variable-level (individual coordinates of xi) influence, we use a linear embedding of the input vector xi. That is, we define\nvi = Wembxi, (Step 1)\nwhere vi ∈ Rm denotes the embedding of the input vector xi ∈ Rr, m the size of the embedding dimension, Wemb ∈ Rm×r the embedding matrix to learn. We can easily choose a more sophisticated but still interpretable representation such as multilayer perceptron (MLP) [13, 28] which has been used for representation learning in EHR data [10].\nWe use two sets of weights for the visit-level attention and the variable-level attention, respectively. The scalars α1, . . . , αi are the visit-level attention weights that govern the influence of each visit embedding v1, . . . ,vi. The vectors β1, . . . ,βi are the variable-level attention weights that focus on each coordinate of the visit embedding v1,1, v1,2, . . . , v1,m, . . . , vi,1, vi,2, . . . , vi,m.\nWe use two RNNs, RNNα and RNNβ, to separately generate α’s and β’s as follows,\ngi,gi−1, . . . ,g1 = RNNα(vi,vi−1, . . . ,v1),\nej = w > αgj + bα, for j = 1, . . . , i\nα1, α2, . . . , αi = Softmax(e1, e2, . . . , ei) (Step 2) hi,hi−1, . . . ,h1 = RNNβ(vi,vi−1, . . . ,v1)\nβj = tanh ( Wβhj + bβ ) for j = 1, . . . , i, (Step 3)\nwhere gi ∈ Rp is the hidden layer of RNNα at time step i, hi ∈ Rq the hidden layer of RNNβ at time step i and wα ∈ Rp, bα ∈ R,Wβ ∈ Rm×q and bβ ∈ Rm are the parameters to learn. The hyperparameters p and q determine the hidden layer size of RNNα and RNNβ, respectively. Note that for prediction at each\ntimestamp, we generate a new set of attention vectors α and β. For simplicity of notation, we do not include the index for predicting at different time steps.\nAnother key idea in RETAIN is to generate the attention vectors by running the RNNs backward in time; i.e., RNNα and RNNβ both take the visit embeddings in a reverse order vi,vi−1, . . . ,v1. This idea is inspired by the common behavior of physicians: When physicians try to diagnose based on the past records, they typically study the patient’s most recent records first, and go back in time. Computationally, running the RNN in reversed time order has several advantages as well: The reverse time order allows us to generate e’s and β’s that dynamically change their values when making predictions at different time steps i = 1, 2, . . . , T . It ensures that the attention vectors will be different at each timestamp and makes the attention generation process computationally more stable.1\nWe generate the context vector ci for a patient up to the i-th visit as follows,\nci = i∑ j=1 αjβj vj , (Step 4)\nwhere denotes element-wise multiplication. We use the context vector ci ∈ Rm to predict the true label yi ∈ {0, 1}s as follows,\nŷi = Softmax(Wci + b), (Step 5)\nwhere W ∈ Rs×m and b ∈ Rs are parameters to learn. We use the cross-entropy to calculate the classification loss as follows,\nL(x1, . . . ,xT ) = − 1\nN N∑ n=1 1 T (n) T (n)∑ i=1 ( y>i log(ŷi) + (1− yi)> log(1− ŷi) ) (1)\nwhere we sum the cross entropy errors from all dimensions of ŷi. In case of real-valued output yi ∈ Rs, we can change the cross-entropy in Eq. (1) to for example mean squared error.\nOverall, our attention mechanism can be viewed as the inverted architecture of the standard attention mechanism for NLP [2] where the words are encoded using RNN and generate the attention weights using MLP. Our method, on the other hand, uses MLP to embed the visit information to preserve interpretation and uses RNN to generate two sets of attention weights, recovering the sequential information as well as mimicking the behavior of physicians.\n1For example, feeding visit embeddings in the original order to RNNα and RNNβ will generate the same e1 and β1 for every time step i = 1, 2, . . . , T . Moreover, in many cases, a patient’s recent visit records deserve more attention than the old records. Then we need to have ej+1 > ej which makes the process computationally unstable for long sequences.\n3 Interpreting RETAIN Finding the important visits for making a prediction can be derived based on the largest αi, which is simple. However, finding influential variables is slightly more involved which will be described next. Note that a clinical visit consists of multiple medical variables, each of which makes different amount of contribution. The contribution of each variable is determined by v, β and α, and interpreting α alone will tell which visit is influential in prediction but not why.\nWe propose a method to interpret the end-to-end behavior of RETAIN. By keeping α and β values fixed as the attention of doctors, we will analyze the changes in the probability of each label yi,1, . . . , yi,s in terms of the change in an original input x1,1, . . . , x1,r, . . . , xi,1, . . . , xi,r. The xj,k that lead to the largest change in yi,d will be the input variable with highest contribution. More formally, given the sequence x1, . . . ,xi, we are trying to predict the probability of the output vector yi ∈ {0, 1}s, which can be expressed as follows\np(yi|x1, . . . ,xi) = p(yi|ci) = Softmax (Wci + b) (2)\nwhere ci ∈ Rm denotes the context vector. According to Step 4, ci is the sum of the visit embeddings v1, . . . ,vi weighted by the attentions α’s and β’s. Therefore Eq (2) can be rewritten as follows,\np(yi|x1, . . . ,xi) = p(yi|ci) = Softmax ( W ( i∑ j=1 αjβj vj ) + b ) (3)\nUsing the fact that the visit embedding vi is the sum of the columns of Wemb weighted by each element of xi, Eq (3) can be rewritten as follows,\np(yi|x1, . . . ,xi) = Softmax ( W ( i∑ j=1 αjβj r∑ k=1 xj,kWemb[:, k] ) + b )\n= Softmax ( i∑ j=1 r∑ k=1 xj,k αjW ( βj Wemb[:, k] ) + b ) (4)\nwhere xj,k is the k-th element of the input vector xj . Eq (4) tells us that the calculation of the likelihood of yi can be completely deconstructed down to the variables at each input x1, . . . ,xi. Therefore we can calculate the contribution ω of the k-th variable of the input xj at time step j ≤ i, for predicting yi as follows,\nω(yi, xj,k) = αjW(βj Wemb[:, k])︸ ︷︷ ︸ Contribution coefficient xj,k︸︷︷︸ Input value , (5)\nwhere the index i of yi is omitted in the αj and βj . As we have described in Section 2.2, we are generating α’s and β’s at time step i in the visit sequence x1, . . . ,xT . Therefore the index i is always assumed for α’s and β’s. Additionally, Eq (5) shows that when we are using a binary input value, the coefficient itself is the contribution. However, when we are using a continuous input value, we need to multiply the coefficient and the input value xj,k to correctly calculate the contribution."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we demonstrate that while RETAIN is competitive with RNNs in terms of performance, we can interpret the knowledge learned by it. Due to lack of space, we only report the results on learning to diagnose (L2D) task and defer the results on disease progression modeling (DPM) to Appendix B."
    }, {
      "heading" : "4.1 Experimental setting",
      "text" : "Source of data: The dataset consists of Electronic Health Records collected by a non-profit health organization. The patients are middle-aged adults chosen for a heart failure study. From the encounter\nrecords, medication orders, procedure orders and problem lists, we extracted visit records consisting of diagnosis, medication and procedure codes. To reduce the dimensionality while preserving the clinical information, we used existing medical groupers to aggregate the codes into input variables. The details of the medical groupers are given in the Appendix A. The statistics of the dataset are provided in Table 1.\nImplementation details: We implemented RETAIN with Theano 0.8 [4]. For training the model, we used Adadelta [37] with the mini-batch of 100 patients. The training was done in a machine equipped with Intel Xeon E5-2630, 256GB RAM, two Nvidia Tesla K80’s and CUDA 7.5.\nBaselines: We use the following baseline models.\n• Logistic regression (LR): We compute the counts of medical codes for each patient based on all her visits as input variables and normalize the vector to zero mean and unit variance. We use the resulting vector to train the logistic regression.\n• MLP: We use the same feature construction as LR, but put a hidden layer of size 256 between the inpuputt and output.\n• RNN: RNN with two hidden layers of size 256 implemented by the GRU. Input sequences x1, . . . ,xi are used. Logistic regression is applied to the top hidden layer. We use two layers of RNN of to match the model complexity of RETAIN.\n• RNN+αM : One layer single directional RNN (hidden layer size 256) along time to generate the input embeddings v1, . . . ,vi. We use the MLP with a single hidden layer of size 256 to generate the visit-level attentions α1, . . . , αi. We use the input embeddings v1, . . . ,vi as the input to the MLP. This baseline corresponds to Figure 1a.\n• RNN+αR: This is similar to RNN+αM but use the reverse-order RNN (hidden layer size 256) to generate the visit-level attentions α1, . . . , αi. We use this baseline to confirm the effectiveness of generating the attentions using reverse time order.\nThe baselines are visually illustrated and compared in Appendix C. We use the same implementation, hyper-parameter exploration and training method for the baselines as described above. We describe in detail the hyper-parameters, regularization and drop-out strategies for the baseline models in Appendix A.\nEvaluation measures: We use two types of metrics to measure the accuracy.\n• Negative log-likelihood on test set: Negative log-likelihood measures the model loss on the test set. The loss can be calculated by Eq (1).\n• Area Under the ROC Curve (AUC): is the area under the receiver operating characteristic curve of comparing ŷi with with the true label yi. AUC is more robust to imbalanced positive/negative prediction labels which makes is appropriate for evaluation of classification accuracy in the heart failure prediction task.\nWe also report the standard deviation of the performance measures by bootstrapping the results on the test set for 10,000 times."
    }, {
      "heading" : "4.2 Heart Failure Prediction",
      "text" : "Objective: Given a visit sequence x1, . . . ,xT , we try to predict if the patient will be diagnosed with heart failure (HF). This can be seen as a special case of DPM where we make a prediction for a single disease at the end of the sequence. Since this is a binary prediction task, we use the logistic sigmoid function instead of the Softmax in Step 5.\nCohort construction: From the source dataset, 3,884 cases are selected and approximately 10 controls are selected for each case (28,903 controls). The case/control selection criteria are fully described in the supplementary section. Cases have index dates to denote the date they are diagnosed with HF. Controls have the same index dates as their corresponding cases. We extract diagnosis codes, medication codes and procedure codes in the 18-months window before the index date.\nTraining details: The patient cohort was divided into the training, validation and test set in a 0.75:0.1:0.15 ratio. The validation set was used to determine the values of the hyper-parameters. The details of hyper-parameter tuning is provided in the supplementary section.\nResults: Table 2 compares the prediction performance of RETAIN against the baselines. There is a large gap in the performance of logistic regression, MLP, and temporal learning algorithms including RNN variants and our RETAIN, because the temporality is modeled. We see that while RETAIN is quite competitive with the best performing RNN variants, it beats RNN in terms of test negative log-likelihood and is very close to the best one in AUC measure. Given the interpretation benefit, RETAIN can be a great choice for clinical decision support applications.\nNote that RNN+αR model are a degenerated version of RETAIN with only scalar attention measures, which is still a competitive model as shown in table 2. This confirms the efficiency of generating attention weights using the RNN. However, RNN+αR model only provides scalar visit-level attention, which is not sufficient for healthcare applications. Patients often receives several medical codes at a single visit, and it will be important to distinguish their relative importance to the target. We show such a case study in section 4.3.\nTable 2 also shows the scalability of RETAIN. The training time is the number of seconds to train the model over the entire training set once, i.e., training time for 1 epoch. The test time is the number of seconds to generate the prediction output for the entire test set. We use the mini-batch of 100 patients in both cases. Note that RNN takes longer time than RNN+αM because of its two-layer structure, whereas RNN+αM uses a single layer RNN. The models that use two RNNs (RNN, RNN+αR, RETAIN)2 take similar time to train for one epoch. However, each model takes different number of epochs to converge. RNN typically takes approximately 10 epochs, RNN+αM and RNN+αR 15 epochs and RETAIN 30 epochs. Lastly, training the attention models (RNN+αM , RNN+αR and RETAIN) for DPM would take considerably longer than L2D, because they will generate context vectors at each time step. RNN, on the other hand, does not require additional computation other than embedding the visit to its hidden layer to predict target labels at each time step. Therefore, in DPM, the training time of the attention models will linearly increase by the length of the input sequence."
    }, {
      "heading" : "4.3 Model Interpretation for Heart Failure Prediction",
      "text" : "We demonstrate the interpretability of RETAIN by studying its behavior in the HF prediction task. We choose a HF patient from the test set and calculate the contribution of the variables (medical codes in this case) for making the binary prediction. Figure 3a is the visualization of the contributions of the variables in each visit. The patient suffered from skin problems, skin disorder (SD), benign neoplasm (BN), excision of skin lesion (ESL), for some time before showing symptoms of HF, cardiac dysrhythmia (CD), heart valve disease (HVD) and coronary atherosclerosis (CA), then being diagnosed with HF at the end. We can see that skin-related\n2RNN uses two layers of RNN, RNN+αR uses one for visit embedding and one for generating α, RETAIN uses each for generating α and β\ncodes from the earlier visits made little contribution to HF prediction as expected. RETAIN properly puts much attention to the HF-related codes that occurred in recent visits.\nIn order to evaluate RETAIN’s ability to generate attentions considering the temporality, we reverse the visit sequence of Figure 3a and feed it to RETAIN. Figure 3b shows the contribution of the medical codes of the reversed visit record. HF-related codes in the old visits are still making positive contributions, but not as much as they did in Figure 3a. Figure 3b also emphasizes RETAIN’s superiority to interpretable, but stationary models such as logistic regression. Stationary models often aggregate past information and remove the temporality from the input data, which can mistakenly lead to the same risk prediction for Figure 3a and 3b. RETAIN, however, can correctly digest the sequential information and calculates the HF risk score of 9.0%, which is significantly lower than that of Figure 3a.\nFigure 3c shows how the contributions of the codes change when we gave the patient proper medications in the early stage. We added two medications from day 219: antiarrhythmics (AA) and anticoagulants (AC), both of which are used to treat cardiac dysrhythmia (CD). We can see that the two medications are making negative contributions as expected, especially towards the end of the record. The medications helped decrease the positive contributions of heart valve disease and cardiac dysrhythmia in the last visit. Indeed, the HF risk prediction (.2165) of Figure 3c is lower than that of Figure 3a (.2474). This suggests that taking proper medications can help the patient in reducing the HF risk."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We present the need for interpretable and accurate predictive model in healthcare applications. Given the power of recurrent neural networks for analyzing sequential data, we proposed RETAIN, which preserves RNN’s predictive power while allowing a higher degree of interpretation. The key idea of RETAIN is to improve the prediction accuracy through a sophisticated attention generation process, while keeping the representation learning part simple for interpretation, making the entire algorithm accurate and interpretable. RETAIN trains two RNN in a reverse time order to efficiently generate the appropriate attention variables. For future work,\nwe plan to developing an interactive visualization system for RETAIN and evaluating RETAIN in healthcare applications."
    }, {
      "heading" : "A Details of the experiment settings",
      "text" : "A.1 Hyper-parameter Tuning We used the validation set to tune the hyper-parameters: visit embedding size m, RNNα’s hidden layer size p, RNNβ’s hidden layer size q, L2 regularization coefficient, and drop-out rates.\nL2 regularization was applied to all weights except the ones in RNNα and RNNβ. Two separate drop-outs were used on the visit embedding vi and the context vector ci. We performed the random search with predefined ranges m, p, q ∈ {32, 64, 128, 200, 256}, L2 ∈ {0.1, 0.01, 0.001, 0.0001}, dropoutvi , dropoutci ∈ {0.0, 0.2, 0.4, 0.6, 0.8}. We also performed the random search with m, p and q fixed to 256.\nThe final value we used to train RETAIN for heart failure prediction is m, p, q = 128, dropoutvi = 0.6, dropoutci = 0.6 and 0.0001 for the L2 regularization coefficient.\nA.2 Code Grouper Diagnosis codes, medication codes and procedure codes in the dataset are respectively using International Classification of Diseases (ICD-9), Generic Product Identifier (GPI) and Current Procedural Terminology (CPT).\nDiagnosis codes are grouped by Clinical Classifications Software for ICD-9[16] which reduces the number of diagnosis code from approximately 14,000 to 283. Medication codes are grouped by Generic Product Identifier Drug Group[24] which reduces the dimension to from approximately 151,000 to 96. Procedure codes are grouped by Clinical Classifications Software for CPT[17], which reduces the number of CPT codes from approximately 9,000 to 238.\nA.3 Training Specifics of the Basline Models • LR: We use 0.01 L2 regularization coefficient for the logistic regression weight.\n• MLP: We use drop-out rate 0.6 on the output of the hidden layer. We use 0.0001 L2 regularization coefficient for the hidden layer weight and the logistic regression weight.\n• RNN: We use drop-out rate 0.6 on the outputs of both hidden layers. We use 0.0001 L2 regularization coefficient for the logistic regression weight. The dimension size of both hidden layers is 256.\n• RNN+αM : We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of the context vector ∑ i αivi. We use 0.0001 L2 regularization coefficient for the hidden layer weight of the\nMLP that generates α’s and the logistic regression weight. The dimension size of the hidden layers in both RNN and MLP is 256.\n• RNN+αR: We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of the context vector ∑ i αivi. We use 0.0001 L2 regularization coefficient for the hidden layer weight of the\nRNN that generates α’s and the logistic regression weight. The dimension size of the hidden layers in both RNNs is 256.\nA.4 Heart Failure Case/Control Selection Criteria Case patients were 40 to 85 years of age at the time of HF diagnosis. HF diagnosis (HFDx) is defined as: 1) Qualifying ICD-9 codes for HF appeared in the encounter records or medication orders. Qualifying ICD-9 codes are displayed in Table 3. 2) a minimum of three clinical encounters with qualifying ICD-9 codes had to occur within 12 months of each other, where the date of diagnosis was assigned to the earliest of the three dates. If the time span between the first and second appearances of the HF diagnostic code was greater than 12 months, the date of the second encounter was used as the first qualifying encounter. The date at which HF diagnosis was given to the case is denoted as HFDx. Up to ten eligible controls (in terms of sex, age, location) were selected for each case, yielding an overall ratio of 9 controls per case. Each control was also\nassigned an index date, which is the HFDx of the matched case. Controls are selected such that they did not meet the operational criteria for HF diagnosis prior to the HFDx plus 182 days of their corresponding case. Control subjects were required to have their first office encounter within one year of the matching HF case patient’s first office visit, and have at least one office encounter 30 days before or any time after the case’s HF diagnosis date to ensure similar duration of observations among cases and controls."
    }, {
      "heading" : "B Results on disease progression modeling",
      "text" : "Objective: Given a sequence of visits x1, . . . ,xT , the goal of DPM is, for each time step i, to predict the codes occurring at the next visit x2, . . . ,xT+1. However, as we are interested in the disease progression, we create a separate set of labels y1, . . . ,yT that do not contain non-diagnosis codes such as medication codes or procedure codes. Therefore yi will contain diagnosis codes from the next visit xi+1.\nDataset: We divide the entire dataset described in Table 1 into 0.75:0.10:0.15 ratio, respectively for training set, validation set, and test set.\nBaseline: We use the same baseline models we used for HF prediction. However, since we are predicting 283 binary labels now, we replace the logistic regression function with the Softmax function. The drop-out and L2 regularization policies remain the same.\nFor LR and MLP, at each step i, we aggregate maximum ten past input vectors3 xi−9, . . . ,xi to create a pseudo-context vector ĉi. LR applies the Softmax function on top of ĉi. MLP places a hidden layer on top of ĉi then applies the Softmax function.\nEvaluation metric: We use the negative log likelihood Eq (1) on the test set to evaluate the model performance. We also use Recall@k as an additional metric to measure the prediction accuracy.\n• Recall@k: Given a sequence of visits x1, . . . ,xT , we evaluate the model performance based on how 3We also tried aggregating all past input vectors x1, . . . ,xi, but the performance was slightly worse than using just ten.\nPrediction accuracy: Table 4 displays the prediction performance of RETAIN and the baselines. We use k = 5, 10 for Recall@k to allow a reasonable number of prediction trials, as well as cover complex patients who often receive multiple diagnosis codes at a single visit.\nRNN shows the best prediction accuracy for DPM. However, considering the purpose of DPM, which is to assist doctors to provide quality care for the patient, black-box behavior of RNN makes it unattractive as a clinical tool. On the other hand, RETAIN performs as well as other attention models, only slightly inferior to RNN, provides full interpretation of its prediction behavior, making it a feasible solution for clinical applications.\nThe interesting finding in Table 4 is that MLP is able to perform as accurately as RNN+αM in terms of Recall@10. Considering the fact that MLP uses aggregated information of past ten visits, we can assume that DPM depends more on the frequency of disease occurrences rather than the order in which they occurred. This is quite different from the HF prediction task, where stationary models (LF, MLP) performed significantly worse than sequential models.\nC Illustration and comparison of the baselines Figure 4 illustrates the baselines used in the experiments and shows the relationship among them."
    } ],
    "references" : [ {
      "title" : "Multiple object recognition with visual attention",
      "author" : [ "J. Ba", "V. Mnih", "K. Kavukcuoglu" ],
      "venue" : "ICLR",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "ICLR",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y. Bengio", "P. Simard", "P. Frasconi" ],
      "venue" : "Neural Networks, IEEE Transactions on, 5(2):157–166",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "Proceedings of SciPy",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The impact of ehealth on the quality and safety of health care: a systematic overview",
      "author" : [ "A.D. Black", "J. Car", "C. Pagliari", "C. Anandan", "K. Cresswell", "T. Bokun", "B. McKinstry", "R. Procter", "A. Majeed", "A. Sheikh" ],
      "venue" : "PLoS Med, 8(1):e1000387",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "author" : [ "R. Caruana", "Y. Lou", "J. Gehrke", "P. Koch", "M. Sturm", "N. Elhadad" ],
      "venue" : "KDD",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Systematic review: impact of health information technology on quality",
      "author" : [ "B. Chaudhry", "J. Wang", "S. Wu", "M. Maglione", "W. Mojica", "E. Roth", "S.C. Morton", "P.G. Shekelle" ],
      "venue" : "efficiency, and costs of medical care. Annals of internal medicine, 144(10):742–752",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Distilling knowledge from deep networks with applications to healthcare domain",
      "author" : [ "Z. Che", "S. Purushotham", "R. Khemani", "Y. Liu" ],
      "venue" : "arXiv preprint arXiv:1512.03542",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "EMNLP",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-layer representation learning for medical concepts",
      "author" : [ "E. Choi", "M.T. Bahadori", "E. Searles", "C. Coffey", "J. Sun" ],
      "venue" : "KDD",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Doctor ai: Predicting clinical events via recurrent neural networks",
      "author" : [ "E. Choi", "M.T. Bahadori", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1511.05942",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio" ],
      "venue" : "NIPS, pages 577–585",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Visualizing higher-layer features of a deep network",
      "author" : [ "D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : "University of Montreal",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Predicting clinical events by combining static and dynamic information using recurrent neural networks",
      "author" : [ "C. Esteban", "O. Staeck", "Y. Yang", "V. Tresp" ],
      "venue" : "arXiv preprint arXiv:1602.02685",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Clinical predictors of progression to Alzheimer disease in amnestic mild cognitive impairment",
      "author" : [ "A.S. Fleisher", "B.B. Sowell", "C. Taylor", "A.C. Gamst", "R.C. Petersen", "L.J. Thal", "f. t. A.D.C. Study" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Bringing cohort studies to the bedside: framework for a ’green button’ to support clinical decision-making",
      "author" : [ "B. Gallego", "S.R. Walter", "R.O. Day", "A.G. Dunn", "V. Sivaraman", "N. Shah", "C.A. Longhurst", "E. Coiera" ],
      "venue" : "Journal of Comparative Effectiveness Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Sequence learning with recurrent networks: analysis of internal representations",
      "author" : [ "J. Ghosh", "V. Karamcheti" ],
      "venue" : "Aerospace Sensing, pages 449–460. International Society for Optics and Photonics",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Costs and benefits of health information technology: new trends from the literature",
      "author" : [ "C.L. Goldzweig", "A. Towfigh", "M. Maglione", "P.G. Shekelle" ],
      "venue" : "Health affairs, 28(2):w282–w293",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra" ],
      "venue" : "arXiv preprint arXiv:1502.04623",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom" ],
      "venue" : "NIPS, pages 1684–1692",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, 9(8):1735–1780",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Information. Medi-span electronic drug file (med-file",
      "author" : [ "D.W.K. C" ],
      "venue" : "v2. http://www.wolterskluwercdi. com/drug-data/medi-span-electronic-drug-file/",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Use of electronic health records in us hospitals",
      "author" : [ "A.K. Jha", "C.M. DesRoches", "E.G. Campbell", "K. Donelan", "S.R. Rao", "T.G. Ferris", "A. Shields", "S. Rosenbaum", "D. Blumenthal" ],
      "venue" : "N Engl J Med",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Visualizing and understanding recurrent networks",
      "author" : [ "A. Karpathy", "J. Johnson", "F.-F. Li" ],
      "venue" : "arXiv preprint arXiv:1506.02078",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Use of diverse electronic medical record systems to identify genetic risk for type 2 diabetes within a genome-wide association study",
      "author" : [ "A.N. Kho", "M.G. Hayes", "L. Rasmussen-Torvik", "J.A. Pacheco", "W.K. Thompson", "L.L. Armstrong", "J.C. Denny", "P.L. Peissig", "A.W. Miller", "W.-Q. Wei", "S.J. Bielinski", "C.G. Chute", "C.L. Leibson", "G.P. Jarvik", "D.R. Crosslin", "C.S. Carlson", "K.M. Newton", "W.A. Wolf", "R.L. Chisholm", "W.L. Lowe" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Q.V. Le" ],
      "venue" : "ICASSP",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Q.V. Le", "N. Jaitly", "G.E. Hinton" ],
      "venue" : "arXiv preprint arXiv:1504.00941",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning to Diagnose with LSTM Recurrent Neural Networks",
      "author" : [ "Z.C. Lipton", "D.C. Kale", "C. Elkan", "R. Wetzell" ],
      "venue" : "ICLR",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "et al",
      "author" : [ "V. Mnih", "N. Heess", "A. Graves" ],
      "venue" : "Recurrent models of visual attention. In NIPS",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "A.M. Rush", "S. Chopra", "J. Weston" ],
      "venue" : "EMNLP",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning individual and population level traits from clinical temporal data",
      "author" : [ "S. Saria", "D. Koller", "A. Penn" ],
      "venue" : "NIPS, Predictive Models in Personalized Medicine workshop",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A probabilistic graphical model for individualizing prognosis in chronic",
      "author" : [ "P. Schulam", "S. Saria" ],
      "venue" : "complex diseases. In AMIA, volume 2015, page 143",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Supervised patient similarity measure of heterogeneous patient records",
      "author" : [ "J. Sun", "F. Wang", "J. Hu", "S. Edabollahi" ],
      "venue" : "ACM SIGKDD Explorations Newsletter, 14(1):16–24",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Show",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio" ],
      "venue" : "attend and tell: Neural image caption generation with visual attention. In ICML",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "M.D. Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20].",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 22,
      "context" : "Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20].",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20].",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20].",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "There is a common belief that one has to trade accuracy for interpretation in favor to simpler models [6].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 24,
      "context" : "via decision trees [27]), 2) case-based reasoning by finding similar patients (e.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "via k-nearest neighbors [18] and distance metric learning [35]), and 3) identifying a list of risk factors (e.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 32,
      "context" : "via k-nearest neighbors [18] and distance metric learning [35]), and 3) identifying a list of risk factors (e.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "via LASSO coefficients [15]).",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 30,
      "context" : "Latent-variable time-series models, such as [33, 34], do capture the temporality, but often have limited interpretation due to abstract state variables.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 31,
      "context" : "Latent-variable time-series models, such as [33, 34], do capture the temporality, but often have limited interpretation due to abstract state variables.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 27,
      "context" : "Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential EHR data to perform various predictive analysis such as learning to diagnose [30] and disease progression modeling [11, 14].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential EHR data to perform various predictive analysis such as learning to diagnose [30] and disease progression modeling [11, 14].",
      "startOffset" : 211,
      "endOffset" : 219
    }, {
      "referenceID" : 13,
      "context" : "Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential EHR data to perform various predictive analysis such as learning to diagnose [30] and disease progression modeling [11, 14].",
      "startOffset" : 211,
      "endOffset" : 219
    }, {
      "referenceID" : 16,
      "context" : "While there have been several attempts at directly interpreting RNNs [19, 26, 8], none of them provide the level of interpretation that can serve the healthcare applications.",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "While there have been several attempts at directly interpreting RNNs [19, 26, 8], none of them provide the level of interpretation that can serve the healthcare applications.",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "While there have been several attempts at directly interpreting RNNs [19, 26, 8], none of them provide the level of interpretation that can serve the healthcare applications.",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 27,
      "context" : "In the case of learning to diagnose (L2D) [30], the input vector xi consists of measurements (possibly continuous) collected by monitoring devices.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers).",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers).",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers).",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 26,
      "context" : "In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers).",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 0,
      "context" : "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].",
      "startOffset" : 138,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].",
      "startOffset" : 138,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].",
      "startOffset" : 138,
      "endOffset" : 153
    }, {
      "referenceID" : 33,
      "context" : "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].",
      "startOffset" : 138,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 19,
      "context" : "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 29,
      "context" : "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 11,
      "context" : "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 1,
      "context" : "The need for attention mechanism can be seen in the language translation task [2]: Representing the entire sentence with one fixed-size vector is inefficient; the neural translation machine usually finds it difficult to translate the given sentence represented by a single vector.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "We can easily choose a more sophisticated but still interpretable representation such as multilayer perceptron (MLP) [13, 28] which has been used for representation learning in EHR data [10].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "We can easily choose a more sophisticated but still interpretable representation such as multilayer perceptron (MLP) [13, 28] which has been used for representation learning in EHR data [10].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "We can easily choose a more sophisticated but still interpretable representation such as multilayer perceptron (MLP) [13, 28] which has been used for representation learning in EHR data [10].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 1,
      "context" : "Overall, our attention mechanism can be viewed as the inverted architecture of the standard attention mechanism for NLP [2] where the words are encoded using RNN and generate the attention weights using MLP.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "8 [4].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 34,
      "context" : "For training the model, we used Adadelta [37] with the mini-batch of 100 patients.",
      "startOffset" : 41,
      "endOffset" : 45
    } ],
    "year" : 2017,
    "abstractText" : "Accuracy and interpretation are two goals of any successful predictive models. Most existing works have to suffer the tradeoff between the two by either picking complex black box models such as recurrent neural networks (RNN) or relying on less accurate traditional models with better interpretation such as logistic regression. To address this dilemma, we present REverse Time AttentIoN model (RETAIN) for analyzing EHR data that achieves high accuracy while remaining clinically interpretable. RETAIN is a two-level neural attention model that can find influential past visits and significant clinical variables within those visits (e.g,. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that more recent clinical visits will likely get higher attention. Experiments on a large real EHR dataset of 14 million visits from 263K patients over 8 years confirmed the comparable predictive accuracy and computational scalability to the state-of-the-art methods such as RNN. Finally, we demonstrate the clinical interpretation with concrete examples from RETAIN.",
    "creator" : "LaTeX with hyperref package"
  }
}