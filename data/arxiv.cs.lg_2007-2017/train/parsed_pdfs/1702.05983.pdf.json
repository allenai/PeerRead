{
  "name" : "1702.05983.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN",
    "authors" : [ "Weiwei Hu", "Ying Tan" ],
    "emails" : [ "ytan}@pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, many machine learning based algorithms have been proposed to detect malware, which extract features from programs and use a classifier to classify programs between benign programs and malware. For example, Schultz et al. proposed to use DLLs, APIs and strings as features for classification [Schultz et al., 2001], while Kolter et al. used byte level N-Gram as features [Kolter and Maloof, 2004; Kolter and Maloof, 2006].\nMost researchers focused their efforts on improving the detection performance (e.g. true positive rate, accuracy and AUC) of such algorithms, but ignored the robustness of these algorithms. Generally speaking, the propagation of malware will benefit malware authors. Therefore, malware authors have sufficient motivation to attack malware detection algorithms.\n∗Prof. Ying Tan is the corresponding author.\nMany machine learning algorithms are very vulnerable to intentional attacks. Machine learning based malware detection algorithms cannot be used in real-world applications if they are easily to be bypassed by some adversarial techniques.\nRecently, adversarial examples of deep learning models have attracted the attention of many researchers. Szegedy et al. added imperceptible perturbations to images to maximize a trained neural network’s classification errors, making the network unable to classify the images correctly [Szegedy et al., 2013]. The examples after adding perturbations are called adversarial examples. Goodfellow et al. proposed a gradient based algorithm to generate adversarial examples [Goodfellow et al., 2014b]. Papernot et al. used the Jacobian matrix to determine which features to modify when generating adversarial examples [Papernot et al., 2016c]. The Jacobian matrix based approach is also a kind of gradient based algorithm.\nGrosse et al. proposed to use the gradient based approach to generate adversarial Android malware examples [Grosse et al., 2016]. The adversarial examples are used to fool a neural network based malware detection model. They assumed that attackers have full access to the parameters of the malware detection model. For different sizes of neural networks, the misclassification rates after adversarial crafting range from 40% to 84%.\nIn some cases, attackers have no access to the architecture and weights of the neural network to be attacked; the target model is a black box to attackers. Papernot et al. used a substitute neural network to fit the black-box neural network and then generated adversarial examples according to the substitute neural network [Papernot et al., 2016b]. They also used a substitute neural network to attack other machine learning algorithms such as logistic regression, support vector machines, decision trees and nearest neighbors [Papernot et al., 2016a]. Liu et al. performed black-box attacks without a substitute model [Liu et al., 2016], based on the principle that adversarial examples can transfer among different models [Szegedy et al., 2013].\nMachine learning based malware detection algorithms are usually integrated into antivirus software or hosted on the cloud side, and therefore they are black-box systems to malware authors. It is hard for malware authors to know which classifier a malware detection system uses and the parameters of the classifier.\nHowever, it is possible to figure out what features a mal-\nar X\niv :1\n70 2.\n05 98\n3v 1\n[ cs\n.L G\n] 2\n0 Fe\nb 20\n17\nware detection algorithm uses by feeding some carefully designed test cases to the black-box algorithm. For example, if a malware detection algorithm uses static DLL or API features from the import directory table or the import lookup tables of PE programs [Microsoft, 2013], malware authors can manually modify some DLL or API names in the import directory table or the import lookup tables. They can modify a benign program’s DLL or API names to malware’s DLL or API names, and vice versa. If the detection results change after most of the modifications, they can judge that the malware detection algorithm uses DLL or API features. Therefore, in this paper we assume that malware authors are able to know what features a malware detection algorithm uses, but know nothing about the machine learning model.\nExisting algorithms mainly use gradient information and hand-crafted rules to transform original samples into adversarial examples. This paper proposes a generative neural network based approach which takes original samples as inputs and outputs adversarial examples. The intrinsic non-linear structure of neural networks enables them to generate more complex and flexible adversarial examples to fool the target model.\nThe learning algorithm of our proposed model is inspired by generative adversarial networks (GAN) [Goodfellow et al., 2014a]. In GAN, a discriminative model is used to distinguish between generated samples and real samples, and a generative model is trained to make the discriminative model misclassify generated samples as real samples. GAN has shown good performance in generating realistic images[Mirza and Osindero, 2014; Denton et al., 2015].\nThe proposed model in this paper is named as MalGAN, which generates adversarial examples to attack black-box malware detection algorithms. A substitute detector is trained to fit the black-box malware detection algorithm, and a generative network is used to transform malware samples into adversarial examples. Experimental results show that almost all of the adversarial examples generated by MalGAN successfully bypass the detection algorithms and MalGAN is very flexible to fool further defensive methods of detection algorithms."
    }, {
      "heading" : "2 Architecture of MalGAN",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "The architecture of proposed MalGAN is shown in Figure 1.\nThe black-box detector is an external system which adopts machine learning based malware detection algorithms. We assume that the only thing malware authors know about the black-box detector is what kind of features it uses. Malware authors do not know what machine learning algorithm it uses and do not have access to the parameters of the trained model. Malware authors are able to get the detection results of their programs from the black-box detector. The whole model contains a generator and a substitute detector, which are both feed-forward neural networks. The generator and the substitute detector work together to attack a machine learning based black-box malware detector.\nIn this paper we only generate adversarial examples for binary features, because binary features are widely used by\nmalware detection researchers and are able to result in high detection accuracy. Here we take API feature as an example to show how to represent a program. If M APIs are used as features, an M -dimensional feature vector is constructed for a program. If the program calls the d-th API, the d-th feature value is set to 1, otherwise it is set to 0.\nThe main difference between this model and existing algorithms is that the adversarial examples are dynamically generated according to the feedback of the black-box detector, while most existing algorithms use static gradient based approaches to generate adversarial examples.\nThe probability distribution of adversarial examples from MalGAN is determined by the weights of the generator. To make a machine learning algorithm effective, the samples in the training set and the test set should follow the same probability distribution or similar probability distributions. While the generator can change the probability distribution of adversarial examples to make it far from the probability distribution of the black-box detector’s training set. In this case the generator has sufficient opportunity to lead the black-box detector to misclassify malware as benign."
    }, {
      "heading" : "2.2 Generator",
      "text" : "The generator is used to transform a malware feature vector into its adversarial version. It takes the concatenation of a malware feature vector m and a noise vector z as input. m is a M -dimensional binary vector. Each element of m corresponds to the presence or absence of a feature. z is a Z-dimensional vector, where Z is a hyper-parameter. Each element of z is a random number sampled from a uniform distribution in the range [0, 1). The effect of z is to allow the generator to generate diverse adversarial examples from a single malware feature vector.\nThe input vector is fed into a multi-layer feed-forward neural network with weights θg . The output layer of this network has M neurons and the activation function used by the last layer is sigmoid which restricts the output to the range (0, 1). The output of this network is denoted as o. Since malware feature values are binary, binarization transformation is applied to o according to whether an element is greater than 0.5 or not, and this process produces a binary vector o′.\nWhen generating adversarial examples for binary malware features we only consider to add some irrelevant features to malware. Removing a feature from the original malware may crack it. For example, if the “WriteFile” API is removed from a program, the program is unable to perform normal writing function and the malware may crack. The non-zero elements of the binary vector o′ act as the irrelevant features to be added to the original malware. The final generated adversarial example can be expressed as m′ = m|o′ where “|” is element-wise binary OR operation.\nm′ is a binary vector, and therefore the gradients are unable to back propagate from the substitute detector to the generator. A smooth function G is defined to receive gradient information from the substitute detector, as shown in Formula 1.\nGθg (m, z) = max (m,o) . (1)\nmax (· , · ) represents element-wise max operation. If an element of m has the value 1, the corresponding result of G is also 1, which is unable to back propagate the gradients. If an element of m has the value 0, the result of G is the neural network’s real number output in the corresponding dimension, and gradient information is able to go through. It can be seen that m′ is actually the binarization transformed version of Gθg (m, z)."
    }, {
      "heading" : "2.3 Substitute Detector",
      "text" : "Since malware authors know nothing about the detailed structure of the black-box detector, the substitute detector is used to fit the black-box detector and provides gradient information to train the generator.\nThe substitute detector is a multi-layer feed-forward neural network with weights θd which takes a program feature vector x as input. It classifies the program between benign program and malware. We denote the predicted probability that x is malware as Dθd(x).\nThe training data of the substitute detector consist of adversarial malware examples from the generator, and benign programs from an additional benign dataset collected by malware authors. The ground-truth labels of the training data are not used to train the substitute detector. The goal of the substitute detector is to fit the black-box detector. The black-box detector will detect this training data first and output whether a program is benign or malware. The predicted labels from the black-box detector are used by the substitute detector."
    }, {
      "heading" : "3 Training MalGAN",
      "text" : "To train MalGAN malware authors should collect a malware dataset and a benign dataset first.\nThe loss function of the substitute detector is defined in Formula 2."
    }, {
      "heading" : "LD =− Ex∈BBBenign log (1−Dθd(x))",
      "text" : "− Ex∈BBMalware logDθd (x).\n(2)\nBBBenign is the set of programs that are recognized as benign by the black-box detector, and BBMalware is the set\nof programs that are detected as malware by the black-box detector.\nTo train the substitute detector, LD should be minimized with respect to the weights of the substitute detector.\nThe loss function of the generator is defined in Formula 3.\nLG = Em∈SMalware,z∼puniform[0,1) logDθd ( Gθg (m, z) ) .\n(3) SMalware is the actual malware dataset, not the malware set labelled by the black-box detector. LG is minimized with respect to the weights of the generator.\nMinimizing LG will reduce the predicted malicious probability of malware and push the substitute detector to recognize malware as benign. Since the substitute detector tries to fit the black-box detector, the training of the generator will further fool the black-box detector.\nThe whole process of training MalGAN is shown in Algorithm 1.\nAlgorithm 1 The Training Process of MalGAN 1: while not converging do 2: Sample a minibatch of malware M 3: Generate adversarial examples M ′ from the genera-\ntor for M 4: Sample a minibatch of benign programs B 5: Label M ′ and B using the black-box detector 6: Update the substitute detector’s weights θd by de-\nscending along the gradient∇θdLD 7: Update the generator’s weights θg by descending\nalong the gradient∇θgLG 8: end while\nIn line 2 and line 4, different sizes of minibatches are used for malware and benign programs. The ratio of M ’s size to B’s size is the same as the ratio of the malware dataset’s size to the benign dataset’s size."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "The dataset used in this paper was crawled from a program sharing website1. We downloaded 180 thousand programs from this website and about 30% of them are malware. API features are used in this paper. An 160-dimensional binary feature vector is construct for each program, based on 160 system level APIs.\nIn order to validate the transferability of adversarial examples generated by MalGAN, we tried several different machine learning algorithms for the black-box detector. The used classifiers include random forest (RF), logistic regression (LR), decision trees (DT), support vector machines (SVM), multi-layer perceptron (MLP), and a voting based ensemble of these classifiers (VOTE).\nWe adopted two ways to split the dataset. The first splitting way regards 80% of the dataset as the training set and the remaining 20% as the test set. MalGAN and the black-box detector share the same training set. MalGAN further picks out 25% of the training data as the validation set and uses the remaining training data to train the neural networks. Some black-box classifiers such as MLP also need a validation set for early stopping. The validation set of MalGAN cannot be used for the black-box detector since malware authors and antivirus vendors do not communicate on how to split dataset. Splitting validation set for the black-box detector should be independent of MalGAN; MalGAN and the black-box detector should use different random seeds to pick out the validation data.\nThe second splitting way picks out 40% of the dataset as the training set for MalGAN, picks out another 40% of the dataset as the training set for the black-box detector, and uses the remaining 20% of the dataset as the test set.\nIn real-world scenes the training data collected by the malware authors and the antivirus vendors cannot be the same. However, their training data will overlap with each other if they collect data from public sources. In this case the actual performance of MalGAN will be between the performances of the two splitting ways.\nAdam [Kingma and Ba, 2014] was chosen as the optimizer. We tuned the hyper-parameters on the validation set. 10 was chosen as the dimension of the noise vector z. The generator’s layer size was set to 170-256-160, the substitute detector’s layer size was set to 160-256-1, and the learning rate 0.001 was used for both the generator and the substitute detector. The maximum number of epochs to train MalGAN was set to 100. The epoch with the lowest detection rate on the validation set is finally chosen to test the performance of MalGAN."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "We first analyze the case where MalGAN and the black-box detector use the same training set. For malware detection, the true positive rate (TPR) means the detection rate of malware. After adversarial attacks, the reduction in TPR can reflect how many malware samples successfully bypass the detection algorithm. TPR on the training set and the test set of\n1https://malwr.com/\noriginal samples and adversarial examples is shown in Table 1.\nFor random forest and decision trees, the TPRs on adversarial examples range from 0.16% to 0.20% for both the training set and the test set, while the TPRs on the original samples are all greater than 93%. When using other classifiers as the black-box detector, MalGAN is able to decrease the TPR on generated adversarial examples to zero for both the training set and the test set. That is to say, for all of the backend classifiers, the black-box detector can hardly detect any malware generated by the generator. The proposed model has successfully learned to bypass these machine learning based malware detection algorithms.\nThe structures of logistic regression and support vector machines are very similar to neural networks and MLP is actually a neural network. Therefore, the substitute detector is able to fit them with a very high accuracy. This is why MalGAN can achieve zero TPR for these classifiers. While random forest and decision trees have quite different structures from neural networks so that MalGAN results in non-zero TPRs. The TPRs of random forest and decision trees on adversarial examples are still quite small, which means the neural network has enough capacity to represent other models with quite different structures. The voting of these algorithms also achieves zero TPR. We can conclude that the classifiers with similar structures to neural networks are in the majority during voting.\nThe convergence curve of TPR on the training set and the validation set during the training process of MalGAN is shown in Figure 2. The black-box detector used here is random forest, since random forest performs very well in Table 1.\nTPR converges to about zero near the 40th epoch, but the convergence curve is a bit shaking, not a smooth one. This curve reflects the fact that the training of GAN is usually unstable. How to stabilize the training of GAN have attracted the attention of many researchers [Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017].\nNow we will analyze the results when MalGAN and the black-box detector are trained on different training sets. Fitting the black-box detector trained on a different dataset is more difficult for the substitute detector. The experimental results are shown in Table 2.\nFor SVM, MLP and VOTE, TPR reaches zero, and TPR of LR is nearly zero. These results are very similar to Table 1. TPRs of random forest and decision trees on adversarial examples become higher compared with the case where MalGAN and the black-box detector use the same training data. For decision trees the TPRs rise to 2.18% and 2.11% on the training set and the test set respectively. However, 2% is still a very small number and the black-box detector will still miss to detect most of the adversarial malware examples. It can be concluded that MalGAN is still able to fool the black-box detector even trained on a different training set."
    }, {
      "heading" : "4.3 Comparison with the Gradient based Algorithm to Generate Adversarial Examples",
      "text" : "Existing algorithms of generating adversarial examples are mainly for images. The difference between image and malware is that image features are continuous while malware features are binary.\nGrosse et al. modified the traditional gradient based algorithm to generate binary adversarial malware examples [Grosse et al., 2016]. They did not regard the malware detection algorithm as a black-box system and assumed that malware authors have full access to the architecture and the weights of the neural network based malware detection model. The misclassification rates of adversarial examples range from 40% to 84% under different hyper-parameters. This gradient based approach under white-box assumption is unable to generate adversarial examples with zero TPR, while MalGAN produces nearly zero TPR with a harder black-box assumption.\nTheir algorithm uses an iterative approach to generate adversarial malware examples. At each iteration the algorithm finds the feature with the maximum likelihood to change the malware’s label from malware to benign. The algorithm modifies one feature at each iteration, until the malware is successfully classified as a benign program or there are no features available to be modified.\nWe tried to migrate this algorithm to attack a random forest based black-box detection algorithm. A substitute neural network is trained to fit the black-box random forest. Adversarial malware examples are generated based on the gradient information of the substitute neural network.\nTPR on the adversarial examples over the iterative process is shown in Figure 3. Please note that at each iteration not all of the malware samples are modified. If a malware sample has already been classified as a benign program at previous iterations or there are no modifiable features, the algorithm will do nothing on the malware sample at this iteration.\nOn the training set and the test set, TPR converges to 93.52% and 90.96% respectively. In this case the black-box random forest is able to detect most of the adversarial examples. The substitute neural network is trained on the original training set, while after several iterations the probability distribution of adversarial examples will become quite different from the probability distribution of the original training set. Therefore, the substitute neural network cannot approximate the black-box random forest well on the adversarial examples. In this case the adversarial examples generated from\nthe substitute neural network are unable to fool the black-box random forest.\nIn order to fit the black-box random forest more accurately on the adversarial examples, we tried to retraining the substitute neural network on the adversarial examples. At each iteration, the current generated adversarial examples from the whole training set are used to retrain the substitute neural network. As shown in Figure 3, the retraining approach make TPR converge to 46.18% on the training set, which means the black-box random forest can still detect about half of the adversarial examples. However, the retrained model is unable to generalize to the test set, sine the TPR on the test set converges to 90.12%. The odd probability distribution of these adversarial examples limits the generalization ability of the substitute neural network.\nMalGAN uses a generative network to transform original samples into adversarial samples. The neural network has enough representation ability to perform complex transformations, making MalGAN able to result in nearly zero TPR on both the training set and the test set. While the representation ability of the gradient based approach is too limited to generate high-quality adversarial examples."
    }, {
      "heading" : "4.4 Retraining the Black-Box Detector",
      "text" : "Several defensive algorithms have been proposed to deal with adversarial examples. Gu et al. proposed to use autoencoders to map adversarial samples to clean input data [Gu and Rigazio, 2014]. An algorithm named defensive distillation was proposed by Papernot et al. to weaken the effectiveness of adversarial perturbations [Papernot et al., 2016d]. Li et al. found that adversarial retraining can boost the robustness of machine learning algorithms [Li et al., 2016]. Chen et al. compared these defensive algorithms and concluded that retraining is a very effective way to defend against adversarial examples, and is robust even against repeated attacks [Chen et al., 2016].\nIn this section we will analyze the performance of MalGAN under the retraining based defensive approach. If an-\ntivirus vendors collect enough adversarial malware examples, the can retrain the black-box detector on these adversarial examples in order to learn their patterns and detect them. Here we only use random forest as the black-box detector due to its good performance. After retraining the black-box detector it is able to detect all adversarial examples, as shown in the middle column of Table 3.\nHowever, once antivirus vendors release the updated blackbox detector publicly, malware authors will be able to get a copy of it and retrain MalGAN to attack the new black-box detector. After this process the black-box detector can hardly detect any malware again, as shown in the last column of Table 3. We found that reducing TPR from 100% to 0% can be done within one epoch during retraining MalGAN. We alternated retraining the black-box detector and retraining MalGAN for ten times. The results are the same as Table 3 for the ten times.\nTo retrain the black-box detector antivirus vendors have to collect enough adversarial examples. It is a long process to collect a large number of malware samples and label them. Adversarial malware examples have enough time to propagate before the black-box detector is retrained and updated. Once the black-box detector is updated, malware authors will attack it immediately by retraining MalGAN and our experiments showed that retraining takes much less time than the first-time training. After retraining MalGAN, new adversarial examples remain undetected. This dynamic adversarial process lands antivirus vendors in a passive position. Machine learning based malware detection algorithms can hardly work in this case."
    }, {
      "heading" : "5 Conclusions",
      "text" : "This paper proposed a novel algorithm named MalGAN to generate adversarial examples from a machine learning based black-box malware detector. A neural network based substitute detector is used to fit the black-box detector. A generator is trained to generate adversarial examples which are able to fool the substitute detector. Experimental results showed that the generated adversarial examples are able to effectively bypass the black-box detector.\nAdversarial examples’ probability distribution is controlled by the weights of the generator. Malware authors are able to frequently change the probability distribution by retraining MalGAN, making the black-box detector cannot keep up with it, and unable to learn stable patterns from it. Once the black-box detector is updated malware authors can immediately crack it. This process making machine learning based malware detection algorithms unable to work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the Natural Science Foundation of China (NSFC) under grant no. 61375119 and the Beijing Natural Science Foundation under grant no. 4162029, and partially supported by National Key Basic Research Development Plan (973 Plan) Project of China under grant no. 2015CB352302."
    } ],
    "references" : [ {
      "title" : "In NIPS 2016 Workshop on Adversarial Training",
      "author" : [ "Martin Arjovsky", "Léon Bottou. Towards principled methods for training generative adversarial networks" ],
      "venue" : "review for ICLR, volume 2016,",
      "citeRegEx" : "Arjovsky and Bottou. 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "and Yevgeniy Vorobeychik",
      "author" : [ "Xinyun Chen", "Bo Li" ],
      "venue" : "Evaluation of defensive methods for dnns against multiple adversarial evasion models.",
      "citeRegEx" : "Chen et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "et al",
      "author" : [ "Emily L Denton", "Soumith Chintala", "Rob Fergus" ],
      "venue" : "Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pages 1486–1494,",
      "citeRegEx" : "Denton et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "In Advances in neural information processing systems",
      "author" : [ "Ian Goodfellow", "Jean PougetAbadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio. Generative adversarial nets" ],
      "venue" : "pages 2672–2680,",
      "citeRegEx" : "Goodfellow et al.. 2014a",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572,",
      "citeRegEx" : "Goodfellow et al.. 2014b",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adversarial perturbations against deep neural networks for malware classification",
      "author" : [ "Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel" ],
      "venue" : "arXiv preprint arXiv:1606.04435,",
      "citeRegEx" : "Grosse et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards deep neural network architectures robust to adversarial examples",
      "author" : [ "Shixiang Gu", "Luca Rigazio" ],
      "venue" : "arXiv preprint arXiv:1412.5068,",
      "citeRegEx" : "Gu and Rigazio. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "pages 470–478",
      "author" : [ "Jeremy Z Kolter", "Marcus A Maloof. Learning to detect malicious executables in the wild. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery", "data mining" ],
      "venue" : "ACM,",
      "citeRegEx" : "Kolter and Maloof. 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The Journal of Machine Learning Research",
      "author" : [ "J Zico Kolter", "Marcus A Maloof. Learning to detect", "classify malicious executables in the wild" ],
      "venue" : "7:2721–2744,",
      "citeRegEx" : "Kolter and Maloof. 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A general retraining framework for scalable adversarial classification",
      "author" : [ "Bo Li", "Yevgeniy Vorobeychik", "Xinyun Chen" ],
      "venue" : "arXiv preprint arXiv:1604.02606,",
      "citeRegEx" : "Li et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Delving into transferable adversarial examples and black-box attacks",
      "author" : [ "Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Song" ],
      "venue" : "arXiv preprint arXiv:1611.02770,",
      "citeRegEx" : "Liu et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Conditional generative adversarial nets",
      "author" : [ "Mehdi Mirza", "Simon Osindero" ],
      "venue" : "arXiv preprint arXiv:1411.1784,",
      "citeRegEx" : "Mirza and Osindero. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow" ],
      "venue" : "arXiv preprint arXiv:1605.07277,",
      "citeRegEx" : "Papernot et al.. 2016a",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Practical black-box attacks against deep learning systems using adversarial examples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "arXiv preprint arXiv:1602.02697,",
      "citeRegEx" : "Papernot et al.. 2016b",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "Papernot et al", "2016c] Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "In Security and Privacy (EuroS&P),",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Distillation as a defense to adversarial perturbations against deep neural networks",
      "author" : [ "Papernot et al", "2016d] Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami" ],
      "venue" : "In Security and Privacy (SP),",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "In Advances in Neural Information Processing Systems",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen. Improved techniques for training gans" ],
      "venue" : "pages 2226– 2234,",
      "citeRegEx" : "Salimans et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "2001",
      "author" : [ "Matthew G Schultz", "Eleazar Eskin", "Erez Zadok", "Salvatore J Stolfo. Data mining methods for detection of new malicious executables. In Security", "Privacy" ],
      "venue" : "S&P 2001. Proceedings. 2001 IEEE Symposium on, pages 38–49. IEEE,",
      "citeRegEx" : "Schultz et al.. 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199,",
      "citeRegEx" : "Szegedy et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "proposed to use DLLs, APIs and strings as features for classification [Schultz et al., 2001], while Kolter et al.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "used byte level N-Gram as features [Kolter and Maloof, 2004; Kolter and Maloof, 2006].",
      "startOffset" : 35,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "used byte level N-Gram as features [Kolter and Maloof, 2004; Kolter and Maloof, 2006].",
      "startOffset" : 35,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "added imperceptible perturbations to images to maximize a trained neural network’s classification errors, making the network unable to classify the images correctly [Szegedy et al., 2013].",
      "startOffset" : 165,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "proposed a gradient based algorithm to generate adversarial examples [Goodfellow et al., 2014b].",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "proposed to use the gradient based approach to generate adversarial Android malware examples [Grosse et al., 2016].",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "used a substitute neural network to fit the black-box neural network and then generated adversarial examples according to the substitute neural network [Papernot et al., 2016b].",
      "startOffset" : 152,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : "They also used a substitute neural network to attack other machine learning algorithms such as logistic regression, support vector machines, decision trees and nearest neighbors [Papernot et al., 2016a].",
      "startOffset" : 178,
      "endOffset" : 202
    }, {
      "referenceID" : 11,
      "context" : "performed black-box attacks without a substitute model [Liu et al., 2016], based on the principle that adversarial examples can transfer among different models [Szegedy et al.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 20,
      "context" : ", 2016], based on the principle that adversarial examples can transfer among different models [Szegedy et al., 2013].",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "The learning algorithm of our proposed model is inspired by generative adversarial networks (GAN) [Goodfellow et al., 2014a].",
      "startOffset" : 98,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "GAN has shown good performance in generating realistic images[Mirza and Osindero, 2014; Denton et al., 2015].",
      "startOffset" : 61,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "GAN has shown good performance in generating realistic images[Mirza and Osindero, 2014; Denton et al., 2015].",
      "startOffset" : 61,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "Adam [Kingma and Ba, 2014] was chosen as the optimizer.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "How to stabilize the training of GAN have attracted the attention of many researchers [Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017].",
      "startOffset" : 86,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "How to stabilize the training of GAN have attracted the attention of many researchers [Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017].",
      "startOffset" : 86,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "How to stabilize the training of GAN have attracted the attention of many researchers [Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017].",
      "startOffset" : 86,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "modified the traditional gradient based algorithm to generate binary adversarial malware examples [Grosse et al., 2016].",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "proposed to use autoencoders to map adversarial samples to clean input data [Gu and Rigazio, 2014].",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "found that adversarial retraining can boost the robustness of machine learning algorithms [Li et al., 2016].",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "compared these defensive algorithms and concluded that retraining is a very effective way to defend against adversarial examples, and is robust even against repeated attacks [Chen et al., 2016].",
      "startOffset" : 174,
      "endOffset" : 193
    } ],
    "year" : 2017,
    "abstractText" : "Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms.Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples’ malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.",
    "creator" : "LaTeX with hyperref package"
  }
}