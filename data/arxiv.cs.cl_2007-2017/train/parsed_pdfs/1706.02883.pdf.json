{
  "name" : "1706.02883.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Overview of the NLPCC 2017 Shared Task: Chinese News Headline Categorization",
    "authors" : [ "Xipeng Qiu", "Jingjing Gong", "Xuanjing Huang" ],
    "emails" : [ "xjhuang}@fudan.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Task Definition",
      "text" : "This task aims to evaluate the automatic classification techniques for very short texts, i.e., Chinese news headlines. Each news headline (i.e., news title) is required to be classified into one or more predefined categories. With the rise of Internet and social media, the text data on the web is growing exponentially. Make a human being to analysis all those data is impractical, while machine learning techniques suits perfectly for this kind of tasks. after all, human brain capacity is too limited and precious for tedious and non-obvious phenomenons.\nFormally, the task is defined as follows: given a news headline x = (x1, x2, ..., xn), where xj represents jth word in x, the object is to find its possible category or label c ∈ C. More specifically, we need to find a function to predict in which category does x belong to.\nc∗ = arg max c∈C f(x; θc), (1)\nwhere θ is the parameter for the function."
    }, {
      "heading" : "2 Data",
      "text" : "We collected news headlines (titles) from several Chinese news websites, such as toutiao, sina, and so on.\nThere are 18 categories in total. The detailed information of each category is shown in Table 1. All the sentences are segmented by using the python Chinese segmentation tool jieba.\nSome samples from training dataset are shown in Table 2.\nLength Figure 1 shows that most of title sentence character number is less than 40, with a mean of 21.05. Title sentence word length is even shorter, most of which is less than 20 with a mean of 12.07.\nThe dataset is released on github https: //github.com/FudanNLP/nlpcc2017_news_headline_ categorization along with code that implement three basic models."
    }, {
      "heading" : "3 Evaluation",
      "text" : "We use the macro-averaged precision, recall and F1 to evaulate the performance.\nThe Macro Avg. is defined as follow:\nMacro avg = 1\nm m∑ i=1 ρi\nAnd Micro Avg. is defined as:\nMicro avg = 1\nN m∑ i=1 wiρi\nar X\niv :1\n70 6.\n02 88\n3v 1\n[ cs\n.C L\n] 9\nJ un\n2 01\n7\nTable 2: Samples from dataset. The first column is Category and the second column is news headline.\nFigure 1: The blue line is character length statistic, and blue line is word length.\nCategory Size Avg. Chars Avg. Words\ntrain 156000 22.06 13.08 dev. 36000 22.05 13.09 test 36000 22.05 13.08\nTable 3: Statistical information of the dataset.\nWhere m denotes the number of class, in the case of this dataset is 18. ρi is the accuracy of ith category, wi represents how many test examples reside in ith category, N is total number of examples in the test set."
    }, {
      "heading" : "4 Baseline Implementations",
      "text" : "As a branch of machine learning, Deep Learning (DL) has gained much attention in recent years due to its prominent achievement in several domains such as Computer vision and Natural Language processing.\nWe have implemented some basic DL models such as neural bag-of-words (NBoW), convolutional neural networks (CNN) [Kim, 2014] and Long short-term memory network (LSTM) [Hochreiter and Schmidhuber, 1997].\nEmpirically, 2 Gigabytes of GPU Memory should be sufficient for most models, set batch to a smaller number if not."
    }, {
      "heading" : "5 Participants Submitted Results",
      "text" : "Participant Macro P Macro R Macro F Accu.\nP1 0.831 0.829 0.830 0.829 P2 0.828 0.825 0.826 0.825 P3 0.818 0.814 0.816 0.814 P4 0.816 0.809 0.813 0.809 P5 0.812 0.809 0.810 0.809 P6 0.811 0.807 0.809 0.807 P7 0.809 0.804 0.806 0.804 P8 0.806 0.802 0.804 0.802 P9 0.803 0.800 0.802 0.800 P10 0.805 0.800 0.802 0.800 P11 0.799 0.798 0.798 0.798 P12 0.797 0.795 0.796 0.795 P13 0.793 0.789 0.791 0.789 P14 0.791 0.789 0.790 0.789 P15 0.792 0.787 0.789 0.786 P16 0.786 0.783 0.785 0.783 P17 0.778 0.775 0.777 0.775 P18 0.785 0.775 0.780 0.775 P19 0.785 0.775 0.780 0.775 P20 0.766 0.765 0.765 0.765 P21 0.768 0.759 0.764 0.759 P22 0.768 0.748 0.758 0.748 P23 0.744 0.729 0.736 0.729 P24 0.729 0.726 0.728 0.726 P25 0.745 0.700 0.722 0.700 P26 0.734 0.688 0.710 0.688 P27 0.698 0.685 0.691 0.685 P28 0.640 0.633 0.637 0.633 P29 0.645 0.629 0.637 0.629 P30 0.437 0.430 0.433 0.430 P31 0.474 0.399 0.433 0.399 P32 0.053 0.056 0.054 0.056\nTable 5: Results submitted by participants.\nThere are 32 participants actively participate and submit they predictions on the test set. The predictions are evaluated and the results are shown in table 5."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Since large amount of data is required for Machine Learning techniques like Deep Learning, we have collected considerable amount of News headline data and contributed to the research community."
    } ],
    "references" : [ {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber", "1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "arXiv preprint arXiv:1408.5882,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "We have implemented some basic DL models such as neural bag-of-words (NBoW), convolutional neural networks (CNN) [Kim, 2014] and Long short-term memory network (LSTM) [Hochreiter and Schmidhuber, 1997].",
      "startOffset" : 113,
      "endOffset" : 124
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we give an overview for the shared task at the CCF Conference on Natural Language Processing & Chinese Computing (NLPCC 2017): Chinese News Headline Categorization. The dataset of this shared task consists 18 classes, 12,000 short texts along with corresponded labels for each class. The dataset and example code can be accessed at https://github.com/FudanNLP/ nlpcc2017_news_headline_categorization.",
    "creator" : "LaTeX with hyperref package"
  }
}