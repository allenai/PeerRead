{
  "name" : "1610.09982.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sentiment Analysis of Review Datasets using Naïve Bayes’ and K-NN Classifier",
    "authors" : [ "Lopamudra Dey", "Sanjay Chakraborty", "Anuraag Biswas", "Beepa Bose", "Sweta Tiwari" ],
    "emails" : [ "lopamudra.dey@heritageit.edu", "sanjay.chakraborty@iemcal.com", "anuraagbiswas111@gmail.com", "beepabose@gmail.com", "sweta.tiwari604@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Such content is often found in social media web sites in the\nform of movie or product reviews, user comments,\ntestimonials, messages in discussion forums etc. Timely\ndiscovery of the sentimental or opinionated web content has\na number of advantages, the most important of all being\nmonetization. Understanding of the sentiments of human\nmasses towards different entities and products enables\nbetter services for contextual advertisements,\nrecommendation systems and analysis of market trends.\nThe focus of our project is sentiment focussed web crawling\nframework to facilitate the quick discovery of sentimental\ncontents of movie reviews and hotel reviews and analysis of\nthe same. We use statistical methods to capture elements of\nsubjective style and the sentence polarity. The paper\nelaborately discusses two supervised machine learning\nalgorithms: K-Nearest Neighbour(K-NN) and Naïve Bayes’\nand compares their overall accuracy, precisions as well as\nrecall values. It was seen that in case of movie reviews Naïve\nBayes’ gave far better results than K-NN but for hotel\nreviews these algorithms gave lesser, almost same\naccuracies.\nIndex Terms —Sentiment Analysis, Naïve Bayes’, K-NN, Supervised Machine Learning, Text Mining.\nI. INTRODUCTION\nData mining is a process of mined valuable data from a large set of data. Several analysis tools of data mining (like, clustering, classification, regression etc,) can be used for sentiment analysis task [13][14]. Sentiment mining is one of the important aspects of data mining\nwhere important data can be mined based on the positive or negative senses of the collected data. Sentiment Analysis also known as Opinion Mining refers to the use of natural language processing, text analysis and computational linguistic to identify and extract subjective information in source materials. Here the source materials refer to opinions / reviews /comments given in various social networking sites [1].The Sentiment found within comments, feedback or critiques provide useful indicators for many different purposes and can be categorized by polarity [2].By polarity we tend to find out if a review is overall a positive one or a negative one. For example:\n1) Positive Sentiment in subjective sentence: “I\nloved the movie Mary Kom”—This sentence is expressed positive sentiment about the movie Mary Kom and we can decide that from the sentiment threshold value of word “loved”. So, threshold value of word “loved” has positive numerical threshold value.\n2) Negative sentiment in subjective sentences:\n“Phata poster nikla hero is a flop movie” defined sentence is expressed negative sentiment about the movie named “Phata poster nikla hero” and we can decide that from the sentiment threshold value of word “flop”. So, threshold value of word “flop” has negative numerical threshold value. Sentiment Analysis is of three different types: Document level, Sentence level and Entity level. However we are studying phrase level sentiment analysis. The traditional text mining concentrates on analysis of facts whereas opinion mining deals with the attitudes [3]. The main fields of research are sentiment classification, feature based sentiment classification and opinion summarizing. Now, the use of sentiment analysis in a commercial environment is growing. This is evident in the increasing number of\nbrand tracking and marketing companies offering this service. Some services include:\n- Tracking users and non-users opinions and ratings on\nproducts and services.\n- Monitoring issues confronting the company so as\nto prevent viral effects.\n- Assessing market buzz, competitor activity and customer trends, fads and fashion.\n- Measuring public response to an activity or company\nrelated issue [4].\nIn this paper for Sentiment Analysis we are\nusing two Supervised Machine Learning algorithms : Naïve Bayes’ and K-Nearest Neighbour to calculate the accuracies, precisions (of positive and negative corpuses) and recall values (of positive and negative corpuses). The difficulties in Sentiment Analysis are an opinion word which is treated as positive side may be considered as negative in another situation. Also the degree of positivity or negativity also has a great impact on the opinions. For example “good” and “very good” cannot be treated same.[2] Although the traditional text processing says that a small change in two pieces of text does not change the meaning of the sentences. However the latest text mining gives room for advanced analysis measuring the intensity of the word. Here is the point where we can scale the accuracy and efficiency of different algorithms [4]. The rest of the paper is organized as follows: Section 2 deals with the related work of our study, Section 3 presents our proposed work (Data sets and data sources used in our study along with the models and methodology used), Section 4 presents all our experimental results, Section 5 presents the conclusion drawn from our survey.\nII. RELATED WORK\nSeveral techniques were used for Sentiment Analysis. Few Related work are as follow: (a)Mori Rimon[3] used the keyword based approach to classify sentiment. He worked on identifying keywords basically adjectives which indicates the sentiment. Such indicators can be prepared manually or derived from Wordnet. (b)Alec co [4] used different machine learning algorithms such as Naïve Bayes’, Support vector machine and maximum entropy. (c)Janice M. Weibe [5] performed document and sentence level classification. He fetched review data from different product destinations such as automobiles, banks, movies and travel. He classified the words into positive and negative categories. He then calculated the overall positive or negative score for the text. If the number of positive words is more than negative then the document is considered positive otherwise negative. (d) Jalaj S. Modha , Gayatri S. Pandi and Sandip J. Modha [6] worked on techniques of handling both subjective as well as objective unstructured data. (e) Theresa Wilson, Janyce Wiebe and Paul Hoffman [7] worked on a new approach on sentiment analysis by first determining whether an expression is neutral or\npolar and then disambiguates the polarity of the polar expression. With this approach the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, hence achieving results which are better than baseline.\nII. PROPOSED WORK"
    }, {
      "heading" : "A) Data source and Data Set",
      "text" : "To conduct the research, two datasets are considered here - Movie Reviews & Hotel Reviews.\n All the movie reviews have been scanned from\nwww.imdb.com.\n All the hotel reviews have been downloaded\nfrom OpinRank Review Dataset\n(http://archive.ics.uci.edu/ml/datasets/OpinRank\n+Review+Dataset)\nThe data set has been prepared by taking 5000 positive\nand 5000 negative reviews from each of the mentioned\nsites."
    }, {
      "heading" : "B) Methodology",
      "text" : "The main goal of the research is to analyse the data from the surveys and to decide whether it is suitable to be analysed with the use of the discussed data mining methods. A graphical description of the processes involve in sentiment analysis is detailed in Figure 1 below.\nknowledge and observed data can be combined. In Naïve Bayes’ technique, the basic idea to find the probabilities of categories given a text document by using the joint probabilities of words and categories. It is based on the assumption of word independence. The starting point is the Bayes’ theorem for conditional probability, stating that, for a given data point x and class C: P (C / x) = P(x/C)/P(x) ( 1)\nFurthermore, by making the assumption that for\na data point x = {x1,x2,...xj}, the probability of each of its attributes occurring in a given class is independent, we can estimate the probability of x as follows:\nP(C/x)=P(C).∏P(xi/C) (2)\nAlgorithm\nInput: a document d\nA fixed set of classes C={c1,c2,…,cj}\nOutput: a predicted class cC\nSteps: 1. Pre-processing: i. About 10,000 reviews were crawled from www.imdb.com / OpinRank Review Dataset ii. Positive reviews and negative reviews were kept in two files pos.txt and neg.txt iii. 2 empty lists were taken, one for positive and one for negative reviews. iv. Sentences of the positive and negative reviews were broken and ‘pos’ and ‘neg’ were appended to each accordingly and were stored in the 2 empty lists created. v. ¾ of these sentences were kept in the dictionary for training while the ¼ were kept for testing. 2. Using chi squared test (explained later) we\ncalculated the score of each of the remaining words and\ninstead of using all of those words we only used the\nbest 10,000.\n3. The classifier was trained using the dataset just\nprepared.\n4. Labelled sentences were kept correctly in reference\nsets and the predicatively labelled version in test sets.\n5. Metrics were calculated accordingly.\nFig 2. Naïve Bayes’ flowchart\nA small example using Naïve Bayes’ is given below,\nSet Docu\nment\nReview Sentence Class\nTrain ing Set 1 I liked the movie pos 2 It’s a good movie. Nice story. pos\n3 Hero’s acting is bad but heroine\nlooks good. Overall nice movie.\npos\n4 Nice songs. But sadly boring\nending.\nneg\nTest Set I like the direction. But boring\nlocations. Overall good movie\npos"
    }, {
      "heading" : "2) k-Nearest Neighbour Classifier",
      "text" : "K-NN is a type of instance-based learning, or lazy learning where the function is only approximated locally and all computation is deferred until classification. It is non parametric method used for classification or regression. In case of classification the output is class membership (the most prevalent cluster may be returned) , the object is classified by a majority vote of its neighbours, with the object being assigned to the class most common among its k nearest neighbours. This rule simply retains the entire training set during learning and assigns to each query a class represented by the majority label of its k-nearest neighbours in the training set. The Nearest Neighbour rule (NN) is the simplest form of K-NN when K = 1. Given an unknown sample and a training set, all the distances between the unknown sample and all the samples in the training set can be computed. The distance with the smallest value corresponds to the sample in the training set closest to the unknown sample. Therefore, the unknown sample may be classified based on the classification of this nearest neighbour. The K-NN is an easy algorithm to\nunderstand and implement , and a powerful tool we have at our disposal for sentiment analysis. KNN is powerful because it does not assume anything about the data, other than a distance measure can be calculated consistently between two instances. As such, it is called non-parametric or non-linear as it does not assume a functional form. The flowchart of k-nn classifier is given in Fig.3.\nAlgorithm:"
    }, {
      "heading" : "1. Pre-processing:",
      "text" : "i). About 10,000 reviews were crawled from www.imdb.com/OpinRank Review Dataset ii. Positive reviews and negative reviews were kept in two files pos.txt and neg.txt iii. 2 empty lists were taken, one for positive and one for negative reviews. iv. Sentences of the positive and negative reviews were broken and ‘pos’ and ‘neg’ were appended to each accordingly and were stored in the 2 empty lists created. v. ¾ of these sentences were kept in the dictionary for training while the ¼ were kept for testing. 2. Training:\ni. Using chi squared test we calculated the score of\neach of the words occurring in the training dataset.\nii. An empty list is created, the dictionary in which the\nwords from training dataset are stored followed by each of their scores thus calculated. ii. for each test review iii. for each word\niv. If it exists in the word score list, add its score to\nreview score\nv. Else find the word in word score list with minimum\njaccard index to the unknown word and add its score to\nthe review score.\nvi. End for at step 3\nvii. End for at step 4\nviii. Find metrics accordingly.\nChi squared test:\n1. Initialize an empty frequency distribution. 2. Initialize an empty conditional frequency distribution (based on words being positive and negative). 3. We fill out the frequency distributions, incrementing the counter of each word within the appropriate distribution. 4. We find the highest-information features is the count of words in positive reviews, words in negative reviews, and total words. 5. We use a chi-squared test (also from NLTK) to score the words. We find each word’s positive information score and negative information score, add them up, and fill up a dictionary correlating the words and scores, which we then return out of the function.\nIV. EXPERIMENTAL RESULTS\nAccuracy, Precision and recall are method used for evaluating the performance of opinion mining. Here accuracy is the overall accuracy of certain sentiment models. Recall (Pos) and Precision (Pos) are the ratio and precision ratio for true positive reviews. Recall (Neg) and Precision (Neg) are the ratio and precision ratio for true negative reviews. In an ideal scenario, all the experimental results are measured according to the Table 1.and accuracy, Precision and recall as explained below [9].\nThe overall accuracies of the three algorithms in 10 rounds of experiments are indicated in Table 2 and Fig.4,\nTable 2. Accuracy comparison on Test Datasets.\nNo. Of experi ments Number of reviews\nin the\ntraining dataset\nAccuracy\nNaïve Bayes\n’\n(movi\ne\nrevie\nws)\nK-NN (movi\ne\nrevie\nws)\nNaïve Bayes\n’\n(hotel revie\nws)\nK-NN (hotel\nreviews)"
    }, {
      "heading" : "1. 100 56.78 47.64 43.11 45.35",
      "text" : ""
    }, {
      "heading" : "2. 200 64.29 55.07 41.26 40.97",
      "text" : ""
    }, {
      "heading" : "3. 500 70.06 58.44 42.56 41.42",
      "text" : ""
    }, {
      "heading" : "4. 1000 73.81 61.48 44.64 41.18",
      "text" : ""
    }, {
      "heading" : "5. 1500 77.23 64.21 48.21 42.01",
      "text" : ""
    }, {
      "heading" : "6. 2000 79.14 66.02 51.28 46.57",
      "text" : ""
    }, {
      "heading" : "7. 2500 79.82 67.89 52.03 47.04",
      "text" : ""
    }, {
      "heading" : "8. 3000 80.27 68.58 52.64 47.03",
      "text" : ""
    }, {
      "heading" : "9. 4000 82.11 69.03 53.92 49.75",
      "text" : ""
    }, {
      "heading" : "10. 4500 82.43 69.81 55.09 52.14",
      "text" : "Fig. 4. Diagrammatic presentation of accuracies in the experiments\nrevie ws)\nws) ws)"
    }, {
      "heading" : "1. 100 59.04 41.35 42.11 44.51",
      "text" : ""
    }, {
      "heading" : "2. 200 64.96 50.97 40.26 40.86",
      "text" : ""
    }, {
      "heading" : "3. 500 69.56 54.42 41.56 40.41",
      "text" : ""
    }, {
      "heading" : "4. 1000 73.64 58.18 43.64 42.21",
      "text" : ""
    }, {
      "heading" : "5. 1500 77.21 62.01 47.21 42.12",
      "text" : ""
    }, {
      "heading" : "6. 2000 80.28 65.57 50.28 45.36",
      "text" : ""
    }, {
      "heading" : "7. 2500 81.03 66.04 51.03 46.14",
      "text" : ""
    }, {
      "heading" : "8. 3000 81.64 67.03 51.64 47.13",
      "text" : ""
    }, {
      "heading" : "9. 4000 82.92 67.75 52.92 47.57",
      "text" : ""
    }, {
      "heading" : "10. 4500 84.09 68.14 54.09 48.21",
      "text" : "Fig 5. Diagrammatic presentation of positive precisions in the experiments\nTable 5. Precision comparison for Negative Corpus on Test Datasets\nNo. Of experime nts Numb er of review\ns in the traini ng datase t\nPrecision for\nnegative corpus:\nNaïve Bayes’ (movie review s) K-NN (movie review s) Naïve Bayes’ (hotel review s) K-NN (hotel review s)"
    }, {
      "heading" : "1. 100 55.43 38.12 48.39 46.21",
      "text" : ""
    }, {
      "heading" : "2. 200 63.67 49.56 42.61 41.63",
      "text" : ""
    }, {
      "heading" : "3. 500 70.59 57.25 50.62 47.32",
      "text" : ""
    }, {
      "heading" : "4. 1000 73.99 62.12 53.81 52.15",
      "text" : ""
    }, {
      "heading" : "5. 1500 77.25 64.48 57.31 54.43",
      "text" : ""
    }, {
      "heading" : "6. 2000 78.09 65.73 58.11 55.69",
      "text" : ""
    }, {
      "heading" : "7. 2500 78.70 66.23 58.4 56.32",
      "text" : ""
    }, {
      "heading" : "8. 3000 79.00 66.47 59.91 56.51",
      "text" : ""
    }, {
      "heading" : "9. 4000 81.33 66.62 61.29 56.66",
      "text" : "10. 4500 81.01 66.73 61.11 56.77"
    }, {
      "heading" : "1. 100 44.33 31.12 32.24 30.35",
      "text" : ""
    }, {
      "heading" : "2. 200 62.04 45.37 43.54 42.41",
      "text" : ""
    }, {
      "heading" : "3. 500 71.34 52.24 41.79 41.86",
      "text" : ""
    }, {
      "heading" : "4. 1000 74.19 56.31 47.44 42.21",
      "text" : ""
    }, {
      "heading" : "5. 1500 77.26 58.24 49.19 44.72",
      "text" : ""
    }, {
      "heading" : "6. 2000 77.26 60.02 50.02 45.03",
      "text" : ""
    }, {
      "heading" : "7. 2500 77.89 61.12 51.77 46.01",
      "text" : ""
    }, {
      "heading" : "8. 3000 78.09 61.53 51.44 46.52",
      "text" : ""
    }, {
      "heading" : "9. 4000 80.87 61.72 51.34 46.25",
      "text" : ""
    }, {
      "heading" : "10. 4500 80.12 61.81 51.84 46.31",
      "text" : ""
    }, {
      "heading" : "1. 100 69.24 39.25 62.33 60.35",
      "text" : ""
    }, {
      "heading" : "2. 200 66.54 55.12 53.51 52.41",
      "text" : ""
    }, {
      "heading" : "3. 500 68.79 53.86 51.81 51.89",
      "text" : ""
    }, {
      "heading" : "4. 1000 73.44 60.21 57.52 52.19",
      "text" : ""
    }, {
      "heading" : "5. 1500 77.19 63.72 59.24 54.77",
      "text" : ""
    }, {
      "heading" : "6. 2000 81.02 65.03 60.11 5513",
      "text" : ""
    }, {
      "heading" : "7. 2500 81.77 66.01 61.83 56.11",
      "text" : ""
    }, {
      "heading" : "8. 3000 82.44 66.52 61.49 56.32",
      "text" : ""
    }, {
      "heading" : "9. 4000 83.34 66.25 61.37 56.35",
      "text" : ""
    }, {
      "heading" : "10. 4500 84.84 66.31 61.88 56.41",
      "text" : "V. CONCLUSION\nThe aim of study is to evaluate the performance for sentiment classification in terms of accuracy, precision and recall. In this paper, we compared two supervised machine learning algorithms of Naïve Bayes’ and KNN for sentiment classification of the movie reviews and hotel reviews. The experimental results show that the classifiers yielded better results for the movie reviews with the Naïve Bayes’ approach giving above 80% accuracies and outperforming than the k-NN approach. However for the hotel reviews, the accuracies are much lower and both the classifiers yielded similar results. Thus we can say Naïve Bayes’ classifier can be used successfully to analyse movie reviews.\nVI. FUTURE WORK\nFor further work we would like to compare try and come up with an efficient sentiment analyser like random forest, Support vector Machine etc. And also try to implement a new algorithm utilizing the benefits of the both algorithms so that it can be used effectively in data forecasting.\nREFERENCES\n[1] Lina L. Dhande and Dr. Prof. Girish K. Patnaik,\n“Analyzing Sentiment of Movie Review Data using\nNaive Bayes Neural Classifier”, IJETTCS, Volume 3,\nIssue 4 July-August 2014, ISSN 2278-6856.\n[2] P.Kalaivani, “Sentiment Classification of Movie\nReviews by supervised machine learning approaches”\net.al,Indian Journal of Computer Science and\nEngineering (IJCSE) ISSN : 0976-5166 Vol. 4 No.4\nAug-Sep 2013.\n[3] Meena Rambocas, João Gama, “Marketing\nResearch: The Role of Sentiment Analysis”, April"
    }, {
      "heading" : "2013, ISSN: 0870-8541.",
      "text" : "[4] Weiguo Fan, Linda Wallace, Stephanie Rich, and\nZhongju Zhang, “Tapping into the Power of Text\nMining”, Journal of ACM, Blacksburg, 2005.\n[5] “Movie review dataset,” [Online]. Available\nhttp://www.cs.cornell.edu/people/pabo/movie-review-\ndata/, [Accessed: October 2013].\n[6] K. M. Leung, “Naive Bayesian classifier,” [Online]\nAvailable:\nhttp://www.sharepdf.com/81fb247fa7c54680a94dc0f3a\n253fd85/naiveBayesianClassifier.pdf, [Accessed:\nSeptember 2013].\n[7] Zhou Yong , Li Youwen and Xia Shixiong “An\nImproved KNN Text Classification Algorithm Based\non Clustering”, journal of computers, vol. 4, no. 3,\nmarch 2009.\n[8] G.Vinodhini, RM.Chandrasekaran “Sentiment\nAnalysis and Opinion Mining: A Survey”,\nInternational journal of advanced research in\ncomputer science and software engineering, Volume 2,\nIssue 6, June 2012.\n[9] Rudy Prabowo1, Mike Thelwall “Sentiment\nAnalysis: A Combined Approach”, Journal of\nInformatics, 3(1):143–157, 2009.\n[10] Walaa Medhat a, Ahmed Hassan, Hoda Korashy,\n“Sentiment analysis algorithms and applications: A\nsurvey”, Ain Shams Engineering Journal, Vol. 5,2014,\npp. 1093–1113.\n[11] Svetlana Kiritchenko, Xiaodan Zhu, Saif M.\nMohammad, “Sentiment Analysis of Short Informal\nTexts”, Journal of Artificial Intelligence Research"
    }, {
      "heading" : "2014, pp. 723-762.",
      "text" : "[12] Jusoh, Shaidah, and Hejab M. Alfawareh.\n\"Techniques, applications and challenging issue in text\nmining.\" International Journal of Computer Science\nIssues (IJCSI) 9, no. 6 , 2012.\n[13] L. Dey and S. Chakraborty, “Canonical PSO Based \uD835\uDC3E-Means Clustering Approach for Real Datasets”, International Scholarly Research Notices, Hindawi Publishing Corporation,Vol.2014,pp.111,2014. [14] R. Dey and S. Chakraborty, “Convex-hull & DBSCAN clustering to predict future weather”, 6th International IEEE Conference and Workshop on Computing and Communication, Canada, 2015, pp.18."
    } ],
    "references" : [ {
      "title" : "Analyzing Sentiment of Movie Review Data using Naive Bayes Neural Classifier”, IJETTCS, Volume 3, Issue 4 July-August",
      "author" : [ "Lina L. Dhande", "Dr. Prof. Girish K. Patnaik" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Marketing Research: The Role of Sentiment Analysis",
      "author" : [ "Meena Rambocas", "João Gama" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Tapping into the Power of Text Mining",
      "author" : [ "Weiguo Fan", "Linda Wallace", "Stephanie Rich", "Zhongju Zhang" ],
      "venue" : "Journal of ACM, Blacksburg,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Naive Bayesian classifier",
      "author" : [ "K.M. Leung" ],
      "venue" : "[Online] Available: http://www.sharepdf.com/81fb247fa7c54680a94dc0f3a 253fd85/naiveBayesianClassifier.pdf, [Accessed: September 2013].",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An Improved KNN Text Classification Algorithm Based  on Clustering",
      "author" : [ "Zhou Yong", "Li Youwen", "Xia Shixiong" ],
      "venue" : "journal of computers,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Sentiment Analysis: A Combined Approach",
      "author" : [ "Rudy Prabowo", "Mike Thelwall" ],
      "venue" : "Journal of Informatics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Sentiment Analysis of Short Informal Texts",
      "author" : [ "Svetlana Kiritchenko", "Xiaodan Zhu", "Saif M. Mohammad" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Techniques, applications and challenging issue in text mining.\" International Journal of Computer Science Issues (IJCSI",
      "author" : [ "Jusoh", "Shaidah", "Hejab M. Alfawareh" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Canonical PSO Based K-Means Clustering Approach for Real Datasets",
      "author" : [ "L. Dey", "S. Chakraborty" ],
      "venue" : "International Scholarly Research Notices, Hindawi Publishing",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Convex-hull & DBSCAN clustering to predict future weather",
      "author" : [ "R. Dey", "S. Chakraborty" ],
      "venue" : "IEEE Conference and Workshop on Computing and Communication,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Several analysis tools of data mining (like, clustering, classification, regression etc,) can be used for sentiment analysis task [13][14].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "Several analysis tools of data mining (like, clustering, classification, regression etc,) can be used for sentiment analysis task [13][14].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "Here the source materials refer to opinions / reviews /comments given in various social networking sites [1].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "The traditional text mining concentrates on analysis of facts whereas opinion mining deals with the attitudes [3].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "- Measuring public response to an activity or company related issue [4].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "Here is the point where we can scale the accuracy and efficiency of different algorithms [4].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Few Related work are as follow: (a)Mori Rimon[3] used the keyword based approach to classify sentiment.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "(b)Alec co [4] used different machine learning algorithms such as Naïve Bayes’, Support vector machine and maximum entropy.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "Modha [6] worked on techniques of handling both subjective as well as objective unstructured data.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : "(e) Theresa Wilson, Janyce Wiebe and Paul Hoffman [7] worked on a new approach on sentiment analysis by first determining whether an expression is neutral or polar and then disambiguates the polarity of the polar expression.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "and accuracy, Precision and recall as explained below [9].",
      "startOffset" : 54,
      "endOffset" : 57
    } ],
    "year" : 2015,
    "abstractText" : "The advent of Web 2.0 has led to an increase in the amount of sentimental content available in the Web. Such content is often found in social media web sites in the form of movie or product reviews, user comments, testimonials, messages in discussion forums etc. Timely discovery of the sentimental or opinionated web content has a number of advantages, the most important of all being monetization. Understanding of the sentiments of human masses towards different entities and products enables better services for contextual advertisements, recommendation systems and analysis of market trends. The focus of our project is sentiment focussed web crawling framework to facilitate the quick discovery of sentimental contents of movie reviews and hotel reviews and analysis of the same. We use statistical methods to capture elements of subjective style and the sentence polarity. The paper elaborately discusses two supervised machine learning algorithms: K-Nearest Neighbour(K-NN) and Naïve Bayes’ and compares their overall accuracy, precisions as well as recall values. It was seen that in case of movie reviews Naïve Bayes’ gave far better results than K-NN but for hotel reviews these algorithms gave lesser, almost same",
    "creator" : "Microsoft® Word 2013"
  }
}