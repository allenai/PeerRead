{
  "name" : "1303.5747.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Generation of Alternative Explanations with Implications for Belief Revision",
    "authors" : [ "Eugene Santos Jr." ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nWe are constantly faced with the problem of explain ing the observations we have gathered with our senses. Our explanations are constructed by assuming certain facts or hypotheses which support our observations. For example, suppose I decide to phone my friend Tony at the office. After several rings, no one has answered the phone. From this, I conclude that Tony is not at the office. Our observation in this case is that no one answered the phone. Our explanation for this is that Tony is not at the office. The reasoning process we have just used is called abductive e:cplanation (Char niak & Shimony [1990]; Hobbs et a!. [1988]; Peng & Reggia [1990]; Selman & Levesque [1990]; Shanahan [1989]). It is often formalized as the process of find ing certain hypotheses which can explain or prove the things we observe.\nAlthough we used the word \"conclude\" in our story, our confidence in our solution may not be absolute. Suppose that I also know for a fact that Tony some times disconnects the phone to take a nap in the of fice. Now, I have an alternative explanation for why the phone was not answered. In general, there are many possible explanations for any given observation,\nbut yet, we often express confidence in one explanation over the others and choose it to be our solution. It is this fact that distinguishes abductive reasoning from deductive reasoning.\nIn current approaches to modeling abduction, confi dence in an explanation is defined by some measure on the set of hypotheses it represents. Such measures include minimal cardinality (Genesereth [1984]; Kautz & Allen [1986]), parsimonious covering theory (Peng & Reggia [1990]), most-probable explanation (Pearl [1988]) and minimal cost proofs (Charniak & Shimony [1990]; Hobbs et a!. [1988]; Stickel [1988]). These ap proaches provide us with a model for choosing a \"best\" explanation.\nIn particular, we are interested in minimal cost proofs found in the cost- based abduction model ( Charniak & Shimony [1990]).1 Under this model, costs are asso ciated with individual hypotheses. The use of a hy pothesis in an explanation incurs the cost associated with the hypothesis. Thus the cost of an explanation is simply the sum of the costs of the individual hypothe ses used. These costs now represent our confidence in each explanation and establishes an ordering on the explanations.\nSince cost-based abduction has been shown to be an NP-Hard problem (Charniak & Shimony [1990]), the runtime of standard searching techniques grows ex ponentially with the size of the problem. In (Santos [1991a]; Santos [1991b]; Santos [1991c]), it was shown that any cost-based abduction problem may be trans formed into an equivalent linear constraint satisfac tion problem, and the latter may be solved by utilizing the highly efficient optimization tools of operations re search. Indeed, empirical studies in (Santos [199la]; Santos [1991c]) showed that the approach is compu tationally practical and superior to search style tech niques. Our linear constraint satisfaction approach ac tually exhibited a subexponential growth rate.\n'Cost -based abduction is a minor variant of weighted abduction (Hobbs et a!. [1988]; Stickel [1988])\n340 Santos\nNow suppose that we further know that my friend Tony spends nearly all of his time in the office work ing, sleeping, and eating. This knowledge will signifi cantly increase the likelihood of the phone being dis connected as an alternative explanation. Even though our measures may still choose the initial explanation, they in general make no promises on how good this choice is with respect to our alternative. This issue is especially important in domains such as medical diag nosis where careful consideration of alternative diag noses/explanations is necessary. Thus, the ability to generate alternative explanations should exist in any complete model of abductive reasoning.\nIn this regard, a major deficiency of message-passing schemes (Pearl (1988]) for belief revision in Bayesian networks is its inability to generate alternative expla nations in an ordered manner beyond the second best. By considering the equivalent problem in terms of con straint systems, we can generate the consecutive next best explanations. In this paper, we present an ap proach based on our linear constraint systems to gen erate alternative explanations in order of cost.\nIn Section 2, we present an overview of constraint systems and cost-based abduction. In Section 3, we present our approach to generating alternative expla nations. In Section 4, we consider how our constraint systems may be applied to belief revision in Bayesian networks. Finally, in Section 5, we conclude our dis cussion and give some final thoughts concerning alter native explanations.\n2 CONSTRAINT SYSTEMS\nWe now present a brief overview of the formulation of cost-based abduction problems as constraint systems. Details and complete proofs can be found in (Santos (1991a]; Santos (1991c]).\nNoTATION. !R denotes the set of real numbers.\nDEFINITION 2.1. A WAODAG (or weighted AND/OR di rected acyclic graphj2 is a 4-tuple ( G, c, r, 5), where:\n1. G is a directed acyclic graph, G = (V, E). 2. c is a function from V x {true, false} to !R, called\nthe cost function.\n9. r is a function from V to {AND, OR}, called the label. A node labeled AND is called an AND-node, etc.\n4. S is a subset of nodes in V called the evidence nodes.\nNoTATION. VH is the subset of nodes with zero indegree called the hypothesis nodes.\n2Slight generalization of (Charniak & Shi mony [1990]).\nDEFINITION 2.2. A truth assignment for a WAODAG W = ( G, c, r, 5) where G = (V, E) is a function e from V to {true, false}. We say that such a function is valid iff the following conditions hold:\n1. For all AND-nodes q, e(q) =true iff for all nodes p such that (p, q) is an edge in E, e(p) = true.\n2. For all OR-nodes q, e(q) =true iff there exists a node p such that (p, q) is an edge in E and e(p) = true.\nFurthermore, we say that e is an explanation iff e lS valid and for each node q inS, e(q) =true.\nDEFINITION 2.3. We define the cost of an explanation e for W = (G, c, r, S) where G = (V, E) as\nC(e) = L c(q, e(q)). qEV\nAn explanation e which minimizes C lS called a best explanation for W.\nConsider the WAODAG representing the situation with our friend Tony (see Figure 2.1). We first assume that there is no cost for assigning a node to false. Next, assume that assigning Tony-in, Tony-sle ping and Tony out to true have costs 5, 4 and 8, respectively, and that the costs of assigning true to all non-hypothesis nodes is zero. The minimal cost proof for this WAODAG is the hypotheses set {Tony-out} with a cost of 8. We now define constraint systems as follows:\nNoTATION. For each node q in V, let Dq {Pi(p, q) is an edge in E}, the parents of q. IDq I is the cardinality of D q.\nDEFINITION 2.4. A constraint system is a 3-tuple (r, I, 'lj;) where r is a finite set of variables, I is a fi nite set of linear inequalities based on r, and 'lj; is a\nOn the Generation of Alternative Explanations with Implications for Belief Revision 341\nfunction from r X {true, false} to !R. Given a WAODAG W = (G, c, r, S) where G = (V, E), we can constMJ.ct a constraint system L(W) = (r, I, ..P) where:\n1. r is a set of variables indezed by v' that is, r = {x9lq E V}. 2 . .,P(x9,X) = c(q,X) for all q E V and X E {true, false}.\n3. I is the collection of all inequalities of the forms given below:\nXq � Xp E I for each p E D9 if r(q) =AND (1)\nL Xp-ID91+1�x9Eiifr(q) =AND (2) pED9\nL Xp 2: x9 E I if r(q) =OR (3) pED9\nx9 2: Xp E I for each p E D9 if r(q) = OR (4) We say that L(W) is induced by W. Furthermore, by including the additional constraints:\nXq = 1 if q E S, (5) we say that the resulting constraint system is induced evidentially by W and is denoted by LE(W). DEFINITION 2. 5. A variable assignment for a constraint system L = (r, I, ..p) is a function s from r to lR. Fur thermore,\n1. If the range of s is {0, 1} , then s is a 0-1 assign ment.\n2. If s satisfies all the constraints in I , then s is a solution for L.\n3. If s is a solution for L and is a 0-1 assignment, then s is a 0-1 solution for L.\nGiven a 0-1 assignment s for L(W), we can construct a truth assignment e for W as follows:\n1. For all q in V, s( x9) = 1 iff e( q) = true. 2. For all q in V, s(x9) = 0 iff e(q) =false.\nConversely, given a truth assignment e for W, we can construct a 0-1 assignment s for L(W). NOTATION. e, and s. denote, respectively, a truth as signment e constructed from a 0-1 assignments, and a 0-1 assignment s constructed from a truth assignment e.\nWe can show that all explanations for a given WAODAG W have corresponding 0-1 solutions for LE(W) and v1ce versa.\nTHEOREM 2.1. If e is an explanation for W, then s. is a solution of L(W). THEOREM 2.2. If s is a 0-1 solution of LE(W), then e, is an explanation for W.\nIt follows from Theorems 2.1 and 2.2 that 0-1 solutions for constraint systems are the counterparts of explana tions for WAODAGs. Thus, by augmenting a WAODAG induced constraint system with a cost function, the notion of the cost of an explanation for a WAODAG can be transformed into the notion of the cost of a 0-1 solution for the constraint system.\nDEFINITION 2.6. Given a constraint system L = (r, I, .,P), we construct a function eL from variable as signments to !R as follows:\neL(s) = L {s(x9).,P(x9, true)+ (1- s(x9)).,P(x9, false)}.\nz.,er\neL is called the objective function of L. DEFINITION 2.7. An optimal 0-1 solution for a con straint system L = (r, I, .,P) is a 0-1 solution which minimizes 8 L.\nClearly, Definition 2.6 is identical to Definition 2.3. Thus, it follows from Theorems 2.1 and 2.2 and the relationship between node assignments and variable assignments that an optimal 0-1 solution in LE (W) is a best explanation for W and vice versa. As we observed in (Santos (1991a); Santos (1991c)), I and 8 L are the elements of a linear program in op erations research (Nemhauser, Kan & Todd (1989)). Extremely efficient and practical optimization tech niques such as the Simplex method and Karmarkar's projective scaling algorithm (N emhauser, Kan & Todd (1989]) are available for use in minimizing 8 L with respect to the constraints in I.\nAlthough solving the linear program was sufficient to obtain an optimal 0-1 solution for most of our test problems in (Santos (1991a); Santos (1991c)), it was sometimes necessary to employ a branch and bound technique using the linear program to compute lower bounds. Complete details concerning the branch and bound algorithm can be found in (Santos (1991a); Santos (1991c)). This technique enables us to avoid searching through all possible solutions by utilizing the lower bounds computed by the linear program as a guide. Experiments performed in (Santos [1991a); Santos (1991c]) shown the practicality and efficiency of this approach for solving cost-based abduction prob lems. Also, it can be applied to any constraint system regardless of whether or not they are WAODAG induced.\n3 GENERATING ALTERNATIVE EXPLANATIONS\nIn abductive explanation, having alternative explana tions is often useful and sometimes necessary. Having the 2nd best, 3rd best, and so on, can provide a useful gauge on the quality of the best explanation. In this\n342 Santos\nsection, we present techniques for extracting alterna tive explanations in order of their associated costs.\nTo generate the alternative explanations, we solve a se quence of constraint systems. This sequence consists of constraint systems each of which are derived from the constraint systems earlier in the sequence. The initial constraint system is the original constraint sys tem which determines the first optimal solution. The subsequent constraint systems are generated using the following schema: Consider L1 = (r, I11 ,P), our initial constraint system. Let s1 be the optimal 0-1 solution of L1. We define a new problem L2 as the successor of L1. L2 is identical to L1 except for the additional constraint\nL:; F(s1,xq):::; lfl-1 x\"Er\nwhere for each Xq E r,\nNote that the new problem does not have s1 as its op timal 0-1 solution since the variable assignment would violate the new constraint.\nLet s2 be the optimal 0-1 solution, if any, to L2• This will be the second best 0-1 solution. To continue the search for the next best explanation, we simply define a successor to the last constraint system, in this case, L2. When the current constraint system does not yield any solution, all possible explanations have been generated and we are finished.\nALGORITHM 3.1. Given a constraint system L = (r,I, ,P), generate all the 0-1 solutions for L in order of cost.\n1. (Initialization} Seth :=I, L1 := (r, h, .P) and k := 1.\n2. Compute the optimal 0-1 solution for Lk . If there is no feasible solution, then go to step 7. Other wise, let Sk be the solution.\n3. k := k + 1. 4· Let h := Ik-1 U Ck-1 where Ck -1 contains the\nsingle constraint\nL:; F(sk-11 xq):::; If I- 1 :z: .. Er\nwhere for each Xq E r,\nF(sk -1, xq) = { (i _ xq) 5. Let Lk :=(f,h,,P). 6. Go to step 2.\nif Sk-l(Xq) = 1 if Sk -1(Xq) = 0\n7. (Solutions) Print s1, s2, . . . , •k-1·\n(6)\nThe method we have just described can be classi fied as a cutting plane method in operations research (Nemhauser, Kan & Todd [1989]). Since each de rived constraint system differs only in an additional constraint from some previously solved problem, effi cient incremental techniques such as the dual simplex method can be applied here in a fashion similar to the one which is used in the branch and bound algorithm.\nTHEoREM 3.1. Constraint system Ln in Algorithm 3.1 determines the n-th best 0-1 solution for L.\nThe algorithm we have just presented can be applied to any constraint system. However, there are certain situations where generating all possible explanations may not be particularly desirable. Returning to our friend Tony above, consider the following additional information: Tony is as likely to be awake as be asleep at any time since he can always get to sleep in any en vironment. This implies that for the hypothesis that Tony is awake, the difference in the cost of being true and it being false is 0. If we look at our original expla nation that Tony is not in the office, we must augment it with our guess as to whether he is asleep or not. With our assumptions, there is no way to choose be tween asleep and awake. However, since Tony is not in the office, the hypothesis involving his consciousness has no impact towards explaining the observation (see Figure 2.1).\nIf the algorithm first chooses that Tony is asleep, then the next alternative would be the same set of assign ments except for Tony being awake. However, this new alternative explanation is uninteresting. In general, it may be the case that we may run into an overly large number of these types of uninteresting explanations. We now proceed to present an approach to deal with this problem.\nDEFINITION 3.1. Given a WAODAG W = (G,c,r,S) where G = (V, E) and H c;; VH, an explanation e for W is said to be consistent with H iff for all h in H, e(h) = true. The base set H(e) of e is the subset of VH consisting of all h in VH where e(h) =true. In WAODAGs, finding the best explanation is tanta mount to finding the best set of hypotheses we need to assume.\nDEFINITION 3.2. The support-set K( e) of an explana tion e is the set consisting of all nodes m in V such that e(m) =true. PROPOSITION 3.2. For every explanation e for W, H(e) = K(e) n VH. The following propositions follow immediately from the properties of WAODAGs:\nPROPOSITION 3.3. Let e1 and e2 be explanations for W. H(e!) == H(e2) iff K(q) = Kh).\nOn the Generation of Alternative Explanations with Implications for Belief Revision 343\nPROPOSITION 3.4. Let e be an e:r.planation for W. For each H( e) � H � Vn, there e:r.ists an e:r.planation e' for W such that H(e') = H.\nTHEOREM 3.5. Let e1 and e2 be e:r.planations for W.\n1. H(e1) � H(e2) iff K(e1) � Kh)· 2. Hh) C H(e2) iff K(e1) C K(e2).\nTHEOREM 3. 6. There e:r.ists a 1-1 and onto mapping be tween 2vH and the set of all possible truth assignments forW.\nTHEOREM 3.7. If e is an e:r.planation for W, then there ezists at least 21VH-H(•)I e:r.planations for W which are consistent with H (e).\nIn general, we see that there are an exponential num ber of explanations for a given WAODAG. However, from Theorem 3.7, it seems that the majority of these explanations are formed from a possibly small number of \"simpler\" and more interesting explanations which utilize smaller numbers of hypotheses. The following question naturally arises: Do these additional expla nations provide any new or important information?\nDEFINITION 3.3. A WAODAG W is monotonic iff for ev ery two ezplanations e1 and e2 for W, K(e!) � K(e2) implies C(e1) :S: C(e2) . W is strictly monotonic iffW is monotonic, and for every two ezplanations e1 and e2 for W, K(e1) C Kh) implies C(e!) < C(e2).\nPROPOSITION 3.8. If c(v, true) 2': c(v, false) for all v in V, then W is monotonic. If c(v, true) > c(v, false) for all v in V, then W is strictly monotonic.\nTHEOREM 3.9. A WAODAG W is monotonic iff for every two ezplanations e1 and e2 for W, H(e1) � H(e2) implies C(e1) :S: C(e2). W is strictly monotonic iffW is monotonic, and for every two ezplanations e1 and e2 for W, H(e1) C Hh) implies C(e1) < C(e2).\nProposition 3.8 and Theorem 3.9 together show that in a monotonic WAODAG, \"simpler\" explanations are preferred due to the lower associated costs. The as sumption of monotonicity is reasonable in many cases as pointed out by (Charniak & Shimony (1990]) and characterized in (Charniak & Goldman (1988]). Our goal is to generate these explanations in order of cost without having to consider the remaining exponential number of explanations.\nDEFINITION 3.4. e is cardinal iff there are no ezplana tions e' such that H(e') C H(e) .\nIntuitively, a cardinal explanation is among the \"sim plest\" of explanations we wish to consider.\nTHEOREM 3.10. If W is strictly monotonic, then any best ezplanation for W is cardinal.\nAll the definitions given above involving WAODAGs can be carried over to WAODAG induced constraint sys-\nterns.\nSimilar to Algorithm 3.1, the best cardinal explana tion, 2nd best, 3rd best, etc. may be generated by con structing a sequence of constraint systems L1, L2, • . .. Instead of introducing the additional constraint (6) to Lk, we introduce\nI: :llq:::: IH(sk-1)1- 1. ZqEH(••-d\nLEMMA 3.11. Let W be strictly monotonic. If sn is the optimal 0-1 solution for the constraint system Ln, then Sn is a cardinal 0-1 solution for L.\nTHEOREM 3.12. Let W be strictly monotonic. The con straint system Ln determines the n-th best cardinal 0-1 solution.\nOur notion of cardinal explanations is very similar to the notion of irredundancy found in parsimonious cov ering theory for modeling medical diagnosis (Peng & Reggia (1990]). A diagnostic problem (Peng & Reggia (1990]) is a two-layer network consisting of a layer of manifestations which are causally affected by a layer of disorders. Given a subset of the manifestations as evidence, a subset of disorders must be chosen to best explain the manifestations based on parsimonious cov ering theory. A collection of disorders which can ex plain the manifestations is called a cover. A cover is said to be irredundant if none of its proper subsets is also a cover.\nA limitation of parsimonious covering theory as pointed out by Peng and Reggia (Peng & Reggia (1990]) is the large number of covers which are con sidered \"best\". In order to further select from these potential explanations, some additional criteria must be used. Basic parsimonious covering theory is ex tended to incorporate probability theory. The poten tial of an explanation is now measured by some prob ability. With the addition of probabilities, care must be taken in choosing which covers are to be inspected. For example, consider the following analogous prob lem in cost-based abduction: A set of disorders D can adequately explain manifestations M. Let d be a fairly common disorder which explains manifesta tion m. Assumed is not in D but m is present in M. Furthermore, assume c(d, F)> c(d, T). Thus, D U {d} is a better explanation than D, despite the fact that D U {d} is a superset of D.\nAlthough this modified algorithm works only for W be ing strictly monotonic, we can modify any non-strictly monotonic problem to make it applicable. In essence, the strict monotonicity simply implies that we should always have a preference for a false assignment over a true assignment. By introducing an arbitrarily small positive difference between the cost for true and the cost for false in the original problem, we can now deter mine the cardinal solutions of the new problem which\n344 Santos\nturns out to be identical to those of the original.\n4 BAYESIAN NETWORKS\nBayesian network$ have become an important tool in modeling probabilistic reasoning. The inherent rep resentational power of these networks provides a very promising approach. In particular, belief revision in Bayesian networks is the process of finding the best interpretation for some given piece of evidence. This, of course, is a cornerstone of abductive explanation.\nSince we are interested in abduction, existing effec tive algorithms for belief revision should be considered. One such algorithm is given by Pearl in (Pearl [1988]) which is based on a message passing scheme. How ever, except for simple networks such as polytrees, the method is rather complicated to apply. Also, as Pearl points out in Chapter 5 in (Pearl [1988]), this algo rithm cannot guarantee the generation of alternative explanations beyond the second best.\nOur goal in this section is to apply our linear constraint satisfaction approach to Bayesian networks. This en tails constructing a constraint system which is com putationally equivalent to the Bayesian network. Al though this could be done by first transforming the Bayesian network into a cost-based abduction graph (Charniak & Shimony [1990]) and then transforming the graph into a constraint system (Santos [1991a); Santos [1991c]), a more natural and straightforward method will be given below. We will show how to di rectly transform a Bayesian network into an equivalent constraint system.\nWe first observe that a Bayesian network can be com pletely described by a finite collection of random vari ables (or simply, r.v.s) and a finite set of conditional probabilities based on the r.v.s. 3\nNoTATION. Throughout the remainder of this paper, upper case italicized letters such as A, B, . . . will rep resent r.v.s and lower case italicized letters such as a, b, . . . will represent the possible assignments to the associated upper case letter r.v., in this case, A, B, . . .. Subscripted upper case letters which are not italicized are variables in a constraint system which explicitly represent the instantiation of the associated r.v. with the item in the subscript. For example, Aa denotes the instantiation of r.v. A with value a.\nNoTATION. Given a r.v. A, the set of possible values for A called the range of A will be denoted by R(A).\nGiven a Bayesian network, we can construct an or dered pair (V, P) where V is the set of r.v.s in the network and P is a set of conditional probabilities asso-\n3We c onsider pri or probabilities t o be degenerate cases of c onditi onal probabilities, i.e., P(A =a) = P(A = a l<f!) where <P is the e mpty set.\nciated with the network. P(A = a[C1 = c1, . . . , Cn = en) E P iff C1, ... , Cn are all the immediate parents of A and there is an edge from C; to A for i = 1, .. . , n in the network. We can clearly see that (V, P) completely describes the Bayesian network.\nDEFINITION 4.1. Given a Bayesian network B = (V, P), an instantiation is an ordered pair (A, a) where A E V and a E R(A). {An instantiation (A, a) is also denoted by A = a and Aa.) A collection of instanti .. tions w is called an instantiation-set iff are no two instantiations (A, a), (A, a') in w such that a -=ft a'.\nAn instantiation represents the event when a r.v. takes on a value from its range. Given an instantiation-set, we can define the notion of the span of an instantiation set.\nDEFINITION 4.2. Given an instantiation-set w for a Bayesian network B = (V, P), we define the span of w, span(w), to be the collection of r.v.s in the first coordinate of the instantiations. Furthermore, an instantiation-set w is said to be complete iff span ( w) = v.\nNoTATION. For each r.v. A and each a in R(A), vA a is the set of all conditional probabilities in P of the form P(A = a[C1 = q, .. . , Cn = en)· For each r.v. A, we define cond(A) as follows: B E cond(A) iff there exists a conditional probability in P of the form P(A = a[ . . . , B = b, ... ).\nDEFINITION 4.3. Given an instantiation-set w {(A1, a1), . . . , (An, an)} for a Bayesian network B (V, P), we define the probability of w to be\nP(w) = P(A1 = a1, . .. , An:::: an)·\nThe goal of belief revision on Bayesian networks is to determine the complete instantiation-set which maxi mizes the associated probability under certain condi tions. In general, these conditions, called evidence, im poses restrictions on what instantiations may be made. The instantiation-set satisfying the evidence with the highest probability is said to be the most probable ex planation for the evidence. We now formalize this as follows:\nDEFINITION 4.4. Given a Bayesian network B = (V, P) , evidence e for B is an instantiation-set for B.\nDEFINITION 4.5. Given instantiation-sets w1, w2 for a Bayesian network B, w2 is said to be consistent with W1 iff W1 <;; W2.\nDEFINITION 4.6. Given evidence e for B, a complete instantiation-set w for B is an explanation for e iff w is consistent with e. Furthermore, w is said to be a most probable explanation for e iff for all explanations w' -=ft w for e, P(w') :<::: P(w).\nOur basic approach in constructing a constraint sys tem from a given Bayesian network is to represent and\nOn the Generation of Alternative Explanations with Implications for Belief Revision 345\nenforce the constraints that exist between any two or more r. v .s.\nGiven a Bayesian network B = (V, P), we construct a constraint system L(B) = (r, I, ,P) as follows:\n1. For each r.v. A in V , let R(A) = {a1, ... ,a,.} and construct the variables Aa., ... , Aa� in r, set ,P(Aa., false) = ,P(Aa., true) = 0 and add the fol lowing constraint to I:\n\"\nLAa, = 1. (7) i=l\n2. For each r.v.A and some a in R(A), for each con ditional probability P(A = aiC1 = c1, ... , C,. = c,. ) in vAa, construct a variable q[Aa I C1 = c1, ... , C,. = c,.] in r such that (for nota tional convenience, we will denote q[Aa I C1 = c1, ... , C,. = c,.] by q in the next two conditions) (a) ,P(q, false) = 0, ,P(q, true) = -log(P(A\naiC1 = c1, ... , C,. = c,.)) , and, (b) Add the following constraint to I:\n\" q � L eke, + Aa -n. (8)\nk=l\n3. Let Y Aa be all the variables q constructed by vAa in step (2). For each r.v. A and some a in R(A), add the following constraint to I:\nAa = L q. qETA a\n(9)\nDEFINITION 4.7. L(B) constructed above is the con straint system induced by B.\nAs we can clearly see, our construction is straight forward and is done in time linear to the size of the Bayesian network. The next theorem show the com plexity of our induced constraint system with respect to the Bayesian network.\nTHEOREM 4.1. Let B = (V, P) be a Bayesian network and L(B) = (f, I, ,P) be the constraint system induced by B. Then\n1. lfl = IPI + l:AEV IR(A)I and 2. III= lV I + IPI + l:AEv IR(A)I.\nIn our construction, (7) guarantees that any r. v. takes on exactly one value. (8) and (9) guarantee that the probability of any complete instantiation-set will be computed with the appropriate set of conditional prob abilities. Variables of the form q[Aa I clc, ' ... ' CncJ are called conditional variables in that they explicitly represent the dependencies between r. v .s and will be the mechanism for computing the probability for any instantiation-set.\nFIG. 4.1. Si mple Bayesian network. The distri bution is as follows: P(C =true lA =true, B =true)= p, P(C =true lA =true, B =false)= P2 P(C =true lA =false, B =true)= p3 P(C =true lA =false, B =false)= P• P(A =true)= Po) P(B = true) = P1o)\nFor example, consider the simple Bayesian network in Figure 4.1. When we have the instantiations {A = true, B = false, C = true}, its associated probability is P2 * pg * (1 - Pw). In the induced constraint system, we expect our variables assignments to be Atrue = 1, Bfalse = 1, Ctrue = 1, q[Ctrue I Atrue> Bfalsel = 1, and all remaining variables to be 0. Since the only costs are associated with the variables Atrue1 Bfalse and q[Ctrue I Atrue1 Bfalsel, the cost of this assignment is -log(pg) -log(1-pw) -log(p2) which is equivalent to -log(p2 * pg * (1- Pw)). NoTATION. For each r.v. A, let �(A) be the set of variables in the induced constraint system constructed for A.\nTHEOREM 4.2. Given a 0-1 solution s for L(B), for each set of variables �(A), there ezists some Aa in �(A) such that Aa = 1 and Aa• = 0 for all Aa• # Aa in �(A).\nTHEOREM 4.3. Given a 0-1 solution s for L(B), for all variables q[Aa I cl =C), . .. , Cn =en], if Aa = clc, = ... = Cnc� = 1, then q[Aa I cl =C), .. . , Cn = Cn] = 1. Theorems 4.2 and 4.3 above verifies our expectations on the legitimate variable assignments. However, Aa = C1c = .. . = Cnc = 0 does not necessarily imply that ' � q[Aa I C1 = c1, . . . , Cn = en] = 0. We could remedy the situation by introducing the following additional constraints:\nq[Aa I cl =C), ... , Cn = Cn]::; Aa, q[Aa IC1 = C), . . . ,Cn =en] :S C;c, fori= 1, . . . , n.\nInstead of increasing the number of constraints, we will show that this can be solved through simple re strictions and modifications to the algorithms applied to general constraint systems.\nDEFINITION 4.8. A 0-1 solution s for L(B) is said to be permissible if for all variables q[Aa I cl = Cj, ... , Cn =\n346 Santos\nq[Aa I cl = cl, . .. ' Cn = Cn ] = 1 only if Aa = C1c, = . . . = Cncft = 1.\nThus our goal is to consider only those 0-1 solutions for L(B) which are permissible. We must now show that calculations on the constructed constraint system are equivalent to those on the Bayesian network for belief revision.\nGiven a 0-1 solution s for L(B), we can construct a complete instantiation-set w, forB as follows: s(Aa) = 1 iff (A, a) E w,. To convert from a complete instantiation-set to a 0-1 solution is slightly trickier. Given a complete instantiation-set w for B, construct a 0-1 solution s, for L(B) as follows: (A, a) E w iff s, (A a) = 1. For each conditional variable q in T A a, set the appropriate value according to w.\nTHEOREM 4.4. If s is a 0-1 solution for L(B), then w, is an instantiation-set for B.\nTHEOREM 4.5. If w is a complete instantiation-set for B, then s, is a permissible 0-1 solution for L(B).\nFrom our construction of instantiation-sets from 0-1 solutions, we notice that more than one 0-1 solution can construct the same instantiation-set. This arises from our previous observation that our expectations are not completely met (Theorem 4.3).\nCoROLLARY 4.6. There is a 1-1 and onto mapping be tween permissible 0-1 solutions for L(B) and complete instantiation-sets for B.\nThis corollary states that we only need to consider the permissible 0-1 solutions in our calculations of com plete instantiation-sets for the Bayesian network.\nDEFINITION 4.9. Let e be some evidence for B= (V, P). We construct L,(B) = (r,J,, 1/J) from L(B) = (r, I, 1/J) as follows: Let I, = I U I' where the constraint A a = 1 is in I' iff (A, a) E e. We say that L,(B) is induced by B with evidence e.\nPROPOSITION 4.7. II. I= III+ lei.\nTHEOREM 4.8. If s is a 0-1 solution for L,(B), then w, is an e:�:planation for e.\nTHEOREM 4. 9. If w is an e:�:planation for e, then s, u a permissible 0-1 solution for L,(B).\nWhen there is some set of evidence given to be ex plained, we only want to consider those instantiation sets which are consistent with the evidence. Theo rems 4.8 and 4.9 above guarantee that the evidence also properly restricts the set of possible permissible 0-1 solutions we wish to consider. Now, we must show that the costs associated to each permissible 0-1 solu tion are directly related to the probability of the cor responding instantiation-set.\nFor the following theorems, assume that L is in duced by a Bayesian network B, w is a complete instantiation-set forB, and s is a permissible 0-1 solu tion for L(B).\nTHEOREM 4.10. 8L(s,) = - log(P(w)).\nTHEOREM 4.11. There e:�:ists a constant a, such that for all e:tplanations W for e, 8L.(s,) = a,  log(P(wle)).\nTHEOREM 4.12. w is a most probable e:�:planation for e iff s, is an optimal 0-1 solution for L,(B).\nTheorem 4.11 guarantees that the probabilistic order ing of instantiation-sets is exactly reversed from the cost ordering imposed on permissible 0-1 solutions. Furthermore, computing the cost for a permissible 0-1 solution immediately determines the probability of its associated instantiation-set.\nTHEOREM 4.13. If 1/J(q, true) > 0 for all conditional variables q in L,(B), then any optimal 0-1 solution for L, (B) is permissible.\nThe condition required in the above theorem can be easily met by increasing the cost of conditional vari ables with 1/J(q, true) = 0 to 1/J(q, true) = 8 where 8 is an arbitrarily small but positive value. This still guar antees proper ordering of the permissible 0-1 solutions as compared to the instantiation-sets.\nSimilarly, we must guarantee that any alternative 0- 1 solutions generated must also be permissible. We can accomplish this by modifying the Algorithm 3.1. Again, instead of introducing the new constraint (6) into Lk we introduce\nL F(sk,Aa) :S 1 �1-1 AaEl>.\nwhere � = {:cl:c E V and :c E �(A) for some r.v. A}.\nTHEOREM 4.14. Ln generates the n-th best permissible optimal 0-1 solution for L,(B).\nWith the transformation of belief revision problems into constraint systems, we now have an alternative approach to solving for the best explanation as well as the consecutive next best. With our linear con straint satisfaction approach, we can utilize the highly efficient computational tools of operations research on the NP-Hard problem of belief revision and explana tion generation. Furthermore, unlike message-passing schemes requiring preprocessing such as clustering on non-polytree topologies, our approach can be directly applied to any Bayesian network.\n5 DISCUSSION\nLinear constraint satisfaction has been shown to be an effective and computationally practical approach\nOn the Generation of Alternative Explanations with Implications for Belief Revision 347\nto solving cost-based abduction (Santos [1991a]; San tos [1991c]). Experimental results comparing our con straint system against existing search style techniques have shown it to be the superior approach.\nIn this paper, we have presented an approach to gen erating alternative explanations within our framework of constraint systems. This approach naturally incor porates the computational tools of operations research in an efficient manner. We have also shown how to ap ply the generation of alternative explanations to cost based abduction and belief revision in Bayesian net works.\nThe necessity of having alternative explanations can also be readily seen in natural language processing. Proper handling of problems such as ambiguity re quires access to the possible explanations in order of best to worst. For example, the WIMP system (Gold man [1990]; Goldman & Charniak [1991]) uses alter native explanations in order to resolve lexical ambi guities. Our approach is especially well suited to this problem since it is characterized by low prior proba bilities making it monotonic within our framework.\nAcknowledgments\nThis work has been supported by the National Sci ence Foundation under grant IRI-8911122 and by the Office of Naval Research, under contract N00014-88-K0589. Special thanks to Eugene Charniak for critical comments and suggestions. Also, thanks to Solomon Shimony, Glenn Carroll and Moises Lejter for careful review of this paper.\nReferences\nCharniak, Eugene & Goldman, Robert [1988], \"A Logic for Semantic Interpretation,\" Proceed ings of the AAAI Conference.\nCharniak, Eugene & Shimony, Solomon E. [1990], \"Probabilistic Semantics for Cost Based Ab duction,\" Proceedings of the 1990 National Conference on Artificial Intelligence.\nGenesereth, Michael R. [1984], \"The Use of Design De scriptions in Automated Diagnosis,\" Artificial Intelligence.\nGoldman, Robert P. [1990], \"A Probabilistic Approach to Language Understanding,\" Department of Computer Science, Brown University, Ph.D. Thesis.\nGoldman, Robert P. & Charniak, Eugene [1991], \"Probabilistic Text Understanding,\" Proceed ings of the Third International Workshop on AI and Statistics, Fort Lauderdale, FL.\nHobbs, Jerry R., Stickel, Mark, Martin, Paul & Ed wards, Douglas [1988], \"Interpretation as Ab duction,\" Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics.\nKautz, Henry A. & Allen, James F. [1986], \"Gener alized Plan Recognition,\" Proceedings of the Fifth Conference of AAAI.\nNemhauser, G. L., Kan, A. H. G. Rinnooy & Todd, M. J. [1989], in Optimization: Handbooks in Op erations Research and Management Science Volume 1, North Holland.\nPearl, Judea [1988], in Probabilistic Reasoning in In telligent Systems: Networks of Plausible Infer ence, Morgan Kaufmann, San Mateo, CA.\nPeng, Y. & Reggia, J. A. [1990], in Abductive Infer ence Models for Diagnostic Problem-Solving, Springer-Verlag.\nSantos, Eugene Jr. [1991a], \"A Linear Constraint Satis faction Approach to Cost-Based Abduction,\" Department of Computer Science, Brown Uni versity, in preparation.\nSantos, Eugene Jr. [1991b], \"Cost-Based Abduction, Linear Constraint Satisfaction, and Alterna tive Explanations,\" to appear in Proceedings of the AAAI Workshop on Abduction.\nSantos, Eugene Jr. [1991c], \"Cost-Based Abduction and Linear Constraint Satisfaction,\" Depart ment of Computer Science, Brown University, Technical Report CS-91-13.\nSelman, Bart & Levesque, Hector J. [1990], \"Abduc tive and Default Reasoning: A Computational Core,\" Proceedings of the Eighth National Conference on Artificial Intelligence.\nShanahan, Murray (1989], \"Prediction is Deduction but Explanation is Abduction,\" IJCAI-89.\nStickel, Mark E. [1988], \"A Prolog-like Inference Sys tem for Computing Minimum-Cost Abductive Explanations in Natural-Language Interpreta tion,\" SRI International, Technical Note 451."
    } ],
    "references" : [ {
      "title" : "A Logic for Semantic Interpretation,",
      "author" : [ "Charniak", "Eugene", "Goldman", "Robert" ],
      "venue" : "Proceed­ ings of the AAAI Conference",
      "citeRegEx" : "Charniak et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Charniak et al\\.",
      "year" : 1988
    }, {
      "title" : "Probabilistic Semantics for Cost Based Ab­ duction,",
      "author" : [ "Charniak", "Eugene", "Shimony", "Solomon E" ],
      "venue" : "Proceedings of the 1990 National Conference on Artificial Intelligence",
      "citeRegEx" : "Charniak et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Charniak et al\\.",
      "year" : 1990
    }, {
      "title" : "The Use of Design De­ scriptions in Automated Diagnosis,",
      "author" : [ "Genesereth", "Michael R" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Genesereth and R.,? \\Q1984\\E",
      "shortCiteRegEx" : "Genesereth and R.",
      "year" : 1984
    }, {
      "title" : "A Probabilistic Approach to Language Understanding,",
      "author" : [ "Goldman", "Robert P" ],
      "venue" : null,
      "citeRegEx" : "Goldman and P.,? \\Q1990\\E",
      "shortCiteRegEx" : "Goldman and P.",
      "year" : 1990
    }, {
      "title" : "Probabilistic Text Understanding,",
      "author" : [ "Goldman", "Robert P", "Charniak", "Eugene" ],
      "venue" : "Proceed­ ings of the Third International Workshop on AI and Statistics,",
      "citeRegEx" : "Goldman et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Goldman et al\\.",
      "year" : 1991
    }, {
      "title" : "Interpretation as Ab­ duction,\" Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics",
      "author" : [ "Hobbs", "Jerry R", "Stickel", "Mark", "Martin", "Paul", "Ed­ wards", "Douglas" ],
      "venue" : null,
      "citeRegEx" : "Hobbs et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Hobbs et al\\.",
      "year" : 1988
    }, {
      "title" : "Gener­ alized Plan Recognition,",
      "author" : [ "Kautz", "Henry A", "Allen", "James F" ],
      "venue" : "Proceedings of the Fifth Conference of AAAI",
      "citeRegEx" : "Kautz et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Kautz et al\\.",
      "year" : 1986
    }, {
      "title" : "Probabilistic Reasoning in In­ telligent Systems: Networks of Plausible Infer­",
      "author" : [ "Pearl", "Judea" ],
      "venue" : null,
      "citeRegEx" : "Pearl and Judea,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl and Judea",
      "year" : 1988
    }, {
      "title" : "Abductive Infer­ ence Models for Diagnostic Problem-Solving",
      "author" : [ "Y. Peng", "J.A. Reggia" ],
      "venue" : null,
      "citeRegEx" : "Peng and Reggia,? \\Q1990\\E",
      "shortCiteRegEx" : "Peng and Reggia",
      "year" : 1990
    }, {
      "title" : "1991a], \"A Linear Constraint Satis­ faction Approach to Cost-Based Abduction,\" Department of Computer Science, Brown Uni­ versity, in preparation",
      "author" : [ "Santos", "Eugene Jr." ],
      "venue" : null,
      "citeRegEx" : "Santos and Jr.,? \\Q1991\\E",
      "shortCiteRegEx" : "Santos and Jr.",
      "year" : 1991
    }, {
      "title" : "1991b], \"Cost-Based Abduction, Linear Constraint Satisfaction, and Alterna­ tive Explanations,\" to appear in Proceedings of the AAAI Workshop on Abduction",
      "author" : [ "Santos", "Eugene Jr." ],
      "venue" : null,
      "citeRegEx" : "Santos and Jr.,? \\Q1991\\E",
      "shortCiteRegEx" : "Santos and Jr.",
      "year" : 1991
    }, {
      "title" : "1991c], \"Cost-Based Abduction and Linear Constraint Satisfaction,\" Depart­ ment of Computer Science, Brown University, Technical Report CS-91-13",
      "author" : [ "Santos", "Eugene Jr." ],
      "venue" : null,
      "citeRegEx" : "Santos and Jr.,? \\Q1991\\E",
      "shortCiteRegEx" : "Santos and Jr.",
      "year" : 1991
    }, {
      "title" : "Abduc­ tive and Default Reasoning: A Computational Core,",
      "author" : [ "Selman", "Bart", "Levesque", "Hector J" ],
      "venue" : "Proceedings of the Eighth National Conference on Artificial Intelligence",
      "citeRegEx" : "Selman et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Selman et al\\.",
      "year" : 1990
    }, {
      "title" : "Prediction is Deduction but Explanation is Abduction,\" IJCAI-89",
      "author" : [ "Shanahan", "Murray" ],
      "venue" : null,
      "citeRegEx" : "Shanahan and Murray,? \\Q1989\\E",
      "shortCiteRegEx" : "Shanahan and Murray",
      "year" : 1989
    }, {
      "title" : "A Prolog-like Inference Sys­ tem for Computing Minimum-Cost Abductive Explanations in Natural-Language Interpreta­ tion,\" SRI International",
      "author" : [ "Stickel", "Mark E" ],
      "venue" : null,
      "citeRegEx" : "Stickel and E.,? \\Q1988\\E",
      "shortCiteRegEx" : "Stickel and E.",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "A limitation of parsimonious covering theory as pointed out by Peng and Reggia (Peng & Reggia (1990]) is the large number of covers which are con­ sidered \"best\".",
      "startOffset" : 63,
      "endOffset" : 101
    } ],
    "year" : 2011,
    "abstractText" : "In general, the best explanation for a given observation makes no promises on how good it is with respect to other alternative explana­ tions. A major deficiency of message-passing schemes for belief revision in Bayesian net­ works is their inability to generate alterna­ tives beyond the second best. In this pa­ per, we present a general approach based on linear constraint systems that naturally gen­ erates alternative explanations in an orderly and highly efficient manner. This approach is then applied to cost-based abduction prob­ lems as well as belief revision in Bayesian net­ works.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}