{
  "name" : "1206.4611.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Convex Feature Learning Formulationfor Latent Task Structure Discovery",
    "authors" : [ "Pratik Jawanpuria", "J. Saketha Nath" ],
    "emails" : [ "pratik.j@cse.iitb.ac.in", "saketh@cse.iitb.ac.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)."
    }, {
      "heading" : "1. Introduction",
      "text" : "The paradigm of Multi-task learning (MTL) involves learning several prediction tasks simultaneously (Caruana, 1997). In contrast to single task learning, here the idea is to synergize the related tasks by appropriate sharing of information within them. Following Evgeniou & Pontil (2004); Jacob et al. (2008); Jalali et al. (2010), tasks are said to be related if the corresponding task parameters are close to each other.\nThe focus of this paper is in the multi-task learning setting where some relevant features could be shared across few related tasks. Such situations arise in several real world applications (Tropp, 2006; Jalali et al., 2010). Existing works in this setting (Turlach et al., 2005; Zhang & Huang, 2008; Negahban & Wainwright, 2009; Jalali et al., 2010) employ a `1/`∞-norm based regularizer that promotes sparsity among features and low variance among the parameters of all the given tasks in the shared feature space. Success of such methods depends on the extent to which the given tasks are related and the extent to which the features are shared among the tasks. In fact, Negahban & Wainwright (2009) show that `1/`∞ regularization could actually perform worse than simple element-wise `1 regularization when the extent to which the features are shared is less than a threshold or when the task parameters are not all close-by. Alternatively, Chen et al. (2010) assume that the relations between the tasks are known and propose employing a regularizer that penalizes deviations in weight vectors for highly correlated tasks. However, in real-world applications such task relations are not known apriori and need to be discovered.\nThe main contribution of this work is a convex formulation that simultaneously discovers groups of related tasks having close-by task parameters, as well as the feature space shared within each group. Here, the search space for the groups of related tasks is taken to be the power-set of the given tasks. Following the set-\nup of Multiple Kernel Learning (MKL) (Bach et al., 2004), the feature space in each group is taken to be that induced by a conic combination of a given set of base kernels. In the special case where the base kernels are chosen to be linear kernels formed by individual input features, this amounts to feature selection.\nNote that Widmer et al. (2010) also attempt a search among all possible groups of tasks; however the runtime of their algorithm is exponential in the number of tasks. Moreover, the shared feature space in each group is assumed to be the input space. The proposed formulation employs a graph-based regularizer that encodes an important structure among the groups of tasks: if there is no feature space under which a group of tasks has close-by task parameters, then there does not exist such a feature space for any of its supersets. Note that this specialty when appropriately exploited by an algorithm may avoid search in potentially large portions of the search space which are anyway not fruitful. In Section 3, an active set algorithm is presented that exploits this specialty and optimally solves the proposed formulation in a time polynomial in the number of groups of related tasks discovered. Note that this number is typically very small compared to the size of the power-set of the given tasks. Simulations on benchmark datasets show that the proposed methodology achieves good generalization and outperforms state-of-the-art multi-task learning techniques in some cases.\nThe rest of the paper is organized as follows. Section 2 formalizes the notation and the problem set-up. The details of the proposed formulation and the algorithm for solving it are described in Sections 3 and 4 respectively. Experimental results are discussed in Section 5. We conclude by summarizing the work and the key contributions."
    }, {
      "heading" : "2. Notations and set-up",
      "text" : "Consider a set T of learning tasks, T in number. The training data for the tth task is denoted by: Dt = {(xti, yti), i = 1, . . . ,m} ∀t = 1, . . . , T , where (xti, yti) represents the ith input/output pair of the tth task. For the sake of notational simplicity, we assume that the number of training examples is the same for all the tasks. The task predictors are assumed to be affine: Ft(x) = 〈ft, φ(x)〉 − bt, t = 1, . . . , T , where ft is the weight vector of the tth task, φ(·) is the feature map and bt is the bias term. Recall that our aim is to discover groups of related tasks from the power-set of T (henceforth denoted by V). To this end, we further assume that ft = ∑ w∈Gt ftw where Gt is the set of all subsets of T containing task t and ftw is the weight\nvector indicating the influence of group/subset w on task t. As we shall detail in the subsequent section, we employ a sparse regularizer that forces many ftw to be zero and hence enables selection of promising groups of related tasks.\nIn addition to discovering groups of related tasks, the proposed formulation also learns the corresponding shared feature spaces induced by conic combinations of base kernels. To this end, let k1, . . . , kn be the given base kernels. Let φj(·) denote the feature map induced by the jth kernel kj , j = 1, . . . , n. Hence, φ(x) = (φ1(x), . . . , φn(x)). Let f jtw represent the projection of ftw onto the φ\nj space. In other words, ftw = (f 1 tw, . . . , f n tw). With this notation, the prediction function for the task t can be rewritten as Ft(x) = 〈ft, φ(x)〉 − bt = ∑ w∈Gt〈ftw, φ(x)〉 − bt\n= ∑ w∈Gt ∑n j=1〈f j tw, φ\nj(x)〉 − bt. Note that if f jtw = 0 ∀t ∈ w, then the feature space corresponding to the jth kernel is absent in the shared feature space of the group w. Hence learning the the task predictors or equivalently the weight vectors f jtw and the bias terms bt amounts to simultaneous discovery of latent task structure as well as the corresponding shared feature spaces. In the subsequent section a novel convex formulation for learning the optimal task predictors in the current set-up is presented."
    }, {
      "heading" : "3. A Novel Convex Formulation",
      "text" : "This section presents the key contribution of the paper — a convex feature learning formulation for latent task structure discovery. Following the well-establish methodology of regularized risk minimization (Vapnik, 1998), we consider the following problem:\nmin ft,bt ∀t\nΩ(f1, . . . , fT ) 2 + C T∑ t=1 m∑ i=1 `(Ft(xti), yti) (1)\nwhere Ω(f1, . . . , fT ) 2 is the regularizer, `(·, ·) is a suitable convex loss function (like the hinge loss) and C is the regularization parameter. In multi-task learning, it is common to choose a regularizer based on some prior knowledge about the relationship among the given tasks. For example, when all the tasks are independent, Ω(f1, . . . , fT ) 2 can be taken as ∑T t=1 ‖ft‖22, leading to a factorization of the problem into problems involving individual tasks. In cases where it is known that all the given tasks have close-by weight vectors (i.e., all tasks are related), the following regularizer may be employed (Evgeniou & Pontil, 2004):\nΩ(f1, . . . , fT ) 2 = µ‖h0‖22 + T∑ t=1 ‖ht‖22, (2)\nwhere ft = h0 +ht ∀ t = 1, . . . , T and µ is the parameter that controls the trade-off between regularizing the mean weight vector h0 and the variance in the weight vectors of the tasks.\nIn the following text, we present a novel regularizer suitable for the current problem. We begin by writing down a basic term in the proposed regularizer, Θjw, which induces close-by feature weights in the group w wrt. the feature space j: Θjw =( µ‖hj0w‖22 + ∑ t∈Tw ‖h j tw‖22 ) 1 2 where f jtw = h j 0w + h j tw. This term is motivated from (2). Note that, Θjw = 0 ⇒ f jtw = 0 ∀ t ∈ w i.e., the shared feature space of the group w does not involve the jth kernel/feature space.\nNow, the terms Θjw, j = 1, . . . , n are combined using a p-norm expression, leading to: ‖Θw‖p =(∑\nj ( Θjw )p) 1p where Θw is the vector with entries as Θjw, j = 1, . . . , n, and p ∈ (1, 2). Such a p-norm promotes sparsity in the selection of the kernel induced feature spaces (i.e., forces many Θjw = 0). With the interpretation of Θjw noted above, essentially this enables feature learning within the wth group of tasks. Also, ‖Θw‖p = 0 ⇒ Θjw = 0 ∀ j = 1, . . . , n ⇒ f j tw = 0 ∀ t ∈ w, j = 1, . . . , n i.e., in case the node w does not contain related tasks (under any feature space induced by combinations of the given base kernels), then it does not influence any of the task predictors Ft.\nWith this interpretation, one naive way of obtaining few promising groups of related tasks (that share a feature space) is by employing a `q, q ∈ (1, 2) norm over the terms ‖Θw‖p, w ∈ V: Ω(f1, . . . , fT ) =(∑\nw∈V (‖Θw‖p) q) 1q . However the problem with this regularizer is that it renders the formulation (1) infeasible for real-world applications as the resultant optimization problem cannot be, in general, solved in a time polynomial in the number of tasks.\nOne key idea in the paper is to employ a graph-based regularizer, that alleviates this problem by exploiting a special structure among the groups of tasks. Note that the groups of tasks can be represented as nodes of a directed acyclic graph with the partial order ⊆, representing the “subset of” relation. It can verified that 〈V,⊆〉 is a lattice. The topmost node of the lattice represents a dummy node – the group with no tasks in it, the second level nodes represents groups with single task and so on. The bottommost node represents the group consisting of all the T tasks. As discussed earlier, we would like to encode into our regularizer the following structure among the groups of tasks: if there is no feature space under which a group of tasks\nhas similar task parameters, then there does not exist such a feature space for any of its supersets. In the context of the present lattice, this is same as saying: if a node w is not selected, then the entire sub-lattice D(w), which consists of all the descendants of w (including w itself), need not be selected. In the following, a regularizer that reflects this special structure is presented.\nMotivated by the graph-based regularizers employed in Zhao et al. (2009); Bach (2008), we propose the following novel regularizer for the problem at hand:\nΩ(f1, . . . , fT ) = ∑ v∈V dv  ∑ w∈D(v) ‖Θw‖qp  1q (3) where q ∈ (1, 2), p ∈ (1, 2) and dv is a parameter that enables encoding prior knowledge regarding the taskrelatedness in the group/node v. For e.g. one may have the prior knowledge that there is no task which is not related to the others. In this case one may choose dv = 0 for all the nodes in the second level of the lattice. Note that the proposed regularizer (3) may also be viewed as a `1, `q, `p mixed-norm regularizer. The `1-norm over the nodes (v ∈ V) of the lattice promotes\nsparsity, and hence we have (∑ w∈D(v) ‖Θw‖qp ) 1 q = 0 for most v ∈ V i.e., few groups of related tasks are\nselected. Moreover, (∑ w∈D(v) ‖Θw‖qp ) 1 q\n= 0⇒ f jtw = 0 ∀ t ∈ w, ∀ j ∈ 1, . . . , n, ∀ w ∈ D(v). In other words if a group/node is not selected (by the 1-norm), then none of its descendants are selected by the formulation — which is exactly the special structure we wanted to encode. The q-norm brings in additional sparsity among the descendants of the groups that are selected by the 1-norm. As we detail later, the key advantage with this regularizer is that it renders the proposed formulation (1), solvable in reasonable time.\nIn the following, a specialized partial dual of (1) with the proposed regularizer (3) is presented. This gives further insights into the working of the proposed formulation and motivates an efficient active set algorithm for solving it. In order to keep notations simple, the dual is presented for the case where each of the given tasks is a binary classification problem and the loss function `(Ft(x), y) is the hinge loss: max (0, 1− yFt(x)). However, it is easy to extend the derivations to other learning settings and convex loss functions as well.\nTheorem 1. In the case where the given tasks are all of binary classification and the hinge loss is employed as the loss function, the dual of (1) with the regularizer\ndefined in (3) is given by1\nmin γ∈∆|V| H(γ) (4)\nwhere ∆|V| = { z ∈ R|V| | z ≥ 0, 1>z = 1 } denotes the simplex of dimension |V| and H is a convex function with H(γ) equal to the optimal value of the following optimization problem:\nmax βt∈Rm ∀t\n∑ t 1 >βt− 12 (∑ w∈V λw(γ)( ∑k j=1(β >Kjwβ) p̄) q̄ p̄ ) 1 q̄ ,\ns.t. 0 ≤ βt ≤ C1 ∀ t, y>t βt = 0 ∀ t, (5)\nwhere yt denotes the vector with entries as yti, β = [β1 . . . βT ]\n>, 1 and 0 denote vectors with all entries as 1 and 0 respectively,\nλw(γ) = (∑ v∈A(w) d q vγ 1−q v ) 1 1−q , A(w) represents the set of ancestors for node w (including w), p̄ = p2(p−1) , q̄ = q 2(q−1) . The easiest way to describe the matrix Kjw ∈ RmT×mT is by writing it as a block matrix of size T × T with the (t1, t2)\nth block as the matrix Kjw(t1, t2) ∈ Rm×m. The (i1, i2)th entry of Kjw(t1, t2) is = µ+1 µ yt1i1yt2i2k j(xt1i1 ,xt2i2) if t1 = t2 ∈ w, 1 µyt1i1yt2i2k\nj(xt1i1 ,xt2i2) if t1, t2 ∈ w, t1 6= t2, 0 otherwise.\nThe dual (4) provides interesting insights into the formulation. To this end, let us begin with an interpretation for the Kjw matrices. From their definition, it is easy to see that Kjw can also be viewed as the gram matrix of training examples from all the tasks with an appropriately defined kernel function kjw. As µ → 0, 1 µ dominates and the kernel function k j w reflects great similarity between examples of tasks in w (and viceversa). Also, the examples from tasks not belonging to w have low similarity with those in w. Hence, the kernel kjw captures the similarity between the tasks in the group w under the jth feature space.\nNow lets focus on the problem (5). In the special case p̄ = q̄, this problem is same as the `q̂-MKL formulation (Kloft et al., 2009) with q̂ = q̄q̄−1 and with base kernels as k̂jw = (λw(γ)) 1 q̄ kjw ∀w ∈ V and ∀j = 1, . . . , n. Hence the problem (5) realizes a sparse combination of these kernels. With the interpretation provided above, this essentially amounts to a sparse selection of groups of related tasks. Hence the problem of latent task structure discovery essentially is posed as an MKL problem (with appropriately defined kernels kjw). However, unlike `q̂-MKL that performs a “flat” kernel selection, here the kernels are\n1Proof appears in the supplementary material\nweighted by a monotonic function of λw(γ) giving rise to a structured selection among the kernels. To see this, let us shift our focus to the dual problem (4). Because of the simplex constraints (i.e., `1 regularization), most of the γv will either be near zero at optimality. From the definition of λw(γ), we obtain: γv = 0 ⇒ λw(γ) = 0 ∀ w ∈ D(v). Also, λw(γ) = 0 implies the entire set of kernels k1w, . . . , k n w are not selected i.e., the group of tasks in w are not related in any feature space under consideration. To summarize, few groups of related tasks that share a feature space are selected and if a group of tasks is unrelated (γv = 0), then all its supersets are unrelated (because, λw(γ) = 0 ∀ w ∈ D(v)). Figure 1 provides an illustration of the dual problem for the case of three tasks (T = 3) and three base kernels (n = 3). The proposed formulation is equivalent to performing a structured selection among n× |V| kernels arranged on a lattice. At each node, there are n kernels that represent the relatedness of tasks in that group. The subsequent section, presents an efficient active-set algorithm for solving the proposed formulation that exploits the special structure in the solution described above."
    }, {
      "heading" : "4. Active-set Algorithm",
      "text" : "Following a common practice for solving large-scale convex sparsity problems (Lee et al., 2007; Bach, 2008), we propose solving the dual (4) using an active set algorithm.\nThe basic idea of the active set algorithm is as follows: the formulation is solved iteratively using improved guesses for the active set, which is defined as the set of w for which γw 6= 0 at optimality. At each iteration the problem restricted to the variables in the active set is solved using an appropriate solver. In order to save computational cost, the size of the initial active set is usually taken to be minimal. After solving the problem with the variables restricted to the current active set, a sufficiency condition for optimality of the solution is verified. In case the solution is optimal, the algorithm terminates. In case it is not, the active set is updated and the restricted problem with the new active set is solved. This process is repeated until optimality is reached. Any prior knowledge related to the structure of the optimal solution may also be incorporated in building the active set at each iteration. In case of problems like (4) with sparse solutions, the hope is that one may not solve the problem with all the variables, γw, which are exponential in T in number.\nIn order to formalize the active-set algorithm, we need i) an initial guess for the active set and a procedure for building/improving the active set after each iteration, ii) a sufficiency condition for verifying optimality of the current solution. Ideally, the complexity of verification of the condition should depend on the active set size rather than the problem size iii) an efficient algorithm for solving the formulation restricted to active set.\nWe begin with the first. Let W represent the active set. The initial guess as well as the methodology for the identification of the promising nodes is motivated by the special structure in the solution of (4): if a node w is not selected in the optimal solution, i.e., γw = 0, none of the descendants of w are selected (since λv(γ) = 0 ∀v ∈ D(w)). Equivalently, it can be stated that a node w can be selected only if all its ancestors are selected i.e., only if γv 6= 0 ∀v ∈ A(w). Due to this observation,W is always maintained to be equal to its hull, where hull(W) is defined to be the set of all the ancestors of the nodes in W. Accordingly, the initial guess for W is taken to be the second level nodes i.e., the singleton task groups. Also, in the subsequent iterations, only those nodes which have all their parents in W are considered as potential candidates for entry inside W.\nTowards the second requirement, we present the following theorem2:\nTheorem 2. For a given active set W such that W = hull(W), let the optimal solution of (4) restricted to W be (γ̂, β̂). Let Θ̂ be the value of the\n2Proof is fairly technical and appears in the supplementary\nterm (∑ w∈V λ̂w(γ̂) (∑k j=1 ( β̂>Kjwβ̂ )p̄) q̄p̄) 1q̄ . Then,\n(γ̂, β̂) is an optimal solution of (4) with duality gap if:\nmax s∈sources(Wc)  ∑ w∈D(s) β̂> (∑ jK j w ) β̂(∑\nv∈A(w)∩D(s) dv\n)2  ≤ Θ̂+2\n(6) where sources(W) is the set of nodes in W with no parent in W and Wc denotes the set of all the nodes present in the lattice V but not in W.\nAs mentioned earlier, the above sufficiency condition is useful only if it can be verified in polynomial time in |W|. Firstly, size of sources(W c) is upperbounded by T |W|. The denominator in the summation, ∑ v∈A(w)∩D(s) dv can be computed in O(T ) provided dv is decomposable as a product. In the simulations we use, dv = 1.5\n|v|. Because of the block structure of the matrices Kjw, the sum over descendants in (6) can be computed in O(T 2m2).\nIn the following, we present an efficient algorithm of solving (4) restricted to W. Note that (4) has a simple constraint set, which is a simplex and the gradient ∇H(γ) can be computed using the Danskin’s theorem (Bertsekas, 1999): the ith component of this sub-gradient is given by (∇H(γ))i =\n−d q i γ −q i 2q̄ × (∑ w∈V λw(γ) (∑k j=1 ( β̄>Kjwβ̄ )p̄) q̄p̄) 1q̄−1×(∑ w∈D(i) λw(γ) q (∑k j=1 ( β̄>Kjwβ̄ )p̄) q̄p̄) , where β̄ is\nan optimal solution of (5) with the given γ. Hence one can employ projected gradient-descent algorithm or any of its variants for solving (4. Here we employ the mirror-descent algorithm (Ben-Tal & Nemirovski, 2001) for solving (4)3. Note that the gradient computation ∇H(γ) requires solving (5) with the given γ. Also, since the constraint set in (5) is similar to that in an SVM, the β̄ will be sparse at optimality. Hence we use a sequential minimal optimization (SMO) algorithm (Platt, 1999) for solving (5). Algorithm 1 summarizes the proposed active-set method. The computational complexity of the active set algorithm is as follows: let the final size of the active set be W . Hence, (4) is solved a maximum of W times. Each run of mirror-descent algorithm takes O(log(W )) iterations (Ben-Tal & Nemirovski, 2001) while in each iteration the dominant computation is that of SMO for solving (5). A conservative complexity estimate for\n3Proofs regarding the applicability of mirror-descent on our problem are detailed in the supplementary material.\nAlgorithm 1 Active Set Algorithm\nInput: Training data Dt ∀t, tolerance . Output: γ, β,W. Initialize W = {w| w ∈ V, |Tw| = 1} (i.e. first level nodes of the lattice V) repeat\nFor the current W, solve for γ, β in (4) using mirrordescent & SMO. Calculate V = nodes violating the condition (6) Update W =W ∪ V\nuntil V is empty\nthe SMO algorithm is O((Tm)3(Wn)2). The computing cost for kernel matrices is O(n(Tm)2), while that of verifying the sufficiency condition is O((Tm)2TW ). Thus the overall complexity is: O(n2W 2m3T 3).\nWe end this section by presenting a variant of the proposed methodology. The motivation for this variant comes from a closer-look at the complexity of the active-set algorithm. Since the active-set always satisfies the condition W = hull(W) and since the complexity depends on the active-set size W ; in practice one cannot realize situations where the group selected is way down the lattice. In other words, it is rare that a group with large number of tasks is selected. However, as shown in simulations, there might exist applications where weight vectors are extremely close-by for all or most of the tasks. Hence realizing a group containing most of the tasks may be beneficial. One simple modification of the proposed methodology for selecting such large groups is: invert the lattice of groups of tasks i.e., revert the parent-child relations, and employ exactly the same formulation (the descendants become the ancestors and vice-versa). It is easy to see that in this case groups involving large number of tasks may be selected; whereas selecting groups involving few tasks is now improbable. Though this modification is simple and interesting, the natural motivation for employing the graph-based regularizer is absent in this case. The graph-based regularizer needs to be motivated purely from a computational perspective in this case."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "In this section we present our empirical studies on the following benchmark multi-task classification and regression datasets: Sarcos A multiple-output regression dataset used in Zhang & Yeung (2009). The aim is to predict inverse dynamics corresponding to the seven degrees-offreedom of a robot arm. The number of tasks is 7 and there are 21 real valued input features. Following Zhang & Yeung (2009), we sampled 2000 random examples from each task.\nParkinson A multi-task regression dataset4. The aim is to predict Parkinson’s disease symptom score for patients at different times using 19 bio-medical features. The dataset has 5,875 observations for 42 patients. The symptom score prediction problem for each patient is considered as a regression task. Thus there are 42 regression tasks with number of instances for each task ranging from 101 to 168. Yale A face recognition dataset from Yale face base5. It contains 165 images of 15 subjects. Following the experimental setup in Zhang & Schneider (2010), each task is defined as the binary classification problem of classifying two subjects. Thus there are 28 tasks and the number of features is 30. Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010). It contains examples collected from various landmine fields. Each example is represented as a 9- dimensional real valued feature vector. Each task is a binary classification problem with the goal being to predict landmines (positive class) or clutter (negative class). Following Xue et al. (2007); Zhang & Schneider (2010), we jointly learn 19 tasks from the landmine fields numbered 1 − 10 and 16 − 24 in the data set. Number of instances in each task varies from 445 to 690. The dataset is highly biased against the positive class. MHC-I A multi-task classification dataset used in Jacob et al. (2008). It contains binding affinities of various peptides with different MHC-I molecules. Each task here is a binary classification problem. We perform experiments on the same 10 tasks reported in Jacob et al. (2008). Total number of instances in the 10 tasks is 1200 and the input space consists of 180 binary features. The number of instances per task varies from 59 to 197 and the the dataset is biased against the positive class. Letter A multi-task classification dataset used in Ji & Ye (2009). It consists of handwritten letters from different writers. Each task is a binary classification problem of distinguishing between pairs of letters. There are 9 such binary classification tasks and we randomly sampled 300 data points per task for our simulations. The input features used are the 8× 16 = 128 binary pixels.\nEach dataset was further randomly split into training and test sets. In Landmine, MHC-I and Letter datasets, random 20%-80% train-test splits were con-\n4Available at http://archive.ics.uci.edu/ml/ datasets/Parkinsons+Telemonitoring\n5Dataset and train-test splits available at http://www. zjucadcg.cn/dengcai/Data/FaceData.html. We used the first 10 splits containing 5 training examples per subject.\nsidered. In the case of Sarcos dataset 15 random samples per task were used for training and the rest for testing. For Parkinson dataset, 5 random examples per task were used in training and the rest for testing.\nWe compare the following multi-task learning techniques in terms of generalization ability: MTL Classical multi-task learning algorithm by Evgeniou & Pontil (2004). Assumes that all tasks are related and have close-by weight vectors. No feature learning is performed. CMTL The clustered multi-task learning formulation proposed in Jacob et al. (2008). Finds clusters of tasks having similar weight vectors. No feature learning is performed.6 DMTL The multi-task feature learning formulation in Jalali et al. (2010). Performs feature selection to discover features shared across all the tasks as well as task-specific features. Also, induces close-by weight vectors for the tasks in the shared feature space.7 MTFL The proposed multi-task feature learning formulation. The base kernels were taken as linear kernels with individual input features. In addition, the linear kernel using all input features was also employed as a base kernel. Thus if the input space dimensionality is n, then we generate n + 1 linear kernels. We did not employ non-linear kernels for the sake of being fair and comparable to DMTL. The parameters p, q were both fixed at 1.5, promoting sparsity in selecting groups of related tasks as well as in selecting the kernel induced feature spaces. Since the base kernels include individual input-feature based linear kernels, this amounts to feature selection.8 STL A baseline approach in which the tasks are\n6Code available at http://cbio.ensmp.fr/~ljacob/ documents/cmtl-code.tgz\n7Code available at http://www.ali-jalali.com/ index_files/L1Linf_LASSO.r\n8Code available at www.cse.iitb.ac.in/~pratik.j/ MTFL_icml12.tar.gz\nlearned independently using SVM. Note that all of the above multi-task learning techniques rely on the same notion of task-relatedness: weight vectors of related tasks are close. Hence a comparison among them is indeed meaningful.\nThe free parameters in all the methods were tuned using nested 3-fold cross validation procedure. The details of the parameter ranges are as follows: in case of MTFL, MTL and STL, the regularization parameter C was chosen from the set {10−3, 10−2, . . . , 103}. MTFL and MTL have an additional parameter µ, which was chosen from the set: {10−3, 10−2, . . . , 10}. CMTL has 4 parameters and we considered 3 values for each leading to 34 = 81 combinations (Zhang & Schneider, 2010). DMTL has 2 parameters and we considered 7 values of each leading to 49 combinations.\nResults of the simulations are summarized in Table 1. In case of regression datasets we report the explained variance, whereas for classification datasets we report AUC (Area Under Curve). In both cases, higher the value reported, the better the algorithm. Also, we report both the mean and standard deviation in the values over 10 random train-test splits. The numbers in the brackets indicate the run-times in minutes with the tuned parameters on a Xeon machine with 16GB RAM. The best result in each dataset is highlighted. In case the best result is with the proposed method (MTFL) and its improvement over state-of-the-art is statistically significant, then we additionally mark it with a ‘*’. In case the best result is with an existing method and its improvement over the proposed method (MTFL) is statistically significant, then we again mark it with a ‘*’. Statistical significance test is performed using the paired t-test at 90% confidence.\nThe proposed method outperformed state-of-the-art in both the regression datasets and achieved significant improvement in case of the Yale dataset. Note that in case of Sarcos dataset, the baseline STL performs bet-\nter than MTL showing that there may be some tasks that are not related to some others and the task structure is non-trivial. Hence discovering the latent task structure is indeed important in this case. The excellent performance of the proposed method indicates that the task structure is well discovered by it.\nAccording to the results, the proposed methodology does not seem to improve over state-of-the-art in case of the MHC-I and Letter datasets. A closer look at the datasets and the predictors achieved with state-ofthe-art showed that the weight vectors are extremely close-by in these datasets. This motivated us to try the inverted lattice trick described towards the end of section 4. With this modified methodology we achieved an improved average AUC of 72.77% and 61.12% respectively on MHC-I and Letter datasets.\nWe end this section with a discussion on the run-time of the proposed method. Note that none of the existing methods attempt an optimal search over the exponentially large space of groups of tasks. Hence, as expected, the run-time of the proposed method is on the higher-side. Though this is the case, it is interesting to note that the extremely large search space (242) in case of the Parkinson dataset is searched in a reasonable time of 23min. Moreover, in most datasets the proposed method achieves better generalization."
    }, {
      "heading" : "6. Conclusions",
      "text" : "In real-world applications it is important to discover groups of related tasks that share a feature space. The main contribution of the work is a convex formulation for solving this problem. Using a novel graph based regularizer, the search in the exponentially large space of groups of tasks is rendered feasible. Experimental results illustrate the efficacy of the proposed approach."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Ganesh Ramakrishnan for insightful discussions on this paper."
    } ],
    "references" : [ {
      "title" : "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning",
      "author" : [ "F. Bach" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bach,? \\Q2008\\E",
      "shortCiteRegEx" : "Bach",
      "year" : 2008
    }, {
      "title" : "Multiple Kernel Learning, Conic Duality, and the SMO Algorithm",
      "author" : [ "F. Bach", "G.R.G. Lanckriet", "M.I. Jordan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bach et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2004
    }, {
      "title" : "Lectures on Modern Convex Optimization: Analysis, Algorithms and Engineering Applications",
      "author" : [ "A. Ben-Tal", "A. Nemirovski" ],
      "venue" : "MPS/ SIAM Series on Optimization,",
      "citeRegEx" : "Ben.Tal and Nemirovski,? \\Q2001\\E",
      "shortCiteRegEx" : "Ben.Tal and Nemirovski",
      "year" : 2001
    }, {
      "title" : "Non-linear Programming",
      "author" : [ "D. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas,? \\Q1999\\E",
      "shortCiteRegEx" : "Bertsekas",
      "year" : 1999
    }, {
      "title" : "Graph-structured multi-task regression and an efficient optimization method for general fused lasso",
      "author" : [ "X. Chen", "S. Kim", "Q. Lin", "J.G. Carbonell", "E.P. Xing" ],
      "venue" : "CoRR, abs/1005.3579,",
      "citeRegEx" : "Chen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2010
    }, {
      "title" : "Regularized multi–task learning",
      "author" : [ "T. Evgeniou", "M. Pontil" ],
      "venue" : "In ACM SIGKDD,",
      "citeRegEx" : "Evgeniou and Pontil,? \\Q2004\\E",
      "shortCiteRegEx" : "Evgeniou and Pontil",
      "year" : 2004
    }, {
      "title" : "Clustered Multi-Task Learning: A Convex Formulation",
      "author" : [ "L. Jacob", "F. Bach", "J.P. Vert" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jacob et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jacob et al\\.",
      "year" : 2008
    }, {
      "title" : "A dirty model for multi-task learning",
      "author" : [ "A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jalali et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jalali et al\\.",
      "year" : 2010
    }, {
      "title" : "An Accelerated Gradient Method for Trace Norm Minimization",
      "author" : [ "Ji", "Shuiwang", "Ye", "Jieping" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ji et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient and accurate lp-norm multiple kernel learning",
      "author" : [ "M. Kloft", "U. Brefeld", "S. Sonnenburg", "P. Laskov", "K.R. M’́uller", "A. Zien" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kloft et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kloft et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient sparse coding algorithms",
      "author" : [ "H. Lee", "A. Battle", "R. Raina", "A.Y. Ng" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lee et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2007
    }, {
      "title" : "Phase transitions for high-dimensional joint support recovery",
      "author" : [ "S. Negahban", "M. Wainwright" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Negahban and Wainwright,? \\Q2009\\E",
      "shortCiteRegEx" : "Negahban and Wainwright",
      "year" : 2009
    }, {
      "title" : "Fast training of support vector machines using sequential minimal optimization, pp. 185–208",
      "author" : [ "J.C. Platt" ],
      "venue" : null,
      "citeRegEx" : "Platt,? \\Q1999\\E",
      "shortCiteRegEx" : "Platt",
      "year" : 1999
    }, {
      "title" : "Algorithms for simultaneous sparse approximation: part ii: Convex relaxation",
      "author" : [ "J.A. Tropp" ],
      "venue" : "Signal Process.,",
      "citeRegEx" : "Tropp,? \\Q2006\\E",
      "shortCiteRegEx" : "Tropp",
      "year" : 2006
    }, {
      "title" : "Inferring latent task structure for multi-task learning by multiple kernel learning",
      "author" : [ "C. Widmer", "N. Toussaint", "Y. Altun", "G. Ratsch" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "Widmer et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Widmer et al\\.",
      "year" : 2010
    }, {
      "title" : "Multitask learning for classification with dirichlet process priors",
      "author" : [ "Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Xue et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2007
    }, {
      "title" : "The sparsity and bias of the lasso selection in high-dimensional linear regression",
      "author" : [ "C. Zhang", "J. Huang" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Zhang and Huang,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang and Huang",
      "year" : 2008
    }, {
      "title" : "Semi-Supervised Multi-Task Regression",
      "author" : [ "Y. Zhang", "D.Y. Yeung" ],
      "venue" : "In ECML/PKDD,",
      "citeRegEx" : "Zhang and Yeung,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhang and Yeung",
      "year" : 2009
    }, {
      "title" : "Learning multiple tasks with a sparse matrix-normal penalty",
      "author" : [ "Zhang", "Yi", "Schneider", "Jeff" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    }, {
      "title" : "Grouped and Hierarchical Model Selection through Composite Absolute Penalties",
      "author" : [ "P. Zhao", "G. Rocha", "B. Yu" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Following Evgeniou & Pontil (2004); Jacob et al. (2008); Jalali et al.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "Following Evgeniou & Pontil (2004); Jacob et al. (2008); Jalali et al. (2010), tasks are said to be related if the corresponding task parameters are close to each other.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "Such situations arise in several real world applications (Tropp, 2006; Jalali et al., 2010).",
      "startOffset" : 57,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "Such situations arise in several real world applications (Tropp, 2006; Jalali et al., 2010).",
      "startOffset" : 57,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "Existing works in this setting (Turlach et al., 2005; Zhang & Huang, 2008; Negahban & Wainwright, 2009; Jalali et al., 2010) employ a `1/`∞-norm based regularizer that promotes sparsity among features and low variance among the parameters of all the given tasks in the shared feature space.",
      "startOffset" : 31,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "Such situations arise in several real world applications (Tropp, 2006; Jalali et al., 2010). Existing works in this setting (Turlach et al., 2005; Zhang & Huang, 2008; Negahban & Wainwright, 2009; Jalali et al., 2010) employ a `1/`∞-norm based regularizer that promotes sparsity among features and low variance among the parameters of all the given tasks in the shared feature space. Success of such methods depends on the extent to which the given tasks are related and the extent to which the features are shared among the tasks. In fact, Negahban & Wainwright (2009) show that `1/`∞ regularization could actually perform worse than simple element-wise `1 regularization when the extent to which the features are shared is less than a threshold or when the task parameters are not all close-by.",
      "startOffset" : 71,
      "endOffset" : 570
    }, {
      "referenceID" : 4,
      "context" : "Alternatively, Chen et al. (2010) assume that the relations between the tasks are known and propose employing a regularizer that penalizes deviations in weight vectors for highly correlated tasks.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "up of Multiple Kernel Learning (MKL) (Bach et al., 2004), the feature space in each group is taken to be that induced by a conic combination of a given set of base kernels.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "Note that Widmer et al. (2010) also attempt a search among all possible groups of tasks; however the runtime of their algorithm is exponential in the number of tasks.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "Motivated by the graph-based regularizers employed in Zhao et al. (2009); Bach (2008), we propose the following novel regularizer for the problem at hand:",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "(2009); Bach (2008), we propose the following novel regularizer for the problem at hand:",
      "startOffset" : 8,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "In the special case p̄ = q̄, this problem is same as the `q̂-MKL formulation (Kloft et al., 2009) with q̂ = q̄ q̄−1 and with base kernels as k̂ w = (λw(γ)) 1 q̄ k w ∀w ∈ V and ∀j = 1, .",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "Following a common practice for solving large-scale convex sparsity problems (Lee et al., 2007; Bach, 2008), we propose solving the dual (4) using an active set algorithm.",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "Following a common practice for solving large-scale convex sparsity problems (Lee et al., 2007; Bach, 2008), we propose solving the dual (4) using an active set algorithm.",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "Note that (4) has a simple constraint set, which is a simplex and the gradient ∇H(γ) can be computed using the Danskin’s theorem (Bertsekas, 1999): the i component of this sub-gradient is given by (∇H(γ))i = − q i γ −q i 2q̄ × (∑ w∈V λw(γ) (∑k j=1 ( β̄Kwβ̄ )p̄) q̄ p̄) 1 −1× (∑ w∈D(i) λw(γ) q (∑k j=1 ( β̄Kwβ̄ )p̄) q̄ p̄) , where β̄ is",
      "startOffset" : 129,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "Hence we use a sequential minimal optimization (SMO) algorithm (Platt, 1999) for solving (5).",
      "startOffset" : 63,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010).",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010). It contains examples collected from various landmine fields.",
      "startOffset" : 63,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010). It contains examples collected from various landmine fields. Each example is represented as a 9dimensional real valued feature vector. Each task is a binary classification problem with the goal being to predict landmines (positive class) or clutter (negative class). Following Xue et al. (2007); Zhang & Schneider (2010), we jointly learn 19 tasks from the landmine fields numbered 1 − 10 and 16 − 24 in the data set.",
      "startOffset" : 63,
      "endOffset" : 403
    }, {
      "referenceID" : 14,
      "context" : "Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010). It contains examples collected from various landmine fields. Each example is represented as a 9dimensional real valued feature vector. Each task is a binary classification problem with the goal being to predict landmines (positive class) or clutter (negative class). Following Xue et al. (2007); Zhang & Schneider (2010), we jointly learn 19 tasks from the landmine fields numbered 1 − 10 and 16 − 24 in the data set.",
      "startOffset" : 63,
      "endOffset" : 429
    }, {
      "referenceID" : 6,
      "context" : "MHC-I A multi-task classification dataset used in Jacob et al. (2008). It contains binding affinities of various peptides with different MHC-I molecules.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "MHC-I A multi-task classification dataset used in Jacob et al. (2008). It contains binding affinities of various peptides with different MHC-I molecules. Each task here is a binary classification problem. We perform experiments on the same 10 tasks reported in Jacob et al. (2008). Total number of instances in the 10 tasks is 1200 and the input space consists of 180 binary features.",
      "startOffset" : 50,
      "endOffset" : 281
    }, {
      "referenceID" : 6,
      "context" : "MHC-I A multi-task classification dataset used in Jacob et al. (2008). It contains binding affinities of various peptides with different MHC-I molecules. Each task here is a binary classification problem. We perform experiments on the same 10 tasks reported in Jacob et al. (2008). Total number of instances in the 10 tasks is 1200 and the input space consists of 180 binary features. The number of instances per task varies from 59 to 197 and the the dataset is biased against the positive class. Letter A multi-task classification dataset used in Ji & Ye (2009). It consists of handwritten letters from different writers.",
      "startOffset" : 50,
      "endOffset" : 564
    }, {
      "referenceID" : 6,
      "context" : "CMTL The clustered multi-task learning formulation proposed in Jacob et al. (2008). Finds clusters of tasks having similar weight vectors.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "CMTL The clustered multi-task learning formulation proposed in Jacob et al. (2008). Finds clusters of tasks having similar weight vectors. No feature learning is performed. DMTL The multi-task feature learning formulation in Jalali et al. (2010). Performs feature selection to discover features shared across all the tasks as well as task-specific features.",
      "startOffset" : 63,
      "endOffset" : 246
    } ],
    "year" : 2012,
    "abstractText" : "This paper considers the multi-task learning problem and in the setting where some relevant features could be shared across few related tasks. Most of the existing methods assume the extent to which the given tasks are related or share a common feature space to be known apriori. In real-world applications however, it is desirable to automatically discover the groups of related tasks that share a feature space. In this paper we aim at searching the exponentially large space of all possible groups of tasks that may share a feature space. The main contribution is a convex formulation that employs a graphbased regularizer and simultaneously discovers few groups of related tasks, having closeby task parameters, as well as the feature space shared within each group. The regularizer encodes an important structure among the groups of tasks leading to an efficient algorithm for solving it: if there is no feature space under which a group of tasks has closeby task parameters, then there does not exist such a feature space for any of its supersets. An efficient active set algorithm that exploits this simplification and performs a clever search in the exponentially large space is presented. The algorithm is guaranteed to solve the proposed formulation (within some precision) in a time polynomial in the number of groups of related tasks discovered. Empirical results on benchmark datasets show that the proposed formulation achieves good generalization and outperforms state-of-the-art multi-task learning algorithms in some cases. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).",
    "creator" : "LaTeX with hyperref package"
  }
}