{
  "name" : "1606.05735.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Student performance prediction using classification data mining techniques Analysis of Student data to find factors influencing student performance",
    "authors" : [ "Muhammed Salman Shamsi", "Jhansi Lakshmi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "increasing rapidly. But due to various factors and inappropriate primary education in India dropout rates are high. Students are unable to excel in core engineering subjects which are complex and mathematical, hence mostly get drop / keep term (kt) in that subject. With the help of data mining techniques we can predict the performance of students in terms of grades and dropout for a subject. This paper compares various techniques such as naïve Bayes, LibSVM, J48, random forest, and JRip and try to choose one of them as per our needs and their accuracy. Based on the rules obtained from this technique(s), we derive the key factors influencing student performance.\nKeywords— dropout; prediction; classification; data mining;\neducation.\nI. INTRODUCTION\nEducation is the key to prosperity of any nation. India is one of the fastest growing nation in the world with largest youth of population [17]. Hence in order to build a skilled workforce education becomes necessary. Students are opting for the fields such as engineering, science and technology. Unfortunately due to lack of quality education at primary level [18], socio-economic, psychological and other diverse factors, students drop out rates are high and performance is low. Hence to improve the quality of engineering graduates such cases of dropout and poor performance must be monitored proactively. Data mining provide us with tools to analyze large set of data to derive meaningful data known as knowledge. This help us to get insight of data and reach to meaningful conclusion. Initially the application of data mining were restricted to business domain but know it is extended to education and is known as EDM. Educational data mining (EDM) deals with the application of data mining tools and techniques to inspect the data at educational institutions for deriving knowledge [1].\nThere are various data mining methods such as classification, clustering and association to analyze such data. Classification is supervised learning method that builds a model to classify a data item into a particular class label. The aim of classification is to predict the future outcome based on the current available data. In clustering the data objects are combined into set of objects known as groups or clusters [12], [15]. The objects within a cluster or a group are highly similar to each other but are dissimilar to the objects in other cluster\n[12]. Dissimilarities and similarities measures are based on the attribute values which describes the objects and often involve distance metrics [12]. Association rule learning involves finding interesting relations between variables in large databases [13]. The aim of association rule learning is to find strong rules in databases based on the various measures of interestingness [13]. Among this we are going to use classification techniques. We have analyzed various classification methods such as naïve Bayes, J48, LibSVM, random forest, JRip for their accuracy on the given set of data. Weka tool used for analysis of data and to build the classification model. Weka provide us with a set of machine learning algorithms which can be used for different data mining tasks. It is an open source tool under GNU license [2]. Weka provide tools for classification, data pre-processing, regression, association rules, clustering and visualization [2].\nThe goal of this study is as follows\n• To obtain the most influencing factors that affects students’ performance. • To find best classification method for student performance prediction in terms of grade and\ndropout.\nII. RELATED WORK\nSince EDM is one of the popular research fields there are numerous papers we have gone through. We hereby discuss some of the works we found most useful for our study.\nMohammed M. Abu Tair, Alaa M. El-Halees [14] in their case study discussed various EDM techniques to improve students’ performance. Data was collected from the college of Science and Technology – Khanyounis for 15years [1993 to 2007]. This data set consists of 3360 record and 18 attribute. They have used various techniques such as association rule mining, classification, outlier detection and clustering to identify the various factors that affects various performance.\nS. Agarwal [3], et al. suggested that placement is based on student performance in qualifying examination and the\ntest. They have used LibSVM algorithm with Radial Basis Kernel, achieving the overall accuracy of around 97.3%. Since placement is one of the most important parameters for quality of education, hence it is immensely necessary that student performance must be improved which is our area of focus throughout the paper.\nM.S. Kamal [4], et al. suggested that the most important factor for dropout are financial conditions, age group and gender. They have used Bayes theorem based on knowledge base to predict the dropout.\nCarlos Márquez-Vera [5], et al. suggested that the most influential factor for dropout and failure is Poor or Not Presented in Physics and Math; Not Presented in Humanities and Reading and Writing; Poor in English and Social. They have obtained classification results on four cases (1) By using all attributes (2) By using best attributes (3) By using Data Balancing (4) By using Cost-Sensitive classification. In all of these ADTree was one of the top performers while others where Prism, JRip and OneR.\nMashael A. Al-Barrak [6], et al. suggested using J48 algorithm to predict final GPA of the student. This paper attempted to find which courses of previous semesters have direct impact on final GPA. In result it was found out that Java1, Database Principles, Software Engineering I, Information security, Computer Ethics, and Project 2 are most important courses affecting the final GPA of the students.\nBrijesh and Pal [7], found out using Bayesian classification that student SSC (metric) grade, living location, medium of instruction, mother qualification, student habits and type of family are the most important factors for the student performance.\nDorina Kabakchieva [8], demonstrated that J48 performance is the best, followed by the JRip and the k-NN classifier. The Bayes classifiers are found to be less accurate than the rest. However, all tested classifiers has overall accuracy less than 70 % which imply that the error rate is high and the predictions are unreliable.\nM.S. Mythili [9], et al. concludes that the attendance, parent education, locality, gender, economic status are the high potential parameters affecting student performance in examination. It is also found that random forest is the most accurate classifier and take less time to build the model than any other classifier.\nQasem A. Al-Radaideh [10], et al. in their paper attempt to find out the main parameters which affects the student performance in a particular course. They have used CRISP framework for data mining for this purpose. ID3, C4.5 decision tree and Naive Bayes where compared. C4.5 was found to be better than others. However it was found the classification accuracy of the top three algorithm was not so\nhigh, hence they concluded that collected sample and attributes are not sufficient to generate a classification model of high quality.\nMrinal Pandey and Vivek Kumar Sharma [11], compared classification methods to predict grades for undergraduate engineering students. In the experiments they have used J48, Simple Cart, and Reptree, NB tree and found J48 as most accurate with overall accuracy of 82.58% using percentage split method and 80.15% using cross-validation.\nM. Ramaswami and R. Bhaskaran [16], developed a CHAID prediction model to predict the performance of students at higher secondary school level. As per their study they found that marks obtained in secondary school level, school location, medium of instruction and type of living are the most influencing factors that affects student performance.\nIII. METHODOLOGY\nA. Data Collection\nIn this study we have taken the data of students belonging to Anjuman-I-Islam’s Kalsekar Technical Campus, Navi Mumbai which is affiliated to Mumbai University. The parameters evaluates academics, socio-economic state, psychological state etc. Parameters taken for this study are shown in Table I.\n15 father_edu Father education status\n16 mother_ed\nu Mother education status\n17 father_ocup Fathers occupation\n18 mother_ocup Mother occupation\n19 challenges Issues in the family.\n20 Cast Caste of the student.t\n21 mother_tongue Native Language.\n22 Orphan Whether orphan or not.\n23 Kts Current Total Number of kts.\n24 Ssc Elementary / Secondary school level grade\n25 Hsc Higher Secondary grade. 26 medium Language of teaching in previous institute. 27 quota Determine educational quota type. 28 city Staying location of student 29 loc_type Type of location (urban/rural) 30 gender Sex of student 31 grade_individual Course grade achieved 32 host Hosteller / Day scholar 33 Tw Term work marks 34 orpr Oral / Practical marks 34 Test Average Test Marks\nB. Data Pre-Processing\nData is loaded into to weka and irrelevant attributes are removed. Attribute filter NumericalToNominal and StringToNominal is applied to convert all the numerical and string attributes to nominal. Chi-Squared filter is used for attribute selection with 20-folds cross-validation. We have choosen two sets of attribute. Set -I for predicting overall dropout i.e kt and Set-II for predicting grade_individual.\nSet-I have 28 attributes from above set in Table I. Attributes such as grade_individual, tw, orpr is removed as they are not relevant to prediction of dropout because a 0 grade point in any of this indicate a dropout which is off-course undefined before the conduction of actual exam. Attributes such as host, loc_type, quota and orphan are removed due to low information gain (average merit < 1.0). Set-II have 31 attributes from the set in Table I. Attributes such as host, loc_type, gender and orphan are removed due to low information gain (average merit < 1.0). Now this data set is used as our training data set.\nC. Evaluating the models\nClassification algorithms such as Naïve Bayes, JRip, LibSVM, J48, and Random Forest are used on training data to build the model which is cross-validated by 10 folds. Table II shows detailed accuracy by class and Table III shows the confusion matrices for each algorithm. This tables represent the analysis done on the Set-I attributes which is used to classify students based on kt attribute. Table II represent report generated by weka for each algorithm for this set.\nAs we compare the weighted average [W.Avg] of each algorithm as shown in Table II we found that the True Positive Rate (TP) which denotes the overall accuracy of classifier is highest for Random Forest (0.833) followed by Naïve Bayes (0.828) and JRip (0.800). However among this three methods the False Positive (FP) Rate is highest in JRip (0.398) followed by Random Forest (0.395) and Naïve Bayes (0.268). However if we look at the CLASS (YES) row which is one of the most important information for the study, it is found that in terms of this class value the most accurate classifier is Naïve Bayes (0.687) followed by JRip (0.508) and Random Forest (0.497).\nTable IV and V represent the analysis done on the Set-II attributes using weka tool which is used to obtain model(s) to predict the theory grade of students represented by grade_individual attribute. This attribute represent grading point of the subject whose values can vary from 0 to 10 which represents the corresponding grade obtained in the subject. Note that the value of -1 indicates that the course does not have the theory exam. Hence the theory exam grade is not applicable.\nAs we compare the weighted average [W.Avg] of each algorithm as shown in Table IV we found that the True Positive Rate (TP) is highest for JRip (0.594) followed by J48 (0.586) and Random Forest (0.554).\n0.949 0.018 0.938 0.949 0.944 0.974 0\n0.959 0.558 0.436 0.959 0.599 0.767 4\n0.000 0.003 0.000 0.000 0.000 0.696 5\n0.000 0.002 0.000 0.000 0.000 0.693 6\n0.034 0.000 1.000 0.034 0.065 0.718 7\n0.000 0.001 0.000 0.000 0.000 0.683 8\n0.000 0.001 0.000 0.000 0.000 0.651 9\n0.000 0.000 0.000 0.000 0.000 0.773 10\n[W.Avg] 0.586 0.179 0.499 0.586 0.476 0.802\nRandom\nForest\n0.948 0.013 0.858 0.948 0.901 0.995 -1\n0.928 0.050 0.845 0.928 0.885 0.985 0\n0.716 0.312 0.508 0.716 0.594 0.762 4\n0.084 0.046 0.819 0.084 0.116 0.650 5\n0.164 0.103 0.212 0.164 0.185 0.675 6\n0.160 0.037 0.275 0.160 0.202 0.768 7\n0.057 0.007 0.167 0.057 0.085 0.745 8\n0.000 0.005 0.000 0.000 0.000 0.747 9\n0.000 0.005 0.000 0.000 0.000 0.864 10\n[W.Avg] 0.554 0.133 0.494 0.554 0.514 0.806\nTable V shows the confusion matrices which denotes the number of tuples classified in class -1 to 10 class out of total instances of 1476.\nG [8] 0 3 30 0 0 1 1 0 0\nH [9] 2 0 15 0 0 0 2 0 0 I [10] 2 0 13 0 0 0 0 0 0\nJ48\nA [-1] 104 1 10 0 0 0 0 0 0\nB [0] 2 318 13 1 1 0 0 0 0\nC [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0\nE [6] 2 2 207 2 0 0 0 0 0\nF [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0\nH [9] 1 0 17 0 0 0 1 0 0\nI [10] 2 0 13 0 0 0 0 0 0\nRando m Forest\nA [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0\nC [4] 4 39 328 25 51 11 0 0 0\nD [5] 0 9 109 14 25 6 1 1 2\nE [6] 3 6 127 18 35 15 4 2 3\nF [7] 6 2 44 10 34 19 3 0 1\nG [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0\nI [10] 2 0 1 4 3 5 0 0 0\nIV. RESULTS\nFrom the all the analysis we have done so far we found out that Random Forest, Naïve Bayes and JRip are the best three methods with the differences of approximately 0.5 to 2.8 % in accuracy for Set-I data. Among this we prefer Naïve Bayes because it give us the most accurate prediction with respect to dropouts i.e. all the instance which fall in class (YES) which is our aim. For Set-II data JRip is the most accurate classifier. This establishes our second objective.\nJRip algorithm also give us a set of rules which help us to derive the most influential factors which affects student performance. “Fig. 1” shows the rules we have derived by using JRip method for Set-I. According to this rules it was found that attendance (att), term test grade (test), travelling time (travel_time), educational year drop (drop_yr), parents annual income (annual_income),metric / ssc percentage (ssc), number of hours spent studying (study_hrs), source of fees (source_fees), number of subject kt’s / drop (kts) and father education (father_edu) are the factors which determines whether the student can get drop or not. Among this term test grade (test) and number of subject kt’s / drop (kts) have high frequency count than other attributes. Hence this are the parameters that highly affects student performance.\n(test = 0) => kt=YES (62.0/5.0)\n(test = 5) and (campus_feedback = 4) and (study_hrs = 2) => kt=YES (23.0/5.0)\n“Fig. 2” shows the rules we have derived by using JRip method for Set-II. According to the rules generated course_id, test, kt, tw, medium and kts are the factors which determines student grade. Among this course id, term test marks(test) and number of subject kt’s (kts) are the most influencing factors based on frequency count which affects students grade. This establishes our first goal.\nV. CONCLUSION\nIn this study we have analyzed the best classification method as per accuracy and our need. We also found out\nvarious factors that affects student performance. But as we look into the factors we have described in result section, you might have notice that it doesn’t include any psychological factors such as family issues, cast, gender, health, family type, etc.\nConsidering above point we can infer following things. First, the student have not disclosed their personal issues due to privacy concern or other factors. Second, the sample we have taken is from a minority institute hence we fail to find caste factor which is not prevalent in such religious minority. Also most of the sample exhibit homogenous nature in health and family attributes, so we fail to find any co relation between them and student performance.\nHence in future study we try to collect large and diverse sample, so as to detect other important factors affecting students’ performance. However this study can be immense benefit to the welfare of students if the parameters obtained are proactively monitored by the institute. Strict remedial action can be taken if the student is showing signs of a drop or poor grade in a subject. This will lead to better results and quality of education by tapping up the potential problem in early stage. Such students can go under counselling, to improve their performance. This will also help to obtain hidden factors which are not covered in the study or are not disclosed by the student due to fear of loss of privacy. This factors can be used in the future study to improve the prediction model in terms of accuracy.\nACKNOWLEDGMENT\nI sincerely like to thanks all the students and staff of AIKTC, computer engineering department who participated in this study and took out the time for filling up the data forms. I would also like to thanks Mr. Tabrez Khan, Head of Department of Computer Engineering, AIKTC for being extremely supportive for conduction of this study.\nREFERENCES\n[1] M. Al-Razgan, A. S. Al-Khalifa, and H. S. Al-Khalifa, \"Educational data mining: A systematic review of the published literature 2006- 2013,\" in Proc. the 1st International Conference on Advanced Data and Information Engineering, 2013, pp. 711-719.\n[2] Weka 3: http://www.cs.waikato.ac.nz/ml/weka/\n[3] Sonali Agarwal, G. N. Pandey, and M. D. Tiwari, “Data Mining in Education: Data Classification and Decision Tree Approach”, in the Journal of e-Education, e-Business, e-Management and e-Learning, Vol. 2, No. 2, April 2012, pp. 140-144.\n[4] Md.Sarwar kamal, Linkon Chowdhury, Sonia Farhana Nimmy, “New Dropout Prediction for Intelligent System”, in the International Journal of Computer Applications (0975 – 8887) Volume 42– No.16, March 2012, pp. 26-31.\n[5] Carlos Márquez-Vera, Cristóbal Romero Morales, and Sebastián Ventura Soto, “Predicting School Failure and Dropout by Using Data Mining Techniques” in the IEEE Journal Of Latin-American Learning Technologies, Vol. 8, No. 1, February 2013, pp. 7-14.\n[6] Mashael A. Al-Barrak and Muna Al-Razgan, “Predicting Students\nFinal GPA Using Decision Trees: A Case Study”, in the International Journal of Information and Education Technology, Vol. 6, No. 7, July 2016, pp. 528-533.\n[7] Brijesh Kumar Bhardwaj and Saurabh Pal, “Data Mining: A prediction for performance improvement using classification”, in the (IJCSIS) International Journal of Computer Science and Information Security, Vol. 9, No. 4, April 2011.\n[8] Dorina Kabakchieva, “Predicting Student Performance by Using Data Mining” , in the Bulgarian Academy Of Sciences cybernetics And Information Technologies , Volume 13, No 1 pp. 61-72.\n[9] M.S. Mythili, Dr. A.R.Mohamed Shanavas, “An Analysis of students’ performance using classification” , in the IOSR Journal of Computer Engineering (IOSR-JCE) e-ISSN: 2278-0661, p- ISSN: 2278-8727 Volume 16, Issue 1, Ver. III (Jan. 2014), pp. 63-69\n[10] Qasem A. Al-Radaideh, Emad M. Al-Shawakfa, Mustafa I. Al-Najjar,\n“Mining Student Data Using Decision Trees” in the International Arab Conference on Information Technology (ACIT’2006), Nov. 2006, Jordan, pp. 1-5.\n[11] Mrinal Pandey, Vivek Kumar Sharma, “A Decision Tree Algorithm Pertaining to the Student Performance Analysis and Prediction”, in the International Journal of Computer Applications (0975 – 8887) Volume 61– No.13, January 2013, pp. 1-5.\n[12] Jiawei Han, Micheline Kamber, Jian Pei, “Data Mining Concepts and Techniques”, 3rd edition, ISBN 978-0-12-381479-1.\n[13] Piatetsky-Shapiro,Gregory (1991), Discovery, analysis, and presentation of strong rules, in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., Knowledge Discovery in Databases, AAAI/MIT Press, Cambridge, MA.\n[14] Mohammed M. Abu Tair, Alaa M. El-Halees, “Mining Educational Data to Improve Students’ Performance: A Case Study” in the International Journal of Information and Communication Technology Research, Volume 2 No. 2, February 2012, ISSN 2223-4985, pp. 140- 146.\n[15] I. H. Witten and E. Frank, “Data mining: Practical Machine Learning Tools and Techniques,” 2nd ed, Morgan-Kaufman Series of Data Management Systems San Francisco Elsevier, 2005, ISBN 978- 0120884070.\n[16] M.Ramaswami and R.Bhaskaran, “A CHAID Based Performance Prediction Model in Educational Data Mining”, in IJCSI International Journal of Computer Science Issues, Vol. 7, Issue 1, No. 1, January 2010, pp. 10-18.\n[17] “The Power of 1.8 Billion”, in State of the World’s Population report, United Nations Population Fund’s (UNFPA ), ISBN: 978-0-89714- 972-3, pp. 5.\n[18] “Creaking, groaning Infrastructure is India’s biggest handicap”, in The economist, http://www.economist.com/node/12749787"
    } ],
    "references" : [ {
      "title" : "Educational data mining: A systematic review of the published literature 2006- 2013",
      "author" : [ "M. Al-Razgan", "A.S. Al-Khalifa", "H.S. Al-Khalifa" ],
      "venue" : "Proc. the 1st International Conference on Advanced Data and Information Engineering, 2013, pp. 711-719.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Data Mining in Education: Data Classification and Decision Tree Approach”, in the Journal of e-Education, e-Business",
      "author" : [ "Sonali Agarwal", "G.N. Pandey", "M.D. Tiwari" ],
      "venue" : "e-Management and e-Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "New Dropout Prediction for Intelligent System”, in the International Journal of Computer Applications",
      "author" : [ "Md.Sarwar kamal", "Linkon Chowdhury", "Sonia Farhana Nimmy" ],
      "venue" : "Volume 42–",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Predicting School Failure and Dropout by Using Data Mining Techniques",
      "author" : [ "Carlos Márquez-Vera", "Cristóbal Romero Morales", "Sebastián Ventura Soto" ],
      "venue" : "in the IEEE Journal Of Latin-American Learning Technologies,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Predicting Students Final GPA Using Decision Trees: A Case Study",
      "author" : [ "Mashael A. Al-Barrak", "Muna Al-Razgan" ],
      "venue" : "in the International Journal of Information and Education Technology,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Data Mining: A prediction for performance improvement using classification",
      "author" : [ "Brijesh Kumar Bhardwaj", "Saurabh Pal" ],
      "venue" : "in the (IJCSIS) International Journal of Computer Science and Information Security,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Mining Student Data Using Decision Trees",
      "author" : [ "Qasem A. Al-Radaideh", "Emad M. Al-Shawakfa", "Mustafa I. Al-Najjar" ],
      "venue" : "in the International Arab Conference on Information Technology (ACIT’2006),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "A Decision Tree Algorithm Pertaining to the Student Performance Analysis and Prediction",
      "author" : [ "Mrinal Pandey", "Vivek Kumar Sharma" ],
      "venue" : "in the International Journal of Computer Applications",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "El-Halees, “Mining Educational Data to Improve Students’ Performance: A Case Study",
      "author" : [ "Mohammed M. Abu Tair", "Alaa M" ],
      "venue" : "in the International Journal of Information and Communication Technology Research, Volume 2 No",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Data mining: Practical Machine Learning Tools and Techniques",
      "author" : [ "I.H. Witten", "E. Frank" ],
      "venue" : "2nd ed, Morgan-Kaufman Series of Data Management Systems San Francisco Elsevier, 2005, ISBN 978- 0120884070.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Educational data mining (EDM) deals with the application of data mining tools and techniques to inspect the data at educational institutions for deriving knowledge [1].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "In clustering the data objects are combined into set of objects known as groups or clusters [12], [15].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "El-Halees [14] in their case study discussed various EDM techniques to improve students’ performance.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "Agarwal [3], et al.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 2,
      "context" : "Kamal [4], et al.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "Carlos Márquez-Vera [5], et al.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "Al-Barrak [6], et al.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "Brijesh and Pal [7], found out using Bayesian classification that student SSC (metric) grade, living location, medium of instruction, mother qualification, student habits and type of family are the most important factors for the student performance.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "Al-Radaideh [10], et al.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "Mrinal Pandey and Vivek Kumar Sharma [11], compared classification methods to predict grades for undergraduate engineering students.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]",
      "startOffset" : 1,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Naïve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "Naïve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "Naïve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Naïve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Naïve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 2,
      "context" : "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 2,
      "context" : "JRip A [-1] 112 0 3 0 0 0 0 0 0 B [0] 0 328 7 0 0 0 0 0 0 C [4] 2 19 433 2 2 0 0 0 0 D [5] 0 3 162 0 1 0 0 0 1 E [6] 0 3 206 0 0 0 1 0 1 F [7] 4 2 107 1 0 3 1 0 1",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "JRip A [-1] 112 0 3 0 0 0 0 0 0 B [0] 0 328 7 0 0 0 0 0 0 C [4] 2 19 433 2 2 0 0 0 0 D [5] 0 3 162 0 1 0 0 0 1 E [6] 0 3 206 0 0 0 1 0 1 F [7] 4 2 107 1 0 3 1 0 1",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "JRip A [-1] 112 0 3 0 0 0 0 0 0 B [0] 0 328 7 0 0 0 0 0 0 C [4] 2 19 433 2 2 0 0 0 0 D [5] 0 3 162 0 1 0 0 0 1 E [6] 0 3 206 0 0 0 1 0 1 F [7] 4 2 107 1 0 3 1 0 1",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "JRip A [-1] 112 0 3 0 0 0 0 0 0 B [0] 0 328 7 0 0 0 0 0 0 C [4] 2 19 433 2 2 0 0 0 0 D [5] 0 3 162 0 1 0 0 0 1 E [6] 0 3 206 0 0 0 1 0 1 F [7] 4 2 107 1 0 3 1 0 1",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 6,
      "context" : "G [8] 0 3 30 0 0 1 1 0 0 H [9] 2 0 15 0 0 0 2 0 0 I [10] 2 0 13 0 0 0 0 0 0",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 6,
      "context" : "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0",
      "startOffset" : 234,
      "endOffset" : 238
    } ],
    "year" : 2016,
    "abstractText" : "Students opting engineering as their disciple is increasing rapidly. But due to various factors and inappropriate primary education in India dropout rates are high. Students are unable to excel in core engineering subjects which are complex and mathematical, hence mostly get drop / keep term (kt) in that subject. With the help of data mining techniques we can predict the performance of students in terms of grades and dropout for a subject. This paper compares various techniques such as naïve Bayes, LibSVM, J48, random forest, and JRip and try to choose one of them as per our needs and their accuracy. Based on the rules obtained from this technique(s), we derive the key factors influencing student performance. Keywords— dropout; prediction; classification; data mining; education.",
    "creator" : "Microsoft® Word 2013"
  }
}