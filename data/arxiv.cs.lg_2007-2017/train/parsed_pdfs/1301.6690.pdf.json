{
  "name" : "1301.6690.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Model based Bayesian Exploration",
    "authors" : [ "Richard Dearden", "Nir Friedman", "David Andre" ],
    "emails" : [ "dearden@cs.ubc.ca", "nir@cs.huji.ac.il", "dandre@cs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we investigate ways to represent and rea son about this uncertainty in algorithms where the sys tem attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q values based on these. These distributions are used to compute a myopic approximation to the value of infor mation for each action and hence to select the action that best balances exploration and exploitation.\n1 Introduction\nReinforcement learning addresses the problem of how an agent should learn to act in dynamic environments. This is an important learning paradigm for domains where the agent must consider sequences of actions to be made throughout its lifetime. The framework underlying much of reinforcement learning is that of Markov Decision Pro cesses (MOPs). These processes describe the effects of ac tions in a stochastic environment, and the possible rewards at various states of the environments. If we have an MOP we can compute the choice of actions that maximizes the expected future reward. The task in reinforcement learning is to achieve this level of performance when the underlying MOP is not known in advance.\nA central debate in reinforcement learning is over the use of models . . Model-free approaches attempt to learn near optimal policies without explicitly estimating the dynamics of the surrounding environment. This is usually done by directly approximating a value function that measures the desirability of each environment state. On the other hand, model-based approaches attempt to estimate a model of the environment's dynamics and use it to compute an estimate of the expected value of actions in the environment.\nA common argument for model-based approaches is that by learning a model the agent can avoid costly repetition of steps in the environment. Instead, the agent can use the model to learn the effects of its actions at various states. This can lead to a significant reduction in the number of steps actually executed by the learner, since it can \"learn\" from simulated steps in the model (Sutton 1990).\nVirtually all of the existing model-based approaches in the literature use simple estimation methods to learn the en vironment, and keep a point-estimate of the environment dynamics. Such estimates ignore the agent's uncertainty about various aspects of the environment's dynamics.\nIn this paper, we advocate a Bayesian approach to model based reinforcement learning. We show that under fairly reasonable assumptions we can represent the posterior dis tribution over possible models given our past experience. This is done with essentially the same cost as maintaining point estimates. Our methods thus allow us to continually update this distribution over possible models as we perform actions in the environment.\nBy representing a distribution over possible models , we can quantify our uncertainty as to what are the best actions to perform. This gives us a handle on the exploitation vs. exploration problem. Roughly speaking, this problem in volves the dilemma of whether to explore- perform new actions that can lead us to uncharted territories- or to ex ploit- perform actions that have the best performance ac cording to our current knowledge. Clearly, the uncertainty about our model and our expectations as to the range of pos sible results of actions play crucial roles in this problem.\nIn a precursor to this work, Dearden et a!. (1998) intro duce a Bayesian model-free approach in which uncertainty about the Q-values of actions is represented using probabil ity distributions. By explicitly reasoning using uncertainty about Q-values, they direct exploration specifically toward poorly known regions of the state space. Their approach is based on a decision-theoretic approach to action selec tion: the agent should choose actions based on the value of the information it can expect to learn by performing them (Howard 1966). Dearden et a!. propose a measure that bal ances the expected gains in performance from exploration - in the form of improved policies - with the expected\ncost of doing a potentially suboptimal action. This mea sure is computed from probability distributions over the Q values of actions.\nIn this paper, we show how to use the posterior distri bution over possible models to estimate the distribution of possible Q-values, and then use these to select actions. This use of models allows us to avoid the problem faced by model-free exploration methods, such as the one used by Dearden et al., that neect to perform repeated actions to propagate values from one state to another. The main ques tion is how to estimate these Q-values from our distribu tion of possible models. We present several methods of stochastic sampling to approximate these Q-value distribu tions. We then evaluate the performance of the resulting Bayesian learning agents on test environments that are de signed to fool many exploration methods.\nIn Section 2 we briefly review the definition of MOPs and the definition of reinforcement learning problems. In Sec tion 3 we discuss a Bayesian approach for learning models. In Section 4 we review the notion of Q-value distributions and the use of value of information for directing exploration and the notion. In Section 5 we propose several sampling methods for estimating Q-value distributions based on the uncertainty about the underlying model. In Section 6 we discuss several approaches of generalizing from the sam ples we get from the aforementioned methods, and how this generalization can improve our algorithms. In Section 7 we compare our methods to Prioritized Sweeping (Moore & Atkeson 1993), a well known model-based reinforcement learning procedure.\n2 Background We assume the reader is familiar with the basic concepts of MOPs (see, e.g., (Kaelbling, Littman & Moore 1996)). We will use the following notation: An MOP is a 4-tuple, (S,A,J1T,PR) where Sis a set of states, A is a set of ac tions, PT ( sl:tt) is a transition model that captures the prob ability of reaching state t after we execute action a at state s, and PR(s.i:tr) is areward model that captures the proba bility of receiving reward rafter executing a at state s. For the reminder of this paper, we assume that possible rewards are a finite subset n of the real numbers.\nIn this paper, we focus on infinite-horizon MOPs with a discount factor I· The agent's aim is to maximize the ex pected discounted total reward it receives. Equivalently, we can compute a optimal value function V* and a Q-function Q*. These functions satisfy the Bellman equations:\nwhere\nV*(s) = maxQ*(s, a), aE.A\nQ*(s, a)= E ( • )[ris, a]+ 1\" PT(s.i:ts')V*(s'). PR 3-+r .l...J 31ES If the agent has access to v• or Q*, it can optimize its ex pected reward by choosing the action a at s that maximizes\nModel based Bayesian Exploration 151\nQ* (s, a) . Given a model, we can compute Q* using a va riety of methods, including value iteration. In this method we repeatedly update an estimate Q of Q* by applying the Bellman equations to get new values of Q(s) for some (or all) of the states.\nReinforcement learning procedures attempt to achieve an optimal policy when the agent does not know PT and PR· Since we do not know the dynamics of the underlying MOP, we cannot compute the Q-value function directly. However, we can estimate it. In model-free approaches one usually es timates Q by treating each step in the environment as a sam ple from the underlying dynamics. These samples are then used for performing updates of the Q-values based on the Bellman equations. In model-based reinforcement learning one usually directly estimates PT (s.i:tt) and PR(sl:tr). The standard approach is then to act as though these approxima tions are correct, compute Q*, and use it to choose actions.\nA standard problem in learning is balancing between planning (i.e., choosing a policy) and execution. Ideally, the agent would compute the optimal value function for its model of the environment each time it updates it. This scheme is unrealistic since finding the optimal policy for a given model is a non-trivial computational task. Fortu nately, we can approximate this scheme if we notice thatthe approximate model changes only slightly at each step. We can hope that the value function from the previous model can be easily \"repaired\" to reflect these changes. This ap proach was pursued in the DYNA (Sutton 1990) frame work, where after the execution of an action, the agent updates its model of the environment, and then performs some bounded number of value propagation steps to up date its approximation of the value function. Each value propagation step locally enforces the Bellman-equation by setting V(s) +-- maXae.A Q(s, a), where Q(s, a) E[PR(s.i:tr)] + 1 Ls'eS fiT(s.i:ts')V(s'), fiT(s.i:ts') and PR(s.:;.r) are the agent's approximate model, and Vis the agent's approximation of the value function.\nThis raises the question of which states should be up dated. Prioritized Sweeping (Moore & Atkeson 1993) is a method that estimates to what extent states would change their value as a consequence of new knowledge of the MDP dynamics or previous value propagations. States are as signed priorities based on the expected size of changes in their values, and states with the highest priority are the ones for which we perform value propagation.\n3 Bayesian Model Learning\nIn this section we describe how to maintain a Bayesian pos terior distribution over MOPs given our experiences in the environment. At each step in the environment, we start at state s, choose an action a, and then observe a new state t and a reward r. We summarize our experience by a se quence of experience tuples (s, a, r, t).\nA Bayesian approach to this learning problem is to main tain a belief state over the possible MOPs. Thus, a belief state Jl. defines a probability density P(M I Jl.). Given an\n152 Dearden, Friedman, and Andre\nexperience tuple (s, a, r, t) we can compute the posterior belief state, which we denote fJ o (s, a, r, t), by Bayes rule:\nP(M I fJ o (s, a, r, t)) oc P( (s, a, r, t) I M) P(M I fJ)\nP(s-'!tt I M) P(s-'!tr I M) P(M I fJ). Thus, the Bayesian approach starts with some prior prob ability distribution over all possible MDPs (we assume that the sets of possible states, actions and rewards are delim ited in advance). As we gain experience, the approach fo cuses the mass of the posterior distribution on those MOPs in which the observed experience tuples are most probable.\nAn immediate question is whether we can represent these prior and posterior distributions over an infinite number of MOPs. We show that this is possible by adopting re sults from Bayesian learning of probabilistic models, such as Bayesian networks (Heckerman 1998). Under carefully chosen assumptions, we can represent such priors and pos teriors in any of several compact manners. We discuss one such choice below.\nTo formally represent our problem, we consider the pa rameterization of MOPs. The simplest parameterization is table bas�, where there are parameters e;,a,t and e:,a,r for the transitiOn and reward models. Thus, for each choice of s and a, the parameters e; ,a = { e; ,a,t : t E S} de fine a distribution over possible states, and the parameters e:,a = { e:,a,r : r E 'R.} define a distribution over possible rewards.1\nWe say that our prior satisfies parameter independence if it has the product form:\nPr(O I fJ) =II II Pr(e;,a I fJ) Pr(O�.a I fJ). (I) a Thus, the prior distribution over the parameters of each lo cal probability term in the MOP is independent of the prior over the others. It turns out that this form is maintained as we incorporate evidence. Proposition 3.1: lfthebeliefstate P(O I fJ) satisfies param eter independence, then P ( e I fJ o ( s, a, r, t)) also satisfies parameter independence.\nAs a consequence, the posterior after we incorporate an ar bitrarily long number of experience tuples also has the prod uct form of(!).\nParameter independence allows us to reformulate the learning problem as a collection of unrelated local learning problems. In each of these, we have to estimate a probabil ity distribution over all states or all rewards. The question is how to learn these distributions. We can use well-known Bayesian methods for learning standard distributions such as multinomials or Gaussian distributions (Degroot 1986).\n1 The methods we describe are easily extend to other param eterizations. In particular, we can consider continuous distribu tions, e.g., Gaussians, over rewards. For clarity of discussion, we focus on multinomial distributions throughout the paper.\nFor the case of discrete multinomials, which we have assumed in our transition and reward models, we can use D�richlet prior\n_ s to represent Pr(e;,a) and Pr(e�,al· These\npnors are conjugate, and thus the posterior after each ob served experience tuple will also be a Dirichlet distribution. In addition, Dirichlet distributions can be described using a small number of hyper-parameters. See Appendix A for a review of Dirichlet priors and their properties.\nIn the case of most MOPs studied in reinforcement learn ing, we expect the transition model to be sparse-there are only a few states that can result from a particular action at a particular state. Unfortunately, if the state space is large, learning with a Dirichlet prior can require many examples to recognize that most possible states are highly unlikely. This problem is addressed by a recent method of learn ing sparse-multinomial priors (Friedman & Singer 1999). Without going into details, the sparse-multinomial priors have the same general properties as Dirichlet priors, but as sume that the observed outcomes are from some small sub sets of the set of possible ones. The sparse Dirichlet priors make predictions as though only the observed outcomes are possible, except that they also assign to novel outcomes. In the MOP setting, a novel outcome is a transition to state t that was not reached from s previously by executing a. See Appendix A for a brief summary of sparse-multinomial pri ors and their properties.\nFor both the Dirichlet and its sparse-multinomial exten sion, we need to maintain the number of times, N(s-'!tt) , state t is observed after executing action a at state s, and similarly, N ( s-'!tr) for rewards. With the prior distributions over the parameters of the MOP, these counts define a poste rior distribution over MOPs. This representation allows us to both predict the probability of the next transition and re ward, and also to compute the probability of every possible MOP and to sample from the distribution of MOPs.\nTo summarize, we assumed parameter independence, and that for each prior in (I) we have either a Dirichlet or sparse multinomial prior. The consequence is that the posterior at each stage in the learning can be represented compactly. This enables us to estimate a distribution over MOPs at each stage.\nIt is easy to extend this discussion for more compact pa rameterizations of the transition and reward models. For example, if each state is described by several attributes, we might use a Bayesian network to capture the MOP dynam ics. Such a structure requires fewer parameters and thus we can learn it with fewer examples. Nonetheless, much of the above discussion and conclusions about parameter independence and Dirichlet priors apply to these models (Heckerman 1998). Standard model-based learning methods maintain a point\nestimate of the model. These point estimates are often close to the mean prediction of the Bayesian method. However, these point estimates do not capture the uncertainty about the model. In this paper, we examine how knowledge of this uncertainty can be exploited to improve exploration.\n4 Value of Information Exploration\nIn a recent paper, Dearden et al. (1998) examined model free Bayesian reinforcement learning. Their approach builds on the notion of Q-value distributions. Recall, that Q* ( s, a) is the expected reward if we execute a at s and then continue with optimal selection of actions. Since dur ing learning we are uncertain about the model, there is a dis tribution over the Q-values at each pair (s, a). This distri bution is induced by the belief state over possible MDPs, and the Q-values for each of these MDPs. In the model free case, Dearden et al. propose an approach for estimat ing Q-value distributions without building a model. This approach makes several strong assumptions that are clearly violated in MDPs. In the next section, we show how we can use our representation of the posterior over models to give estimates of Q-value distributions. Before we do that, we briefly review how Dearden et al. use the Q-value distri butions for selecting actions, as we use this method in the current work.\nThe approach of Dearden et al is based on the decision theoretic ideas of value ofinformation (Howard !966). The application of these ideas in this context is reminiscent of its use in tree search (Russell & Wefald 1991 ), which can also be seen as a form of exploration. The idea is to balance the expected gains from exploration-in the form of improved policies-against the expected cost of doing a potentially suboptimal action.\nTo formally define the approach, we need to introduce some notation. We denote by q,,a a possible value of Q*(s, a) in some MDP. We treat these quantities as ran dom variables that depend on our belief state. (For clarifica tion of the following discussion, we do not explicitly refer ence the belief state in the mathematical notation.) We now consider what can be gained by learning the true value q;,a of q,,a. How would this knowledge change the agent's fu ture rewards? Clearly, if this knowledge does not change the agent's policy, then future rewards would not change. Thus, the only interesting scenarios are those where the new knowledge does change the agent's policy. This can happen in two cases: (a) when the new knowledge shows that an action previously considered sub-optimal is revealed as the best choice (given the agent's beliefs about other actions), and (b) when the new knowledge indicates that an action that was previously considered best is actually inferior to other actions.\nFor case (a), suppose that a1 is the best action; that is, E[q,,a,J ?: E[q,,a•] for all other actions a'. Moreover sup pose that the new knowledge indicates that a is a better ac tion; that is, q;,a > E[q,,aJ Thus, we expect the agent to gain q;,a-E[q,,a,] by virtue of performing a instead of a*.\nFor case (b), suppose that a 1 is the action with the highest expected value and a2 is the second-best action. If the new knowledge indicates that q,,a, < E[q,,a,]. then the agent should perform a2 instead of a1 and we expect it to gain E[q,,a,]- q;,a,·\nCombining these arguments, the gain from learning the\nModel based Bayesian Exploration 153\nvalue of q;,a of q,,a is:\nif a= a1 and q;,a < E[q,,a,] if a =f. a1 and q;,a > E[q,,a,] otherwise\nwhere, again, a1 and a2 are the actions with the best and second best expected values respectively. Since the agent does not know in advance what value will be revealed for q; a, we need to compute the expected gain given our prior b�liefs. Hence the expected value of perfect information about q,,a is:\n00\nVPI(s, a)= J Gains,a(x) Pr(q,,a = x)dx (2) -oo\nThe computation of this integral depends on how we repre sent our distributions over q,,a. We return to this issue be low.\nThe value of perfect information gives an upper bound on the myopic value of information for exploring action a. The expected cost incurred for this exploration is given by the difference between the value of a and the value of the cur rent best action, i.e., maxa• E[q,,a•] - E[q,,a]· This sug gests we choose the action that maximizes\nVPI(s,a)- (m'!-xE[q,,a•]-E[q,,a]). a\nClearly, this strategy is equivalent to choosing the action that maximizes:\nE[q,,a] + VPI(s, a).\nWe see that the value of exploration estimate is used as a way of boosting the desirability of different actions. When the agent is confident of the estimated Q-values, the VPI of each action is close to 0, and the agent will always choose the action with the highest expected value.\n5 Estimating Q-Value Distributions\nHow do we estimate the Q-value distributions? We now ex amine several methods of different complexity and bias.\n5.1 Naive Sampling Perhaps the simplest approach is to simulate the definition of a Q-value distribution. Since there are an infinite number of possible MDPs, we cannot afford to compute Q-values for each. Instead, we sample k MDPs: M1, . .. , Mk from the distribution Pr(M I p.). We can solve each MOP us ing standard techniques (e.g., value iteration or linear pro gramming). For each state s and action a, we then have a I I t. t k h ; . h . IQ samp e so u ton q,,a, ... , q,,a, w ere q,,a IS t e optima - value, Q*(s, a), given the i'th MDP. From this sample we can estimate properties of the Q-distribution. For general if)', we denote the weight of each sample, given p., as w�. Initially these weights are all equal to I.\n154 Dearden, Friedman, and Andre\nGiven these samples, we can estimate the mean Q-value as\nE[ ] � 1 \" ; ; q,,a I\"V � L.....t wp.q,,a· L...J i WJJ i\nSimilarly, we can estimate the VPI by summing over the k MDPs:\nVPI(s, a)� -L 1 . \" w; Gain, 0(q; ) . ·W' L...J p. , ,,a I JJ i This approach is straightforward; however, it requires an efficient sampling procedure. Here again the assumptions we made about the priors helps us. If our prior has the form of (1), then we can sample each distribution (pT(s�t) or PR(s�r)) independently of the rest. Thus, the sampling problem reduces to sampling from \"simple\" posterior dis tributions. For Dirichlet priors there are known sampling methods. For the sparse-multinomials the problem is a bit more complex, but solvable. In Appendix A we describe both sampling methods.\n5.2 Importance Sampling An immediate problem with the naive sampling approach is that it requires several global computations (e.g., comput ing value functions for MDPs) to evaluate each action made by the agent. This is clearly too expensive. One possible way of avoiding these repeated computations is to reuse the same sampled MDPs for several steps. To do so, we can use ideas from imponance sampling.\nIn importance sampling we want to a sample from Pr(M I p/) but for some reasons, we actually sample from Pr(M I ll)· We adjust the weight of each sample appro priately to correct for the difference between the sampling distribution (e.g., Pr(M I ll)) and the target distribution (e.g., Pr(M Ill')):\n; _ Pr(M; Ill') ; w�, - Pr(Mi Ill) Ww We now use the weighted sum of samples to estimate the mean and the VPI of different actions. It is easy to verify that the weighted sample leads to correct prediction when we have a large number of samples. In practice, the success of importance sampling depends on the difference between the two distributions. If an MDP M has low probability ac cording to Pr(M Ill), then the probability of sampling it is small, even ifPr(M Ill') is high.\nFortunately for us, the differences between the beliefs before and after observing an experience tuple are usually small. We can easily show that Proposition 5.1:\nW�o(s,a,r,t) Pr(M Ill o (s,a,r,t)) w Pr(M I ll) � Pr( (s, a, r, t) I M) Pr((s, a, r, t) Ill) w�\nThe term Pr( (s, a, r, t) I M) is easily computed from M, and Pr( (s, a, r, t) Ill) can be easily computed based on our posteriors. Thus, we can easily re-weight the sampled mod els after each experience is recorded and use the weighted sum for choosing actions. Note that re-weighting of models is fast, and since we have already computed the Q-value for each pair (s, a) in each of the models, no additional compu tations are needed.\nOf course, the original set of models we sampled be comes irrelevant as we learn more about the underlying MDP. We can use the total weight of the sampled MDPs to track how unlikely they are given the observations. Ini tially this weight is k. As we learn more it usually be comes smaller. When it becomes smaller than some thresh old kmin, we sample k - kmin new MDPs from our current belief state, assigning each one weight 1 and thus bringing the total weight of the sample to k again. We then need only to solve the newly sampled MDPs.\nTo summarize, we sample k MDPs, solve them, and use the k Q-values to estimate properties of the Q-value distri bution. We re-weight the samples at each step to reflect our newly gained knowledge. Finally, we have an automatic method for detecting when new samples are required.\n5.3 Global Sampling with Repair The global sampling approach of the previous section has one serious deficiency. It involves computing global solu tions tc. MDPs which can be very expensive. Although we can reuse MDPs from previous steps, this approach still re quires us to sample new MDPs and solve them quite often.\nAn alternative idea is to keep updating each of the sam pled MDPs. Recall that after observing an experience tuple (s, a, r, t), we only change the posterior over o; and gr . .,,a 6,a Thus, instead of re-weighting the sample M;, we can up date, or repair, it by re-sampling o;,. and��� .•. If the orig-\n-;\nina) sample M; was sampled from Pr(M I J.l), then it eas ily follows that the repaired Mi is sampled from Pr(M I J.l o (s, a, r, t)).\nOf course, once we modify M; its Q-value function changes. However, all of these changes are consequences of the new values of the dynamics at ( s, a) . Thus, we can use prioritized sweeping to update the Q-value computed for M;. This sweeping performs several Bellman updates to correct the values of states that are affected by the change in the model. 2\nThis suggests the following algorithm. Initially, we sam ple k MDPs from our prior belief state. At each step we:\n• Observe an experience tuple ( s, a, r, t) • Update Pr(o;,a) by t, and Pr(ll�,a) by r. • For each i = 1, ... , k, sample o;·,�, II�:� from the new\nPr(ll;,a) and Pr(ll�,a), respectively. • For each i = 1, ... , k run a local instantiation of prior\nitized sweeping to update the Q-value function of M;.\nThus, our approach is quite similar to standard model based learning with prioritized sweeping, but instead of run ning one instantiation of prioritized sweeping, we run k in stantiations in parallel, one for each sampled MDP. The re pair to the sampled MDPs ensures that they constitute a sample from the current belief state, and the local instantia tions of prioritized sweeping ensure that the Q-values com puted in each of these MDPs is a good approximation to the true value.\nAs with the other approaches we have described, after we invoke the k prioritized sweeping instances we use the k samples from each q,,a to select the next actions using VPI computations.\nFigure I shows a single run of learning where the actions selected were fixed and each of the three methods was used to estimate the Q-values of a state. Initially the means and variances are very high, but as the agent gains more experi ence, the means converge on the true value of the state, and the variances tend towards zero. These results suggest that the repair and importance sampling approaches both pro vide reasonable approximations to naive global sampling.\n5.4 Local Sampling Until now we have considered using global samples of MDPs. An alternative approach is to try to maintain for each ( s, a) an estimate of the Q-value distribution, and to update these distributions using. a local, Bellman-update like, propagation rule. To understand this approach, recall the Bellman equation:\nq,,a = E[pR(s.!+r)] + 1 L: PT(s.!+s') m'l'xq,•,a'· a 61ES 2Generalized prioritized sweeping (Andre, Friedman & Parr 1997) allows us to extend prioritized sweeping to these approx imate settings. When using approximate models or value func tions, one must address the problem of calculating the states on which to#�timate the priority.\nModel based Bayesian Exploration 155\nIn our current setting, the terms q,',a' are random vari ables that depend on our current estimate of Q-value dis tributions. The probabilities PT ( s.!+s') are also random variables that depend on our posterior on o; ,a, and finally E[pR(s.!+r)] is also a random variable that depends on the posterior on ll�,a· Thus, we can sample from q,,a, by jointly sampling from all of these distributions, i.e., q,',a' for all states, PT(s.!+s'), and PR(s.!+r)., and then computing the Q-value. If we repeat this sampling step k times, we get k samples from a single bellman iteration for q,,a.\nStarting with our beliefs about the model and about the Q-value distribution of all states, we can sample from the distribution of q,,a. To make this procedure manageable, we assume that we can sample from each q,',a' indepen dently. This assumption does not hold in general MDPs, since the distribution of different Q-values are correlated (by the Bellman equation). However, we might hope that the exponential decay will weaken these dependencies.\nWe are now left with the question how to use the k sam ples from q, ,a. The simplest approach is to use the sam ples as a representation of our approximation of the distri bution of q,,a. We can compute the mean and VPI from a set of samples, as we did in the global sampling ap proach. Similarly, we can re-sample from this represen tation by randomly choosing one of the points. This re sults in a method that is similar to recent sampling methods that have been used successfully in monitoring complex dy namic processes (Kanazawa, Koller & Russell 1995).\nThis gives us a method for performing a Bellman-update on our Q-value distributions. To get a good estimate of these distributions we need to repeat these updates. Here we can use a prioritized sweeping like algorithm that performs updates based on an estimate of which Q-value distribution can be most affected by the updates to other Q-value distri butions.\n6 Generalization and Smoothing\nIn the approaches described above we generated samples from the Q-value distributions, and effectively used a col lection of points to represent the approximation to the Q Value distribution. A possible problem with this represen tation approach is that we use a fairly simplistic representa tion to describe a complex distribution. This suggests that we should generalize from the k samples by using standard generalization methods.\nThis is particularly important in the local sampling ap proach. Here we also use our representation of the Q-value distribution to propagate samples for other Q-value distri butions. Experience from monitoring tasks in stochastic processes suggest that introducing generalization can dras tically improve performance (Koller & Fratkina 1998).\nPerhaps the simplest approach to generalize from the k samples is to assume that the Q-value distribution has a par ticular parametric form, and then to fit the parameters to the samples. The first approach that comes to mind is fit ting a Gaussian to the k samples. This captures the first two\n156 Dearden, Friedman, and Andre\n\"\" ,-----,.._.,.,,-----o:,.,=,r.,c=:=-> Glu .. liln 1\\pprDX - �n8 es11mat10n -M·-·· \"\"' \" \"' \"\"'\n.i 005 ! 004\n\"\"'\n\"'\nJ \" \"\n\" . ' i < \"\n\"'\nL-���mw����� .. -�� ·.�-7--7.,-�,��-�-� \"'\"' Figure 2: Samples, Gaussian approximation, and Kernel estimates of a Q-value distribution after 100, 300, and 700 steps of Naive global sampling on the same run as Figure 1.\nmoments of the sample, and allows simple generalization. Unfortunately, because of the max() terms in the Bellman equations, we expect the Q-value distribution to be skewed to the positive direction. If this skew is strong, then fitting a Gaussian would be a poor generalization from the sample.\nAt the other end of the spectrum are non-parametric ap proaches. One of the simplest ones is K erne/ estimation (see for example (Bishop 1995)). In this approach, we ap proximate the distribution over Q ( s, a) by a sum of Gaus sians with a fixed variance, one for each sample. This ap proach can be effective if we are careful in choosing the variance parameter. A too small variance, will lead to a spiky distribution, a too large variance, will lead to an overly smooth and flat distribution. We use a simple rule for estimating the kernel width as a function of the mean (squared) distance between points.3\nOf course, there are many other generalization methods we might consider using here, such as mixture distributions. However, these two approaches provide us with initial ideas on the effect of generalization in this context.\nWe must also compute the VPI of a set of generalized dis tributions made up of Gaussians or kernel estimates. This is simply a matter of solving the integral given in Equation 2\n3This rule is motivated by a leave-one-out cross-validation es timate of the kernel widths. Let q1, .•. , q\" be the k samples. We want to find the kernel width 17 that maximizes the tenn\n1(172) = 2:log(2:f(q'lq',172)) ;:#i\nwhere f(q'lq1, 17) is the Gaussian pdf with mean q1 and variance 172• Using Jensen's inequality, we have that\n1(172) � 2: 2: log f(q' I<T', 172 ) I ;;t.i\nProposition 6.1 : The. value of 172 that mnximizes I;, I:H'i log f(q' I <I, 172) is td. where dis the average distance among samples:\nd = k(k � 1) 2: :E(q'- q')2 ' J:¢1\nwhere Pr(q,,a = x) is computed from the generalized prob ability distribution for state s and action a. This integration can be simplified to a term where the main cost is an evalua tion of the cdf of a Gaussian distribution (e.g., see (Russell & Wefald 1991). This function, however, is implemented in most language libraries (e.g., using the erf() function in the C-library), and thus can be done quite efficiently.\nFigure 2 shows the effects of Gaussian approximation and kernel estimation smoothing (using the computed ker nel width) on the sample values used to generate the Q distributions in Figure 1 for three different time steps. Early in the run Gaussian approximation produces a very poor ap proximation because the samples are quite widely spread and very skewed, while kernel estimation provides a much better approximation to the observed distribution. For this reason, we expect kernel estimation to perform better than Gaussian approximation for computing VPI.\n7 Experimental Results\nFigure 3 shows two domains of the type on which we have tested our algorithms. Each is a four action maze domain in which the agent begins at the point marked S and must collect the flag F and deliver it to the goal G. The agent re ceives a reward of 1 for each flag it collects and then moves to the goal state, and the problem is then reset. If the agent enters the square marked T (a trap) it receives a reward of -10. Each action (up, down, left, right) succeeds with prob ability 0.9 if that direction is clear, and with probabilityO.l , moves the agent perpendicular to the desired direction. The \"trap\" domain has 18 states, the \"maze\" domain 56.\nWe evaluate the algorithms by computing the average (over 10 runs) future discounted reward received by the agent. We use this measure rather than the value of the learned policy because exploratory agents rarely actually follow either the greedy policy they have discovered or their current exploration policy for very long. For comparison we use prioritized sweeping (Moore & Atkeson 1993) with the Tbored parameter optimized for each problem.\nFigure 4 shows the performance of a representative sam ple of our algorithms on the trap domain. Unless they are based on a very small number of samples, all of the Bayesian exploration methods outperform prioritized\nModel based Bayesian Exploration 157\nsweeping. This is due to their more cautious approach to the trap state. Although they are uncertain about it, they know that its value is probably bad, and hence do not explore it further after a small number of visits.\nFigure 5 compares prioritized sweeping with our Q-value estimation techniques on the larger maze domain. As the graph shows, our techniques perform better than prioritize sweeping early in the learning process. They explore more widely initially, and do a better job of avoiding the trap state once they find it. Of the three techniques, global sampling performs best, although its computational requirements are considerable - about ten times as much as sampling with repair. Importance sampling runs about twice as fast as global sampling but converges relatively late on this prob lem, and did not converge on all trials.\nFigure 6 shows the relative performance of the three smoothing methods, again on the larger domain. To exag gerate the effects of smoothing, only 20 samples were used to produce this graph. Kernel estimation performs very well, while no smoothing failed to find the optimal (two flag) strategy on two out of ten runs. Gaussian approxima tion was slow to settle on a policy, it continued to make ex ploratory actions after 1500 steps while all the other algo rithms had converged by then.\nWe are currently investigating the performance of the al gorithm on both more complex maze domains and random MOPS, and also the effectiveness of the local sampling ap proach we have described.\n8 Discussion\nThis paper makes two main contributions. First, we show how to maintain Bayesian belief states about MOPs. We show that this can be done in a simple manner by using ideas that appear in Bayesian learning of probabilistic mod els. Second, we discuss how to use the Bayesian belief state to choose actions in a way that balances exploration and ex ploitation. We adapt the value of information approach of Dearden et a!. (1998) to this model-based setup and show how to approximate the Q-value distributions needed for making these choices.\nA recent approach to exploration that is related to our work is that of Kearns and Singh ( 1998). Their approach divides the set of states in to two groups. The known states are ones for which the learner is quite confident about the transition probabilities. That is, the learner believes that its estimate of the transition probabilities is close enough to the true distribution. All other states are considered un known states. In Kearns and Singh's proposal, the learner constructs a policy over the known states. This policy takes into account both exploitation and the possibility of find ing better rewards in unknown states (which are considered as highly-rewarding). When it finds itself in an unknown state, the agent chooses actions randomly. The algorithm proceeds in phases, after each one it reclassifies the states and recomputes the policy on the known states. Kearns and Singh's proposal is significant in that it is the first one for\n158 Dearden, Friedman, and Andre\nwhich we have polynomial guarantees on number of steps needed to get to a good policy. However, this algorithm was not implemented or tested, and it is not clear how fast it learns in real domains."
    }, {
      "heading" : "O ur exploration strategy also keeps a record of how con",
      "text" : "fident we are in each state (i.e., Bayesian posterior), and also chooses actions based on their expected rewards (both known rewards, and possible exploration rewards). The main difference is that we do not commit to a binary classi fication of states, but instead choose actions in a way that takes into account the possible value of doing the explo ration. This leads to exploitation, even before we are ex tremely confident in the dynamics at every state in the \"in teresting\" parts of the domain.\nThere are several directions for future research. First, we are currently conduc;ing experiments on larger domains to show how our method scales up. We are also interested in applying it to more compact model representations (e.g., us ing dynamic Bayesian networks), and to problems with con tinuous state spaces.\nFinally, the most challenging future direction is to deal with the actual value of information of an action rather than myopic estimates. This problem can stated as an MDP over belief states. However, this MDP is extremely large, and requires some approximations to find good policies quickly. Some of the ideas we introduced here, such as the re-weighting of sampled MDPs might allow us to address this computational task.\nAcknowledgements\nWe are grateful for useful comments from Craig Boutilier and Stuart Russell. Richard Dearden was supported by a Killam Predoctoral fellowship and by IRIS Phase-III project \"Dealing with Actions\" (BAC). Some of this work was done while Nir Friedman was at U.C. Berkeley. Nir Friedman and David Andre were supported in part by ARO under the MURI program \"Integrated Approach to Intelligent Systems\", grant number DAAH04-96-l-0341, and by ONR under grant number N00014-97-1-0941. Nir Friedman was also supported through the generosity of the M ichael Sacher Trust. David Andre was also supported by a DOD National Defense Science and Engineering Grant.\nA Dirichlet and Sparse-Multinomial Priors\nLet X be a random variable that can take L possible values from a set E. Without loss of generality, let E = { I , ... L}. We are given a training set D that contains the outcomes of N independent draws x1, ... , xN of X from an unknown multinomial distribution P*. The multinomial estimation problem is to find a good approximation for p•.\nThis problem can be stated as the problem of predicting the outcome xN+I given x1, ... , xN. Given a prior dis tribution over the possible multinomial distributions, the Bayesian estimate is:\n= J P(xN+I I IJ, �)P(IJ I xl' ... ' xN' �)diJ (3) where IJ = (IJ1, ... , IJL) is a vector that describes possible values of the (unknown) probabilities P*(l), ... , P*(L), and � is the \" context\" variable that denote all other assump tions about the domain.\nThe posterior probability of IJ can be rewritten as: P(!Jix1, ... ,xN,�) oc P(x1, ... ,xN IIJ,�)P(IJI<)\nP(IJ 1 �)II o{\"•, (4) where N; is the number of occurrences of the symbol i in the training data.\nDirichlet distributions are a parametric family that is conjugate to the multinomial distribution. That is, if the prior distribution is from this family, so is the posterior. A Dirichlet prior for X is specified by hyper-parameters aq, .. . , aL, and has the form:\nP(IJ I 0 ()( II or·- I (2:8; =I and 8; � 0 for all i ) i\nwhere the proportion depends on a normalizing constant that ensures that this is a legal density function (i.e., inte gral of P(O 1 �) over all parameter values is 1). Given a Dirichlet prior, the initial prediction for each value of X is\nIt is easy to see that, if the prior is a Dirichlet prior with hyper-parameters a1, ... , aL, then the posterioris aDirich let with hyper-parameters a1 + N 1 , . .. , aL + N L. Thus, we get that the prediction for X N + 1 is\n( N+l · I 1 N ) a;+ N; PX =t x, ... ,x .� =\"' ( . N·)· L..j aJ + J In some situations we would like to sample a vector IJ ac cording to the distribution P(B I �). This can be done us ing a simple procedure: Sample values y 1 , ... , YL such that each y; � Gamma( a;, I) and then normalize to get a prob ability distribution, where Gamma( a, {3) is the Gamma dis tribution. Procedures for sampling from these distributions can be found in (Ripley 1987).\nFriedman and Singer (1999) introduce a structured prior that captures our uncertainty about the set of\"feasible\" val ues of X. Define a random variable V that takes values from the set 2E of possible subsets of E. The intended se mantics for this variable, is that if we know the value of V, then B; > 0 iff i E V.\nClearly, the hypothesis V = E' (for E' <:;; E) is consis tent with training data only if E' contains all the indices i for which N; > 0. We denote by E0 the set of observed sym bols. That is, E0 = {i: N; > 0}, and we let k0 = IE01. Suppose we know the value of V. Given this assumption, we can define a Dirichlet prior over possible multinomial\ndistributions (} if we use the same hyper-parameter a for each symbol in V. Formally, we define the prior:\nP(!JIV) ex II !Jf-1 (L= IJ; = 1 and IJ; = 0 for all i � V) iEV\nUsing Eq. (4), we have that: P(XN+1 = i 1 x1 , . . . , x\", V) = { b r*Njy (5)\nifi E V otherwise\n(6) Now consider the case where we are uncertain about the\nactual set of feasible outcomes. We construct a two tiered prior over the values of V. We start with a prior over the size of V, and assume that all sets of the same cardinality have the same prior probability. We let the random variable S denote the cardinality of V. We assume that we are given a distribution P(S = k) fork = 1, . . . , L. We define the prior over sets to be P(V I S = k) = (�) -1. This prior is a sparse-multinomial with parameters a and Pr(S = k).\nFriedman and Singer show that how we can efficiently predict using this prior. Theorem A.l: (Friedman & Singer 1999) Given a sparse multinomial prior, the probability of the.next symbol is\nP(xN+1 = i I D)= { k�t�JvC(D, L) n!k. ( 1- C(D, L)) where\nMoreover,\nwhere\nL k0a + N C(D,L) = L ka+N P(k I D). k=k0\nif i E y:,o ifi tf_ y:,o\nk! r(ka) mk = P(S = k) (k- k?)! r(ka + N)\nand r(x) = J000 tx-1c1dt is the gamma function. Thus, \"L k\"a+N\nC(D, L) = L..k::k• ka+N mk Lk'�k· mk We can think of C(D, L) as scaling factor that we apply to the Dirichlet prediction that assumes that we have seen all of the feasible symbols. The quantity 1 - C( D, L) is the probability mass assigned to novel (i.e., unseen) outcomes.\nIn some of the methods discussed above we need to sam ple a parameter vector from a sparse-multinomial prior. Probable parameter vectors according to such a prior are sparse, i.e., contain few non-zero entries. The choice of the non-zero entries among the outcomes that were not ob served is done with uniform probability. This presents a\nModel based Bayesian Exploration 159\ncomplication since each sample will depend on some unob served states. To \"smooth\" this behaviour we sample from the distributionover V\" combined with the novel event. We sample a value of k from P(S = kiD). We then, sam ple from the Dirichlet distribution of dimension k where the first k0 elements are assigned hyper-parameter a+ N;, and the rest are assigned hyper-parameter a. The sampled vec tor of probabilities describes the probability of outcomes in vo and additional k - k\" events. We combine these latter probabilities to be the probability of the novel event.\nReferences\nAndre, D., Friedman, N. & Parr, R. (1997), Generalized priori tized sweeping, in 'Advances in Neural information Process ing Systems·, Vol. I 0.\nBishop, C. M. (1995), Neural Networks for Pattern Recognition, Oxford University Press, Oxford.\nDearden, R., Friedman, N. & Russell, S. (1998), Bayesian Q leaming, in 'Proceedings of the Fifteenth National Confer ence on Artificial intelligence (AAAI-98)'.\nDegroot, M. H. (1986), Proability and Statistics, 2nd edn, Addison-Wesley, Reading, Mass.\nFriedman, N. & Singer, Y. (1999), Efficient bayesian parameter estimation in large discrete domains, in 'Advances in Neu ral Information Processing Systems II', MIT Press, Cam bridge, Mass.\nHeckerman, D. (1998), A tutorial on learning with Bayesian net works, in M. I. Jordan, ed., 'Learning in Graphical Models', Kluwer, Dordrecht, Netherlands.\nHoward, R. A. ( 1966), 'Information value theory', IEEE Transac tions on Systems Science and Cybernetics SSC-2, 22-26.\nKaelbling, L. P., Littman, M. L. & Moore, A. W. (1996), 'Rein forcement learning: A survey', Journal of Artificial intelli gence Research 4, 237-285.\nKanazawa, K., Koller, D. & Russell, S. (1995), Stochastic simula tion algorithms for dynamic probabilistic networks, in 'Pro ceedings of the Eleventh Conference on Uncertainty in Arti ficial Intelligence (UAI-95)', Morgan Kaufmann, Montreal.\nKeams, M. & Singh, S. (1998), Near-optimal performance for re inforcement learning in polynomial time, in 'Proceedings of the Fifteenth Int. Conf. on Machine Learning', Morgan Kaufmann.\nKoller, D. & Fratkina, R. (1998), Using learning for approxima tion in stochastic processes, in 'Proceedings of the Fifteenth International Conference on Machine Learning', Morgan Kaufmann, San Francisco, Calif.\nMoore, A. W. & Atkeson, C. G. (1993), 'Prioritized sweeping reinforcement learning with less data and less time', Ma chine Learning 13, I 03-130.\nRipley, B. D. (1987), Stochastic Simulation, Wiley, NY.\nRussell, S. J. & Wefald, E. H. (1991), Do the Right Thing: Studies in Limited Rationality, MIT Press, Cambridge, Mass.\nSutton, R. S. (1990), Integrated architectures for learning, plan ning, and reacting based on approximating dynamic pro gramming, in 'Proceedings of the Seventh Int. Conf. on Ma chine Learning', Morgan Kaufmann, pp. 216-224."
    } ],
    "references" : [ {
      "title" : "Generalized priori­ tized sweeping, in 'Advances in Neural information",
      "author" : [ "D. Andre", "N. Friedman", "R. Parr" ],
      "venue" : "Process­ ing Systems·,",
      "citeRegEx" : "Andre et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Andre et al\\.",
      "year" : 1997
    }, {
      "title" : "Neural Networks for Pattern Recognition",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop,? \\Q1995\\E",
      "shortCiteRegEx" : "Bishop",
      "year" : 1995
    }, {
      "title" : "Bayesian Q­ leaming, in 'Proceedings of the Fifteenth National Confer­ ence on Artificial intelligence (AAAI-98)",
      "author" : [ "R. Dearden", "N. Friedman", "S. Russell" ],
      "venue" : null,
      "citeRegEx" : "Dearden et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Dearden et al\\.",
      "year" : 1998
    }, {
      "title" : "Proability and Statistics, 2nd edn, Addison-Wesley",
      "author" : [ "M.H. Degroot" ],
      "venue" : null,
      "citeRegEx" : "Degroot,? \\Q1986\\E",
      "shortCiteRegEx" : "Degroot",
      "year" : 1986
    }, {
      "title" : "A tutorial on learning with Bayesian net­",
      "author" : [ "D. Heckerman" ],
      "venue" : "ed., 'Learning in Graphical Models',",
      "citeRegEx" : "Heckerman,? \\Q1998\\E",
      "shortCiteRegEx" : "Heckerman",
      "year" : 1998
    }, {
      "title" : "Information value theory",
      "author" : [ "R.A. Howard" ],
      "venue" : "IEEE Transac­ tions on Systems Science and Cybernetics",
      "citeRegEx" : "Howard,? \\Q1966\\E",
      "shortCiteRegEx" : "Howard",
      "year" : 1966
    }, {
      "title" : "Stochastic simula­ tion algorithms for dynamic probabilistic networks, in 'Pro­ ceedings of the Eleventh Conference on Uncertainty in Arti­ ficial Intelligence (UAI-95)",
      "author" : [ "K. Kanazawa", "D. Koller", "S. Russell" ],
      "venue" : null,
      "citeRegEx" : "Kanazawa et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Kanazawa et al\\.",
      "year" : 1995
    }, {
      "title" : "Near-optimal performance for re­ inforcement learning in polynomial time, in 'Proceedings of the Fifteenth",
      "author" : [ "M. Keams", "S. Singh" ],
      "venue" : "Int. Conf. on Machine Learning',",
      "citeRegEx" : "Keams and Singh,? \\Q1998\\E",
      "shortCiteRegEx" : "Keams and Singh",
      "year" : 1998
    }, {
      "title" : "Using learning for approxima­ tion in stochastic processes, in 'Proceedings of the Fifteenth International Conference on Machine Learning",
      "author" : [ "D. Koller", "R. Fratkina" ],
      "venue" : null,
      "citeRegEx" : "Koller and Fratkina,? \\Q1998\\E",
      "shortCiteRegEx" : "Koller and Fratkina",
      "year" : 1998
    }, {
      "title" : "Prioritized sweeping­ reinforcement learning with less data and less time",
      "author" : [ "A.W. Moore", "C.G. Atkeson" ],
      "venue" : "Ma­ chine Learning",
      "citeRegEx" : "Moore and Atkeson,? \\Q1993\\E",
      "shortCiteRegEx" : "Moore and Atkeson",
      "year" : 1993
    }, {
      "title" : "Stochastic Simulation, Wiley, NY",
      "author" : [ "B.D. Ripley" ],
      "venue" : null,
      "citeRegEx" : "Ripley,? \\Q1987\\E",
      "shortCiteRegEx" : "Ripley",
      "year" : 1987
    }, {
      "title" : "Do the Right Thing: Studies in Limited Rationality",
      "author" : [ "S.J. Russell", "E.H. Wefald" ],
      "venue" : null,
      "citeRegEx" : "Russell and Wefald,? \\Q1991\\E",
      "shortCiteRegEx" : "Russell and Wefald",
      "year" : 1991
    }, {
      "title" : "Integrated architectures for learning, plan­ ning, and reacting based on approximating dynamic pro­ gramming",
      "author" : [ "R.S. Sutton" ],
      "venue" : "'Proceedings of the Seventh Int. Conf. on Ma­ chine Learning',",
      "citeRegEx" : "Sutton,? \\Q1990\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "steps actually executed by the learner, since it can \"learn\" from simulated steps in the model (Sutton 1990).",
      "startOffset" : 95,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "tion: the agent should choose actions based on the value of the information it can expect to learn by performing them (Howard 1966).",
      "startOffset" : 118,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "This ap­ proach was pursued in the DYNA (Sutton 1990) frame­ work, where after the execution of an action, the agent updates its model of the environment, and then performs some bounded number of value propagation steps to up­ date its approximation of the value function.",
      "startOffset" : 40,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "We show that this is possible by adopting re­ sults from Bayesian learning of probabilistic models, such as Bayesian networks (Heckerman 1998).",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "We can use well-known Bayesian methods for learning standard distributions such as multinomials or Gaussian distributions (Degroot 1986).",
      "startOffset" : 122,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "Nonetheless, much of the above discussion and conclusions about parameter independence and Dirichlet priors apply to these models (Heckerman 1998).",
      "startOffset" : 130,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "In a recent paper, Dearden et al. (1998) examined model­ free Bayesian reinforcement learning.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "One of the simplest ones is K erne/ estimation (see for example (Bishop 1995)).",
      "startOffset" : 64,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "Procedures for sampling from these distributions can be found in (Ripley 1987).",
      "startOffset" : 65,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "Procedures for sampling from these distributions can be found in (Ripley 1987). Friedman and Singer (1999) introduce a structured prior that captures our uncertainty about the set of\"feasible\" val­ ues of X.",
      "startOffset" : 66,
      "endOffset" : 107
    } ],
    "year" : 2011,
    "abstractText" : "Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classi­ cal notion of Value of Informationthe expected im­ provement in future decision quality arising from the tnformation acquired by exploration. Estimating this quantity requires an assessment of the agent's uncer­ tainty about its current value estimates for states. In this paper we investigate ways to represent and rea­ son about this uncertainty in algorithms where the sys­ tem attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q­ values based on these. These distributions are used to compute a myopic approximation to the value of infor­ mation for each action and hence to select the action that best balances exploration and exploitation.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}