{
  "name" : "1406.3332.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "firstname.lastname@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n33 32\nv1 [\ncs .C\nV ]\n1 2\nJu n\n20 14"
    }, {
      "heading" : "1 Introduction",
      "text" : "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29]. The architecture of CNNs is relatively simple and consists of successive layers organized in a hierarchical fashion. Each layer involves convolutions with learned filters followed by a non-linearity, and downsampling operations called “feature pooling”. The resulting image representation seems to achieve some invariance to image perturbations and to encode complex visual patterns [32]. Training CNNs remains however difficult since high-capacity networks may involve billions of parameters to learn, which requires both high computational power, e.g., GPUs, and appropriate regularization techniques [18, 21, 29].\nThe exact nature of invariance achieved by CNNs is also not well understood. Only recently, the invariance of some related architectures has been characterized; this is the case for the wavelet scattering transform [8], or the hierarchical models of [7]. Our work revisits convolutional neural networks, but adopts a significantly different approach than the traditional one. We use indeed kernels [26], which are natural tools to model invariance [14]. Inspired by the hierarchical kernel descriptors of [2], we propose a reproducing kernel that produces invariant multi-layer image representations.\nOur main contribution is an approximation scheme called convolutional kernel network (CKN) to make the kernel approach computationally feasible. Such a scheme turns out to be a new type of CNN, which differs from classical ones in its objective function. The network is trained to linearly approximate the kernel, and thus the procedure involves no supervision. Another difference is in the non-linear functions that we use. Interestingly, they resemble rectified linear units [1, 29], even though they naturally emerge from the approximation scheme and were not handcrafted.\nAs a result, we bridge a gap between kernel methods and neural networks, and we believe that such a direction is fruitful for the future. Our network is learned without supervision since the label\n∗LEAR team, Inria Grenoble, Laboratoire Jean Kuntzmann, CNRS, Univ. Grenoble Alpes, France.\ninformation is only used subsequently in a support vector machine (SVM). Yet, we achieve near state-of-the-art results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with simple architectures, few parameters to learn, and no data augmentation. Open-source code for learning our convolutional kernel networks will be provided upon publication of the paper."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "There were several attempts in the past to build kernel-based methods that mimic deep neural networks; we only review here the most related to our approach.\nArc-cosine kernels. Kernels for building deep large-margin classifiers have been introduced in [10]. For any pair of vectors, the single-layer arc-cosine kernel relies on an integral representation, and multilayer extensions are built by multiple kernel compositions. Similarly, our kernels rely on a integral representation, and enjoy a multilayer construction. However, in contrast to arccosine kernels: (i) we build our sequence of kernels by convolutions, using local information over spatial neighborhoods (as opposed to compositions, using global information); (ii) we propose a new training procedure for learning a compact representation of the kernel in a data-dependent manner.\nMultilayer derived kernels. Kernels that enjoy invariance properties for visual recognition were proposed in [7]. Such kernels are built with a parameterized “neural response” function, which consists in computing the maximal response of a base kernel over a local neighborhood. Multiple layers are then built by iteratively renormalizing the response kernels and pooling using neural response functions. Learning is performed by plugging the obtained kernel in a SVM. In contrast to [7], we propagate information up, from lower to upper layers, by using sequences of convolutions. Furthermore, we propose a simple and effective data-dependent way to learn a compact representation of our kernels and show that we obtain near state-of-the-art performance on several benchmarks.\nHierarchical kernel descriptors. The kernels proposed in [2, 3] produce multilayer image representations for visual recognition tasks. We discuss in details these kernels in the next section: our paper generalizes them and establishes a strong link with convolutional neural networks."
    }, {
      "heading" : "2 Convolutional Multilayer Kernels",
      "text" : "The convolutional multilayer kernel is a generalization of the hierarchical kernel descriptors introduced in computer vision [2, 3]. The kernel produces a sequence of image representations that are built one on top of each other in a multilayer fashion. Each layer can be interpreted as a non-linear transformation of the previous one with additional invariance. We call these layers image feature maps1, and formally define them as follows:\nDefinition 1. An image feature map ϕ is a function ϕ : Ω → H, where Ω is a (usually discrete) subset of [0, 1]d representing “coordinates” in the image and H is a Hilbert space.\nFor all practical examples in this paper, Ω is a two-dimensional grid and corresponds to different locations in a two-dimensional image. In other words, Ω is a set of pixel coordinates. Given z in Ω, the point ϕ(z) corresponds to some characteristics of the image at location z, or in a neighborhood of z. For instance, a color image of size m × n with three channels, red, green, and blue, may be represented by an initial feature map ϕ0 : Ω0 → H0, where Ω0 is an m × n regular grid, H0 is the Euclidean space R3, and ϕ0 provides the color pixel values. With the multilayer scheme, non-trivial feature maps will be obtained subsequently, which will encode more complex image characteristics. With this terminology in hand, we now introduce the convolutional kernel, first, for a single layer.\nDefinition 2 (Convolutional Kernel with Single Layer). Let us consider two images represented by two image feature maps, respectively ϕ and ϕ′ : Ω → H, where Ω is a set of pixel locations, and H is a Hilbert space. The one-layer convolutional kernel between ϕ and ϕ′ is defined as\nK(ϕ, ϕ′) := ∑\nz∈Ω\n∑\nz′∈Ω\n‖ϕ(z)‖H ‖ϕ′(z′)‖H e − 1\n2β2 ‖z−z′‖2\n2e− 1 2σ2 ‖ϕ̃(z)−ϕ̃′(z′)‖2 H , (1)\n1In the kernel literature, “feature map” denotes the mapping between data points and their representation in a RKHS [26]. Here, feature maps refer to spatial maps, as usual in the neural network literature [22].\nwhere β and σ are smoothing parameters of Gaussian kernels, and ϕ̃(z) and ϕ̃(z′) are normalized versions of ϕ(z) and ϕ′(z′), respectively. More precisely, ϕ̃(z) :=(1/max(‖ϕ(z)‖H , ε))ϕ(z), and the definition is similar for ϕ′(z′).2\nIt is easy to see that K is a positive definite kernel since it only involves sums and products of positive definite kernels [26]. It compares normalized features ϕ̃(z) and ϕ̃′(z′) extracted at all locations z and z′ with a Gaussian kernel. Another Gaussian function compares the locations z and z′ providing invariance to local deformations. Indeed, when β goes to infinity, the kernel becomes invariant to the positions z and z′ of the features ϕ(z) and ϕ′(z′). When β is small, only features placed at the same location z = z′ are compared to each other, and the kernel has no shift-invariance. Before moving to a model with additional layers, let us present a few concrete examples when applying the convolutional kernel to simple input feature maps ϕ0 : Ω0 → H0. Gradient map. Assume that H0=R2 and that ϕ0(z) provides the two-dimensional gradient of the image at pixel z, which is often computed with first-order differences along each dimension. Then, the quantity ‖ϕ0(z)‖H0 is the gradient intensity, and ϕ̃0(z) is its orientation, which can be characterized by a particular angle—that is, there exists θ in [0; 2π] such that ϕ̃0(z) = [cos(θ), sin(θ)]. The resulting kernel K is exactly the kernel descriptor introduced in [2, 3] for natural image patches.\nPatch map. In that setting, ϕ0 associates to a location z an image patch of size m ×m centered at z. Then, the space H0 is simply Rm×m, and ϕ̃0(z) is a contrast-normalized version of the patch, which is a useful transformation for visual recognition according to classical findings in computer vision [19]. When the image is encoded with three color channels, patches are of size m×m× 3. We now define the multilayer convolutional kernel, generalizing some ideas of [2]. Definition 3 (Multilayer Convolutional Kernel). Let us consider a set Ωk−1 ⊆ Rd and a Hilbert space Hk−1. We build a new set Ωk and a new Hilbert space Hk as follows: (i) choose a patch shape Pk defined as a bounded symmetric subset of [0, 1]d, and a set of coordinates Ωk such that for all zk in Ωk, the patch {zk}+ Pk is a subset of Ωk−1;3\n(ii) define the convolutional kernel Kk on the “patch” feature maps Pk → Hk−1, by replacing in (1), Ω by Pk, H by Hk−1, and σ, β by appropriate smoothing parameters σk, βk. We denote by Hk the Hilbert space for which Kk is a reproducing kernel. An image represented by a feature map ϕk−1 : Ωk−1 → Hk−1 at layer k−1 is now encoded in the k-th layer as ϕk : Ωk → Hk, where for all zk in Ωk, ϕk(zk) is the representation in Hk of the patch feature map z 7→ ϕk−1(zk + z) for z in Pk. Concretely, the kernel Kk between two patches of ϕk and ϕ′k at respective locations zk and z ′ k is\n∑\nz∈Pk\n∑\nz′∈Pk\n‖ϕk(zk + z)‖Hk ‖ϕ ′ k(z ′ k + z ′)‖Hk e − 1 2β2 k ‖z−z′‖2 2e − 1 2σ2 k ‖ϕ̃k(zk+z)−ϕ̃′k(z′k+z′)‖2Hk . (2)\nIn Figure 1(a), we illustrate the interactions between the different sets of coordinatesΩk, patches Pk, and feature spaces Hk across layers. For two-dimensional grids, a typical patch shape is a square, for example P := {−1/n, 0, 1/n} × {−1/n, 0, 1/n} for a 3 × 3 patch in an image of size n × n. Information encoded in the k-th layer differs from the (k− 1)-th one in two aspects: first, each point ϕk(zk) in layer k contains information about several points from the (k−1)-th layer and can possibly represent more complex patterns; second, the new feature map is more shift-invariant than the previous one due to the term involving the parameter βk in Eq. (2).\nThe multilayer convolutional kernel slightly differs from the hierarchical kernel descriptors of [2] but exploits similar ideas. Bo et al. [2] define indeed several ad hoc kernels for representing local information in images, such as gradient, color, or shape. These kernels are close to the one defined in (1) but with a few variations. Some of them do not use normalized features ϕ̃(z), and these kernels use different weighting strategies for the summands of (1) that are specialized to the image modality, e.g., color, or gradient, whereas we use the same weight ‖ϕ(z)‖H ‖ϕ′(z′)‖H for all kernels. We propose instead the generic formulation (1), which appears to perform well in practice. We believe that such a generalization is useful per se, but our main contribution comes in the next section, where we use the kernel as a new tool for learning invariant convolutional neural networks.\n2When Ω is not discrete, the notation ∑ in (1) should be replaced by the Lebesgue integral ∫\nin the paper. 3For two sets A and B, the Minkowski sum A+B is defined as {a + b : a ∈ A, b ∈ B}."
    }, {
      "heading" : "3 Training Invariant Convolutional Kernel Networks",
      "text" : "We now show that a natural approximation scheme for the multilayer convolutional kernel gives rise to a convolutional neural network. In other words, the approximation can be achieved with a sequence of spatial convolutions with learned filters, non-linearities, and pooling operations.\nSeveral schemes have been proposed for approximating a non-linear kernel with a linear one, such as the Nyström method and its variants [5, 30], which consist of projecting the data onto a finitedimensional subspace of the Hilbert space for which the kernel is reproducing. Random sampling techniques in the Fourier domain for shift-invariant kernels are also popular [24]. Being able to linearly approximate convolutional kernels is critical because computing the full kernel matrix on a database of images is computationally infeasible, even for a moderate number of images (≈ 10 000) and moderate number of layers. For this reason, Bo et al. [2] use the Nyström method for their hierarchical kernel descriptors. In our paper, we show that a particular convolutional neural network fits well the structure of the convolutional kernel, leading to an approach enjoying classical benefits of CNNs such as efficient prediction at test time."
    }, {
      "heading" : "3.1 Fast Approximation of the Gaussian Kernel",
      "text" : "A recurrent component of our formulation is the Gaussian kernel. In this section, we show that an approximation scheme involves a linear operation with learned filters followed by a pointwise non-linearity. Our starting point is the next lemma, which can be obtained after a simple calculation.\nLemma 1 (Expansion of the Gaussian Kernel). For all x and x′ in Rm, and σ > 0,\ne− 1 2σ2 ‖x−x′‖22 =\n(\n2\nπσ2\n) m 2 ∫\nw∈Rm e−\n1 σ2 ‖x−w‖22e− 1 σ2 ‖x′−w‖22dw. (3)\nThe lemma gives us an infinite-dimensional linear representation [ √ Ce−(1/σ\n2)‖x−w‖22 ]w∈Rm for all x in Rm, where C is the constant in front of the integral. To obtain a finite-dimensional representation, we need to approximate the integral with a weighted finite sum, which is a classical problem in numerical analysis and statistics (see [28] and chapter 8 of [6] for a review). We choose instead a data-driven approach, and consider two different scenarios:\nSmall dimension, m ≤ 2. When the data lives in a compact set of Rm, the integral in (3) can be approximated by uniform sampling over a large enough set. We choose such a strategy for two types of kernels from Eq. (1): (i) the spatial kernels e−(1/2β 2)‖z−z′‖22 ; (ii) the term e−(1/2σ 2)‖ϕ̃(z)−ϕ̃(z)′‖2 H when ϕ is the “gradient map” presented in Section 2. In the latter case, H = R2 and ϕ̃(z) is the gradient orientation. We typically sample a few orientations as explained in Section 3.4.\nHigher dimensions. When the dimension is high, uniform sampling suffers from the curse of dimensionality. Therefore, we choose to leverage the intrinsic low-dimensionality of the data. We learn importance weights η = [ηl] p l=1 in R p + and sampling points W = [wl] m l=1 in R\nm×p on training data x1, . . . ,xn in Rm with the following data-driven non-convex formulation\nmin η∈Rp\n+ ,W∈Rm×p\n\n\n1\nn2\n∑\n(i,j)\n(\ne− 1 2σ2 ‖xi−xj‖ 2 2 −\np ∑\nl=1\nηle − 1 σ2 ‖xi−wl‖ 2 2e− 1 σ2 ‖xj−wl‖ 2 2\n)2 \n . (4)\nThe resulting representation of a new data point x is simply the vector [ √ ηle −(1/σ2)‖x−wl‖ 2 2 ]pl=1 in Rp. We use the method (4) for approximating the Gaussian kernels e−(1/2σ 2)‖ϕ̃(z)−ϕ̃′(z′)‖2\nH in (1), by proceeding as follows: first, we assume that we already have a finite-dimensional approximation of ϕ̃(z)—say, of dimension m—which we denote by ψ̃(z); second, we apply (4) to the kernel e−(1/2σ 2)‖ψ̃(z)−ψ̃′(z′)‖22 by using a database of features ψ̃(z) in Rm obtained from training data.\nWe can now start relating our approach to neural networks. After learning the parameters W and η, computing the finite-dimensional approximation in H of a unit-norm vector x only involves a linear operation followed by a non-linearity. Indeed, the quantities e−(1/σ 2)‖x−wl‖ 2 2 can be written as fl(w ⊤ l x), where fl is the real-valued function u 7→ e−(1+‖wl‖ 2 2)/σ\n2+2u/σ2 . Since we apply (4) to normalized data ψ̃(z), we expect the ℓ2-norm of the sampling points wl to to be close to one, and the functions fl to have the form: u 7→ e(2/σ\n2)(u−1) for u = w⊤l x in [−1, 1]. In Figure 2, we show that we obtain a shape resembling the “rectified linear unit” function used in neural networks [29]."
    }, {
      "heading" : "3.2 Approximating the Single-Layer Convolutional Kernel",
      "text" : "With the methodology presented in the previous section, we now introduce an approximation scheme for the convolutional kernel K(ϕ, ϕ′) from Definition 2. We proceed as follows:\n(i) we assume that for all ϕ(z) and ϕ′(z′), we already know some finite-dimensional approximations ψ(z) and ψ′(z′) in Rm;\n(ii) the kernels e−(1/2β 2)‖z−z′‖22 are approximated by uniform sampling: we define a set of equally\nspaced points N , such that e−(1/2β2)‖z−z′‖22 ≈ C′ ∑ c∈N e −(1/β2)‖z−c‖22e−(1/β 2)‖z′−c‖22 , and C′ is a constant independent of z and z′.\n(iii) we learn some weights η in Rp+ and sampling points W = [w1, . . . ,wp] in R m×p to approxi-\nmate the kernels e−(1/2σ 2)‖ϕ̃(z)−ϕ̃′(z′)‖2 H following the approach of the previous section.\nNote that all parameter choices, such as N , β, and σ, are discussed in Section 3.4. By plugging the above approximations in Eq. (1) and interchanging the sums, we get\nK(ϕ, ϕ′) ≈ C′ ∑\nc∈N\np ∑\nl=1\n[\n∑\nz∈Ω\nζl(z)e−(1/β 2)‖c−z‖22\n][\n∑\nz′∈Ω\nζl(z′)e−(1/β 2)‖c−z′‖22\n]\n, (5)\nwhere we have introduced the quantity\nζl(z) := ‖ψ(z)‖2 √ ηle −(1/σ2)‖ψ̃(z)−wl‖ 2 2 ,\nand also, by analogy, the quantity ζl′(z′) for ϕ′. Each function ζl : Ω → R can be interpreted as a spatial map, where ζl(z) represents a non-linear filter response involving the non-linear functions\npresented in Figure 2. According to (5), we need to subsample these maps ζl at some points N after convolving them with the two-dimensional filter z 7→ √ C′e−(1/β\n2)‖z‖22 , leading to a new set of p maps denoted by ξl : N → R. In the same way, a set of p maps ξl′ are obtained for ϕ′, and\nK(ϕ, ϕ′) ≈ ∑\nc∈N\np ∑\nl=1\nξl(c)ξl′(c′). (6)\nThe maps ζl and ξl coincide with the terminology of “feature maps” from neural networks. Building the maps ξl from ζl is achieved by Gaussian filtering and subsampling at predefined points c in N , which is also called a “linear pooling step” in neural networks, and “downsampling” with a Gaussian anti-aliasing filter in signal processing. In the next section, we show that approximating the multilayer convolutional kernel can be achieved similarly, and leads to a particular CNN."
    }, {
      "heading" : "3.3 Convolutional Kernel Networks",
      "text" : "We have seen that for approximating the single-layer convolutional kernel on a feature map ϕ0 : Ω0 → H0 where H0 is the Euclidean space Rm0 , we need to (i) compute the linear responses ϕ̃0(z) ⊤ wl; (ii) apply a pointwise non-linearity to obtain the maps ζl; (iii) perform linear pooling with Gaussian filtering to obtain new maps ξl. When in addition ϕ0 is the “patch map” defined in Section 2, the vectors wl can be interpreted as spatial filters, and the dot products ϕ̃0(z)⊤wl as convolutions. As a result, we have just described a one-layer convolutional neural network.\nWhen the kernel involves several layers ϕ0, . . . , ϕk, as in Definition 3, the methodology of the previous section allows us to build a linear approximation of the kernel K(ϕk, ϕ′k) under one condition: we need to know in advance finite-dimensional approximations ψk(zk) of ϕk(zk) for all zk in Ωk. We assume that such a condition is always satisfied for H0, which we choose finite-dimensional. Therefore, it remains to show how to build ψk(zk) when the condition holds for k − 1. From Definition 3, ϕk(zk) in Hk is the representation derived from the kernel Kk presented in Eq. (2) of the patch feature map z 7→ ϕk−1(zk + z). To achieve our goal, we apply the methodology of the previous section to Kk, by replacing in (5) Ω by Pk, β by βk, σ by σk, p by pk, and N by Nk. Then, we obtain an approximation ψk(zk) of dimension mk = |Nk|pk. Note that since the patches may overlap, computing the vectors ψk(zk) can be done efficiently for a given image represented by ϕk−1 : Ωk−1 → Hk−1. As in the previous section, let us define the maps ζlk : Ωk−1 → R as ζlk(z) := ‖ψk−1(z)‖2 √ ηle −(1/σ2k)‖ψ̃k−1(z)−wl‖ 2 2 for all z in Ωk−1 and l = 1, . . . , pk. Given the maps ζlk, all vectors ψk(zk) can be computed in O(|Ωk||Nk||Pk|pk) operations, whereas the maps ζlk require storing O(|Ωk−1|pk) real values. At this point, we have obtained a computationally feasible approximation scheme that is fairly general regarding the choice of domains Ωk and patches Pk. However, a last approximation allows us to fill in the gap between our approach and multilayer CNNs under a few conditions: (i) Ωk−1 and Ωk are grids; (ii) Pk is a square with same spacing as Ωk−1; (iii) Nk is a square containing 0 with same spacing as Ωk; Then, we can build the maps ξkl as in the previous section, by convolving the maps ζlk with the Gaussian filter z 7→ √ C′e−(1/β\n2)‖z‖22 , followed by subsampling with the same spacing as Ωk. Such a construction is illustrated in Figure 1(b). As a result, a |Nk|pk-dimensional approximation of ϕk(zk) is simply the pk ×Nk patch [ξlk(zk + z)]l,z∈Nk . Consequently, obtaining the desired approximationψk(zk) requires extracting a patch from pk maps of size O(|Ωk|), and the dot products w⊤l ψk(zk) are spatial convolutions on the “pooled” maps ξ l k . We have now obtained the convolutional kernel network displayed in Figure 1(b)."
    }, {
      "heading" : "3.4 Practical Implementation: Parameter Setting and Optimization",
      "text" : "The parameters that our convolutional kernel network requires are the following quantities: N (number of layers), βk, σk (smoothing parameters); pk (number of filters per layer), Nk (to obtain ψk(zk) from the maps ξlk), the downsampling factor between ξ l k and ζ l k, and the initial maps ζ l 0. Other parameters appearing in the original kernel formulation of Section 2, such as Pk, Ωk, do not need to be chosen since they are implicitly linked with the ones we have just listed.\nFirst, N , pk, Nk, and the downsampling factors are classical parameters of CNNs, which are left to the discretion of the user. The sets Nk represent patches on the maps ξlk, as shown in Figure 1(b),\nand are typically small. We tried the sizes m×m with m = 3, 4, 5 for the first layer, and m = 2, 3 for the upper ones. The number of filters pk in our experiments is in the set {50, 100, 200, 400, 800}. The downsampling factor is always chosen to be 2 between two consecutive layers, whereas the last layer is downsampled to produce final maps ξlN of a small size—say, 5× 5 or 4× 4. The initial map ζl0 also needs to be set up. When using the “patch map” of Section 2, the maps ζ l 0 are simply the input image and N1 defines the patch size. For the “gradient map”, we have p0 = 2, the maps ζl0 carry the image gradient, and we set N1 = {0}, representing a 1× 1 patch. Finally, the parameters βk are chosen according to signal processing principles. The purpose of the filter z →7→ e−(1/β2)‖z‖22 is indeed to remove high frequencies from the maps ζlk before subsampling. When downsampling by a factor κ, we simply choose β to be κ times the spacing of two pixels on ζlk. Similarly, the parameter σ1 when sampling p1 orientations for the “patch maps” is set to 2π/p1, where p1 = 12 in all our experiments. The parameter σk in (4) is chosen according to a heuristic rule. We set σk to the 0.1 quantile of the training data ‖ψk−1(z)− ψ′k−1(z′)‖2. Regarding the optimization problem (4), we learn the parameters W,η of the layers from bottom to top. Stochastic gradient descent (SGD) is a natural candidate for this task since an enormous amount of training data is available. For the purpose of this paper, we have preferred to use L-BFGS-B [9] on 300 000 pairs of randomly selected training data points, and initializing the weights W with the K-means algorithm. L-BFGS-B is a parameter-free state-of-the-art batch method, which is probably not as fast as SGD, but which is much easier to use. Our goal was to demonstrate the performance of a new type of convolutional network, and we leave as future work any speed improvement."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we present experiments on visual recognition tasks, and start by visualizing the filters we learn on natural image patches. All experiments were performed using a Matlab implementation, which will be made available upon publication of the paper, and an L-BFGS-B solver [9], interfaced in Matlab by Stephen Becker. Since our work is not focused on speed, we run the L-BFGS-B algorithm for 4 000 iterations, which seems to ensure convergence of the objective function to a stationary point. The image representations obtained with our convolutional kernel network, represented by the last map ξk, are used in a linear support vector machine. We use the software package LibLinear [16] modified to handle dense matrices. The image representations are centered, rescaled to have unit ℓ2-norm on average, and the parameter C of the SVM is always selected on a validation set or by 5-fold cross-validation in the range 2i, i = −15 . . . , 15."
    }, {
      "heading" : "4.1 Discovering the Structure of Natural Image Patches",
      "text" : "Unsupervised learning was first used for discovering the underlying structure of natural image patches by Olshausen and Field [23]. Without making any a priori assumption about the data except a parsimony principle, the method is able to produce small prototypes that resemble Gabor wavelets—that is, spatially localized oriented basis functions. The results were found impressive by the scientific community and their work received substantial attention. It is also known that such results can also be achieved with CNNs [25]. We show in this section that this is also the case for convolutional kernel networks, even though they are not explicitly trained to reconstruct data.\nFollowing [23], we randomly select a database of 300 000 whitened natural image patches of size 12× 12 and learn p = 256 filters W using the formulation (4). We initialize W with Gaussian random noise without performing the K-means step, in order to ensure that the output we obtain is not an artifact of the initialization. In Figure 3, we display the filters associated to the top-128 largest weights ηl. Among the 256 filters, 197 exhibit interpretable Gabor-like structures and the rest was less interpretable. To the best of our knowledge, this is the first time that the explicit kernel map of the Gaussian kernel for whitened natural image patches is shown to be related to Gabor wavelets."
    }, {
      "heading" : "4.2 Digit Classification on MNIST",
      "text" : "The MNIST dataset [22] consists of 60 000 images of handwritten digits for training and 10 000 for testing. We use two types of initial maps in our networks: the “patch map”, denoted by CNK-PM and the “gradient map”, denoted by CNK-GM. We follow the evaluation methodology of [25] for comparison when varying the training set size. We follow Section 3.4 for building the networks. We select the SVM parameter by 5-fold cross validation when the training size is smaller than 20 000, or otherwise, we keep 10 0000 examples from the training set for validation. We report in Table 1 the results obtained for four simple architectures. CKN-GM1 is the simplest one: its second layer uses 3× 3 patches and only p2 = 50 filters, resulting in a network with 5 400 parameters. Yet, it achieves an outstanding performance of 0.58% error on the full dataset. The best performing, CKN-GM2, is similar to CKN-GM1 but uses p2 = 400 filters. When working with raw patches, two layers (CKN-PM2) gives better results than one layer. More details about the network architectures are provided in the supplementary material. In general, our method achieves a state-of-the-art accuracy for this task since lower error rates have only been reported by using data augmentation [11]."
    }, {
      "heading" : "4.3 Visual Recognition on CIFAR-10 and STL-10",
      "text" : "We now move to the more challenging datasets CIFAR-10 [20] and STL-10 [13]. We again follow Section 3.4 for choosing all parameters. We select the best architectures on a validation set of 10 000 examples from the training set for CIFAR-10, and by 5-fold cross-validation on STL-10 since the latter only contains 100 training examples per class. We report the results for CKN-GM, defined in the previous section, without exploiting color information, and CKN-PM when working on raw RGB patches whose mean color is subtracted. The best selected models have always two layers. with 800 filters for the top layer. Since CKN-PM and CKN-GM exploit a different information, we also report a combination of such two models, CKN-CO, by concatenating normalized image representations together. The standard deviations for STL-10, obtained using the 10 training folds, was always below 0.7% for our results. Our approach appears to be competitive with the state of the art, especially on STL-10 where only one method does better than ours, despite the fact that our models only use 2 layers and require learning few parameters (see supplementary material)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have proposed a new methodology for combining kernels and convolutional neural networks. We show that mixing the ideas of these two concepts is fruitful, since we achieve near state-of-the-art performance on several datasets such as MNIST, CIFAR-10, and STL10, with simple architectures and no data augmentation. Some challenges regarding our work are left open for the future. The first one is the use of supervision to better approximate the kernel for the prediction task. The second consists in leveraging the kernel interpretation of our convolutional neural networks to better understand the theoretical properties of the feature spaces that these networks produce."
    }, {
      "heading" : "A List of Architectures Reported in the Experiments",
      "text" : "We present in details the architectures used in the paper in Table 3."
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Y. Bengio" ],
      "venue" : "Found. Trends Mach. Learn.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Object recognition with hierarchical kernel descriptors",
      "author" : [ "L. Bo", "K. Lai", "X. Ren", "D. Fox" ],
      "venue" : "Proc. CVPR",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Kernel descriptors for visual recognition",
      "author" : [ "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Unsupervised feature learning for RGB-D based object recognition",
      "author" : [ "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "Experimental Robotics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient match kernel between sets of features for visual recognition",
      "author" : [ "L. Bo", "C. Sminchisescu" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Large-Scale Kernel Machines (Neural Information Processing)",
      "author" : [ "L. Bottou", "O. Chapelle", "D. DeCoste", "J. Weston" ],
      "venue" : "The MIT Press",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On invariance in hierarchical models",
      "author" : [ "J.V. Bouvrie", "L. Rosasco", "T. Poggio" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Invariant scattering convolution networks",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "IEEE T. Pattern Anal., 35(8):1872– 1886",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A limited memory algorithm for bound constrained optimization",
      "author" : [ "R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu" ],
      "venue" : "SIAM J. Sci. Comput., 16(5):1190–1208",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Large-margin classification in infinite neural networks",
      "author" : [ "Y. Cho", "L.K. Saul" ],
      "venue" : "Neural Comput., 22(10)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-column deep neural networks for image classification",
      "author" : [ "D. Ciresan", "U. Meier", "J. Schmidhuber" ],
      "venue" : "Proc. CVPR",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Selecting receptive fields in deep networks",
      "author" : [ "A. Coates", "A.Y. Ng" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "A. Coates", "A.Y. Ng", "H. Lee" ],
      "venue" : "Proc. AISTATS",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Training invariant support vector machines",
      "author" : [ "D. Decoste", "B. Schölkopf" ],
      "venue" : "Mach. Learn., 46(1-3):161– 190",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "DeCAF: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "preprint arXiv:1310.1531",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "J. Mach. Learn. Res., 9:1871–1874",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Discriminative learning of sum-product networks",
      "author" : [ "R. Gens", "P. Domingos" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Maxout networks",
      "author" : [ "I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "What is the best multi-stage architecture for object recognition? In Proc",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun" ],
      "venue" : "ICCV",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : "Tech. Rep.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "P. IEEE, 86(11):2278–2324",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "author" : [ "B.A. Olshausen", "D.J. Field" ],
      "venue" : "Nature, 381(6583):607–609",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
      "author" : [ "M. Ranzato", "F.-J. Huang", "Y-L. Boureau", "Y. LeCun" ],
      "venue" : "Proc. CVPR",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Kernel methods for pattern analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    }, {
      "title" : "Learning invariant representations with local transformations",
      "author" : [ "K. Sohn", "H. Lee" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Spline models for observational data",
      "author" : [ "G. Wahba" ],
      "venue" : "SIAM",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L. Wan", "M.D. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Using the Nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "Adv. NIPS",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Stochastic pooling for regularization of deep convolutional neural networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "preprint arXiv:1301.3557",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "preprint arXiv:1311.2901v3",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 28,
      "context" : "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 31,
      "context" : "The resulting image representation seems to achieve some invariance to image perturbations and to encode complex visual patterns [32].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : ", GPUs, and appropriate regularization techniques [18, 21, 29].",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 20,
      "context" : ", GPUs, and appropriate regularization techniques [18, 21, 29].",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 28,
      "context" : ", GPUs, and appropriate regularization techniques [18, 21, 29].",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "Only recently, the invariance of some related architectures has been characterized; this is the case for the wavelet scattering transform [8], or the hierarchical models of [7].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "Only recently, the invariance of some related architectures has been characterized; this is the case for the wavelet scattering transform [8], or the hierarchical models of [7].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 25,
      "context" : "We use indeed kernels [26], which are natural tools to model invariance [14].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "We use indeed kernels [26], which are natural tools to model invariance [14].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "Inspired by the hierarchical kernel descriptors of [2], we propose a reproducing kernel that produces invariant multi-layer image representations.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Interestingly, they resemble rectified linear units [1, 29], even though they naturally emerge from the approximation scheme and were not handcrafted.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 28,
      "context" : "Interestingly, they resemble rectified linear units [1, 29], even though they naturally emerge from the approximation scheme and were not handcrafted.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "Yet, we achieve near state-of-the-art results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with simple architectures, few parameters to learn, and no data augmentation.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "Yet, we achieve near state-of-the-art results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with simple architectures, few parameters to learn, and no data augmentation.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Yet, we achieve near state-of-the-art results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with simple architectures, few parameters to learn, and no data augmentation.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "Kernels for building deep large-margin classifiers have been introduced in [10].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Kernels that enjoy invariance properties for visual recognition were proposed in [7].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "In contrast to [7], we propagate information up, from lower to upper layers, by using sequences of convolutions.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "The kernels proposed in [2, 3] produce multilayer image representations for visual recognition tasks.",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "The kernels proposed in [2, 3] produce multilayer image representations for visual recognition tasks.",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "The convolutional multilayer kernel is a generalization of the hierarchical kernel descriptors introduced in computer vision [2, 3].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "The convolutional multilayer kernel is a generalization of the hierarchical kernel descriptors introduced in computer vision [2, 3].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "An image feature map φ is a function φ : Ω → H, where Ω is a (usually discrete) subset of [0, 1] representing “coordinates” in the image and H is a Hilbert space.",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : "In the kernel literature, “feature map” denotes the mapping between data points and their representation in a RKHS [26].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : "Here, feature maps refer to spatial maps, as usual in the neural network literature [22].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : "It is easy to see that K is a positive definite kernel since it only involves sums and products of positive definite kernels [26].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The resulting kernel K is exactly the kernel descriptor introduced in [2, 3] for natural image patches.",
      "startOffset" : 70,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "The resulting kernel K is exactly the kernel descriptor introduced in [2, 3] for natural image patches.",
      "startOffset" : 70,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "Then, the space H0 is simply R, and φ̃0(z) is a contrast-normalized version of the patch, which is a useful transformation for visual recognition according to classical findings in computer vision [19].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "We now define the multilayer convolutional kernel, generalizing some ideas of [2].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "We build a new set Ωk and a new Hilbert space Hk as follows: (i) choose a patch shape Pk defined as a bounded symmetric subset of [0, 1], and a set of coordinates Ωk such that for all zk in Ωk, the patch {zk}+ Pk is a subset of Ωk−1; (ii) define the convolutional kernel Kk on the “patch” feature maps Pk → Hk−1, by replacing in (1), Ω by Pk, H by Hk−1, and σ, β by appropriate smoothing parameters σk, βk.",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "The multilayer convolutional kernel slightly differs from the hierarchical kernel descriptors of [2] but exploits similar ideas.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "[2] define indeed several ad hoc kernels for representing local information in images, such as gradient, color, or shape.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Several schemes have been proposed for approximating a non-linear kernel with a linear one, such as the Nyström method and its variants [5, 30], which consist of projecting the data onto a finitedimensional subspace of the Hilbert space for which the kernel is reproducing.",
      "startOffset" : 136,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "Several schemes have been proposed for approximating a non-linear kernel with a linear one, such as the Nyström method and its variants [5, 30], which consist of projecting the data onto a finitedimensional subspace of the Hilbert space for which the kernel is reproducing.",
      "startOffset" : 136,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "Random sampling techniques in the Fourier domain for shift-invariant kernels are also popular [24].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "[2] use the Nyström method for their hierarchical kernel descriptors.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 27,
      "context" : "To obtain a finite-dimensional representation, we need to approximate the integral with a weighted finite sum, which is a classical problem in numerical analysis and statistics (see [28] and chapter 8 of [6] for a review).",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "To obtain a finite-dimensional representation, we need to approximate the integral with a weighted finite sum, which is a classical problem in numerical analysis and statistics (see [28] and chapter 8 of [6] for a review).",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 28,
      "context" : "In Figure 2, we show that we obtain a shape resembling the “rectified linear unit” function used in neural networks [29].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : "Figure 2: In red, we plot the function u 7→ max(u, 0) often called “rectified linear unit” [29].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "For the purpose of this paper, we have preferred to use L-BFGS-B [9] on 300 000 pairs of randomly selected training data points, and initializing the weights W with the K-means algorithm.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "All experiments were performed using a Matlab implementation, which will be made available upon publication of the paper, and an L-BFGS-B solver [9], interfaced in Matlab by Stephen Becker.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "We use the software package LibLinear [16] modified to handle dense matrices.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 22,
      "context" : "Unsupervised learning was first used for discovering the underlying structure of natural image patches by Olshausen and Field [23].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 24,
      "context" : "It is also known that such results can also be achieved with CNNs [25].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : "Following [23], we randomly select a database of 300 000 whitened natural image patches of size 12× 12 and learn p = 256 filters W using the formulation (4).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 30,
      "context" : "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "The MNIST dataset [22] consists of 60 000 images of handwritten digits for training and 10 000 for testing.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 24,
      "context" : "We follow the evaluation methodology of [25] for comparison when varying the training set size.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "In general, our method achieves a state-of-the-art accuracy for this task since lower error rates have only been reported by using data augmentation [11].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "We now move to the more challenging datasets CIFAR-10 [20] and STL-10 [13].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "We now move to the more challenging datasets CIFAR-10 [20] and STL-10 [13].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 26,
      "context" : "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 17,
      "context" : "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 30,
      "context" : "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.",
      "startOffset" : 36,
      "endOffset" : 40
    } ],
    "year" : 2017,
    "abstractText" : "An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.",
    "creator" : "LaTeX with hyperref package"
  }
}