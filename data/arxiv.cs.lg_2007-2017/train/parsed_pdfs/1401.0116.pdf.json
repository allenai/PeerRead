{
  "name" : "1401.0116.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Controlled Sparsity Kernel Learning∗",
    "authors" : [ "Dinesh Govindaraj", "Raman Sankaran" ],
    "emails" : [ "dinesh.govindaraj@alcatel-lucent.com", "ramans@csa.iisc.ernet.in", "sreedal.menon@alcatel-lucent.com", "chiru@csa.iisc.ernet.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a popular front of research in recent times due to its success in application problems like Object Categorization. This success is due to the fact that MKL has the ability to choose from a variety of feature kernels to identify the optimal kernel combination. But the initial formulation of MKL was only able to select the best of the features and misses out many other informative kernels presented. To overcome this, the Lp norm based formulation was proposed by Kloft et. al. This formulation is capable of choosing a non-sparse set of kernels through a control parameter p. Unfortunately, the parameter p doesnot have a direct meaning to the number of kernels selected. We have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand. In this work, we propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can strictly control the number of kernels which we wish to select. The CSKL formulation introduces a parameter t which directly corresponds to the number of kernels selected. It is important to note that a search in t space is finite and fast as compared to p. We have also provided an efficient Reduced Gradient Descent based algorithm to solve the CSKL formulation, which is proven to converge. Through our experiments on the Caltech101 Object Categorization dataset, we have also shown that one can acheive better accuracies than the previous formulations through the right choice of t."
    }, {
      "heading" : "1 Introduction",
      "text" : "Support Vector Machines(SVMs) [15] have emerged as powerful tools for classification problems. The key to\n∗Work Done in the year 2011\naccurate classification using SVMs is the choice of Kernel functions(for definition of Kernel function please see [16]). This issue was first studied in Lanckriet et. al. [2] where the problem of Multiple kernel learning(MKL) was first introduced. They have been successfully applied to a variety of domains e.g. text, object recognition [10, 13], protein structures[20]. Even though the idea was to explore the space of all possible linear combinations of the specified kernels, the functional framework associated with it could only select the best kernel from the set of specified kernels. Recently, many other approaches have been proposed to overcome this limitation[6, 7]. While some of them select all the kernels and some have sparse solutions that choose a subset of the specified kernels in a weighted combination, none of them have explicit control over sparsity.\nDue to lack of explicit control, in many application scenarios Non-Sparse solutions end up selecting some bad kernels also which leads to reduction in the discriminative power of the combination kernel. We show experimental evidence of this phenomenon. One might argue that if the kernels given were all good kernels, this problem will not persist. But that does not take away the fact that a lower-accuracy good kernel can still bring down the accuracy of a better kernel. In most of the recent publications, we do not get a glimpse of the original problem as the space of kernels explored is very small and most of the kernels have almost equal power of representation.\nWhile sparse solutions[2, 5] overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels, once again, there is no way to control the sparsity of the solution. This inherits most of the problems of selecting one and selecting all kernels due to the lack of control. The most relevant problem is that it misses out on some\nar X\niv :1\n40 1.\n01 16\nv1 [\ncs .L\nG ]\n3 1\nD ec\nimportant features by selecting lesser number of kernels than optimal. In the case of applications like Object Recognition, the necessity of Non-Sparse solutions have been brought to light [12, 6]. This is due to the fact that different kernels represent different features necessary for the task and dropping some of them or most of them will lead to a bad combination kernel. These flaws are shown in our experiments as well.\nThis work builds a variable sparsity solution that has explicit control over the number of kernels selected overcoming all these problems. We show the effect and need of strict control of sparsity through our experiments on the application of Object Recognition. Along the way, we have also extended the MKL framework to nu-SVMs which allow us better control of the number of support vectors and training error as well.\nIn the following section 2, we present a review of the existing work on MKL. Section 3 introduces the CSKL formulation for the C-SVM and ν-SVM. Section 4 presents the algorithms to solve the proposed formulations. Section 5 demonstrates the usefulness of the CSKL formulation on a toy dataset and the Caltech101 real world Object Categorization dataset."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multiple Kernel Learning(MKL) was initially proposed by Lanckriet et. al. [2]. They introduced an SemiDefinite programming(SDP) approach to solve for the combination kernel. As SDP becomes intractable with increase in size and number of kernels, Bach et.al [3] reformulated MKL by considering each feature as a block and applying the l1 norm across the blocks and l2 norm within each block. For this formulation several algorithms[4, 5, 6] were proposed to speed up the optimization process. [4] provides an Semi-Infinite Linear Programming(SLIP) based algorithm which decreases the training time to large extent. SimpleMKL [5] proposed by Rakotomamonjy et.al. derived a formulation which is equivalent to the block l1 norm based formulation and provided a Reduced Gradient Descent based algorithm that is faster than the SLIP algorithm proposed previously. The dual of the SimpleMKL formulation is given by,\nmax αi\n∑ i αi − 1 2 ∑ i,j αiαjyiyj ( ∑ m γmKm (xi, xj))\ns. t. ∑ i αiyi = 0\n0 ≤ αi ≤ C∀i∑ m γi = 1, γi ≥ 0,∀i(2.1)\nWhile all these approaches discussed a sparse solution to MKL, on understanding the need for non-sparse solutions, researchers have been exploring the space of non-sparse formulations in recent times. To acheive\nnon-sparsity, [6] group the kernels and apply l∞ norm across the groups and l1 norm within the groups. They have also proposed a Mirror Descent Algorithm for solving MKL formulations which is much faster than SimpleMKL. Especially when number of kernels are high. Kloft et.al.[7] apply general lp norm to kernels and they show that Non-Sparse MKL generalizes much better than sparse MKL. The dual of the lp norm based MKL formulation as proposed by Kloft et.al. looks like\nmax αi\n∑ i αi − 1 2 (∑ m (∑ i,j αiαjyiyjKm (xi, xj) ) p p−1 ) p−1 p\ns. t. ∑ i αiyi = 0\n0 ≤ αi ≤ C∀i(2.2)\nThey have shown that when p = 1, the formulation is equivalent to SimpleMKL and as p moves to ∞, it explores non-sparse solutions. But the value of p lacks a direct meaning or implication to the number of kernels selected.\nEven though the details of sparse and non-sparse solutions have been explored, none of these formulations have explicit control of sparsity for their solutions. As we have demonstrated in the experiments section, strict control of sparsity is highly valuable. Hence we propose a formulation, where we can parametrically control the total number of kernels selected and an efficient reduced gradient descent based algorithm to solve it. We have also experimentally shown that our formulation will be able to better state-of-the-art performance on the Caltech101[19] dataset for object categorization through strict control of sparsity."
    }, {
      "heading" : "3 Controlled Sparsity Kernel Learning",
      "text" : "In this section we introduce the new Controlled Sparsity Kernel Learning (CSKL) formulation and prove that this formulation can explicitly control the sparsity of kernel selection through a parameter t. We derive the CSKL formulation by modifying the dual of MKL [2]. Lets start with the MKL dual [2]\nmin γ≥0,K ω(K)(= max α∈Sm −1 2 α>Y KY α+ m∑ i=0 αi)\ntr(K) = δ\nK = n∑ i=1 γiKi(3.3)\nwhere Sm = {α ∈ Rm|0 ≤ α ≤ C, yTC = 0}. Denote by dj tj δ = α\n>Y KjY α and tj = Trace(Kj). As ω(K) is convex, one can interchange the min and the\nmax. Now, the dual looks like\nmax α∈Sn,d min γ≥0 − ∑ j γjdj tj δ + ∑ i αi\nγ>t = δ\ndj = α >Y KjY α(3.4)\nDefine γ′j = γj tj δ then the constraint γ >t = δ can be rewritten as ∑ j γ ′ j = 1. The l∞ norm can be represented as (3.5) ‖v‖inf = max∑ i γi≤1,γi≥0 ∑ j γjvj\nfor any vj ≥ 0 . Given this, Equation (3.4) can be restated as\nmax α∈Sn,d −‖d‖∞ + ∑ i αi\ndj = α >Y KjY α(3.6)\nThis formulation (3.6) results in a sparse selection of kernels as shown in [7]. Similarly, equation (2.2) can also be rewritten as,\nmax α∈Sn,d −‖d‖p∗ + ∑ i αi\ndj = α >Y KjY α(3.7)\np∗ = p\np− 1 (3.8)\nThe above formulation(3.7) is referred to as Lp MKL throughout this paper. Even though above formulation (3.7) uses generic norm over d, there is no guarantee of explicit control over sparsity. In next section, we derive our CSKL formulation by modifying the norm on d.\n3.1 CSKL formulation Let v ∈ Rn+ denote the space of n dimensional vectors with all components positive, i.e. vi > 0. Let v(i) be the ith largest component of v, i.e. v(1) ≥ v(2) . . . , v(n) Consider the following convex function on gt : Rn+ → R, gt(v) =∑t i=1 v(i) where t is a positive integer less than n.\nWe present our first claim by this theorem\nTheorem 3.1. If v ∈ Rn+ such that v(n) > 0 and gt(v) defined as before then\ngt(v) = max γ\nγT v\ns.t. n∑ i=1 γi = t, 0 ≤ γi ≤ 1,∀i(3.9)\nand at optimality γi = 1,whenever vi > v(t) and γi = 0,whenever vi < v(t)\nProof. We begin by constructing the Lagrangian of the problem\nL(γ, a, µ, β) =γ>v − a ( n∑ i=0 γi − t ) (3.10)\n+ n∑ i=1 βiγi − n∑ i=0 µi (γi − 1)(3.11)\nwhere the lagrange multipliers are µ, β and a. Apart from the feasibility conditions on γ and the nonnegativity constraints on the lagrange multipliers µ and β the KKT conditions reads as\n∂L\n∂γi = 0 =⇒ a+ µi = βi + vi(3.12)\nα ( n∑ i=0 γi − t ) = 0(3.13)\nβiγi = 0(3.14) µi (γi − 1) = 0(3.15)\nThe proof hinges on that fact that a = v(t) satisfies the KKT conditions. We note that both βi and µi cannot be simultaneously positive. If vi < a, then (3.12) could be obtained by setting βi > 0 and µi = 0. As βi > 0 then γi = 0. Again if vi > a, then (3.12) could be obtained by setting βi = 0 and µi > 0. As µi > 0 then γi = 1. Interestingly note that if vi = a as both µi = βi = 0 and 1 > γi > 0. Let us now suppose that a = v(t) The constraint\n∑n i=1 γi = t can now be written as∑\ni:vi<v(t) γi︸ ︷︷ ︸ T1\n+ ∑\ni:vi=v(t) γi︸ ︷︷ ︸ T2\n+ ∑\ni:vi>v(t) γi︸ ︷︷ ︸ T3 = t\nDue to observations made before it is straightforward to see that T1 = 0 and T3 ≤ t − 1. One can always choose feasible γi ∀ vi = v(t) such that T2 = t − T3 This establishes the fact that a = v(t) indeed satisfies the KKT conditions and for which γi = 1(γi = 0) if vi > v(t)(vi < v(t)).\nAs KKT conditions are necessary and sufficient for this problem [14] we see that at optimality γ>v =∑t i=1 v(i) obtained by substituting the γ obtained before. This completes the proof.\nBy introducing gt(d) to the dual (As in Eqn. 3.6) we get the following CSKL formulation,\nmax α∈Sn,d −gt(d) + ∑ i αi\ndj = α >Y KjY α\n(3.16)\nNote that CSKL formulation (3.16) explicitly controls the sparsity of kernel selection by varying t as is evident from Theorem 3.1.\n3.1.1 ν-CSKL A variant of SVM is the ν-SVM [1] where parameter C is replaced by a parameter ν = [0, 1]. Here, the parameter ν is lower bound on the fraction of number support vector and an upper bound on the fraction of margin errors. In this section we extend our CSKL formulation to ν-SVM. The dual of ν-SVM is given by,\nmax α − 1 2 α>Y KY α\ns.t. 0 ≤ αi ≤ 1\nm , m∑ i=1 αiyi = 0, m∑ i=1 αi ≥ ν\n(3.17)\nIntroducing MKL to the dual of ν-SVM and rewriting it similar to equation (3.6).\nmax α\n− ‖d‖∞\ns.t. 0 ≤ αi ≤ 1\nm , m∑ i=1 αiyi = 0, m∑ i=1 αi ≥ ν\ndj = α >Y KjY α(3.18)\nWe now introduce our CSKL formulation in the setting of ν-SVM.\nmax α\n− gt(d)\ns.t. 0 ≤ αi ≤ 1\nm , m∑ i=1 αiyi = 0, ν ≥ m∑ i=1 αi\ndj = α >Y KjY α(3.19)\nThe above formulation is denoted as ν-CSKL throughout this paper."
    }, {
      "heading" : "4 Algorithms for solving CSKL formulations",
      "text" : "We present an alternating optimization scheme for solving the ν − CSKL formulation. For a fixed γ, we solve the following maximization for α,\nmax α\n− γT d\ns.t. 0 ≤ αi ≤ 1\nm , m∑ i=1 αiyi = 0, ν ≥ m∑ i=1 αi\ndj = α >Y KjY α(4.20)\nNote that in above problem γ should satisfy the conditions ∑n i=1 γi = t, 0 ≤ γi ≤ 1,∀i. We can\nuse standard Sequential Minimal Optimization(SMO) solver for the above problem. Once optimal α∗ is calculated, we compute d as, dj = α ∗>Y KjY α ∗. Next step is to solve for γ. We can find the optimal γ by solving gt(d) using Reduced Gradient Descent or a Linear Programming based Gradient Descent.\nAlgorithm 1 ν-CSKL Algorithm\nRequire: xTKjx > 0,∀x 6= 0,∀j INPUT: N = number of kernels γ = tN objOld = 0 while δ ≤ do\nSolve α using SMO solver with kernel K =∑N j γjKj Compute dj = α TY KjY α Solve gt(d) using Reduced Gradient Descent or Linear Programming based solver obj = − 12α TY (∑N j=1 γjKj ) Y α δ = obj − objOld objOld = obj\nend while\nIn Algorithm 2, we present our Reduced Gradient Algorithm to solve γ. The SVM solver is used to obtain J (see Algorithm 2). The Descent Direction D is defined as per Algorithm 3. In the case of ν-CSKL, the value of J (α, γ) = −γT d while in the case of C-CSKL it is −γT d + αT e while the rest of the framework remains the same.\nDue to our assumptions on K, in both the cases, J is convex and differentiable with Lipschitz gradient wrt. γ [17]. For such functions the Reduced Gradient Method converges with bounds as defined in [18].\nWe also present a linear programming based approach to solve for gt(d). We use some standard LP Solver to solve the following linear program for finding descent direction D for gt(d).\nmin D\nφTD\nDT 1 = 0, − γ ≥ D ≥ 1− γ(4.21)\nwhere φm = ∂J ∂γm\nand step size (S) can be found by using line search. γ is updates as γnew = γ + SD. Though this algorithm is found to converge for K > 0 we have no bounds on its convergence as yet."
    }, {
      "heading" : "5 Experiments.",
      "text" : "To illustrate the benefits of CSKL formulation, we give results on Synthetic data and the Caltech101 [19] realworld Object Categorization dataset. We compare CSKL\nAlgorithm 2 Reduced Gradient Algorithm for Solving gt(d)\nJ(α, γ) = − 12α TY (∑N j=1 γjKj ) Y α Set µ = argminm (abs (γm − 0.5)) Set Jnew = J − 1, γnew = γ Compute φm =\nδJ δγm for m = 1, . . . , N\nCompute the descend direction D(d, µ, φ) Set Dnew = D while Jnew < J do γ = γnew D = Dnew ν1 = argmin\n(m|Dm<0) − γmDm\nν2 = argmin (m|Dm>0)\n1−γm Dm\nSmax = min ( − γν1Dν1 , 1−γν2 Dν2 ) ν = argmin\nν1,ν2\n( − γν1Dν1 , 1−γν2 Dν2 ) γnew = γ + SmaxD, Dnew (µ) = Dµ −Dν Dnew (ν) = 0 Compute Jnew(α, γnew)\nend while Linesearch along D for S ∈ [0, Smax] γ ← γ + SD\nAlgorithm 3 Calculating the Descent Direction\nINPUT: Kernel Weights γ, Selected Pivot µ, Calculated Gradients φ OUTPUT: Optimal Descent Direction D(γ, µ, φ) for m = 1 to N do Dm = 0 if (γm == 0 & φm − φµ > 0) then Dm = 0\nelse if (γm == 1 & φm − φµ < 0) then Dm = 0 else if (γm > 0 & m! = µ) then Dm = −φm − φµ else if (m == µ) then Dm = ∑ ν!=µ,γν>0\n(φν − φµ) end if\nend for\nalgorithm with SimpleMKL [Equation 2.1] 1 which is a sparse selection algorithm, and Lp MKL [Equation 2.2] with p = 2 which is a non-sparse selection algorithm 2.\n5.1 Datasets In this section, we describe the datasets we used for our experiments.\n5.1.1 Synthetic Dataset To show the effect of noisy kernels, we generated n = 18 kernels out of which 16 are informative kernels and 2 are noisy kernels. To build these kernels, we sampled m = 500 datapoints with dimension d = 3 from two independent Gaussian distributions with covariance as the identity matrix and different means(µ1 = 0 and µ2 = 3, Datapoints sampled from different Gaussians are assumed to belong to different classes). We generated four kernels (two gaussian(σ = and σ =) and two polynomial kernels(σ = and σ =)) for each dimension seprately and all together(4∗3+4 = 16). On top of this, we also added two carefully chosen noisy kernels to this kernel set.\n5.1.2 Caltech101 The Caltech101 dataset has 102 categories of images such as airplanes, cars, leopards, etc. It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier. Using the method followed by [10] 3, we extract the following 4 descriptors : PhowColor, PhowGray, GeometricBlur and SelfSimilarity. Each descriptor gives rise to a distance matrix. We create multiple Gaussian kernels for each descriptor by varying the Gaussian width parameter used to generate the kernel. We currently used 5 width values in the log space of -4 to 0. Hence we arrive at a total of 20 kernels. The number of binary classification problems are 5151 and 102, for 1-vs-1 and 1-vs-rest classification approaches respectively.\n5.2 Need for Control Over Sparsity The key result we wish to establish is that by suitable variation of parameter t in CSKL, one can combine good kernels and eliminate noisy kernels and achieve better generalization than other MKL formulations.\nIn the Synthetic dataset setting t = 1 will facilitate sparse selection, and t = n facilitates a complete non-sparse selection. As shown in the figure 1, the CSKL formulation clearly outperforms both sparse and non-sparse MKL by setting t = 4. It is clear\n1Implementation downloaded from http://asi.insarouen.fr/enseignants/∼arakotom/code/mklindex.html 2Implementation available in the Shogun toolbox : http://www.shogun-toolbox.org/ 3http://www.robots.ox.ac.uk/ vgg/software/MKL/v1.0/\nthat setting t = 4 in CSKL gives better generalization performance than both t = 1 and t = 18. This clearly shows neither sparse nor complete non-sparse is good for this dataset. CSKL is the only formulation which can capture all good kernels but still eliminate the noisy kernels by tuning parameter t.\nIn order to demonstrate that neither sparse nor non-sparse solutions are always the best in real-world datasets, we take all the binary 1-vs-1 and 1-vsRest classifiers in Caltech101 dataset and compare SimpleMKL and L2 MKL solutions. Figures 2, 3 show the ratio of improvement in accuracy of L2 MKL over SimpleMKL. It is evident from the figures that neither of the algorithms are always the best. Thus, depending on the binary classification problem, we need to have different controls on the sparsity to achieve the stateof-the-art performance. Clearly this motivates that, to achieve the desired sparsity, we can use the CSKL formulation instead of either SimpleMKL or L2 MKL. In next section we show how CSKL can achieve better performance than other algorithms.\n5.3 Performance of CSKL We apply our CSKL algorithm, and compare its performance against the other state-of-the-art algorithms SimpleMKL and L2 MKL. We take the highest accuracy achieved by CSKL across various values of parameter t for the comparison.\nFigures 4, 5 show the ratio of improvement in accuracy of CSKL over L2 MKL and SimpleMKL.\nWe also present here overall performance of CSKL on Caltech101 dataset. Figure 8 shows the performance of CSKL as t is varied. For comparison, we have shown a straight line which shows the average accuracy achieved by SimpleMKL and L2 MKL. Figure 8 clearly shows that all the 20 kernels are not necessary, since the CSKL accuracy more or less saturates after t > 4. The result also shows that a sparse selection algorithm like SimpleMKL wont be most efficient algorithm in terms of the accuracy achieved. And the performance of CSKL is almost equal to that of L2 MKL, but the latter selects all the provided kernels, while we can achieve competitive accuracy with the former itself at t = 4. Note that no other formulation can give this flexibility to users to select exactly four best performing kernels. It is natural to use use t = 4 here because number of descriptors used is four. Hence the experiments demonstrated in this section provide a proper justification for the usage of the CSKL formulation.\n5.4 Discussion To analyze more on why CSKL achives better accuracy, we plot the histogram of number of descriptors selected when CSKL outperforms L2\nMKL and SimpleMKL in the binary classification problems. We see that, in the figures 9, 11, SimpleMKL only selects one or two descriptors whereas CSKL select all the descriptors. For the cases where SimpleMKL doesnt perform the best, non-sparse combination might be a better choice, and this is emperically confirmed in the figures 9, 11. Similarly from the figures 10, 12, we see that L2 MKL selects all the descriptors whereas CSKL does not select all descriptors most of the cases. These are expected, since, for the cases where L2 MKL perform low, it may mean that a non-sparse classification is preferable. And the same is reflected in the figures 10, 12.\nOut of the 5151 binary classification problems in the 1-vs-1 setting, ν-CSKL performed better in 5112 and 1498 cases against L2 MKL and SimpleMKL respectively. Similarly in the 1-vs-Rest setting, out of the 102 classification problems, the numbers turned out to be 97 and 91 against L2 MKL and SimpleMKL respectively.\nFinally, Figure 13 shows the number of descriptors selected as t is increased. We can infer that as t increased beyond 4, all the descriptors are selected. This is also not surprising since all the 4 descriptors used in our experiment are independent and experimentally they have been shown to aid the accuracy of the Object Categorization problem."
    }, {
      "heading" : "6 Conclusion.",
      "text" : "As we have seen, both Sparse and Non-Sparse MKL have their handicaps depending on the classification problem at hand. Niehter of them are always the best. Also, in such problems the time taken to calculate the features is one the biggest bottlenecks. For all these reasons, a formulation with strict control of sparsity would be the best solution to have. One can then tune the sparsity parameter t and select the best set of kernels for any particular classification problem. We have described one such formulation in this paper along with the associated solution algorithms. We have also shown the superior performance of this formulation with respect to both the Sparse and Non-Sparse formulations of MKL for the application problem of Object Categorization."
    } ],
    "references" : [ {
      "title" : "Learning the Kernel Matrix with Semidefinite Programming",
      "author" : [ "G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Multiple Kernel Learning",
      "author" : [ "F. Bach", "G.R.G. Lanckriet", "M.I. Jordan" ],
      "venue" : "Conic Duality, and the SMO Algorithm, International Conference on Machine Learning",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "SimpleMKL",
      "author" : [ "A. Rakotomamonjy", "F. Bach", "S. Canu", "Y Grandvalet" ],
      "venue" : "Journal of Machine Learning Research, vol 9, pages 2491-2521",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation",
      "author" : [ "Saketha Nath Jagarlapudi", "Dinesh Govindaraj", "Raman S", "Chiranjib Bhattacharyya", "Aharon Ben-Tal", "K.R. Ramakrishnan" ],
      "venue" : "Proceedings of the Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Efficient and Accurate Lp-Norm Multiple Kernel Learning",
      "author" : [ "Marius Kloft", "Ulf Brefeld", "Soeren Sonnenburg", "Pavel Laskov", "Klaus-Robert Mller", "Alexander Zien" ],
      "venue" : "Proceedings of the Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Composite Kernel Learning",
      "author" : [ "M. Szafranski", "Y. Grandvalet", "A. Rakotomamonjy" ],
      "venue" : "Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A Visual Vocabulary for Flower Classification",
      "author" : [ "Maria-Elena Nilsback", "Andrew Zisserman" ],
      "venue" : "Proceedings  of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Learning the Discriminative Power Invariance Trade-off",
      "author" : [ "M. Varma", "D. Ray" ],
      "venue" : "Proceedings of the International Conference on Computer Vision",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Automated Flower Classification over a Large Number of Classes",
      "author" : [ "Maria-Elena Nilsback", "Andrew Zisserman" ],
      "venue" : "Proceedings of the Sixth Indian Conference on Computer Vision, Graphics & Image Processing,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "On Feature Combination for Multiclass Object Classification",
      "author" : [ "Peter Gehler", "Sebastian Nowozin" ],
      "venue" : "Proceedings of the Twelfth IEEE International Conference on Computer Vision,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Support kernel machines for object recognition",
      "author" : [ "A. Kumar", "C. Sminchisescu" ],
      "venue" : "IEEE International Conference on Computer Vision",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning with Kernels Support Vector Machines, Regularization, Optimization, and Beyond",
      "author" : [ "Bernhard Schlkopf", "Alexander J. Smola" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Numerical Optimization: Theoretical and Practical Aspects (Universitext)",
      "author" : [ "Bonnans", "J. Frédéric", "Gilbert", "Jean Charles", "Lemaréchal", "Claude", "Sagastizábal", "Claudia A" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories",
      "author" : [ "R. Fergus L. Fei-Fei", "P. Perona" ],
      "venue" : "In IEEE",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "Probabilistic multi-class multi-kernel learning: On protein fold recognition and remote homology",
      "author" : [ "Damoulas", "T. Girolami M" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "∗Work Done in the year 2011 accurate classification using SVMs is the choice of Kernel functions(for definition of Kernel function please see [16]).",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "[2] where the problem of Multiple kernel learning(MKL) was first introduced.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "text, object recognition [10, 13], protein structures[20].",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "text, object recognition [10, 13], protein structures[20].",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "text, object recognition [10, 13], protein structures[20].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "Recently, many other approaches have been proposed to overcome this limitation[6, 7].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "Recently, many other approaches have been proposed to overcome this limitation[6, 7].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "While sparse solutions[2, 5] overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels, once again, there is no way to control the sparsity of the solution.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "While sparse solutions[2, 5] overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels, once again, there is no way to control the sparsity of the solution.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "In the case of applications like Object Recognition, the necessity of Non-Sparse solutions have been brought to light [12, 6].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "In the case of applications like Object Recognition, the necessity of Non-Sparse solutions have been brought to light [12, 6].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "[2].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "al [3] reformulated MKL by considering each feature as a block and applying the l1 norm across the blocks and l2 norm within each block.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "For this formulation several algorithms[4, 5, 6] were proposed to speed up the optimization process.",
      "startOffset" : 39,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "For this formulation several algorithms[4, 5, 6] were proposed to speed up the optimization process.",
      "startOffset" : 39,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "SimpleMKL [5] proposed by Rakotomamonjy et.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "To acheive non-sparsity, [6] group the kernels and apply l∞ norm across the groups and l1 norm within the groups.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "[7] apply general lp norm to kernels and they show that Non-Sparse MKL generalizes much better than sparse MKL.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "We have also experimentally shown that our formulation will be able to better state-of-the-art performance on the Caltech101[19] dataset for object categorization through strict control of sparsity.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "We derive the CSKL formulation by modifying the dual of MKL [2].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Lets start with the MKL dual [2]",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "6) results in a sparse selection of kernels as shown in [7].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "γ [17].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "To illustrate the benefits of CSKL formulation, we give results on Synthetic data and the Caltech101 [19] realworld Object Categorization dataset.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Using the method followed by [10] , we extract the following 4 descriptors : PhowColor, PhowGray, GeometricBlur and SelfSimilarity.",
      "startOffset" : 29,
      "endOffset" : 33
    } ],
    "year" : 2014,
    "abstractText" : "Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a popular front of research in recent times due to its success in application problems like Object Categorization. This success is due to the fact that MKL has the ability to choose from a variety of feature kernels to identify the optimal kernel combination. But the initial formulation of MKL was only able to select the best of the features and misses out many other informative kernels presented. To overcome this, the Lp norm based formulation was proposed by Kloft et. al. This formulation is capable of choosing a non-sparse set of kernels through a control parameter p. Unfortunately, the parameter p doesnot have a direct meaning to the number of kernels selected. We have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand. In this work, we propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can strictly control the number of kernels which we wish to select. The CSKL formulation introduces a parameter t which directly corresponds to the number of kernels selected. It is important to note that a search in t space is finite and fast as compared to p. We have also provided an efficient Reduced Gradient Descent based algorithm to solve the CSKL formulation, which is proven to converge. Through our experiments on the Caltech101 Object Categorization dataset, we have also shown that one can acheive better accuracies than the previous formulations through the right choice of t.",
    "creator" : "LaTeX with hyperref package"
  }
}