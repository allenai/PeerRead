{
  "name" : "1609.06492.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Document Image Coding and Clustering for Script Discrimination",
    "authors" : [ "Darko Brodić", "Alessia Amelio", "Zoran N. Milivojević", "Milena Jevtić" ],
    "emails" : [ "mjevtic}@tf.bor.ac.rs", "aamelio@dimes.unical.it", "zoran.milivojevic@vtsnis.edu.rs" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Historical documents, Feature extraction, Script recognition, Clustering"
    }, {
      "heading" : "1 Introduction",
      "text" : "Script recognition has a great importance in document image analysis and optical character recognition [1]. Typically, it represents a process of automatic recognition of script by computer in scanned documents [2]. This process usually reduces the number of different symbol classes, which is then considered for classification [3].\nThe proposed methods for script recognition have been classified as global or local ones [1]. Global methods divide the image of the document into larger blocks to be normalized and cleaned from the noise. Then, statistical or frequency-domain analysis is employed on the blocks. On the contrary, local methods divide the document image into small blocks of text, called connected components, on which feature analysis, i.e., black pixel runs, is applied [4]. This last method is\n∗(Corresponding author)\nar X\niv :1\n60 9.\n06 49\n2v 1\n[ cs\n.C V\n] 2\nmuch more computationally heavy than global one, but apt to deal with noisy document images. In any case, previously proposed methods reach an accuracy in script identification between 85% and 95% [1].\nIn this paper, we present a new method for discrimination of documents written in different scripts. In contrast to many previous methods, it can be used prior or during the preprocessing stage. It is primarily based on feature extraction from the bounding box method, its height and center point position in the text line. Hence, there is no need to identify the single characters to differentiate scripts. For this reason, it is particularly useful when the documents are noisy. Furthermore, it maps the connected components of the text to only 4 different codes similarly as in [5], which used character code shapes. In this way, the number of variables is considerably reduced, determining a computer non-intensive procedure. A modified version of a clustering method is proposed and applied to the extracted features for grouping documents given in the same script. Experiments performed on Balkan medieval documents in old Cyrillic, angular and round Glagolitic scripts, and German documents in Antiqua and Fraktur scripts determine an accuracy up to 100%. The main application of the proposed approach can be used in the cultural heritage area, i.e., in script recognition and classification of historical documents, which includes their origin as well as the influence of different cultural centers to them.\nThe paper is organized as follows. Section 2 introduces the coding phase and mapping of the text to 1-D image. Section 3 presents the clustering method. Section 4 describes the experiment and discusses it. Finally, Section 5 draws a conclusion."
    }, {
      "heading" : "2 Script Coding",
      "text" : "Coding phase transforms the script into a uniformly coded text which is subjected to feature extraction. It is composed of two main steps: (i) mapping of the text based on typographical features into an image, by adopting text line segmentation, blob extraction, blob heights and center point detection; (ii) extraction of features from image based on run-length and local binary pattern analysis."
    }, {
      "heading" : "2.1 Mapping based on typographical features",
      "text" : "First, the text of the document is transformed into a 1-D image based on its typographical features. Text is segmented into text lines by employing the horizontal projection profile. It is adopted for detecting a central line of reference for each text line. A bounding box is traced to each blob, i.e., letter. It is used to derive the distribution of the blob heights and its center point. Typographical classification of the text is based on these extracted features. Figure 1 shows this step of the algorithm on a short medieval document from Balkan region written in old Cyrillic script.\nBounding box heights and center point locations can determine the categorization of the corresponding blobs into the following classes [6]: (i) base letter (0), (ii) ascender letter (1), (iii) descendent letter (2), and (iv) full letter (3). Figure 2 depicts the classification based on typographical features.\nStarting from this classification, text is transformed into a gray-level 1-D image. In fact, the following mapping is realized: base letter to 0, ascender letter to 1, descendent letter to 2, and full letter to 3 [7]. It determines the coding of the text into a long set of numerical codes 0, 1, 2, 3. Each code has a correspondence with a gray-level, determining the 1-D image. Figure 3 shows the procedure of text coding."
    }, {
      "heading" : "2.2 Feature extraction",
      "text" : "Texture is adopted to compute statistical measures useful to differentiate the images. Run-length analysis can be employed on the obtained 1-D image to create a feature vector of 11 elements representing the document. It computes the following features: (i) short run emphasis (SRE), (ii) long run emphasis (LRE), (iii) gray-level non-uniformity (GLN), (iv) run length non-uniformity (RLN), (v) run percentage (RP) [8], (vi) low gray-level run emphasis (LGRE) and (vii) high graylevel run emphasis (HGRE) [9], (viii) short run low gray-level emphasis (SRLGE), (ix) short run high gray-level emphasis (SRHGE), (x) long run low gray-level emphasis (LRLGE), and (xi) long run high gray-level emphasis (LRHGE) [10]. Local Binary Pattern (LBP) analysis can be suitable to obtain only 4 different features from 00 to 11, if the document is represented by 4 gray level images [11]. However, this number of features is not sufficient for a good discrimination. Hence, LBP is extended to Adjacent Local Binary Pattern (ALBP) [12], which is the horizontal co-occurrence of LBP. It determines 16 features from 0000 to 1111, from which the histogram is computed as a 16-dimensional feature vector [13]. Run-length feature vectors and ALBP feature vectors can be\nemployed for classification and discrimination of scripts in text documents."
    }, {
      "heading" : "3 Clustering Analysis",
      "text" : "Discrimination of feature vectors representing documents in different scripts is performed by an extension of Genetic Algorithms Image Clustering for Document Analysis (GA-ICDA) method [14]. GA-ICDA is a bottom-up evolutionary strategy, for which the document database is represented as a weighted graph G = (V,E,W ). Nodes V correspond to documents and edges E to weighted connections, where W is the set of weights, modeling the affinity degree among the nodes. A node v ∈ V is linked to a subset of its h-nearest neighbor nodes nnhv = {nnhv (1), ..., nnhv (k)}. They represent the k documents most similar to the document of that node. Similarity is based on the L1 norm of the corresponding feature vectors, while h parameter influences the size of the neighborhood. Hence, the similarity w(i, j) between two documents i and j is expressed as:\nw(i, j) = e− d(i,j)2 a2 (1)\nwhere d(i, j) is the L1 norm between i and j and a is a local scale parameter. Then, a node ordering f is established, which is a one-to-one association between graph nodes and integer labels, f : V → {1, 2, ..., n}, n = |V |. Given the node v, the difference is computed between its label f(v) and the labels of the nodes in nnhv . Hence, edges are considered only between v and the nodes in nnhv for which the label difference |f(v)− f(nnhv (j)| is less than a threshold T . It is employed for each node in V , to realize the adjacency matrix of G with low bandwidth. It represents a graph where the connected components, which are the clusters of documents in a given script, are better visible.\nFinally, G is subjected to an evolutionary clustering method to detect clusters of nodes. Then, to refine the obtained solution, a merging procedure is applied on clusters. At each step, the pair of clusters < Ci, Cj > with minimum mutual distance is selected and merged, until a fixed cluster number is reached. The distance between Ci and Cj is computed as the L1 norm between the two farthest document feature vectors, one for each cluster.\nA modification is introduced in the base version of GA-ICDA to be more suitable with complex discrimination tasks like differentiation of historical documents given in different scripts. It consists of extending the similarity concept expressed in Equation (1) to a more general characterization.\nIt is realized by substituting the exponent ’2’ in Equation (1) with a parameter α, to obtain a smoothed similarity computation between the nodes in G, when necessary. It is very useful in such a complex context, where documents appear as variegated, for which their mutual distance can be particularly high, even if they belong to the same script typology. Because a lower exponent in Equation (1) determines a higher similarity value from the corresponding distance value, it allows to mitigate the problem.\nHence, the similarity w(i, j) between two documents i and j is now defined as:\nw(i, j) = e− d(i,j)α a2 (2)"
    }, {
      "heading" : "4 Experimental Results",
      "text" : "The proposed method is evaluated on two complex custom oriented databases. The first one is a collection of labels from Balkan region hand-engraved in stone and hand-printed on paper written in old Cyrillic, angular and round Glagolitic scripts. The database contains 5 labels in old Cyrillic, 10 labels in angular and 5 labels in round Glagolitic, for a total of 20 labels. The second database is composed of 100 historical German documents mainly from the J. W. von Goethes poems, written in Antiqua and Fraktur scripts. The experiment consists of employing the modified GA-ICDA on the run-length and ALBP feature vectors computed from the documents in the two databases, for testing the efficacy in correctly differentiating the script types. A comparison is performed between GA-ICDA with modification and other 4 clustering methods: the base version of GA-ICDA, Complete Linkage Hierarchical clustering, Self-Organizing-Map (SOM) and K-Means, well-known for document categorization [15]. A trial and error procedure is applied on benchmark documents, different from the databases, for tuning the parameters of the methods. Those providing the best solution on the benchmark are employed for clustering. Hence, α parameter is fixed to 1. Precision, Recall, F-Measure (computed for each script class) and Normalized Mutual Information (NMI) are adopted as performance measures for clustering evaluation [16]. Each method has been executed 100 times and average value of measures together with standard deviation have been computed.\nTables 1 and 2 report the results of the experiment respectively on the first and second database.\nFigure 4 shows the corresponding results in graphical form. It is worth noting that GA-ICDA with modification performs considerably better than the other clustering methods for both the databases and that adopted modification determines an improvement in the final result with respect to the base version of GA-ICDA. Also, the standard deviation is always zero. It confirms the stability of the obtained results."
    }, {
      "heading" : "5 Conclusions",
      "text" : "The paper proposed a new method for differentiation of script type in text documents. In the first step, the document was mapped into a uniformly coded text. Then, it was transformed into 1-D gray-level image, from which texture features were extracted. A modified version of the GA-ICDA method was adopted on feature vectors for document discrimination based on script typology. A huge experimentation on two complex databases of historical documents proved the effectiveness of\nthe proposed method. Future work will extend the experiment on large datasets of labels engraved on different materials, like bronze, and will compare the method with other classification algorithms."
    } ],
    "references" : [ {
      "title" : "Script recognition A review",
      "author" : [ "D. Ghosh", "T. Dube", "A. Shivaprasad" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, vol.32, no.12, pp.2142-2161",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Indian script character recognition: A survey",
      "author" : [ "U. Pal", "B.B. Chaudhuri" ],
      "venue" : "Pattern Recognition, vol.37, no.9, pp.1887-1899",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Twenty years of document image analysis in PAMI",
      "author" : [ "N. Nagy" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, vol.22, no.1, pp.38-62",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A generalised framework for script identification",
      "author" : [ "G.D. Joshi", "S. Garg", "J. Sivaswamy" ],
      "venue" : "International Journal of Document Analysis and Recognition, vol.10, no.2, pp.55-68",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Language determination: Natural language processing from scanned document images",
      "author" : [ "P. Sibun", "A.L. Spitz" ],
      "venue" : "Proc. of the 4th Conference on Applied Natural Language Processing, Las Vegas, USA, pp.423-433",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Optical font recognition using typographical features",
      "author" : [ "A.W. Zramdini", "R. Ingold" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, vol.20, no.8, pp.877-882",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "An approach to the script discrimination in the Slavic documents",
      "author" : [ "D. Brodić", "Z.N. Milivojević", "Č.A. Maluckov" ],
      "venue" : "Soft Computing, vol.19, no.9, pp.2655-2665",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Texture analysis using gray level run lengths",
      "author" : [ "M.M. Galloway" ],
      "venue" : "Computer, Graphics and Image Processing, vol.4, no.2, pp.172-179",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Use of gray value distribution of run lengths for texture analysis",
      "author" : [ "A. Chu", "C.M. Sehgal", "J.F. Greenleaf" ],
      "venue" : "Pattern Recognition Letters, vol.11, no.6, pp.415-419",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Image characterizations based on joint gray-level run-length distributions",
      "author" : [ "B.R. Dasarathy", "E.B. Holder" ],
      "venue" : "Pattern Recognition Letters, vol.12, no.8, pp.497-502",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "A comparative study of texture measures with classification based on featured distributions",
      "author" : [ "T. Ojala", "M. Pietikainen", "D. Harwood" ],
      "venue" : "Pattern Recognition, vol.29, no.1, pp.51-59",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Feature extraction based on co-occurrence of adjacent local binary patterns",
      "author" : [ "R. Nosaka", "Y. Ohkawa", "K. Fukui" ],
      "venue" : "Proc. of the 5th Pacific Rim Symposium on Image and Video Technology, Gwanju, South Korea, pp.82-91",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Classification of the scripts in medieval documents from Balkan region by run-length texture analysis",
      "author" : [ "D. Brodić", "A. Amelio", "Z.N. Milivojević" ],
      "venue" : "Proc. of the 22nd Conference on Neural Information Processing, Istanbul, Turkey, pp.442-450",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Characterization and distinction between closely related south Slavic languages on the example of Serbian and Croatian",
      "author" : [ "D. Brodić", "A. Amelio", "Z.N. Milivojević" ],
      "venue" : "Proc. of the 16th International Conference on Computer Analysis of Images and Patterns, Valletta, Malta, pp.654- 666",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mining Text Data",
      "author" : [ "C.C. Aggarwal", "C. Zhai" ],
      "venue" : "Springer USA",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Recent Developments in Document Clustering",
      "author" : [ "N.O. Andrews", "E.A. Fox" ],
      "venue" : "Tech. rep., Computer Science, Virginia Tech.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Script recognition has a great importance in document image analysis and optical character recognition [1].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "Typically, it represents a process of automatic recognition of script by computer in scanned documents [2].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : "This process usually reduces the number of different symbol classes, which is then considered for classification [3].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "The proposed methods for script recognition have been classified as global or local ones [1].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : ", black pixel runs, is applied [4].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "In any case, previously proposed methods reach an accuracy in script identification between 85% and 95% [1].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, it maps the connected components of the text to only 4 different codes similarly as in [5], which used character code shapes.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Bounding box heights and center point locations can determine the categorization of the corresponding blobs into the following classes [6]: (i) base letter (0), (ii) ascender letter (1), (iii) descendent letter (2), and (iv) full letter (3).",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "In fact, the following mapping is realized: base letter to 0, ascender letter to 1, descendent letter to 2, and full letter to 3 [7].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "It computes the following features: (i) short run emphasis (SRE), (ii) long run emphasis (LRE), (iii) gray-level non-uniformity (GLN), (iv) run length non-uniformity (RLN), (v) run percentage (RP) [8], (vi) low gray-level run emphasis (LGRE) and (vii) high graylevel run emphasis (HGRE) [9], (viii) short run low gray-level emphasis (SRLGE), (ix) short run high gray-level emphasis (SRHGE), (x) long run low gray-level emphasis (LRLGE), and (xi) long run high gray-level emphasis (LRHGE) [10].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 8,
      "context" : "It computes the following features: (i) short run emphasis (SRE), (ii) long run emphasis (LRE), (iii) gray-level non-uniformity (GLN), (iv) run length non-uniformity (RLN), (v) run percentage (RP) [8], (vi) low gray-level run emphasis (LGRE) and (vii) high graylevel run emphasis (HGRE) [9], (viii) short run low gray-level emphasis (SRLGE), (ix) short run high gray-level emphasis (SRHGE), (x) long run low gray-level emphasis (LRLGE), and (xi) long run high gray-level emphasis (LRHGE) [10].",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 9,
      "context" : "It computes the following features: (i) short run emphasis (SRE), (ii) long run emphasis (LRE), (iii) gray-level non-uniformity (GLN), (iv) run length non-uniformity (RLN), (v) run percentage (RP) [8], (vi) low gray-level run emphasis (LGRE) and (vii) high graylevel run emphasis (HGRE) [9], (viii) short run low gray-level emphasis (SRLGE), (ix) short run high gray-level emphasis (SRHGE), (x) long run low gray-level emphasis (LRLGE), and (xi) long run high gray-level emphasis (LRHGE) [10].",
      "startOffset" : 488,
      "endOffset" : 492
    }, {
      "referenceID" : 10,
      "context" : "Local Binary Pattern (LBP) analysis can be suitable to obtain only 4 different features from 00 to 11, if the document is represented by 4 gray level images [11].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 11,
      "context" : "Hence, LBP is extended to Adjacent Local Binary Pattern (ALBP) [12], which is the horizontal co-occurrence of LBP.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "It determines 16 features from 0000 to 1111, from which the histogram is computed as a 16-dimensional feature vector [13].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "Discrimination of feature vectors representing documents in different scripts is performed by an extension of Genetic Algorithms Image Clustering for Document Analysis (GA-ICDA) method [14].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 14,
      "context" : "A comparison is performed between GA-ICDA with modification and other 4 clustering methods: the base version of GA-ICDA, Complete Linkage Hierarchical clustering, Self-Organizing-Map (SOM) and K-Means, well-known for document categorization [15].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 15,
      "context" : "Precision, Recall, F-Measure (computed for each script class) and Normalized Mutual Information (NMI) are adopted as performance measures for clustering evaluation [16].",
      "startOffset" : 164,
      "endOffset" : 168
    } ],
    "year" : 2016,
    "abstractText" : "The paper introduces a new method for discrimination of documents given in different scripts. The document is mapped into a uniformly coded text of numerical values. It is derived from the position of the letters in the text line, based on their typographical characteristics. Each code is considered as a gray level. Accordingly, the coded text determines a 1-D image, on which texture analysis by run-length statistics and local binary pattern is performed. It defines feature vectors representing the script content of the document. A modified clustering approach employed on document feature vector groups documents written in the same script. Experimentation performed on two custom oriented databases of historical documents in old Cyrillic, angular and round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the superiority of the proposed method with respect to well-known methods in the state-of-the-art.",
    "creator" : "LaTeX with hyperref package"
  }
}