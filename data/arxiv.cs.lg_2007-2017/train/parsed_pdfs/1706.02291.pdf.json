{
  "name" : "1706.02291.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SOUND EVENT DETECTION USING SPATIAL FEATURES AND CONVOLUTIONAL RECURRENT NEURAL NETWORK",
    "authors" : [ "Sharath Adavanne", "Pasi Pertilä", "Tuomas Virtanen" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— Sound event detection, multichannel audio, spatial features, convolutional recurrent neural network\n1. INTRODUCTION\nSound event detection (SED) task involves recognizing the onset and offset of a sound event in an acoustic scene and further labeling the sound event. The world we live in offers a rich variety of sound events. For example, recognizing environmental sounds [1][2] will give an idea about the local biodiversity. Detecting sound events such as glass breaking and alarm detection can be used for surveillance [3][4]. Furthermore, the detected sound events can be used as a mid-level representation to help retrieval of content based query [5].\nTraditionally SED systems have been using monaural audio. Temko et al. [6] proposed to use multichannel audio, and combined classification likelihoods across channels. While the multichannel audio was used, the actual potential of multichannel features was not exploited. Features like time difference of arrival (TDOA) and mel-band energies from the multichannel audio can potentially help the system differentiate the overlapping sound events. Similar multichannel features have been proposed in automatic speech recognition (ASR) [7] and source separation [8]. Just like humans have evolved\nThe research leading to these results has received funding from the European Research Council under the European Unions H2020 Framework Programme through ERC Grant Agreement 637422 EVERYSOUND. The authors also wish to acknowledge CSC-IT Center for Science, Finland, for computational resources.\nto exploit the spatial data available at their ears (multichannel) to identify both isolated and polyphonic sound events [9], we can potentially train the SED systems to learn similar spatial information with multichannel data. Recently, such spatial features motivated by the binaural hearing of humans were proposed and shown to be promising for SED task in [10]. Although the features showed improvement over monoaural features, the dataset was too small (around one hour) to conclusively prove the superiority of binaural spatial features (referred as binaural features in future).\nIn this paper, we propose to use low-level features and compare it with using high-level features. For example, we compare using generalized cross-correlation with phase based weighting (GCC-PHAT ) instead of the high-level TDOA feature which is extracted from GCC-PHAT , and show that the network learns powerful representation from just the lowlevel features. We show that arranging features from each channel as different layers of a multi-layered input volume enables the network to learn the sound events in multichannel audio better than a simple concatenation of the features. We propose to extend the convolutional recurrent neural network (CRNN) to handle more than one feature type and use a bidirectional LSTM. Finally, we evaluate the improvement of using binaural over monaural features on the 19 hours large TUT-SED 2016 dataset.\nWe present the binaural features used for SED in Section 2, the extended CRNN architecture in Section 3, the experimental set-up and results on two different real-life datasets in Section 4 and our conclusions in Section 5.\n2. BINAURAL FEATURES FOR POLYPHONIC SED Polyphonic SED is the task of recognizing overlapped sound events along with the isolated sound events. The proposed polyphonic SED system has two parts, feature extraction, and a neural network. The neural network described in Section 3 outputs a vector for every sound event class, where each entry in the vector indicates if the sound event was active or not. The feature extraction part extracts the following binaural features at a constant hop length of 20 ms."
    }, {
      "heading" : "2.1. Binaural mel-band energies",
      "text" : "Sound sources which have different spatial locations have different intensities in the binaural channels. Furthermore, most overlapping sound events have different frequency spread in\nar X\niv :1\n70 6.\n02 29\n1v 1\n[ cs\n.S D\n] 7\nJ un\n2 01\n7\nthe spectrum. The combination of this intensity difference in different bands of frequencies can be exploited to differentiate overlapping sound events. This idea is motivated from the interaural intensity difference (IID) used by humans [9].\nLog mel-band energies (referred as mel in future) extracted from both of the binaural channels using 40 mel-bands in 40 ms Hamming window are used as the features. A neural network which is capable of performing linear operations, which includes the difference, can learn to obtain the IID information from these channel-wise energies. By using the channel-wise energies instead of the multichannel energy difference directly, we allow the network to learn other potentially more informative features."
    }, {
      "heading" : "2.2. Time difference of arrival vs cross-correlation",
      "text" : "Based on how the sound sources are spatially located with respect to the binaural microphones, they might have different TDOA values. Furthermore, sound events which are overlapping do not always have the same frequency spread in the spectrum. The combination of this TDOA difference in different frequency bands can be exploited by a network to differentiate overlapping sound events. We implemented it by dividing the spectral frame into five mel-bands and calculating the TDOA values in each of the bands. The TDOA is estimated using the GCC-PHAT [11]. The GCC-PHAT for each mel-band b is extracted separately:\nRb(∆12, t) = N−1∑ k=0 Hb(k) X1(k, t) ·X∗2 (k, t) |X1(k, t)||X2(k, t)| e i2πk∆12 N , (1) where, X1 and X2 are the FFT coefficients of the two binaural channels. X1(k, t) specifies the coefficient at time frame t and kth frequency bin, of the total N bins. Hb(k) is the magnitude response of the bth band in B mel-bands and ∆12 ∈ [−τmax, τmax], where τmax = 30 is the maximum sample delay for a sound wave to travel between binaural microphones. Finally, the peak magnitude for each mel-band and time frame is picked in the GCC-PHAT by τ(b, t) = argmax\n∆12\n{|Rb(∆12, t)|}.\nTDOA’s for each band are extracted using multi-resolution windows of 120 ms, 240 ms, and 480 ms to accommodate sound events of variable length. Five TDOA values picked from five bands, for each of the three resolutions, results in 15 TDOA values per time frame.\nNeural networks have the potential to learn powerful representations from the raw data. We investigate this by using low-level GCC-PHAT and comparing it with high-level TDOA feature (which are picked from the GCC-PHAT ). GCC-PHAT ’s are extracted using Eq. 1 with B set to one. To have a factorizable feature length for max pooling, 60 GCC-PHAT values are picked in -29 to +30 lag for each of the three multi-resolution (same as TDOA), amounting to 180 GCC-PHAT values per time frame. By using GCC-PHAT instead of TDOA, we take the data-oriented approach and get rid of empirical limitations and let the network learn the representation best suited for the problem."
    }, {
      "heading" : "2.3. Dominant frequencies vs auto-correlation",
      "text" : "In [10], it was shown that the three most dominant frequencies and their magnitudes (referred as dom-freq in future) helped in the SED task. This was motivated by the idea that overlapping sound events do not always have the same dominant frequencies, and the network can learn to differentiate these overlapped events using the dominant frequencies. The dom-freq values were picked from thresholded parabolically-interpolated STFT [12] in the 100 to 4000 Hz range from each of the binaural channels in frames of 40 ms. We continue to use this feature in this paper.\nThe pitch is a perceptual feature which human listeners have been using to recognize overlapping sound events [13]. One of the prominent way to estimate pitch values are from the auto-correlation (ACR). In the presented work, ACR is calculated on the binaural channels by time domain autocorrelation in 40 ms windows and choosing 400 correlation values in the range of 107.5 Hz to 4410 Hz. This was selected to be close to the dom-freq extraction range and the number of correlation values easily factorizable during max pooling.\n3. CONVOLUTIONAL RECURRENT NEURAL NETWORK\nThe best results to date in polyphonic SED was reported in [14], where an architecture exploiting the combined modeling capacities of a convolutional neural network (CNN), recurrent neural network (RNN) and fully connected (FC) layer termed as the convolutional recurrent neural network (CRNN) was proposed. We use this CRNN network and extend it for multichannel audio features.\nFeatures from each channel of the multichannel features are layered one over the other to form a volume. More concretely, M frames of a feature, each of length L, from two channels are layered into a M × L × 2 volume. On slicing such a volume along a particular time frame, we get all the multichannel features corresponding to that time frame. The two-dimensional CNN’s by design are built to learn on such volumes, i.e., it initially learns channel-wise filter weights, and further builds an activation map that is obtained as a combination of these channel-wise filter weights, which serves as the inter-channel information. This way we enable the CNN layers in the initial stages of the CRNN network to learn interchannel information from multichannel features. We report the improvement in performance of using such a volume input over simple multichannel feature concatenation (M×2L) in Section 4.4.\nSeparate volumes of each of the multichannel features are created. T time frames of 40 mel features from the two binaural channels are layered into one volume of size T ×40×2. When using dom-freq, dominant frequencies and their magnitudes are treated as different features, and since their feature lengths are the same (3) we layer them in T × 3× 4. For ACRwe layer the 400 correlation values of each channel into a T × 400 × 2 volume. Similarly, the three multi-resolution"
    }, {
      "heading" : "100 3x3 filters, 2D CNN, ReLUs Batch normalization",
      "text" : ""
    }, {
      "heading" : "1x2 max pool",
      "text" : ""
    }, {
      "heading" : "50% dropout",
      "text" : ""
    }, {
      "heading" : "100 3x3 filters, 2D CNN, ReLUs Batch normalization",
      "text" : ""
    }, {
      "heading" : "1x2 max pool",
      "text" : ""
    }, {
      "heading" : "50% dropout",
      "text" : ""
    }, {
      "heading" : "100 3x3 filters, 2D CNN, ReLUs Batch normalization",
      "text" : ""
    }, {
      "heading" : "1x2 max pool",
      "text" : ""
    }, {
      "heading" : "50% dropout",
      "text" : "TDOA features are layered to T × 5× 3 and the 60 values of GCC-PHAT are layered to T × 60× 3.\nSeparate CNN’s are used to learn local shift-invariant features in each of these volumes as shown in Figure 1. Since the dimensions of mel, GCC-PHAT , and ACR are high, we use three CNN layers followed by max pooling to reduce the final feature map dimension to T × 5× 100. When using TDOA and dom-freq features, a single 100-filter CNN layer is used without max pooling. To keep the time information intact for final sound event onset and offset detection, we do not apply max pooling in time (T ) axis. Post CNNs, the feature maps are merged using concatenation and fed to two consecutive bi-directional long short term memory (LSTM). The output layer is a fully-connected time distributed layer which has as many units as the number of classes in the dataset. A sigmoid activation function is used at the output layer to allow several classes to be predicted as active simultaneously. We refer to this as the CBRNN system in future.\nBatch normalization [15] is used in all the CNN layers. A 50% dropout [16] is utilized in all CNNs and LSTMs to avoid over-fitting of the network. The combined architecture was trained by backpropagation through time [17] using Adam optimizer [18] and binary cross-entropy objective. Early stopping was used to reduce overfitting if the F-score (Section 4.2) did not change for 50 epochs. A sequence length of 100 frames (2 seconds) and a batch size of 32 was chosen after calibrating. At test time the sigmoid layer outputs are thresholded with a fixed value of 0.5.\n4. EVALUATION AND RESULTS"
    }, {
      "heading" : "4.1. Datasets",
      "text" : "The proposed SED system is evaluated on two real-life datasets -TUT Sound Events 2009 (TUT-SED 2009) [19] and TUT Sound Events 2016 Development set (TUT-SED 2016)\n[20]. Both datasets have been recorded using in-ear microphones. TUT-SED 2009 has been used for SED in monaural context [14], but no previous work has reported using the binaural recordings on this dataset. TUT-SED 2016 was published as part of the DCASE 2016 challenge [21], to allow public benchmarking. TUT-SED 2009 is fifteen times larger than TUT-SED 2016, by showing considerable improvement on TUT-SED 2009 we can conclusively say the proposed system is learning and exploiting spatial information.\nAll the work proposed in this paper is done in a contextindependent manner, i.e., we train a single system to learn sound event classes across contexts.\nThe first dataset - TUT-SED 2009 consists of 103 binaural recordings from 10 different contexts (listed in Table 2). Each context consists of 8 to 14 recordings which vary from 10 to 30 minutes, amounting to an overall length of 1133 minutes. The recordings have been manually annotated, and the annotated events have been grouped into 61 event classes [19]. Each context has 9-16 event classes, while some events occur in multiple contexts, some are context specific. The dataset defines five-folds for training, validation, and testing.\nThe second dataset - TUT-SED 2016 consists of 22 binaural recordings for two contexts - home and residential area, amounting to 78 minutes. The home context has ten recordings with 11 sound event classes, and the residential area has 12 recordings with seven sound event classes [20]. The dataset defines four-folds for training and testing. We use 20% of the training data for validation, and the same validation is used for all our evaluations."
    }, {
      "heading" : "4.2. Metrics",
      "text" : "The SED system output is evaluated with the reference in fixed length intervals, also called as segment-based evaluation [22]. For each segment k, the following are calculated (i) true positive (TP (k)): total number of events active in both reference and system output segment. (ii) False positive (FP (k)): total number of events active in system output segment but not in reference. (iii) False negative (FN(k)): total number of events active in reference segment but not in system output. The first metric, F-score is then calculated as,\nF = 2 ·\n∑K k=1 TP (k)\n2 · ∑K k=1 TP (k) + ∑K k=1 FP (k) + ∑K\nk=1 FN(k) (2)\nThe second metric, error rate (ER) evaluates the system output based on the number of insertions (I), deletions (D) and substitutions (S).\nER =\n∑K k=1 S(k) + ∑K k=1D(k) + ∑K k=1 I(k)∑K\nk=1N(k) (3)\nWhere N(k) is the number of sound events marked as active in the reference segment k, and\nS(k) = min(FN(k), FP (k)) (4) D(k) = max(0, FN(k)− FP (k)) (5) I(k) = max(0, FP (k)− FN(k)) (6)\nWe use a segment length of one second for ER and F-score estimation. The evaluation metrics are calculated for each context separately and averaged result is presented."
    }, {
      "heading" : "4.3. Baseline",
      "text" : "The proposed CBRNN architecture with binaural features is compared with the state of the art monaural SED system introduced in [14]. The system used 40 monaural log mel-band energies (mel-monaural) as features. The network had three CNN’s each of 96 filters, followed by max pooling in frequency axis reducing the dimension to one. The feature map from CNN was then fed to three LSTMs with 256 units each. The output was a fully-connected layer with units equal to the number of classes in the dataset."
    }, {
      "heading" : "4.4. Results",
      "text" : "Table 1 shows the metrics for multi-layered input of the binaural log mel-band energy features (mel) and concatenating it (mel-concat) for TUT-SED 2009 dataset. Using a multilayered input is seen to perform relatively better than a simple concatenation. Similar improvement was observed using multi-layered input of TDOA, dom-freq,GCC-PHAT and ACR (not tabulated).\nFrom Table 1 we see that using binaural features improves both the ER and F-scores over monaural features (mel-monaural) across datasets. While the dom-freq and mel feature combination gave the best performance in TUTSED 2009, TDOA andmel performed the best for TUT-SED 2016. In numbers, using binaural over monaural features on the same network gives an absolute F-score improvement of 2.7% for TUT-SED 2009 and 6.1% for TUT-SED 2016. By showing this improvement on a larger dataset like TUT-SED 2009, we can more confidently say that the network is truly learning the binaural information.\nFrom the metrics in Table 1 and 2 we see that the performance of usingGCC-PHAT instead of TDOA orACR instead of dom-freq, is comparable. This is a significant result, showing that the network can learn equivalent information of powerful high-level features from just the low-level features. Thereby making the features dataset independent and relieving the tuning of parameters like the number of dom-freq and TDOA values.\nMost of the sound event classes were seen to be recognized better with the binaural features. Since we cannot present all the 79 classes of the two datasets in this paper, we\nshow the context based F-scores for TUT-SED 2009 dataset in Table 2. A general observation is that the dom-freq / ACR and mel are useful for indoor and sound intense environment (bus, hallway, office, and basketball), while TDOA / GCC-PHAT and mel are seen to help in outdoor contexts (beach and street). This also explains why dom-freq and mel gave better results for TUT-SED 2009. While TUT-SED 2016 had one each of indoor and outdoor contexts, TUT-SED 2009 had more indoor contexts than outdoor.\nThe proposed CBRNN architecture using the same melmonaural feature used in CRNN-baseline achieved an Fscore of 68.0% for TUT-SED 2009 and 29.7% for TUT-SED 2016 (Table 1). The difference in the scores with respect to CRNN-baseline can be associated with using a higher dimensional input to LSTM’s in the proposed CBRNN.\n5. CONCLUSION In this paper, we extended convolutional recurrent neural networks to handle multiple feature classes and process featuremaps using bi-directional LSTM’s. A multi-layered input of multichannel features which enables the network to learn sound events in a multichannel audio better was proposed. Low-level features were used in place of high-level features, and the network was shown to learn high-level equivalent information from simple low-level features. The performance of the system was evaluated on two datasets - a larger dataset for proving that the binaural features truly help in improving the sound event detection, and a public dataset, to allow other researchers to benchmark. The proposed network using binaural spatial features was shown to recognize sound events better than using just the monaural features.\n6. REFERENCES\n[1] K. J. Piczak, “Environmental sound classification with convolutional neural networks,” in IEEE International Workshop on Machine Learning for Signal Processing (MLSP), 2015.\n[2] S. Chu, S. Narayanan, and C. J. Kuo, “Environmental sound recognition with time-frequency audio features,” in IEEE Transactions on Audio, Speech, and Language Processing, 2009.\n[3] M. Crocco, M. Cristani, A. Trucco, and V. Murino, “Audio surveillance: A systematic review,” in ACM Computing Surveys (CSUR), 2016.\n[4] A. Harma, M. F. McKinney, and J. Skowronek, “Automatic surveillance of the acoustic activity in our living environment,” in IEEE International Conference on Multimedia and Expo (ICME), 2005.\n[5] M. Xu, C. Xu, L. Duan, J. S. Jin, and S. Luo, “Audio keywords generation for sports video analysis,” in ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 2008.\n[6] A. Temko, C. Nadeu, and J. Biel, “Acoustic event detection: SVM-based system and evaluation setup in CLEAR’07,” in Springer-Verlag, Berlin, 2008.\n[7] A. Schwarz, C. Huemmer, R. Maas, and W. Kellermann, “Spatial diffuseness features for DNN-based speech recognition in noisy and reverberant environments,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[8] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio source separation with deep neural networks,” in IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.\n[9] J. W. Strutt, “On our perception of sound direction,” in Philosophical Magazine, 1907.\n[10] S. Adavanne, G. Parascandolo, P. Pertilä, T. Heittola, and T. Virtanen, “Sound event detection in multichannel audio using spatial and harmonic features,” in Detection and Classification of Acoustic Scenes and Events (DCASE), 2016.\n[11] C. Knapp and C. Carter, “The generalized correlation method for estimation of time delay,” in IEEE Transactions on Acoustics, Speech, and Signal Processing, 1976.\n[12] J. O. Smith, Sinusoidal Peak Interpolation, in Spectral Audio Signal Processing, accessed\n23.06.2016, online book, 2011 edition. [Online]. Available: https://ccrma.stanford.edu/∼jos/sasp/ Sinusoidal Peak Interpolation.htm\n[13] A. S. Bregman, “Auditory scene analysis: The perceptual organization of sound,” in MIT Press, 1990.\n[14] E. Cakir, G. Parascandolo, T. Heittola, H. Huttunen, and T. Virtanen, “Convolutional recurrent neural networks for polyphonic sound event detection,” in IEEE/ACM TASLP Special Issue on Sound Scene and Event Analysis, 2017, accepted for publication.\n[15] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” CoRR, vol. abs/1502.03167, 2015.\n[16] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” in Journal of Machine Learning Research (JMLR), 2014.\n[17] P. J. Werbos, “Backpropagation through time: what it does and how to do it,” in Proceedings of the IEEE, 1990.\n[18] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in arXiv:1412.6980 [cs.LG], 2014.\n[19] T. Heittola, A. Mesaros, A. Eronen, and T. Virtanen, “Audio context recognition using audio event histograms,” in European Signal Processing Conference (EUSIPCO), 2010.\n[20] A. Mesaros, T. Heittola, and T. Virtanen, “TUT database for acoustic scene classification and sound event detection,” in European Signal Processing Conference (EUSIPCO), 2016.\n[21] “Detection and classification of acoustic scenes and events (DCASE),” 2016. [Online]. Available: http://www.cs.tut.fi/sgn/arg/dcase2016/tasksound-event-detection-in-real-life-audio\n[22] A. Mesaros, T. Heittola, and T. Virtanen, “Metrics for polyphonic sound event detection,” in Applied Sciences, 2016."
    } ],
    "references" : [ {
      "title" : "Environmental sound classification with convolutional neural networks",
      "author" : [ "K.J. Piczak" ],
      "venue" : "IEEE International Workshop on Machine Learning for Signal Processing (MLSP), 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Environmental sound recognition with time-frequency audio features",
      "author" : [ "S. Chu", "S. Narayanan", "C.J. Kuo" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, 2009.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Audio surveillance: A systematic review",
      "author" : [ "M. Crocco", "M. Cristani", "A. Trucco", "V. Murino" ],
      "venue" : "ACM Computing Surveys (CSUR), 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Automatic surveillance of the acoustic activity in our living environment",
      "author" : [ "A. Harma", "M.F. McKinney", "J. Skowronek" ],
      "venue" : "IEEE International Conference on Multimedia and Expo (ICME), 2005.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Audio keywords generation for sports video analysis",
      "author" : [ "M. Xu", "C. Xu", "L. Duan", "J.S. Jin", "S. Luo" ],
      "venue" : "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 2008.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Acoustic event detection: SVM-based system and evaluation setup in CLEAR’07",
      "author" : [ "A. Temko", "C. Nadeu", "J. Biel" ],
      "venue" : "Springer-Verlag, Berlin, 2008.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Spatial diffuseness features for DNN-based speech recognition in noisy and reverberant environments",
      "author" : [ "A. Schwarz", "C. Huemmer", "R. Maas", "W. Kellermann" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multichannel audio source separation with deep neural networks",
      "author" : [ "A.A. Nugraha", "A. Liutkus", "E. Vincent" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On our perception of sound direction",
      "author" : [ "J.W. Strutt" ],
      "venue" : "Philosophical Magazine, 1907.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1907
    }, {
      "title" : "Sound event detection in multichannel audio using spatial and harmonic features",
      "author" : [ "S. Adavanne", "G. Parascandolo", "P. Pertilä", "T. Heittola", "T. Virtanen" ],
      "venue" : "Detection and Classification of Acoustic Scenes and Events (DCASE), 2016.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The generalized correlation method for estimation of time delay",
      "author" : [ "C. Knapp", "C. Carter" ],
      "venue" : "IEEE Transactions on Acoustics, Speech, and Signal Processing, 1976.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Sinusoidal Peak Interpolation, in Spectral Audio Signal Processing, accessed 23.06.2016, online book, 2011 edition. [Online]. Available: https://ccrma.stanford.edu/∼jos/sasp/ Sinusoidal Peak Interpolation.htm",
      "author" : [ "J.O. Smith" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Auditory scene analysis: The perceptual organization of sound",
      "author" : [ "A.S. Bregman" ],
      "venue" : "MIT Press, 1990.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Convolutional recurrent neural networks for polyphonic sound event detection",
      "author" : [ "E. Cakir", "G. Parascandolo", "T. Heittola", "H. Huttunen", "T. Virtanen" ],
      "venue" : "IEEE/ACM TASLP Special Issue on Sound Scene and Event Analysis, 2017, accepted for publication.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "CoRR, vol. abs/1502.03167, 2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research (JMLR), 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "P.J. Werbos" ],
      "venue" : "Proceedings of the IEEE, 1990.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv:1412.6980 [cs.LG], 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Audio context recognition using audio event histograms",
      "author" : [ "T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen" ],
      "venue" : "European Signal Processing Conference (EUSIPCO), 2010.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "TUT database for acoustic scene classification and sound event detection",
      "author" : [ "A. Mesaros", "T. Heittola", "T. Virtanen" ],
      "venue" : "European Signal Processing Conference (EU- SIPCO), 2016.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Metrics for polyphonic sound event detection",
      "author" : [ "A. Mesaros", "T. Heittola", "T. Virtanen" ],
      "venue" : "Applied Sciences, 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For example, recognizing environmental sounds [1][2] will give an idea about the local biodiversity.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "For example, recognizing environmental sounds [1][2] will give an idea about the local biodiversity.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Detecting sound events such as glass breaking and alarm detection can be used for surveillance [3][4].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "Detecting sound events such as glass breaking and alarm detection can be used for surveillance [3][4].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, the detected sound events can be used as a mid-level representation to help retrieval of content based query [5].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "[6] proposed to use multichannel audio, and combined classification likelihoods across channels.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "Similar multichannel features have been proposed in automatic speech recognition (ASR) [7] and source separation [8].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "Similar multichannel features have been proposed in automatic speech recognition (ASR) [7] and source separation [8].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "to exploit the spatial data available at their ears (multichannel) to identify both isolated and polyphonic sound events [9], we can potentially train the SED systems to learn similar spatial information with multichannel data.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "Recently, such spatial features motivated by the binaural hearing of humans were proposed and shown to be promising for SED task in [10].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "This idea is motivated from the interaural intensity difference (IID) used by humans [9].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "The TDOA is estimated using the GCC-PHAT [11].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "In [10], it was shown that the three most dominant frequencies and their magnitudes (referred as dom-freq in future) helped in the SED task.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "The dom-freq values were picked from thresholded parabolically-interpolated STFT [12] in the 100 to 4000 Hz range from each of the binaural channels in frames of 40 ms.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "The pitch is a perceptual feature which human listeners have been using to recognize overlapping sound events [13].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "The best results to date in polyphonic SED was reported in [14], where an architecture exploiting the combined modeling capacities of a convolutional neural network (CNN), recurrent neural network (RNN) and fully connected (FC) layer termed as the convolutional recurrent neural network (CRNN) was proposed.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "Batch normalization [15] is used in all the CNN layers.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "A 50% dropout [16] is utilized in all CNNs and LSTMs to avoid over-fitting of the network.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 16,
      "context" : "The combined architecture was trained by backpropagation through time [17] using Adam optimizer [18] and binary cross-entropy objective.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "The combined architecture was trained by backpropagation through time [17] using Adam optimizer [18] and binary cross-entropy objective.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : "The proposed SED system is evaluated on two real-life datasets -TUT Sound Events 2009 (TUT-SED 2009) [19] and TUT Sound Events 2016 Development set (TUT-SED 2016) [20].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "The proposed SED system is evaluated on two real-life datasets -TUT Sound Events 2009 (TUT-SED 2009) [19] and TUT Sound Events 2016 Development set (TUT-SED 2016) [20].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "TUT-SED 2009 has been used for SED in monaural context [14], but no previous work has reported using the binaural recordings on this dataset.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "The recordings have been manually annotated, and the annotated events have been grouped into 61 event classes [19].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 19,
      "context" : "The home context has ten recordings with 11 sound event classes, and the residential area has 12 recordings with seven sound event classes [20].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 20,
      "context" : "The SED system output is evaluated with the reference in fixed length intervals, also called as segment-based evaluation [22].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "The proposed CBRNN architecture with binaural features is compared with the state of the art monaural SED system introduced in [14].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "CRNN baseline [14] 0.",
      "startOffset" : 14,
      "endOffset" : 18
    } ],
    "year" : 2017,
    "abstractText" : "This paper proposes to use low-level spatial features extracted from multichannel audio for sound event detection. We extend the convolutional recurrent neural network to handle more than one type of these multichannel features by learning from each of them separately in the initial stages. We show that instead of concatenating the features of each channel into a single feature vector the network learns sound events in multichannel audio better when they are presented as separate layers of a volume. Using the proposed spatial features over monaural features on the same network gives an absolute F-score improvement of 6.1% on the publicly available TUT-SED 2016 dataset and 2.7% on the TUT-SED 2009 dataset that is fifteen times larger.",
    "creator" : "LaTeX with hyperref package"
  }
}