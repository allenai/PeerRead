{
  "name" : "1706.02295.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generative-Discriminative Variational Model for Visual Recognition",
    "authors" : [ "Chih-Kuan Yeh", "Yao-Hung Hubert Tsai", "Yu-Chiang Frank Wang" ],
    "emails" : [ "jason6582@gmail.com", "yaohungt@cs.cmu.edu", "ycwang@citi.sinica.edu.tw" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10]. Compared to traditional learning approaches, learning DNN is to produce an over-parameterized model, which generally requires a large-scale dataset during the training stage. As a result, if the size of the training dataset is not sufficiently large, how to derive DNN models while alleviating possible overfitting problems would be of interest. For example, dropout [25] is a regularization technique which randomly removes a fraction of neurons during the training phase, and batch normalization [12] imposes constraints for normalizing the produced representations.\nAnother preferable strategy for alleviating overfitting is to derive latent feature representation by learning generative models. For example, Gaussian Processes Latent Variable Models (GPLVM) [17] construct low-dimensional manifolds by observing a small number of instances. More specifically, it maps latent variables with a predetermined prior into the data space via a Gaussian process. Variational autoencoders (VAE) [13] derives deep latent spaces by utilizing stochastic variational inference, which scales efficiently to the datasets with varying sizes. However, the above methods cannot handle supervised learning tasks. Salakhutdinov et al. [21] apply generative Restricted Boltzmann Machines for learning Deep Boltzmann Machines, which is latter used to fine-tune a classification network. Susskind et al. [26] use a gated MRF at the lowest level of deep belief network for recognition. However, to reach the goal of supervised learning, existing generative-\nar X\niv :1\n70 6.\n02 29\n5v 1\n[ cs\n.L G\n] 7\nbased approaches typically require a different (network) component which is specifically trained for introducing the discriminative ability.\nIn this paper, we propose a discriminative latent variable model, referred to as generativediscriminative variational model (GDVM), which can be implemented by the use of deep neural networks. We approach standard supervised learning tasks by casting discriminative learning (i.e. recognition, classification tasks) as a generative learning problem. We derive deep generative models with an intermediate latent variable, which is inferred from the input data and allows a generative process with additional discriminative capabilities. By maximizing the conditional log-likelihood via the framework of stochastic gradient variational Bayes (SGVB), our GDVM can be utilized in existing DNNs with marginally additional computation costs. We further show that our non-deterministic inference function for recognition would further maximize the margin between classes in the latent space. This is the reason why, as supported by our experiments, promising recognition results can be achieved by using our GDVM for solving classification problems.\nWe highlight our contributions of the paper as follows:\n• We propose a generative-discriminative variational model (GDVM), which uniquely casts standard supervised recognition learning into a generative learning process, and can be easily integrated into existing DNN structures.\n• We provide detailed derivations with fundamental supports, which explain how our model jointly exhibits generative and discriminative abilities for supervised learning. We also present insight discussions and explain how our GDVM differs from maximum-margin and existing generative DNN models.\n• The non-deterministic inference function in our GDVM (i.e., Q(z|x) as detailed in Sect. 3) enforces data separation. As verified by a variety of recognition tasks (i.e., multi-class classification, multi-label classification, and zero-shot learning), improved recognition performance can be achieved."
    }, {
      "heading" : "2 Related Work",
      "text" : "Prevent Overfitting in DNN: In order to train DNNs with sufficient generalization ability and avoid overfitting, researchers have proposed different regularizers when learning parameter weights (e.g., [30]). More recently, a line of works in preventing overfitting in DNN is achieved by suppressing correlation between network activations [25, 12, 4]. Alternatively, Liao et al. [18] encourage parsimonious representations as a form of regularization. Nevertheless, the main idea of regularizing DNN is to limit the search space of the network parameters. Along with a generative variational learning framework, we regularize the distribution of the derived latent representation to fit a given simple prior, which would introduce the additional separation between classes and thus result in improved recognition performance.\nDeep Generative Models: Generative models aim to capture data distributions so that a proper feature representation can be produced for achieving learning tasks like image inpainting [33] and feature disentanglement [20, 2]. A number of deep generative models have been proposed and attracted the attention from the researchers, including Deep Boltzmann machine [21] and Deep Belief Networks [11]. Recently, Generative Adversarial Network (GAN) [7] and Variational Autoencoder [13] have shown remarkable performances on a variety of learning tasks. Recently, deep generative models can achieve promising results on recognition tasks [26, 21, 19] by combining a discriminative objective function with a generative one. Our model is built upon variational inference and learning with the goal of solving classification problems. As detailed later, our model can be easily implemented via learning DNN/CNN models, which uniquely integrates and optimizes the discriminative and generative objectives.\nConditional Generative Models: Stochastic Feed-forward Neural Network (SFNN) [28] and Conditional Variational Autoencoder (CVAE) [24, 32] are both directed graphical models, which derive distributions of output variables based on the input data for structured output prediction (e.g., image completion or image segmentation). SFNN utilizes stochastic feedforward network with hidden layers composed of both deterministic and stochastic variables but suffers from non-trivial inferences. While CVAE utilizes Gaussian latent variables in their models, it uses different networks (with different inputs) to inference the latent variable in training and testing. The above inefficiency or\ninconsistency during training and testing might not be preferable in practical tasks. In this paper, our proposed variational model is learned with generative and discriminative abilities and focuses on supervised recognition tasks. In later sections, we will explain how our approach differs from existing generative models for supervised learning, and why the improved separation between data of different classes can be expected."
    }, {
      "heading" : "3 Generative-Discriminative Variational Model (GDVM)",
      "text" : ""
    }, {
      "heading" : "3.1 Variational Model for Supervised Learning",
      "text" : "Let X = {x1, . . . , xN} denote a set of N training instances, and Y = {y1, . . . , yN} as the corresponding training target outputs. For supervised learning, the goal is to learn a function F with a loss metric L so that the the expected loss of L(F(Xt),Yt) would be minimized for given test data Xt and their ground truth outputs Yt. For deep learning, F is often parameterized by layers of neural networks Φ.\nIn this paper, we introduce a latent variable z, which is to generatively model the conditional probability P (y|x) for supervised learning tasks. With the introduced latent variable z, the maximization of the above conditional probability can be written as\nP (y|x) = ∫ P (y|x, z)P (z)dz, (1)\nwhere P (y|x, z) can be calculated by a neural network function Φ(x, z). We note that, if y is unknown, z is assumed to be sampled from a given prior independent of the data distribution for x. In the context of this work, we have this prior as N (0; I) for simplicity. Unfortunately, the direct maximization of (1) requires the sampling of a large number of z from the given prior, which would not be computationally efficient. Thus, as suggested in [13, 6], we wish to perform such sampling while satisfying (1). That is, we choose to maximize (1) with z which makes P (y|x, z) not negligible. To be more precise, we aim at estimating P (z|x, y) for achieving the above goal. For the task of supervised learning, the input data x would contain the target information of y. Thus, we allow a function Q(z|x) which approximates the intractable posterior outputs P (z|x, y)."
    }, {
      "heading" : "3.2 The Variational Bound",
      "text" : "With the introduced latent variable z, we now relate P (y|x) to Ez∈QP (y|x, z), which is the key of variational Bayesian methods. To start, we derive the Kullback-Leiber divergence (KL-divergence, abbreviated as KL) between Q(z|x) and P (z|x, y) as follows:\nKL ( Q(z|x)||P (z|x, y) ) = Ez∼Q(·|x)[logQ(z|x)− logP (z|x, y)]. (2)\nBy applying the Bayes rule to P (z|x, y), we have the following equation containing Ez∈QP (y|x, z) and P (y|x):\nlogP (y|x)−KL ( Q(z|x)||P (z|x, y) ) = Ez∼Q(·|x)[logP (y|x, z)]−KL ( Q(z|x)||P (z|x) ) . (3)\nDue to the non-negativity of KL-divergence, we can effectively view the right hand side in (3) as the variational lower bound of P (y|x). By maximizing this lower bound, the maximization of P (y|x) and minimization of KL ( Q(z|x)||P (z|x, y) ) would be equivalently and implicitly achieved. The former\nmeets our goal of supervised learning, while the latter allows us to obtain an accurate estimation for the intractable term P (z|x, y) with Q(z|x). In our proposed model, we perform stochastic gradient descent on the right hand side of (3) and the optimization process will be discussed in the later sections."
    }, {
      "heading" : "3.3 Optimizing The Variational Bound",
      "text" : "As noted above, optimizing the variational bound of P (y|x) is to maximize (3). More specifically, this is realized by the maximization of the RHS of (3), with additional insights as we now discuss.\nLet Q(·) and P (·) as two neural networks, and z as the hidden layer. As illustrated in Figure 1, we approach the maximization of Ez∼Q(·|x)[logP (y|x, z)] in (3) by optimizing this network structure. On the other hand, minimization of KL ( Q(z|x)||P (z|x) ) in (3) can be regarded as regularizing the\nhidden layer of z, which enforces the output distribution of Q(z|x) to fit a given prior (e.g.,N (0, I) in our work). With networks Q(·) and P (·) properly selected and optimized, we can therefore maximize the variational lower bound of P (y|x) accordingly. In Figure 1, we particularly have the output distribution of Q(·) to be N (µ(x),Σ(x)), where µ(·) and Σ(·) are two neural network components. With this design, the above regularization of z can be integrated easily as follows. Recall that z is sampled from the prior distribution of N (0, I), which is independent of x when y is unknown. Thus, the output distribution of P (z|x) would be N (0, I), and the KL-Divergence between Q(z|x) and P (z|x) can be computed as:\nKL ( N (µ(x),Σ(x))||N (0, I) ) = 1\n2 [tr(Σ(x)) + (µ(x))>(µ(x))− k − log det(Σ(x))], (4)\nwhere k is the dimensionality of z. With a properly designed function Q (as in our GDVM), minimization of KL ( Q(z|x)||P (z|x) ) can be simply achieved by applying stochastic gradient\ndecent.\nAnother advantage of using our network design Q is that, maximization of Ez∼Q(·|x)[logP (y|x, z)] can also be realized by using neural network techniques. Without advancing the network structure like ours, the dependence of Ez∼Q(·|x)[logP (y|x, z)] on Q cannot be observed after sampling z from an arbitrarily designed Q. This is the reason why we apply the reparameterization trick [13] in constructing network components in Q, which relates the output variable z to N (µ(x),Σ(x)) and maintains its dependency on Q. As a result, stochastic gradient descent can be directly applied to update Ez∼Q(·|x)[logP (y|x, z)].\nWhile we show that our design of Q(·) makes maximizing the variational bound of P (y|x) tractable, we further explain this process via a representation learning aspect of view. As depicted in Figure 1, µ(x) can be seen as a standard neural network mapping from the input x to the latent variable z, and Σ(x) can be seen as the observed data variation. Take recognition tasks for example, by maximizing Ez∼Q(·|x)[logP (y|x, z)], our goal is to achieve satisfactory recognition performances with outputs of Σ(·) imposed on the latent space. We note that, if two input instances with distinct labels result in similar representations in the latent space (i.e., similar µ(x)), it is possible to learn a complex classification model for separating one from another. However, with the introduction of Σ(x) in the latent representation, it is less likely to have the same model to exhibit comparable discriminating ability. As a result, the loss observed at the output of the network (e.g., Figure 1(b)) would be propagated to update both µ(·) and Σ(·). As illustrated in Figure 2, this allows the learning of latent representation z for a improved recognition performance.\nAlgorithm 1: Learning of Generative-Discriminative Variational Model Input: Training set D = {(xi, yi)}Ni=1 Randomly initialize neural network Φ, µ,Σ. repeat\nRandomly select a batch of instances xj and yj Sample j from N (0, I) Let zj = j × Σ(xj) + µ(xj) Minimize the loss function in (5) Update Φ, µ,Σ via back propagation\nuntil Converge Output: Network parameters Φ, µ,Σ\nAs for calculating P (y|x, z) in (3), we also advance a neural network Φ(·) as depicted in Figure 1(b). In the context of this work, the learning of this network is achieved by optimizing Ez∼Q(·|x)[P (y|x, z)] It is worth noting that, the input z of the network Φ is sampled from the encoding function/network Q(·|x) with input x. Thus, we simply have Φ(x, z) = Φ(z). More precisely, we let the neural network Φ(z) learned from z ∼ Q(·|x), and Ez∼Q(·|x)[P (y|x, z)] can be calculated accordingly. Given training data X and Y with the proposed network structure consisting of Φ, µ, and Σ, we now summarize the overall loss function as:\nLall = N∑ i=1 {L(yi,Ez∼Q(·|xi)[Φ(z)])+β{ 1 2 [k+log det(Σ(xi))]−tr(Σ(xi))−µ(xi)>µ(xi)}}, (5)\nNote that the first term calculates the loss function for classification. As noted in (4), the second term enforces the distribution of Q(z|x) to fit N (0, I). Selected via validation, β is a parameter balancing between the two terms in (3). With (5) calculated, we can update the network via stochastic back propagation. Algorithm 1 summaries the learning process of our proposed method.\nOnce the learning of the above network architecture is complete, prediction of the output y for each test input xi can be performed via the following deterministic inference process1:\nyi = arg maxy P (y|xi, z), z = Ez∼Q(·|xi)[z], (6)\nwhere Ez∼Q(·|xi)[z] is computed as µ(xi)."
    }, {
      "heading" : "3.4 Additional Remarks",
      "text" : "Comparison to Variational Models: Our proposed variational learning architecture can be viewed as a deep directed graphical model with Gaussian latent variables as Kingma et al. [13] did. In [13],\n1Alternatively, we can draw multiple z from the prior distribution N (0; I) and use the average of P (y|x, z) as prediction.\nVariational Aotoencoder (VAE) is designed to model latent feature representations in an unsupervised learning setting. To generalize to supervised learning settings, Conditional Variational Autoencoder (CVAE) [24] is a conditional directed graphical model, whose inputs x modulate a Gaussian prior on latent variables z for producing the outputs y. When training CVAE, a recognition network θr(z|x, y) is learned to sample z for deriving the generative network θg(y|x, z). On the other hand, the prediction stage of CVAE requires a (different) prior network θp(z|x) for producing z. However, the networks utilized for CVAE training and prediction are different (i.e., θr and θg vs. θp and θg), which might not be optimal.\nExtended from CVAE, Gaussian stochastic neural network (GSNN) [24] applies the same network architecture for θr and θp. Although this makes the training and prediction process consistent, such design might result in deterministic z, which would degrade the model into a non-variational one. In this paper, we advance a unified variational model for supervised learning, in which we fit the distribution of z as N (0, I) when minimizing the classification loss (i.e., (5)). Later in experiments, we will compare our results with those produced by GSNN for supporting the above remark.\nComparison to Maximum-Margin Methods: In a representation learning point of view, our method resembles approaches with maximum margin criteria (MMC), which share the same goal of increasing data separation. However, most MMC-based approaches [29, 5, 8] aim at maximizing a pre-determined margin of data from different classes. Instead, our model enforces the separation of nearby instances (i.e., µ(x)) with distinct labels without the need to explicitly describe the margin. As explained in Section 3.3, this is achieved by back-propagating the loss of (5) to update our network architecture, which allows proper µ(x) and Σ(x) for describing the observed data.\nComparison to Noise-Imposing Learning Methods: We note that, adding noise to a hidden layer has previously been seen in Dropout [25], while denoising autoencoder [31] and data augmentation methods [15] can be considered as imposing noisy/variation information in the input layer. As mentioned in Section 3.3, our introduction of Σ(x) in Q(z|x) can be viewed as adding data-dependent variation information into the derived latent representation z. Such information is learned by our proposed network architecture without the prior knowledge of the data. In the following section, we will also provide quantitative evaluation and comparisons to confirm the effectiveness of our method."
    }, {
      "heading" : "4 Experiments",
      "text" : "For evaluating the performance of our proposed GDVM, we conduct experiments on multi-class classification, multi-label classification, and zero-shot learning. We compare our method to a baseline CNN as well as GSNN [24], which is implemented via setting β = 0 in (5) without recurrent connection. In our experiments, we use the same network structures (i.e., networks Φ and µ) for the baseline CNN, GSNN, and our proposed model, and perform parameter selection via validation using 20% of the training data."
    }, {
      "heading" : "4.1 Multi-Class Classification",
      "text" : "We consider the benchmark dataset of CIFAR10 [14] for multi-class classification2. CIFAR10 consists of 60,000 32×32 images with 10 categories. We use 50,000 images across all categories for training and the remaining ones for testing. For the baseline CNN architecture, we build a network based on VGG [22] with fewer convolution and fully connected neurons due to the small size of images in CIFAR10. We choose the dimension for z as 64. The detailed network structure is presented in the supplementary, which is learned via SGD optimizer with learning rate 0.05 and momentum 0.9. We perform validation on the total epochs in the range of [200, 300, 400], and select β in (5) from the range of [0.1, 0.5, 1.0]. Finally, we report average accuracy of 5 runs for evaluation and comparison.\nTo assess the ability of our GDVM in alleviating overfitting, we perform experiments using [5000, 10000, 20000, 50000] training instances. In Table 1, we observe that our approach performed favorably against the baseline CNN model with or without dropout. When the size of training data is 20, 000, we further observe a 10% improvement (e.g., 76.05 vs. 66.39). This supports the use of our model for achieving satisfactory performance given a limited amount of training data. It can also be seen that GSNN did not necessarily produce improved performance over the baseline CNN. This implies that, without regularizing the latent representation like ours (see (5)), GSNN might not\n2Due to space limit, experiments on the MNIST dataset are presented in the supplementary.\nexhibit sufficient generative capability especially with small training data sizes. Figure 3 visualizes the latent spaces derived by standard CNN and our model. From this figure, it is clear that our method resulted in improved margin for separating data of different classes."
    }, {
      "heading" : "4.2 Multi-Label Classification on NUS-WIDE LITE",
      "text" : "We now evaluate the performance of our GDVM on multi-label classification. Different from multiclass classification, multi-label classification requires the prediction of multiple labels to an input instance. With the proposed generative-discriminative variational model, we aim at verifying that the derived latent representation would exhibit promising ability in describing data with multiple labels, while the difference between data can still be properly identified.\nTo perform the experiments, we consider the multi-label dataset of NUS-WIDE LITE [3], which includes 55,615 images with a total of 81 concepts (i.e., labels). Given the pre-specified training and test sets of this dataset, we apply convolution layers of Alexnet pre-trained on ImageNet as the network components for all methods of interest, and Binary Cross Entropy is utilized for calculating the loss function. The detailed network structure is presented in the supplementary. An Adam optimizer with learning rate 0.001 is used, and the number of training epochs is set to 20. We perform validation for β in the range of [0.1, 0.5, 1.0, 5.0, 10.0]. For evaluation and comparison, we report the average of results (in terms of Micro-F1 and Macro-F1) on test data with 10 runs. The results are listed in Table 2, which confirm the effectiveness of our GDVM, which performed favorably against standard CNN and GSNN. The lack of latent space regularization of GSNN again was not able to achieve comparable results to those reported by CNN, which not only implies the possible non-variational model was produced by GSNN but also supports the use of our proposed network architecture for exhibiting both generative and discriminative abilities."
    }, {
      "heading" : "4.3 Zero-Shot Learning",
      "text" : "Finally, we apply our GDVM for solving a more challenging task of zero-shot learning, which requires one to recognize instances of classes that are unseen during training. We take the AWA dataset [16], and follow the same data split as did in [1]. We choose Cross Modal Transfer (CMT) [23] as the\nbaseline CNN model for zero-shot learning, which performs recognition using the semantic space derived from training data of seen classes.\nWhen training our GDVM for zero-shot learning, we view the input images as x and the output y as the semantic vector of the associated class. In other words, with target outputs y as semantic vectors of interest, we approach zero-shot learning by solving a regression task using our proposed model. The convolution layers in the network structures are those of pre-trained GoogLeNet, with detailed network structures presented in the supplementary.\nAs for the validation data, it is applied for choosing the total number epochs, as well as β in (5) between [0.0001, 0.001.0.01, 0.1]. Note that the β value is expected to be smaller since L2-distance is used for loss calculation in CMT rather than binary-cross-entropy as used in previous CNN models. We report the average of top-1 accuracy for 10 runs, and present the results in Table 3. From this table, it is clear that CMT with the introduction of our GDVM was able to achieve the best results among all CNN-based methods. This confirms the effectiveness of the derived latent space for describing and discriminating between (attribute) data, which would be preferable for the challenging task of zero-shot learning."
    }, {
      "heading" : "4.4 Computational Cost",
      "text" : "As shown in Figure 1, the difference between the network architecture of our GDVM and that of standard DNN is that we introduce an additional layer Σ(x) with an operation layerN (µ(x),Σ(x)) = µ(x) + × Σ(x). Since Σ(x) shares all layers except for the highest one with µ(x), the additional cost for training such network components would be marginal. For quantitative comparisons, we train a neural network with a mini-batch size of 100 on CIFAR10 and evaluate the results over 100 epochs. For the baseline CNN with dropout, we observed the training time per image as 296.52 microseconds, while ours (also with dropout) with 299.22 microseconds. The computation time estimates were performed on a GTX 1080 GPU. Therefore, we confirm that our GDVM was able to achieve promising classification performance without significantly sacrificing the training time."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we presented a novel generative-discriminative variational model (GDVM), which uniquely advances a generative model with a deterministic discriminative objective for supervised learning. Our GDVM is implemented via stochastic neural networks with Gaussian latent variables, while the network architecture can be easily built upon existing CNN structures with marginal computational costs. We discussed the differences between our GDVM and existing generative or noise-imposing DNN models, and we explained why GDVM could be viewed as maximum-margin models without the need to explicitly determine the margin. As verified by a variety of classification tasks, our GDVM was shown to be preferable over baseline or state-of-the-art DNN models, especially when the training set size was not sufficiently large."
    }, {
      "heading" : "6 Supplement: Multi-Class Classification on MNIST",
      "text" : "We perform experiments on multi-class classification using the hand-written digit dataset of MNIST, which consists of 60,000 images of 10 categories. Following the data split of [14], we apply the CNN architecture for training our GDVM (see Sect. 7 for detailed structure), and utilize the RMSProp optimizer with the learning rate of 0.001 and ρ of 0.9. With 20% of training data as the validate set, we select β from [0.1, 0.5, 1.0, 5.0, 10.0] on the total epochs in the range of [25, 50, 100, 150]. Finally, we report the average accuracy on test data of 10 runs.\nClassification Results:\nWe perform experiments with [500, 1000, 2000, 4000] training instances. Throughout the classification experiments, we fix the dimension for latent layer z as 64. Table 4 lists and compares the results. From this table, we see that our GDVM generally achieved improved or comparable results, compared to standard CNN and GSNN. We note that, nevertheless, all DNN-based methods with dropout were able to achieve the average accuracy of 98.88% with the assess of full training data set of MNIST.\nVisualization Results: We additionally show visualization results for our model. Since the image in MNIST has a lower resolution of 28 × 28 pixels, we set the dimension of the latent layer z as 2 for visualization purpose. Thus, without the need to apply dimension reduction technique like t-SNE, we are able to observe the data distribution of µ(x) directly. As illustrated in Fig. 4, the latent layer µ(x) of our GDVM resulted in better separation between classes, while that of standard CNN was not\nas satisfactory. This supports the above quantitative evaluation, and verifies the use of our GVDM would be preferable for classification."
    }, {
      "heading" : "7 Supplement: Details of The Network Architectures",
      "text" : "We now present the network architecture of our generative-discriminative variational model. The network architectures for multi-class classification on CIFAR10 are shown in Tables 5 and 6. Those of MNIST are shown in Tables 7 and 8. For multi-label classification on NUSWIDE LITE, Tables 9 and 10 list our network designs, and those for zero-shot learning on AWA are shown in Tables 11 and 12. For the experiments using DNN models without dropout, we simply remove all dropout layers when learning networks."
    } ],
    "references" : [ {
      "title" : "Evaluation of output embeddings for fine-grained image classification",
      "author" : [ "Z. Akata", "S. Reed", "D. Walter", "H. Lee", "B. Schiele" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2927–2936,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1607.07539,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Nus-wide: a real-world web image database from national university of singapore",
      "author" : [ "T.-S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y. Zheng" ],
      "venue" : "Proceedings of the ACM international conference on image and video retrieval, page 48. ACM,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reducing overfitting in deep networks by decorrelating representations",
      "author" : [ "M. Cogswell", "F. Ahmed", "R. Girshick", "L. Zitnick", "D. Batra" ],
      "venue" : "arXiv preprint arXiv:1511.06068,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine learning, 20(3):273–297,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Tutorial on variational autoencoders",
      "author" : [ "C. Doersch" ],
      "venue" : "arXiv preprint arXiv:1606.05908,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "Advances in neural information processing systems, pages 2672–2680,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Large scale max-margin multi-label classification with priors",
      "author" : [ "B. Hariharan", "L. Zelnik-Manor", "M. Varma", "S. Vishwanathan" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 423–430,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural computation, 18(7):1527–1554,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "stat, 1050:10,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Attribute-based classification for zero-shot visual object categorization",
      "author" : [ "C.H. Lampert", "H. Nickisch", "S. Harmeling" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(3):453–465,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Gaussian process latent variable models for visualisation of high dimensional data",
      "author" : [ "N.D. Lawrence" ],
      "venue" : "Nips, volume 2, page 5,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Learning deep parsimonious representations",
      "author" : [ "R. Liao", "A. Schwing", "R. Zemel", "R. Urtasun" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5076–5084,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A deep generative deconvolutional image model",
      "author" : [ "Y. Pu", "W. Yuan", "A. Stevens", "C. Li", "L. Carin" ],
      "venue" : "Artificial Intelligence and Statistics, pages 741–750,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning to disentangle factors of variation with manifold interaction",
      "author" : [ "S. Reed", "K. Sohn", "Y. Zhang", "H. Lee" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1431–1439,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep boltzmann machines",
      "author" : [ "R. Salakhutdinov", "G. Hinton" ],
      "venue" : "Artificial Intelligence and Statistics, pages 448–455,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Zero-shot learning through cross-modal transfer",
      "author" : [ "R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng" ],
      "venue" : "Advances in neural information processing systems, pages 935–943,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "K. Sohn", "H. Lee", "X. Yan" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3483–3491,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research, 15(1):1929–1958,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On deep generative models with applications to recognition",
      "author" : [ "J. Susskind", "V. Mnih", "G. Hinton" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning stochastic feedforward neural networks",
      "author" : [ "Y. Tang", "R.R. Salakhutdinov" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 530–538,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Max-margin markov networks",
      "author" : [ "B. Taskar", "C. Guestrin", "D. Koller" ],
      "venue" : "Advances in neural information processing systems, pages 25–32,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological), pages 267–288,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol" ],
      "venue" : "Journal of Machine Learning Research, 11(Dec):3371–3408,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An uncertain future: Forecasting from static images using variational autoencoders",
      "author" : [ "J. Walker", "C. Doersch", "A. Gupta", "M. Hebert" ],
      "venue" : "European Conference on Computer Vision, pages 835–851. Springer,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Semantic image inpainting with perceptual and contextual losses",
      "author" : [ "R. Yeh", "C. Chen", "T.Y. Lim", "M. Hasegawa-Johnson", "M.N. Do" ],
      "venue" : "arXiv preprint arXiv:1607.07539,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10].",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10].",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 26,
      "context" : "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "The recent advances in deep neural networks (DNN) have shown promising results on a variety machine leaning tasks including image classification [9, 15], machine translation [27] and speech recognition [10].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 24,
      "context" : "For example, dropout [25] is a regularization technique which randomly removes a fraction of neurons during the training phase, and batch normalization [12] imposes constraints for normalizing the produced representations.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "For example, dropout [25] is a regularization technique which randomly removes a fraction of neurons during the training phase, and batch normalization [12] imposes constraints for normalizing the produced representations.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : "For example, Gaussian Processes Latent Variable Models (GPLVM) [17] construct low-dimensional manifolds by observing a small number of instances.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Variational autoencoders (VAE) [13] derives deep latent spaces by utilizing stochastic variational inference, which scales efficiently to the datasets with varying sizes.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 20,
      "context" : "[21] apply generative Restricted Boltzmann Machines for learning Deep Boltzmann Machines, which is latter used to fine-tune a classification network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] use a gated MRF at the lowest level of deep belief network for recognition.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : ", [30]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 24,
      "context" : "More recently, a line of works in preventing overfitting in DNN is achieved by suppressing correlation between network activations [25, 12, 4].",
      "startOffset" : 131,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "More recently, a line of works in preventing overfitting in DNN is achieved by suppressing correlation between network activations [25, 12, 4].",
      "startOffset" : 131,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "More recently, a line of works in preventing overfitting in DNN is achieved by suppressing correlation between network activations [25, 12, 4].",
      "startOffset" : 131,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "[18] encourage parsimonious representations as a form of regularization.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "Deep Generative Models: Generative models aim to capture data distributions so that a proper feature representation can be produced for achieving learning tasks like image inpainting [33] and feature disentanglement [20, 2].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 19,
      "context" : "Deep Generative Models: Generative models aim to capture data distributions so that a proper feature representation can be produced for achieving learning tasks like image inpainting [33] and feature disentanglement [20, 2].",
      "startOffset" : 216,
      "endOffset" : 223
    }, {
      "referenceID" : 1,
      "context" : "Deep Generative Models: Generative models aim to capture data distributions so that a proper feature representation can be produced for achieving learning tasks like image inpainting [33] and feature disentanglement [20, 2].",
      "startOffset" : 216,
      "endOffset" : 223
    }, {
      "referenceID" : 20,
      "context" : "A number of deep generative models have been proposed and attracted the attention from the researchers, including Deep Boltzmann machine [21] and Deep Belief Networks [11].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : "A number of deep generative models have been proposed and attracted the attention from the researchers, including Deep Boltzmann machine [21] and Deep Belief Networks [11].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : "Recently, Generative Adversarial Network (GAN) [7] and Variational Autoencoder [13] have shown remarkable performances on a variety of learning tasks.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 12,
      "context" : "Recently, Generative Adversarial Network (GAN) [7] and Variational Autoencoder [13] have shown remarkable performances on a variety of learning tasks.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "Recently, deep generative models can achieve promising results on recognition tasks [26, 21, 19] by combining a discriminative objective function with a generative one.",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 20,
      "context" : "Recently, deep generative models can achieve promising results on recognition tasks [26, 21, 19] by combining a discriminative objective function with a generative one.",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "Recently, deep generative models can achieve promising results on recognition tasks [26, 21, 19] by combining a discriminative objective function with a generative one.",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 27,
      "context" : "Conditional Generative Models: Stochastic Feed-forward Neural Network (SFNN) [28] and Conditional Variational Autoencoder (CVAE) [24, 32] are both directed graphical models, which derive distributions of output variables based on the input data for structured output prediction (e.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : "Conditional Generative Models: Stochastic Feed-forward Neural Network (SFNN) [28] and Conditional Variational Autoencoder (CVAE) [24, 32] are both directed graphical models, which derive distributions of output variables based on the input data for structured output prediction (e.",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 31,
      "context" : "Conditional Generative Models: Stochastic Feed-forward Neural Network (SFNN) [28] and Conditional Variational Autoencoder (CVAE) [24, 32] are both directed graphical models, which derive distributions of output variables based on the input data for structured output prediction (e.",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "Thus, as suggested in [13, 6], we wish to perform such sampling while satisfying (1).",
      "startOffset" : 22,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "Thus, as suggested in [13, 6], we wish to perform such sampling while satisfying (1).",
      "startOffset" : 22,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "This is the reason why we apply the reparameterization trick [13] in constructing network components in Q, which relates the output variable z to N (μ(x),Σ(x)) and maintains its dependency on Q.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "[13] did.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "In [13],",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 23,
      "context" : "To generalize to supervised learning settings, Conditional Variational Autoencoder (CVAE) [24] is a conditional directed graphical model, whose inputs x modulate a Gaussian prior on latent variables z for producing the outputs y.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "Extended from CVAE, Gaussian stochastic neural network (GSNN) [24] applies the same network architecture for θr and θp.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : "However, most MMC-based approaches [29, 5, 8] aim at maximizing a pre-determined margin of data from different classes.",
      "startOffset" : 35,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "However, most MMC-based approaches [29, 5, 8] aim at maximizing a pre-determined margin of data from different classes.",
      "startOffset" : 35,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "However, most MMC-based approaches [29, 5, 8] aim at maximizing a pre-determined margin of data from different classes.",
      "startOffset" : 35,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "Comparison to Noise-Imposing Learning Methods: We note that, adding noise to a hidden layer has previously been seen in Dropout [25], while denoising autoencoder [31] and data augmentation methods [15] can be considered as imposing noisy/variation information in the input layer.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 30,
      "context" : "Comparison to Noise-Imposing Learning Methods: We note that, adding noise to a hidden layer has previously been seen in Dropout [25], while denoising autoencoder [31] and data augmentation methods [15] can be considered as imposing noisy/variation information in the input layer.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "Comparison to Noise-Imposing Learning Methods: We note that, adding noise to a hidden layer has previously been seen in Dropout [25], while denoising autoencoder [31] and data augmentation methods [15] can be considered as imposing noisy/variation information in the input layer.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 23,
      "context" : "We compare our method to a baseline CNN as well as GSNN [24], which is implemented via setting β = 0 in (5) without recurrent connection.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "We consider the benchmark dataset of CIFAR10 [14] for multi-class classification2.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "For the baseline CNN architecture, we build a network based on VGG [22] with fewer convolution and fully connected neurons due to the small size of images in CIFAR10.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "To perform the experiments, we consider the multi-label dataset of NUS-WIDE LITE [3], which includes 55,615 images with a total of 81 concepts (i.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "We take the AWA dataset [16], and follow the same data split as did in [1].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "We take the AWA dataset [16], and follow the same data split as did in [1].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 22,
      "context" : "We choose Cross Modal Transfer (CMT) [23] as the",
      "startOffset" : 37,
      "endOffset" : 41
    } ],
    "year" : 2017,
    "abstractText" : "The paradigm shift from shallow classifiers with hand-crafted features to endto-end trainable deep learning models has shown significant improvements on supervised learning tasks. Despite the promising power of deep neural networks (DNN), how to alleviate overfitting during training has been a research topic of interest. In this paper, we present a Generative-Discriminative Variational Model (GDVM) for visual classification, in which we introduce a latent variable inferred from inputs for exhibiting generative abilities towards prediction. In other words, our GDVM casts the supervised learning task as a generative learning process, with data discrimination to be jointly exploited for improved classification. In our experiments, we consider the tasks of multi-class classification, multi-label classification, and zero-shot learning. We show that our GDVM performs favorably against the baselines or recent generative DNN models.",
    "creator" : "LaTeX with hyperref package"
  }
}