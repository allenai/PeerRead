{
  "name" : "1703.07384.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Ontology Based Pivoted normalization using Vector – Based Approach for information Retrieval",
    "authors" : [ "Vishal Jain", "Mayank Singh" ],
    "emails" : [ "vishaljain83@ymail.com,", "mayanksingh2005@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Ontology Based Pivoted normalization using Vector – Based Approach for information Retrieval\nVishal Jain1 and Dr. Mayank Singh2\n1Research Scholar, Computer Science and Engineering Department, Lingaya’s University, Faridabad 2Associate Professor, Computer Science and Engineering Department, Lingaya’s University, Faridabad\n1vishaljain83@ymail.com, 2mayanksingh2005@gmail.com ABSTRACT An ample amount of documents present on web puts the users in state of dilemma. Users get confused about relevance of documents. Relevance means how closely the given query matches large number of documents. Many information extraction techniques\nare used for extracting documents but they all are in vain. The paper deals with the problem of classification, analyzing and extraction of web documents by using one of information extraction methods called Ontology Based Web Content Mining Methodology. We have evaluated proposed methodology in two specific domains- weather domain (web pages containing information about weather forecasting and analysis) and Google TM collection (web pages containing news). The proposed methodology is procedural i.e. it follows finite number of steps that extracts relevant documents according to user’s query. It is based on principles of Data Mining for analyzing web data. Data Mining first adapts integration of data to generate warehouse. Then, it extracts useful information with the help of algorithm. The task of representing extracted documents is done by using Vector Based Statistical Approach that represents each document in set of Terms. Keywords Data Mining, Ontology, Ontology Web Content Mining Methodology, WORDnet, Vector Based Approach"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Data Mining is called as Knowledge Discovery in Databases (KDD) [1]. It is multi-level field i.e. it includes different areas like Database Systems, Information Retrieval (IR), Machine Learning etc. Prediction and Description are considered as two goals of Data Mining where Prediction involves use of some variables or records in database to predict\nfuture values of other variables while Description finds useful patterns describing the given data. Initial Target Data Pre-Processed Final Data Model Data (P1) (P2) Data (P3) (P4) (P5)\nFigure 1: KDD Process [2]\nBuilding Ontology needs attention of domain expert that represents concepts and relations between them for a given domain. There are many algorithms used for extracting and discovering knowledge from structured data like Naïve Bayes, K-Means etc. The proposed methodology builds ontology for a given domain by using phases of data mining like Data preparation, Data Mapping, extracting knowledge from mapped data etc. Then, classification algorithm is used for writing generated ontology expressed in OWL and XML languages. There are various uses of Ontology: • Used for knowledge sharing and reuse. • Can improve understanding between concepts. • It is useful in Semantic Web that is information in machine form. • Some search engines use ontology for finding relevant pages related to given query. The paper is divided into following sections: Section 2 gives information about following concepts: • Domain Ontology • Stages of Ontology Based Web Content Mining Methodology • Increasing accuracy of classification of web documents using WORDnet. Section 3 describes representation schemes of documents extracted during Ontology Based Phrase Extractor by using Statistical Vector Based Approach. Section 4 concludes about given paper.\n2. CLASSIFYING AND ANALYSING WEB DOCUMENTS Many information extraction methods and techniques were used but they all are in vain. So we need more intelligent system to gather useful information from huge amount of data. Problem: - To find meaningful and informative documents with help of Data Mining algorithms and then interpreting mining results in expressive way. Solution: - Ontology Based Web Content Mining Methodology Approach involved: - The proposed methodology uses concept of Domain Ontology [3]. Domain Ontology organizes concepts, relations and instances into given domain. This approach is used because it resolves synonyms and reducing confusion among agents"
    }, {
      "heading" : "2.1 Ontology Web Based Content Mining",
      "text" : "Ontology Based Web Content Mining represents conceptual information about given domain. It shows document representation, extraction of relevant information from text documents and creates classification models. This methodology is followed that uses the ideas and principles of Data Mining to analyze web data."
    }, {
      "heading" : "2.2 Building Ontology for given domain",
      "text" : "Importance: - Since traditionally domain experts were not so intelligent that they could represent complete knowledge related to query. So, there is need for updating knowledge frequently. It leads to building of ontology.\nIt takes data from given database located at back end server. Data Preparation is included in order to completely understand the meaning of data and lists all tables and attributes that are present in database. Data Mapping states that data is to be represented according to some algorithm. Mapper is used for data mapping. Mapper converts Input data into normal format so that it satisfies user’s requirements in Building Classification algorithm phase."
    }, {
      "heading" : "2.3 Gathering of Information about Documents",
      "text" : "It involves use of Ontology Based Phrase Extractor. Its specification is as follows:\n• Input = Web documents + Domain Ontology and User Abstraction level (K)\n• Output = Documents associated with vector terms (ti) and weights (wi). Process: - Extractor prepares XML file containing instances of ontology with their relationships in hierarchy level. In WORDnet, phrase collection means relevant phrases with their associated concepts of ontology. To extract concepts, we use disambigutive function dis (t) that shows semantically concept for terms (ti) based on given topic. Phrase Extractor as name suggests scans the phrases and as it finds some relevant matter, it refers to related concepts. Each web document is represented as vector of < term ti>, <weight wi> pairs which is extracted from Phrase extractor module."
    }, {
      "heading" : "2.4 Classification Algorithm",
      "text" : "This ontology building algorithm [4] is written on basis of decision tree as follows: INPUT OUTPUT (i) Decision Tree (ii) Distinct Nodes (iii) Distinct Tree branches (iv) Target Attribute (v) GetBranches ()- It is function to get all branches having given node (vi) GetLeafBranch ()- A function to get branch of leaf node (vii) GetClass ()- To get class that shows tree branch (viii) CreateIndividual ()- To create instance of leaf node Ontology\nAlgorithm Begin For each n Class C = n C.Id = N.n DatatypePr DP.Id = N. Dp.AddDo Class c in For each b Dp.AddDo End for End Working: - in OWL la node and Each child name and v using Data All the bra using Get ontology, t get all uniq 2.4 Increa document About WO Miller [5]. Nouns, Ve of synonym and solves In this pap contains 15 example of is as follows: ode N of decis ew (owl: Clas ame operty DP = n name + “Valu main (C); its child node// ranch B of Get main (B.GetC The algorithm nguage. It cr assigns them node of spec alue by acqui typeProperty D nches includin Branches () raverse decisio ue nodes. sing accurac s using WORD RDnet: - WO It is a large rbs, Adverbs, s. Each of s queries throug er, we have us 5287 synonym ontology used ion nodes s) ew (owl: Data e” // It will ad Branches (N) lass (C)) generates on eates class for with unique ific parent no ring property P class. g specific nod function. F n tree from ro y of classific net RDnet was in lexical databa Adjectives com ynonyms repr h search and ed WORDnet s. WORDnet in experimen typeProperty) d property of tology written each distinct ID and name de is assigned of parent node e are returned or generating ot node till we ation of web vented by A se of English bine into sets esents concept lexical results version 3.1. It is one of best ts . . . .\n2.4.1 This class Onto forec abstr syste abstr exper\nResu class Baye WOR expre algor abstr abstr 3. DOC APP Vect appro given in gi docu statis Our E.g. form one f 3.1 R Spac The repre these docu\n2\n4\n6\n8\n10\nA C C U R A C Y %\nExperiment experiment i\nification of logy. We teste asting and action level ( m is tested fo action. We imental evalu\nlts- The ab ification Data s Net and Dnet. We c ssions of dif ithm that action and s action REPRESENT UMENTS ROACH or based ap aches. Such query into TE ven query and ments. These tically. aim is to remo play is word s like played, orm of above w elation betw e terms can be sent documen terms. We ge ment called as\n0\n0\n0\n0\n0\n0\nC4.5 Ba N\n72 5\n80\ns done to im web docume d system in tw GoogleTM. k) related to r both domai\nhave used ations.\nove experim Mining alg SVM are im an classify ferent datase\nshows accur hows less ac\nATION O USING VE\nproach is approaches b RMS. Terms are extracted words are cou\nve different f entered by u playing etc. U\nord in search een Term Ve\nphrases, n-g t as set of ter t set of terms t Space.\nys et Naïve Bayes SV\n6\n87\n5\n76 72\nALGORITHMS\nprove accura nts using D o domains- w There is sp each domain ns before and WORDnet\nent shows orithms like\nproved by simple words ts. Naïve Bay ate result b curate result\nF EXTRAC CTOR BA\none of stat reak documen are words that automatically nted and mea\norms of same ser. It has v ser can specify ing query. ctors in Doc\nrams etc. W ms. Take OR hat represents\nM\n5 65 Before Abstrac\nAfter Abstrac\ncy in omain eather ecific . Our after for\nthat C4.5, using and es is efore after\nTED SED\nistical ts or occur form sured\nword. arious only\nument\ne will ing of entire\ntion\ntion\nT1 OR T2 OR T3 OR …..Tn = Space A document consisting set of terms (space) is called Document Space. Numeric weights are assigned to each term in document that estimates effectiveness of document comparing them with other documents. Each term has different weight in same document. The weights assigned to each term in document Di are expressed as coordinates of document i.e. Di (x,y). So, it is called as Vector from origin to point defined by weight of terms. Y P’ (x,y) P X Term Space: - Each document is represented as dimension. It has some coordinates (weights). Each point is considered as vector. If term is not found in document, then it is assigned as zero weight. Representation of terms in matrix form Combination of document space and term space is represented by Document - by Term Matrix. In this, each row is document Di (in term space) and each column is term (in document space). Di x Term Representation of Query in document space A query entered by user is a set of terms having same weights assigned to it. Query may be in natural language also. In this, it is processed like document which includes removal of redundant words. If query contain terms that are not in document, then it represent dimensions in document space."
    }, {
      "heading" : "3.2 Assignment of weights to terms",
      "text" : "Weight of terms means importance of term i.e. how relevant it is. Weights are assigned by special scheme called as Term Frequency * Inverse Document Frequency (tf * idf) Term Frequency (tf): - It defines number of terms occurred in document. So, it varies from one document to another. Inverse Document Frequency (idf): - It means how many times the given term is distributed in document. It gives probability of terms occurred in document. idf= ln N/n where N= number of documents n = number of relevant documents\nInference: - If all documents are relevant, then idf is zero. We can say that for distinguishing relevant and non relevant documents, the terms in document must be different from given topic so that they can be used for comparing with other documents. Why idf is multiplied by tf? It is done so that good descriptor terms have more importance than bad terms. Good terms are those that occur in small number of documents while Bad terms are those that occur in large number of documents."
    }, {
      "heading" : "3.3 Normalization of Term Vectors",
      "text" : "Weights are normalized according to variable document size. Here we will describe Normalization of term frequency (tf). In this, tf is divided by maximum term frequency tfmax i.e. tf/tfmax. It is defined as frequency of term that occurs mostly in documents. So, we generate factor that lies between 0 and 1. This kind of normalization is called as Maximum Normalization. It is given as: [0.5 + (0.5 * tf /tfmax)] where tf varies from 0.5 to 1. Effect of tf: - The importance of term in given document depends on its frequency of occurrence as compared to other terms in same document. Terms are variables. They can change anytime. Drawback: - Since, normalization factor depends only on frequency of documents. The problem is that terms having higher weights can replace terms with lower weights. E.g. A document is about computer design. It includes various components, hardware software. Let us consider hardware is highly weighted term that occurs six times in document. It will occur most because it is used in building computer. Then, the frequency of this term will replace all other terms by factor of 3. Solution: - Logarithmic Term Frequency In this, we take natural log plus constant i.e. log (tf) +1. Its normalization factor does not depend on maximum term frequency (tfmax). It reduces the effect of term with high frequency such that two terms Tf1 & Tf2 >0 then [Log (tf2) +1 / log (tf1) +1] < tf2 / tf1."
    }, {
      "heading" : "3.3.1 Normalization by Vector Length",
      "text" : "In this, every component of vector is calculated. Each component is divided by Euclidian length of vector.\nR = x i + y j Euclidian Length of R =√ x2 + y2 Cosine Normalization = x / √ x2 + y2, y / √ x2 + y2 = x / √r2cos2 + r2sin2 It is called cosine normalization because normalized vector √cos2 + sin2 has length = 1. It is written as n^=1. Cosine normalization reduces the effect of single term with high frequency by combining it with other low weighted terms. Since vector length is function of all vector components i.e. tdf *idf weights. So, weight of high frequency term is reduced by idf factor. Cosine normalization takes into account the weights of all terms in a given document. It is done for short documents rather than longer documents because short documents are about single topic relevant to given query. For every document or query, there is a stage when all the terms that are retrieved are also relevant. It is indicated by probability of relevance = Probability of retrieval. It has led to development of Correction Factor. It is factor that maps old normalization function (Cosine normalization) into new function. Concept of Pivot Normalization This correction factor rotates the old normalized function clockwise around crossover point (point where probability of relevance = Probability of retrieval) so that normalization values below that point are greater and values above it are lesser. The crossover point is called PIVOT. It is called as Pivot Normalization. Old = new New K=1 (Slope) Old Pivot normalization focuses on correcting the document normalization. Before pivoted normalization, old normalization = new normalization. When it is rotated clockwise around pivot, then new normalization = Slope * old normalization + constant. Here slope should be less than 1. Putting arbitrary value of pivot for both old and new normalizations, we get final result i.e. [Pivoted normalization = Slope * old normalization + (1 – slope) * Pivot].\nUse of pivoted normalization increases probability of retrieving longer documents although longer documents have both relevant s well as non relevant terms."
    }, {
      "heading" : "4. CONCLUSION",
      "text" : "The paper presents Ontology Web Based Content Mining methodology that helps in classification, identification and extraction of large number of documents present on web. It follows certain number of steps for generating ontology. We have conducted an experiment using WORDnet. The main credit of this work goes to domain ontology in representing documents. Use of WORDnet leads to improvement in classification of web documents with the help of synonyms as it has large collection of similar words related to particular search. Ontology Phrase extractor produces web documents that consists of multiple pages with multiple categories. Each web document is represented as vector of < term ti>, <weight wi> pairs. They are represented using one of statistical Information retrieval (IR) approaches known as Vector- based Approach. The paper also represents terms in document space and normalized them using concept of Pivoted normalization."
    }, {
      "heading" : "ACKNOWLEDGEMENT",
      "text" : "I, Vishal Jain would like to give my sincere thanks to Prof. M. N. Hoda, Director, Bharati Vidyapeeth’s Institute of Computer Applications and Management (BVICAM), New Delhi for giving me opportunity to do Ph.D from Lingaya’s University, Faridabad."
    } ],
    "references" : [ {
      "title" : "Uthuruswamy, “Data Mining and Knowledge discovery in databases",
      "author" : [ "R.U. Fayyad" ],
      "venue" : "“Communications of the ACM,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Implementation of Multi Agent Systems with ontology in Data Mining”, “International",
      "author" : [ "Vishal Jain", "Gagandeep Singh", "Dr. Mayank Singh" ],
      "venue" : "Journal Of Research In Computer Application & Management (IJRCM),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Improving Classification of Multi-Lingual Web  Documents using Domain Ontologies",
      "author" : [ "Litvak", "M. Last", "Kisilevich" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Applying data mining for ontology building",
      "author" : [ "Abd-Elraham Elsayed", "Samhaa Ram", "Mahmod Rafea" ],
      "venue" : "“ACM Conference",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Wordnet: An Online Lexical Database",
      "author" : [ "G.A. Miller", "Beckwith", "Gross" ],
      "venue" : "“International Journal of Lexicography”,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Combining Statistical and semantic approaches to translation of ontologies and taxonomies",
      "author" : [ "John McCrae", "Mauricio Espinoza" ],
      "venue" : "“In Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation”,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "A Vector Space Model for Semantic Similarity Calculation and OWL Ontology Alignment”, “DEXA",
      "author" : [ "R. Tous", "J. Delgado" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "A Vector Based Method of Ontology Matching",
      "author" : [ "Zahra Eidoon", "Naseer Yazdani" ],
      "venue" : "“IEEE Third National Conference on Semantics, Knowledge and Grid”,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Ontology Matching: Machine Learning Approach",
      "author" : [ "A. Doan", "J. Madhavan", "A. Halevy" ],
      "venue" : "“Handbook on Ontologies in Information Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "INTRODUCTION Data Mining is called as Knowledge Discovery in Databases (KDD) [1].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Figure 1: KDD Process [2] Building Ontology needs attention of domain expert that represents concepts and relations between them for a given domain.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "uses concept of Domain Ontology [3].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "4 Classification Algorithm This ontology building algorithm [4] is written on basis of decision tree as follows:",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "4 Increa document About WO Miller [5].",
      "startOffset" : 34,
      "endOffset" : 37
    } ],
    "year" : 2013,
    "abstractText" : "Research Scholar, Computer Science and Engineering Department, Lingaya’s University, Faridabad Associate Professor, Computer Science and Engineering Department, Lingaya’s University, Faridabad vishaljain83@ymail.com, mayanksingh2005@gmail.com ABSTRACT An ample amount of documents present on web puts the users in state of dilemma. Users get confused about relevance of documents. Relevance means how closely the given query matches large number of documents. Many information extraction techniques are used for extracting documents but they all are in vain. The paper deals with the problem of classification, analyzing and extraction of web documents by using one of information extraction methods called Ontology Based Web Content Mining Methodology. We have evaluated proposed methodology in two specific domainsweather domain (web pages containing information about weather forecasting and analysis) and Google TM collection (web pages containing news). The proposed methodology is procedural i.e. it follows finite number of steps that extracts relevant documents according to user’s query. It is based on principles of Data Mining for analyzing web data. Data Mining first adapts integration of data to generate warehouse. Then, it extracts useful information with the help of algorithm. The task of representing extracted documents is done by using Vector Based Statistical Approach that represents each document in set of Terms.",
    "creator" : "PScript5.dll Version 5.2"
  }
}