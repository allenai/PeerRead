{
  "name" : "1409.3813.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Incorporating Semi-supervised Features into Discontinuous Easy-first Constituent Parsing",
    "authors" : [ "Yannick Versley" ],
    "emails" : [ "versley@cl.uni-heidelberg.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The SPMRL shared task 2014 (Seddah et al., 2014) augments the 2013 shared task dataset – dependency and constituent trees for several languages, including discontinuous constituent trees for Swedish and German – with unlabeled data that allows for semisupervised parsing approaches.\nThe following sections explain (i) how multilingual adaptation was performed using the joint information from dependency data and constituency data (with the help of the Universal POS tagset mapping (Petrov et al., 2012) where available), in section 2.1; (ii) the use of word clusters for semi-supervised parsing (section 3.1, and (iii) the addition of a bigram dependency language model to the parser (section 3.2)."
    }, {
      "heading" : "2 Easy-first parsing of discontinuous constituents",
      "text" : "The EAFI parser uses the easy-first parsing approach of Goldberg and Elhalad (2010) for discontinuous constituent parsing. It starts with the sequence of terminals with word forms, lemmas, and part-of-speech tags, and progressively applies the parsing action that has been classified as most certain. Because the classifier only uses features from a small window around the action, the number of feature vectors that have to be computed and scored is linear in the number of words, contrary to approaches that perform parsing based on a dynamic programming approach.\nBy using a swap action similar to the online reordering approach of Nivre et al. (2009), EAFI is able to perform nonprojective constituent parsing in sub-quadratic time, with an actual time consumption being close to linear.\nIn order to learn the classifier for the next action, the training component of EAFI runs the parsing process until the first error (early stopping, cf. Collins and Roark, 2004). The feature vectors of the erroneous action and of the highest-scoring action are used to perform a regularized AdaGrad update (Duchi et al., 2011)."
    }, {
      "heading" : "2.1 Multilingual Adaptations",
      "text" : "The EaFi parser uses two components that are language-specific and not contained in the treebank that is used for training: the first is a head table that is used to induce the head (pre-)terminal of a constituent, and the second is a list of “special” part-of-speech tags where the parser augments the POS values in features with the word forms.\nWe induce a head table by interposing the dependency data in CoNLL format with the constituency trees. Based on this, the actual head of a phrase is the a preterminal that has a governor outside the phrase.\nThis work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/\nar X\niv :1\n40 9.\n38 13\nv1 [\ncs .C\nL ]\n1 2\nSe p\n20 14\nA constituent may have multiple heads when constituency and dependency criteria do not match exactly; In cases such as coordination constructions or appositions, the head determination always follows the rules of the dependency scheme. The head constituent is the constituent that has the head (preterminal) as part of its yield.\nFrom these observed head constituents, we then try to derive a head table in the format used by RPARSE (Maier, 2010) and DISCODOP (van Cranenburgh, 2012) by finding a priorization of the head constituent labels that fits the actual heads maximally well.\nFor each constituent label, we start with a candidate set of all labels of head constituents, and try to find a priorization of these constituent labels that fits the observed head constituents:\n1. Look for phrases where two daughter constituent labels from the candidate set occur. These are called conflicts because the assigned head would depend on the order in the head rule.\nThe score of a label is the number of wins (where this label and another candidate co-occur and this label has the actual head) minus the number of losses (where this label and another candidate co-occur and the other candidate is the actual head constituent).\n2. The label from the candidate set with the highest score is appended to the rule. To decide on rightto-left or left-to-right precedence, look at conflicts between two instances of this label and count the number of conflicts that have been resolved towards the right/left constituent of those cases.\nRemove the label from the candidate set, and all conflicts that contain this candidate. If any labels remain in the candidate set, start again at (1.)\nFor the list of special categories, our intuition is that these will be most useful in cases such as PP attachment (which motivated their treatment as a special case in the case of Goldberg and Elhalad (2010), and possibly conjunctions.\nWe use the Universal Tagset Mapping of Petrov et al. (2012) where it is available to make a three-way split between normal POS tags, closed-class POS tags, and punctuation.\n• In the case of tagsets that have a Universal POS tag mapping, and tags that are mapped to ADP (adpositions) and CONJ (conjunctions) are included in the closed-class tags. Tags that have a universal POS mapping as . (punctuation) count as punctuation.\n• In the case where no such mapping is available, we look at the count of types and tokens.\nIf a tag has more instances containing punctuation than those containing containing a letter, and the number of tokens that contain a letter is less than 5, this tag is treated as punctuation.\nIf a tag has more than 100 occurrences, while it only occurs with less than 40 different word forms, it is treated as a closed-class tag."
    }, {
      "heading" : "2.2 General tuning",
      "text" : "EaFi uses online learning with a hash kernel to realize the learning of parameters – in particular, AdaGrad updates (Duchi et al., 2011) with forward-backward splitting (FOBOS) for L1 regularization (Duchi and Singer, 2009). Several parameters influence the performance of the parser:\n• The size of the weight vector – because a hash kernel is used, collisions of the hash function on the available dimension can have a negative impact on the performance.\nWe use a 400MB weight vector, which still allows training on modestly-sized machines, but leaves room for more features than the 80MB weight vector used by Versley (2014).\n• The size of the regularization parameter. As FOBOS does not modify the weights between updates, smaller parameters seem to work better than larger ones. Goldberg (2013) suggests a value of λ = 0.05∣D∣ , due to Alexandre Passos, where ∣D∣ is the number of decisions per epoch.\nIn our case, a value of λ = 0.001N (for N the number of sentences in the training set) works considerably better than the λ = 0.1N that was used in the initial results reported by Versley (2014)."
    }, {
      "heading" : "3 Integrating semi-supervised features",
      "text" : ""
    }, {
      "heading" : "3.1 Using word clusters",
      "text" : "Augmenting word forms with word clusters is univerally recognized as a straightforward way to improve the generalization performance of a parser. In discriminative parsers such as the dependency parser of Koo et al. (2008), features that use surface forms are complemented by duplicated features where the word forms are (wholly or in part) replaced by clusters. A discriminative framework also allows to use both clusters and reduced clusters.\nCandito and Seddah (2010) have shown that word clusters can productively be incorporated into a generative parser such as the Berkeley Parser, which uses a PCFG with latent annotations (PCFG-LA). In their case, they augment the clusters with suffixes to improve the parser’s ability to assign the correct part-of-speech tags.\nAs EAFI uses discriminative parsing, we followed Koo et al. in providing duplicates of features where word form features are replaced by features using clusters.1\nFor all bigrams m,n (both bigrams, and the skip bigrams n−1n2 and n0n2, the supervised model already includes the combinations\nWmWn WmCn CmWn WmWn\nof words and the category. For each kind K of clusters, we additionally include combination of category and the cluster of the respective head word:\nCmKn KmCn CmKmCn CmCnKm CmKmCnKn\nWe made experiments with the original clusters and with the clusters shortened to 6 bits and 4 bits, respectively, in which the full clusters performed best. The final model combines features using the full clusters with features using the 6-bit cluster prefixes."
    }, {
      "heading" : "3.2 Using a Dependency Bigram Language Model",
      "text" : "For models with a generative component, self-training (as in McClosky et al., 2006) can provide tangible benefits. Indeed, Suzuki et al. (2009) show that it is possible to reach improvements in dependency parsing beyond what is possible with word clustering when combining a discriminative model that uses word clusters with an ensemble of generative models that are used as features.\n1Thanks to Djamé Seddah for providing providing Brown clusters for these languages.\nWhile the approach of Suzuki et al. works with a dynamic programming model of parsing, Zhu et al. (2013) show that it is also possible to use lexical dependency statistics learned from a large corpus to improve a state-of-the-art shift-reduce parser for constituents.\nFollowing Zhu et al., we add features to indicate, for the position pairs (0,1), (1,2), (0,2), whether they belong to the top-10% quantile of non-zero values for one particular head word (HI), to the top-30% (MI), have a non-zero value (LO), or a zero value (NO). The association scores are either determined on the raw counts (Raw), on proportions normalized on the head word (L1) or scored using theG2 likelihood ratio of Dunning (1993).\nThe bigram association strength feature is taken both by itself and paired with the POS tags of the words in question."
    }, {
      "heading" : "4 Experiments",
      "text" : "Among the treebanks used in the SPMRL shared task, German and Swedish have discontinuous constituents – in this case, German has a large number of them (about ten thousand on the five thousand sentences of the test set), while Swedish only has very few (only fifty discontinuous phrases in the 600 sentences of the test set).\nBased on prior experiments, learning on the larger German dataset was run for 15 epochs, whereas training on the Swedish dataset was run for 30 epochs."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "Table 1 shows how the adaptations to the purely supervised part of EAFI influence the results based on results for German gold tags. In particular, the data-driven head table and special POS tags has a slight positive effect. Increasing the size of the weight vector does not seem to have strong effect, which implies that the existing weight vector is sufficient for the feature set used in the experiments. However, a different setting for the regularization constant yields a rather large difference (almost +4%), indicating that the previous setting was suboptimal.\nIn tables 2 and 3, we find the supervised initial results together with experiments regarding the use of clusters and their granularity, and the use of features based on the bigram language model.\nBoth for Swedish and for German, we see that adding cluster-based features improves the results\nconsiderably, with an increase of +1.3% in the case of German and of slightly more than +3.5% in Swedish for predicted tags and +1.7% and +2.6%, respectively, for gold tags.\nWe also see that the shortest version of the clusters (4bit) works less well than the others, while clusters shortened to 6-bit prefixes are relatively close to the results using full clusters."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we have reported adaptations with the dual goal of, firstly, using the EAFI engine for parsing multiple languages by harnessing existing dependency conversions and tagset mappings to provide head rules and lists of closed-class tags; secondly, of improving on these supervised learning results by incorporating features based on data from large corpora without manual annotation, namely Brown clusters and a dependency bigram language model.\nExperimental results show that these two improvements are well-suited to improve the capabilities of the parser. At the same time, they demonstrate that techniques that are well-known in dependency parsing can also be harnessed to create parsers for discontinuous constituent structures that work better than existing parsers that are based on treebank LCFRS grammars, making it a practical solution for the parsing of discontinuous structures such as extraposition and scrambling."
    } ],
    "references" : [ {
      "title" : "Parsing word clusters",
      "author" : [ "Candito", "Marie-Helene", "Djamé Seddah." ],
      "venue" : "Proceedings of the First Workshop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2010).",
      "citeRegEx" : "Candito et al\\.,? 2010",
      "shortCiteRegEx" : "Candito et al\\.",
      "year" : 2010
    }, {
      "title" : "Incremental parsing with the perceptron algorithm",
      "author" : [ "Collins", "Michael", "Brian Roark." ],
      "venue" : "ACL-04.",
      "citeRegEx" : "Collins et al\\.,? 2004",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2004
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi", "John", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research 12:2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient learning with forward-backward splitting",
      "author" : [ "Duchi", "John", "Yoram Singer." ],
      "venue" : "Proceedings of Neural Information Processing Systems (NIPS 2009).",
      "citeRegEx" : "Duchi et al\\.,? 2009",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2009
    }, {
      "title" : "Accurate methods for the statistics of surprise and coincidence",
      "author" : [ "Dunning", "Ted." ],
      "venue" : "Computational Linguistics 19(1):61–74.",
      "citeRegEx" : "Dunning and Ted.,? 1993",
      "shortCiteRegEx" : "Dunning and Ted.",
      "year" : 1993
    }, {
      "title" : "Dynamic-oracle transition-based parsing with calibrated probabilistic output",
      "author" : [ "Goldberg", "Yoav." ],
      "venue" : "Proceedings of IWPT 2013.",
      "citeRegEx" : "Goldberg and Yoav.,? 2013",
      "shortCiteRegEx" : "Goldberg and Yoav.",
      "year" : 2013
    }, {
      "title" : "An efficient algorithm for easy-first non-directional dependency parsing",
      "author" : [ "Goldberg", "Yoav", "Michael Elhalad." ],
      "venue" : "Proceedings of NAACL-2010.",
      "citeRegEx" : "Goldberg et al\\.,? 2010",
      "shortCiteRegEx" : "Goldberg et al\\.",
      "year" : 2010
    }, {
      "title" : "Simple semi-supervised dependency parsing",
      "author" : [ "Koo", "Terry", "Xavier Carreras", "Michael Collins." ],
      "venue" : "ACL 2008.",
      "citeRegEx" : "Koo et al\\.,? 2008",
      "shortCiteRegEx" : "Koo et al\\.",
      "year" : 2008
    }, {
      "title" : "Direct parsing of discontinuous constituents in German",
      "author" : [ "Maier", "Wolfgang." ],
      "venue" : "Proceedings of the NAACL-HLT First Workshop on Statistical Parsing of Morphologically Rich Languages.",
      "citeRegEx" : "Maier and Wolfgang.,? 2010",
      "shortCiteRegEx" : "Maier and Wolfgang.",
      "year" : 2010
    }, {
      "title" : "Reranking and self-training for parser adaptation",
      "author" : [ "McClosky", "David", "Eugene Charniak", "Mark Johnson." ],
      "venue" : "CoLing/ACL 2006.",
      "citeRegEx" : "McClosky et al\\.,? 2006",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2006
    }, {
      "title" : "An improved oracle for dependency parsing with online reordering",
      "author" : [ "Nivre", "Joakim", "Marco Kuhlmann", "Johan Hall." ],
      "venue" : "Proceedings of the 11th International Conference on Parsing Technologies (IWPT).",
      "citeRegEx" : "Nivre et al\\.,? 2009",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2009
    }, {
      "title" : "A universal part-of-speech tagset",
      "author" : [ "Petrov", "Slav", "Dipanjan Das", "Ryan McDonald." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation.",
      "citeRegEx" : "Petrov et al\\.,? 2012",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2012
    }, {
      "title" : "An empirical study of semisupervised structured conditional models for dependency parsing",
      "author" : [ "Suzuki", "Jun", "Hideki Isozaki", "Xavier Carreras", "Michael Collins." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Suzuki et al\\.,? 2009",
      "shortCiteRegEx" : "Suzuki et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient parsing with linear context-free rewriting systems",
      "author" : [ "van Cranenburgh", "Andreas" ],
      "venue" : "EACL",
      "citeRegEx" : "Cranenburgh and Andreas.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cranenburgh and Andreas.",
      "year" : 2012
    }, {
      "title" : "Experiments with easy-first nonprojective constituent parsing",
      "author" : [ "Versley", "Yannick." ],
      "venue" : "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Language.",
      "citeRegEx" : "Versley and Yannick.,? 2014",
      "shortCiteRegEx" : "Versley and Yannick.",
      "year" : 2014
    }, {
      "title" : "Improving shift-reduce constituency parsing with large-scale unlabeled data",
      "author" : [ "Zhu", "Muhua", "Jingbo Zhu", "Huizhen Wang." ],
      "venue" : "Natural Language Engineering .",
      "citeRegEx" : "Zhu et al\\.,? 2013",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "The following sections explain (i) how multilingual adaptation was performed using the joint information from dependency data and constituency data (with the help of the Universal POS tagset mapping (Petrov et al., 2012) where available), in section 2.",
      "startOffset" : 199,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "The feature vectors of the erroneous action and of the highest-scoring action are used to perform a regularized AdaGrad update (Duchi et al., 2011).",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "By using a swap action similar to the online reordering approach of Nivre et al. (2009), EAFI is able to perform nonprojective constituent parsing in sub-quadratic time, with an actual time consumption being close to linear.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "We use the Universal Tagset Mapping of Petrov et al. (2012) where it is available to make a three-way split between normal POS tags, closed-class POS tags, and punctuation.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "EaFi uses online learning with a hash kernel to realize the learning of parameters – in particular, AdaGrad updates (Duchi et al., 2011) with forward-backward splitting (FOBOS) for L1 regularization (Duchi and Singer, 2009).",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "In discriminative parsers such as the dependency parser of Koo et al. (2008), features that use surface forms are complemented by duplicated features where the word forms are (wholly or in part) replaced by clusters.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "In discriminative parsers such as the dependency parser of Koo et al. (2008), features that use surface forms are complemented by duplicated features where the word forms are (wholly or in part) replaced by clusters. A discriminative framework also allows to use both clusters and reduced clusters. Candito and Seddah (2010) have shown that word clusters can productively be incorporated into a generative parser such as the Berkeley Parser, which uses a PCFG with latent annotations (PCFG-LA).",
      "startOffset" : 59,
      "endOffset" : 325
    }, {
      "referenceID" : 9,
      "context" : "2 Using a Dependency Bigram Language Model For models with a generative component, self-training (as in McClosky et al., 2006) can provide tangible benefits. Indeed, Suzuki et al. (2009) show that it is possible to reach improvements in dependency parsing beyond what is possible with word clustering when combining a discriminative model that uses word clusters with an ensemble of generative models that are used as features.",
      "startOffset" : 104,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "While the approach of Suzuki et al. works with a dynamic programming model of parsing, Zhu et al. (2013) show that it is also possible to use lexical dependency statistics learned from a large corpus to improve a state-of-the-art shift-reduce parser for constituents.",
      "startOffset" : 22,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "While the approach of Suzuki et al. works with a dynamic programming model of parsing, Zhu et al. (2013) show that it is also possible to use lexical dependency statistics learned from a large corpus to improve a state-of-the-art shift-reduce parser for constituents. Following Zhu et al., we add features to indicate, for the position pairs (0,1), (1,2), (0,2), whether they belong to the top-10% quantile of non-zero values for one particular head word (HI), to the top-30% (MI), have a non-zero value (LO), or a zero value (NO). The association scores are either determined on the raw counts (Raw), on proportions normalized on the head word (L1) or scored using theG likelihood ratio of Dunning (1993). The bigram association strength feature is taken both by itself and paired with the POS tags of the words in question.",
      "startOffset" : 22,
      "endOffset" : 706
    } ],
    "year" : 2014,
    "abstractText" : "This paper describes adaptations for EAFI, a parser for easy-first parsing of discontinuous constituents, to adapt it to multiple languages as well as make use of the unlabeled data that was provided as part of the SPMRL shared task 2014.",
    "creator" : "LaTeX with hyperref package"
  }
}