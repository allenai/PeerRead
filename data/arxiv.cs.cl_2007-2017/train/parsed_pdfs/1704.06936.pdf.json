{
  "name" : "1704.06936.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "}@is.naist.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n06 93\n6v 1\n[ cs\n.C L\n] 2\n3 A\npr 2\n01 7\nin which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the stateof-the-art results on English and Japanese CCG parsing.1"
    }, {
      "heading" : "1 Introduction",
      "text" : "Supertagging in lexicalized grammar parsing is known as almost parsing (Bangalore and Joshi, 1999), in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word. Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al., 2016), in which the probability of a CCG tree y on a sentence x of length N is the product of the probabilities of supertags (categories) ci (locally factored model):\nP (y|x) = ∏\ni∈[1,N ]\nPtag(ci|x). (1)\nBy not modeling every combinatory rule in a derivation, this formulation enables us to employ efficient A* search (see Section 2), which finds the most probable supertag sequence that can build a well-formed CCG tree.\nAlthough much ambiguity is resolved with this supertagging, some ambiguity still remains. Figure 1 shows an example, where the two CCG\n1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.\nparses are derived from the same supertags. Lewis et al.’s approach to this problem is resorting to some deterministic rule. For example, Lewis et al. (2016) employ the attach low heuristics, which is motivated by the right-branching tendency of English, and always prioritizes (b) for this type of ambiguity. Though for English it empirically works well, an obvious limitation is that it does not always derive the correct parse; consider a phrase “a house in Paris with a garden”, for which the correct parse has the structure corresponding to (a) instead.\nIn this paper, we provide a way to resolve these remaining ambiguities under the locally factored model, by explicitly modeling bilexical dependencies as shown in Figure 1. Our joint model is still locally factored so that an efficient A* search can be applied. The key idea is to predict the head of every word independently as in Eq. 1 with a strong unigram model, for which we utilize the scoring model in the recent successful graph-based dependency parsing on LSTMs (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Specifically, we extend the bi-directional LSTM (bi-\nLSTM) architecture of Lewis et al. (2016) predicting the supertag of a word to predict the head of it at the same time with a bilinear transformation.\nThe importance of modeling structures beyond supertags is demonstrated in the performance gain in Lee et al. (2016), which adds a recursive component to the model of Eq. 1. Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural network every time it searches for a new node. Our model does not resort to the recursive networks while modeling tree structures via dependencies.\nWe also extend the tri-training method of Lewis et al. (2016) to learn our model with dependencies from unlabeled data. On English CCGbank test data, our model with this technique achieves 88.8% and 94.0% in terms of labeled and unlabeled F1, which mark the best scores so far.\nBesides English, we provide experiments on Japanese CCG parsing. Japanese employs freer word order dominated by the case markers and a deterministic rule such as the attach low method may not work well. We show that this is actually the case; our method outperforms the simple application of Lewis et al. (2016) in a large margin, 10.0 points in terms of clause dependency accuracy."
    }, {
      "heading" : "2 Background",
      "text" : "Our work is built on A* CCG parsing (Section 2.1), which we extend in Section 3 with a head prediction model on bi-LSTMs (Section 2.2)."
    }, {
      "heading" : "2.1 Supertag-factored A* CCG Parsing",
      "text" : "CCG has a nice property that since every category is highly informative about attachment decisions, assigning it to every word (supertagging) resolves most of its syntactic structure. Lewis and Steedman (2014) utilize this characteristics of the grammar. Let a CCG tree y be a list of categories 〈c1, . . . , cN 〉 and a derivation on it. Their model looks for the most probable y given a sentence x of length N from the set Y (x) of possible CCG trees under the model of Eq. 1:\nŷ = arg max y∈Y (x)\n∑\ni∈[1,N ]\nlog Ptag(ci|x).\nSince this score is factored into each supertag, they call the model a supertag-factored model.\nExact inference of this problem is possible by A* parsing (Klein and D. Manning, 2003), which\nuses the following two scores on a chart:\nb(Ci,j) = ∑\nck∈ci,j\nlog Ptag(ck|x),\na(Ci,j) = ∑\nk∈[1,N ]\\[i,j]\nmax ck log Ptag(ck|x),\nwhere Ci,j is a chart item called an edge, which abstracts parses spanning interval [i, j] rooted by category C . The chart maps each edge to the derivation with the highest score, i.e., the Viterbi parse for Ci,j . ci,j is the sequence of categories on such Viterbi parse, and thus b is called the Viterbi inside score, while a is the approximation (upper bound) of the Viterbi outside score.\nA* parsing is a kind of CKY chart parsing augmented with an agenda, a priority queue that keeps the edges to be explored. At every step it pops the edge e with the highest priority b(e) + a(e) and inserts that into the chart, and enqueue any edges that can be built by combining e with other edges in the chart. The algorithm terminates when an edge C1,N is popped from the agenda. A* search for this model is quite efficient because both b and a can be obtained from the unigram category distribution on every word, which can be precomputed before search. The heuristics a gives an upper bound on the true Viterbi outside score (i.e., admissible). Along with this the condition that the inside score never increases by expansion (monotonicity) guarantees that the first found derivation on C1,N is always optimal. a(Ci,j) matches the true outside score if the onebest category assignments on the outside words (argmaxck log Ptag(ck|x)) can comprise a wellformed tree with Ci,j , which is generally not true.\nScoring model For modeling Ptag , Lewis and Steedman (2014) use a log-linear model with features from a fixed window context. Lewis et al. (2016) extend this with bi-LSTMs, which encode the complete sentence and capture the long range syntactic information. We base our model on this bi-LSTM architecture, and extend it to modeling a head word at the same time.\nAttachment ambiguity In A* search, an edge with the highest priority b+a is searched first, but as shown in Figure 1 the same categories (with the same priority) may sometimes derive more than one tree. In Lewis and Steedman (2014), they prioritize the parse with longer dependencies, which they judge with a conversion rule from a CCG\ntree to a dependency tree (Section 4). Lewis et al. (2016) employ another heuristics prioritizing low attachments of constituencies, but inevitably these heuristics cannot be flawless in any situations. We provide a simple solution to this problem by explicitly modeling bilexical dependencies."
    }, {
      "heading" : "2.2 Bi-LSTM Dependency Parsing",
      "text" : "For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) in which each dependency arc is scored directly on the outputs of bi-LSTMs. Though the model is first-order, bi-LSTMs enable conditioning on the entire sentence and lead to the state-of-the-art performance. Note that this mechanism is similar to modeling of the supertag distribution discussed above, in that for each word the distribution of the head choice is unigram and can be precomputed. As we will see this keeps our joint model still locally factored and A* search tractable. For score calculation, we use an extended bilinear transformation by Dozat and Manning (2016) that models the prior headness of each token as well, which they call biaffine."
    }, {
      "heading" : "3 Proposed Method",
      "text" : ""
    }, {
      "heading" : "3.1 A* parsing with Supertag and Dependency Factored Model",
      "text" : "We define a CCG tree y for a sentence x = 〈xi, . . . , xN 〉 as a triplet of a list of CCG categories c = 〈c1, . . . , cN 〉, dependencies h = 〈h1, . . . , hN 〉, and the derivation, where hi is the head index of xi. Our model is defined as follows:\nP (y|x) = ∏\ni∈[1,N ]\nPtag(ci|x) ∏\ni∈[1,N ]\nPdep(hi|x).\n(2)\nThe added term Pdep is a unigram distribution of the head choice.\nA* search is still tractable under this model.\nThe search problem is changed as:\nŷ = arg max y∈Y (x)\n(\n∑\ni∈[1,N ]\nlogPtag(ci|x)\n+ ∑\ni∈[1,N ]\nlog Pdep(hi|x)\n)\n,\nand the inside score is given by:\nb(Ci,j) = ∑\nck∈ci,j\nlog Ptag(ck|x) (3)\n+ ∑\nk∈[i,j]\\{root(hCi,j)}\nlogPdep(hk|x),\nwhere hCi,j is a dependency subtree for the Viterbi parse on Ci,j and root(h) returns the root index. We exclude the head score for the subtree root token since it cannot be resolved inside [i, j]. This causes the mismatch between the goal inside score b(C1,N ) and the true model score (log of Eq. 2), which we adjust by adding a special unary rule that is always applied to the popped goal edge C1,N .\nWe can calculate the dependency terms in Eq. 3 on the fly when expanding the chart. Let the currently popped edge be Ai,k, which will be combined with Bk,j into Ci,j . The key observation is that only one dependency arc (between root(hAi,k) and root(hBk,j)) is resolved at every combination (see Figure 2). For every rule C → A B we can define the head direction (see Section 4) and Pdep is obtained accordingly. For example, when the right child B becomes the head, b(Ci,j) = b(Ai,k) + b(Bk,j) + logPdep(hl = m|x), where l = root(hAi,k) and m = root(h B k,j) (l < m).\nThe Viterbi outside score is changed as:\na(Ci,j) = ∑\nk∈[1,N ]\\[i,j]\nmax ck log Ptag(ck|x)\n+ ∑\nk∈L\nmax hk log Pdep(hk|x),\nwhere L = [1, N ] \\ [k′|k′ ∈ [i, j], root(hCi,j) 6= k′]. We regard root(hCi,j) as an outside word since its head is undefined yet. For every outside word we independently assign the weight of its argmax\nhead, which may not comprise a well-formed dependency tree. We initialize the agenda by adding an item for every supertag C and word xi with the score a(Ci,i) = ∑\nk∈I\\{i}max logPtag(ck|x) + ∑\nk∈I max log Pdep(hk|x). Note that the dependency component of it is the same for every word."
    }, {
      "heading" : "3.2 Network Architecture",
      "text" : "Following Lewis et al. (2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena. See Figure 3 for the overall network architecture, where Ptag and Pdep share the common bi-LSTM hidden vectors.\nFirst we map every word xi to their hidden vector ri with bi-LSTMs. The input to the LSTMs is word embeddings, which we describe in Section 6. We add special start and end tokens to each sentence with the trainable parameters following Lewis et al. (2016). For Pdep, we use the biaffine transformation in Dozat and Manning (2016):\ng dep i = MLP dep child(ri), g dep hi = MLP dephead(rhi), Pdep(hi|x) (4)\n∝ exp((gdepi ) TWdepg dep hi +wdepg dep hi ),\nwhere MLP is a multilayered perceptron. Though Lewis et al. (2016) simply use an MLP for mapping ri to Ptag , we additionally utilize the hidden vector of the most probable head hi = argmaxh′i Pdep(h ′ i|x), and apply ri and rhi to a bilinear function:2\ng tag i = MLP tag child(ri), g tag hi = MLP taghead(rhi), (5)\nℓ = (gtagi ) TUtagg tag hi +Wtag\n[\ng tag i g tag hi\n]\n+ btag,\nPtag(ci|x) ∝ exp(ℓc),\nwhere Utag is a third order tensor. As in Lewis et al. these values can be precomputed before search, which makes our A* parsing quite efficient."
    }, {
      "heading" : "4 CCG to Dependency Conversion",
      "text" : "Nowwe describe our conversion rules from a CCG tree to a dependency one, which we use in two pur-\n2 This is inspired by the formulation of label prediction in Dozat and Manning (2016), which performs the best among other settings that remove or reverse the dependence between the head model and the supertag model.\nposes: 1) creation of the training data for the dependency component of our model; and 2) extraction of a dependency arc at each combinatory rule during A* search (Section 3.1). Lewis and Steedman (2014) describe one way to extract dependencies from a CCG tree (LEWISRULE). Below in addition to this we describe two simpler alternatives (HEADFIRST and HEADFINAL), and see the effects on parsing performance in our experiments (Section 6). See Figure 4 for the overview.\nLEWISRULE This is the same as the conversion rule in Lewis and Steedman (2014). As shown in Figure 4c the output looks a familiar English dependency tree.\nFor forward application and (generalized) forward composition, we define the head to be the left argument of the combinatory rule, unless it matches either X/X or X/(X\\Y ), in which case the right argument is the head. For example, on “Black Monday” in Figure 4a we choose Monday as the head of Black. For the backward rules, the conversions are defined as the reverse of the corresponding forward rules. For other rules, RemovePunctuation (rp) chooses the non punctuation argument as the head, while Conjunction (Φ) chooses the right argument.3\n3When applying LEWISRULE to Japanese, we ignore the feature values in determining the head argument, which we find often leads to a more natural dependency structure. For example, in “tabe ta” (eat PAST), the category of auxiliary verb “ta” is Sf1\\Sf2 with f1 6= f2, and thus Sf1 6= Sf2 . We choose “tabe” as the head in this case by removing the feature values, which makes the category X\\X .\nOne issue when applying this method for obtaining the training data is that due to the mismatch between the rule set of our CCG parser, for which we follow Lewis and Steedman (2014), and the grammar in English CCGbank (Hockenmaier and Steedman, 2007) we cannot extract dependencies from some of annotated CCG trees.4 For this reason, we instead obtain the training data for this method from the original dependency annotations on CCGbank. Fortunately the dependency annotations of CCGbank matches LEWISRULE above in most cases and thus they can be a good approximation to it.\nHEADFINAL Among SOV languages, Japanese is known as a strictly head final language, meaning that the head of every word always follows it. Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-toright dependency arcs. Inspired by this tradition, we try a simple HEADFINAL rule in Japanese CCG parsing, in which we always select the right argument as the head. For example we obtain the head final dependency tree in Figure 4e from the Japanese CCG tree in Figure 4b.\nHEADFIRST We apply the similar idea as HEADFINAL into English. Since English has the opposite, SVO word order, we define the simple “head first” rule, in which the left argument always becomes the head (Figure 4d).\n4 For example, the combinatory rules in Lewis and Steedman (2014) do not contain Nconj → N N in CCGbank. Another difficulty is that in English CCGbank the name of each combinatory rule is not annotated explicitly.\nThough this conversion may look odd at first sight it also has some advantages over LEWISRULE. First, since the model with LEWISRULE is trained on the CCGbank dependencies, at inference, occasionally the two components Pdep and Ptag cause some conflicts on their predictions. For example, the true Viterbi parse may have a lower score in terms of dependencies, in which case the parser slows down and may degrade the accuracy. HEADFIRST, in contract, does not suffer from such conflicts. Second, by fixing the direction of arcs, the prediction of heads becomes easier, meaning that the dependency predictions become more reliable. Later we show that this is in fact the case for existing dependency parsers (see Section 5), and in practice, we find that this simple conversion rule leads to the higher parsing scores than LEWISRULE on English (Section 6)."
    }, {
      "heading" : "5 Tri-training",
      "text" : "We extend the existing tri-training method to our models and apply it to our English parsers.\nTri-training is one of the semi-supervised methods, in which the outputs of two parsers on unlabeled data are intersected to create (silver) new training data. This method is successfully applied to dependency parsing (Weiss et al., 2015) and CCG supertagging (Lewis et al., 2016).\nWe simply combine the two previous approaches. Lewis et al. (2016) obtain their silver data annotated with the high quality supertags. Since they make this data publicly available 5, we obtain our silver data by assigning dependency\n5 https://github.com/uwnlp/taggerflow\nstructures on top of them.6\nWe train two very different dependency parsers from the training data extracted from CCGbank Section 02-21. This training data differs depending on our dependency conversion strategies (Section 4). For LEWISRULE, we extract the original dependency annotations of CCGbank. For HEADFIRST, we extract the head first dependencies from the CCG trees. Note that we cannot annotate dependency labels so we assign a dummy “none” label to every arc. The first parser is graph-based RBGParser (Lei et al., 2014) with the default settings except that we train an unlabeled parser and use word embeddings of Turian et al. (2010). The second parser is transition-based lstm-parser (Dyer et al., 2015) with the default parameters.\nOn the development set (Section 00), with LEWISRULE dependencies RBGParser shows 93.8% unlabeled attachment score while that of lstm-parser is 92.5% using gold POS tags. Interestingly, the parsers with HEADFIRST dependencies achieve higher scores: 94.9% by RBGParser and 94.6% by lstm-parser, suggesting that HEADFIRST dependencies are easier to parse. For both dependencies, we obtain more than 1.7 million sentences on which two parsers agree.\nFollowing Lewis et al. (2016), we include 15 copies of CCGbank training set when using these silver data. Also to make effects of the tri-train samples smaller we multiply their loss by 0.4."
    }, {
      "heading" : "6 Experiments",
      "text" : "We perform experiments on English and Japanese CCGbanks."
    }, {
      "heading" : "6.1 English Experimental Settings",
      "text" : "We follow the standard data splits and use Sections 02-21 for training, Section 00 for development, and Section 23 for final evaluation. We report labeled and unlabeled F1 of the extracted CCG semantic dependencies obtained using generate program supplied with C&C parser.\nFor our models, we adopt the pruning strategies in Lewis and Steedman (2014) and allow at most 50 categories per word, use a variable-width beam with β = 0.00001, and utilize a tag dictionary, which maps frequent words to the possible\n6We annotate POS tags on this data using Stanford POS tagger (Toutanova et al., 2003).\nsupertags7. Unless otherwise stated, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).\nWe use as word representation the concatenation of word vectors initialized to GloVe8 (Pennington et al., 2014), and randomly initialized prefix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes appearing less than two times in the training data are mapped to “UNK”.\nOther model configurations are: 4-layer biLSTMs with left and right 300-dimensional LSTMs, 1-layer 100-dimensional MLPs with ELU non-linearity (Clevert et al., 2015) for all MLP depchild, MLP dep head, MLP tag child and MLP tag head, and the Adam optimizer with β1 = 0.9, β2 = 0.9, L2 norm (1e−6), and learning rate decay with the ratio 0.75 for every 2,500 iteration starting from 2e−3, which is shown to be effective for training the biaffine parser (Dozat and Manning, 2016)."
    }, {
      "heading" : "6.2 Japanese Experimental Settings",
      "text" : "We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg9 (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.\nFor Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector10, and 100- dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution (dos Santos and Zadrozny, 2014). We do not use affix vectors as affixes are less informative in Japanese. All characters appearing less than two times are mapped to “UNK”. We use the same parameter settings as English for bi-LSTMs, MLPs, and optimization.\nOne issue in Japanese experiments is evaluation. The Japanese CCGbank is encoded in a different format than the English bank, and no standalone script for extracting semantic dependencies is available yet. For this reason, we evaluate the parser outputs by converting them to bunsetsu\n7We use the same tag dictionary provided with their biLSTM model.\n8 http://nlp.stanford.edu/projects/glove/\n9https://github.com/mynlp/jigg 10 http://www.cl.ecei.tohoku.ac.jp/\nm-suzuki/jawiki vector/\ndependencies, the syntactic representation ordinary used in Japanese NLP (Kudo andMatsumoto, 2002). Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha11 and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b. For example, the sentence in Figure 4e is segmented as “Boku wa | eigo wo | hanashi tai”, from which we extract two dependencies (Boku wa) ← (hanashi tai) and (eigo wo) ← (hanashi tai). We perform this conversion for both gold and output CCG trees and calculate the (unlabeled) attachment accuracy. Though this is imperfect, it can detect important parse errors such as attachment errors and thus can be a good proxy for the performance as a CCG parser."
    }, {
      "heading" : "6.3 English Parsing Results",
      "text" : "Effect of Dependency We first see how the dependency components added in our model affect the performance. Table 1 shows the results on the development set with the several configurations, in which “w/o dep” means discarding the depen-\n11 http://taku910.github.io/cabocha/\ndency terms of the model and applying the attach low heuristics (Section 1) instead (i.e., a supertagfactored model; Section 2.1). We can see that for both LEWISRULE and HEADFIRST, adding dependency terms improves the performance.\nChoice of Dependency Conversion Rule To our surprise, our simple HEADFIRST strategy always leads to better results than the linguistically motivated LEWISRULE. The absolute improvements by tri-training are equally large (about 1.0 points), suggesting that our model with dependencies can also benefit from the silver data.\nExcluding Normal Form Constraints One advantage of HEADFIRST is that the direction of arcs is always right, making the structures simpler and more parsable (Section 5). From another viewpoint, this fixed direction means that the constituent structure behind a (head first) dependency tree is unique. Since the constituent structures of CCGbank trees basically follow the normal form (NF), we hypothesize that the model learned with HEADFIRST has an ability to force the outputs in NF automatically. We summarize the results without the NF constraints in Table 2, which shows that the above argument is correct; the number of violating NF rules on the outputs of HEADFIRST is much smaller than that of LEWISRULE (89 vs. 283). Interestingly the scores of HEADFIRST slightly increase from the models with NF (e.g., 86.8 vs. 86.6 for CCGbank), suggesting that the NF constraints hinder the search of HEADFIRST models occasionally.\nResults on Test Set Parsing results on the test set (Section 23) are shown in Table 3, where we compare our best performing HEADFIRST dependency model without NF constraints with the several existing parsers. In the CCGbank experi-\nment, our parser shows the better result than all the baseline parsers except C&C with an LSTM supertagger (Vaswani et al., 2016). Our parser outperforms EasySRL by 0.5% and our reimplementation of that parser (EasySRL reimpl) by 0.9% in terms of labeled F1. In the tri-training experiment, our parser shows much increased performance of 88.8% labeled F1 and 94.0% unlabeled F1, outperforming the current state-of-theart neuralccg (Lee et al., 2016) that uses recursive neural networks by 0.1 point and 0.3 point in terms of labeled and unlabeled F1. This is the best reported F1 in English CCG parsing.\nEfficiency Comparison We compare the efficiency of our parser with neuralccg and EasySRL reimpl.12 The results are shown in Table 4. For the overall speed (the third row), our parser is faster than neuralccg although lags behind EasySRL reimpl. Inspecting the details, our supertagger runs slower than those of neuralccg and EasySRL reimpl, while in A* search our parser processes over 7 times more sentences than neuralccg. The delay in supertagging can be attributed to several factors, in particular the differences in network architectures including the number of biLSTM layers (4 vs. 2) and the use of bilinear transformation instead of linear one. There are also many implementation differences in our parser (C++A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ TensorFlow (Abadi et al., 2015) supertagger and recursive neural model in C++ DyNet (Neubig et al., 2017))."
    }, {
      "heading" : "6.4 Japanese Parsing Result",
      "text" : "We show the results of the Japanese parsing experiment in Table 5. The simple application of Lewis\n12This experiment is performed on a laptop with 4-thread 2.0 GHz CPU.\net al. (2016) (Supertag model) is not effective for Japanese, showing the lowest attachment score of 81.5%. We observe a performance boost with our method, especially with HEADFINAL dependencies, which outperforms the baseline shift-reduce parser by 1.1 points on category assignments and 4.0 points on bunsetsu dependencies.\nThe degraded results of the simple application of the supertag-factored model can be attributed to the fact that the structure of a Japanese sentence is still highly ambiguous given the supertags (Figure 5). This is particularly the case in constructions where phrasal adverbial/adnominal modifiers (with the supertag S/S) are involved. The result suggests the importance of modeling dependencies in some languages, at least Japanese."
    }, {
      "heading" : "7 Related Work",
      "text" : "There is some past work that utilizes dependencies in lexicalized grammar parsing, which we review briefly here.\nFor Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), there are studies to use the predicted dependency structure to improve HPSG parsing accuracy. Sagae et al. (2007) use dependencies to constrain the form of the output tree. As in our method, for every rule (schema) application they define which child becomes the head and impose a soft constraint that these dependencies agree with the output of the dependency parser. Our method is different\nin that we do not use the one-best dependency structure alone, but rather we search for a CCG tree that is optimal in terms of dependencies and CCG supertags. Zhang et al. (2010) use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags.\nIn the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric. Instead of modeling dependencies alone, our method finds a CCG derivation that has a higher dependency score. Lewis et al. (2015) present a joint model of CCG parsing and semantic role labeling (SRL), which is closely related to our approach. They map each CCG semantic dependency to an SRL relation, for which they give the A* upper bound by the score from a predicate to the most probable argument. Our approach is similar; the largest difference is that we instead model syntactic dependencies from each token to its head, and this is the key to our success. Since dependency parsing can be formulated as independent head selections similar to tagging, we can build the entire model on LSTMs to exploit features from the whole sentence. This formulation is not straightforward in the case of multi-headed semantic dependencies in their model."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We have presented a new A* CCG parsing method, in which the probability of a CCG tree is decomposed into local factors of the CCG categories and its dependency structure. By explicitly modeling the dependency structure, we do not require any deterministic heuristics to resolve attachment ambiguities, and keep the model locally factored so that all the probabilities can be precomputed before running the search. Our parser efficiently finds the optimal parse and achieves the state-of-the-art performance in both English and Japanese parsing."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are grateful to Mike Lewis for answering our questions and your Github repository from which we learned many things. We also thank Yuichiro Sawai for the faster LSTM implementation. This work was in part supported by JSPS\nKAKENHI Grant Number 16H06981, and also by JST CREST Grant Number JPMJCR1301."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org",
      "author" : [ "sudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "sudevan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "sudevan et al\\.",
      "year" : 2015
    }, {
      "title" : "Supertagging: An Approach to Almost Parsing",
      "author" : [ "Srinivas Bangalore", "Aravind K Joshi." ],
      "venue" : "Computational linguistics 25(2):237–265.",
      "citeRegEx" : "Bangalore and Joshi.,? 1999",
      "shortCiteRegEx" : "Bangalore and Joshi.",
      "year" : 1999
    }, {
      "title" : "Wide– Coverage Efficient Statistical Parsing with CCG and Log-Linear Models",
      "author" : [ "Stephen Clark", "James R. Curran." ],
      "venue" : "Computational Linguistics, Volume 33, Number 4, December 2007 http://aclweb.org/anthology/J07-4004.",
      "citeRegEx" : "Clark and Curran.,? 2007",
      "shortCiteRegEx" : "Clark and Curran.",
      "year" : 2007
    }, {
      "title" : "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
      "author" : [ "Djork-Arné Clevert", "Thomas Unterthiner", "Sepp Hochreiter." ],
      "venue" : "CoRR abs/1511.07289. http://arxiv.org/abs/1511.07289.",
      "citeRegEx" : "Clevert et al\\.,? 2015",
      "shortCiteRegEx" : "Clevert et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Character-level Representations for Part-of-Speech Tagging",
      "author" : [ "Cı́cero Nogueira dos Santos", "Bianca Zadrozny" ],
      "venue" : null,
      "citeRegEx" : "Santos and Zadrozny.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Zadrozny.",
      "year" : 2014
    }, {
      "title" : "Deep Biaffine Attention for Neural Dependency Parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "CoRR abs/1611.01734. http://arxiv.org/abs/1611.01734.",
      "citeRegEx" : "Dozat and Manning.,? 2016",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2016
    }, {
      "title" : "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "A. Noah Smith." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient Normal-Form Parsing for Combinatory Categorial Grammar",
      "author" : [ "Jason Eisner." ],
      "venue" : "34th Annual Meeting of the Association for Computational Linguistics. http://aclweb.org/anthology/P96-1011.",
      "citeRegEx" : "Eisner.,? 1996",
      "shortCiteRegEx" : "Eisner.",
      "year" : 1996
    }, {
      "title" : "Normal– form parsing for Combinatory Categorial Grammars with generalized composition and type-raising",
      "author" : [ "Julia Hockenmaier", "Yonatan Bisk." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). Col-",
      "citeRegEx" : "Hockenmaier and Bisk.,? 2010",
      "shortCiteRegEx" : "Hockenmaier and Bisk.",
      "year" : 2010
    }, {
      "title" : "CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank",
      "author" : [ "Julia Hockenmaier", "Mark Steedman." ],
      "venue" : "Computational Linguistics 33(3):355–396. http://www.aclweb.org/anthology/J07-3004.",
      "citeRegEx" : "Hockenmaier and Steedman.,? 2007",
      "shortCiteRegEx" : "Hockenmaier and Steedman.",
      "year" : 2007
    }, {
      "title" : "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics 4:313–327.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "A* Parsing: Fast Exact Viterbi Parse Selection",
      "author" : [ "Dan Klein", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Klein and Manning.,? 2003",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2003
    }, {
      "title" : "Japanese Dependency Analysis using Cascaded Chunking",
      "author" : [ "Taku Kudo", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 6th Conference on Natural Language Learning, CoNLL 2002, Held in cooperation with COLING 2002, Taipei, Taiwan, 2002.",
      "citeRegEx" : "Kudo and Matsumoto.,? 2002",
      "shortCiteRegEx" : "Kudo and Matsumoto.",
      "year" : 2002
    }, {
      "title" : "Global Neural CCG Parsing with Optimality Guarantees",
      "author" : [ "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for",
      "citeRegEx" : "Lee et al\\.,? 2016",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Low-Rank Tensors for Scoring Dependency Structures",
      "author" : [ "Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Lei et al\\.,? 2014",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint A* CCG Parsing and Semantic Role Labelling",
      "author" : [ "Mike Lewis", "Luheng He", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 1444–",
      "citeRegEx" : "Lewis et al\\.,? 2015",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2015
    }, {
      "title" : "LSTM CCG Parsing",
      "author" : [ "Mike Lewis", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Associa-",
      "citeRegEx" : "Lewis et al\\.,? 2016",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2016
    }, {
      "title" : "A* CCG Parsing with a Supertag-factored Model",
      "author" : [ "Mike Lewis", "Mark Steedman." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, pages 990–",
      "citeRegEx" : "Lewis and Steedman.,? 2014",
      "shortCiteRegEx" : "Lewis and Steedman.",
      "year" : 2014
    }, {
      "title" : "DyNet: The Dynamic Neural Network Toolkit",
      "author" : [ "Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin." ],
      "venue" : "arXiv",
      "citeRegEx" : "Kong et al\\.,? 2017",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2017
    }, {
      "title" : "Jigg: A Framework for an Easy Natural Language Processing Pipeline",
      "author" : [ "Hiroshi Noji", "Yusuke Miyao." ],
      "venue" : "Proceedings of ACL2016 System Demonstrations. Association for Computational Linguistics, pages 103–108.",
      "citeRegEx" : "Noji and Miyao.,? 2016",
      "shortCiteRegEx" : "Noji and Miyao.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP). pages 1532– 1543. http://www.aclweb.org/anthology/D14-1162.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Head-driven phrase structure grammar",
      "author" : [ "Carl Pollard", "Ivan A Sag." ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Pollard and Sag.,? 1994",
      "shortCiteRegEx" : "Pollard and Sag.",
      "year" : 1994
    }, {
      "title" : "HPSG Parsing with Shallow Dependency Constraints",
      "author" : [ "Kenji Sagae", "Yusuke Miyao", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics,",
      "citeRegEx" : "Sagae et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Sagae et al\\.",
      "year" : 2007
    }, {
      "title" : "The Syntactic Process",
      "author" : [ "Mark Steedman." ],
      "venue" : "The MIT Press.",
      "citeRegEx" : "Steedman.,? 2000",
      "shortCiteRegEx" : "Steedman.",
      "year" : 2000
    }, {
      "title" : "Chainer: a Next-Generation Open Source Framework for Deep Learning",
      "author" : [ "Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton." ],
      "venue" : "Proceedings of Workshop on Machine Learning Systems (LearningSys) in",
      "citeRegEx" : "Tokui et al\\.,? 2015",
      "shortCiteRegEx" : "Tokui et al\\.",
      "year" : 2015
    }, {
      "title" : "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network",
      "author" : [ "Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Confer-",
      "citeRegEx" : "Toutanova et al\\.,? 2003",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2003
    }, {
      "title" : "Word Representations: A Simple and General Method for Semi-Supervised Learning",
      "author" : [ "Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Associa-",
      "citeRegEx" : "Turian et al\\.,? 2010",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Japanese Dependency Structure Analysis Based on Maximum Entropy Models",
      "author" : [ "Kiyotaka Uchimoto", "Satoshi Sekine", "Hitoshi Isahara." ],
      "venue" : "Ninth Conference of the European Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Uchimoto et al\\.,? 1999",
      "shortCiteRegEx" : "Uchimoto et al\\.",
      "year" : 1999
    }, {
      "title" : "Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources",
      "author" : [ "Sumire Uematsu", "Takuya Matsuzaki", "Hiroki Hanaoka", "Yusuke Miyao", "Hideki Mima." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the",
      "citeRegEx" : "Uematsu et al\\.,? 2013",
      "shortCiteRegEx" : "Uematsu et al\\.",
      "year" : 2013
    }, {
      "title" : "Supertagging With LSTMs",
      "author" : [ "Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-",
      "citeRegEx" : "Vaswani et al\\.,? 2016",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2016
    }, {
      "title" : "Structured Training for Neural Network Transition-Based Parsing",
      "author" : [ "David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-",
      "citeRegEx" : "Weiss et al\\.,? 2015",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2015
    }, {
      "title" : "Shift-Reduce CCG Parsing with a Dependency Model",
      "author" : [ "Wenduan Xu", "Stephen Clark", "Yue Zhang." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association",
      "citeRegEx" : "Xu et al\\.,? 2014",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2014
    }, {
      "title" : "A Simple Approach for HPSG Supertagging Using Dependency Information",
      "author" : [ "Yao-zhong Zhang", "Takuya Matsuzaki", "Jun’ichi Tsujii" ],
      "venue" : "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter",
      "citeRegEx" : "Zhang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Supertagging in lexicalized grammar parsing is known as almost parsing (Bangalore and Joshi, 1999), in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word.",
      "startOffset" : 71,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al., 2016), in which the probability of a CCG tree y on a sentence x of length N is the product of the probabilities of supertags (categories) ci (locally factored model):",
      "startOffset" : 116,
      "endOffset" : 162
    }, {
      "referenceID" : 16,
      "context" : "Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al., 2016), in which the probability of a CCG tree y on a sentence x of length N is the product of the probabilities of supertags (categories) ci (locally factored model):",
      "startOffset" : 116,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "Supertagging in lexicalized grammar parsing is known as almost parsing (Bangalore and Joshi, 1999), in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word. Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al.",
      "startOffset" : 72,
      "endOffset" : 343
    }, {
      "referenceID" : 15,
      "context" : "Lewis et al.’s approach to this problem is resorting to some deterministic rule. For example, Lewis et al. (2016) employ the attach low heuristics, which is motivated by the right-branching tendency of English, and always prioritizes (b) for this type of ambiguity.",
      "startOffset" : 0,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "1 with a strong unigram model, for which we utilize the scoring model in the recent successful graph-based dependency parsing on LSTMs (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 135,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "1 with a strong unigram model, for which we utilize the scoring model in the recent successful graph-based dependency parsing on LSTMs (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 135,
      "endOffset" : 192
    }, {
      "referenceID" : 14,
      "context" : "LSTM) architecture of Lewis et al. (2016) predicting the supertag of a word to predict the head of it at the same time with a bilinear transformation.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "The importance of modeling structures beyond supertags is demonstrated in the performance gain in Lee et al. (2016), which adds a recursive component to the model of Eq.",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "The importance of modeling structures beyond supertags is demonstrated in the performance gain in Lee et al. (2016), which adds a recursive component to the model of Eq. 1. Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural network every time it searches for a new node. Our model does not resort to the recursive networks while modeling tree structures via dependencies. We also extend the tri-training method of Lewis et al. (2016) to learn our model with dependencies from unlabeled data.",
      "startOffset" : 98,
      "endOffset" : 501
    }, {
      "referenceID" : 13,
      "context" : "The importance of modeling structures beyond supertags is demonstrated in the performance gain in Lee et al. (2016), which adds a recursive component to the model of Eq. 1. Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural network every time it searches for a new node. Our model does not resort to the recursive networks while modeling tree structures via dependencies. We also extend the tri-training method of Lewis et al. (2016) to learn our model with dependencies from unlabeled data. On English CCGbank test data, our model with this technique achieves 88.8% and 94.0% in terms of labeled and unlabeled F1, which mark the best scores so far. Besides English, we provide experiments on Japanese CCG parsing. Japanese employs freer word order dominated by the case markers and a deterministic rule such as the attach low method may not work well. We show that this is actually the case; our method outperforms the simple application of Lewis et al. (2016) in a large margin, 10.",
      "startOffset" : 98,
      "endOffset" : 1029
    }, {
      "referenceID" : 17,
      "context" : "Lewis and Steedman (2014) utilize this characteristics of the grammar.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "Scoring model For modeling Ptag , Lewis and Steedman (2014) use a log-linear model with features from a fixed window context.",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "Lewis et al. (2016) extend this with bi-LSTMs, which encode the complete sentence and capture the long range syntactic information.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "In Lewis and Steedman (2014), they prioritize the parse with longer dependencies, which they judge with a conversion rule from a CCG",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "Lewis et al. (2016) employ another heuristics prioritizing low attachments of constituencies, but inevitably these heuristics cannot be flawless in any situations.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) in which each dependency arc is scored directly on the outputs of bi-LSTMs.",
      "startOffset" : 100,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) in which each dependency arc is scored directly on the outputs of bi-LSTMs.",
      "startOffset" : 100,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) in which each dependency arc is scored directly on the outputs of bi-LSTMs. Though the model is first-order, bi-LSTMs enable conditioning on the entire sentence and lead to the state-of-the-art performance. Note that this mechanism is similar to modeling of the supertag distribution discussed above, in that for each word the distribution of the head choice is unigram and can be precomputed. As we will see this keeps our joint model still locally factored and A* search tractable. For score calculation, we use an extended bilinear transformation by Dozat and Manning (2016) that models the prior headness of each token as well, which they call biaffine.",
      "startOffset" : 133,
      "endOffset" : 736
    }, {
      "referenceID" : 14,
      "context" : "Following Lewis et al. (2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "(2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena.",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "(2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena. See Figure 3 for the overall network architecture, where Ptag and Pdep share the common bi-LSTM hidden vectors. First we map every word xi to their hidden vector ri with bi-LSTMs. The input to the LSTMs is word embeddings, which we describe in Section 6. We add special start and end tokens to each sentence with the trainable parameters following Lewis et al. (2016). For Pdep, we use the biaffine transformation in Dozat and Manning (2016):",
      "startOffset" : 11,
      "endOffset" : 514
    }, {
      "referenceID" : 5,
      "context" : "(2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena. See Figure 3 for the overall network architecture, where Ptag and Pdep share the common bi-LSTM hidden vectors. First we map every word xi to their hidden vector ri with bi-LSTMs. The input to the LSTMs is word embeddings, which we describe in Section 6. We add special start and end tokens to each sentence with the trainable parameters following Lewis et al. (2016). For Pdep, we use the biaffine transformation in Dozat and Manning (2016):",
      "startOffset" : 11,
      "endOffset" : 588
    }, {
      "referenceID" : 15,
      "context" : "Though Lewis et al. (2016) simply use an MLP for mapping ri to Ptag , we additionally utilize the hidden vector of the most probable head hi = argmaxhi Pdep(h ′ i|x), and apply ri and rhi to a bilinear function:",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "2 This is inspired by the formulation of label prediction in Dozat and Manning (2016), which performs the best among other settings that remove or reverse the dependence between the head model and the supertag model.",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "Lewis and Steedman (2014) describe one way to extract dependencies from a CCG tree (LEWISRULE).",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "LEWISRULE This is the same as the conversion rule in Lewis and Steedman (2014). As shown in Figure 4c the output looks a familiar English dependency tree.",
      "startOffset" : 53,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "One issue when applying this method for obtaining the training data is that due to the mismatch between the rule set of our CCG parser, for which we follow Lewis and Steedman (2014), and the grammar in English CCGbank (Hockenmaier and Steedman, 2007) we cannot extract dependencies from some of annotated CCG trees.",
      "startOffset" : 218,
      "endOffset" : 250
    }, {
      "referenceID" : 16,
      "context" : "One issue when applying this method for obtaining the training data is that due to the mismatch between the rule set of our CCG parser, for which we follow Lewis and Steedman (2014), and the grammar in English CCGbank (Hockenmaier and Steedman, 2007) we cannot extract dependencies from some of annotated CCG trees.",
      "startOffset" : 156,
      "endOffset" : 182
    }, {
      "referenceID" : 27,
      "context" : "Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-toright dependency arcs.",
      "startOffset" : 28,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-toright dependency arcs.",
      "startOffset" : 28,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "4 For example, the combinatory rules in Lewis and Steedman (2014) do not contain Nconj → N N in CCGbank.",
      "startOffset" : 40,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : "This method is successfully applied to dependency parsing (Weiss et al., 2015) and CCG supertagging (Lewis et al.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : ", 2015) and CCG supertagging (Lewis et al., 2016).",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "Lewis et al. (2016) obtain their silver data annotated with the high quality supertags.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "The first parser is graph-based RBGParser (Lei et al., 2014) with the default settings except that we train an unlabeled parser and use word embeddings of Turian et al.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "The second parser is transition-based lstm-parser (Dyer et al., 2015) with the default parameters.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "The first parser is graph-based RBGParser (Lei et al., 2014) with the default settings except that we train an unlabeled parser and use word embeddings of Turian et al. (2010). The second parser is transition-based lstm-parser (Dyer et al.",
      "startOffset" : 43,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "Following Lewis et al. (2016), we include 15 copies of CCGbank training set when using these silver data.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 17,
      "context" : "For our models, we adopt the pruning strategies in Lewis and Steedman (2014) and allow at most 50 categories per word, use a variable-width beam with β = 0.",
      "startOffset" : 51,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "We annotate POS tags on this data using Stanford POS tagger (Toutanova et al., 2003).",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "Unless otherwise stated, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "Unless otherwise stated, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "Unless otherwise stated, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).",
      "startOffset" : 59,
      "endOffset" : 175
    }, {
      "referenceID" : 20,
      "context" : "We use as word representation the concatenation of word vectors initialized to GloVe (Pennington et al., 2014), and randomly initialized prefix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al.",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : ", 2014), and randomly initialized prefix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes appearing less than two times in the training data are mapped to “UNK”.",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "Other model configurations are: 4-layer biLSTMs with left and right 300-dimensional LSTMs, 1-layer 100-dimensional MLPs with ELU non-linearity (Clevert et al., 2015) for all MLP dep child, MLP dep head, MLP tag child and MLP tag head, and the Adam optimizer with β1 = 0.",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "75 for every 2,500 iteration starting from 2e−3, which is shown to be effective for training the biaffine parser (Dozat and Manning, 2016).",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 28,
      "context" : "We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "C&C (Clark and Curran, 2007) 85.",
      "startOffset" : 4,
      "endOffset" : 28
    }, {
      "referenceID" : 29,
      "context" : "7 w/ LSTMs (Vaswani et al., 2016) 88.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "EasySRL (Lewis et al., 2016) 87.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "EasySRL (Lewis et al., 2016) 88.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "neuralccg (Lee et al., 2016) 88.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 29,
      "context" : "ment, our parser shows the better result than all the baseline parsers except C&C with an LSTM supertagger (Vaswani et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "0% unlabeled F1, outperforming the current state-of-theart neuralccg (Lee et al., 2016) that uses recursive neural networks by 0.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "There are also many implementation differences in our parser (C++A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ TensorFlow (Abadi et al.",
      "startOffset" : 126,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "Noji and Miyao (2016) 93.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 21,
      "context" : "For Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), there are studies to use the predicted dependency structure to improve HPSG parsing accuracy.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "For Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), there are studies to use the predicted dependency structure to improve HPSG parsing accuracy. Sagae et al. (2007) use dependencies to constrain the form of the output tree.",
      "startOffset" : 48,
      "endOffset" : 187
    }, {
      "referenceID" : 32,
      "context" : "Zhang et al. (2010) use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014).",
      "startOffset" : 108,
      "endOffset" : 149
    }, {
      "referenceID" : 31,
      "context" : "In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014).",
      "startOffset" : 108,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric. Instead of modeling dependencies alone, our method finds a CCG derivation that has a higher dependency score. Lewis et al. (2015) present a joint model of CCG parsing and semantic role labeling (SRL), which is closely related to our approach.",
      "startOffset" : 109,
      "endOffset" : 365
    } ],
    "year" : 2017,
    "abstractText" : "We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the stateof-the-art results on English and Japanese CCG parsing.",
    "creator" : "LaTeX with hyperref package"
  }
}