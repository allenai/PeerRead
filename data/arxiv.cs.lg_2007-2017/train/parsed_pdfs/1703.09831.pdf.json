{
  "name" : "1703.09831.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment",
    "authors" : [ "Haonan Yu", "Haichao Zhang" ],
    "emails" : [ "haonanyu@baidu.com", "zhanghaichao@baidu.com", "xuwei06@baidu.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nThe development of a sophisticated language system is a very crucial part of achieving humanlevel intelligence for a machine. Language semantics, when grounded in perception experience, can encode knowledge about perceiving the world. This knowledge is transferred from task to task, which empowers the machine with generalization ability. It is argued that a machine has to go through physical experience in order to learn human-level semantics [Kiela et al., 2016], i.e., a process of human-like language acquisition. However, current machine learning techniques do not have a reasonably fast learning rate to make this happen. Thus we choose to model this problem in a virtual environment, as the first step towards training a physical intelligent machine.\nHuman generalize surprisingly well when learning new concepts and skills through natural language instructions. We are able to apply an existing skill to newly acquired concepts with\nar X\niv :1\n70 3.\n09 83\n1v 1\n[ cs\n.C L\n] 2\n8 M\nar 2\nlittle difficulty. For example, a person who has learned how to execute the command “cut X with knife” when X equals to apple, will do correctly when X is something else he knows, e.g., pear or orange, even though he may have never been asked to cut anything other than apple before.\nThis paper describes a framework that demonstrates the zero-shot learning ability of an agent in a specific task, namely, learning to navigate in a 2D maze-like environment called XWORLD (Figure 1). We are interested in solving a similar task that is faced by a baby who is learning to walk and navigate, at the stage of learning his parents’ language. The parents might give some simple navigation command consisting of only two or three words in the beginning, and gradually increase the complexity of the command as time goes by. Meanwhile, the parents might teach the baby the language in some other task such as object recognition. After the baby understands the language and masters the navigation skill, he could immediately navigate to a new concept that is learned from object recognition but never appeared in the navigation command before.\nWe train our baby agent across many learning sessions in XWORLD. In each session, the agent perceives the environment through a sequence of raw-pixel images, a natural language command issued by a teacher, and a set of rewards. The agent also occasionally receives the teacher’s questions on object recognition whenever certain conditions are triggered. By exploring the environment, the agent learns simultaneously the visual representations of the environment, the syntax and semantics of the language, and how to navigate itself in the environment. The whole framework employed by the agent is trained end to end from scratch by gradient descent. We test the agent under three different command conditions, two of which require that the agent generalizes to interpret unseen commands and words, and that the framework architecture is modular so that other modules such as visual perception and action will still work properly under such circumstance. Our experiments show that the agent performs equally well (∼ 90% average success rate) in all conditions. Moreover, several baselines that simply learn a joint embedding for image and language yield poor results.\nIn summary, our main contributions are:\n◦ A new navigation task that integrates both vision and language for deep reinforcement learning (RL). Moreover, the language is not pre-parsed [Sukhbaatar et al., 2016] or -linked [Mikolov et al., 2015, Sukhbaatar et al., 2016] to the environment. Instead, the agent has to learn everything from scratch and ground the language in vision. ◦ Multi-task transfer learning of language for speeding up RL. Language acquisition in an auxiliary task helps the agent understand the navigation command much faster, and thus master the navigation skill much faster. ◦ The zero-shot learning ability by leveraging the compositionality of both the language and the model architecture. We believe that this ability is a crucial component of human-level intelligence."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work is inspired by the research of multiple disciplines. Our XWORLD is similar to the MazeBase environment [Sukhbaatar et al., 2016] in that both are 2D rectangular grid world. One big difference is that their quasi-natural language is already parsed and linked to the environment. They put more focus on reasoning and planning but not language acquisition. On the contrary, we emphasize on how to ground the language in vision and generalize the ability of interpreting the language. There are several challenging 3D environments for RL such as Kempka et al. [2016] and Jaderberg et al. [2017]. The visual perception problems posed by them are much more difficult than ours. However, these environments do not require language understanding. Our agent needs to learn to interpret different goals from different natural language commands.\nOur setting of language learning shares some similar ideas of the AI roadmap proposed by Mikolov et al. [2015]. Like theirs, we also have a teacher in the environment that assigns tasks and rewards to the agent. The teacher provides additional help for language learning by asking questions to the agent in an object recognition task. Unlike their proposal of entirely using linguistic channels, our tasks involve multiple modalities and are more close to human experience.\nThe importance of compositionality and modularity of a learning framework has been discussed at length in cognitive science by Lake et al. [2016]. The compositionality of our framework is inspired by the ideas in Neural Programmer [Neelakantan et al., 2016] and Neural Module Networks [Andreas et al., 2016a,b]. Neural Programmer is trained with backpropagation by employing soft operations\non databases. Neural Module Networks assemble several primitive modules according to questions in Visual Question Answering (VQA). It depends on an external parser to convert each sentence to one or several candidate parse trees and thus cannot be trained end to end. We adapt their primitive modules to our framework with differentiable computation units to enable gradient calculation.\nThe auxiliary recognition task is essentially image VQA [Gao et al., 2015, Ren et al., 2015, Lu et al., 2016, Andreas et al., 2016a,b, Teney and Hengel, 2016, Yang et al., 2016]. The navigation task can also be viewed as a VQA problem if the actions are treated as answer labels. Moreover, it is a zero-shot VQA problem (i.e., test questions containing unseen concepts) which has not been well addressed yet.\nOur language acquisition problem is closely related to some recent work on grounding language in images and videos [Yu and Siskind, 2013, Rohrbach et al., 2016, Gao et al., 2016]. The navigation task is also relevant to robotics navigation under natural language command [Chen and Mooney, 2011, Barrett et al., 2015]. However, they either assume annotated navigation paths in the training data or do not ground language in vision. As XWORLD is a virtual environment, we currently do not address mechanics problems encountered by a physical robot, but focus on its mental model building."
    }, {
      "heading" : "3 XWORLD Environment",
      "text" : "We first briefly describe the XWORLD environment. More details are in Appendix 8.3. XWORLD is a 2D grid world (Figure 1). An agent interacts with the environment over a number of time steps T , with four actions: up, down, left, and right. It does so for many sessions. At the beginning of each session, a teacher starts a timer and issues a natural language command asking the agent to reach a location referred to by objects in the environment. There might be other objects as distractors. Thus the agent needs to differentiate and navigate to the right location. It perceives the entire environment through RGB pixels with an egocentric view (Figure 2c). If the agent correctly executes the command before running out of time, it gets a positive reward R+. Whenever it hits a wall or steps on an object that is not the target, it gets a negative reward R−w or R − o , respectively. The agent also receives a small negative reward R−t at every step as a punishment for wandering around. After each session, both the environment and the agent are reset randomly.\nSome example commands are (the parentheses contain environment configurations that are withheld from the agent, same below):\n◦ Please navigate to the apple. (There is an apple, a banana, an orange, and a grape.) ◦ Can you move to the grid between the apple and the banana? (There is an apple and a banana.\nThe apple and the banana are separated by one empty grid.) ◦ Could you please go to the red apple? (There is a green apple, a red apple, and a red cherry.) The difficulty of this navigation task is that, at the very beginning the agent knows nothing about the language: every word appears equally meaningless. After trials and errors, it has to figure out the language syntax and semantics in order to correctly execute the command.\nWe add an auxiliary recognition task to help the agent learn the language. While it is exploring the environment, the teacher asks object-related questions whenever certain conditions are triggered (all conditions are listed in Appendix 8.3). The answers are always single words and provided by the teacher for supervision. Some example QA pairs are:\n◦ Q:What is the object in the north? A:Banana. (The agent is by the south of a banana, by the north of an apple, and by the west of a cucumber.) ◦ Q:Where is the banana? A:North. (The agent is by the south of a banana and the east of an apple.) ◦ Q:What is the color of the object in the west of the apple? A:Yellow. (An apple has a banana on its\nwest and a cucumber on its east.)\nWe expect the agent to learn the language much faster given this auxiliary task."
    }, {
      "heading" : "4 Compositional Framework for Zero-shot Navigation",
      "text" : "Our framework contains four major modules: a language module, a recognition module, a visual perception module, and an action module. The design of the framework is mostly driven by the need of navigating to new objects (Figure 1b) that never appear in the commands (only appearing as\nanswers in the recognition module but never in the language module Figure 2a). We see three crucial properties for the framework:\n◦ The language module must be compositional. It needs to process a sentence while preserving the (major) sentence structure. One example would be a parser that outputs a parse tree. ◦ Inductive bias [Lake et al., 2016] must be learned from existing sentences. The language module\nknows how to parse a sentence with a known structure if a word position is filled with a completely new word. ◦ Language grounding (Figure 2a) and recognition (Figure 2b) have to be reduced to (approximately) the same problem. This ensures that language grounding trained on n − 1 words still works properly on the nth word which is trained from the recognition.\nThere is no existing framework for image captioning or VQA exhibits all such properties. Specifically, it is difficult, if not impossible, for a simple (gated) Recurrent Neural Network (RNN) to satisfy the first two properties when a sentence has a rich structure. We show that word attention [Bahdanau et al., 2015], visual attention, and external memory [Graves et al., 2014] are the keys.\nRecognition We assume a feature map F ∈ RD×N with D channels and a spatial dimension of N . Consider classifying the feature fn ∈ RD at location n on the map, into m out of M words with probability P (m|fn) = SoftmaxM (sᵀmfn), where sᵀm is the mth row of the Softmax matrix S ∈ RM×D. Also consider grounding (i.e., computing attention) the mth word em ∈ RD in F at location n by A(n|em) = SoftmaxN (fᵀnem) which produces a sum-to-one attention map. Our observation is the similarity between P (m|fn) and A(n|em): both reply on dot product computation (sᵀf or fᵀe). Inspired by the transposed weight sharing scheme [Mao et al., 2015], we set S = Eᵀ where E is the word embedding table. As a result, sᵀf and fᵀe now compute the same quantity, namely, the similarity between word embedding and feature. Therefore, both max\nE,F P (m|fn) and\nmax E,F\nA(n|em) are optimized towards\nem ∝ {\nfn C(fn) = m −fn C(fn) 6= m ,\nwhere C denotes the correct label of the feature. Our recognition module assumes an attention map a ∈ RN from the language module and a feature map F ∈ RD×N from the visual perception module. The attention map highlights the interesting region according to the teacher’s question. We extract a feature by weighted averaging the feature map with the attention map: f̃ = Fa. When this feature is classified, the Softmax could potentially output any word such as object name, color, or location. Thus the feature map F has to contain both visual and spatial information. Suppose the teacher asks two questions that result in the same attention map: “Where is the apple?” and “What is the object on the west?”. In both cases, the attention map highlights the same region on the agent’s west. As a result, the extracted feature f̃ will be the same for both questions, although the answers should be different (one is west and the other\nis apple). To resolve this conflict, the question intention has to be understood. The agent needs to understand that the first question asks the location while the second question asks the object name. We use a simple gated RNN [Cho et al., 2014] to encode and summarize a question to generate an embedding mask xq ∈ [0, 1]D. Then the mask and the feature is element-wise multiplied to yield a masked feature 1 f∗ = f̃ ◦ xq which is used for the final classification (Figure 2b). The details of generating the embedding mask are shown in Figure 5a Appendix 8.5.\nVisual Perception The visual perception module receives an environment image and outputs a feature map F ∈ RD×N and an environment map g ∈ RN (Figure 2c). We feed the environment image into a Convolutional Neural Network (CNN) whose output Fv ∈ RD\n′×N (D′ < D) has the same spatial dimension with the number of grids N in the original image, where each pixel of Fv corresponds to a grid. One can instead segment object boundaries and average features inside each object. We leave such to future work. We further convolve Fv with a 1 × 1 filter to get the environment map g. This map is input to the action module together with the navigation attention map a. We expect the environment map to encode the locations of all objects and walls so that the agent knows which locations to avoid. The feature map Fv contains only visual information. We stack it onto another feature map Fs ∈ R(D−D\n′)×N that represents spatial information in the egocentric view (F = [Fv,Fs]). This spatial feature map is a parameter matrix that needs to be learned by the agent.\nLanguage A sentence is compositional by nature. Understanding its structure is crucial for a correct interpretation. For example, the question “What is in the west of the apple?” implies operations Describe(F,Transform[west](Find[apple](F))). If the word west is replaced with east, the interpretation result would be much different. It is difficult for a simple (gated) RNN to output a compact embedding that encodes all such structural information, as the embedding is largely determined by the majority of the sentence. Thus our language module should also be compositional, namely, it knows how to (implicitly) assemble itself differently according to different input sentences.\nCommands and questions are processed by the same language module (Figure 2a). The result is an attention map a ∈ RN that highlights interesting regions. For navigation, the regions indicate the target locations. For recognition, they indicate which features on the feature map to be recognized. We treat each sentence as a program command. The core of our language module is a differentiable programmer that executes a command in several steps. At each step the programmer has access to the feature map F and the intermediate result in the previous step. By attending to different words, the programmer is able to assemble a latent network across steps. At the final step the programmer outputs an attention map a (Figure 2 Right). Below we describe the details of the programmer.\nA sentence of length L is first converted to a sequence of word embeddings el by looking up the embedding table E ∈ RD×M . The embeddings are then projected2 to syntax embeddings and functionality embeddings efl (Figure 4 Appendix 8.5). The syntax embeddings are fed into a Bidirectional RNN [Schuster and Paliwal, 1997] to obtain sentence context vectors ecl . The last forward state and the first backward state are concatenated and projected to a booting vector (Figure 6 Appendix 8.5). The programmer controller, designed as a gated RNN [Cho et al., 2014], is initialized with this booting vector. Given a fixed number of programming steps S, at each step s the controller computes the attention for each word l from its sentence context vector ecl :\ncs,l = SoftmaxL (CosSim (hs, g(We c l + b))) , CosSim(z, z′) = z ᵀz′\n‖z‖‖z′‖ ,\nwhere W and b are projection parameters, hs is the RNN state, and g is the activation function. Then the word embeddings, the context vectors, and the functionality embeddings are all weighted averaged by the attention: ẽs = ∑ l cs,l · el, ẽcs = ∑ l cs,l · ecl , ẽfs = ∑ l cs,l · e f l . The averaged context vector ẽcs is fed back to the controller to tell it how to update its hidden state.\nThe averaged word embedding ẽs represents what to be grounded at step s. Similar to recognition, we generate an embedding mask xws ∈ [0, 1]D as a projection of the averaged functionality embedding ẽfs (Figure 5b Appendix 8.5), for computing the masked embedding e ∗ s = ẽs ◦ xws . The masked embedding is convolved as a 1 × 1 filter with the feature map to obtain an egocentric attention 1The embedding mask should be really applied to the embedding instead of the feature. However, because the classification relies on computing dot products of the two, we have fᵀ(e ◦ x) = (f ◦ x)ᵀe. 2We use “projection” to denote the process of going through one or more fully-connected (FC) layers.\nmap a′′s = Softmax(F ∗ e∗s). Assume that in the previous step we have cached an attention map a′s−1 which is essentially a one-slot external memory. The programmer updates it by convolution as = a ′′ s ∗a′s−1. This convolution is used to approximate the 2D translation of spatial attention, given that the attention map of a location word is egocentric and needs to be imposed on object attention. Then the programmer caches a new attention map by a convex combination of the previously cached one and the current output: a′s = (1−σ)a′s−1 +σas, where the Sigmoid gate σ is a scalar projection of the current controller state hs (Figure 2 Right). At the beginning of programming, we set a′0 = i where i is a map of which only the center pixel is one and the rest are zeros. The final output attention map a is set to aS .\nCompared to Neural Module Networks [Andreas et al., 2016a], our soft word attention implicitly supports the CombineAnd and CombineOr operations by attending to multiple words simultaneously. The embedding mask supports the DescribeColor, DescribeLocation, and DescribeName operations by masking different entries of embeddings according to different questions. As a result, we end up with a simple yet effective implementation of the network modularity.\nAction The action module (Figure 2) assumes an attention map a from the language module and an environment map g from the visual perception module. These two maps contain all the information needed by the agent to move itself in the environment. We stack the two maps and input them to a two-layer CNN whose output is projected to a state vector q that summarizes the environment. The state vector is further projected to a distribution π(q, y) over the four actions. At each time step, the agent takes action y with a probability of α · 0.25 + (1− α) · π(q, y), where α is the rate of random exploration. The state vector is also projected to a scalar V (q) as the approximate value function."
    }, {
      "heading" : "5 Training",
      "text" : "Our training objective contains two sub-objectives, one for navigation and the other for recognition\nL(θ) = LRL(θ) + LSL(θ), where θ are the joint parameters of the framework. Most parameters are shared between the two tasks 3. We compute the recognition loss LSL as the multi-class cross entropy with the gradients\n∇θLSL(θ) = EQ [−∇θ logPθ(m|f∗θ )] , where EQ is the expectation over all the questions asked by the teacher in all training sessions, m is the correct answer to each question, and f∗θ is the corresponding feature. We compute the navigation loss LRL(θ) as the negative expected reward −Eπθ [r] the agent receives by following its policy πθ. With the Actor-Critic (AC) algorithm [Sutton and Barto, 1998], we have the approximate gradients\n∇θLRL(θ) = −Eπθ [(∇θ log πθ(qθ, y) +∇θVθ(qθ)) (r + γVθ−(qθ−)− Vθ(qθ))] where θ− are the target parameters that are periodically (every J minibatches) copied from θ, r is the immediate reward, γ is the discount factor, qθ− is the next state after taking action y at state qθ, and πθ and Vθ are the policy and value output by the action module. Since the expectations EQ and Eπθ are different, we optimize the two sub-objectives separately over the same number of minibatches. For effective training, we employ Curriculum Learning [Bengio et al., 2009] and Experience Replay [Mnih et al., 2015] with Prioritized Sampling [Schaul et al., 2016] (Appendix 8.4)."
    }, {
      "heading" : "6 Experiments",
      "text" : "We use Adagrad [Duchi et al., 2011] with a learning rate of 10−5 for Stochastic Gradient Descent (SGD). In all experiments, we set the batch size to 16 and train 200k batches. The target parameters θ− are updated every J = 2k batches. All the parameters have a default weight decay equal to 10−4× batch size. For each layer, by default its parameters have zero mean and a standard deviation of 1/ √ N , where N is the number of parameters of that layer. The agent has 500k exploration steps in total, and the exploration rate α decreases linearly from 1 to 0. We fix the number of programming steps S as 3. We train each model with 4 random initializations. The whole framework is implemented with PaddlePaddle 4 and trained end to end. More implementation details are described in Appendix 8.1.\n3In all the figures of our framework, components with the same name share the same set of parameters. 4https://github.com/PaddlePaddle/Paddle\nZero-shot Navigation Our primary question is whether the agent has the zero-shot navigation ability of executing previously unseen commands. We setup four command conditions for training the agent.\n◦ Standard The training command set has the same distribution with the test command set. ◦ NC Some word combinations are excluded from the training command set, even though all the\nwords are in it. We specifically consider three types of word combinations: (object, location), (object, color), and (object, object). We enumerate all combinations for each type and randomly hold out 10% from the teacher in navigation. ◦ NWNav&NWNavRec Some object words are excluded from navigation training, and are trained only in recognition and tested in navigation as new concepts. NWNavRec guarantees that the new words will not appear in questions but only in answers while NWNav does not. We randomly hold out 10% of the object words.\nOur framework is trained under each condition with the same hyperparameters. For testing, we put the held-out combinations/words back to the commands (i.e., Standard condition) and test 10k sessions in total over four navigation subtasks nav_obj, nav_col_obj, nav_nr_obj, and nav_bw_obj (Appendix 8.3). We compute the success rates where success means that the agent reaches the target location in time in a session. Figure 3a shows the training reward curves and Table 1a contains the success rates. The curves are close to each other, which is expected because a 10% reduction of the commands barely changes the learning difficulty. We get almost the same success rates for all the conditions, and obtain high zero-shot success rates. The results of NWNavRec show that although some new object concepts are learned from a completely different problem, they can be tested on navigation without any model retraining or finetuning. Baselines To demonstrate that our framework architecture is necessary, we also test four baselines:\n◦ SimpleAttention We modify our framework by replacing the programmer with a simple attention model. Given a sentence, we use an RNN to output an embedding which is convolved as a 3× 3 filter with the visual feature map to get an attention map. The rest of the framework is unchanged (Figure 7 Appendix 8.5). This baseline is an ablation to show the necessity of the programmer. ◦ NoTransShare We do not tie the word embedding table E to the transposed Softmax matrix Sᵀ. In\nthis case, language grounding and recognition classification are not guaranteed to be the same. This baseline is an ablation to show the impact of the transposed sharing on the training convergence.\n◦ VIS-LSTM Following Ren et al. [2015], we use CNN to get an image embedding which is then projected to the word embedding space and used as the first word of the sentence. The sentence goes through an RNN whose last state is used for navigation and recognition (Figure 8 Appendix 8.5). ◦ Multimodal We implement a multimodal framework [Mao et al., 2015]. The framework uses CNN to get an image embedding and RNN to get a sentence embedding. Then the two embeddings are projected to a common feature space for navigation and recognition (Figure 9 Appendix 8.5).\nThe last three baselines are trained only under the Standard command condition, where their performance is already very poor. SimpleAttention is trained under all the four command conditions. Again we test these baselines for 10k sessions. The training reward curves and the success rates are shown in Figure 3b and Table 1, respectively. VIS-LSTM and Multimodal have poor results because they do not ground language in vision. Surprisingly, NoTransShare converges much slower than our framework does. One possible reason is that the correct behavior of the language module is hard to be found by SGD if no constraint on word embedding is imposed. Although SimpleAttention is able to perform well, without word-attention programming, its ability of sentence understanding is limited. More importantly, Table 1a shows that it almost fails on zero-shot commands (especially on NWNavRec). Interestingly, in nav_bw_obj it has a much higher zero-shot success rate than in the other subtasks. The reason is that with the 3× 3 filter, it always highlights the location between two objects without detecting their classes, because usually there is only one qualified pair of objects in the environment for nav_bw_obj. In such case, SimpleAttention does not really generalize to unseen commands.\nVisualization and Analysis Our framework produces intermediate results that can be easily visualized and analyzed. One example has been shown in Figure 2, where the environment map is produced by a trained visual perception module. It detects exactly all the obstacles and goals. The map constitutes the learned prior knowledge for navigation: all walls and goals should be avoided by default because they incur negative rewards. This prior, combined with the attention map produced for a command, contains all information the agent needs to navigate. The attention maps are usually very precise (Figure 10 Appendix 8.5), with some rare cases in which there are flaws, e.g., when the agent needs to navigate between two objects. This is due to our simplified assumption on 2D geometric translation: the attention map of a location word is treated as a filter and the translation is modeled as convolution. This results in attention diffusion in the above case. To address this issue, more complicated transformation can be used (e.g., FC layers).\nWe also visualize the programming process. We observe that the programmer is able to shift its focus across steps (Figure 11 Appendix 8.5). In the first example, the programmer essentially does Transform[southeast](Find[cabbage](F)). In the second example, it essentially performs Transform[between](CombineOr(Find[apple](F), Find[coconut](F))).\nWe find the attention and environment maps very reliable through visualization. This is verified by the ∼100% QA accuracy in recognition. However, in Table 1 the best success rate is still ∼ 5% away from the perfect. Further analysis reveals that the agent tends to stuck in loop if the target location is behind a long wall, although it has a certain chance to bypass it (Figure 12 Appendix 8.5). Thus we believe that the discrepancy between the good map quality and the imperfect performance results from our action module. Currently the action module learns a direct mapping from an environment state to an action. There is no support for either history remembering or route planning [Tamar et al., 2016]. Since our focus here is zero-shot navigation, we leave such improvements to future work."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have demonstrated an end-to-end compositional framework for a virtual agent to generalize an existing skill to new concepts without model retraining or finetuing. Such generalization is made possible by reusing knowledge that is learned in other tasks and encoded by language. By assembling words in different ways, the agent is able to tackle new tasks while exploiting existing knowledge. Such ability is crucial for fast learning and good generalization. We reflect these important ideas in the design of our framework and apply it to a concrete example: zero-shot navigation in XWORLD. Our framework is just one possible implementation. Some components of the framework can still be improved. Our claim is not that an intelligent agent must have a mental model as the presented one, but it has to possess several crucial properties discussed in Section 1 and Section 4. Currently the agent explores in a 2D environment. In the future, we plan to migrate the agent to a 3D world like Malmo [Johnson et al., 2016]. There will be several new challenges, e.g., visual perception and geometric transformation will be more difficult to model. We hope that the current framework provides some preliminary insights on how to train a similar agent in a 3D environment."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Yuanpeng Li, Liang Zhao, and Yi Yang for their helpful discussions."
    }, {
      "heading" : "8 Appendix",
      "text" : ""
    }, {
      "heading" : "8.1 Implementation Details",
      "text" : "The agent at each time step receives a 156× 156 RGB image. This image is egocentric and includes both the environment and the black padding region. The agent processes the input image with a CNN that has four convolutional layers: (3, 3, 64), (2, 2, 64), (2, 2, 512), (1, 1, 512), where (x, y, z) represents z x× x filters with stride y. All the four layers have the ReLU activation function. The output is the visual feature map with 512 channels. We stack it along the channel dimension with another parametric spatial feature map of the same sizes. This spatial feature map is initialized with zero mean and standard deviation (Figure 2).\nThe agent also receives a navigation command at the beginning of a session. The same command is repeated until the end of the session. The agent may or may not receive a question at every time step. The dimensions of the word embedding, syntax embedding, and functionality embedding are 1024, 128, and 128, respectively. The word embedding table is initialized with zero mean and a standard deviation of 1. The hidden FC layers for computing the syntax and functionality embeddings have 512 units (Figure 4). The bidirectional RNN for computing sentence contexts has a state size of 128 in both directions. The output controller booting vector and sentence context also have a length of 128. The state size of the controller RNN is equal to the length of the booting vector (Figure 2 Right). The hidden FC layer for converting a functionality embedding to an embedding mask has a size of 128. The RNN used for summarizing the question intention has 128 states (Figure 5). All FC layers and RNN states in the language and recognition module use Tanh as the activation function. The only exception is the FC layer that outputs the embedding mask (Sigmoid).\nIn the action module, the CNN for processing the attention map and the environment map has two convolutional layers (3, 1, 64) and (3, 1, 4), both with paddings of 1. They are followed by three FC layers that all have 512 units. All five layers use the ReLU activation function."
    }, {
      "heading" : "8.2 Baseline Models",
      "text" : "The language module of SimpleAttention sets the word embedding size to 1024. The RNN has the same size with the word embedding. The FC layer that produces the 3 × 3 filter has an output size of 4608 which is 9 times the channel number of the visual feature map. The rest of the layer configuration is the same with our framework. VIS-LSTM has a CNN with four convolutional layers (3, 2, 64), (3, 2, 64), (3, 2, 128), and (3, 1, 128). This is followed by three FC layers with size 1024. The word embedding and the RNN both have sizes of 1024. The RNN output goes through three FC hidden layers of size 512 either for recognition or navigation. The layer size configuration of Multimodal is the same with VIS-LSTM. The outputs of all layers here are ReLU activated except for the last FC layer of the CNN used by VIS-LSTM. The activation is instead linear so that the output image embedding is in the same space with the word embedding. Except for SimpleAttention, we do not tie the transposed Softmax matrix to the embedding table."
    }, {
      "heading" : "8.3 XWORLD Setup",
      "text" : "We configure square environments with sizes ranging from 3 to 7. We fix the size of the environment image by padding walls for smaller environments. Different sessions may have different map sizes. In each session,\n◦ The number of time steps T is four times the map size. ◦ The number of objects on the map is from 1 to 3. ◦ The number of wall grids on the map is from 0 to 10. ◦ The positive reward R+ when the agent reaches the correct location is set to 1. The negative\nrewards R−w for hitting walls and R − o for stepping on non-target objects are set to −0.2 and −1,\nrespectively. The time step penalty R−t is set to −0.1. The teacher has a vocabulary size of 104, including 2 punctuation marks. There are 9 locations, 4 colors, and 40 distinct object classes. Each object class has 2.85 object instances on average. Every time the environment is reset, a number of object classes are randomly sampled and an object instance is randomly sampled for each class. There are in total 16 types of sentences the teacher can speak, including 4 types of navigation commands and 12 types of recognition questions. Each sentence type\nhas multiple non-recursive natural-language templates, and corresponds to a subtask the agent must learn to perform. In total there are 256,832 distinct sentences with 92,442 for the navigation task and 164,390 for the recognition task. The sentence length ranges from 2 to 12.\nThe object, location, and color words of the teacher’s language are listed below. These are the content words with actual meanings that can be grounded in the environment. All the other words are treated as grammatical words whose embeddings are only for interpreting sentence structures. The differentiation between content and grammatical words is automatically learned by the agent based on the teacher’s language and the environment. All words have the same form of representation.\nObject Location Color Other\napple, avocado, banana, blueberry, butterfly, east, west, green, ?, ., and, block, by, can, color, could, cabbage, cat, cherry, circle, coconut, north, south, red, destination, direction, does, find, go, cucumber, deer, dog, elephant, fig, northeast, northwest, blue, goal, grid, have, identify, in, is, locate, fish, frog, grape, hedgehog, ladybug, southeast, southwest, yellow located, location, me, move, name, lemon, lion, monkey, octopus, orange, between navigate, near, nothing, object, of, on, owl, panda, penguin, pineapple, pumpkin, one, OOV, please, property, reach, say, rabbit, snake, square, squirrel, star, side, target, tell, the, thing, three, to, strawberry, triangle, turkey, turtle, watermelon two, what, where, which, will, you, your\nThe sentence types that the teacher can speak are listed below. Each sentence type corresponds to a subtask. The triggering condition describes when the teacher says that type of sentences. Besides the conditions shown, an extra condition for navigation commands is that the target location must be reachable from the current agent location. An extra condition for color-related questions is that the object color must be one of the four defined colors, and objects with other colors will be ignored in these questions. If at a time step there are multiple conditions triggered, we randomly sample one sentence type for navigation and another for recognition. After the sentence type is sampled, we generate the command or question according to the corresponding sentence templates.\nSentence Type Example Triggering Condition (Subtask)\nnav_obj Please go to the apple. [C0] Beginning of a session. & [C1] The referred object has a unique name. nav_col_obj Could you please move to the red apple? [C0] & [C2] There are multiple objects that either have the same name but different colors, or have different names but the same color. nav_nr_obj The north of the apple is your destination. [C0] & [C1] nav_bw_obj Navigate to the grid between apple and [C0] & [C3] Both referred objects have\nbanana please. unique names and are separated by one grid.\nrec_col2obj What is the red object? [C4] There is only one object that has the referred color. rec_obj2col What is the color of the apple? [C1] rec_loc2obj Please tell the name of the object in the south. [C5] The agent is near the referred object. rec_obj2loc What is the location of the apple? [C1] & [C5] rec_loc2col What color does the object in the east have? [C5] rec_col2loc Where is the red object located? [C4] & [C5] rec_loc_obj2obj Identify the object which is in the east of the apple. [C1] & [C6] The referred object is near another object rec_loc_obj2col What is the color of the east to the apple? [C1] & [C6] rec_col_obj2loc Where is the red apple? [C2] & [C5] rec_bw_obj2obj What is the object between apple and banana? [C7] Both referred objects have\nunique names and are separated by one object.\nrec_bw_obj2loc Where is the object between apple and banana? [C7] & [C8] The agent is near the object in the middle. rec_bw_obj2col What is the color of the object between apple [C7] and banana?"
    }, {
      "heading" : "8.4 Experience Replay and Curriculum Learning",
      "text" : "We employ Experience Replay [Mnih et al., 2015] for training both the navigation and recognition tasks. The environment inputs, rewards, and the actions taken by the agent at the most recent 10k time steps are stored in a replay buffer. During training, every time two minibatches of the same number of experiences are sampled from the buffer, one for computing ∇θLSL(θ) and the other for computing ∇θLRL(θ). For the former, only individual experiences are sampled. We uniformly sample experiences from a subset of the buffer which contains the teacher’s questions. For the latter, we need to sample transitions (i.e., pairs of experiences) so that TD error can be computed. We sample from the entire buffer using the Rank-based Sampler [Schaul et al., 2016] which has proven to increase the learning efficiency by prioritizing rare experiences in the buffer.\nBecause in the beginning the language is quite ambiguous, it is difficult for the agent to start learning with a complex environment setup. Thus we exploit Curriculum Learning [Bengio et al., 2009] to gradually increase the environment complexity. We gradually change the following things linearly in proportional to min(1, G′ / G), where G′ is the number of sessions so far and G is the number of curriculum sessions:\n◦ The number of grids of the environment. ◦ The number of objects in the environment. ◦ The number of wall grids. ◦ The number of possible object classes that can appear in the environment. ◦ The length of a navigation command or a recognition question. We find that this curriculum is crucial for efficient learning, because in the early phase the agent is able to quickly master the meanings of the location and color words given only small ambiguity. After this, these words are used to guide the optimization when more and more new sentence structures and objects are added. In the experiments, we set G = 10k during training while do not use any curriculum during test (maximal difficulty).\nWatermelon is the Reach the grid between Red ladybug is the Can you go to the destination. grape and deer. target. south of the elephant?"
    } ],
    "references" : [ {
      "title" : "Neural module networks",
      "author" : [ "J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Andreas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to compose neural networks for question answering",
      "author" : [ "J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Andreas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Robot language learning, generation, and comprehension",
      "author" : [ "D.P. Barrett", "S.A. Bronikowski", "H. Yu", "J.M. Siskind" ],
      "venue" : "arXiv preprint arXiv:1508.06161,",
      "citeRegEx" : "Barrett et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Barrett et al\\.",
      "year" : 2015
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston" ],
      "venue" : "In Proceedings of the 26th annual international conference on machine learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning to interpret natural language navigation instructions from observations",
      "author" : [ "D.L. Chen", "R.J. Mooney" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Chen and Mooney.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chen and Mooney.",
      "year" : 2011
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. van Merrienboer", "Ç. Gülçehre", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Are you talking to a machine? dataset and methods for multilingual image question",
      "author" : [ "H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Gao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2015
    }, {
      "title" : "Physical causality of action verbs in grounded language understanding",
      "author" : [ "Q. Gao", "M. Doering", "S. Yang", "J.Y. Chai" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Gao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural turing machines",
      "author" : [ "A. Graves", "G. Wayne", "I. Danihelka" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning with unsupervised auxiliary tasks",
      "author" : [ "M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2017
    }, {
      "title" : "The malmo platform for artificial intelligence experimentation",
      "author" : [ "M. Johnson", "K. Hofmann", "T. Hutton", "D. Bignell" ],
      "venue" : "In Proc. 25th International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "ViZDoom: A Doom-based AI research platform for visual reinforcement learning",
      "author" : [ "M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Jaśkowski" ],
      "venue" : "In IEEE Conference on Computational Intelligence and Games,",
      "citeRegEx" : "Kempka et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kempka et al\\.",
      "year" : 2016
    }, {
      "title" : "Virtual embodiment: A scalable long-term strategy for artificial intelligence research",
      "author" : [ "D. Kiela", "L. Bulat", "A.L. Vero", "S. Clark" ],
      "venue" : "In NIPS Workshop,",
      "citeRegEx" : "Kiela et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2016
    }, {
      "title" : "Building machines that learn and think like people",
      "author" : [ "B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman" ],
      "venue" : "Behavioral and Brain Sciences, pages 1–101,",
      "citeRegEx" : "Lake et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2016
    }, {
      "title" : "Hierarchical question-image co-attention for visual question answering",
      "author" : [ "J. Lu", "J. Yang", "D. Batra", "D. Parikh" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Lu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning like a child: Fast novel visual concept learning from sentence descriptions of images",
      "author" : [ "J. Mao", "X. Wei", "Y. Yang", "J. Wang", "Z. Huang", "A.L. Yuille" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Mao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2015
    }, {
      "title" : "A roadmap towards machine intelligence",
      "author" : [ "T. Mikolov", "A. Joulin", "M. Baroni" ],
      "venue" : "arXiv preprint arXiv:1511.08130,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2015
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural programmer: Inducing latent programs with gradient descent",
      "author" : [ "A. Neelakantan", "Q.V. Le", "I. Sutskever" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploring models and data for image question answering",
      "author" : [ "M. Ren", "R. Kiros", "R. Zemel" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ren et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Grounding of textual phrases in images by reconstruction",
      "author" : [ "A. Rohrbach", "M. Rohrbach", "R. Hu", "T. Darrell", "B. Schiele" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Rohrbach et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rohrbach et al\\.",
      "year" : 2016
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Schaul et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2016
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "M. Schuster", "K.K. Paliwal" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Schuster and Paliwal.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Mazebase: A sandbox for learning from games",
      "author" : [ "S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1511.07401,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Value iteration networks",
      "author" : [ "A. Tamar", "S. Levine", "P. Abbeel", "Y. WU", "G. Thomas" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Tamar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tamar et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-shot visual question answering",
      "author" : [ "D. Teney", "A. v. d. Hengel" ],
      "venue" : "arXiv preprint arXiv:1611.05546,",
      "citeRegEx" : "Teney and Hengel.,? \\Q2016\\E",
      "shortCiteRegEx" : "Teney and Hengel.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "It is argued that a machine has to go through physical experience in order to learn human-level semantics [Kiela et al., 2016], i.",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : "Moreover, the language is not pre-parsed [Sukhbaatar et al., 2016] or -linked [Mikolov et al.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "Our XWORLD is similar to the MazeBase environment [Sukhbaatar et al., 2016] in that both are 2D rectangular grid world.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "There are several challenging 3D environments for RL such as Kempka et al. [2016] and Jaderberg et al.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "[2016] and Jaderberg et al. [2017]. The visual perception problems posed by them are much more difficult than ours.",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "Our setting of language learning shares some similar ideas of the AI roadmap proposed by Mikolov et al. [2015]. Like theirs, we also have a teacher in the environment that assigns tasks and rewards to the agent.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "The compositionality of our framework is inspired by the ideas in Neural Programmer [Neelakantan et al., 2016] and Neural Module Networks [Andreas et al.",
      "startOffset" : 84,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "The importance of compositionality and modularity of a learning framework has been discussed at length in cognitive science by Lake et al. [2016]. The compositionality of our framework is inspired by the ideas in Neural Programmer [Neelakantan et al.",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "◦ Inductive bias [Lake et al., 2016] must be learned from existing sentences.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "We show that word attention [Bahdanau et al., 2015], visual attention, and external memory [Graves et al.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : ", 2015], visual attention, and external memory [Graves et al., 2014] are the keys.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Inspired by the transposed weight sharing scheme [Mao et al., 2015], we set S = ET where E is the word embedding table.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "We use a simple gated RNN [Cho et al., 2014] to encode and summarize a question to generate an embedding mask x ∈ [0, 1].",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "The syntax embeddings are fed into a Bidirectional RNN [Schuster and Paliwal, 1997] to obtain sentence context vectors el .",
      "startOffset" : 55,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "The programmer controller, designed as a gated RNN [Cho et al., 2014], is initialized with this booting vector.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "With the Actor-Critic (AC) algorithm [Sutton and Barto, 1998], we have the approximate gradients",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "For effective training, we employ Curriculum Learning [Bengio et al., 2009] and Experience Replay [Mnih et al.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : ", 2009] and Experience Replay [Mnih et al., 2015] with Prioritized Sampling [Schaul et al.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : ", 2015] with Prioritized Sampling [Schaul et al., 2016] (Appendix 8.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "We use Adagrad [Duchi et al., 2011] with a learning rate of 10−5 for Stochastic Gradient Descent (SGD).",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "◦ Multimodal We implement a multimodal framework [Mao et al., 2015].",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "◦ VIS-LSTM Following Ren et al. [2015], we use CNN to get an image embedding which is then projected to the word embedding space and used as the first word of the sentence.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : "There is no support for either history remembering or route planning [Tamar et al., 2016].",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "In the future, we plan to migrate the agent to a 3D world like Malmo [Johnson et al., 2016].",
      "startOffset" : 69,
      "endOffset" : 91
    } ],
    "year" : 2017,
    "abstractText" : "We tackle a task where an agent learns to navigate in a 2D maze-like environment called XWORLD. In each session, the agent perceives a sequence of raw-pixel frames, a natural language command issued by a teacher, and a set of rewards. The agent learns the teacher’s language from scratch in a grounded and compositional manner, such that after training it is able to correctly execute zero-shot commands: 1) the combination of words in the command never appeared before, and/or 2) the command contains new object concepts that are learned from another task but never learned from navigation. Our deep framework for the agent is trained end to end: it learns simultaneously the visual representations of the environment, the syntax and semantics of the language, and the action module that outputs actions. The zero-shot learning capability of our framework results from its compositionality and modularity with parameter tying. We visualize the intermediate outputs of the framework, demonstrating that the agent truly understands how to solve the problem. We believe that our results provide some preliminary insights on how to train an agent with similar abilities in a 3D environment.",
    "creator" : "LaTeX with hyperref package"
  }
}