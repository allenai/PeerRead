{
  "name" : "1703.07261.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Black-Box Data-efficient Policy Search for Robotics",
    "authors" : [ "Konstantinos Chatzilygeroudis", "Roberto Rama", "Rituraj Kaushik", "Dorian Goepp", "Vassilis Vassiliades", "Jean-Baptiste Mouret" ],
    "emails" : [ "jean-baptiste.mouret@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— Learning and Adaptive Systems, DataEfficient Learning\nI. INTRODUCTION\nReinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4]. Rather than aborting their mission when something goes wrong, they could carry on by discovering new behaviors autonomously. Nevertheless, to be useful in such situations, learning has to happen in a few minutes, typically within a few trials. This scarcity of data makes it difficult to exploit the many recent machine learning techniques (e.g., deep learning) that rely on the availability of very large datasets or fast simulations1 [6]. As a consequence, robot learning has to consider other approaches, with the explicit goal of requiring as little interaction time as possible between the robot and the environment.\nWhen data are scarce, a general principle is to extract as much information as possible from them. In the case of\n*Corresponding author: jean-baptiste.mouret@inria.fr All authors have the following affiliations: - Inria, Villers-lès-Nancy, F-54600, France - CNRS, Loria, UMR 7503, Vandœuvre-lès-Nancy, F-54500, France - Université de Lorraine, Loria, UMR 7503, Vandœuvre-lès-Nancy, F-54500, France This work received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (GA no. 637972, project “ResiBots”) and the European Commission through the project H2020 AnDy (GA no. 731540).\n1As an illustration, the deep Q-learning algorithm needed about 38 days of interaction to learn to play Atari 2600 games [5] (only four possible actions), which would be hardly conceivable to a achieve with a robot.\nrobotics, this means that all the state variables that are available should be collected at every time-step and be used by the learning algorithm. This contrasts with many direct policy search approaches (e.g., policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.\nOne of the best ways to take advantage of this sequential state recording is to learn a dynamical model of the robot [11], and then exploit it either for model-predictive control [12] or to find an optimal policy offline [7]. However, such approaches assume that the model is “good enough” to predict future states for all the possible states. This is often not the case when only a few episodes have been performed, as many states have not been observed yet. Learning with a dynamical model therefore often requires acquiring enough points to learn an accurate model, which, in turn, increases the interaction time.\nThis challenge can be overcome by taking into account the uncertainty of the dynamical model: if the algorithm “knows” that a prediction is unreliable, it can balance the risks of trying something that might fail with the potential benefits. The PILCO (Probabilistic Inference for Learning COntrol) algorithm [13], which is one of the state-of-theart algorithms for data-efficient model-based policy search, follows this strategy by alternating between two steps, (1) learning a dynamical model with Gaussian processes [14], (2) using a gradient-based optimizer to search for a policy that maximizes the expected reward, taking the uncertainty of the model into account. Thanks to this process, PILCO achieves remarkable data-efficiency.\nNevertheless, analytical algorithms like PILCO have two main issues that may not be apparent at first sight. First, they impose several constraints on the reward functions and policies that prevent the use of arbitrary rewards (e.g., PILCO can only be used with distance-based rewards so far) and of non-derivable policies (e.g., parameterized state automata, like in [9]). Second, they require a large computation time to optimize the policy (e.g., typically more than 5 minutes on a modern computer between each episode for the cart-pole benchmark), because they rely on computationally expensive methods to do approximate inference for each step of the policy evaluation [13].\nIn this paper, we introduce a novel policy search algorithm that tackles these two problems while maintaining the dataefficiency of analytical algorithms. Our main insight is that while the analytic approach is efficient on a sequential computer, it cannot take advantage of the multi-core archi-\nar X\niv :1\n70 3.\n07 26\n1v 2\n[ cs\n.R O\n] 2\n2 Ju\nl 2 01\n7\ntectures now present in every computer. By contrast, Monte Carlo approaches and population-based black-box optimizers like CMA-ES [15] (1) do not put any constraint on the reward functions and policies, and (2) are straightforward to parallelize, which can make them competitive with analytical approaches when several cores are available. Our second insight is that it is not necessary to explicitly compute accurate approximations of the expected reward when the optimization is performed with rank-based algorithms designed for noisy functions (e.g., CMA-ES [15]), which saves a lot of computation: only the ranking of potential solutions matters. Thus, it is possible to define a data-efficient, blackbox policy search algorithm that is competitive with gradientbased, analytical approaches.\nWe call our algorithm Black-DROPS, for Black-box Dataefficient RObot Policy Search. It is a model-based policy search algorithm which: • takes into account the uncertainty of the dynamical\nmodel when searching for a policy; • is as data-efficient as state-of-the-art, analytical algo-\nrithms, that is, it requires similar interaction time; • performs a more global search than gradient-based al-\ngorithms, that is, it can escape from some local optima; • is at least as fast as state-of-the-art, analytical methods\nwhen several cores are used, that is, it requires similar or lower computation time; in addition, it is likely to be faster with future computers with more cores; • does not impose any constraint on the reward function (in particular, the reward function can be learned); • does not impose any constraint on the policy representation (any parameterized policy can be used).\nWe demonstrate these features with two families of policies, feed-forward neural networks and Gaussian processes, applied to two classic control benchmarks in simulation, the inverted pendulum and the cart-pole swing-up, as well as a physical 4-DOF robotic arm."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "Direct policy search (PS) methods have been successful in robotics as they can easily be applied in highdimensional continuous state-action RL problems [7]. REINFORCE [16] is an early policy gradient method which performs exploration of the action space using probabilistic policies. It suffers, however, from slow convergence due to the high variance in its gradient estimates. Policy gradients with parameter-based exploration (PGPE) [17] address this problem by transferring exploration to parameter space. In particular, PGPE samples deterministic policies at the start of each episode by maintaining a separate Gaussian distribution for each parameter of the policy, whose mean and variance are adapted during training. The PoWER (Policy learning by Weighting Exploration with the Returns) algorithm [18] uses probability-weighted averaging, which has the property of following the natural gradient without computing it [18]. PoWER, however, assumes that the immediate rewards sum to a constant number and are always positive, which complicates the design of reward functions. The Policy Improve-\nments with Path Integrals (PI2) [19] algorithm does not make such an assumption. When the reward function is compatible with both PoWER and PI2, the algorithms have identical performance [19].\nA limitation of PGPE is that it does not consider any correlations between dimensions in parameter space. This can be addressed by the Natural Evolution Strategies (NES) [20] and Covariance Matrix Adaptation ES (CMA-ES) [15] families of algorithms, which are population-based Black-Box Optimizers (BBO). Both NES and CMA-ES iteratively update a search distribution by calculating an estimated gradient on the distribution parameters (mean and covariance matrix). At each generation, they sample a set of solutions (i.e., policy parameters) and rank them based on their fitness (i.e., expected return). NES performs gradient ascent along the natural gradient, which normalizes the update with respect to uncertainty. CMA-ES updates the distribution by exploiting the technique of evolution paths to average-out random effects over the generations. NES and CMA-ES are closely related, as the latter performs an approximate natural gradient ascent [21]. Interestingly, a variant of PI2 with a simplified parameter perturbation and update method outperforms PI2 and was shown to be a special case of CMA-ES [22]. In general, any BBO can be used for direct PS. Bayesian optimization [23] is a particular family of BBO that can be very data-efficient by building a surrogate model of the objective function (i.e., the expected return) and exploring this model in a clever way (e.g., using upper confidence bounds [24]). It can drastically decrease the evaluation time when optimizing gaits [10] or when finding compensatory behaviors for damaged robots [2].\nThe data-efficiency of direct PS can be further increased by learning the model (i.e., transition and reward function) of the system from data and inferring the optimal policy from the model [7]. Probabilistic models have been more successful than deterministic ones, as they provide an estimate about the uncertainty of their approximation which can be incorporated into long-term planning [13]. For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares conditional density estimation in [30].\nEarly examples of such model-based PS include applications on helicopter hovering [25], [26] and blimp control [28]. These works employ the PEGASUS algorithm which can transform a stochastic Markov Decision Process (MDP) or partially-observable MDP (POMDP) into a deterministic POMDP [31]. It does so by fixing in advance the sequence of random numbers associated with the state transitions. This simple modification significantly reduces the time needed to optimize the policy, as it removes the noise from the evaluation of an initially noisy objective function.\nBoth the model-based PGPE [30] and the PILCO [13] algorithm use gradient-based policy updates. Rather than using Monte Carlo sampling, as in model-based PGPE, PILCO performs deterministic approximate inference by explicitly incorporating the model uncertainty into long-term predictions. This procedure is done by approximating the\nprobability distribution over trajectories with a Gaussian that has the same mean and covariance (moment matching). The gradient of the expected return is then computed analytically with respect to the policy parameters. This makes PILCO dependent on differentiable reward and policy functions.\nGradient-free methods, such as the Model-Based Relative Entropy PS (M-REPS) [29] and the Model-Based Guided PS (M-GPS) [27], do not have these requirements. Both algorithms place a KL-divergence constraint on the cost function to bound the distance between the old trajectory distribution and the newly estimated one at each policy improvement step. This constraint limits the information loss of the updates [32]. M-GPS turns the policy optimization problem into a supervised learning one, allowing the use of high-dimensional policy representations such as deep neural networks. However, M-GPS makes strong assumptions about the task at hand, by assuming that time-varying Gaussians can approximate the local dynamics. In contrast, M-REPS uses GPs for model learning, and the REPS algorithm (which can be seen as a BBO) for policy search.\nOverall, the current consensus [7] is that (1) modelbased algorithms are more data-efficient than direct PS, (2) in model-based PS, it is crucial to account for potential model errors during policy learning, and (3) deterministic approximate inference and analytic computation of policy gradients is required to make model-based PS computationally tractable. In this paper, we focus on the latter and explore a parallel BBO algorithm for policy optimization."
    }, {
      "heading" : "III. PROBLEM FORMULATION",
      "text" : "We consider dynamical systems of the form:\nxt+1 = xt + f(xt,ut) + w (1)\nwith continuous-valued states x ∈ RE and controls u ∈ RF , i.i.d. Gaussian system noise w, and unknown transition dynamics f .\nOur objective is to find a deterministic policy π, u = π(x|θ), which maximizes the expected long-term reward when following policy π for T time steps:\nJ(θ) = E [ T∑ t=1 r(xt) ∣∣∣θ] (2)\nwhere r(xt) is the immediate reward of being in state x at time t. We assume that π is a function parameterized by θ ∈ RΘ and that the immediate reward function r(x) ∈ R might be unknown to the learning algorithm."
    }, {
      "heading" : "IV. APPROACH",
      "text" : ""
    }, {
      "heading" : "A. Learning dynamics model with Gaussian processes",
      "text" : "We would like to have a model f̂ that approximates as accurately as possible the unknown dynamics f of our systems and provides uncertainty information. We rely on Gaussian processes (GPs) to do so. A GP is an extension of multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [14].\nAs inputs, we use tuples made of the state vector xt and the action vector ut, that is, x̃t = (xt,ut) ∈ RE+F ; as training targets, we use the difference between the current state vector and the next one: ∆xt = xt+1 − xt ∈ RE . We use E independent GPs to model each dimension of the difference vector ∆xt . For each dimension d = 1 . . . E of ∆xt , the GP is computed as (kf̂d is the kernel function):\nf̂d(x̃) ∼ GP(µf̂d(x̃), kf̂d(x̃, x̃ ′)) (3)\nAssuming Dd1:t = {fd(x̃1), ..., fd(x̃t)} is a set of observations, we can query the GP at a new input point x̃∗:\np(f̂d(x̃∗)|Dd1:t, x̃∗) = N (µf̂d(x̃∗), σ 2 f̂d (x̃∗)) (4)\nThe mean and variance predictions of this GP are computed using a kernel vector k f̂d = k(D d 1:t, x̃∗), and a kernel matrix Kf̂d , with entries K ij f̂d = kf̂d(x̃i, x̃j):\nµf̂d(x̃∗) = k T f̂d K−1 f̂d Dd1:t σ2 f̂d (x̃∗) = kf̂d(x̃∗, x̃∗)− k T f̂d K−1 f̂d k f̂d (5)\nIn this paper, we use the exponential kernel with automatic relevance determination [14]:\nkf̂d(x̃p, x̃q) = σ 2 dexp(−\n1 2 (x̃p − x̃q)TΛ−1d (x̃p − x̃q))\n+ δpqσ 2 nd\n(6)\nwhere δpq equals to 1 when p = q and 0 otherwise, and [Λd, σ 2 d, σ 2 nd\n] is the vector of hyper-parameters of the kernel (length scales for each dimension of the observations, signal variance and noise) found through Maximum Likelihood Estimation [14]. We use the limbo C++11 library for GP regression [33]."
    }, {
      "heading" : "B. Learning the immediate reward function with a GP",
      "text" : "Similarly to the dynamical model, we use a GP to learn the immediate reward function, which associates a reward r(x) ∈ R to each state x:\nr̂(x) ∼ GP(µr(x), kr(x,x′)) (7)\nThe GP predictions are calculated similarly to Eq. 4 and 5."
    }, {
      "heading" : "C. Policy Evaluation",
      "text" : "Our goal is to maximize the expected cumulative reward (Eq. 2), which requires predicting the state evolution given an uncertain transition model and an uncertain reward model. To do so in a deterministic way2, PILCO proposes to approximate the distribution of state xt+1 given the distribution of state xt and the action ut using moment matching [13], and then propagates from state to state until reaching the end of the episode; however, this sequential approach accumulates errors over time, is not easy to parallelize, and is computationally expensive. As an alternative, we can compute a Monte Carlo approximation of the final distribution: at each step we sample a new state according to the GP of the model and its reward according to the reward model, query the\n2Additionally, PILCO requires the reward function to be known a priori.\npolicy to choose the actions, and use this new state to sample the next state. By performing this process many times, we can get a good estimate of the expected cumulative reward, but many samples are needed to obtain a good estimate [29].\nHere we adopt a different approach. Like in Monte Carlo estimation, we propagate from state to state by sampling according to the models. However, we consider that each of these rollouts is a measurement of a function G(θ) that is the actual function J(θ) perturbed by a noise N(θ):\nG(θ) = J(θ) +N(θ)\n= T∑ t=1 r̂(xt−1 + f̂(xt−1,ut−1)) (8)\nwhere f̂(xt−1,ut−1) ∼ N (µf̂ (x̃t−1),Σf̂ (x̃t−1)) is a realization of a normally distributed random vector according to Eq. 4, r̂(x) ∼ N (µr(x), σ2r(x)) is a realization of a normally distributed random value according to Eq. 7 and ut−1 = π(xt−1|θ). We would like to maximize its expectation:\nE [ G(θ) ] = E [ J(θ) +N(θ) ] = E [ J(θ) ] + E [ N(θ) ] (9)\nAnd since ∀x E [ E[x] ] = E[x]:\nE [ G(θ) ] = J(θ) + E [ N(θ) ] (10)\nWe assume that E[N(θ)] = 0 for all θ ∈ RΘ and therefore maximizing E[G(θ)] is equivalent to maximizing J(θ)."
    }, {
      "heading" : "D. Policy search",
      "text" : "Seeing the maximization of J(θ) as the optimization of a noisy function allows us to maximize it without computing or estimating it explicitly: we only use the noisy measurements in the optimization algorithm. To do so, we build on all the work about noisy function optimization [34], [35], and especially on CMA-ES, one of the most successful blackbox optimizer for noisy functions [15]. CMA-ES (Fig. 1) performs four steps at each generation k: (1) sample λ new candidates according to a multi-variate\nGaussian distribution of mean mk and covariance σ2kCk, that is, θi ∼ N (mk, σ2kCk) for i ∈ 1, · · · , λ; (2) rank the λ sampled candidates based on their (noisy) performance G(θi); (3) compute mk+1 by averaging the µ best candidates: mk+1 = 1 µ ∑µ i=1 θi;\n(4) update the covariance matrix to reflect the distribution of the µ best candidates.\nOverall, these steps are only marginally impacted by noise in the performance function, as confirmed by empirical experiments with noisy functions [34]. More precisely, the only decision that matters is whether a solution belongs to the µ best ones (step 2), that is, a precise ranking is not needed and errors can only happen at the boundaries between the low-performing and high-performing solutions. In addition, if a candidate is misclassified because of the noise, the impact of this error will be smoothed out by the average when computing mk+1 (step 3). One can also observe that because CMA-ES samples several solutions around a mean mk, it performs many evaluations of similar parameters, which are then averaged: this implicit averaging [34], [36] has many similarities with re-evaluating noisy solutions to estimate their expectation.\nModern implementations of CMA-ES add several refinements to compute the covariance matrix, to take into account successive steps, and to restart the process with more exploration when it reaches an optimum. In this work, we use BIPOP-CMA-ES with restarts [37], which is one of best CMA-ES variants on benchmarks with both noiseless and noisy functions [37], [38].\nOn top of these refinements, we follow the strategy proposed by Hansen et al. [35] to improve the behavior of CMA-ES with noisy functions (called UH-CMA-ES). The starting idea is that uncertainty is a problem for a rankbased algorithm if and only if, for two potential candidates θ1 and θ2 the variation due to N(θ1) and N(θ2) exceeds the difference |J(θ1) − J(θ2)| and thus their ordering is changed. If the variation tends to exceed this difference, we cannot conclude only from two measurements G(θ1), G(θ2), whether J(θ1) > J(θ2) or J(θ1) < J(θ2) holds. If we view |J(θ1)−J(θ2)| as the signal and the variations due to N(θ) as noise, then it follows that one way to improve the quality of the ranking without re-evaluating solutions many times (which would reduce noise) is to increase the signal.\nWe therefore implement the following strategy: (1) at each generation, we quantify the uncertainty of the ranking by reevaluating λreev < λ randomly selected candidates from the population and count the number of rank changes (see [35] for a detailed description of uncertainty quantification), (2) if the uncertainty is above a user-defined threshold, then we increase the variance of the population (σk in step 1 of CMA-\nES). In addition of reducing the uncertainty of the ranking when needed, this strategy has an interesting consequence: in uncertain search-space regions, CMA-ES moves faster (it makes bigger steps), which means that the algorithm favors regions that are more certain (when they are as promising as uncertain regions) and is not “trapped” in uncertain regions. We use a modified version of the libcmaes C++11 library3."
    }, {
      "heading" : "E. Black-box Data-efficient Robot Policy Search",
      "text" : "Putting everything together, we get the Black-DROPS algorithm (Alg. 1). Firstly, NR random episodes of T time steps are conducted on the robot (Alg. 1: lines 4-12). In the learning loop, first we learn a probabilistic model of the dynamics and a model of the reward function, and then we optimize E [ G(θ) ] given this learned models using BIPOPCMAES with uncertainty handling (Alg. 1: lines 14-16). Lastly, the best policy πθ∗ is executed on the robot, more data is collected and the main loop continues until the task is learned.\nAlgorithm 1 Black-DROPS 1: procedure BLACK-DROPS 2: Define policy π : x× θ → u 3: D = ∅ 4: for i = 1→ NR do . NR random episodes 5: Set robot to initial state x0 6: for j = 0→ T − 1 do . perform the episode 7: uj = random action() 8: xj+1, r(xj+1) = execute on robot(uj ) 9: D = D ∪ {x̃j →∆xj } 10: R = R ∪ {xj+1 → r(xj+1)} 11: end for 12: end for 13: while task 6= solved do 14: Model learning: train E GPs given data D 15: Reward learning: train 1 GP given data R 16: θ∗ = argmaxθ E [ G(θ) ] using BIPOP-CMA-ES . Sec. IV-D 17: Set robot to initial state x0 18: for j = 0→ T − 1 do . perform the episode 19: uj = π(xj |θ∗) 20: xj+1, r(xj+1) = execute on robot(uj ) 21: D = D ∪ {x̃j →∆xj } 22: R = R ∪ {xj+1 → r(xj+1)} 23: end for 24: end while 25: end procedure"
    }, {
      "heading" : "V. EXPERIMENTAL SETUP",
      "text" : ""
    }, {
      "heading" : "A. Policy Representations",
      "text" : "To highlight the flexibility of Black-DROPS, we use a GP-based policy [13] and a feed-forward neural networkbased one. Any other parameterized policy can be used (e.g., dynamic movement primitives).\n1) GP Policy: If we only consider the mean, a Gaussian process can be used to map states to actions, that is, to define a policy:\nπ(x) = umaxκ(µ(x)) = umaxκ(k T (K + σ2nI) −1x) (11)\nwhere umax is the maximum value of u (different for each action dimension), κ is a squashing function like the one\n3https://github.com/beniz/libcmaes\nused in [13], x is the input state vector to the policy, K is the covariance matrix and its elements are computed using the exponential kernel with automatic relevance determination as in Eq. 6. Here, we set signal noise, σ2n = 0.01. The vector [Λ, σ2f ] and the pseudo-observations (inputs & targets when learning the GP) constitute the parameters of the policy.\n2) Neural Network Policy: The network function of the ith layer of the network is given by yi = φi(Wiyi−1 +bi), where Wi and bi are the weight matrix and bias vector, yi−1 and yi are the input and output vector and φi is the activation function. Throughout the paper, we use configurations with one hidden layer and the hyperbolic tangent as the activation function φ for all the layers, leading to:\nπ(x) = umaxy1 = umaxφ(W1y0 + b1)\nand y0 = φ(W0x + b0) (12)"
    }, {
      "heading" : "B. Metrics",
      "text" : "1) Reward as interaction time increases: This metric assesses the quality of the solutions and the data-efficiency of each algorithm.\n2) Speed-up when more cores are available: This metric assesses how well each algorithm scales as the available hardware resources increase, independently of the particular implementation (e.g., MATLAB vs C++)."
    }, {
      "heading" : "C. Remarks",
      "text" : "We evaluate Black-DROPS on the pendulum and cart-pole tasks and compare it to PILCO using 120 replicates over different CPU configurations. As an additional baseline, we evaluate a variant of our approach using deterministic GP models of the dynamics (i.e., using only the mean of the GPs) to quantify the importance of considering the uncertainty (variance) of the model in policy optimization. For BlackDROPS and the baseline we use two different policies: a neural network policy (with one hidden layer and 10 hidden units) and a GP policy (with 10 and 20 pseudo-observations for the pendulum and the cart-pole task respectively). For PILCO we used only the GP policy with the same parameters as for the other algorithms.\nWe additionally evaluate Black-DROPS on a 4-DOF arm task to validate that it can be used with more complex and interesting robots, that it can be used when the reward function is unknown, and that it works on a real robotic platform. We use only the neural network policy for this task, as it performed better in the simpler benchmarks.\nFor all the tasks, an episode corresponds to applying the same policy for a duration of 4 s and the sampling/control rate is 10Hz. The source code of the experiments can be found at https://github.com/resibots/blackdrops."
    }, {
      "heading" : "VI. RESULTS",
      "text" : ""
    }, {
      "heading" : "A. Task 1: Inverted Pendulum",
      "text" : "This simulated system consists of a freely swinging pendulum with mass m = 1 kg and length l = 1m. The objective is to learn a controller to swing the pendulum up and to balance it in the inverted position applying a torque.\n• State: xpend = [θ̇, θ] ∈ R2, x0 = [0, 0]. • Actions: upend = upend ∈ R, −2.5 ≤ upend ≤ 2.5N . • To avoid angle discontinuities, we transform the input\nof the GPs, the reward function, and the policy to be:\nxinput = [θ̇, cos(θ), sin(θ)] ∈ R3\nThe MATLAB implementation of PILCO uses this transformation by default4. • Reward: We use the same reward function as PILCO5. This is a saturating distance-based reward function:\nr(x) = exp(− 1 2σ2c (x− x∗)TQ(x− x∗)) (13)\nwhere σc controls the width of the reward function, Q is a weight matrix, x∗ is the target state and r(x) ∈ [0, 1]. We set x∗ = [∗, cos(π), sin(π)], σc = 0.25 and Q to ignore the angular velocity θ̇ of the pendulum.\nIn this task, both Black-DROPS and PILCO solve the task in about 3 episodes (12 s of interaction time — including the random episode, Fig. 2A), but Black-DROPS finds higher-performing policies (Fig. 2A-B), with both the neural network and the GP policy. When the number of cores is increased, the computation time required by Black-DROPS decreases almost linearly (Fig. 3A), whereas PILCO only slightly benefits from having more than 4 cores (as PILCO is not a parallel algorithm, we think that the improvement between 1 and 4 cores stems from MATLAB’s ability to parallelize some linear algebra operations). With more than 8 cores, Black-DROPS outperforms PILCO in computation speed and can be from 1.25 to 3.3 times faster when 12 cores\n4http://mlg.eng.cam.ac.uk/pilco/ 5PILCO uses a cost function, but it is straightforward to transform it in\na reward function.\nare available6 (Fig. 3B). In addition, given a budget of 15 episodes, Black-DROPS succeeds more often than PILCO in finding a working policy (Table I): Black-DROPS always solves the task whereas PILCO fails once in ten runs.\nSurprisingly, taking into account the uncertainty of the model does not seem to be necessary in this task (the “No Var” baselines perform the same as Black-DROPS, see Fig. 2). This result most probably stems from the fact that the dynamics of the system are simple enough for the GPs to model almost perfectly with one or two episodes."
    }, {
      "heading" : "B. Task 2: Cart-pole Swing-Up",
      "text" : "This simulated system consists of a cart with mass M = 0.5 kg running on a track and a freely swinging pendulum\n6While some of the runtime differences can stem from the language used (e.g., C++ being faster than MATLAB or MATLAB being faster at matrix computations), what matters is that a parallel algorithm with enough CPUs can eventually outperform a sequential gradient-based approach.\nwith mass m = 0.5 kg and length l = 0.5m attached to the cart. The state of the system contains the position of the cart, the velocity of the cart, the angle of the pendulum and the angular velocity of the pendulum. The objective is to learn a controller that applies horizontal forces on the cart to swing the pendulum up and balance it in the inverted position in the middle of the track. • State: xcp = [ẋ, x, θ̇, θ] ∈ R4, x0 = [0, 0, 0, 0]. • Actions: ucp = ucp ∈ R, −10 ≤ ucp ≤ 10N . • To avoid angle discontinuities, we transform the input\nof the GPs, the reward, and the policy to be:\nxinput = [ẋ, x, θ̇, cos(θ), sin(θ)] ∈ R5\n• Reward: We set x∗ = [∗, 0, ∗, cos(π), sin(π)], σc = 0.25, Q to ignore ẋ and θ̇, and use Eq. 13.\nThe results for the cart-pole are very similar to those obtained with the inverted pendulum (Fig. 4A-B): BlackDROPS and PILCO have similar data-efficiency (about 4 episodes or 16 s of interaction time to find a working policy with PILCO, 5 episodes or 20 s with Black-DROPS) but Black-DROPS finds higher-performing policies, most probably because its search algorithm (CMA-ES with restarts) is less local than the gradient-based optimizer used in PILCO.\nUsing the variance helps more in this task than in the pendulum task: the variants of Black-DROPS without uncertainty handling are less data-efficient and have more variance. However, they are still able to find policies that are higherperforming than those found by PILCO (Fig. 4B). In terms of success rate, Black-DROPS fails as often as PILCO and,\nas expected, the variants fail more often than PILCO and Black-DROPS (see Table I).\nSimilar to the pendulum task, here also Black-DROPS takes advantage of multiple cores to highly speed-up its computation and is 1.6 times faster than PILCO when 12 cores are available (Fig. 5)."
    }, {
      "heading" : "C. Task 3: 4-DOF Manipulator",
      "text" : "We applied Black-DROPS on a physical velocitycontrolled 4-DOF robotic arm (Fig. 6, 10 replicates). We assume that we can only observe the angles of the joints of the arm and that the reward function rarm is initially unknown. The arm begins in the up-right position and the objective is to learn a controller so that the end-effector quickly reaches a certain position (shown in Fig. 6A). We compare Black-DROPS with the baseline without variance. • State: xarm = [q0, q1, q2, q3] ∈ R4, x0 = [0, 0, 0, 0]. • Actions: uarm = [v0, v1, v2, v3] ∈ R4, where −1.0 ≤ vi ≤ 1.0 rad/s, i = 0, 1, 2, 3.\n• Reward: The unknown (to the algorithm) reward function has a form similar to Eq. 13:\nrarm(x) = exp(− 1\n2σ2c ‖px − p∗‖) (14)\nwhere σc = 0.1, px corresponds to the end-effector position in state x, p∗ is the goal position of the endeffector and rarm(x) ∈ [0, 1]. • To avoid angle discontinuities, we transform the input to the GPs and the policy to be:\nxinput = [cos(q0), sin(q0), cos(q1), sin(q1), cos(q2),\nsin(q2), cos(q3), sin(q3)] ∈ R8\nThe results show that Black-DROPS is able to find a working policy within 5 episodes (including the initial random one) and outperforms the baseline which needs around 6 episodes (Fig. 6B). Black-DROPS, also, shows less variance and converges to high quality controllers faster (6 episodes vs 8-9). A video of a typical run is available as supplementary material (also at https://youtu.be/kTEyYiIFGPM)."
    }, {
      "heading" : "VII. CONCLUSION AND DISCUSSION",
      "text" : "Black-DROPS lifts several constraints imposed by analytical approaches (reward and policy types) while being competitive in terms of data-efficiency and computation time. In three different tasks, it achieved similar results as the stateof-the-art (PILCO) while being faster when multiple cores are used. We expect that the ability of Black-DROPS to scale with the number of cores will be even more beneficial on future computers with more cores and/or with GPUs.\nUsing the variance in the optimization is one of the key components to learn with as little interaction time as possible. However, the learned dynamics models are only confident in areas of the state space previously visited and thus could drive the optimization into local optima when multiple and diverse solutions exist. In future work, we will investigate ways of exploring more without impacting data-efficiency.\nFinally, even with 12 cores, although faster than analytical approaches, Black-DROPS still requires around 25 minutes for completing 8 episodes in the cart-pole task. The main issue is the cubic computational complexity of the prediction of the GPs (we are doing around 64,000,000 GP queries per episode). Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42]."
    } ],
    "references" : [ {
      "title" : "How UGVs physically fail in the field",
      "author" : [ "J. Carlson", "R.R. Murphy" ],
      "venue" : "IEEE Trans. on Robotics, vol. 21, no. 3, pp. 423–437, 2005.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Robots that can adapt like animals",
      "author" : [ "A. Cully", "J. Clune", "D. Tarapore", "J.-B. Mouret" ],
      "venue" : "Nature, vol. 521, no. 7553, pp. 503–507, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reset-free Trial-and-Error Learning for Data-Efficient Robot Damage Recovery",
      "author" : [ "K. Chatzilygeroudis", "V. Vassiliades", "J.-B. Mouret" ],
      "venue" : "arXiv:1610.04213, 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Emergency response to the nuclear accident at the Fukushima Daiichi Nuclear Power Plants using mobile rescue robots",
      "author" : [ "K. Nagatani" ],
      "venue" : "Journal of Field Robotics, vol. 30, no. 1, pp. 44–63, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih" ],
      "venue" : "Nature, vol. 518, no. 7540, pp. 529–533, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature, vol. 521, no. 7553, pp. 436–444, 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A survey on policy search for robotics",
      "author" : [ "M.P. Deisenroth", "G. Neumann", "J. Peters" ],
      "venue" : "Foundations and Trends in Robotics, vol. 2, no. 1, pp. 1–142, 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Policy gradient reinforcement learning for fast quadrupedal locomotion",
      "author" : [ "N. Kohl", "P. Stone" ],
      "venue" : "Proc. of ICRA, vol. 3. IEEE, 2004, pp. 2619–2624.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Bayesian optimization for learning gaits under uncertainty",
      "author" : [ "R. Calandra", "A. Seyfarth", "J. Peters", "M. Deisenroth" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence (AMAI), 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Automatic gait optimization with Gaussian process regression",
      "author" : [ "D.J. Lizotte", "T. Wang", "M.H. Bowling", "D. Schuurmans" ],
      "venue" : "IJCAI, vol. 7, 2007, pp. 944–949.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Model learning for robot control: a survey",
      "author" : [ "D. Nguyen-Tuong", "J. Peters" ],
      "venue" : "Cognitive Processing, vol. 12, no. 4, pp. 319–340, 2011.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Gaussian processes for data-efficient learning in robotics and control",
      "author" : [ "M.P. Deisenroth", "D. Fox", "C.E. Rasmussen" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 2, pp. 408–423, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Gaussian processes for machine learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Completely derandomized selfadaptation in evolution strategies",
      "author" : [ "N. Hansen", "A. Ostermeier" ],
      "venue" : "Evolutionary computation, vol. 9, no. 2, pp. 159–195, 2001.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "R.J. Williams" ],
      "venue" : "Machine learning, vol. 8, no. 3-4, pp. 229–256, 1992.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Policy gradients with parameter-based exploration for control",
      "author" : [ "F. Sehnke" ],
      "venue" : "Proc. of Artificial Neural Networks. Springer, 2008, pp. 387–396.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Policy search for motor primitives in robotics",
      "author" : [ "J. Kober", "J. Peters" ],
      "venue" : "Machine Learning, vol. 84, pp. 171–203, 2011.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A generalized path integral control approach to reinforcement learning",
      "author" : [ "E. Theodorou", "J. Buchli", "S. Schaal" ],
      "venue" : "JMLR, vol. 11, pp. 3137– 3181, 2010.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Natural evolution strategies",
      "author" : [ "D. Wierstra" ],
      "venue" : "JMLR, vol. 15, no. 1, pp. 949–980, 2014.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Bidirectional relation between CMA evolution strategies and natural evolution strategies",
      "author" : [ "Y. Akimoto", "Y. Nagata", "I. Ono", "S. Kobayashi" ],
      "venue" : "Proc. of PPSN. Springer, 2010, pp. 154–163.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Robot skill learning: From reinforcement learning to evolution strategies",
      "author" : [ "F. Stulp", "O. Sigaud" ],
      "venue" : "Paladyn, Journal of Behavioral Robotics, vol. 4, no. 1, pp. 49–61, 2013.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Taking the human out of the loop: A review of Bayesian optimization",
      "author" : [ "B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas" ],
      "venue" : "Proc. of the IEEE, vol. 104, no. 1, pp. 148–175, 2016.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Using confidence bounds for exploitation-exploration tradeoffs",
      "author" : [ "P. Auer" ],
      "venue" : "JMLR, vol. 3, pp. 397–422, 2002.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Autonomous helicopter control using reinforcement learning policy search methods",
      "author" : [ "J.A. Bagnell", "J.G. Schneider" ],
      "venue" : "Proc. of ICRA, vol. 2. IEEE, 2001, pp. 1615–1620.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Autonomous inverted helicopter flight via reinforcement learning",
      "author" : [ "A.Y. Ng", "A. Coates", "M. Diel", "V. Ganapathi", "J. Schulte", "B. Tse", "E. Berger", "E. Liang" ],
      "venue" : "Experimental Robotics IX. Springer, 2006, pp. 363–372.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning neural network policies with guided policy search under unknown dynamics",
      "author" : [ "S. Levine", "P. Abbeel" ],
      "venue" : "Proc. of NIPS, 2014, pp. 1071–1079.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Gaussian processes and reinforcement learning for identification and control of an autonomous blimp",
      "author" : [ "J. Ko", "D.J. Klein", "D. Fox", "D. Haehnel" ],
      "venue" : "Proc. of ICRA, 2007, pp. 742–747.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Model-based contextual policy search for dataefficient generalization of robot skills",
      "author" : [ "A. Kupcsik" ],
      "venue" : "Artificial Intelligence, 2014.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Model-based policy gradients with parameter-based exploration by least-squares conditional density estimation",
      "author" : [ "V. Tangkaratt", "S. Mori", "T. Zhao", "J. Morimoto", "M. Sugiyama" ],
      "venue" : "Neural Networks, vol. 57, pp. 128–140, 2014.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "PEGASUS: a policy search method for large MDPs and POMDPs",
      "author" : [ "A.Y. Ng", "M. Jordan" ],
      "venue" : "Proc. of Uncertainty in Artificial Intelligence. Morgan Kaufmann, 2000, pp. 406–415.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Reinforcement learning of motor skills with policy gradients",
      "author" : [ "J. Peters", "S. Schaal" ],
      "venue" : "Neural Networks, vol. 21, no. 4, pp. 682–697, 2008.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Limbo: A fast and flexible library for Bayesian optimization",
      "author" : [ "A. Cully", "K. Chatzilygeroudis", "F. Allocati", "J.-B. Mouret" ],
      "venue" : "arxiv:1611.07343, 2016.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Evolutionary optimization in uncertain environments-a survey",
      "author" : [ "Y. Jin", "J. Branke" ],
      "venue" : "IEEE Trans. on Evolutionary Computation, vol. 9, no. 3, pp. 303–317, 2005.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A method for handling uncertainty in evolutionary optimization with an application to feedback control of combustion",
      "author" : [ "N. Hansen", "A.S. Niederberger", "L. Guzzella", "P. Koumoutsakos" ],
      "venue" : "IEEE Trans. on Evolutionary Computation, vol. 13, no. 1, pp. 180–197, 2009.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Genetic algorithms, selection schemes, and the varying effects of noise",
      "author" : [ "B.L. Miller", "D.E. Goldberg" ],
      "venue" : "Evolutionary Computation, vol. 4, no. 2, pp. 113–131, 1996.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Benchmarking a BI-population CMA-ES on the BBOB- 2009 function testbed",
      "author" : [ "N. Hansen" ],
      "venue" : "Proc. of GECCO. ACM, 2009, pp. 2389– 2396.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Benchmarking a BI-population CMA-ES on the BBOB-2009 noisy testbed",
      "author" : [ "——" ],
      "venue" : "Proc. of GECCO. ACM, 2009, pp. 2397–2402.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Patchwork kriging for large-scale gaussian process regression",
      "author" : [ "C. Park", "D. Apley" ],
      "venue" : "arXiv preprint arXiv:1701.06655, 2017.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Distributed gaussian processes",
      "author" : [ "M.P. Deisenroth", "J.W. Ng" ],
      "venue" : "arXiv:1502.02843, 2015.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Improving PILCO with bayesian neural network dynamics models",
      "author" : [ "Y. Gal", "R.T. McAllister", "C.E. Rasmussen" ],
      "venue" : "Data-Efficient Machine Learning workshop, 2016.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "author" : [ "Y. Gal", "Z. Ghahramani" ],
      "venue" : "Proc. of ICML, 2015.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Reinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "Reinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : "Reinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Reinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : ", deep learning) that rely on the availability of very large datasets or fast simulations1 [6].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "1As an illustration, the deep Q-learning algorithm needed about 38 days of interaction to learn to play Atari 2600 games [5] (only four possible actions), which would be hardly conceivable to a achieve with a robot.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "One of the best ways to take advantage of this sequential state recording is to learn a dynamical model of the robot [11], and then exploit it either for model-predictive control [12] or to find an optimal policy offline [7].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "One of the best ways to take advantage of this sequential state recording is to learn a dynamical model of the robot [11], and then exploit it either for model-predictive control [12] or to find an optimal policy offline [7].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 11,
      "context" : "The PILCO (Probabilistic Inference for Learning COntrol) algorithm [13], which is one of the state-of-theart algorithms for data-efficient model-based policy search, follows this strategy by alternating between two steps, (1) learning a dynamical model with Gaussian processes [14], (2) using a gradient-based optimizer to search for a policy that maximizes the expected reward, taking the uncertainty of the model into account.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "The PILCO (Probabilistic Inference for Learning COntrol) algorithm [13], which is one of the state-of-theart algorithms for data-efficient model-based policy search, follows this strategy by alternating between two steps, (1) learning a dynamical model with Gaussian processes [14], (2) using a gradient-based optimizer to search for a policy that maximizes the expected reward, taking the uncertainty of the model into account.",
      "startOffset" : 277,
      "endOffset" : 281
    }, {
      "referenceID" : 8,
      "context" : ", parameterized state automata, like in [9]).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : ", typically more than 5 minutes on a modern computer between each episode for the cart-pole benchmark), because they rely on computationally expensive methods to do approximate inference for each step of the policy evaluation [13].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 13,
      "context" : "By contrast, Monte Carlo approaches and population-based black-box optimizers like CMA-ES [15] (1) do not put any constraint on the reward functions and policies, and (2) are straightforward to parallelize, which can make them competitive with analytical approaches when several cores are available.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : ", CMA-ES [15]), which saves a lot of computation: only the ranking of potential solutions matters.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 6,
      "context" : "Direct policy search (PS) methods have been successful in robotics as they can easily be applied in highdimensional continuous state-action RL problems [7].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : "REINFORCE [16] is an early policy gradient method which",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "Policy gradients with parameter-based exploration (PGPE) [17] address this problem by transferring exploration to parameter space.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "The PoWER (Policy learning by Weighting Exploration with the Returns) algorithm [18] uses probability-weighted averaging, which has the property",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "of following the natural gradient without computing it [18].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "The Policy Improvements with Path Integrals (PI) [19] algorithm does not make such an assumption.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "When the reward function is compatible with both PoWER and PI, the algorithms have identical performance [19].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "This can be addressed by the Natural Evolution Strategies (NES) [20] and Covariance Matrix Adaptation ES (CMA-ES) [15] families of algorithms, which are population-based Black-Box Optimizers (BBO).",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "This can be addressed by the Natural Evolution Strategies (NES) [20] and Covariance Matrix Adaptation ES (CMA-ES) [15] families of algorithms, which are population-based Black-Box Optimizers (BBO).",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : "NES and CMA-ES are closely related, as the latter performs an approximate natural gradient ascent [21].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 20,
      "context" : "and was shown to be a special case of CMA-ES [22].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "Bayesian optimization [23] is a particular family of BBO that can be very data-efficient by building a surrogate model of the objective function (i.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : ", using upper confidence bounds [24]).",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "It can drastically decrease the evaluation time when optimizing gaits [10] or when finding compensatory behaviors for damaged robots [2].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "It can drastically decrease the evaluation time when optimizing gaits [10] or when finding compensatory behaviors for damaged robots [2].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : ", transition and reward function) of the system from data and inferring the optimal policy from the model [7].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "Probabilistic models have been more successful than deterministic ones, as they provide an estimate about the uncertainty of their approximation which can be incorporated into long-term planning [13].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 23,
      "context" : "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 25,
      "context" : "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 27,
      "context" : "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 28,
      "context" : "conditional density estimation in [30].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "Early examples of such model-based PS include applications on helicopter hovering [25], [26] and blimp control [28].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "Early examples of such model-based PS include applications on helicopter hovering [25], [26] and blimp control [28].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "Early examples of such model-based PS include applications on helicopter hovering [25], [26] and blimp control [28].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 29,
      "context" : "(MDP) or partially-observable MDP (POMDP) into a deterministic POMDP [31].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 28,
      "context" : "Both the model-based PGPE [30] and the PILCO [13] algorithm use gradient-based policy updates.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "Both the model-based PGPE [30] and the PILCO [13] algorithm use gradient-based policy updates.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 27,
      "context" : "Gradient-free methods, such as the Model-Based Relative Entropy PS (M-REPS) [29] and the Model-Based Guided PS (M-GPS) [27], do not have these requirements.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "Gradient-free methods, such as the Model-Based Relative Entropy PS (M-REPS) [29] and the Model-Based Guided PS (M-GPS) [27], do not have these requirements.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 30,
      "context" : "This constraint limits the information loss of the updates [32].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "Overall, the current consensus [7] is that (1) modelbased algorithms are more data-efficient than direct PS, (2) in model-based PS, it is crucial to account for potential model errors during policy learning, and (3) deterministic approximate inference and analytic computation of policy gradients is required to make model-based PS computationally tractable.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "A GP is an extension of multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [14].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 12,
      "context" : "In this paper, we use the exponential kernel with automatic relevance determination [14]:",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "where δpq equals to 1 when p = q and 0 otherwise, and [Λd, σ 2 d, σ 2 nd ] is the vector of hyper-parameters of the kernel (length scales for each dimension of the observations, signal variance and noise) found through Maximum Likelihood Estimation [14].",
      "startOffset" : 249,
      "endOffset" : 253
    }, {
      "referenceID" : 31,
      "context" : "We use the limbo C++11 library for GP regression [33].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "To do so in a deterministic way2, PILCO proposes to approximate the distribution of state xt+1 given the distribution of state xt and the action ut using moment matching [13], and then propagates from state to state until reaching the end of the episode; however, this sequential approach accumulates errors over time, is not easy to parallelize, and is compu-",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "Please note that in this example we use a bigger population than in our work, as advised by the authors of CMA-ES [15].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "By performing this process many times, we can get a good estimate of the expected cumulative reward, but many samples are needed to obtain a good estimate [29].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 32,
      "context" : "To do so, we build on all the work about noisy function optimization [34], [35], and especially on CMA-ES, one of the most successful blackbox optimizer for noisy functions [15].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 33,
      "context" : "To do so, we build on all the work about noisy function optimization [34], [35], and especially on CMA-ES, one of the most successful blackbox optimizer for noisy functions [15].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "To do so, we build on all the work about noisy function optimization [34], [35], and especially on CMA-ES, one of the most successful blackbox optimizer for noisy functions [15].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 32,
      "context" : "Overall, these steps are only marginally impacted by noise in the performance function, as confirmed by empirical experiments with noisy functions [34].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 32,
      "context" : "One can also observe that because CMA-ES samples several solutions around a mean mk, it performs many evaluations of similar parameters, which are then averaged: this implicit averaging [34], [36] has many similarities with re-evaluating noisy solutions to estimate their expectation.",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 34,
      "context" : "One can also observe that because CMA-ES samples several solutions around a mean mk, it performs many evaluations of similar parameters, which are then averaged: this implicit averaging [34], [36] has many similarities with re-evaluating noisy solutions to estimate their expectation.",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 35,
      "context" : "In this work, we use BIPOP-CMA-ES with restarts [37], which is one of best CMA-ES variants on benchmarks with both noiseless and noisy functions [37], [38].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 35,
      "context" : "In this work, we use BIPOP-CMA-ES with restarts [37], which is one of best CMA-ES variants on benchmarks with both noiseless and noisy functions [37], [38].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 36,
      "context" : "In this work, we use BIPOP-CMA-ES with restarts [37], which is one of best CMA-ES variants on benchmarks with both noiseless and noisy functions [37], [38].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 33,
      "context" : "[35] to improve the behavior of CMA-ES with noisy functions (called UH-CMA-ES).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "We therefore implement the following strategy: (1) at each generation, we quantify the uncertainty of the ranking by reevaluating λreev < λ randomly selected candidates from the population and count the number of rank changes (see [35]",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 11,
      "context" : "To highlight the flexibility of Black-DROPS, we use a GP-based policy [13] and a feed-forward neural networkbased one.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "com/beniz/libcmaes used in [13], x is the input state vector to the policy, K is the covariance matrix and its elements are computed using the exponential kernel with automatic relevance determination as in Eq.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "where σc controls the width of the reward function, Q is a weight matrix, x∗ is the target state and r(x) ∈ [0, 1].",
      "startOffset" : 108,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "1, px corresponds to the end-effector position in state x, p∗ is the goal position of the endeffector and rarm(x) ∈ [0, 1].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 37,
      "context" : "Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 38,
      "context" : "Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 39,
      "context" : "Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 40,
      "context" : "Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42].",
      "startOffset" : 159,
      "endOffset" : 163
    } ],
    "year" : 2017,
    "abstractText" : "The most data-efficient algorithms for reinforcement learning (RL) in robotics are based on uncertain dynamical models: after each episode, they first learn a dynamical model of the robot, then they use an optimization algorithm to find a policy that maximizes the expected return given the model and its uncertainties. It is often believed that this optimization can be tractable only if analytical, gradient-based algorithms are used; however, these algorithms require using specific families of reward functions and policies, which greatly limits the flexibility of the overall approach. In this paper, we introduce a novel model-based RL algorithm, called BlackDROPS (Black-box Data-efficient RObot Policy Search) that: (1) does not impose any constraint on the reward function or the policy (they are treated as black-boxes), (2) is as dataefficient as the state-of-the-art algorithm for data-efficient RL in robotics, and (3) is as fast (or faster) than analytical approaches when several cores are available. The key idea is to replace the gradient-based optimization algorithm with a parallel, blackbox algorithm that takes into account the model uncertainties. We demonstrate the performance of our new algorithm on two standard control benchmark problems (in simulation) and a low-cost robotic manipulator (with a real robot).",
    "creator" : "LaTeX with hyperref package"
  }
}