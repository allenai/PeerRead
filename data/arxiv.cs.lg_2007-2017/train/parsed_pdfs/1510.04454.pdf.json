{
  "name" : "1510.04454.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Markov decision processes with policy iteration",
    "authors" : [ "Yao Ma", "Hao Zhang", "Masashi Sugiyama" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\nThe online Markov decision process (MDP) is a generalization of the classical Markov decision process that incorporates changing reward functions. In this paper, we propose practical online MDP algorithms with policy iteration and theoretically establish a sublinear regret bound. A notable advantage of the proposed algorithm is that it can be easily combined with function approximation, and thus large and possibly continuous state spaces can be efficiently handled. Through experiments, we demonstrate the usefulness of the proposed algorithm."
    }, {
      "heading" : "1 Introduction",
      "text" : "A generalization of the classical shortest path problem in graph theory, called the stochastic shortest path problem (Bertsekas and Tsitsiklis, 1996), considers a probability distribution over all possible next nodes. A standard way to solve the stochastic shortest path problem is to formulate it as a Markov decision process (MDP) and find a policy that maximizes the cumulative reward over the path. In the MDP problem, the agent chooses the best action according to the current state and moves to the next state following the Markovian dynamics. A fixed reward function assigns a reward value to each state-action pair.\nA generalization of MDP, called the online MDP, considers the situation where the reward function changes over time. At each time step, the learning agent decides the strategy of choosing actions by using the knowledge of past reward functions. Then, the current reward function which is chosen by the environment is revealed to the agent after observing its behavior. The goal of online MDP is to minimize the regret against the best offline policy, which is the optimal fixed policy in hindsight. We expect that the regret vanishes as the time step T tends to infinity, implying that the agent can behave as well as the best offline policy asymptotically.\nMany online problems can be solved as online MDP problems. By setting the optimization variables as the state, the online MDP algorithm chooses the change of variables (action) which performs reasonably well in a non-stationary environment. Even-Dar et al. (2009) presented several typical online problems which can be formulated as online MDP perfectly, e.g. paging, k-server, metrical task system and stochastic inventory control.\nThe online MDP problem was first introduced by Even-Dar et al. (2003, 2009) and an expert-based MDP algorithm (MDP-E) was proposed, which was shown to achieve regret O( √ T |A|) (|A| is the cardinality of action space) by placing an expert algorithm on every state. Furthermore, the MDP-E algorithm was proved to achieve regret O(L2 √ T log |A|) for online MDP problems with L-layered state space (Neu et al., 2010a). However, the MDP-E algorithm is not computationally feasible for problems with large state space, since it needs to put the expert algorithm on every state.\nAnother online MDP algorithm called the lazy follow-the-perturbed-leader (lazyFPL) (Yu et al., 2009) follows the main idea of the FPL algorithm which solves the Bellman equation using the average reward function. The ”lazy” behavior of the lazyFPL algorithm divides the time horizon into short periods and the policy is only updated at the end of each period. This lazy-FPL algorithm was proved to achieve sublinear regret O(T 3/4+ǫ log T (|S|+ |A|)|A|2) for ǫ ∈ (0, 1/3).\nSimilarly to lazy-FPL, the online relative entropy policy search (O-REPS) algorithm (Zimin and Neu, 2013) also requires to solve an optimization problem at the end of each time step. It was shown that the O-REPS algorithm achieves regret O(L √ T log(|S||A|/L)) for online MDP problems with L-layer state space. Thus, the regret bound of O-REPS is much sharper than those for the MDP-E algorithm when L is large. However, OREPS requires the length of time horizon T to be finite, because the step size for parameter update needs to be set as a function of T . Therefore, it cannot be directly extended to problems with infinite time horizon. By introducing the stationary occupation measure, Dick et al. (2014) proposed the mirror descent with approximation projections algorithm, which formulate the online MDP problem as online linear opti-\nmization. Their theoretical results show that the regret is bounded by O( √ T ) where the finite state space assumption is essential.Yu et al. (2009), Abbasi-Yadkori et al. (2013), and Neu et al. (2012) considered even more challenging online MDP problems under unknown or changing transition dynamics.\nRecently, Ma et al. (2014) proposed the online policy gradient (OPG) algorithm for online MDP problems with continuous state and action spaces, and it was proved to achieve regret O( √ T ) under the concavity assumption about the expected average reward function. Although the OPG algorithm is natural and efficient for continuous problems, the concavity assumption may not be realistic in practice.\nThe aim of this paper is to develop a novel algorithm for solving online MDPs that is computationally efficient and performs well in problems with large state spaces. More specifically, we propose a policy iteration algorithm for online MDPs (OMDPPI), which has a close form update rule at each time step. We prove that our proposed algorithm achieves a sublinear regret with respect to a policy set. We further extend the proposed OMDP-PI algorithm with linear function approximation, which is essential for large (continuous) state space.\nThe remainder of this paper is organized as follows. In Section 2, we give the formal definition of online MDPs. In Section 3 we give the details of the proposed algorithm and analyze its regret. A generalization of the proposed algorithm with linear function approximation is also analyzed here. In Section 4, we present a discussion on solving online MDPs with stochastic iteration. In Section 5, we demonstrate the performance of the proposed algorithm in simulation experiments. In Section 6, we compare the related works with the proposed algorithm. Finally, in Section 7, we conclude the paper."
    }, {
      "heading" : "2 Problem definition and preliminaries",
      "text" : "In this section, we present the formal definition and involved preliminaries of the online MDP problem."
    }, {
      "heading" : "2.1 Online Markov decision process",
      "text" : "First, we formulate the problem of online MDP learning (Even-Dar et al., 2003, 2009) specified by {S,A, P, [rt]t=1,...,T}, where\n• S is the state space, and |S| is the cardinality of state space.\n• A is the action space, and |A| is the cardinality of action space.\n• P : S × S × A → [0, 1] is the transition probability, where p(s′|s,a) gives the\nconditional probability of next state s′ by taking action a at state s. We assume that the transition probability is available for the agent.\n• r1, . . . , rT is the reward function sequence, and only r1, . . . , rt are observed at\ntime step 1 ≤ t ≤ T .\nAt the end of each time step t = 1, . . . , T , trajectory ht is observed:\nht = {s1,a1, r1(s,a), . . . , st,at, rt(s,a)}.\nThe objective of an online MDP algorithm is to produce a strategy of choosing an action at time step t after observing ht. More specifically, let πt(a|s), ∀s ∈ S,a ∈ A be a stochastic time-dependent policy, which is the conditional probability of action a to be taken at state s at time step t.\nAn online MDP algorithm A learns a time dependent policy that maximizes the\nexpected cumulative rewards:\nRA(T ) =\nT ∑ t=1 Eπt [rt(st,at)|A] ,\nwhere π1, . . . , πT is the policy sequence generated by algorithm A and Eπt [·|A] denotes the expectation over the joint state-action distribution pt(s,a|A) = p(st = s|A)πt(a|s) at time step t.\nHowever, given that no information is available about future reward functions, directly analyzing the expected cumulative rewards is not meaningful. Here, in the same way as standard online learning literature (Cesa-Bianchi and Lugosi, 2006), we consider the regret against the best offline time independent policy π∗ in the policy set Π:\nLA(T ) = Rπ∗(T )−RA(T ).\nMore precisely, Rπ∗(T ) is the return of π∗ the best offline time independent policy:\nRπ∗(T ) = Eπ∗\n[\nT ∑ t=1 rt(st,at)\n]\n= sup π∈Π Eπ\n[\nT ∑ t=1 rt(st,at)\n]\n,\nwhere Eπ[·] denotes the expectation over the state-action joint distribution given policy π. Note that the regret we consider here is different from previous literature (Even-Dar et al., 2003, 2009; Zimin and Neu, 2013; Dick et al., 2014): we compare the performance of algorithm A against the best offline policy within a specific policy set Π. Namely instead of the best deterministic greedy policy, we consider a set of “efficient” policies, e.g., Gibbs policies with all possible parameters.\nWe expect that the regret LA(T ) is sublinear with respect to T , which means that the regret tends to zero as T tends to infinity and thus algorithm A performs as well as\nthe best offline policy π∗ asymptotically."
    }, {
      "heading" : "2.2 Preliminaries",
      "text" : "Next, we introduce some necessary notions for discussing online MDP problems. First, we show some criterion for evaluating the performance of any stochastic policy. For any policy π ∈ Π, the expected average reward ρ(π) is defined as\nρr(π) = Es∼dπ(s),a∼π [r(s,a)]\n= ∑\ns∈S\n∑ a∈A dπ(s)π(a|s)r(s,a),\nwhere dπ(s) is the stationary state distribution that satisfies\ndπ(s ′) =\n∑ s∈S dπ(s) ∑ a∈A π(a|s)p(s′|s,a).\nIt has been shown that every ergodic MDP has a unique stationary state distribution. In this paper, we assume that for all π ∈ Π the target MDP is ergodic.\nAnother way to evaluate the policy is to define the value function as\nVπr (s) = Eπ [ ∞ ∑\ni=1\n(r(si,ai)− ρr(π))|s1 = s ] ,\nFor any arbitrary reward function r(s,a) and transition probability p(s′|s,a), there exist at least one optimal policy π+ ∈ Π such that\nVπ+r (s) ≥ Vπr (s), ∀π ∈ Π, s ∈ S,\nρr(π +) ≥ ρr(π), ∀π ∈ Π.\nSimilarly, the state-action function is defined as\nQπr (s,a) = Eπ\n[\n∞ ∑ i=1 (r(si,ai)− ρr(π))|s1 = s,a1 = a ] .\nSince the optimal value function leads to the optimal policy, MDP is often solved by deriving the optimal value function (Sutton and Barto, 1998). So far, various efficient methods for approximating the optimal value function have been proposed. However, these algorithms were not proved to converge to the value function corresponding to the optimal deterministic policy. For this reason, in this paper we only consider the stochastic policy, since the convergence guarantee is provided (Tsitsiklis and Roy, 1999)."
    }, {
      "heading" : "3 Online MDPs with policy iteration",
      "text" : "In this section, we introduce the proposed method for online MDPs. The key idea of the proposed algorithm is motivated by the Lazy FPL algorithm by Yu et al. (2009), which performs linear programming to obtain the ‘leader’ policy. As Yu et al. (2009) pointed out, solving linear programming may not be appropriate for problems with large (continuous) state space. For this reason, we employ a policy iteration type method together with a stochastic policy in our proposed method."
    }, {
      "heading" : "3.1 Algorithm",
      "text" : "Firstly, we define the policy improvement operator Γ : π(a|s) = Γ(r(s,a), V (s)), where r(s,a) is an arbitrary reward function, V (s) is an arbitrary value function. Below we use Γ(r, V ) instead of Γ(r(s,a), V (s)) for notational simplicity. Now we introduce two assumptions on the defined operator Γ.\nAssumption 1. For an arbitrary reward function r and two arbitrary value functions\nV1(s) and V2(s), the policies π1 = Γ(r, V1) and π2 = Γ(r, V2) satisfy\n‖π1(s, ·)− π2(s, ·)‖1 ≤ ξ‖V1(·)− V2(·)‖∞,\nwhere ξ > 0 is the Lipschitz constant depending on the specific policy model. ‖ · ‖1 denotes the L1 norm, ‖ · ‖∞ denotes the infinity norm in this paper.\nAssumption 2. For an arbitrary value function V (s) and two arbitrary reward functions r(s,a) and r′(s,a), the policies π = Γ(r, V ) and π′ = Γ(r′, V ) satisfy\n‖π(s, ·)− π′(s, ·)‖1 ≤ξ‖r(s, ·)− r′(s, ·)‖∞,\nThe Gibbs policy is a popular model which was demonstrated to work well:\nπ(a|s) = exp 1 κ\n(\nr(s,a) + ∑ s′∈S p(s ′|s,a)V (s′)\n)\n) ∑\na′∈A exp 1 κ\n(\nr(s,a′) + ∑ s′∈S p(s ′|s,a′)V (s′)\n) ,\nwhere κ is the exploration parameter. We can show that the Gibbs policy satisfies Assumption 1 and Assumption 2 (the proofs are provided in Appendix A).\nThroughout this paper, we only consider stochastic policies that satisfy the above two assumptions. Let Π be the set of policies generated by the operator Γ. Then our proposed online MDP with policy iteration (OMDP-PI) algorithm is given as follows:\n• Initialize the value function V0(s) = 0, ∀s ∈ S.\n• for t = 1, . . . ,∞\n1. Observe the current state st = s.\n2. Improve the policy as πt = Γ(r̂t−1, Vt−1), where\nr̂t−1(s,a) = 1\nt− 1\nt−1 ∑ k=1 rk(s,a).\n3. Take action at = a by following πt.\n4. The reward function rt(s,a) is revealed.\n5. Update the value function according to\nVt(s) = (1− γt)Vt−1(s) + γtVπtrt (s), (1)\nwhere the step size is γt = 1/t.\nIt is well known (Sutton and Barto, 1998) that the value function satisfies\nVπr (s) = Eπ [ r(s,a)− ρr(π) + ∑\ns′∈S\np(s′|s,a)Vπr (s′) ] .\nThe above equation can be rewritten in matrix form as\nVπr = R(π)− e|S|ρr(π) + P πVπr , (2)\nwhere Vπr is the |S|-dimensional column vector whose sth element is Vπr (s). R(π) is the |S|-dimensional column vector whose sth element is ∑\na∈A π(a|s)r(s,a). P π\nis the transition matrix induced by the policy π, whose ss′th element is pπ(s|s′) = ∑\na∈A π(a|s)p(s′|s,a). e|S| is the |S|-dimensional column vector with all ones. It is well known (Sutton and Barto, 1998) that the above equation has no unique solution. Here we introduce the following constraint on the value function:\nEs∼dπ(s)[Vπr (s)] = Es∼dπ(s),a∼π [ ∞ ∑\ni=1\n(r(s,a)− ρr(π)) ] = 0.\nBy this constraint, the solution of Equ.(2) becomes unique and satisfies\nVπr = R(π)− e|S|ρr(π) + P πVπr − e|S|d⊤π Vπr , (3)\nwhere dπ is the |S|-dimensional column vector whose sth element is dπ(s).\nThen the update rule (1) can be expressed in closed form as\nVt = (1− γt)Vt−1 + γt(I|S| − P πt + e|S|d⊤πt)−1(Rt(πt)− e|S|ρrt(πt)).\nSince the stationary distribution can be obtain by the eigenvector corresponding to the unit eigenvalue, we can calculate ρrt(πt) directly. Then, Vt(s) can be obtained directly without solving an optimization problem when the state space is not large (continuous). In the following sections, we will introduce an approximation method to handle large (continuous) state space problems."
    }, {
      "heading" : "3.2 Regret analysis",
      "text" : "In this section, we provide a regret analysis for the proposed OMDP-PI algorithm. Firstly, we introduce several essential assumptions involved in the proof. Similarly to the previous works (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions.\nAssumption 3. For all π ∈ Π, there exist a positive constant τ such that two arbitrary state distributions d(s) and d′(s) satisfy\n∑ s∈S ∑ s′∈S |d(s)− d′(s)|pπ(s′|s) ≤ e−1/τ ∑ s∈S |d(s)− d′(s)|.\nAssumption 4. The reward functions satisfy\nrt(s,a) ∈ [0, 1], ∀s ∈ S, ∀a ∈ A, ∀t = 1, . . . , T.\nUnder these assumptions, the regret of the OMDP-PI algorithm for a policy set Π is\nbounded as follows:\nTheorem 1. After T time steps, the regret against the best offline time independent policy of the OMDP-PI algorithm is bounded as\nLOMDP−PI(T ) ≤ 2− e−1/τ 1− e−1/τ CξT Cv +\n(\n6τξ(2− e−1/τ ) 1− e−1/τ + 2τ 3\n)\nlnT\n+\n(\n6τξ(2− e−1/τ ) 1− e−1/τ + 2τ 3 + 2τ 3eτ+2 + 4τ\n)\n,\nwhere C = 6τ(2−Cv+ 1Cv + 1−Cv 1+Cv ), Cv = ξCπ, and Cπ is a positive constant such that for all π1, π2 ∈ Π,\n‖Vπ1r − Vπ2r ‖∞ ≤ Cπ‖π1 − π2‖1.\nThe existence of Cπ is proved in Appendix E.\nRemark. The regret bound in Theorem 1 is sublinear with respect to T when Cv < 1. However, the quality of the policy is limited when Cv is small. Since the smaller the constant Cv is, the poorer the performance of the best offline policy is. In an extreme case, where all the policies in the set Π perform equally, when Cv = 0.\nTo prove the claimed result in Theorem 1, we decompose the regret into three parts in the same way as previous works (Even-Dar et al., 2003, 2009; Abbasi-Yadkori et al., 2013; Ma et al., 2014):\nLA(T ) =\n(\nEπ∗\n[\nT ∑ t=1 rt(st,at)\n]\n− T ∑\nt=1\nρrt(π ∗)\n)\n+\n(\nT ∑ t=1 ρrt(π ∗)− T ∑ t=1 ρrt(πt)\n)\n+\n(\nT ∑ t=1 ρrt(πt)− Eπt\n[\nT ∑ t=1 rt(st,at)\n])\n.\nThe first term has been analyzed in previous works (Even-Dar et al., 2003, 2009; Ma et al., 2014), which is bounded as\nEπ∗\n[\nT ∑ t=1 rt(st,at)\n]\n− T ∑\nt=1\nρrt(π ∗) ≤ 2τ.\nBelow, we bound the second and the third terms in Lemma 2 and Lemma 3 which are proved in Appendix B and Appendix C.\nLemma 2. After T time steps, the policy sequence π1, . . . , πT given by OMDP-PI and the best offline policy π∗ ∈ Π satisfy\nT ∑ t=1 ρrt(π ∗)− T ∑ t=1 ρrt(πt) ≤ 2− e−1/τ 1− e−1/τ ( CξTCv + 6τξ lnT + 6τξ ) ,\nwhere C = 6τ(2− Cv + 1Cv + 1−Cv 1+Cv ).\nLemma 3. After T time steps, the policy sequence π1, . . . , πT given by OMDP-PI satisfies\nT ∑ t=1 ρrt(πt)− Eπt\n[\nT ∑ t=1 rt(st,at)\n]\n≤ 2τ 3 lnT + 2τ 3 + 2τ 3e(τ+2) + 2τ.\nSummarizing these bounds, we can obtain Theorem 1."
    }, {
      "heading" : "3.3 OMDP-PI algorithm with approximation",
      "text" : "When considering large (continuous) state space in online MDP problems, it is essential to apply a function approximation technique. Tsitsiklis and Roy (1999) introduced the linear function approximation of the value function for stochastic policies. A significant benefit of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999). Below we present their theoretical results for discrete (possibly continuous) state space.\nBy following the same idea as Tsitsiklis and Roy (1999), we use the linear approxi-\nmation of the value function:\nV̂(s) = θ⊤φ(s),\nwhere θ ∈ Θ is the approximation parameter, and Θ ⊂ RK is the parameter space, φ(s) is the basis function. At each time step t, the value function Vπtrt (s) is approximated as follows:\n• for i = 1, 2, . . . until convergence\n1. Observe the state si.\n2. Take action ai following πt.\n3. Observe the next state si+1 and the reward rt(si,ai)\n4. Update the approximation parameter as\nθi+1 = θi + αt(rt(si,ai)− ρ̂πtrt (i) + θ⊤i φ(si+1)− θ⊤i φ(si))\nand\nρ̂πtrt (i+ 1) = (1− αt)ρ̂πtrt (i) + αtrt(si,ai),\nwhere the step size αt satisfies\n∞ ∑ t=1 αt = ∞ and ∞ ∑ t=1 α2t < ∞.\nThe approximation parameter was proved to converge to the unique solution of the following equation (Tsitsiklis and Roy, 1999):\nP(Rt(πt)− e|S|ρrt(πt) + P πtθ⊤φ) = θ⊤φ, (4)\nwhere Rt(πt) is the |S|-dimensional column vector whose sth element is rt(s, πt) = ∑\na∈A πt(a|s)rt(s,a). P is the projection operator such that for all V ∈ R|S|,\nP(V ) = argmin V̄ ∈{θ⊤φ|θ∈RK} ‖V − V̄ ‖Dπt ,\nwhere Dπt is the diagonal matrix with the stationary distribution on the diagonal. It is clear that P is the projection from the |S|-dimensional real space to the space spanned by the basis function. The approximation sequence ρ̂πtrt (1), ρ̂ πt rt (2), . . . satisfies\nlim i→∞\nρ̂πtrt (i) → ρrt(πt), with probability 1.\nFurthermore, by using Theorem 3 in Tsitsiklis and Roy (1999), the approximation error is bounded as\n‖(I|S| − e|S|d⊤πt)θ∗⊤t φ−Vπtrt ‖Dπt ≤ 1√\n1− e−2/τ inf θ∈RK ‖(I|S| − e|S|d⊤πt)θ⊤φ−Vπtrt ‖Dπt ,\nwhere θ∗t is the unique solution to Eq.(4) at time step t. We observe that the approximation error is zero when the linear approximation model is capable of exactly recovering the true value function."
    }, {
      "heading" : "4 Online MDPs with stochastic iteration",
      "text" : "In this section, we introduce a more general framework of our proposed method for online MDPs. More specifically, we extend our algorithm to use stochastic iteration (Bertsekas and Tsitsiklis, 1996) for policy evaluation together with policy improvement to solve online MDPs.\nA general form of the stochastic iteration algorithm (Szita et al., 2002; Csáji and Monostori,\n2008) can be expressed as\nVt(s) = (1− γt(s))Vt−1(s) + γt(s) ((HtVt−1)(s) + wt(s)) , (5)\nwhere Vt ∈ R|S|, Ht : R|S| → R|S|, ∀t = 1, . . . , T is an operator on value functions, γt is the step size, and wt(s) is a noise term. Similarly to the Eq.(5), we define the update\nrule as\nVt(s) = (1− γt(s))Vt−1(s) + γt(s)((Hπtt Vt−1)(s) + wt(s)), (6)\nwhere πt = Γ(r̂t−1, Vt−1) satisfies Assumption 1 and Assumption 2. Note that the update rule (6) is different from standard stochastic iteration (5), where the operator Ht is replaced by the controlled operator Hπt which the OMDP-PI algorithm uses: Hπtt Vt−1(s) = Vπtrt (s). Additionally, we require the following assumptions.\nAssumption 5. The controlled operator Hπt is a contraction mapping with respect to the value function. This means that, for two arbitrary value functions V and V ′ and two policies π = Γ(r, V ), π′ = Γ(r, V ′), there exist a no negative constant βt < 1 such that\n‖Hπt V −Hπt V ′‖ ≤ βt‖V − V ′‖,\nand there exist a fixed function V ∗t satisfies\nHtV ∗ t = V ∗ t .\nAssumption 6. For all t = 1, . . . , T , the noisy terms wt(s) satisfy\nE[wt(s)] = 0 and E[w 2 t (s)] < Cw < ∞,\nwhere Cw is a positive constant.\nAssumption 7. The step size γt satisfies\n∞ ∑ t=1 γt = ∞ and ∞ ∑ t=1 γ2t < ∞.\nAssumption 8. The value functions sequence V1(s), . . . , VT (s) generated by Eq.(6) satisfies\nlim T→∞ ‖V ∗T −max π∈Π Vπr̂T ‖∞ = 0.\nThen we have the following theorem:\nTheorem 4. If Assumptions 5-8 hold, the value function sequence V1(s), . . . , VT (s) generated by Eq.(6) satisfies\nlim T→∞ LA(T ) = 0.\nProof. By using Theorem 20 in Csáji and Monostori (2008), we have\nlim T→∞\n‖VT − Vπ ∗ r̂T ‖∞ = 0,\nwhere π∗ = argmaxπ∈Π ρr̂T (π) is the best offline policy. Since πT+1 = Γ(r̂T , VT ) and πT , we obtain\nlim T→∞ ‖πT − π∗‖1 ≤ lim T→∞ (‖πT+1 − π∗‖1 + ‖πT+1 − πT‖1)\n≤ lim T→∞ (ξ‖VT − Vπ ∗ r̂T ‖∞ + ‖πT+1 − πT‖1) ≤ lim T→∞ (ξ‖VT − Vpi ∗ r̂T ‖∞ + ξ‖r̂T+1 − r̂T‖∞ + ξ‖VT − VT−1‖∞) = 0.\nIn the above derivation we used limT→∞ ‖VT − VT−1‖∞ = 0, which can be obtained by the update rule (6). The above result shows that the policies generated by the value sequence converges to the best offline policy as T goes to infinity. Hence, the claimed result hold by following the same line as the proof of Theorem 1.\nMany popular reinforcement learning algorithms based on value functions such as\nthe temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration. Theorem 4 shows that any stochastic iteration method that satisfies Assumptions 5-8 could be used to derive an online MDPs algorithm with sublinear regret."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we experimentally illustrate the behavior of the proposed online algorithm.\nThe goal of the grid world problem is to let an agent walk in the grid environment from the start block to the destination block. We conduct experiments on the grid world based on the Inverse Reinforcement Learning (IRL) toolkit1(Levine et al., 2011).\nIn our experiments, the grid world has 16 × 16 states with 2 actions in each state, which correspond to moving east and north. Each action has a 30% chance of moving in the other direction. The 256 states are further joined into 16 super-grids, each of which consists of 4× 4 states with the same reward.\nIn each episode, the agent tries to find a trajectory from the south-west corner to the north-east corner, with the highest cumulative rewards. In the north or east border states, the agent can only move east or north. We set T = 100000 and randomly change the rewards at episodes t = 1, 5001, 10001, ..., 95001. The proposed algorithm and the best offline algorithm (obtained using the standard MDP solver included in the IRL toolkit) are run on the grid world.\nFigure 1 shows the trajectories found by the offline policy and proposed OMDP-PI algorithm at episodes t = 25000, 50000, 75000, 100000. The darker the state is, the lower average reward it has. The direction of triangles shows the obtained policies. The states with red triangles indicate trajectories of the agent. Figure 2 shows the average regret and cumulative reward as functions of the number of episodes.\nThe results in Figure 2 show that the regret of the OMDP-PI algorithms vanishes,\n1http://graphics.stanford.edu/projects/gpirl\nsubstantiating that our theoretical analysis is valid."
    }, {
      "heading" : "6 Comparison with previous works",
      "text" : "• Expert algorithm based methods (Even-Dar et al., 2003, 2009; Neu et al., 2010a,\n2014): The basic idea of expert algorithm based methods is to put an expert algorithm in every state. By taking a close look at these algorithms, the idea does not take advantage of the state structure of the MDP problem. The OMDPPI algorithm can be easily combined with function approximation. Since it is popular to simplify the large state space problem by using the linear span of the state features, the OMDP-PI algorithm is natural to handle the large state space online MDPs.\n• Online linear optimization based methods (Zimin and Neu, 2013; Dick et al., 2014):\nBy introducing the stationary occupancy measures over state-action pairs, the online MDP problems can be solved as the online linear optimization problems. The O( √ T ) regret bounds are proved for fixed time horizon online MDPs. More specifically, the step size parameter is optimized by using the length of the time horizon T . Moreover, the stationary occupancy measures are defined over finite state and action spaces, and thus it is not clear that whether the state-action probability density function could be learned by using their propose methods without parametrization. The OMDP-PI algorithm with function approximation parameterized the state-action density through the linear model of the value function.\n• Linear programming based method (Yu et al., 2009): Our OMDP-PI is motivated\nby the Lazy-FPL algorithm, which solves a linear programming problem at the end of each phase. Instead of obtaining the best policy by the linear programming, the OMDP-PI algorithm obtains the value function of the current policy which is much more efficient than the linear programming. As we showed in the update rule, the policy evaluation could be performed in O(|S|2.3728639 + |S|2|A|) where the matrix inversion could be solved in O(|S|2.3728639) (Le Gall, 2014)."
    }, {
      "heading" : "7 Conclusion and future work",
      "text" : "As a generalization of MDP, online MDP is a promising model which can handle many online problem with guaranteed performance. In this paper, we proposed a policy iteration algorithm with a closed form update rule for online MDP problems. We showed that the proposed algorithm achieves sublinear regret for a policy set. A notable fact is that the proposed algorithm is still practical for online MDP problems with large (continuous) state space. We showed that the propose algorithm can be easily combined with function approximation with theoretical guarantee. We illustrated the performance of the proposed algorithm through grid-world experiments.\nOur future work will extend the proposed algorithm to the bandit feedback scenario, where the full information of the reward function is not revealed to the agent. Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Csáji and Monostori, 2008; Szita et al., 2002) is also an important future direction."
    }, {
      "heading" : "A Proof of Gibbs policy",
      "text" : "Proof. The KL divergence of two Gibbs policies π, π′ generated by two different value function V and V ′ is\nD(π(·|s) ‖ π′(·|s))\n= Eπ\n[\n∑ s′∈S\n1 κ p(s′|s,a) (V (s′)− V ′(s′))\n]\n+ log\n∑\na′∈A exp 1 κ\n((\nr(s,a′) + ∑ s′∈S p(s ′|s,a′)V ′(s′)\n))\n∑\na′∈A exp 1 κ\n((\nr(s,a′) + ∑ s′∈S p(s ′|s,a′)V (s′)\n))\n= Eπ\n[\n1 κ ∑\ns′∈S\np(s′|s,a) (V (s′)− V ′(s′)) ]\n+ log\n∑\na′∈A exp 1 κ (r(s,a′) +\n∑\ns′∈S p(s ′|s,a′)V (s′) +\n∑\ns′∈S p(s ′|s,a′)(V ′(s′)− V (s′)))\n∑\na′∈A exp 1 κ (r(s,a′) +\n∑\ns′∈S p(s ′|s,a′)V (s′))\n= Eπ\n[\n1 κ ∑\ns′∈S\np(s′|s,a) (V (s′)− V ′(s′)) ]\n+ log\n∑\na′∈A exp 1 κ\n((\nr(s,a′) + ∑ s′∈S p(s ′|s,a′)V (s′)\n))\nexp 1 κ\n∑\ns′∈S p(s ′|s,a′)(V ′(s′)− V (s′))\n∑\na′∈A exp 1 κ\n((\nr(s,a′) + ∑ s′∈S p(s ′|s,a′)V (s′)\n))\n= Eπ\n[\n1 κ ∑\ns′∈S\np(s′|s,a) (V (s′)− V ′(s′)) ]\n+ logEπ\n[\n1 κ ∑\ns′∈S\np(s′|s,a) (V ′(s′)− V (s′)) ]\n≤ ‖ 1 κ\n∑\ns′∈S p(s ′|s,a) (V (s′)− V ′(s′)) ‖2∞\n4 .\nFrom the Pinsker’s inequality, there is\n‖π(·|s)− π′(·|s)‖1 ≤ ‖V (s)− V ′(s)‖∞√\n2κ ,\nSimilarly, the KL divergence of two Gibbs policies π, π′ generated by two different reward function r(s,a) and r′(s,a) is\nD(π(·|s)||π′(·|s))\n= Eπ\n[\n1 k (r(s, ·)− r′(s, ·))\n]\n+ log\n∑\na′∈A exp 1 κ (r′(s,a′) +\n∑\ns′∈S p(s ′|s,a′)V (s′))\n∑\na′∈A exp 1 κ (r(s,a′) +\n∑\ns′∈S p(s ′|s,a′)V (s′))\n= Eπ\n[\n1 k (r(s, ·)− r′(s, ·))\n]\n+ log\n∑\na′∈A exp 1 κ (r(s,a′) +\n∑\ns′∈S p(s ′|s,a′)V (s′) + r′(s,a′)− r(s,a′))\n∑\na′∈A exp 1 κ (r(s,a′) +\n∑\ns′∈S p(s ′|s,a′)V (s′))\n= Eπ\n[\n1 κ (r(s, ·)− r′(s, ·))\n]\n+ logEπ\n[\n1 κ (r′(s, ·)− r(s, ·))\n]\n≤ ‖ 1 κ (r(s, ·)− r′(s, ·))‖2∞\n4 .\nFrom the Pinsker’s inequality, we can conclude the proof as\n‖π(·|s)− π′(·|s)‖1 ≤ ‖r(s, ·)− r′(s, ·)‖∞√\n2κ .\nwhich concludes the proof."
    }, {
      "heading" : "B Proof of Lemma 2",
      "text" : "Proposition 5. The value functions sequence V1(s), . . . , VT (s) generated by the Equ(1) satisfies\n‖Vπ ∗ t\nr̂t (·)− Vt(·)‖∞ ≤ CCv(t+ 1)Cv−1,\nwhere C = 6τ(2− Cv + 1Cv + 1−Cv 1+Cv ), and π∗t = argmaxπ∈Π ρr̂t(π).\nBy Proposition 3 in Ma et al. (2014) and Proposition 4.1 in Yu et al. (2009), we\nobtain the following result\nT ∑ t=1 (ρrt(π ∗)− ρrt(πt)) ≤ T ∑ t=1 (ρrt(π ∗ t )− ρrt(πt)) ≤ T ∑ t=1 2− e−1/τ 1− e−1/τ ‖π ∗ t − πt‖1\nThe result in Proposition 5 leads the following inequalities\nT ∑ t=1 (ρrt(π ∗)− ρrt(πt)) ≤ T ∑\nt=1\n2− e−1/τ 1− e−1/τ (‖π ∗ t−1 − πt‖1 + ‖π∗t − π∗t−1‖1)\n≤ T ∑\nt=1\n2− e−1/τ 1− e−1/τ ξ(‖V π∗t−1 r̂t−1 − Vt−1‖∞ + 4τ + 2 t )\n≤ 2− e −1/τ 1− e−1/τ ( Cξ Cv TCv + 6τξ lnT + 6τξ ) ."
    }, {
      "heading" : "C Proof of Lemma 3",
      "text" : "The proof is following the same line as previous works(Even-Dar et al., 2003, 2009; Ma et al., 2014), we rewrite the proof with our notations. By the definition of the expected average reward function, we have\nT ∑ t=1 ρrt(πt)− Eπt\n[\nT ∑ t−1 rt(st,at)\n]\n= T ∑\nt=1\n∑ s∈S ∑ a∈A (dπt(s)πt(a|s)− dA,t(s)πt(a|s)) rt(s,a)\n≤ T ∑\nt=1\n∑ s∈S |dπt(s)− dA,t(s)|,\nwhere dA,t(s) is the state distribution at time step t by following the policy sequence π1, . . . , πt generated by the OMDP-PI algorithm. The last line can be obtain by rt(s,a) ∈ [0, 1], ∀t = 1, . . . , T .\nFor all k = 2, . . . , t, we have following results\n‖dA,k − dπt‖1\n= ‖dA,k−1P πk − dπt−1P πt‖1 ≤ ‖dA,k−1P πk − dA,k−1P πt‖1 + ‖dA,k−1P πt − dπt−1P πt‖1 ≤ (ln (t− 1)− ln (k − 1)) + e−1/τ‖dA,k−1 − dπt−1‖1,\nRecurring the above result, we have\n‖dA,t − dπt‖1 ≤ t ∑\nk=2\n(ln (t− 1)− ln (k − 1))e−(t−k)/τ + e−t/τ‖d1 − dπt‖1\n≤ (1 + τ) ( τ 2\nt− 1 + τe −(t−τ−2)/τ\n)\n+ 2e−t/τ ,\nwhere the last inequality follows by\nt ∑ k=2 (ln (t− 1)− ln (k − 1))e−(t−k)/τ = ∫ t\n2\n(ln (t− 1)− ln (k − 1))e−(t−k)/τdk + ln (t− 1)e−(t−2)/τ\n= τ\n∫ t\n2\n(ln (t− 1)− ln (k − 1))de −(t−k)/τ\ndk dk + ln (t− 1)e−(t−2)/τ\n≤ τ ∫ t\n2\ne−(t−k)/τ\nk − 1 dk = τ 2\n∫ t\n2\n1 k − 1 de−(t−k)/τ dk dk\n≤ τ 2\nt− 1 + τ 2\n∫ t\n2\ne−(t−k)/τ (k − 1)2 dk\n= τ 2\nt− 1 + τ 2\n∫ t\nτ+2\ne−(t−k)/τ (k − 1)2 dk + ∫ τ+2\n2\ne−(t−k)/τ (k − 1)2 dk\n≤ τ 2 t− 1 + τ 2 τ + 1 ∫ t\n2\ne−(t−k)/τ k − 1 dk + ∫ τ+2\n2\ne−(t−k)/τ (k − 1)2 dk.\nHence, we have\n∫ t\n2\ne−(t−k)/τ k − 1 dk ≤ ( 1 + 1 τ )( τ 2 t− 1 + τe −(t−τ−2)/τ ) .\nThe claimed result in Lemma 3 can be obtained as\nT ∑ t=1 ‖dA,t − dπt‖1 ≤ 2τ 3 lnT + 2τ 3 + 2τ 3e(τ+2) + 2τ."
    }, {
      "heading" : "D Proof of Proposition 5",
      "text" : "Proposition 6. For arbitrary reward function r(s,a), the corresponding value functions induced by two arbitrary policy π1 and π2 satisfy\n‖Vπ1r − Vπ2r ‖∞ ≤ Cπ‖π1 − π2‖1.\nwhere Cπ is a positive constant.\nLet us define an auxiliary sequence of functions Vπ ∗ t\nr̂t (s), t = 1, . . . , T which is\ndefined as\nVπ ∗ t\nr̂t (s) = Eπ∗t\n[\n∞ ∑ i=1 (r̂t(s,a)− ρr̂t(π∗t )) ] .\nIn above definition, π∗t is the optimal policy which satisfies\nπ∗t = argmax π∈Π ρr̂t(π),\nand for all s ∈ S, there is\nVπ ∗ t\nr̂t (s) ≥ Vπr̂t(s), ∀π ∈ Π.\nIt is simple to verify that the value function is linear with respect to the reward function,\ni.e., Vπr̂t(s) = 1t ∑t k=1 Vπrt(s). Hence, we can rewrite the sequence as\nVπ ∗ t+1 r̂t+1 (s) =Vπ ∗ t r̂t (s) +\n1\nt + 1\n(\nt+1 ∑ k=1 Vπ ∗ t+1 rk (s)− t+ 1 t t ∑ k=1 Vπ∗trk (s) )\n=Vπ ∗ t\nr̂t (s) +\n1\nt + 1 Vπ\n∗\nt+1 rt+1 (s)−\n1\nt(t+ 1)\nt ∑ k=1 Vπ∗trk (s) + 1 t + 1\n(\nt ∑ k=1 Vπ ∗ t+1 rk (s)− t ∑ k=1 Vπ∗trk (s) )\n=(1− 1 t+ 1 )Vπ ∗ t r̂t (s) + 1 t+ 1 Vπ ∗ t+1 rt+1 (s) + t t+ 1 ( Vπ ∗ t+1 r̂t (s)− Vπ ∗ t r̂t (s) ) ≤(1− 1 t+ 1 )Vπ ∗ t r̂t (s) + 1 t+ 1 Vπ ∗ t+1 rt+1 (s),\nwhere the last inequality can be obtained by the fact that π∗t is the optimal policy satisfies\nVπ ∗ t r̂t (s) ≥ Vπ\n∗ t+1\nr̂t (s), ∀s ∈ S.\nOn the other hand, we can derive the lower bound as\nVπ ∗ t+1 r̂t+1 (s) =Vπ ∗ t r̂t (s) +\n1\nt + 1\n(\nt+1 ∑ k=1 Vπ ∗ t+1 rk (s)− t+ 1 t t ∑ k=1 Vπ∗trk (s) )\n=Vπ ∗ t\nr̂t (s) +\n1\nt + 1\n(\nt+1 ∑ k=1 Vπ ∗ t+1 rk (s)− t+ 1 t t+1 ∑ k=1 Vπ∗trk (s) ) + 1 t Vπ∗trt+1(s)\n=Vπ ∗ t\nr̂t (s) +\n1\nt + 1\n(\nt+1 ∑ k=1 Vπ ∗ t+1 rk (s)− t+1 ∑ k=1 Vπ∗trk (s) ) − 1 t(t + 1) t+1 ∑ k=1 Vπ∗trk (s) + 1 t Vπ∗trt+1(s)\n=Vπ ∗ t r̂t (s) + (Vπ\n∗ t+1\nr̂t+1 (s)− Vπ ∗ t r̂t+1 (s))− 1 t + 1 Vπ ∗ t r̂t (s) + 1 t + 1 Vπ∗trt+1(s)\n≥(1− 1 t+ 1 )Vπ ∗ t r̂t (s) + 1 t+ 1 Vπ∗trt+1(s),\nwhere the last inequality comes from the fact π∗t+1 is the optimal policy satisfies\nVπ ∗ t+1 r̂t+1 (s) ≥ Vπ ∗ t r̂t+1 (s), ∀s ∈ S.\nThen, we can obtain the following result\n|Vπ ∗ t+1\nr̂t+1 (s)− Vt+1(s)| ≤ (1−\n1\nt+ 1 )|Vπ ∗ t r̂t (s)− Vt(s)|+\n1\nt+ 1 ∆t+1. (7)\nIn above inequality, ∆t+1 = max{|V π∗t+1 rt+1 (s)−V πt+1rt+1 (s)|, |V π∗t rt+1(s)−V πt+1rt+1 (s)|}, which satisfies\n∆t+1 ≤Cπ max{‖π∗t+1 − πt+1‖1, ‖π∗t − πt+1‖1}\n≤Cπ(‖π∗t − πt+1‖1 + ‖π∗t − π∗t+1‖1) ≤Cv‖Vπ ∗ t r̂t − Vt‖∞ + (4τ + 2)Cv t+ 1 .\nThe first term of the last inequality can be obtain by setting Cv = ξCπ. The second part follows by the upper bound and the lower bound of Vπ ∗ t+1\nr̂t . Next we show the bound of\n‖Vπ ∗ t\nr̂t − Vt‖∞ by recurring Equ.(7)\n‖Vπ ∗ t r̂t − Vt‖∞ ≤ (4τ + 2)Cv t2\n+ t−1 ∑\nk=1\n(4τ + 2)Cv k2\nt ∏\nm=k+1\n(\n1− 1− Cv m\n)\n.\nLet us take the logarithm of ∏t\nm=k+1\n(\n1− 1−Cv m\n)\n, there is\nln t ∏\nm=k+1\n(\n1− 1− Cv m\n)\n= t ∑\nm=k+1\n(ln (m− 1 + Cv)− lnm)\n≤ t ∑\nm=k+1\n−1 + Cv m\n≤ −(1− Cv) ∫ t+1\nk+1\n1\nm dm = −(1− Cv) ln\nt + 1 k + 1 ,\nwhere the first inequality holds since the logarithm function is concave. Thus we derive\nthe bound as\n‖Vπ ∗ t\nr̂t − Vt‖∞ ≤(4τ + 2)Cv\nt ∑ k=1 1 k2 (k + 1)1−Cv (t+ 1)1−Cv\n≤ (4τ + 2)Cv (t+ 1)1−Cv\nt ∑ k=1 1 k2 ( k1−Cv + (1− Cv)k−Cv )\n≤ (4τ + 2)Cv (t+ 1)1−Cv\n[\n2− Cv + ∫ t\n1\nk−Cv−1dk + (1− Cv) ∫ t\n1\nk−Cv−2dk\n]\n= (4τ + 2)Cv (t+ 1)1−Cv\n[\n2− Cv + 1 Cv − t\n−Cv Cv + 1− Cv 1 + Cv − 1− Cv 1 + Cv t−Cv−1 ]\n≤CCv(t+ 1)Cv−1,\nwhere C = 6τ(2−Cv + 1Cv + 1−Cv 1+Cv ). In above results, the second inequality follows by Taylor’s theorem."
    }, {
      "heading" : "E Proof of Proposition 6",
      "text" : "Let us define the operator T πVπr (s) = Eπ [ r(s,a)− ρr(π) + ∑ s′∈S p(s ′|s,a)Vπr (s′) ] . we can obtain\nVπ1r (s)− Vπ2r (s)\n= T π1Vπ1r (s)− T π2Vπ2r (s) = (T π1Vπ1r (s)− T π2Vπ1r (s)) + (T π2Vπ1r (s)− T π2Vπ2r (s)) .\nBy the definition of the operator, we rewrite the first term as\nT π1Vπ1r (s)− T π2Vπ1r (s)\n= Eπ1\n[\nr(s,a)− ρr(π1) + ∑\ns′∈S\np(s′|s,a)Vπ1r (s′) ]\n− Eπ2\n[\nr(s,a)− ρr(π2) + ∑\ns′∈S\np(s′|s,a)Vπ1r (s′) ]\n= (Eπ1 [Q π1 r (s,a)]− Eπ2 [Qπ1r (s,a)]) + (ρr(π2)− ρr(π1)) = (Qπ1r (s, π1)−Qπ1r (s, π2)) + Es∼dπ2 (s)[Q π1 r (s, π2)−Qπ1r (s, π1)].\nThe second term can be expressed as\nT π2Vπ1r (s)− T π2Vπ2r (s)\nEπ2\n[\nr(s,a)− ρr(π2) + ∑\ns′∈S\np(s′|s,a)Vπ1r (s′)− r(s,a) + ρr(π2)− ∑\ns′∈S\np(s′|s,a)Vπ2r (s) ]\n= Es′∼pπ2 (s′|s)[Vπ1r (s′)− Vπ2r (s′)].\nBy summing up the above results, we obtain\nVπ1r (s)− Vπ2r (s)\n= (Qπ1r (s, π1)−Qπ1r (s, π2)) + Es∼dπ2 (s)[Q π1 r (s, π2)−Qπ1r (s, π1)]\n+ Es′∼pπ2(s′|s)[Vπ1r (s′)− Vπ2r (s′)].\nIn matrix notation, there is\nVπ1r − Vπ2r = (Qπ1,π1r −Qπ1,π2r )− e|S|d⊤π2(Q π1,π1 r −Qπ1,π2r ) + P π2(Vπ1r − Vπ2r ),\nwhere Vπr and Qπ,π ′ r are the length |S| vectors whose sth element isVπr (s) and Qπr (s, π′), respectively. e|S| denotes the length |S| vector with all elements equal to 1. dπ is the\n|S|-dimensional vector whose sth element is dπ(s). P π is defined as the transition matrix induced by the policy π and the transition p(s′|s,a). Thus, we obtain\n(I|S| − P π2)(Vπ1r − Vπ2r ) = (I|S| − e|S|d⊤π2)(Q π1,π1 r −Qπ1,π2r ).\nIt is known that the Bellman equation with average reward function has no unique solution. However, the unique value function satisfies d⊤π Vπr = 0. Hence, we add this condition to the above equation as\n(I|S| − P π2)(Vπ1r − Vπ2r )\n= (I|S| − e|S|d⊤π2)(Q π1,π1 r −Qπ1,π2r )− e|S|d⊤π1V π1 r + e|S|d ⊤ π2 Vπ2r = (I|S| − e|S|d⊤π2)(Q π1,π1 r −Qπ1,π2r )− e|S|d⊤π2(V π1 r − Vπ2r )− (e|S|d⊤π1 − e|S|d ⊤ π2)V π1 r\nThen, by rearranging the above result:\n(I|S| − P π2 + e|S|d⊤π2)(V π1 r − Vπ2r ) = (Qπ1,π1r −Qπ1,π2r )− (P π1sa − P π2sa )Qπ1r ,\nwhere P πsa is the |S| × |A| matrix whose (s,a)th element is dπ(s)π(a|s). Using Proposition 12 in Ma et al. (2014), we obtain\n‖Vπ1r − Vπ2r ‖∞ ≤ 2− 2e−1/τ 1− e−1/τ ‖(I|S| − P π2 + e|S|d ⊤ π2 )−1Qπ1r ‖∞‖π1 − π2‖1,\nwhich concludes the proof by setting maxπ∈Π 2−2e −1/τ 1−e−1/τ ‖(I|S|−P π +e|S|d⊤π )−1Qπr ‖∞ ≤ Cπ."
    } ],
    "references" : [ {
      "title" : "Online learning in Markov decision processes with adversarially chosen transition probability distributions",
      "author" : [ "Y. Abbasi-Yadkori", "P. Bartlett", "V. Kanade", "Y. Seldin", "C. Szepesvari" ],
      "venue" : "Advances in Neural Information Processing Systems 26, pages 2508–2516.",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? 2013",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2013
    }, {
      "title" : "Neuro-Dynamic Programming",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific, 1st edition.",
      "citeRegEx" : "Bertsekas and Tsitsiklis,? 1996",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis",
      "year" : 1996
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Cambridge University Press, New York, NY, USA.",
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? 2006",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2006
    }, {
      "title" : "Value function based reinforcement learning in changing Markovian environments",
      "author" : [ "B.C. Csáji", "L. Monostori" ],
      "venue" : "Journal of Machine Learning Research, 9:1679– 1709.",
      "citeRegEx" : "Csáji and Monostori,? 2008",
      "shortCiteRegEx" : "Csáji and Monostori",
      "year" : 2008
    }, {
      "title" : "Online learning in Markov decision processes with changing cost sequences",
      "author" : [ "T. Dick", "A. György", "C. Szepesvári" ],
      "venue" : "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 512–520.",
      "citeRegEx" : "Dick et al\\.,? 2014",
      "shortCiteRegEx" : "Dick et al\\.",
      "year" : 2014
    }, {
      "title" : "Experts in a Markov decision process",
      "author" : [ "E. Even-Dar", "S.M. Kakade", "Y. Mansour" ],
      "venue" : "Advances in Neural Information Processing System, pages 401–408.",
      "citeRegEx" : "Even.Dar et al\\.,? 2003",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2003
    }, {
      "title" : "Online Markov decision processes",
      "author" : [ "E. Even-Dar", "S.M. Kakade", "Y. Mansour" ],
      "venue" : "Mathematics of Operations Research, 34(3):726–736. 21",
      "citeRegEx" : "Even.Dar et al\\.,? 2009",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2009
    }, {
      "title" : "Powers of tensors and fast matrix multiplication",
      "author" : [ "F. Le Gall" ],
      "venue" : "Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation, ISSAC ’14, pages 296–303, New York, NY, USA. ACM.",
      "citeRegEx" : "Gall,? 2014",
      "shortCiteRegEx" : "Gall",
      "year" : 2014
    }, {
      "title" : "Nonlinear inverse reinforcement learning with Gaussian processes",
      "author" : [ "S. Levine", "Z. Popovic", "V. Koltun" ],
      "venue" : "Advances in Neural Information Processing Systems 24, pages 19–27.",
      "citeRegEx" : "Levine et al\\.,? 2011",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2011
    }, {
      "title" : "An online policy gradient algorithm for Markov decision processes with continuous states and actions",
      "author" : [ "Y. Ma", "T. Zhao", "K. Hatano", "M. Sugiyama" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II, pages 354–369.",
      "citeRegEx" : "Ma et al\\.,? 2014",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2014
    }, {
      "title" : "Online Markov decision processes under bandit feedback",
      "author" : [ "G. Neu", "G. András", "C. Szepesvári", "A. Antos" ],
      "venue" : "IEEE Transactions on Automatic Control, 59(3):676– 691.",
      "citeRegEx" : "Neu et al\\.,? 2014",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2014
    }, {
      "title" : "The online loop-free stochastic shortest-path problem",
      "author" : [ "G. Neu", "A. György", "C. Szepesvári" ],
      "venue" : "The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 231–243.",
      "citeRegEx" : "Neu et al\\.,? 2010a",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2010
    }, {
      "title" : "The adversarial stochastic shortest path problem with unknown transition probabilities",
      "author" : [ "G. Neu", "A. György", "C. Szepesvári" ],
      "venue" : "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12), volume 22, pages 805–813. 22",
      "citeRegEx" : "Neu et al\\.,? 2012",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2012
    }, {
      "title" : "Online Markov decision processes under bandit feedback",
      "author" : [ "G. Neu", "A. Gyrgy", "C. Szepesvri", "A. Antos" ],
      "venue" : "Advances in Neural Information Processing System,NIPS, pages 1804–1812.",
      "citeRegEx" : "Neu et al\\.,? 2010b",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine learning, pages 9–44.",
      "citeRegEx" : "Sutton,? 1988",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "MDPs: Learning in varying environments",
      "author" : [ "I. Szita", "B. Takács", "A. Lörincz" ],
      "venue" : "Journal of Machine Learning Research, 3:145–174.",
      "citeRegEx" : "Szita et al\\.,? 2002",
      "shortCiteRegEx" : "Szita et al\\.",
      "year" : 2002
    }, {
      "title" : "Average cost temporal-difference learning",
      "author" : [ "J.N. Tsitsiklis", "B.V. Roy" ],
      "venue" : "Automatica, 35:1799–1808.",
      "citeRegEx" : "Tsitsiklis and Roy,? 1999",
      "shortCiteRegEx" : "Tsitsiklis and Roy",
      "year" : 1999
    }, {
      "title" : "Markov decision processes with arbitrary reward processes",
      "author" : [ "J.Y. Yu", "S. Mannor", "N. Shimkin" ],
      "venue" : "Mathematics of Operations Research, 34(3):737–757.",
      "citeRegEx" : "Yu et al\\.,? 2009",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2009
    }, {
      "title" : "Online learning in episodic Markovian decision processes by relative entropy policy search",
      "author" : [ "A. Zimin", "G. Neu" ],
      "venue" : "Advances in Neural Information Processing Systems 26, pages 1583–1591. 23",
      "citeRegEx" : "Zimin and Neu,? 2013",
      "shortCiteRegEx" : "Zimin and Neu",
      "year" : 2013
    }, {
      "title" : "Proof of Lemma 3 The proof is following the same line as previous works(Even-Dar",
      "author" : [ ],
      "venue" : "Ma et al.,",
      "citeRegEx" : "C,? \\Q2003\\E",
      "shortCiteRegEx" : "C",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "1 Introduction A generalization of the classical shortest path problem in graph theory, called the stochastic shortest path problem (Bertsekas and Tsitsiklis, 1996), considers a probability distribution over all possible next nodes.",
      "startOffset" : 132,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction A generalization of the classical shortest path problem in graph theory, called the stochastic shortest path problem (Bertsekas and Tsitsiklis, 1996), considers a probability distribution over all possible next nodes. A standard way to solve the stochastic shortest path problem is to formulate it as a Markov decision process (MDP) and find a policy that maximizes the cumulative reward over the path. In the MDP problem, the agent chooses the best action according to the current state and moves to the next state following the Markovian dynamics. A fixed reward function assigns a reward value to each state-action pair. A generalization of MDP, called the online MDP, considers the situation where the reward function changes over time. At each time step, the learning agent decides the strategy of choosing actions by using the knowledge of past reward functions. Then, the current reward function which is chosen by the environment is revealed to the agent after observing its behavior. The goal of online MDP is to minimize the regret against the best offline policy, which is the optimal fixed policy in hindsight. We expect that the regret vanishes as the time step T tends to infinity, implying that the agent can behave as well as the best offline policy asymptotically. Many online problems can be solved as online MDP problems. By setting the optimization variables as the state, the online MDP algorithm chooses the change of variables (action) which performs reasonably well in a non-stationary environment. Even-Dar et al. (2009) presented several typical online problems which can be formulated as online MDP perfectly, e.",
      "startOffset" : 133,
      "endOffset" : 1561
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, the MDP-E algorithm was proved to achieve regret O(L √ T log |A|) for online MDP problems with L-layered state space (Neu et al., 2010a).",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "Another online MDP algorithm called the lazy follow-the-perturbed-leader (lazyFPL) (Yu et al., 2009) follows the main idea of the FPL algorithm which solves the Bellman equation using the average reward function.",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "Similarly to lazy-FPL, the online relative entropy policy search (O-REPS) algorithm (Zimin and Neu, 2013) also requires to solve an optimization problem at the end of each time step.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "By introducing the stationary occupation measure, Dick et al. (2014) proposed the mirror descent with approximation projections algorithm, which formulate the online MDP problem as online linear opti3",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "Yu et al. (2009), Abbasi-Yadkori et al.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "(2009), Abbasi-Yadkori et al. (2013), and Neu et al.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "(2009), Abbasi-Yadkori et al. (2013), and Neu et al. (2012) considered even more challenging online MDP problems under unknown or changing transition dynamics.",
      "startOffset" : 8,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "(2009), Abbasi-Yadkori et al. (2013), and Neu et al. (2012) considered even more challenging online MDP problems under unknown or changing transition dynamics. Recently, Ma et al. (2014) proposed the online policy gradient (OPG) algorithm for online MDP problems with continuous state and action spaces, and it was proved to achieve regret O( √ T ) under the concavity assumption about the expected average reward function.",
      "startOffset" : 8,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "Here, in the same way as standard online learning literature (Cesa-Bianchi and Lugosi, 2006), we consider the regret against the best offline time independent policy π in the policy set Π: LA(T ) = Rπ∗(T )−RA(T ).",
      "startOffset" : 61,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "Note that the regret we consider here is different from previous literature (Even-Dar et al., 2003, 2009; Zimin and Neu, 2013; Dick et al., 2014): we compare the performance of algorithm A against the best offline policy within a specific policy set Π.",
      "startOffset" : 76,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "Note that the regret we consider here is different from previous literature (Even-Dar et al., 2003, 2009; Zimin and Neu, 2013; Dick et al., 2014): we compare the performance of algorithm A against the best offline policy within a specific policy set Π.",
      "startOffset" : 76,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "Since the optimal value function leads to the optimal policy, MDP is often solved by deriving the optimal value function (Sutton and Barto, 1998).",
      "startOffset" : 121,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "For this reason, in this paper we only consider the stochastic policy, since the convergence guarantee is provided (Tsitsiklis and Roy, 1999).",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "The key idea of the proposed algorithm is motivated by the Lazy FPL algorithm by Yu et al. (2009), which performs linear programming to obtain the ‘leader’ policy.",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "The key idea of the proposed algorithm is motivated by the Lazy FPL algorithm by Yu et al. (2009), which performs linear programming to obtain the ‘leader’ policy. As Yu et al. (2009) pointed out, solving linear programming may not be appropriate for problems with large (continuous) state space.",
      "startOffset" : 81,
      "endOffset" : 184
    }, {
      "referenceID" : 15,
      "context" : "It is well known (Sutton and Barto, 1998) that the value function satisfies V r (s) = Eπ [",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "It is well known (Sutton and Barto, 1998) that the above equation has no unique solution.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 18,
      "context" : "Similarly to the previous works (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions.",
      "startOffset" : 32,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "Similarly to the previous works (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions.",
      "startOffset" : 32,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "To prove the claimed result in Theorem 1, we decompose the regret into three parts in the same way as previous works (Even-Dar et al., 2003, 2009; Abbasi-Yadkori et al., 2013; Ma et al., 2014): LA(T ) = (",
      "startOffset" : 117,
      "endOffset" : 192
    }, {
      "referenceID" : 9,
      "context" : "To prove the claimed result in Theorem 1, we decompose the regret into three parts in the same way as previous works (Even-Dar et al., 2003, 2009; Abbasi-Yadkori et al., 2013; Ma et al., 2014): LA(T ) = (",
      "startOffset" : 117,
      "endOffset" : 192
    }, {
      "referenceID" : 9,
      "context" : "The first term has been analyzed in previous works (Even-Dar et al., 2003, 2009; Ma et al., 2014), which is bounded as Eπ∗ [ T ∑",
      "startOffset" : 51,
      "endOffset" : 97
    }, {
      "referenceID" : 17,
      "context" : "A significant benefit of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999).",
      "startOffset" : 96,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "Tsitsiklis and Roy (1999) introduced the linear function approximation of the value function for stochastic policies.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "Tsitsiklis and Roy (1999) introduced the linear function approximation of the value function for stochastic policies. A significant benefit of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999). Below we present their theoretical results for discrete (possibly continuous) state space. By following the same idea as Tsitsiklis and Roy (1999), we use the linear approximation of the value function: V̂(s) = θφ(s), 13",
      "startOffset" : 0,
      "endOffset" : 389
    }, {
      "referenceID" : 17,
      "context" : "The approximation parameter was proved to converge to the unique solution of the following equation (Tsitsiklis and Roy, 1999): P(Rt(πt)− e|S|ρrt(πt) + P tθφ) = θφ, (4) where Rt(πt) is the |S|-dimensional column vector whose sth element is rt(s, πt) =",
      "startOffset" : 100,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "Furthermore, by using Theorem 3 in Tsitsiklis and Roy (1999), the approximation error is bounded as ‖(I|S| − e|S|d⊤πt)θ∗⊤ t φ−Vt rt ‖Dπt ≤ 1 √ 1− e−2/τ inf θ∈RK ‖(I|S| − e|S|d⊤πt)θ⊤φ−Vπ rt ‖Dπt , where θ t is the unique solution to Eq.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "More specifically, we extend our algorithm to use stochastic iteration (Bertsekas and Tsitsiklis, 1996) for policy evaluation together with policy improvement to solve online MDPs.",
      "startOffset" : 71,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "A general form of the stochastic iteration algorithm (Szita et al., 2002; Csáji and Monostori, 2008) can be expressed as Vt(s) = (1− γt(s))Vt−1(s) + γt(s) ((HtVt−1)(s) + wt(s)) , (5) where Vt ∈ R, Ht : R → R, ∀t = 1, .",
      "startOffset" : 53,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "A general form of the stochastic iteration algorithm (Szita et al., 2002; Csáji and Monostori, 2008) can be expressed as Vt(s) = (1− γt(s))Vt−1(s) + γt(s) ((HtVt−1)(s) + wt(s)) , (5) where Vt ∈ R, Ht : R → R, ∀t = 1, .",
      "startOffset" : 53,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.",
      "startOffset" : 128,
      "endOffset" : 198
    }, {
      "referenceID" : 15,
      "context" : "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.",
      "startOffset" : 128,
      "endOffset" : 198
    }, {
      "referenceID" : 14,
      "context" : "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.",
      "startOffset" : 128,
      "endOffset" : 198
    }, {
      "referenceID" : 1,
      "context" : "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.",
      "startOffset" : 223,
      "endOffset" : 279
    }, {
      "referenceID" : 15,
      "context" : "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.",
      "startOffset" : 223,
      "endOffset" : 279
    }, {
      "referenceID" : 2,
      "context" : "By using Theorem 20 in Csáji and Monostori (2008), we have lim T→∞ ‖VT − V ∗ r̂T ‖∞ = 0, where π = argmaxπ∈Π ρr̂T (π) is the best offline policy.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "We conduct experiments on the grid world based on the Inverse Reinforcement Learning (IRL) toolkit1(Levine et al., 2011).",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "• Online linear optimization based methods (Zimin and Neu, 2013; Dick et al., 2014): By introducing the stationary occupancy measures over state-action pairs, the online MDP problems can be solved as the online linear optimization problems.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "• Online linear optimization based methods (Zimin and Neu, 2013; Dick et al., 2014): By introducing the stationary occupancy measures over state-action pairs, the online MDP problems can be solved as the online linear optimization problems.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 18,
      "context" : "• Linear programming based method (Yu et al., 2009): Our OMDP-PI is motivated 19",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Csáji and Monostori, 2008; Szita et al.",
      "startOffset" : 73,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Csáji and Monostori, 2008; Szita et al.",
      "startOffset" : 73,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Csáji and Monostori, 2008; Szita et al., 2002) is also an important future direction.",
      "startOffset" : 164,
      "endOffset" : 211
    }, {
      "referenceID" : 16,
      "context" : "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Csáji and Monostori, 2008; Szita et al., 2002) is also an important future direction.",
      "startOffset" : 164,
      "endOffset" : 211
    } ],
    "year" : 2015,
    "abstractText" : "The online Markov decision process (MDP) is a generalization of the classical Markov decision process that incorporates changing reward functions. In this paper, we propose practical online MDP algorithms with policy iteration and theoretically establish a sublinear regret bound. A notable advantage of the proposed algorithm is that it can be easily combined with function approximation, and thus large and possibly continuous state spaces can be efficiently handled. Through experiments, we demonstrate the usefulness of the proposed algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}