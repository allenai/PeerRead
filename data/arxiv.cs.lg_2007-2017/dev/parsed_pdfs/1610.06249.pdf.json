{
  "name" : "1610.06249.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multilevel Anomaly Detection for Mixed Data",
    "authors" : [ "Kien Do", "Truyen Tran", "Svetha Venkatesh" ],
    "emails" : [ "dkdo@deakin.edu.au", "truyen.tran@deakin.edu.au", "svetha.venkatesh@deakin.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Anomalies are those deviating from the norm. Unsupervised anomaly detection often translates to identifying low density regions. Major problems arise when data is high-dimensional and mixed of discrete and continuous attributes. We propose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an ensemble method that estimates the sparse regions across multiple levels of abstraction of mixed data. The hypothesis is for domains where multiple data abstractions exist, a data point may be anomalous with respect to the raw representation or more abstract representations. To this end, our method sequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying depths. Each DBN is an energy-based detector at a predefined abstraction level. At the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann Machine that models the density of mixed data. Predictions across the ensemble are finally combined via rank aggregation. The proposed MIXMAD is evaluated on high-dimensional real-world datasets of different characteristics. The results demonstrate that for anomaly detection, (a) multilevel abstraction of highdimensional and mixed data is a sensible strategy, and (b) empirically, MIXMAD is superior to popular unsupervised detection methods for both homogeneous and mixed data."
    }, {
      "heading" : "1 Introduction",
      "text" : "A vital intelligent function and survival skill for living organism is detecting anomalies, that is, those deviating from the norm. Except for a few deadly instances, we learn to detect anomalies by observing and exploring, without supervision. Unsupervised anomaly detection does not assume any domain knowledge about abnormality, and hence it is cheap and pervasive. A disciplined approach is to identify instances lying in low density regions [11]. However, estimating density in high-dimensional and mixed-type settings is difficult [12, 22, 35].\nUnder these conditions, existing non-parametric methods that define a data cube to estimate the relative frequency of data are likely to fail. It is because the number of cube\n∗Centre for Pattern Recognition and Data Analytics, Deakin University, Geelong, Australia. †dkdo@deakin.edu.au ‡truyen.tran@deakin.edu.au §svetha.venkatesh@deakin.edu.au\ngrows exponentially with the data dimensions, thus a cube with only a few or no observed data points needs not be a low density region. An alternative is to use distance to k nearest neighbors, assuming that the larger the distance, the less dense the region [5]. But distance is neither well-defined under mixed types nor meaningful in a high-dimensional space [1, 35]. Solving both challenges is largely missing in the literature as existing work targets either single-type high-dimensions (e.g., see [35] for a recent review) or mixed data (e.g., see [12, 22] for latest attempts).\nTo tackle the challenges jointly, we advocate learning data representation through abstraction, a strategy that (a) transforms mixed-data into a homogeneous representation [27], and (b) represents the multilevel structure of data [8]. The hypothesis is a data point may be anomalous with respect to either the raw representation or higher abstractions. We call it the Multilevel Anomaly Detection (MAD) hypothesis. For example, an image may be anomalous not because its pixel distribution differs from the rest, but because its embedded concepts deviate significantly from the norm. Another benefit of learning higher-level data representation is that through abstraction, regularities and consistencies may be readily revealed, making it easier to detect deviations. To test the MAD hypothesis, we present a new densitybased method known as MIXMAD, which stands for MIXed data Multilevel Anomaly Detection. MIXMAD generalizes the recent work in [12] for mixed data by building multiple abstractions of data. For data abstraction, we leverage recent advances in unsupervised deep learning to abstract the data into multilevel low-dimensional representations [8].\nWhile deep learning has revolutionized supervised prediction [21], its application to unsupervised anomaly detection is very limited [33]. With MIXMAD we build a sequence of Deep Belief Nets (DBNs) [17] of increasing depths. Each DBN is a layered model that allows multiple levels of data abstraction. The top layer of the DBN is used as an anomaly detector. A key observation to DBN-based anomaly detection is that the perfect density estimation is often not needed in practice. All we need is a ranking of data instances by increasing order of abnormality. An appropriate anomaly scoring function is free-energy of abstracted data, which equals negative-log of data density up to an additive constant.\nMIXMAD offers the following procedure to test the MAD hypothesis: First apply multiple layered abstractions to the data, and then estimate the anomalies at each level. Finally, the anomaly score is aggregated across levels. While this\n1\nar X\niv :1\n61 0.\n06 24\n9v 1\n[ cs\n.L G\n] 2\n0 O\nct 2\n01 6\nbears some similarity with the recent ensemble approaches [2, 4], the key difference is MAD relies on multiple data abstractions, not data resampling or random subspaces which are still on the original data level. In MIXMAD, as the depth increases and the data representation is more abstract, the energy landscape gets smoother, and thus it may detect different anomalies. For reaching anomaly consensus across depth-varying DBNs, MIXMAD employs a simple yet flexible rank aggregation method based on p-norm.\nWe validate MIXMAD through an extensive set of experiments against well-known shallow baselines, which include the classic methods (PCA, GMM, RBM and one-class SVM), as well as state-of-the-art mixed-type methods (ODMAD [20], BMM [9], GLM-t [22] and Mv.RBM [12]). The experiments demonstrate that (a) multilevel abstraction of data is important in anomaly detection, and (b) MIXMAD is superior to popular unsupervised detection methods for both homogeneous and mixed data.\nIn summary, we claim the following contributions:\n• Stating the hypothesis of Multilevel Anomaly Detection (MAD) that argues for reaching agreement across multiple abstractions of data.\n• Deriving MIXMAD, an efficient ensemble algorithm to test MAD. MIXMAD builds a sequence of Deep Belief Nets, each of which is an anomaly detector. All detectors are then combined using a flexible p-norm aggregation that allows tuning along the conservative/optimistic axis.\n• A comprehensive evaluation of MIXMAD on highdimensional datasets against a large suite of competing methods."
    }, {
      "heading" : "2 Background",
      "text" : "Anomaly detection on high-dimensional and mixed data has attracted a wide range of methods, but the two challenges are tackled independently rather than jointly as in this paper. High-dimensional data suffers from ‘curse of dimensionality’ also known more concretely as ‘distance concentration effect’, irrelevant attributes and redundant attributes, which together cause failure of low-dimensional techniques [1]. Popular anomaly detection approaches targeting high-dimensions include feature selection, dimensionality reduction (such as using PCA) and subspace analysis (readers are referred to [35] for a recent survey and in-depth discussion).\nMixed data has received a moderate attention. A method called LOADED [15] defines the score on the discrete subspace and combines with a correlation matrix in the continuous subspace. A related method called ODMAD [20] opts for stage-wise detection in each subspace. A different strategy is employed in [9], where scores for discrete and continuous spaces are computed separately, then combined using a\nmixture model. The work in [34] introduces Pattern-based Anomaly Detection (POD), where a pattern consists of a discrete attribute and all continuous attributes. Scores of all patterns are then combined. Methods with joint distribution of all attributes are introduced recently in [12, 22] using latent variables to link all types together. We adapt the work in [12] to represent mixed data into a homogeneous form using Mixed-variate Restricted Boltzmann Machines [27]. The homogeneous representation can then be abstracted into higher semantic levels in order to test the MAD hypothesis stated earlier.\nThe recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32]. A common strategy is to use unsupervised deep networks to detect features, which are then fed into well-established detection algorithms [32]. Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26]. A problem with this reconstruction error is that the final model still operates on raw data, which can be noisy and high-dimensional. A more fundamental problem is that reconstruction error does not reflect data density [19]. A better approach is to use deep networks to estimate the energy directly [12, 13, 33]. This resembles in principle our free-energy function presented in Sec. 3.3, but differs in the network construction procedure. It does not reflect the multilevel abstraction hypothesis we are pursuing.\n3 MIXMAD: MIXed data Multilevel Anomaly Detection\nWe present MIXMAD, an ensemble algorithm for MIXed data Multilevel Anomaly Detection. Given a data instance x we estimate the unnormalized density, which is the true density up to a multiplicative constant: P̃ (x) = cP (x). An instance is declared as anomaly if its density is lower than a threshold:\n(1) log P̃ (x) ≤ β\nfor some threshold β estimated from data. Here − log P̃ (x) serves as the anomaly scoring function."
    }, {
      "heading" : "3.1 Prelim: Shallow Model for Mixed Data.",
      "text" : "For subsequent development, let us briefly review a probabilistic graphical model known as Mixed-variate Restricted Boltzmann Machines (Mv.RBM) [27] for modelling mixed data. Let x be an input vector of N (mixed-type) elements, and h ∈ {0, 1}K be a binary hidden vector, Mv.RBM defines the joint distribution as follows:\nP (x,h) ∝ exp (−E(x,h))\nwhere E(x,h) is energy function decomposed as follows:\n(2) E(x,h) = N∑ i=1 Ei(xi) + K∑ k=1\n( −bk +\nN∑ i=1 Gik(xi)\n) hk\nThe sub-energy functions Ei(xi) and Gik(xi) are typespecific. For example, Ei(xi) = −aixi for binary; Ei(xi) = 0.5x2i −aixi for Gaussian; and log xi!−aixi for Poisson. The three types share the same form of input-hidden mapping: Gik(xi) = Wikxi.\nMv.RBM can be used for outlier detection [12] by noticing that:\n(3) P (x) ∝ ∑ h exp (−E(x,h)) = exp (−F (x))\nwhere F (x) = − log ∑\nh exp (−E(x,h)) is known as freeenergy. In other words, F (x) = − logP (x) + c or equivalently F (x) = − log P̃ (x), where c is a constant and P̃ (x) is unnormalized density (see Eq. (1)). Thus we can use the free-energy as an anomaly score to rank data instances, following the decision rule in Eq. (1). The computational cost of free-energy scales linearly with number of dimensions making it an attractive scoring in practice:\n(4)\nF (x) = N∑ i=1 Ei(xi)− K∑ k=1 log\n( 1 + exp ( bk −\nN∑ i=1 Gik(xi)\n))\nFor training, we adopt the standard CD-1 procedure [16]. This method approximates the gradient of the data loglikelihood using one random sample per data point. The sample is generated using one-step MCMC starting from the observed data itself."
    }, {
      "heading" : "3.2 Extending Deep Belief Nets for Mixed Data.",
      "text" : "Deep Belief Network (DBN) [17] is a generative model of data. It assumes that the data x is generated from hidden binary variables h1, which is generated from higher hidden binary variables h2 and so on. Two consecutive layers in DBN form a Restricted Boltzmann Machine (RBM), which models either P ( x,h1 ) at the bottom level, or P ( hl,hl+2\n) at higher levels. A DBN is usually trained by learning a stack of RBMs in a layer-wise fashion. First a RBM is trained on the input data, its weights are then frozen. The hidden posterior is used to generate input for the next RBM, i.e., h ∼ P ( h1 | x ) . The process is repeated until the last RBM. This procedure of freezing the lower weights has been shown to optimize the variational bound of the data likelihood logP (x) [18]. Overall, an DBN is a mixed-graph whose the top RBM remains undirected, but the lower cross-layer connections are directed toward the data.\nThe original DBNs are designed for single data type, primarily binary or Gaussian. Here we extend DBNs to accommodate mixed data. In particular, the training steps of DBNs are kept, but the bottom RBM is now a Mv.RBM. The Mv.RBM transforms mixed input x into a homogeneous binary representation through h ∼ P ( h1 | x ) . The subsequent RBMs are for binary inputs as usual."
    }, {
      "heading" : "3.3 Abstracted Anomaly Detection Using Deep Belief Nets.",
      "text" : "Although the stagewise learning procedure that gives rise to DBN optimizes the lower bound of logP (x), it is still not possible to estimate the bound for density-based anomaly detection. Let L be the number of hidden layers. Existing methods typically use DBNs to (a) learn high-level features through P (hL | hL−1) and feed to existing anomaly detectors (e.g., [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26]. Here we propose an alternative to use DBN directly for anomaly detection.\nThe idea is to recognize that the RBM at the top of the DBN operates on data abstraction hL, and the RBM’s prior density PL (hL) can replace P (x) in Eq. (3). Recall that the input hl to the intermediate RBM at level l is an abstraction of the lower level data. It is generated through sampling the posterior P (hl | hl−1) for l ≥ 2 and P (hl | x) for l = 1 as\nfollows:\n(5) hl ∼ { B (σ (bl−1 + Wl−1hl−1)) for l ≥ 2 B ( σ ( b1k − ∑N i=1Gik(xi) )) for l = 1\nwhere B stands for Bernoulli distribution and σ(z) = [1 + e−z] −1 . The prior density PL (hL) can be rewritten as:\n(6) P (hL) ∝ exp (−FL(hL))\nwhere\n(7) FL (hL) = −b ′ LhL − KL∑ k=1 log (1 + exp (aLk + WLkhL))\nThis abstracted free-energy, like that of Mv.RBM in Eq. (4), can also be used as an anomaly score of abstracted data, and the anomaly region is defined as:\nR ∈ {x | FL (hL) ≥ β}\nOnce the DBN has been trained, the free-energy can be approximated by a deterministic function, where the intermediate input hl in Eq. (5) is recursively replaced by:\n(8) hl ≈ { σ (bl−1 + Wl−1hl−1) for l ≥ 2 σ ( b1k − ∑N i=1Gik(xi) ) for l = 1"
    }, {
      "heading" : "3.4 Multilevel Detection Procedure With DBN Ensemble.",
      "text" : "Recall that our Multilevel Anomaly Detection (MAD) hypothesis is that for domains where multiple data abstractions exist (e.g., in images & videos), an anomaly can be detected on one or more abstract representations. Each level of abstraction would detect abnormality in a different way. For example, assume an indoor setting where normal images contain regular arrangement of furniture. An image of a room with random arrangement (e.g., a chair in a bed) may appear normal at the pixel level, and at the object class level, but not at the object context level. This suggests the following procedure: apply multiple abstraction levels, and at each level, estimate an anomaly score, then combine all the scores.\nSince free-energies in Eq. (7) differ across levels, direct combination of anomaly scores is not possible. A sensible approach is through rank aggregation, that is, the freeenergies at each level are first used to rank instances from the lowest to the highest energy. The ranks now serve as anomaly scores which are compatible across levels.\n3.4.1 p-norm Rank Aggregation.\nOne approach to rank aggregation is to find a ranking that minimizes the disagreement with all ranks [3]. However,\nthis minimization requires searching through a permutation space of size N ! for N instances, which is intractable. Here we resort to a simple technique: Denoted by rli ≥ 0 the rank anomaly score of instance i at level l, the aggregation score is computed as:\n(9) r̄i(p) =\n( L∑\nl=1\nrpli )1/p where p > 0 is a tuning parameter.\nThere are two detection regimes under this aggregation scheme. The detection at p < 1 is conservative, that is, individual high outlier scores are suppressed in favor of a consensus. The other regime is optimistic at p > 1, where the top anomaly scores tend to dominate the aggregation. This aggregation subsumes several methods as special cases: p = 1 reduces to the classic Borda count when sli is rank position; p =∞ reduces to the max: limp→∞ r̄i(p) = maxl {rli}."
    }, {
      "heading" : "3.4.2 Separation of Abstraction and Detection.",
      "text" : "Recall from that we use RBMs for both abstraction (Eq. (5)) and anomaly detection (Eq. (7)). Note that data abstraction and anomaly detection have different goals – abstraction typically requires more bits to adequately disentangle multiple factors of variation [8], whereas detection may require less bits to estimate a rank score. Fig. 2 presents the multilevel anomaly detection algorithm. It trains one Mv.RBM and (L− 1) DBNs of increasing depths – from 2 to L – with time complexity linear in L. They produce L rank lists, which are then aggregated using Eq. (9)."
    }, {
      "heading" : "4 Experiments",
      "text" : "This section reports experiments and results of MIXMAD on a comprehensive suite of datasets. We first present the cases for single data type in Section 4.1, then extend for mixed data in Section 4.2."
    }, {
      "heading" : "4.1 Homogeneous Data.",
      "text" : "We use three high-dimensional real-world datasets with very different characteristics: handwritten digits (MNIST), Internet ads and clinical records of birth episodes.\n• The MNIST has 60, 000 gray images of size 28× 28 for training and 10, 000 images for testing1. The raw pixels are used as features (784 dimensions). Due to ease of visualization and complex data topology, this is an excellent data for testing anomaly detection algorithms. We use digit ’8’ as normal and a small portion (˜5%) of other digits as outliers. This proves to be a challenging digit compared to other digits – see Fig. 3 (left) for\n1http://yann.lecun.com/exdb/mnist/\nfailure of pixel-based k-nearest neighbor. We randomly pick 3,000 training images and keep all the test set.\n• The second dataset is InternetAds with 5% anomaly injection as described in [10]. As the data size is moderate (1,682 instances, 174 features), no train/test splitting is used.\n• The third dataset consists of birth episodes collected from an urban hospital in Sydney, Australia in the period of 2011–2015 [28]. Preterm births are considered anomalous as they have a serious impact on the survival and development of the babies [31]. In general, births occurring within 37 weeks of gestation are considered preterm [6]. We are also interested in early preterm births, e.g., those occurring within 34 weeks of gestation. This is because the earlier the birth, the more severe the case, leading to more intensive care. Features include 369 clinically relevant facts collected in the first few visits to hospital before 25 weeks of gestation. The data is randomly split into a training set of 3,000 cases, and a test set of 5,104 cases.\nAll data are normalized into the range [0,1], which is known to work best in [10]. Data statistics are reported in Table 1."
    }, {
      "heading" : "4.1.1 Models implementation.",
      "text" : "We compare the proposed method against four popular shallow unsupervised baselines – k-NN, PCA, Gaussian\nmixture model (GMM), and one-class SVM (OCSVM) [11]. (a) The k-NN uses the mean distance from a test case to the k nearest instances as outlier score [5]. We set k = 10 with Euclidean distance. (b) For PCA, the α% total energy is discarded, where α is the estimated outlier rate in training data. The reconstruction error using the remaining eigenvectors is used as the outlier score. (c) The GMMs have four clusters and are regularized to work with high dimensional data. The negative log-likelihood serves as outlier score. (d) The OCVSMs have RBF kernels with automatic scaling. We also consider RBM [13] as baseline, which is a special case of our method where the number of layers is set to L = 1.\nFor MIXMAD, abstraction RBMs have the same number of hidden units while detection RBM usually have smaller number of hidden units. All RBMs are trained using CD-1 [16] with batch size of 64, learning rate of 0.3 and 50 epochs. Table 2 lists model parameters used in experimentation.\nWe use the following evaluation measures: Area Under ROC Curve (AUC), and NDCG@T. The AUC reflects the average discrimination power across the entire dataset, while the NDCG@T places more emphasis on the top retrieved cases."
    }, {
      "heading" : "4.1.2 Results.",
      "text" : "To see how MIXMAD combines evidences from detectors in the ensemble, we run the algorithms: RBM, the DBN with 2 layers, and the MIXMAD that combines RBM and DBN results. Fig. 3 plots detected images by the RBM/DBN/MIXMAD against the classic k-NN. The k-NN fails for 15 out of 20 cases, mostly due to the variation in stroke thickness, which is expected for pixel-based matching. The RBM and DBN have different errors, confirming that anomalies differ among abstraction levels. Finally, the ensemble of RBM/DBN, then MIXMAD improves the detection significantly. The error is\nmostly due to the high variation in styles (e.g., 8 with open loops).\nTable 3 reports the Area Under the ROC Curve (AUC) for all methods and datasets. Overall MIXMAD with 2 or 3 hidden layers works well. The difference between the baselines and MIXMAD is amplified in the NDCG measure, as shown in Table 4. One possible explanation is that the MIXMAD is an ensemble – an outlier is considered outlier if it is detected by all detectors at different abstraction levels. One exception is the max-aggregation (where p→∞ in Eq. (9)), where the detection is over-optimistic."
    }, {
      "heading" : "4.2 Mixed Data.",
      "text" : "We use data from [12] where the data statistics are reported in Table 5. To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores. The detection performance on test data is reported in Table 6. Abstraction works well for L = 2, where performance is generally better than the shallow Mv.RBM. However, when one more layer is added, the results are mixed. This pattern is indeed not new as it resembles what can be seen across the literature of DBNs for classification tasks [17, 24]. One possible conjecture is that at a certain higher level, signals become too abstract, and that the distribution become too flat to really distinguish between truly low and high density regions. This suggests further research on selection of abstraction levels."
    }, {
      "heading" : "5 Discussion and Conclusion",
      "text" : "As an evidence to the argument in Section 3.4.2 about separating the abstraction and detection RBMs, we found that the sizes of the RBMs that work well on the MNIST do not resemble those often found in the literature (e.g., see [17]). For example, typical numbers of hidden units range from 500 to 1,000 for a good generative model of digits. However, we observe that 10 to 20 units for detection RBMs and 50-100 units for abstraction RBMs work well in our experiments, regardless of the training size. This suggests that the number of bits required for data generation is higher than those required for anomaly detection. This is plausible since accurate data generation model needs to account for all factors of variation and a huge number of density modes. On the other hand, anomaly detection model needs only to identify low density regions regardless of density modes. An alternative explanation is that since the CD-1 procedure used to train RBMs (see Section 3.2) creates deep energy wells around each data points, an expressive model may lead to more false alarms. Thus, the smoothness of the energy surface may play an important role in anomaly detection. Our MIXMAD algorithm offers a consensus among multiple energy surface, and thus can be considered as a way to mitigate the energy wells issue.\nThere has been an unexpected connection between the construction procedure of DBNs and the variational renomarlization groups in physics [23]. In particular, with layerwise construction, the data is rescaled – the higher layer operates on a coarser data representation. This agrees with our initial motivation for the MAD hypothesis.\nFinally, although not implemented here, the MIXMAD lends itself naturally to detecting anomalies in multimodal data with diverse modal semantics. For example, an image can be equipped with high-level tags and several visual representations. Each data representation can be modelled as a Mv.RBM at the right level of abstraction. The upper RBMs then integrate all information into coherent representations [29]."
    }, {
      "heading" : "5.1 Conclusion.",
      "text" : "In this paper we have tackled the double challenge of highdimensions and mixed-data in anomaly detection. We first proposed the Multilevel Anomaly Detection (MAD) hypothesis in that a data point is anomalous with respect to one or more levels of data abstraction. To test the hypothesis, we introduced MIXMAD, a procedure to train a sequence of Deep Belief Networks, each of which provides a ranking of anomalies. All rankings are then aggregated through a simple p-norm trick. Experiments on both single-type and mixed-type data confirmed that (a) learning data representation through multilevel abstraction is a sensible strategy for high-dimensional settings; and (b) MIXMAD is a competitive\nmethod. There are rooms for improvement however. First, going very deep has not proved very successful. DBNs have demonstrated its usefulness in abstraction, but there exist other possibilities [30, 33]. Finally, the simple p-norm rank aggregation can be replaced by a more sophisticated method for selecting and building right abstraction levels [36]."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partially supported by the Telstra-Deakin Centre of Excellence in Big Data and Machine Learning."
    } ],
    "references" : [ {
      "title" : "On the surprising behavior of distance metrics in high dimensional space",
      "author" : [ "Charu C Aggarwal", "Alexander Hinneburg", "Daniel A Keim" ],
      "venue" : "In International Conference on Database Theory,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Theoretical foundations and algorithms for outlier ensembles",
      "author" : [ "Charu C Aggarwal", "Saket Sathe" ],
      "venue" : "ACM SIGKDD Explorations Newsletter,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Aggregating inconsistent information: ranking and clustering",
      "author" : [ "Nir Ailon", "Moses Charikar", "Alantha Newman" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Ensemble anomaly detection from multi-resolution trajectory features",
      "author" : [ "Shin Ando", "Theerasak Thanomphongphan", "Yoichi Seki", "Einoshin Suzuki" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Fast outlier detection in high dimensional spaces",
      "author" : [ "Fabrizio Angiulli", "Clara Pizzuti" ],
      "venue" : "In European Conference on Principles of Data Mining and Knowledge Discovery,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "The distribution of clinical phenotypes of preterm birth syndrome: implications for prevention",
      "author" : [ "Fernando C Barros", "Aris T Papageorghiou", "Cesar G Victora", "Julia A Noble", "Ruyan Pang", "Jay Iams", "Leila Cheikh Ismail", "Robert L Goldenberg", "Ann Lambert", "Michael S Kramer" ],
      "venue" : "JAMA pediatrics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Deep belief networks for false alarm rejection in forward-looking ground-penetrating radar",
      "author" : [ "John Becker", "Timothy C Havens", "Anthony Pinar", "Timothy J Schulz" ],
      "venue" : "In SPIE Defense+ Security, pages 94540W–94540W. International Society for Optics and Photonics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Yoshua Bengio", "Aaron Courville", "Pascal Vincent" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "A practical outlier detection approach for mixed-attribute data",
      "author" : [ "Mohamed Bouguessa" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study",
      "author" : [ "Guilherme O Campos", "Arthur Zimek", "Jörg Sander", "Ricardo JGB Campello", "Barbora Micenková", "Erich Schubert", "Ira Assent", "Michael E Houle" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Anomaly detection: A survey",
      "author" : [ "Varun Chandola", "Arindam Banerjee", "Vipin Kumar" ],
      "venue" : "ACM computing surveys (CSUR),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Outlier detection on mixed-type data: An energy-based approach",
      "author" : [ "Kien Do", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh" ],
      "venue" : "International Conference on Advanced Data Mining and Applications (ADMA",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Network anomaly detection with the restricted Boltzmann machine",
      "author" : [ "Ugo Fiore", "Francesco Palmieri", "Aniello Castiglione", "Alfredo De Santis" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "An intrusion detection model based on deep belief networks",
      "author" : [ "Ni Gao", "Ling Gao", "Quanli Gao", "Hai Wang" ],
      "venue" : "In Advanced Cloud and Big Data (CBD),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Loaded: Link-based outlier and anomaly detection in evolving data sets",
      "author" : [ "Amol Ghoting", "Matthew Eric Otey", "Srinivasan Parthasarathy" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "The potential energy of an autoencoder",
      "author" : [ "Hanna Kamyshanska", "Roland Memisevic" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Detecting outliers in highdimensional datasets with mixed attributes",
      "author" : [ "Anna Koufakou", "Michael Georgiopoulos", "Georgios C Anagnostopoulos" ],
      "venue" : "In DMIN,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Discovering anomalies on mixed-type data using a generalized student-t based approach",
      "author" : [ "Yen-Cheng Lu", "Feng Chen", "Yating Wang", "Chang- Tien Lu" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "An exact mapping between the variational renormalization group and deep learning",
      "author" : [ "Pankaj Mehta", "David J Schwab" ],
      "venue" : "arXiv preprint arXiv:1410.3831,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Deep Boltzmann Machines",
      "author" : [ "R. Salakhutdinov", "G. Hinton" ],
      "venue" : "In Proceedings of 20th AISTATS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Automated fault detection using deep belief networks for the quality inspection of electromotors",
      "author" : [ "Jianwen Sun", "Reto Wyss", "Alexander Steinecker", "Philipp Glocker" ],
      "venue" : "tm-Technisches Messen,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Structured denoising autoencoder for fault detection and analysis",
      "author" : [ "Takaaki Tagawa", "Yukihiro Tadokoro", "Takehisa Yairi" ],
      "venue" : "In ACML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Mixed-variate restricted Boltzmann machines",
      "author" : [ "T. Tran", "D.Q. Phung", "S. Venkatesh" ],
      "venue" : "In Proc. of 3rd Asian Conference on Machine Learning (ACML), Taoyuan,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Preterm birth prediction: Deriving stable and interpretable rules from high dimensional data",
      "author" : [ "Truyen Tran", "Wei Luo", "Dinh Phung", "Jonathan Morris", "Kristen Rickard", "Svetha Venkatesh" ],
      "venue" : "Conference on Machine Learning in Healthcare, LA,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Learning deep representation of multityped objects and tasks",
      "author" : [ "Truyen Tran", "Dinh Phung", "Svetha Venkatesh" ],
      "venue" : "arXiv preprint arXiv:1603.01359,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2010
    }, {
      "title" : "Predicting preterm birth is not elusive: Machine learning paves the way to individual wellness",
      "author" : [ "Ilia Vovsha", "Ashwath Rajan", "Ansaf Salleb-Aouissi", "Anita Raja", "Axinia Radeva", "Hatim Diab", "Ashish Tomar", "Ronald Wapner" ],
      "venue" : "In 2014 AAAI Spring Symposium Series,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "A deep learning approach for detecting malicious JavaScript code",
      "author" : [ "Yao Wang", "Wan-dong Cai", "Peng-cheng Wei" ],
      "venue" : "Security and Communication Networks,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Deep structured energy based models for anomaly detection",
      "author" : [ "Shuangfei Zhai", "Yu Cheng", "Weining Lu", "Zhongfei Zhang" ],
      "venue" : "arXiv preprint arXiv:1605.07717,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2016
    }, {
      "title" : "An effective pattern based outlier detection approach for mixed attribute data",
      "author" : [ "Ke Zhang", "Huidong Jin" ],
      "venue" : "In Australasian Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2010
    }, {
      "title" : "A survey on unsupervised outlier detection in highdimensional numerical data",
      "author" : [ "Arthur Zimek", "Erich Schubert", "Hans-Peter Kriegel" ],
      "venue" : "Statistical Analysis and Data Mining,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2012
    }, {
      "title" : "On evaluation of outlier rankings and outlier scores",
      "author" : [ "Hans-Peter Kriegel" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "A disciplined approach is to identify instances lying in low density regions [11].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "However, estimating density in high-dimensional and mixed-type settings is difficult [12, 22, 35].",
      "startOffset" : 85,
      "endOffset" : 97
    }, {
      "referenceID" : 20,
      "context" : "However, estimating density in high-dimensional and mixed-type settings is difficult [12, 22, 35].",
      "startOffset" : 85,
      "endOffset" : 97
    }, {
      "referenceID" : 33,
      "context" : "However, estimating density in high-dimensional and mixed-type settings is difficult [12, 22, 35].",
      "startOffset" : 85,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "An alternative is to use distance to k nearest neighbors, assuming that the larger the distance, the less dense the region [5].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "But distance is neither well-defined under mixed types nor meaningful in a high-dimensional space [1, 35].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "But distance is neither well-defined under mixed types nor meaningful in a high-dimensional space [1, 35].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : ", see [35] for a recent review) or mixed data (e.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 11,
      "context" : ", see [12, 22] for latest attempts).",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 20,
      "context" : ", see [12, 22] for latest attempts).",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "To tackle the challenges jointly, we advocate learning data representation through abstraction, a strategy that (a) transforms mixed-data into a homogeneous representation [27], and (b) represents the multilevel structure of data [8].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 7,
      "context" : "To tackle the challenges jointly, we advocate learning data representation through abstraction, a strategy that (a) transforms mixed-data into a homogeneous representation [27], and (b) represents the multilevel structure of data [8].",
      "startOffset" : 230,
      "endOffset" : 233
    }, {
      "referenceID" : 11,
      "context" : "MIXMAD generalizes the recent work in [12] for mixed data by building multiple abstractions of data.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "For data abstraction, we leverage recent advances in unsupervised deep learning to abstract the data into multilevel low-dimensional representations [8].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 31,
      "context" : "While deep learning has revolutionized supervised prediction [21], its application to unsupervised anomaly detection is very limited [33].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "With MIXMAD we build a sequence of Deep Belief Nets (DBNs) [17] of increasing depths.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "bears some similarity with the recent ensemble approaches [2, 4], the key difference is MAD relies on multiple data abstractions, not data resampling or random subspaces which are still on the original data level.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "bears some similarity with the recent ensemble approaches [2, 4], the key difference is MAD relies on multiple data abstractions, not data resampling or random subspaces which are still on the original data level.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "We validate MIXMAD through an extensive set of experiments against well-known shallow baselines, which include the classic methods (PCA, GMM, RBM and one-class SVM), as well as state-of-the-art mixed-type methods (ODMAD [20], BMM [9], GLM-t [22] and Mv.",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 8,
      "context" : "We validate MIXMAD through an extensive set of experiments against well-known shallow baselines, which include the classic methods (PCA, GMM, RBM and one-class SVM), as well as state-of-the-art mixed-type methods (ODMAD [20], BMM [9], GLM-t [22] and Mv.",
      "startOffset" : 230,
      "endOffset" : 233
    }, {
      "referenceID" : 20,
      "context" : "We validate MIXMAD through an extensive set of experiments against well-known shallow baselines, which include the classic methods (PCA, GMM, RBM and one-class SVM), as well as state-of-the-art mixed-type methods (ODMAD [20], BMM [9], GLM-t [22] and Mv.",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 11,
      "context" : "RBM [12]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "High-dimensional data suffers from ‘curse of dimensionality’ also known more concretely as ‘distance concentration effect’, irrelevant attributes and redundant attributes, which together cause failure of low-dimensional techniques [1].",
      "startOffset" : 231,
      "endOffset" : 234
    }, {
      "referenceID" : 33,
      "context" : "Popular anomaly detection approaches targeting high-dimensions include feature selection, dimensionality reduction (such as using PCA) and subspace analysis (readers are referred to [35] for a recent survey and in-depth discussion).",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 14,
      "context" : "A method called LOADED [15] defines the score on the discrete subspace and combines with a correlation matrix in the continuous subspace.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 19,
      "context" : "A related method called ODMAD [20] opts for stage-wise detection in each subspace.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "A different strategy is employed in [9], where scores for discrete and continuous spaces are computed separately, then combined using a mixture model.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : "The work in [34] introduces Pattern-based Anomaly Detection (POD), where a pattern consists of a discrete attribute and all continuous attributes.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 11,
      "context" : "Methods with joint distribution of all attributes are introduced recently in [12, 22] using latent variables to link all types together.",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "Methods with joint distribution of all attributes are introduced recently in [12, 22] using latent variables to link all types together.",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "We adapt the work in [12] to represent mixed data into a homogeneous form using Mixed-variate Restricted Boltzmann Machines [27].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "We adapt the work in [12] to represent mixed data into a homogeneous form using Mixed-variate Restricted Boltzmann Machines [27].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "A common strategy is to use unsupervised deep networks to detect features, which are then fed into well-established detection algorithms [32].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26].",
      "startOffset" : 130,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : "Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26].",
      "startOffset" : 130,
      "endOffset" : 145
    }, {
      "referenceID" : 23,
      "context" : "Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26].",
      "startOffset" : 130,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : "Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26].",
      "startOffset" : 130,
      "endOffset" : 145
    }, {
      "referenceID" : 18,
      "context" : "A more fundamental problem is that reconstruction error does not reflect data density [19].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "A better approach is to use deep networks to estimate the energy directly [12, 13, 33].",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "A better approach is to use deep networks to estimate the energy directly [12, 13, 33].",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : "A better approach is to use deep networks to estimate the energy directly [12, 13, 33].",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 25,
      "context" : "RBM) [27] for modelling mixed data.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "RBM can be used for outlier detection [12] by noticing that:",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "i=1 Gik(xi) )) For training, we adopt the standard CD-1 procedure [16].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "Deep Belief Network (DBN) [17] is a generative model of data.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 17,
      "context" : "This procedure of freezing the lower weights has been shown to optimize the variational bound of the data likelihood logP (x) [18].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 30,
      "context" : ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].",
      "startOffset" : 81,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].",
      "startOffset" : 81,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].",
      "startOffset" : 81,
      "endOffset" : 96
    }, {
      "referenceID" : 24,
      "context" : ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].",
      "startOffset" : 81,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "One approach to rank aggregation is to find a ranking that minimizes the disagreement with all ranks [3].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "Note that data abstraction and anomaly detection have different goals – abstraction typically requires more bits to adequately disentangle multiple factors of variation [8], whereas detection may require less bits to estimate a rank score.",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 9,
      "context" : "• The second dataset is InternetAds with 5% anomaly injection as described in [10].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : "• The third dataset consists of birth episodes collected from an urban hospital in Sydney, Australia in the period of 2011–2015 [28].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 29,
      "context" : "Preterm births are considered anomalous as they have a serious impact on the survival and development of the babies [31].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "In general, births occurring within 37 weeks of gestation are considered preterm [6].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "All data are normalized into the range [0,1], which is known to work best in [10].",
      "startOffset" : 39,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "All data are normalized into the range [0,1], which is known to work best in [10].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "mixture model (GMM), and one-class SVM (OCSVM) [11].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : "(a) The k-NN uses the mean distance from a test case to the k nearest instances as outlier score [5].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "We also consider RBM [13] as baseline, which is a special case of our method where the number of layers is set to L = 1.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "All RBMs are trained using CD-1 [16] with batch size of 64, learning rate of 0.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "We use data from [12] where the data statistics are reported in Table 5.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 19,
      "context" : "To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "This pattern is indeed not new as it resembles what can be seen across the literature of DBNs for classification tasks [17, 24].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "This pattern is indeed not new as it resembles what can be seen across the literature of DBNs for classification tasks [17, 24].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : ", see [17]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 21,
      "context" : "There has been an unexpected connection between the construction procedure of DBNs and the variational renomarlization groups in physics [23].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 27,
      "context" : "The upper RBMs then integrate all information into coherent representations [29].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "BMM [9, 12] – 0.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "BMM [9, 12] – 0.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "67 ODMAD [12, 20] – 0.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "67 ODMAD [12, 20] – 0.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "52 GLM-t [12, 22] – – – 0.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : "52 GLM-t [12, 22] – – – 0.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "RBM [12] 0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 28,
      "context" : "DBNs have demonstrated its usefulness in abstraction, but there exist other possibilities [30, 33].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 31,
      "context" : "DBNs have demonstrated its usefulness in abstraction, but there exist other possibilities [30, 33].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 34,
      "context" : "Finally, the simple p-norm rank aggregation can be replaced by a more sophisticated method for selecting and building right abstraction levels [36].",
      "startOffset" : 143,
      "endOffset" : 147
    } ],
    "year" : 2016,
    "abstractText" : "Anomalies are those deviating from the norm. Unsupervised anomaly detection often translates to identifying low density regions. Major problems arise when data is high-dimensional and mixed of discrete and continuous attributes. We propose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an ensemble method that estimates the sparse regions across multiple levels of abstraction of mixed data. The hypothesis is for domains where multiple data abstractions exist, a data point may be anomalous with respect to the raw representation or more abstract representations. To this end, our method sequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying depths. Each DBN is an energy-based detector at a predefined abstraction level. At the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann Machine that models the density of mixed data. Predictions across the ensemble are finally combined via rank aggregation. The proposed MIXMAD is evaluated on high-dimensional real-world datasets of different characteristics. The results demonstrate that for anomaly detection, (a) multilevel abstraction of highdimensional and mixed data is a sensible strategy, and (b) empirically, MIXMAD is superior to popular unsupervised detection methods for both homogeneous and mixed data.",
    "creator" : "LaTeX with hyperref package"
  }
}