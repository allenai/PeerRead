{
  "name" : "1501.00437.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Shai Ben-David" ],
    "emails" : [ "shai@uwaterloo.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 1.\n00 43\n7v 1\nOne approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such “clusterable” instances. In other words, hope that “Clustering is difficult only when it does not matter”1 (the CDNM thesis for short). We believe that to some extent this may indeed be the case. This paper provides a survey of recent papers along this line of research and a critical evaluation their results. Our bottom line conclusion is that that CDNM thesis is still far from being formally substantiated.\nWe start by discussing which requirements should be met in order to provide formal support the validity of the CDNM thesis. In particular, we list some implied requirements for notions of clusterability. We then examine existing results in view of these requirements and outline some research challenges and open questions.\n1This phrase is in fact a title of a recent paper – [18]."
    }, {
      "heading" : "1 Introduction",
      "text" : "The goal of this note is two-fold. First, I would like to provide a personally biased overview of the research concerning the computational complexity of clustering under data niceness assumptions. Having worked in this area for quite some time now, I feel that while the TCS community appreciates work that may have practical relevance (and clustering is clearly a task that arises in many applications), sometimes in this area there is a significant gap between research motivation and the actual technical results it yields. A secondary aim of this paper is to call the attention of the theoretical research community to some such gaps and encourage further work along directions that might have otherwise seemed resolved."
    }, {
      "heading" : "1.1 Alternatives to worst-case for measuring computational complexity",
      "text" : "Computational complexity theory aims to provide tools for the quantification and analysis of the computational resources needed for algorithms to perform computational tasks. Worst-case complexity is by far the best known, most researched and best understood approach to computational complexity theory. In particular, NP-hardness is a worst-case-instance notion. By saying that a task is NP-hard (and assuming P 6= NP ), we imply that for every algorithm, there exist infinitely many instances on which it will have to work hard. However, for many problems this measure is unrealistically pessimistic compared to the experience of solving them for practical instances. A problem may be NP–hard and still have algorithms that solve it efficiently for any instance that is likely to occur in practice or any instance for which one cares to find an optimal solution for.\nSeveral approaches have been proposed to bringing computational complexity theory closer to the actual hardness faced when solving optimization problems on real data. Average Case Complexity ([20], [11]), analyzes run time w.r.t. some given probability distribution over the input instances. Smoothed Analysis ([25]) examines the running time of a given algorithm by taking the worst case over all inputs of the average runtime of the algorithm over some vicinity of the input. A different approach is to have a notion of “well-behaved-instances”, so that on one hand it is reasonable to expect that instances one comes across in applications are so well behaved, and on the other hand there exist algorithms that can solve any well behaved input in polynomial time. Various earlier approaches have addressed computational hardness by defining subset of relatively-easy instances (most no-\ntably, the area of parameterized complexity ([19])). [9], and [16] propose general notions of tamed instances that apply across different problems. Both of these papers apply some type of robustness to perturbations as the key property of such well behaved instances. Algorithms that efficiently solve NP-hard problems on such perturbation robust instances have been shown to exist for agnostic learning of half-spaces ([13]) and for graph partitioning problems ( [17]). [6] formalized a uniqueness of the optimal solution criterion as a notion of well behaved clustering instances, which can also be applied to other types of problems. In this note we will focus on the application of such approaches to clustering. We will discuss those, as well as other notions of niceness-of-instances that are specific to clustering problems, as a basis for alternatives-to-worst-case-complexity analysis of clustering tasks."
    }, {
      "heading" : "1.2 A focus on clustering tasks",
      "text" : "Clustering is a very useful paradigm that is being applied in a wide range of data exploration tasks. The term “clustering” should be thought of as an umbrella notion for a big and varied collection of tasks and algorithmic paradigms. Here, we focus on clustering tasks that are defined as discrete optimization problems. Most of those optimization problems are NP-hard. We wish to examine whether this hardness remains an issue when we restrict our attention to “clusterable data” - data for which a meaningful clustering exists (one can argue that when there is no cluster structure in a given data set, there is no point in applying a clustering algorithm to it). In other words, we wish to evaluate to what extent current theoretical work supports the “Clustering is difficult only when it does not matter” (CDNM) thesis.\nFor the sake of concreteness, we will focus on two popular clustering objectives, k-means and k-median."
    }, {
      "heading" : "1.3 Outline of the paper",
      "text" : "We start this note by listing, in Section 2, what we think are requirements from notions of clusterability aiming to substantiate the CDNM thesis. In Section 4, we list various notions of clusterability that have been proposed in the context of this line of research. These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (α, ǫ) Perturbation Resilience, [8]; ǫ -Separatedness, [22]; Uniqueness of optimum, [6] (they call it\n(c, ǫ)-approximation-stablility); α-center stability, [5]; and (1 + α) Weak Deletion Stability, [4].\nThe main body of this paper is an examination, in Section 5, of how well do the current notions and results meet the requirements (of Section 2). To get a sense of how strict a clusterability condition is, we consider an optimal clustering of data sets that satisfy that condition and examine the implied bounds on the ratio between the average distance of a data point to its own cluster center and the distance between centers of different clusters (or the distance of a point from centers of clusters it does not belong to). By analyzing the results pertaining to the proposed notions of clusterability listed above, we show, for example, that,\n• The values of ǫ for which ǫ -Separatedness is shown (in [22]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, the average distance of a point from its cluster center should be smaller than the minimal distance between distinct cluster centers by a factor of at least 200.\n• The values of parameters for which (c, ǫ) approximation stability is shown (in [6]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, for all but an ǫ-fraction of the input points, the distance of a point to its own cluster center is smaller than its distance to the next closest center by at least 20 times the average point-to-its-cluster-center-distance.\n• The values of α for which (1 + α) weak deletion stability is shown (in [4]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, the vast majority of the clusters are so distant from the rest of the data points that any point outside such a cluster is further from the center of that cluster by at least log(k) times the ”average radius” of its own cluster.\nOur conclusion is that the currently available theory is still far from substantiating the CDNM thesis. In particular, while additive perturbation robustness, with any non-zero robustness parameter, gives rise to algorithms that find the optimal clusterings in time polynomial in the input size and its dimension, as far as currently published results go, non of the requirements listed above allows finding optimal clustering solutions in time polynomial in the number of target clusters, k, unless the corresponding parameters are set to values that hold only for extremely well clusterable data sets2.\n2The above consequences of the required clusterability conditions are obtained by examining the parameter values and constants that are implicit in the asymptotic formulation of the efficiency results in the above cited papers. One should note that these negative statements reflect only the\nIn Section 6 we discuss these discouraging results further, highlight some implied open problems and propose directions in which this line of research should, in our opinion, proceed."
    }, {
      "heading" : "2 Requirements from notions of clusterability",
      "text" : "We begin this discussion by stating requirements that (we believe) a notion of clusterability should satisfy to be applied for supporting the “Clustering is Difficult only when it does Not Matter” (CDNM, in short) thesis. At this point those requirements will be stated as qualitative, high level, statements. We discuss more concrete quantitative formulations in Section 5 .\n1. It should be reasonable to assume that most (or at least a significant proportion of) the inputs one may care to cluster in practice satisfy the clusterability notion.\nSome disclaimer is in place here; Of course, we do not have any way to guarantee that unseen practical instances will satisfy any non-trivial requirement. However, this type of consideration can serve as a way to filter out clusterability conditions that are too restrictive. Furthermore, when a good data generative model is available, one can formalize requirements pertaining to a high probably of having the generated instances satisfy the given clusterability notion.\n2. In order to support the CDNM thesis, a notion of clusterability should be such that there exist efficient algorithms that are guaranteed to find a good clustering (minimizing the objective function, or getting very close to it) for any input that satisfies that clusterability requirement.\nThe next two requirements may be more debatable. Their significance is motivated by considering practical aspects of clustering applications. Assume we do have some clusterability condition and a guarantee that the algorithm we are about to run is efficient on instances satisfying it. Still, when we get some real input, there is no guarantee that it satisfies that clusterability condition. If it does not, and\ncurrent state of knowledge, and are not proven lower bounds. For some of the above notions of clusterability, we also discuss lower bounds on the parameter values required to overcome the NP-hardness of the clustering tasks.\nwe run our algorithm, it may either run for too long or terminate with some suboptimal solution. However, for most of the NP-hard clustering problems, there is no efficient way of measuring how far from optimal a given clustering solution is. We are therefore in the risk of not being able to protect against bad solutions. This consideration implies a third desirable requirement – the ability to distinguish between clusterable and non-clusterable input data sets. Namely,\n3. There exists an efficient algorithm for testing clusterability. Namely, given an instance (X, d), the algorithm determines whether it satisfies the clusterability requirement or not.\nAnother advantage of having a notion of clusterability satisfy this requirement is that it will allow a direct evaluation of the extent to which the notion satisfies Requirement 1 above. Namely, having an efficient clusterability -checking algorithm, one could apply it to collections of representative practical clustering inputs from various domain and evaluate to what extent the clusterability requirement actually holds for such clustering tasks.\nA forth, somewhat orthogonal, desiderata relates to existing common clustering algorithms. Namely,\n4. Some commonly used clustering algorithm can be guaranteed to perform well (i.e., run in polytime and find close-to-optimal solutions) on all instances satisfying the clusterability assumption.\nRequirement 4 is important if our goal is to understand what is happening nowadays in clustering work by providing a theoretical explanation for the success of common clustering algorithms on real data. However, even when failing it, requirement 2 may lead to the development of new clustering algorithms, which may have independent merits.\nThe main Open Question: Find a notion of clusterability that satisfies the requirements above (or even just the first two)."
    }, {
      "heading" : "3 Definitions and basic notions",
      "text" : "We consider clustering tasks that can be described as follows:\n• The input is a finite subset X of a metric space (Y, d) 3, and some number k. When X = Y or Y is some Euclidean space (with the Euclidean distance), we omit mentioning it explicitly.\n• The solution space S is a collection of partitionings of the input set X into k subsets (a.k.a. k-clusterings).\n• The problem is determined by an objective function, O, that maps pairs of (Instance, Clustering) to the real numbers. The goal of the algorithm is to find a clustering in the solution space that minimizes this objective for the given input instance. We let CO(X, d) be ArgMinC∈SO(C, (X, d)) (namely the set of all clusterings in the solution space that minimize the objective cost for the input). Finally, given some objective function, let OPT (X, d) denote the cost of an optimal clustering or (X, d) (this value, depends, of course, on the objective function in question. However, to simplify the notation, we suppress this dependence on the objective). We also suppress the distance function d when it is clear from the context and when it is the Euclidean distance.\nWe zoom in even further and consider only “center based” clustering objectives. For such problems, a clustering is defined by a set of k points (centers), the partition associated with such a set of centers is the Voronoi partition it induces over the input set, and the objective function has the form O((X, d), (c1, . . . ck)) = ∑\nx∈X F (mini≤k d(x, ci)), for some non-decreasing function F : R → R. This family of clustering objectives includes common tasks such as,\n• k-means, where O((X, d), (c1, . . . ck)) = ∑ x∈X(mini≤k d(x, ci) 2,\n• k-median, where O((X, d), (c1, . . . ck)) = ∑ x∈X mini≤k d(x, ci) and\n• k-medoids, where the objective as the same as in k-median, but the cluster centers, c1, . . . , ck, are required to be members of X .\nGiven such a clustering, we call the subset of X in the Voronoi cell of each center ci the i’th cluster and denote in by Ci. By extending the format of the objective function to ∑\ni≤k G(|Ci|) ∑ x∈Ci F (d(x, ci)), for some non-decreasing G, one can\ncapture some additional common clustering objectives like the sum-of-inclusterdistances (MinSum). In this note we focus on the k-means, k-median and the k-medoids objectives.\n3In some cases, d is not required to satisfy the triangle inequality, in which case we call it a dissimilarity function rather than a metric.\nAll of these three clustering-motivated discrete optimization problems are known to be NP-hard, and even NP-hard to approximate (some such hardness results are stated quantitatively later).\nThroughout this paper, we will use m to denote the input size (that is m = |X|) and use D(X) to denote the diameter of the input set, namely, D(X) = max{d(x, y) : x, y ∈ X}. When the data is a subset of some Euclidean space, R\nn, we use n to denote that dimension. Furthermore, unless otherwise stated, whenever we discuss the k-means objective, we assume that X ⊆ Rn."
    }, {
      "heading" : "3.1 Measures of clustering approximations",
      "text" : "When it comes to approximation algorithms for clustering there is another technical point to be aware of, namely, the way in which one measures the difference between an optimal solution and an approximate one. There are at least two different approaches of quantifying that gap. The first, and probably also the most common one, is to consider only the cost of the solutions. In other words, given a clustering objective function O an input set X and a clustering, C of it, say that C is an ǫ cost-approximate good solution if O(C,X) ≤ Opt(X)(1 + ǫ) (alternatively, one could consider additive approximations to the cost, namely, requiring that O(C,X) ≤ Opt(X)+ǫ. Additive approximations arise naturally in the context of statistical machine learning, where approximate solutions are computed based on small samples of the input data). A different type of approximations, more specific to clustering problems, is to define some measure of distance between solutions, such as some distance between the center vectors of two center based clusterings, say Dcenters(C,C ′) def = infπ∈Π maxi≤k d(ci, c ′ π(i)), where Π is the set permutations of the cluster indices {1, . . . , k}, and c1, . . . ck, c′1, . . . c′k are the cluster centers of C and C ′ (respectively), or Derr(C,C ′) def = (1/|X|) infπ∈Π ∑\ni∈{1,...k} |Ci∆C ′π(i)|. Having such a measure of distance between clustering solutions, an approximation algorithm is required to come up with a clustering that is close to an optimal clustering w.r.t. that measure.\nThere are some implications between these notions of clustering approximations. In particular, note that for, say, the k-means objective |O(C,X) − O(C ′, X)| ≤ Dcenters(C,C ′) (for every input set X and clusterings C, C ′). Roughly speaking, approximation is hardest with respect to the Derr distance. Some of the clusterability conditions discussed below imply that such an approximation follows from approximations w.r.t. the other measures. For example, the\nUniqueness of optimum condition (see below) explicitly requires that clustering solutions that have objective cost close the optimal one are also close w.r.t. the Derr measure. Also, the Additive perturbation robustness clusterability implies that any good enough approximation (of an optimal clustering) w.r.t. the Dcenters is in itself an optimal clustering."
    }, {
      "heading" : "4 Notions of clusterability",
      "text" : "In the past few years there have been several interesting publications along the lines described above, showing that for various notions of clusterability there are indeed algorithms that find optimal clusterings in polytime for all appropriately clusterable instances. Below is a (possibly not exhaustive) list of major notions of clusterability that have been discussed in that context4. Most of these definitions can be applied to any of the above mentioned clustering objectives.\n1. Perturbation Robustness: An input data set is perturbation robust if small perturbations of it do not result in a change of the optimal clustering for that set.\n(a) Additive perturbation robustness (APR) [1]5: An input set (X, d) is ǫ-APR if for some optimal k-clustering C, for every d′, if |d(x, y) − d′(x, y)| ≤ ǫ for every x, y ∈ X , then C ∈ CO(X, d′). Namely, an optimal clustering of the input (X, d) remains optimal for any small (additive) perturbation of this input6. Since this additive condition is not scale invariant, we implicitly add the assumption that the diameter of the input set, maxx,y∈X d(x, y), is at most 1 (otherwise the stability parameter should be multiplied by that diameter).\n(b) Multiplicative perturbation robustness (MPR) [16]: An input set (X, d) is α-MPR if for some optimal k-clustering C such that for every d′,\n4The reader should be aware that different papers use different terminology for similar notions (and similar terminology for different notions), so my choice of terminology below is not always consistent with other publications.\n5The definition of robustness, as well as the implied efficiency of clustering result, in [1] are particular cases of a more general definition and more general results of [9]. For the sake of conciseness and due to its similarity to other notions discussed below, we present here only this case.\n6 The definition in [1] is formulated as robustness w.r.t. perturbations of the cluster centers of the optimal solution. However, it can be readily seen that the two definitions are equivalent.\nif d(x, y) ≤ d′(x, y) ≤ αd(x, y) for every x, y ∈ X , then C ∈ CO(X, d\n′). Namely, an optimal clustering of the input (X, d) that remains optimal for any small (multiplicative) perturbation of this input.\n(c) [8] propose the following relaxation of the MPR requirement: A data set (X, d) is (α, ǫ)-perturbation resilient if there exists some optimal k-clustering C such that for every d′, if d(x, y) ≤ d′(x, y) ≤ αd(x, y) for every x, y ∈ X , then for some C ′ ∈ CO(X, d′), Derr(C,C ′) ≤ ǫ.\n2. ǫ -Separatedness: [22]7 discuss clustering w.r.t. the k-means objective. They define an input data set (X, d) to be ǫ-separated for k if the k-means cost of the optimal k-clustering of (X, d) is less then ǫ2 times the cost of the optimal (k − 1)-clustering of (X, d).\n3. Uniqueness of optimum: [7]8 define a data set to be (c, ǫ)-approximationstable with respect to some target clustering CT if every clustering C of X whose objective cost over (X, d) is within a factor c of the objective cost of CT (on (X, d)) is ǫ-close to CT w.r.t. some natural notion of between - clustering distance. It is easily seen that such a condition holds with respect to any CT if and only if it holds (up to constant factors) w.r.t. the optimal clustering for (X, d) (see Fact 2.2. of [7]). We relate to this property as Uniqueness of Optimum since it rules out the possibility of having two significantly different close-to-optimal-cost solutions.\n4. α-center stability: [5] define an instance (X, d) to be α-center stable (with respect to some center based clustering objective O) if for any optimal clustering C ∈ CO(X, d) defined by centers c1, . . . ck (of the clusters C1, . . . Ck respectively), for every i ≤ k and every x ∈ Ci, and every j 6= i, αd(x, ci) < d(x, cj). Namely, points are closer by a factor α to their own cluster center than to any other cluster center.\n5. (1 + α) Weak Deletion Stability: [4] define an instance for k-clustering to satisfy the (1 + α) Weak Deletion Stability condition if, for all i 6= j,\nOPT (i→j) > (1 + α)OPT,\n7This is a journal version of [21], where the definition and the main results were initially introduced.\n8This is a journal version of [6], where the definition and the main results were initially introduced.\nwhere OPT is the cost of the optimal clustering of that instance, and, if the optimal clustering is determined by centers (c1, . . . , ck) and the optimal clusters are (C1, . . . , Ck), then OPT (i→j) is the cost of the clustering obtained by by removing the center ci and assigning all the points in Ci to the center cj . Note that an instance for k-clustering is ǫ-separated, then it satisfies the ǫ2-WDS condition for that k.\nAs varied as the above list of proposed notions may sound, it turns out that almost all (except for the additive perturbation robustness, which is also the only one that does not yield efficiency for large k) imply that data satisfying them is structured such that the vast majority of the data points can be assigned to compact clusters that are very widely separated (or that all but a small fraction of the clusters are such). We provide quantitative versions of this claim in Section 5.2. In fact, this common characteristic of the notions is the main feature that is being used in showing that, under such conditions, clustering can be carried out efficiently."
    }, {
      "heading" : "5 To what extent do the notions meet the require-",
      "text" : "ments listed above?\nWhile all of the above notions sound intuitively plausible (concrete arguments supporting that plausibility can be found in the papers presenting them), the quantitative values of the clusterability assumptions are essential for evaluating that plausibility. We shall see below that the currently known results concerning these notions yield the desired efficiency of computation only when the clusterability parameters are set to values that are beyond what one might expect practical inputs to satisfy.\nTo keep this note focused, we provide a relatively high level view of some of the major relevant results. However, since the actual values of the parameters (that define the clusterability notions) determine both the runtime of the algorithms and the restrictiveness of the clsuterability conditions, these concrete values are needed when we wish to evaluate and the gap between what we currently know and the optimistic CDNM thesis."
    }, {
      "heading" : "5.1 Computational efficiency of clustering clusterable inputs",
      "text" : "An important distinction in this context concerns the meaning of hardness of computation. Clustering tasks where the clusters are determined by selecting cluster centers from the input set can clearly always be solved in time mk (where m is the input size and k is the number of clusters), by performing an exhaustive search over all possible cluster centers. For such problems, the term ”feasible” usually refers to run time bounded by a polynomial in both m and k. On the other hand, tasks like k-means, where the input set resides in some euclidean space, Rn, and cluster centers can be arbitrary points in that space, are often NP hard already for fixed values of k (e.g., k = 2) when one takes the space dimension n as a parameter that the runtime is a function of. For such problems, algorithms that have polynomial dependence on m and n may be considered “feasible” even if they have exponential dependence on k. Of course, in order to have solutions that are also polynomial in k, the requirements on the input instances are more demanding.\nWe summarize the main relevant results according to the different notions of clusterability that they require from the input instances (we let m denote the size of the input set X and k the target number of clusters);\n1. Additive perturbation robustness (APR): [1] show that for every centerbased clustering objective and every µ > 0 there exists an algorithm that\nruns in time O ( mk/µ 2 ) and finds the optimal k-means clustering for every\ninstance that is µ-APR. Using the results of [10] the parameter m in the runtime can be replaced by nk\nµ2ǫ2 if one settles for a solution whose cost is\nat most OPT (X) + ǫ|X|D(X) (recall that D(X) is the diameter of the input set). Recalling that for fixed k the k-means problem is NP hard when the Euclidean dimension, n, is considered an input parameter, the µ-APR condition allows to get rid of the dependence of n in the runtime, and replace it by dependence on the robustness parameter µ.\nIf we allow µ to depend on m, we get runtime poly (m), as long as k/µ2 = O(logm/ log logm) and 1/ǫ and n are upper bounded by polylogm and poly m, respectively.\n2. Multiplicative perturbation robustness (MPR): [5] show that for every ǫ > 0 there exists an algorithm that finds an optimal solution to the kmedian clustering problem for all inputs that are (3 − ǫ)- MPR in time O (m2 +mk2). [8] improve these results to assuming only (1 + √ 2)-MPR\nas well as obtaining similar results for the Min-Sum objective. They also prove an efficient approximation result under the weaker assumption that the optimal input clustering is an approximation for the optimal of any multiplicative α perturbation of that input. [15] show that for the Max-Cut objective (which considers clustering into k = 2 clusters), there exist algorithms that find the optimal solution for any √ m-MPR input in time polynomial in m (they also show the existence of efficient algorithms for solving Max-Cut under other data assumptions. However, as the focus of this note are centerbased clustering tasks, we do not elaborate on those results).\nFurthermore, [8] show that there is a polynomial time algorithm that for any α > 2+ √ 7, and any (α, ǫ)-perturbation resilient (for the k-median objective) input for which the smallest cluster in the optimal clustering contains at least 5ǫm many points, finds a clustering that is ǫ-close to an optimal clustering.\n3. ǫ -Separatedness: [22] focus of the k-means objective. They show propose a variant of the Lloyd algorithm that, for k = 2 assuming ǫ-separatedness of the input, run in time linear in m and n (the Euclidean dimension) and yields a clustering solution C that with probability (1 − O(ρ)), has cost O(C,X) ≤ OPT (X)\n1−ρ where ρ = Θ(ǫ2). For the k-means problem for ar-\nbitrary k, they get, under the same assumption, a (different) variant to the Lloyd algorithm that yields a clustering solution C that, with probability (1−O(√ǫ)), has cost O(C,X) ≤ OPT (X) 1−ǫ2\n1−37ǫ2 in time O(mkn+ k3n).\n4. Uniqueness of optimum: [7] propose algorithms that, for data sets that are (1 + α, ǫ)-approximation-stable find, in time polynomial in m and k clusterings that are O(ǫ+ǫ/α) close (w.r.t. the between-clusterings distance Derr) to the optimal clusterings w.r.t. the k-means and w.r.t. the k-median objectives. 5. α-center stability: [8] present an algorithm that, for any α ≥ 1 + √ 2 out-\nputs an optimal k-median clustering, as well as a binary hierarchical clustering tree for which the optimal k-means clustering is a pruning of that tree, in time polynomial in m and k.\n6. (1 + α) Weak Deletion Stability: For the k-means objective, [4] propose an algorithm that given any positive k, ǫ and α, for any input X satisfying the (1 + α) Weak Deletion Stability condition it finds a clustering C such that O(C) ≤ (1 + ǫ)OPT (X) in time mO(1)(k logm)poly(1/ǫ,1/α)."
    }, {
      "heading" : "5.2 How restrictive are the clusterability parameters required for the efficiency of computation results?",
      "text" : "In this subsection we examine the clusterability conditions listed in terms of the degree of separation between clusters that these conditions require (in the optimal clustering) when their clusterability (or data niceness) parameters assume values that suffice for showing efficiency of the corresponding proposed clustering algorithms. To measure those cluster-separation requirements, we focus of the relationship between the average distance of a point to the center of its cluster, AvDis = OPT/m (which can be thought of as the average “cluster radius” in an optimal clustering), to the minimal distance of a point from any other center, w2(x) = mini 6=i(x) d(x, ci), where c1, . . . , ck are the cluster centers in an optimal clustering of the given data set and i(x) is the index of the cluster a point x belongs to. As we shall argue below, most of the results cited above require a rather large value of w2(x)/AvDis, for most of the points x in the input set.\n1. Perturbation Robustness:\n(a) Additive perturbation robustness (APR): The first point to note about the efficiency results of [1] is that they focus on the case of fixed number of clusters and therefore their runtime upper bounds are not polynomial in k. Furthermore, although that run time is polynomial for any fixed k, the degree of that polynomial is impractically high, k/µ2, where µ is the robustness parameter. It is also worthwhile noting that these efficiency results are shown only for data residing in any Hilbert space, it is not known if they extend to data in arbitrary metric spaces.\n(b) Multiplicative perturbation robustness (MPR): It is not difficult to see that for any α a data set that is α-MPR is also α-center stable (see, e.g., [8]). In fact, most of the efficiency of clustering results for data satisfying MPR conditions actually use only the implied center stability properties. We will see below hardness results for clustering under center stability conditions. While not implying hardness for clustering under MPR, they do show inherent limitations of the proof techniques\n(and algorithms) used so far for clustering under this clusterability assumption.\nHow restrictive is the requirement of 2-center stability for real data? For concreteness, consider the very simplistic assumption that the data is nicely confined to k balls, B(c1, r1), . . . B(ck, rk), where (c1, . . . , ck) are the centers of the clusters in the optimal k-clustering and the ri’s are the radii of these balls. Such data satisfied the 2-center stability requirement if (and only if) for every i 6= j, d(ci, cj) ≥ 3max{ri, rj}. When the clusters are not ball shaped, the requirement may become more complicated. In particular, in Euclidean spaces (considering again the optimal k-clustering), denoting by ri,j the distance from ci of the furthest point in the cluster Ci along the line segment connecting ci an d cj , the 2-center stability requirement implies that d(ci, cj) ≥ 3ri,j for every i 6= j. The (α, ǫ)-perturbation resiliency condition relaxes this requirement by allowing for some points to fail the strict requirement “α times closer to your own center than to any other center”. However, the efficiency of clustering results under this condition ([8]) apply only when the number of such violations does not exceed the number of points in the smallest cluster. In particular, for every k the fraction of violations parameter ǫ is upper bounded by 1/k, shrinking to zero as k grows.\n2. ǫ -Separatedness:\nIn Section 5.1, the results are cited the way they appear in [22]. To evaluate how strict are the separateness conditions required for the efficiency results there, we take closer look at the actual constants behind the asymptotic notation; The parameter ρ equals 100ǫ 2\n1−ǫ2 . This implies that in order to have\nany significant success probability in the above results, ǫ2 should be at most 1/200. In other words, the benefits of the ǫ-separatedness condition kick in only when the cost of optimal k-means clustering is at most 1/200 times the cost of the optimal (k − 1)-clustering. Furthermore, the big O notation in these results hide constant factors that make those parameter settings even more demanding.\nLemma 3.1 of that paper may help to better appreciate how severe are such requirements. That lemma states that for the 2-means problem, for ǫ-separated inputs, in the optimal clustering, the average distance of data\npoints to their centers is less than O(ǫ2) times the distance between those centers. This lemma can be readily extended to k-mean clustering for any k > 1.Namely,\nLemma 1. Let k be at least 2 and let X be any subset of euclidean space that satisfies the ǫ -Separatedness condition for k-means. Let C be an optimal k-means clustering of X and c1, . . . , ck its cluster centers. Finally, let ri denote the mean square distance of the points in the i’th cluster from their center, ci. Then for any i ≤ k,\nri ≤ ǫ2\n1− ǫ2 minj 6=i ||ci − cj||.\nIn other words, in order satisfy the ǫ-separatedness clusterability condition, with a parameter ǫ that suffices to guarantee success of the [22] proposed algorithm, the data must be organized in small clusters that are extremely well separated. Under such conditions, it is not surprising that a sampling distribution that aims to pick a set of pairwise far points end up picking a representative residing in different clusters.\n3. Uniqueness of optimum: To appreciate the tradeoffs between data niceness requirements and the efficiency of the clustering algorithm (of [7] ) on such data, it is worthwhile to review Lemma 3.1 of that paper. The lemma examines the implications of the (c, ǫ)-approximation-stability assumption on the degree of separations between clusters in data satisfying that assumption.\nLemma 2 ([7]). 9 If an instance X satisfies the (1 + α, ǫ)-approximationstability condition for the k-median objective, then\n(a) In the optimal clustering of X , all but 6ǫm of the data points satisfy w2(x)− w(x) ≥ αAvDis2ǫ . (b) For any t > 0, at most tǫm/α many points have w(x) ≥ αAvDis tǫ .\nThe main issue with the efficiency results under this condition is the constants implicit in the O(ǫ) notation of those results. The proof of Theorem\n9While [7] phrases its results w.r.t. to some ”target clustering” CT , here, for the sake of concreteness, and easier comparison with the other papers discussed, we consider the case where that target clustering is an optimal clustering w.r.t. the relevant clustering objective. The results w.r.t. a different target clustering are essentially the same up to an additive term of ǫ = Derr(CT , COPT ).\n3.9 there shows that the (under (1 + α, ǫ)-approximation-stability condition) the algorithm gets a 4b-approximation of the optimal (or target) clustering, where b ≤ (6 + 40/α)ǫm. Since any clustering is trivially an mapproximation, the result is only meaningful once (6 + 40/α)ǫ << 1, and in particular, α/ǫ > 40. However, in light of Lemma 2, this implies that for the vast majority of the points x in the input set, w2(x)−w(x) ≥ 20AvDis - a rather strong between-clusters-separation requirement.\n4. α-center stability: This is probably the condition for which our theoretical understanding is most complete. On one hand we have the [8] efficiency result for α > 1 + √ 2, and on the other hand there is an almost matching\nlower bound:\nTheorem 3 ([14]). 10 For any ǫ > 0 the problem of finding the optimal k-median clustering for (2− ǫ)-center stable inputs is NP-hard.\nIt is worthwhile to note that this results addresses the setup in which k is part of the input. It does not imply NP-hardness for the problem for any fixed number of clusters. Furthermore, it is obtained using a metric that is not Euclidean. For data in Euclidean spaces a similar result probably applies with a somewhat lower value of α. Another relevant result of [14] is that once the parameter α exceeds 2+ √ 3, data satisfying the α-center stability condition is somewhat trivial. For such data sets, for any x, y, z, whenever x, y are in the same cluster and z is in a different cluster (w.r.t. an optimal clustering) then d(x, y) < d(x, z) (this is called perturbation resiliency). This property allows a simple dynamic programming algorithm to find the optimal clustering in time O(m2).\nIn conclusion, from the viewpoint of α-center stability, there is relatively little gap between being NP-hard and being (almost) trivially clusterable.\n5. (1 + α) Weak Deletion Stability: The [4] bound on the running time of the algorithm has only polynomial explicit dependence on the number of clusters k. However, it has exponential dependence on the niceness parameter 1/α (the deletion stability requirement becomes less restrictive with smaller α). The following claims address the relationship between that parameter and the number of clusters. Our conclusion is that, as long as the\n10This is a journal version of [23] where the result initially appeared.\nclusterability requirement, parameterized by α, is not extremely strong, the running time formula of [4] is, in fact, exponential in k.\nClaim 1. If (X, d) is (1 + α) WDS, then 1/α > k OPT mdmin .\nRuntime implications: Recall that OPT m is the average of the square distance between data points and the centers of their clusters in the optimal clustering. Since 1/α is in the exponent of the runtime bound, it follows that as long as the ratio between the average distance of points to their cluster centers and the minimum distance between the centers (of the optimal clustering) does not grow superpolynomially with the number of clusters, k, the runtime bound is, in fact, exponential in k.\nProof of Claim 1. Let C = (C1, . . . , Ck) be an optimal clustering of (X, d). Note that, for every i ≤ k, if cj is the closest center to ci then OPT (i→j) ≤ OPT + |Ci|di (since by assigning the points of Ci to some center cj the cost associated with each point of Ci grows by at most d(ci, cj) = di). Pick i such that |Ci| ≤ 1/k and di = dmin (such i exists since at least one of the clusters contains at most m/k points). It follows that for such an i, for j ∈ ArgMin{d(ci, cj)}, OPT (i→j) ≤ OPT + mdmin/k. The (1 + α) WDS property of (X, d) therefore implies that mdmin/k > αOPT , which is equivalent to the inequality that the claim states.\nFurthermore, [4] show the following similar manifestation of the strong implications on the (1 + α) WDS condition, in terms of the lower bounds it implies on between-cluster-centers distances:\nClaim 2. For any (1 + α) WDS k-median instance, for any center ci of its optimal clustering and any data point x /∈ Ci,\nd(x, ci) ≥ αOPT\n2|Ci|\nand for any (1 + α) WDS k-means instance, for any center ci of its optimal clustering and any data point x /∈ Ci,\nd2(x, ci) ≥ αOPT\n4|Ci|\nDiscussion: Rewriting these bounds (for concreteness, the bound for kmedian) in terms of the the average distance of a point in X from the center of its cluster, AvDis = OPT\nm , it reads d(x, ci) ≥ α m2|Ci|AvDis. Since for\nevery k and every t there are at most k/t clusters of size > mt/k. In particular, there are at most log(k) many clusters of size > m/ log(k). It follows that for all but log(k) of the k clusters Ci, for every point x outside the cluster, d(x, ci) ≥ 0.5α log(k)AvDis. In other words, if one considers the case of large k (which is the source of computational difficulty that the clusterability condition is aimed to overcome), for any fixed α, for any instance satisfying the (1 + α) WDS condition, the vast majority of the clusters are so distant from the rest of the data points that any point outside such a cluster is further from the center of that cluster by a factor of log(k) compared to the ”average radius” of the clusters."
    }, {
      "heading" : "5.3 Efficient testability of the clusterability conditions",
      "text" : "When it comes to testing whether a given clustering instance satisfies any of the above clusterability conditions, a key point to note is that they are all phrased in terms of condition pertaining to the optimal clustering of the given data. Finding such optimal clusterings is NP-hard. Furthermore, as far as I am aware, there exist no efficient algorithm for testing, given a data set (X, d) and a k clustering of it, C, whether C is an optimal clustering for (X, d) (say, w.r.t. either the k-means or the k-median objective). I therefore conjecture that testing each of the conditions we have discussed here is NP-hard.\nSome of those conditions can be also phrased as a niceness property of a given clustering (rather than a property of the data). For example,\nGiven a k clustering C for an instance (X, d), defined by a vector of centers, c1, . . . ck, say that C is α-center stable if for every i ≤ k and every x ∈ Ci and j 6= i, αd(x, ci) < d(x, cj).\nHowever, it is easy to see that a clustering that satisfies such a property in not necessarily optimal, and that the fact that (X, d) allows such a clustering does not imply that it is nicely clusterable; As a simple example, consider 2-means for an instance X ⊆ R2 that consists of 1000 points, 999 of them evenly spread in the unit ball and the last point at (0, 50). The clustering that has the unit ball as one cluster, and the outlier point as the other (singleton) cluster, satisfies the givencluster version of α-center stability for α = 50. However, this data set is not α-center stable for any α > 1.\nIn fact, the positive results of [8], showing efficient clustering algorithm for (1 + √ 2)-center stable data, can be rephrased as follows: There is an efficient\nalgorithm that when applied to any (1 + √ 2)-center stable data set, outputs a\nbinary hierarchical cluster tree such that every (1 + √ 2)-center-stable clustering C of that data set is the result of some pruning of that tree. Given such a tree, for any feasibly computable objective function, the tree can be efficiently searched to find its minimum cost pruning w.r.t. this objective.\nThe niceness condition concerning the input data is only invoked to show that the minimum cost pruning of the tree is also a minimum cost clustering of that data set.\nSimilarly, one can define, for a given k clustering C of a set (X, d), when C is (1 + α)-weakly-deletion stable. Once again, for every value of α, there are examples of data sets for which there are clustering? that are (1 + α)-weakly-deletion stable, and yet are not optimal k-means (or k-median) clusterings. However, it is not clear to me if for arbitrarily large values of α there exist instances (X, d) that have a clustering C that is (1 + α)-weakly-deletion stable and yet (X, d) is not (1 + α)-weakly-deletion stable.\nAn easier goal than coming up with a useful notion of clusterability that is efficiently testable, is to come up with a notion of niceness of a given clustering C, such that one can efficiently test if a clustering solution C for an instance (X.d) satisfies that requirement, and so that if it does, it is guaranteed to be an optimal clustering for the (X, d). As far as I am aware no such notion currently exits. As noted above, it seems to be an open question whether the notion of a clustering C being (1 + α)-weakly-deletion stable (for some sufficiently large α) implies that the domain set of such a clustering is necessarily (1 + α)-weakly-deletion stable."
    }, {
      "heading" : "5.4 Implications for common practical clustering algorithms",
      "text" : "Among all the works surveyed in this note, only one, the results of [22], address (a feasible variant of) a practical algorithm - the popular Lloyd clustering algorithm. It would be very interesting to come up with results showing that some popular clustering algorithm (or an application of a practical approximation algorithm) efficiently yield guaranteed good quality clusterings, under some other, or more relaxed, niceness of data conditions. The recent work of [3] can be viewed as a step in that direction. They ask under which separation condition do various convex relaxations exactly recover the “correct” clustering. However, that work addresses a different version of clustering problems, in which one assumes that the data is generated by some parameterized generative model (a balanced mix-\nture of spherical Gaussians, in the case of that paper), and aims to recover those parameters."
    }, {
      "heading" : "6 Conclusions",
      "text" : "Several notions of clusterability have been proposed so far. Depending on the values of the parameters defining those notions, each of them ranges from being very lenient to a highly constraining data requirement. For each notion there is a parameter range so that, for data conforming to the clusterability requirement in that range, an optimal clustering can be rather trivially found. Clusterability with parameter values that suffice for the currently available efficient clustering results turns out to be rather strong requirements, that eminently restricts the practical significance of the currently available results.\nThe current failure to support the CDNM thesis can stem from various sources. First, of course, maybe the thesis is just false. My personal belief is that, while it may very well be the case that some practical clustering tasks are indeed computationally hard for some real data instances, there are many more cases where data of practical interest does yield not-too-hard-to-find meaningful clusterings (though, of course, most of the time we have no way of knowing whether those are optimal clusterings in any formal sense of optimality).\nAnother explanation to the shortcomings of current results is that they may just be an artifact of the algorithms and proof techniques that we currently have. Maybe one could eventually come up with efficient algorithms that will cluster well under much less restrictive parameter settings of the clusterability notions listed in this note. Indeed, for most of those notions we do not have any closeto-matching computational hardness lower bounds. I doubt if that is indeed the case. As mentioned above, for the notion of α-center stability, the gap between the parameter values sufficient for efficient clustering and those that imply NP hardness is very small, (1 + √ 2) vs 2. Furthermore, the α-center stability is a central notions, in the sense that almost any other of the notions of clusterability discussed above implies it (or some variants of that condition), and the current results rely on those implications for proving the efficiency of clustering under those conditions.\nI believe that part of the answer is that we have not yet discovered the appropriate notions of clusterability. In light of the results surveyed in this paper, I think that notions of clusterability that aim to substantiate the CDNM statement should not be just a way of formalizing large between-clusters separation. Apparently,\nas demonstrated above, such assumptions become too restrictive before they yield efficient clustering results.\nFinally, in the last paragraph of concluding remarks below, I would like to argue that if we really wish to model clustering as it is required and used in applications, the formulation of clustering tasks as computational problems should be revisited and revised."
    }, {
      "heading" : "6.1 Call for a change of perspective on the complexity of clustering",
      "text" : "All the papers surveyed above, as well as most of the current theoretical work on the computational complexity of clustering, focus on concrete clustering objectives aiming to find the best clustering for a given number of clusters. However, the practice of clustering is widely varied. There are applications, like clustering for detecting record duplications in data bases (say, records of patients from various hospitals and clinics), where the user does not set the number of clusters in advance, and aims to detect sets of mutually similar items to the extent that such sets occur in the input data. In other applications, like vector quantization for signal transmission or facility location tasks, while the objective function is usually fixed (say, k-means), there is no implicit “target clustering” and the usefulness of a resulting clustering is not diminished by having various different close-to-optimal solutions. In some such applications k is externally determined, however, it is also common to consider optimizing some “compression vs distortion” tradeoffs, rather than aiming for a fixed number of clusters.\nFurthermore, while the restriction of the problem of finding a good clustering to a given number of clusters k may make practical sense when k is small, for data sets that yield a very large number of clusters it is harder to imagine realistic situations in which that number, k, should be fixed independently of the particular input data set. Still, most of the work surveyed above focuses on analyzing the asymptotic, w.r.t. k, computational complexity of k-clustering where k is determined as part of the problem input.\nIn many cases, the actual goal of clustering procedures is to find some meaningful structure of the given data, and is not committed to any fixed objective function or any fixed number of clusters. The currently available theoretical research does not provide satisfactory formalizations of such “flexible” clustering tasks11,\n11There haas been some recent work theoretically analyzing a notion of statistical stability (with respect to independent samplings) as a tool for determining an appropriate number of clusters as\nlet alone an analysis of their computational complexity. It may well be the case that our intuition of clustering being feasible on practically relevant cases stems from clustering tasks that do not fit into the rigid fixed-k-fixed-objective framework of clustering."
    }, {
      "heading" : "7 Some followup open problems",
      "text" : "Of course, in view of the results we have surveyed, the most obvious open problem is still the status of the CDNM thesis. The challenges referred to in the above Conclusions section may also be viewed as “open problems”. However, in this section, we wish to list some more technical and concrete problems whose answers will advance our understanding of the main topics of this paper.\n1. Lower bounds under the above clusterability assumptions: So far, it seems that the only notion of clusterability for which we have at this point meaningful lower bounds (on the computational complexity of finding an optimal clustering for data satisfying the condition) is the α-center stability. Even for that notion, the lower bounds of [14] require the input data to be an arbitrary metric space and do not apply to data in a Euclidean space. An obvious, though of relatively minor significance question is: For which values of α does the problem of optimizing the k-means or k-median objectives for α-center stable instances become NP hard for instances in Euclidean space, or for instances in R2? More significant open questions are finding parameter values for which the other notions of clusterability become NP hard, and zooming in on the range of parameter values for which clustering under those clusterability conditions can be feasibly carried out.\na function of the input data, e.g., [12], [24]. However, the conclusions if this work are mainly negative, showing that some proposed approaches may not work as intended.\n2. Clustering via linkage based hierarchical clustering trees: The algorithm of [8] is based on a linkage-based12 agglomerative construction of a cluster tree that is guaranteed to have any (1 + √ 2)-center stable clustering\nof the input data as a pruning of that tree. For which other notions of clustering niceness can one have an appropriate, feasibly computable, linkage based clustering that is guaranteed to output a tree with such a properly (that is, every “nice” clustering is a obtained by some pruning of the tree)?\n3. The relationship between being a nice clustering and optimizing common clustering objectives: Which notions of clustering niceness imply that\n(a) If a data set (X, d) allows such a “nice clustering”, then that clustering is bound to be an optimal k-means (or k-median) clustering of (X, d).\n(b) If a data set (X, d) allows such a “nice clustering” then there must be an optimal k-means (or k-median) clustering of (X, d) that is a nice clustering (for that notion of niceness).\n(c) If a data set (X, d) allows such a “nice clustering” then it is unique (namely, there exist no other similarly nice clustering of X, d)).\n4. Applying common approximation techniques to clustering optimization problems: Pick any common approximation technique (like linear programming relaxations) and come up with some naturally sounding notions of clusterability under which such an approximation algorithms is guaranteed to find the optimal clustering (rather than just approximating it) efficiently. Under which clusterability conditions will the approximations guaranteed for such algorithms be better than known hardness approximation lower bounds for clustering arbitrary instances?\n12Linkage-based clustering algorithms are algorithms that, given some clustering instance (X, d) define a notion of dissimilarity over subsets of X , d̂, and then contract a tree whose nodes are labeled by subsets of X as follows: The leaves are all the singleton sunsets {x}x∈X , and repetitively, it picks a pair of node subsets Ai, Bi minimizing the dissimilarity d̂(A,B) over all node subsets that have already been generated and creates a parent node, labeled A∪B above these two nodes, until a (root node) labels X is reached. See [2] for a more detailed discussion."
    }, {
      "heading" : "Acknowledgements",
      "text" : "I am grateful to Shalev Ben-David, Lev Rayzin and Ruth Urner for insightful discussions concerning this paper."
    } ],
    "references" : [ {
      "title" : "Clusterability: A theoretical study",
      "author" : [ "Margareta Ackerman", "Shai Ben-David" ],
      "venue" : "In AISTATS, pages",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Characterization of linkage-based clustering",
      "author" : [ "Margareta Ackerman", "Shai Ben-David", "David Loker" ],
      "venue" : "In COLT 2010 - The 23rd Conference on Learning",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Relax, no need to round: Integrality of clustering formulations",
      "author" : [ "Pranjal Awasthi", "Afonso Bandera", "Moses Charikar", "Ravishankar Krishnaswami", "Soledad Voilar", "Rachel Ward" ],
      "venue" : "CoRR, Stat.ML,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Stability yields a ptas for k-median and k-means clustering",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Center-based clustering under perturbation stability",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "Inf. Process. Lett.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Approximate clustering without the approximation",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta" ],
      "venue" : "In SODA,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Clustering under approximation stability",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Clustering under perturbation resilience",
      "author" : [ "Maria-Florina Balcan", "Yingyu Liang" ],
      "venue" : "In ICALP",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Alternative measures of computational complexity with applications to agnostic learning",
      "author" : [ "Shai Ben-David" ],
      "venue" : "In TAMC, pages 231–235,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "A framework for statistical clustering with constant time approximation algorithms for k-median and k-means clustering",
      "author" : [ "Shai Ben-David" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "On the theory of average case complexity",
      "author" : [ "Shai Ben-David", "Benny Chor", "Oded Goldreich", "Michael Luby" ],
      "venue" : "In STOC,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1989
    }, {
      "title" : "Stability of k -means clustering",
      "author" : [ "Shai Ben-David", "Dávid Pál", "Hans-Ulrich Simon" ],
      "venue" : "In COLT,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Efficient learning of linear perceptrons",
      "author" : [ "Shai Ben-David", "Hans-Ulrich Simon" ],
      "venue" : "In NIPS, pages 189–195,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2000
    }, {
      "title" : "Data stability in clustering: A closer look",
      "author" : [ "Shalev Ben-David", "Lev Reyzin" ],
      "venue" : "Theoretical Computer Science, page To appear,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "On the practically interesting instances of maxcut",
      "author" : [ "Yonatan Bilu", "Amit Daniely", "Nati Linial", "Michael Saks" ],
      "venue" : "In STACS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Are stable instances easy",
      "author" : [ "Yonatan Bilu", "Nathan Linial" ],
      "venue" : "In ICS, pages 332–341,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Are stable instances easy? Combinatorics",
      "author" : [ "Yonatan Bilu", "Nathan Linial" ],
      "venue" : "Probability & Computing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Clustering is difficult only when it does not matter",
      "author" : [ "Amit Daniely", "Nati Linial", "Michael Saks" ],
      "venue" : "CoRR, abs/1205.4891,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Average case complete problems",
      "author" : [ "Leonid A. Levin" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1986
    }, {
      "title" : "The effectiveness of lloyd-type methods for the k-means problem",
      "author" : [ "Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "The effectiveness of lloyd-type methods for the k-means problem",
      "author" : [ "Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Data stability in clustering: A closer look",
      "author" : [ "Lev Reyzin" ],
      "venue" : "In ALT,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Stability and model selection in k-means clustering",
      "author" : [ "Ohad Shamir", "Naftali Tishby" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Smoothed analysis of algorithms: why the simplex algorithm usually takes polynomial time",
      "author" : [ "Daniel A. Spielman", "Shang-Hua Teng" ],
      "venue" : "In STOC,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "1This phrase is in fact a title of a recent paper – [18].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "Average Case Complexity ([20], [11]), analyzes run time w.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "Average Case Complexity ([20], [11]), analyzes run time w.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "Smoothed Analysis ([25]) examines the running time of a given algorithm by taking the worst case over all inputs of the average runtime of the algorithm over some vicinity of the input.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 8,
      "context" : "[9], and [16] propose general notions of tamed instances that apply across different problems.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "[9], and [16] propose general notions of tamed instances that apply across different problems.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 12,
      "context" : "Algorithms that efficiently solve NP-hard problems on such perturbation robust instances have been shown to exist for agnostic learning of half-spaces ([13]) and for graph partitioning problems ( [17]).",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : "Algorithms that efficiently solve NP-hard problems on such perturbation robust instances have been shown to exist for agnostic learning of half-spaces ([13]) and for graph partitioning problems ( [17]).",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 5,
      "context" : "[6] formalized a uniqueness of the optimal solution criterion as a notion of well behaved clustering instances, which can also be applied to other types of problems.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (α, ǫ) Perturbation Resilience, [8]; ǫ -Separatedness, [22]; Uniqueness of optimum, [6] (they call it",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (α, ǫ) Perturbation Resilience, [8]; ǫ -Separatedness, [22]; Uniqueness of optimum, [6] (they call it",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (α, ǫ) Perturbation Resilience, [8]; ǫ -Separatedness, [22]; Uniqueness of optimum, [6] (they call it",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (α, ǫ) Perturbation Resilience, [8]; ǫ -Separatedness, [22]; Uniqueness of optimum, [6] (they call it",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (α, ǫ) Perturbation Resilience, [8]; ǫ -Separatedness, [22]; Uniqueness of optimum, [6] (they call it",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "(c, ǫ)-approximation-stablility); α-center stability, [5]; and (1 + α) Weak Deletion Stability, [4].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "(c, ǫ)-approximation-stablility); α-center stability, [5]; and (1 + α) Weak Deletion Stability, [4].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "• The values of ǫ for which ǫ -Separatedness is shown (in [22]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, the average distance of a point from its cluster center should be smaller than the minimal distance between distinct cluster centers by a factor of at least 200.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "• The values of parameters for which (c, ǫ) approximation stability is shown (in [6]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, for all but an ǫ-fraction of the input points, the distance of a point to its own cluster center is smaller than its distance to the next closest center by at least 20 times the average point-to-its-cluster-center-distance.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "• The values of α for which (1 + α) weak deletion stability is shown (in [4]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, the vast majority of the clusters are so distant from the rest of the data points that any point outside such a cluster is further from the center of that cluster by at least log(k) times the ”average radius” of its own cluster.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "(a) Additive perturbation robustness (APR) [1]5: An input set (X, d) is ǫ-APR if for some optimal k-clustering C, for every d, if |d(x, y) − d(x, y)| ≤ ǫ for every x, y ∈ X , then C ∈ CO(X, d).",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "(b) Multiplicative perturbation robustness (MPR) [16]: An input set (X, d) is α-MPR if for some optimal k-clustering C such that for every d,",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "5The definition of robustness, as well as the implied efficiency of clustering result, in [1] are particular cases of a more general definition and more general results of [9].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "5The definition of robustness, as well as the implied efficiency of clustering result, in [1] are particular cases of a more general definition and more general results of [9].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "6 The definition in [1] is formulated as robustness w.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "(c) [8] propose the following relaxation of the MPR requirement: A data set (X, d) is (α, ǫ)-perturbation resilient if there exists some optimal k-clustering C such that for every d, if d(x, y) ≤ d(x, y) ≤ αd(x, y) for every x, y ∈ X , then for some C ′ ∈ CO(X, d), Derr(C,C ) ≤ ǫ.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "ǫ -Separatedness: [22]7 discuss clustering w.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Uniqueness of optimum: [7]8 define a data set to be (c, ǫ)-approximationstable with respect to some target clustering CT if every clustering C of X whose objective cost over (X, d) is within a factor c of the objective cost of CT (on (X, d)) is ǫ-close to CT w.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "of [7]).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "α-center stability: [5] define an instance (X, d) to be α-center stable (with respect to some center based clustering objective O) if for any optimal clustering C ∈ CO(X, d) defined by centers c1, .",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "(1 + α) Weak Deletion Stability: [4] define an instance for k-clustering to satisfy the (1 + α) Weak Deletion Stability condition if, for all i 6= j,",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "7This is a journal version of [21], where the definition and the main results were initially introduced.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "8This is a journal version of [6], where the definition and the main results were initially introduced.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Additive perturbation robustness (APR): [1] show that for every centerbased clustering objective and every μ > 0 there exists an algorithm that",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "Using the results of [10] the parameter m in the runtime can be replaced by nk μǫ if one settles for a solution whose cost is at most OPT (X) + ǫ|X|D(X) (recall that D(X) is the diameter of the input set).",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "Multiplicative perturbation robustness (MPR): [5] show that for every ǫ > 0 there exists an algorithm that finds an optimal solution to the kmedian clustering problem for all inputs that are (3 − ǫ)- MPR in time O (m +mk).",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "[8] improve these results to assuming only (1 + √ 2)-MPR",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "[15] show that for the Max-Cut objective (which considers clustering into k = 2 clusters), there exist algorithms that find the optimal solution for any √ m-MPR input in time polynomial in m (they also show the existence of efficient algorithms for solving Max-Cut under other data assumptions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, [8] show that there is a polynomial time algorithm that for any",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 20,
      "context" : "ǫ -Separatedness: [22] focus of the k-means objective.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Uniqueness of optimum: [7] propose algorithms that, for data sets that are (1 + α, ǫ)-approximation-stable find, in time polynomial in m and k clusterings that are O(ǫ+ǫ/α) close (w.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "α-center stability: [8] present an algorithm that, for any α ≥ 1 + √ 2 outputs an optimal k-median clustering, as well as a binary hierarchical clustering tree for which the optimal k-means clustering is a pruning of that tree, in time polynomial in m and k.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "(1 + α) Weak Deletion Stability: For the k-means objective, [4] propose an algorithm that given any positive k, ǫ and α, for any input X satisfying the (1 + α) Weak Deletion Stability condition it finds a clustering C such that O(C) ≤ (1 + ǫ)OPT (X) in time m(k logm).",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "(a) Additive perturbation robustness (APR): The first point to note about the efficiency results of [1] is that they focus on the case of fixed number of clusters and therefore their runtime upper bounds are not polynomial in k.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : ", [8]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 7,
      "context" : "However, the efficiency of clustering results under this condition ([8]) apply only when the number of such violations does not exceed the number of points in the smallest cluster.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "1, the results are cited the way they appear in [22].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "In other words, in order satisfy the ǫ-separatedness clusterability condition, with a parameter ǫ that suffices to guarantee success of the [22] proposed algorithm, the data must be organized in small clusters that are extremely well separated.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "Uniqueness of optimum: To appreciate the tradeoffs between data niceness requirements and the efficiency of the clustering algorithm (of [7] ) on such data, it is worthwhile to review Lemma 3.",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "Lemma 2 ([7]).",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 6,
      "context" : "9While [7] phrases its results w.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 7,
      "context" : "On one hand we have the [8] efficiency result for α > 1 + √ 2, and on the other hand there is an almost matching lower bound:",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "Theorem 3 ([14]).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "Another relevant result of [14] is that once the parameter α exceeds 2+ √ 3, data satisfying the α-center stability condition is somewhat trivial.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "(1 + α) Weak Deletion Stability: The [4] bound on the running time of the algorithm has only polynomial explicit dependence on the number of clusters k.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "10This is a journal version of [23] where the result initially appeared.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "clusterability requirement, parameterized by α, is not extremely strong, the running time formula of [4] is, in fact, exponential in k.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, [4] show the following similar manifestation of the strong implications on the (1 + α) WDS condition, in terms of the lower bounds it implies on between-cluster-centers distances:",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "In fact, the positive results of [8], showing efficient clustering algorithm for",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "Among all the works surveyed in this note, only one, the results of [22], address (a feasible variant of) a practical algorithm - the popular Lloyd clustering algorithm.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "The recent work of [3] can be viewed as a step in that direction.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "Even for that notion, the lower bounds of [14] require the input data to be an arbitrary metric space and do not apply to data in a Euclidean space.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : ", [12], [24].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 22,
      "context" : ", [12], [24].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "Clustering via linkage based hierarchical clustering trees: The algorithm of [8] is based on a linkage-based12 agglomerative construction of a cluster tree that is guaranteed to have any (1 + √ 2)-center stable clustering of the input data as a pruning of that tree.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "See [2] for a more detailed discussion.",
      "startOffset" : 4,
      "endOffset" : 7
    } ],
    "year" : 2015,
    "abstractText" : "It is well known that most of the common clustering objectives are NPhard to optimize. In practice, however, clustering is being routinely carried out. One approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such “clusterable” instances. In other words, hope that “Clustering is difficult only when it does not matter”1 (the CDNM thesis for short). We believe that to some extent this may indeed be the case. This paper provides a survey of recent papers along this line of research and a critical evaluation their results. Our bottom line conclusion is that that CDNM thesis is still far from being formally substantiated. We start by discussing which requirements should be met in order to provide formal support the validity of the CDNM thesis. In particular, we list some implied requirements for notions of clusterability. We then examine existing results in view of these requirements and outline some research challenges and open questions. 1This phrase is in fact a title of a recent paper – [18].",
    "creator" : "LaTeX with hyperref package"
  }
}