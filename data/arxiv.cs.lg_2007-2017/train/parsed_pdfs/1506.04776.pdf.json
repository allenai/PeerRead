{
  "name" : "1506.04776.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Encog: Library of Interchangeable Machine Learning Models for Java and C#",
    "authors" : [ "Jeff Heaton" ],
    "emails" : [ "jeffheaton@acm.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n04 77\n6v 1\n[ cs\n.M S]\n1 5\nKeywords: java, c#, neural network, support vector machine, open source software"
    }, {
      "heading" : "1. Intention and Goals",
      "text" : "This paper describes the Encog API for Java and C# that is provided as a JAR or DLL library. The C# version of Encog is also compatible with the Xamarin Mono package. Encog has an active community that has provided many enhancements that are beyond the scope of this paper. This includes extensions such as Javascript, GPU processing, C/C++ support, Scala support, and interfaces to various automated trading platforms. The scope of this paper is limited to the Java and C# API.\nEncog allows the Java or C# programmer to experiment with a wide range of machine language models using a simple, consistent interface for clustering, regression, and classifications. This allows the programmer to construct applications that discover which model provides the most suitable fit for the data. Encog provides basic tools for automated model selection. Most Encog models are implemented as efficient multithreaded algorithms to reduce processing time. This often allows Encog to perform more efficiently than many other Java and C# libraries, as demonstrated empirically by Taheri (2014) and Matviykiv and Faitas (2012). Luhasz et al. (2013) and Ramos-Pollán et al. (2012) also saw favorable results when evaluating Encog to similar libraries.\nThe Encog’s API is presented in an intuitive object-oriented paradigm that allows various models, optimization algorithms, and training algorithms to be highly interchangeable. However, beneath the API, the models are represented as one and two-dimensional arrays. This internal representation allows for highly efficient calculation. The API shields the programmer from the complexity of model calculation and fitting.\nHeaton\nEncog contains nearly 400 unit tests to ensure consistency between the Java and C# model implementations. Expected results are calculated and cross-checked between the two platforms. A custom pseudorandom number generator (PRNG) is used in both language’s unit tests to ensure that even stochastic models produce consistent, verifiable test results.\nEncog contains nearly 150 examples to demonstrate the use of the API in a variety of scenarios. These examples include simple prediction, time series, simulation, financial applications, path finding, curve fitting, and other applications. Documentation for Encog is provided as Java/C# docs and an online wiki. Additionally, discussion groups and a Stack Overflow tag are maintained for support. Links to all of these resources can be found at http://www.encog.org."
    }, {
      "heading" : "2. Framework Overview",
      "text" : "The design goal of Encog is to provide interchangeable models with efficient, internal implementations. The Encog framework supports machine learning models with multiple training algorithms. These models are listed here:\n• Adaline, Feedforward, Hopfield, PNN/GRNN, RBF & NEAT neural networks\n• generalized linear regression (GLM)\n• genetic programming (tree-based)\n• k-means clustering\n• k-nearest neighbors\n• linear regression\n• self-organizing map (SOM)\n• simple recurrent network (Elman and Jordan)\n• support vector machine (SVM)\nEncog provides optimization algorithms such as particle swarm optimization (PSO) (Poli, 2008), genetic algorithms (GA), Nelder-Mead and simulated annealing. These algorithms can optimize a vector to minimize a loss function; consequently, these algorithms can fit model parameters to datasets.\nPropagation-training algorithms for neural network fitting, such as back propagation (Rumelhart et al., 1988), resilient propagation (Riedmiller and Braun, 1992), LevenbergMarquardt (Marquardt, 1963), quickpropagation (Fahlman, 1988), and scaled conjugate gradient (Møller, 1993) are included. Neural network pruning and model selection can be used to find optimal network architectures. Neural network architectures can be automatically built by a genetic algorithm using NEAT and HyperNEAT (Stanley and Miikkulainen, 2002).\nA number of preprocessing tools are built into the Encog library. Collected data can be divided into training, test, and validation sets. Time-series data can be encoded into data windows. Quantitative data can be normalized by range or z-score to prevent biases in some models. Masters (1993) normalizes qualitative data using one-of-n encoding or equilateral encoding. Encog uses these normalization techniques.\nEncog also contains extensive support for genetic programming using a tree representation (Koza, 1993). A full set of mathematical and programming functions are provided. Additionally, new functions can be defined. Constant nodes can either be drawn from a constant pool or generated as needed. Rules can optionally be added to simplify expressions and penalize specific genome patterns."
    }, {
      "heading" : "3. API Overview",
      "text" : "One of the central design philosophies of Encog is to allow models to be quickly interchanged without a great deal of code modification. A classification example will demonstrate this interchangeability, using the iris dataset (Fisher, 1936). Portions of this classification example are presented in this paper, using the Java programming language. The complete example, in both Java and C#, is provided in the Encog Quick Start Guide (available from http://www.encog.org. The Quick Start Guide also provides regression and time-series examples.\nThe following example learns to predict the species of an iris flower by using four types of measurements from each flower. To begin, the program loads the iris dataset’s CSV file. In addition to CSV, Encog contains classes to read fixed-length text, JDBC, ODBC, and XML data sources. The iris dataset is loaded, and the four measurement columns are defined as continuous values.\nVersatileDataSource source = new CSVDataSource(irisFile, false, CSVFormat.DECIMAL_POINT); VersatileMLDataSet data = new VersatileMLDataSet(source); data.defineSourceColumn(\"sepal-length\", 0, ColumnType.continuous); data.defineSourceColumn(\"sepal-width\", 1, ColumnType.continuous); data.defineSourceColumn(\"petal-length\", 2, ColumnType.continuous); data.defineSourceColumn(\"petal-width\", 3, ColumnType.continuous);\nThe species of Iris is defined as nominal value. Defining the columns as continuous, nominal or ordinal allows Encog to determine the appropriate way to encode these data for a model. For specialized cases, it is possible to override Encog’s encoding defaults for any model type.\nColumnDefinition outputColumn = data.defineSourceColumn(\"species\", 4, ColumnType.nominal);\nOnce the columns have been defined, the file is analyzed to determine minimum, maximum, and other statistical properties of the columns. This allows the columns to be properly normalized and encoded by Encog for modeling.\ndata.analyze(); data.defineSingleOutputOthersInput(outputColumn);\nNext the model type is defined to be a feedforward neural network.\nEncogModel model = new EncogModel(data); model.selectMethod(data, MLMethodFactory.TYPE_FEEDFORWARD);\nOnly the above line needs to be changed to switch to model types that include the following:\n• MLMethodFactory.SVM: support vector machine\n• MLMethodFactory.TYPE RBFNETWORK: RBF neural network\nHeaton\n• MLMethodFactor.TYPE NEAT: NEAT neural network\n• MLMethodFactor.TYPE PNN: probabilistic neural network\nNext the dataset is normalized and encoded. Encog will automatically determine the correct normalization type based on the model chosen in the last step. For model validation, 30% of the data are held back. Though the validation sampling is random, a seed of 1001 is used so that the items selected for validation remain constant between program runs. Finally, the default training type is selected.\ndata.normalize(); model.holdBackValidation(0.3, true, 1001); model.selectTrainingType(data);\nThe example trains using a 5-fold cross-validated technique that chooses the model with the best validation score. The resulting training and validation errors are displayed.\nMLRegression bestMethod = (MLRegression)model.crossvalidate(5, true); System.out.println( \"Training error: \" + EncogUtility.calculateRegressionError(\nbestMethod, model.getTrainingDataset())); System.out.println( \"Validation error: \" + EncogUtility.calculateRegressionError\n(bestMethod, model.getValidationDataset()));\nDisplay normalization parameters and final model.\nNormalizationHelper helper = data.getNormHelper(); System.out.println(helper.toString()); System.out.println(\"Final model: \" + bestMethod);"
    }, {
      "heading" : "4. Future Plans and Conclusions",
      "text" : "A number of enhancements are planned for Encog. Gradient boosting machines (GBM) and deep learning are two future model additions. Several planned enhancements will provide interoperability with other machine learning packages. Future versions of Encog will have the ability to read and write Weka Attribute-Relation File Format (ARFF) and libsvm data files. Encog will gain the ability to load and save models in the Predictive Model Markup Language (PMML) format. A code contribution by Mosca (2012) will soon be integrated, enhancing Encog’s ensemble learning capabilities."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The Encog community has been very helpful for bug reports, bug fixes, and feature suggestions. Contributors to Encog include Olivier Guiglionda, Seema Singh, César Roberto de Souza, and others. A complete list of contributors to Encog can be found at the GitHub repository: https://github.com/encog. Alan Mosca, Department of Computer Science and Information Systems, Birkbeck, University of London, UK, created Encog’s ensemble functionality. Matthew Dean, Marc Fletcher and Edmund Owen, Semiconductor Physics Research Group, University of Cambridge, UK, created Encog’s RBF Neural network model."
    } ],
    "references" : [ {
      "title" : "An empirical study of learning speed in back-propagation networks",
      "author" : [ "S. Fahlman" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Fahlman.,? \\Q1988\\E",
      "shortCiteRegEx" : "Fahlman.",
      "year" : 1988
    }, {
      "title" : "The use of multiple measurements in taxonomic problems",
      "author" : [ "R. Fisher" ],
      "venue" : "Annals of Eugenics,",
      "citeRegEx" : "Fisher.,? \\Q1936\\E",
      "shortCiteRegEx" : "Fisher.",
      "year" : 1936
    }, {
      "title" : "Genetic programming - on the programming of computers by means of natural selection. Complex adaptive systems",
      "author" : [ "J. Koza" ],
      "venue" : null,
      "citeRegEx" : "Koza.,? \\Q1993\\E",
      "shortCiteRegEx" : "Koza.",
      "year" : 1993
    }, {
      "title" : "Data mining considerations for knowledge acquisition in real time strategy games",
      "author" : [ "G. Luhasz", "V. Munteanu", "V. Negru" ],
      "venue" : "In IEEE 11th International Symposium on Intelligent Systems and Informatics,",
      "citeRegEx" : "Luhasz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Luhasz et al\\.",
      "year" : 2013
    }, {
      "title" : "An algorithm for least-squares estimation of nonlinear parameters",
      "author" : [ "D. Marquardt" ],
      "venue" : "SIAM Journal on Applied Mathematics,",
      "citeRegEx" : "Marquardt.,? \\Q1963\\E",
      "shortCiteRegEx" : "Marquardt.",
      "year" : 1963
    }, {
      "title" : "Practical Neural Network Recipes in C++",
      "author" : [ "T. Masters" ],
      "venue" : null,
      "citeRegEx" : "Masters.,? \\Q1993\\E",
      "shortCiteRegEx" : "Masters.",
      "year" : 1993
    }, {
      "title" : "Faitas. Data classification of spectrum analysis using neural network",
      "author" : [ "O.O. Matviykiv" ],
      "venue" : "Lviv Polytechnic National University,",
      "citeRegEx" : "Matviykiv,? \\Q2012\\E",
      "shortCiteRegEx" : "Matviykiv",
      "year" : 2012
    }, {
      "title" : "A scaled conjugate gradient algorithm for fast supervised learning",
      "author" : [ "M. Møller" ],
      "venue" : "NEURAL NETWORKS,",
      "citeRegEx" : "Møller.,? \\Q1993\\E",
      "shortCiteRegEx" : "Møller.",
      "year" : 1993
    }, {
      "title" : "Extending encog: A study on classifier ensemble techniques",
      "author" : [ "A. Mosca" ],
      "venue" : "Master’s thesis, Birkbeck, University of London,",
      "citeRegEx" : "Mosca.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mosca.",
      "year" : 2012
    }, {
      "title" : "Analysis of the publications on the applications of particle swarm optimisation",
      "author" : [ "R. Poli" ],
      "venue" : "J. Artif. Evol. App.,",
      "citeRegEx" : "Poli.,? \\Q2008\\E",
      "shortCiteRegEx" : "Poli.",
      "year" : 2008
    }, {
      "title" : "A software framework for building biomedical machine learning classifiers through grid computing resources",
      "author" : [ "Raúl Ramos-Pollán", "Miguel Ángel Guevara-López", "Eugénio C. Oliveira" ],
      "venue" : "J. Medical Systems,",
      "citeRegEx" : "Ramos.Pollán et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ramos.Pollán et al\\.",
      "year" : 2012
    }, {
      "title" : "Rprop - a fast adaptive learning algorithm",
      "author" : [ "M. Riedmiller", "H. Braun" ],
      "venue" : "Technical report, Proc. of ISCIS VII), Universitat,",
      "citeRegEx" : "Riedmiller and Braun.,? \\Q1992\\E",
      "shortCiteRegEx" : "Riedmiller and Braun.",
      "year" : 1992
    }, {
      "title" : "Neurocomputing: Foundations of research",
      "author" : [ "D. Rumelhart", "G. Hinton", "R. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1988
    }, {
      "title" : "Evolving neural networks through augmenting topologies",
      "author" : [ "K. Stanley", "R. Miikkulainen" ],
      "venue" : "Evol. Comput.,",
      "citeRegEx" : "Stanley and Miikkulainen.,? \\Q2002\\E",
      "shortCiteRegEx" : "Stanley and Miikkulainen.",
      "year" : 2002
    }, {
      "title" : "Benchmarking and comparing encog, neuroph and joone neural networks. http://goo.gl/A56iyx",
      "author" : [ "T. Taheri" ],
      "venue" : null,
      "citeRegEx" : "Taheri.,? \\Q2014\\E",
      "shortCiteRegEx" : "Taheri.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "This often allows Encog to perform more efficiently than many other Java and C# libraries, as demonstrated empirically by Taheri (2014) and Matviykiv and Faitas (2012).",
      "startOffset" : 122,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "This often allows Encog to perform more efficiently than many other Java and C# libraries, as demonstrated empirically by Taheri (2014) and Matviykiv and Faitas (2012). Luhasz et al.",
      "startOffset" : 140,
      "endOffset" : 168
    }, {
      "referenceID" : 3,
      "context" : "Luhasz et al. (2013) and Ramos-Pollán et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "Luhasz et al. (2013) and Ramos-Pollán et al. (2012) also saw favorable results when evaluating Encog to similar libraries.",
      "startOffset" : 0,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "Encog provides optimization algorithms such as particle swarm optimization (PSO) (Poli, 2008), genetic algorithms (GA), Nelder-Mead and simulated annealing.",
      "startOffset" : 81,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Propagation-training algorithms for neural network fitting, such as back propagation (Rumelhart et al., 1988), resilient propagation (Riedmiller and Braun, 1992), LevenbergMarquardt (Marquardt, 1963), quickpropagation (Fahlman, 1988), and scaled conjugate gradient (Møller, 1993) are included.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : ", 1988), resilient propagation (Riedmiller and Braun, 1992), LevenbergMarquardt (Marquardt, 1963), quickpropagation (Fahlman, 1988), and scaled conjugate gradient (Møller, 1993) are included.",
      "startOffset" : 31,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : ", 1988), resilient propagation (Riedmiller and Braun, 1992), LevenbergMarquardt (Marquardt, 1963), quickpropagation (Fahlman, 1988), and scaled conjugate gradient (Møller, 1993) are included.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : ", 1988), resilient propagation (Riedmiller and Braun, 1992), LevenbergMarquardt (Marquardt, 1963), quickpropagation (Fahlman, 1988), and scaled conjugate gradient (Møller, 1993) are included.",
      "startOffset" : 116,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : ", 1988), resilient propagation (Riedmiller and Braun, 1992), LevenbergMarquardt (Marquardt, 1963), quickpropagation (Fahlman, 1988), and scaled conjugate gradient (Møller, 1993) are included.",
      "startOffset" : 163,
      "endOffset" : 177
    }, {
      "referenceID" : 13,
      "context" : "Neural network architectures can be automatically built by a genetic algorithm using NEAT and HyperNEAT (Stanley and Miikkulainen, 2002).",
      "startOffset" : 104,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : ", 1988), resilient propagation (Riedmiller and Braun, 1992), LevenbergMarquardt (Marquardt, 1963), quickpropagation (Fahlman, 1988), and scaled conjugate gradient (Møller, 1993) are included. Neural network pruning and model selection can be used to find optimal network architectures. Neural network architectures can be automatically built by a genetic algorithm using NEAT and HyperNEAT (Stanley and Miikkulainen, 2002). A number of preprocessing tools are built into the Encog library. Collected data can be divided into training, test, and validation sets. Time-series data can be encoded into data windows. Quantitative data can be normalized by range or z-score to prevent biases in some models. Masters (1993) normalizes qualitative data using one-of-n encoding or equilateral encoding.",
      "startOffset" : 117,
      "endOffset" : 718
    }, {
      "referenceID" : 2,
      "context" : "Encog also contains extensive support for genetic programming using a tree representation (Koza, 1993).",
      "startOffset" : 90,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "A classification example will demonstrate this interchangeability, using the iris dataset (Fisher, 1936).",
      "startOffset" : 90,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "A code contribution by Mosca (2012) will soon be integrated, enhancing Encog’s ensemble learning capabilities.",
      "startOffset" : 23,
      "endOffset" : 36
    } ],
    "year" : 2015,
    "abstractText" : "This paper introduces the Encog library for Java and C#, a scalable, adaptable, multiplatform machine learning framework that was first released in 2008. Encog allows a variety of machine learning models to be applied to datasets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from http://www.encog.org.",
    "creator" : "LaTeX with hyperref package"
  }
}