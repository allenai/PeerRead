{
  "name" : "1602.02514.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Fast K-Means with Accurate Bounds",
    "authors" : [ "James Newling", "François Fleuret" ],
    "emails" : [ "JAMES.NEWLING@IDIAP.CH", "FRANCOIS.FLEURET@IDIAP.CH" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The k-means problem is to compute a set of k centroids to minimise the sum over data-points of the squared distance to the nearest centroid. It is an NP-hard problem for which various effective approximation algorithms exist. The most popular is often referred to as Lloyd’s algorithm, or simply as the k-means algorithm. It has applications in data compression, data classification, density estimation and many other areas, and was recognised in Wu et al. (2008) as one of the top-10 algorithms in data mining.\nLloyd’s algorithm relies on a two-step iterative process: In the assignment step, each sample is assigned to the cluster whose centroid is nearest. In the update step, cluster centroids are updated in accordance with their assigned samples. Lloyd’s algorithm is also called the exact k-means\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nalgorithm, as there is no approximation in either of the two steps. This name can lead to confusion as the algorithm does not solve the k-means problem exactly.\nThe linear dependence on the number of clusters, the number of samples and the dimension of the space, means that Lloyd’s algorithm requires upwards of a billion floating point operations per round on standard datasets such as those used in our experiments (§4). This, coupled with slow convergence and the fact that several runs are often performed to find improved solutions, can make it slow.\nLloyd’s algorithm does not state how the assignment and update steps should be performed, and as such provides a scaffolding on which more elaborate algorithms can be constructed. These more elaborate algorithms, often called accelerated exact k-means algorithms, are the primary focus of this paper. They can be dropped-in wherever Lloyd’s algorithm is used.\n1.1. Approximate k-means\nAlternatives to exact k-means have been proposed. Certain of these rely on a relaxation of the assignment step, for example by only considering certain clusters according to some hierarchical ordering (Nister & Stewenius, 2006), or by using an approximate nearest neighbour search as in Philbin et al. (2007). Others rely on a relaxation of the update step, for example by using only a subset of data to update centroids (Frahling & Sohler, 2006; Sculley, 2010).\nWhen comparing approximate k-means clustering algorithms such as those just mentioned, the two criteria of interest are the quality of the final clustering, and the computational requirements. The two criteria are not independent, making comparison between algorithms difficult and often preventing their adoption. When comparing accelerated exact k-means algorithms on the other hand, all algorithms produce the same final clustering, and so comparisons can be made based on speed alone. Once an accelerated exact k-means algorithm has been confirmed to provide a speedup, it is rapidly adopted, automatically inheriting the trust which the exact algorithm has gained through its simplicity ar X\niv :1\n60 2.\n02 51\n4v 6\n[ st\nat .M\nL ]\n1 1\nSe p\n20 16\nand extensive use over several decades.\n1.2. Accelerated Exact k-means\nThe first published accelerated k-means algorithms borrowed techniques used to accelerate the nearest neighbour search. Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al. (2002). These algorithms relied on storing centroids in special data structures, enabling nearest neighbor queries to be processed without computing distances to all k centroids.\nThe next big acceleration (Elkan, 2003) came about by maintaing bounds on distances between samples and centroids, frequently resulting in more than 90% of distance calculations being avoided. It was later shown (Hamerly, 2010) that in low-dimensions, it is more effective to keep bounds on distances to only the two nearest centroids, and that in general bounding-based algorithms are significantly faster than tree-based ones. Further bounding-based algorithms were proposed by Drake (2013) and Ding et al. (2015), each providing accelerations over their predecessors in certain settings. In this paper, we continue in the same vain."
    }, {
      "heading" : "1.3. Our Contribution",
      "text" : "Our first contribution (§3.1) is a new bounding-based accelerated exact k-means algorithm, the Exponion algorithm. Its closest relative is the Annular algorithm (Drake, 2013), which is the current state-of-the-art accelerated exact kmeans algorithm in low-dimensions. We show that the Exponion algorithm is significantly faster than the Annular algorithm on a majority of low-dimensional datasets.\nOur second contribution (§3.2) is a technique for making bounds tighter, allowing further redundant distance calculations to be eliminated. The technique, illustrated in Figure 1, can be applied to all existing bounding-based kmeans algorithms.\nFinally, we show how certain of the current state-of-theart algorithms can be accelerated through strict simplifications (§2.2 and §2.6). Fully parallelised implementations of all algorithms are provided under an open-source license at https://github.com/idiap/eakmeans"
    }, {
      "heading" : "2. Notation and baselines",
      "text" : "We describe four accelerated exact k-means algorithms in order of publication date. For two of these we propose simplified versions which offer natural stepping stones in understanding the full versions, as well being faster (§4.1.2).\nOur notation is based on that of Hamerly (2010), and only where necessary is new notation introduced. We use for ex-\nample N for the number of samples and k for the number of clusters. Indices i and j always refer to data and cluster indices respectively, with a sample denoted by x(i) and the index of the cluster to which it is assigned by a(i). A cluster’s centroid is denoted as c(j). We introduce new notation by letting n1(i) and n2(i) denote the indices of the clusters whose centroids are the nearest and second nearest to sample i respectively.\nNote that a(i) and n1(i) are different, with the objective in a round of k-means being to set a(i) to n1(i). a(i) is a variable maintained by algorithms, changing within loops whenever a better candidate for the nearest centroid is found. On the other hand, n1(i) is introduced purely to aid in proofs, and is external to any algorithmic details. It can be considered to be the hidden variable which algorithms need to reveal.\nAll of the algorithms which we consider are elaborations of Lloyd’s algorithm, and thus consist of repeating the assignment step and update step, given respectively as\na(i)← n1(i), i ∈ {1, . . . , N} (1) c(j)← ∑ i:a(i)=j x(i)\n‖i : a(i) = j‖ , j ∈ {1, . . . , k}. (2)\nThese two steps are repeated until there is no change to any a(i), or some other stopping criterion is met. We reiterate that all the algorithms discussed provide the same output at each iteration of the two steps, differing only in how a(i) is computed in (1).\n2.1. Standard algorithm (sta)\nThe Standard algorithm, henceforth sta, is the simplest implementation of Lloyd’s algorithm. The only variables kept are x(i) and a(i) for i ∈ {1, . . . , N} and c(j) for j ∈ {1, . . . , k}. The assignment step consists of, for each i, calculating the distance from x(i) to all centroids, thus revealing n1(i).\n2.2. Simplified Elkan’s algorithm (selk)\nSimplified Elkan’s algorithm, henceforth selk, uses a strict subset of the strategies described in Elkan (2003). In addition to x(i), a(i) and c(j), the variables kept are p(j), the distance moved by c(j) in the last update step, and bounds l(i, j) and u(i), maintained to satisfy,\nl(i, j) ≤ ‖x(i)− c(j)‖, u(i) ≥ ‖x(i)− c(a(i))‖.\nThese bounds are used to eliminate unnecessary centroiddata distance calculations using,\nu(i) < l(i, j) =⇒ ‖x(i)− c(a(i))‖ < ‖x(i)− c(j)‖ =⇒ j 6= n1(i). (3)\nWe refer to (3) as an inner test, as it is performed within a loop over centroids for each sample. This as opposed to an outer test which is performed just once per sample, examples of which will be presented later.\nTo maintain the correctness of the bounds when centroids move, bounds are updated at the beginning of each assignment step with\nl(i, j)← l(i, j)− p(j), u(i)← u(i) + p(a(i)). (4)\nThe validity of these updates is a simple consequence of the triangle inequality (proof in SM-B.1). We say that a bound is tight if it is known to be equal to the distance it is bounding, a loose bound is one which is not tight. For selk, bounds are initialised to be tight, and tightening a bound evidently costs one distance calculation.\nWhen in a given round u(i) ≥ l(i, j), the test (3) fails. The first time this happens, both u(i) and l(i, j) are loose due to preceding bound updates of the form (4). Tightening either bound may result in the test succeeding. Bound u(i) should be tightened before l(i, j), as it reappears in all tests for sample i and will thus be reused. In the case of a test failure with tight u(i) and loose l(i, j) we tighten l(i, j). A test failure with u(i) and l(i, j) both tight implies that centroid j is nearer to sample i than the currently assigned cluster centroid, and so a(i)← j and u(i)← l(i, j).\n2.3. Elkan’s algorithm (elk)\nThe fully-fledged algorithm of Elkan (2003), henceforth elk, adds to selk an additional strategy for eliminating distance calculations in the assignment step. Two further variables, cc(j, j′), the matrix of inter-centroid distances, and s(j), the distance from centroid j to its nearest other centroid, are kept. A simple application of the triangle inequality, shown in SM-B.2, provides the following test,\ncc(a(i), j)\n2 > u(i) =⇒ j 6= n1(i). (5)\nelk uses (5) in unison with (3) to obtain an improvement on the test of elk, of the form,\nmax ( l(i, j), cc(a(i), j)\n2\n) > u(i) =⇒ j 6= n1(i). (6)\nIn addition to the inner test (6), elk uses an outer test, whose validity follows from that of (5), given by,\ns(a(i))\n2 > u(i) =⇒ n1(i) = a(i). (7)\nIf the outer test (7) is successful, one proceeds immediately to the next sample without changing a(i).\n2.4. Hamerly’s algorithm (ham)\nThe algorithm of Hamerly (2010), henceforth ham, represents a shift of focus from inner to outer tests, completely foregoing the inner test of elk, and providing an improved outer test.\nThe k lower bounds per sample of elk are replaced by a single lower bound on all centroids other than the one assigned,\nl(i) ≤ min j 6=a(i) ‖x(i)− c(j)‖.\nThe variables p(j) and u(i) used in elk have the same definition for ham. The test for a sample i is\nmax ( l(i), s(a(i))\n2\n) > u(i) =⇒ n1(i) = a(i), (8)\nwith the proof of correctness being essentially the same as that for the inner test of elk. If test (8) fails, then u(i) is made tight. If test (8) fails with u(i) tight, then the distances from sample i to all centroids are computed, revealing n1(i) and n2(i), allowing the updates a(i) ← n1(i), u(i) ← ‖x(i) − c(n1(i))‖ and l(i) ← ‖x(i) − c(n2(i))‖. As with elk, at the start of the assignment step, bounds need to be adjusted to ensure their correctness following the update step. This is done via,\nl(i)← l(i)− arg max j 6=a(i) p(a(i)), u(i)← u(i) + p(a(i)).\n2.5. Annular algorithm (ann)\nThe Annular algorithm of Drake (2013), henceforth ann, is a strict extension of ham, adding one novel test. In addition to the variables used in ham, one new variable b(i) is required, which roughly speaking is to n2(i) what a(i) is to n1(i). Also, the centroid norms ‖c(j)‖ should be computed and sorted in each round.\nUpon failure of test (8) with tight bounds in ham, ‖x(i) − c(j)‖ is computed for all j ∈ {1, . . . , k} to reveal n1(i) and n2(i). With ann, certain of these k calculations can be eliminated. Define the radius, and corresponding set of cluster indices,\nR(i) = max (u(i), ‖x(i)− c(b(i))‖) , J (i) = {j : |‖c(j)‖ − ‖x(i)‖| ≤ R(i)}. (9)\nThe following implication (proved in SM-B.3) is used\nj 6∈ J (i) =⇒ j 6∈ {n1(i), n2(i)}.\nThus only distances from sample i to centroids of the clusters whose indices are in J (i) need to be calculated for n1(i) and n2(i) to be revealed. Once n1(i) and n2(i) revealed, a(i), u(i) and l(i) are updated as per ham, and b(i)← n2(i).\nNote that by keeping an ordering of ‖c(j)‖ the set J (i) can be determined in Θ(log(K)) operations with two binary searches, one for each of the inner and outer radii of J (i).\n2.6. Simplified Yinyang (syin) and Yinyang (yin) algorithms\nThe basic idea with the Yinyang algorithm (Ding et al., 2015) and the Simplified Yinyang algorithm, henceforth yin and syin respectively, is to maintain consistent lower bounds for groups of clusters as a compromise between the k − 1 lower bounds of elk and the single lower bound of ham. In Ding et al. (2015) the number of groups is fixed at one tenth the number of centroids. The groupings are determined and fixed by an initial clustering of the centroids. The algorithm appearing in the literature most similar to yin is Drake’s algorithm of (Drake & Hamerly, 2012), not to be confused with ann. According to Ding et al. (2015), Drake’s algorithm does not perform as well as yin, and we thus choose not to consider it in this paper.\nDenote by G the number of groups of clusters. Variables required in addition to those used in sta are p(j) and u(i), as per elk, G(f), the set of indices of clusters belonging to the f ’th group, g(i), the group to which cluster a(i) belongs, q(f) = maxj∈G p(j), and bound l(i, f), maintained to satisfy,\nl(i, f) ≤ arg min j∈G(f)\\{a(i)} ‖x(i)− c(j)‖.\nFor both syin and yin, both an outer test and group tests are used. To these, yin adds an inner test. The outer test is\nmin f∈{1,...,G}\nl(i, f) > u(i) =⇒ a(i) = n1(i). (10)\nIf and when test (10) fails, group tests of the form\nl(i, f) > u(i) =⇒ a(i) 6∈ G(f), (11)\nare performed. As with elk and ham, if test (11) fails with u(i) loose, u(i) is made tight and the test reperformed.\nThe difference between syin and yin arises when (11) fails with u(i) tight. With syin, the simple approach of computing distances from x(i) to all centroids in G(f), then updating l(i, f), l(i, g(i)), u(i), a(i) and g(i) as necessary, is taken. With yin a final effort at eliminating distance calculations by the use of a local test is made, as described in SM-C.1. As will be shown (§4.1.2), it is not clear that the local test of yin makes it any faster. Finally, we mention how u(i) and l(i, f) are updated at the beginning of the assignment step for the syin and yin,\nl(i, f)← l(i, f)− arg max j∈G(f) p(a(i)),\nu(i)← u(i) + p(a(i))."
    }, {
      "heading" : "3. Contributions and New Algorithms",
      "text" : "We first present (§3.1) an algorithm which we call Exponion, and then (§3.2) an improved bounding approach.\n3.1. Exponion algorithm (exp)\nLike ann, exp is an extension of ham which adds a test to filter out j 6∈ {n1(i), n2(i)} when test (8) fails. Unlike ann, where the filter is an origin-centered annulus, exp has as filter a ball centred on centroid a(i). This change is motivated by the ratio of volumes of an annulus of width r at radius w and a ball of radius r from the origin, which is d ( w r )d−1 in Rd. We expect r to be greater than w, whence the expected improvement. Define,\nR(i) = 2u(i) + s(a(i)),\nJ (i) = {j : ‖c(j)− c(a(i))‖ ≤ R(i)}. (12)\nThe underlying test used (proof in SM-B.4) is\nj 6∈ J (i) =⇒ j 6∈ {n1(i), n2(i)}.\nIn moving from ann to exp, the decentralisation from the origin to the centroids incurs two costs, one which can be explained algorithmically, the other is related to cache memory.\nRecall that ann sorts ‖c(j)‖ in each round, thus guaranteeing that the set of candidate centroids (9) can be obtained in O(log(k)) operations. To guarantee that the set of candidate centroids (12) can be obtained with O(log(k)) operations requires that ‖c(j) − c(a(i))‖ be sorted. For this to be true for all samples requires sorting ‖c(j) − c(j′)‖ for all j ∈ {1, . . . , k}, increasing the overhead of sorting from O(k log k) to O(k2 log k).\nThe cache related cost is that, unless samples are ordered by a(i), the bisection search performed to obtain J (i) is done with a different row of c(j, j′) for each sample, resulting in cache memory misses.\nTo offset these costs, we replace the exact sorting of ccwith a partial sorting, paying for this approximation with additional distance calculations. We maintain, for each centroid, dlog2 ke concentric annuli, each succesive annulus containing twice as many centroids as the one interior to it. For cluster j, annulus f ∈ {1, . . . , dlog2 ke} is defined by inner and outer radii e(j, f − 1) and e(j, f), and a list of indices w(j, f) with |w(j, f)| = 2f , where\nw(j, f) = {j′ : e(j, f − 1) < ‖c(j′)− c(j)‖ ≤ e(j, f)}.\nNote that w(j, f) is not an ordered set, but there is an ordering between sets,\nj′ ∈ w(j, f), j′′ ∈ w(j, f + 1) =⇒ ‖c(j′)− c(j)‖ < ‖c(j′′)− c(j)‖.\nGiven a search radius R(i), without a complete ordering of c(j, j′) we cannot obtain J (i) inO(log(k)) operations, but we can obtain a slightly larger set J ∗(i) defined by\nf∗(i) = min{f : e(a(i), f) ≥ R(i)}, J ∗(i) = ⋃\nf≤f∗(i)\nw(j, f),\nin log log(k) operations. It is easy to see that |J ∗(i)| ≤ 2|J (i)|, and so using the partial sorting cannot cost more than twice the number of distance calculations."
    }, {
      "heading" : "3.2. Improving bounds (sn to ns)",
      "text" : "In all the algorithms presented so far, upper bounds (lower bounds) are updated in each round with increments (decrements) of norms of displacements. If tests are repeatedly successful, these increments (decrements) accumulate. Consider for example the upper bound update,\nut0+1(i)← ut0(i) + pt0(a(i)),\nwhere subscripts denote rounds. The upper bound after δt such updates without bound tightening is\nut0+δt(i) = ut0(i) + t+δt−1∑ t′=t0 pt′(a(i)). (13)\nThe summation term is a (s)um of (n)orms of displacement, thus we refer to it as an sn-bound and to an algorithm using only such an update scheme as an sn-algorithm. An alternative upper bound at round t0 + δt is,\nut0+δt(i) = ut0(i) + ∥∥∥∥∥ t0+δt−1∑ t′=t0 ct′+1(i)− ct′(i) ∥∥∥∥∥ , = ut0(i) + ‖ct0+δt(i)− ct0(i)‖. (14)\nBound (14) derives from the (n)orm of a (s)um, and hence we refer to it as an ns-bound. An ns-bound is guaranteed to be tighter than its equivalent sn-bound by a trivial application of the triangle inequality (proved in SM-B.5). We have presented an upper ns-bound, but lower ns-bound formulations are similar. In fact, for cases where lower bounds apply to several distances simultaneously, due to the additional operation of taking a group maximum, there are three possible ways to compute a lower bound, details in §SMC.2.\n3.3. Simplified Elkan’s algorithm-ns (selk-ns)\nIn transforming an sn-algorithm into an ns-algorithm, additional variables need to be maintained. These include a record of previous centroids C, where C(j, t) = ct(j), and displacement of c(j) with respect to previous centroids, P (j, t) = ‖c(j) − ct(j)‖. We no longer keep rolling bounds for each sample, instead we keep a record of when most recently bounds were made tight and the distances then calculated. For Simplified Elkan’s Algorithmns, henceforth selk-ns, we define T (i, j) to be the last time ‖x(i)− c(j)‖ was calculated, with corresponding distance l(i, j) = ‖x(i) − cT (i,j)(j)‖. We emphasise that l(i, j) is defined differently here to in selk, with u(i) similarly redefined as u(i) = ‖x(i)− cT (i,a(i))(a(i))‖.\nThe underlying test is\nu(i) + P (a(i), T (i, a(i))) < l(i, j)−P (j, T (i, j)) =⇒ j 6= n1(i).\nAs with selk, the first bound failure for sample i results in u(i) being updated, with subsequent failures resulting in l(i, j) being updated to the current distance. In addition, when u(i) (l(i, j)) is updated, T (i, a(i)) (T (i, j)) is set to the current round.\nDue to the additional variables C,P and T , the memory requirement imposed is larger with selk-ns than with selk-sn. Ignoring constants, in round t the memory requirement assuming samples of size O(d) is,\nmemns = O(Nd+Nk + ktd),\nwhere x, l and C are the principal contributors to the above three respective terms. selk consists of only the first two terms, and so when t > N/min(k, d), the dominant memory consumer in selk-ns is the new variable C. To guarantee that C does not dominate memory consumption, an sn-like reset is performed in rounds {t : t ≡ 0 mod (N/min(k, d))}, consisting of the following updates,\nu(i)← u(i) + P (a(i), T (i, a(i))), l(i, j)← l(i, j)− P (j, T (i, j)), T (i, j)← t,\nand finally the clearing of C."
    }, {
      "heading" : "3.4. Changing Bounds for Other Algorithms",
      "text" : "All sn- to ns- coversions are much the same as that described in Section 3.3. We have implemented versions of elk, syin and exp using ns-bounds, which we refer to as elk-ns, syin-ns and exp-ns respectively."
    }, {
      "heading" : "4. Experiments and Results",
      "text" : "Our first set of experiments are conducted using a single core. We first establish that our implementations of baseline algorithms are as fast or faster than existing implementations. Having done this, we consider the effects of the novel algorithmic contributions presented, simplification, the Exponion algorithm, and ns-bounding. The final set of experiments are conducted on multiple cores, and illustrate how all algorithms presented parallelise gracefully.\nWe compare 23 k-means implementations, including our own implementations of all algorithms described, original implementations accompanying the papers (Hamerly, 2010; Drake, 2013; Ding et al., 2015), and implementations in two popular machine learning libraries, VLFeat and mlpack. We use the following notation to refer to implementations: {codesource-algorithm}, where codesource is one of bay (Hamerly, 2015), mlp (Curtin et al., 2013), pow (Low et al., 2010), vlf (Vedaldi & Fulkerson, 2008) and own (our own code), and algorithm is one of the algorithms described.\nUnless otherwise stated, times are wall times excluding data loading. We impose a time limit of 40 minutes and a memory limit of 4 GB on all {dataset, implementation, k, seed} runs. If a run fails to complete in 40 minutes, the corresponding table entry is ‘t’. Similarly, failure to execute with 4GB of memory results in a table entry ‘m’. We confirm that for all {dataset, k, seed} triplets, all implementations which complete within the time and memory constraint take the same number of iterations to converge to a common local minimum, as expected.\nThe implementations are compared over the 22 datasets presented in Table 1, for k ∈ {100, 1000}, with 10 distinct centroid initialisations (seeds). For all {dataset, k, seed} triplets, the 23 implementations are run serially on a machine with an Intel i7 processor and 8MB of cache memory. All experiments are performed using double precision floating point numbers.\nFindings in Drake (2013) suggest that the best algorithm to use for a dataset depends primarily on dimension, where in low-dimensions, ham and ann are fastest, in highdimensions elk is fastest, and in intermediate dimensions an approach maintaining a fractional number of bounds, Drake’s algorithm, is fastest. Our findings corroborate these on real datasets, although the lines separating the\nthree groups are blurry. In presenting our results we prefer to consider a partitioning of the datasets into just two groups about the dimension d = 20. ham and its derivatives are considered for d < 20, elk and its derivatives for d ≥ 20, and syin and yin for all d."
    }, {
      "heading" : "4.1. Single core experiments",
      "text" : "A complete presentation of wall times and number of iterations for all {dataset, implementation, k} triplets is presented over two pages in Tables 9 and 10 (§SM-D). Here we attempt to summarise our findings. We first compare implementations of published algorithms (§4.1.1), and then show how selk and syin often outperform their more complex counterparts (§4.1.2). We show that exp is in general much faster than ann (§4.1.3), and finally show how using ns-bounds can accelerate algorithms (§4.1.4) ."
    }, {
      "heading" : "4.1.1. COMPARING IMPLEMENTATIONS OF BASELINES",
      "text" : "There are algorithmic techniques which can speedup all kmeans algorithms discussed in this paper, we mention a few which we use. One is pre-computing the squares of norms of all samples just once, and those of centroids once per round. Another, first suggested in Hamerly (2010), is to update the sum of samples by considering only those samples whose assignment changed in the previous round. A third optimisation technique is to decompose while-loops which contain inner branchings dependant on the tightness of upper bounds into separate while-loops, eliminating unnecessary comparisons. Finally, while there are no large matrix operations with bounding-based algorithms, in highdimensions distance calculations can be accelerated by the use of SSE, as in VLFeat, or by fast implementations of BLAS, such as OpenBLAS (Xianyi, 2016).\nOur careful attention to optimisation is reflected in Table 7 (§A), where implementations of elk, ham, ann and yin are compared. The values shown are ratios of mean runtimes using another implementation (column) and our own implementation of the same algorithm, on a given dataset\n(row). Our implementations are faster in all but 4 comparisons."
    }, {
      "heading" : "4.1.2. BENEFITS OF SIMPLIFICATION",
      "text" : "We compare published algorithms elk and yin with their simplified counterparts selk and syin. The values in Table 2 are ratios of mean runtimes using simplified and original algorithms, values less than 1 mean that the simplified version is faster. We observe that selk is faster than elk in 16 of 18 experiments, and syin is faster than yin in 43 of 44 experiments, often dramatically so.\nIt is interesting to ask why the inventors of elk and yin did not instead settle on algorithms selk and syin respectively. A partial answer might relate to the use of BLAS, as the speedup obtained by simplifying yin to syin never exceeds more than 10% when BLAS is deactivated. syin is more responsive to BLAS than yin as it has larger matrix multiplications due to it not having a final filter."
    }, {
      "heading" : "4.1.3. FROM ANNULAR TO EXPONION",
      "text" : "We compare the Annular algorithm (ann) with the Exponion algorithm (exp). The values in Table 3 are ratios of mean runtimes (columns qt) and of mean number of distance calculations (columns qau). Values less than 1 denote better performance with exp. We observe that exp is markedly faster than ann on most low-dimensional datasets, reducing by more than 30% the mean runtime in 17 of 22 experiments. The primary reason for the speedup is the reduced number of distance calculations.\nTable 4 summarises how many times each of the snalgorithms is fastest on the 44 {dataset, k} experiments, ns-algorithms excluded. The 13 experiments on which exp is fastest are all very low-dimensional (d < 5), the 24 on which syin is fastest are intermediate (8 < d < 69) and selk or elk are fastest in very high dimensions (d > 73). For a detailed comparison across all algorithms, consult Tables 9 and 10 (§SM-D)."
    }, {
      "heading" : "4.1.4. FROM SN TO NS BOUNDING",
      "text" : "For each of the 44 {dataset, k} experiments, we compare the fastest sn-algorithm with its ns-variant. The results are presented in Table 5. Columns ‘x’ denote the fastest snalgorithm. Values are ratios of means over runs of some quantity using the ns- and sn- variants. The ratios are qt (runtimes), qa (number of distance calculations in the assignment step) and qau (total number of distance calculations).\nIn all but 8 of 44 experiments (italicised), we observe a speedup using ns-bounding, by up to 45%. As expected, the number of distance calculations in the assignment step is never greater when using ns-bounds, however the total number of distance calculations is occasionally increased due to initial variables being maintained."
    }, {
      "heading" : "4.2. Multicore experiments",
      "text" : "We have implemented parallelised versions of all algorithms described in this paper using the C++11 thread support library. To measure the speedup using multiple cores, we compare the runtime using four threads to that using one thread on a non-hyperthreading four core machine.\nThe results are summarised in Table 6, where near fourfold speedups are observed."
    }, {
      "heading" : "5. Conclusion and future work",
      "text" : "The experimental results presented show that the nsbounding scheme makes exact k-means algorithms faster, and that our Exponion algorithm is significantly faster than existing state-of-the-art algorithms in low-dimensions. Both can be seen as good default choices for k-means clus-\ntering on large data-sets.\nThe main practical weakness that remains is the necessary prior selection of which algorithm to use, depending on the dimensionality of the problem at hand. This should be addressed through an adaptive procedure able to select automatically the optimal algorithm through an efficient exploration/exploitation strategy. The second and more prospective direction of work will be to introduce a sharing of information between samples, instead of processing them independently."
    }, {
      "heading" : "Acknowledgements",
      "text" : "James Newling was funded by the Hasler Foundation under the grant 13018 MASH2.\nA. Table Comparing Implementations"
    } ],
    "references" : [ {
      "title" : "Multidimensional binary search trees used for associative searching",
      "author" : [ "Bentley", "Jon Louis" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Bentley and Louis.,? \\Q1975\\E",
      "shortCiteRegEx" : "Bentley and Louis.",
      "year" : 1975
    }, {
      "title" : "MLPACK: A scalable C++ machine learning library",
      "author" : [ "Curtin", "Ryan R", "Cline", "James R", "Slagle", "Neil P", "March", "William B", "P. Ram", "Mehta", "Nishant A", "Gray", "Alexander G" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Curtin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Curtin et al\\.",
      "year" : 2013
    }, {
      "title" : "Accelerated k-means with adaptive distance bounds",
      "author" : [ "Drake", "Jonathan", "Hamerly", "Greg" ],
      "venue" : "In 5th NIPS Workshop on Optimization for Machine Learning,",
      "citeRegEx" : "Drake et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Drake et al\\.",
      "year" : 2012
    }, {
      "title" : "Using the triangle inequality to accelerate k-means",
      "author" : [ "Elkan", "Charles" ],
      "venue" : "In Machine Learning, Proceedings of the Twentieth International Conference (ICML",
      "citeRegEx" : "Elkan and Charles.,? \\Q2003\\E",
      "shortCiteRegEx" : "Elkan and Charles.",
      "year" : 2003
    }, {
      "title" : "A fast kmeans implementation using coresets",
      "author" : [ "Frahling", "Gereon", "Sohler", "Christian" ],
      "venue" : "In Proceedings of the Twenty-second Annual Symposium on Computational Geometry,",
      "citeRegEx" : "Frahling et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Frahling et al\\.",
      "year" : 2006
    }, {
      "title" : "Making k-means even faster",
      "author" : [ "Hamerly", "Greg" ],
      "venue" : "In SDM, pp. 130–140,",
      "citeRegEx" : "Hamerly and Greg.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hamerly and Greg.",
      "year" : 2010
    }, {
      "title" : "Graphlab: A new parallel framework for machine learning",
      "author" : [ "Low", "Yucheng", "Gonzalez", "Joseph", "Kyrola", "Aapo", "Bickson", "Danny", "Guestrin", "Carlos", "Hellerstein", "Joseph M" ],
      "venue" : "In Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Low et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2010
    }, {
      "title" : "Scalable recognition with a vocabulary tree",
      "author" : [ "Nister", "David", "Stewenius", "Henrik" ],
      "venue" : "In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2,",
      "citeRegEx" : "Nister et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nister et al\\.",
      "year" : 2006
    }, {
      "title" : "A fast nearest-neighbor search algorithm",
      "author" : [ "M.T. Orchard" ],
      "venue" : "In Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Orchard,? \\Q1991\\E",
      "shortCiteRegEx" : "Orchard",
      "year" : 1991
    }, {
      "title" : "Object retrieval with large vocabularies and fast spatial matching",
      "author" : [ "J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Philbin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Philbin et al\\.",
      "year" : 2007
    }, {
      "title" : "Acceleration of k-means and related clustering algorithms. volume",
      "author" : [ "S.J. Phillips" ],
      "venue" : "Lecture Notes in Computer Science. Springer,",
      "citeRegEx" : "Phillips,? \\Q2002\\E",
      "shortCiteRegEx" : "Phillips",
      "year" : 2002
    }, {
      "title" : "Web-scale k-means clustering",
      "author" : [ "D. Sculley" ],
      "venue" : "In Proceedings of the 19th International Conference on World Wide Web,",
      "citeRegEx" : "Sculley,? \\Q2010\\E",
      "shortCiteRegEx" : "Sculley",
      "year" : 2010
    }, {
      "title" : "VLFeat: An open and portable library of computer vision algorithms",
      "author" : [ "A. Vedaldi", "B. Fulkerson" ],
      "venue" : "http: //www.vlfeat.org/,",
      "citeRegEx" : "Vedaldi and Fulkerson,? \\Q2008\\E",
      "shortCiteRegEx" : "Vedaldi and Fulkerson",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Others rely on a relaxation of the update step, for example by using only a subset of data to update centroids (Frahling & Sohler, 2006; Sculley, 2010).",
      "startOffset" : 111,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "Certain of these rely on a relaxation of the assignment step, for example by only considering certain clusters according to some hierarchical ordering (Nister & Stewenius, 2006), or by using an approximate nearest neighbour search as in Philbin et al. (2007). Others rely on a relaxation of the update step, for example by using only a subset of data to update centroids (Frahling & Sohler, 2006; Sculley, 2010).",
      "startOffset" : 237,
      "endOffset" : 259
    }, {
      "referenceID" : 8,
      "context" : "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al.",
      "startOffset" : 48,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al.",
      "startOffset" : 48,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al. (2002). These algorithms relied on storing centroids in special data structures, enabling nearest neighbor queries to be processed without computing distances to all k centroids.",
      "startOffset" : 48,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al. (2002). These algorithms relied on storing centroids in special data structures, enabling nearest neighbor queries to be processed without computing distances to all k centroids. The next big acceleration (Elkan, 2003) came about by maintaing bounds on distances between samples and centroids, frequently resulting in more than 90% of distance calculations being avoided. It was later shown (Hamerly, 2010) that in low-dimensions, it is more effective to keep bounds on distances to only the two nearest centroids, and that in general bounding-based algorithms are significantly faster than tree-based ones. Further bounding-based algorithms were proposed by Drake (2013) and Ding et al.",
      "startOffset" : 48,
      "endOffset" : 812
    }, {
      "referenceID" : 8,
      "context" : "Examples are the adaptation of the algorithm of Orchard (1991) in Phillips (2002), and the use of kdtrees (Bentley, 1975) in Kanungo et al. (2002). These algorithms relied on storing centroids in special data structures, enabling nearest neighbor queries to be processed without computing distances to all k centroids. The next big acceleration (Elkan, 2003) came about by maintaing bounds on distances between samples and centroids, frequently resulting in more than 90% of distance calculations being avoided. It was later shown (Hamerly, 2010) that in low-dimensions, it is more effective to keep bounds on distances to only the two nearest centroids, and that in general bounding-based algorithms are significantly faster than tree-based ones. Further bounding-based algorithms were proposed by Drake (2013) and Ding et al. (2015), each providing accelerations over their predecessors in certain settings.",
      "startOffset" : 48,
      "endOffset" : 835
    }, {
      "referenceID" : 1,
      "context" : "We use the following notation to refer to implementations: {codesource-algorithm}, where codesource is one of bay (Hamerly, 2015), mlp (Curtin et al., 2013), pow (Low et al.",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : ", 2013), pow (Low et al., 2010), vlf (Vedaldi & Fulkerson, 2008) and own (our own code), and algorithm is one of the algorithms described.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "We use the following notation to refer to implementations: {codesource-algorithm}, where codesource is one of bay (Hamerly, 2015), mlp (Curtin et al., 2013), pow (Low et al., 2010), vlf (Vedaldi & Fulkerson, 2008) and own (our own code), and algorithm is one of the algorithms described. Unless otherwise stated, times are wall times excluding data loading. We impose a time limit of 40 minutes and a memory limit of 4 GB on all {dataset, implementation, k, seed} runs. If a run fails to complete in 40 minutes, the corresponding table entry is ‘t’. Similarly, failure to execute with 4GB of memory results in a table entry ‘m’. We confirm that for all {dataset, k, seed} triplets, all implementations which complete within the time and memory constraint take the same number of iterations to converge to a common local minimum, as expected. The implementations are compared over the 22 datasets presented in Table 1, for k ∈ {100, 1000}, with 10 distinct centroid initialisations (seeds). For all {dataset, k, seed} triplets, the 23 implementations are run serially on a machine with an Intel i7 processor and 8MB of cache memory. All experiments are performed using double precision floating point numbers. Findings in Drake (2013) suggest that the best algorithm to use for a dataset depends primarily on dimension, where in low-dimensions, ham and ann are fastest, in highdimensions elk is fastest, and in intermediate dimensions an approach maintaining a fractional number of bounds, Drake’s algorithm, is fastest.",
      "startOffset" : 136,
      "endOffset" : 1234
    } ],
    "year" : 2016,
    "abstractText" : "We propose a novel accelerated exact k-means algorithm, which outperforms the current stateof-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3× faster. We also propose a general improvement of existing stateof-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, obtaining speedups in 36 of 44 experiments, of up to 1.8×. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments.",
    "creator" : "LaTeX with hyperref package"
  }
}