{
  "name" : "1302.4949.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Characterization of the Dirichlet Distribution with Application to Learning Bayesian Networks",
    "authors" : [ "Dan Geiger", "David Heckerman" ],
    "emails" : [ "dang@cs.", "heckerma@microsoft" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We provide a new characterization of the Dirichlet distribution. This characterization implies that under assumptions made by sev eral previous authors for learning belief net works, a Dirichlet prior on the parameters is inevitable.\n1 Introduction\nIn recent years, several researchers have investigated Bayesian methods for learning belief networks [CH91, Bu91, SDLC93, HGC94]. These approaches all have the same basic components: a scoring metric and a search procedure. The scoring metric takes data and a network structure and returns a score reflecting the goodness-of-fit of the data to the structure. A search procedure generates networks for evaluation by the scoring metric. These approaches use the two com ponents to identify a network structure or set of struc tures that can be used to predict future hypotheses or infer causal relationships.\nThe Bayesian approach cim be described as follows. Suppose we have a domain of variables {u1, ... , un} = U, and a set of cases { C1, ... , Cm} = D where each case is an instance of some or of all the variables in U. We sometimes refer to D as a database. Let (Bs, Bp) be a belief network, that is, Bs is a di rected acyclic graph , each node i of B. is associated with a random variable u; and Bp is a set of con ditional distributions, p( u; !u;1, . • . , u;k), 1 :::; i :::; n, where u;1, • • • , Uik are the variables corresponding to the parents of node i in Bs. (For more details, consult [Pe88]). Let B� stand for the hypothesis that cases are drawn from a belief network having the structure Bs. Then a Bayesian measure of the goodness-of-fit of a belief network structure Bs is p(B� ID, �) given by p(B�ID,�) = c · p(B�I�)p(DIB�,�) where cis a nor malizing factor and � is the current state of knowledge.\nTo compute p(DIB�, �) in closed form several assump tions were made. First, the database D is a multino-\n*Part of this work was done while the author visited Microsoft Research Center\nmial sample from some belief network (Bs, Bp ). Sec ond, for each network structure the parameters asso ciated with one node are independent of the parame ters associated with other nodes (global independence [SL90]) and the parameters associated within a node given one instance of its parents are independent of the parameters of that node given other instances of its parent nodes (local independence [SL90]). Third, if a node has the same parents in two distinct net works then the distribution of the parameters asso ciated with this node are identical in both networks (parameter modularity [HGC94]). Forth, each case is complete. Fifth, the distribution of the parameters associated with each node is Dirichlet. The last two assumptions are made so as to create a conjugate sampling situation, namely, after data is seen the distributions of the parameters stay in the same family- the Dirichlet family. A relaxation of the assumption of complete cases was carried out by pre vious works (e.g., [SDLC93]). The contribution of this paper is a characterization of the Dirichlet distribution which enables one to show that the fifth assumption is implied from the first three assumptions and from one additional plausible assumption that if B1 and B2 are equivalent belief networks (i.e., they represent the same independence assumptions) then the events B? and B� are equivalent as well (hypothesis equiva lence [HGC94]). We make this self-evident assumption explicit because it does not hold for causal networks where two edges with opposing directions correspond to distinct events. Our contribution can be described using common sta tistical terminology as follows. We use this termi nology because our result might be found applicable in other statistical uses of the Dirichlet distribution and because it falls under the broad area of charac terizations of probability distribution functions. Sup pose s and t are two discrete random variables hav ing finite domains, {s;}7=1 and {tj}j=1, respectively. We wish to infer the joint probability p(s, t) from a sample of pairs of values ( s;, tj) of s and t. The standard Bayesian approach to this statistical infer ence problem is to associate with p( s;, tj) a parameter B;j (often called the multinomial parameter), assign { B;j 11 :::; i :::; k, 1 :::; j :::; n} a prior joint pdf and com pute the posterior joint pdf of { B;j} given the observed\nA Characterization of the Dirichlet Distribution 197\nset of pairs of values. There are two closely-related variants to this approach which can be described as follows. Let B;. = L,j=1 B;j stand for the multinomial parame ter associated with p( s = s;) and let Bjli = B;j / L,j B;j stand for the multinomial parameter associated with p(t = tjls = s;). Furthermore, let BJ. = {Bd7�11 and BJii = {Bjl;}j�}. We assume that {BJ., BJI 1, ... , BJik} are mutually independent and that each has a prior pdf. Now according to Bayesian practice we com pute the joint posterior appropriately. That is, we up date the pdf for BJ. according to the counts of s = s; in the observed pairs and update the pdf of B Jli ac cording to the counts of t = tj in all pairs in which s = s;. In a symmetric fashion, let B-j = 'L-7=1 B;j, B;u = B;i/ L,, B;j, B.J = {B-j}j�l and BIIi = {B;Ii}f�/. Now we assume that {B.J, B!Il, ... , BIIn} are mutually independent and that each has a prior pdf and we com pute the posterior pdf for B.J according to the counts of t = tj and the posterior pdf of BIIi according to the counts of s = s; in all pairs in which t = tj . To make these techniques operational one must choose a specific prior pdf for the multinomial parameters. The standard choice of a pdf for { B;j} is a Dirichlet pdf usually for pragmatic reasons. When such a choice is made, it can be shown that {BJ., BJI 1, ···, BJik} are indeed mutually independent and that each has a prior Dirichlet pdf. Similarly, { B.J, BIll, ... , B !In} are mutu ally independent and each has a prior Dirichlet pdf. The surprising result proved in this article is that if these independence assertions are assumed to hold, and under the assumption of (strictly) positive pdfs, then a prior Dirichlet pdf for { B;j} is the only possi ble choice. The assumption of strictly positive pdfs can possibly be dropped without affecting the conclu sion but we have not carried out a proof of this claim. The implication of this result to learning Bayesian net works is discussed in Section 3. A preliminary account of analogous results for Gaussian networks is reported in Section 4.\n2 Background and Technical Summary\nThe Dirichlet pdf is defined as follows. Let ¢1, .. . , ¢1 be positive random variables that sum to 1 . Then ¢1, ... , ¢1-1 have a Dirichlet pdf f if\n('\\'I ) I f(\"' \"' ) r L..J i-1 a, II -�.01.-1 (1) 'f' 1 , ···, 'f'l-1 = II' (\n· ) 'f'; i= 1 r a, i=1 where ¢1 = 1 - L,::;� ¢; and a; are positive constants (See, e.g., [De70, Wi62]). We use the following conventions. Suppose { B;j}, 1 :::; i :::; k, 1 :::; j :::; n, is a set of positive random vari ables that sum to 1. Let B;., B.j, BJ., B.J, Bn;, B;li, B Jli, and B11i be defined as in the introduction. Conse quently, B;.Bili = B.jBilj for every i and j. Let fu\nbe the joint pdf of { B;j}, !I be the pdf of B I·, and h1; be the pdf of (J Jli . Similarly, let h be the pdf of B.J, and /Jij be the pdf of (J IIi. Finally, let fiJ be the joint pdf of BJ., BJI , ... , BJik and fJI be the joint pdf of B.J, Bm, ... , BIIn· A Dirichlet pdf for { B;j} is given by\nk n fu({B;j}) = c IIII B'0'i-l i=1 j=1\n(2)\nwhere Bkn = 1- L,AB;j, A= { (i, j) l1:::; i,j:::; n,i f. k or j f. n}, c is the normalization constant and aij are positive constants.\nWe observe that fu and !I J are related through a change of variables. Since both { Bd7=1 and { Bjli }j=1 are defined in terms of {B;j} and since B;j = B;.Bjli, there exists a one-to-one and onto correspondence be tween {B;j} and {B;.} U {Bjl;}. The Jacobian Jk,n of this transformation is given by\n[HGC95].\nk Jkn = II or.-1 i= 1\n(3)\nThe following lemma provides a known property of the Dirichlet distribution. A slightly weaker version is stated in [DL93] (Lemma 7.2).\nLemma 1 Let {B;j}, 1 � i � k, 1 :::; j � n, whe�e k and n are integers greater than 1, be a set of pos� tive random variables having a Dirichlet distribution. Then, fi ( BJ.) is Dirichlet, hli ( B Jli) is Dirichlet for ev ery i, 1:::; i:::; k, and {BJ., BJ1 1, ... , BJid are mutually independent.\nProof: Set B;j = B;.Bjli in Eq. 2, multiply by Jkn, and regroup terms. D\nThe main claim of this article is that, under the as sumption of a positive pdf for { B;j}, the converse holds as well. More specifically, we prove the following the orem.\nTheorem 2 Let {B;j}, 1 :::; i :::; k, 1 :::; j :::; n, L,;j B;j = 1, where k and n are integers greater than 1, be positive random variables having a positive pdf fu({B;j}). If {BJ., BJI 1 , ... , 6Jid are mutually independent and {B.J, Bm, ... , BIIn} are mutually independent, then fu ( { B;j}) is Dirichlet. Recall that fu can be written both in terms of !I J and in terms of h 1 by a change of variables and using the Jacobian given by Equation 3. Since both repre sentations must be equal, and using the independence assumptions made by Theorem 2 to factor fi J and f JI, we get the equality,\n(4)\n198 Geiger and Heckerman\nThis equality, which is in fact a functional equation, summarizes the independence assumptions stated in Theorem 2. Methods for solving functional equations such as Eq. 4, that is, finding all functions that satisfy them un der different regularity assumptions, are discussed in [Ac66]. We use the following technique. First, we show that any positive solution to Eq. 4 must be differen tiable in any order (Aczel, 66, Section 4.2.2, \"Deduc tion of differentiability from integrability\" ). Then we take repeated derivatives of Eq. 4 and obtain a differ ential equation the solution of which after appropriate specialization is the general solution of Eq. 4 (Aczel, 66, Section 4.2, \"Reduction to differential equations\" ). The proof is given is the appendix. Note that when n = k = 2 and by renaming of variable and function names, Eq. 4 can be written as follows:\nyz y(1 - z) !o(y)g1(z)g2(w) =go(x)ft(-;)h( 1_x ) (5)\nwhere x = yz + {1 - y)w\nand where y, z and w replace B.j=l> Bi=lli=l, Bi=lli=2> respectively.\n3 Implications For Learning\nWe now explain how our characterization applies to learning belief networks. We concentrate on belief net works for two discrete variables s and t whose joint distribution is p( s, t) . The n-variate case is discussed in [HGC95]. There are three possible belief networks with two nodes. The network that contains no edge between its two nodes s and t, a network s -+ t and the network t -+ s. The first network Bo corresponds to the assertion that s and t are independent while the second network B1 and the third one B2 assert that s and t are dependent. The last two belief net works are equivalent, B1 represents the factorization p( s, t) = p(s)p(t , s) and B2 represents the factorization p(s, t) = p (t)p(s t) . We shall first examine the two complete networks B1 and B2. We assume that if two networks B1 and B2 are equivalent (as is the case in our example) then the corresponding events Bf and B� are equivalent (hypothesis equivalence [HGC94]). Recalling the no tations introduced in the introduction, we have that Bi. = L::;=l Bij stand for the multinomial parameters associated with p(s = si ) and Bjli = Bij/ Lj Bij stand for the multinomial parameters associated with p(t = ti l s = si) · Thus, /IJ((h., 8J1 1 , ... , BJikiBf) = !IJ((h., BJ1 1, . . . , BJikiB�)\nDue to these equalities and using local and global in dependence to factor !IJ and f JI, we immediately ob tain Equation 4 (dropping the conditioning events is valid because Bf and B� are equivalent). Thus for the two complete networks the only possible prior on their parameters is, according to Theorem 2, the Dirichlet distribution. Note that we only use three assumptions: a multi nomial sampling situation, local and global indepen dence, and hypothesis equivalence. Implicitly, since we condition on Bf, is the assumption that each complete structure has a positive probability to be manifested. The prior for any non-complete network follows from the assumption of parameter modularity which says that the pdf associated with a node under the assump tion that a specific network generates the data is the same as the pdf of the parameters of that node given another network generates the data provided that the set of parents is identical in the two networks. In our two-variables network, for example, the parameters B;. which are associated with node s have the same pdf when conditioned on B1 and when conditioned on B0 because in both networks s has the same set of parents {the empty set) and similarly for node t. That is,\n/;(Bi.IB?, �) = J;(B;.IB3,�) h wj IB�, e) = h {O.j IBi, e)\nThese equalities imply that the prior for the parame ters of B0 is Dirichlet as well. Thus, parameter mod ularity is the assumption that extends our result from complete to non-complete networks. This result of the inevitable choice of a Dirichlet prior for two-variables networks is easily generalized to the n-variate case by induction and without the need to solve any additional functional equations. The induc tive proof uses the fact that a cluster of variables each having a Dirichlet distribution is distributed Dirichlet as well. For details consult [HGC95]. Recall that the exponents of Bij of a Dirichlet distribu tion can be written as N Cl!ij - 1 where N is the \"equiv alent sample size\" (the size of an imaginary database of complete cases-the prior sample-upon which the prior Dirichlet is based) and Cl!ij is the expectation of Bij . The equivalent sample size reflects the confidence of the user and Cl!ij represents the relative frequency of the pair ( i, j) in the prior sample. A joint Dirich let prior is therefore quite restricting because it allows only one equivalent sample size for the entire domain. That is, there is no way to express different confidence levels regarding the parameters of different parts of the network. Thus the practical ramification of our charac terization is that the commonly-made global and local independence assumption is inappropriate whenever a single equivalent sample size is not sufficient to de scribe prior knowledge. Such a situation occurs, for example, if knowledge about B1. is more precise than knowledge about B Jli. One possibility for overcoming this limitation of the Dirichlet prior is to replace the notion of a single equiv alent sample size with equivalent database. Namely,\nA Characterization of the Dirichlet Distribution 199\nwe ask a user to imagine that she was initially com pletely ignorant about a domain, having an uninfor mative prior with equivalent sample size(s) close to the lower bound. Then, we ask the user to specify a database De that would produce a posterior den sity that reflects her current state of knowledge. This database may contain incomplete cases. Then, to score a real database D, we score the database De U D, us ing the uninformative prior and a learning algorithm that handles missing data. This way of specifying a prior yields a mixture of Dirichlet distributions which, according to our result, cannot satisfy the local and global independence assumption.\n4 Discussion\nThe independence assumptions made by Theorem 2 can be divided into two parts: { 0 Jil, . . . , 0 Jik} are mutually independent and {Ori1, ... , Orin} are mutu ally independent (local independence) and Or. is in dependent of { 0 Jil, . . . , 0 Jik} and O.J is independent of { 0 Ill, . . . , 0 lin} (global independence). A natural question to ask is whether global independence alone implies a joint Dirichlet pdf for { O;j}. This question is particularly interesting in light of the analysis of decomposable graphical models given by [DL93]. Dawid and Lauritzen term a pdf that satisfies global independence a strong hyper-Markov law and show the importance of such laws in the analysis of decomposable graphical models. We now show that the class of strong hyper-Markov laws is larger than the Dirichlet class.\nWhen n = k = 2, and using the notations of Equa tion 5 the new functional equation can be written as follows:\nyz y(1 - z) fo(y)g(z, w) = go(x)f( --;• 1 _ x ) (6)\nwhere x = yz + (1- y)w. Note that Eq. 5 is obtained from this equation by setting g(z, w) = g1 (z)g2( w) and f(tl, t2) = f1 (tl)h(h). These equalities correspond to local independence. Let fu be a joint pdf of { O;j} given by\nwhere K is the normalization constant, o:;j are posi tive constants and H is an arbitrary positive integrable function. That this pdf satisfies global independence can be easily verified. It can in fact be shown, by solv ing Eq. 6, that every positive strong Hyper Markov law can be written in this form (when n = 2 and k = 2}. This solution includes the Dirichlet family as a proper subclass. Since H is a single function that does not depend on a particular network, one can conclude that if local pa rameter independence is assumed to hold in one net work, then fu must still be Dirichlet and therefore,\ndue to Lemma 1, local parameter independence must hold for all networks. We have so far proved this claim for two-variables networks but we believe it holds for the n-variate case as well. As a final comment, we should mention that a func tional equation which restricts the possible prior dis tributions for the parameters of Bayesian networks can be formulated for other sampling situations not nec essarily for the multinomial sampling which was as sumed in our discussion so far. As another example, consider a two-continuous-variables domain { x 1, x2} having a bivariate-normal distribution. Constructing a prior for the parameters of such Gaussian networks and performing the prior-to-posterior analysis was car ried out in [GH94, HG95]. Let {ml,v1,m2il,b12,v2id and {m2, v2, m1i2, b21, v1i2} denote the parameters for the network structures x1 -+ x2 and x1 f-- x2, respec tively. That is, m1 is the mean of x1 and v1 is the variance for x1. Collectively, these are the parameters associated with node x1 in the first network. The pa rameters associated with node x2 are the conditional mean m2i1, the regression coefficient b12 of x2 given x1 and the conditional variance v2p. Now assuming global parameter independence and hypothesis equiv alence and using the Jacobian given in [HG95] yields the functional equation\nwhere !1, hil• h, and !li2 are arbitrary density func tions, and where\nb _ b12V1 21- -V2 v2il v1 vli2 = -v2\nThese relationship are well known from path analysis and can be derived from Eq. 4 in [HG95]. We have solved this functional equation and found that the only integrable solutions are such that f1 ( v1) is an inverse gamma distribution, that is, 1/v1 has a gamma distribution, f1 ( m1lv1) is a normal distribution, and similarly for h ( m2, v2). The conditional distribu tion hil (b12, v2il) has an interesting form. An inverse gamma distribution for v2p times a Normal distribu tion for b12 times an arbitrary function H(b12jv2p). The arbitrary function is not surprising since the func tional equation only encodes global independence and so the solution depends on an arbitrary function just as for multinomial sampling (Equation 7}. The natural question is now what does local indepen dence mean for Gaussian networks. Because the sub jective variance of b12 actually depends on v2i1, we can not assume that b12 and v2i1 are independent. The an swer is that local independence for Gaussian networks means that the standardized regression coefficient b12 is independent of the conditional variance v2i1 at each\n200 Geiger and Heckerman\nnode. When adding this assumption, which fully par allels the discrete case, we get that H is the exponen tial function and therefore /21I(b12Jv21t) is a normal distribution and hp ( v21 t) is an inverse Gamma distri bution. Consequently, it can further be shown that a bivari ate normal-Wishart distribution is the only possible prior on the joint space parameters (i.e., the inverse covariance matrix and the vector of means) if we as sume global parameter independence, local parameter independence for one network and hypothesis equiva lence. Indeed this was the prior chosen by [GH94). An analogous result holds for the n-variate case as well.\nAcknowledgment\nWe thank J. D. Aczel and M. Ungarish for valuable comments, S. Altschuler and L. Wu for their help with Lemma 3, M. Israeli for his help with Lemma 4 and G. Cooper who helped us define the notion of an equiva lent database.\nReferences\n[Ac66] J. Aczel, Lectures on Functional Equations and Their Applications, Academic Press, New York, 1966.\n[Bu91) W. Buntine, Theory refinement on Bayesian networks, Proceedings of Seventh Conference on Uncertainty in Artificial Intelligence, Los Angeles, CA, Morgan Kaufmann, July, 1991, 52-60.\n[CH91] G. Cooper and E. Herskovits, A Bayesian method for the induction of probabilistic networks from data, Section on Medical Informatics, Stanford University, January, 1991, Technical Report, SMI91-1. Also in, Proceedings of Seventh Conference on Uncertainty in Artificial Intelligence, Los Angeles, CA, Morgan Kaufmann, July, 1991, 52-60. Final version in Machine Learning, 1992, 9:309-347.\n[DL93] P. Dawid and S. Lauritzen, Hyper Markov laws in statistical analysis of decomposable graphi cal models, Annals of Statistics, 21:1272-1317, 1993.\n[De70) M. DeGroot, Optimal Statistical Decisions, McGraw-Hill, New York.\n[GH94] D. Geiger and D. Heckerman, Learning Gaus sian networks, Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence, Seattle, WA, Morgan Kaufmann, July 1994, 235-243.\n[GH95] D. Geiger and D. Heckerman, A characteriza tion of the Dirichlet distribution through global and local independence, Computer Science Department, Technical report 9506, February 1995. A prelimi nary report appears as Microsoft Research Report, TR-94-16.\n[HG95) D. Heckerman and D. Geiger, Learning Bayesian networks: A unification for discrete and Gaussian domains. In this proceedings.\n[HGC94] D. Heckerman, D. Geiger, and D. Chicker ing, Learning Bayesian networks: The combination of knowledge and statistical data, Proceedings of Tenth Conference on Uncertainty in Artificial Intel ligence, Seattle, WA, Morgan Kaufmann, July 1994, 293-301.\n[HGC95] D. Heckerman, D. Geiger, and D. Chicker ing, Learning Bayesian networks, Machine Learning, 1995, to appear.\n[Pe88] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, 1988, Morgan Kaufmann, San Mateo, CA.\n[SL90) D. Spiegelhalter and S. Lauritzen, Sequential updating of conditional probabilities on directed graphical structures, Networks, 20, 579-605, 1990.\n[SDLC93] D. Spiegelhalter, A. Dawid, S. Lauritzen, and R. Cowell, Bayesian analysis in expert systems, Statistical Science, 8, 219-282, 1993.\n[Wi62] S. Wilks, Mathematical Statistics, Wiley and Sons, New York.\nAppendix: Proof 1\nDiffereniability from Integrability\nBy renaming of variable and function names, and by absorbing the Jacobians into the new function defini tions, Eq. 4 can be written as follows:\nn fo(Y1, ···, Yn-1) ITgj(Z1 ,j, . . . , Zk-1,j) = j=1\nk )II (Zi1Y1 Zi,n-1Yn-1 ) 9o(x1, · · · , Xk-1 /; --, . . · , --'---i=1 X; X; where\nn\n(9)\nX;= I:>ijYj, 1:::; i:::; k -1 (10) j=1 k-1\nZkj = 1 -I: Zij, 1 :S j :S n i= 1 and where\nn-1 Yn = 1- LYi > j=1\nk-1 Xk = 1- L:x; i=1\n(11)\nNote that the free variables in Eq. 9 are Y1, .. . , Yn-1 (Yj replaces O .j) and Zij, 1 :S i :S k - 1, 1 :S j :S n ( Z;j replaces O;li). All other variables which appear in Eq. 9 are defined by Eqs. 10 and 11.\n1This proof first appeared in (GH95).\nA Characterization of the Dirichlet Distribution 201\nFurthermore, we may consider :1:1 . . . , :&k (x ; replaces B;.) and W;j = z;�ri' 1 � i � k, 1 � j � n-1 (w;j replaces Bj/i) to be free variables and rewrite Eq. 9 in term of these variables, namely,\nk go(Xt, . . . , Xk-t) II /; { w;,t , ... , Wi,n-t) =\nn ( II W1,j:l:1 Wk-1,jXk-1 ) ) fo Y1 , · · · , Yn- 1) gj(-.-, ... , . {12 i==1 YJ YJ\nwhere k\nYi = \"L: W;jX;, 1 � j � n- 1 {13) i==1 n-1 Win = 1 - L: W;j, j=1\nand where Xk and Yn are defined by Eq. 11. This symmetric representation of Eq. 9 will be used in the derivation of its solution. We assume that all functions mentioned in Eq. 9 orig inated from pdfs and thus are (Lebesgue) integrable in their domain. We shall now show that this assump tion implies that each set of functions that solves Eq. 9 consists of functions for which any finite-order partial derivative exists for every point in their domain. The importance of this technical claim is that in order to find all positive integrable functions that satisfy Eq. 9, it is permissible to take any derivative at any point in the domain because it exists. By setting z;j = 1/ k, for all i and j, in Equa tion 9 we get that fo(Y1, . . . , Yn-1) is proportional to n7=1 f;(y1' 0 0 0 ' Yn-1) 0 Similarly, by setting W;j = 1/n in Eq. 12, g0(x1, . . . , Xk-1) is proportional to n;=1 gj(X1, 0 0 0 'Xk-1)· Thus if we prove that each gj, j = 1, ... , n, has any-order derivative, then so does g0. Furthermore, any property that we prove about gj, j = 1, ... , n, holds true for /; , i = 1, ... , k, due to the symmetric representation of Eq. 9 given by Eq. 12. Since all functions are positive, we can take the loga rithm of Eq. 9. Since all functions are integrable and positive then so are their logarithms. Let now j0 be an index such that 1 � io � n. We take a logarithm of Eq. 9 and integrate the resulting equation wrt 2 all variables except for the variables Z;j0, 1 � i < k. Consequently, we obtain,\nk\n) L: \"(Zi1Y1 Zi,n-1Yn-1 )] Z y ( ) ... ,Xk-1 + f; --, .. . , d j0d 14 Xi Xi •=1 where h(x) stands for In h(x), M is a constant, Y = {Y1, ... , Yn _1), Zj0 is a vector containing all variables\n2with respect to\nZ;j except those where j = j o, Dj is the domain of gj, and Dy the domain of fo. Since, the right hand-side of Eq. 14 is integrable, it follows that gj0 is continuous for every 1 � io � n. Hence, g0 is continuous as well. Thus, due to the sym metric functional equation (Eq. 12), /; are also contin uous functions. Having now continuous functions on the right hand-side of Eq. 14, it follows that gj0 has a first derivative wrt each of its arguments. Thus, due to Eq. 12, each /; also has a first derivative wrt each of its arguments. Consequently, by Eq. 14, it follows that gj0 has a second derivative wrt each of its arguments. Repeating this argument yields the desired conclusion that all positive integrable functions that solve Equa tion 9 have any partial derivative at any point in their domain. 3\nThe Binary Solution\nWe shall now find all positive integrable solutions of Eq. 9 when k = n = 2. This derivation is different from the general derivation which is given in the following sections, however, the basic method of repeatedly dif ferentiating the functional equation and subsequently solving the resulting differential equations is similar. When n = k = 2, the functional equation can be writ ten as follows:\nyz y(1- z) f0(y)gt(z)g2(w) = go(x)ft( -;;)12( 1_ x ) (15)\nwhere\nLet\nand\nx = yz + (1 -y)w\n\"I d !; (t) = dt Inf;(t)\ng;(t) = :t lng;(t)\n(16)\n(17)\n(18)\nTaking the logarithm and then a derivative once wrt y, once wrt z and once wrt w of Eq. 15 yields the following three equations,\nJ�(y) - (z- w)g�(x) = z� R { yz )+ X X (1-z){1-w)j'( y {1-z)) (19) {1-x)2 2 1-x\n•1 ( ) •1 ( ) _ yw(1 -y) 1.1 ( yz) 91 z - Y9o x - 2 1 - -X X (1-w)(1-y)y •1 y(1- z) {20) {1 -X )2 /2 ( 1 -X )\n·I ( ) (1 ) \"1 ( ) _ yz(1-y)f·1(yz)+ 92 w - -Y 9o x - - 1 -x2 x y(1 -z){1-y) j' ( y(1 -z)) (21) (1-x)2 2 1-x\n3Note that, by definition, a pdf does not include a delta function. Otherwise it is called a generalized pdf (gpdf, [De70]). An integral of a gpdf need not be continuous.\n202 Geiger and Heckerman\nSolving 11 e;:·) and iH u<t:.,z)) from Eqs. 20 and 21 yields,\ny(1-y)(w-z) j�(y z\n) = -(1-y)(1-w)g�(x) x2 x +(1-z)g�(z)-y(1-z)gb(x) + (1-w)g�(w)(22)\ny(1-y)(w-z) /'( y( 1-z)) = zg' (z) (1-x)2 2 1-x 1\n-yzgti(x) + wg�(w)-(1-y)wgb(x) (23)\nNow plugging Eqs. 22 and 23 into Eq. 19 and collecting all the terms involving Yb(x), g� (z), g�(w) and /b(Y), yields,\nh(y, z, w)g�(x) = z(1-z)g� (z) + w(1-w)g�(w) -y(1-y)(w- z)/�(y) (24)\nwhere\nh(y, z, w) = y(1-y)(w-z)2 + yz(1-z) + (1 -y)(1 -w)w\nTaking a derivative wrt z of Eq. 24 yields,\nhz(Y, z, w)g�(x) + yh(y, z, w)g�(x) = (1-2z)g� (z) + z(1-z)gi(z) + y(1-y)f�(y) (25)\nwhere hz is the partial derivative of h wrt z, given by,\nhz(Y, z, w) = -2y(1-y)(w-z) + y(1- 2z) Similarly, taking a derivative wrt w of Eq. 24 yields,\nhw(Y, z, w)g�(x) + (1-y)h(y, z, w)g� (x) = (1-2w)g�(w) + w(1-w)g�(w)-y(1-y)j�(y) (26) where hw is the partial derivative of h wrt w, given by,\nhw(Y, z, w)g�(x) = 2y(1-y)(w-z) + (1-y)(1-2w) Eqs. 25 and 26 together with\n(1-y)hz(Y, z, w)-yhw(Y, z, w) =: 0 yield\n{1-2w)g�(w) + w(1-w)g�(w) = {1-y)/�(y)\n+ 1-y [{1-2z)g� (z) + z{1-z)g�(z)] y\n(27) Since w does not appear in the right hand side of this equation, we get,\n{1-2w)g�(w) + w(1-w)g�(w) = c1 (28) where c1 is an arbitrary constant. Eq. 28 is a first order linear differential equation the general solution of which is given by,\n,, ( ) b 92 w = w(1- w) c1 1-2w 2 w(1-w)\nwhere b is an arbitrary constant and w(1�w) is the homogeneous solution. Thus,\n,, ( ) Q' (J 92 w = -- --w 1-w where a and (J are arbitrary constants defined by a = b- � and (J = -(b + � ). Hence, using J ajw dw = ln w'\", we get 92(w) = cwa(1-w)l3 where cis a third arbitrary constant. From Eq. 27 we also get,\n(1-2z)g�(z) + z(1-z)g�'(z) = _!2!!__ + yf�(y) 1-y Hence both sides are equal to a constant, say c2. Con sequently,\nand\njt ( ) _ c2 c1 JOy - - --y 1-y\na' (3' g�(z) = -- --z 1-z Consequently, fo(y), 91(z) and 92(w) all have the Dirichlet functional form and each function depends on three constants. Due to the symmetric representa tion of Eq. 9 given by Eq. 12, we conclude that g o, !1, and h are Dirichlet as well.\nPreliminary Lemmas\nWe now provide several lemmas that are needed for the derivation of the general solution of Eq. 9.\nLemma 3 The general solution of the following par tial differential equation for J(x1, ... , xn),\nf + x;/.,, + Xjfxi = 0 (29) in the domain (0, oo )m, is given by,\n1 x; J(x1, ... ,xn) = - h(-,x1 , ... ,Xi-1,Xi+b···, X; Xj\nXj-11Xj+l1 • • • 1Xn) (30) or, equivalently, by\n1 X; f(x1, ... ,xn) = -g(-,xt , ... ,x;_l,Xi+l•···· Xj Xj\nXj-11Xj+11 • • • ,xn) (31) where h and 9 are arbitrary differentiable functions having n -1 arguments.\nProof: Let s =X; and t = �- Thus, fx, = fs + � ft, J f.,i = -tift· Hence, after a change of variables, the differential equation becomes\nf +sf,= 0 and therefore,\n1 f = -h(t, X11 • • • 1 Xi-11 Xi+11 • • • 1 Xj-11 Xj+11 • • • 1 Xn) s By changing the roles of Xi and Xj in this derivation, we get the other form of f. D\nA Characterization of the Dirichlet Distribution 203\nLemma 4 The general solution of the following par tial differential equation for f(x1, . .. , Xn),\nis given by\na f3 f:r:; -!:r:. = - + -1 Xi Xj\nf (X 1 , ... , X n) = a ln Xi -j3ln X j + h (Xi + X j , X 1 ,\n(32)\n· · · , Xi-1, Xi+1> · · . , Xj-1, Xj+1> · · ·, Xn) (33) where h is an arbitrary differentiable function having n - 1 arguments.\nProof: Let s = x; + Xj and t = x;- Xj· Thus, f:r: , = !s+ ft, !:r:i = f, -ft. Hence, after a change of variables, the differential equation becomes\na f3 !t = --+-- s +t s-t\nIntegrating wrt t and changing back to the original variables yields the desired solution. 0\nLemma 5 Let J(x1, ... , xn) be a twice-differentiable function. If for all 1 :S i < j :S n,\nj(x1, ... , Xn) =a; lnx; + aj lnxj + /;j(x; + Xj, x1, . . . , Xi-1, Xi+1> ... , Xj-1, Xj+1> ... , Xn)\nwhere /ij are arbitrary twice differentiable functions having n -1 arguments, then\nn n f(x1, ... , Xn) = g(L: x;) +I: a; ln x;\ni:1 i=1 (34)\nwhere g is an arbitrary twice-differentiable function.\nProof: We shall prove the following stronger claim. For every 2 :::; l ::; n, and for every permutation of the indices of Xt, ... , Xn,\nl l J(x1, .. . , Xn) = h1(L: x;, X1+1, ... , Xn) +I: a; lnx;\ni=1 i=1 (35)\nwhere h1 is an arbitrary twice differentiable function. The function h1 depends on the permutation, although this fact is not reflected in our notation. The base case l = 2 is assumed by the lemma and the case l = n is needed to be proven. By the induction hypothesis we assume Eq. 35 and for the permutation\n(1, ... , n) -+ (l, 1, ... , l-1, l + 1, ... , n)\nwe also assume (by the induction hypothesis),\n1-1 f(x1, ... , Xn) = gl(X!, Xl+l +I: X;, Xl+2, ... , Xn)\ni=1 1-1\n+ L b; ln x; + bl+1 ln xl+1 (36) i=1\nLet x = L:;�;;;i x;, c; = b;- a; and x = (xl+2, ... , Xn)· From Eqs. 35 and 36 we get,\nh1(X1 +X, Xl+1, x) = gl(Xl, Xl+1 +X, x)+ 1-1 I: c; ln x; - a1ln X!+ b1+1ln X!+1 i=1\n(37)\nSet x; = 1/2(1- 1), i = 1, ... ,l-1. Thus, x = 1/2 and Eq. 37 yields, h1(X1 + 1/2, X1+1> X)= gl(Xl, Xl+1 + 1/2, x)+\n1-1\nI: c; ln(1/2(l-1))-a1 ln x1 + b1+1 ln xl+1 (38) i:1\nPlugging Eq. 38 into Eq. 37 and letting gl(Xl, Xl+1 +X, X):= gl(Xl, Xl+1 +X, x)-a1 ln x1 (39)\nyields,\ngl(Xl +X - 1/2, Xl+l + 1/2, x) = gl(X!, Xl+1 +X, x)+ 1-1 1-1\nI: c; lnx;-I: c; ln(2( l-1)) (40) i=1 i=1\nBy taking a derivative wrt Xj, 1 :::; j :S l -1 of Eq. 40 we get,\ngl(Xl +X-1/2, X!+1 + 1/2, x)t = CjjXj + g1(X1, Xl+l +X, x)2 (41)\nwhere the indices 1 and 2 indicate the argument of g1 wrt which a derivative is taken. Similarly by taking the derivatives wrt X! we get,\n91(x1 + x -1/2,xl+1 + 1/2,x)t = 9I(xc,xl+1 + x,x)t ( 42)\nConsequently,\ngc(Xl, X1+1 +X, x)t-gc(Xl, Xl+l +X, x)2 = CjjXj (43) for j = 1, ... , l - 1. we now show that Cj = 0. If l > 2, then set j =it and j = h, 1 :Sit < h :S l-1, in Eq. 43 and subtract the two equations. Consequently, citfxb = chfxh and therefore Cj, = Cj2 = 0. If l = 2, then, x = X1 and Eq. 43 becomes\ngc(Xz, X3 + X1, x)t-gc(Xz, X3 + X1, x)2 = ctfx1 (44) Let u = X1 + x3, w = x1 -x3 and rewrite the last equation,\n_ ( _ ) _ ( _) 2c1 91 Xz, U, X 1-91 Xz, u, X 2 = -u+ w\n(45)\nSince the left hand side is not a function of w we have C1 = 0. Now let s = xc + (x + Xl+t), t = Xc -(x + xc+t) and rewrite the differential equation (Eq. 43) by changing variables to s, t and x. Since Cj = 0, we get,\n! [g1(s, t, x)] = 0 (46)\n204 Geiger and Heckerman\nThus, Bl(s, t, x) = i(s, x) where i is a function of just s and x. Consequently, by switching back to the original variables, we get,\n1+1 1+1 J(x1, .. . , Xn) = L a; In Xi+ i(L x;, X1+ 2, ... , Xn) i= 1 1=1\n(47) Since this equation can be derived for any permutation of the indices of Xi, the induction is completed. D 5 The General Solution\nWe now solve Eq. 9 for any n and k. First we assume both n and k are strictly greater than 2. We use the following notations:\na 91(t1, ... ,tk-I); = Ot; lng1(t1, ... ,tk-d (48)\na a 91(t1, ... ,tk-1);j = -;--;-ln g1(t1, ... ,tk-d\nut; utj a\nf1(t1, ... ,tn-1); = -;-ln jl(t1, ... ,tn-d ut;\na a f1(t1, .. . ,tn-1)ij = -;--;-ln fl(t1,· · ·•tn-1) ut; utj\nAlso we use the following notations:\nX= (x1, ... ,xk_I), Y = (Y1, · · ·, Yn-d, Zj = (z1,j, ... , Zk-1,j), {49) w.. _ (\n!il.1Ll. Zi,n-lYn-1 ) t- :Ci , • . • , Xi\nFor example, 9j(Zj) stands for 9j(Zl,j, ... , Zk-1,j)· By taking the logarithm and then a derivative wrt Z;j (1 � i � k-1,1 � j � n- 1) of Eq. 9, we get,\ngj(Zj)i =Yj [�f;(W;), [-z�r] + :J'(Wi)jl +(50}\nBy settingi = i1 andi = i2, 1 � i1 < i2 � k-1 (k 2: 3) in Eq. 50, subtracting the resulting two equations, and dividing by Yi, we get,\n1 - [gi (Zi );, - 9i (Zj );,] = [go(X);, - go(X)i,] + Yi � [f;,(W;2)I [z��YI ] _ k(W;,)I [z��YI]] 1=1 ,, .,\n(51)\nTaking now the logarithm and then a derivative wrt Z;n (1 � i � k-1) of Eq. 9 yields,\nZ;IYI [n-1 [ J] 9n(Zn); = Yn9o(X); + Yn tt f;(W;)I - x� +\n(52)\nSimilarly, by setting i = i1 and i = i2, 1 � i1 < i2 � k-1 in Eq. 52, subtracting the resulting two equations, and dividing by Yn, we get,\n1 - [gn(Znk- 9n(Zn);,] = 9o(X);1- go(X);,+ (53) Yn � [f;,(W;2)1 [z��YI ] _ fit(W;,)I [z��YI ]] 1=1 '2 ,,\nSubtracting Eq. 53 from Eq. 51 and setting j = h, yields,\nYn f;, (W;, )h f;, (W;2 )h where 1 � i1 < i2 � k- 1, 1 � h � n- 1.\n(54}\nNow we take a derivative wrt z;d1 of Eq. 54 and obtain,\nSimilarly, we take a derivative wrt z;1n of Eq. 54 and obtain,\n_ _!_ [gn(Zn)i1i1- 9n(Zn);,;J = - Y; /;, (W;Jit + Yn X;1 Yn � f· (W.· ) . [- Zi11YI] x· L...J ,, ,, J,l x? '1 1=1 '1 (56)\nEqs. 55 and 56 yield, 1 1\n- 2- [gil (Zit);,;, - 9it (Zit );,;J + 2[9n(Zn);,;, �1 � 1 - 9n(Zn);,;J = -2- /;, (W;,)itit {57) X;,\nNow, we take a derivative wrt z;d2 of Eq. 54 where 1 � h � n- 1, h :/; h (n 2: 3), and obtain,\nEqs. 56 and 58 yield (h :/;h), 1 1 2 [gn(Zn)i1i1- 9n(Zn)i2iJ = - 2-/;1 (W;,)itj, {59) Yn X;1\nA Characterization of the Dirichlet Distribution 205\nPutting Eqs. 57 and 59 into Eq. 58 and recalling (from Eq. 10) that\nwe get,\nn-1 ZhnYn = Xi1 - L Zi1!Yl\nl=l\n....!._ /•, (W<t)Jt = - z;d, (gh (Zj,)s1it - 9it (Zit),,;,] Xil Yit + Zitn [g,..(Zn)i1i1- 9n(Zn)i2it] (60) Yn\nSimilarly, we derive an analogue to Eq. 55 by taking a derivative wrt z;2j1 (instead of wrt z;d1) of Eq. 54, follow the same steps up to Eq. 60, and get,\nz;,j, [git (Zit);,;, - 931 (Zit),,,,] Yit + [g,..(Zn)i}i2- 9n(Zn)i2i2] (61)\nYn\nPlugging Eqs. 60 and 61 into Eq. 54 and collecting all terms involving Yn in one side and all terms not involving Yn on the other side implies that each side is equal to a constant, say c, namely,\n1 - [g·(Z·)· :J J ., Yj 9j(Zj);,] + Zid [gj(Zj)itil- 9j(Zj);,;J Yi\n+ z;,J [gj(Zj);1;2- 9i(Zj)h;2] c (62) Yj\nwhere 1 :S j :S n. This equation holds for every value of Yj and therefore c = 0. Thus we obtain, [gj(Zj)i,- 9;(Zj)i2]\n+ Zid [gj(Zj)itil- 9i(Zj);,;,] +\nz;,j [gj(Zj)iti2- 9i(ZJ)i2;,] = 0 (63)\nLet h(Zj) = Oj(ZJ);1 Oj(ZJ);,. Thus Eq. 63 can be written as follows,\n0 (64)\nLemma 3 provides the general solution for h and thus,\nwhere Z�t;2,j stands for\nand where [Jj is an arbitrary function having one ar gument less than 9i, or also by Lemma 3,\n(66)\nwhere again Yj is an arbitrary function having one ar gument less than !!i. Similarly, since /; and Oi play a symmetric role in .t;;q. 9 as shown by Eq. 12 and hence have the same form, we get\nwhere Wjd2,; stands for\n(ZilYl Zi,Jt-lYb-1 Zi,it+lYit+l ' .. . ' ' ) . . . ' X; Xi Xi Zi1}2-1Yj2-l Zi,}2+1YJ>+l ZinYn ) ' ' .. . , Xi Xi Xi\nor also, we have,\nNow, by setting j = j1 and j 12 m Eq. 54 and subtracting the resulting equations, we get,\n9;1(Zi,)i1- Yit (ZJ1)i, Yh(Zh)i,- Yh(ZJ,);. _ (69) Y11 Yj,\nf;t(W;,)h- f;,(W;t)h _ f;,(W;,)h- f;1(W;2)i2\nPlugging Eqs. 65 through 68 into Eq. 69 yields,\nZi,j1YJ1 f- ( \"'li' Yn W ) il Ziti2Yi2 l jl j2 ,tl\nz;,j,Y;,\n=\nf- (� w ) 12 Zi2j2Yj2 l jlj2,i2 Zi,i2Yh\n(70)\nNote that the variables in Zi, i2,j1 do not appear else where in this equation. Therefore, Dii is only a function of its first argument. Similarly, Yh• h, and h2 are only functions of their first argument. Thus Eq. 70 can be rewritten as follows,\n- 9· (-)-!· (-} =- g· (-) 1 [- X -:.. X ] 1 [- Z X 31 y It z W :12 W By taking a derivative wrt y of Eq. 72, we get,\nzl ( x ) Dit !i\ny2\n(72)\n(73)\n206 Geiger and Heckerman\nSetting y = w, we see that g;1 (t) = /3it and gil (t) = /3it t + a it where a it and f3it are constants. Plugging this result into Eq. 65 yields,\na f3 9j(Zj)i1- 9j(Zj)i2 = - + - (74) z;d z;2j where 1 � i 1 < i2 � k -1. Eq. 7 4 is a first-order partial differential equation the general solution of which is given by Lemma 4. Con sequently, due to Eq. 48, we get,\n(t t ) taitita;2j (t + t t 9j 1, . . . , k-1 =it ;2 9j •• ;2, 1, . . . , t;.-1 , t it + 1 , 0 0 . , t;2-1 , ti2+ 1 , 0 0 . , tk_ t) (75)\nNow due to Lemma 5, we have,\ngj(t 1 , ... ,t k-1) = [[{ t�'j] Gj(�t ;) Similarly,\nt.(t\" . . . , t._,) = [Q tf\"] F;(� t;) (76) (77)\nwhich is obtained by repeating the derivation starting at Eq. 12 rather then at Eq. 9. Note that we have al most derived the Dirichlet functional form. It remains to derive the form of the functions F; and Gj . In Eq. 9 let z1 · = z2j = · · · = Zf:j for 1 � j � n. Thus, according to Bq. 10, Z;j = x;. t.;onsequently, we get,\nn fo(Y1, · ·., Yn-1) II 9j(X1, · ·., Xk-t} = j=1\nk go(x1, ... , Xk-d II /;(Y1, ... , y,_t) (78) i=1\nEqs. 9 and 78 yield, n ( ) k J,·(!ll1ll. Zi,n-lYn-1) II 9j Z1,j, ... , Zk-1,n =II s :r:; , • • • , :r:;\nj=1 9j(X 1, ... , Xk-1) i=1 /;(Y1, . .. , y,_1)\nPlugging Eqs. 76 and 77 into Eq. 79, we get,\nThus, using Zkj = 1-\"'E-7;;/ Z;j (Eq. 10), II Gj(L;-1 Zij) [ n - k-1 ] - k-1 j=1 Gj(Li=1 Xi)\n(79)\n(81)\nwhere for 1 � i � k- 1 and 1 � j � n- 1, c;j = a;j - /3;j, F;(t) = (1- t)-a,,. F;(t) Gj(t) = (1-t)-f3k1Gj(t)\nand where Fk(t) = F k(t) and Gn(t) = Gn(t). We will show that F;(t), i = 1, ... , k- 1, are constants. Con sequently, due to Eq. 77, /; has a Dirichlet functional form. That the function fk also has a Dirichlet func tional form can be obtained by choosing z1j as a dependent variable defined by Z1j = 1-\"'E-7=2 Zij instead of Zkj as defined by Eq. 10 and repeating the same arguments. By symmetric arguments each gj also has a Dirichlet functional form.\nLet Yj = �, for all j, 1 � j � n and Zij = /; for all i and j, 1 � i � k, 1 � j � n - 1. Hence, the only free variables remaining in Eq. 81 are z;, where 1 � i � k1. Note that x; = \"'£_'j=1 ZijYj = \"k-;/ + �z;,, 1 � i � k-1, and so Gj(\"'E-7;;/ x; ) is a function of \"'E-7;;{ Zin· Also Gj (\"'E-7;;/ Zij) is a constant for 1 � j � n -1 and a function of \"'E-7;11 Z;n for j = n. Consequently, Eq. 81 becomes,\nf(I:Zin)=IT F;( c ) [c+ dz;n ] a; i=1 i=1 c + dz;n C (82) where c = \"k-;;1 , d = � and a; = \"'£_j;;11 Cij· Note that Zkn = 1 ->.7;;11 Z;n and so the k-th term on the right hand side OiEq. 81 is absorbed, along with some constants, into the definition of f in Eq. 82. Let t; = c+�z;,.; Z;n = � 1;/'. Taking the logarithm of Eq. 82, we get,\n{83)\nTaking a derivative wrt t;u 1 � i1 � k-1, we get, k-1 1 I\n__ c !''(_:\"' ---=-!!.) = [ln t:- a'• F· (t· )] (84) dt? d L...J t· '1 11 '1 •• i=1 •\nThus, f'(!J \"'E-7;;11 1;/;) must be a constant. Hence, by integrating Eq. 84,\nF;(t)=c;t a•elf, 1�i:Sk-1 (85) where K is a constant not depending on i. To complete the derivation we substitute Eq. 85 into Eq. 81, let Yj = �, for 1 :5 j :5 n and Z;j = /; ex cept zn, 1 � i :5 k -1 which remain free variables. Consequently, we get\nk-1 k-1 ( ) a· z·1 + Wo ' K \"\\'k-l __J__ g(L: Zil) = II . z�il e L-i=l 'il +wo i=1 i=1 s1\nA Characterization of the Dirichlet Distribution 207\nwhere w0 = nJ;2. Therefore, K = 0, a; = 0, and F; is a constant as claimed. Thus,\n(86)\n(87)\nWe now comment on how the derivation changes when n = 2 and k > 3. The case n > 3 and k = 2 follows as well due to the symmetric functional equation (Equa tion 12).\nNote that up to Eq. 57 the derivation is valid when n = 2. Furthermore, note that the sum in Eq. 56 consists now of one term where l = h = 1. Thus, Eq. 56 and Eq. 57 yield, using x; = Zij.Yj1 + ZinYn (n=2, ii = 1),\nJ;1 (W;Jh = Z;1n [gn(Zn)i1i1- 9n(Zn)i2iJ-X;l Yn Zi,jl [gjl {Zjl);l;l- 9j1 {Zjl);2;J (88) Yh Similarly,\nx;2 = Z;2n [gn(Zn)i1i2- 9n(Zn)i2i2]Yn Zi2h [gjl (Zj, );1;2 - 9h (ZjJi2i2] (89) Yh\nwhich is obtained by taking a derivative wrt z;2j. of Eq. 54 (instead of wrt z;dJ and repeating the deriva tion up to Eq. 57. Plugging Eqs. 88 and 89 into Eq. 54 and collecting all terms involving Yn in one side and all terms not involving Yn on the other side implies that each side is equal to a constant, say c, namely, we obtain the partial differential equation for gj(Zj), 1 :::; j $ n, given by Eq. 62. Consequently, as given by Eq. 65 and because n = 2,\n(90)\nand,\n{91)\nAlso, when n = 2, we have x; = z;h Yii + ZinYn, and hence,\nPlugging Eqs. 90 and 93 into Eq. 54 yields�\nThis equation parallels Eq. 71 where (h is replaced by n) and can be solved in the same way. Thus Eq. 76 is obtained. Eq. 77, on the other hand, needs no proof when n = 2 because an arbitrary function f(x) defined on (0, 1) can always be written as f(x) = x\"'g(x) where g(x) = x-\"'f(x). The rest of the derivation follows closely the previous section.\nThe Joint Distribution\nWe have so far shown that, under the assumptions made by Theorem 2, !I ((h.) and h1• ( B Jli) are Dirich let. Similarly, !J(B.J) and fi ii(Biij) have been shown to be Dirichlet as well. We now show that if !I, his, h and !Iii are all Dirichlet, then the joint distribution fu ( { B;j}) must also be a Dirichlet. We can write,\nn fn(B.J, Bill, ... , Bilk) = !J(B.J) IT hli (BIIi) j=1\nk k n - IT B\"'j-1 IT IT B\"''li-1 c -j ilj j=1 j=1 i=1\nBut fu(BJ., B Jl1, ... , B Jlk) can be expressed using fn by two applications of the Jacobian given by Eq. 3. Thus we get,\nwhere B.j = 2:::::; B;.Bjli· Since fu can be expressed, due to local and global independence, as a product of !I, h11, . . . , h 1n each of which has been shown to be a Dirichlet, it follows from Eq. 95 that the exponent coefficients for B-j, 1 :::; j :::; n, must vanish. Conse quently, fu ( { B;j}), which is obtained from Eq. 95 by �ulti�lying with {ll7.=0Jf� 1} -1 and using the rela twnsh1p B;j = Bii;B;., 1s Umchlet."
    } ],
    "references" : [ {
      "title" : "Lectures on Functional Equations and Their Applications",
      "author" : [ "J. Aczel" ],
      "venue" : "Academic Press, New York",
      "citeRegEx" : "Ac66",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "Theory refinement on Bayesian networks",
      "author" : [ "W. Buntine" ],
      "venue" : "Proceedings of Seventh Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Buntine,? \\Q1991\\E",
      "shortCiteRegEx" : "Buntine",
      "year" : 1991
    }, {
      "title" : "A Bayesian method for the induction of probabilistic networks from data, Section on",
      "author" : [ "G. Cooper", "E. Herskovits" ],
      "venue" : "Medical Informatics, Stanford University,",
      "citeRegEx" : "Cooper and Herskovits,? \\Q1991\\E",
      "shortCiteRegEx" : "Cooper and Herskovits",
      "year" : 1991
    }, {
      "title" : "Hyper Markov laws in statistical analysis of decomposable graphi­ cal models",
      "author" : [ "P. Dawid", "S. Lauritzen" ],
      "venue" : "Annals of Statistics, 21:1272-1317",
      "citeRegEx" : "DL93",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Learning Gaus­ sian networks",
      "author" : [ "D. Geiger", "D. Heckerman" ],
      "venue" : "Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Geiger and Heckerman,? \\Q1994\\E",
      "shortCiteRegEx" : "Geiger and Heckerman",
      "year" : 1994
    }, {
      "title" : "A characteriza­ tion of the Dirichlet distribution through global and local independence",
      "author" : [ "D. Geiger", "D. Heckerman" ],
      "venue" : "Computer Science Department, Technical report 9506, February",
      "citeRegEx" : "GH95",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Chicker­ ing, Learning Bayesian networks: The combination of knowledge and statistical data",
      "author" : [ "D. Heckerman", "D. Geiger" ],
      "venue" : "Proceedings of Tenth Conference on Uncertainty in Artificial Intel­ ligence,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1994
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks",
      "author" : [ "J. Pearl" ],
      "venue" : "Plausible Inference,",
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Sequential updating of conditional probabilities on directed graphical structures, Networks",
      "author" : [ "D. Spiegelhalter", "S. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "Spiegelhalter and Lauritzen,? \\Q1990\\E",
      "shortCiteRegEx" : "Spiegelhalter and Lauritzen",
      "year" : 1990
    }, {
      "title" : "Bayesian analysis in expert systems",
      "author" : [ "D. Spiegelhalter", "A. Dawid", "S. Lauritzen", "R. Cowell" ],
      "venue" : "Statistical Science, 8, 219-282",
      "citeRegEx" : "SDLC93",
      "shortCiteRegEx" : null,
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : ", [SDLC93]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "A slightly weaker version is stated in [DL93] (Lemma 7.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "4, that is, finding all functions that satisfy them un­ der different regularity assumptions, are discussed in [Ac66].",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "This question is particularly interesting in light of the analysis of decomposable graphical models given by [DL93].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "[Ac66] J.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "[DL93] P.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "[GH95] D.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "[SDLC93] D.",
      "startOffset" : 0,
      "endOffset" : 8
    } ],
    "year" : 2011,
    "abstractText" : "We provide a new characterization of the Dirichlet distribution. This characterization implies that under assumptions made by sev­ eral previous authors for learning belief net­ works, a Dirichlet prior on the parameters is inevitable.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}