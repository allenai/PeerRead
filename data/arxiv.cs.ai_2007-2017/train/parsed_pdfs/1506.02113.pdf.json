{
  "name" : "1506.02113.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Selective Greedy Equivalence Search: Finding Optimal Bayesian Networks Using a Polynomial Number of Score Evaluations",
    "authors" : [ "David Maxwell", "Christopher Meek" ],
    "emails" : [ "dmax@microsoft.com", "meek@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We introduce Selective Greedy Equivalence Search (SGES), a restricted version of Greedy Equivalence Search (GES). SGES retains the asymptotic correctness of GES but, unlike GES, has polynomial performance guarantees. In particular, we show that when data are sampled independently from a distribution that is perfect with respect to a DAG G defined over the observable variables then, in the limit of large data, SGES will identify G’s equivalence class after a number of score evaluations that is (1) polynomial in the number of nodes and (2) exponential in various complexity measures including maximum-number-of-parents, maximum-cliquesize, and a new measure called v-width that is at least as small as—and potentially much smaller than—the other two. More generally, we show that for any hereditary and equivalenceinvariant property Π known to hold in G, we retain the large-sample optimality guarantees of GES even if we ignore any GES deletion operator during the backward phase that results in a state for which Π does not hold in the commondescendants subgraph."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Greedy Equivalence Search (GES) is a score-based search algorithm that searches over equivalence classes of Bayesian-network structures. The algorithm is appealing because (1) for finite data, it explicitly (and greedily) tries to maximize the score of interest, and (2) as the data grows large, it is guaranteed—under suitable distributional assumptions—to return the generative structure. Although empirical results show that the algorithm is efficient in realworld domains, the number of search states that GES needs to evaluate in the worst case can be exponential in the number of domain variables.\nIn this paper, we show that if we assume the generative distribution is perfect with respect to some DAG G defined over the observable variables, and if G is known to be constrained by various graph-theoretic measures of complexity, then we can disregard all but a polynomial number of the backward search operators considered by GES while retaining the large-sample guarantees of the algorithm; we call this new variant of GES selective greedy equivalence search or SGES. Our complexity results are a consequence of a new understanding of the backward phase of GES, in which edges (either directed or undirected) are greedily deleted from the current state until a local minimum is reached. We show that for any hereditary and equivalenceinvariant property known to hold in generative model G, we can remove from consideration any edge-deletion operator between X and Y for which the property does not hold in the resulting induced subgraph over X, Y, and their common descendants. As an example, if we know that each node has at most k parents, we can remove from consideration any deletion operator that results in a common child with more than k parents.\nWe define a new notion of complexity that we call v-width. For a given generative structure G, v-width is necessarily smaller than the maximum clique size, which is necessarily smaller than or equal to the maximum number of parents per node. By casting limited v-width and other complexity constraints as graph properties, we show how to enumerate directly over a polynomial number of edge-deletion operators at each step, and we show that we need only a polynomial number of calls to the scoring function to complete the algorithm.\nThe main contributions of this paper are theoretical. Our definition of the new SGES algorithm deliberately leaves unspecified the details of how to implement its forward phase; we prove our results for SGES given any implementation of this phase that completes with a polynomial number of calls to the scoring function. A naive implementation is to immediately return a complete (i.e., no independence) graph using no calls to the scoring function, but this choice is unlikely to be reasonable in practice, particularly in dis-\nar X\niv :1\n50 6.\n02 11\n3v 1\n[ cs\n.L G\n] 6\nJ un\n2 01\ncrete domains where the sample complexity of this initial model will likely be a problem. Whereas we believe it an important direction, our paper does not explore practical alternatives for the forward phase that have polynomial-time guarantees.\nThis paper, which is an expanded version of Chickering and Meek (2015) and includes all proofs, is organized as follows. In Section 2, we describe related work. In Section 3, we provide notation and background material. In Section 4, we present our new SGES algorithm, we show that it is optimal in the large-sample limit, and we provide complexity bounds when given an equivalence-invariant and hereditary property that holds on the generative structure. In Section 5, we present a simple synthetic experiment that demonstrates the value of restricting the backward operators in SGES. We conclude with a discussion of our results in Section 6."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "It is useful to distinguish between approaches to learning the structure of graphical models as constraint based, score based or hybrid. Constraint-based approaches typically use (conditional) independence tests to eliminate potential models, whereas score-based approaches typically use a penalized likelihood or a marginal likelihood to evaluate alternative model structures; hybrid methods combine these two approaches. Because score-based approaches are driven by a global likelihood, they are less susceptible than constraint-based approaches to incorrect categorical decisions about independences.\nThere are polynomial-time algorithms for learning the best model in which each node has at most one parent. In particular, the Chow-Liu algorithm (Chow and Liu, 1968) used with any equivalence-invariant score will identify the highest-scoring tree-like model in polynomial time; for scores that are not equivalence invariant, we can use the polynomial-time maximum-branching algorithm of Edmonds (1967) instead. Gaspers et al. (2012) show how to learn k-branchings in polynomial time; these models are polytrees that differ from a branching by a constant k number of edge deletions.\nWithout additional assumptions, most results for learning non-tree-like models are negative. Meek (2001) shows that finding the maximum-likelihood path is NP-hard, despite this being a special case of a tree-like model. Dasgupta (1999) shows that finding the maximum-likelihood polytree (a graph in which each pair of nodes is connected by at most one path) is NP-hard, even with bounded indegree for every node. For general directed acyclic graphs, Chickering (1996) shows that finding the highest marginallikelihood structure under a particular prior is NP-hard, even when each node has at most two parents. Chickering at al. (2004) extend this same result to the large-sample\ncase.\nResearchers often assume that the training-data “generative” distribution is perfect with respect to some model class in order to reduce the complexity of learning algorithms. Geiger et al. (1990) provide a polynomial-time constraint-based algorithm for recovering a polytree under the assumption that the generative distribution is perfect with respect to a polytree; an analogous score-based result follows from this paper. The constraint-based PC algorithm of Sprites et al. (1993) can identify the equivalence class of Bayesian networks in polynomial time if the generative structure is a DAG model over the observable variables in which each node has a bounded degree; this paper provides a similar result for a score-based algorithm. Kalish and Buhlmann (2007) show that for Gaussian distributions, the PC algorithm can identify the right structure even when the number of nodes in the domain is larger than the sample size. Chickering (2002) uses the same DAGperfectness-over-observables assumption to show that the greedy GES algorithm is optimal in the large-sample limit, although the branching factor of GES is worst-case exponential; the main result of this paper shows how to limit this branching factor without losing the large-sample guarantee. Chickering and Meek (2002) show that GES identifies a “minimal” model in the large-sample limit under a less restrictive set of assumptions.\nHybrid methods for learning DAG models use a constraintbased algorithm to prune out a large portion of the search space, and then use a score-based algorithm to select among the remaining (Friedman et al., 1999; Tsamardinos et al., 2006). Ordyniak and Szeider (2013) give positive complexity results for the case when the remaining DAGs are characterized by a structure with constant treewidth.\nMany researchers have turned to exhaustive enumeration to identify the highest-scoring model (Gillispie and Perlman, 2001; Koivisto and Sood 2004; Silander and Myllymäki, 2006; Kojima et al, 2010). There are many complexity results for other model classes. Karger and Srebro (2001) show that finding the optimal Markov network is NP-complete for treewidth > 1. Narasimhan and Bilmes (2004) and Shahaf, Chechetka and Guestrin (2009) show how to learn approximate limited-treewidth models in polynomial time. Abeel, Koller and Ng (2005) show how to learn factor graphs in polynomial time."
    }, {
      "heading" : "3 NOTATION AND BACKGROUND",
      "text" : "We use the following syntactical conventions in this paper. We denote a variable by an upper case letter (e.g., A) and a state or value of that variable by the same letter in lower case (e.g., a). We denote a set of variables by a bold-face capitalized letter or letters (e.g., X). We use a corresponding bold-face lower-case letter or letters (e.g., x) to denote an assignment of state or value to each variable in a given\nset. We use calligraphic letters (e.g., G, E) to denote statistical models and graphs.\nA Bayesian-network model for a set of variables U is a pair (G,θ). G = (V,E) is a directed acyclic graph—or DAG for short—consisting of nodes in one-to-one correspondence with the variables and directed edges that connect those nodes. θ is a set of parameter values that specify all of the conditional probability distributions. The Bayesian network represents a joint distribution over U that factors according to the structure G.\nThe structure G of a Bayesian network represents the independence constraints that must hold in the distribution. The set of all independence constraints implied by the structure G can be characterized by the Markov conditions, which are the constraints that each variable is independent of its non-descendants given its parents. All other independence constraints follow from properties of independence. A distribution defined over the variables from G is perfect with respect to G if the set of independences in the distribution is equal to the set of independences implied by the structure G.\nTwo DAGs G and G′ are equivalent1—denoted G ≈ G′—if the independence constraints in the two DAGs are identical. Because equivalence is reflexive, symmetric, and transitive, the relation defines a set of equivalence classes over network structures. We will use [G]≈ to denote the equivalence class of DAGs to which G belongs.\nAn equivalence class of DAGs F is an independence map (IMAP) of another equivalence class of DAGs E if all independence constraints implied by F are also implied by E . For two DAGs G and H, we use G ≤ H to denote that [H]≈ is an IMAP of [G]≈; we use G < H when G ≤ H and [H]≈ 6= [G]≈.\nAs shown by Verma and Pearl (1991), two DAGs are equivalent if and only if they have the same skeleton (i.e., the graph resulting from ignoring the directionality of the edges) and the same v-structures (i.e., pairs of edges X → Y and Y ← Z where X and Z are not adjacent). As a result, we can use a partially directed acyclic graph— or PDAG for short—to represent an equivalence class of DAGs: for a PDAG P , the equivalence class of DAGs is the set that share the skeleton and v-structures with P2.\nWe extend our notation for DAG equivalence and the DAG IMAP relation to include the more general PDAG structure. In particular, for a PDAG P , we use [P]≈ to denote the corresponding equivalence class of DAGs. For any pair of PDAGs P and Q—where one or both may be a DAG—we\n1We make the standard conditional-distribution assumptions of multinomials for discrete variables and Gaussians for continuous variables so that if two DAGs have the same independence constraints, then they can also model the same set of distributions.\n2The definitions for the skeleton and set of v-structures for a PDAG are the obvious extensions to these definitions for DAGs.\nuse P ≈ Q to denote [Q]≈ = [P]≈ and we use P ≤ Q to denote [Q]≈ is an IMAP of [P]≈. To avoid confusion, for the remainder of the paper we will reserve the symbols G andH for DAGs.\nFor any PDAG P and subset of nodes V, we use P[V] to denote the subgraph of P induced by V; that is, P[V] has as nodes the set V and has as edges all those from P that connect nodes in V. We use NAX,Y to denote, within a PDAG, the set of nodes that are neighbors of X (i.e., connected with an undirected edge) and also adjacent to Y (i.e., without regard to whether the connecting edge is directed or undirected).\nAn edge in G is compelled if it exists in every DAG that is equivalent to G. If an edge in G is not compelled, we say that it is reversible. A completed PDAG (CPDAG) C is a PDAG with two additional properties: (1) for every directed edge in C, the corresponding edge in G is compelled and (2) for every undirected edge in C the corresponding edge in G is reversible. Unlike non-completed PDAGs, the CPDAG representation of an equivalence class is unique. We use PaPY to denote the parents of node Y in P . An edge X → Y is covered in a DAG if X and Y have the same parents, with the exception that X is not a parent of itself."
    }, {
      "heading" : "3.1 Greedy Equivalence Search",
      "text" : "Algorithm GES(D)\nInput : Data D Output: CPDAG C C ←− FES(D) C ←− BES(D, C) return C\nFigure 1: Pseudo-code for the GES algorithm.\nThe GES algorithm, shown in Figure 1, performs a twophase greedy search through the space of DAG equivalence classes. GES represents each search state with a CPDAG, and performs transformation operators to this representation to traverse between states. Each operator corresponds to a DAG edge modification, and is scored using a DAG scoring function that we assume has three properties. First, we assume the scoring function is score equivalent, which means that it assigns the same score to equivalent DAGs. Second, we assume the scoring function is locally consistent, which means that, given enough data, (1) if the current state is not an IMAP of G, the score prefers edge additions that remove incorrect independences, and (2) if the current state is an IMAP of G, the score prefers edge deletions that remove incorrect dependences. Finally, we assume the scoring function is decomposable, which means\nwe can express it as:\nScore(G,D) = n∑\ni=1\nScore(Xi,Pa G i ) (1)\nNote that the data D is implicit in the right-hand side Equation 1. Most commonly used scores in the literature have these properties. For the remainder of this paper, we assume they hold for the scoring function we use.\nAll of the CPDAG operators from GES are scored using differences in the DAG scoring function, and in the limit of large data, these scores are positive precisely for those operators that remove incorrect independences and incorrect dependences.\nThe first phase of the GES—called forward equivalence search or FES—starts with an empty (i.e., no-edge) CPDAG and greedily applies GES insert operators until no operator has a positive score; these operators correspond precisely to the union of all single-edge additions to all DAG members of the current (equivalence-class) state. After FES reaches a local maximum, GES switches to the second phase—called backward equivalence search or BES— and greedily applies GES delete operators until no operator has a positive score; these operators correspond precisely to the union of all single-edge deletions from all DAG members of the current state.\nTheorem 1. (Chickering, 2002) Let C be the CPDAG that results from applying the GES algorithm tom records sampled from a distribution that is perfect with respect to DAG G. Then in the limit of large m, C ≈ G.\nThe role of FES in the large-sample limit is only to identify a state C for which G ≤ C; Theorem 1 holds for GES under any implementation of FES that results in an IMAP of G. The implementation details can be important in practice because what constitutes a “large” amount of data depends on the number of parameters in the model. In theory, however, we could simply replace FES with a (constant-time) algorithm that sets C to be the no-independence equivalence class.\nThe focus of our analysis in the next section is on a modified version of BES, and the details of the delete operator used in this phase are important. We detail the preconditions, scoring function, and transformation algorithm for a delete operator in Figure 2. We note that we do not need to make any CPDAG transformations when scoring the operators; it is only once we have identified the highest-scoring (non-negative) delete that we need to make the transformation shown in the figure. After applying the edge modifications described in the foreach loop, the resulting PDAG P is not necessarily completed and hence we may have to convert P into the corresponding CPDAG representation. As shown by Chickering (2002), this conversion can be accomplished easily by using the structure of P to extract a\nOperator: Delete(X,Y,H) applied to C\n• Preconditions X and Y are adjacent H ⊆ NAY,X H = NAY,X \\H is a clique\n• Scoring Score(Y, {PaCY ∪H} \\X)−Score(Y,X ∪Pa C Y ∪H)\n• Transformation Remove edge between X and Y foreach H ∈ H do\nDAG that we then convert into a CPDAG by undirecting all reversible edges. The complexity of this procedure for a P with n nodes and e edges is O(n · e), and requires no calls to the scoring function."
    }, {
      "heading" : "4 SELECTIVE GREEDY EQUIVALENCE SEARCH",
      "text" : "In this section, we define a variant of the GES algorithm called selective GES—or SGES for short—that uses a subset of the GES operators. The subset is chosen based on a given property Π that is known to hold for the generative structure G. Just like GES, SGES—shown in Figure 3—has a forward phase and a backward phase.\nFor the forward phase of SGES, it suffices for our theoretical analysis that we use a method that returns an IMAP of G (in the large-sample limit) using only a polynomial number of insert-operator score calls. For this reason, we call this phase poly-FES. A simple implementation of poly-FES is to return the no-independence CPDAG (with no score calls), but other implementations are likely more useful in practice.\nThe backward phase of SGES—which we call selective backward equivalence search (SBES)—uses only a subset of the BES delete operators. This subset must necessarily include all Π-consistent delete operators—defined below— in order to maintain the large-sample consistency of GES, but the subset can (and will) include additional operators for the sake of efficient enumeration.\nThe DAG properties used by SGES must be equivalence invariant, meaning that for any pair of equivalent DAGs,\neither the property holds for both of them or it holds for neither of them. Thus, for any equivalence-invariant DAG property Π, it makes sense to say that Π either holds or does not hold for a PDAG. As shown by Chickering (1995), a DAG property is equivalence invariant if and only if it is invariant to covered-edge reversals; it follows that the property that each node has at most k parents is equivalence invariant, whereas the property that the length of the longest directed path is at least k is not. Furthermore, the properties for SGES must also be hereditary, which means that if Π holds for a PDAG P it must also hold for all induced subgraphs of P . For example, the max-parent property is hereditary, whereas the property that each node has at least k parents is not. We use EIH property to refer to a property that is equivalence invariant and hereditary.\nDefinition 1. Π-Consistent GES Delete A GES delete operatorDelete(X,Y,H) is Π consistent for CPDAG C if, for the set of common descendants W of X and Y in the resulting CPDAG C′, the property holds for the induced subgraph C′[X ∪ Y ∪W].\nIn other words, after the delete, the property holds for the subgraph defined byX , Y , and their common descendants.\nAlgorithm SGES(D,Π)\nInput : Data D, Property Π Output: CPDAG C C ←− poly-FES C ←− SBES(D, C, Π) return C\nAlgorithm SBES(D, C,Π)\nInput : Data D, CPDAG C, Property Π Output: CPDAG\nRepeat Ops←− Generate Π-consistent delete operators for C Op←− highest-scoring operator in Ops if score of Op is negative then return C C ←− Apply Op to C\nFigure 4: Pseudo-code for the SBES algorithm."
    }, {
      "heading" : "4.1 LARGE-SAMPLE CORRECTNESS",
      "text" : "The following theorem establishes a graph-theoretic justification for considering only the Π-consistent deletions at each step of SBES.\nTheorem 2. If G < C for CPDAG C and DAG G, then for any EIH property Π that holds on G, there exists a Π-\nconsistent Delete(X,Y,H) that when applied to C results in the CPDAG C′ for which G ≤ C′.\nWe postpone the proof of Theorem 2 to the appendix. The result is a consequence of an explicit characterization of, for a given pair of DAGs G and H such that G < H, an edge in H that we can either reverse or delete in H such that for the resulting DAGH′, we have G ≤ H′3. Theorem 3. Let C be the CPDAG that results from applying the SGES algorithm to (1) m records sampled from a distribution that is perfect with respect to DAG G and (2) EIH property Π that holds on G. Then in the limit of large m, C ≈ G.\nProof: Because the scoring function is locally consistent, we know poly-FES must return an IMAP of G. Because SBES includes all the Π-consistent delete operators, Theorem 2 guarantees that, unless C ≈ G, there will be a positive-scoring operator."
    }, {
      "heading" : "4.2 COMPLEXITY MEASURES",
      "text" : "In this section, we discuss a number of distributional assumptions that we can use with Theorem 3 to limit the number of operators that SGES needs to score. As discussed in Section 2, when we assume the generative distribution is perfect with respect to a DAG G, then graph-theoretic assumptions about G can lead to more efficient training algorithms. Common assumptions used include (1) a maximum parent-set size for any node, (2) a maximum-clique4 size among any nodes and (3) a maximum treewidth. Treewidth is important because the complexity of exact inference is exponential in this measure.\nWe can associate a property with each of these assumptions that holds precisely when the DAG G satisfies that assumption. Consider the constraint that the maximum number of parents for any node in G is some constant k. Then, using “PS” to denote parent size, we can define the property ΠkPS to be true precisely for those DAGs in which each node has at most k parents. Similarly we can define ΠkCL and ΠkTW to correspond to maximum-clique size and maximum treewidth, respectively.\nFor two properties Π and Π′, we write Π ⊆ Π′ if for every DAG G for which Π holds, Π′ also holds. In other words, Π is a more constraining property than is Π′. Because the lowest node in any clique has all other nodes in the clique as parents, it is easy to see that ΠkPS ⊆ Π k−1 CL . Because the treewidth for DAG G is defined to be the size of the largest clique minus one in a graph whose cliques are at least as large as those in G, we also have ΠkTW ⊆ Π k−1 CL . Which\n3Chickering (2002) characterizes the reverse transformation of reversals/additions in G, which provides an implicit characterization of reversals/deletions in H.\n4We use clique in a DAG to mean a set of nodes in which all pairs are adjacent.\nproperty to use will typically be a trade-off between how reasonable the assumption is (i.e, less constraining properties are more reasonable) and the efficiency of the resulting algorithm (i.e., more constraining properties lead to faster algorithms).\nWe now consider a new complexity measure called v-width, whose corresponding property is less constraining than the previous three, and somewhat remarkably leads to an efficient implementation in SGES. For a DAG G, the v-width is defined to be the maximum of, over all pairs of nonadjacent nodes X and Y , the size of the largest clique among common children of X and Y . In other words, v-width is similar to the maximum-clique-size bound, except that the bound only applies to cliques of nodes that are shared children of some pair of non-adjacent nodes. With this understanding it is easy to see that, for the property ΠkVW corresponding to a bound on the v-width, we have ΠkCL ⊆ ΠkVW .\nTo illustrate the difference between v-width and the other complexity measures, consider the two DAGs in Figure 5. The DAG in Figure 5(a) has a clique of size K, and consequently a maximum-clique size of K and a maximum parent-set size of K − 1. Thus, if K is O(n) for a large graph of n nodes, any algorithm that is exponential in these measures will not be efficient. The v-width, however, is zero for this DAG. The DAG in Figure 5(b), on the other hand, has a v-width of K.\nIn order to use a property with SGES, we need to establish that it is EIH. For ΠkPS , Π k CL and Π k VW , equivalenceinvariance follows from the fact that all three properties are covered-edge invariant, and hereditary follows because the corresponding measures cannot increase when we remove nodes and edges from a DAG. Although we can establish EIH for the treewidth property ΠkTW with more work, we omit further consideration of treewidth for the sake of space."
    }, {
      "heading" : "4.3 GENERATING DELETIONS",
      "text" : "In this section, we show how to generate a set of deletion operators for SBES such that all Π-consistent deletion operators are included, for any Π ∈ {ΠkPS ,ΠkCL,ΠkVW }. Furthermore, the total number of deletion operators we generate is polynomial in the number of nodes in the domain and exponential in k.\nOur approach is to restrict the Delete(X,Y,H) operators based on the H sets and the resulting CPDAG C′. In particular, we rule out candidate H sets for which Π does not hold on the induced subgraph C′[H ∪X ∪ Y ]; because all nodes in H will be common children of X and Y in C′—and thus a subset of the common descendants of X and Y—we know from Definition 1 (and the fact that Π is hereditary) that none of the dropped operators can be Πconsistent.\nBefore presenting our restricted-enumeration algorithm, we now discuss how to enumerate delete operators without restrictions. As shown by Andersson et al. (1997), a CPDAG is a chain graph whose undirected components are chordal. This means that the induced sub-graph defined over NAY,X—which is a subset of the neighbors of Y—is an undirected chordal graph. A useful property of chordal graphs is that we can identify, in polynomial time, a set of maximal cliques over these nodes5; let C1, ...,Cm denote the nodes contained within these m maximal cliques, and let H = NAY,X \\ H be the complement of the shared neighbors with respect to the candidate H. Recall from Figure 2 that the preconditions for any Delete(X,Y,H) include the requirement that H is a clique. This means that for any valid H, there must be some maximal clique Ci that contains the entirety of H; thus, we can generate all operators (without regard to any property) by stepping through each maximal clique Ci in turn, initializing H to be all nodes not in Ci, and then generating a new operator corresponding to expanding H by all subsets of nodes in Ci. Note that if NAY,X is itself a clique, we are enumerating over all 2|NAY,X | operators.\nAs we show below, all three of the properties of interest impose a bound on the maximum clique size among nodes in H. If we are given such a bound s, we know that any “expansion” subset for a clique that has size greater than s will result in an operator that is not valid. Thus, we can implement the above operator-enumeration approach more efficiently by only generating subsets within each clique that have size at most s. This allows us to process each clique Ci with only O(|Ci + 1|s) calls to the scoring function. In addition, we need not enumerate over any of the subsets of Ci if, after removing this clique from the graph, there remains a clique of size greater than s; we define the\n5Blair and Peyton (1993) provide a good survey on chordal graphs and detail how to identify the maximal cliques while running maximum-cardinality search.\nAlgorithm SELECTIVE-GENERATE-OPS(C, X, Y, s)\nInput : CPDAG C with adjacent X ,Y and limit s Output: Ops = {H1, . . . ,Hm}\nfunction FilterCliques({C1, . . . ,Cm}, s) to be the subset of cliques that remain after imposing this constraint. With this function, we can define SELECTIVE-GENERATEOPS as shown in Figure 6 to leverage the max-clique-size constraint when generating operators; this algorithm will in turn be used to generate all of the CPDAG operators during SBES.\nExample: In Figure 7, we show an example CPDAG for which to run SELECTIVE-GENERATE-OPS(C, X , Y , s) for various values of s. In the example, there is a single clique C = {A,B} in the set NAY,X , and thus at the top of the outer foreach loop, the set H0 is initialized to the empty set. If s = 0, the only subset of C with size zero is the empty set, and so that is added to Ops and the algorithm returns. If s = 1 we add, in addition to the empty set, all singleton subsets of C. For s ≥ 2, we add all subsets of C.\nNow we discuss how each of the three properties impose a constraint s on the maximum clique among nodes in H, and consequently the selective-generation algorithm in Figure 6 can be used with each one, given an appropriate bound s. For both ΠkVW and Π k CL, the k given imposes an explicit bound on s (i.e., s = k for both). Because any clique in H of size r will result in a DAG member of the resulting equivalence class having a node in that clique with at least r+ 1 parents (i.e., r− 1 from the other nodes in the clique, plus both X and Y ), we have for ΠkPS , s = k − 1.\nWe summarize the discussion above in the following proposition.\nProposition 1. Algorithm SELECTIVE-GENERATE-OPS applied to all edges using clique-size bound s generates all Π-consistent delete operators for Π ∈ {Πs+1PS ,ΠsCL,ΠsVW }.\nWe now argue that running SBES on a domain of n variables when using Algorithm SELECTIVE-GENERATE-OPS with a bound s requires only a polynomial number in n of calls to the scoring function. Each clique in the inner loop of the algorithm can contain at most n nodes, and therefore we generate and score at most (n+1)s operators, requiring at most 2(n + 1)s calls to the scoring function. Because the cliques are maximal, there can be at most n of them considered in the outer loop. Because there are never more than n2 edges in a CPDAG, and we will delete at most all of them, we conclude that even if we decided to rescore every operator after every edge deletion, we will only make a polynomial number of calls to the scoring function.\nFrom the above discussion and the fact that SBES completes using at most a polynomial number of calls to the scoring function, we get the following result for the full SGES algorithm.\nProposition 2. The SGES algorithm, when run over a domain of n variables and given Π ∈ {Πs+1PS ,ΠsCL,ΠsVW }, runs to completion using a number of calls to the DAG scoring function that is polynomial in n and exponential in s."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "In this section, we present a simple synthetic experiment comparing SBES and BES that demonstrates the value of pruning operators. In our experiment we used an oracle scoring function. In particular, given a generative model G, our scoring function computes the minimum-descriptionlength score assuming a data size of five billion records, but without actually sampling any data: instead, we use exact inference in G (i.e., instead of counting from data) to compute the conditional probabilities needed to compute the expected log loss. This allows us to get near-asymptotic behavior without the need to sample data. To evaluate the cost of running each algorithm, we counted the number of times the scoring function was called on a unique node and parent-set combination; we cached these scores away so that if they were needed multiple times during a run of the algorithm, they were only computed (and counted) once.\nIn Figure 8, we show the average number of scoringfunction calls required to complete BES and SBES when starting from a complete graph over a domain of n binary variables, for varying values of n. Each average is taken over ten trials, corresponding to ten random generative models. All variables in the domain were binary. We generated the structure of each generative model as follows. First, we enumerated all node pairs by randomly permuting the nodes and taking each node in turn with all of its predecessors in turn. For each node pair in turn, we chose to attempt an edge insertion with probability one half. For each attempt, we added an edge if doing so (1) did not create a cycle and (2) did not result in a node having more than two parents; if an edge could be added in either direction, we chose the direction at random. We sampled the conditional distributions for each node and each parent configuration from a uniform Dirichlet distribution with equivalent-sample size of one. We ran SBES with Π2PS .\n25000\nOur results show clearly the exponential dependence of BES on the number of nodes in the clique, and the increasing savings we get with SBES, leveraging the fact that Π2PS holds in the generative structure.\nNote that to realize large savings in practice, when GES runs FES instead of starting from a dense graph, a (relatively sparse) generative distribution must lead FES to an equivalence class containing a (relatively dense) undirected clique that is subsequently “thinned” during BES. We can synthesize challenging grid distributions to force FES into such states, but it is not clear how realistic such distributions are in practice. When we re-run the clique experiment above, but where we instead start both BES and SBES from the model that results from running FES (i.e., with no polynomial-time guarantee), the savings from SBES are\nsmall due to the fact that the subsequent equivalence classes do not contain large cliques."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "Through our selective greedy equivalence search algorithm SGES, we have demonstrated how to leverage graph-theoretic properties to reduce the need to score graphs during score-based search over equivalence classes of Bayesian networks. Furthermore, we have shown that for graph-theoretic complexity properties including maximum-clique size, maximum number of parents, and v-width, we can guarantee that the number of score evaluations is polynomial in the number of nodes and exponential in these complexity measures.\nThe fact that we can use our approach to selectively choose operators for any hereditary and equivalence invariant graph-theoretic property provides the opportunity to explore alternative complexity measures. Another candidate complexity measure is the maximum number of vstructures. Although the corresponding property does not limit the maximum size of a clique in H, it limits directly the size |H| for every operator. Thus it would be easy to enumerate these operators efficiently. Another complexity measure of interest is treewidth, due to the fact that exact inference in a Bayesian-network model is takes time exponential in this measure.\nThe results we have presented are for the general Bayesiannetwork learning problem. It is interesting to consider the implications of our results for the problem of learning particular subsets of Bayesian networks. One natural class that we discussed in Section 2 is that of polytrees. If we assume that the generative distribution is perfect with respect to a polytree then we know the v-width of the generative graph is one. This implies, in the limit of large data, that we can recover the structure of the generative graph with a polynomial number of score evaluations. This provides a scorebased recovery algorithm analogous to the constraint-based approach of Geiger et al. (1990).\nWe presented a simple complexity analysis for the purpose of demonstrating that SGES uses a only polynomial number of calls to the scoring function. We leave as future work a more careful analysis that establishes useful constants in this polynomial. In particular, we can derive tighter bounds on the total number of node-and-parent-configurations that are needed to score all the operators for each CPDAG, and by caching these configuration scores we can further take advantage of the fact that most operators remain valid (i.e., the preconditions still hold) and have the same score after each transformation.\nFinally, we plan to investigate practical implementations of poly-FES that have the polynomial-time guarantees needed for SGES.\nAppendices\nIn the following two appendices, we prove Theorem 2."
    }, {
      "heading" : "A Additional Background",
      "text" : "In this section, we introduce additional background material needed for the proofs.\nA.1 Additional Notation\nTo express sets of variables more compactly, we often use a comma to denote set union (e.g., we write X = Y,Z as a more compact version of X = Y ∪Z). We also will sometimes remove the comma (e.g., YZ). When a set consists of a singleton variable, we often use the variable name as shorthand for the set containing that variable (e.g., we write X = Y \\ Z as shorthand for X = Y \\ {Z}).\nWe say a nodeN is a descendant of Y ifN = Y or there is a directed path from Y toN . We useH-descendant to refer to a descendant in a particular DAG H. We say a node N is a proper descendant of Y if N is a descendant of Y and N 6= Y . We use NonDeHY to denote the non-descendants of node Y in G. We use PaHY ↓X1X2...Xn as shorthand for PaHY \\ {X1, . . . , Xn}. For example, to denote all the parents of Z inH except for X and Y , we use PaHZ↓XY .\nA.2 D-separation and Acvite Paths\nThe independence constraints implied by a DAG structure are characterized by the d-separation criterion. Two nodes A and B are said to be d-separated in a DAG G given a set of nodes S if and only if there is no active path in G between A and B given S. The standard definition of an active path is a simple path for which each node W along the path either (1) has converging arrows (i.e., → W ←) and W or a descendant of W is in S or (2) does not have converging arrows and W is not in S. By simple, we mean that the path never passes through the same node twice.\nTo simplify our proofs, we use an equivalent definition of an active path—that need not be simple—where each node W along the path either (1) has converging arrows andW is in S or (2) does not have converging arrows andW is not in S. In other words, instead of allowing a segment→ W ← to be included in a path by virtue of a descendant of W belonging to S, we require that the path include the sequence of edges from W to that descendant and then back again. For those readers familiar with the celebrated “Bayes ball” algorithm of Shachter (1998) for testing d-separation, our expanded definition of an active path is simply a valid path that the ball can take between A and B.\nWe use X⊥⊥GY|Z to denote the assertion that DAG G imposes the constraint that variables X are independent of variables Y given variables Z.When a nodeW along a path\nhas converging arrows, we say that W is a collider at that position in the path.\nThe direction of each terminal edge in an active path—that is, the first and last edge encountered in a traversal from one end of the path to the other—is important for determining whether we can append two active paths together to make a third active path. We say that a path π(A,B) is into A if the terminal edge incident to A is oriented toward A (i.e., A ←). Similarly, the path is into B if the terminal edge incident to B is oriented toward B. If a path is not into an endpoint A, we say that the path is out of A. Using the following result from Chickering (2002), we can combine active paths together.\nLemma 1. (Chickering, 2002) Let π(A,B) be an S-active path between A and B, and let π(B,C) be an S-active path between B and C. If either path is out of B, then the concatenation of π(A,B) and π(B,C) is an S-active path between A and C.\nGiven a DAG H that is an IMAP of DAG G, we use the d-separation criterion in two general ways in our proofs. First, we identify d-separation facts that hold inH and conclude that they must also hold in G. Second, we identify active paths in G and conclude that there must be corresponding active paths inH.\nA.3 Independence Axioms\nIn many of our proofs, we would like to reason about the independence facts that hold in DAG G without knowing what its structure is, which makes using the d-separation criterion problematic. As described in Pearl (1988), any set of independence facts characterized by the d-separation criterion also respect the independence axioms shown in Figure 9. These axioms allow us to take a set of independence facts in some unknown G (e.g., that are implied by d-separation inH), and derive new independence facts that we know must also hold in G.\nThroughout the proofs, we will often use the Symmetry axiom implicitly. For example, if we have A⊥⊥B,C|D we might claim that B⊥⊥A|C,D follows from Weak Union, as opposed to concluding A⊥⊥B|C,D from Weak Union and then applying Symmetry. We will frequently identify independence constraints in H and conclude that they hold in G, without explicitly justifying this with because G ≤ H. For example, we will say:\nBecause A is a non-descendant of B in H, it follows from the Markov conditions that A⊥⊥GB|Pa H B .\nIn other words, to be explicit we would say that A⊥⊥HB|Pa H B follows from the Markov conditions, and the independence holds in G because G ≤ H.\nThe Composition axiom states that if X is independent of both Y and W individually given Z, then X is independent of them jointly. If we have more than two such sets that are independent of X, we can apply the Composition axiom repeatedly to combine them all together. To simplify, we will do this combination implicitly, and assume that the Composition axiom is defined more generally. Thus, for example, we might have:\nBecause X⊥⊥Y |Z for every Y ∈ Y, we conclude by the Composition axiom that X⊥⊥Y|Z."
    }, {
      "heading" : "B Proofs",
      "text" : "In this section, we provide a number of intermediate results that lead to a proof of Theorem 2.\nB.1 Intermediate Result: “The Deletion Lemma”\nGiven DAGs G and H for which G < H, we say that an edge e from H is deletable in H with respect to G if, for the DAGH′ that results after removing e fromH, we have G ≤ H. We will say that an edge is deletable in H or simply deletable if G or both DAGs, respectively, are clear from context. The following lemma establishes necessary and sufficient conditions for an edge to be deletable. Lemma 2. Let G and H be two DAGs such that G ≤ H. An edge X → Y is deletable in H with respect to G if and only if Y⊥⊥GX|PaHY \\X .\nProof: Let H′ be the DAG resulting from removing the edge. The “only if” follows immediately because the given independence is implied by H′. For the “if”, we show that for every nodeA and every nodeB ∈ NonDeH ′\nA , the independence A⊥⊥GB|PaH ′\nA holds (in G). We need only consider (A,B) pairs for whichB is a descendant inH but not inH′; if the “descendant” relationship has not changed, we know the independence holds by virtue of G ≤ H and the fact that deleting an edge results in strictly more independence constraints.\nThe proof follows by induction on the length of the longest directed path in H′ from Y to B. For the base case (see Figure 10a and Figure 10b), we start with a longest path of length zero; in other words, B = Y . Because A is an\nancestor of Y in H, both it and its parents must be nondescendants of Y in H, and therefore the Markov conditions inH imply\nY⊥⊥GA,PaHA |Pa H Y (2)\nGiven the independence fact assumed in the lemma, we can apply the Contraction axiom to remove X from the conditioning set in (2), and then apply the Weak Union axiom to move PaHA into the conditioning set to conclude\nY⊥⊥GA|PaHY \\X,Pa H A (3)\nNeither Y nor its new parents PaHY \\X can be descendants ofA inH′, elseB would remain a descendant ofA after the deletion, and thus we conclude by the Markov conditions inH that\nA⊥⊥GPaHY \\X|Pa H A (4)\nApplying the Contraction axiom to (3) and (4), we have\nA⊥⊥GY |PaHA\nand because PaH ′ A = Pa H A the lemma follows.\nFor the induction step (see Figure 10c and Figure 10d), we assume the lemma holds for all nodes whose longest path from Y is ≤ k, and we consider a B for which the longest path from Y is k + 1. Consider any parent P of node B. If P is a descendant of Y , the longest path from Y to B must be ≤ k, else we have a path to B that is longer than k + 1. If P is not a descendant of Y , then P is also not a descendant of A in H, else B would be a descendant of A inH′. Thus, for every parent P , we conclude\nA⊥⊥GP |PaHA\neither by the induction hypothesis or by the fact that P is a non-descendant of A in H. From the Composition axiom we can combine these individual parents together, yielding\nA⊥⊥GPaHB |Pa H A (5)\nBecause B is a descendant of A inH, we know that A and all of its parents PaHA are non-descendants of B in H, and thus\nB⊥⊥GA,PaHA |Pa H B (6)\nApplying the Weak Union axiom to (6) yields\nB⊥⊥GA|PaHA ,Pa H B (7)\nand finally applying the Contraction axiom to (5) and (7) yields A⊥⊥GB|PaHA Because the parents of A are the same in both H and H′, the lemma follows.\nB.2 Intermediate Result: “The Deletion Theorem”\nWe define the pruned variables for G and H—denoted Prune(G,H)– to be the subset of the variables that remain after we repeatedly remove from both graphs any common sink nodes (i.e., nodes with no children) with the same parents in both graphs. For V = Prune(G,H), let V denote the complement of V. Note that every node in V has the same parents and children in both G and H, and that G[V] = H[V].\nWe use G-leaf to denote any node in G that has no children. For any G-leaf L, we say that L is anH-lowest G-leaf if no proper descendant of L in H is a G-leaf . Note that we are discussing two DAGs in this case: L is a leaf in G, and out of all nodes that are leaves in G, L is the one that is lowest in the other DAG H. To avoid ambiguity, we often prefix other common graph concepts (e.g., G-child andH-parent) to emphasize the specific DAG to which we are referring.\nWe need the following result from Chickering (2002).\nLemma 3. (Chickering, 2002) Let G andH be two DAGs containing a node Y that is a sink in both DAGs and for which PaGY = Pa H Y . Let G′ and H′ denote the subgraphs of G and H, respectively, that result by removing node Y and all its in-coming edges. Then G ≤ H if and only if G′ ≤ H′.\nBy repeatedly applying Lemma 3, the following corollary follows immediately.\nCorollary 1. Let V = Prune(G,H). Then G ≤ H if and only if GV ≤ HV.\nWe now present the “deletion theorem”, which is the basis for Theorem 2.\nTheorem 4. Let G and H be DAGs for which G ≤ H, let V = Prune(G,H), and let L be any H[V]-lowest G[V]leaf. Then,\n1. If L does not have any H[V]-children, then for every D ∈ V that is an H[V]-parent of L but not a G[V]parent of L, D → L is deletable inH.\n2. If L has at least one H[V]-child, let A be any H[V]highest child; one of the following three properties must hold inH:\n(a) L→ A is covered.\n(b) There exists an edge A← B, where L and B are not adjacent, and either L → A or A ← B (or both) are deletable.\n(c) There exists an edgeD → L, whereD andA are not adjacent, and either D → L or L → A (or both) are deletable.\nProof: As a consequence of Corollary 1, the lemma holds if and only if it holds for any graphs G and H for which there are no nodes that are sinks in both graphs with the same parents; in other words, G = GV and H = HV. Thus, to vastly simplify the notation for the remainder of the proof, we will assume that this is the case, and therefore L is a leaf node in G, A is a highest child of L in H, and the restriction of B and D to V is vacuous.\nFor case (1), we know that PaGL ⊆ Pa H L , else there would be some edge in X → L in G for which X and L are not adjacent in H, contradicting G ≤ H. Because L is a leaf in G, all non-parents must also be non-descendants, and hence L⊥⊥GX|PaGL for all X . It follows that for every D ∈ {PaHL \\Pa G L}, D → Y is deletable inH. There must exist such a D, else L would be in V = Prune(G,H).\nFor case (2), we now show that at least one of the properties must hold. Assume that the first property does not hold, and demonstrate that one of the other two properties must hold. If the first property does not hold then we know that in H either there exists an edge A ← B where B is not a parent of L, or there exists an edge D → L where D is not a parent of A. Thus the pre-conditions of at least one of the remaining two properties must hold.\nSupposeH contains the edgeA← B whereB is not a parent of L. Then we conclude immediately from Corollary 2 that either L→ A or A← B is deletable inH.\nSupposeH contains the edgeD → L whereD is not a parent of A. Then the set D containing all parents of L that are not parents of A is non-empty. Let R = PaHA ∩ Pa H L be the shared parents of L and A, and let T = PaH A↓RL be the remaining non-L parents of A inH, so that we have PaHA = L,R,T and Pa H L = R,D. Because no node in D is a child or a descendant of A, lest H contains a cycle, we know that H contains the following independence constraint that must hold in G:\nA⊥⊥GD|L,R,T (8)\nBecause L is a leaf node in G, it is impossible to create a new active path by removing it from the conditioning set, and hence we also know\nA⊥⊥GD|R,T (9)\nApplying the Weak Transitivity axiom to Independence 8 and Independence 9, we conclude either A⊥⊥GL|R,T— in which case L→ A is deletable–or\nL⊥⊥GD|R,T (10)\nWe know that no node in T can be a descendant of L, or else A would not be the highest child of L. Thus, because L is independent of any non-descendants given its parents we have\nL⊥⊥GT|R,D (11)\nApplying the Intersection axiom to Independence 10 and Independence 11, we have\nL⊥⊥GD|R (12)\nIn other words, L is independent of all of the nodes in D given the other parents. By applying the Weak Union axiom, we can pull all but one of the nodes in D into the conditioning set to obtain\nL⊥⊥GD|R, {D \\D} (13)\nand hence D → L is deletable for each such D.\nB.3 Intermediate Result: “Add A Singleton Descendant to the Conditioning Set”\nThe intuition behind the following lemma is that if L is an H-lowest G-leaf , no v-structure below L in H can be “real” in terms of the dependences in G: for any Y below L that is independent of some other node X , they remain independent when we condition on any singleton descendant Z of Y , even if Z is also a descendant of X . The lemma is stated in a somewhat complicated manner because we want to use it both when (1) X and Y are adjacent but the edge is known to be deletable and (2) X and Y are not adjacent. We also find it convenient to include, in addition to Y ’s non-X parents, an arbitrary additional set of nondescendants S.\nLemma 4. Let Y be any H-descendant of an H-lowest G-leaf . If\nY⊥⊥GX|Pa H Y ↓X ,S\nfor {X,S} ⊆ NonDeHY , then Y⊥⊥GX|Pa H Y ↓X ,S, Z for any properH-descendant Z of Y .\nProof: To simplify notation, let R = PaHY ↓X ,S. Assume the lemma does not hold and thus Y 6⊥⊥GX|R, Z. Consider any (R, Z)-active path πXY between X and Y in G. Because Y⊥⊥GX|R, this path cannot be active without Z in the conditioning set, which means that Z must be on the path, and it must be a collider in every position it occurs. Without loss of generality, assumeZ occurs exactly once as a collider along the path (we can simply delete the sub-path between the first and last occurrence of Z, and the resulting path will remain active), and let πXZ be the sub-path from X to Z along πXY , and let πZY be the sub-path from Z to Y along πXY .\nBecause Z is a proper descendant of Y in H, and Y is a descendant of an H-lowest G-leaf , we know Z cannot be a G-leaf , else it would be lower than L in H. That means\nthat in G, there is a directed path πZL′ = Z → . . . → L′ consisting of at least one edge from Z to some G-leaf . No node T along this path can be in R, else we could splice in the path Z → . . . → T ← . . . ← Z between πXZ and πZY , and the resulting path would remain active without Z in the conditioning set. Note that this means that L′ cannot belong to PaHY ↓X ⊆ R. Similarly, the path cannot reach X or Y , else we could combine this out-of-Z path with πZY or πXZ , respectively, to again find an R-active path between X and Y . We know that in H, L′ must be a non-descendant of Y , else L′ would be a lower G-leaf than L in H. Because X ∪ R contains all of Y ’s parents and none of its descendants, and because (as we noted) L′ cannot be inX∪R, we knowH contains the independence Y⊥⊥HL′|X,R. But we just argued that the (directed) path πZL′ in G does not pass through any of X,Y,R, which means that it constitutes an out-of Z (R, X)-active path that can be combined with πZY to produce a (R, X)-active path between Z and L′, yielding a contradiction.\nB.4 Intermediate Result: The “Weak-Transitivity Deletion” Lemma\nThe next lemma considers a collider X → Z ← Y in H where either there is no edge between X and Y (i.e., the collider is a v-structure) or the edge is deletable. The lemma states that if X and Y remain independent when conditioning on their common child—where all the non{X,Y, Z} parents of all three nodes are also in the conditioning set—then one of the two edges must be deletable.\nLemma 5. Let X → Z and Y → Z be two edges in H. If X⊥⊥GY |PaHX↓Y ,Pa H Y ↓X ,Pa H Z↓XY and X⊥⊥GY |PaHX↓Y ,Pa H Y ↓X ,Pa H Z↓XY , Z (i.e., Z added to the conditioning set), then at least one of the following must hold: Z⊥⊥GX|PaHZ↓X or Z⊥⊥GY |Pa H Z↓Y .\nProof: Let S = {PaHX↓Y ,Pa H Y ↓X}\\Pa H Z↓XY be the (nonX and non-Y ) parents of X and Y that are not parents of Z, and let R = PaHZ↓XY be all of Z’s parents other than X and Y . Using this notation, we can re-write the two conditions of the lemma as:\nX⊥⊥GY |R,S (14)\nand X⊥⊥GY |Z,R,S (15)\nFrom the Weak Transitivity axiom we conclude from these two independences that either Z⊥⊥GX|R,S or Z⊥⊥GX|R,S. Assume the first of these is true\nZ⊥⊥GX|R,S (16)\nIf we apply the Composition axiom to the independences in Equation 14 and Equation 16 we get X⊥⊥HY,Z|R,S; applying the Weak Union axiom we can then pull Y into\nthe conditioning set to get:\nZ⊥⊥HX|{Y,R},S (17)\nBecause {Y,R}, X is precisely the parent set of Z, and because S (i.e., the parents of Z’s parents) cannot contain any descendant of Z, we know by the Markov conditions that\nZ⊥⊥HS|{Y,R}, X (18)\nApplying the Intersection Axiom to the independences in Equation 17 and Equation 18 yields:\nZ⊥⊥HX|Y,R\nBecause Y,R = PaHZ↓X , this means the first independence implied by the lemma follows.\nIf the second of the two independence facts that follow from Weak Transitivity hold (i.e., if Z⊥⊥GX|R,S), then a completely parallel application of axioms leads to the second independence implied by the lemma.\nB.5 Intermediate Result: The “Move Lower” Lemma\nLemma 6. Let Y be any H-descendant of an H-lowest G-leaf . If there exists an X ∈ NonDeHY that has a commonH-descendant with Y and for which\nY⊥⊥GX|PaHY ↓X\nthen there exists an edge W → Z that is deletable in H, where Z is a properH-descendant of Y .\nProof: Let Z be the highest common descendant of Y and X , let DY be the lowest descendant of Y that is a parent of Z, and let DX be any descendant of X that is a parent of Z. We know that either (1) DY = Y and DX = X or (2) DY and DX are not adjacent and have no directed path connecting them; if this were not the case, andH contained a path DY → . . . → DX (DX → . . . → DY ) then DX (DY ) would be a higher common descendant than Z. This means that in either case (1) or in case (2), we have\nDY⊥⊥GDX |PaHDY ↓DX (19)\nFor case (1), this is given to us explicitly in the statement of the lemma, and for case (2), PaHDY ↓DX = Pa H DY and thus the independence holds from the Markov conditions in H because DX is a non-descendant of DY . Because in both cases we know there is no directed path from DY to DX , we know that all of PaHDX↓DY are non-descendants of DY , and thus we can add them (via Composition and Weak Union) to the conditioning set of Equation 19:\nDY⊥⊥GDX |PaHDY ↓DX ,Pa H DX↓DY (20)\nFor any PZ ∈ PaHZ↓DY DX (i.e., any parent of Z excluding DY and DX ), we know that PZ cannot be a descendant of\nDY , else PZ would have been chosen instead of DY as the lowest descendant of Y that is a parent of Z. Thus, we can yet again add to the conditioning set (via Composition and Weak Union) to get:\nDY⊥⊥GDX |PaHDY ↓DX ,Pa H DX↓DY ,Pa H Z↓DY DX (21)\nBecause no member of the conditioning set in Equation 21 is a descendant of DY , and because DY , by virtue of being a descendant of Y , must also be a descendant of the Hlowest G-leaf , we conclude from Lemma 4 that for (proper H-descendant of Y ) Z we have:\nDY⊥⊥GDX |PaHDY ↓DX ,Pa H DX↓DY ,Pa H Z↓DY DX , Z\n(22) Given Equation 21 and Equation 22, we can apply Lemma 5 and conclude either (1) Z⊥⊥GDY |PaHZ↓DY and hence DY → Z is deletable in H or (2) Z⊥⊥GDX |PaHZ↓DX and hence DX → Z is deletable inH Corollary 2. Let L be an H-lowest G-leaf , and let A be anyH-highest child of L. If there exists an edge A← B in H for which L and B are not adjacent, then either L→ A or A← B is deletable inH.\nProof: Because L is equal to (and thus a descendant of) an H-lowest G-leaf , it satisfies the requirement for “Y ” in the statement of Lemma 6. Because A is the highest child of L, B cannot be a descendant of L and thus satisfies the requirement of “X” in the statement of Lemma 6. From the proof of the lemma, if we choose A to be the highestcommon descendant (i.e., “Z”), the corollary follows by noting that because A is the highest H-child of L, L must be a lowest parent of A, and thus we can choose DY = L DX = B.\nB.6 Intermediate Result: “The Move-Down Corollary”\nCorollary 3. Let X → Y be any deletable edge within H for which Y is a descendant of an H-lowest G-leaf . Then there exists an edge Z → W that is deletable in H for which Z and W have no common descendants.\nProof: If X and Y have a common descendant, we know from Lemma 6 that there must be another deletable edge Z → W for which W is a proper descendant of Y , and thus Z and W satisfy the conditions for “X” and “Y ”, respectively, in the statement of Lemma 6, but with a lower “Y ” than we had before. Because H is acyclic, if we repeatedly apply this argument we must reach some edge for which the endpoints have no common descendants.\nB.7 Main Result: Proof of Theorem 2\nTheorem 2 If G < C for CPDAG C and DAG G, then for any EIH property Π that holds on G, there exists a Πconsistent Delete(X,Y,H) that when applied to C results in the CPDAG C′ for which G ≤ C′.\nProof: Consider any DAG H0 in [C]≈. From Theorem 4, we know that there exists either a covered edge or deletable edge inH0; if we reverse any covered edge in DAGHi, the resulting DAG Hi+1 (which is equivalent to Hi) will be closer to G in terms of total edge differences, and therefore because H0 6= G we must eventually reach an H = Hi for which Theorem 4 identifies a deletable edge e. The edge e inH satisfies the preconditions of Corollary 3, and thus we know that there must also exist a deletable edge X → Y in H for which X and Y have no common descendants in H[V] for V = Prune(G,H).\nLet H′ be the DAG that results from deleting the edge X → Y in H. Because there is a GES delete operator corresponding to every edge deletion in every DAG in [C]≈, we know there must be a set H for which the operator Delete(X,Y,H)—when applied to C—results in C′ = [H′]≈. Because X → Y is deletable in H, the operator satisfies the IMAP requirement in the theorem. For the remainder of the proof, we demonstrate that it is Πconsistent.\nBecause all directed edges in C′ are compelled, these edges must exist with the same orientation in all DAGs in [C′]≈; it follows that any subset W of the common descendants of X and Y in C′ must also be common descendants of X and Y in H′. But because X and Y have no common descendants in the “pruned” subgraphH[V], we know that W is contained entirely in the complement of V, which meansH[W] = G[W]; becauseH′ is the same asH except for the edge X → Y , we concludeH′[W] = G[W].\nWe now consider the induced subgraph H′[W ∪X ∪ Y ] that we get by “expanding” the graph H′[W] to include X and Y . Because X and Y are not adjacent in H′, and becauseH′ is acyclic, any edge inH′[W ∪X ∪ Y ] that is not inH′[W] must be directed from either X or Y into a node in the descendant set W. Because all nodes in W are in the complement of V, these new edges must also exist in G, and we conclude H′[W ∪X ∪ Y ] = G[W ∪X ∪ Y ]. To complete the proof, we note that because Π is hereditary, it must hold on H′[W ∪X ∪ Y ]. From Proposition ??, we know H′[W ∪X ∪ Y ] ≈ C′[W ∪X ∪ Y ]), and therefore because Π is equivalence invariant, it holds for C′[W ∪X ∪ Y ]."
    } ],
    "references" : [ {
      "title" : "Learning factor graphs in polynomial time and sample complexity",
      "author" : [ "Pieter Abbeel", "Daphne Koller", "Andrew Y. Ng" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "A characterization of Markov equivalence classes for acyclic digraphs",
      "author" : [ "Steen A. Andersson", "David Madigan", "Michael D. Perlman" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "An introduction to chordal graphs and clique trees",
      "author" : [ "Jean R.S. Blair", "Barry W. Peyton" ],
      "venue" : "In Graph Theory and Sparse Matrix Computations,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1993
    }, {
      "title" : "A transformational characterization of Bayesian network structures",
      "author" : [ "David Maxwell Chickering" ],
      "venue" : "Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "Learning Bayesian networks is NP-Complete",
      "author" : [ "David Maxwell Chickering" ],
      "venue" : "Learning from Data: Artificial Intelligence and Statistics V,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1996
    }, {
      "title" : "Optimal structure identification with greedy search",
      "author" : [ "David Maxwell Chickering" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "Finding optimal Bayesian networks",
      "author" : [ "David Maxwell Chickering", "Christopher Meek" ],
      "venue" : "Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "Selective greedy equivalence search: Finding optimal Bayesian networks using a polynomial number of score evaluations",
      "author" : [ "David Maxwell Chickering", "Christopher Meek" ],
      "venue" : "In Proceedings of the Thirty First Conference on Uncertainty in Artificial Intelligence, Amsterdam, Netherlands,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Selective greedy equivalence search: Finding optimal Bayesian networks using a polynomial number of score evaluations",
      "author" : [ "David Maxwell Chickering", "Christopher Meek" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Large-sample learning of Bayesian networks is NP-hard",
      "author" : [ "David Maxwell Chickering", "Christopher Meek", "David Heckerman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Approximating discrete probability distributions with dependence trees",
      "author" : [ "C. Chow", "C. Liu" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1968
    }, {
      "title" : "Learning polytrees",
      "author" : [ "S. Dasgupta" ],
      "venue" : "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Optimum branching",
      "author" : [ "J. Edmonds" ],
      "venue" : "J. Res. NBS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1967
    }, {
      "title" : "Learning bayesian network structure from massive datasets: The “sparse candidate",
      "author" : [ "Nir Friedman", "Iftach Nachman", "Dana Peer" ],
      "venue" : "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1999
    }, {
      "title" : "On finding optimal polytrees",
      "author" : [ "Serge Gaspers", "Mikko Koivisto", "Mathieu Liedloff", "Sebastian Ordyniak", "Stefan Szeider" ],
      "venue" : "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence. AAAI Press,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Learning causal trees from dependence information",
      "author" : [ "Dan Geiger", "Azaria Paz", "Judea Pearl" ],
      "venue" : "In Proceedings of the Eighth National Conference on Artificial Intelligence - Volume 2,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1990
    }, {
      "title" : "Enumerating Markov equivalence classes of acyclic digraph models",
      "author" : [ "Steven B. Gillispie", "Michael D. Perlman" ],
      "venue" : "Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2001
    }, {
      "title" : "Estimating high-dimensional directed acyclic graphs with the pc algorithm",
      "author" : [ "Markus Kalisch", "Peter Buhlmann" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Learning Markov networks: Maximum bounded tree-width graphs",
      "author" : [ "David Karger", "Nathan Srebro" ],
      "venue" : "In 12th ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2001
    }, {
      "title" : "Exact bayesian structure discovery in bayesian networks",
      "author" : [ "Mikko Koivisto", "Kismat Sood" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Optimal search on clustered structural constraint for learning Bayesian network structure",
      "author" : [ "Kaname Kojima", "Eric Perrier", "Seiya Imoto", "Satoru Miyano" ],
      "venue" : "Journal of Machine Learning Resarch,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Finding a path is harder than finding a tree",
      "author" : [ "Christopher Meek" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2001
    }, {
      "title" : "Pac-learning bounded tree-width graphical models",
      "author" : [ "Mukund Narasimhan", "Jeff Bilmes" ],
      "venue" : "In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2004
    }, {
      "title" : "Parameterized complexity results for exact bayesian network structure learning",
      "author" : [ "Sebastian Ordyniak", "Stefan Szeider" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1988
    }, {
      "title" : "Learning thin junction trees via graph cuts",
      "author" : [ "Dafna Shahaf", "Anton Chechetka", "Carlos Guestrin" ],
      "venue" : "Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "A simple approach for finding the globally optimal bayesian network structure",
      "author" : [ "Tomi Silander", "Petri Myllymäki" ],
      "venue" : "In Proceedings of the Twenty Second Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2006
    }, {
      "title" : "Causation, Prediction, and Search",
      "author" : [ "Peter Spirtes", "Clark Glymour", "Richard Scheines" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1993
    }, {
      "title" : "The max-min hill-climbing Bayesian network structure learning algorithm",
      "author" : [ "Ioannis Tsamardinos", "Laura E. Brown", "Constantin F. Aliferis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2006
    }, {
      "title" : "Equivalence and synthesis of causal models",
      "author" : [ "Thomas Verma", "Judea Pearl" ],
      "venue" : "Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1991
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We introduce Selective Greedy Equivalence Search (SGES), a restricted version of Greedy Equivalence Search (GES). SGES retains the asymptotic correctness of GES but, unlike GES, has polynomial performance guarantees. In particular, we show that when data are sampled independently from a distribution that is perfect with respect to a DAG G defined over the observable variables then, in the limit of large data, SGES will identify G’s equivalence class after a number of score evaluations that is (1) polynomial in the number of nodes and (2) exponential in various complexity measures including maximum-number-of-parents, maximum-cliquesize, and a new measure called v-width that is at least as small as—and potentially much smaller than—the other two. More generally, we show that for any hereditary and equivalenceinvariant property Π known to hold in G, we retain the large-sample optimality guarantees of GES even if we ignore any GES deletion operator during the backward phase that results in a state for which Π does not hold in the commondescendants subgraph.",
    "creator" : "LaTeX with hyperref package"
  }
}