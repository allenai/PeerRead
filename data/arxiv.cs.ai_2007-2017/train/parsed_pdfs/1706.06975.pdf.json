{
  "name" : "1706.06975.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the enumeration of sentences by compactness",
    "authors" : [ "Mark A. Stalzer" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "This work flowed from a comment in the concluding remarks of a recent review (2016) of work in data-driven scientific discovery[1]. Specifically,\n. . . it may be within current computing and algorithmic technology to infer the Maxwell Equations directly from data given knowledge of vector calculus.\nThis short paper reports on recent progress towards this objective in the broader context of combinatorics and there are applications beyond science. The overarching goal is to develop methods that can quickly discover compact theories from data. Most data intensive analysis techniques are based on machine learning or statistics. They are quite useful, but do not lead to deep understanding or insight.\nAbstractly, think of an alphabet A = [A,B,C, . . .] where any letter can appear once in a sentence and the length of the alphabet is n. This is a simple combinatorial enumeration problem and the solution to the number of sets of size m (later m will be relabeled q) taken from A is C(n,m). However, what if the symbols — letters — in the alphabet have different weights? What if the alphabet is more like A = [A = 1, B = 1, C = 4, D = 4, E = 4, F = 4, G = 7, H = 7, I = 7, J = 7,K = 4, L = 7]. This can dramatically decrease the enumeration size as shown in the next section, and the underlying motivation is shown in Sec. 3.\nJulia. The code is written in Julia[2], a relatively recent language (roughly 2012) that is both easy to use and has high performance. Julia can be programmed at a high expressive level, and yet given enough type information it automatically generates efficient machine code. The code is a Julia meta-program that writes candidate theories in Julia that are then validated against data. Ironically, one optimization that is used is to circumvent the Julia compilation step when first\nar X\niv :1\n70 6.\n06 97\n5v 1\n[ cs\n.A I]\n1 5\nJu n\nvalidating a theory: if the theory looks promising it can then be compiled for further, more rapid, validation. The full code with some partial results is shown in the Appendix."
    }, {
      "heading" : "2 Algorithm",
      "text" : "The algorithm enumerates sets of increasing complexity q, where q is the sum of the alphabet letter weights in a given candiate theory. It can be thought of as a form Depth-First Iterative Deepening (DFID)[3] first formalized by R. E. Korf in 1985. Optimality flows from a theorem by Korf:\nTheorem 1 (Korf 4.2) Depth-first iterative-deepening is asymptotically optimal among bruteforce tree searches in terms of time, space, and length of solution.\nBy length of solution, Korf means the depth of the search where a solution is found. For this code compactness is the sum of the symbol weights along a potential solution branch in the search; as will be shown in Sec. 3.\nIt is perhaps easiest to think of the algorithm inductively. There is a data structure theos that holds all theorems (sets) of length q and it is built up from q = 1. The base cases are the singleton theories of a given complexity, so for the alphabet A we have theos[1] = [A, B] and theos[4] = [D, ...] and so on. So the base cases, such as q = 1 are all set; and then for q > 1 we use a q : l,m “Squeeze”. At step q consider all theories that can possibly be of length q, marching l upward from 1 and m downward from q − 1 in a kind of double iteration. The correctness is immediate by Korf 4.2 and the fact that q = l+m, too short theories are discarded (< q), and set elements are unique. The Julia code is in the Appendix.\nPerformance. The timings are in shown in Tab. 1. The total times are Fast = 0.006s and Slow = 20.1s.1 A graph is in Fig. 1 – compactness matters.\n1The machine was a MacBook Pro (Retina, 13-inch, Late 2013) running a 2.8 GHz Intel Core i7 single-threaded. The version of Julia was 0.5. The time varies a bit depending on my laptop’s mood."
    }, {
      "heading" : "3 Motivation",
      "text" : "The underlying motivation was described in the Introduction, namely rediscovering the Maxwell Equations. Here is the decoder ring.\nThe Maxwell Equations, up to constants, are [C], [D], the E,B divergence equations, both of complexity 4; and the field coupling equations of [G,F ] and [H,E], of complexity 11. Also, the fields equations of light are [I,K] and [J, L], each of complexity 14[4]. The complexity metric is just 1+ the number of space-time derivatives taken.\nThere are many avenues for future development as briefly listed below.\n• Validation and constant determination. Fundamental constants such as c were known at the time of Maxwell. However, constants can be automatically determined via linear algebra in this case.\n• Bigger data and parallelism. The data sets needed for the Maxwell Equations re-discovery are small but semantically very rich. Other data sets, such as for macro economics, will be\nfar larger. Here Julia’s on-the-fly compilation (of candidate theories) and support for parallel processing with be very helpful, and this is one of the reasons the language was chosen2.\n• The fully general equations can be re-discovered just by adding a current J and source region ρ to the alphabet A with the appropriate validation step.\nBut, perhaps the most exciting extension is to apply this search algorithm to other domains such as thermodynamics, macro economics, and chaos. Work is progressing in these areas, and focusing on the applicable representation language and semantics.\nThe purpose of this short paper was not to get into the motivating physics, but to look at the combinatorics which is of much broader utility.\nMaterials\nThe Julia code is attached in the Appendix. The code is distributed under a Creative Commons Attribution 4.0 International Public License."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is funded by the Gordon and Betty Moore Foundation through Grant GBMF4915 to the Caltech Center for Data-Driven Discovery. Discussions with Mr. William Xu of Caltech Math/Computer Science were very helpful. The author is grateful to Prof. S.G. Djorgovski of Caltech Astronomy and Prof. V. Chandler of KGI Natural Sciences for their support.\nAppendix\njulia> theos 14-element Array{Array{Set{Char},N},1}:\nSet{Char}[Set([’A’]),Set([’B’])] Set{Char}[Set([’A’,’B’])] Set{Char}[] Set{Char}[Set([’C’]),Set([’D’]),Set([’E’]),Set([’F’])] Set{Char}[Set([’A’,’C’]),Set([’B’,’C’]),Set([’A’,’D’]),Set([’B’,’D’]),Set([’E’,’A’]),\nSet([’E’,’B’]),Set([’A’,’F’]),Set([’B’,’F’])]\nSet{Char}[Set([’A’,’B’,’C’]),Set([’A’,’B’,’D’]),Set([’E’,’A’,’B’]),Set([’A’,’B’,’F’])] Set{Char}[Set([’G’]),Set([’H’]),Set([’I’]),Set([’J’]),Set([’K’]),Set([’L’])]\n# ... up to complexity 14.\n#-*- mode: Julia # TheoSea enumeration algorithm, May 29, 2017 # # Mark A. Stalzer, Caltech, stalzer@caltech.edu # # The code is distributed under a Creative Commons Attribution 4.0\n2The author encourages the Julia developers to continue work on threads as that model is natural for many-core processors. For example, the main thread could enumerate candidate theories and then send them to several worker threads for validation. At any instant several theories would be under consideration.\n# International Public License. If you use this work please attribute # to M. Stalzer, \"On the enumeration of sentences by compactness’’, # arXiv, June 2017. # # This research was funded by the Gordon and Betty Moore Foundation # through Grant GBMF4915 to the Caltech Center for Data-Driven Discovery.\n# Complexity alphabet, a simple one # alph = [(’A’, 1), (’B’, 1), (’C’, 3), (’D’, 4), (’E’, 4)]\n# Contrast these two alphabets with the same number of symbols to # see how much complexity helps in the enumeration time. alph = [(’A’, 1), (’B’, 1), (’C’, 4), (’D’, 4), (’E’, 4),\n(’F’, 4), (’G’, 7), (’H’, 7), (’I’, 7), (’J’, 7), (’K’, 7), (’L’, 7)]\n# #alph = [(’A’, 1), (’B’, 1), (’C’, 1), (’D’, 1), (’E’, 1), # (’F’, 1), (’G’, 1), (’H’, 1), (’I’, 1), (’J’, 1), (’K’, 1), (’L’, 1)] #\n# Maximal possible theory size q with user limit MAX_COMP MAX_COMP = 14 max_q = min(sum([a[2] for a in alph]), MAX_COMP)\n# Seed theos data structure with singleton theories by complexity theos = Array{Array{Set{Char}}}(max_q)\nfunction init_theos()\nfill!(theos, [])\nfor a in alph\nif theos[a[2]] == [] theos[a[2]] = [Set([a[1]])] else push!(theos[a[2]], Set([a[1]])) end\nend\nglobal valids = []\nend\n# Theory validation function for candidate theories function valid(cand_theo)\ntrue\nend\n# March theories of increasing complexity q until max_q function march() for q in 1:max_q\ntic() # q:l,m Squeeze #\n# All singleton theories of size q already in theos as base cases # Generate other theories by combinations of l+m=q squeezing together m = q-1; l = 1 while m >= l\nfor m_el in theos[m]\nfor l_el in theos[l]\nml_el = union(m_el, l_el) if length(ml_el) == length(m_el) + length(l_el) # < q?\nif theos[q] == [] theos[q] = [ml_el] elseif !(ml_el in theos[q]) # Nonredundant\npush!(theos[q], ml_el)\nend\nend\nend\nend m -= 1; l += 1\nend\n# VALIDATE final theos[q] append!(valids,\n[(q, i) for i in 1:length(theos[q]) if valid(theos[q][i])])\nprintln(toq())\nend end\n# init_theos, march once, init_theos, march twice to get timings right init_theos() @time march() init_theos() @time march()"
    } ],
    "references" : [ {
      "title" : "A preliminary review of influential works in data-driven discovery",
      "author" : [ "M. Stalzer", "C. Mentzel" ],
      "venue" : "SpringerPlus, (5)1266,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Julia: A fresh approach to numerical computing",
      "author" : [ "J. Bezanson et. al" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2017
    }, {
      "title" : "Depth-First Iterative-Deepening: An optimal admissible tree search",
      "author" : [ "R.E. Korf" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1985
    }, {
      "title" : "Maxwell’s Equations and the Principles of Electromagnetism",
      "author" : [ "R. Fitzpatrick" ],
      "venue" : "Firewall Media, Sec",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This work flowed from a comment in the concluding remarks of a recent review (2016) of work in data-driven scientific discovery[1].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "The code is written in Julia[2], a relatively recent language (roughly 2012) that is both easy to use and has high performance.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "It can be thought of as a form Depth-First Iterative Deepening (DFID)[3] first formalized by R.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "The base cases are the singleton theories of a given complexity, so for the alphabet A we have theos[1] = [A, B] and theos[4] = [D, .",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "The base cases are the singleton theories of a given complexity, so for the alphabet A we have theos[1] = [A, B] and theos[4] = [D, .",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "Also, the fields equations of light are [I,K] and [J, L], each of complexity 14[4].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "# Maximal possible theory size q with user limit MAX_COMP MAX_COMP = 14 max_q = min(sum([a[2] for a in alph]), MAX_COMP)",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "for a in alph if theos[a[2]] == [] theos[a[2]] = [Set([a[1]])] else push!(theos[a[2]], Set([a[1]])) end end",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "for a in alph if theos[a[2]] == [] theos[a[2]] = [Set([a[1]])] else push!(theos[a[2]], Set([a[1]])) end end",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "for a in alph if theos[a[2]] == [] theos[a[2]] = [Set([a[1]])] else push!(theos[a[2]], Set([a[1]])) end end",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "for a in alph if theos[a[2]] == [] theos[a[2]] = [Set([a[1]])] else push!(theos[a[2]], Set([a[1]])) end end",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "for a in alph if theos[a[2]] == [] theos[a[2]] = [Set([a[1]])] else push!(theos[a[2]], Set([a[1]])) end end",
      "startOffset" : 93,
      "endOffset" : 96
    } ],
    "year" : 2017,
    "abstractText" : "Presented is a Julia meta-program that discovers compact theories from data if they exist. It writes candidate theories in Julia and then validates: tossing the bad theories and keeping the good theories. Compactness is measured by a metric: such as the number of space-time derivatives. The underlying algorithm is applicable to a wide variety of combinatorics problems and compactness serves to cut down the search space.",
    "creator" : "LaTeX with hyperref package"
  }
}