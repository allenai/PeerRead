{
  "name" : "1110.3741.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "coolmark@umich.edu", "xukevin@umich.edu", "jcalder@umich.edu", "hero@umich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n11 0.\n37 41\nv3 [\ncs .L\nG ]\n7 J\nan 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Anomaly detection is an important problem that has been studied in a variety of areas and used in diverse applications including intrusion detection, fraud detection, and image processing [1, 2]. Many methods for anomaly detection have been developed using both parametric and non-parametric approaches. Non-parametric approaches typically involve the calculation of dissimilarities between data samples. For complex high-dimensional data, multiple dissimilarity measures corresponding to different criteria may be required to detect certain types of anomalies. For example, consider the problem of detecting anomalous object trajectories in video sequences. Multiple criteria, such as dissimilarity in object speeds or trajectory shapes, can be used to detect a greater range of anomalies than any single criterion. In order to perform anomaly detection using these multiple criteria, one could first combine the dissimilarities using a linear combination. However, in many applications, the importance of the criteria are not known in advance. It is difficult to determine how much weight to assign to each dissimilarity measure, so one may have to choose multiple weights using, for example, a grid search. Furthermore, when the weights are changed, the anomaly detection algorithm needs to be re-executed using the new weights.\nIn this paper we propose a novel non-parametric multi-criteria anomaly detection approach using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies without having to choose weights for different criteria. Pareto optimality is the typical method for defining optimality when there may be multiple conflicting criteria for comparing items. An item is said to be Pareto-optimal if there does not exist another item that is better or equal in all of the criteria. An item that is Pareto-optimal is optimal in the usual sense under some combination, not necessarily linear, of the criteria. Hence, PDA is able to detect anomalies under multiple combinations of the criteria without explicitly forming these combinations.\nThe PDA approach involves creating dyads corresponding to dissimilarities between pairs of data samples under all of the dissimilarity measures. Sets of Pareto-optimal dyads, called Pareto fronts, are then computed. The first Pareto front (depth one) is the set of non-dominated dyads. The second Pareto front (depth two) is obtained by removing these non-dominated dyads, i.e. peeling off the first front, and recomputing the first Pareto front of those remaining. This process continues until no dyads remain. In this way, each dyad is assigned to a Pareto front at some depth (see Figure 1 for illustration). Nominal and anomalous samples are located near different Pareto front depths; thus computing the front depths of the dyads corresponding to a test sample can discriminate between nominal and anomalous samples. The proposed PDA approach scales linearly in the number of criteria, which is a significant improvement compared to selecting multiple weights via a grid search, which scales exponentially in the number of criteria. Under assumptions that the multi-criteria dyads can be modeled as a realizations from a smooth K-dimensional density we provide a mathematical analysis of the behavior of the first Pareto front. This analysis shows in a precise sense that PDA can outperform a test that uses a linear combination of the criteria. Furthermore, this theoretical prediction is experimentally validated by comparing PDA to several state-of-the-art anomaly detection algorithms in two experiments involving both synthetic and real data sets.\nThe rest of this paper is organized as follows. We discuss related work in Section 2. In Section 3 we provide an introduction to Pareto fronts and present a theoretical analysis of the properties of the first Pareto front. Section 4 relates Pareto fronts to the multi-criteria anomaly detection problem, which leads to the PDA anomaly detection algorithm. Finally we present two experiments in Section 5 to evaluate the performance of PDA."
    }, {
      "heading" : "2 Related work",
      "text" : "Several machine learning methods utilizing Pareto optimality have previously been proposed; an overview can be found in [3]. These methods typically formulate machine learning problems as multi-objective optimization problems where finding even the first Pareto front is quite difficult. These methods differ from our use of Pareto optimality because we consider multiple Pareto fronts created from a finite set of items, so we do not need to employ sophisticated methods in order to find these fronts.\nHero and Fleury [4] introduced a method for gene ranking using Pareto fronts that is related to our approach. The method ranks genes, in order of interest to a biologist, by creating Pareto fronts of the data samples, i.e. the genes. In this paper, we consider Pareto fronts of dyads, which correspond to dissimilarities between pairs of data samples rather than the samples themselves, and use the distribution of dyads in Pareto fronts to perform multi-criteria anomaly detection rather than ranking.\nAnother related area is multi-view learning [5, 6], which involves learning from data represented by multiple sets of features, commonly referred to as “views”. In such case, training in one view helps to\nimprove learning in another view. The problem of view disagreement, where samples take different classes in different views, has recently been investigated [7]. The views are similar to criteria in our problem setting. However, in our setting, different criteria may be orthogonal and could even give contradictory information; hence there may be severe view disagreement. Thus training in one view could actually worsen performance in another view, so the problem we consider differs from multi-view learning. A similar area is that of multiple kernel learning [8], which is typically applied to supervised learning problems, unlike the unsupervised anomaly detection setting we consider.\nFinally, many other anomaly detection methods have previously been proposed. Hodge and Austin [1] and Chandola et al. [2] both provide extensive surveys of different anomaly detection methods and applications. Nearest neighbor-based methods are closely related to the proposed PDA approach. Byers and Raftery [9] proposed to use the distance between a sample and its kth-nearest neighbor as the anomaly score for the sample; similarly, Angiulli and Pizzuti [10] and Eskin et al. [11] proposed to the use the sum of the distances between a sample and its k nearest neighbors. Breunig et al. [12] used an anomaly score based on the local density of the k nearest neighbors of a sample. Hero [13] and Sricharan and Hero [14] introduced non-parametric adaptive anomaly detection methods using geometric entropy minimization, based on random k-point minimal spanning trees and bipartite k-nearest neighbor (k-NN) graphs, respectively. Zhao and Saligrama [15] proposed an anomaly detection algorithm k-LPE using local p-value estimation (LPE) based on a k-NN graph. These k-NN anomaly detection schemes only depend on the data through the pairs of data points (dyads) that define the edges in the k-NN graphs.\nAll of the aforementioned methods are designed for single-criteria anomaly detection. In the multicriteria setting, the single-criteria algorithms must be executed multiple times with different weights, unlike the PDA anomaly detection algorithm that we propose in Section 4."
    }, {
      "heading" : "3 Pareto depth analysis",
      "text" : "The PDA method proposed in this paper utilizes the notion of Pareto optimality, which has been studied in many application areas in economics, computer science, and the social sciences among others [16]. We introduce Pareto optimality and define the notion of a Pareto front.\nConsider the following problem: given n items, denoted by the set S, and K criteria for evaluating each item, denoted by functions f1, . . . , fK , select x ∈ S that minimizes [f1(x), . . . , fK(x)]. In most settings, it is not possible to identify a single item x that simultaneously minimizes fi(x) for all i ∈ {1, . . . ,K}. A minimizer can be found by combining the K criteria using a linear combination of the fi’s and finding the minimum of the combination. Different choices of (nonnegative) weights in the linear combination could result in different minimizers; a set of items that are minimizers under some linear combination can then be created by using a grid search over the weights, for example.\nA more powerful approach involves finding the set of Pareto-optimal items. An item x is said to strictly dominate another item x∗ if x is no greater than x∗ in each criterion and x is less than x∗ in at least one criterion. This relation can be written as x ≻ x∗ if fi(x) ≤ fi(x∗) for each i and fi(x) < fi(x∗) for some i. The set of Pareto-optimal items, called the Pareto front, is the set of items in S that are not strictly dominated by another item in S. It contains all of the minimizers that are found using linear combinations, but also includes other items that cannot be found by linear combinations. Denote the Pareto front by F1, which we call the first Pareto front. The second Pareto front can be constructed by finding items that are not strictly dominated by any of the remaining items, which are members of the set S \\ F1. More generally, define the ith Pareto front by\nFi = Pareto front of the set S \\\n\n\ni−1 ⋃\nj=1\nFj\n\n .\nFor convenience, we say that a Pareto front Fi is deeper than Fj if i > j."
    }, {
      "heading" : "3.1 Mathematical properties of Pareto fronts",
      "text" : "The distribution of the number of points on the first Pareto front was first studied by BarndorffNielsen and Sobel in their seminal work [17]. The problem has garnered much attention since; for a\nsurvey of recent results see [18]. We will be concerned here with properties of the first Pareto front that are relevant to the PDA anomaly detection algorithm and thus have not yet been considered in the literature. Let Y1, . . . , Yn be independent and identically distributed (i.i.d.) on Rd with density function f : Rd → R. For a measurable set A ⊂ Rd, we denote by FA the points on the first Pareto front of Y1, . . . , Yn that belong to A. For simplicity, we will denote F1 by F and use |F| for the cardinality of F . In the general Pareto framework, the points Y1, . . . , Yn are the images in Rd of n feasible solutions to some optimization problem under a vector of objective functions of length d. In the context of this paper, each point Yl corresponds to a dyad Dij , which we define in Section 4, and d = K is the number of criteria. A common approach in multi-objective optimization is linear scalarization [16], which constructs a new single criterion as a convex combination of the d criteria. It is well-known, and easy to see, that linear scalarization will only identify Pareto points on the boundary of the convex hull of ⋃\nx∈F(x + R d +), where R d + = {x ∈ Rd |xi ≥ 0, i = 1 . . . , d}.\nAlthough this is a common motivation for Pareto methods, there are, to the best of our knowledge, no results in the literature regarding how many points on the Pareto front are missed by scalarization. We present such a result here. We define\nL = ⋃\nα∈Rd +\nargmin x∈Sn\n{\nd ∑\ni=1\nαixi\n}\n, Sn = {Y1, . . . , Yn}.\nThe subset L ⊂ F contains all Pareto-optimal points that can be obtained by some selection of weights for linear scalarization. We aim to study how largeL can get, compared to F , in expectation. In the context of this paper, if some Pareto-optimal points are not identified, then the anomaly score (defined in section 4.2) will be artificially inflated, making it more likely that a non-anomalous sample will be rejected. Hence the size of F \\ L is a measure of how much the anomaly score is inflated and the degree to which Pareto methods will outperform linear scalarization.\nPareto points in F \\ L are a result of non-convexities in the Pareto front. We study two kinds of non-convexities: those induced by the geometry of the domain of Y1, . . . , Yn, and those induced by randomness. We first consider the geometry of the domain. Let Ω ⊂ Rd be bounded and open with a smooth boundary ∂Ω and suppose the density f vanishes outside of Ω. For a point z ∈ ∂Ω we denote by ν(z) = (ν1(z), . . . , νd(z)) the unit inward normal to ∂Ω. For T ⊂ ∂Ω, define Th ⊂ Ω by Th = {z + tν | z ∈ T, 0 < t ≤ h}. Given h > 0 it is not hard to see that all Pareto-optimal points will almost surely lie in ∂Ωh for large enough n, provided the density f is strictly positive on ∂Ωh. Hence it is enough to study the asymptotics for E|FTh | for T ⊂ ∂Ω and h > 0. Theorem 1. Let f ∈ C1(Ω) with infΩ f > 0. Let T ⊂ ∂Ω be open and connected such that\ninf z∈T\nmin(ν1(z), . . . , νd(z)) ≥ δ > 0, and {y ∈ Ω : y x} = {x}, for x ∈ T.\nThen for h > 0 sufficiently small, we have\nE|FTh | = γn d−1 d + δ−d−1O\n(\nn d−2 d\n)\nas n → ∞,\nwhere γ = d−1(d!) 1 dΓ(d−1)\n∫\nT\nf(z) d−1 d (ν1(z) · · · νd(z)) 1 d dz.\nThe proof of Theorem 1 is postponed to Appendix A. Theorem 1 shows asymptotically how many Pareto points are contributed on average by the segment T ⊂ ∂Ω. The number of points contributed depends only on the geometry of ∂Ω through the direction of its normal vector ν and is otherwise independent of the convexity of ∂Ω. Hence, by using Pareto methods, we will identify significantly more Pareto-optimal points than linear scalarization when the geometry of ∂Ω includes non-convex regions. For example, if T ⊂ ∂Ω is non-convex (see left panel of Figure 2) and satisfies the hypotheses of Theorem 1, then for large enough n, all Pareto points in a neighborhood of T will be unattainable by scalarization. Quantitatively, if f ≥ C on T , then E|F \\ L| ≥ γn d−1d + δ−d−1O(n d−2d ), as n → ∞, where γ ≥ d−1(d!) 1dΓ(d−1)|T |δC d−1d and |T | is the d − 1 dimensional Hausdorff measure of T . It has recently come to our attention that Theorem 1 appears in a more general form in an unpublished manuscript of Baryshnikov and Yukich [19].\nWe now study non-convexities in the Pareto front which occur due to inherent randomness in the samples. We show that, even in the case where Ω is convex, there are still numerous small-scale non-convexities in the Pareto front that can only be detected by Pareto methods. We illustrate this in the case of the Pareto box problem for d = 2.\nTheorem 2. Let Y1, . . . , Yn be independent and uniformly distributed on [0, 1]2. Then\n1 2 lnn+O(1) ≤ E|L| ≤ 5 6 lnn+O(1), as n → ∞.\nThe proof of Theorem 2 is also postponed to Appendix A. A proof that E|F| = lnn + O(1) as n → ∞ can be found in [17]. Hence Theorem 2 shows that, asymptotically and in expectation, only between 12 and 5 6 of the Pareto-optimal points can be obtained by linear scalarization in the Pareto box problem. Experimentally, we have observed that the true fraction of points is close to 0.7. This means that at least 16 (and likely more) of the Pareto points can only be obtained via Pareto methods even when Ω is convex. Figure 2 gives an example of the sets F and L from the two theorems."
    }, {
      "heading" : "4 Multi-criteria anomaly detection",
      "text" : "Assume that a training set XN = {X1, . . . , XN} of nominal data samples is available. Given a test sample X , the objective of anomaly detection is to declare X to be an anomaly if X is significantly different from samples in XN . Suppose that K > 1 different evaluation criteria are given. Each criterion is associated with a measure for computing dissimilarities. Denote the dissimilarity between Xi and Xj computed using the measure corresponding to the lth criterion by dl(i, j).\nWe define a dyad by Dij = [d1(i, j), . . . , dK(i, j)]T ∈ RK+ , i ∈ {1, . . . , N}, j ∈ {1, . . . , N} \\ i. Each dyad Dij corresponds to a connection between samples Xi and Xj . Therefore, there are in total (\nN 2\n)\ndifferent dyads. For convenience, denote the set of all dyads by D and the space of all dyads RK+ by D. By the definition of strict dominance in Section 3, a dyad Dij strictly dominates another dyad Di∗j∗ if dl(i, j) ≤ dl(i∗, j∗) for all l ∈ {1, . . . ,K} and dl(i, j) < dl(i∗, j∗) for some l. The first Pareto front F1 corresponds to the set of dyads from D that are not strictly dominated by any other dyads from D. The second Pareto front F2 corresponds to the set of dyads from D \\ F1 that are not strictly dominated by any other dyads from D \\ F1, and so on, as defined in Section 3. Recall that we refer to Fi as a deeper front than Fj if i > j."
    }, {
      "heading" : "4.1 Pareto fronts of dyads",
      "text" : "For each sample Xn, there are N − 1 dyads corresponding to its connections with the other N − 1 samples. Define the set of N − 1 dyads associated with Xn by Dn. If most dyads in Dn are located at shallow Pareto fronts, then the dissimilarities between Xn and the other N − 1 samples are small under some combination of the criteria. Thus, Xn is likely to be a nominal sample. This is the basic idea of the proposed multi-criteria anomaly detection method using PDA.\nWe construct Pareto fronts F1, . . . ,FM of the dyads from the training set, where the total number of fronts M is the required number of fronts such that each dyad is a member of a front. When a test sample X is obtained, we create new dyads corresponding to connections between X and training samples, as illustrated in Figure 1. Similar to many other anomaly detection methods, we connect each test sample to its k nearest neighbors. k could be different for each criterion, so we denote ki as the choice of k for criterion i. We create s =\n∑K i=1 ki new dyads, which we denote by the set\nAlgorithm 1 PDA anomaly detection algorithm. Training phase:\n1: for l = 1 → K do 2: Calculate pairwise dissimilarities dl(i, j) between all training samples Xi and Xj 3: Create dyads Dij = [d1(i, j), . . . , dK(i, j)] for all training samples 4: Construct Pareto fronts on set of all dyads until each dyad is in a front\nTesting phase: 1: nb ← [ ] {empty list} 2: for l = 1 → K do 3: Calculate dissimilarities between test sample X and all training samples in criterion l 4: nbl ← kl nearest neighbors of X 5: nb ← [nb, nbl] {append neighbors to list} 6: Create s new dyads Dnewi between X and training samples in nb 7: for i = 1 → s do 8: Calculate depth ei of Dnewi 9: Declare X an anomaly if v(X) = (1/s)\n∑s i=1 ei > σ\nDnew = {Dnew1 , Dnew2 , . . . , Dnews }, corresponding to the connections between X and the union of the ki nearest neighbors in each criterion i. In other words, we create a dyad between X and Xj if Xj is among the ki nearest neighbors1 of X in any criterion i. We say that Dnewi is below a front Fl if Dnewi ≻ Dl for some Dl ∈ Fl, i.e. Dnewi strictly dominates at least a single dyad in Fl. Define the depth of Dnewi by\nei = min{l |Dnewi is below Fl}.\nTherefore if ei is large, then Dnewi will be near deep fronts, and the distance between X and the corresponding training sample is large under all combinations of the K criteria. If ei is small, then Dnewi will be near shallow fronts, so the distance between X and the corresponding training sample is small under some combination of the K criteria."
    }, {
      "heading" : "4.2 Anomaly detection using depths of dyads",
      "text" : "In k-NN based anomaly detection algorithms such as those mentioned in Section 2, the anomaly score is a function of the k nearest neighbors to a test sample. With multiple criteria, one could define an anomaly score by scalarization. From the probabilistic properties of Pareto fronts discussed in Section 3.1, we know that Pareto methods identify more Pareto-optimal points than linear scalarization methods and significantly more Pareto-optimal points than a single weight for scalarization2.\nThis motivates us to develop a multi-criteria anomaly score using Pareto fronts. We start with the observation from Figure 1 that dyads corresponding to a nominal test sample are typically located near shallower fronts than dyads corresponding to an anomalous test sample. Each test sample is associated with s new dyads, where the ith dyad Dnewi has depth ei. For each test sample X , we define the anomaly score v(X) to be the mean of the ei’s, which corresponds to the average depth of the s dyads associated with X . Thus the anomaly score can be easily computed and compared to the decision threshold σ using the test\nv(X) = 1\ns\ns ∑\ni=1\nei H1 ≷ H0 σ.\nPseudocode for the PDA anomaly detector is shown in Algorithm 1. In Appendix C we provide details of the implementation as well as an analysis of the time complexity and a heuristic for choosing the ki’s that performs well in practice. Both the training time and the time required to test a new\n1If a training sample is one of the ki nearest neighbors in multiple criteria, then multiple copies of the dyad corresponding to the connection between the test sample and the training sample are created.\n2Theorems 1 and 2 require i.i.d. samples, but dyads are not independent. However, there are O(N2) dyads, and each dyad is only dependent on O(N) other dyads. This suggests that the theorems should also hold for the non-i.i.d. dyads as well, and it is supported by experimental results presented in Appendix B.\nsample using PDA are linear in the number of criteria K . To handle multiple criteria, other anomaly detection methods, such as the ones mentioned in Section 2, need to be re-executed multiple times using different (non-negative) linear combinations of the K criteria. If a grid search is used for selection of the weights in the linear combination, then the required computation time would be exponential in K . Such an approach presents a computational problem unless K is very small. Since PDA scales linearly with K , it does not encounter this problem."
    }, {
      "heading" : "5 Experiments",
      "text" : "We compare the PDA method with four other nearest neighbor-based single-criterion anomaly detection algorithms mentioned in Section 2. For these methods, we use linear combinations of the criteria with different weights selected by grid search to compare performance with PDA."
    }, {
      "heading" : "5.1 Simulated data with four criteria",
      "text" : "First we present an experiment on a simulated data set. The nominal distribution is given by the uniform distribution on the hypercube [0, 1]4. The anomalous samples are located just outside of this hypercube. There are four classes of anomalous distributions. Each class differs from the nominal distribution in one of the four dimensions; the distribution in the anomalous dimension is uniform on [1, 1.1]. We draw 300 training samples from the nominal distribution followed by 100 test samples from a mixture of the nominal and anomalous distributions with a 0.05 probability of selecting any particular anomalous distribution. The four criteria for this experiment correspond to the squared differences in each dimension. If the criteria are combined using linear combinations, the combined dissimilarity measure reduces to weighted squared Euclidean distance.\nThe different methods are evaluated using the receiver operating characteristic (ROC) curve and the area under the curve (AUC). The mean AUCs (with standard errors) over 100 simulation runs are shown in Table 1(a). A grid of six points between 0 and 1 in each criterion, corresponding to 64 = 1296 different sets of weights, is used to select linear combinations for the single-criterion methods. Note that PDA is the best performer, outperforming even the best linear combination."
    }, {
      "heading" : "5.2 Pedestrian trajectories",
      "text" : "We now present an experiment on a real data set that contains thousands of pedestrians’ trajectories in an open area monitored by a video camera [20]. Each trajectory is approximated by a cubic spline curve with seven control points [21]. We represent a trajectory with l time samples by\nT =\n[\nx1 x2 . . . xl y1 y2 . . . yl\n]\n,\nwhere [xt, yt] denote a pedestrian’s position at time step t.\nWe use two criteria for computing the dissimilarity between trajectories. The first criterion is to compute the dissimilarity in walking speed. We compute the instantaneous speed at all time steps along each trajectory by finite differencing, i.e. the speed of trajectory T at time step t is given by √\n(xt − xt−1)2 + (yt − yt−1)2. A histogram of speeds for each trajectory is obtained in this manner. We take the dissimilarity between two trajectories to be the squared Euclidean distance between their speed histograms. The second criterion is to compute the dissimilarity in shape. For each trajectory, we select 100 points, uniformly positioned along the trajectory. The dissimilarity between two trajectories T and T ′ is then given by the sum of squared Euclidean distances between the positions of T and T ′ over all 100 points.\nThe training sample for this experiment consists of 500 trajectories, and the test sample consists of 200 trajectories. Table 1(b) shows the performance of PDA as compared to the other algorithms using 100 uniformly spaced weights for linear combinations. Notice that PDA has higher AUC than the other methods under all choices of weights for the two criteria. For a more detailed comparison, the ROC curve for PDA and the attainable region for k-LPE (the region between the ROC curves corresponding to weights resulting in the best and worst AUCs) is shown in Figure 3 along with the first 100 Pareto fronts for PDA. k-LPE performs slightly better at low false positive rate when the best weights are used, but PDA performs better in all other situations, resulting in higher AUC. Additional discussion on this experiment can be found in Appendix D."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we proposed a new multi-criteria anomaly detection method. The proposed method uses Pareto depth analysis to compute the anomaly score of a test sample by examining the Pareto front depths of dyads corresponding to the test sample. Dyads corresponding to an anomalous sample tended to be located at deeper fronts compared to dyads corresponding to a nominal sample. Instead of choosing a specific weighting or performing a grid search on the weights for different dissimilarity measures, the proposed method can efficiently detect anomalies in a manner that scales linearly in the number of criteria. We also provided a theorem establishing that the Pareto approach is asymptotically better than using linear combinations of criteria. Numerical studies validated our theoretical predictions of PDA’s performance advantages on simulated and real data."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Zhaoshi Meng for his assistance in labeling the pedestrian trajectories. We also thank Daniel DeWoskin for suggesting a fast algorithm for computing Pareto fronts in two criteria. This work was supported in part by ARO grant W911NF-09-1-0310."
    }, {
      "heading" : "Appendix A Proofs of Theorems 1 and 2",
      "text" : "Before presenting the proofs of Theorems 1 and 2 we need a preliminary result.\nLemma 1. For any n ≥ 1 and A ⊂ Rd measurable, we have\nE|FA| = n ∫\nA\nf(x)\n(\n1− ∫\ny x\nf(y)dy\n)n−1\ndx. (1)\nProof. Since Y1, . . . , Yn are i.i.d, we have E|FA| = nP (Y1 ∈ F). Conditioning on Y1 we obtain E|FA| = n ∫ Rd f(x)P (Y1 ∈ F |Y1 = x)dx. The proof is completed by noting that\nP (Y1 ∈ F |Y1 = x) = {( 1− ∫ y x f(y)dy )n−1\n, x ∈ A, 0, x 6∈ A.\nProof of Theorem 1. By selecting h > 0 smaller, if necessary, we can write (1) as\nE|FTh | = ∫\nT\n∫ h\n0\nnf(x)\n(\n1− ∫\ny x\nfdy\n)n−1\n(1 +O(t))dtdz, (2)\nwhere x = z+ tν(z) for z ∈ T . Since ∂Ω is smooth, we can approximate T near z by a hyperplane with normal ν(z). By the assumption that {y ∈ Ω : y x} = {x} we can make h > 0 smaller, if neceessary, so that {y ∈ Ω : y x} is approximately a simplex with side lengths t/νi(z). Hence\n∫\ny x\nf(y)dy = (f(z) +O(t/δ))\n∫\ny x\ndy\n= f(z)td\nd!ν1(z) · · · νd(z) +O\n(\ntd+1\nδd+1\n)\n.\nSubstituting this into (2), we have\nE|FTh | = ∫\nT\n∫ h\n0\nn(f(z) +O(t))\n(\n1− f(z)t d\nd!ν1(z) · · · νd(z) +O(td+1/δd+1)\n)n−1\ndtdz. (3)\nWe can now do an asymptotic analysis of the inner integral which is a special case of the general equation\nAn :=\n∫ h\n0\ntλ(1− atd +O(btd+1))n−1dt, λ ∈ [0, 1], a, b > 0.\nMaking the change of variables −s = (n− 1) ln(1− atd +O(btd+1)) and simplifying, we obtain\nAn = 1\n(a(n− 1)) 1+λd\n∫ P (n−1)\n0\n(\n1 d s 1+λ d −1 + b (n− 1) 1d O(s 2+λ d −1)\n)\ne−sds,\nwhere P = − ln(1− ahd + bO(hd+1)).\nWe can, of course, choose h small enough so that P is finite and positive. Recalling the definition of the Gamma function, Γ(z) = ∫∞\n0 tz−1e−tdt, we see that\nAn = Γ ( 1+λ d )\nd(an) 1+λ d\n+O\n(\nb\nn 2+λ d\n)\n.\nNote that we are keeping track of O(b) terms because b = O(1/δd+1) may become large at different points of T , whereas O(1/a) is uniformly bounded independent of δ along T . Applying this to (3) with\na = f(z)\nd!ν1(z) · · · νd(z) , and b = δ−(d+1),\ncompletes the proof.\nProof of Theorem 2. Since Y1, . . . , Yn are i.i.d., we have E|L| = nP (Y1 ∈ L). For (x, y) ∈ [0, 1]2 let Dx,y be the event that Y1 = (x, y) and (x, y) ∈ F . Conditioning on Dx,y we have\nE|L| = n ∫ 1\n0\n∫ 1\n0\n(1− xy)n−1P ((x, y) ∈ L |Dx,y) dxdy\n= n\n∫ 1 2\n0\n∫ 1 2\n0\n(1− xy)n−1P ((x, y) ∈ L |Dx,y) dxdy +O(1). (4)\nDefine A = {\n(u, v) ∈ [0, 1]2 | 0 < u < x, y < v < 2y − uy x } ,\nand B = {\n(u, v) ∈ [0, 1]2 | x < u < 1, 0 < v < 2y − uy x } .\nLet E be the event that A and B each contain at least one sample from Y2, . . . , Yn. If E occurs, then (x, y) is in the interior of the convex hull of F and hence (x, y) 6∈ L. Let F denote the event that none of the samples from Y2, . . . , Yn fall in A ∪B. If F occurs, then we clearly have (x, y) ∈ L. It follows that P (F |Dx,y) ≤ P ((x, y) ∈ L |Dx,y) ≤ P (Ec |Dx,y). Conditioned on Dx,y, the samples Y2, . . . , Yn remain independent. The conditional density function of each remaining sample is fYi |Dx,y (u, v) = 1 1−xy . Let EA (resp. EB) denote the event that no samples from Y2, . . . , Yn are drawn from A (resp. B). Then Ec = EA ∪ EB and F = EA ∩ EB . Noting that |A| = |B| = 12xy, we see that\nP (Ec |Dx,y) = P (EA |Dx,y) + P (EB |Dx,y)− P (EA ∩EB |Dx,y)\n= 2\n(\n1− xy 2(1− xy)\n)n−1\n− ( 1− xy 1− xy\n)n−1\n,\nand\nP (F |Dx,y) = P (EA ∩ EB |Dx,y) = ( 1− xy 1− xy\n)n−1\n.\nSubstituting this into (4), we obtain\nE|L| ≤ n ∫ 1 2\n0\n∫ 1 2\n0\n2\n(\n1− 3 2 xy\n)n−1\n− (1− 2xy)n−1 dxdy,\nand\nE|L| ≥ n ∫ 1 2\n0\n∫ 1 2\n0\n(1− 2xy)n−1 dxdy.\nA short calculation (change variables to u = anxy and v = x) shows that ∫ 1\n2\n0\n∫ 1 2\n0\nn(1− axy)n−1dxdy = 1 a lnn+O(1).\nApplying this result to the bounds above completes the proof."
    }, {
      "heading" : "Appendix B Experimental support for Theorems 1 and 2",
      "text" : "Independence of Y1, . . . , Yn is built into the assumptions of Theorems 1 and 2, but it is clear that dyads (as constructed in Section 4) are not independent. Each dyad Di,j represents a connection between two independent samples Xi and Xj . For a given dyad Di,j , there are 2(N − 2) corresponding dyads involving Xi or Xj and these are clearly not independent from Di,j . However, all other dyads are independent from Di,j . So while there are O(N2) dyads, each dyad is independent from all other dyads except for a set of size O(N). Since Theorems 1 and 2 deal with asymptotic results, this suggests they should hold for the dyads even though they are not i.i.d. In this section we present some experimental results that support this non-rigorous statement.\nWe first drew samples uniformly in [0, 1]2 and computed the dyads corresponding to the two criteria |∆x| and |∆y|, which denote the absolute differences between the x and y coordinates, respectively.\nThe domain of the resulting dyads is again the box [0, 1]2, as shown in Figure 4(a), so this experiment tests Theorem 2. In this case, Theorem 2 suggests that F \\ L should grow logarithmically. Figure 5(a) shows the sample means versus number of dyads and a best fit logarithmic curve of the form y = α lnn, where n = (\nN 2\n)\ndenotes the number of dyads. A linear regression on y/ lnn versus lnn gave α = 0.3142 which falls in the range specified by Theorem 2.\nWe next looked to find criteria that induce domains other than boxes in order to test Theorem 1. A somewhat contrived example involves the criteria |∆x| + |∆y| and |∆x| − |∆y|, which, when applied to uniformly sampled data on [0, 1]2, yields dyads sampled on a diamond domain, as shown in Figure 4(b). In this case, Theorem 1 suggests that F\\L should grow as √n. Figure 5(b) shows the sample means versus number of dyads and a best fit curve of the form y = αnβ . A linear regression on ln y versus lnn gave α = 1.1642 and β = 0.5007. Although this example may not be practical, it is simply meant to illustrate the applicability of Theorem 1 for non-independent samples. In each experiment, we varied the number of dyads between 106 to 109 in increments of 106 and computed the size of F \\ L after each increment. We ran each experiment 1, 000 times to compute the sample means shown in Figure 5.\nAlgorithm 2 Fast non-dominated sorting.\nRequire: Arrays X and Y of length n (the values of the two criteria) 1: Sort X and Y according to X in ascending order 2: while X and Y are nonempty do 3: Add (X(1), Y (1)) to current Pareto front 4: y ← Y (1) 5: for i = 2 → length(X) do 6: if Y (i) ≤ y then 7: Add (X(i), Y (i)) to current Pareto front 8: y ← Y (i) 9: Remove current Pareto front from X ,Y"
    }, {
      "heading" : "Appendix C Implementation of PDA anomaly detector",
      "text" : "Pseudocode for the PDA anomaly detector was presented as Algorithm 1 in Section 4.2. The training phase involves creating (\nN 2\n)\ndyads corresponding to all pairs of training samples. Computing all pairwise dissimilarities in each criterion requires O(mKN2) floating-point operations (flops), where m denotes the number of dimensions involved in computing a dissimilarity. The Pareto fronts are constructed by non-dominated sorting. In Section C.1 we present a fast algorithm for nondominated sorting in two criteria; for more than two criteria, we use the non-dominated sort of Deb et al. [22] that constructs all of the Pareto fronts using O(KN4) comparisons in the worst case.\nThe testing phase involves creating dyads between the test sample and the kl nearest training samples in criterion l, which requires O(mKN) flops. For each dyad Dnewi , we need to calculate the depth ei. This involves comparing the test dyad with training dyads on multiple fronts until we find a training dyad that is dominated by the test dyad. ei is the front that this training dyad is a part of. Using a binary search to select the front and another binary search to select the training dyads within the front to compare to, we need to make O(K log2 N) comparisons (in the worst case) to compute ei. The anomaly score is computed by taking the mean of the s ei’s corresponding to the test sample; the score is then compared against a threshold σ to determine whether the sample is anomalous. As mentioned in the Section 4.2, both the training and testing phases scale linearly with the number of criteria K .\nC.1 Fast non-dominated sorting for two criteria\nWe present here a fast algorithm for non-dominated sorting in two criteria. The standard algorithm of Deb et al. [22] takes O(n2) time and requires O(n2) memory, where n = (\nN 2\n)\nis the number of dyads. In our experience, the memory requirement is the largest obstacle to applying Pareto methods to large data sets. Our algorithm runs in O(n3/2) time on average and requires O(n) memory. It is based on the following observation: if the data set is sorted in ascending order in the first criterion, then the first point is Pareto-optimal, and each subsequent Pareto-optimal point can be found by searching for the next point in the sorted list that is not dominated by the most recent addition to the Pareto front. For two criteria, there are on average O( √ n) Pareto fronts, and finding each front with this algorithm requires visiting at most n points, hence the O(n3/2) average complexity. The worst case complexity is O(n2) occurring when each Pareto front consists of a single point. Pseudocode for the algorithm is shown in Algorithm 2. It has recently come to our attention that an O(n lnn) algorithm exists for the canonical anti-chain partition problem [23], which is equivalent to non-dominated sorting in two criteria, and can also be used to quickly construct the Pareto fronts.\nC.2 Selection of parameters\nThe parameters to be selected in PDA are k1, . . . , kK , which denote the number of nearest neighbors in each criterion. We connect each test sample X to a training sample Xj if Xj is one of the ki nearest neighbors of X in terms of the dissimilarity measure defined by criterion i. We now discuss how these parameters k1, . . . , kK can be selected. For simplicity, first assume that there is only one criterion, so that a single parameter k is to be selected. PDA is able to detect an anomaly if the distribution of its dyads with respect to the Pareto fronts differs from that of a nominal sample.\nSpecifically the mean of the depths of the dyads (the ei’s) corresponding to an anomalous sample must be higher than that of a nominal sample. If k is chosen too small, this may not be the case, especially if there are training samples present near an anomalous sample, in which case, the dyads corresponding to the anomalous sample may reside near shallow fronts much like a nominal sample. On the other hand, if k is chosen too large, many dyads may correspond to connections to training samples that are far away, even if the test sample is nominal, which also makes the mean depths of nominal and anomalous samples more similar.\nWe propose to use the properties of k-nearest neighbor graphs (k-NNGs) constructed on the training samples to select the number of training samples to connect to each test sample. We construct symmetric k-NNGs, i.e. we connect samples i and j if i is one of the k nearest neighbors of j or j is one of the k nearest neighbors of i. We begin with k = 1 and increase k until the k-NNG of the training samples is connected, i.e. there is only a single connected component. By forcing the k-NNG to be connected, we ensure that there are no isolated regions of training samples. Such isolated regions could possibly lead to dyads corresponding to anomalous samples residing near shallow fronts like nominal samples, which is undesirable. By keeping k small while retaining a connected k-NNG, we are trying to avoid the problem of having too many dyads so that even a nominal sample may have many dyads located near deep fronts. This method of choosing k to retain connectivity has been used as a heuristic in other unsupervised learning problems, such as spectral clustering [24]. Note that by requiring the k-NNG to be connected, we are implicitly assuming that the training samples consist of a single class or multiple classes that are in close proximity. If the training samples contain multiple well-separated classes, such an approach may not work well.\nNow let’s return to the situation PDA was designed for, with K different criteria. For each criterion i, we construct a ki-NNG using the corresponding dissimilarity measure and increase ki until the ki-NNG is connected. We then connect each test sample to s = ∑K i=1 ki training samples. Note that we are choosing each ki independent of the other criteria, which is probably not an optimal approach. In principle, an approach that chooses the ki’s jointly could perform better; however, such an approach would add to the complexity. We choose separate ki’s for each criterion, which we find is necessary to obtain good performance when different dissimilarities have varying scales and properties. There are, however, pathological examples where the independent approach could choose ki’s poorly, such as the well-known example of two moons. These examples typically involve multiple well-separated classes, which may be problematic as previously mentioned. How to choose the ki’s when the training samples contain multiple well-separated classes is beyond the scope of this paper and is an area for future work. We find the proposed heuristic to work well in practice, including for both examples presented in Section 5."
    }, {
      "heading" : "Appendix D Additional discussion on pedestrian trajectories experiment",
      "text" : "Figure 6 shows some abnormal trajectories and nominal trajectories detected using PDA. Recall that the two criteria used are walking speed and trajectory shape. Anomalous trajectories could have anomalous speeds or shapes (or both), so some anomalous trajectories in Figure 6 may not look anomalous by shape alone. We find that the heuristic proposed in Section C.2 for choosing the ki’s performs quite well in this experiment, as shown in Figure 7. Specifically, the AUC obtained when using the parameters chosen by the proposed heuristic is very close to the AUC obtained when using\nthe optimal parameters, which are not known in advance. As discussed in Section 5.2, it is also higher than the AUCs of all of the single-criterion anomaly detection methods, even under the best choice of weights."
    } ],
    "references" : [ {
      "title" : "A survey of outlier detection methodologies. Artificial Intelligence Review 22(2):85–126",
      "author" : [ "V.J. Hodge", "J. Austin" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Anomaly detection: A survey",
      "author" : [ "V. Chandola", "A. Banerjee", "V. Kumar" ],
      "venue" : "ACM Computing Surveys",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Pareto-based multiobjective machine learning: An overview and case studies",
      "author" : [ "Y. Jin", "B. Sendhoff" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Pareto-optimal methods for gene ranking",
      "author" : [ "A.O. Hero III", "G. Fleury" ],
      "venue" : "The Journal of VLSI Signal Processing",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T. Mitchell" ],
      "venue" : "In Proceedings of the 11th Annual Conference on Computational Learning Theory",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "A co-regularization approach to semisupervised learning with multiple views",
      "author" : [ "V. Sindhwani", "P. Niyogi", "M. Belkin" ],
      "venue" : "In Proceedings of the Workshop on Learning with Multiple Views,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Multi-view learning in the presence of view disagreement",
      "author" : [ "C.M. Christoudias", "R. Urtasun", "T. Darrell" ],
      "venue" : "In Proceedings of the Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Multiple kernel learning algorithms",
      "author" : [ "M. Gönen", "E. Alpaydın" ],
      "venue" : "Journal of Machine Learning Research 12(Jul):2211–2268",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Nearest-neighbor clutter removal for estimating features in spatial point processes",
      "author" : [ "S. Byers", "A.E. Raftery" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Fast outlier detection in high dimensional spaces",
      "author" : [ "F. Angiulli", "C. Pizzuti" ],
      "venue" : "In Proceedings of the 6th European Conference on Principles of Data Mining and Knowledge Discovery",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "A geometric framework for unsupervised anomaly detection: Detecting intrusions in unlabeled data",
      "author" : [ "E. Eskin", "A. Arnold", "M. Prerau", "L. Portnoy", "S. Stolfo" ],
      "venue" : "In Applications of Data Mining in Computer Security",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "LOF: Identifying density-based local outliers",
      "author" : [ "M.M. Breunig", "H.-P. Kriegel", "R.T. Ng", "J. Sander" ],
      "venue" : "In Proceedings of the ACM SIGMOD International Conference on Management of Data",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2000
    }, {
      "title" : "Geometric entropy minimization (GEM) for anomaly detection and localization",
      "author" : [ "III A.O. Hero" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Efficient anomaly detection using bipartite k-NN graphs",
      "author" : [ "K. Sricharan", "A.O. Hero III" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Anomaly detection with score functions based on nearest neighbor graphs",
      "author" : [ "M. Zhao", "V. Saligrama" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Multicriteria optimization. Lecture Notes in Economics and Mathematical Systems 491",
      "author" : [ "M. Ehrgott" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "On the distribution of the number of admissible points in a vector random sample",
      "author" : [ "O. Barndorff-Nielsen", "M. Sobel" ],
      "venue" : "Theory of Probability and its Applications,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1966
    }, {
      "title" : "Maximal points and Gaussian fields. Unpublished. URL http://www.math.illinois.edu/ ̃ymb/ps/by4.pdf",
      "author" : [ "Y. Baryshnikov", "J.E. Yukich" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Statistical models of pedestrian behaviour in the Forum. Master’s thesis, University of Edinburgh",
      "author" : [ "B. Majecka" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II",
      "author" : [ "K. Deb", "S. Agrawal", "A. Pratap" ],
      "venue" : "Meyarivan",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2000
    }, {
      "title" : "Maximum k-chains in planar point sets: Combinatorial structure and algorithms",
      "author" : [ "S. Felsner", "L. Wernisch" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1999
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Statistics and Computing",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Anomaly detection is an important problem that has been studied in a variety of areas and used in diverse applications including intrusion detection, fraud detection, and image processing [1, 2].",
      "startOffset" : 188,
      "endOffset" : 194
    }, {
      "referenceID" : 1,
      "context" : "Anomaly detection is an important problem that has been studied in a variety of areas and used in diverse applications including intrusion detection, fraud detection, and image processing [1, 2].",
      "startOffset" : 188,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "Several machine learning methods utilizing Pareto optimality have previously been proposed; an overview can be found in [3].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "Hero and Fleury [4] introduced a method for gene ranking using Pareto fronts that is related to our approach.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "Another related area is multi-view learning [5, 6], which involves learning from data represented by multiple sets of features, commonly referred to as “views”.",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Another related area is multi-view learning [5, 6], which involves learning from data represented by multiple sets of features, commonly referred to as “views”.",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "The problem of view disagreement, where samples take different classes in different views, has recently been investigated [7].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "A similar area is that of multiple kernel learning [8], which is typically applied to supervised learning problems, unlike the unsupervised anomaly detection setting we consider.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Hodge and Austin [1] and Chandola et al.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "[2] both provide extensive surveys of different anomaly detection methods and applications.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "Byers and Raftery [9] proposed to use the distance between a sample and its kth-nearest neighbor as the anomaly score for the sample; similarly, Angiulli and Pizzuti [10] and Eskin et al.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Byers and Raftery [9] proposed to use the distance between a sample and its kth-nearest neighbor as the anomaly score for the sample; similarly, Angiulli and Pizzuti [10] and Eskin et al.",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "[11] proposed to the use the sum of the distances between a sample and its k nearest neighbors.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] used an anomaly score based on the local density of the k nearest neighbors of a sample.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "Hero [13] and Sricharan and Hero [14] introduced non-parametric adaptive anomaly detection methods using geometric entropy minimization, based on random k-point minimal spanning trees and bipartite k-nearest neighbor (k-NN) graphs, respectively.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : "Hero [13] and Sricharan and Hero [14] introduced non-parametric adaptive anomaly detection methods using geometric entropy minimization, based on random k-point minimal spanning trees and bipartite k-nearest neighbor (k-NN) graphs, respectively.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "Zhao and Saligrama [15] proposed an anomaly detection algorithm k-LPE using local p-value estimation (LPE) based on a k-NN graph.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "The PDA method proposed in this paper utilizes the notion of Pareto optimality, which has been studied in many application areas in economics, computer science, and the social sciences among others [16].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 16,
      "context" : "1 Mathematical properties of Pareto fronts The distribution of the number of points on the first Pareto front was first studied by BarndorffNielsen and Sobel in their seminal work [17].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 15,
      "context" : "A common approach in multi-objective optimization is linear scalarization [16], which constructs a new single criterion as a convex combination of the d criteria.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "It has recently come to our attention that Theorem 1 appears in a more general form in an unpublished manuscript of Baryshnikov and Yukich [19].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : ", Yn be independent and uniformly distributed on [0, 1].",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 16,
      "context" : "A proof that E|F| = lnn + O(1) as n → ∞ can be found in [17].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "The nominal distribution is given by the uniform distribution on the hypercube [0, 1].",
      "startOffset" : 79,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "We now present an experiment on a real data set that contains thousands of pedestrians’ trajectories in an open area monitored by a video camera [20].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "0 t(1− at +O(bt))dt, λ ∈ [0, 1], a, b > 0.",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "For (x, y) ∈ [0, 1] let Dx,y be the event that Y1 = (x, y) and (x, y) ∈ F .",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Define A = { (u, v) ∈ [0, 1] | 0 < u < x, y < v < 2y − uy x }",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : ", and B = { (u, v) ∈ [0, 1] | x < u < 1, 0 < v < 2y − uy x }",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "We first drew samples uniformly in [0, 1] and computed the dyads corresponding to the two criteria |∆x| and |∆y|, which denote the absolute differences between the x and y coordinates, respectively.",
      "startOffset" : 35,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "Figure 4: 990 dyads constructed with two different sets of criteria from 45 samples uniformly distributed in [0, 1].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "The domain of the resulting dyads is again the box [0, 1], as shown in Figure 4(a), so this experiment tests Theorem 2.",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "A somewhat contrived example involves the criteria |∆x| + |∆y| and |∆x| − |∆y|, which, when applied to uniformly sampled data on [0, 1], yields dyads sampled on a diamond domain, as shown in Figure 4(b).",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 19,
      "context" : "[22] that constructs all of the Pareto fronts using O(KN) comparisons in the worst case.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[22] takes O(n) time and requires O(n) memory, where n = ( N 2 )",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "It has recently come to our attention that an O(n lnn) algorithm exists for the canonical anti-chain partition problem [23], which is equivalent to non-dominated sorting in two criteria, and can also be used to quickly construct the Pareto fronts.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 21,
      "context" : "This method of choosing k to retain connectivity has been used as a heuristic in other unsupervised learning problems, such as spectral clustering [24].",
      "startOffset" : 147,
      "endOffset" : 151
    } ],
    "year" : 2013,
    "abstractText" : "We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.",
    "creator" : "LaTeX with hyperref package"
  }
}