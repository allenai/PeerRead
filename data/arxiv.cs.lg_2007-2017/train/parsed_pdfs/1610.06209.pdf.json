{
  "name" : "1610.06209.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Structured adaptive and random spinners for fast machine learning computations",
    "authors" : [ "Mariusz Bojarski", "Anna Choromanska", "Krzysztof Choromanski", "Francois Fagan", "Cedric Gouy-Pailler", "Anne Morvan", "Nouri Sakr", "Tamas Sarlos", "Jamal Atif" ],
    "emails" : [ "mbojarski@nvidia.com", "achoroma@cims.nyu.edu", "kchoro@google.com", "ff2316@columbia.edu", "cedric.gouy-pailler@cea.fr", "anne.morvan@cea.fr", "nts2122@columbia.edu", "stamas@google.com", "jamal.atif@dauphine.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We consider an efficient computational framework for speeding up several machine learning algorithms with almost no loss of accuracy. The proposed framework relies on projections via structured matrices that we call Structured Spinners, which are formed as products of three structured matrix-blocks that incorporate rotations. The approach is highly generic, i.e. i) structured matrices under consideration can either be fully-randomized or learned, ii) our structured family contains as special cases all previously considered structured schemes, iii) the setting extends to the non-linear case where the projections are followed by non-linear functions, and iv) the method finds numerous applications including kernel approximations via random feature maps, dimensionality reduction algorithms, new fast cross-polytope LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more. The proposed framework comes with theoretical guarantees characterizing the capacity of the structured model in reference to its unstructured counterpart and is based on a general theoretical principle that we describe in the paper. As a consequence of our theoretical analysis, we provide the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the HD3HD2HD1 structured matrix [Andoni et al., 2015]. The exhaustive ex-\n1equal contribution\nperimental evaluation confirms the accuracy and efficiency of structured spinners for a variety of different applications."
    }, {
      "heading" : "1 Introduction",
      "text" : "A striking majority of machine learning frameworks performs projections of input data via matrices of parameters, where the obtained projections are often passed to a possibly highly nonlinear function. In the case of randomized machine learning algorithms, the projection matrix is typically Gaussian with i.i.d. entries taken from N (0, 1). Otherwise, it is learned through the optimization scheme. A plethora of machine learning algorithms admits this form. In the randomized setting, a few examples include variants of the Johnson-Lindenstrauss Transform applying random projections to reduce data dimensionality while approximately preserving Euclidean distance [Ailon and Chazelle, 2006, Liberty et al., 2008, Ailon and Liberty, 2011], kernel approximation techniques based on random feature maps produced from linear projections with Gaussian matrices followed by nonlinear mappings [Rahimi and Recht, 2007], [Le et al., 2013, Choromanski and Sindhwani, 2016, Huang et al., 2014], [Choromanska et al., 2016], LSH-based schemes [Har-Peled et al., 2012, Charikar, 2002, Terasawa and Tanaka, 2007], including the fastest known variant of the cross-polytope LSH [Andoni et al., 2015], algorithms solving convex optimization problems with random sketches of Hessian matrices [Pilanci and Wainwright, 2015, Pilanci and Wainwright, 2014], quantization techniques using random projection trees, where splitting in each node is determined by a projection of data onto Gaussian direction [Dasgupta and Freund, 2008], and many more. ar X iv :1 61 0. 06 20 9v 1\n[ cs\n.L G\n] 1\nThe classical example of machine learning nonlinear models where linear projections are learned is a multi-layered neural network [LeCun et al., 2015, Goodfellow et al., 2016], where the operations of linear projection via matrices with learned parameters followed by the pointwise nonlinear feature transformation are the building blocks of the network’s architecture. These two operations are typically stacked multiple times to form a deep network.\nThe computation of projections takes Θ(mn|X |) time, where m× n is the size of the projection matrix, and |X | denotes the number of data samples from a dataset X . In case of high-dimensional data, this comprises a significant fraction of the overall computational time, while storing the projection matrix frequently becomes a bottleneck in terms of space complexity.\nIn this paper, we propose the remedy for both problems, which relies on replacing the aforementioned algorithms by their “structured variants”. The projection is performed by applying a structured matrix from the family that we introduce as Structured Spinners. Depending on the setting, the structured matrix is either learned or its parameters are taken from a random distribution (either continuous or discrete if further compression is required). Each structured spinner is a product of three matrix-blocks that incoporate rotations. A notable member of this family is a matrix of the form HD3HD2HD1, where Dis are either random diagonal ±1-matrices or adaptive diagonal matrices and H is the Hadamard matrix. This matrix is used in the fastest known cross-polytope LSH method introduced in [Andoni et al., 2015].\nIn the structured case, the computational speedups are significant, i.e. projections can be calculated in o(mn) time, often in O(n logm) time if Fast Fourier Transform techniques are applied. At the same time, using matrices from the family of structured spinners leads to the reduction of space complexity to sub-quadratic, usually at most linear, or sometimes even constant.\nThe key contributions of this paper are:\n• The family of structured spinners providing a highly parametrized class of structured methods and, as we show in this paper, with applications in various randomized settings such as: kernel approximations via random feature maps, dimensionality reduction algorithms, new fast cross-polytope LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more.\n• A comprehensive theoretical explanation of the effectiveness of the structured approach based on structured spinners. Such analysis was provided in the literature before for a strict subclass of a very general family of structured matrices\nthat we consider in this paper, i.e. the proposed family of structured spinners contains all previously considered structured matrices as special cases, including the recently introduced P -model [Choromanski and Sindhwani, 2016]. To the best of our knowledge, we are the first to theoretically explain the effectiveness of structured neural network architectures. Furthermore, we provide first theoretical guarantees for a wide range of discrete structured transforms, in particular for the fastest known cross-polytope LSH method [Andoni et al., 2015] based HD3HD2HD1 discrete matrices.\nOur theoretical methods in the random setting apply the relatively new Berry-Esseen type Central Limit Theorem results for random vectors.\nOur theoretical findings are supported by empirical evidence regarding the accuracy and efficiency of structured spinners in a wide range of different applications. Not only do structured spinners cover all already existing structured transforms as special instances, but also many other structured matrices that can be applied in all aforementioned applications."
    }, {
      "heading" : "2 Related work",
      "text" : "This paper focuses on structured matrices, which were previously explored in the literature mostly in the context of the Johnson-Lindenstrauss Transform (JLT) [Johnson and Lindenstrauss, 1984], where the high-dimensional data is linearly transformed and embedded into a much lower dimensional space while approximately preserving the Euclidean distance between data points. Several extensions of JLT have been proposed, e.g. [Liberty et al., 2008, Ailon and Liberty, 2011, Ailon and Chazelle, 2006, Vyb́ıral, 2011]. Most of these structured constructions involve sparse [Ailon and Chazelle, 2006, Dasgupta et al., 2010] or circulant matrices [Vyb́ıral, 2011, Hinrichs and Vybral, 2011] providing computational speedups and space compression.\nMore recently, the so-called Ψ-regular structured matrices (Toeplitz and circulant matrices belong to this wider family of matrices) were used to approximate angular distances [Choromanska et al., 2016] and signed Circulant Random Matrices were used to approximate Gaussian kernels [Feng et al., 2015]. Another work [Choromanski and Sindhwani, 2016] applies structured matrices coming from the so-called P-model, which further generalizes the Ψ-regular family, to speed up random feature map computations of some special kernels (angular, arc-cosine and Gaussian). These techniques did not work for discrete structured constructions, such as the HD3HD2HD1 matrices, or\ntheir direct non-discrete modifications, since they require matrices with low (polylog) chromatic number of the corresponding coherence graphs.\nLinear projections are used in the LSH setting to construct codes for given datapoints which speed up such tasks as approximate nearest neighbor search. A notable set of methods are the so-called cross-polytope techniques introduced in [Terasawa and Tanaka, 2007] and their aforementioned discrete structured variants proposed in [Andoni et al., 2015] that are based on the Walsh-Hadamard transform. Before our work, they were only experimentally verified to produce good quality codes.\nFurthermore, a recently proposed technique based on the so-called Newton Sketch provides yet another example of application for structured matrices. The method [Pilanci and Wainwright, 2015, Pilanci and Wainwright, 2014] is used for speeding up algorithms solving convex optimization problems by approximating Hessian matrices using so-called sketch matrices. Initially, the sub-Gaussian sketches based on i.i.d. sub-Gaussian random variables were used. The disadvantage of the sub-Gaussian sketches lies in the fact that computing the sketch of the given matrix of size n × d requires O(mnd) time, where m × n in the size of the sketch matrix. Thus the method is too slow in practice and could be accelerated with the use of structured matrices. Some structured approaches were already considered, e.g. sketches based on randomized orthonormal systems were proposed [Pilanci and Wainwright, 2015].\nAll previously considered methods focus on the randomized setting, whereas the structured matrix instead of being learned is fully random. In the context of adaptive setting, where the parameters are being learned instead, we focus in this paper on multi-layer neural networks. We emphasize though that our approach is much more general and extends beyond this setting. Structured neural networks were considered before, for instance in [Yang et al., 2015], where the so-called Deep Fried Neural Convnets were proposed. Those architectures are based on the adaptive version of the Fastfood transform used for approximating various kernels [Le et al., 2013], which is a special case of structured spinner matrices.\nDeep Fried Convnets apply adaptive structured matrices for fully connected layers of the convolutional networks. The structured matrix is of the form: SHGΠHB, where S, G, and B are adaptive diagonal matrices, Π is a random permutation matrix, and H is the Walsh-Hadamard matrix. The method reduces the storage and computational costs of matrix multiplication step from, often prohibitive, O(nd) down to O(n) storage and O(n log d) computational cost, where d and n denote the size of consequitive layers of the network. At the same time, this approach does not\nsacrifice the network’s predictive performance.\nThe Adaptive Fastfood approach elegantly complements previous works dedicated to address the problem of huge overparametrization of deep models with structured matrices, e.g. the method of [Denil et al., 2013] represents the parameter matrix as a product of two low rank factors and, similarly to Adaptive Fastfood, applies both at train and test time, [Sainath et al., 2013] introduces low-rank matrix factorization to reduce the size of the fully connected layers at train time, and [Li, 2013] uses low-rank factorizations with SVD after training the full model. These methods, as well as approaches that consider kernel methods in deep learning [Cho and Saul, 2009, Mairal et al., 2014, Dai et al., 2014, Huang et al., 2014], are conveniently discussed in [Yang et al., 2015].\nStructured neural networks are also considered in [Sindhwani et al., 2015], where low-displacement rank matrices are applied for linear projections. The advantage of this approach over Deep Fried Convnets is due to the high parametrization of the family of lowdisplacement rank matrices allowing the adjustment of the number of parameters learned based on accuracy and speedup requirements.\nThe class of structured spinners proposed in this work is more general than Deep Fried Convnets or low displacement rank matrices, but it also provides much easier structured constructions, such as HD3HD2HD1 matrices, where Dis are adaptive diagonal matrices. Furthermore, to the best of our knowledge we are the first to prove theoretically that structured neural networks learn good quality models, by analyzing the capacity of the family of structured spinners.\n3 The family of Structured Spinners\nBefore introducing the family of structured spinners, we explain notation. If not specified otherwise, matrix D is a random diagonal matrix with diagonal entries taken independently at random from {−1,+1}. By Dt1,...,tn we denote the diagonal matrix with diagonal equal to (t1, ..., tn). For a matrix A = {ai,j}i,j=1,...,n ∈ Rn×n, we denote by ‖A‖F its Frobenius norm, i.e. ‖A‖F = √∑ i,j∈{1,...,n} a 2 i,j , and by ‖A‖2 its spectral norm, i.e. ‖A‖2 = supx6=0 ‖Ax‖2 ‖x‖2 . We denote by H the L2-normalized Hadamard matrix. We say that r is a random Rademacher vector if every element of r is chosen independently at random from {−1,+1}.\nFor a vector r ∈ Rk and n > 0 let C(r, n) ∈ Rn×nk be a matrix, where the first row is of the form (rT , 0, ..., 0) and each subsequent row is obtained from the previous one by right-shifting in a circulant manner the previous one by k. For a sequence of matrices W1, ...,Wn ∈ Rk×n we denote by V(W1, ...,Wn) ∈ Rnk×n a matrix\nobtained by vertically stacking matrices: W1, ...,Wn.\nEach structured matrix Gstruct ∈ Rn×n from the family of structured spinners is a product of three main structured components/blocks, i.e.:\nGstruct = M3M2M1, (1)\nwhere matrices M1,M2 and M3 satisfy conditions:\nCondition 1: Matrices: M1 and M2M1 are (δ(n), p(n))-balanced isometries. Condition 2: M2 = V(W 1, ...,Wn)Dρ1,...,ρn for some (∆F ,∆2)-smooth set: W 1, ...,Wn ∈ Rk×n and some i.i.d sub-Gaussian random variables ρ1, ..., ρn with sub-Gaussian norm K. Condition 3: M3 = C(r, n) for r ∈ Rk, where r is random Rademacher/Gaussian in the random setting and is learned in the adaptive setting.\nMatrix Gstruct is a structured spinner with parameters: δ(n), p(n),K,ΛF ,Λ2. We explain the introduced conditions below.\nDefinition 1 ((δ(n), p(n))-balanced matrices) A randomized matrix M ∈ Rn×m is (δ(n), p(n))balanced if for every x ∈ Rm with ‖x‖2 = 1 we have: P[‖Mx‖∞ > δ(n)√n ] ≤ p(n).\nRemark 1 One can take as M1 a matrix HD1 since, as we will show in the Supplement, matrix HD1 is (log(n), 2ne− log2(n) 8 )-balanced.\nDefinition 2 ((∆F ,∆2)-smooth sets) A deterministic set of matrices W1, ...,Wn ∈ Rk×n is (ΛF ,Λ2)smooth if:\n• ‖Wi1‖2 = .. = ‖W i n‖2 for i = 1, ..., n, where W i j\nstands for the jth column of Wi, • for i 6= j and l = 1, ..., n we have: (Wil)T ·W j l = 0, • maxi,j ‖(Wj)TWi‖F ≤ ΛF and maxi,j ‖(Wj)TWi‖2 ≤ Λ2.\nRemark 2 If the unstructured matrix G has rows taken from the general multivariate Gaussian distribution with diagonal covariance matrix Σ 6= I then one needs to rescale vectors r accordingly. For clarity, we assume here that Σ = I and we present our theoretical results for that setting.\nAll structured matrices previously considered are special cases of a wider family of structured spinners (for clarity, we will explicitly show it for some important special cases). We have:\nLemma 1 The following matrices: GcircD2HD1,√ nHD3HD2HD1 and √ nHDg1,...,gnHD2HD1,\nwhere Gcirc is Gaussian circulant, are valid structured spinners for δ(n) = log(n), p(n) = 2ne− log2(n)\n8 , K = 1, ΛF = O( √ n) and Λ2 = O(1). The same is true if one replaces Gcirc by a Gaussian Hankel or Toeplitz matrix.\n3.1 The role of three blocks M1, M2, and M3\nThe role of blocks M1, M2, M3 can be intuitively explained. Matrix M1 makes vectors “balanced”, so that there is no dimension that carries too much of the L2-norm of the vector. The balanceness property was already applied in the structured setting [Ailon and Chazelle, 2006].\nThe role of M2 is more subtle and differs between adaptive and random settings. In the random setting, the cost of applying the structured mechanism is the loss of independence. For instance, the dot products of the rows of a circulant Gaussian matrix with a given vector x are no longer independent, as it is the case in the fully random setup. Those dot products can be expressed as a dot product of a fixed Gaussian row with different vectors v. Matrix M2 makes these vectors close to orthogonal. In the adaptive setup, the “close to orthogonality” property is replaced by the independence property.\nFinally, matrix M3 defines the capacity of the entire structured transform by providing a vector of parameters (either random or to be learned). The near-independence of the aforementioned dot products in the random setting is now implied by the nearorthogonality property achieved by M2 and the fact that the projections of the Gaussian vector or the random Rademacher vector onto “almost orthogonal directions” are “close to independent”. The role of the three matrices is described pictorially in Figure 1.\n3.2 Stacking together Structured Spinners\nWe described structured spinners as square matrices, but in practice we are not restricted to those, i.e. one can construct an m× n structured spinner for m ≤ n from the square n × n structured spinner by taking its first m rows. We can then stack vertically these independently constructed m × n matrices to obtain an k × n matrix for both: k ≤ n and k > n. We think about m as another parameter of the model that tunes the “structuredness” level, i.e. larger values of m indicate more structured approach while smaller values lead to more random matrices (m = 1 case is the fully unstructured one)."
    }, {
      "heading" : "4 Theoretical results",
      "text" : "We now show that structured spinners can replace their unstructured counterparts in many machine learning\nalgorithms with minimal loss of accuracy.\nLet AG be a machine learning algorithm applied to a fixed dataset X ⊆ Rn and parametrized by a set G of matrices G ∈ Rm×n, where each G is either learned or Gaussian with independent entries taken from N (0, 1). Assume furthermore, that AG consists of functions f1, ..., fs, where each fi applies a certain matrix Gi from G to vectors from some linear space Li of dimensionality at most d. Note that for a fixed dataset X function fi is a function of a random vector\nqfi = ((Gix 1)T , ..., (Gix di)T )T ∈ Rdi·m,\nwhere dim(Li) = di ≤ d and x1, ...,xdi stands for some fixed basis of Li.\nDenote by f ′i the structured counterpart of fi, where Gi is replaced by the structured spinner (for which vector r is either learned or random). We will show that f ′is “resemble” fis distribution-wise. Surprisingly, we will show it under very weak conditions regarding fis, In particular, they can be nondifferentiable, even non-continuous.\nNote that the above setting covers a wide range of machine learning algorithms. In particular:\nRemark 3 In the kernel approximation setting with random feature maps one can match each pair of vectors x,y ∈ X to a different f = fx,y. Each f computes the approximate value of the kernel for vectors x and y.\nThus in that scenario s = (|X |\n2\n) and d = 2 (since one\ncan take: Lf(x,y) = span(x,y)).\nRemark 4 In the vector quantization algorithms using random projection trees one can take s = 1 (the algorithm A itself is a function f outputting the partitioning of space into cells) and d = dintrinsic, where dintrinsic is an intrinsic dimensionality of a given dataset X (random projection trees are often used if dintrinsic n)."
    }, {
      "heading" : "4.1 Random setting",
      "text" : "We need the following definition.\nDefinition 3 A set S is b-convex if it is a union of at most b pairwise disjoint convex sets.\nFix a funcion fi : Rdi·m → V , for some domain V . Our main result states that for any S ⊆ V such that f−1i (S) is measurable and b-convex for b not too large, the probability that fi(qfi) belongs to S is close to the probability that f ′i(qf ′i ) belongs to S.\nTheorem 1 (structured random setting) Let A be a randomized algorithm using unstructured Gaussian matrices G and let s, d and fis be as at the beginning of the section. Replace the unstructured matrix G by one of structured spinners defined in Section 3 with blocks of m rows each. Then for n large enough, = omd(1) and fixed fi with probability psucc at least:\n1−2p(n)d−2 ( md\n2\n) e −Ω(min( 2n2 K4Λ2 F δ4(n) , n K2Λ2δ 2(n) )) (2)\nwith respect to the random choices of M1 and M2 the following holds for any S such that f−1i (S) is measurable and b-convex:\n|P[fi(qfi) ∈ S]− P[f ′ i(qf ′i ) ∈ S]| ≤ bη,\nwhere the the probabilities in the last formula are with respect to the random choice of M3, η = δ3(n)\nn 2 5\n, and\nδ(n), p(n),K,ΛF ,Λ2 are as in the definition of structured spinners from Section 3.\nRemark 5 The theorem does not require any strong regularity conditions regarding fis (such as differentiability or even continuity). In practice, b is often a small constant. For instance, for the angular kernel approximation where fis are non-continuous and for S-singletons, we can take b = 1 (see Supplement).\nNow let us think of fi and f ′ i as random variables, where randomness is generated by vectors qfi and qf ′i respectively. Then, from Theorem 1, we get:\nTheorem 2 Denote by FX the cdf of the random variable X and by φX its characteristic function. If fi is convex or concave in respect to qfi , then for every t the following holds: |Ffi(t)− Ff ′i (t)| = O( δ3(n)\nn 2 5\n). Further-\nmore, if fi is bounded then: |φfi(t)−φf ′i (t)| = O( δ3(n)\nn 2 5\n).\nTheorem 1 implies strong accuracy guarantees for the specific structured spinners. As a corollary we get:\nTheorem 3 Under assumptions from Theorem 1 the probability psucc from Theorem 1 reduces to: 1 − 4ne− log2(n) 8 d − 2 ( md 2 ) e −Ω( 2n log4(n) ) for the structured\nmatrices √ nHD3HD2HD1, √ nHDg1,...,gnHD2HD1 as well as for the structured matrices of the form GstructD2HD1, where Gstruct is Gaussian circulant, Gaussian Toeplitz or Gaussian Hankel matrix.\nAs a corollary of Theorem 3, we obtain the following result showing the effectiveness of the cross-polytope LSH with structured matrices HD3HD2HD1 that was only heuristically confirmed before [Andoni et al., 2015].\nTheorem 4 Let x,y ∈ Rn be two unit L2-norm vectors. Let vx,y be the vector indexed by all (2m)\n2 ordered pairs of canonical directions (±ei,±ej), where the value of the entry indexed by (u,w) is the probability that: h(x) = u and h(y) = w, and h(v) stands for the hash of v. Then with probability at least: psuccess = 1 − 8ne− log2(n) 8 − 2 ( 2m 2 ) e −Ω( 2n log4(n) ) the version of the stochastic vector v1x,y for the unstructured Gaussian matrix G and its structured counterpart v2x,y for the matrix HD3HD2HD1 satisfy: ‖v1x,y−v2x,y‖∞ ≤ log3(n)n− 2 5 + c , for n large enough, where c > 0 is a universal constant. The probability above is taken with respect to random choices of D1 and D2.\nFor angles in the range [0, π3 ] the result above leads to the same asymptotics of the probabilities of collisions as these in Theorem 1 of [Andoni et al., 2015] given for the unstructured cross-polytope LSH.\nThe proof for the discrete structured setting applies Berry-Esseen-type results for random vectors (details are in the Supplement) showing that for n large enough ±1 random vectors r act similarly to Gaussian vectors."
    }, {
      "heading" : "4.2 Adaptive setting",
      "text" : "The following theorem explains that structured spinners can be used to replace unstructured fully connected neural network layers performing dimensionality reduction (such as hidden layers in certain autoencoders)\nprovided that input data has low intrinsic dimensionality. These theoretical findings were confirmed in experiments that will be presented in the next section. We will use notation from Theorem 1.\nTheorem 5 Consider a matrix M ∈ Rm×n encoding the weights of connections between a layer l0 of size n and a layer l1 of size m in some learned unstructured neural network model. Assume that the input to layer l0 is taken from the d-dimensional space L (although potentially embedded in a much higher dimensional space). Then with probability at least\n1−2p(n)d−2 ( md\n2\n) e −Ω(min( t2n2 K4Λ2 F δ4(n) , tn K2Λ2δ 2(n) )) (3)\nfor t = 1md and with respect to random choices of M1 and M2, there exists a vector r defining M3 (see: definition of the structured spinner) such that the structured spinner Mstruct = M3M2M1 equals to M on L."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section we consider a wide range of different applications of structured spinners: localitysensitive hashing, kernel approximations, and finally neural networks. Experiments with Newton sketches are deferred to the Supplement. Experiments were conducted using Python. In particular, NumPy is linked against a highly optimized BLAS library (Intel MKL). Fast Fourier Transform is performed using numpy.fft and Fast Hadamard Transform is using ffht from [Andoni et al., 2015]. To have a fair comparison, we have set up: OMP NUM THREADS = 1 so that every experiment is done on a single thread. Every parameter of the structured spinner matrix is computed in advance, such that obtained speedups take only matrix-vector products into account. All figures should be read in color."
    }, {
      "heading" : "5.1 Locality-Sensitive Hashing (LSH)",
      "text" : "In the first experiment, we consider cross-polytope LSH. In Figure 2, we compare collision probabilities for the low dimensional case (n = 256), where for each interval, collision probability has been computed for 20000 points. Results are shown for one hash function (averaged over 100 runs). We report results for a random 256 × 64 Gaussian matrix G and five other types of matrices from a family of structured spinners (descending order of number of parameters): GcircK2K1, GToeplitzD2HD1, Gskew−circD2HD1, HDg1,...,gnHD2HD1, and HD3HD2HD1, where Ki, GToeplitz, and Gskew−circ are respectively a Kronecker matrix with discrete entries, Gaussian Toeplitz and Gaussian skew-circulant matrices.\nAll matrices from the family of structured spinners show high collision probabilities for small distances and\nCollision probabilities with cross−polytope LSH\nlow ones for large distances. As theoretically predicted, structured spinners do not lead to accuracy losses. All considered matrices give almost identical results."
    }, {
      "heading" : "5.2 Kernel approximation",
      "text" : "In the second experiment, we approximate the Gaussian and angular kernels using Random Fourier features. The Gaussian random matrix (with i.i.d. Gaussian entries) can be used to sample random Fourier features with a specified σ. This Gaussian random matrix is replaced with specific matrices from a family of structured spinners for Gaussian and angular kernels. The obtained feature maps are compared. To test the quality of the structured kernels’ approximations, we compute Gram-matrix reconstruction error as in [Choromanski and Sindhwani, 2016] : ||K−K̃||F||K||F , where K, K̃ are respectively the exact and approximate Gram-matrices, as a function of the number of random features. When number of random features k is greater than data dimensionality n, we apply block-mechanism described in 3.2.\nFor the Gaussian kernel, Kij = e −||xi−xj || 2 2 2σ2 and for the\nangular kernel, Kij = 1− θπ with θ = cos −1( xTi xj ||xi||2||xj ||2 ). For the approximation, K̃i,j = 1√ d′ s(Axi) T 1√ d′ s(Axj) where s(x) = e −ix σ and K̃i,j = 1 − dH(s(Axi),s(Axj))d′ where s(x) = sign(x) respectively. In both cases, function s is applied pointwise. dH stands for the Hamming distance and xi, xj are points from the dataset.\nWe used two datasets: G50C (550 points, n = 50) and USPST (test set, 2007 points, n = 256). The results for the USPST dataset are given in the Supplement. For Gaussian kernel, bandwidth σ is set to 17.4734 for G50C and to 9.4338 for USPST. The choice of σ comes from [Choromanski and Sindhwani, 2016] in order to have comparable results. The results are averaged over 10 runs and the following matrices have been tested: Gaussian random matrix G, GcircK2K1, GToeplitzD2HD1, Gskew−circD2HD1, HDg1,...,gnHD2HD1 and HD3HD2HD1.\nFigure 5 shows results for the G50C dataset. In case of G50C dataset, for both kernels, all matrices from the family of structured spinners perform similarly to a random Gaussian matrix. HD3HD2HD1 performs better than all other matrices for a wide range of sizes of random feature maps. In case of USPST dataset (see: Supplement), for both kernels, all matrices from the family of structured spinners again perform similarly to a random Gaussian matrix (except GcircK2K1 which gives relatively poor results) and HD3HD2HD1 is giving the best results. Finally, the efficiency of structured spinners does not depend on the dataset.\nTable 1 shows substantial speedups obtained by the structured spinner matrices. The speedups are computed as time(G)/time(T), where time(G) and time(T) are the runtimes for respectively a random Gaussian matrix and a structured spinner matrix."
    }, {
      "heading" : "5.3 Neural networks",
      "text" : "Finally, we performed experiments with neural networks using two different network architectures. The first one is a fully-connected network with two fully connected layers (we call it MLP), where we refer to the size of the hidden layer as h, and the second one is a convolutional network with following architecture:\n• Convolution layer with filter size 5× 5, 4 feature maps + ReLU + Max Pooling (region 2× 2 and step 2× 2) • Convolution layer with filter size 5× 5, 6 feature\nmaps + ReLU + Max Pooling (region 2× 2 and step 2× 2) • Fully-connected layer (h outputs) + ReLU • Fully-connected layer (10 outputs) • LogSoftMax.\nExperiments were performed on the MNIST data set. In both experiments, we re-parametrized each matrix of weights of fully connected layers with a structured\nGram matrix reconstruction error G50C dataset for the Gaussian kernel\nGram matrix reconstruction error G50C dataset for the angular kernel\nMLP neural network error\nConvolutional neural network error\nHD3HD2HD1 matrix from a family of structured spinners. We compare this setting with the case where the unstructured parameter matrix is used. Note that in case when we use HD3HD2HD1 only linear number of parameters is learned (the Hadamard matrix is deterministic and even does not need to be explicitly stored, instead Walsh-Hadamard transform is used). Thus the network has significantly less parameters than in the unstructured case, e.g. for the MLP network we have O(h) instead of O(input size× h) parameters.\nIn Figure 4 and Table 2 we compare respectively the test error and running time of the unstructured and structured approaches. Figure 4 shows that for large enough h, neural networks with structured spinners achieve similar performance to those with unstructured projections, while at the same time using structured spinners lead to significant computational savings as shown in Table 2. As mentioned before, the HD3HD2HD1neural network is a simpler construction than the Deep Friend Convnet, however one can replace it with any structured spinner to obtain compressed neural network architecture of a good capacity."
    }, {
      "heading" : "5.4.1 Proof of Remark 1",
      "text" : "This result first appeared in [Ailon and Chazelle, 2006]. The following proof was given in [Choromanski and Sindhwani, 2016], we repeat it here for completeness. We will use the following standard concentration result.\nLemma 2 (Azuma’s Inequality) Let X1, ..., Xn be a martingale and assume that −αi ≤ Xi ≤ βi for some positive constants α1, ..., αn, β1, ..., βn. Denote X =∑n i=1Xi. Then the following is true:\nP[|X − E[X]| > a] ≤ 2e − a2 2 ∑n i=1 (αi+βi) 2\n(4)\nProof: Denote by x̃j an image of xj under transformation HD. Note that the ith dimension of x̃j is given by the formula: x̃ji = hi,1x j 1 + ...+ hi,nx\nj,n, where hl,u stands for the lth element of the uth column of the randomized Hadamard matrix HD. First, we use Azuma’s Inequality to find an upper bound on the probability that |x̃ji | > a, where a = log(n)√ n . By Azuma’s Inequality, we have:\nP[|hi,1xj1 + ...+ hi,nxj,n| ≥ a] ≤ 2e− log2(n) 8 . (5)\nWe use: αi = βi = 1√ n . Now we take the union bound over all n dimensions and the proof is completed.\n5.4.2 Structured Spinners-equivalent definition\nWe will introduce here an equivalent definition of the model of structured spinners that is more technical (thus we did not give it in the main body of the paper), yet more convenient to work with in the proofs.\nNote that from the definition of structured spinners we can conclude that each structured matrix Gstruct ∈ Rn×n from the family of structured spinners is a product of three main structured blocks, i.e.:\nGstruct = B3B2B1, (6)\nwhere matrices B1,B2,B3 satisfy two conditions that we give below.\nCondition 1: Matrices: B1 and B2B1 are (δ(n), p(n))-balanced isometries. Condition 2: Pair of matrices (B2,B3) is (K,ΛF ,Λ2)-random.\nBelow we give the definition of (K,ΛF ,Λ2)randomness.\nDefinition 4 ((K,ΛF ,Λ2)-randomness) A pair of matrices (Y,Z) ∈ Rn×n×Rn×n is (K,ΛF ,Λ2)-random if there exists r ∈ Rk, and a set of linear isometries φ = {φ1, ..., φn}, where φi : Rn → Rk, such that:\n• r is either a ±1-vector with i.i.d. entries or Gaussian with identity covariance matrix,\n• for every x ∈ Rn the jth element (Zx)j of Zx is of the form: rT · φj(x),\n• there exists a set of i.i.d. sub-Gaussian random variables {ρ1, ..., ρn} with sub-Gaussian norm at most K, mean 0, the same second moments and a (ΛF ,Λ2)-smooth set of matrices {Wi}i=1,...,n such that for every x = (x1, ..., xn) T , we have:\nφi(Yx) = W i(ρ1x1, ..., ρnxn) T ."
    }, {
      "heading" : "5.4.3 Proof of Lemma 1",
      "text" : "Proof: Let us first assume the GcircD2HD1-setting (analysis for Toeplitz Gaussian or Hankel Gaussian is completely analogous). In that setting, it is easy to see that one can take r to be a Gaussian vector (this vector corresponds to the first row of Gcirc). Furthermore linear mappings φi are defined as: φi((x0, x1, ..., xn−1) T ) = (xn−i, xn−i+1, ..., xi−1) T , where operations on indices are modulo n. The value of δ(n) and p(n) come from the fact that matrix HD1 is used as a (δ(n), p(n))-balanced matrix and from Remark 1. In that setting, sequence (ρ1, ..., ρn) is discrete and corresponds to the diagonal of D2. Thus we have: K = 1. To calculate ΛF and Λ2, note first that matrix W1 is defined as I and subsequent Wis are given as circulant shifts of the previous ones (i.e. each row is a circulant shift of the previous row). That observation comes directly from the circulant structure of Gcirc. Thus we have: ΛF = O( √ n) and Λ2 = O(1). The former is true since each Ai,j has O(n) nonzero entries and these are all 1s. The latter is true since each\nnontrivial Ai,j in that setting is an isometry (this is straightforward from the definition of {Wi}i=1,...,n). Finally, all other conditions regarding Wi-matrices are clearly satisfied (each column of each Wi has unit L2 norm and corresponding columns from different Wi and Wj are clearly orthogonal).\nNow let us consider the setting, where the structured matrix is of the form: √ nHD3HD2HD1. In that case, r corresponds to a discrete vector (namely, the diagonal of D3). Linear mappings φi are defined as: φi((x1, ..., xn) T ) = ( √ nhi,1x1, ..., √ nhi,nxn)\nT , where (hi,1, ..., hi,n) T is the ith row of H. One can also notice that the set {Wi}i=1,...,n is defined as: wia,b =√ nhi,aha,b. Let us first compute the Frobenius norm of the matrix Ai,j , defined based on the aforementioned sequence {Wi}i=1,...,n. We have:\n‖Ai,j‖2F = ∑\nl,t∈{1,...,n}\n( n∑ k=1 wjk,lw i k,t) 2\n= n2 ∑\nl,t∈{1,...,n}\n( n∑ k=1 hj,khk,lhi,khk,t) 2 (7)\nTo compute the expression above, note first that for r1 6= r2 we have:\nθ = ∑ k,l hr1,khr1,lhr2,khr2,l\n= ∑ k hr1,khr2,k ∑ l hr1,lhr2,l = 0, (8)\nwhere the last equality comes from fact that different rows of H are orthogonal. From the fact that θ = 0 we get:\n‖Ai,j‖2F = n2 ∑\nr=1,...,n ∑ k,l h2i,rh 2 j,rh 2 r,kh 2 r,l\n= n · n2( 1√ n )8 · n2 = n. (9)\nThus we have: ΛF ≤ √ n.\nNow we compute ‖Ai,j‖2. Notice that from the definition of Ai,j we get that\nAi,j = Ei,jFi,j , (10)\nwhere the lth row of Ei,j is of the form (hj,1h1,l, ..., hj,nhn,l) and the t\nth column of Fi,j is of the form (hi,1h1,t, ..., hi,nhn,t) T . Thus one can easily verify that Ei,j and Hi,j are isometries (since H is) thus Ai,j is also an isometry and therefore Λ2 = 1. As in the previous setting, remaining conditions regarding matrices Wi are trivially satisfied (from the basic properties of Hadamard matrices). That completes the proof."
    }, {
      "heading" : "5.4.4 Proof of Theorem 1",
      "text" : "Let us briefly give an overview of the proof before presenting it in detail. Challenges regarding proving accuracy results for structured matrices come from the fact that, for any given x ∈ Rn, different dimensions of y = Gstructx are no longer independent (as it is the case for the unstructured setting). For matrices from the family of structured spinners we can, however, show that with high probability different elements of y correspond to projections of a given vector r (see Section 3) into directions that are close to orthogonal. The “close-to-orthogonality” characteristic is obtained with the use of the Hanson-Wright inequality that focuses on concentration results regarding quadratic forms involving vectors of sub-Gaussian random variables. If r is Gaussian, then from the well-known fact that projections of the Gaussian vector into orthogonal directions are independent, we can conclude that dimensions of y are “close to independent”. If r is a discrete vector then we need to show that for n large enough, it “resembles” the Gaussian vector. This is where we need to apply the aforementioned techniques regarding multivariate Berry-Esseen-type central limit theorem results.\nProof: We will use notation from Section 3 and previous sections of the Supplement. We assume that the model with structured matrices stacked vertically, each of m rows, is applied. Without loss of generality, we can assume that we have just one block since different blocks are chosen independently. Let Gstruct be a matrix from the family of structured spinners. Let us assume that Gstruct is used by a function f operating in the d-dimensional space and let us denote by x1, . . . ,xd some fixed orthonormal basis of that space. Our first goal is to compute: y1 = Gstructx 1, ...,yd = Gstructx d. Denote by x̃i the linearly transformed version of x after applying block B1, i.e. x̃ i = B1x i. Since B1 is (δ(n), p(n))-balanced), we conclude that with probability at least: pbalanced ≥ 1− dp(n) each element of each x̃i has absolute value at most δ(n)√\nn . We shortly say that\neach x̃i is δ(n)-balanced. We call this event Ebalanced.\nNote that by the definition of structured spinners, each yi is of the form:\nyi = (rT · φ1(B2x̃i), ..., rT · φm(B2x̃i))T . (11)\nFor clarity and to reduce notation, we will assume that r is n-dimensional. To obtain results for vectors r of different dimensionality D, it suffices to replace in our analysis and theoretical statements n by D. Let us denote A = {φ1(B2x̃1), ..., φm(B2x̃1), ..., φ1(B2x̃d), ..., φm(B2x̃d))}. Our goal is to show that with high probability (in respect to random choices of B1 and B2) for all vi,vj ∈ A, i 6= j the following is true:\n|(vi)T · vj | ≤ t (12)\nfor some given 0 < t 1.\nFix some t > 0. We would like to compute the lower bound on the corresponding probability. Let us fix two vectors v1,v2 ∈ A and denote them as: v1 = φi(B2x), v 2 = φj(B2y) for some x = (x1, ..., xn) T and y = (y1, ..., yn) T . Note that we have (see denotation from Section 3):\nφi(B2x) = (w i 11ρ1x1 + ...\n+ wi1,nρnxn, ..., w i n,1ρ1x1 + ...+ w i n,nρnxn) T (13)\nand\nφj(B2y) = (w j 11ρ1y1 + ...+ w j 1,nρnyn, ...,\nwjn,1ρ1y1 + ...+ w j n,nρnyn) T . (14)\nWe obtain: (v1)T ·v2 = ∑\nl∈{1,...,n},u∈{1,...,n}\nρlρu( n∑ k=1 xlyuw i k,uw j k,l).\n(15)\nWe now show that, under assumptions from Theorem 1, the expected value of the expression above is 0. We have:\nE[(v1)T · v2] = E[ ∑\nl∈{1,...,n}\nρ2l xlyl( n∑ k=1 wik,lw j k,l)], (16)\nsince ρ1, ..., ρn are independent and have expectations equal to 0. Now notice that if i 6= j then from the assumption that corresponding columns of matrices Wi and Wj are orthogonal, we get that the above expectation is 0. Now assume that i = j. But then x and y have to be different and thus they are orthogonal (since they are taken from the orthonormal system transformed by an isometry). In that setting we get:\nE[(v1)T · v2] = E[ ∑\nl∈{1,...,n}\nρ2l xlyl( n∑ k=1 (wik,l) 2)]\n= τw n∑ l=1 xlyl = 0, (17)\nwhere τ stands for the second moment of each ρi, w is the squared L2-norm of each column of W\ni (τ and w are well defined due to the properties of structured spinners). The last inequality comes from the fact that x and y are orthogonal. Now if we define matrices Ai,j as in the definition of the model of structured spinners then we see that\n(v1)T · v2 = ∑\nl,u∈{1,...,n}\nρlρuT i,j l,u , (18)\nwhere: T i,jl,u = xlyuA i,j l,u.\nNow we will use the following inequality:\nTheorem 6 (Hanson-Wright Inequality) Let X = (X1, ..., Xn)\nT ∈ Rn be a random vector with independent components Xi which satisfy: E[Xi] = 0 and have sub-Gaussian norm at most K for some given K > 0. Let A be an n × n matrix. Then for every t ≥ 0 the following is true:\nP[XTAX− E[XTAX] > t]\n≤ 2e −cmin( t2 K4‖A‖2 F , t K2‖A‖2 ) , (19)\nwhere c is some universal positive constant.\nNote that, assuming δ(n)-balancedness, we have: ‖Ti,j‖F ≤ δ 2(n) n ‖A i,j‖F and ‖Ti,j‖2 ≤ δ 2(n) n ‖A i,j‖2.\nNow we take X = (ρ1, ..., ρn) T and A = Ti,j in the theorem above. Applying the Hanson-Wright inequality in that setting, taking the union bound over all pairs of different vectors vi,vj ∈ A (this number is exactly:( md 2 ) ) and the event Ebalanced, finally taking the union bound over all s functions fi, we conclude that with probability at least:\npgood = 1− p(n)ds − 2 ( md\n2\n) se −Ω(min( t2n2 K4Λ2 F δ4(n) , tn K2Λ2δ 2(n) )) (20)\nfor every f any two different vectors vi,vj ∈ A satisfy: |(vi)T · vj | ≤ t.\nNote that from the fact that B2B1 is (δ(n), p(n))balanced and from Equation 20, we get that with probability at least:\npright = 1− 2p(n)ds − 2 ( md\n2\n) se −Ω(min( t2n2 K4Λ2 F δ4(n) , tn K2Λ2δ 2(n) )) . (21)\nfor every f any two different vectors vi,vj ∈ A satisfy: |(vi)T · vj | ≤ t and furthermore each vi is δ(n)balanced.\nAssume now that this event happens. Consider the vector\nq′ = ((y1)T , ..., (yd)T )T ∈ Rmd. (22)\nNote that q′ can be equivalently represented as:\nq′ = (rT · v1, ..., rT · vmd), (23)\nwhere: A = {v1, ...,vmd}. From the fact that φiB2 and B1 are isometries we conclude that: ‖vi‖2 = 1 for i = 1, ....\nNow we will need the following Berry-Esseen type result for random vectors:\nTheorem 7 (Bentkus [Bentkus, 2003]) Let X1, ...,Xn be independent vectors taken from Rk with common mean E[Xi] = 0. Let S = X1 + ... + Xn. Assume that the covariance operator C2 = cov(S) is invertible. Denote βi = E[‖C−1Xi‖32] and β = β1 + ...+ βn. Let C be the set of all convex subsets of Rk. Denote ∆(C) = supA∈C |P[S ∈ A]− P[Z ∈ A]|, where Z is the multivariate Gaussian distribution with mean 0 and covariance operator C2. Then:\n∆(C) ≤ ck 14 β (24)\nfor some universal constant c.\nDenote: Xi = (riv 1 i , ..., riv k i ) T for k = md, r = (r1, ..., rn) T and vj = (vj1, ..., v j n). Note that q\n′ = X1 + ...+ Xn. Clearly we have: E[Xi] = 0 (the expectation is taken with respect to the random choice of r). Furthermore, given the choices of v1, ...,vk, random vectors X1, ..,Xn are independent.\nLet us calculate now the covariance matrix of q′. We have:\nq′i = r1v i 1 + ...+ rnv i n, (25)\nwhere: q′ = (q′1, ...,q ′ k).\nThus for i1, i2 we have:\nE[q′i1q ′ i2 ] = n∑ j=1 vi1j v i2 j E[r 2 j ] + 2 ∑ 1≤j1<j2≤n vi1j1v i2 j2 E[rj1rj2 ]\n= (vi1)T · vi2 , (26)\nwhere the last equation comes from the fact rj are either Gaussian from N (0, 1) or discrete with entries from {−1,+1} and furthermore different rjs are independent.\nTherefore if i1 = i2 = i, since each v i has unit L2-norm, we have that E[q′iq′i] = 1, (27) and for i1 6= i2 we get:\n|E[q′i1q ′ i2 ]| ≤ t. (28)\nWe conclude that the covariance matrix Σq′ of the distribution q′ is a matrix with entries 1 on the diagonal and other entries of absolute value at most t.\nFor t = ok(1) small enough and from the δ(n)balancedness of vectors v1, ...,vk we can conclude that:\nE[‖C−1Xi‖32] = O(E[‖Xi‖32]) = O( √ ( k\nn )3δ3(n)),\n(29)\nNow, using Theorem 7, we conclude that\nsup A∈C |P[q′ ∈ A]− P[Z ∈ A]| = O(k 14n · k\n3 2\nn 3 2\nδ3(n))\n= O( δ3(n)√ n k 7 4 ), (30)\nwhere Z is taken from the multivariate Gaussian distribution with covariance matrix I + E and C is the set of all convex sets. Now if we apply the above inequality to the pairwise disjoint convex sets A1, ..., Aj , where A1 ∪ ...∪Aj = f−1i (S) and l ≤ b (such sets exist form the b-convexity of f−1i (S)), take η = δ3(n)√ n k 7 4 ,\n= t = omd(1) and take n large enough, the statement of the theorem follows."
    }, {
      "heading" : "5.4.5 Proof of Theorem 2",
      "text" : "Proof: Let us assume that fi is a convex function of qfi (if fi is concave then the proof completely analogous). For any t ∈ R let St = {qfi : fi(qfi) ≤ t} for fi and St = {qf ′i : f ′ i(qf ′i ) ≤ t} for f ′ i . From the convexity assumption we get that St is a convex set. Thus we can directly apply Theorem 1 and the result regarding cdf functions follows. To obtain the result regarding the characteristic functions, notice first that we have:\nφX(t) = ∫ 1 −1 P[cos(tX) > s]ds+i ∫ 1 −1 P[sin(tX) > s]ds (31) The event {cos(tX) > s} for t 6= 0 is equivalent to: X ∈ ∪I∈II for some family of intervals I. Similar observation is true for the event {sin(tX) > s}.\nIn our scenario, from the fact that fi is bounded, we conclude that the corresponding families I are finite. Furthermore, the probability of belonging to a particular interval can be expressed by the values of the cdf function in the endpoints of that interval. From this observation and the result on cdfs that we have just obtained, the result for the characteristic functions follows immediately."
    }, {
      "heading" : "5.4.6 Proof of Theorem 3",
      "text" : "Proof: This comes directly from Theorem 1 and Lemma 1."
    }, {
      "heading" : "5.4.7 Proof of Theorem 4",
      "text" : "Proof: For clarity we will assume that the structured matrix consists of just one block of m rows and will compare its performance with the unstructured variant of m rows (the more general case when the structured matrix is obtained by stacking vertically many blocks is analogous since the blocks are chosen independently).\nConsider the two-dimensional linear space H spanned by x and y. Fix some orthonormal basis B = {u1,u2} of H. Take vectors q and q′. Note that they are 2mdimensional, where m is the number of rows of the block used in the structured setting. From Theorem 3 we conclude that will probability at least psuccess, where psuccess is as in the statement of the theorem the following holds for any convex 2m-dimensional set\nA: |P[q( ) ∈ A]− P[q′ ∈ A]| ≤ η, (32)\nwhere η = log 3(n)\nn 2 5\n. Take two corresponding entries of\nvectors v1x,y and v 2 x,y indexed by a pair (ei, ej) for some fixed i, j ∈ {1, ...,m} (for the case when the pair is not of the form (e, ej), but of a general form: (±ei,±ej) the analysis is exactly the same). Call them p1 and p2 respectively. Our goal is to compute |p1 − p2|. Notice that p1 is the probability that h(x) = ei and h(y) = ej for the unstructured setting and p2 is that probability for the structured variant.\nLet us consider now the event E1 = {h(x) = ei∧h(y) = ej}, where the setting is unstructured. Denote the corresponding event for the structured setting as E2. Denote q = (q1, ..., q2m). Assume that x = α1u 1+α2u 2 for some scalars α1, α2 > 0. Denote the unstructured Gaussian matrix by G. We have:\nGx = α1Gu 1 + α2Gu 2 (33)\nNote that we have: Gu1 = (q1, ..., qm) T and Gu2 = (qm+1, ..., q2m) T . Denote by A(ei) the set of all the points in Rm such that their angular distance to ei is at most the angular distance to all other m−1 canonical vectors. Note that this is definitely the convex set. Now denote:\nQ(ei) = {(q1, ..., q2m)T ∈ R2m : α1(q1, ..., qm) T + α2(qm+1, ..., q2m) T ∈ A(ei)}. (34)\nNote that since A(ei) is convex, we can conclude that Q(ei) is also convex. Note that\n{h(x) = ei} = {q ∈ Q(ei)}. (35)\nBy repeating the analysis for the event {h(y) = ej}, we conclude that:\n{h(x) = ei ∧ h(y) = ej} = {q ∈ Y (ei, ej)} (36)\nfor convex set Y (ei, ej) = Q(ei) ∩Q(ej). Now observe that\n|p1 − p2| = |P[q ∈ Y (ei, ej)]− P[q′ ∈ Y (ei, ej)]| (37)\nThus we have:\n|p1 − p2| ≤ |P[q ∈ Y (ei, ej)]− P[q( ) ∈ Y (ei, ej)]| + |P[q( ) ∈ Y (ei, ej)]− P[q′ ∈ Y (ei, ej)]| (38)\nTherefore we have:\n|p1 − p2| ≤ |P[q ∈ Y (ei, ej)]− P[q( ) ∈ Y (ei, ej)]|+ η. (39) Thus we just need to upper-bound:\nξ = |P[q ∈ Y (ei, ej)]− P[q( ) ∈ Y (ei, ej)]|. (40)\nDenote the covariance matrix of the distribution q( ) as I + E. Note that E is equal to 0 on the diagonal and the absolute value of all other off-diagonal entries is at most .\nDenote k = 2m. We have\nξ = |A−B| ,\nwhere A = 1\n(2π) k 2 √ det(I + E) ∫ Y (ei,ej) e− xT (I+E)−1x 2 dx\nand B = 1\n(2π) k 2 ∫ Y (ei,ej) e− xT x 2 dx.\nExpanding: (I + E)−1 = I − E + E2 − ..., noticing that |det(I + E)− 1| = O( 2m), and using the above formula, we easily get:\nξ = O( ). (41)\nThat completes the proof.\n5.4.8 b-convexity for angular kernel approximation\nLet us now consider the setting, where linear projections are used to approximate angular kernels between paris of vectors via random feature maps. In this case, the linear projection is followed by the pointwise nonlinear mapping, where the applied nonlinear mapping is a sign function. The angular kernel is retrieved from the Hamming distance between {−1,+1}-hashes obtained in such a way. Note that in this case we can assign to each pair x,y of vectors from a database a function fx,y that outputs the binary vector which length is the size of the hash and with these indices turned on for which the hashes of x and y disagree. Such a binary vector uniquely determines the Hadamard distance between the hashes. Notice that for a fixed-length hash fx,y produces only finitely many outputs. If S is a set-singleton consisting of one of the possible outputs, then one can notice (straightforwardly from the way the hash is created) that f−1x,y(S) is an intersection of the convex sets (as a function of qfx,y). Thus it is convex and thus for sets S which are singletons we can take b = 1."
    }, {
      "heading" : "5.4.9 Proof of Theorem 5",
      "text" : "In this section, we show that by learning vector r ∈ Rk from the definition above, one can approximate well any matrix M ∈ Rm×n learned by the neural network, providing that the size k or r is large enough in comparison with the number of projections and the intrinsic dimensionality d of the data X .\nTake the parametrized structured spinner matrix Mstruct ∈ Rm×n with a learnable vector r. Let M ∈ Rm×n be a matrix learned in the unstructured setting.\nLet B = {x1, ...,xd} be some orthonormal basis of the linear space, where data X is taken from.\nProof: Note that from the definition of the parametrized structured spinner model we can conclude that with probability at least p1 = 1− p(n) with respect to the choices of M1 and M2 each Mstructx i is of the form:\nMstructx i = (rT · z1(qi), ..., rT · zm(qi))T , (42)\nwhere each zj(q i) is of the form:\nzj(q i) = (wj1,1ρ1q i 1+w j 1,nρnq i n, ..., w j k,1ρ1q i 1+w j k,nρnq i n) T\n(43)\nand B′ = {q1, ...,qd} is an orthonormal basis such that: ‖qi‖∞ ≤ δ(n)√n for i = 1, ..., n.\nNote that the system of equations:\nMstructxi = Mxi (44)\nfor i = 1, ..., d has the solution in r if the vectors from the set A = {zj(qi) : j = 1, ...,m, i = 1, ...d} are independent.\nConstruct a matrix G ∈ Rmd×k, where rows are vectors from A. We want to show that rank(G) = md. It suffices to show that det(GGT ) 6= 0. Denote B = GGT . Note that Bi,j = (v\ni)Tvj , where A = {v1, ...,vmd}. Take two vectors va,vb ∈ A. Note that from the definition of A we get:\n(va)Tvb = ∑\nl∈{1,...,n},u∈{1,...,n}\nρlρuxlyu( k∑ s=1 wis,lw j s,u)\n(45) for some i, j and some vectors x = (x1, ..., xn)\nT , y = (y1, ..., yn) T . Furthermore,\n• i = j and x = y if a = b,\n• ‖x‖2 = ‖y‖2 = 1,\n• xTy = 0 or x = y and i 6= j for a 6= b.\nWe also have:\nE[(va)Tvb] = E[ ∑\nl∈{1,...,n}\nρ2l xlyl( k∑ s=1 wis,lw j s,u)]. (46)\nFrom the previous observations and the properties of matrices W1, ...,Wn we conclude that the entries of the diagonal of B are equal to 1. Furthermore, all other entries are 0 on expectation. Using Hanson-Wright\ninequality, we conclude that for any t > 0 we have: |Bi,j | ≤ t for all i 6= j with probability at least: psucc = 1−2p(n)d−2 ( md\n2\n) e −cmin( t2n2 K4Λ2 F δ4(n) , tn K2Λ2δ 2(n) ) .\nIf this is the case, we let B̃ ∈ R(md)×(md) be a matrix with diagonal entries B̃i,i = 0 and off-diagonal entries B̃i,j = −Bi,j . Furthermore, let B∗ ∈ R(md)×(md) be a matrix with diagonal entries B∗i,i = 0 and off-diagonal entries B∗i,j = t.\nFollowing a similar argument as in [Brent et al., 2014], note that B∗ = t(J − I) where J is the matrix of all ones (thus of rank 1) and I is the identity matrix. Then the eigenvalues of B∗ are t(md− 1) with multiplicity 1 and t(0− 1) with multiplicity (md− 1). We, thereby, are able to explicitly compute det(I−B∗)=(1− t(md− 1))(1 + t)md−1.\nIf ρ(B∗) ≤ 1, we can apply Theorem 1 of [Brent et al., 2014] by replacing F with B∗ and E with\nB̃. For the convenience of the reader, we state their theorem here: Let F ∈ Rn×n with non-negative entries and ρ(F ) ≤ 1. Let E ∈ Rn×n with entries | ei,j |≤ fi,j , then det(I−E) ≥ det(I− F).\nThat is: if ρ(B∗) ≤ 1, then\ndet(I−B∗) = (1− t(md− 1))(1 + t)md−1\n≤ det(I− B̃) = det(B). (47)\nThe final step is to observe that: ρ(B∗) ≤ 1 ⇐⇒ max{| t(md − 1) |, | −t |} = t(md − 1) ≤ 1 ⇐⇒ t ≤ 1md−1 . Using this result, we, hence, see that det(B) ≥ (1− t(md− 1))(1 + t)md−1 ≥ 0, in particular det(B) > 0 for t = 1md . That completes the proof."
    }, {
      "heading" : "5.4.10 Additional experiments",
      "text" : "This experiment focuses on the Newton sketch approach [Pilanci and Wainwright, 2015], a generic optimization framework. It guarantees super-linear convergence with exponentially high probability for selfconcordant functions, and a reduced computational complexity compared to the original second-order Newton method. The method relies on using a sketched version of the Hessian matrix, in place of the original one. In the subsequent experiment we show that matrices from the family of strucured spinners can be used for this purpose, thus can speed up several convex optimization problems solvers.\nWe consider the unconstrained large scale logistic regression problem, i.e. given a set of n observations {(ai, yi)}i=1..n, with ai ∈ Rd and yi ∈ {−1, 1}, find\nGram matrix reconstruction error USPST dataset for the Gaussian kernel\nGram matrix reconstruction error USPST dataset for the angular kernel\n+\nConvergence analysis\nx ∈ Rd minimizing the cost function\nf(x) = n∑ i=1 log(1 + exp(−yiaTi x)) .\nThe Newton approach to solving this optimization problem entails solving at each iteration the least squares equation ∇2f(xt)∆t = −∇f(xt), where\n∇2f(xt) =\nATdiag\n( 1\n1 + exp(−aTi x) (1− 1 1 + exp(−aTi x) )\n) A\n∈ Rd×d\nis the Hessian matrix of f(xt), A = [aT1 a T 2 · · · aTn ] ∈ Rn×d, ∆t = xt+1 − xt is the increment at iteration t and ∇f(xt) ∈ Rd is the gradient of the cost function. In [Pilanci and Wainwright, 2015] it is proposed to consider the sketched version of the least square equation, based on a Hessian square root of ∇2f(xt), denoted ∇2f(xt)1/2 =\ndiag (\n1 1+exp(−aTi x) (1− 1 1+exp(−aTi x)\n) )1/2\nA ∈ Rn×d. The least squares problem at each iteration t is of the form:(\n(St∇2f(xt)1/2)TSt∇2f(xt)1/2 ) ∆t = −∇f(xt) ,\nwhere St ∈ Rm×n is a sequence of isotropic sketch matrices. Let’s finally recall that the gradient of the cost function is\n∇f(xt) = n∑ i=1 ( 1 1 + exp(−yiaTi x) − 1 ) yiai .\nIn our experiment, the goal is to find x ∈ Rd, which minimizes the logistic regression cost, given a dataset\n{(ai, yi)}i=1..n, with ai ∈ Rd sampled according to a Gaussian centered multivariate distribution with covariance Σi,j = 0.99\n|i−j| and yi ∈ {−1, 1}, generated at random. Various sketching matrices St ∈ Rm×n are considered.\nIn Figure 6 we report the convergence of the Newton sketch algorithm, as measured by the optimality gap defined in [Pilanci and Wainwright, 2015], versus the iteration number. As expected, the structured sketched versions of the algorithm do not converge as quickly as the exact Newton-sketch approach, however various matrices from the family of structured spinners exhibit equivalent convergence properties as shown in the figure.\nWhen the dimensionality of the problem increases, the cost of computing the Hessian in the exact Newton-sketch approach becomes very large [Pilanci and Wainwright, 2015], scaling as O(nd2). The complexity of the structured Newtonsketch approach with the matrices from the family of structured spinners is instead only O(dn log(n) +md2). Figure 6 also illustrates the wall-clock times of computing single Hessian matrices and confirms that the increase in number of iterations of the Newton sketch compared to the exact sketch is compensated by the efficiency of sketched computations, in particular Hadamard-based sketches yield improvements at the lowest dimensions."
    } ],
    "references" : [ {
      "title" : "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform",
      "author" : [ "Ailon", "Chazelle", "N. 2006] Ailon", "B. Chazelle" ],
      "venue" : null,
      "citeRegEx" : "Ailon et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2006
    }, {
      "title" : "An almost optimal unrestricted fast JohnsonLindenstrauss transform",
      "author" : [ "Ailon", "Liberty", "N. 2011] Ailon", "E. Liberty" ],
      "venue" : "In SODA",
      "citeRegEx" : "Ailon et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2011
    }, {
      "title" : "Practical and optimal LSH for angular distance",
      "author" : [ "Andoni et al", "A. 2015] Andoni", "P. Indyk", "T. Laarhoven", "I.P. Razenshteyn", "L. Schmidt" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Bounds on determinants of perturbed diagonal matrices. arXiv:1401.7084",
      "author" : [ "Brent et al", "R.P. 2014] Brent", "J.H. Osborn", "W.D. Smith" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Kernel methods for deep learning",
      "author" : [ "Cho", "Saul", "Y. 2009] Cho", "L.K. Saul" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2009
    }, {
      "title" : "Binary embeddings with structured hashed projections",
      "author" : [ "Choromanska et al", "A. 2016] Choromanska", "K. Choromanski", "M. Bojarski", "T. Jebara", "S. Kumar", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Recycling randomness with structure for sublinear time kernel expansions",
      "author" : [ "Choromanski", "Sindhwani", "K. 2016] Choromanski", "V. Sindhwani" ],
      "venue" : null,
      "citeRegEx" : "Choromanski et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Choromanski et al\\.",
      "year" : 2016
    }, {
      "title" : "Scalable kernel methods via doubly stochastic gradients",
      "author" : [ "Dai et al", "B. 2014] Dai", "B. Xie", "N. He", "Y. Liang", "A. Raj", "Balcan", "M.-F", "L. Song" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "A sparse johnson: Lindenstrauss transform",
      "author" : [ "Dasgupta et al", "A. 2010] Dasgupta", "R. Kumar", "T. Sarlos" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "Random projection trees and low dimensional manifolds",
      "author" : [ "Dasgupta", "Freund", "S. 2008] Dasgupta", "Y. Freund" ],
      "venue" : null,
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2008
    }, {
      "title" : "Predicting parameters in deep learning",
      "author" : [ "Denil et al", "M. 2013] Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N.D. Freitas" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Random feature mapping with signed circulant matrix projection",
      "author" : [ "Feng et al", "C. 2015] Feng", "Q. Hu", "S. Liao" ],
      "venue" : "In IJCAI",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning. Book in preparation for",
      "author" : [ "Goodfellow et al", "I. 2016] Goodfellow", "Y. Bengio", "A. Courville" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Approximate nearest neighbor: Towards removing the curse of dimensionality",
      "author" : [ "Har-Peled et al", "S. 2012] Har-Peled", "P. Indyk", "R. Motwani" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "al. et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2012
    }, {
      "title" : "Johnson-lindenstrauss lemma for circulant matrices",
      "author" : [ "Hinrichs", "Vybral", "A. 2011] Hinrichs", "J. Vybral" ],
      "venue" : "Random Struct. Algorithms,",
      "citeRegEx" : "Hinrichs et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hinrichs et al\\.",
      "year" : 2011
    }, {
      "title" : "Kernel methods match deep neural networks on timit",
      "author" : [ "Huang et al", "2014] Huang", "P.-S", "H. Avron", "T. Sainath", "V. Sindhwani", "B. Ramabhadran" ],
      "venue" : "In ICASSP",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Extensions of Lipschitz mappings into a Hilbert space",
      "author" : [ "Johnson", "Lindenstrauss", "W. 1984] Johnson", "J. Lindenstrauss" ],
      "venue" : "In Conference in modern analysis and probability,",
      "citeRegEx" : "Johnson et al\\.,? \\Q1984\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 1984
    }, {
      "title" : "Fastfood-computing hilbert space expansions in loglinear time",
      "author" : [ "Le et al", "Q. 2013] Le", "T. Sarlós", "A. Smola" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Restructuring of deep neural network acoustic models with singular value decomposition",
      "author" : [ "J. Li" ],
      "venue" : "In Interspeech",
      "citeRegEx" : "Li,? \\Q2013\\E",
      "shortCiteRegEx" : "Li",
      "year" : 2013
    }, {
      "title" : "Dense fast random projections and lean Walsh transforms",
      "author" : [ "Liberty et al", "E. 2008] Liberty", "N. Ailon", "A. Singer" ],
      "venue" : "In RANDOM",
      "citeRegEx" : "al. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2008
    }, {
      "title" : "Convolutional kernel networks",
      "author" : [ "Mairal et al", "J. 2014] Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Randomized sketches of convex programs with sharp guarantees",
      "author" : [ "Pilanci", "Wainwright", "M. 2014] Pilanci", "M.J. Wainwright" ],
      "venue" : "In ISIT",
      "citeRegEx" : "Pilanci et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pilanci et al\\.",
      "year" : 2014
    }, {
      "title" : "Newton sketch: A linear-time optimization algorithm with linear-quadratic convergence. CoRR, abs/1505.02250",
      "author" : [ "Pilanci", "Wainwright", "M. 2015] Pilanci", "M.J. Wainwright" ],
      "venue" : null,
      "citeRegEx" : "Pilanci et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pilanci et al\\.",
      "year" : 2015
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "Rahimi", "Recht", "A. 2007] Rahimi", "B. Recht" ],
      "venue" : null,
      "citeRegEx" : "Rahimi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2007
    }, {
      "title" : "Low-rank matrix factorization for deep neural network training with high-dimensional output targets",
      "author" : [ "Sainath et al", "T.N. 2013] Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran" ],
      "venue" : "In ICASSP",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Structured transforms for small-footprint deep learning",
      "author" : [ "Sindhwani et al", "V. 2015] Sindhwani", "T.N. Sainath", "S. Kumar" ],
      "venue" : "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Spherical LSH for approximate nearest neighbor search on unit hypersphere",
      "author" : [ "Terasawa", "Tanaka", "K. 2007] Terasawa", "Y. Tanaka" ],
      "venue" : "WADS",
      "citeRegEx" : "Terasawa et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Terasawa et al\\.",
      "year" : 2007
    }, {
      "title" : "Deep fried convnets",
      "author" : [ "Yang et al", "Z. 2015] Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : ", 2013] introduces low-rank matrix factorization to reduce the size of the fully connected layers at train time, and [Li, 2013] uses low-rank factorizations with SVD after training the full model.",
      "startOffset" : 117,
      "endOffset" : 127
    } ],
    "year" : 2017,
    "abstractText" : "We consider an efficient computational framework for speeding up several machine learning algorithms with almost no loss of accuracy. The proposed framework relies on projections via structured matrices that we call Structured Spinners, which are formed as products of three structured matrix-blocks that incorporate rotations. The approach is highly generic, i.e. i) structured matrices under consideration can either be fully-randomized or learned, ii) our structured family contains as special cases all previously considered structured schemes, iii) the setting extends to the non-linear case where the projections are followed by non-linear functions, and iv) the method finds numerous applications including kernel approximations via random feature maps, dimensionality reduction algorithms, new fast cross-polytope LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more. The proposed framework comes with theoretical guarantees characterizing the capacity of the structured model in reference to its unstructured counterpart and is based on a general theoretical principle that we describe in the paper. As a consequence of our theoretical analysis, we provide the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the HD3HD2HD1 structured matrix [Andoni et al., 2015]. The exhaustive exequal contribution perimental evaluation confirms the accuracy and efficiency of structured spinners for a variety of different applications.",
    "creator" : "LaTeX with hyperref package"
  }
}