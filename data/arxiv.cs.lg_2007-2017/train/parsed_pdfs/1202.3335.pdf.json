{
  "name" : "1202.3335.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Automatic Structure Discovery for Large Source Code",
    "authors" : [ "Etienne Gagnon", "Laurie Hendren", "Tamara Munzner", "Peter Sweeney", "Heidar Pirzadeh", "Abdelwahab Hamou-Lhadj", "Timothy Lethbridge", "Luay Alawneh" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Automatic Structure Discovery for Large Source Code Page 1 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for\nLarge Source Code\nBy Sarge Rogatch\nMaster Thesis\nUniversiteit van Amsterdam,\nArtificial Intelligence, 2010\nAutomatic Structure Discovery for Large Source Code Page 2 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "Acknowledgements",
      "text" : "I would like to acknowledge the researchers and developers who are not even aware of this project, but their findings have played very significant role:  Soot developers: Raja Vall´ee-Rai, Phong Co, Etienne Gagnon, Laurie Hendren, Patrick\nLam, and others.\n TreeViz developer: Werner Randelshofer  H3 Layout author and H3Viewer developer: Tamara Munzner  Researchers of static call graph construction: Ondˇrej Lhot´ak, Vijay Sundaresan, David\nBacon, Peter Sweeney\n Researchers of Reverse Architecting: Heidar Pirzadeh, Abdelwahab Hamou-Lhadj,\nTimothy Lethbridge, Luay Alawneh\n Researchers of Min Cut related problems: Dan Gusfield, Andrew Goldberg, Maxim\nBabenko, Boris Cherkassky, Kostas Tsioutsiouliklis, Gary Flake, Robert Tarjan\nAutomatic Structure Discovery for Large Source Code Page 3 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nContents\n1 Abstract ................................................................................................................................ 6 2 Introduction .......................................................................................................................... 7\n2.1 Project Summary .......................................................................................................... 8 2.2 Global Context ........................................................................................................... 10 2.3 Relevance for Artificial Intelligence .......................................................................... 10 2.4 Problem Analysis ....................................................................................................... 11 2.5 Hypotheses ................................................................................................................. 11 2.6 Business Applications ................................................................................................ 12 2.7 Thesis Outline ............................................................................................................ 15 3 Literature and Tools Survey ............................................................................................... 16\n3.1 Source code analysis .................................................................................................. 16 3.1.1 Soot ........................................................................................................................ 17 3.1.2 Rascal ..................................................................................................................... 18 3.2 Clustering ................................................................................................................... 18 3.2.1 Particularly Considered Methods ........................................................................... 20\n3.2.1.1 Affinity Propagation ...................................................................................... 20 3.2.1.2 Clique Percolation Method ............................................................................ 22\n3.2.1.3 Based on Graph Cut ....................................................................................... 22 3.2.2 Other Clustering Methods ...................................................................................... 24\n3.2.2.1 Network Structure Indices based ................................................................... 25 3.2.2.2 Hierarchical clustering methods .................................................................... 27\n4 Background ........................................................................................................................ 29\n4.1 Max Flow & Min Cut algorithm ................................................................................ 29\n4.1.1 Goldberg’s implementation ................................................................................... 29 4.2 Min Cut Tree algorithm ............................................................................................. 30\n4.2.1 Gusfield algorithm ................................................................................................. 30 4.2.2 Community heuristic .............................................................................................. 31 4.3 Flake-Tarjan clustering .............................................................................................. 31\n4.3.1 Alpha-clustering ..................................................................................................... 31 4.3.2 Hierarchical version ............................................................................................... 32 4.4 Call Graph extraction ................................................................................................. 33 4.5 The Problem of Utility Artifacts ................................................................................ 34 4.6 Various Algorithms .................................................................................................... 36 5 Theory ................................................................................................................................ 37\n5.1 Normalization ............................................................................................................ 37 5.1.1 Directed Graph to Undirected ................................................................................ 38\n5.1.2 Leverage ................................................................................................................. 39 5.1.3 An argument against fan-out analysis .................................................................... 40 5.1.4 Lifting the Granularity ........................................................................................... 40 5.1.5 An Alternative ........................................................................................................ 43 5.2 Merging Heterogeneous Dependencies ..................................................................... 44 5.3 Alpha-search .............................................................................................................. 45 5.3.1 Search Tree ............................................................................................................ 45 5.3.2 Prioritization .......................................................................................................... 46 5.4 Hierarchizing the Partitions ....................................................................................... 47 5.5 Distributed Computation ............................................................................................ 48 5.6 Perfect Dependency Structures .................................................................................. 49 5.6.1 Maximum Spanning Tree ...................................................................................... 50\nAutomatic Structure Discovery for Large Source Code Page 4 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n5.6.2 Root Selection Heuristic ........................................................................................ 51\n6 Implementation and Specification ..................................................................................... 53 6.1 Key Choices ............................................................................................................... 54\n6.1.1 Reducing Real- to Integer- Weighted Flow Graph ................................................ 54 6.1.2 Results Presentation ............................................................................................... 54 6.2 File formats ................................................................................................................ 54 6.3 Visualization .............................................................................................................. 55 6.4 Processing Pipeline .................................................................................................... 55 7 Evaluation .......................................................................................................................... 58 7.1 Experiments ............................................................................................................... 58\n7.1.1 Analyzed Software and Dimensions ...................................................................... 58 7.2 Interpretation of the Results ....................................................................................... 59\n7.2.1 Architectural Insights ............................................................................................. 60 7.2.2 Class purpose from library neighbors .................................................................... 61 7.2.2.1 Obvious from class name ............................................................................... 62 7.2.2.2 Hardly obvious from class name .................................................................... 64 7.2.2.3 Not obvious from class name ......................................................................... 65\n7.2.2.4 Class name seems to contradict the purpose .................................................. 66 7.2.3 Classes that act together ......................................................................................... 67\n7.2.3.1 Coupled classes are in different packages ..................................................... 68 7.2.3.2 Coupled classes are in the same package ....................................................... 69 7.2.4 Suspicious overuse of a generic artifact ................................................................. 70 7.2.5 Differentiation of coupling within a package ........................................................ 72 7.2.6 A package of omnipresent classes ......................................................................... 74 7.2.7 Security attack & protection .................................................................................. 77\n7.2.8 How implemented, what does, where used ............................................................ 77 8 Further Work ...................................................................................................................... 79\n8.1 A Self-Improving Program ........................................................................................ 79 8.2 Cut-based clustering................................................................................................... 79 8.3 Connection strengths .................................................................................................. 79\n8.4 Alpha-threshold.......................................................................................................... 79 9 Conclusion ......................................................................................................................... 81\n9.1 Major challenges ........................................................................................................ 83\n9.1.1 Worst-case complexity........................................................................................... 83 9.1.2 Data extraction ....................................................................................................... 83 9.1.3 Noise in the input ................................................................................................... 83 9.1.4 Domain specifics .................................................................................................... 84\n9.1.5 Evaluation of the results......................................................................................... 84 9.2 Gain over the state of the art ...................................................................................... 84\n9.2.1 Practical contributions ........................................................................................... 84 9.2.2 Scientific contributions .......................................................................................... 86\n10 Appendices ......................................................................................................................... 88\n10.1 Evidence ..................................................................................................................... 88 10.1.1 A package of omnipresent client artifacts .......................................................... 88 10.1.2 Security mechanism: identify and circumscribe ................................................ 90 10.1.3 Subsystems ......................................................................................................... 91 10.1.4 Insight on the implementation ........................................................................... 92\n10.1.5 An overcomplicated subsystem ......................................................................... 94 10.1.6 Divide & Conquer .............................................................................................. 95 10.1.7 Utility Artifacts .................................................................................................. 98\nAutomatic Structure Discovery for Large Source Code Page 5 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.2 Statistical Measures ................................................................................................... 99\n10.2.1 Packages by ubiquity ......................................................................................... 99 10.2.2 Architectural Fitness of SE Class couplings .................................................... 101 10.3 Analyzed Source Code Examples ............................................................................ 103 10.3.1 Dependency analysis ........................................................................................ 103\n10.3.1.1 A Java class that does not use calls or external field accesses................. 103 10.3.1.2 Some classes whose dependencies are not specific at all ........................ 103 10.3.1.3 Dependencies lost at Java compile time .................................................. 105 10.3.1.4 Problematic cases ..................................................................................... 106 10.3.2 Call Graph Extraction ...................................................................................... 108\n10.3.3 Class name contradicts the purpose ................................................................. 110 10.4 Visualizations ........................................................................................................... 111\n10.4.1 State of the art tool STAN ............................................................................... 111 10.4.2 Cfinder (Clique Percolation Method) .............................................................. 113 10.4.3 Cluster Tree in Text ......................................................................................... 115\n10.4.3.1 Indentation by Height .............................................................................. 115 10.4.3.2 Bracketed presentation ............................................................................. 116 10.4.4 Sunray Representation ..................................................................................... 117 10.4.5 Sunburst ........................................................................................................... 118 10.4.6 Hyperbolic tree (plane) .................................................................................... 118 10.4.7 Circular TreeMap ............................................................................................. 120\n10.4.7.1 View on the whole program ..................................................................... 120 10.4.7.2 Parts of the architecture ........................................................................... 120 10.4.8 H3 Sphere Layout ............................................................................................ 123\n10.4.8.1 Near class CrmSOAP ............................................................................... 123\n10.4.8.2 Classes that act together ........................................................................... 124 11 References ........................................................................................................................ 127\nAutomatic Structure Discovery for Large Source Code Page 6 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "1 Abstract",
      "text" : "In this work we attempt to infer software architecture from source code automatically. We have studied and used unsupervised learning methods for this, namely clustering. The state of the art source code (structure) analysis methods and tools were explored, and the ongoing research in software reverse architecting was studied. Graph clustering based on minimum cut trees is a recent algorithm which satisfies strong theoretical criteria and performs well in practice, in terms of both speed and accuracy. Its successful applications in the domain of Web and citation graphs were reported. To our knowledge, however, there has been no application of this algorithm to the domain of reverse architecting. Moreover, most of existing software artifact clustering research addresses legacy systems in procedural languages or C++, while we aim at modern object-oriented languages and the implied character of relations between software engineering artifacts. We consider the research direction important because this clustering method allows substantially larger tasks to be solved, which particularly means that we can cluster software engineering artifacts at class-level granularity while earlier approaches were only able to do clustering at package-level on real-world software projects. Given the target domain and the supposed way of usage, a number of aspects must be researched, and these are the main contributions of our work:\n- extraction of software engineering artifacts and relations among them (using state of the art tools), and presentation of this information as a graph suitable for clustering - edge weight normalization: we have developed a directed-to-undirected graph normalization, which is specific to the domain and alleviates the widely-known and\nessential problem of utility artifacts\n- parameter (alpha) search strategy for hierarchical clustering and the algorithm for merging the partitions into the hierarchy in arbitrary order - distributed version for cloud computing - a solution for an important issue in the clustering results, namely, too many sibling\nclusters due to almost acyclic graph of relations between them, which is usually the case in the source code domain;\n- an algorithm for computing package/namespace ubiquity metric, which is based on the statistics of merge operations that occur in the cluster tree\nA prototype incorporating the above points has been implemented within this work. Experiments were performed on real-world software projects. The computed clustering hierarchies were visualized using state of the art tools, and a number of statistical metrics over the results was calculated. We have also analyzed the encountered remaining issues and provided the promising further work directions. It is not possible to infer similar architectural insights with any existing approach; an account is given in this paper. We conclude that our integrated approach is applicable to large software projects in object-oriented languages and produces meaningful information about source code structure.\nAutomatic Structure Discovery for Large Source Code Page 7 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "2 Introduction",
      "text" : "As the size of software systems increases, the algorithms and data structures of the computation no longer constitute the major design problems. When systems are constructed from many components, the organization of the overall system—the software architecture — presents a new set of design problems. This level of design has been addressed in a number of ways including informal diagrams and descriptive terms, module interconnection languages, templates and frameworks for systems that serve the needs of specific domains, and formal models of component integration mechanisms [Gar1993]. The software architecture of a program or computing system is the structure or structures of the system, which comprise software components, the externally visible properties of those components, and the relationships between them. The term also refers to documentation of a system’s software architecture. Documenting software architecture facilitates communication between stakeholders, documents early decisions about high-level design, and allows reuse of design components and patterns between projects [Bass2003].\nSoftware architecture determines the quality attributes exhibited by the system such as\nfault-tolerance, backward compatibility, extensibility, flexibility, reliability, maintainability, availability, security, usability, and other –ities. When performing Software quality analysis, we can split the features upon analysis into two principal categories:\n apparent: how the software behaves and looks\n latent: what is the potential of the software, what is in its source code and documentation\nThis is illustrated in Figure 2-1 below:\nWe can analyze the apparent features directly. But in order to analyze the latent features, we need to analyze the source code. The latter is effort-intensive if performed manually. Software\nAutomatic Structure Discovery for Large Source Code Page 8 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\narchitecture is a high-level view of the source code, describing the important facts and omitting the details.\n In the ideal case, the software architecture is available (in a document) and reflects the source code precisely. Then quality analysis performed only on the software\narchitecture will give a good coverage of the latent features (perhaps, except minor unknown bugs).\n In the worst case, only the source code is available for the software, with no documentation at all, i.e. the architecture is not known. Then we can either descend to\nmanual source code analysis, or… try to infer the software architecture from the source code automatically!\n Usually, software is not well documented: the software architecture is either too loosely described in the documentation, or only available for some parts of the software, or\nbecomes out-of-sync with the actual source code. In this case, we can utilize the available fragments for semi-supervised inference of the software architecture from the source-code (data) and the documentation (labels).\nThe dashed bidirectional arrow on the picture above denotes that:\n the actual software architecture (how the source code is written) can become inconsistent with the claimed software architecture (how it is designed in the\ndocumentation). Development in a rush, time pressure, quick wins, hacks and workarounds are some of the reasons why it usually happens so;\n even when there is no explicit software architecture (no documentation), there is some implicit software architecture which is in the source code (the actual architecture).\nSoftware maintenance and evolution is an essential part of the software life cycle. In an ideal situation, one relies on system documentation to make any change to the system that preserves system’s reliability and other quality attributes [Pir2009]. However it has been shown in practice that documentation associated with many existing systems is often incomplete, inconsistent, or even inexistent [Let2003], which makes software maintenance a tedious and humanintensive task. This is further complicated by the fact that key developers, knowledgeable of the system’s design, commonly move to new projects or companies, taking with them valuable technical and domain knowledge about the system [ACDC2000].\nThe objective of design and architecture recovery techniques is to recover high-level\ndesign views of the system such as its architecture or any other high-level design models from low-level system artifacts such as the source code. Software engineers can use these models to gain an overall understanding of the system that would help them accomplish effectively the maintenance task assigned to them [Pir2009].\nThe most dominating research area in architecture reconstruction is the inference of the\nstructural decomposition. At the lower level, one groups global declarations such as variables, routines, types, and classes into modules. At the higher level, modules are clustered into subsystems. In the result there are flat or hierarchical modules. Hierarchical modules are often called subsystems. While earlier research focused on flat modules for procedural systems, newer research addresses hierarchical modules [Kosc2009]."
    }, {
      "heading" : "2.1 Project Summary",
      "text" : "All but trivial changes in software systems require a global understanding of the system\nto be changed. Such non-trivial tasks include migrations, auditing, application integration, or impact analysis. A global understanding cannot be achieved by looking at every single statement. The source code provides a huge amount of details in which we cannot see forest for the trees. Instead, to understand large systems, we need a more coarse-grained map - software\nAutomatic Structure Discovery for Large Source Code Page 9 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\narchitecture. Software architecture reconstruction is the form of reverse engineering in which architectural information is reconstructed for an existing system. [Kosc2009]\nMany companies have huge repositories of source code, often in different programming\nlanguages. Automatic source code analysis tools also produce a lot of data with issues, metrics, and dependencies in the source code. This data has to be processed and visualized in order to give insightful information to IT quality experts, developers, users and managers.\nThere are a number source code visualization methods and tools that address this problem\nwith different levels of success. In this project we plan to apply the Artificial Inteligence techniques to the problem of source code visualization. Applications of AI (Cluster Analysis) to collaboration, word association and protein interaction analysis [Pal2005]; social network and WWW analysis [Fla2004], where also lots of data must be processed, are well known and produce fruitful results. We hope in this project to identify similar opportunities in the software visualization domain. We further realize that our task is best characterized as reverse architecting, a term appearing in the literature [Riv2000]: reverse architecting is a flavour of reverse engineering that concerns with the extraction of software architecture models from the system implementation.\nThe known Artificial Intelligence algorithms, such as clustering of graphs, either\noptimize specific statistical criteria, or exploit the underlying structure or other known characteristics of the data. In our case, the data is extracted from the source code of software. The vertices of the graph upon analysis are software engineering artifacts, where the artifacts can be of different granularity: from instructions/operators to methods/fields and then to classes, modules, packages and libraries. The edges of our graph are dependencies between the artifacts, which also have different granularities in their turn: from edges of the control flow graph, to edges of the method call and field access graphs, and then to edges of the class coupling graph, the package usage and library dependency graphs.\nWithin the scope of this project we view the following stages:\n1 Extract the SE artifacts and their relations, such as function/method call and field access graphs, inheritance/subtyping relations and metrics, which is a matter of pre-requisite tools.\nThough some uncertain decision making is needed even at this stage (e.g. polymorphism handling within static call graph extraction), we take the state of the art methods and do not focus on their improvement, however we try to use the best of available pre-requisites and use several of them in case they are non-dominated, i.e. none of them is better in all the aspects.\n2 Devise an automatic grouping of the extracted artifacts in order to achieve meaningful visualizations of them. We focus on this. 3 Visualize the results and analyze source code quality taking into account the results of clustering. These tasks are also hard - the former involves automatic graph layout and the\nlatter involves uncertain decision making - and thus left to the state of the art tools or human experts.\nWe implement a prototype called InSoAr, abbreviated from “Infer Software Architecture”. As different viewpoints specify what information should be reconstructed (in our particular case of automatic reconstruction, inferred by our program) and help to structure the reconstruction process [Kosc2009], we disambiguate the meaning in which we use “(reversed) software architecture” in the context of the goal we pursue and the major facts our program infers to nested software decomposition. This term is adopted in the existing works on reverse architecting ([END2004], [UpMJ2007], [Andre2007]). We decompose a set of software engineering artifacts (e.g. Java classes) according to the coupling between SE artifacts. We assume that nested software decomposition, in which artifacts serving similar purpose or acting in a composite mechanism are grouped together, is the most insightful and desirable for software engineers. This is confirmed in [Kosc2009] (see section 2 of the thesis), and we\nAutomatic Structure Discovery for Large Source Code Page 10 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\ndiscuss further in the thesis the works that attempt to create nested software decompositions ([Ser2008], [Maqb2007], [Rays2000], [Pate2009])."
    }, {
      "heading" : "2.2 Global Context",
      "text" : "Existing Software Visualization tools extract various metrics about the source code, like\nnumber of lines, comments, complexity and object-oriented design metrics as well as dependencies in the source code like call graphs, inheritance/subtyping and other relations first. As the next step they visualize the extracted data and present it to the user in an interactive manner, by allowing zooming and drill-down or expand/collapse. Examples of these tools are STAN [Stan2009], SQuAVisiT [Rou2007], DA4Java [Pin2008] and Rascal [Kli2009, also personal communication with Paul Klint].\nA common problem of such tools is that there are too many SE artifacts to look at\neverything at once. According to [Stan2009]: “To get useful graphs, we have to carefully select the perspectives and scopes. Otherwise we’ll end up with very big and clumsy graphs. For example, we may want to look at an artifact to see how its contained artifacts interact or how the artifact itself interacts with the rest of the application”. The DA4Java tool [Pin2008], attempts to solve this problem by allowing the user to add or remove the artifacts the user wants to see in the visualization, and, also, to drill down/up from containing artifacts to the contained artifacts (e.g. from packages to classes).\nWe want to solve the problem of the overwhelming number of artifacts by grouping them\nusing AI techniques such as clustering, learning and classification, so that a reasonable number of groups is presented to the user.\nFrom the available AI techniques graph clustering is known to be applied in the software\nvisualization domain. It seems that many clusterizers of software artifacts are employing nonMinCut based techniques, refer to [Maqb2007] for a broad review of the existing clusterizers and [Ser2008] for a recent particular clusterizer. There is some grounding for this, as according to [Ding2001] a MinCut-based clustering algorithm tends to produce skewed cuts. In the other words: a very small subgraph is cut away in each cut. However, this might not be a problem for graphs from the domain of source code analysis. Another motivation for applying MinCutbased clustering algorithms in our domain arises due to the fact that software normally has clearly-defined entry points (sources, in terms of MaxFlow-like algorithms) and less clearlydefined exit points (sinks, in terms of MaxFlow). A good choice of sink points is also a matter of research, while the current candidates in mind are: library functions, dead-end functions (which do not call any others), and the program runtime termination points.\nFor extraction of Software Engineering artifacts we plan to use the existing tools such\nas Soot (Sable group of McGill University) [Soot1999] and Rascal (CWI) [Kli2009]. Soot builds a complete model of the program upon analysis, either from the source code or from the compiled Java byte-code. The main value of this tool for our project is that it implements some heuristics for static call graph extraction."
    }, {
      "heading" : "2.3 Relevance for Artificial Intelligence",
      "text" : "Many AI methods require parameters, and the performance of the methods depends on\nthe choice of parameter values. The best choice of methods or parameter values is almost always domain-specific, usually problem-specific and even sometimes data-specific. We want to investigate these peculiarities for the domain of source code visualization and reverse engineering.\nAutomatic clustering algorithms have a rich history in the artificial intelligence\nliterature, and in not so recent years have been applied to understanding programs written in procedural languages [Man1998]. The purpose of an automatic clustering algorithm in artificial intelligence is to group together similar entities. Automatic clustering algorithms are used\nAutomatic Structure Discovery for Large Source Code Page 11 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nwithin the context of program understanding to discover the structure (architecture) of the program under study [Rays2000]. One example of the specifics of software clustering is that we want to cluster entities based on their unity of purpose rather than their unity of form. It is not useful to cluster all four-letter variables together, even though they are similar [Rays2000]. In this project we attempt to regard relations that expose the unity of purpose, and we use the notion of similarity in this meaning.\nOne of long-term goals of Artificial Intelligence is creation of a self-improving\nprogram. Perhaps, this can be approached by implementing a program that does reverse engineering and then forward engineering of its own source code. In between there must be high-level understanding of the program, its architecture. It is not clear, what understanding is, but seems it has much in common with the ability to visualize, explain to others and predict behavior. This project is a small step towards automatic comprehension of software by software."
    }, {
      "heading" : "2.4 Problem Analysis",
      "text" : "The particular problem of interest is inference of the architecture and different facts about\nsoftware from its source code. It is desired that the high-level view on software source code is provided to human experts automatically, omitting the details that do not need human attention at this stage.\nSoftware products often lack documentation on the design of their source code, or\nsoftware architecture. Although full-fledged documentation can only be created by human designers, an automatic inference tool can also provide some high-level overview of source code by means of grouping, generalization and abstraction. Such a tool could also point the places where human experts should pay attention to. Semi-automatic inference can be used when documentation is partially available.\nThe key step that we make in this project is graph clustering, which splits the source code\ninto parts, i.e. performs grouping. We suppose that this will help with the generalization over software artifacts and the detection of layers of abstraction within the source code.\nBy generalization we mean the detection of the common purpose which software artifacts\nin a group serve. One way to determine the purpose is by exploiting the linguistic knowledge found in the source code, such as identifier names and comments. This was done in [Kuhn2007], however they did not partition software engineering artifacts into structurally coupled groups prior to linguistic information retrieval. We believe that formal relations (e.g. function calls or variable accesses) should be taken into account first, and then the linguistic relations should be analyzed within the identified (e.g. by means of call graph clustering) groups, rather than doing vocabulary analysis across the whole source code.\nBy abstraction we mean the identification of abstraction layers within the source code.\nFor example, if all indirect (i.e. mediated) calls from group A and B to groups C and D go through group E, and there are no direct calls from {A, B} to {C, D}, then it is likely that group E serves as a layer of abstraction between groups {A, B} and {C,D}.\nThe aforementioned decisions need uncertain inference and error/noise tolerance. Thus\nwe think that the problem should be approached with AI techniques."
    }, {
      "heading" : "2.5 Hypotheses",
      "text" : "In the beginning of this project we had hypotheses as listed below.\n1) By applying Cluster Analysis to a graph of SE artifacts and their dependencies, we can save human efforts on some common tasks within Software Structure Analysis, namely\nidentification of coupled artifacts and breaking down the system’s complexity.\nAutomatic Structure Discovery for Large Source Code Page 12 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n2) Partitional clustering will provide better nested software decompositions than the widely used (and to our knowledge, the only for clustering large number of SE\nartifacts) hierarchical clustering, which is in fact a greedy algorithm\n3) Semi-supervised learning of software architecture from source code (unlabelled data) and architecture design (labeled data) can be used to improve results over usual\nclustering, which is unsupervised learning A few comments for this hypothesis:\n Here we assume that the explicit architecture, i.e. the design documentation\ncreated by human experts, provides some partitioning of SE artifacts into groups. By means of Cluster Analysis we try to infer the implicit architecture, which is the architecture of how the source code is actually written, and we also get some partitioning of SE artifacts into groups.\n It is obvious that this task can also be viewed as classification: for each SE\nartifact the learning algorithm outputs the architectural component (or, in terms of Machine Learning, the class, but do not confuse with SE classes) which the artifact belongs to, and perhaps also the certainty of this decision.\n When the explicit architecture is only partially available (which is always the\ncase, except for completely documented trivial software, where ‘completely’ stands for ‘each SE artifact’), we can think of several approaches for classifier training:\ni. Train the classifier on the documented part of this software ii. Train the classifier on other similar software which is better documented\niii. Train the classifier on library functions, which are usually documented best of all (e.g. Java & Sun libraries).\n4) Improvements in call graph clustering results can be achieved through integration with some of the following: class inheritance hierarchy, shared data graph, control flow\ngraph, package/namespace structure, identifier names (text), supervision (documented architecture pieces), etc.\nEvidence for the first hypothesis is provided mostly in section 7.2. The second hypothesis is discussed theoretically, mostly in sections 3.2.2.2 and 3.2.1.3.\nWe have only discussed hypothesis 3, as implementation and experiments would take\ntoo much time. In the resulting software decompositions we can see that library SE artifacts indeed give insight about the purpose of client-code artifacts appearing nearby. In section 5.2 we provide evidence and argue theoretically in support of hypothesis 3: proper weighting of different kinds of relations can be learnt on training data (source code with known nested decomposition) and then applied to novel software.\nFor hypothesis 4, empirical results show that indeed the resulting hierarchical structure\nlooks better when multiple kinds of relations are given on input of the clustering algorithm, and the reasons are theoretically discussed in section 5.2."
    }, {
      "heading" : "2.6 Business Applications",
      "text" : "Consider a company that is proposed to do maintenance for a software product. Having\na visualization tool, the company can analyze the quality of the source code, so the company knows the risks associated with the software and can estimate the difficulty and expensiveness of its maintenance more accurately. To be able to do this we need to:\n1 extract the architecture (even when the architecture was not designed from the very beginning, there is always the actual implicit architecture, which is how the source code\nactually written);\n2 and in some way derive the evaluation of the source code from the result of step 1.\nAutomatic Structure Discovery for Large Source Code Page 13 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nBoth steps are problematic in the sense that they require uncertain inference and decision making, which is a task of Artificial Intelligence in case we want to do this automatically. Within the scope of this project we focus on step 1 and leave step 2 to a human expert.\nIn this section we consider the value of this project for potential target groups, and then\nfor particular stakeholders of a project: administrators, managers, developers, testers and users. But first of all, below is the grounding of why reverse architecting software is valuable. According to [Riv2000]:\n Software Development domain is characterized by fast changing requirements. Developers are forced to evolve the systems very quickly. For this reason, the\ndocumentation about the internal architecture becomes rapidly obsolete. To make fast changes to the software system, developers need a clear understanding of the underlying architecture of the products.\n To reduce costs several products share software components developed in different projects. This generates many dependencies that are often unclear and hard to manage.\nDevelopers need to analyze such dependency information either when reusing the components or when testing the products. They often look for “the big picture” of the system that is a clear view of the major system components and their interactions. They also need a clear and updated description of the interfaces (and services) provided by the components.\n The developers of a particular component need to know its clients in order to analyze the impact of changes on the users. They also want to be able to inform their customers\nof the changes and to discuss with them the future evolution of the component.\n When defining a common architecture for the product family, architects have to identify the commonalties and variable parts of the products. This requires comparing\nthe architectures of several products.\n The developers need a quick method for extracting the architecture models of the system and to analyze them. The teams often use an iterative development process that\nis characterized by frequent releases. The architecture models could be a valuable input for reviewing the system at the end of each iteration. This increases the need for an automatic approach to extract the architecture model.\nIn our view, if a visualization tool is developed, one that can reverse-architect a software from source code and provide concise accurate high-level information to its users, the effect of employing this tool will be comparable to the effect of moving from assembly language to C in the past.\nBelow are the benefits that a tool allowing to reverse-engineer and visualize the\nsoftware architecture from the source code provides to different stakeholders. It is likely that the list is far not complete, and that it will be extended, improved and detailed in the process of development of the tool, as new facts become apparent. Administrators Reduced expenses\n The development team is more productive. Reduced risks\n Consider a company that is proposed to do maintenance for a software product. This company has a tool to analyze the quality of the\nsoftware more precisely, so the company knows the risks associated with the software and can estimate the difficulty of its maintenance more accurately.\nIncreased product lifetime and safety factor\n The curse of complexity is alleviated. Better decisions\nAutomatic Structure Discovery for Large Source Code Page 14 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n The company can estimate the quality of software products more precisely, thus knows the situation better, and this leads to better\ndecisions.\nManagers Better control of task progress\n Functionality + Quality = Efforts + Duration Now there is a better way to check whether a task was performed in\nfewer efforts by means of reducing the quality.\nNew team members are trained faster\n Usually, developers which are new to the project spend much time studying the existing system, especially when little documentation is\navailable.\nFewer obstacles to “introduce new resource” action:\n When while checking an ongoing project a manager determines that the project is likely not to fit the deadline, and the deadline is strict,\nthe possible actions to alleviate this are: either shrink the functionality, or reduce the quality, or add a new developer. However, the latter action is usually problematic due to the necessity to train the developer on this particular project.\nIt is easier to recover after “reduce quality” action. When producing a Work Breakdown Structure, it is easier to identify:\n reusage capabilities\n task interdependencies\n task workloads Finally, it is easier to manage the changing requirements to a software product.\nDevelopers The tool helps developers to:\n take architectural decisions\n identify the proper usage for the existing code and the original intents of its implementers\n search for possible side-effects of executing some statement, introducing a fix or new feature\n identify the causes of a bug The tool also partially relieves developers from maintaining documentation, given that the source code is good: logical, consistent and self-explanatory.\nTesters Testers will get a way to better\n identify the possible control paths\n determine the weak places of the software When a bug reported by a client is not obvious to reproduce, taking a look at the visualization of the software can help to figure out why.\nUsers Users are provided with better services.\n Support is more prompt: o Issues are fixed faster, as it is easier for developers to find the\ncauses and to devise the fixes.\no Requested new features are implemented faster, as it is easier for developers to understand the existing system, and the\nreusage recall is higher.\n More powerful software, because: o developers can build more complex systems\n More stable software, because it is easier to\nAutomatic Structure Discovery for Large Source Code Page 15 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\no determine the weak parts of the software by developers and testers o determine the possible control paths and cover them with tests"
    }, {
      "heading" : "2.7 Thesis Outline",
      "text" : "In this section we have introduced the state of the art in the area of Reverse Architecting and placed Artificial Intelligence techniques in this context. Further we discuss the candidate AI techniques in section 3, analyze the weaknesses of the existing approaches and give counterexamples. We also discuss state of the art in source code analysis in this section, as we need some source code analysis in order to extract input data for our approach.\nSection 4 provides the background material for proper understanding of our\ncontributions by the reader.\nThe theory we developed in order to implement the project is given in section 5. As we\noften used problem-solving approach, we are not always confident about the originality, and optimality or superiority of the solutions we devised. Certainly, we admit this as a weakness of our paper in section 9. The most-likely to be original, optimal or superior solutions are put in section 5. The solutions suspected to be non-original are discussed together with the background material in section 4. The solutions known or likely to be far from optimal are discussed together with our experiments (section 7) or implementation and specification (section 6). We do not implement to our knowledge inferior solutions, if efforts-to-value tradeoff allows given the effort limit.\nThe empirical evidence for the quality of clustering and meaningfulness of the\nproduced software decompositions is given in section 7.2. We give further visual examples, evidence and proofs in the appendix. Please, note that the appendix is also an important part of the work, as we put some parts there in order not to overload the textual information of the thesis with huge visual examples and source code. We also discuss those visual examples partially in the appendix, though in a less formal way, necessary for their comprehension.\nWe give a list of problems that we could not solve within the limits of this project, see\nsection 8. Finally, we summarize our contributions and discuss the approach in section 9.\nAutomatic Structure Discovery for Large Source Code Page 16 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "3 Literature and Tools Survey",
      "text" : "Architecture reconstruction typically involves three steps [Kosc2009]:\n1 Extract raw data on the system. 2 Apply the appropriate abstraction technique. 3 Present or visualize the information.\nWithin this project we perform integrative task over the listed above steps. We select suitable state of the art methods and tools, adhering to realistic estimations on the practical needs, port the methods and tools from other domain into ours and solve the arising issues."
    }, {
      "heading" : "3.1 Source code analysis",
      "text" : "In the literature they distinguish two types of source code analysis:\n static (the subject program is NOT run)\n dynamic (the subject program is executed) Source code analysis is the process of extracting information about a program from its source code or artifacts (e.g., from Java byte code or execution traces) generated from the source code using automatic tools. Source code is any static, textual, human readable, fully executable description of a computer program that can be compiled automatically into an executable form. To support dynamic analysis the description can include documents needed to execute or compile the program, such as program inputs [Bink2007]. According to [Kli2009], source code analysis is also a form of programming.\nCall graphs depict the static, caller-callee relation between “functions” in a program.\nWith most source/target languages supporting functions as the primitive unit of composition, call graphs naturally form the fundamental control flow representation available to understand/develop software. They are also the substrate on which various interprocedural analyses are performed and are integral part of program comprehension/testing [Nara2008].\nIn this project we consider call graph as the most important source of relations between\nsoftware engineering artifacts. Thus most of our interest in source code analysis falls into call graph extraction. This is also the most difficult, and computer time- and space-consuming operation among all the extractions we perform as pre-requisites. Extraction of other relations, such as inheritance, field access and type usage is more straightforward and mostly reduces to parsing. Call graphs are commonly used as input for automatic clustering algorithms, the goal of which is to extract the high level structure of the program under study. Determining call graph for a procedural program is fairly simple. However, this is not the case for programs written in object-oriented languages, due to polymorphism. A number of algorithms for the static construction of an object-oriented program’s call graph have been developed in the compiler optimization literature in recent years. [Rays2000]\nIn the context of software clustering, we attempt to infer the unity of purpose of entities\nbased on their relations, commonly represented in such abstractions as data dependency graphs and call graphs [Rays2000]. The latter paper experiments with 3 most common algorithms for the static construction of the call graph of an object-oriented program, available at that time:\n Naïve\nThis algorithm assumes that the actual and the implementing types are the same as the declared type. The benefits are: no extra analysis, sufficient for the purposes of a non-optimizing compiler, and very simple.\n Class Hierarchy Analysis (CHA) [Diw1996] is a whole-program analysis that\ndetermines the actual and implementing types for each method invocation based on the type structure of the program. The whole program is not always available for analysis, not only for trivial (but common) reasons of absence of a .jar file, but also\nAutomatic Structure Discovery for Large Source Code Page 17 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\ndue to features such as reflection and remote method invocation. CHA is flow and context insensitive.\n Rapid Type Analysis (RTA) [Bac1996] uses the set of instantiated types to\neliminate spurious invocation arcs from the graph produced by CHA. This analysis is particularly effective when a program is only using a small portion of a large library, which is often the case in Java [Rays2000]. This is also the case in our project: out of 7.5K of classes upon analysis, 6.5K are library classes. Studies have shown that RTA is a significant improvement over CHA, often resolving 80% of the polymorphic invocations to a single target [Bac1996].\nAt the time of [Rays2000] experiments, RTA was considered to be the best practical algorithm for call graph construction in object-oriented languages. The improved methods we are using in this project, Spark [Lho2003] and VTA [Sun1999] [Kwo2000], were under development.\nThe authors of [Rays2000] were only able to conclude that the choice of call graph\nconstruction algorithm does indeed affect the automatic clustering process, but not whether clustering of more accurate call graphs will produce more accurate clustering results. Assessment of both call graph and clustering accuracy is a fundamental difficulty.\n3.1.1 Soot\nSoot [Soot1999] is a framework originally aimed at optimizing Java bytecode. However, we use it first of all for parsing and obtaining structured in-memory representation of the source code upon our analysis. The framework is open-source software implemented in Java, and this is important as it gives an opportunity to modify the source code of the tool in order to tune it for our needs.\nSoot supports several intermediate representations for Java bytecode analyzed with it:\nBaf, a streamlined representation of bytecode which is simple to manipulate; Jimple, a typed 3- address intermediate representation suitable for optimization; and Grimp, an aggregated version of Jimple suitable for decompilation [Soot1999]. Another intermediate representation implemented in the recent years is Shimple, a Static Single Assignment-form version of the Jimple representation. SSA-form guarantees that each local variable has a single static point of definition which significantly simplifies a number of analyses [EiNi2008].\nOur fact extraction and the prerequisites for our analyses are built on top of the Jimple\nintermediate representation. The prerequisites are call graph extractors, namely, Variable Type Analysis (VTA) [Sun1999] and Spark [Lho2003], a flexible framework for experimenting with points-to analyses for Java. Soot can analyze isolated source/bytecode files, but for call graph extraction whole-program mode [EiNi2008, p.19] is required. In this mode Soot first reads all class files that are required by an application, by starting with the main root class or all the classes supplied in the directories to process, and recursively loading all classes used in each newly loaded class. The complete application means that all the entailed libraries, including java system libraries, are processed and represented in memory structurally. This causes crucial performance and scalability issues, as it was tricky to make Soot fit in 2GB RAM while processing the software projects we further used in our experiments on clustering.\nAs each class is read, it is converted into the Jimple intermediate representation. After\nconversion, each class is stored in an instance of a SootClass, which in turn contains information like its name, signature (fully-qualified name), its superclass, a list of interfaces\nthat it implements, and a collection of SootField`s and SootMethod`s. Each SootMethod contains information like its (fully-qualified) name, modifier, parameters, locals, return type and a list of Jimple 3-address code instructions. All parameters and locals have declared types. Soot can produce Jimple intermediate representation directly from the Java bytecode in class files, and not only from high-level Java programs, thus we can analyze Java bytecode that has been produced by any compiler, optimizer, or other tool. [Sun1999]\nAutomatic Structure Discovery for Large Source Code Page 18 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n3.1.2 Rascal\nIn this section we discuss recent state of the art developments in source code analysis and manipulation (SCAM) domain, placing our project and research in this context. Most of the research addresses explicit facts, while we aim at identification of the implicit facts (architecture), as there is an inference step (namely, clustering) between SE artifact relations and presentation to a user.\nSCAM is a large and diverse area both conceptually and technologically. Many\nautomated software engineering tools require tight integration of techniques for source code analysis and manipulation, but integrated facilities that combine both domains are scarce because different computational paradigms fit each domain best. Both domains depend on a wide range of concepts such as grammars and parsing, abstract syntax trees, pattern matching, generalized tree traversal, constraint solving, type inference, high fidelity transformations, slicing, abstract interpretation, model checking, and abstract state machine. Rascal is a domainspecific language that integrates source code analysis and manipulation at the conceptual, syntactic, semantic and technical level [Kli2009]. The goals of Rascal are:\n To remove the cognitive and computational overhead of integrating analysis and\ntransformation tools\n To provide a safe and interactive environment for constructing and experimenting\nwith large and complicated source code analyses and transformations such as, for instance, needed for refactorings\n To be easily understandable by a large group of computer programming experts\nVisualization of software engineering artifacts is important. CWI/SEN1 research group is developing Rascal within The Meta-Environment, a framework for language development, source code analysis and source code transformation: http://www.meta-environment.org/ . This framework could use the results of this project, providing input and taking output. There is no call graph clustering in the framework yet. The research group is currently developing a visualization framework, which could graphically illustrate the results of this project too."
    }, {
      "heading" : "3.2 Clustering",
      "text" : "Clustering is a fundamental task in machine learning [Ra2007]. Given a set of data\ninstances, the goal is to group them in a meaningful way, with the interpretation of grouping dictated by the domain. In the context of relational data sets – that is, data whose instances are connected by a link structure representing domain-specific relationships or statistical dependency – the clustering task becomes a means for identifying communities within networks. For example, in the bibliographic domain explored by both [Ra2007] and [Fla2004], they find networks of scientific papers. Interpreted as a graph, vertices (papers) are connected by an edge when one cites the other. Given a specific paper (or group of papers), one may try to find out more about the subject matter by pouring through the works cited, and perhaps the works they cite as well. However, for a sufficiently large network, the number of papers to investigate quickly becomes overwhelming. By clustering the graph, we can identify the community of relevant works surrounding the paper in question.\nAn example of value that clustering can bring into graph comprehension is illustrated in\nFigure 3-1 below. Both pictures on the left and on the right are adjacency matrices of the same graph. However, the vertices (which are row and column labels) in the right picture are ordered according to the cluster they belong to, so that vertices of the same cluster go subsequently. The matrix on the right is almost quasi-diagonal, thus we can look at the contracted graph of 17 vertices (one vertex per cluster) instead of the original graph of 210 vertices. The edges of the contracted graph will reflect the exceptions that prevent the adjacency matrix on the right from being strictly quasi-diagonal, and the weights of those edges reflect the cardinality of the exceptions.\nAutomatic Structure Discovery for Large Source Code Page 19 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nNo single definition of a cluster in graphs is universally accepted [Sch2007], thus there\nare some intuitive desirable cluster properties mentioned in the literature. In the setting of graphs, each cluster should be connected: there should be at least one, preferably several paths connecting each pair of vertices within a cluster. If a vertex [u] cannot be reached from a vertex [v], they should not be grouped in the same cluster. Furthermore, the paths should be internal to the cluster: in addition to the vertex set [C] being connected in [G], the subgraph induced by [C] should be connected in itself, meaning that it is not sufficient for two vertices [v] and [u] in [C] to be connected by a path that passes through vertices in [V\\C], but they also need to be connected by a path that only visits vertices included in [C]. As a consequence, when clustering a disconnected graph with known components, the clustering should usually be conducted on each component separately, unless some global restriction on the resulting clusters is imposed. In some applications, one may wish to obtain clusters of similar order and/or density, in which case the clusters computed in one component also influence the clusterings of other components. This also makes sense in the domain of software engineering artifacts clustering when we are analyzing disjoint libraries with intent to look at their architecture from the same level of abstraction.\nIt is generally agreed upon that a subset of vertices forms a good cluster if the induced\nsubgraph is dense, but there are relatively few connections from the included vertices to vertices in the rest of the graph [Sch2007]. Still, there are multiple possible ways of defining density. At this point there are two things worthy to notice:\n1 [Sch2007] uses the notion of cut size, c(C, V\\C) to measure the sparsity of connections from cluster [C] to the rest of the graph, and this matches to the central clustering\napproach we use in our work: Graph Clustering based on Minimum Cut Trees [Fla2004]. Minimum cuts play central role there in both inter-cluster and intra-cluster connection density evaluation.\n2 For calculation of both inter- and intra- cluster densities, in the formulas of [Sch2007, page 33] they use “maximum number of edges possible” as the denominator. However,\nthey consider the number of edges in a complete graph as the maximum number of edges possible, which can be wrong due to the specific of the underlying data (not all the graph configurations are possible, i.e. the denominator must be far less than the number of edges in a complete graph), and this can cause density estimation problems and skew the results.\nConsidering the connectivity and density requirements given in [Sch2007], semantically useful clusters lie somewhere in between the two extremes: the loosest – a connected component, and the strictest – a maximal clique. Connected components are easily computed in O(|V|+|E|) time, while clique detection is NP-complete. An example of good (left), worse (middle) and bad (right) cluster is given in Figure 3-2 below.\n The cluster on the left is of good quality, dense and introvert.\nAutomatic Structure Discovery for Large Source Code Page 20 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nIt is not always clear whether each vertex should be assigned fully to a cluster or could it instead have different “levels of membership” in several clusters? [Sch2007] In Java classes clustering, such a situation is easily imaginable: a class can be converting data from XML document into a database, and hence could be clustered into “XML” with 0.3 membership, for example, and “database” with a membership level of 0.4. The coefficients can be normalized\n either per-cluster: the sum of all membership levels over all classes belonging to this\ncluster equals to 1.0\n or per-class: the sum of all membership levels over all the clusters which this class\nbelongs to equals to 1.0\nA solution, hierarchical disjoint clustering, would sometimes create a supercluster (parent or indirect ancestor) to include all classes related to XML and database, but the downside is that there can be classes dealing with database but having no relation to XML whatsoever. This is the solution adopted in our work; however, due to the aforementioned downside, an alternative seems interesting too: fuzzy graph clustering [Dong2006]. In a fuzzy graph, each edge is assigned a degree of presence in the graph. Different non-fuzzy graphs can be obtained by leaving only the edges with presence level exceeding a certain threshold. The algorithm of [Dong2006] exploits a connectivity property of fuzzy graphs. It first preclusters the data into subclusters based on the distance measure, after which a fuzzy graph is constructed for each subcluster and a thresholded non-fuzzy graph for the resulting graph is used to define what constitutes a cluster.\n3.2.1 Particularly Considered Methods\nFlake-Tarjan clustering, also known as graph clustering based on minimum cut trees [Fla2004], was used as the core clustering approach in this work. However, a number of other clustering methods were considered within our research too. It was concluded that all of them are either inapplicable due to our problem size (in terms of algorithmic complexity), or inferior to FlakeTarjan clustering in terms of clustering performance (the accuracy of the results and the usefulness of the measure which the methods are aiming to optimize)."
    }, {
      "heading" : "3.2.1.1 Affinity Propagation",
      "text" : "Affinity propagation [Fre2007] is a clustering algorithm that takes as input measures of similarity between pairs of data points and simultaneously considers all data points as potential exemplars. Real-valued messages are exchanged between data points until a high-quality set of\nexemplars and corresponding clusters gradually emerges. A derivation of the affinity propagation algorithm stemming form an alternative, yet equivalent, graphical model is proposed in [Giv2009]. The new model allows easier derivations of message updates for extensions and modifications of the standard affinity propagation algorithm.\nIn the initial set of data points (in our case, software engineering artifacts, e.g. Java\nclasses), affinity propagation (AP) pursues the goal of finding a subset of exemplar points that best describe the data. AP associates each data point with one exemplar, resulting in a partitioning of the whole data set into clusters. The measure which AP maximizes is the overall sum of similarities between data points and their exemplars, called net similarity. It is important to note why a degenerate solution doesn’t occur. The net similarity is not maximized when every data point is assigned to its own singleton exemplar because it is usually the case that a gain in similarity a data point achieves by assigning itself to an existing exemplar is\nhigher than the preference value. The preference of point i , called p(i) or i)s(i, , is the a priori\nsuitability of point i to serve as an exemplar. Preferences can be set to a global (shared) value, or customized for particular data points. High values of the preferences will cause affinity propagation to find many exemplars (clusters), while low values will lead to a small number of exemplars (clusters). A good initial choice for the preference is the minimum similarity or the median similarity [Fre2007].\nAffinity propagation iteratively improves the clustering result (net similarity), and the\ntime required for one iteration is asymptotically equal to the number of edges in the similarity graph. In their experiments [Fre2007] authors used some fixed number of iterations, but one can also run iterations until some pre-defined time limit is exceeded.\nNormally, the algorithm takes NxN adjacency matrix on input, but we cannot allow this\nin our problem because such a solution is not scalable to large software projects. In a large software project there can be 0.1 millions of software engineering artifacts (e.g., Java classes), but only 1-2 millions of relations among them (method calls, field accesses, inheritance, etc), i.e. the graph upon analysis is very sparse. On medium-size software projects, consisting of no more than 10 000 artifacts at the selected granularity (e.g. classes), however, it is feasible to compute an adjacency matrix using some transitive formula for similarity of artifacts which do not have a direct edge in the initial graph of relations, thus it is worthy to mention the practical constraints for affinity propagation. The number of scalar computations per iteration is equal to a constant times the number of input similarities, where in practice the constant is approximately 1000. The number of real numbers that need to be stored is equal to 8 times the number of input similarities. So, the number of data points is usually limited by memory, because we need N 2 similarities for N data points.\nThough affinity propagation has a sparse version, the variety of the resulting clustering\nconfigurations becomes very limited in this case. In a sparse version, the similarity between any two points not connected in the input graph is viewed as negative infinity by the algorithm. Below are two main consequences of this:\n If there is no edge between point A and point B in the input graph, then point A will\nnever be selected as an exemplar for point B.\n If there is no path of length no more than 2 between points C and D, then affinity\npropagation will never assign points C and D into the same cluster\nThis is illustrated on Figure 3-3 below.\nA B C\nD\nAutomatic Structure Discovery for Large Source Code Page 22 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nWhen clustering software engineering artifacts, e.g. Java/C# classes, it seems reasonable that sometimes we want some classes to get into the same cluster even though there is no path of length no more than 2 between them. We conclude that affinity propagation is not applicable to our problem thus."
    }, {
      "heading" : "3.2.1.2 Clique Percolation Method",
      "text" : "Cfinder is a tool for finding and visualizing overlapping groups of nodes in networks, based on\nthe Clique Percolation Method [Pal2005]. Within this project, we used it “as is” in an attempt to cluster software engineering artifacts using state of the art tools from a different domain, namely, social network analysis. In contrast to Cfinder/CPM, other existing community finders for large networks, including the core method used in our project, find disjoint communities. According to [Pal2005], most of the actual networks are made of highly overlapping cohesive groups of nodes.\nThough Cfinder is claimed to be “fast and efficient method for clustering data\nrepresented by large graphs, such as genetic or social networks and microarray data” and “very efficient for locating the cliques of large sparse graphs”, our experiments showed that it is not applicable to our domain de facto, for both scalability and result usefulness issues. When our original graph, containing 7K vertices (Java classes) and 1M edges (various relations), was given on input of Cfinder, it did not produce any results in reasonable time. When we reduced the graph to client-code artifacts only, resulting in 1K classes and 10K edges, Cfinder still did not finish computations after 16 hours; however, at least it produced some results which could be visualized with Cfinder. It produced one community with several cliques in it, see Figure 10-8 in the appendix. The selected nodes belong to the same clique. Unfortunately, hardly any architectural insight can be captured from this picture even when zoomed, see Figure 10-9 in the appendix.\nWe suppose that the reason for such poor behavior of Clique Percolation Method in our\ndomain, as opposed to collaboration, word association, protein interaction and social networks [Pal2005], resides in the specific of our data, namely, software engineering artifacts and relations between them. Our cliques are often huge and nested, thus the computational complexity of CPM approaches its worst case.\nCertainly, we have studied Cfinder too superficially, and perhaps there is indeed a way\nto reduce our problem into one feasible to solve with CPM, but after spending reasonable amount of efforts on this grouping approach we conclude that either it is inapplicable, or much more efforts must be spent in order to get useful results with it."
    }, {
      "heading" : "3.2.1.3 Based on Graph Cut",
      "text" : "A group of clustering approaches is based on graph cut. The problem of minimum cut in a graph is well studied in computer science. An exact solution can be computed in reasonable polynomial time. In a bipartition clustering problem, i.e. only two clusters are needed, minimum cut algorithm can be applied in order to find them. The vertices of the input graph represent the data points and the edges between them are weighted with the affinities between the data points. Intuitively, the fewer high affinity edges are cut, the better the division into two coherent and mutually different parts will be [Bie2006].\nIn the simplest min cut algorithm, a connected graph is partitioned into two subgraphs\nwith the cut size minimized. However, this often results in a skewed cut, i.e. a very small subgraph is cut away [Ding2001]. This problem could largely be solved by using some cut cost functions proposed in the literature in the context of clustering, among which the average cut cost (Acut) and the normalized cut cost (Ncut). Acut cost seems to be more vulnerable to outliers (atypical data points, meaning that they have low affinity to the rest of the sample) [Bie2006]. However, skewed cuts still occur when the overlaps between clusters are large\nAutomatic Structure Discovery for Large Source Code Page 23 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n[Ding2001] and, finally, both optimizing the Acut and Ncut costs are NP-complete problems [Shi2000].\nIn the fully unsupervised-learning scenario, no prior information is given as to which\ngroup data points belong to. In machine learning literature these target groups are called “classes”, but do not confuse with Java classes. Besides this clustering scenario, in the transduction scenario the group labels are specified for some data points. Transduction, or semi-supervised learning, received much attention in the past years as a promising middle group between supervised and unsupervised learning, but major computational obstacles were inhibiting its usage, despite the fact that many natural learning situations directly translate into a transduction problem. In graph cut approaches, the problem of transduction can naturally be approached by restricting the search for a low cost graph cut to graph cuts that do not violate the label information [Bie2006].\nFast semi-definite programs relaxations of [Bie2006] made it possible to find a better\ncut than the one found using spectral relaxations of [Shi2000], and the authors in their experiments were able to process graphs of up to 7K vertices and 41K edges within reasonable time and memory. However, this is still far not enough for our problem, as even a medium-size project has about 1M of relations between software engineering artifacts.\nPaper [Ding2001] proposes another cut-based graph partition method, which is based\non min-max clustering principle: the similarity or association between two subgraphs (cut set) is minimized, while the similarity or association within each subgraph (summation of similarity between all pairs of nodes within a subgraph) is maximized. The authors present a simple minmax cut function together with a number of theoretical analyses, and show that min-max cut always leads to more balanced cuts than the ratio cut [Hag1992] and the normalized cut [Shi2000]. As the optimal solution for their min-max cut function is NP-complete, the authors used a relaxed version which leads to a generalized eigenvalue problem. The second lowest eigenvector, also called the Fiedler vector, provides a linear search order (Fiedler order). Thus the min-max cut algorithm (Mcut) provides both a well-defined objective and a clear procedure to search for the optimal solution [Ding2001]. The authors report that the Mcut outperformed the other methods on a number of newsgroup text datasets. Unfortunately, the computational complexity of the algorithm is not obvious from the article, except that the computation of Fiedler vector can be done in |)||(| VEO  , but the number of data points used in their\nexperiments did not exceed 400, which is far too little for our problem.\nOne important detail about bipartition-based graph clustering approaches is the\ntransition from 2-cluster to multiple-cluster solution. If this is done by means of recursive bipartition of the formerly identified clusters, then the solution is hierarchical in nature, which is good for source code structure discovery. However, this algorithm is also greedy, thus clustering quality can be unacceptable. Apparently, the optimal partition of a system into 3 components can differ much from the solution received by first bipartitioning the system, and then bipartitioning one of the components. This is illustrated in Figure 3-1 below.\nAutomatic Structure Discovery for Large Source Code Page 24 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nFinally, the main clustering method selected for the implementation of our project is also cutbased [Fla2004]. Though it produces hierarchical clustering, it does not suffer from the issue of greedy approaches demonstrated above. This is because the hierarchy arises due to the clustering criteria used, namely, vertices of the example graph in Figure 3-4 are sent into three sibling clusters as soon as the key parameter (alpha) is small enough, and until alpha gets even smaller to send all the vertices into one parent cluster. Depending on the input graph, there might be no value of the key parameter at which a certain amount of clusters is produced (e.g. 2 in our example), thus there can be a threshold from 3 clusters to 1 cluster incorporating all the vertices of those child clusters.\n3.2.2 Other Clustering Methods\nOther clustering methods were studied without experiments within this project. Most of the methods did not pass the early cut stage because they are not scalable to large graphs. First of all, methods that require complete adjacency matrix on input were discarded, as it implies at least N 2 operations while our graph is sparse. Then, greedy hierarchical clustering approaches, either agglomerative or divisive, were left out. We, however, find it important to discuss the confusion observed in the literature on software architecture recovery, e.g. the work reported in [Czi2007] and a number of hierarchical clustering for software architecture recovery approaches discussed in [Maqb2007]. While the titles say “hierarchical clustering”, for the quality of results it is crucial to distinguish how the hierarchy emerges: whether it happens due to the greedy order in which clusters are identified, or it is data-driven. The clustering algorithm we use in our project falls into the latter category. We discuss those from the former category in subsection 3.2.2.2 below. The superiority of partitional clustering methods in the domain of software source code is confirmed in [Czi2008], where the authors improve their own earlier results of [Czi2007] by means of using partitional clustering instead of hierarchical agglomerative clustering used in their earlier work pursuing the same goal of automatic source code refactoring. The partitional clustering method used in [Czi2008] is k-medoids [Kau1990] with some heuristics for choosing the number of medoids (clusters) and the initial medoids, while the heuristics are domain-specific.\nAnother point that we consider worth discussing in a subsection is not a clustering\nmethod itself, but rather a technique that allow a series of clustering methods to work in a drastically reduced computational complexity without major precision losses, as reported in [Ra2007]. We do not use any of the clustering methods, e.g. k-medoids [Kau1990] or GirvanNewman [Gine2002], accelerated with this network structure indices technique in our project for the following reasons:\n [Ra2007] admits the superiority of Flake-Tarjan [Fla2004] clustering method in\nterms of clustering result quality.\n The only argument of [Ra2007] against minimum cut tree based clustering methods\nis that “they are not scalable to large graphs”. However, it seems that the authors of [Ra2007] were not aware of the actual computational complexity of Flake-Tarjan clustering, in terms of both worst-case and usual-case. There is some rationale behind this, as Flake-Tarjan clustering relies on the computation of maximum flow in a graph, which is believed to be a very hard polynomial algorithm. The widely known Dinic’s algorithm for max flow in a real-valued network works in\n)(| 2 EVO , and the push-relabel algorithm referenced by [Fla2004] works in\n)log(|\n2\nE\nV EVO  time. This could give the authors of [Ra2007] a wrong idea on\nthe scalability of minimum cut tree based clustering. However, recent developments\nAutomatic Structure Discovery for Large Source Code Page 25 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nin max flow algorithm allow computing the minimum cut in as little as\n)loglog),min((\n2\n3/2 U\nE\nV EVEO  , where U is the maximum capacity of the\nnetwork [Gol1998]. Our practical studies shown that the actual running time of Goldberg’s implementation (see section 4.1.1 of the thesis) of push-relabel based\nmax flow algorithm is nearly )( E in the usual case. As this algorithm requires\nintegral arc capacities, we have developed within our work a method to approximate real-valued max flow problem with an integral one that satisfied the needs of our project, namely, the property that allowed minimum cut tree based hierarchical clustering [Fla2004] was not lost due to conversion from real-valued to integral max flow problem.\n One clustering algorithm improved in [Ra2007], namely Girvan-Newman\n[Gir2002], is greedy hierarchical (divisive) clustering. It is not said whether there are some non-greedy hierarchical clustering algorithms can be improved with network structure indices technique.\n In [Ra2007] the authors only worked with non-weighted graphs. It is not clear\nwhether the technique can still handle weighted graphs, and if so, whether realvalued weights are possible.\n Maximum flow algorithms for non-weighted graphs have smaller algorithmic\ncomplexity too: e.g. Dinic blocking flow algorithm for network with unit-capacities\nterminates in )( EVO  [Dini1970]. Thus Flake-Tarjan clustering having Dinic’s\nalgorithm in the backend would work much faster, but this is only possible in networks with unit capacities.\nThus we conclude that Flake-Tarjan clustering algorithm [Fla2004] is fast enough, produces both better clustering quality than the rival approaches, and a data-driven hierarchical clustering that is very desired for software architecture domain."
    }, {
      "heading" : "3.2.2.1 Network Structure Indices based",
      "text" : "Simple clustering methods, like a new graphical adaptation of the k-medoids algorithm [Kau1990] and the Girvan-Newman [Gir2002] method based on edge betweenness centrality, can be effective at discovering the latent groups or communities that are defined by the link structure of a graph. However, many approaches rely on prohibitively expensive computations, given the size of relational data sets in the domain of source code analysis. Network structure indices (NSIs) are a proven technique for indexing network structure and efficiently finding short paths [Ra2007]. In the latter paper they show how incorporating NSIs into these graph clustering algorithms can overcome these complexity limitations.\nThe k-medoids algorithm [Kau1990] can be thought of as a discrete adaptation of the k-\nmeans data clustering method [MaQu1967]. The inputs to the algorithm are k, the number of clusters to form, and a distance measure that maps pairs of data points to a real value. The procedure is as follows: (1) randomly designate k instances to serve as “seeds” for the k clusters; (2) assign the remaining data points to the cluster of the nearest seed using the distance measure; (3) calculate the medoids of each cluster; and (4) repeat steps 2 and 3 using the medoids as seeds until the clusters stabilize. In a graph, medoids are chosen by computing the local closeness centrality [Ra2007] among the nodes in each cluster and selecting the node with the greatest centrality score. One issue with k-medoids approach is similar to the problem of sparse version of Affinity Propagation we discussed in 3.2.1.1: in contrast to the dataclustering counterpart k-medoids, graph distance is highly sensitive to the edges that exist in the graph. Adding a single “short-cut” link to a graph can reduce the graph diameter, altering\nAutomatic Structure Discovery for Large Source Code Page 26 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nthe graph distance between many pairs of nodes. Second issue arises when graph distances are integers. In this case nodes are often equidistant to several cluster medoids. [Ra2007] resolves the latter conflicts by randomly selecting a cluster; however, this can result in clusterings that do not converge. This is further resolved by a threshold on the fraction of non-converged clusters.\nThe Girvan-Newman algorithm [Gir2002] is a divisive clustering technique based on\nthe concept of edge betweenness centrality. Betweenness centrality is the measure of the proportion of shortest paths between nodes that pass through a particular link. Formally, betweenness is defined for each edge Ee as:\n, where v)g(u, is the total number of geodesic paths between\nnodes u and v , and v)(u,ge is the number of geodesic paths\nbetween u and v that pass through e .\nA geodesic path in [Ra2007] is simply the shortest path in a graph. Note that there can be multiple shortest paths, i.e. they all have the same length but pass through different chains of edges. Also, the methods of [Ra2007] work with non-weighted graphs, i.e. each edge has length 1.\nThe algorithm ranks the edges in the graph by their betweenness and removes the edge\nwith the highest score. Betweenness is then recalculated on the modified graph, and the process is repeated. At each step, the set of connected components of the graph is considered a clustering. If the desired number of clusters is known a priori (as with k-medoids), we halt when the desired number of components (clusters) is obtained.\nThe main problem with the two clustering algorithms described above is algorithmic\ncomplexity, and this also applies to many other approaches, but the above two were studied in [Ra2007] and accelerated dramatically with network structure indices. The baseline clustering algorithms are intractable for large graphs:\n For k-medoids clustering, calculation and storage of pairwise node distances can be\ndone in )VO( 3 time and )VO( 2 space with Floyd-Warshall algorithm (can be found\nin e.g. [CLR2003]).\n For Girvan-Newman clustering, calculation of edge betweenness for the links in a graph\nis an )VO( E operation.\nA network structure index (NSI) is a scalable technique for capturing graph structure [Ra2007]. The index consists of a set of node annotations combined with a distance measure. NSIs enable fast approximation of graph distances and can be paired with a search algorithm to efficiently discover short (note, not the shortest!) paths between nodes in the graph. A distance to zone\n(DTZ) index was employed in [Ra2007]. The DTZ indexing process creates d independent sets of random partitions (called dimensions) by stochastically flooding the graph. Each dimension consists of z random partitions (called zones). DTZ annotations store the distances between each node and all zones across each dimension. The approximate distance between two nodes u and v is defined as:\n  d ddDTZ uzonevdistvzoneudistvuD ))(,())(,(),(\n, where ))(,( vzoneudistd is the length of the shortest path between u and the closest node in\nthe same zone as v . Creating the DTZ index requires )O( dzE  time and )O( dzV \nspace. Typically they select V d z,  , thus DTZ index can be created and stored in a fraction\nof the time and space it takes to calculate the exact graph distances for all pairs of nodes in the graph. The results of empirical study of the speed improvement achieved with NSIs are\n   Vvu\ne\nvug\nvug eB\n, ),(\n),( )(\nAutomatic Structure Discovery for Large Source Code Page 27 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nillustrated in Figure 3-5 below [Ra2007]. The top line shows bidirectional breadth-first search, which can become intractable for even moderate-size graphs. The middle line shows an optimal best first search, which represents a lower bound on the run time for any search-based method. The lower line shows an NSI-based method, DTZ with 10 dimensions and 20 nodes."
    }, {
      "heading" : "3.2.2.2 Hierarchical clustering methods",
      "text" : "Most clustering algorithms can be classified into two popular techniques: partitional and hierarchical clustering. Hierarchical clustering methods represent a major class of clustering techniques [Czi2007]. There are two types of hierarchical clustering algorithms: agglomerative and divisive. Given a set of n objects,\n The agglomerative (bottom-up) methods begin with n singletons (sets with one\nelement), merging them until a single cluster is obtained. At each step, the most similar two clusters are chosen for merging.\n The divisive (top-down) methods start from one cluster containing all n objects and\nsplit it until n clusters are obtained.\nThe agglomerative clustering algorithms differ in the way the two most similar clusters are determined and the linkage-metric used: single, complete or average.\n Single link algorithms merge the clusters whose distance between their closest\nobjects is the smallest.\n Complete link algorithms merge the clusters whose distance between their most\ndistant objects is the smallest.\n Average link algorithms merge the clusters in which the average of distances between\nthe objects from the clusters is the smallest.\nIn general, complete link algorithms generate compact clusters while single link algorithms generate elongated clusters. Thus, complete link algorithms are generally more useful than single link algorithms [Czi2007]. Average link clustering is a compromise between the sensitivity of complete-link clustering to outliers and the tendency of single-link clustering to form long chains that do not correspond to the intuitive notion of clusters as compact, spherical objects [Man1999].\nIn addition to the above mentioned issues of agglomerative clustering approaches, and\nthe suspicious averaging of distances, the issue we discussed near Figure 3-4 still remains too,\nAutomatic Structure Discovery for Large Source Code Page 28 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nnamely, the greedy nature of such algorithms. On the other hand, partitional clustering algorithms look at all the data at once, and produce a partition of the data points into some number of clusters. According to [Jain1999], the partitional techniques usually produce clusters by optimizing a criterion function defined either locally (on a subset of the patterns) or globally (defined over all of the patterns). At this point it is worthy to notice that Flake-Tarjan clustering algorithm optimizes the criteria globally, see section 4.3.1.\nIn [Jain1999] they provide taxonomy of clustering algorithms, see Figure 3-6 below.\nThe main clustering algorithm we use in our project was not yet invented at the time the review [Jain1999] was written, and falls into “Graph Theoretic” category under “Partitional” clustering approaches. We are stressing this to prevent confusion of it with the hierarchical clustering approaches present in the literature on the basis of the fact that the clustering algorithm [Fla2004] produces clustering hierarchy too. Still, it is a partitional clustering method, well grounded theoretically and free of the disadvantages of greedy algorithms.\nA recent review of multiple hierarchical clustering approaches applied to the domain of software architecture recovery, [Maqb2007], concluded that the performance of the state of the art algorithms is poor. The authors mention arbitrary decisions taken inside the clustering algorithms as a core source of problems. We have demonstrated another source of problems near Figure 3-4, namely, greedy nature of the algorithms. Furthermore, all the algorithms described there work in at least )O(n 2 algorithmic complexity, as they take n by n similarity\nmatrix on input, where n is the number of software engineering artifacts. In our work, we propose an approach that is scalable to large sparse graphs of relations between software engineering artifacts, and the clustering decisions are strongly grounded by the theory of [Fla2004].\nAutomatic Structure Discovery for Large Source Code Page 29 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "4 Background",
      "text" : "The pre-requisite methods and tools used in our project are described and formalized in this section. We also find it worthy to discuss the known challenges arising in the domain of software engineering artifacts clustering. In section 5 we provide the theory we devised on top of the background material given here."
    }, {
      "heading" : "4.1 Max Flow & Min Cut algorithm",
      "text" : "The maximum flow problem and its dual, the minimum cut problem, are classical\ncombinatorial problems with a wide variety of scientific and engineering applications. In a graph denoting a flow network, edge weights denote the capacities, i.e. the amount of substance that can flow through a connection between points (vertices). The task is assignment a certain amount of flow to each connection (pipe), so that the total flow from a source vertex to a sink vertex is maximized. More background about this classical problem can be found in [CLR2003]. Here we just mention the differences with the shortest-path problem that is a prevailent pre-requisite for other clustering algorithms:\n High weight is good for flow, but bad for short path.\n A path is a chain of edges, while flow can go in multiple parallel directions.\n There is no (polynomial) solution for “longest path” problem, but there are solutions for maximum flow.\nMost efficient algorithms for the maximum flow are based on the blocking flow and the\npush-relabel methods. The shortest augmenting path algorithm, the blocking flow method, and the push-relabel method use a concept of distance, taking the length of every residual arc to be unit (one). Using a more general, binary length function [Gol1998] substantially improved the previous time bounds. As a potential further improvement direction, the authors mention considering length functions that depend on the distance labels of the endpoints of an arc in addition to the arc’s residual capacity.\nWithin our project we do not seek to improve the speed of the max flow algorithm and\ntake it as is with algorithmic complexity:\n)loglog),min((\n2\n3/2 U\nE\nV EVEO \nIn a typical graph for our experiments, 10K vertices and 1M edges, this amounts to 1M*464*8*32 = 119G of trivial operations in the worst case. However, the lossless heuristics [Che1997] for push-relabel based implementation of max flow (see subsection 4.1.1) kept the actual number of scalar operations to about 100M. So we use binary blocking flow for theoretical estimations of algorithmic complexity and Goldberg’s implementation of pushrelabel based algorithm with heuristics, which works fascinating in practice.\n4.1.1 Goldberg’s implementation\nIn this project we used Goldberg’s implementation of push-relabel algorithm solving max flow problem, http://www.avglab.com/andrew/soft.html , see “HIPR” which is an improvement over the H_PRF version described in [Che1997]. This implementation performs much less scalar operations than the worst-case estimation due to lossless (in terms of optimality) speedup heuristics. In [Che1997] the authors point out a problem family on which all the known max flow methods have quadratic (in the number of vertices) time growth rate. However, even for this problem family their best implementation (H_PRF) processed a graph of 65K vertices and 98K edges in reasonable time\nAutomatic Structure Discovery for Large Source Code Page 30 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAs maximum flow computation is the bottleneck in our project, it is important that the\nimplementation is highly optimized, including low-level optimizations. This implementation is written in C and includes some heuristics that allow computing the maximum flow much faster than the worst-time complexity estimation. In our practical needs we observe that the algorithm computes max flow in a graph of 10K vertices and 1M edges in 0.02 seconds on a 1.7GHz computer with 2MB cache memory, while processor cache size is important as most of the time is spent in cache misses."
    }, {
      "heading" : "4.2 Min Cut Tree algorithm",
      "text" : "Cut trees, introduced in [GoHu1961] and also known as Gomory-Hu trees, represent the structure of all ts  cuts of undirected graphs in a compact way. Cut trees have many applications, but in our project we use them for clustering as described in [Fla2004]. All known algorithms for building cut trees use a minimum ts  cut (see 4.1 above) as a subroutine [GoTs2001]. In [GoHu1961] they showed how to solve the minimum cut tree problem using\n1n minimum cut computations and graph contractions, where n is the number of vertices in\nthe graph. An efficient implementation of this algorithm is non-trivial due to subgraph contraction operations used [GoTs2001]. Gusfield [Gus1990] proposed an algorithm that does\nnot use graph contraction; all 1n minimum ts  cut computations are performed on the input graph. We use this algorithm in our project for one more reason in addition to the above mentioned: it is possible to apply the community heuristic, specific to the purpose for which we need computation of minimum cut tree [Fla2004]. Both Gusfield algorithm and the community heuristic are described in the subsections.\nThe input to the cut tree problem is an undirected graph ),( EVG  , in which edges\nhave capacities, each denoting the maximum possible amount of flow through an edge. We say that an edge crosses the cut if its two endpoints are on different sides of the cut. Capacity of a\ncut is the sum of capacities of edges crossing the cut. For Vts , , an ts  cut is a cut such\nthat s and t are on different sides of it. A minimum ts  cut is an ts  cut of minimum capacity. A (global) minimum cut is a minimum ts  cut over all ts, pairs. A cut tree is a\nweighted tree T on V with the following property. For every pair of distinct vertices s and\nt , let e be a minimum weight edge on the unique path from s to t in T . Deleting e from T\nseparates T into two connected components, X and Y . Then ),( YX is a minimum ts  cut.\nNote that T is not a subgraph of G , i.e. edges of T do not need to be in G .\n4.2.1 Gusfield algorithm\nIn [Gus1990] they provide a simple method for implementing minimum cut tree algorithm, which does not involve graph contraction [GoHu1961] and works in the same algorithmic\ncomplexity. Below is the pseudo code of Gusfield algorithm: (1) For all vertices, i=2…n, set prev[i]=1; (2) For all vertices, s=2…n do (3) t = prev[s]; (4) Calculate a minimal cut (S,T) and the value [w] of a maximal flow in the graph,\nusing [s] as the source and [t] as the sink\n(5) Add edge (s,t) with weight [w] to the resulting tree (6) For all vertices [i] from S /* the source-side vertices after the cut */ (7) If i > s and prev[i]==t then (8) prev[i] = s; (9) End; (10) End; (11) End;\nAutomatic Structure Discovery for Large Source Code Page 31 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nApart from simplicity of the algorithm, we also use it because the community heuristic [Fla2004] (also described in section 4.2.2 of the thesis) can be applied thus reducing the required number of max flow computations substantially.\n4.2.2 Community heuristic\nThe running time of the basic cut clustering algorithm [Fla2004] is equal to the time to calculate the minimum cut tree, plus a small overhead for extracting the subtrees under the\nartificial sink t . But calculating the min-cut tree can be equivalent to computing 1n maximum flows in the worst case for [GoHu1961], and always for [Gus1990] which we provided in section 4.2.1 and use in our project. Fortunately, [Fla2004] proves a property that allows to find clusters much faster, in practice, usually in time equal to the total number of clusters times the time to compute max flow.\nThe gist of the community heuristic follows. If the cut between some node v and t\nyields the community S (vertices on the source-side of the cut), then we do not use any of the\nnodes in S as subsequent sources to find minimum cuts with t , since according to a lemma\nproved in [Fla2004] their communities would be subsets of S . Instead, we mark the vertices of\nS as being in community S , and later if S becomes part of a larger community S  we mark\nall nodes of S as being part of S  .\nThe heuristic relies on the order in which we iterate over the vertices of the graph, as\nopposed to the baseline Gusfield algorithm (section 4.2.1) which passes the vertices in arbitrary order. It is desired that the largest clusters are identified first. As proposed in [Fla2004], we sort all nodes according to the sum of the weights of their adjacent edges, in decreasing order."
    }, {
      "heading" : "4.3 Flake-Tarjan clustering",
      "text" : "In [Fla2004] they introduce simple graph clustering methods based on minimum cuts within the graph. The cut clustering methods are general enough to apply to any kind of graph but, according to the authors of the paper, are well-suited for graphs where the link structure implies a notion of reference, similarity or endorsement. The authors experiment with Web and citation graphs in their work.\nGiven an undirected graph ),( EVG and a value of parameter  , the basic clustering\nalgorithm of [Fla2004], which we call “Alpha-clustering” (see section 4.3.1) due to the presence of parameter  , finds a community for each vertex with respect to an artificial sink t\nadded to the graph G . The artificial sink is connected to each node of G via an undirected edge of capacity  . The community of vertex s with respect to vertex t is the set of vertices on the source-side of the minimum cut between vertex s as the source, and vertex t as the sink.\nIn the hierarchical version of Flake-Tarjan clustering algorithm, we can observe that the\nalgorithm does not depend on parameter  in case all the breakpoints of parametric max flow [Bab2006] have been considered, where parametric edges are those connecting the artificial sink to the rest of the graph. Thus given the input graph, a hierarchical clustering is produced on output. It is very important to stress that there are no parameters to tune, unlike parameter\nk in k-medoids clustering [Kau1990] or exemplar preferences in affinity propagation\n[Fre2007]. The resulting clustering is completely data-driven.\n4.3.1 Alpha-clustering\nParameter  serves as an upper bound of the inter-cluster edge capacity and a lower bound of the intra-cluster edge capacity, according to a theorem proven in [Fla2004]:\nAutomatic Structure Discovery for Large Source Code Page 32 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nLet ),( EVG be an undirected graph, Vs a source, and connect an artificial sink t\nwith edges of capacity  to all nodes. Let S be the community of s with respect to t .\nFor any non-empty P and Q , such that SQP  and {}QP , the following\nbounds always hold:\n),min(\n),(),(\nQP\nQPc\nSV\nSVSc \n\n \nThe left side of the inequality bounds the inter-community edge-capacity, thus guaranteeing that communities will be relatively disconnected. Here ),( SVSc  is the cut size (the sum of\nthe capacities of edges going from the left set of vertices to the right set) between the vertices\nin S and the rest of the graph.\nThe right side of the inequality means that for any cut inside the community S , even\nthe minimum one, its value (i.e. the sum of edges crossing the cut) will be at least  times the minimum of the cardinalities over the two sides of the cut. In the other words:\n If we want to separate 1 vertex from a cluster (containing at least 2 vertices), we\nhave to cut away edges with total weight at least   If we want to separate 2 vertices from a cluster (containing at least 4 vertices), we\nhave to cut away edges with total weight at least 2  If we want to separate 3 vertices from a cluster (containing at least 6 vertices), we\nhave to cut away edges with total weight at least 3  And so on.\nAs  goes to 0, the cut clustering algorithm will produce only one cluster, namely the entire graph G , as long as G is connected. On the other extreme, as  goes to infinity, there will be\nn trivial clusters, all singletons. When a particular number of clusters is needed, say k , we\ncan apply binary search in order to determine the value of  that produces the number of\ncluster closest to k . When a hierarchy of clusters is needed (see section 4.3.2), the results of clustering using multiple values of  must be merged. The basic clustering algorithm, as in [Fla2004], is shown in Figure 4-1 below.\n4.3.2 Hierarchical version\nThe hierarchical cut clustering algorithm provides a means to look at graph G in a more structured, multi-level way [Fla2004]. In contrast to the greedy hierarchical clustering algorithms, discussed in 3.2.2.2 and used in the state of the art reverse architecting approaches ([Maqb2007], [Czi2007]), the hierarchality of clusters produced by Flake-Tarjan clustering algorithm follows from the nesting property proven in [Fla2004], namely, clusters produced\nAutomatic Structure Discovery for Large Source Code Page 33 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nusing lower values of  are always supersets of clusters produced at higher values of  in the basic cut clustering algorithm\nThe hierarchical cut clustering algorithm of [Fla2004] is given in Figure 4-2 below. The\nauthors propose to contract clusters produced with higher values of  before running the algorithm on smaller values of  . However, this puts a constraint on the order in which we can try different values of  . If we want to try smaller values of  first, e.g. because we are limited in time and want to get more high-level views on the software system first, instead of contracting the input graph we should rather be able to merge clustering obtained at an arbitrary  into a globally maintained clustering hierarchy, as devised within our project and described in section 5.4 of the thesis."
    }, {
      "heading" : "4.4 Call Graph extraction",
      "text" : "A dynamic call graph is a record of an execution of the program, e.g., as output by a\nprofiler. Thus, a dynamic call graph can be exact, but only describes one run of the program. A static call graph is a call graph intended to represent every possible run of the program. The exact static call graph is undecidable, so static call graph algorithms are generally overapproximations. That is, every call relationship that occurs is represented in the graph, and possibly also some call relationships that would never occur in actual runs of the program. Below are some examples of the difficulties encountered when generating call graph from source code (static):\n polymorphism: depending on the class of object assigned to a variable of the base class, different methods are called\n invariants: if in the code below x >= 0 always, then the call to func2() actually never occurs:\no if(x < 0) { func1(x); } else { func2(x); }\n contextuality: in the example above, we can consider the reasons for x to be negative or non-negative, and mark in the call graph the fact that either func1() or func2() can be\ncalled from the current function depending on the context.\nSo, both dynamic and static call graph generation have drawbacks:\n static: the call graph is imprecise\n dynamic: we need many runs to ensure that the source code is covered enough In our prototype there is a point at which the program does not care whether static or dynamic call graph is supplied. In principle, we accept any graph of relations on input without binding to a programming language or static/dynamic kinds of analysis. In the experiments within this\nAutomatic Structure Discovery for Large Source Code Page 34 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nproject, however, we used static call graph extracted from source code in Java using Soot [Soot1999] and the approaches for virtual method call resolution available within the framework: [Sun1999], [Lho2003]."
    }, {
      "heading" : "4.5 The Problem of Utility Artifacts",
      "text" : "Not all component dependencies have the same level of importance. This applies particularly to utility components which tend to be called by many other components of the system, and as such they encumber the structure of the system without adding much value to its understandability [Pir2009]. A research about the properties of utility artifacts [HaLe2004] concluded that:\n Utilities can have different scope, i.e. not only at the system level.  Utilities are often packaged together, but not necessarily  Utilities implement general design concepts at a lower level of abstraction than\nthose design concepts\nThe common practice for detecting utilities is to use heuristics that are based on computing a component’s fan-in and fan-out. The rationale behind this is that [HaLe2004]:\n something that is called from many places is likely to be a utility, whereas  something that itself makes many calls to other components is likely to be too\ncomplex and too highly coupled to be considered a utility.\nAn exhaustive review of the existing reverse architecting approaches based on clustering and the ways they detect and remove utilities is given in [Pir2009]. Among these approaches are [Man1998] / [Bunch1999], [Mull1990], [Wen2005], [Pate2009]. In [ACDC2000] they used somewhat different approach. As in the first phase of ACDC algorithm they simulate the way software engineers group entities into subsystems, the authors observed and used the fact that software engineers tend to group components with large fan-in into one cluster: support library cluster containing the set of utilities of the system.\nTo our knowledge, all the existing reverse architecting approaches that address the\nproblem of utility artifacts at all, detect and remove utility artifacts from further analysis. In our project we devise and implement weighting of relations according to their chance to be utility calls/dependencies, and the theory is given in section 5.1. In [Roha2008] they do use weighting according to utility measures developed within that work, however, that weighting applies to components (vertices of the graph) in contrast to edges (relations) in our project. Furthermore, they do not run clustering after weight assignment.\nThe major technique used for detection of utility artifacts is fan-in analysis, where the\nvariations are based on the exploration of the component dependency graph built from static analysis of the system [Pir2009]. Dependencies include method calls, generalization, realization, type usage, field access, and others. Some approaches represent the cardinality of dependencies with weights on the edges of the dependency graph, e.g. [Stan2009]. The rationale behind using fan-in analysis as indication of the extent to which an artifact can be considered utility is as follows: the more calls a component has from different places (i.e. the more incoming edges in the static component graph), then the more purposes it likely has, and hence the more likely it is to be a utility, and the researchers currently converge on this rationale [Mull1990], [Pate2009], [HaLe2006], [Roha2008].\nThe weak points of the approaches that attempt to solve the problem of utility detection\nare listed in [Pir2009]. Those approaches use evidently more complicated fan-in analysis than we do in this project, sometimes they even combine fan-in with fan-out analysis. However, the strong point of our approach is that the solution of utility artifacts problem is shared between pre-clustering phase and the clustering itself, see section 5.1. At the pre-clustering phase we estimate “utilityhood” of software engineering artifacts and relations. Then the clustering\nAutomatic Structure Discovery for Large Source Code Page 35 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nphase smoothes the likely utility connections because those connections are assigned low weight in the pre-clustering (normalization) phase.\nThough [HaLe2006] experimented with combination of fan-in and fan-out analysis in\norder to determine the extent to which a component can be considered a utility, their metrics were only able to detect system-scope utilities. We argue that this issue was encountered because the authors were trying to solve the problem of detection, thus they had to introduce a threshold in order to make a decision. But thresholds differ for the whole system and for utilities in local subsystems; furthermore, local utilities do not necessary have the same decision threshold across different subsystems.\nA counterexample against fan-in analysis alone was given in [Roha2008], we show it\ntoo in Figure 4-3 below. It is arguably whether C2 is a utility indeed, but C3 apparently is, according to utility rationale discussed above. However, functions usually call only functions at lower level levels of abstraction, thus, a utility function either does not call any others or calls mostly utility functions [HaLe2006], [HaLe2004]. Thus, C2 is likely to be a utility too. However, fan-in analysis alone would not detect it as such.\nThe above example is not a problem for our approach, as the connection weight between C3 and C2 stays strong (see section 5.1), thus C3 will be clustered with C2 first (in the bottom of the clustering hierarchy or, in other words, at a high value of parameter alpha, see [Fla2004] and section 4.3.2). Thus if some magic oracle (which we do not have explicitly in our approach) deems C3 to be a utility, then C2 will be the closest to it in terms of unity of purpose, according to the clustering results. Afterwards it is not hard to infer that C2 is a utility too. In our work we also give a counterargument against fan-out analysis, which arises in practice due to impreciseness of call graph extraction (discussed in sections 3.1 and 4.4) and the kind of errors the state of the art call graph analyses make, namely, due to polymorphism there appear excessive calls to multiple derived classes (subtypes) in the call graph that never occur in practice. We observed this in our experiments (appendix 10.3.2, also 10.3.1) and provide our argument in section 5.1.3 below.\nFinally, it makes sense to mention that [HaLe2004] identify (in their reasoning, not\nautomatically) different kinds of utilities:\n Utilities derived from the usage of a particular programming language. An example\nis a class that implements Enumeration interface in Java.\n Utilities derived from the usage of a particular programming paradigm. For\nexample, accessor methods or initializing functions\n Utilities that implement data structures (inserting, removing, sorting)  Mathematical functions  Input/Output Operations\nAutomatic Structure Discovery for Large Source Code Page 36 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nOur results show (see appendix 10.1.7) that not only utility artifact problem has been alleviated, but also utility artifacts are categorized according to their purpose, likewise the other artifacts."
    }, {
      "heading" : "4.6 Various Algorithms",
      "text" : "A number of classical algorithms in computer science were used in order to implement this project, and thus appear in this paper. The most important of them were discussed earlier in this section. Below we give a short list and remarks about the rest. A reader that needs more background can refer to [CLR2003], [AHU1983] and [Knu1998].\nAlgorithm Remarks\nBreadth & Depth First Searches\nPriority Queue\nPriority Blocking Queue Java Minimum Spanning Tree Tree Traversals, Metrics & Manipulations Height, depth, cardinality, etc. Lowest Common Ancestor Disjoint-set data structure / union-find algorithm\nReindexing Techniques Graph contraction\nSubgraph/subset processing\nReusable full-indexing map Insertion/Removal: O(1)\nCreation: O(nIndices) Listing: O(nStoredItems)\nDynamic Programming For the statistics Suffix Tree For the statistics\nAutomatic Structure Discovery for Large Source Code Page 37 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "5 Theory",
      "text" : "Within this work we have devised and used some theory needed in order to:\n Apply the clustering method of [Fla2004] to the source code analysis domain  Solve the issues encountered during the application, namely, excessive number of\nsibling nodes (aka alpha-threshold). This happens due to the specifics of the domain, namely, software system are usually nearly-hierarchical, i.e. there are few (ideally, no) cycles.  Optimize the search direction in order to get the most important solutions as early as\npossible during the iterative runtime of the hierarchical clustering algorithm\n Allow parallel computation, as the clustering process still takes considerable time\nThe following subsections provide this theory in the amount necessary to implement our system. Some proofs and empirical evaluations require considerable efforts and are thus left out of our scope.\nWe represent the source code of software as a directed graph ),( EVG , with nV \nvertices and mE  edges. Each vertex corresponds to a software engineering artifact\n(usually, a class of object-oriented languages, e.g. Java class) and each directed edge to a relation between software engineering artifacts, e.g. method call or class inheritance. Also, we usually assume that G is connected, as otherwise each component can be analyzed separately unless some global restrictions on clustering granularity are posed."
    }, {
      "heading" : "5.1 Normalization",
      "text" : "In section 4.5 we have discussed the problem of utility artifacts. Our practical experiments have confirmed that with Flake-Tarjan clustering algorithm also produces degenerate results in case we cluster the graph of relations “as is”, i.e. in case each relation corresponds to an edge of weight 1 in the input graph for clustering.\nMoreover, the graph of relations between software engineering artifacts is directed,\nhowever Flake-Tarjan clustering is restricted to undirected graphs due to the underlying minimum cut tree algorithm [GoHu1961], which is only known for undirected graphs even though its own underlying algorithm, max flow [Gol1998], is available for both directed and undirected graphs.\nExtending Flake-Tarjan clustering algorithm to work with directed graphs is both hard\ntheoretical and risky task (see section 8). Thus within this project we decided to convert directed graph into undirected by means of normalization. The authors of [Fla2004] in their experiments used normalization similar to the first iteration of HITS [HITS1999] and to PageRank [Brin1998]. Each node distributes a constant amount of weight over its out-bound edges. The fewer pages a node points to, the more influence it can pass to its neighbors that it points to. In their experiment with CiteSeer citations between documents [Fla2004], the authors normalize over all outbound edges for each node (so that the total sums to unity), remove edge-directions, and combine parallel edges. Parallel edges resulting from two directed edges are resolved by summing the combined weight.\nHowever, in the domain of software source code, it seems more reasonable to\nnormalize over the incoming arcs rather than outgoing, so that each node receives a constant (or logarithmic) amount of weight from its incoming edges. This is grounded in [Pir2009], as they review many works on utility artifact detection and point the fact that utility functions are called by many other components as a main property. In the literature on reverse architecting the exploitation of this property is called fan-in analysis, discussed in section 4.5 of the thesis too.\nThe crucial difference in how our approach addresses the problem of utility artifacts,\ncompared to other existing approaches (see section 4.5), is in the following. The existing approaches focus on detection of utility artifacts with the goal of further removal prior to\nAutomatic Structure Discovery for Large Source Code Page 38 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nclustering. Our solution for the utility artifacts issue is split between pre-clustering and clustering phases, thus we are not concerned with the problem of detection, which would entail further binary decision on whether to remove an artifact before clustering.\n5.1.1 Directed Graph to Undirected\nOur conversion from directed to undirected graph works as follows. We discard arcs that loop a vertex to itself. Apparently, we lose some information in this step, namely, the fact that the corresponding SE artifact references (calls, uses, etc) itself. However, we do not see a way to make use of this information without damaging clustering quality. The latter was observed in our experiments.\nFor each vertex j , its fan-in is calculated as the sum of weights of all the incoming\narcs in the initial graph:\n i jij wS ,\nEach arc in the graph is then replaced with a normalized arc having weight jiw , ~ , which\nwithout leverage (section 5.1.2 below) amounts to:\nj\nji\nji S\nw w ,\n, ~ \nIn the target undirected graph, an edge between vertices i and j receives weight jiu , equal\nto the sum of the weights of the opposite-directed arcs:\nijjiijji wwuu ,,,, ~~ \nLet’s define jU as the total weight of edges adjacent to vertex j in the target undirected\ngraph (adjacent weight):\n  \n\n\n  \n \ni i\nij\nj\nji\ni\njij S\nw\nS\nw uU ,,\n,\nThe following properties can be observed:  Each vertex j receives a constant (C=1) amount of weight via arcs jiw , ~\n The total weight of edges adjacent to a vertex in the undirected graph can be both more or\nless than C\n If a vertex has at least one incoming arc in the directed graph, it will have adjacent weight\nat least C in the undirected graph\n In practice, there are seldom SE artifacts that only use others, but are not used from any\nplace thus do not have an incoming arc. Thus in most cases: 1jU\n Exclusions: artifacts that are called externally (e.g. thread entry points or contexts\nlaunched from Spring framework) may not have any incoming arcs when the input graph does not contain relations of the whole program (e.g. the code of system libraries\nor Spring framework is not available). For such artifacts, it can happen that 1jU\n It seems that 1jU (or equivalent for the leveraged counterpart from section 5.1.2)\ncan be the (partial) cause of alpha-threshold issue observed, see section 5.6\nIf vertices are SE methods, and edges are method calls, then a vertex has high adjacent weight when the corresponding method calls many methods infrequently called from other places. Frequency is calculated by the number of occurrences in the source code.\nAutomatic Structure Discovery for Large Source Code Page 39 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nObviously, such a conversion can be performed in )log( VEO scalar operations by\nmeans of two passes through all the arcs of the graph: first, calculate the values of jS ; second, calculate the weight jiw , ~ for each arc and combine it with ijw , ~ (if this opposite arc is\npresent) using balanced trees of incident vertices for each vertex. Practically, we use hash maps here.\n5.1.2 Leverage\nIn the previous section we gave formulas that force vertices to receive constant amount of weight, i.e. for each vertex j :\nCw i ji  1~ , However, it seems not reasonable to discard the cardinality of references to an SE artifact completely. Thus we use logarithmic leverage of the bound on the weight that a vertex can receive from the incoming arcs. In this case, the values of the discounted arc weight jiw , ~ and\nthe adjacent weight for a vertex jU are instead calculated as follows:\nj\nj\nji\nji S S\nw w log~ ,\n, \nijjiijji wwuu ,,,, ~~ \n  \n\n\n  \n \ni\ni\ni\nij\nj\nj\nji\ni\njij S S\nw S\nS\nw uU loglog ,,\n,\nIn our view, the usage of leveraged estimation of connection strength, as described above, pursues (and, empirically, achieves) the following objectives:\n Alleviate the problem of utility artifacts (also characteristic for the normalization\ndescribed in section 5.1.1)\n Regard the scale of connectedness rather than the magnitude. E.g., the scale of\ndifference between 2 and 4 connections is the same as the one between 100 and 200. The former distinguishes between more and less coupled high-level (in other interpretation, specific) SE artifacts. The latter distinguishes between more and less omnipresent utility (general-purpose) artifacts.\nBy looking at the clusterings produced with and without leverage, and comparing some inherent indicators, namely\n the range of parameter alpha between single-cluster and all-singleton-clusters results\nof partitional clustering [Fla2004]\n the number of excessive sibling clusters in the clustering hierarchy due to alpha-\nthreshold (also, see section 5.6)\nThough we are not able to provide a comparison in percentage, as evaluation of clustering quality is not a straightforward task in itself, from the experiments, indicators as described above and subjective evaluation of the resulting clustering hierarchy we conclude that leverage improves clustering quality for this algorithm [Fla2004] in software source code domain.\nComparing to the literature, we can observe that some kind of logarithmic leverage is\nused in other reverse architecting approaches [HaLe2006], [Roha2008]. The utilityhood metric of [HaLe2006] consists of two factors multiplied: one is the fan-in based ratio (straight division of the fan-in cardinality by the number of artifacts), another is based on fan-out with logarithmic leverage. Their rationale is that fan-in is much more important than fan-out,\nAutomatic Structure Discovery for Large Source Code Page 40 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nhowever, fan-out should also play role in the utilityhood of an artifact (we have discussed this too in section 4.5). [Roha2008] approves this rationale and adopts a derived approach for estimating the impact of component modification in their TWI (Two Way Impact) metric. However, the logarithmic multiplier still stays in the part responsible for fan-out (in the terms of [Roha2008], it is Class Efferent Impact) and fan-in is still represented by a direct ratio of cardinalities, in contrast to our approach.\n5.1.3 An argument against fan-out analysis\nIn the existing reverse architecting approaches, e.g. [Roha2008] and [HaLe2006], they use fan-out analysis in addition to fan-in. By doing this they attempt to make use of the second part of the rationale for utility detection (we discussed it in section 4.5), namely: something that itself makes many calls to other components is likely to be too complex and too highly coupled to be considered a utility [HaLe2004], [Pir2009].\nHowever, in case the underlying data for fan-in/fan-out analysis is a call graph\nextracted from source code of a program in object-oriented language, we can observe that such a relation graph has excessive outgoing arcs, which are noise (illustrated in section 10.3.2). This happens due to impreciseness of call graph extraction (section 4.4). Though the existing heuristics for call graph extraction ([Sun1999], [Bac1996], [Lho2003]) can alleviate this problem, they cannot eliminate it and we are still getting vertices with excessive fan-out.\nThus in practice we argue against fan-out analysis. Though we agree that a component\nthat makes many calls is likely to be complex and highly coupled, thus utilityhood of such a component should be discounted with respect to a metric inferred from pure fan-in analysis, we can only do this when our graph of relations does not have excessive outgoing arcs, i.e. in theory or in dynamic analysis. In practice of static analysis, for each polymorphic call site there is usually only a single or a few calls to some most specific subtypes that actually occur and are designed by software engineers, and the rest is noise. Thus, by discounting the utilityhood for the components containing such call sites due to high fan-out, we would propagate the mistake. We suppose to achieve more noise-tolerant solution by not using fanout analysis (at least, in the form of ratio or logarithmic multiplier).\n5.1.4 Lifting the Granularity\nThe input graph contains relations between SE artifacts of different granularity. There are method to method, method to field, method to class and class to class relations. Analyzing Java programs, we generalize any less than class-level artifacts as members (nested classes do not fall into this category), see section 5.2. In this project we experimented with only classlevel artifact clustering. Thus we have to lift the relations involving less than class-level artifacts to the class-level, i.e. lift the granularity to class level. An alternative is given in section 5.1.5. For the approaches and a discussion about lifting the component dependencies in general one can refer to [Kri1999].\nIn couple with the normalization that we are discussing in section 5.1, we see two\nprincipal options for lifting the granularity:\n1 Before normalization 2 After normalization\nAdopting the first option, we would first aggregate all the arcs in the initial directed graph G ~ which connect members of the same class, and connect the vertices (which represent SE\nclasses) of a derived graph G with arcs having the aggregated weights. This corresponds to\nthe lifting of [Kri1999]. In the next step we would normalize the directed graph G as described in our previous sections.\nAdopting the second option, we attempt to tolerate the noise in the input graph G ~ and\nimprove the quality of our heuristic addressing utility artifact problem. An alternative solution pursuing the same goal is proposed in 5.1.5. Below is the rationale for the heuristic that show in this section and choose to implement in our project. An error in utilityhood estimation for a\nsingle member artifact is smoothed by utilityhood estimations for the rest of artifacts which are members of the same SE class, in case lifting to class-level occurs after normalization. We empirically observed that, indeed, option 2 leads to better clustering results than option 1. This is further confirmed by the indicators intrinsic to the clustering method used: alphathreshold and excessive number of sibling clusters (see section 5.6).\nFormally, consider  V ~ is the initial set of vertices where a vertex can correspond to both a member-level and a\nclass-level SE artifact,\n the heterogeneous relations between SE artifacts in the initial directed graph G ~ constitute\nits set of arcs E ~\nwhereas arc from vertex i to vertex j has weight jia , ,\n V is a subset of V ~ consisting of class-level SE artifacts, and a class-level artifact is never\nalso a member-level artifact,\n the membership relations are defined by mapping VVM  ~ : , where membership\nrelations are only defined from a member-level artifact to a class-level artifact and, for convenience, each class-level artifact maps to itself,\nIf we lift the granularity prior to normalization, we get undirected graph 1G with edge weights\njiu , )1( and properties (section 5.1.1) as follows:\n   jlMikMlk lkji aw )(,)(, ,,\n)1(\n;    Vi\njij wS , )1()1( ; )1( ,\n)1(\n,\n)1( ~\nj\nji ji\nS\nw w  ;\n)1(\n,\n)1(\n,\n)1(\n,\n)1(\n, ~~\nijjiijji wwuu  Merging the formulas in order to demonstrate the intuition about the resulting weights in the undirected graph, we get:\n \n\n \n\n \n\n \n\n\n\n\n\n\nVj jMliMk\nkl\njMliMk\nkl\nVi jMliMk\nlk\njMliMk\nlk\nijji a\na\na\na\nuu\n)(),(\n,\n)(),(\n,\n)(),(\n,\n)(),(\n, )1(\n,\n)1(\n,\n11\n11\n11\n11\nIf we fix i and let    )( , 1 iMk nlkn ac and    Vk nlkn aC ~ , , where )(}{ 1 jMln  ,\nwe can observe that the left summands of formulas Figure 5-4 and Figure 5-5 (and by analogy, the right summands too) relate to each other as:\nN\nN\nN\nN jiji\nC\nc\nC\nc\nC\nc\nCCC\nccc ww \n\n  ...\n...\n...~~\n2\n2\n1\n1\n21\n21 )2(\n,\n)1(\n,\nAutomatic Structure Discovery for Large Source Code Page 43 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nWe hope this can further be used for formal study of the effect of noise, but leave this out of the scope of this paper. Call graph extraction methods put some noise ([Bac1996], [Sun1999], [Lho2003], also section 3.1) into the resulting graph, either by adding calls which never occur, or drop some calls which however may occur (section 4.4). In general, we can\ndesignate this noise as 10 ,  lk for each arc lka , in the input graph of relations, meaning\nthat there are lklk a ,, )1(   “true” calls and there are lklk a ,,  “false” calls.\n5.1.5 An Alternative\nIn principle we could, without lifting the granularity, normalize and then run Flake-Tarjan hierarchical clustering algorithm over heterogeneous graph consisting of both member- and class-level SE artifacts as vertices, and heterogeneous relations between SE artifacts as edges. We could try to lift the granularity from member-level to class-level after clustering of this graph has been performed.\nWe argue that this solution can produce a better clustering hierarchy, in terms of how\nwell it reflects the actual decomposition of the software system, because, compared to the solution of section 5.1.4, less information is lost prior to clustering. Namely, the information loss occurs in the following:  By aggregating edge weights over all the members of a SE class, we get a single (if any)\nedge (relation) between any two SE classes.\n A SE class becomes connected to other SE classes with edges, where for each edge its\nweight represents the connection strength between the two classes.\n However, some members of an SE class, vertex v , in the initial graph might be more\nconnected with members of one SE class, vertex 1v , and the other members of that class\nv might be more connected with members of another SE class 2v .\n An example of two cases which clustering will not be able to distinguish due to this\ninformation loss is illustrated in Figure 5-7 below.\nConsider a system of 5 classes (rectangles) containing 3 members (adjacent squares) each. The member-level relations (edges), which should be considered present:\nAutomatic Structure Discovery for Large Source Code Page 44 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n- in both cases are drawn in blue; - only in case A – in black; - only in case B – in red.\nIt is obvious that in case A the graph contains 2 separate cycles, drawn in yellow, however, in case B there is a single cycle traversing all the five SE classes, drawn in green. The above is drawn on the left side of Figure 5-7. It is reasonable that case A and case B determine different decompositions of the system, illustrated in the top and in the bottom of the right side of Figure 5-7 correspondingly. The difference is that in case B (single cycle, bottom diagram) the classes {C2, C3, C4, C5} do not constitute a subsystem without class C1, even though {C2, C3} and {C4, C5} constitute subsystems of which the whole system can be composed by adding {C1}. On the other hand, in case A (two cycles, upper diagram) classes {C2, C3, C4, C5} constitute a subsystem, which is a combination of two disjoint subsystems. In practice, fact “disjoint” will be replaced with “loosely-connected” (in terms of connection density, i.e. do not confuse with weakly connected components in a directed graph), and instead of the criterion of a “connected component”, the criterion of a “cluster” will be used.\nAn apparent disadvantage of member-level clustering is the computational\ncomplexity. In our experiments we observed 14.5 times more members than classes usually. However, in addition to this disadvantage, it is not clear on how to lift the granularity to classlevel after clustering at member-level. Each class contains several members, each its member will appear somewhere in the member-wise clustering hierarchy. How to arrange the classes into a hierarchy then, having the data on where their members appear in the member-wise hierarchy?\nOne approach is (weighted) voting. However, a problem arises: member-wise\npartitional clusterings will have the nesting property ([Fla2004], also section 4.3.2), but after voting it is most likely to be lost at class-level. At this point many options arise for solving this problem, e.g.\n1 For each pair of classes, count how many times their members form pairs through appearing together and at each level of the member-wise hierarchy. We get a sparse\nmatrix of counts, perhaps also weighted by depth/height of the node, at which pairs were encountered. We can now run a clustering algorithm on this new matrix as edge weights, perhaps with some normalization. This solution seems to be also vulnerable to the issue displayed in Figure 5-7, though less than a solution that losses member-wise relation information in the very beginning.\n2 Start building a new tree. Let each class node to appear at the position where the (weighted) majority of its members has appeared in a subtree of the member-wise\nhierarchy. This solution is prone to non-deep hierarchies with excessive number of sibling nodes, and the latter would hinder comprehensibility.\nDue to practical difficulties (risen computational complexity) and many reasonable options without a single good theoretical option, we did not develop this alternative within the current project further."
    }, {
      "heading" : "5.2 Merging Heterogeneous Dependencies",
      "text" : "The phase of extraction of relations between software engineering artifacts can produce various kinds of relations. In this project we used:\n1 Method-to-method calls 2 Class-to-class inheritance 3 Method-to-class field access 4 Method-to-class type usage: a method has statements with operands of that class) 5 Method-to-class parameter & return values: a method takes parameters or returns\nvalues of which are instances of a certain class\nNote that the kind 5 is not exhausted by kind 4, as e.g. methods in an interface do not have bodies, thus do not have statements.\nAutomatic Structure Discovery for Large Source Code Page 45 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nNow the question is how to consider the various kinds of relations for the inference of\nsoftware structure. We see the following principal options.\nOption 1: Clusterize the graphs of homogenous relations separately, i.e. one graph per\none kind of relation. Then combine the resulting multiple hierarchies into a single one. This solution has the same root disadvantage as the one discussed in 5.1.5, namely, it is not clear on how to merge the hierarchies.\nThe challenge of nearly-hierarchical input data for clustering in software engineering\ndomain is discussed in section 5.6. Thus, another disadvantage arises from the fact that a graph representing a single kind of relation is even more nearly-hierarchical than a graph combining multiple relations (dependencies) between SE artifacts. For example, inheritance always forms a directed acyclic graph (DAG) of relations. In Java programming language, if we consider only classes (not interfaces, i.e. only “extends” but not “implements” kind of inheritance), it is always a tree. In C++ it can still be a DAG.\nOption 2: Combine the multiple graphs into a single prior to clustering. This has the\nsame disadvantage comparing to option 1 as discussed in section 5.1.5 (obviously, a similar counterexample can be given by analogy), namely, loss of information about the kind of relation which an edge in the input graph for clustering represents. On the other hand, an strong side of this option is in the fact that a graph combining multiple kinds of relations is less likely to be nearly a tree, thus this solution alleviates the issue discussed in section 5.6.\nIn this project we implement option 2, and point out option 1 together with the similar\nalternative discussed in section 5.1.5 as a direction for further research. We use equal weight for one relation of each type. An improved approach could try to learn the optimal weights by means of training on systems, for which authoritative decompositions are available, comparing its performance using an appropriate metric for nested software decompositions (see [UpMJ2007], [END2004]), and then use the same weights for merging the relations of novel software."
    }, {
      "heading" : "5.3 Alpha-search",
      "text" : "Basic cut clustering algorithm (section 4.3), given some value of the parameter alpha, produces a partition of vertices into groups, i.e. flat decomposition of the system upon analysis. For smaller values of the parameter alpha, there are fewer groups. For higher value of alpha, there are more groups. The groups have nesting property [Fla2004], i.e. they naturally form a hierarchy. The exact hierarchy can be computed by the hierarchical clustering algorithm (section 4.3.2), but this requires running the basic cut clustering algorithm (section 4.3.1) over all the values of parameter alpha producing different number of clusters. There are can be many flow breakpoint alpha-s that can be found fast [Bab2006], of\nthem no more than 2V produce different number of clusters.\nIn our experiments, calculation of clustering for a single alpha was taking 4.5 minutes\nfor 7K vertices, thus it is not feasible to do this operation 2V times. In order to produce as\nmuch as possible result within limited time, we perform the most important probes first. We used a binary search tree approach, described in the subsections.\n5.3.1 Search Tree\nAn initial interval  maxmin ; is chosen as the root of the tree, such that min yields a single cluster and max yields many singleton clusters. These bounds can be found with binary search, as proposed in [Fla2004], but in practice we just use values that produce small enough and large enough number of clusters correspondingly.\nEach child node in the search tree corresponds to a half of the interval of its parent, so\nthat node  rl  ; will have children        2 ; rll   and        r rl   ; 2 . It is convenient to\nAutomatic Structure Discovery for Large Source Code Page 46 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nview the space of alpha values as a tree because in the search algorithm we can then maintain the following invariant:\n At each iteration, there is a tree of alpha-values for which probes (runs of basic cut\nclustering algorithm) have been already performed\n We can use any leaf node as the base for the next probe\nThus the search tree does not have to be balanced. We can do more probes in a more interesting interval (where more fine-grained decomposition of the system will say more to a software engineer), and less probes in another. An illustration of alpha space and search tree is given in Figure 5-8 below. The alpha-interval for each node is denoted with a block arrow.\nEach iteration is an attempt to improve the clustering hierarchy, consisting of the following steps:\n Select a leaf node, and without loss of generality consider its interval is  rl  ;\n Calculate the alpha value for the next probe: 2\nrl\nm\n   \n Run the basic cut clustering algorithm using m  Add two child nodes into the search tree, corresponding to intervals  ml  ; and  rm  ;\nA node does not add a left child (or a right child, by analogy) into the search tree in case\n)()( ml kk   , where )(k is the number of clusters produced by the basic cut clustering\nalgorithm using this value of parameter  . All the above gives a base for prioritization described in the following section.\n5.3.2 Prioritization\nIn the beginning we put the root node corresponding to the whole interval  maxmin ; into a priority queue. It is also a leaf node at this moment, as no child nodes have been added. In the previous section we showed that any leaf node can be taken at each iteration for the next probe. Thus we can maintain invariant that there are only leaf nodes in the priority queue, and chose a reasonable priority function.\nAutomatic Structure Discovery for Large Source Code Page 47 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nLet )(k be the number of clusters that the basic cut clustering algorithm produces\nusing parameter  . Consider we have performed a probe for m  and are now going to push children of node  rl  ; , which are  ml  ; and  rm  ; , into the queue. Then for child  ml  ; (and by analogy, for child  rm  ; ) we set the following priority:\n2 /1)}()(),()(min{)log())()(log( 2, ml\nmrlmlmlmml kkkkkkP    \nBelow is the motivation for each of the summands constituting mlP , :\n 2))()(log( lm kk   forces the intervals spanning large number of clusters (from\n)( mk  to )( lk  ) to be considered earlier.\n )log( lm   forces large intervals to be considered earlier. Here “large” refers to the\ndifference of  , in contrast to the previous point where the difference of the number of clusters is regarded.\n )}()(),()(min{ mrlm kkkk   forces the more balanced intervals to be\nconsidered earlier. This summand contributes most of all into the priority, as it is a big improvement when we e.g. split an interval of 2000 clusters into parts of 1000 and 1000, rather than 1998 and 2.\n 2\n/1 ml   forces to make probes for small values of alpha earlier. The probes at\nsmall value of alpha yield decisions about the upper (closer to the root) levels of the clustering hierarchy.\nOf multiple priority functions considered, the one given in this section demonstrated the best value-for-time in our experiments."
    }, {
      "heading" : "5.4 Hierarchizing the Partitions",
      "text" : "The way we merge the partitions produced by the basic cut clustering algorithm differs from the simple hierarchization method described and employed in [Fla2004] because we do not pass all the alpha-s from the highest till the smallest determined by parametric max flow algorithm as flow breakpoints (see [Bab2006]), but instead we run the basic cut clustering using the “most-desired” alpha as determined by our prioritization heuristic (section 5.3). Thus we must be able to merge the outcome of basic cut clustering algorithm (a partition of vertices into clusters) into the globally maintained clustering hierarchy for arbitrary alpha. In this way we allow arbitrary order of passing through the values of parameter alpha.\nThe need for this ability is further motivated by the intent to compute in parallel\n(section 5.5). Different processors may compute single-alpha clustering (one run of the basic cut clustering algorithm) with different speed, not only due to the difference in computational power, but also because the running time of basic cut clustering algorithm is, in practice, proportional to the number of clusters in the resulting partition. As we discussed in section 4.2.2, this happens due to the community heuristic described in [Fla2004].\nWe solve the following problem: given the global clustering tree, and a result of basic\ncut clustering for  which is not yet in the tree, transform the tree so that it reflects the result of clustering for this new  . Formally:\n Let T be the global clustering tree, in which leaf nodes denote SE artifacts and inner\nnodes denote clusters at different levels of the hierarchy, and the height of the tree is\n)(Theight\n Let vpar be the parent node for node v in T\nNote, that the above formula forbids a case when there are two vertices )(, 2iCvu  and\n)( 1jCv while )( 1jCu .\nOur task is: integrate the clustering result )(C into the global clustering tree T .\nNow we can define it formally. For each )()(  CCi  , find vertex p in T such that there\nexists ))(( palphaC j having )(iC as its subset, i.e. ))(()( palphaCC ji  , but none of the\nnodes in the subtree of p satisfies this requirement. Taking into account the formula in\nFigure 5-9, this task amounts to finding node p such that:\npchivvalphapalpha  ),()( "
    }, {
      "heading" : "5.5 Distributed Computation",
      "text" : "Automatic Structure Discovery for Large Source Code Page 49 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "5.6 Perfect Dependency Structures",
      "text" : "It is now easy to notice that in the cluster tree (section 5.4) alpha-threshold can imply a parent\nnode, i.e. a cluster produced by the basic cut clustering at l , having an excessive number of\nchildren, while every child corresponds to a cluster produced at r . All the lr kk  child clusters do not need to have the same parent however, as demonstrated in a counterexample, see Figure 5-12 below:\nAutomatic Structure Discovery for Large Source Code Page 50 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nIn practice, some nodes in the cluster tree do indeed have an excessive number of children. In our source code domain experiments we observed that there is always one alpha-threshold\nentailing a single node with many children. For example, an alpha-threshold from 433lk to\n3839rk clusters while all 3406 child clusters appear under the same parent in the cluster\ntree. We observed similar effect using any options for:  normalization (sections 5.1.1 and 5.1.2), including the case of no normalization (just\nsumming up the weights of the opposite directed arcs),\n or granularity lifting (section 5.1.4),  or production of the input graph from the various dependencies between SE artifacts (5.2),  or software project upon analysis and the set of libraries included (section 7.1). Thus we conclude that the phenomenon is intrinsic to the domain of software source code, to the best of our knowledge and empirical evidence. Apparently, this phenomenon hinders comprehension of nested software decompositions produced with hierarchical Flake-Tarjan clustering algorithm.\nWhile further study of this phenomenon is a hard theoretical task (but see section 8.4),\nwe make a reasonable assumption that the phenomenon occurs due to a specific property of the underlying data, namely, almost perfectly hierarchical structure of dependencies is a common practice, while software engineers do their best to achieve this."
    }, {
      "heading" : "5.6.1 Maximum Spanning Tree",
      "text" : "Consider the issue of an excessive number of children (due to alpha-threshold, section 5.6) occurred for some node in the cluster tree, thus its cluster has many nested clusters at the immediately next level, i.e. the decomposition is flat. A flat decomposition containing many items is not nearly as comprehensible as if we hierarchize the items so that a kind of divide-nconquer approach is applicable for comprehension of the subsystem. Thus let us hierarchize the flat decomposition.\nLet pC be the parent cluster containing np (excessive number of) child clusters\npnppp CCC ,2,1, ,...,, , thus pnpppp CCCC ,2,1, ... . For a cluster C let\n)(CV be the set of vertices of the input graph (they are also the leaf nodes of the cluster tree,\nand they are also SE artifacts like Java classes) which constitute the cluster C .\nFirst of all, we create graph ),( ppp EVG , where pV contains np vertices, i-th vertex\nstands for i-th cluster ipC , , and each edge in pE has weight jie , equal to the aggregated weight over all the edges of the input (SE artifact relation) graph connecting a vertex from\ncluster iC to a vertex from cluster jC .\nSecond, we assume that there is an almost perfect hierarchy in pG , and the rest is\nnoise. Thus our task is to filter “signal” from “noise”. The hierarchy is the signal, and the\nAutomatic Structure Discovery for Large Source Code Page 51 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\ncycles in pG are noise. “Hierarchy” can be formalized as the subset of pp ET  being a\ntree, in which an edge from parent to a child denotes the decomposition intended by software engineers (e.g. reduction from a task to subtask, or from general to specific, etc). Noise is the\nrest of edges, namely pp TE  , and each of them is either a violation of the architecture (e.g.\na “hack” written by a software engineer), or the noise propagated from call graph construction (section 4.4), or a minor relation between SE artifacts."
    }, {
      "heading" : "5.6.2 Root Selection Heuristic",
      "text" : "Obviously, the same maximum spanning tree is illustrated both in the left and in the right of the above picture. However, the understanding about which SE artifact is more highlevel/general or low-level/specific totally differs.\nAutomatic Structure Discovery for Large Source Code Page 52 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nOf the considered options for selection of the root, two seemed reasonable and we did\nexperiments with them.\nOption 1. The intention is to select the root in such a way, that heavy cycles (the noise\nremoved from pG ) appear as far as possible from the root, i.e. closer to the leaves. The\nalgorithm for this option is given below:\n(1) Sort all the edges of graph pG in the order of their weights, heavier first\n(2) Let U be the set of disjoint subsets of the vertices of pG\n(3) Passing the edges of pG from the heavier to the lightest, do\n(4) (1) If the edge is in the tree pT , unite in U its incident vertices\n(5) (2) Else, find the path between its incident vertices through only the edges in pT ,\nand unite in U all the vertices encountered on the path\n(6) (3) If U has become a single subset, stop the passing of edges. (7) End. (8) The last united vertex (or the weighted middle of the path, if multiple), becomes the\nroot of pT\nDisjoint-set data structure and union-find algorithm was used for U , see section 4.6. The\nalgorithmic complexity of the root selection is:  )(log pppp VkVEEO  where )(nk is the inverse Ackermann function.\nOption 2. The intention is to select the central node, while the selection is prioritized\nby the weights of the edges in the tree only (i.e. not in pG ). The algorithm in this case is\nprioritized breadth-first search starting from the leaves. Initially, all the leaves are put into the priority queue. When a vertex is removed from the queue, we decrease the “to go” counter for its single adjacent vertex. If “to go” counter becomes 1, this adjacent vertex is put into the queue with priority equal to the weight of the incident edge (the more weight, the earlier will be removed). “To go” counter for a vertex denotes the number of adjacent vertices which have not yet been regarded, and is initially equal to its degree. The last vertex pushed into the\nqueue becomes the root of our maximum spanning tree pT . The algorithmic complexity of\nthis root selection option is: )log( pp VVO  .\nIn practice, the second root selection option is producing empirically much better\nhierarchies. The results we are showing throughout the paper are processed with this heuristic after hierarchical clustering. We can see (sections 7 and 10) that indeed, the problem of excessive child clusters has been alleviated, and SE artifacts are still grouped according to their unity of purpose."
    }, {
      "heading" : "6 Implementation and Specification",
      "text" : "We implemented parallel computation of hierarchical Flake-Tarjan clustering within this project as multiple OS-processes on our double-core processor, each working in separate directory. Changing the prototype to working on multiple computers amounts to sharing the parent directory over the network and launching remote processes rather than local.\nThe choice of programming language was\ndriven by whether we need speed of implementation or runtime speed of the program. Most of InSoAr, 14K lines of code, is implemented in Java: the source code is 434KB in size, and it was all written by one programmer, the author of the thesis, within the short time period of this project. Some state-of-the-art source code metrics over InSoAr are produced with STAN ([Stan2009]) and demonstrated in Figure 6-1 to the right.\nThe bottleneck part, minimum cut tree\nalgorithm (section 4.2.1) using the community heuristic (section 4.2.2), is implemented in C, and uses Goldberg’s implementation of maximum flow algorithm (section 4.1.1) modified for our needs. We used all possible including low-level optimizations for the bottleneck part.\nA visualization of InSoAr at package-level (not\nthe class-level InSoAr operates) with, to our knowledge, the best state-of-the art structure analysis tool STAN [Stan2009] is given in appendix 10.4.1, and a zoomed-out version in Figure 6-2 below. The shadow is the sliding window visible in full size.\nFigure 6-1 Metrics over InSoAr\nAutomatic Structure Discovery for Large Source Code Page 54 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "6.1 Key Choices",
      "text" : "Most of the key choices are theoretical, thus described under sections 3, 4 and 5. We do not provide a blow by blow description of InSoAr due to the nature of the paper, limit in pages and size of the system. Below are the most important, though applied aspects.\n6.1.1 Reducing Real- to Integer- Weighted Flow Graph\nAfter normalization (section 5.1) we get an undirected graph with real-valued edge weights. Flake-Tarjan clustering algorithm (section 4.3) also uses real-valued parameter “alpha” in order to prepare a minimum cut tree task (section 4.2). The algorithm solving the min cut tree problem relies on computations of maximum flow in a graph (section 4.1). Though there are algorithms solving maximum flow problem for real-valued edge capacities, however, they are much slower. Both the fastest known max flow algorithm (we use it for theoretical bounds on the worst-case complexity, section 4.1) and the best known implementation of another max flow algorithm (section 4.1.1) we used in practice, require integer arc or edge capacities. Thus we must convert from real- to integer-weighted graph.\nFor each vertex in the graph we calculate the sum of weights of the adjacent edges. Then\nwe adjust the weights proportionally, so that they have the largest possible integer values, taking into account the limitations of 32-bit and 64-bit integers. The latter two are used as edge capacities and excess flow in Goldberg’s implementation of push-relabel max flow (section 4.1.1). Our experiments have shown that max flow solution never became suboptimal due to this conversion.\n6.1.2 Results Presentation\nThe result of hierarchical clustering is a tree (more precisely, a forest, when there are multiple disjoint components in the software artifact dependency graph), where\n- Leaves are classes of the software upon analysis and its libraries. - Inner nodes are clusters at different levels. - There is at least one root per disjoint component. - Multiple roots per disjoint component appear in case the selected lower bound of alpha\nwas not low enough to unite all the nodes of that component into a single cluster.\nAs it is not trivial to present the results in a comprehensible form, some aspects of the used presentation approaches are described further. Our main representation of the results is in text format. Not going into the details of each value, we see 3 principal ways to represent a tree:\n1 Indented by Depth 2 Indented by Height 3 Bracketed\nThe first is more convenient to view, as nested clusters (or SE artifacts, if leaves) appear under their parents. An example of this presentation is in Figure 7-3. However, this presentation takes a lot of space on hard drive. The second presentation has an advantage that SE artifacts (the nodes that have labels) always appear in the beginning of a line, as they are leaves thus have height 0. However, effective comprehension of this presentation needs some training, see appendix 10.4.3.1. This presentation also takes less space, as nodes are often at large depth, but rarely at large height. The third, bracketed presentation, aims to show much more labels (leaf nodes) on a limited space. Inner nodes do not take a line each, but are grouped in one line and represented as brackets. An example is in Figure 10-10."
    }, {
      "heading" : "6.2 File formats",
      "text" : "A number of file formats is used at various stages of the software engineering artifacts extraction and clustering pipeline. It does not make sense to describe them in detail at this stage. Thus we give a short list of the formats:\nAutomatic Structure Discovery for Large Source Code Page 55 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n Text (identifiers like “call graph” etc. appear for historical reasons, indeed they contain\nheterogeneous relations):  Literal graph of relations (user friendly form): litCallGraph.txt  Computer-friendly form of the graph of relations: callGraph.txt, ccgClasses.txt,\nccgMembership.txt, ccgMethods.txt)\n Cluster tree: ctHier.txt, perfTree.txt, treeXXXX.txt, hitPerfClu.txt (height-indented),\nctBracketed.txt (bracketed presentation)\n Inputs for Cfinder (list of arcs)  Inputs for H3Viewer: h3reduced2.lvlist, h3sphere.lvlist  Inputs/outputs for a process performing basic cut clustering: passOrder.txt, intGraph.txt\n(DIMACS format), ver2node.txt, ftClusters.txt, ftcConOut.txt\n XML:\n Cluster Tree XML  Per-package statistics in XML  Input for TreeViz: perfTv*.XML\nFor example, below is a short description of the cluster tree XML format. Several XML representations were considered, e.g. an XML element corresponding to a cluster tree node could contain properties like “alpha” and “heads” as nested XML elements along with an XML element “children” which would list all the child nodes and their subtrees. However, we attempted to choose a representation that is easier to view by a human, and this should be the one that contains only child nodes as the child XML elements for a node, i.e. homogenous.\nThe root node looks like below: <clusterTree vertexCount=”7474” nodeCount=”7721” rootCount=”6476” disjointCount=”2”>\nBelow is an example of an inner node (cluster), “alb” is the alpha at which the cluster was\nproduced, “djComp” is the number of its disjoint component: <node id=”7477” childCount=”2” alb=”0.01780273437500000000” heads=”5287, 7710” djComp=”1”>\nBelow is an example of a leaf node, i.e. a SE artifact (Java class in this case): <node id=”4578” label=”net.sf.freecol.client.control.InGameInputHandler” djComp=”1” />"
    }, {
      "heading" : "6.3 Visualization",
      "text" : "Pure XML or HTML formats, GraphViz and FreeMind tools were considered. However, we chose the following visualization tools because they perform well at large trees:  H3Viewer: http://graphics.stanford.edu/~munzner/h3/download.html\n This tool can draw large trees in 3D hyperbolic space\n TreeViz: http://www.randelshofer.ch/treeviz/index.html\n This tool supports 7 different presentations for large trees"
    }, {
      "heading" : "6.4 Processing Pipeline",
      "text" : "The runtime of the analyzer is divided into stages, where outputs from a preceding stage are inputs to a succeeding stage. Outputs are flushed into files. This allows reusing the results of a stage without re-running it, as well as substituting different implementations of a stage, e.g. Java or C#, static or dynamic call graph extractors. Below is a diagram of the present stages:\nAutomatic Structure Discovery for Large Source Code Page 56 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nIn Figure 6-3, processing stages are drawn as rectangles, while inputs or results are drawn as parallelograms. The pipeline takes source code on input. Source code should be built, in order to resolve library dependencies. Then call graph and other relations between software engineering artifacts must be extracted. In the current implementation, using Soot to process Java programs, we produce the graph of relations in user-friendly form. Java classes are on the outer level, inside are methods and fields, and for each method there is a list of relations with other member- or class-level SE artifacts. If something can produce such a graph of relations from other programming language, e.g. C# or C++, we do not depend on programming language since this point. A graph of relations produced by means of dynamic analysis is also an option here. Then we run a stage called “SE Relations Compactor”, which converts the graph of relations into a\nAutomatic Structure Discovery for Large Source Code Page 57 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\ncomputer-friendly form. Though it is also a text format, it occupies substantially less space and is easier to read into memory. “SE Relations Compactor” also performs some reindexing, so that further stages a released from these operations after input.\nImplementation of the further stages follows the theory we gave in section 5. After\nloading the graph of relations, a stage called “Granularity Selector” allows to choose whether we are going to clusterize at class- or method-level, and can be used to lift the granularity prior to clustering. Its output is a directed graph of relations between SE artifacts. “Undirected Real Normalizer” converts a directed graph to undirected, normalizes and lifts the granularity to class-level, if necessary. It holds a graph, from which the initial “Cluster Tree” can be built. The initial cluster tree contains all the SE artifacts as leaf nodes, which are children of one fake root, even if in different disjoint components. “Cluster Tree” is updated incrementally by “Hierarchizer”. The latter maintains the alpha search tree, prepares a new task for basic cut clustering processor, receives the result and merges it into the global cluster tree. “Partitional Clusterizer” is a separate, probably remote, process that performs a single flat clustering using Flake-Tarjan algorithm, taking the input from a file and producing output into a file. There is always an option to use named pipes instead of files here, so that slow hard drive is not needed.\nThe current cluster tree is flushed every certain amount of minutes to disk. This is\n“Incremental Cluster Hierarchy”. We can force the pipeline to stop by creating file “shutdown.sig”. Then the latest hierarchy is also saved to disk. “Perfectizer” addresses the issue and computes solution we discussed in section 5.6. It takes the result of hierarchical clustering on input, and produces perfected result in the same format. This step is, of course, optional. The rest of the pipeline after “Perfectizer” addresses various presentations, evaluation and post-inference.\nAutomatic Structure Discovery for Large Source Code Page 58 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "7 Evaluation",
      "text" : "The main premise for high quality of a produced clustering hierarchy is the theoretical grounding of the clustering algorithm we used: the quality of the produced clusters is bounded by strong minimum cut and expansion criteria [Fla2004]. We consider cut size a rational criterion in the domain of software engineering because the sum of edge weights reflects the amount of interaction (relations) between SE artifacts (e.g. Java classes), which a software engineer needs to study in the source code in order to understand coupling between either two SE artifacts, or two groups (communities, clusters) of SE artifacts. This matches the main idea behind maxflow/min-cut clustering technique, according to [Fla2004]: “to create clusters that have small inter-cluster cuts (i.e. between clusters) and relatively large intra-cluster cuts (i.e. within clusters). This guarantees strong connectedness within the clusters and is also a strong criterion for a good clustering, in general.”\nAssuming from the above that the clustering algorithm performs well, we should study\nwhether this quality has not been lost due to the adaptations we used for the clustering to work in the domain of software source code, see section 5. These adaptations also include extraction of the call graph and other relations between SE artifacts, which is data, specific to the domain. We stress that not only the quality of the clustering method is important, but it is also important that its input data is adequate and of high quality, see sections 4.4 and 3.1.1.\nAnother theoretical premise for high-quality of the reconstructed architecture is that we\nhave incorporated a solution (section 5.1, which follows state of the art best practices discussed in section 4.5) to the main problem for clustering in source code domain, according to the literature [Pir2009], – utility artifacts."
    }, {
      "heading" : "7.1 Experiments",
      "text" : "In the largest of our experiments we processed software containing\n 2.07M (2 070 645) graph edges (relations) over heterogeneous set of vertices (SE\nartifacts) containing\n 11.2K (11 199) Java classes,  163K (163183) of class member level artifacts (methods and fields)\nThis is a real-world medium-size project provided by KPMG for our experiments during the internship. The client code contains about 500 classes, thus the remaining 10.7K classes are in libraries, which include Java system libraries, Spring framework, Hibernate, Apache commons, JBPM, Mule, Jaxen, Log4j, Dom4j, and others. Together with libraries the project becomes 22 times bigger and falls into category of large software.\nNote that our conception of a medium-sized project differs significantly from the claimed\nin other scientific works. In [Pate2009] they analyze (mostly, clusterize) a project containing 147 classes in 10 packages. In practical software engineering this project must be classified as small or even above-tiny. In contrast, just the client code of our medium-sized projects contains 500- 1000 Java classes. The total number of Java classes we clustered hierarchically is 11 199 in the largest experiment, and 7000-7500 in the usual experiments.\nFurthermore, we suspect that the input data of related works analyzing only a part of the\nprogram (e.g. only client code) was far not as precise as ours, because advanced call graph extraction techniques (VTA, Spark in Soot) require analysis of the whole program with libraries, and even simulate native calls of the Java Virtual Machine [Lho2003]\n7.1.1 Analyzed Software and Dimensions\nFreeCol is an open source game similar to Civilization or Colonization. Its source code is in Java and available here: http://www.freecol.org/download.html . The project is medium-size, containing about 1000 of client-code classes. Together with libraries it becomes about 7.5K classes. Thus we used it in our experiments. The extracted graph of relations contains 1M edges for this project.\n“dem0” project is a web application that also provides web services, uses Spring\nframework and works with database through Hibernate. It is not open-source, thus we are only showing the parts for which we received permission from KPMG. This project contains about 500 classes of client code and many classes in libraries. In order for Soot to fit in 2GB memory limit during VTA (variable type analysis) call graph construction, we had to limit the number of library classes to 6.5K. In the largest experiment we used RTA (rapid type analysis) for call graph construction, thus it was possible to process all 10.7K library classes with Soot. In the former case, the graph of relations contained 0.5M edges, while in the latter there were 2M edges."
    }, {
      "heading" : "1 InSoAr processing",
      "text" : "1.1 Clustering hierarchy we demonstrate in this paper 72 hours,\n0.6GB RAM\n1.2 Acceptable results (differences are visible empirically, conclusions\nneed statistical studies)\n1-2 hours\n1.3 The largest experiment (11.2K classes, 2M relations) 1.3GB RAM,\n120 hours\n2 Call graph construction (and other relations with Soot) 2.1 VTA in usual experiments, 7.5K classes: 2GB RAM\n0.5 hour\n2.2. RTA in the largest experiment, 11.2K classes:\n(VTA gets out of memory in this case)\n2GB RAM, 2 hours\n3 Basic cut clustering (one alpha, in a separate process) 3.1 In the usual experiments 4.5 minutes 3.2 In the largest experiment 20 minutes 3.3 Memory requirement, no more than 35MB"
    }, {
      "heading" : "7.2 Interpretation of the Results",
      "text" : "Automatic Structure Discovery for Large Source Code Page 60 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nFor a cluster node (which is non-leaf, inner node), not only the depth (distance from the root), but also the height (distance to the remotest leaf in its subtree) should be considered.\n7.2.1 Architectural Insights\nIn the subsequent sections we provide an account of particular facts, which become apparent to a sufficiently experienced software engineer by browsing the (various presentations of) the results produced with our prototype. Mining these facts with state of the art tools is either not possible, or requires immense efforts, e.g. browsing and interpreting manually many lines of source code. In general, we call the inferred facts “architectural insights”, as they help the viewer to, at least, get a first impression of the source code, and mostly comprehend the decomposition of the software system into subsystems. Taking 10M lines of source code on input, InSoAr produces only about 10K nodes of cluster tree on output. The gain in comprehension is 1000 times, which is, roughly, calculated from the number of items necessary to scan in order to get a global understanding of the system, see section 2.1.\nHaving a nested software decomposition provided by InSoAr, a software engineer can\neffectively apply divide&conquer approach for software comprehension (appendix 10.1.6), or detect cross-package subsystems implementing complicated logic (appendix 10.1.5). One can also observe some metrics calculated after architecture reconstruction, and we give some examples in appendix 10.2. These metrics can give idea of how ubiquitous a package is (i.e. how broad in the architecture the classes of this package are spread, appendix 10.2.1), and how well couplings between SE artifacts fit the implicit architecture (appendix 10.2.2). Often, insights not only about architecture, but also about implementation can be captured. We give such examples in appendix 10.1.4.\nCertainly, the list cannot be exhaustive as these are only example architectural insights\nwe could think of and describe within limited time and pages. We invite the reader to browse the hierarchy on his/her own by downloading the clustering hierarchy of a demo project and the H3 sphere visualizer from the internet. Below are the links:  Data files. Leaf nodes of the trees correspond to Java classes of libraries and client code.\nInner nodes correspond to clusters at different levels in the hierarchy. Client source code (the\napplication, i.e. non-libraries) is in package com.dem0.*  In XML: http://zrobim.info/InSoAr/Demo/ds0/clusterTree.xml  In H3Viewer format: http://zrobim.info/InSoAr/Demo/ds0/h3sphereCT.lvhist  In TreeViz format: http://zrobim.info/InSoAr/Demo/ds0/treevizCT.xml\n H3Viewer: please, download it from the website of its developer:\nhttp://graphics.stanford.edu/~munzner/h3/download.html\n TreeViz website: http://www.randelshofer.ch/treeviz/index.html\n BUT: we have tuned TreeViz within our project, so that it shows client-code artifacts in\ngreen (and the rest is in orange), and lists descendants of a subtree upon mouse hover, when no more than 100. Download the archive and unpack the two files into the same directory before running.\n Tuned TreeViz: http://zrobim.info/InSoAr/Demo/ds0/TreeVizCliFi.zip\nAutomatic Structure Discovery for Large Source Code Page 61 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n7.2.2 Class purpose from library neighbors\nLibrary neighbors can tell an experienced software engineer a lot about the purpose of client code classes, see Figure 7-2 below. This follows directly from the criteria for clustering: dense interaction (many calls, field accesses, type usages) between SE classes within a cluster and relatively loose interaction between classes from different clusters.\nThe crucial advantage that software engineers acquire having software structure inferred\nwith InSoAr is in the following. In order to figure out the purpose of library classes, as well as other facts like requirements, constraints and limitations, one usually can read the documentation. Application classes, on the other hand, are not well documented (section 2), thus software engineers would have to scan and interpret manually the source code of the class. However, having our clustering hierarchy, a software engineer can simply read the documentation for library classes which are coupled with the application classes upon analysis.\nIn the above figure, we see a subtree with classes serving the same purpose, as can be understood from their names. One fact that we easily infer is that the application’s subsystem for time and\nscheduling relies on JodaTime library rather than inferior Java system library for time.\nAutomatic Structure Discovery for Large Source Code Page 62 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "7.2.2.1 Obvious from class name",
      "text" : "One can argue that the clustering hierarchy does not bring any value about the purpose of a SE class when the class appears near similarly named, sometimes library classes, because class purpose was already obvious from class name, as in Figure 7-3 below. In this figure we see that application class com.kpmg.kpo.web.security.EmployeeUserDetailsService and others appear coupled (descendants of cluster #8604) with library classes org.springframework.security.userdetails.UserDetailsService and org.springframework.security.userdetails.UserDetails. However, the point is:  The fact that these similarly named classes got into the same cluster tells us about good\narchitectural style: classes with similar names serve a similar purpose. The purpose of the library classes is known from the documentation.\n Application class “…EmployeeUserDetailsService” is most coupled with library classes\nwhich are supposed to serve this purpose, and not with something else, which would be architecture violation\nIn addition to the above points, nearby we also see classes with very different names and from very different packages, e.g.\n GrantedAuthority from library package org.springframework.security,  AssignRolesCommand from com.kpmg.kpo.web.binding,  ApplicationManager from com.kpmg.kpo.domain  anonymous nested classes of EmployeeRole from com.kpmg.kpo.domain\nAs a result, a human software engineer is provided with an insight about the subsystem, which:\n…manages user details, where the users are most likely employees, and there is a dedicated service for this, which is based on the standard service of Spring framework\nAutomatic Structure Discovery for Large Source Code Page 63 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\naddressing this purpose. When a user becomes authorized by the subsystem, a corresponding security token is issued (class …GrantedAuthority), which is a string (look at nested class StringAuthority under …EmployeeUserDetails). When authorization fails (perhaps, only for the reason that there is no such user/employee), …UsernameNotFoundException is thrown. The latter is a standard exception from Spring framework, thus it is likely that the client code (application classes) does not handle this exception at all or in full, but rather relies on the standard facilities of Spring framework, otherwise a more specific exception inheriting …UsernameNotFoundException would be implemented in the application and appear nearby in the clustering hierarchy. The set of business entities which an employee can access is determined through assignment of roles, application class …EmployeeRole, and roles are assigned using com.kpmg.kpo.web.binding.AssignRolesCommand, which is likely to occur when a privileged user takes the corresponding action from web UI.\nWe wrote the above paragraph without looking at a single line of source code of either of the mentioned classes, even more, having almost no experience with Spring framework, just principal understanding of programming concepts.\nAutomatic Structure Discovery for Large Source Code Page 64 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "7.2.2.2 Hardly obvious from class name",
      "text" : "In contrast to the previous example, it is not that easy to realize the purpose of a class called com.kpmg.kpo.generated.jaxws.crm.CrmSOAP. CRM is likely to stand for Customer Relations Management and SOAP is the well known (otherwise, it is as easy as a search in Google) Simple Object Access Protocol for exchanging structured information for web services. The latter two potential concepts are pretty distant from one another. Its situation in the clustering hierarchy makes things much more clear, namely, the following facts becomes apparent to a human software engineer:  …CrmSOAP is much more about SOAP than CRM, because it is clustered together with\nSOAP-related library classes.\n If the software engineer was not familiar with SOAP, after seeing the clustering hierarchy\nhe/she can realize that XML underlies SOAP, because the neighbor library classes are in javax.xml package\nSee Figure 7-4 below and a 3D view on the same part of the cluster tree in appendix 10.4.8.1.\nAutomatic Structure Discovery for Large Source Code Page 65 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "7.2.2.3 Not obvious from class name",
      "text" : "In this example a class is called …AuditEntryDTO which says nothing about its purpose, unless we know that the software project is heavily related to Auditing business and lookup DTO in Wikipedia: http://en.wikipedia.org/wiki/Data_transfer_object . After the above two steps we know still do not know why it is “Entry”, i.e. entry of what?\nHowever, a glance at the clustering hierarchy makes things clear; perhaps even replacing\nthe need for the two aforementioned steps, see Figure 7-5 below. Apparently, there is some logging (classes containing “Trail”, which is a synonym for logging, and ConsoleAuditLogImpl). That is why “Entry” – it is an entry of some log (namely, audit log). And the logging is implemented as a service, transferring AuditEntryDTO objects between software application subsystems."
    }, {
      "heading" : "7.2.2.4 Class name seems to contradict the purpose",
      "text" : "While browsing through the clustering hierarchy we encountered an example where class name seems to contradict the purpose. Though a class is named com.kpmg.kpo.action.GenericHandler, it appears in the cluster that addresses Java Regular Expressions and expression evaluation in JBPM, http://www.jboss.org/jbpm . This is a strong claim involving also doubts about clustering quality, thus we looked into the source code of the class (provided in section 10.3.3), which is obviously confirming the result of clustering.\nLet us look at this case closer in terms of software quality. The fact that class name\ncontradicts the purpose does not just mean that the class is named incorrectly, which can seem a minor defect. Indeed, it means that the designers of the architecture saved efforts (i.e. took “reduce quality” action, as we discussed in section 2.6) at some point during software development cycle. What can be the reason for not naming a class properly? Most likely, it happened because the purpose was not identified properly, and identification of purpose constitutes significant amount of design efforts.\nWe identify two\nconsequence of such fact, for programmers and for business:  Programmers (developers\nworking directly with source code) get a wrong idea about the purpose of the class when considering its reusage, e.g. through inheritance, modification (adding/removing/changi ng methods) or simply usage from another place in the software\n Companies that buy or\ntake for outsourcing services the source code containing such architectural violations, get less value than they may think they get, as at some point the earlier saved design efforts will “pay-off” with unexpected expenses\n7.2.3 Classes that act together\nIn our view, the most valuable inference InSoAr makes is detection of sets of classes that act together. This follows directly from the property of clustering and the data we analyze: coupling of SE classes within a cluster is higher than coupling between clusters. To our knowledge, there is no means to identify efficiently (in terms of human efforts) such groups with any existing static or dynamic code analysis tools. As was discussed in section 2.2, state of the art tools either allow user to select a set of SE artifacts for which the user wants to see their couplings, or to drill down from packages to subpackages, classes and methods ([Stan2009], [Rou2007], [Pin2008]). In contrast, we do this globally, for all the SE classes at once. Less coupled classes get into a group only after more coupled classes have been sent into that group, where the former stands for higher levels of the clustering hierarchy (closer to the root), and the latter stands for lower levels (closer to the leaves).\nBelow is an example a piece of XML output that demonstrates the claim. <node id=”8837” childCount=”2” alb=”0.84384472656250000000” heads=”7179, 8897”>\n<node id=”8897” childCount=”2” alb=”0.89071943359375000000” heads=”7179”>\n<node id=”7179” childCount=”2” alb=”2.00008750000000000000” heads=”241”>\n<node id=”9104” childCount=”2” alb=”2.12508671874999950000” heads=”241”>\n<node id=”241” label=”com.kpmg.kpo.domain.TaskInstance” /> <node id=”654” label=”com.kpmg.kpo.service.impl.AbstractTaskInstanceService” />\n</node> <node id=”790” label=”com.kpmg.kpo.web.view.TaskInstanceView” />\n</node> <node id=”550” label=”com.kpmg.kpo.jbpm.AssignToEmployee” />\n</node> <node id=”8898” childCount=”2” alb=”0.93759414062500000000” heads=”219”>\n<node id=”219” label=”com.kpmg.kpo.domain.PeerStatusType” /> <node id=”712” label=”com.kpmg.kpo.usertypes.PeerStatusTypeUserType” />\n</node>\n</node>\nAutomatic Structure Discovery for Large Source Code Page 68 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "7.2.3.1 Coupled classes are in different packages",
      "text" : "Detection of class coupling across packages/namespaces is important for the reasons discussed throughout the paper (implicit architecture without scanning millions of lines of source code manually), we just give a few examples below as the evidence that InSoAr does grouping of classes together according to their unity of purpose, which can be validated from the names of the classes.\nAutomatic Structure Discovery for Large Source Code Page 69 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "7.2.3.2 Coupled classes are in the same package",
      "text" : "As software engineers often put most coupled classes in the same package/namespace, in addition to naming them similarly, the fact that classes from the same package appear nearby in the clustering hierarchy can serve as validation for the clustering results. We can observe a match of the explicit and implicit architecture in this case. A useful fact that becomes apparent after looking at a cluster of classes from the same package, differentiation of coupling, is discussed in section 7.2.5.\nIn Figure 7-8 below we can see a number of classes that act together. Class\nPortfolioManagerComponent is from package com.kpmg.esb.mule.component, while the rest of classes are from package com.kpmg.service.portfoliomanager. We can infer that, most likely, PortfolioManagerComponent is a high level class that operates the simple classes in its cluster. Classes PortfolioManagerComponent, PortfolioManagerType and ServiceType (cluster #8964) are the most coupled among the group displayed in the figure. The second\nAutomatic Structure Discovery for Large Source Code Page 70 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n7.2.4 Suspicious overuse of a generic artifact\nIn this example we see a class called GenericComponentException which is, however, coupled with class DocumentComponent from a different package, see Figure 7-9. We rather mention the fact that the classes are from different packages for convenience of the reader, in order not to forget that state of the art tools cannot help. However, the observation that helps to discover an issue here is in the fact that the class representing the exception is called “Generic…” while in the cluster tree we can see that it is coupled and thus serves error-handling for a specific class DocumentComponent. We can guess (without looking at the source code, thus saving efforts 1000 times) that this happened  either because the purpose of GenericComponentException was not well identified while\ndesigning the architecture, and it should rather be called DocumentComponentException (or something even more specific – a study of the source code is needed),\n or because even though the purpose of GenericComponentException was well identified\nand at some places in the source code it indeed serves as a generic artifact (e.g. as the base for inheritance to more specific exceptions), during the evolution of software it happened that this “generic” artifact was too heavily used in class DocumentComponent.\nIn the second case, a suggested improvement of the architecture is to create another exception class specific to DocumentComponent, e.g. “DocumentComponentException” and refactor the source code of DocumentComponent to make it using this dedicated specific artifact.\nWith the two points above we have exhausted the possible cases, i.e. there is no reason to\ncall an exception-class GenericComponentException while it mostly serves (and is mostly coupled with) a class called DocumentComponent. Thus the source code is not optimal, while detecting such a defect in novel source code (i.e. when there is no programmer that knows about it) is not possible with state of the art tools, except that by scanning all the source code line by line. The benefit of clustering is obvious: 10M lines of source code vs. 10K nodes in the cluster tree.\nAt any rate, the detected architecture violation says about saved efforts during the design\nof the software, and will result in unexpected expenses later, by analogy to what we discussed in section 7.2.2.4.\n7.2.5 Differentiation of coupling within a package\nAutomatic Structure Discovery for Large Source Code Page 73 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nclasses with very different purposes (in contrast to what we observe here) is demonstrated in section 7.2.6. In principle, InSoAr differentiated the coupling within that package too, but there is a separate fact to be discussed because the classes appeared scattered across the system.\nDescribing more extensively, Figure 7-10 shows two representations of a set of classes,\nwhile not all the classes from the left side have to be present in the right side: the rest can appear somewhere else in the cluster tree. An alphabetical list of classes in a package is on the left, and this is what a software engineer sees with state of the art tools (IDEs). A subtree containing many of the classes from the list is displayed on the right, and this is what we can see in the cluster tree produced by InSoAr. The task here is that a user needs to infer the purpose of these classes or how they are related to each other, including a global understanding, i.e. not just pairwise relations. Our argument is that this is much (in this context, our “much” usually means 1000 times across the paper) easier to do having the cluster tree.\nWhen the class names in the package are not very meaningful, accomplishing of this task\nfor a human expert amounts to scanning the source code of the classes, which is usually 1000 lines per class. Even after scanning the source code, there is a comprehensional difficulty in taking into account thousands of the observed facts at once (for humans). To alleviate this, human experts need some diagrams to be drawn, which is a mechanical difficulty. In the remaining case when the class names are very meaningful, the user can pick out the groups from\nthe list, which is debatabely )log( NNO  operations in the mind of the user (if the user follows\nsorting based on pairwise similarity comparison, and disjoint subsets unification algorithms), where N is the number of classes in the package: the classes are sorted alphabetically, however the first token is not necessary the one that gives the user an idea about proper grouping, think of GetClientsResponse and SendClientsResponse in Figure 7-10 above. Even in this rare case of very meaningful names of classes in a package containing many classes, obviously, a software engineer benefits from having the cluster tree.\nFrom Figure 7-10, as well as Figure 10-10 (appendix 10.4.3.2) and Figure 7-11\ndemonstrating more or less the same fragment of the clustering hierarchy, we can see that coupling of classes differs even though they are in the same package and appear as a plain list in IDE. We claim that this differentiation is an important feature that facilitates program comprehension by a software engineer. For example, we immediately see that class ObjectFactory has a different nature than GetClientsRequest, or GetClientsResponse, or others from that upper group in Figure 7-10. The bottom group is the most coupled within itself, and then to the rest of classes than any other class shown in the figure. Without looking at a single line of source code, we guess that ObjectFactory is some manager-class, while PermissionsType (note “s” after “Permission”) and SimpleClientType are the most thorough watched by it.\nOn the other hand, we also see in Figure 7-11 evidence for correctness of grouping.\nClasses PermissionType (note the absence of “s” after “Permission”) and AccesRightType got clustered together, and guess from the names that this is semantically true.\nAutomatic Structure Discovery for Large Source Code Page 74 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nApparently, representational power differs across the three our textual representations of\ncluster tree, in terms of number of labels (only leaf nodes have them) that can be shown to a user within limited space and the easiness of interpretation of the presented information by the user (software engineer). The bracketed approach, Figure 10-10 in appendix 10.4.3.2, is the most powerful in terms of the number of labels (classes, leaf nodes) that can be displayed within the same space. However, efficient comprehension of this representation needs some training and familiarity with nested structures, i.e. trees where only leaf nodes have labels.\n7.2.6 A package of omnipresent classes\nAnother example is essential for understanding of our endeavor and the advance over state of the art tools. In section 7.2.5 we have shown that InSoAr can differentiate coupling within a package and thus facilitate comprehension of the package and classes in it. However, the classes from that package were devoted to more or less the same purpose.\nIn this section we show a principally distinct case, where even though classes are in the\nsame package, they serve different purposes. Figure 7-12 shows on the left how such a package looks in Integrated Development Environment (IDE), and splitting classes into packages is an instance of explicit architecture being declared and implemented by software engineers. However, according to how the source code was written (the implicit architecture), each (well, almost each) of these classes is coupled with a distinct group of classes from other packages (serves a distinct purpose), and this is shown on the right of Figure 7-12. Further evidence is provided in appendix 10.1.1. Thus we conclude that developers grouped classes into com.kpmg.kpo.domain package according to some more high-level purpose, e.g. because the\nAutomatic Structure Discovery for Large Source Code Page 75 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nclasses are omnipresent (and our decomposition of the software system says that they are indeed omnipresent).\nOn the other hand, we can also see that package com.kpmg.kpo.domain is 28 th (out of\n98) in ubiquity among client-code container artifacts (packages, or classes that have nested classes) and ranked as 338 th (out of 1235) among all the containers including libraries, see Figure 10-3. Its average merge height (the second column) is 20, which is not high relatively. This means that the classes of this package become united into a single subsystem (containing classes from other packages too) more or less soon, not too far from the bottom of the hierarchy. Thus we conclude that there is still another high-level purpose, except omnipresence discussed in the previous paragraph.\nWithout looking at a single line of the source code of either of these classes, we will not\nbe surprised in case their mission is to support Object-to-Relational Mapping (ORM) 2 , where a class is also a table in the database, and instances of this class are also rows of that table 3 . We conclude this from the following facts (and of course, InSoAr gave us those facts):  class com.kpmg.kpo.domain.DomainEntity got clustered with com.kpmg.kpo.dao.Dao4\nand com.kpmg.kpo.dao.DaoFactory (see the top-right part of Figure 7-12)\n class com.kpmg.kpo.domain.AuditLevel got clustered with\ncom.kpmg.kpo.dao.AuditLevelDao (see the bottom-right part of Figure 7-12)\nFurthermore, we see that this ORM is supplied by Hibernate 5 technology, as in Figure 10-1 (appendix 10.1.1) class com.kpmg.kpo.domain.ArchiveEntry got clustered with com.kpmg.kpo.dao.hibernate.HibernateArchiveDao class.\nTo recap, just by looking at the hierarchy produced with InSoAr, we realized:\n The high-level purpose a group of SE artifacts (Java classes here) serves  The lower-level purpose for each SE artifact, by looking at the classes with which it is\ncoupled\nNo other tool can to this extent facilitate comprehension of software system by humans.\n2 ORM: http://en.wikipedia.org/wiki/Object-relational_mapping 3 In software engineering, it is proper to speak about instances of classes here, because an object is an instantiated class. 4 DAO – Data Access Object, http://en.wikipedia.org/wiki/Data_access_object#Advantages 5 Hibernate – is an ORM library for Java, http://en.wikipedia.org/wiki/Hibernate_%28Java%29\nAutomatic Structure Discovery for Large Source Code Page 76 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 77 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n7.2.7 Security attack & protection\nSecurity attack and protection is usually a dual task, like cryptography and cryptanalysis. Thus discussing one we usually mean both. In terms of software protection, many schemes rely on incomprehensibility of the protection mechanism for an attacker. An example is injection of serial key/license checking code (instructions, subroutines, classes) all across the program being protected. Apparently, this leads to coupling of SE artifacts in the program onto the security mechanism, and the participants of security mechanism itself will be clustered together, as we discussed in general for classes that act together (section 7.2.3). In the above scenario (injection all across the program), the security mechanism becomes a group of utility artifacts, and we will observe the same effect as in appendix 10.1.7 or section 7.2.6 for utility or omnipresent artifacts in general. Thus, an attacker is able to identify and circumscribe the security mechanism and study its couplings efficiently using the general techniques we discuss in this paper, subsections of 7.2 and appendixes 10.1, 10.4.3 – 10.4.8.\nThis approach works even when only binaries are available (Soot extracts relations from\nbinary code too) and when the binary code is obfuscated 6 . Security mechanism will get clustered together anyway, and non-obfuscated neighboring artifacts (library classes or at least lower-level OS subroutines) will discover its purpose. In the rare case when everything is obfuscated down to machine code level, i.e. I/O ports and interrupts, clustering of dynamically extracted graph of relations can be used for efficient discovery of the security mechanism.\nPursuing the goal of protection, one can do the same: study how the security mechanism\nlooks after clustering, whether it is easy to identify and circumscribe, and whether the latter information provides an attacker with sufficient means for breaking the security. There are some nuances, however: even though the security mechanism may seem strong to a defender using one parameter set when clustering, another parameter set (e.g. by selecting some other options from discussed in sections 5.1, 5.2, 5.3.2 and 5.6.2) may still exhibit the weaknesses of the security mechanism.\nThe evidence – (a part of) security mechanism identified in “dem0” project – is provided\nin appendix 10.1.2:  Even if we did not know the purpose of client-code classes EmployeeUserDetailsService,\nEmployeeUserDetails and StringAuthority (nested in EmployeeUserDetails), e.g. due to obfuscation, we could determine it from the library neighbors from package org.springframework.security.\n If we know weaknesses of classes coupled with the security mechanism (and thus clustered\ntogether), we can attempt to exploit them to compromise the security mechanism, even though the latter is strong itself. Examples of such potential targets visible in the picture in appendix 10.1.2 are:  WebservableObjectInToAuditableObjectTransformer: how is it about boundary\ncases?\n org.apache.log4j.Logger: can we inject our code into this class, or substitute it\nentirely by changing the CLASSPATH on the server upon attack?\nInSoAr is not an ultimate tool for compromising security. Structural security is vulnerable. Algorithmic (e.g. Petri networks) or mathematical (e.g. factorization) methods will sustain.\n7.2.8 How implemented, what does, where used\nIn the two figures below we see the classes serving time and scheduling clustered together in a subtree. We see that the time & scheduling subsystem has recurrence rules, rule factories and a rule service, Figure 7-13. There is a base class com.kpmg.reccurrence.RecurrenceRule, while weekly and monthly recurrence rules inherit from it. A more specific part of this subsystem is shown in appendix 7.2.8. We see that there is also quarterly recurrence rule and factory. The\n6 Obfuscated source or binary code is the one that has been made difficult to understand, by e.g. replacing Java class names with some meaningless strings. http://en.wikipedia.org/wiki/Obfuscated_code\nAutomatic Structure Discovery for Large Source Code Page 78 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 79 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "8 Further Work",
      "text" : ""
    }, {
      "heading" : "8.1 A Self-Improving Program",
      "text" : "In order to make a program that improves itself we first need to make a program that improves programs and point it to itself. In its turn, prior to making a program that improves programs, we need a program that understands programs, at least in the way humans do. Obviously, ability to understand requires ability to analyze as a prerequisite. State of the art source code analysis tools exist, but they do processing without understanding. In this project we have implemented a program that infers structure of source code to facilitate its further comprehension by humans. A further work direction is implementing a program that attempts to comprehend the structure without humans and then does some forward-engineering of improved source code."
    }, {
      "heading" : "8.2 Cut-based clustering",
      "text" : "As we have discussed in section 5.1, Flake-Tarjan clustering [Fla2004] is only available for undirected graphs. This restriction is posed by minimum cut tree algorithm [GoHu1961], but maximum flow algorithms are available for both directed and undirected graphs. To satisfy this restriction, in our project we were converting directed graph of software engineering artifact relations into undirected using normalization akin to the one described in [Fla2004] and PageRank [Brin1998]. Though the clustering demonstrated good results, it is obvious that important information is lost during the conversion from directed to undirected graph. Within our project we have also tried to eliminate the constraint posed by minimum cut tree, as we do not need a full-fledged cut tree, but only the edges separating the artificial sink from the rest of the tree [Fla2004]. However, this is a hard theoretical task being too risky given the nature of our project (master thesis).\nThus, as a direction for further work we propose eliminating the requirement of\nundirectedness from Flake-Tarjan clustering, thus devising a version that takes a directed graph on input, uses directed max flow algorithm in the backend, and somehow workarounds minimum cut tree exploiting the fact that we need clustering of a directed graph, but not the entire correct minimum cut tree of an undirected graph."
    }, {
      "heading" : "8.3 Connection strengths",
      "text" : "Our normalization, motivated in section 4.5 and provided in section 5.1, lets clustering produce good results (section 7) alleviating the problem of utility artifacts. It is interesting to investigate, whether edge weighting considering more properties (fan-out, graph-wide facts) can result in even better clustering hierarchy."
    }, {
      "heading" : "8.4 Alpha-threshold",
      "text" : "We observed this phenomenon during the adaptation of Flake-Tarjan clustering into the domain of source code, and discussed it in 5.6 proposing an ad-hoc solution that alleviates the issue. However, it is still interesting to analyze and formalize the cases when this phenomenon occurs, and its extent, in terms of the properties of the input graph. In our intuition, the following two theoretical facts should lead to a sound theoretical conclusion: 1 The central theorem of [Fla2004], discussed in section 4.3.1:\n),min(\n),(),(\nQP\nQPc\nSV\nSVSc \n\n \nAutomatic Structure Discovery for Large Source Code Page 80 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n2 The formalization of alpha-threshold phenomenon we provided in section 5.6:\n0),()(   tt kKk\nIn the further work one should investigate, why there exists alpha t such that for any small\nepsilon, there is a community S such that there is no partition qSSS  ...1 in which each\niS can satisfy the bicriterion (fact 1) using some alpha from the epsilon-range of t , i.e.\n];[   tt .\nAutomatic Structure Discovery for Large Source Code Page 81 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    }, {
      "heading" : "9 Conclusion",
      "text" : "In the whole, we conclude that the solution we used in this project is practical. The software processed by our prototype is large, real-world and typical (see section 7.1). The clustering hierarchy produced is meaningful for software engineers (section 7.2, also 10.4), correlates with the known explicit architecture (appendix 10.2.1) and reflects the existing implicit architecture (sections 7.2, 10.2.2 and 10.3.3) providing valuable facts for software engineers which can not be observed using state of the art tools except that by scanning millions of lines of source code manually.\nWe conclude that the method we devised (section 5.1) for alleviating the problem of\nutility artifacts (section 4.5) and directed-to-undirected conversion of the relation graph in the domain of software source code, works well in practice. The empirical proof is provided in section 10.1.7. Not only utility artifacts did not confuse the clustering results, but also they were clustered together reflecting the unity of purpose. This can be viewed as (perhaps, a prerequisite for) the categorization concluded to be desirable in [HaLe2004].\nIn section 5.1.5, a disadvantage of lifting SE artifact granularity prior to clustering was\ninvestigated, namely, information loss. The possible solutions for lifting the granularity to class level after member-level clustering were given. We concluded not to adopt any of the alternative solutions due to practical reasons (computational complexity) and lack of a reasonably grounded solution.\nThe alternative solutions discussed in section 5.1.5 and 5.2 (option 1) have the\nfollowing fact in common: they both rely on the merge operation for two nested decompositions inferred using different features, either different members of a SE class or different kinds or relations between SE artifacts. The strong point of both solutions is reduced information loss, if compared to the solution we implemented in this project. Thus we conclude that by researching a suitable merge operation, one has a possibility to make two improvements at once.\nOne crucial contribution is the scale at which our reverse-architecting approach can\noperate. While the existing approaches are only able to process small or tiny software (e.g. [Pate2009] takes 147 Java classes on input), whereas we process up to 11 199 classes in our experiments and bump into the limits of the tool that provides input relations for our prototype, namely, call graph extraction ([Soot1999], [Bac1996], [Sun1999], [Lho2003]) exhausts 2GB of memory on a 32-bit machine. This is not a problem for a 64-bit machine, and companies who have huge projects also have appropriate hardware (we do not mean supercomputers by the latter). We speculate that a 64-bit machine with 100GB of RAM and 32 processors should be sufficient for analyzing any real-world software project with our tool and, less surely, the prerequisite tools in reasonable time.\nWe stress the importance of operational scale in reverse architecting, as reverse\narchitecting is mostly to address the issue of overwhelming complexity, which arises in large software projects and causes incomprehensibility. Certainly, speed of processing does not bring any value without quality of the results. The hierarchical clustering technique [Fla2004] underlying our reverse-architecting approach is well grounded theoretically, giving the premises for claims about the resulting quality, even though there is no unbiased indicator of software clustering quality available (except the unlikely case of the exact match) because such an estimation is a subjective task for human software engineers. Even though there are some metrics for software clustering quality proposed in the literature ([UpMJ2007], [END2004], [MoJo1999]), not only their adequacy but also their scale is in question: e.g. in [UpMJ2007] their experiments with the devised metric UpMoJo are limited to hierarchical decompositions of no more than 346 artifacts and average height 4. Research and experiments with automatic\nAutomatic Structure Discovery for Large Source Code Page 82 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nmetrics of clustering quality would require large efforts which we can not allow within this project, thus we leave this as a direction for the future work. Apart from strongly grounded clustering algorithm of [Fla2004], we have studied the literature on reverse engineering topics, incorporated the best practices and ideas from there (see sections 3 and 4), and developed our own theory necessary for adaptation of the clustering algorithm into the domain of software source code and containing our ideas for improvement as well (section 8). In addition to the theoretical premises for high quality of the resulting clustering hierarchies, our experiments confirmed that software artifacts in the results are indeed grouped according to their unity of purpose (as motivated in section 2) and, apparently, the visualized hierarchies (section 10) are comprehensible and meaningful for software engineers.\n[Nara2008] makes the following note on how graph topology could influence software\nprocesses:\nUnderstanding call graph structure helps one to construct tools that assist the developers in comprehending software better. For instance, consider a tool that extracts higher-level structures from program call graph by grouping related, lower-level functions. Such a tool, for example, when run on a kernel code base, would automatically decipher different logical subsystems, say, networking, file system, memory management or scheduling. Devising such a tool amounts to finding appropriate similarity metric(s) that partition the graph so that nodes within a partition are “more” similar compared to nodes outside. Understandably, different notions of similarities entail different groupings.\nSuch a tool has been implemented in our project. In the results section above we show that different logical subsystems are indeed identified. The similarity metric we use is the amount of interaction between software engineering artifacts. However, a desirable ability which we do not yet have in our tool is inference of cluster labels. This direction appears in [Kuhn2007], however, in their turn the authors propose to combine linguistic topics with formal application concepts as a future work.\nFinally, we give an account of the weak sides of our project. We are limited in available\nefforts, and the nature of the project (master thesis) constrains us to certain decisions and strategy, such as avoiding risky research directions (in an attempt to invent, e.g. see section 8) and preference of breadth (multiple approaches; extraction, format conversion, clustering, presentation, import/export, visualization, statistics; clustering and software engineering literature review, comparison to rival approaches, implementation, specification, experiments) rather than depth (single best approach; devising new theory, implementation according to best SE practices).  Our claims about the resulting clustering quality, also in comparison to the other\napproaches, are mostly theoretical and empirical.\n Our statistical proof (section 10.2.1) exploits the assumption that more high-level Java\nclasses have shorter package prefixes, in terms of token count (name depth). This is not always true, as there can be higher- and lower- level classes within one package (e.g. see section 7.2.5), as well as there are low-level classes having short prefixes (think of\njava.lang.String).\n Statistical comparison to other reverse architecting approaches is desired, using the same\nexperimental setting (the same software upon analysis). However, this is very effortconsuming without bringing much value into the result of the project in terms of clustering quality and speed.\n More illustrative statistical proof for theory and claims is desired.\nAutomatic Structure Discovery for Large Source Code Page 83 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n Our theory should be checked for originality. Though we tried our best to review the\nexisting approaches, it is not possible to prove that something does not exist, perhaps in different terms or in a different domain.\n We often use problem-solving approach: given an intention, define the problem, solve it\nand implement the solution. Thus we do not always know whether someone else has already solved the same problem, and if so, how our solution relates to his/her in terms of precision, speed, advantages and disadvantages. This saves huge amount of efforts, up to 99% in our view, thus letting us to implement more solutions although having less evidence for their originality and optimality or superiority."
    }, {
      "heading" : "9.1 Major challenges",
      "text" : "Applicability of the clustering algorithm and the success of all this endeavor of integration were not obvious since the beginning. The following subsections list the major challenges were identified prior to the start or during the project.\n9.1.1 Worst-case complexity\nThe theoretical estimations on the worst-case complexity seemed prohibiting. Flake-Tarjan clustering uses minimum cut tree algorithm, which uses maximum flow algorithm up to ||V\ntimes in the worst case. Hierarchical version of the clustering algorithm adds factor of ||V\nfurther. Thus, the total algorithmic complexity is:\n)loglog),min((\n2\n3/22 U\nE\nV EVEVO \nFor the source code of a typical medium-size software project of (for example) 10K classes and 1M relations, the number operations is 18109.11  . This could take a thousand of years on the usual computer on which the results we showed in the thesis were indeed produced in 72 hours within our project. This was achieved due to careful choice of the implementations and the underlying heuristics.\nThe software projects we used in our experiments are typical, as most of their classes\nare Java library classes. Thus we conclude that the clustering method is applicable in general.\n9.1.2 Data extraction\nAnalyzing real-world software projects is a challenge even for tools that extract the data we use on input of our tool. In order to make Soot to extract the call graph we had to study its design and implementation, tune the parameters carefully and even change the source code of Soot in order to allow call graph extraction (and effectively, whole-program analysis) without providing and analyzing all the libraries on which the software upon analysis depends.\n9.1.3 Noise in the input\nCall graph extraction is far from precise. By manually analyzing the extracted call graph and the original source code, we observe that many call relations are absent from the call graph though the source code clearly states the presence, and vice versa, there are many calls in the call graph which never occur during program execution and are not designed to occur by the developers.\nAnother cause of noise in the input for architecture reconstruction algorithm is the\nmistakes made by software engineers due to lack of global understanding of the system. These mistakes violate the implicit architecture, or even sometimes the explicit one. The fact of\nAutomatic Structure Discovery for Large Source Code Page 84 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nviolation of the latter can be proved when the documentation or any other form of explicit archiecture, e.g. packaging structure, is available.\nThus formal relation graph’s “comprehension” of the source code differs a lot from the\ncomprehension of developers who wrote the source code. As can be seen from the resulting clustering, this noise has been successfully tolerated.\n9.1.4 Domain specifics\nThe data configurations specific to the domain are omnipresent utility artifacts and almost perfectly hierarchical structure of software. We have discussed these issues within the thesis, and devised and implemented solutions, which also constitute the major theoretical contributions of our work.\n9.1.5 Evaluation of the results\nIt was challenging to evaluate the reversed architecture due to the common problems in Artificial Intelligence and other relevant fields:\n Human to machine intelligence gap: while a machine can only calculate some measure over the results, humans can find them meaningful, useful, easy to comprehend, etc.\n Lack of objective criteria: even when software engineers discuss some architecture (either the currently documented, or prospective architectural decisions), arguments\noften bump into the philosophy of software engineering. Different experts adopt different approaches, or they just like some decisions more than other.\n Lack of labelled data: real-world (at least, non-trivial) software projects never have documentation of hierarchical architecture till SE class-level, i.e. “target” nested\nsoftware decomposition which we could train on or compare with. Furthermore, architectural documentation is usually not a hierarchy.\n Lack of adequate measures: a counterexample to [END2004] is provided in [UpMJ2007], while the latter is not scalable to large nested software decompositions.\nWhat we did manually is a brute-force evaluation of the reversed architecture by looking at subtrees of the cluster tree and arguing for the useful and adequate (reflecting the actual architecture) facts that a software engineer can see in those subtrees (section 7). There is an advantage in this kind of evidence too: we provide realistic evaluations as usually concluded by humans, rather than abstract measures that might not reflect what humans want to see."
    }, {
      "heading" : "9.2 Gain over the state of the art",
      "text" : "9.2.1 Practical contributions\nThe output of our prototype needs human analysis in the end. However, we stress the gain in comprehensibility: instead of scanning and manually interpreting millions of lines of source code, human software engineers need to look at a few thousands of nodes in the clustering hierarchy to get architectural insights, e.g. those described in 7.2. The latter section contains the typical actions the humans should take for this, though mainly it is a matter of experience and natural intelligence.\nTo summarize, leaf node labels (i.e. the names of SE classes) are heavily used for both\nvalidation of the architecture (e.g. class name must not confuse a software engineer about its purpose) and validation of the quality of clustering. The latter is possible because our approach does not use textual information at any stage of inference, either identifier (type, variable) names, keywords, comments or whatever. InSoAr’s inference is purely based on formal relations between software engineering artifacts. The fact that SE classes having similar labels (the same textual features, e.g. the words composing package names or class names) appear\nAutomatic Structure Discovery for Large Source Code Page 85 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nnearby in the clustering hierarchy (under the same parent, in the same subtree) says both about the quality of the architecture (decoupled, class purposes are well-defined) and the quality of our result.\nFrom the above table we also see that the gain in comprehensibility over non-clustered\ngraph of extracted relations, calculated as the ratio of the item numbers, is nearly-100 times: 822K edges in the input graph vs. 11.4K nodes in the cluster tree. Note that the graph of relations is not just the call graph:\n 2/3 are indeed, method call relations (call graph)\n 1/3 are other relations (field access, inheritance, type usage, parameter and return types) The examples in section 7 illustrate that the determined clusters make it possible for a software engineer to infer the purpose of SE classes from the names of these classes and the neighbouring classes in the cluster tree. This is useful even in case the purpose of a class is obvious from its name, as its position in the cluster tree validates its proper naming, assuming that the quality of the clustering is high, which was also concluded.\nThe central inference our tool does is identification of hierarchical groups of classes\nthat act together (section 7.2.3). In composition with identified purposes (the paragraph above), this can be used for obtaining overviews of systems that lack documentation, or documenting subsystems (including the private case of a single-class subsystem). Along the way it simplifies detection of anomalies in the software system by a software engineer, e.g. overcomplicated coupled groups as in appendix 10.1.5.\nDifferentiation of coupling within a package (section 7.2.5) presents a software\nengineer with a structure while the explicit architecture shows a plain list, which can be much harder to comprehend in case there are many SE classes in the list. On the other hand, when classes belonging to different subsystems appear in the same package due to some more high-\nAutomatic Structure Discovery for Large Source Code Page 86 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nlevel property (section 7.2.6), a software engineer can observe the actual implicit subsystem for each class in the cluster tree. These facts particularly help a software engineer in refactoring and identification of subsystems affected by a change: as we see in section 7.2.6, the subsystem affected by a change in a class from that package is not the package, but the implicit subsystem with which it is coupled. And that is the one inferred by our tool.\nApparently, security mechanism is a private case of a subsystem, section 7.2.7. Thus\nwith our tool software engineers can inspect structural vulnerabilities of a software system. As shown in section 7.2.8 and appendix 10.1.4, sometimes also insight on the implementation can be captured.\nThe ultimate goal is in allowing Divide&Conquer approach to software comprehension,\nhowever it is hard to support such claim, as visualizational problems are encountered in the upper levels (near the root) of the cluster tree, namely: there become too many leaves (labelled nodes) in a subtree, thus some inference of cluster labels is needed. We provide the evidence we currently have in appendix 10.1.5. Still, software engineers can start labelling subsystems from the bottom level up. From Figure 9-1 above we see that for 7.5 million lines of source code, there are only 3.9K inner nodes (i.e. subsystems) in the cluster tree. Labelling these nodes manually for the sake of Divide&Conquer opportunity can be a reasonable task, given that our tool provides a mean to identify the purpose of subsystems near leaf nodes cheaply.\nTo recapitulate, in section 7.2 we discussed the practical facts that can be inferred\nautomatically using the approach we devised. These facts can not be inferred with any other state of the art software engineering tool. This list is not exhaustive, as there are only facts we could think of and discuss illustratively. Altogether, we characterize these facts as architectural insights with practical applications in reverse engineering, software quality and security analysis.\n9.2.2 Scientific contributions\nAn efficient algorithm for high-quality clustering of large software has been invented.\nThe algorithm is based on Flake-Tarjan clustering (sections 4.3) thus inherits its\nintrinsic hierarchical property, optimization of graph cut criteria, and a premise for high-quality as reported in [Fla2004] in general. Our contributions on top of Flake-Tarjan clustering algorithm are provided in section 5. Of them, the following are specific to the domain of source code:\n Edge weight normalization (section 5.1): incorporates the recent conclusions in the literature on Reverse Engineering (section 4.5) about the main domain-specific problem\n– utility artifacts – and proposes directed-to-undirected graph conversion (as FlakeTarjan algorithm requires undirected graph on input) based on utilitihood rationale from the literature (fan-in analysis).\n Perfectization (section 5.6): makes adjustments to the hierarchical results of FlakeTarjan clustering, so that a specific property of data (namely, nearly-perfect hierarchical\nstructure of software as a good practice) does not confuse the clustering result.\nThe following our contributions are improvements over hierarchical Flake-Tarjan clustering algorithm in general:\n Distributed version (section 5.5, using the contributions of sections 5.3 and 5.4): motivated by the need for hierarchical clustering of large software, a distributed\nversion allows running multiple basic (section 4.3.1) Flake-Tarjan clustering probes in parallel, one processor per one value of parameter alpha. The results are then merged into a single hierarchy, as described in section 5.4.\n Prioritized alpha-search (section 5.3): in the absence of time to compute the result of basic Flake-Tarjan clustering for each necessary (i.e. potentially producing different\nAutomatic Structure Discovery for Large Source Code Page 87 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nnumber of clusters) value of parameter alpha, it allows taking the most important probes first, so that the more important decisions about the clustering hierarchy are taken earlier.\nA purely theoretical, within our work, contribution is given in section 5.2: it discusses the potential solutions for considering multiple kinds of formal relations between SE artifacts during clustering, however we did not have time to implement semi-supervised learning proposed there. It currently serves as evidence for our hypothesis 3. In our current clustering algorithm we use the same merge-weight (equal to 1) for each kind of SE relations.\nMinor contributions include:\n Reduction of real- to integer- valued flow graph (section 6.1.1): this allows substitution of integer-capacities max flow algorithms into Flake-Tarjan clustering, instead of more\ncomputationally expensive real-capacities max flow algorithms.\n A review of state of the art clustering and source code analysis methods and tools under section 3, and pre-requisites from clustering, source code analysis and reverse engineering\nin section 4.\nAutomatic Structure Discovery for Large Source Code Page 88 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10 Appendices\n10.1 Evidence\nThis section contains evidence for the claims about properties and quality of the resulting clustering hierarchy. This does not include evidence involving source code demonstration: such evidence is listed in section 10.3 below.\n10.1.1 A package of omnipresent client artifacts\nHere we continue the evidence for the claim discussed in 7.2.6.\nAutomatic Structure Discovery for Large Source Code Page 90 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.1.2 Security mechanism: identify and circumscribe\nSee section 7.2.7 for the discussion.\n10.1.3 Subsystems\nIn Figure 10-2 below we continue illustrating the time & scheduling subsystem, as appeared in the clustering hierarchy and discussed in section 7.2.8. This part is closer to the bottom of the cluster tree, as can be seen from the non-branching nodes on the right.\nAutomatic Structure Discovery for Large Source Code Page 92 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.1.4 Insight on the implementation\nBelow we show obvious examples of insights that the reversed architecture gives to software engineers.\nFor humans it is now easy to make a note that, e.g. client code class HibernateWorkflowTemplateDao works with Criterion, Restrictions and LogicalExpression of Hibernate library; in its turn, it is likely to be used by SimpleExpression\nAutomatic Structure Discovery for Large Source Code Page 93 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 94 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nThe part of cluster tree demonstrated below tells software engineers multiple architectural facts. • Coupling structure – JMSUtil – AbstractDocumentHandler • Purpose from library neighbors • How documents are handled\n– via JMS – Messages, Sessions, Connections\n10.1.5 An overcomplicated subsystem\nIn the picture below we see a number of classes with, concluding from the names, different purposes acting as a single mechanism. The classes are also from different packages, thus we cannot detect this coupling efficiently using state of the art tools. However, when changing the software, it is important to identify the extent of subsystem to be changed. By circumscribing a subsystem subject for change, we narrow down the search space of side effects.\nThis is a fragment of cluster tree for project FreeCol, which is an open-source game.\nThe nature of task the classes are performing, AI player, validates the intuition about complexity of the subsystem and the clustering result.\nAutomatic Structure Discovery for Large Source Code Page 95 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.1.6 Divide & Conquer\nAutomatic Structure Discovery for Large Source Code Page 96 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 97 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 98 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.1.7 Utility Artifacts\nIn the figure below we can see that utility artifacts have been indeed identified and clustered\ntogether, even fairly exhibiting their unity of purpose. StringBuilder and StringBuffer\nare unarguably utility classes in Java. Descendants of cluster id9792 are these 2 classes\ntogether with others serving a similar purpose, except the subtree of id9791. The latter subtree contains the rest of the program, and the artifacts there can usually be viewed as more high-level or specific (in the meaning opposed to general-purpose artifacts).\nAutomatic Structure Discovery for Large Source Code Page 99 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.2 Statistical Measures\n10.2.1 Packages by ubiquity\nIn these statistical measures we investigated how class name length (in tokens 7 ) correlates with the position in the cluster tree. For each node in the cluster tree we calculated how many token-wise suffixes of each name are matched (i.e. have the same token-wise prefix) in the subtree of the node. Suffix Tree data structure and Dynamic Programming (section 4.6) allowed computing it fast (subquadratic complexity). Then we computed multiple averages (per token-wise prefix):\n average match depth: the average depth of cluster tree node at which the suffixes of this prefix matched\n average match height: the same, but height is averaged\n average number of nodes in the subtree: the same, but the number of nodes in the subtree of a cluster tree node is averaged\n7 For example, java.lang.String has 3 tokens: java, lang and String\nAutomatic Structure Discovery for Large Source Code Page 100 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.2.2 Architectural Fitness of SE Class couplings\nIn the Figure 10-5 below we see that 10% of relations between SE artifacts violate the implicit architecture, while 90% fit it very well. In the next diagram, Figure 10-6, we see that 80% of weighted misfits is constituted by 16% of relations (couplings).\nMisfitness of Client-Code Class Couplings\n0\n500\n1000\n1500\n2000\n2500\n3000\n1 191 381 571 761 951 1141 1331 1521 1711 1901 2091 2281 2471\nMisfitness over Weight\n1\n10\n100\n1000\n10000\n0.01 0.1 1 10 100\nAutomatic Structure Discovery for Large Source Code Page 103 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.3 Analyzed Source Code Examples\n10.3.1 Dependency analysis\n10.3.1.1 A Java class that does not use calls or external field accesses\npackage com.kpmg.esb.mule.component;\nimport com.kpmg.kpo.service.ServiceMessageRuleService;\n/**\n* Common abstract super class that provides injection mechanism for services * */\npublic abstract class AbstractPersistableComponent {\n/**\n* serviceMessageRuleService injected by Spring */\nprivate ServiceMessageRuleService serviceMessageRuleService;\npublic ServiceMessageRuleService getServiceMessageRuleService() {\nreturn serviceMessageRuleService;\n}\n/**\n* Accessor method. * * @param serviceMessageRule */\npublic void setServiceMessageRule(ServiceMessageRuleService serviceMessageRule) {\nthis.serviceMessageRuleService = serviceMessageRule;\n}\n}\n10.3.1.2 Some classes whose dependencies are not specific at all\npackage com.kpmg.kpo.action;\nimport org.jbpm.graph.def.ActionHandler; import org.jbpm.graph.exe.ExecutionContext;\npublic class SendFile extends GenericHandler implements ActionHandler {\nprivate static final long serialVersionUID = -5704117378931708811L;\nprivate String messageType;\n@Override public void execute(ExecutionContext executionContext) throws Exception {\nSystem.out.println(messageType);\n}\npublic void setMessageType(String messageType) {\nthis.messageType = messageType;\n}\n}\nPerhaps objects of this class are used as items in array or linked data structures.\nAutomatic Structure Discovery for Large Source Code Page 104 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nFirstly, the type of an item (CompleteTaskCommand in this case) plays a considerable role in decision making regarding the further handling of the item and further program flow. Conditional flow branches basing on instanceof operator result. Secondly, specific features of an instance of this class are stored in String fields, <comment> and <transition> in our example.\nPackage com.kpmg.kpo.web.binding;\nimport java.io.Serializable; import com.kpmg.kpo.domain.TaskInstance;\npublic class CompleteTaskCommand implements Serializable {\n/**\n* SerialVersionUID, required by Serializable. */\nprivate static final long serialVersionUID = 1L;\nprivate TaskInstance task; private String comment; private String transition;\n/**\n* @return the task */\npublic TaskInstance getTask() {\nreturn task;\n} /**\n* @param task the task to set */\npublic void setTask(TaskInstance task) {\nthis.task = task;\n//Reset other fields if we choose a new Task. This.comment = null; this.transition = null;\n} /**\n* @return the comment */\npublic String getComment() {\nreturn comment;\n} /**\n* @param comment the comment to set */\npublic void setComment(String comment) {\nthis.comment = comment;\n} /**\n* @return the transition */\npublic String getTransition() {\nreturn transition;\n} /**\n* @param transition the transition to set */\npublic void setTransition(String transition) {\nthis.transition = transition;\n}\n}\nAutomatic Structure Discovery for Large Source Code Page 105 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.3.1.3 Dependencies lost at Java compile time\nCertain relations are lost while compiling .java files into .class files. This occurs for an\nexample class below: package com.kpmg.kpo; /**\n* Global variables for the worklfow */\npublic final class WorkFlowVariables {\n/**\n* Class cannot be instantiated. */\nprivate WorkFlowVariables() { } /**\n* Constant name used to store the transition map. */\npublic static final String TRANSITIONS = “transitions”; /**\n* Constant name use to identify the domain peer in jBPM. */\npublic static final String PEER = “peer”; /**\n* The comma-separated list of undo-actions. */\npublic static final String UNDO_ACTIONS = “undo_actions”; /**\n* Due-date for a specific Task. */\npublic static final String DUE_DATE = “dueDate”; /**\n* Warning start date for a specific task. */\npublic static final String WARNING_START_DATE = “warningStartDate”; /**\n* Key to store the default transition (if any) under. */\npublic static final String DEFAULT_TRANSITION = “default_transition”;\n}\nWhenever a string constant from WorkFlowVariables class is used in java source code, e.g. WorkFlowVariables.PEER, its value is substituted into the binary code (“peer” in our example) rather than a reference to field PEER of type WorkFlowVariables. See the example that uses WorkFlowVariables.PEER below. As a consequence of this fact, vertex WorkFlowVariables gets no adjacent edges in the relations graph and thus becomes an\norphan, i.e. the single vertex in a disjoint component. Package com.kpmg.kpo.action;\nimport java.io.Serializable;\nimport org.jbpm.context.exe.ContextInstance; import org.jbpm.graph.def.ActionHandler; import org.jbpm.graph.exe.ExecutionContext;\nimport com.kpmg.kpo.WorkFlowVariables; import com.kpmg.kpo.domain.MessageType; import com.kpmg.kpo.domain.WorkflowInstance;\n/**\n* Generic action handler for action that need a {@link MessageType} and the id * of a {@link WorkflowInstance} * <p> * The taskName may be provided. It can be derived from the task that is * executed but a designer may need to choose a different taskName as a * different task in essence is responsible for firing the event. For example a\nAutomatic Structure Discovery for Large Source Code Page 106 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n* reset task does execute this handler but the task that executed the reset * task itself is the taskName we want to provide. * */\npublic abstract class AbstractDocumentHandler extends GenericHandler implements ActionHandler {\nprivate static final long serialVersionUID = 7525089866530362953L;\nprivate String messageType; private String taskName;\n@Override public void execute(ExecutionContext executionContext) throws Exception {\nContextInstance jbpmContext = executionContext.getContextInstance();\nfinal WorkflowInstance peer = (WorkflowInstance) jbpmContext\n.getVariable(WorkFlowVariables.PEER);\nif ( taskName == null || taskName.equals(“”)) {\ntaskName = executionContext.getEventSource().getName();\n}\nSerializable command = getCommandObject(peer.getId(), taskName, messageType,\nexecutionContext);\nJMSUtil.getInstance().sendByJMS(getQueueName(), command);\n}\npublic void setMessageType(String messageType) {\nthis.messageType = messageType;\n}\nabstract String getQueueName();\nabstract Serializable getCommandObject(Long workflowInstanceId,\nString taskName, String messageType, ExecutionContext context);\n}\n10.3.1.4 Problematic cases\npackage com.kpmg.kpo.audittrail.impl;\nimport java.util.UUID; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.ThreadFactory;\nimport com.kpmg.kpo.audittrail.AuditLog; import com.kpmg.kpo.dto.AuditEntryDTO;\n/**\n* AuditLog implementation that delegates to another AuditLog running in a * separate thread. This fire-and-forget approach helps in keeping audit trail * logging fast, yet the code invoking these log statements does not know about * success or failure of storing the log entry in the database (nor about * validation results). * <p/> * As a side-effect, this delegating AuditLog generates a GUID returned by the * log method (and sets that GUID on the AuditEntryData instance passed to the * delegate). * <p/> * The assumption is that using “unmanaged” threads in Jboss is OK (within\nAutomatic Structure Discovery for Large Source Code Page 107 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n* certain bounds, of course). */\npublic final class DelegatingAuditLogImpl implements AuditLog {\nprivate AuditLog delegate; private ExecutorService executorService;\npublic DelegatingAuditLogImpl(AuditLog delegate,\nExecutorService executorService) {\nthis.delegate = delegate; this.executorService = executorService;\n}\npublic DelegatingAuditLogImpl(AuditLog delegate) {\n// The following ExecutorService is guaranteed to execute the log calls\nsequentially\nthis(delegate, Executors.newSingleThreadExecutor(new ThreadFactory() {\npublic Thread newThread(Runnable r) {\nThread t = Executors.defaultThreadFactory().newThread(r); t.setName(“audittrail-“ + t.getName()); return t;\n}\n}));\n}\npublic String log(AuditEntryDTO data) {\nString guid = UUID.randomUUID().toString(); final AuditEntryDTO dataWithGuid = data.withGuid(guid);\n// This is fire-and-forget. If the delegate’s log call in the separate thread\nthrows an exception,\n// it will not affect this thread.\nThis.executorService.execute(new Runnable() {\npublic void run() {\nDelegatingAuditLogImpl.this.delegate.log(dataWithGuid);\n}\n});\nreturn guid;\n}\n}\nAutomatic Structure Discovery for Large Source Code Page 108 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.3.2 Call Graph Extraction\nThe class StatusType is an Enum (Java). Obviously, it does not call any methods from\ncom.sun.imageio package indeed, and the source code confirms that. However, call graph extraction adds noise which we can see in the figure below. The figure demonstrates relations between SE artifacts lifted to class-level. The number “238” in the left-top corner stands for\nthe number of other classes with whom class StatusType has relations.\nAutomatic Structure Discovery for Large Source Code Page 109 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nThe aforementioned class-level noise results from the underlying method-level noise. CHA\ncall graph extraction encountered a call to Object.clone() from\nStatusType.values() and concluded many calls to different classes derived from\nObject possible, however those calls never occur indeed and were by no means intended by software engineers designing and implementing the source code. The data containing noise calls is illustrated in the figure below. The number “145” stands for the number of destinations which method StatusType.values() may call, according to this call graph extraction approach. This number is also the number of outgoing arcs from the corresponding vertex in our input graph we would get in case we use this call graph extraction approach.\nAutomatic Structure Discovery for Large Source Code Page 110 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nPoints-to analysis techniques (RTA, VTA, Spark) help to alleviate this problem substantially, though, at the cost of some mistakenly dropped calls too. See the picture below.\n10.3.3 Class name contradicts the purpose\nWe have to support a strong claim about proper clustering result from section 7.2.2.4 with source code of the class whose name contradicts the purpose. We can see that indeed the source code works mostly with regular expressions and JBPM expression evaluation. Thus, the clustering was correct, while the name of the class is deceiptive.\npackage com.kpmg.kpo.action; import java.util.regex.Matcher; import java.util.regex.Pattern; import org.jbpm.graph.exe.ExecutionContext; import org.jbpm.jpdl.el.impl.JbpmExpressionEvaluator;\npublic abstract class GenericHandler {\n//Prepare to identify any EL expressions, #{…}, regex: “#\\{.*?\\}”. Private static final String EL_PATTERN_STRING = “#\\\\{.*?\\\\}”; //Turn the pattern string into a regex pattern class. Private static final Pattern EL_PATTERN = Pattern.compile(EL_PATTERN_STRING);\n//Evaluate the input as a possible EL expression. Protected Object evaluateEL(String inputStr, ExecutionContext ec) {\nif (inputStr == null) { return null; }\nAutomatic Structure Discovery for Large Source Code Page 111 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nMatcher matcher = EL_PATTERN.matcher(inputStr); if (matcher.matches()) { //input is one big EL expression\nreturn JbpmExpressionEvaluator.evaluate(inputStr, ec);\n} else {\nreturn inputStr;\n}\n}\n/* Treats input as a possible series of EL expressions and concatenates what is found. */ protected String concatenateEL(String inputStr, ExecutionContext ec) {\nif (inputStr == null) { return null; } Matcher matcher = EL_PATTERN.matcher(inputStr); StringBuffer buf = new StringBuffer(); while (matcher.find()) {\n// Get the match result String elExpr = matcher.group(); // Evaluate EL expression Object o = JbpmExpressionEvaluator.evaluate(elExpr, ec); String elValue = “”; if (o != null) {\nelValue = String.valueOf(JbpmExpressionEvaluator.evaluate(elExpr, ec));\n} // Insert the calculated value in place of the EL expression matcher.appendReplacement(buf, elValue);\n} matcher.appendTail(buf); // Deliver result if (buf.length() > 0) {\nreturn buf.toString();\n} else { return null; }\n}\n/* Returns true if the value is a String which contains the pattern delineating an EL\nexpression. */\nprotected boolean hasEL(Object value) {\nif (value instanceof String) {\nMatcher matcher = EL_PATTERN.matcher((String) value); return matcher.find();\n} return false;\n}\n/* Returns true if the value is a String which in its entirety composes one EL expression.\n*/\nprotected boolean isEL(Object value) {\nif (value instanceof String) {\nMatcher matcher = EL_PATTERN.matcher((String) value); return matcher.matches();\n} return false;\n}\n}\n10.4 Visualizations\n10.4.1 State of the art tool STAN\nBelow is visualization of a part (which fits a sheet) of InSoAr at package-level with a state of the art tool STAN:\nAutomatic Structure Discovery for Large Source Code Page 112 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.4.2 Cfinder (Clique Percolation Method)\nAutomatic Structure Discovery for Large Source Code Page 114 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 115 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.4.3 Cluster Tree in Text\n10.4.3.1 Indentation by Height\nAutomatic Structure Discovery for Large Source Code Page 116 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.4.3.2 Bracketed presentation\nAutomatic Structure Discovery for Large Source Code Page 117 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.4.4 Sunray Representation\nAutomatic Structure Discovery for Large Source Code Page 118 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.4.5 Sunburst\n10.4.6 Hyperbolic tree (plane)\nAutomatic Structure Discovery for Large Source Code Page 119 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 120 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.4.7 Circular TreeMap\n10.4.7.1 View on the whole program\n10.4.7.2 Parts of the architecture\nAutomatic Structure Discovery for Large Source Code Page 121 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 122 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 123 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.4.8 H3 Sphere Layout\n10.4.8.1 Near class CrmSOAP\nAutomatic Structure Discovery for Large Source Code Page 124 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n10.4.8.2 Classes that act together\nAutomatic Structure Discovery for Large Source Code Page 125 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 126 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\nAutomatic Structure Discovery for Large Source Code Page 127 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n11 References [GoHu1961] Gomory, Hu (1961). Multi-terminal network flows. In J. of the Society for Industrial and Applied Mathematics, Vol. 9, No. 4 (Dec, 1961), pp. 551-570, 1961\n[MaQu1967] MacQueen (1967). Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, 1, pp. 281-297, 1967\n[Dini1970] Dinic (1970). Algorithm for Solution of a Problem of Maximum Flow in Networks with Power Estimation. In Soviet Math. Doklady, 11: pp.1277-1280, 1970\n[AHU1983] Aho, Hopcroft, Ullman (1983). Data Structures and Algorithms. Addison-Wesley Longman Publishing Co, 427 pages, 1983\n[Mull1990] Muller, Uhl (1990). Composing Subsystem Structures using (k, 2)-partite Graphs. In Proc. Of the International Conference on Software Maintenance, pp.12-19, 1990\n[Gus1990] Gusfield (1990). Very simple methods for all pairs network flow analysis. In SIAM J. Comput. 19 , pp. 143-155, 1990\n[Kau1990] Kaufman, Rousseeuw (1990). Finding groups in data. An introduction to cluster analysis. In Wiley Series in Probability and Mathematical Statistics. Applied Probability and Statistics, New York: Wiley, 1990\n[Hag1992] Hagen, Kahng (1992). New spectral methods for ratio cut partitioning and clustering. In IEEE Trans. On CAD, 11, pp.1074-1085, 1992\n[Gar1993] Garlan, Shaw (1993). An Introduction to Software Architecture http://www.cs.cmu.edu/afs/cs/project/vit/ftp/pdf/intro_softarch.pdf\n[Bac1996] Bacon, Sweeney (1996). Fast static analysis of C++ virtual function calls. In Proceedings of OOPSLA, pp.324-341, 1996\n[Diw1996] Diwan, Moss, McKinley (1996). Simple and effective analysis of statically-typed object-oriented programs. In Proceedings of OOPSLA, pp. 292-305, 1996\n[Che1997] Cherkassky, Goldberg (1997). On implementing push-relabel method for the maximum flow problem. In Algorithmica, Vol.19, No 4, pp.390-410, Springer, 1997\n[Brin1998] Brin, Page (1998). Anatomy of a Large-Scale Hypertextual Web Search Engine. In Proc. 7 th International World Wide Web Conference, 1998\n[Gol1998] Goldberg, Rao (1998). Beyond the flow decomposition barrier. In Journal of the ACM, Vol 45-5, p.783-797, 1998 [Knu1998] Knuth (1998). The Art of Computer Programming, .. ISBN:0201485419, Vol. 1-3, 2 nd edition, Addison-Wesley, 1998\n[Man1998] Mancoridis, Mitchell, Rorres, Chen, Gansner (1998). Using automatic clustering to produce high-level system organizations of source code. In Proc. Of IEEE 6 th International Workshop on Program Comprehension, 1998\n[Bunch1999] Mancoridis, Mitchell, Chen, Gansner (1999). Bunch: A Clustering Tool for the Recovery and Maintenance of Software System Structures. In Proc. Of the International Conference on Software Maintenance, pp. 50-62, 1999\n[HITS1999] Kleinberg (1999). Authoritative sources in a hyperlinked environment. J. of the ACM, Vol. 46, No. 5, pp.604-632, 1999\n[GoTs2001] Goldberg, Tsioutsiouliklis (2001). Cut Tree Algorithms: An Experimental Study. In J. of Algorithms 38, pp. 51-83, 2001\n[Jain1999] Jain, Murty, Flynn (1999). Data Clustering: A Review. In ACM Computing Surveys, 31, No.3,pp. 264-323, 1999\n[Kri1999] Krikhaar (1999). Software Architecture Reconstruction. PhD thesis. University of Amsterdam, 1999\n[Man1999] Manning, Schutze (1999). Foundations of Statistical Natural Language Processing. The MIT Press, 1999\n[MoJo1999] Tzerpos, Holt (1999). MoJo: A distance metric for software clusterings. In Proc. Of the 6 th Working Conference on Reverse Engineering, pp. 187-193, 1999\nAutomatic Structure Discovery for Large Source Code Page 128 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n[Soot1999] Vallée-Rai, Hendren, Sundaresan, Lam, Gagnon, Co (1999). Soot – a Java Bytecode Optimization Framework. In Proceedings of IBM Centre for Advanced Studies Conference, CASCON 1999\n[Sun1999] Sundaresan (1999). Practical techniques for virtual call resolution in Java. Master’s thesis, McGill University, 1999\n[Kwo2000] Kwok (2000). Notes on VTA implementation is Soot. Technical Note, Sable Research Group, McGill University, 2000\n[Rays2000] Rayside, Reuss, Hedges, Kontogiannis (2000). The Effect of Call Graph Construction Algorithms for Object-Oriented Programs on Automatic Clustering. In Proc. Of the 8 th International Workshop on Program Comprehension, IEEE, 2000\n[Riv2000] Riva (2000). Reverse Architecting: an Industrial Experience Report. In Proceedings of the 7th Working Conference on Reverse Engineering (WCRE2000), 2000\n[Shi2000] Shi, Malik (2000). Normalized cuts and image segmentation. In IEEE Transactions on Pattern Analysis and Machine Learning, 22, No.8, pp.888-905, 2000\n[ACDC2000] Tzerpos, Holt (2000). “ACDC: An algorithm for comprehension-driven clustering”, In Proc. Of the 7 th Working Conference on Reverse Engineering, pp. 258-267, 2000\n[Ding2001] Ding, Zha, Gu, Simon (2001). A Min-max Cut Algorithm for Graph Partitioning and Data Clustering. In Proc. of International Conference on Data Mining, IEEE, pp. 107-114, 2001\n[Gine2002] Girvan, Newman (2002). Community structure in social and biological networks. In Proceedings of the National Academy of Sciences, 99, 7821-7826, 2002\n[Bass2003] Bass, Clements, Kazman (2003). Software Architecture In Practice. Second Edition. Boston: Addison-Wesley,pp. 21–24, 2003\n[CLR2003] Cormen, Leiserson, Rivest (2003). Introduction to Algorithms. MIT Press / McGraw Hill, 4 th printing, 2003\n[Let2003] Lethbridge, Singer, Forward (2003). How software engineers use documentation: the state of the practice. In IEEE Software special issue: the State of the Practice of Software Engineering, Vol. 20, No. 6, pp. 35-39, 2003\n[Lho2003] Lhotak, Hendren (2003). Scaling Java points-to analysis using Spark. In Proc. Of the 12 th International Conference on Compiler Construction, pp.153-169, 2003\n[END2004] Shtern, Tzerpos (2004). A framework for the Comparison of Nested Software Decompositions. In Proc. Of the 11 th Working Conference on Reverse Engineering, pp.284-292, 2004\n[Fla2004] Flake, Tarjan, Tsioutsiouliklis (2004). Graph Clustering and Minimum Cut Trees. In Internet Mathematics, Vol. 1, No. 4, pp. 385-408, 2004\n[HaLe2004] Hamou-Lhadj, Lethbridge (2004). Reasoning about the Concept of Utilities. In Proc. Of the ECOOP International Workshop on Practical Problems of Programming in the Large, LNCS, Vol. 3344, pp.10-22, 2004\n[Pal2005] Palla, Derényi, Farkas, Vicsek. (2005) Uncovering the overlapping community structure of complex networks in nature and society. In Nature 435, 814. http://www.cfinder.org/\n[Wen2005] Wen, Tzerpos (2005). Software Clustering based on Omnipresent Object Detection. In Proc of the 13 th International Workshop on Program Comprehension, pp. 269-278, 2005\n[Bab2006] Babenko, Goldberg (2006). Experimental Evaluation of a Parametric Flow Algorithm. Technical Report, Microsoft Research, 2006\n[Bie2006] De Bie, Cristianini (2006). Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems. In Journal of Machine Learning Research 7, pp. 1409-1436, 2006\n[Dong2006] Dong, Zhuang, Chen, Tai (2006). A hierarchical clustering algorithm based on fuzzy graph connectedness. In Fuzzy Sets and Systems, Vol. 157, No. 13, pp. 1760-1774, Elsevier, 2006\n[HaLe2006] Hamou-Lhadj, Lethbridge (2006). Summarizing the Content of Large Traces to Facilitate the Understanding of the Behavior of a Software System. In Proc. Of the 14 th IEEE International Conference on Program Comprehension (ICPC`06), pp. 181-190, 2006\n[Andre2007] Andreopolos, An, Tzerpos, Wang (2007). Clustering large software systems at multiple layers. In Information and Software Technology 49, No 3, pp.244-254, 2007\nAutomatic Structure Discovery for Large Source Code Page 129 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010\n[Kuhn2007] Kuhn, Ducasse, Girba (2007). Semantic Clustering: Identifying Topics in Source Code. In J. on Information and Software Technology, Vol. 49, No 3, pp. 230-243\n[Bink2007] Binkley (2007). Source Code Analysis: A Road Map. In Future of Software Engineering, IEEE, 2007\n[Czi2007] Czibula, Serban (2007). Hierarchical Clustering for Software Systems Restructuring. In INFOCOMP Journal of Computer Science, 6, No. 4, p. 43-51, Brasil, 2007\n[Fre2007] Frey, Dueck (2007). Clustering by Passing Messages Between Data Points. In Science, Vol 315, No. 5814, pp. 972-976, 2007 http://www.psi.toronto.edu/index.php?q=affinity%20propagation\n[Maqb2007] Maqbool, Babri (2007). Hierarchical Clustering for Software Architecture Recovery. In IEEE Transactions on Software Ingineering, Vol. 33, No. 11, pp. 759-780, 2007 http://shannon2.uwaterloo.ca/~soveisgh/software/hierarchical%20clustering.pdf\n[Ra2007] Rattigan, Maier, Jensen (2007). Graph clustering with network structure indices. In Proc. of 24 th International Conference on Machine Learning (ICML ’07), Vol. 227, pp. 783-390, 2007. http://videolectures.net/icml07_rattigan_gcns/\n[Rou2007] Roubtsov, Telea, Holten (2007). SQuAVisiT: A Software Quality Assessment and Visualization Toolset. In Proceedings 7th International Conference on Source Code Analysis and Manipulation, pp.155-156, IEEE, 2007\n[Sch2007] Schaeffer (2007). Graph Clustering. Survey. Computer Science Review, 1(1):27–64, Elsevier , 2007.\n[UpMJ2007] Shtern, Tzerpos (2007). Lossless Comparison of Nested Software Decompositions. In WCRE: Proc. Of the 14 th Working Conference on Reverse Engineering, pp.249-258, IEEE, 2007\n[Czi2008] Czibula, Czibula (2008). A Partitional Clustering Algorithm for Improving The Structure of Object-Oriented Software Systems. Univ. Babes-Bolyai, Informatica, Vol LIII-2, 2008\n[EiNi2008] Einarsson, Nielsen (2008). A Survivor’s Guide to Java Program Analysis with Soot. BRICS, Department of Computer Science, University of Aarhus, Denmark, 2008\n[Nara2008] Narayan, Gopinath, Varadarajan (2008). Structure and Interpretation of Computer Programs. In Proc. Of the 2 nd IFIP/IEEE International Symposium on Theoretical Aspects of Software Engineering, pp.73-80, 2008\n[Pin2008] Pinzger, Graefenhain, Knab, Gall (2008). A Tool for Visual Understanding of Source Code Dependencies. In Proc. Int’l Conf. Program Comprehension, pages 254–259. IEEE Computer Society, 2008\n[Roha2008] Rohatgi, Hamou-Lhadj, Rilling (2008). An Approach for Mapping Features to Code Based on Static and Dynamic Analysis. In Proc. Of the 16 th IEEE International Conference on Program Comprehension (ICPC), pp. 236-241 2008\n[Ser2008] Serban, Czibula (2008). Object-Oriented Software Systems Restructuring through Clustering. In Proc. Of 9 th Intl. Conference on Artificial Intelligence and Soft Computing, ICAISC, pp. 693-704,2008.\n[Kli2009] Klint, Storm, Vinju (2009). Rascal: a Domain Specific Language for Source Code Analysis and Manupulation. In Proc. Of 9 th International Working Conference on Source Code Analysis and Manipulation, IEEE, pp. 168-177, 2009\n[Pate2009] Patel, Hamou-Lhadj, Rilling (2009). Software Clustering Using Dynamic Analysis and Static Dependencies. In Proc. of the 13 th European Conference on Software Maintenance and Reengineering (CSMR’09), Architecture-Centric Maintenance of Large-Scale Software Systems, pp. 27-36, 2009\n[Kosc2009] Koschke (2009). Architecture Reconstruction: Tutorial on Reverse Engineering to the Architectural Level. In ISSSE 2006-2008, LNCS 5413, pp. 140-173, 2009.\n[Giv2009] Givoni, Frey (2009). A Binary Variable Model for Affinity Propagation. In Neural Comput., 21, No. 6, pp. 589-600, 2009\n[Pir2009] Pirzadeh, Alawneh, Hamou-Lhadj (2009). Quality of the Source Code for Design and Architecture Recovery Techniques: Utilities are the Problem. In Ninth International Conference on Quality Software, pp. 465-469, 2009\n[Stan2009] Odysseus Software (2009). Tool STAN – Structure Analysis for Java. White paper, www.stan4j.com\nAutomatic Structure Discovery for Large Source Code Page 130 of 130\nMaster Thesis, AI Sarge Rogatch, University of Amsterdam July 2010"
    } ],
    "references" : [ {
      "title" : "Multi-terminal network flows",
      "author" : [ "Gomory", "Hu" ],
      "venue" : "In J. of the Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "Gomory and Hu,? \\Q1961\\E",
      "shortCiteRegEx" : "Gomory and Hu",
      "year" : 1961
    }, {
      "title" : "Composing Subsystem Structures using (k, 2)-partite Graphs",
      "author" : [ "Muller", "Uhl" ],
      "venue" : "In Proc. Of the International Conference on Software Maintenance,",
      "citeRegEx" : "Muller and Uhl,? \\Q1990\\E",
      "shortCiteRegEx" : "Muller and Uhl",
      "year" : 1990
    }, {
      "title" : "Simple and effective analysis of statically-typed object-oriented programs",
      "author" : [ "Diwan", "Moss", "McKinley" ],
      "venue" : "In Proceedings of OOPSLA,",
      "citeRegEx" : "Diwan et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Diwan et al\\.",
      "year" : 1996
    }, {
      "title" : "On implementing push-relabel method for the maximum flow problem",
      "author" : [ "Cherkassky", "Goldberg" ],
      "venue" : "In Algorithmica,",
      "citeRegEx" : "Cherkassky and Goldberg,? \\Q1997\\E",
      "shortCiteRegEx" : "Cherkassky and Goldberg",
      "year" : 1997
    }, {
      "title" : "Using automatic clustering",
      "author" : [ "Mancoridis", "Mitchell", "Rorres", "Chen", "Gansner" ],
      "venue" : null,
      "citeRegEx" : "Mancoridis et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Mancoridis et al\\.",
      "year" : 1998
    }, {
      "title" : "Bunch: A Clustering Tool for the Recovery and Maintenance of Software System Structures",
      "author" : [ "Mancoridis", "Mitchell", "Chen", "Gansner" ],
      "venue" : "In Proc. Of the International Conference on Software Maintenance,",
      "citeRegEx" : "Mancoridis et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Mancoridis et al\\.",
      "year" : 1999
    }, {
      "title" : "Data Clustering: A Review",
      "author" : [ "Jain", "Murty", "Flynn" ],
      "venue" : "In ACM Computing Surveys,",
      "citeRegEx" : "Jain et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 1999
    }, {
      "title" : "The Effect of Call Graph Construction Algorithms for Object-Oriented Programs on Automatic Clustering",
      "author" : [ "Rayside", "Reuss", "Hedges", "Kontogiannis" ],
      "venue" : "In Proc. Of the",
      "citeRegEx" : "Rayside et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Rayside et al\\.",
      "year" : 2000
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "Shi", "Malik" ],
      "venue" : "In IEEE Transactions on Pattern Analysis and Machine Learning,",
      "citeRegEx" : "Shi and Malik,? \\Q2000\\E",
      "shortCiteRegEx" : "Shi and Malik",
      "year" : 2000
    }, {
      "title" : "A Min-max Cut Algorithm for Graph Partitioning and Data Clustering",
      "author" : [ "Ding", "Zha", "Gu", "Simon" ],
      "venue" : "In Proc. of International Conference on Data Mining,",
      "citeRegEx" : "Ding et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2001
    }, {
      "title" : "Software Architecture In Practice",
      "author" : [ "Bass", "Clements", "Kazman" ],
      "venue" : "Second Edition. Boston: Addison-Wesley,pp",
      "citeRegEx" : "Bass et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bass et al\\.",
      "year" : 2003
    }, {
      "title" : "Reasoning about the Concept of Utilities",
      "author" : [ "Hamou-Lhadj", "Lethbridge" ],
      "venue" : "In Proc. Of the ECOOP International Workshop on Practical Problems of Programming in the Large, LNCS,",
      "citeRegEx" : "Hamou.Lhadj and Lethbridge,? \\Q2004\\E",
      "shortCiteRegEx" : "Hamou.Lhadj and Lethbridge",
      "year" : 2004
    }, {
      "title" : "Uncovering the overlapping community structure of complex networks in nature and society",
      "author" : [ "Palla", "Derényi", "Farkas", "Vicsek" ],
      "venue" : "In Nature",
      "citeRegEx" : "Palla et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Palla et al\\.",
      "year" : 2005
    }, {
      "title" : "Experimental Evaluation of a Parametric Flow Algorithm",
      "author" : [ "Babenko", "Goldberg" ],
      "venue" : "Technical Report, Microsoft Research,",
      "citeRegEx" : "Babenko and Goldberg,? \\Q2006\\E",
      "shortCiteRegEx" : "Babenko and Goldberg",
      "year" : 2006
    }, {
      "title" : "Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems",
      "author" : [ "De Bie", "Cristianini" ],
      "venue" : "In Journal of Machine Learning Research",
      "citeRegEx" : "Bie and Cristianini,? \\Q2006\\E",
      "shortCiteRegEx" : "Bie and Cristianini",
      "year" : 2006
    }, {
      "title" : "Summarizing the Content of Large Traces to Facilitate the Understanding of the Behavior of a Software System",
      "author" : [ "Hamou-Lhadj", "Lethbridge" ],
      "venue" : "In Proc. Of the",
      "citeRegEx" : "Hamou.Lhadj and Lethbridge,? \\Q2006\\E",
      "shortCiteRegEx" : "Hamou.Lhadj and Lethbridge",
      "year" : 2006
    }, {
      "title" : "Clustering large software systems",
      "author" : [ "Andreopolos", "An", "Tzerpos", "Wang" ],
      "venue" : null,
      "citeRegEx" : "Andreopolos et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Andreopolos et al\\.",
      "year" : 2007
    }, {
      "title" : "Hierarchical Clustering for Software Architecture Recovery",
      "author" : [ "Maqbool", "Babri" ],
      "venue" : "In IEEE Transactions on Software Ingineering,",
      "citeRegEx" : "Maqbool and Babri,? \\Q2007\\E",
      "shortCiteRegEx" : "Maqbool and Babri",
      "year" : 2007
    }, {
      "title" : "Graph clustering with network structure indices",
      "author" : [ "Rattigan", "Maier", "Jensen" ],
      "venue" : "In Proc. of",
      "citeRegEx" : "Rattigan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rattigan et al\\.",
      "year" : 2007
    }, {
      "title" : "SQuAVisiT: A Software Quality Assessment and Visualization Toolset",
      "author" : [ "Roubtsov", "Telea", "Holten" ],
      "venue" : "In Proceedings 7th International Conference on Source Code Analysis and Manipulation,",
      "citeRegEx" : "Roubtsov et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Roubtsov et al\\.",
      "year" : 2007
    }, {
      "title" : "A Survivor’s Guide to Java Program Analysis with Soot",
      "author" : [ "Einarsson", "Nielsen" ],
      "venue" : "BRICS, Department of Computer Science,",
      "citeRegEx" : "Einarsson and Nielsen,? \\Q2008\\E",
      "shortCiteRegEx" : "Einarsson and Nielsen",
      "year" : 2008
    }, {
      "title" : "Structure and Interpretation of Computer Programs",
      "author" : [ "Narayan", "Gopinath", "Varadarajan" ],
      "venue" : "In Proc. Of the",
      "citeRegEx" : "Narayan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2008
    }, {
      "title" : "An Approach for Mapping Features to Code Based on Static and Dynamic Analysis",
      "author" : [ "Rohatgi", "Hamou-Lhadj", "Rilling" ],
      "venue" : "In Proc. Of the",
      "citeRegEx" : "Rohatgi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rohatgi et al\\.",
      "year" : 2008
    }, {
      "title" : "Software Clustering Using Dynamic Analysis",
      "author" : [ "Patel", "Hamou-Lhadj", "Rilling" ],
      "venue" : null,
      "citeRegEx" : "Patel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Patel et al\\.",
      "year" : 2009
    }, {
      "title" : "A Binary Variable Model for Affinity Propagation",
      "author" : [ "Givoni", "Frey" ],
      "venue" : "In Neural Comput.,",
      "citeRegEx" : "Givoni and Frey,? \\Q2009\\E",
      "shortCiteRegEx" : "Givoni and Frey",
      "year" : 2009
    }, {
      "title" : "Quality of the Source Code for Design and Architecture Recovery Techniques: Utilities are the Problem",
      "author" : [ "Pirzadeh", "Alawneh", "Hamou-Lhadj" ],
      "venue" : "In Ninth International Conference on Quality Software,",
      "citeRegEx" : "Pirzadeh et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Pirzadeh et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "String getQueueName(); abstract Serializable getCommandObject(Long workflowInstanceId,Serializable getCommandObject(Long workflowInstanceId, String taskName, String messageType, ExecutionContext context);",
    "creator" : "Microsoft® Word 2010"
  }
}