{
  "name" : "1609.01926.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A modular architecture for transparent computation in Recurrent Neural Networks",
    "authors" : [ "Giovanni S. Carmantinia", "Peter beim Graben", "Mathieu Desroches", "Serafim Rodrigues" ],
    "emails" : [ "giovanni.carmantini@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Gödelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the\n∗Corresponding author Email address: giovanni.carmantini@gmail.com (Giovanni S. Carmantini)\nPreprint submitted to Neural Networks September 8, 2016\nar X\niv :1\n60 9.\n01 92\n6v 1\n[ cs\n.N E\ndesign of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.\nKeywords: Automata Theory, Recurrent Artificial Neural Networks, Representation Theory, Nonlinear Dynamical Automata, Neural Symbolic Computation, Versatile Shift"
    }, {
      "heading" : "1. Introduction",
      "text" : "The relation between symbolic computation and neural dynamics is one of the most pertinent problems in computational neuroscience, artificial intelligence, and cognitive science. On the one hand, symbolic computation is generically codified in terms of production systems, formal languages, algorithms and automata (Hopcroft and Ullman, 1979). On the other hand, neural dynamics in artificial neural networks (ANN) is described by nonlinear evolution laws (Hertz et al., 1991). Approaches to connect these different realms of research go back to the seminal paper of McCulloch and Pitts (1943) on networks of idealized two-state neurons that behave as logic gates. Furthermore, fundamental work by Kleene (1956) and Minsky (1967) demonstrated the equivalence between such networks and finite-state automata, and thus digital computers (which are essentially large-scale networks of logic gates). Later examples for connectionist modeling of symbolic computation are the speech perception and production models TRACE by McClelland and Elman (1986) and NETtalk by Sejnowski and Rosenberg (1987). A further important step was achieved by Elman when introducing simple recurrent networks (SRN) as prediction devices for letters in words (Elman, 1990) and syntactic categories in sentences (Elman, 1995). SRN found a number of successful applications in linguistics and cognitive science (Tabor et al., 1997; Christiansen and Chater, 1999; Lawrence et al., 2000; Farkas and Crocker, 2008) where formal grammars have been employed for the generation of training sets. After training, grammatical relations emerged in the connectivity and activation patterns of the network’s hidden layer which could be examined through clustering and principal component analysis (PCA).\nA key problem of this and similar approaches based on eliminative connectionism (Blutner, 2011), a theoretical stance aiming at the elimination of symbolic representations in connectionist models, is that the emerging rep-\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nresentations, while comparable in a metric space through empirical methods such as clustering or PCA, do not allow inferences about the syntactic or structural relationships of the symbolic training data. This is even more the case with contemporary deep-learning (Li, 2014; Bengio et al., 2013), and reservoir computing approaches featuring large networks of randomly and recurrently connected nonlinear units (Dominey, 1995; Jaeger, 2001; Maass et al., 2002; Steil, 2004). For that reason, another branch of research, which we may call transparent connectionism, has been developed in the framework of vector symbolic architectures (VSA) (Mizraji, 1989; Smolensky, 1990; Smolensky and Legendre, 2006a,b; Gayler, 2006; Gayler et al., 2010; beim Graben and Potthast, 2009). Here, one explicitly starts with the symbolic data structures and processes, which are first decomposed into socalled filler-role bindings and then used to create vectorial images through tensor product representations (Smolensky, 1990; beim Graben and Potthast, 2009). These serve as training patterns for subsequent connectionist modeling. In contrast to eliminative connectionism where representations that emerge during training are to a great extent opaque, representations in VSAs are completely transparent as they can be resolved in each step of the encoding procedure. Depending on the structure of the chosen vector space one arrives at different kinds of integrated connectionist/symbolic architectures (ICS) (Smolensky, 1990; Smolensky and Legendre, 2006a,b): Gödelizations for one-dimensional representations in the field of real numbers, proper vectorial representations for finite-dimensional vector spaces, and functional representations for infinite-dimensional vector spaces (beim Graben and Potthast, 2009). Importantly, Siegelmann and Sontag (1991, 1995) used a combination of Gödelization and localist finite-dimensional representation to prove that Recursive ANNs (R-ANN) with rational weights and ramp activation functions can simulate any n-tape (n ≥ 2) stack machine – or, equivalently, any Turing machine (TM) and any partial recursive function – when endowed with a specific localist architecture. Moreover, Siegelmann and Sontag showed that a R-ANN consisting of 886 units can simulate a universal Turing machine (UTM). Recent work by Cabessa (Cabessa and Siegelmann, 2012; Cabessa and Villa, 2012, 2013) extends these results on R-ANNs to the realm of interactive computation (Wegner, 1998), a framework studying systems that can interact with the environment throughout their computation (as opposed to the framework of classical computation, where the interaction is limited to the input-output exchange), proving that R-ANNs are equivalent in power to interactive TMs.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nVery-large-scale and reservoir-like neural network approaches can also rely on VSA as a key ingredient, as in the neural engineering framework (Eliasmith et al., 2012; Stewart et al., 2014), which employs semantic pointers for addressing symbolic representations in activation space, and recent work at the interface between reservoir computing and connectionist/symbolic approaches (Hinaut and Dominey, 2013; Hinaut et al., 2014)\nIn contrast, the present work focuses on parsimonious VSA implementations, building upon the seminal results from Siegelmann and Sontag (1991, 1995), and work from Moore (1990, 1991) who has shown that nonlinear dynamical automata (NDA), piecewise-affine linear dynamical systems on the unit square, can simulate the dynamics of any TM in real-time1 when the machine is represented as a generalized shift (GS) on dotted sequences. In this work we first extend Moore’s results by showing that NDA can support the real-time simulation of a range of models of computation, including but not limited to Turing Machines (of course, TMs can simulate any other model of computation of lesser or equal power, but not necessarily in real-time; see Section 2.1.1 for a discussion). We achieve this by relaxing the definition of GS, which leads to a novel and more expressive shift map, the versatile shift (VS) which enables the parsimonious and real-time emulation of symbolic computation in a range of models. We then show that VS dynamics can be mapped to NDA dynamics on the unit square through Gödelization. Finally, we present a mapping between VS and R-ANNs through NDA (extending preliminary results shown in Carmantini et al., 2015).\nSymbolic models of computation distinguish between data, operations on data and the control of these operations. For example, automata implement a set of symbolic operations and its control through a look-up table (the transition function), and the data as a string encoding the so-called configuration of the automaton. In grammars and term rewriting systems, operations are instead defined as a set of substitution/rewriting rules on some symbolic string, where the application of these rules is controlled by a set of conditions. NDA can perform symbolic computation on a vectorial space while preserving, in their formulation, the division between data, operations on data, and their control. Basing our construction on NDA, we derive an architecture that also preserves this division, thus obtaining networks that are\n1In a real-time simulation, a single computation step in the original model is mapped to a single computation step in the model simulating it.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\ntransparent, modular and parsimonious. Importantly, the operations embedded within the architecture we propose herein are not only distinguishable in activation space, but are also spatially localized, while still relying on a distributed representation of the symbolic data. The granular modularity of the architecture brought about by its relation with NDA differentiates our approach from previous work, and has important consequences for the constructive mapping of interactive automata networks (IANs) to R-ANNs, and for the possibility of correlational studies with electrophysiological data, which we will discuss in subsequent Sections.\nWe illustrate our approach by means of two examples. As a first example, we construct a central pattern generator (CPG) from a finite-state automaton for gait patterns of quadruped animals (Grillner and Zangger, 1975; Collins and Richmond, 1994; Golubitsky et al., 1999). The neuronal sequential activations by CPGs are usually modeled through networks of coupled nonlinear oscillators that undergo symmetry-breaking bifurcations under changes in their driving input (Golubitsky et al., 1999, 1998; Schöner et al., 1990; Collins and Richmond, 1994). We show that our construction, although symbolically inspired, allows the investigation of similar bifurcation scenarios. Additionally, the results of these example are relevant to the design of CPGs for the control of robotic locomotion (Ijspeert, 2008). As a second example, we show how our approach is ideally suited to tackle the mapping of interactive machines to neural networks, because of the separation in the network architecture of data, transformations and their control. This makes it straightforward to construct R-ANNs simulating networks of automata that e.g. share states, are organized in complex hierarchies, or are bound by interactions of conditions in the application of symbolic transformations. We demonstrate this by constructing an interactive automata network (IAN) that implements a diagnosis and repair parser for syntactic language processing (Lewis, 1998) and by subsequently mapping it to a R-ANN performing the same computation. We are then able to derive vectorial observables from the network; specifically, we compute synthetic event-related brain potentials (synth-ERPs, Barrès et al., 2013) and discuss their relation with event-related potentials as measured in experiments involving garden-path sentences (Frisch et al., 2004).\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001."
    }, {
      "heading" : "2. Methods",
      "text" : "The present Section outlines our general method which allows the mapping of a range of models of computation to R-ANNs. In Figure 1 we summarize the complete mapping procedure to accompany its exposition. Our construction is a two-step process. We first define a Versatile shift (a generalization of the shift map introduced in Moore, 1990) that emulates some model of computation, and we subsequently encode its dynamics on the unit square via Gödelization, obtaining a two-dimensional piecewise affine-linear map on the unit square, i.e. a NDA. As a second step, the NDA is mapped onto a first-order R-ANN, which is endowed with an architecture that captures the NDA’s three key components: i) a state, encoding the symbolic data of the model of computation; ii) a set of affine-linear transformations, encoding its operations on data; iii) a switching rule that selects the relevant affine-linear transformation to apply given the state, thus implementing the control of the symbolic operations.\nNext, the theoretical methods employed are discussed in detail. In the presentation of various objects from Formal Language Theory and Automata Theory, we essentially follow the well-established definitions in Hopcroft and Ullman (1979), and in Sipser (2006)."
    }, {
      "heading" : "2.1. Elements of Symbolic Computation",
      "text" : "A symbol is meant to be a distinguished element from a finite set A, which we call an alphabet. Symbols can be concatenated, i.e. for a, b ∈ A, ab ≡ (a, b) ∈ A2. A sequence of symbols w ∈ An is called a word of length n, denoted n = |w|. The set of words of all possible lengths w of finite length |w| ≥ 0 is denoted A∗ (for |w| = 0, w = denotes the “empty word”)."
    }, {
      "heading" : "2.1.1. From Generalized to Versatile Shifts",
      "text" : "The theory of symbolic dynamics (Lind and Marcus, 1995) is a tool to study dynamical systems based on the discretization of time and space in order to interpret trajectories in a vectorial space as discrete sequences of infinite strings of symbols. Importantly, its theoretical apparatus can also be used to do the opposite, mapping sequences of strings to a vectorial space. We start by redefining a representation for strings of symbols, the dotted sequence.\nAccording to Moore (1990, 1991), a dotted sequence s ∈ AZ on an alphabet A is a two-sided infinite sequence of symbols “s = . . . d−2 d−1 . d0 d1 d2 . . .”\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nwhere di ∈ A, for all indices i ∈ Z. Here, the dot “.” is simply used as a mnemonic sign, indicating that the index 0 is to its right. A shift space MS = (A\nZ, σ) is then given by a shift map σ : AZ → AZ (Lind and Marcus, 1995), such that σ(s)i = (s)i+1, i.e. σ shifts all symbols in s one place to the left (or, equivalently, shifts the dot one place to the right). Similarly, it is possible to define an inverse to the shift map, σ−1, shifting all symbols in s one place to the right (or, equivalently, the dot one place to the left).\nNotice how shifting the dot in a dotted sequence to the left or the right resembles the movement of the read-write head of a Turing machine on its tape (see section 2.1.2 for more details on Turing machines). In order to fully attain the power of Turing machines, Moore (1990, 1991) endows the shift space MS with three additional maps\nF : AZ → Z ⊕ : AZ × (A ∪ {φ})Z → AZ\nG : AZ → (A ∪ {φ})Z, (1)\nsuch that their composition Ω(s) = σF (s)(s ⊕ G(s)) can fully simulate any Turing machine. The augmented shift space MGS = (A\nZ,Ω) is called generalized shift (GS) if there is an open interval of indices around the dot, called Domain of Dependence DoD = (kl, kr) (kl ≤ 0 ≤ kr), such that F (s) and G(s) only depend on the content of s within the DoD, F (s) determines a number of left shifts (F (s) > 0), right shifts (F (s) < 0), or no shift at all (F (s) = 0) and G(s) maps the symbols si within the DoD onto other symbols gi, while all symbols outside the DoD are mapped onto an auxiliary symbol φ. Finally, the composition operator overwrites all symbols si within the DoD through their images gi under G while not changing s outside the DoD, i.e. (s⊕ g)i = si if gi = φ, but (s⊕ g)i = gi if gi 6= φ.2\nAccording to Moore’s proof (Moore, 1990, 1991), any Turing machine can be realized as a GS MGS. Since Turing machines can be programmed to simulate the computation carried out by any model of lower or equal computational power, such as finite-state automata or push-down automata,\n2In his 1991 paper, Moore actually defines the DoD of a GS as a finite set of integers which need not be consecutive, and introduces a second finite set of integers, the Domain of Effect (DoE) to indicate the cells to be rewritten (as a function of the cells in the DoD). Nevertheless, it is always possible, given any GS with arbitrary DoD and DoE, to construct an equivalent GS as defined here; we thus decided to propose a simplified definition.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nthis implies that these can also be described in terms of equivalent GSs. In practice, however, simulating other automata via Turing machines will lead to rather complicated machine tables even for the simplest symbolic algorithms, and thus to unnecessarily complicated shift spaces. In fact, different automata implement different atomic operations, so that a Turing machine can require multiple computation steps to simulate a single computation step of another automaton, even when the automaton is computationally less powerful. Therefore, we introduce a novel shift space to which we shall henceforth refer as versatile shift (VS), which will allow us to represent automata configuration dynamics on dotted sequences in a more straightforward and parsimonious fashion, simulating it in real-time. Our construction essentially relies on a redefinition of the concept of dotted sequence. Above, the dot was only used as a mnemonic symbol without any functional implication. Now, we introduce the dot as a meta-symbol which can be concatenated with two words v1, v2 ∈ A∗ through v = v1.v2. Let Â∗ denote the set of these dotted words. Moreover, let Z− = {i | i < 0, i ∈ Z} and Z+ = {i | i ≥ 0, i ∈ Z} the sets of negative and non-negative indices. We can then reintroduce the notion of a dotted sequence as follows. Let s ∈ AZ be a bi-infinite sequence of symbols such that s = wαvwβ with v ∈ Â∗ as a dotted word v = v1.v2 and wαv1 ∈ AZ − and v2wβ ∈ AZ + . Through this definition, the indices of s are inherited from the dotted word v and are thus not explicitly prescribed. Whereas GSs can only rewrite each symbol in their DoD with a new one, VSs are endowed with a more general rewriting operation, substituting dotted words in their DoD with other dotted words of equal or different lengths (as already hinted, yet not implemented, by Moore, 1990). This adds expressiveness to VSs, allowing for the parsimonious real-time simulation of a range of automata (see Figure 2 for a pictorial representation of the difference in substitution operations between GSs and VSs).\nMore formally, we define a VS as a pair MV S = (A Z,Ω), with AZ being\nthe space of dotted sequences, and Ω : AZ → AZ defined by\nΩ(s) = σF (s)(s⊕G(s)) (2)\nwith F : AZ → Z ⊕ : AZ ×AZ → AZ\nG : AZ → AZ, (3)\nwhere the operator “⊕” substitutes the dotted word v1.v2 ∈ Â∗ in s with\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\na new dotted word v̂1.v̂2 ∈ Â∗ specified by G, while F (s) = F |Â∗(v1.v2) determines the number of shift steps as for the GS above. The action of F , G and ⊕ in the VS depends on a finite dotted sub-sequence v1.v2 inside the original dotted sequence s = wαvwβ, as determined by the DoD of the VS, again defined as a set of consecutive integers denoting cell positions on the original dotted sequence. The DoD of a GS can be specified by an open interval (kl, kr) on the integers, with kl ≤ 0 and kr ≥ 0. Additionally, for a DoD = (kl, kr), it is useful to define DoDα = (kl, 0) and DoDβ = (−1, kr) to denote the left and right part of the complete DoD on dotted sequences α.β, with DoD = DoDα ∪DoDβ. The set V of dotted words that can appear in the DoD of a VS is a subset of Â∗, and can be defined as V = {v | v = v1.v2 ∈ Â∗, |v1| = |DoDα|, |v2| = |DoDβ|}.\nTo illustrate how VSs act on dotted sequences, consider for example the dotted sequence “wo.rd”, and define a VS Ωexwith\nDoD = (−2, 1) = {−1, 0},\nG : { o.r 7→ a.n a.n 7→ on.dere,\nF : { o.r 7→ 0 a.n 7→ 1,\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nthen, applying Ωex to “wo.rd” once yields\nΩex(wo.rd) = σ F (wo.rd) ( wo.rd⊕G(wo.rd) ) = σF (wo.rd) ( wo.rd⊕ a.n\n) = σF (wo.rd) ( wa.nd\n) = σ0 ( wa.nd\n) = wa.nd\nand applying it again to the resulting “wa.nd” dotted sequence yields\nΩex(wa.nd) = σ F (wa.nd) ( wa.nd⊕G(wa.nd) ) = σF (wa.nd) ( wa.nd⊕ on.dere\n) = σF (wa.nd) ( won.dered\n) = σ1 ( won.dered\n) = wo.ndered\nwhere the DoD of the input string has been highlighted for clarity (again, contrast this with the pictorial representation given in Figure 2). Note that a VS reduces to a GS in the special case when G always substitutes a dotted sequence with one of the same (finite) length in both the left and the right sub-sequences, as in the previous example where wo.rd⊕G(wo.rd) = wo.rd⊕ a.n = wa.nd.\nA point worth noting is that endowing VS with the rewriting capability extends the GS in the direction of semi-Thue systems (also known as string rewriting systems), a universal model of computation introduced by Axel Thue in 1914 (see chapter 7 of Davis et al., 1994). These rewriting systems play an important role, for example, in algebraic specifications of abstract data structures, equational programming, program transformation and automated theorem proving, where the conditional and successive application of a finite set of rewrite rules transforms a given symbolic structure."
    }, {
      "heading" : "2.1.2. Simulation of Various Automata by Versatile Shifts",
      "text" : "We will now discuss how a range of automata can be simulated in real-time by VSs by choosing appropriate dotted sequence representations of machine configurations, and by constructing F and G to reproduce the machine’s operations and their conditional application.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nFinite-state machines. The finite-state machine (FSM) model of computation has been introduced by McCulloch and Pitts in 1943 and is widely used to describe systems in many application fields, ranging from computer science to engineering and biology, to name a few. At every step of a computation a FSM is in one of a finite set of states, and it can change its state as a result of an incoming input signal. More formally, a FSM can be defined as a 5-tuple MFSM = (Q,T, q0, F, δ), where Q is a finite set of control states, T is the input alphabet, q0 ∈ Q is the starting state, F ⊆ Q is a set of accept states, and δ : Q×T→ Q is a transition function defined as follows:\nδ : (qt, d0t) 7→ qt+1, (4)\nwhere qt, qt+1 ∈ Q are states, and d0t ∈ T is an input symbol. At each computation step, a FSM reads its current state qt, consumes (i.e. reads and discards) its current input symbol dt, and transitions to a new state qt+1 = δ(qt, dt) as prescribed by its transition function. It is possible to encode FSM configurations on dotted sequences as\nqt . d0t d1t . . . dnt (5)\nwhere qt, d0t and d1t . . . dnt are respectively the state, input symbol, and the rest of the unconsumed input of the FSM at time t. A VS simulating a FSM in real-time can be constructed by defining the Domain of Dependence to be DoD = (−2, 1) = {−1, 0}, F to always map to 0, and G so that for all qt ∈ Q, dt ∈ T: G : qt.d0t 7→ qt+1. (6) where qt+1 = δ(qt, d0t).\nPush-down automata and Context-Free Grammars. A push-down automaton (PDA) is a computing machine that has sequential access to its input and can manipulate a stack memory by popping and pushing symbols on top of it. More formally, a PDA can be defined as a 6-tuple MPDA = (Q,N,T, q0, F, δ), where Q is a finite set of control states, N is the stack alphabet, T is the input alphabet, q0 ∈ Q is the starting state, F ⊆ Q is a set of accept states, and δ is a transition function. If F = ∅, the PDA accepts its input when both the input tape and the stack are empty, and it is thus said to accept by empty stack.\nA Deterministic PDA is a PDA in which any configuration of the machine defines at most one transition. As the mapping of non-deterministic\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nautomata computation to Neural Networks is outside the scope of this work, in what follows we will only discuss Deterministic PDAs. Determinism will thus be implied from this point on. The transition function of a PDA is defined as follows:\nδ : Q×T ∪ { } ×N→ Q× (N ∪ { }). (7)\nAt each computation step, a PDA consumes an input symbol, pushes or pops a symbol on the top of its stack, and changes state as prescribed by its transition function applied to the current state qt, currently read input symbol d0t , and the current top-of-stack symbol s0t . In particular, if s0t . . . smt is the current content of the stack, transitions of the form\nδ : (qt, d0t , s0t) 7→ (qt+1, )\napply a pop operation, such that the new stack content becomes equal to s1t . . . smt . Push operations are instead applied by transitions of the form\nδ : (qt, d0t , s0t) 7→ (qt+1, s0t+1),\nso that the updated stack contains the symbols s0t+1s0t . . . smt . Finally, for transitions of the form\nδ : (qt, , s0t) 7→ (qt+1, χ),\nthe PDA does not consume any input symbol (i.e. it does not access its input at all), but either pops its top-of-stack, if χ = , or pushes symbol χ, if χ ∈ N.\nPDA configurations can be encoded on dotted sequences as follows:\nsmt . . . s0t︸ ︷︷ ︸ st qt . d0t . . . dnt︸ ︷︷ ︸ dt\n(8)\nwhere qt, dt and st are respectively the state, the unconsumed input and the content of the stack of the automaton in reversed order at time t.\nA VS simulating a PDA in real-time can be constructed from the PDA’s transition function by defining the Domain of Dependence to be DoD = (−3, 1) = {−2,−1, 0}, F to always map to 0, and G so that, given δ : (qt, κ, s0t) 7→ (qt+1, χ),\nG : { s0t qt . κ 7→ qt+1 . if χ = s0t qt . κ 7→ s0t χ qt+1 . otherwise.\n(9)\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nPDA recognize the class of languages generated by context-free grammars (CFG). PDA and CGFs are thus equivalent in power. A CFG specifies a language, i.e. a set of strings on some alphabet, by defining how its words can be constructed, moving from a distinguished starting symbol and applying substitution rules until a string of unsubstitutable symbols (terminals) is reached.\nA CFG can be formally defined as a 4-tuple GCF = (N,T, R, S), where N is a set of non-terminal symbols, T is a set of terminal symbols, R ⊂ N× (N∪T)∗ a set of substitution rules and S a distinguished start symbol. In particular, each rule in R can be written as X → w, with X ∈ N and w ∈ (N ∪T)∗.\nFor example, let us define a CFG Gex with N = {S}, T = {(, [, ), ]}, and R containing the rules\nS→(S) S→[S] S→ .\nThen Gex generates the language Lex of balanced round and square brackets. By applying the substitution rules we can in fact derive any string in that language. For illustration purposes, an example derivation would be: S → [S] → [(S)] → [()] ∈ Lex. It is always possible to construct, given any CFG, a PDA recognizing its language, and viceversa.\nTop-down recognizers. In one of the examples presented later in the text, we will make use of top-down recognizers (TDRs, see Aho and Ullman, 1972) that can process locally unambiguous non-left-recursive CFGs3. TDRs are a subclass of PDA that can simulate rule expansion to accept languages generated by non-left-recursive CFGs. Given any CFG GCF that is not leftrecursive, it is possible to construct a TDR that can parse strings belonging to the context-free language generated by that grammar. If the input string of a TDR constructed from GCF is in the language generated by that grammar (and thus it can be derived by the grammar), then the TDR will end its\n3A recursive CFG is a CFG including rules A → uAv that expand a non-terminal symbol A into a string containing the same non-terminal. A CFG is called left-recursive if such rules appear in the form A → Aw. A CFG is locally unambiguous if there are no two rules expanding the same nonterminal.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\ncomputation with an empty stack and input, and is said to accept the string by empty stack. We are specifically interested in TDRs that process locally unambiguous CFGs, which have the additional property of needing only one state to perform their computation. To construct such a TDR from a locally unambiguous non-left-recursive CFG GCF = (N,T, R, S) it is sufficient to define its δ function in the following way:\nδ : { (q0, a, a) 7→ (q0, ) for all a ∈ T (q0, , X) 7→ (q0, w) for all (X → w) ∈ R\n(10)\nwhere X ∈ N is a non-terminal, w ∈ (N ∪ T)∗ is a string of terminals and non-terminals, and q0 is the TDR’s only state. Note that in the definition above we endow TDRs with the additional capability of pushing strings w on the stack rather than single symbols.\nAs our TDRs only have one state q0, we can describe their machine configuration without referring to the current state. It is thus possible to encode TDR configurations on dotted sequences as follows:\nsmt . . . s0t︸ ︷︷ ︸ st . d0t . . . dnt︸ ︷︷ ︸ dt\n(11)\nwhere dt and st are respectively the unconsumed input and the content of the stack of the automaton in reverse order at time t. Similarly, simpler VSs than those needed to simulate PDAs can be constructed from a TDR’s transition function, by defining the Domain of Dependence to be DoD = (−2, 1) = {−1, 0}, F to always map to 0 and G to mirror Equation 10 so that\nG : { a .a 7→ . X.a 7→ w. (12)\nfor all a ∈ T, (X → w) ∈ R.\nTuring machines. A Turing machine (TM) is an automaton with read-write random access to a two-sided infinite tape (Turing, 1937; Sipser, 2006). TMs are central to the Theory of Computation, and they are thought to be powerful enough to model any physically realizable computation (with assumptions of unbounded resources). A TM has an in-built tape (doubly-infinite one dimensional memory with one symbol capacity at each memory location) and a finite-state controller endowed with a read-write head that follows the\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\ninstructions encoded by the transition function. At each step of the computation, given the current state and the current symbol read by the read-write head, the controller determines via a δ transition function the writing of a symbol on the current memory location, a shift of the read-write head to the memory location to the left (L) or to the right (R) of the current one, and the transition to a new state for the next computation step. Formally, a TM (Turing, 1937) can be defined as a 7-tuple MTM = (Q,N,T, q0,t, F, δ), where Q is a finite set of control states, N is a finite set of tape symbols also containing the blank symbol t, T ⊂ N \\ {t} is the input alphabet, q0 ∈ Q is the starting state, F ⊂ Q is a set of ‘halting’ states reached at the end of the computation and δ : Q × T → Q × T × {L,R} is a partial transition function, the so-called machine table, that determines the dynamics of the machine. In particular, δ is defined as follows:\nδ : (qt, d0t) 7→ (qt+1, d0t+1 ,m) (13)\nwhere qt, qt+1 ∈ Q are the state of the machine before and after the transition, d0t , d0t+1 ∈ N are respectively the read and rewritten symbol, and m ∈ {L,R} denotes the shift of the read-write head to the left or to the right.\nAt a given computation step, the content of the tape together with the position of the read-write head and the current controller state define a machine configuration. It is possible to encode TM configurations on dotted sequences as follows:\ns = . . . d−2t d−1t︸ ︷︷ ︸ lt qt . d0t d1t d2t . . .︸ ︷︷ ︸ rt , (14)\nwhere lt describes the part of the tape to the left of the read-write head, rt describes the part to its right, qt describes the current state of the machine controller, and the central dot denotes the current position of the read-write head, i.e. d0t , the symbol to its right.\nA VS simulating a TM in real-time can be constructed from the TM’s transition function by defining the Domain of Dependence to be DoD = (−3, 1) = {−2,−1, 0}, andG and F so that, given δ : (qt, d0t) 7→ (qt+1, d̂0t ,m),\nG : { d−1t qt . d0t 7→ d−1t d̂0t . qt+1 if m = R d−1t qt . d0t 7→ qt+1 d−1t . d̂0t if m = L\nF : { d−1t qt . d0t 7→ −1 if m = R d−1t qt . d0t 7→ +1 if m = L\n(15)\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nfor all d−1t ∈ N. The following example will clarify how the VS defined as above (Equation 15) can simulate a TM. Consider, for instance, the dotted sequence “wq0.ord”, and define a TM such that δ : (q0, o) 7→ (q1, a,R) and δ : (q1, r) 7→ (q1, n,L). Then a computation step of the TM starting from the “wq0.ord” configuration would yield a new configuration “waq1.rd”; by running the TM again, this time starting from “waq1.rd”, a computation step would yield “wq1.and”, as prescribed by the transition function we defined. Constructing a VS Ωex as specified by Equation 15 and applying it to “wq0.ord”:\nΩex(wq0.ord) = σ F (wq0.ord) ( wq0.ord⊕G(wq0.ord) ) = σ−1 ( wq0.ord⊕ wa.q1\n) = σ−1 ( wa.q1rd\n) = waq1.rd\n(16)\nand by applying it again to the resulting “waq1.rd” dotted sequence we obtain\nΩex(waq1.rd) = σ F (waq1.rd) ( waq1.rd⊕G ( waq1.rd ) = σ+1 ( waq1.rd⊕ q0a.n\n) = σ+1 ( wq0a.nd\n) = wq0.and\n(17)\nwhere the DoD of the input string to the VS has been highlighted for clarity. Note that the dotted representation of the machine configuration requires index −1 to always contain the machine state. For this reason, it is not enough to only rewrite the symbols in {−1, 0} (i.e. the machine state and the current symbol under the read-write head) to simulate a TM, as intuition would instead suggest. In fact, a VS first applies a rewriting of its DoD, and then shifts the resulting dotted sequence to the left (when F (s) = −1) or the right (when F (s) = +1). In particular, the shift is needed to simulate the movement of the read-write head on the machine tape. In order to make sure that at the end of the substitution and shift the machine state is correctly placed at its reserved index −1, the substitution must leave it displaced one place to the right if a left shift is to be applied (as in Equation 16), or one to the left in case of a right shift (as in Equation 17). This last case requires the additional dependence of the VS on index −2. Furthermore, note that our construction is equivalent to that from Moore (1990, 1991): the VS defined\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nin Equation 15 is nothing more than the GS introduced by Moore to prove the equivalence between GSs and TMs."
    }, {
      "heading" : "2.2. Introducing Nonlinear Dynamical Automata",
      "text" : "We will now discuss how VSs, and thus the models of symbolic computation they can simulate, can be mapped to piecewise affine-linear systems on a vectorial space, obtaining nonlinear dynamical automata."
    }, {
      "heading" : "2.2.1. Gödel Encodings and the Symbol Plane",
      "text" : "A Gödel encoding (or Gödelization, see Gödel, 1931) allows one to uniquely assign a real number to a sequence such that the space of one-sided infinite sequences can be mapped to the real interval [0, 1].4 For completeness, Gödelization is subsequently discussed alongside its graphical representation, provided in Figure 3.\nLet AN be the space of one-sided infinite sequences over an alphabet A containing |A| = g symbols, and s = d1d2 . . . a sequence in this space, with dk being the k-th symbol in s. Additionally, let γ : A → N be a one-to-one function associating each symbol in the alphabet A with a natural number. Then a Gödelization is a mapping from AN to [0, 1] ⊂ R defined as follows:\nψ(s) := ∞∑ k=1 γ(dk)g −k. (18)\nConveniently, Gödelization can also be employed on a dotted sequence α.β ∈ AZ — herein representing a machine configuration — by splitting it into its two one-sided constituents α′ (the reversed α) and β. Defining two Gödel encodings ψx and ψy for α\n′ and β respectively, induces a two-dimensional representation for α.β, i.e. ( ψx(α ′), ψy(β) ) , known as symbol plane or symbologram, which is contained in the unit square [0, 1]2 ⊂ R2. In encoding dotted sequences α.β representing configurations of the machines we consider in this paper, α often only ever contains states as first symbols, and\n4A Gödel encoding maps sequences on some alphabet A to real numbers through the use of a base-b expansion, with b = |A|. It can be proven that any base-b expansion represents a real number, and that any real number has a unique base-b representation under a weak condition. The uniqueness of the Gödel encoding (and decoding) of any sequence follows from the same proof.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\ntape symbols in the rest of the sequence. In this case we can define a more refined Gödelization that covers all of the representational space [0, 1] ∈ R:\nψx(s) := γq(d1)n −1 q + ∞∑ k=1 γs(dk+1)n −k s n −1 q , (19)\nwhere γq and γs respectively enumerate the set of states Q and the tape alphabet A, and where nq = |Q|, ns = |A|."
    }, {
      "heading" : "2.2.2. Versatile Shifts as Affine-Linear Transformations",
      "text" : "Push and pop operators can be defined on one-sided infinite sequences AN on some alphabet A. The push operator is defined so that s b adds the contents of a word b ∈ A∗ to the beginning of s ∈ AN, whereas the pop operator is defined so that ps removes the first p symbols in s. We will now show that Gödelizing a sequence resulting from the application of pop and push operations is equivalent to applying an affine-linear transformation on the original Gödelized sequence. We will then show that VSs on a dotted sequence α.β can be mapped to push and pop operations on its one-sided constituents α′ and β. Let s = d1d2d3 . . . be a one-sided infinite sequence on an alphabet A. Applying a pop operation p to s yields ps = dp+1dp+2dp+3 . . ., while pushing a word b = b1 . . . br to the beginning of s yields s b = b1 . . . brd1d2 . . . . In this case\nψ (s) = γ(d1)g −1 + γ(d2)g −2 + γ(d3)g −3 + . . . ,\nso that\nψ( ps) = γ(dp+1)g−1 + γ(dp+2)g−2 + γ(dp+3)g−3 + . . .\n= ψ(s) · gp − p∑ i=1 γ(di)g p−i,\nand\nψ(s b) = γ(b1)g−1 + . . .+ γ(br)g−r+ γ(d1)g −(r+1) + γ(d2)g −(r+2) + . . .\n= ψ(s) · g−r + r∑ i=1 γ(bi)g −i,\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nproving that the resulting Gödelized sequences can be obtained by applying affine-linear transformations to the original Gödelized sequences. For both pop and push operations, the parameters of the affine-linear transformations only depend on the number and identities on the symbols that are respectively removed from or added to the beginning of the original sequence. This is of particular importance in the framework of interactive computation (Wegner, 1998), where the newly added symbol stems from the network’s interaction with its environment. Accordingly, the symbol b becomes represented by a linear operator acting on the system’s state space, analogous to quantum operators acting on Hilbert spaces (beim Graben et al., 2008).\nAs previously discussed, a VS defines two operations on dotted sequences, a substitution operation s⊕G(s) which replaces the dotted sub-sequence in the DoD of the shift with a new dotted sequence G(s), and a shift operation σF (s) shifting the symbols in s to the left or to the right by F (s) positions. Let s ⊕ G(s) = wαu.vwβ ⊕ û.v̂ be a substitution replacing the dotted sub-sequence u.v in s with the dotted word û.v̂, then s ⊕ G(s) can be straightforwardly mapped to pop and push operations on u′wα\n′ and vwβ, the one-sided constituents of the original dotted sequence s, as follows:\nwαu.vwβ ⊕ û.v̂ = ( ( |u′|u′wα′) û′ )′ . ( ( |v|vwβ) v̂ ) = (wα\n′ û′)′ . (wβ v̂) = wαû.v̂wβ\nshowing that substitutions on dotted sequences can be mapped to pop and push operations on its one-sided constituents. A left shift σ−1 and a right shift σ1 on a dotted sequence α.β = . . . d−2 d−1 . d0 d1 . . . can be mapped to push and pop operations on its one-sided constituents as follows:\nσ−1(. . . d−2 d−1 . d0 d1 . . .) = (α ′ d0)′.( 1β)\n= . . . d−1 d0 . d1 d2 . . . ,\nand\nσ1(. . . d−2 d−1 . d0 d1 . . .) = ( 1α′)′.(β d−1) = . . . d−3 d−2 . d−1 d0 . . . ,\nshowing that shifts on dotted sequences can be mapped to pop and push operations on its one-sided constituents. Any arbitrary shift σk with k ∈ Z can be\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nobtained by composition of left and right shifts; as the composition of affinelinear transformations is an affine-linear transformation, the Gödelization of a sequence resulting from the composition of shift operations is equivalent to an affine-linear transformation on the original Gödelized sequence. We have thus shown that VSs on dotted sequences can be mapped to pop and push operations on one-sided infinite sequences, and that the Gödelization of these operations can be mapped to affine-linear transformations on the original sequences. On the symbologram, each substitution and shift operation on a Gödelized dotted sequence α.β by a VS involves two affine-linear transformations, one acting on the Gödelized α′ (the reversed α) and one on the Gödelized β. The parameters of the affine-linear transformations only depend on the symbols of the dotted sequence in the DoD of the VS. All dotted sequences which share the same DoD symbols are thus associated to the same pair of affine-linear transformations. For this reason, the symbologram representation of VSs leads to piecewise affine-linear maps on rectangular partitions of the unit square, referred to as a nonlinear dynamical automata (Tabor, 2000; Tabor et al., 2013; beim Graben et al., 2004, 2008)."
    }, {
      "heading" : "2.2.3. Nonlinear Dynamical Automata",
      "text" : "A nonlinear dynamical automaton (NDA) is a triple MNDA = (X,P,Φ), where P is a rectangular partition of the unit square X = [0, 1]2 ⊂ R2, that is\nP = {Di,j ⊂ X| 1 ≤ i ≤ m, 1 ≤ j ≤ n, m, n ∈ N}, (20)\nso that each cell is defined as Di,j = Ii × Jj, with Ii, Jj ⊂ [0, 1] being real intervals for each bi-index (i, j), with Di,j ∩ Dk,l = ∅ if (i, j) 6= (k, l), and⋃ i,j D\ni,j = X. The couple (X,Φ) is a time-discrete dynamical system with phase space X and the flow Φ : X → X is a piecewise affine-linear map such that Φ|Di,j := Φ i,j, with Φi,j having the following form:\nΦi,j(x) = ( ai,jx ai,jy ) + ( λi,jx 0 0 λi,jy )( x y ) . (21)\nNote that the NDA, as any piecewise affine-linear system, also requires a switching rule Θ(x, y) ∈ {(i, j)| 1 ≤ i ≤ m, 1 ≤ j ≤ n}, which selects the appropriate branch, and thus dynamics (i.e. Φ(x, y) = Φi,j(x, y) ⇐⇒ Θ(x, y) = (i, j)). A mapping between a VS and a NDA can be defined following the methods outlined in Section 2.2.1 and Section 2.2.2, therefore enabling the derivation of the parameters of the NDA. That is, first each cell\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nDi,j = Ii × Jj can be seen as containing all the Gödelized dotted sequences α.β which agree (i.e. have the same symbols) in the Domain of Dependence. In particular, the Ii interval contains all the DoD-agreeing Gödelized α\n′ (the reversed α) sub-sequences, whereas the Jj interval contains all the DoDagreeing Gödelized β sub-sequences. This leads to a partition of the unit square with a number i of I intervals equal to the number of possible onesided sub-sequences that can appear in the left DoD of the VS, and a number j of J intervals equal to the number of possible one-sided sub-sequences that can appear in the right DoD. For example, for a VS simulating a FSM, the left Domain of Dependence DoDα = {−1} of the dotted sequences representing machine configurations only ever contains states, and the right Domain of Dependence DoDβ = {0} only ever contains input symbols. In this case the number of Ii intervals becomes equal to the number of states nq = |Q| in the FSM, and the number of Jj intervals equal to the number of input symbols ns = |T|, where Q and T are respectively the set of states and that of input symbols in the FSM. For a VS simulating a TM, instead, the left Domain of Dependence DoDα = {−2,−1} only ever contains states at index −1, and tape symbols at index −2, and the right Domain of Dependence DoDβ = {0} always contains tape symbols. This leads to a partition of the unit square with a number of Ii intervals equal to m = nqns, and one of Jj intervals equal to n = ns, leading to a total of nqn 2 s cells, where ns is the number of symbols in the tape alphabet N and nq is the number of states in Q. Following Section 2.2.2, substitutions and shifts on a sequence can be mapped to affine-linear transformations on its Gödelization. For this reason, each cell in the partition P of the unit square is associated with a different affine-linear transformation with parameters (ai,jx , a i,j y ) and (λ i,j x , λ i,j y ), which can be derived using the methods outlined in Section 2.2.2. Therefore a model of computation can be represented as a NDA by means of its Gödelized VS representation."
    }, {
      "heading" : "2.3. Solution Map between NDA and R-ANNs",
      "text" : "The design of the map between the NDA and a first order R-ANN follows a conceptually natural and simple solution, which attempts to mimic the affinelinear dynamics (given by Equation 21) of the NDA on the partitioned unit square (see Carmantini et al., 2015 for preliminary work in this direction).\nLet ρ(·) denote the proposed map. The objective is to map the orbits of the NDA (i.e. Φi,j(x, y)) to orbits of the R-ANN, denoted as ζ i,j(x, y). The role of ρ is to encode both the affine-linear dynamics within each partition cell\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\n(Di,j) and to emulate the transitions from cell to cell by suitably activating certain neural units within the R-ANN. To achieve this, we propose a network architecture with three layers, namely a machine configuration layer (MCL), a branch selection layer (BSL) and a linear transformation layer (LTL), as depicted in Figure 4. Therefore, we generically define the proposed map as follows:\nζ = ρ(I,A,Φ,Θ), (22)\nwhere I2×2 is the identity matrix that maps (identically) the initial conditions of the NDA to the R-ANN and A is the synaptic weight matrix that defines the network architecture, which will be discussed in subsequent Sections. In addition, ρ generates different neural dynamics for each type of the neural units, i.e. ζ = (ζ1, ζ2, ζ3), corresponding to MCL, BSL and LTL, respectively. The details of the R-ANN architecture and its dynamics will now be presented."
    }, {
      "heading" : "2.3.1. Network Architecture and Neural Dynamics",
      "text" : "The simulation of a NDA orbit within the R-ANNs is distributed among MCL, BSL and LTL. Since Φi,j(x) is a two-dimensional de-coupled discrete map it suggests only two neural units in a read-out layer, which is a role taken by the MCL. We refer to the two MCL units as cx and cy. At each\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\ncomputation step the MCL stores the encoding of the current machine configuration, which is then passed on to the BSL and LTL units. Subsequently, two sets of BSL units (bx and by) functionally act as a switching system that determines to which cell Di,j the current machine configuration belongs, triggering the appropriate units within two sets of LTL units (tx and ty), effectively emulating the application of an affine-linear transformation Φi,j on an encoded machine configuration. This action corresponds to the application of a symbolic operation by the original machine, leading to a configuration update. The result of the transformation is then fed back to the MCL, representing the configuration (i.e. the machine’s symbolic data) for the next computation step. These successive transformations effectively emulate the action of a NDA, where for every computational step an affinelinear transformation is applied to the values encoding the representation of the machine configuration. The neural units in the various layers make use of either the Heaviside (H) or the Ramp (R) activation functions defined as follows (see also Figure 5):\nH(x) =\n{ 0 if x < 0\n1 if x ≥ 0 R(x) =\n{ 0 if x < 0\nx if x ≥ 0. (23)"
    }, {
      "heading" : "2.3.2. Machine Configuration Layer",
      "text" : "The MCL encodes the state of the simulated NDA, and thus the data of the simulated automaton, while acting as a read-out neural layer. At\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nthe same time it mediates at each computation step the transmission of the current Gödel encoding of the emulated machine’s configuration to the BSL and LTL units. Since the Gödel encoding of a dotted sequence representing a machine configuration consists of two values (see Section 2.2.1), this implies that the MCL solely requires two neural units (cx and cy) to code for the current configuration. As a consequence, the initialization of the R-ANNs is performed in this layer, where the initial conditions (ψx(α\n′), ψx(β)) are identically transformed (via I) by the map ρ(·) as follows:\n(cx, cy) = (ψx(α ′), ψx(β)) ≡ ζ1 = ρ(I, ·, ·, ·)|(ψx(α′),ψx(β)) (24)\nFollowing every computation step, these neural units receive inputs from the LTL units and are subsequently activated via the ramp activation function (Equation 23); in other words ζ1 ≡ (cx, cy) = (R( ∑ i t i x), R( ∑ j t j y)). Finally, these synaptically project onto the BSL and LTL neural units (refer to Figure 6 for details of the connectivity)."
    }, {
      "heading" : "2.3.3. Branch Selection Layer",
      "text" : "The BSL acts as a control unit that enables the sequential mapping of the orbits of the NDA, Φi,j(x, y), to orbits of the R-ANNs, ζ i,j(x, y). Specifically, the BSL functionally embodies the switching rule Θ(x, y) and coordinates the dynamic switching between LTL units. Sequentially, under the action of BSL units, only a single pair of LTL units (ti,jx , t i,j y ) dedicated to emulate Φ i,j become active, which then operate on an encoded Machine configuration. In particular, the BSL units make sure that (ti,jx , t i,j y ) become active only if (cx, cy) ∈ Di,j = Ii × Jj, with Ii = [ξi, ξi+1) being the i-th interval on the x-axis and Jj = [ηj, ηj+1) being the j-th interval on the y-axis. The switching rule is mapped by ρ(·) as follows:\nζ2(x, y) = ρ(·, ·, ·,Θ(x, y) = {i, j}) (25)\nThe implementation of ζ2(x, y) is mediated by two sets of neural units, i) the bx set with m units (the number of I intervals on the x-axis) and ii) the by set with n units (the number of J intervals on the y axis), which are activated via a Heaviside activation function (Equation 23) after receiving excitatory inputs with synaptic weight 1 from the MCL layer (i.e. cx and cy units) in the following way:\nbix = H(cx − ξi) with ξi = min(Ii), bjy = H(cy − ηj) with ηj = min(Jj).\n(26)\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nThat is, the activation of the BSL units depends on a threshold, implemented here as a synaptic projection from an always-active bias unit, that is defined as the minimum of the intervals Ii and Jj respectively for the b i x and b i y units. This has the effect of centering the threshold towards the left boundary of each interval (i.e. a bias of −ξi for bix unit and −ηj for bjy). Therefore, if the read-out (i.e. encoded machine configuration) of the cx and cy units in the MCL corresponded to a point on the unit square belonging to cell Di,j, then the bix unit would be triggered active as well as all units b k x with k < i. The same would occur for neurons bjy and all neural units b k y with k < j.5 Upon excitation, these BSL units then synaptically project to the relevant LTL units, (ti,jx , t i,j y ) that are naturally inactive due to a strong inhibitory bias with magnitude h (the role and value of h will be clarified in the subsequent Section). Specifically, each neural unit bix establishes synaptic excitatory connections (with weight h\n2 ) to all LTL units within the cells\nDk,i (i.e. (tk,ix , t k,i y )) and also project with synaptic inhibitory connections (with weight −h 2 ) to all LTL units within the cells Dk,i−1 (i.e. (tk,i−1x , t k,i−1 y )), where k = 1, . . .m; for a graphical depiction see Figure 6. Similarly, each neural unit bjy projects with synaptic excitatory connections (with weight h 2 ) to all LTL units within the cells Dj,k (i.e. (tj,kx , t j,k y )) and also projects with synaptic inhibitory connections (with weight −h 2\n) to all LTL units within the cells Dj−1,k (i.e. (tj−1,kx , t j−1,k y )), where k = 1, . . . n; see Figure 6. The combined effect of the bix units and b j y is therefore to counterbalance through their synaptic weights the natural inhibition (of bias h) of the LTL units in cell Di,j. In other words each couple of LTL units (ti,jx , t i,j y ) receives an input Bix +B j y, defined as follows:\nBix = b i x\nh 2 + bi+1x −h 2\nBjy = b j y\nh 2 + bj+1y −h 2 ,\n(27)\nwhere the input sum\nBix +B j y =  h if (cx, cy) ∈ Di,j h 2\nif cx ∈ Ii, cy 6∈ Jj or cx 6∈ Ii, cy ∈ Jj 0 if (cx, cy) 6∈ Di,j\n(28)\n5Note that the action of the BSL could be equivalently implemented by interval indicator functions represented as linear combinations of Heaviside functions.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nonly triggers the relevant LTL unit if it reaches the value h. That is, if the pair (ti,ix , t i,j y ), is selected by the BSL units (and thus (cx, cy) ∈ Di,j), then Bix + B j y = h. Otherwise B i x + B j y is either equal to h 2\nor 0. An example of this mechanism is shown in Figure 6, where the LTL units in cell D1,2 are activated via mediation of bx = {b1x, b2x, b3x} and by = {b1y, b2y}. Here, both b3x and b2y are not excited since respectively cx and cy are not activated enough to drive them towards their threshold. However, b2x excites (with weights h 2 ) the LTL units in cell D2,2 and D1,2 and inhibits (with weights −h 2\n) the LTL units in cell D2,1 and D1,1. Equally, b2y excites (with weights h 2 ) the LTL units in cell D2,1, D2,2 and D2,3 and inhibits (with weights −h 2\n) the LTL units in cells D1,1, D1,2 and D1,3. The b1x and b 1 y units excite respectively cells {D2,1, D1,1} and {D1,1, D1,2, D1,3}, but these do not inhibit any cells (due to boundary conditions)."
    }, {
      "heading" : "2.3.4. Linear Transformation Layer",
      "text" : "The LTL embodies the set of affine-linear transformations of the NDA from which the network is constructed, and thus the set of symbolic operations defined by the transition table of the simulated automaton. This endows the LTL with the functional ability of generating an updated encoded machine configuration from the current one. That is, the affine-linear transformation of a NDA, Φi,j(x, y) = (λi,jx x+ a i,j x , λ i,j y y+ a i,j y ) within a cell D\ni,j is simulated by the LTL unit (ti,jx , t i,j y ). This induces the following mapping:\n(ti,jx , t i,j y ) = ζ i,j 3 (x, y) = ρ(·, ·,Φi,j(x, y), ·). (29)\nThis affine-linear transformation is implemented in the form of synaptic computation, which is only triggered when the BSL units provide enough excitation enabling the two neural units (ti,jx , t i,j y ) to cross their threshold value and execute the operation. The read-out of this process is as follows:\nti,jx = R(λ i,j x cx + a i,j x − h+Bix +Bjy) ti,jy = R(λ i,j y cy + a i,j y − h+Bix +Bjy),\n(30)\nthat is, initially the LTL units are rendered inactive with a strong inhibition bias h implemented as a synaptic projection from a bias unit, which is defined as follows:\n− h 2 ≤ −max i,j,k (ai,jk + λ i,j k ) with k = {x, y}. (31)\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nThis results from the fact that each BSL inputs Bix and B i y contribute respectively to half of the necessary excitation (h 2 ), that sum up and counterbalance the LTL’s natural inhibition (refer to Equation 27 and Equation 28). The LTL units also receive inputs from the MCL units (cx, cy), which are respectively modulated by the synaptic weights (λi,jx , λ i,j y ) and once the LTL units cross their threshold (mediated by the ramp activation function) then the intrinsic constant LTL neural dynamics (ai,jx , a i,j y ) completes the desired affine-linear transformation. The read-out is an updated encoded machine configuration, which is then synaptically projected back to the MCL units (cx, cy), initiating the next computation step (related to the original machine)."
    }, {
      "heading" : "2.4. Neuronal Observation Models",
      "text" : "In order to compare connectionist simulation results with experimental evidence from neurophysiology or psychology, one needs a mapping from the high-dimensional neural activation space Γ ⊂ Rn into a much lowerdimensional observation space that is spanned by p ∈ N observables ϕk : Γ → R (1 ≤ k ≤ p). A standard method for such a projection is PCA (Elman, 1991). If PCA is restricted to the first principal axis, the resulting scalar variable could be conceived as a measure of the overall activity in the neural network (as in beim Graben et al., 2008). Other important scalar observables that have been discussed in the literature are Smolensky’s harmony (Smolensky, 1986)\nH = ∑ ij uiwijuj\nwith u = (ui) as the network’s activation vector and W = (wij) its synaptic weight matrix, or Amari’s mean network activity (Amari, 1974)\nA = 1\nn ∑ i ui . (32)\nThe development of biophysically inspired observation models is an important research field in computational neuroscience (beim Graben and Rodrigues, 2013) as it could eventually lead to “synthetic” local field potentials (LFPs), electroencephalogram (EEG), or event-related brain potentials (ERPs) (Barrès et al., 2013). We shall use Amari’s measure (32) to derive such synthetic ERPs in what follows.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001."
    }, {
      "heading" : "3. Results",
      "text" : "The implementation of the R-ANN discussed in the previous Sections simulates a NDA in real-time and thus simulates its associated machine in real-time. More formally, it can be shown that under the map ρ(·) the commutativity property, ζ ◦ ρ = ρ ◦ Φ (see commutative diagram of Figure 1) is satisfied. The NDA simulation (and thus the machine simulation) by the R-ANN is achieved by a combination of synaptic and neural computation among three neural types (MCL, BSL, and LTL) and with a total of neural units equal to\nnunits = 2 + nα + nβ + 2nαnβ + 1 (33)\nwhere nα and nβ are the number of sub-sequences that can appear respectively in the left and right Domain of Dependence of the VS from which the NDA and the R-ANN are constructed. That is, a total of 2 MCL units, (nα+nβ) BSL units, 2nαnβ LTL units and a bias unit, that establish synaptic connections according to a synaptic weight matrix A of size (nunits × nunits) following the connectivity pattern described in Figure 4. Specifically, the synaptic weights in A are entries from the set {0, 1, h\n2 , −h 2 } ∪ {ai,jk − h | i =\n1, . . . , nαnβ , j = 1, . . . , nβ , k = x, y}, with the second set being the set of biases. A point worth mentioning is that the original formulation of the NDA relied on a simple Gödel encoding of the machine configurations, but subsequent work highlighted the advantages of using a more flexible representation by employing Cylinder sets, in order to preserve important structural relationships of the symbolic descriptions and to facilitate modeling (beim Graben and Potthast, 2009; beim Graben et al., 2008, 2004). Our R-ANN can be extended to incorporate a Cylinder set encoding of machine configurations by simply doubling the MCL and LTL layer.\nAn important modeling issue to consider is that of the halting conditions for the ANN, i.e. when to consider the computation as terminated. VSs, on which NDA and consequently our ANN model depend, do not define explicit halting conditions. However, two equally reasonable choices of halting conditions could be employed as follows. The first one is that of using a homunculus (beim Graben et al., 2004), an external observer which decides to intervene on the computation once some condition is met (for example, halting the computation when the input is in a certain region of the unit square). The second one is that of using a fixed point condition: implementing a machine halting state as an Identity branch on the NDA. This way a halting configuration will result in a fixed point on the NDA, and thus on\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nthe R-ANN. In other words, the network’s computation halts if and only if\nζ1(x ′, y′) = (x′, y′). (34)\nA halting by homunculus could be more appropriate in the context of interactive computation (beim Graben et al., 2008; Wegner, 1998) where constant and non-terminating interaction with the environment is assumed, or in cognitive modeling, where different kinds of fixed points, either desired or unwanted ones, are required in order to describe sequential decision problems (Rabinovich et al., 2008), such as linguistic garden paths (beim Graben et al., 2004, 2008).\nWe will now present two examples to demonstrate the strength of our developed methodology in mapping automata computation to R-ANN computation in real-time (an additional example on Turing Machines is available in the supplementary materials). The source code for all the examples is freely accessible via Carmantini (2015)."
    }, {
      "heading" : "3.1. Example 1: Finite-State Locomotive Pattern Generator",
      "text" : "FSMs are at the basis of many state-of-the-art approaches to the construction of locomotion controllers for articulated robots (see for example Alvarez-Alvarez et al., 2012; Collins and Ruina, 2005). They are easy to design, implement, and debug, and their relation with animal gait is well characterized (McGhee, 1968). On the other hand, recent research in robot locomotion control shows an increasing interest towards alternative approaches based on CPGs, neural networks capable of producing rhythmic patterns of activation in absence of rhythmic input sources. In his 2008 paper, Ijspeert presented the benefits and drawbacks of CPGs with respect to other approaches for robot locomotion control. We briefly summarize the benefits identified by the author: i) the rhythmic behavior supported by CPGs is robust to the transient perturbation of state variables; ii) CPGs are well-suited for distributed implementations (such as in modular robots); iii) CPGs reduce the dimensionality of the control problem by introducing few high-level control parameters allowing for the modulation of the locomotion; iv) CPGs are ideally suited for the integration of sensory feedback through coupling terms in the differential equations of the controller; v) CPGs often work well with learning and optimization algorithms. On the other hand, as specified by the author, CPG-based approaches are still lacking of a sound design methodology and theoretical grounding for their description. In the example\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\npresented in this Section, we will show how our mapping could aid the design of CPGs producing arbitrary patterns for locomotion in robots, starting from a FSM description of the desired rhythmic pattern. By combining the two approaches, the design of these controllers benefits from the solid theoretical grounding of FSM-based locomotion and from its ease of design and implementation. To contextualize our derived CPG in terms of familiar animal locomotion, we qualitatively model the results of a well-known experiment on cat gait.\nIn their seminal work, Shik et al. (1966) applied different levels of electrical stimulation to the midbrain of a decerebrated cat. The authors observed transitions in the gait of the animal as an increasing level of stimulation was applied, eliciting first a walk, then a trot and finally a gallop gait. Our theoretical framework can qualitatively reproduce these experimental observations, by deriving a R-ANN which generates the relevant gait patterns, and reproduces the transition between them as a function of the applied stimulus strength. To keep the exposition simple, we will only consider the walk and gallop gaits, and the transition between the two. In the study of the mammalian quadruped gait, the four legs are numbered so that each gait can be associated with a certain sequence, given by the order in which the legs touch the ground over one gait cycle. The left and right hind legs are associated respectively with the numbers 1 and 2, and the left and right fore legs are associated respectively with the numbers 3 and 4. The gait cycle is assumed to start when the left hind leg touches the ground. A walk gait is thus defined by the sequence (1, 3, 2, 4), and a gallop gait is defined by the sequence (1, 2, 3, 4). At a very high level, the computation carried out by the CPG in charge of producing the gait patterns in the quadruped mammalian can be informally stated as: if stimulation from midbrain is low, sequentially activate legs following pattern (1, 3, 2, 4). If it is high, sequentially activate legs from pattern (1, 2, 3, 4). We can implement the low level and high level of stimulation as the two input symbols of a FSM, and construct the δ transition function to sequentially reproduce the two patterns by switching between states. The FSM can thus be defined as in Table 2. This FSM can now be mapped (via our proposed approach) into a R-ANN, consisting in this case of 22 neural units (according to Equation 33). The chosen gamma functions for the Gödel encoding of this FSM are defined as\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nfollows:\nγs(σ) :=\n{ 0 if σ = <lo>\n1 if σ = <hi> γq(q) :=  0 if q = q1 1 if q = q2 2 if q = q3\n3 if q = q4\nThe step-by-step dynamics of the derived R-ANN can be observed in Figure 7. Here we use the machine’s input as the substrate for the external stimulus, which is ultimately encoded by the neural unit cy within our RANN as shown in the bottom plot of Figure 7. Note how we manipulate the activation of cy to gradually increase from a low to a high level of stimulation. That is, we introduce a continuous control parameter into an originally pure symbolic model, enabling us to carry out a bifurcation study in analogy with traditional coupled oscillator models (Golubitsky et al., 1999, 1998; Schöner et al., 1990; Collins and Richmond, 1994). Under this stimulation, the RANN defined by the mapping qualitatively reproduces the key features of the CPG involved in the locomotion and transitions described in Shik et al. (1966). In particular, it is possible to observe how low levels of stimulation elicit the production of the walk gait cycle, whereas an increase in the level of stimulation induces a sudden transition to the gallop gait cycle.\nThis key relation between the stimulation level (i.e a real control parameter) and the computation carried out by the network, which can be related to the underlying symbolic space thanks to the mapping, depends upon an informed decision in the gamma numbering of the states for the Gödel encoding. In fact, the chosen gamma numbering ensures that the unit square encoding of machine configurations where <lo> is the current input symbol\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\ncorresponds to all points (x, y) such that x < ψy(<hi>) where ψy is defined as in Equation 18, and specifically ψy(<hi>) = γs(<hi>)g −1 s = 1 2 . In terms of the underlying NDA representation, increasing the activation of cy until its value reaches and exceeds 1\n2 corresponds to forcing the encoded machine\nstate to cross the boundary between cells associated to a <lo> input symbol to those associated to a <hi> input symbol, thus causing a transition between a walk and a gallop gait. Note that in this example, we do not model halting conditions for the derived network, as it is not clear what halting means in the context of the computation performed by CPGs.\nTo summarize, we derived a CPG from a FSM description of a locomotion controller, inspired by results on the generation of gait patterns in the cat midbrain. By doing so, we outlined a new design methodology for CPGbased locomotion control in robots which does not suffer from some of the drawbacks of other CPG approaches, by grounding the description and design of the CPG on the theoretical grounding of FSM-based approaches. Some problematic aspects of the methodology we outlined are due to the discretetime nature of our mapping. In fact, fully realizing the benefits of CPG-based approaches summarized at the beginning of this Section requires continuous time models. This notwithstanding, we believe that the proof of concept we provide here already shows encouraging results for future developments.\nAs an additional remark, the methods we describe in this paper are ideally suited for the deriving of neural networks implementing paradigms of interactive computation, as we will demonstrate shortly. This is especially relevant for the design of CPGs. In fact, recent research has unveiled a surprising degree of hierarchical organization in mammalian respiratory CPGs, which allows for a highly robust and flexible pattern production that can adapt to a variety of conditions (see for example work by Smith et al., 2013, 2007). Our methodology easily accommodates the mapping of hierarchies of automata to hierarchically organized neural networks, as we demonstrate in the next example through the modeling of garden-path parsing, a concept employed in language processing (beim Graben et al., 2004). Importantly, networks of automata could be used to design complex pattern generation in modular robots (see Spröwitz et al., 2014 for a recent example of modular robots using a distributed CPG for locomotion)."
    }, {
      "heading" : "3.2. Example 2: Interactive Automata Networks",
      "text" : "Interactive computation (Wegner, 1998) is a recent theoretical development that seeks to formalize the complexity of interactions that we observe in\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nreal-world computing. In classical Automata Theory, the interaction between an automaton and the external world is restricted to an input-output relation. That is, the external world provides an input, the automaton performs its computation on that input, and then returns an output to the external world. Within the framework of interactive computation, instead, automata can interact with the external world (and with other automata) at every step of their computation. External forces can act on the configuration of the automaton, and the configuration can itself affect the external world. Clearly, this framework provides a much richer language to describe models of computation, and is especially useful to express notions of compositionality and concurrency. These constructs are essential not only in the study of modern computing systems, but also in the context of cognitive modeling. In this example, we will build a model of the human processing of locally ambiguous sentences by constructing a network of interactive automata. Through this proof-of-concept, we want to demonstrate the flexibility of our approach by showing how it can be seamlessly used to construct neural networks implementing interactive systems. In order to do so, we choose a system that i) is simple enough to allow for clear exposition, but complex enough to carry out a meaningful computation; ii) is composed by a range of different automata; iii) incorporates different forms of interaction between its automata components.\nGarden-path sentences are locally ambiguous sentences that induce the temporary production of an erroneous parse by the reader, which is then forced to reconsider their interpretation of the previously presented material in order to finally reach a correct parse. Consider for example the sentence “I convinced her children are noisy”. In reading the sentence, the reader first constructs an intermediate parse where “her children” is the object of the phrase “I convinced”. After reading the rest of the sentence, the reader realizes that the intermediate parse was incorrect: “her” is the object of “I convinced”, and “children are noisy” is a subordinate clause. The reader thus reanalyzes the sentence to produce a correct parse. Osterhout et al. (1994) have shown that the reanalysis of a sentence due to a garden-path is associated in the brain of the reader with a positive deflection 600 milliseconds (P600) after the onset of a garden-path – the word “are” in the example above – in sequentially presented sentences, as measured by a trial averaged electroencephalogram (thus obtaining event-related brain potentials).\nMany proposals have been advanced to account for the mechanisms underlying the reanalysis of incorrectly parsed sentences due to garden-path\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\neffects. In our model, we implement the reanalysis through a diagnosis and repair mechanism, described in Lewis (1998). By this account, the parser tries to incrementally build a parse as the sentence material is presented. If a dead-end is reached (i.e. the parser becomes stuck in a garden-path), the parser diagnoses the need for reanalysis, and the search space of possible continuations of the parse is modified by some repair operator that “bridges” the dead-end to another point in the search space, allowing the parser to correctly complete the processing of the sentence. The parser model we create implements this mechanism to process garden-path sentences where the local ambiguity is given by the incorrect assignment of the subject and object grammatical constituents.\nIn many languages, native speakers have been shown to prefer to interpret an ambiguous nominal constituent as a subject rather than an object. Consider for example the following two sentences, extracted from the ERP study on ambiguous pronouns by Frisch et al. (2004) on German speakers. Both sentences start with\nNachdem die Kommissarin den Detektiv getroffen hatte . . . After the cop the detective had met . . .\n“After the cop had met the detective, . . . ”\nOne of the sentences then continues with a clause in subject-object order (s-o sentence), i.e. the preferred order in the parsing of ambiguous constituents:\n(s-o sentence)\n. . . sah sies den Schmugglero . . . saw she the smuggler\n. . . “she saw the smuggler”\nIn this case, the reader correctly interprets “sie” to be the subject of the second clause, and “den Schmuggler” as the object (as “den Schmuggler” is in the accusative case, thus specifying a direct object to the verb “sah”). The second sentence is instead in the dispreferred object-subject order (o-s sentence):\n(o-s sentence)\n. . . sah sieo der Schmuggler s . . . saw she the smuggler\n. . . “the smuggler saw her”\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nThe psycholinguistic study by Frisch et al. (2004) has shown that the reader first tries to apply the preferred subject-object parsing strategy to this clause (and sentences with similar subject/object pronoun ambiguity). The reader thus initially interprets “sie” as the subject of the clause in nominative case, expecting it to be followed by the object in accusative. Upon further reading, however, they realize that “der Schmuggler” is in the nominative case instead, and thus has to be the subject. This leads the reader to reconsider the previous material to correctly parse “sie” as a pronoun in accusative case, the direct object of the verb “sah”. This reanalysis was observed as a P600 effect in the ERP. At a high level of abstraction (beim Graben et al., 2004), we can capture the structure of these sentences through a CFG G with production rules:\nS→ s o (s-o) S→ o s, (o-s)\nwhere S is a distinguished starting non-terminal, and where the s and o terminals stand respectively for “subject” and “object” phrase.\nIn our model we thus split the G grammar into two grammars Gs-o and Go-s, comprising respectively of the s-o and o-s production rules (beim Graben et al., 2004), and reflecting the existence of two strategies in the parsing of sentences with subject/object pronoun ambiguity. To recognize the two different sentence structures, our model is endowed with two specialized TDRs, constructed from the Gs-o and Go-s grammars as shown in section 2.1.2. Initially, the s-o TDR is tried on the input, to model the subject-object interpretation preference. In case it fails because of a garden path, the model acts as prescribed by a diagnosis and repair account. That is, it first diagnoses that a problem has arisen in parsing, repairs the parse, and finally switches strategy to correctly parse the input. In order to implement the diagnosis step, our model needs a way to monitor the state of the parse and extract the relevant diagnostic information. We implement this through a Diagnosis PDA (see Table 3), which compares the current parse with that from the previous time step; if the parse didn’t change, that means that the parser is stuck and can’t process the input further. In that case the Diagnosis PDA changes its state to an “error” state, thus implementing the diagnosis step. The repair step is realized by introducing a Repair VS , that can be described by the following rewriting rule:\ns o . w → o s . w, (35)\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\ncorresponding to a reanalysis of the ambiguous sentence in terms of the dispreferred object-subject sentence structure. Once the sentence has been reanalyzed and thus the parse repaired, the second parser can proceed to process the input until it has been completely consumed and the stack is emptied. In order to switch strategies, our model needs a higher-level controller that has access to diagnostic information about the current parse, and decides which parsing strategy to apply. In particular, this controller should first activate the preferred s-o TDR. If the parser failed (as signaled by the Diagnosis PDA) then the higher-level controller should first activate the Repair VS to allow for the reanalysis of the ambiguous sentence, and subsequently activate the o-s TDR. We implement the high level controller through a Strategy FSM (see Table 4), endowed with the capability of selectively activating the s-o and o-s TDRs, as well as the Repair VS, by switching its internal state. This machine receives the diagnostic information provided by the Diagnosis PDA as input. The FSM has three states, namely an “s-o” state, a “repair” state, and an “o-s” state. By switching between these states, the FSM can activate the respective automata. Note that this form of interaction is not defined for the VS introduced in Section 2.1.1. That is, we do not define a way for a VS to “call” other shifts. Extending VSs to incorporate notions\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nof compositionality and concurrency will allow the refining of the mapping presented in this paper to reflect these new capabilities. For the moment, we just want to demonstrate the possibilities opened by the present work; for this reason, we will implement the “subroutine” capability in our neural network through a familiar mechanism already encountered in the previous Sections, ignoring momentarily the missing theoretical details and leaving their definition for future work.\nTo avoid race conditions, at most one automaton in the interactive network can re-write symbols in a sub-sequence at any given computation step . The “parse” sub-sequence can only be read, but not re-written, by the Diagnosis PDA. Similarly, the “diagnosis” sub-sequence can only be read, but not re-written, by the Strategy FSM. Furthermore, the selective activation of the s-o TDR, the o-s TDR, and the Repair VS operated by the Strategy FSM ensures that at any given computation step only one between these automata can perform symbolic re-writing on the “input” and “parse” sub-sequences.\nTo map the system of interactive automata to a R-ANN, we first convert each of its component in the familiar way, as described in the previous Sections. That is, the s-o and o-s TDRs, the Repair VS, the Diagnosis\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nPDA, and the Strategy FSM are first converted to VSs acting on dotted sequences, then mapped to their NDA representation and finally to R-ANNs. The Gödelizations of the “input”, “parse” and “strategy” sub-sequences are defined as in Equation 18, with each of the gamma enumerating functions defined as follows:\nγinput := {(t, 0), (S, 1), (o, 2), (s, 3)} γparse := {(t, 0), (o, 1), (s, 2)}\nγdiagnosis := {(pdaqidle, 0), (pdaqparsing, 1), (pdaqerror, 2)} (36)\nwhere each function is represented as a set of (σ, k) pairs, with σ being a symbol and k ∈ N its enumeration. The Gödelization of the “diagnosis” sub-sequence is instead defined as in Equation 19, with\nγstrategy := {(fsmqs-o, 0), (fsmqo-s, 1), (fsmqrepair, 2)}\nenumerating the states of the Diagnosis PDA, and γparse (already defined in Equation 36) enumerating its stack symbols. Having mapped each of the machines to a R-ANN, we can use the derived networks as components of the overall system architecture (see Figure 9 for the full architecture). In order to simplify the exposition, we construct the overall network to feature only one set of recurrent connections. To do so, we endow our architecture with 4 Configuration Layers (CLs), containing the “strategy”, “diagnosis”, “parse”, and “input” sub-sequences. Between each CL and the next, the network components derived from the automata are connected to perform their part of the processing on the relevant subsequences. In particular, if the VS representation of an automaton acts on some α.β dotted sequence, the input of its associated network component is connected to the units encoding the α and β subsequences in the i-th CL, whereas its output (which is a recurrent connection to the MCL layer in the original mapping) is connected to the units encoding α and β in the (i+ 1)-th CL. The final CL is connected with a I4×4 synaptic weight matrix to the first CL layer (i.e. each unit encoding a subsequence of the last CL is connected with a weight of 1 to the same unit in the first CL). Finally, to implement the subroutine call capabilities of the strategy FSM, we add a Meta branch selection layer that takes the “strategy” subsequence as input, and is connected with the lateral inhibition connection pattern specified in Section 2.3.3 to the s-o and o-s TDRs, and to the Repair VS. Note how this creates a nested structure, with the s-o TDR, the o-s TDR, and the Repair VS functioning as higher-level symbolic\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\noperations of a Parser machine. This is reflected in the nested structure of the Parser R-ANN sub-network, where the lower-level machines function as cells in a LTL, controlled by the Meta BSL (see Figure 9).\nIn Figure 10 we show the network activation when two different sentence structures are presented in input. In particular, note the serial activation of the s-o TDR, Repair VS and o-s TDR sub-networks when a object-subject sentence is presented. By mapping the parser from a machine evolving in a symbolic space to a neural network evolving in a vectorial space, we are now able to compute synthetic event-related potentials, or “synth-ERPs”, (beim Graben et al., 2008; Barrès et al., 2013) as trial-averages of the mean network activation, as discussed in Figure 11. This is achieved by calculating the mean global network activation according to Amari (1974) (Equation 32) for a simulation over 100 trials for each input stimulus, where random initial conditions compatible with the symbologram representation of the input are prepared according to beim Graben et al. (2008). In brief, symbologramcompatible random initial conditions are generated through the Gödelization of sequences of the form wαu.vwβ, where u.v is the dotted sequence describing the input to the system, and wα, wβ ∈ A∗ are random sequences of symbols in A.\nAs Figure 11 reveals, the network shows a P600-like effect in the processing of garden-path sentences, with a peak of increased and sustained activation with respect to the control condition. The simplified model of garden-path processing we presented here does not yet allow for a direct quantitative comparison with experiments such as in Frisch et al. (2004) (in fact, a carefully crafted model would require a level of detail and attention which goes beyond the scope of this paper). Yet, these simulations could be the starting point for more detailed statistical correlation analyses (beim Graben and Drenhaus, 2012; Frank et al., 2015) in future work, relating these computations to electrophysiological measurements."
    }, {
      "heading" : "4. Discussion and Outlook",
      "text" : "In this study we have developed a constructive, transparent, modular and parsimonious mapping from symbolic algorithms to neural networks. We first introduced a novel shift map, the versatile shift, that extends the generalized shift and allows for the real-time simulation of a range of symbolic models of computation. We then showed how VSs can be represented on a vectorial space through Gödelization, obtaining piecewise affine-linear systems\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\ns-o sentence\no-s sentence\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nTim e step\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nM ea\nn ac\ntiv at\nio n\nDiagnosis Repair Cont rol Garden path\nS.so os.so o.o ε.ε\nS.os os.os os.os so.os s.s\nε.ε ε.ε ε.ε ε.ε ε.ε ε.ε ε.ε\nε.ε ε.ε ε.ε ε.ε ε.ε ε.ε\nε.ε ε.ε\nε.ε ε.ε\nInitialization\nFigure 11: Synthetic P600 event-related brain potential as mean Network activation for random cloud of initial conditions. In this figure we show the mean global network activation calculated through Equation 32 for each time step of two simulations, averaged over 100 trials. For each of the two simulations, we run the network presenting at time t = 2 one of 100 random inputs generated compatibly to the symbologram representation of one of two sequences. In other words, noise is added to each input such that, if the input was generated by Gödelizing a sequence of length n, decoding the input would yield the original sequence in the first n symbols, with the rest being a random symbolic continuation. If stronger noise was added instead, that would have prevented the network to correctly perform its computation, as we would have destroyed essential input information. In blue, we show the averaged mean activation (light blue) and its standard deviation (dark blue) for a presented input encoding the sequence S.so, representing an input sequence in subject-object order, i.e. the network’s preferred order as explained in Section 3.2. Note that the parsing is completed at t = 5. In red, the averaged mean activation (light red) and its standard deviation (dark red) for an input encoding the sequence S.os, representing an input sequence in object-subject order, leading to a garden path in the parsing of the input. The time at which the diagnosis (t = 4) and repair (t = 5) steps are carried out in the symbolic interactive system (and thus in its recurrent artificial neural network mapping) is indicated by arrows. We also report, at the top and bottom of the plot, the configuration of the parser networks as a dotted sequence for each time step, for respectively the garden path and the control condition. Note how the garden-path processing is associated with a strong divergence in activation starting from time t = 5, and followed by a longer tail than that of the network in the control (preferred) condition. This reflects the additional computation needed by the network to successfully resolve the garden path in parsing, and qualitatively corresponds to the P600 event-related brain potential measured in psycholinguistics experiments (see Section 3.2). Furthermore, note that in both conditions the network starts and returns to a “resting state”, waiting for input to process from the external world, implementing a notion of continuous computation which is the hallmark of interactive systems.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\non the unit square known as nonlinear dynamical automata (Tabor, 2000; Tabor et al., 2013; beim Graben et al., 2004, 2008). Finally, we presented a modular R-ANN architecture that simulates the dynamics of NDA. The proposed architecture consists of three layers: a machine configuration layer representing the NDA state, and thus the symbolic data in the simulated automaton; a branch selection layer implementing the NDA switching rule, thus characterizing the automaton’s decision space, or control; and the linear transformation layer implementing the set of piecewise affine-linear functions in the NDA, i.e. the vectorial representation of the symbolic operations defined in the transition table of the simulated automaton. Additionally, the linear transformation layer is itself modular, in that each operation specified by the δ transition function of the simulated automaton is applied by a specific pair of units in the layer.\nThe mapping can be used to simulate any Turing machine through RANNs, thus making the architecture universal (an example of the mapping on Turing Machines is reported in the supplementary materials). In particular, it is possible to simulate the 7-states 4-symbols UTM by Minsky (1962) in real-time with a R-ANN consisting of 259 units 6 (see Equation 33), and the 6-states 4-symbols UTM by Neary and Woods (2009) with one consisting of 223 units.\nIt is important to analyze some of the modeling choices that have been made in the R-ANN architecture we described. A choice worth discussing is that of implementing biases as synaptic projections from an always-active unit as opposed to implementing them as parameters intrinsic to the individual units. We decided for simplicity to add a bias unit. Nonetheless, a parameterized bias would have been equally reasonable. While it does not have strong bearings on the model here discussed, it is interesting to note that the specific choice of implementation does more or less put the emphasis on a predominantly synaptic computation versus a computation which is more distributed between the synaptic and the neuron level, reflecting similar issues to be considered in the biological domain. A second consideration concerns the cell’s boundaries in the NDA. In fact, the distance between the right bound of a cell and the left bound of the next one is zero. This poses some challenges, as even extremely small noise on the state vector at a\n6This implies a reduction factor of 1/3 when compared to the solution by Siegelmann and Sontag (1991, 1995), which simulates Minsky’s UTM with a network of 886 units.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nboundary can lead to an erroneous application of the switching rule on the real state, and thus to a disruption of the computation. This is of course reflected in the dynamics of the associated R-ANN as well. Siegelmann and Sontag (1995) solve this issue by using a Cantor encoding as opposed to a simple Gödelization, ensuring a greater than zero distance between two encoded configurations with different leading symbols. The same methods can be applied here. Interestingly, by switching to a Cantor encoding, the Heaviside units in the BSL layer can be substituted with functionally equivalent Ramp units, so that the R-ANN would only make use of linear units à la Siegelmann and Sontag.\nWe will now first discuss the advantages of our approach over those based on eliminative connectionism, and then the advances that the present work brings to transparent connectionism.\nCompared to eliminative approaches, our work allows the direct interpretation of the representations and the dynamics in the derived network in terms of symbolic computation. This has many important consequences. First, while conventional neural networks have to be trained on large data sets (usually using backpropagation or related algorithms, see Werbos, 1990) our method does not require any training, as the synaptic weight matrix is explicitly designed from the machine table of the encoded automaton. Emergent representations and operations are not opaquely encoded in several hidden layers but transparently realized through Gödelization of symbolic configurations. Second, even when considering learning applications – which we plan to explore in future developments – the derived approach could bring about the exciting possibility of a symbolic read-out of a learned algorithm from the network weights; Note that in this architecture all weights are necessarily fixed, with the exception of the connections encoding the symbolic operations in the simulated automaton, i.e. those between the MCL and the LTL layer. Third, anchoring the computation of the network to well-understood computation models is worthwhile when tackling problems that can benefit from the integration of the two perspectives. In the first example, we constructed a R-ANN (24 units) performing a FSM machine computation abstracting a CPG for animal locomotion. FSMs are widely used in locomotion controllers in robotics, because of their simplicity and strong theoretical grounding in relation to animal locomotion. On the other hand, neural implementations of CPG have many desirable characteristics (as discussed in Ijspeert, 2008) that are not present in FSM-based implementations, but they are difficult to engineer. We showed that by integrating the two approaches we can tackle\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nthe problem of pattern generation in robotic locomotion more effectively. Of course, a satisfactory solution would entail the use of continuous-time models in the mapping; nevertheless, our preliminary results already present distinct benefits in the integration of the two approaches as compared with their use in isolation. Fourth, having a complete understanding of the network’s inner workings allows for the intelligent manipulation of its parameters. In the discussed CPG example, understanding the computation carried out by the derived network allowed us to introduce a continuous control parameter eliciting a bifurcation in the dynamics of the network, as present in systems of coupled nonlinear oscillator models (Golubitsky et al., 1999, 1998; Schöner et al., 1990; Collins and Richmond, 1994), widely studied in the CPG literature.\nIn regards to previous work on transparent connectionism, our work advances the field in several ways. As a first advancement, by introducing VSs we are now able to use NDA to simulate a broad range of symbolic computation models in real-time, extending the original work by Moore (1990, 1991). Interestingly, it would be straightforward to define n-sided infinite dotted sequences (where the dot splits a sequence in its n one-sided infinite components), and extended VSs on these. By Gödelization, we would obtain NDA on the n-dimensional hypercube, which could be simulated by R-ANNs through a straightforward extension of the architecture presented in this work. This would further extend the range of real-time simulable computational models to automata with multiple tapes or stacks (Aho, 1969; Weir, 1994). Secondly, by basing our construction on NDA, we obtain an architecture characterized by a fully distributed representation coupled with a granular modularity, differentiating our approach from previous work and granting a series of advantages. The mapping is transparent not only with regards to the representations (the data), but also with regards to the symbolic operations defined in the simulated computational model and their control, all clearly localizable in the architecture. We regard this as an advance in itself (in line with the goals of transparent connectionism), but it also allows, for example, for the straightforward mapping of interactive automata networks to R-ANNs. This is of fundamental importance, as the framework of interactive computation provides a rich language for the description of many complex systems, for example in cognitive modeling. In the second example we constructed a network of interacting automata as a diagnosis and repair model (Lewis, 1998; beim Graben et al., 2004, 2008) for the reanalysis of linguistic garden path sentences. The network consisted of three PDA (two\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nof them as TDRs), a VS, and one FSM as a master control program, with each component carrying out a specific and intelligible task in the overall computation. We then mapped this network to a R-ANN (266 units), thus obtaining a symbolic/connectionist implementation of a cognitive model. Interestingly, due to the multiple levels of hierarchical organization that can be present in the automata network (which comprises nesting, as in the diagnosis and repair network) and, thus, in the derived R-ANN, one could even speculate about thermodynamic limit networks when the number of modules approaches infinity, presenting emergent scale-free or small world properties (Albert and Barabási, 2002). The granular modularity of our approach is also a key advancement when considering the possibility of correlational studies with neurophysiological measurements. In previous work we showed how to devise large-scale biophysical observation models in order to correlate top-down modeling approaches with neurophysiological data obtained from bottom-up measurements (Amari, 1974; beim Graben and Rodrigues, 2013). The process involves associating neural units of our model with neuronal masses (Lopes da Silva et al., 1974; Jansen and Rit, 1995) or Hebbian cell assemblies (Hebb, 1949; Wennekers and Palm, 2009; Huyck, 2009) in large-scale brain models, as investigated, e.g., in neural field theory. With this setup we then show that our observational models lead to improved interpretation, e.g of “synthetic event-related brain potentials” (as discussed in Section 2.4, see beim Graben et al., 2008; Barrès et al., 2013) as used in computational neurolinguistics studies (Gigley, 1985; beim Graben and Drenhaus, 2012; Barrès et al., 2013), where mental/cognitive states can be associated to metastable states of a dynamical system. In the second example presented here, we computed Amari’s mean activation (Amari, 1974) as an observation model for the diagnose and repair R-ANN, in order to obtain synthetic ERPs (beim Graben et al., 2008; Barrès et al., 2013). Qualitatively, the computed signal exhibited a similar divergence between conditions as measured in the experiment presented in Frisch et al. (2004). While preliminary, these are already encouraging results for the development of our approach in this direction. In future work, we envisage that it will be possible to selectively correlate electrophysiological measurements with specific components in a derived R-ANN, as informed by a suitable symbolic model for the computation underlying the measured quantities. As a third point of interest, the architecture presents a clear 2D spatial organization in its layout, particularly at the level of LTL (as highlighted in Figure 6). In a NDA, different transformations are applied based on the position of the Gödelized automaton data on the unit square.\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nIn the R-ANN architecture, this is implemented through the BSL, which performs a form of spatial pattern matching, activating a specific pair of units in the LTL through a lateral inhibition mechanism. When considering extensions to models of higher complexity, the functionality of BSL and LTL could be implemented through the use of a grid of units with receptive fields, as defined for example in self-organizing maps (SOMs, see Kohonen, 1982; Kohonen and Somervuo, 1998).\nIn future work, we plan to overcome fundamental issues with the current model which have bearing both in relation to learning applications and to the extension of the model to continuous dynamics. For what concerns the learning of algorithms from data, the current model suffers from a missing end-to-end differentiability, due to the use of Gödel encodings. This is a serious limitation, as it prevents the use of gradient descent methods for the training of the network’s weights. Future work will have to address this limitation, possibly relying on methods of data access and manipulation akin to modern R-ANN approaches such as in Weston et al. (2014); Graves et al. (2014); Grefenstette et al. (2015); Joulin and Mikolov (2015); Sukhbaatar et al. (2015). Encouraging work on the learning of exponential state growth languages by Fractal Learning Neural Networks (Tabor, 2003, 2011) could also inform a revised trainable architecture.\nWith regards to the extension of the model to continuous dynamics, there are many ways in which this could be achieved in future work. Importantly, we are mostly interested in extensions to continuous-time models that are excitable. In such systems, trajectories can be perturbed away from a stable equilibrium (or rest state) and come back to it only after a large excursion (or spike) in the phase space, upon sufficiently strong input; biophysical examples of excitable models were initiated in Hodgkin and Huxley, 1952. One possibility would be to first extend the mapping to discrete-time excitable models (as in map-based neuronal models, see Ibarz et al., 2011; Girardi-Schappo et al., 2013), and then move to continuous time via so-called suspension procedures. There are some potential issues in this endeavor. First of all it would be crucial to first explore and understand the possible relationships between excitable regimes in neural models and symbolic dynamics in a computation. That is, to answer the question: how does the excitability property translate in the realm of symbolic computation? We think there could be meaningful answers to this question when tackled through the framework of interactive computation. Another potential issue is that the suspension process is nonunique and non-trivial in the general case; moreover, it does not guarantee\nc©2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license. Formal publication available at https://dx.doi.org/10.1016/j.neunet.2016.09.001.\nthat the excitability property will be preserved. Excitability is a crucial matter when dealing with neural tissue of lower brain structures, such as the Brain stem, where it is possible to neurophysiologically identify clear and small neuronal networks. However, neural networks models are not the most appropriate level of description for higher cortical structures, due to the presence of large and highly interconnected neuronal masses. Models of these structures express slow but large scale processes as measured by LFP/EEG. In this context, an alternative approach to achieve continuous-time dynamics, which we have already explored to some extent in previous work, is by the framework of heteroclinic dynamics, where Turing machine configurations can be interpreted as metastable states with attracting and repelling directions (beim Graben and Potthast, 2009; Tsuda, 2001; Rabinovich et al., 2008; Krupa, 1997), or by the framework of multipletime scale dynamical systems (Desroches et al., 2013; Fernández-Garćıa et al., 2015)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research has been supported by a Heisenberg fellowship (GR 3711/1- 2) of the German Research Foundation (DFG) awarded to PbG."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Gödelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the ∗Corresponding author Email address: giovanni.carmantini@gmail.com (Giovanni S. Carmantini) Preprint submitted to Neural Networks September 8, 2016 ar X iv :1 60 9. 01 92 6v 1 [ cs .N E ] 7 S ep 2 01 6 design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.",
    "creator" : "LaTeX with hyperref package"
  }
}