{
  "name" : "1708.00088.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Algorithms for Active Learning",
    "authors" : [ "Philip Bachman", "Alessandro Sordoni", "Adam Trischler" ],
    "emails" : [ "<phbachma@microsoft.com>,", "<alsordon@microsoft.com>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "For many real-world tasks, labeled data is scarce while unlabeled data is abundant. It is often possible, at some cost, to obtain labels for the unlabeled data. In active learning, a model selects which instances to label so as to maximize some combination of task performance and data efficiency. Active learning is motivated by the observation that a model may perform better while training on less labeled data if it can choose the data on which it trains (Cohn et al., 1996). E.g., in SVMs (Schölkopf & Smola, 2002) only the support vectors affect the decision boundary. If one could identify the support vectors in advance, the classifier trained on the resulting set of examples would obtain the same decision boundary with less data and computation.\nActive learning can benefit a variety of practical scenarios. For example, preference information for a new user in a movie recommender system may be scarce, and recommendations for the new user could be improved by carefully selecting several movies for her to rate (Sun et al., 2013b; Houlsby et al., 2014; Aggarwal, 2016). Likewise, collecting labels for a medical imaging task may be costly because it requires a specialist (Hoi et al., 2006), and the cost could be reduced by carefully selecting which images to label.\nVarious heuristics for selecting instances to label have been\n*Equal contribution 1Microsoft Maluuba, Montreal, Canada. Correspondence to: P. Bachman <phbachma@microsoft.com>, A. Sordoni <alsordon@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nproposed in the active learning literature, such as choosing the instance whose label the model is most uncertain about, or the instance whose label is expected to maximally reduce the model’s uncertainty about other instances (GiladBachrach et al., 2005; Settles, 2010; Houlsby et al., 2011). We propose moving away from engineered selection heuristics towards learning active learning algorithms end-to-end via metalearning. Our model interacts with labeled items for many related tasks in order to learn an active learning strategy for the task at hand. In recommendation systems, for example, ratings data for existing users can inform a strategy that efficiently elicits preferences for new users who lack prior rating data, thus bootstrapping the system quickly out of the cold-start setting (Golbandi et al., 2010; 2011; Sun et al., 2013a; Kawale et al., 2015). A learned active learning strategy could outperform task-agnostic heuristics by sharing experience across related tasks. In particular, the model’s (i) data representation, (ii) strategy for selecting items to label, and (iii) prediction function constructor could all co-adapt. Moving from pipelines of independentlyengineered components to end-to-end learning has lead to rapid improvements in, e.g., computer vision, speech recognition, and machine translation (Krizhevsky et al., 2012; Hannun et al., 2014; He et al., 2016; Wu et al., 2016).\nWe base our model on the Matching Networks (MN) introduced by Vinyals et al. (2016). We extend the MN’s one-shot learning ability to settings where labels are not available a priori. We cast active learning as a sequential decision problem: at each step the model requests the label for a particular item in a pool of unlabeled items, then adds this item to a labeled support set, which is used for MN-style prediction. We train our model end-to-end with backprop and reinforcement learning. We expedite the training process by allowing our model to observe and mimic a strong selection policy with oracle knowledge of the labels.\nWe demonstrate empirically that our proposed model learns effective active learning algorithms in an end-to-end fashion. We evaluate the model on “active” variants of existing oneshot learning tasks for Omniglot (Lake et al., 2015; Vinyals et al., 2016; Santoro et al., 2016), and show that it can learn efficient label querying strategies. We also test the model’s ability to learn an algorithm for bootstrapping a recommender system using the MovieLens dataset, showing it holds promise for application in more practical settings.\nar X\niv :1\n70 8.\n00 08\n8v 1\n[ cs\n.L G\n] 3\n1 Ju\nl 2 01\n7"
    }, {
      "heading" : "2. Related Work",
      "text" : "Various heuristics have been proposed to guide the selection of which examples to label during active learning (Settles, 2010). For instance, Lewis & Gale (1994) and Tong & Chang (2001) developed policies based on the confidence of the classifier, while Gilad-Bachrach et al. (2005) used the disagreement of a committee of classifiers. Houlsby et al. (2011) presented an approach based on Bayesian information theory, in which examples are selected in order to maximally reduce the entropy of the posterior distribution over classifier parameters.\nThe idea of learning an active learning algorithm end-to-end, via meta active learning, was recently investigated by Woodward & Finn (2016). Building on the memory-augmented neural network (MANN) (Santoro et al., 2016), the authors developed a stream-based active learner. In stream-based active learning the model decides, while observing items presented in an exogenously-determined order, whether to predict each item’s label or to pay a cost to observe its label. Our proposed model instead falls into the class of poolbased active learners, i.e. it has access to a static collection of unlabeled data and selects both the items for which to observe labels, and the order in which to observe them.\nActive learning can be useful when the cost incurred for labeling an item may be traded for lower prediction error, and where the model must be data efficient (e.g. in medical imaging (Hoi et al., 2006)). We explicitly train our model to balance between task performance and labeling cost. In this sense, we build an anytime active learner (Zilberstein, 1996), with the model trained at each step to output the best possible prediction on the evaluation set.\nOur model builds on the matching-networks (MN) architecture presented by Vinyals et al. (2016), which enables “one-shot” learning, i.e. learning the appearance of a class from just a single example of that class (Santoro et al., 2016; Koch, 2015). Vinyals et al. (2016) assume that at least one example per class exists in the labeled support set available to the model. Confronted with the harder task of composing a labeled support set from a larger pool of unlabeled examples, we show that the active learning policy learnt by our model obtains, in some cases, an equally effective support set. As in the recent one-shot learning work of Santoro et al. (2016) and Vinyals et al. (2016), and the active learning work of Woodward & Finn (2016), we evaluate our model on the Omniglot dataset. This dataset was developed for the foundational one-shot learning work of Lake et al. (2015), which focused on probabilistic program induction.\nThe cold-start problem is ubiquitous in recommendation systems (Aggarwal, 2016; Lika et al., 2014; Harpale & Yang, 2008; Sun et al., 2013b; Elahi et al., 2016). Instead of bootstrapping from a cold-start by randomly selecting items for\na user to rate, an active learner asks for particular items to help learn a strong user model more quickly. In model-free strategies (Rashid et al., 2008), items are selected according to general empirical statistics such as popularity or informativeness. These approaches are computationally cheap, but lack the benefits of adaptation and personalization. Proposals for learning an adaptive selection strategy have been made in the form of Bayesian methods that learn the parameters of a user model (Houlsby et al., 2014; Harpale & Yang, 2008), and in the form of decision-trees learned from existing ratings (Sun et al., 2013b). An extensive review can be found in Elahi et al. (2016). Intuitively, our model learns a compact, parametric representation of a decision tree end-toend, by directly maximizing task performance. We evaluate our active learner on MovieLens-20M, a standard dataset for recommendation tasks.\nWe provide hints to our model during training using samples from an oracle policy that knows all the labels. Related approaches have been explored in previous work on imitation learning and learning to search (Ross & Bagnell, 2014; Chang et al., 2015). These methods, which focus the cost of sampling from the oracle policy on states visited by the model policy, have recently been adopted by researchers working with deep networks for representation learning (Zhang & Cho, 2017; Sun et al., 2017)."
    }, {
      "heading" : "3. Model Description",
      "text" : "We now present our model, which metalearns algorithms for active learning. Our model metalearns by attempting to actively learn on tasks sampled from a distribution over tasks, using supervised feedback to improve its expected performance on new tasks drawn from a similar distribution. Succinctly, our model solves each task by adaptively selecting items for the labeled support set used by a Matching Network (Vinyals et al., 2016) to classify test items. The full support set from which our model selects these examples contains both labeled and unlabeled data.\nFor a summary of our model, see the architecture diagram in Figure 1, the optimization objectives in Equations 2, 3, and 5, and the pseudo-code in Algorithm 1. We present a formal description of our meta active learning task in Section 3.1. We describe the details of our model in Section 3.2, and our approach to parameter optimization in Section 3.3."
    }, {
      "heading" : "3.1. Task Description",
      "text" : "Our model refines its behaviour over many training episodes, in order to maximize performance during test episodes not encountered in training. In each episode, our model interacts with a support set S ≡ {(x, y)} comprising items x for which the model can request labels y, and a similarlydefined evaluation set E ≡ {(x̂, ŷ)}. Let Sut ≡ {(x, ·)}\ndenote the set of items in the support set whose labels are still unknown after t label queries, and let Skt ≡ {(x, y)} denote the complementary set of items whose labels are known. Let St denote the joint set of labeled and unlabeled items after t label queries. Let the real-valued vector ht denote the control state of our model after viewing t labels, and let R(E,St, ht) denote the reward won by our model when predicting labels for the evaluation set based on the information it has received after t label queries. We assume all functions depend on the model parameters θ, and omit this dependence from our notation for brevity.\nWe define the prediction reward as follows:\nR(E,St, ht) ≡ ∑\n(x̂,ŷ)∈E\nlog p(ŷ|x̂, ht, St), (1)\nwhich gives log-likelihood of the predictions p(ŷ|x̂, ht, St) on the evaluation set. The prediction ŷ conditions on: the test item x̂, the current control state ht, and the current labeled/unlabeled support set St. For tests on Omniglot (see Section 4.1), we use negative cross-entropy on the class labels, and for MovieLens (see Section 4.2), we use the negative Root Mean Squared Error (RMSE).\nAt each step t of active learning, the model requests the label for an item x from the set Sut−1 and updates its control state from ht−1 to ht based on the response. Together, ht and St determine the model’s predictions for test items and the model’s decision about which label to request next. Algorithm 1 describes this process in detail, and Section 3.2 formally describes the functions used in Algorithm 1.\nThe idealized objective for training our model is:\nmaximize θ E (S,E)∼D\n[ E\nπ(S,T ) [ T∑ t=1 R(E,St, ht) ]] , (2)\nin which T is the max number of label queries to perform, (S,E) indicates an episode sampled from some distribution D, and π(S, T ) indicates unrolling the model’s active learning policy π for T steps on support set S. Unrolling π produces the intermediate states {(S1, h1), ..., (ST , hT )}.\nTo optimize this objective, our model repeatedly samples an episode (S,E), then unrolls π for T steps of active learning, and maximizes the prediction reward R(E,St, ht) at each step. Alternately, our model could maximize only the reward at the final step. We maximize reward at each step in order to promote anytime behaviour – i.e. the model should perform as well as possible after each label query. Anytime behaviour is desirable in many practical settings, e.g. for movie recommendations the model should be robust to early termination while eliciting the preferences of a new user.\nDuring training, for computational efficiency, our model\nmaximizes the following approximation of Equation 2:\nE (S,E)∼D\n[ E\nπ(S,T ) [ T∑ t=1 R̃(Sut , St, ht) +R(E,ST , hT ) ]] ,\n(3)\nin which R̃(Sut , St, ht) is a prediction reward for unlabeled items in the support set. We assume labels for the full support set are available during training. We compute R̃(Sut , St, ht) using a fast prediction module, and compute R(E,St, ht) using a slow prediction module. The fast and slow prediction rewards can be obtained by substituting the appropriate predictions into Equation 1. Sections 3.2.5 and 3.2.6 describe these modules in detail."
    }, {
      "heading" : "3.2. Model Architecture Details",
      "text" : "Our model comprises multiple modules: context-free and context-sensitive encoding, controller, selection, reading, fast prediction, and slow prediction. We present an overview of our model in Fig. 1 and Alg. 1, which describe how our model’s modules perform active learning. The rest of this subsection describes the individual modules in more detail.\nAlgorithm 1 End-to-end active learning loop (for Eq. 3) 1: # encode items in S with context-sensitive encoder 2: # and encode items in E with context-free encoder 3: S = {(x, y)}, Su0 = {(x, ·)}, Sk0 = ∅, E = {(x̂, ŷ)} 4: for t = 1 . . . T do 5: # select next instance 6: i← SELECT(Sut−1, Skt−1, ht−1) 7: # read labeled instance and update controller 8: (xi, yi)← READ(S, i) 9: ht ← UPDATE(ht−1, xi, yi) 10: # update known / unknown set 11: Skt ← Skt−1 ∪ {(xi, yi)} 12: Sut ← Sut−1 \\ {(xi, ·)} 13: # perform fast prediction (save loss for training) 14: LSt ← FAST-PRED(S, Sut , Skt , ht) 15: end for 16: # perform slow prediction (save loss for training) 17: LET ← SLOW-PRED(E,SuT , SkT , hT )"
    }, {
      "heading" : "3.2.1. CONTEXT-[FREE|SENSITIVE] ENCODING",
      "text" : "The context-free encoder associates each item with an embedding independent of the context in which the item was presented. For our Omniglot tests, this encoder is a convnet with two convolutional layers that downsample via strided convolution, followed by another convolutional layer and a fully-connected linear layer which produces the final context-free embedding. For our MovieLens tests, this encoder is a simple look-up-table mapping movie ids to\nembeddings. We denote the context-free encoding of item xi ∈ S as x′i, and similarly define x̂′i for x̂i ∈ E.\nThe context-sensitive encoder produces an embedding x′′i for each item xi ∈ S based on the context-free embeddings x′j : ∀xj ∈ S. The context-sensitive encoder is not applied to items in the evaluation set. Our model uses a modified form of the encoder from Matching Networks (Vinyals et al., 2016). Specifically, we run a bidirectional LSTM (Hochreiter & Schmidhuber, 1997; Schuster & Paliwal, 1997) over all context-free embeddings for the support set, and then add a linear function of the concatenated forwards and backwards states to the context-free embedding x′i to get x ′′ i .\nWe can write this as follows:\nx′′i = x ′ i +We [ ~hi; ~hi ] , (4)\nin which ~hi gives the forward encoder state for item xi, ~hi gives the backward encoder state, and We is a trainable matrix. We compute the forward states ~hi as in a standard LSTM, processing the support set items xi sequentially following a random order. We compute the backward states ~hi by processing the sequence of concatenated x′i and ~hi vectors in reverse."
    }, {
      "heading" : "3.2.2. READING",
      "text" : "This module concatenates the embedding x′′i and label yi for the item indicated by the selection module, and linearly transforms them before passing them to the controller (Alg. 1, line 8)."
    }, {
      "heading" : "3.2.3. CONTROLLER",
      "text" : "At each step t, the controller receives an input rt from the reading module which encodes the most recently read item/label pair. Additional inputs could take advantage of task-specific information. The control module performs an LSTM update:\nht = LSTM(ht−1, rt).\nWe initialize h0 for each episode (S,E) using the final state of the backwards LSTM in the context-sensitive encoder which processed the support set S. In principal, this allows the controller to condition its behaviour on the full unlabeled contents of the support set (Alg. 1, line 9)."
    }, {
      "heading" : "3.2.4. SELECTION",
      "text" : "At each step t, the selection module places a distribution Put over all unlabeled items x u i ∈ Sut . It then samples the index of an item to label from Put , and feeds it to the reading module (Alg. 1, line 6).\nOur model computes Put using a gated, linear combination of features which measure controller-item similarity and item-item similarity. For each item, we compute the controller-item similarity features:\nbit = x ′′ i Wbht,\nwhere Wb is a trainable matrix and indicates elementwise multiplication. We also compute the following six itemitem similarity features: [max|mean|min] cosine similarity to any labeled item, and [max|mean|min] cosine similarity to any unlabeled item. We concatenate the controller-item\nsimilarity features and item-item similarity features to get a vector dit. We also compute a gating vector: gt = σ(Wght), in which Wg is a trainable matrix and σ(·) indicates the standard sigmoid.\nFor each xui ∈ Sut , we compute the selection logit:\npit = (gt dit)>wp,\nwhere wp indicates a trainable vector. Finally, we compute Put by normalizing over the logits p i t : ∀xui ∈ Sut via softmax. This module performs worse when the controller-item or item-item features are removed. Our model intelligently adapts these heuristics to the task at hand. We provide pseudo-code for how these modules interact during active learning in Algorithm 1."
    }, {
      "heading" : "3.2.5. FAST PREDICTION",
      "text" : "The fast prediction module makes an attention-based prediction for each unlabeled item xui ∈ Sut using its cosine similarities to the labeled items xkj ∈ Skt , which are sharpened by a non-negative matching score γit between x u i and the control state ht. The cosine similarities are taken between the context-sensitive embeddings x′′i and x ′′ j of the respective items. These do not change with t and may be precomputed before unrolling the active learning policy. Predictions from this module are thus fast to compute while unrolling the policy (Alg. 1, line 14). The cosine similarities may be reused in the selection module for computing item-item similarity features, further amortizing their cost.\nFor each unlabeled xui , we compute a set of attention weights over the labeled xkj ∈ Skt by applying a softmax to the relevant cosine similarities, using γit as a temperature for the softmax. We compute the sharpening term as follows:\nγit = exp((x ′′ i ) >Wγht),\nwhere Wγ indicates a trainable matrix. This module performs significantly worse without the sharpening term. The final fast prediction is formed by taking a convex combination of the labels yj for the labeled xkj ∈ Skt using the computed attention weights."
    }, {
      "heading" : "3.2.6. SLOW PREDICTION",
      "text" : "The slow prediction module implements a modified Matching Network prediction, which accounts for the distinction between labeled and unlabeled items in St, and conditions on the active learning control state ht (Alg. 1, line 17).\nGiven the context-free embedding x̂′ for some held-out example x̂ ∈ E, the state ht, and required initial values, this module predicts a label by iterating the steps:\n1. mk = LSTM(mk−1, x̃k−1, x̂′, ht)\n2. x̂′′ = x̂′ +Wmmk 3. ãk = attend(x̂′′, Skt ) 4. x̃k, ỹk = attRead(ãk, Skt )\nHere, LSTM is an LSTM update, mk is the matching state at step k, x̂′′ is the match-sensitive embedding of x̂ at step k, Wm is a trainable matrix, ãk are the matching attention weights at step k, x̃k is the “item attention” result from step k, and ỹk is the “label attention” result from step k. For details of the attend and attRead functions, refer to Vinyals et al. (2016). As a final prediction, this module returns the label attention result ỹK from the Kth (final) step of iterative matching. In our tests we fix K = 3.\nNote: our model contains many linear transforms Wv. Our model adds bias and gain terms to all of these transforms, as described for weight normalization (Salimans & Kingma, 2016). We omit these terms for brevity. Similarly, we use layer normalization in our active learning and matching controller LSTMs (Ba et al., 2016)."
    }, {
      "heading" : "3.3. Training the Model",
      "text" : "We optimize the parameters of our model using a combination of backpropagation and policy gradients. For a clear review of optimization techniques for general stochastic computation graphs, see Schulman et al. (2015).\nUsing the notation from Section 3.1 and following the approach of (Schulman et al., 2015), we can write the gradient of our training objective as follows:\n∇θR(S,E, θ) = (5)\nE p(~S|(S,E))\n( ∇θ log p(~S|(S,E)) [ R(~S) ] +∇θR(~S) ) ,\nin which R(S,E, θ) denotes the expected reward won by the active learning model while working on episode (S,E). ~S denotes the set of intermediate states {(St, ht)} generated by the model while working with (S,E). R(~S) denotes the sum of rewards (as described in Equation 3) received by the model while working on episode (S,E). In the term ∇θR(~S), all decisions made by the model to produce ~S are treated as constant. Taking the expectation of Equation 5 with respect to episodes (S,E) ∼ D gives us an unbiased gradient estimator for the objective in Equation 3.\nRather than using the gradients in Equation 5 directly, we optimize the model parameters using Generalized Advantage Estimation (Schulman et al., 2016), which provides an actor-critic approach for approximately optimizing the policy gradients in Equation 5. For more details on how Generalized Advantage Estimation helps reach a favourable bias-variance trade-off in policy gradient estimation, see the source paper (Schulman et al., 2016). We apply the GAE updates using ADAM (Kingma & Ba, 2015)."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Omniglot",
      "text" : "We run our first experiments on the Omniglot dataset (Lake et al., 2015) consisting of 1623 characters from 50 different alphabets, each hand-written by 20 different people. Following Vinyals et al. (2016), we divide the dataset into 1200 characters for training and keep the rest for testing. When measuring test performance, our model interacts with characters it did not encounter during training.\nFor the context-free embedding function we use a threelayer convolutional network. The first two layers use 5× 5 convolutions with 64 filters and downsample with a double stride. The third layer uses a 3 × 3 convolution with 64\nfilters and no downsampling. These layers produce a 7 × 7× 64 feature map that we flatten and pass through a fully connected layer. All convolutional layers use the leaky ReLU nonlinearity (Maas et al., 2013).\nWe setup N -way, K-shot Omniglot classification as follows. We randomly pick N character classes from the available train/test classes. Then, we build a support set by randomly sampling 5 items for each character class, e.g. in the 5-way setting, there are 25 items in the support set. The held-out set is always obtained by randomly sampling 1 item per class. In our active learning setting, K-shot is proportional to how many labels the model can acquire. In the N -way, K-shot setting, the model asks for NK labels before performing held-out prediction. For example, in 5-way, 1-shot classification, the model asks for 5 labels. Following each label query, we also measure anytime performance of the fast prediction module on the items remaining in Sut . Note that the 1-shot setting is particularly challenging for our model, as it needs to ask for different classes at each step, and the ability to identify missing classes is limited by the accuracy of the underlying one-shot classifier.\nWe compare our active learner to four baselines. To compute a pessimistic estimate of potential performance, we use a matching network where we label NK items chosen at random from the full support set (Matching Net (random)). As the labels are randomly sampled, it is possible that a given class is never represented among the labeled items and the model cannot classify perfectly, even in principle. To compute an optimistic estimate of potential performance, we measure the “ideal” matching network accuracy by labeling a class-balanced subset of items from the full support set (Matching Net (balanced)). This baseline represents a highly-performant policy that the active learner can, in principle, learn. For the last baseline (Min-Max-Cos), we formulate a heuristic policy. At each active learning step, we select the item which has minimum maximum cosine similarity to unlabeled items in the support set. This heuristic selects item that are different from each other, a strategy well-suited to the Omniglot classification task where items are drawn from a consistent set of underlying classes.\nWe report the results in Table 1. Matching Networks operating on a randomly sampled set of labels suffer the most in 1-shot scenarios, where the probability of all classes being represented is particularly low (especially in the 10- way case). Overall, the active policy nearly matches the performance of the optimistic balanced Matching Network baseline. Degradation of performance by 2.2% is observed for the 1-shot, 10-way case. This is not surprising since augmenting the number of classes in the support set, while keeping the number of shots fixed, considerably increases the difficulty of the problem for the active learner. Figure 2 shows a roll-out of the model policy in the 10-way setting.\nFigure 3 provides results for the more challenging setting of 20-way classification. We tested two properties of our model: its anytime performance beyond the 1-shot setting, and its ability to generalize to problems with more classes than were seen during training. The model performed well on 20-way classification, and quickly approached the optimistic performance estimate after acquiring more labels. We also found that policies trained for as little as 10-way classification could generalize to the 20-way setting.\nOur model relies on a number of moving parts. When designing the architecture, we followed the simple approach of minimizing changes to the original Matching Network from Vinyals et al. (2016). We now provide ablation test results for several parts of our model. In the 10-way, 1-shot setting accuracy dropped from 94.5 to 86.0 when we removed attention temperature from the fast prediction module. Reducing the number of matching steps from 3 to 2 or 1 had no significant effect in this setting. Removing the context-sensitive encoder also had no significant effect. Streamlining our architecture is clearly a useful topic for future work aimed at scaling our approach to more realistic settings."
    }, {
      "heading" : "4.2. MovieLens",
      "text" : ""
    }, {
      "heading" : "4.2.1. SETUP",
      "text" : "We test our model in the “cold-start” collaborative filtering scenario using the publicly available MovieLens-20M dataset.1 The dataset contains approximately 20M ratings on 27K movies by 138K users. The ratings are on an ordinal 10-point scale, from 0.5 to 5 with intervals of 0.5. We subsample the dataset by selecting 4000 movies and 6000 users with the most ratings. After filtering, the dataset contains approximately 1M ratings. We partition the data randomly into 5000 training users and 1000 test users. The training set represents the users already in the system who are used to fit the model parameters. We use the test users to evaluate our active learning approach. For each user, we randomly pick 50 ratings to include in the support set (movies that the user can be queried about) and 10 movies and ratings for the heldout set. We ensure that movies in the held-out set and in the\n1Available at http://grouplens.org/datasets/ movielens/\nsupport set do not overlap. We train our active learner to minimize the mean-squared error (MSE) with respect to the true rating. We adapt the prediction modules of our model to output the rating for a held-out item as follows: we compute a convex combination of the ratings of “visible” movies in the support set (the movies the active learner has already queried about), where the weights are given by the final attention step of the slow predictor. Although more complex strategies are possible, we empirically found this simple strategy to work well in our experiments. For evaluation, we sample 25000 episodes (each comprising 50 support ratings and 10 held-out ratings from some user) from the test set and we compute the average per-user root mean-squared error (RMSE). We report the average performance obtained by 3 runs with different random seeds."
    }, {
      "heading" : "4.2.2. MOVIE EMBEDDINGS",
      "text" : "For each movie, we pretrain an embedding vector by decomposing the full user/movie rating matrix using a latent factor model (Koren, 2010). This process only uses the training set. For each user u and movie m, we estimate the true rating ru,m with a linear model r̂u,m = x>u xm + bu + bm + β, where xu, xm are the user and movie embedding respectively and bu, bm, β are the user, movie, and global bias, respectively. We train the latent factor model by minimizing the mean squared error between true rating r and predicted rating r̂. We use the trained xm as input representations for the movies throughout our experiments."
    }, {
      "heading" : "4.2.3. RESULTS",
      "text" : "In Figure 4 we report the results of our active model against various baselines. The Regression baseline performs regularized linear regression on movies from the support set whose ratings have been observed incrementally in random order. Because of the small amount of training data, for each additional label we tune the regularization parameter by monitoring performance on a separate set of validation episodes. The Gaussian Process baseline selects the next movie to label in proportion to the variance of the predictive posterior distribution over its rating. This gives an idea of the impact of using MN one-shot capabilities rather than standard regression techniques. The Popular-Entropy, Min-\nMax-Cos, and Entropy Sampling baselines train our model end-to-end, but using fixed selection policies. Specifically, we train our architecture end-to-end, but instead of training an active learning policy through the select module we choose items from the support set incrementally according to a heuristic policy. This gives an idea of the importance of learning the selection policy. The Popular-Entropy policy, adapted from the cold-start work of Rashid et al. (2002), scores each item in the support set a priori, according to the logarithm of its popularity multiplied by the entropy of the item’s ratings measured across users. This strategy aims to first collect the ratings for those movies that are both popular and have been rated differently by different users. Although it is simplistic, the policy achieves competitive performance for bootstrapping a system from a cold-start setting (Elahi et al., 2016). The Min-Max-Cos policy is identical to the synonymous baseline used for Omniglot, i.e. it selects the unrated movie which has minimum maximum cosine similarity to any of the rated movies. Roughly, this selects the unrated movie which differs most from the rated movies. Entropy Sampling selects movies in proportion to\nrating prediction entropy.\nThe active policy learned end-to-end outperforms the baselines in terms of RMSE, particularly after requesting only the first few labels. After 10 ratings, our model achieves an improvement of 2.5% in RMSE against the best baseline. Unsurprisingly, the gap diminishes with a higher number of labels requested. After observing 5 labels, the PopularEntropy baseline and our architecture equipped with the Min-Max-Cos heuristic converge toward the active policy but never quite match it. For MovieLens, where labels are user-dependent and not tied to an underlying class, a datadriven selection policy may adapt better to the task. This contrasts with the Omniglot setting, where there is no aspect of personalization and Active MN and Min-Max-Cos perform similarly. The Min-Max-Cos heuristic is designed to not select items similar to those it has already seen, but selecting similar items can be beneficial in personalized settings (Elahi et al., 2016)."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We introduced a model that learns active learning algorithms end-to-end. Our goal was to move away from engineered selection heuristics towards strategies learned directly from data. Our model leverages labeled instances from different but related tasks to learn a selection strategy for the task at hand, while simultaneously adapting its representation of the data and its prediction function. We evaluated the model on “active” variants of one-shot learning tasks for Omniglot, demonstrating that its policy approaches an optimistic performance estimate. On a cold-start collaborative filtering task derived from MovieLens, the model outperforms several baselines and shows promise for application in more realistic settings."
    } ],
    "references" : [ {
      "title" : "Learning to search better than your teacher",
      "author" : [ "Chang", "Kai-Wei", "Krishnamurthy", "Akshay", "Agarwal", "III Alekh", "Hal Daumé", "Langford", "John" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Chang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2015
    }, {
      "title" : "Active learning with statistical models",
      "author" : [ "Cohn", "David A", "Ghahramani", "Zoubin", "Jordan", "Michael I" ],
      "venue" : "Journal of artificial intelligence research,",
      "citeRegEx" : "Cohn et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Cohn et al\\.",
      "year" : 1996
    }, {
      "title" : "A survey of active learning in collaborative filtering recommender systems",
      "author" : [ "Elahi", "Mehdi", "Ricci", "Francesco", "Rubens", "Neil" ],
      "venue" : "Computer Science Review,",
      "citeRegEx" : "Elahi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Elahi et al\\.",
      "year" : 2016
    }, {
      "title" : "Query by committee made real",
      "author" : [ "Gilad-Bachrach", "Ran", "Navot", "Amir", "Tishby", "Naftali" ],
      "venue" : "In Proceedings of the 18th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Gilad.Bachrach et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gilad.Bachrach et al\\.",
      "year" : 2005
    }, {
      "title" : "On bootstrapping recommender systems",
      "author" : [ "Golbandi", "Nadav", "Koren", "Yehuda", "Lempel", "Ronny" ],
      "venue" : "In Proceedings of the 19th ACM International Conference on Information and Knowledge Management, CIKM",
      "citeRegEx" : "Golbandi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Golbandi et al\\.",
      "year" : 2010
    }, {
      "title" : "Adaptive bootstrapping of recommender systems using decision trees",
      "author" : [ "Golbandi", "Nadav", "Koren", "Yehuda", "Lempel", "Ronny" ],
      "venue" : "In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM",
      "citeRegEx" : "Golbandi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Golbandi et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep speech: Scaling up end-to-end speech recognition",
      "author" : [ "Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam", "Ng", "Andrew Y" ],
      "venue" : "arXiv preprint arXiv:1412.5567v2,",
      "citeRegEx" : "Hannun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hannun et al\\.",
      "year" : 2014
    }, {
      "title" : "Personalized active learning for collaborative filtering",
      "author" : [ "Harpale", "Abhay S", "Yang", "Yiming" ],
      "venue" : "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "Harpale et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Harpale et al\\.",
      "year" : 2008
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Batch mode active learning and its application to medical image classification",
      "author" : [ "Hoi", "Steven CH", "Jin", "Rong", "Zhu", "Jianke", "Lyu", "Michael R" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Hoi et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hoi et al\\.",
      "year" : 2006
    }, {
      "title" : "Bayesian active learning for classification and preference learning",
      "author" : [ "Houlsby", "Neil", "Huszár", "Ferenc", "Ghahramani", "Zoubin", "Lengyel", "Máté" ],
      "venue" : "arXiv preprint arXiv:1112.5745,",
      "citeRegEx" : "Houlsby et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2011
    }, {
      "title" : "Coldstart active learning with robust ordinal matrix factorization",
      "author" : [ "Houlsby", "Neil", "Hernandez", "Jose", "Ghahramani", "Zoubin" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Houlsby et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient thompson sampling for online matrix-factorization recommendation",
      "author" : [ "Kawale", "Jaya", "Bui", "Hung H", "Kveton", "Branislav", "Tran-Thanh", "Long", "Chawla", "Sanjay" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Kawale et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kawale et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik P", "Ba", "Jimmy" ],
      "venue" : "[cs.LG],",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Siamese neural networks for one-shot image recognition",
      "author" : [ "Koch", "Gregory" ],
      "venue" : "PhD thesis, University of Toronto,",
      "citeRegEx" : "Koch and Gregory.,? \\Q2015\\E",
      "shortCiteRegEx" : "Koch and Gregory.",
      "year" : 2015
    }, {
      "title" : "Factor in the neighbors: Scalable and accurate collaborative filtering",
      "author" : [ "Koren", "Yehuda" ],
      "venue" : "ACM Transactions on Knowledge Discovery from Data (TKDD),",
      "citeRegEx" : "Koren and Yehuda.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koren and Yehuda.",
      "year" : 2010
    }, {
      "title" : "Imagenet classification with deep neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Human-level concept learning through probabilistic program induction",
      "author" : [ "Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B" ],
      "venue" : null,
      "citeRegEx" : "Lake et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2015
    }, {
      "title" : "A sequential algorithm for training text classifiers",
      "author" : [ "Lewis", "David D", "Gale", "William A" ],
      "venue" : "In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "Lewis et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 1994
    }, {
      "title" : "Facing the cold start problem in recommender systems",
      "author" : [ "Lika", "Blerina", "Kolomvatsos", "Kostas", "Hadjiefthymiades", "Stathes" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "Lika et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lika et al\\.",
      "year" : 2014
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Maas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2013
    }, {
      "title" : "Getting to know you: learning new user preferences in recommender systems",
      "author" : [ "Rashid", "Al Mamunur", "Albert", "Istvan", "Cosley", "Dan", "Lam", "Shyong K", "McNee", "Sean M", "Konstan", "Joseph A", "Riedl", "John" ],
      "venue" : "In Proceedings of the 7th international conference on Intelligent user interfaces,",
      "citeRegEx" : "Rashid et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Rashid et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning preferences of new users in recommender systems: an information theoretic approach",
      "author" : [ "Rashid", "Al Mamunur", "Karypis", "George", "Riedl", "John" ],
      "venue" : "ACM SIGKDD Explorations Newsletter,",
      "citeRegEx" : "Rashid et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rashid et al\\.",
      "year" : 2008
    }, {
      "title" : "Reinforcement and imitation learning via interactive no-regret learning",
      "author" : [ "Ross", "Stéphane", "Bagnell", "J. Andrew" ],
      "venue" : "[cs.LG],",
      "citeRegEx" : "Ross et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2014
    }, {
      "title" : "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
      "author" : [ "Salimans", "Tim", "Kingma", "Diederik P" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "One-shot learning with memoryaugmented neural networks",
      "author" : [ "Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy" ],
      "venue" : "arXiv preprint arXiv:1605.06065,",
      "citeRegEx" : "Santoro et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning with kernels: support vector machines, regularization, optimization, and beyond",
      "author" : [ "Schölkopf", "Bernhard", "Smola", "Alexander J" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 2002
    }, {
      "title" : "Gradient estimation using stochastic computation graphs",
      "author" : [ "Schulman", "John", "Heess", "Nicolas", "Weber", "Theophane", "Abbeel", "Pieter" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "High-dimensional continuous control using generalized advantage estimation",
      "author" : [ "Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael I", "Abbeel", "Pieter" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Schulman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2016
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "M. Schuster", "K.K. Paliwal" ],
      "venue" : "Transactions on Signal Processing,",
      "citeRegEx" : "Schuster and Paliwal,? \\Q1997\\E",
      "shortCiteRegEx" : "Schuster and Paliwal",
      "year" : 1997
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "Settles", "Burr" ],
      "venue" : "University of Wisconsin, Madison,",
      "citeRegEx" : "Settles and Burr.,? \\Q2010\\E",
      "shortCiteRegEx" : "Settles and Burr.",
      "year" : 2010
    }, {
      "title" : "Learning multiple-question decision trees for cold-start recommendation",
      "author" : [ "Sun", "Mingxuan", "Li", "Fuxin", "Lee", "Joonseok", "Zhou", "Ke", "Lebanon", "Guy", "Zha", "Hongyuan" ],
      "venue" : "In Proceedings of the Sixth ACM International Conference on Web Search and Data Mining, WSDM ’13,",
      "citeRegEx" : "Sun et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning multiple-question decision trees for cold-start recommendation",
      "author" : [ "Sun", "Mingxuan", "Li", "Fuxin", "Lee", "Joonseok", "Zhou", "Ke", "Lebanon", "Guy", "Zha", "Hongyuan" ],
      "venue" : "In Proceedings of the sixth ACM international conference on Web search and data mining,",
      "citeRegEx" : "Sun et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2013
    }, {
      "title" : "Deeply aggrevated: Differentiable imitation learning for sequential prediction",
      "author" : [ "Sun", "Wen", "Venkatraman", "Arun", "Gordon", "Geoffrey J", "Boots", "Byron", "Bagnell", "J Andrew" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Sun et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2017
    }, {
      "title" : "Support vector machine active learning for image retrieval",
      "author" : [ "Tong", "Simon", "Chang", "Edward" ],
      "venue" : "In Proceedings of the ninth ACM international conference on Multimedia,",
      "citeRegEx" : "Tong et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Tong et al\\.",
      "year" : 2001
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Active one-shot learning",
      "author" : [ "Woodward", "Mark", "Finn", "Chelsea" ],
      "venue" : "In NIPS Workshop,",
      "citeRegEx" : "Woodward et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Woodward et al\\.",
      "year" : 2016
    }, {
      "title" : "Query-efficient imitation learning for end-to-end autonomous driving",
      "author" : [ "Zhang", "Jiakai", "Cho", "Kyunghyun" ],
      "venue" : "American Association for Artificial Intelligence (AAAI),",
      "citeRegEx" : "Zhang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Using anytime algorithms in intelligent systems",
      "author" : [ "Zilberstein", "Shlomo" ],
      "venue" : "AI magazine,",
      "citeRegEx" : "Zilberstein and Shlomo.,? \\Q1996\\E",
      "shortCiteRegEx" : "Zilberstein and Shlomo.",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Active learning is motivated by the observation that a model may perform better while training on less labeled data if it can choose the data on which it trains (Cohn et al., 1996).",
      "startOffset" : 161,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "For example, preference information for a new user in a movie recommender system may be scarce, and recommendations for the new user could be improved by carefully selecting several movies for her to rate (Sun et al., 2013b; Houlsby et al., 2014; Aggarwal, 2016).",
      "startOffset" : 205,
      "endOffset" : 262
    }, {
      "referenceID" : 10,
      "context" : "Likewise, collecting labels for a medical imaging task may be costly because it requires a specialist (Hoi et al., 2006), and the cost could be reduced by carefully selecting which images to label.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "proposed in the active learning literature, such as choosing the instance whose label the model is most uncertain about, or the instance whose label is expected to maximally reduce the model’s uncertainty about other instances (GiladBachrach et al., 2005; Settles, 2010; Houlsby et al., 2011).",
      "startOffset" : 227,
      "endOffset" : 292
    }, {
      "referenceID" : 4,
      "context" : "In recommendation systems, for example, ratings data for existing users can inform a strategy that efficiently elicits preferences for new users who lack prior rating data, thus bootstrapping the system quickly out of the cold-start setting (Golbandi et al., 2010; 2011; Sun et al., 2013a; Kawale et al., 2015).",
      "startOffset" : 241,
      "endOffset" : 310
    }, {
      "referenceID" : 13,
      "context" : "In recommendation systems, for example, ratings data for existing users can inform a strategy that efficiently elicits preferences for new users who lack prior rating data, thus bootstrapping the system quickly out of the cold-start setting (Golbandi et al., 2010; 2011; Sun et al., 2013a; Kawale et al., 2015).",
      "startOffset" : 241,
      "endOffset" : 310
    }, {
      "referenceID" : 17,
      "context" : ", computer vision, speech recognition, and machine translation (Krizhevsky et al., 2012; Hannun et al., 2014; He et al., 2016; Wu et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : ", computer vision, speech recognition, and machine translation (Krizhevsky et al., 2012; Hannun et al., 2014; He et al., 2016; Wu et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : ", computer vision, speech recognition, and machine translation (Krizhevsky et al., 2012; Hannun et al., 2014; He et al., 2016; Wu et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 36,
      "context" : "We base our model on the Matching Networks (MN) introduced by Vinyals et al. (2016). We extend the MN’s one-shot learning ability to settings where labels are not available a priori.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "We evaluate the model on “active” variants of existing oneshot learning tasks for Omniglot (Lake et al., 2015; Vinyals et al., 2016; Santoro et al., 2016), and show that it can learn efficient label querying strategies.",
      "startOffset" : 91,
      "endOffset" : 154
    }, {
      "referenceID" : 36,
      "context" : "We evaluate the model on “active” variants of existing oneshot learning tasks for Omniglot (Lake et al., 2015; Vinyals et al., 2016; Santoro et al., 2016), and show that it can learn efficient label querying strategies.",
      "startOffset" : 91,
      "endOffset" : 154
    }, {
      "referenceID" : 26,
      "context" : "We evaluate the model on “active” variants of existing oneshot learning tasks for Omniglot (Lake et al., 2015; Vinyals et al., 2016; Santoro et al., 2016), and show that it can learn efficient label querying strategies.",
      "startOffset" : 91,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "For instance, Lewis & Gale (1994) and Tong & Chang (2001) developed policies based on the confidence of the classifier, while Gilad-Bachrach et al. (2005) used the disagreement of a committee of classifiers.",
      "startOffset" : 126,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "For instance, Lewis & Gale (1994) and Tong & Chang (2001) developed policies based on the confidence of the classifier, while Gilad-Bachrach et al. (2005) used the disagreement of a committee of classifiers. Houlsby et al. (2011) presented an approach based on Bayesian information theory, in which examples are selected in order to maximally reduce the entropy of the posterior distribution over classifier parameters.",
      "startOffset" : 126,
      "endOffset" : 230
    }, {
      "referenceID" : 26,
      "context" : "Building on the memory-augmented neural network (MANN) (Santoro et al., 2016), the authors developed a stream-based active learner.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "in medical imaging (Hoi et al., 2006)).",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 26,
      "context" : "learning the appearance of a class from just a single example of that class (Santoro et al., 2016; Koch, 2015).",
      "startOffset" : 76,
      "endOffset" : 110
    }, {
      "referenceID" : 34,
      "context" : "Our model builds on the matching-networks (MN) architecture presented by Vinyals et al. (2016), which enables “one-shot” learning, i.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 25,
      "context" : "learning the appearance of a class from just a single example of that class (Santoro et al., 2016; Koch, 2015). Vinyals et al. (2016) assume that at least one example per class exists in the labeled support set available to the model.",
      "startOffset" : 77,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : "learning the appearance of a class from just a single example of that class (Santoro et al., 2016; Koch, 2015). Vinyals et al. (2016) assume that at least one example per class exists in the labeled support set available to the model. Confronted with the harder task of composing a labeled support set from a larger pool of unlabeled examples, we show that the active learning policy learnt by our model obtains, in some cases, an equally effective support set. As in the recent one-shot learning work of Santoro et al. (2016) and Vinyals et al.",
      "startOffset" : 77,
      "endOffset" : 527
    }, {
      "referenceID" : 25,
      "context" : "learning the appearance of a class from just a single example of that class (Santoro et al., 2016; Koch, 2015). Vinyals et al. (2016) assume that at least one example per class exists in the labeled support set available to the model. Confronted with the harder task of composing a labeled support set from a larger pool of unlabeled examples, we show that the active learning policy learnt by our model obtains, in some cases, an equally effective support set. As in the recent one-shot learning work of Santoro et al. (2016) and Vinyals et al. (2016), and the active learning work of Woodward & Finn (2016), we evaluate our model on the Omniglot dataset.",
      "startOffset" : 77,
      "endOffset" : 553
    }, {
      "referenceID" : 25,
      "context" : "learning the appearance of a class from just a single example of that class (Santoro et al., 2016; Koch, 2015). Vinyals et al. (2016) assume that at least one example per class exists in the labeled support set available to the model. Confronted with the harder task of composing a labeled support set from a larger pool of unlabeled examples, we show that the active learning policy learnt by our model obtains, in some cases, an equally effective support set. As in the recent one-shot learning work of Santoro et al. (2016) and Vinyals et al. (2016), and the active learning work of Woodward & Finn (2016), we evaluate our model on the Omniglot dataset.",
      "startOffset" : 77,
      "endOffset" : 609
    }, {
      "referenceID" : 18,
      "context" : "This dataset was developed for the foundational one-shot learning work of Lake et al. (2015), which focused on probabilistic program induction.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "The cold-start problem is ubiquitous in recommendation systems (Aggarwal, 2016; Lika et al., 2014; Harpale & Yang, 2008; Sun et al., 2013b; Elahi et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "The cold-start problem is ubiquitous in recommendation systems (Aggarwal, 2016; Lika et al., 2014; Harpale & Yang, 2008; Sun et al., 2013b; Elahi et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : "In model-free strategies (Rashid et al., 2008), items are selected according to general empirical statistics such as popularity or informativeness.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "Proposals for learning an adaptive selection strategy have been made in the form of Bayesian methods that learn the parameters of a user model (Houlsby et al., 2014; Harpale & Yang, 2008), and in the form of decision-trees learned from existing ratings (Sun et al.",
      "startOffset" : 143,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : ", 2013b; Elahi et al., 2016). Instead of bootstrapping from a cold-start by randomly selecting items for a user to rate, an active learner asks for particular items to help learn a strong user model more quickly. In model-free strategies (Rashid et al., 2008), items are selected according to general empirical statistics such as popularity or informativeness. These approaches are computationally cheap, but lack the benefits of adaptation and personalization. Proposals for learning an adaptive selection strategy have been made in the form of Bayesian methods that learn the parameters of a user model (Houlsby et al., 2014; Harpale & Yang, 2008), and in the form of decision-trees learned from existing ratings (Sun et al., 2013b). An extensive review can be found in Elahi et al. (2016). Intuitively, our model learns a compact, parametric representation of a decision tree end-toend, by directly maximizing task performance.",
      "startOffset" : 9,
      "endOffset" : 792
    }, {
      "referenceID" : 0,
      "context" : "Related approaches have been explored in previous work on imitation learning and learning to search (Ross & Bagnell, 2014; Chang et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 142
    }, {
      "referenceID" : 34,
      "context" : "These methods, which focus the cost of sampling from the oracle policy on states visited by the model policy, have recently been adopted by researchers working with deep networks for representation learning (Zhang & Cho, 2017; Sun et al., 2017).",
      "startOffset" : 207,
      "endOffset" : 244
    }, {
      "referenceID" : 36,
      "context" : "Succinctly, our model solves each task by adaptively selecting items for the labeled support set used by a Matching Network (Vinyals et al., 2016) to classify test items.",
      "startOffset" : 124,
      "endOffset" : 146
    }, {
      "referenceID" : 36,
      "context" : "Our model uses a modified form of the encoder from Matching Networks (Vinyals et al., 2016).",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 36,
      "context" : "For details of the attend and attRead functions, refer to Vinyals et al. (2016). As a final prediction, this module returns the label attention result ỹK from the Kth (final) step of iterative matching.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 28,
      "context" : "For a clear review of optimization techniques for general stochastic computation graphs, see Schulman et al. (2015).",
      "startOffset" : 93,
      "endOffset" : 116
    }, {
      "referenceID" : 28,
      "context" : "1 and following the approach of (Schulman et al., 2015), we can write the gradient of our training objective as follows:",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 29,
      "context" : "Rather than using the gradients in Equation 5 directly, we optimize the model parameters using Generalized Advantage Estimation (Schulman et al., 2016), which provides an actor-critic approach for approximately optimizing the policy gradients in Equation 5.",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 29,
      "context" : "For more details on how Generalized Advantage Estimation helps reach a favourable bias-variance trade-off in policy gradient estimation, see the source paper (Schulman et al., 2016).",
      "startOffset" : 158,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "We run our first experiments on the Omniglot dataset (Lake et al., 2015) consisting of 1623 characters from 50 different alphabets, each hand-written by 20 different people.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "We run our first experiments on the Omniglot dataset (Lake et al., 2015) consisting of 1623 characters from 50 different alphabets, each hand-written by 20 different people. Following Vinyals et al. (2016), we divide the dataset into 1200 characters for training and keep the rest for testing.",
      "startOffset" : 54,
      "endOffset" : 206
    }, {
      "referenceID" : 21,
      "context" : "All convolutional layers use the leaky ReLU nonlinearity (Maas et al., 2013).",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 36,
      "context" : "When designing the architecture, we followed the simple approach of minimizing changes to the original Matching Network from Vinyals et al. (2016). We now provide ablation test results for several parts of our model.",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "Although it is simplistic, the policy achieves competitive performance for bootstrapping a system from a cold-start setting (Elahi et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 21,
      "context" : "The Popular-Entropy policy, adapted from the cold-start work of Rashid et al. (2002), scores each item in the support set a priori, according to the logarithm of its popularity multiplied by the entropy of the item’s ratings measured across users.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "The Min-Max-Cos heuristic is designed to not select items similar to those it has already seen, but selecting similar items can be beneficial in personalized settings (Elahi et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 187
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.",
    "creator" : "LaTeX with hyperref package"
  }
}