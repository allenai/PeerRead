{
  "name" : "1705.11105.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Zhenzhou Wu", "Sean Saito" ],
    "emails" : [ "hyciswu@gmail.com", "sean.saito@u.yale-nus.edu.sg" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Large hierarchical classification with more than 10000 categories is a challenging task (Partalas et al. (2015)). Traditionally, hierarchical classification can be done by training a classifier on the flattened labels (Babbar et al. (2013)) or by training a classifier at each hierarchical node (Silla Jr & Freitas (2011)), whereby each hierarchical node is a decision maker of which subsequent node to route to. However, the second method scales inefficiently with the number of categories (> 10000 categories). Models such as hierarchical-SVM (Vural & Dy (2004)) becomes difficult to train when there are 10000 SVMs in the entire hierarchy. Therefore for large number of categories, the hierarchy tree is flattened to produce single labels. While training becomes easier, the data then loses prior information about the labels and their structural relationships.\nIn this paper, we model the large hierarchical labels with layers of neurons directly. Unlike the traditional structural modeling with classifier at each node, here we represent each label in the hierarchy simply as a neuron."
    }, {
      "heading" : "2 MODEL",
      "text" : "HiNet has different procedures for training and inference. During training, as illustrated in Figure 2, the model is forced to learn MAP (Maximum a Posteriori) hypothesis over predictions at different hierarchical levels independently. Since the hierarchical layers contain shared information as child node is conditioned on the parent node, we employ a combined cost function over errors across different levels. A combined cost allows travelling of information across levels which is equivalent to transfer learning between levels.\nDuring inference, after predicting the posterior distribution at each level of the hierarchy, we employ a greedy downpour algorithm to efficiently infer the MAP (Maximum a Posteriori) hierarchical trace (Figure 4) from the posterior predictions at each level (Figure 2)."
    }, {
      "heading" : "2.1 HIERARCHY AS LAYERS OF NEURONS",
      "text" : "Using layers of neurons to model hierarchy is very efficient and flexible. It can be easily used to model a Directed Acyclic Graph (DAG) (Figure 1a) or a Tree (Figure 1b) by masking out the unnecessary connections. Unlike node-based architecture (Silla Jr & Freitas (2011); Vens et al.\nar X\niv :1\n70 5.\n11 10\n5v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\n01 7\n(2008); Dumais & Chen (2000)), whereby each node is an object with pointers to its child and parent, and takes up large memory, neural network models the connections as compact matrix which takes up much less memory. In order to model hierarchies of different length, we append a stop neuron (red neuron in Figure 1) at each layer. So a top-down path will end when it reaches the stop neuron."
    }, {
      "heading" : "2.2 TRAINING",
      "text" : "Figure 2 shows the model for transfer learning with combined cost function. Given an input feature X with multiple levels of outputs {y(1),y(2), . . . ,y(n)}, where the outputs may have inter-level dependencies p(y(k)|y(1:k−1),y(k+1:n)). For each output level from network fθk(X) = y(k) and\nits corresponding label ỹ(k). The combined cost is defined as E = ∑n k ( ỹ(k) − fθk(X) )2 allows the parameters θk from different levels to exchange knowledge."
    }, {
      "heading" : "2.3 INFERENCE",
      "text" : "Downpour Algorithm During inference, as illustrated in Figure 4, the model will output a normalized probability distribution at each level {y(1),y(2), . . . ,y(k)}, where y(k) = {y(k)ak } is a vector with indexes ak ∈ {1, . . . , n}, where n is the size of layer k, and ∑ ak y (k) ak = 1. From the second\nFigure 4: The above figure illustrates the downpour algorithm (Algorithm 1) for deriving the MAP trace.\nGiven output posteriors y(l) = {y(l)a } at each layer l ∈ {1, . . . ,K} Define T (l)i = [ ] as MAP trace of neuron i at level l T\n(1) i = [i]\nfor layer l = 2 to k do Find MAP parent A = argmaxa y (l) b y (l−1) a\nT (l) b = T (l−1) A .append(b) Update y(l)b = maxa y (l) b y (l−1) a end Find MAP level for stop neuron s L = argmaxl y (l) s return T (L)s\nAlgorithm 1: Downpour algorithm for inferencing the MAP trace.\nlevel onwards, we include a stop neuron (red neuron) which is used for stopping the hierarchical trace. The path of the trace from top down ends in a stop neuron (red neuron). Define the MAP trace up to level k which ends at stop neuron s as T (k)ak=s = arg max\na1:k−1 p(a1, a2, . . . , ak−1, ak = s).\nThe objective of the downpour in finding the MAP from the hierarchy is equivalent to finding the maximum MAP trace out of all MAP traces that ends in a stop neuron from different levels which is T\n(L) aL=s where L = argmaxk T (k) ak=s. The probability of MAP trace at level k can be derived greedily\nfrom the MAP trace at level k − 1 as\np(T (k)ak ) = maxak−1 p(ak|ak−1)p(T (k−1)ak−1 ) (1)\nFrom Equation 1, we can derive Theorems 2.1-2.3 which prove that Downpour Algorithm will always yield the MAP trace of T (L)aL=s = maxn p(T (n) an=s) ≥ p(a1, a2, . . . , am = s) ∀m.\nTheorem 2.1. For a greedy downpour that ends at a stop neuron at level n with MAP trace T (n)an , then p(Sm) ≤ p(T (n)an ) for every sequence Sm = {a1, a2, . . . , an, . . . , am} of m ≥ n that pass through an or ends at an. Refer to Appendix A for proof.\nTheorem 2.2. For a MAP trace T (n)an=s that ends at stop neuron s at level n such that p(T (n) an=s) ≥ p(T (n) an ) ∀an 6= s, then p(Sm) ≤ p(T (n) an=s) for every sequence Sm = {a1, a2, . . . , am} of m > n. Refer to Appendix A for proof.\nTheorem 2.3. The maximum of the MAP traces that end in a stop neuron from each level is T (L)ak=s where L = argmaxk p(T (k) ak=s) is the MAP for the hierarchy, which means p(T (L) ak=s) ≥ p(Sm) ∀m. Refer to Appendix A for proof."
    }, {
      "heading" : "3 RESULTS AND CONCLUSION",
      "text" : "We compared HiNet with a Flatten Network which have the same architecture except the output layer for HiNet is hierarchical as illustrated in Figure ?? and flatten for Flatten Network. The number of outputs for Flatten Network corresponds to the number of classes in the dataset. From the results, HiNet out-performs Flatten Network for both a Tree hierarchical dataset with much lesser parameters. We see that the number of parameters in the classification layer for Flatten Network is exponential to the maximum length of the trace. Thus for very deep hierarchies, the number of parameters in Flatten Network will be exponentially large while HiNet is always polynomial. This makes HiNet not only better architecture in terms of accuracy but also way more efficient in parameters space."
    }, {
      "heading" : "4 APPENDIX A",
      "text" : "Proof for Theorem 2.1 For a greedy downpour that ends at a stop neuron at level n with MAP trace T (n)an , then p(Sm) ≤ p(T (n) an ) for every sequence Sm = {a1, a2, . . . , an, . . . , am} of m ≥ n that pass through an or ends at an.\nProof. for m = n, that is p(T (n)an ) ≥ p(Sn). By definition T (1) a1 = a1\np(T (n)an ) = maxan−1 p(an|an−1)p(T (n−1)an−1 )\n= max an−1 p(an|an−1)max an−2 p(an−1|an−2)p(T (n−2)an−2 )\n= max a1:n−1\np(an|an−1)p(an−1|an−2) . . . p(a1)\n= max a1:n−1 p(an, an−1, . . . , a1)\n≥ p(an, an−1, . . . , a1)\n(2)\nfor m > n, we just need to prove that p(Sm) ≤ p(Sn)\np(Sm) = p(Sn)p(an+1, an+2, . . . , am|Sn) ≥ p(Sn)\n(3)\nsince p(an+1, an+2, . . . , am|Sn) ≤ 1\nProof for Theorem 2.2 For a MAP trace T (n)an=s that ends at stop neuron s at level n such that p(T (n) an=s) ≥ p(T (n) an ) ∀an 6= s, then p(Sm) ≤ p(T (n) an=s) for every sequence Sm = {a1, a2, . . . , am} of m > n.\nProof. from Theorem 2.1, p(Sm) ≤ p(T (n)an ) ≤ p(T (n) an=s).\nProof for Theorem 2.3 The maximum of the MAP traces that end in a stop neuron from each level is T (L)ak=s where L = argmaxk p(T (k) ak=s) is the MAP for the hierarchy, which means p(T (L) ak=s) ≥ p(Sm) ∀m.\nProof. From Theorem 2.2, we have p(T (n)an=s) ≥ p(Sm) for every m > n, and from Theorem 2.1, we have p(T (n)an=s) ≥ p(a1, a2, . . . , an = s). Therefore maxn p(T (n) an=s) ≥ p(a1, a2, . . . , am = s) ∀m."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Traditionally, classifying large hierarchical labels with more than 10000 distinct traces can only be achieved with flatten labels. Although flatten labels is feasible, it misses the hierarchical information in the labels. Hierarchical models like HSVM by Vural & Dy (2004) becomes impossible to train because of the sheer number of SVMs in the whole architecture. We developed a hierarchical architecture based on neural networks that is simple to train. Also, we derived an inference algorithm that can efficiently infer the MAP (maximum a posteriori) trace guaranteed by our theorems. Furthermore, the complexity of the model is only O(n) compared to O(n) in a flatten model, where h is the height of the hierarchy.",
    "creator" : "LaTeX with hyperref package"
  }
}