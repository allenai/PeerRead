{
  "name" : "1703.08136.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Visually grounded learning of keyword prediction from untranscribed speech",
    "authors" : [ "Herman Kamper", "Shane Settle", "Gregory Shakhnarovich", "Karen Livescu" ],
    "emails" : [ "klivescu}@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1. Introduction Current automatic speech recognition (ASR) systems use supervised models trained on huge amounts of annotated resources. In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1–5]. Here we consider the problem of grounding unlabelled speech when paired with images. Annotating speech is expensive and sometimes impossible, e.g. for endangered or unwritten languages [6]; grounding speech using co-occurring visual contexts could be a way to train systems in such lowresource scenarios [7]. This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8–10], and for understanding language acquisition in humans, who have access to visual cues for grounding [11–14].\nSpecifically, we are interested in the setting considered in [15, 16], where natural images of scenes are paired with spoken descriptions, and neither the images nor speech are labelled. Both [15] and [16] used paired neural networks to map images and speech into a common semantic space where matched images and spoken captions are close to each other. This approach allows images to be retrieved using speech and vice versa. The same task was also considered in earlier work on tagging mobile phone images with spoken descriptions [17, 18]. Despite the practical relevance, and interesting extensions in follow-on work [7, 19], this joint mapping approach does not give an explicit grounding of speech in terms of textual labels.\nHere we consider the possibility of using externally trained computer vision systems, which do have access to textual labels, to provide (noisy) supervision for untranscribed speech. Concretely, we use an external image-to-words multi-label visual classifier, predicting for an image a set of words that refer to aspects of the scene. Using soft labels (probabilities) from\nthis vision system, we train a convolutional neural network to map spoken captions to these soft unordered word targets. The result is a speech model that can predict which words (from a fixed vocabulary defined by the vision system) occur in a spoken utterance—acting as a spoken bag-of-words (BoW) classifier.\nThe previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models. Our approach can be seen as a further way to exploit vision systems, by also using their textual classification output. We are thus transferring labels from a high-resourced modality (vision, for which extensive resources are available) to a low-resource modality (speech, where resources are scarce for many languages).\nWe first apply our word prediction model to two tasks: BoW prediction, where the aim is to predict an unordered set of words that occur in a given utterance, and keyword spotting, where the task is to retrieve all utterances in a collection that contain a given textual keyword. Promising results are achieved on both tasks. Analysis shows that many of the model errors are semantically related to the correct labels, e.g. the model retrieves the speech utterance “a dog runs in the grass” for the textual keyword “field”. These “errors” may be desirable in certain settings. So in a final task, we evaluate our model as a semantic keyword spotter, where it achieves performance much closer to that of an oracle model trained using ground-truth transcriptions."
    }, {
      "heading" : "2. Related work",
      "text" : "Our work intersects with several other research directions. Recent studies have shown that using extra visual features from the scene in which the speech occurs can improve conventional ASR [20, 21]. These systems still rely on labelled speech data, while our aim is to use vision to ground untranscribed speech. There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22–26]. The study of [26] particularly influenced our approach, since they build a speech system using textual BoW labels (§3.1). In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27–30]. In natural language processing, images have also been used to capture aspects of meaning (semantics) of written language; see [31, 32] for reviews. Other studies have considered multimodal modelling of sounds (not speech) with text and images [33–35], and phonemes with images [36]."
    }, {
      "heading" : "3. Word prediction from images and speech",
      "text" : "Given a corpus of parallel images and spoken captions, neither with textual labels, we propose a method to train a spoken word prediction model using labels obtained from the visual modality."
    }, {
      "heading" : "3.1. Model overview",
      "text" : "Every training image I is paired with a spoken caption of frames X = x1,x2, . . . ,xT (e.g. MFCCs). We use a vision system to\nar X\niv :1\n70 3.\n08 13\n6v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\n01 7\ntag I with soft textual labels, giving targets to train the speech network f(X) to predict which words are present in X . The network f(X) therefore acts as a spoken bag-of-words (BoW) classifier (disregarding the order and quantity of words). No transcriptions are used during training. When applying the trained f(X), only speech input is used (and no image). The approach is illustrated in Figure 1, and below we give complete details.\nIf we knew which words occur in training utterance X , we could construct a multi-hot vector ybow ∈ {0, 1}W , with W the vocabulary size, and each dimension ybow,w a binary indicator for whether word w occurs in X . In [26], transcriptions were used to obtain this type of ideal BoW supervision. Instead of a transcription for X , we only have access to the paired image I . We use a multi-label visual classifier (with parameters γ) which, instead of binary indicators, produces soft targets yvis ∈ [0, 1]W , with yvis,w = P (w|I,γ) the probability of wordw being present given image I . In Figure 1, yvis would ideally be close to 1 for w corresponding to words such as “hat”, “man” and “shirt”, and close to 0 for irrelevant dimensions. This vision system is fixed: during training (below), vision parameters γ are never updated.\nGiven yvis as target, we train the word prediction model f(X). This model (with parameters θ) consists of a convolutional neural network (CNN) over the speech X , as shown on the right in Figure 1. We interpret each dimension of the output as fw(X) = P (w|X,θ). Note that f(X) is not a distribution over the vocabulary, since any number of terms in the vocabulary can be present in an utterance; rather, each dimension fw(X) can have any value in [0, 1]. We train this speech network using the cross-entropy loss, which (for a single training example) is:\nL(f(X),yvis) = − W∑\nw=1\n{yvis,w log fw(X) +\n(1− yvis,w) log [1− fw(X)]} (1)\nIf we had yvis,w ∈ {0, 1}, as in ybow, this could be described as the summed log loss of W binary classifiers. The size-W vocabulary of our system is implicitly specified by the vision system."
    }, {
      "heading" : "3.2. Two convolutional architectures over speech",
      "text" : "We consider two different convolutional architectures for f(X). Both deal with the variable number of frames in X by pooling over the entire output of their final convolution layer. As input layer, both use a one-dimensional convolution only over time, covering a number of frames and the entire frequency axis.\nThe first architecture is shown schematically in Figure 1. It is a CNN based on [16, 37], consisting of several convolution\nand max pooling layers (final pooling covering the entire output), followed by fully connected layers. A sigmoid activation is used for the final output f(X), and ReLUs in intermediate layers.\nThe second architecture is the one from Palaz, Synnaeve and Collobert [26], referred to as PSC. It was originally developed for ideal BoW supervision (§3.1), with the aim of not only doing spoken BoW classification, but also locating where words occur in the speech. PSC aims to do this by explicitly building the vocabulary into its final convolutional layer, as illustrated in Figure 2. The final convolution is linear with W output filters, matching the system vocabulary. The outputs of these final filters are h1,h2, . . . ,hT ′ , with hi ∈ RW . The idea is that hi,w gives a score for word w occurring in the time span corresponding to output i, thus giving an estimate of where w would occur in X . To obtain the network output s(X) ∈ RW , PSC does not use mean or max pooling, but rather an intermediate option:\nsw(X) = 1\nr log  1 T ′ T ′∑ t=1 exp (r ht,w(X))  (2) with sw(X) giving an overall unnormalized score for word w being present in X . This logsumexp pooling is equivalent to mean pooling when r → 0 and max pooling for r →∞; Palaz et al. note that this intermediate method improves PSC’s location prediction capability (refer to [26]). The final output of the network is f(X) = σ(s(X)), with σ the sigmoid function. We use r = 1, ReLU activations, and no intermediate pooling."
    }, {
      "heading" : "3.3. The vision system",
      "text" : "In image captioning, the goal is to produce a natural language description of a scene [27–30]. In contrast, rather than a fluent sentence, here we want a vision tagging system [38–40] that predicts an unordered set of words (nouns, adjectives, verbs) that accurately describe aspects of the scene (Figure 1, left). This is a multi-label binary classification task, where for each word we must predict whether it is appropriate for the image.\nWe train our vision tagging system on the Flickr30k data set [41], which contains 30k images, each with a set of 5 captions, which we convert into a BoW after removing stop words. Given a limited set of task-specific training data, such as Flickr30k, a common approach is to start with a visual representation learned as a part of end-to-end training on a larger data set (possibly for a different task), and then adapt it to the task at hand. We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].1 Specifically, we use VGG-16 [43], but replace the final\n1The ImageNet output itself is not well-suited to our setting, since it performs a single multi-way classification among a set of image classes.\nclassification layer with four 1024-unit ReLU layers, followed by a binary classifier for word occurrence. We train this multilabel visual classifier (with parameters γ) on Flickr30k, with the output layer limited to the W = 1000 most common word types in the image captions. The VGG-16 parameters are fixed during training; only the final layers that we add on top are trained.\nNote that Flickr30k is distinct from the parallel imagespeech data used in our experiments (§4). The vision system is also trained and then fixed (parameters γ is not updated in §4).\n4. Experiments"
    }, {
      "heading" : "4.1. Experimental setup",
      "text" : "We train our word prediction model on the data set of parallel images and spoken captions of [44], containing 8000 images with 5 spoken captions each. The audio comprises around 37 hours of active speech. The data comes with train, development and test splits containing 30 000, 5000 and 5000 utterances, respectively. Speech is parametrized as MFCCs with first and second order derivatives, giving 39-dimensional input.2 Utterances longer than 8 s are truncated (99.5% of utterances are shorter than 8 s).\nTraining images are passed through the vision system (§3.3), producing soft targets yvis for training the word prediction model f(X) on the unlabelled speech. We consider two architectures for f(X), referred to as VisionSpeechCNN and VisionSpeechPSC, respectively (see §3.2). VisionSpeechCNN is structured as follows: 1-D ReLU convolution with 64 filters over 9 frames; max pooling over 3 units; 1-D ReLU convolution with 256 filters over 10 units; max pooling over 3 units; 1-D ReLU convolution with 1024 filters over 11 units; max pooling over all units; 2048- unit fully-connected ReLU; and the 1000-unit sigmoid output. VisionSpeechPSC is structured as follows: 1-D ReLU convolution with 64 filters over 9 frames; four 1-D ReLU convolutions, each with 64 filters over 10 units; 1-D linear convolution with W = 1000 filters over 10 units; and logsumexp pooling followed by the final sigmoid activation. We arrived at these two structures starting from those in [16] and [26], respectively, and then tuned them on our development data.\nWe also obtain upper and lower bounds on performance. As an upper bound, we train two oracle models, OracleSpeechCNN and OracleSpeechPSC, with the same structures as the two VisionSpeech models above. These models are trained on ideal BoW supervision (§3.1): we obtain ybow targets for the 1000 most common words in the transcriptions of the 30 000 speech training utterances, after removing stop words. Next, as a lowerbound baseline, we use a unigram language model prior that gives the unigram probability of each keyword as estimated from the transcriptions. This baseline gives an indication of how much better our models do than simply hypothesizing common words. Note that the textual transcriptions are used only for the baseline and oracle models and for evaluation: neither of the VisionSpeech models ever see any parallel speech and text.\nAll models were implemented in TensorFlow [45]. Based on development tuning, they are trained using Adam [46] for 25 epochs with a minibatch of 128 and a learning rate of 0.0001 for all models, except those based on PSC, which uses 0.001."
    }, {
      "heading" : "4.2. Spoken bag-of-words prediction",
      "text" : "We first consider the task of predicting which words are present in a given test utterance. Given input X , our model gives a score fw(X) ∈ [0, 1] for every word w in its vocabulary, and\n2We also tried filterbanks; MFCCs always worked similarly or better.\na child in a black shirt running boy, child, young a small dog runs through a field dog, field, grass, running, runs a snowboarder jumping in the air with a person riding a ski lift in the background air, snow, snowboarder\na white dog with black spots jumps in midair dog, jumping, jumps, white\nthe man in a blue sweater is climbing down the rocks climber, climbing, rock\nthese can be used for spoken BoW prediction. To make a hard prediction, we set a threshold α and output labels for allw where fw(X) > α. We compare the predicted BoW labels to the true set of words in the transcriptions, and calculate precision, recall and F -score across all word types in the reference transcriptions (not only the 1000 words in the system vocabulary). To compare performance independently of α, we report average precision (AP), the area under the precision-recall curve as α is varied.\nTable 1 presents BoW prediction performance for the different models at two operating points for α, to show the trade-off between precision and recall. The unigram baseline achieves nontrivial performance, indicating that some words are commonly used across the utterances in the data set. Both VisionSpeech models substantially outperform this baseline at both α’s, and in AP. Although the VisionSpeech models still lag far behind the two oracle models, the VisionSpeech models are trained without seeing any parallel speech and text. The precision of 61.3% of VisionSpeechCNN at α = 0.7 is therefore noteworthy, since it shows that (although we miss many words in terms of recall), a relatively high-precision textual labelling system can be obtained using only images and unlabelled speech. For the oracle models, the PSC architecture is beneficial, outperforming its CNN counterpart by all measures; but for the VisionSpeech models, the PSC model falls slightly behind. We discuss this below.\nTable 2 gives examples of the type of output produced by the VisionSpeechCNN model. To better analyze the model’s behavior, we examine a selection of words that the model predicts that do not occur in the corresponding reference transcriptions. Figure 3 shows some of these “false alarm words”, along with the most common words that do occur in the corresponding utterances. In many cases, the predicted words are variants of the correct words: e.g. for an incorrect prediction of “snow”, most of the reference transcriptions contain the word “snowy”. Other confusions are semantic in nature, e.g. “young” is predicted when “girl” is present, and “trick” when “ramp” is present."
    }, {
      "heading" : "4.3. Keyword spotting",
      "text" : "Our model can also be naturally used as a keyword spotter: given a text query, the goal is to retrieve all of the utterances in the test set containing spoken instances of that query. We randomly select 20 textual keywords from the VisionSpeech output vocabulary as queries. For evaluation, we use three metrics [47, 48]: P@10 is the average precision (across keywords, in %) of the 10 highest-scoring proposals; P@N is the average precision of the top N proposals, with N the number of true occurrences of the keyword; and equal error rate (EER) is the average error rate at which the false acceptance and false rejection rates are equal.\nTable 3 shows keyword spotting results. The trend in relative performance is similar to that of Table 1: the unigram baseline performs worst, the VisionSpeech models give reasonable scores, and the oracle models perform best. The very high oracle performance indicates that the constrained nature of the data used here (narrow domain, relatively small vocabulary) makes the task fairly easy when true transcriptions are available. Nevertheless, it is again noteworthy that both VisionSpeech models obtain a P@10 of around 50% at an EER of 23%, without using any text.\nTo give a qualitative view of VisionSpeechCNN’s errors, Table 4 shows examples of incorrectly matched utterances for some keywords. As before, many of these erroneous utterances contain either variants of the keyword (e.g. “play” and “playing”) or are semantically related (e.g. “young” and “little girl”). Although these matches would seem reasonable and even desirable in some settings, they are penalized under the metrics in Table 3."
    }, {
      "heading" : "4.4. Semantic keyword spotting",
      "text" : "To investigate this issue quantitatively, we considered the top 10 proposed utterances for each keyword for each model, and relabelled as correct those utterances that either contained keyword variants or were semantically related. This allows us to report P@10 for the task of semantic keyword spotting, as shown in Table 5 (the other metrics would require us to semantically label all test utterances). Compared to Table 3, the semantic keyword spotting performance is better than exact keyword spotting scores for all models. However, the VisionSpeech models im-\nTable 4: Examples of incorrectly retrieved utterances when VisionSpeechCNN is used for keyword spotting.\nKeyword Example of incorrectly matched utterance Type\nModel P@10\nUnigram baseline 10.0\nVisionSpeechCNN 82.5 VisionSpeechPSC 73.0\nOracleSpeechCNN 99.5 OracleSpeechPSC 100.0\nprove most, with VisionSpeechCNN improving by almost 30% absolute. Moreover, while the oracle models improved mainly due to variant matches, the VisionSpeech models had about equal numbers of relabelled variant and semantic matches.\nIn Tables 3 and 5 we again see that while PSC is superior to CNN for the oracle models, this is not the case for the VisionSpeech models. As mentioned in §3.2, PSC is intended to also estimate word locations. Our results suggest that when trained on transcriptions (oracle models), there is a benefit in attempting to capture aspects of word order. However, when trained through visual grounding, the output of VisionSpeechPSC produces high probabilities for several semantically related words, and there is far less structure in the order of these words."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We have introduced a new way of using images to learn from untranscribed speech. By using a visual image-to-word classifier to provide soft labels for the speech, we are able to learn a neural speech-to-keyword prediction system. Our best model achieves a spoken bag-of-words precision of more than 60%, and a keyword spotting P@10 of more than 50% with an equal error rate of 23%. The model achieves this performance without access to any parallel speech and text. Further analysis shows that the model’s mistakes are often semantic in nature, e.g. confusing “boys” and “children”. To quantify this, we evaluated our model as a semantic keyword spotter, where the task is to find all utterances in a corpus that are semantically related to the textual keyword query. In this setting, our model achieves a semantic P@10 of more than 80%. Future work will consider how semantic search in speech can be formalized, and how the visual component of our approach can be explicitly tailored to obtain an improved visual grounding signal for unlabelled speech.\nAcknowledgements: We thank Gabriel Synnaeve and David Harwath for assistance with data and models, as well as Shubham Toshniwal and Hao Tang for helpful feedback. This research was funded by NSF grant IIS-1433485. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.\n6. References [1] A. S. Park and J. R. Glass, “Unsupervised pattern discovery in\nspeech,” IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186–197, 2008.\n[2] A. Jansen et al., “A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition,” in Proc. ICASSP, 2013.\n[3] C.-y. Lee, T. O’Donnell, and J. R. Glass, “Unsupervised lexicon discovery from acoustic input,” Trans. ACL, vol. 3, pp. 389–403, 2015.\n[4] M. Versteegh, X. Anguera, A. Jansen, and E. Dupoux, “The Zero Resource Speech Challenge 2015: Proposed approaches and results,” in Proc. SLTU, 2016.\n[5] H. Kamper, A. Jansen, and S. J. Goldwater, “Unsupervised word segmentation and lexicon discovery using acoustic word embeddings,” IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669–679, 2016.\n[6] L. Besacier, E. Barnard, A. Karpov, and T. Schultz, “Automatic speech recognition for under-resourced languages: A survey,” Speech Commun., vol. 56, pp. 85–100, 2014.\n[7] G. Chrupała, L. Gelderloos, and A. Alishahi, “Representations of language in a model of visually grounded speech signal,” arXiv preprint arXiv:1702.01991, 2017.\n[8] J. Luo, B. Caputo, A. Zweig, J.-H. Bach, and J. Anemüller, “Object category detection using audio-visual cues,” in Proc. ICVS, 2008.\n[9] M. Sun and H. Van hamme, “Joint training of non-negative Tucker decomposition and discrete density hidden Markov models,” Comput. Speech Lang., vol. 27, no. 4, pp. 969–988, 2013.\n[10] T. Taniguchi, T. Nagai, T. Nakamura, N. Iwahashi, T. Ogata, and H. Asoh, “Symbol emergence in robotics: A survey,” Adv. Robotics, vol. 30, no. 11-12, pp. 706–728, 2016.\n[11] L. Smith and C. Yu, “Infants rapidly learn word-referent mappings via cross-situational statistics,” Cognition, vol. 106, no. 3, pp. 1558– 1568, 2008.\n[12] E. D. Thiessen, “Effects of visual information on adults and infants auditory statistical learning,” Cognitive Sci., vol. 34, no. 6, pp. 1093–1106, 2010.\n[13] O. J. Räsänen, “Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions,” Speech Commun., vol. 54, pp. 975–997, 2012.\n[14] S. Frank, N. H. Feldman, and S. J. Goldwater, “Weak semantic context helps phonetic learning in a model of infant language acquisition,” in Proc. ACL, 2014.\n[15] G. Synnaeve, M. Versteegh, and E. Dupoux, “Learning words from images and speech,” in NIPS Workshop Learn. Semantics, 2014.\n[16] D. Harwath, A. Torralba, and J. R. Glass, “Unsupervised learning of spoken language with visual context,” in Proc. NIPS, 2016.\n[17] T. J. Hazen, B. Sherry, and M. Adler, “Speech-based annotation and retrieval of digital photographs,” in Proc. Interspeech, 2007.\n[18] X. Anguera, J. Xu, and N. Oliver, “Multimodal photo annotation and retrieval on a mobile phone,” in Proc. ICMIR, 2008.\n[19] D. Harwath and J. R. Glass, “Learning word-like units from joint audio-visual analysis,” arXiv preprint arXiv:1701.07481, 2017.\n[20] F. Sun, D. Harwath, and J. R. Glass, “Look, listen, and decode: Multimodal speech recognition with images,” in Proc. SLT, 2016.\n[21] A. Gupta, Y. Miao, L. Neves, and F. Metze, “Visual features for context-aware speech recognition,” in Proc. ICASSP, 2017.\n[22] G. Aimetti, R. K. Moore, and L. ten Bosch, “Discovering an optimal set of minimally contrasting acoustic speech units: A point of focus for whole-word pattern matching,” in Proc. Interspeech, 2010.\n[23] V. Renkens and H. Van hamme, “Mutually exclusive grounding for weakly supervised non-negative matrix factorisation,” in Proc. Interspeech, 2015.\n[24] L. Duong, A. Anastasopoulos, D. Chiang, S. Bird, and T. Cohn, “An attentional model for speech translation without transcription,” in Proc. NAACL, 2016, pp. 949–959.\n[25] S. Bansal, H. Kamper, A. Lopez, and S. J. Goldwater, “Towards speech-to-text translation without speech recognition,” in Proc. EACL, 2017.\n[26] D. Palaz, G. Synnaeve, and R. Collobert, “Jointly learning to locate and classify words using convolutional networks,” in Proc. Interspeech, 2016.\n[27] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image caption generator,” in Proc. CVPR, 2015.\n[28] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating image descriptions,” in Proc. CVPR, 2015.\n[29] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions to visual concepts and back,” in Proc. CVPR, 2015.\n[30] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for visual recognition and description,” in Proc. CVPR, 2015.\n[31] C. H. Silberer, “Learning visually grounded meaning representations,” Ph.D. dissertation, The University of Edinburgh, 2015.\n[32] R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. IkizlerCinbis, F. Keller, A. Muscat, and B. Plank, “Automatic description generation from images: A survey of models, datasets, and evaluation measures,” J. Artif. Intell. Res., vol. 55, pp. 409–442, 2016.\n[33] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and A. Torralba, “Ambient sound provides supervision for visual learning,” in Proc. ECCV, 2016.\n[34] Y. Aytar, C. Vondrick, and A. Torralba, “Soundnet: Learning sound representations from unlabeled video,” in Proc. NIPS, 2016, pp. 892–900.\n[35] A. K. Vijayakumar, R. Vedantam, and D. Parikh, “SoundWord2Vec: Learning word representations grounded in sounds,” arXiv preprint arXiv:1703.01720, 2017.\n[36] L. Gelderloos and G. Chrupała, “From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning,” Proc. COLING, 2016.\n[37] H. Kamper, W. Wang, and K. Livescu, “Deep convolutional acoustic word embeddings using word-pair side information,” in Proc. ICASSP, 2016.\n[38] K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D. M. Blei, and M. I. Jordan, “Matching words and pictures,” J. Mach. Learn. Res., vol. 3, pp. 1107–1135, 2003.\n[39] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid, “Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation,” in Proc. ICCV, 2009.\n[40] M. Chen, A. X. Zheng, and K. Q. Weinberger, “Fast image tagging.” in Proc. ICML, 2013.\n[41] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” Trans. ACL, vol. 2, pp. 67–78, 2014.\n[42] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in Proc. CVPR, 2009.\n[43] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[44] D. Harwath and J. Glass, “Deep multimodal semantic embeddings for speech and images,” in Proc. ASRU, 2015.\n[45] M. Abadi et al., “TensorFlow: Large-scale machine learning on heterogeneous systems,” 2015. [Online]. Available: http: //tensorflow.org/\n[46] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.\n[47] T. J. Hazen, W. Shen, and C. White, “Query-by-example spoken term detection using phonetic posteriorgram templates,” in Proc. ASRU, 2009.\n[48] Y. Zhang and J. R. Glass, “Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams,” in Proc. ASRU, 2009."
    } ],
    "references" : [ {
      "title" : "Unsupervised pattern discovery in speech",
      "author" : [ "A.S. Park", "J.R. Glass" ],
      "venue" : "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186–197, 2008.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition",
      "author" : [ "A. Jansen" ],
      "venue" : "Proc. ICASSP, 2013.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Unsupervised lexicon discovery from acoustic input",
      "author" : [ "C.-y. Lee", "T. O’Donnell", "J.R. Glass" ],
      "venue" : "Trans. ACL, vol. 3, pp. 389–403, 2015.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The Zero Resource Speech Challenge 2015: Proposed approaches and results",
      "author" : [ "M. Versteegh", "X. Anguera", "A. Jansen", "E. Dupoux" ],
      "venue" : "Proc. SLTU, 2016.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings",
      "author" : [ "H. Kamper", "A. Jansen", "S.J. Goldwater" ],
      "venue" : "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669–679, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Automatic speech recognition for under-resourced languages: A survey",
      "author" : [ "L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz" ],
      "venue" : "Speech Commun., vol. 56, pp. 85–100, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Representations of language in a model of visually grounded speech signal",
      "author" : [ "G. Chrupała", "L. Gelderloos", "A. Alishahi" ],
      "venue" : "arXiv preprint arXiv:1702.01991, 2017.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Object category detection using audio-visual cues",
      "author" : [ "J. Luo", "B. Caputo", "A. Zweig", "J.-H. Bach", "J. Anemüller" ],
      "venue" : "Proc. ICVS, 2008.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models",
      "author" : [ "M. Sun", "H. Van hamme" ],
      "venue" : "Comput. Speech Lang., vol. 27, no. 4, pp. 969–988, 2013.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Symbol emergence in robotics: A survey",
      "author" : [ "T. Taniguchi", "T. Nagai", "T. Nakamura", "N. Iwahashi", "T. Ogata", "H. Asoh" ],
      "venue" : "Adv. Robotics, vol. 30, no. 11-12, pp. 706–728, 2016.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Infants rapidly learn word-referent mappings via cross-situational statistics",
      "author" : [ "L. Smith", "C. Yu" ],
      "venue" : "Cognition, vol. 106, no. 3, pp. 1558– 1568, 2008.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Effects of visual information on adults and infants auditory statistical learning",
      "author" : [ "E.D. Thiessen" ],
      "venue" : "Cognitive Sci., vol. 34, no. 6, pp. 1093–1106, 2010.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions",
      "author" : [ "O.J. Räsänen" ],
      "venue" : "Speech Commun., vol. 54, pp. 975–997, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Weak semantic context helps phonetic learning in a model of infant language acquisition",
      "author" : [ "S. Frank", "N.H. Feldman", "S.J. Goldwater" ],
      "venue" : "Proc. ACL, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning words from images and speech",
      "author" : [ "G. Synnaeve", "M. Versteegh", "E. Dupoux" ],
      "venue" : "NIPS Workshop Learn. Semantics, 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Unsupervised learning of spoken language with visual context",
      "author" : [ "D. Harwath", "A. Torralba", "J.R. Glass" ],
      "venue" : "Proc. NIPS, 2016.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Speech-based annotation and retrieval of digital photographs",
      "author" : [ "T.J. Hazen", "B. Sherry", "M. Adler" ],
      "venue" : "Proc. Interspeech, 2007.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Multimodal photo annotation and retrieval on a mobile phone",
      "author" : [ "X. Anguera", "J. Xu", "N. Oliver" ],
      "venue" : "Proc. ICMIR, 2008.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning word-like units from joint audio-visual analysis",
      "author" : [ "D. Harwath", "J.R. Glass" ],
      "venue" : "arXiv preprint arXiv:1701.07481, 2017.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Look, listen, and decode: Multimodal speech recognition with images",
      "author" : [ "F. Sun", "D. Harwath", "J.R. Glass" ],
      "venue" : "Proc. SLT, 2016.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Visual features for context-aware speech recognition",
      "author" : [ "A. Gupta", "Y. Miao", "L. Neves", "F. Metze" ],
      "venue" : "Proc. ICASSP, 2017.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Discovering an optimal set of minimally contrasting acoustic speech units: A point of focus for whole-word pattern matching",
      "author" : [ "G. Aimetti", "R.K. Moore", "L. ten Bosch" ],
      "venue" : "Proc. Interspeech, 2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Mutually exclusive grounding for weakly supervised non-negative matrix factorisation",
      "author" : [ "V. Renkens", "H. Van hamme" ],
      "venue" : "Proc. Interspeech, 2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An attentional model for speech translation without transcription",
      "author" : [ "L. Duong", "A. Anastasopoulos", "D. Chiang", "S. Bird", "T. Cohn" ],
      "venue" : "Proc. NAACL, 2016, pp. 949–959.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards speech-to-text translation without speech recognition",
      "author" : [ "S. Bansal", "H. Kamper", "A. Lopez", "S.J. Goldwater" ],
      "venue" : "Proc. EACL, 2017.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Jointly learning to locate and classify words using convolutional networks",
      "author" : [ "D. Palaz", "G. Synnaeve", "R. Collobert" ],
      "venue" : "Proc. Interspeech, 2016.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan" ],
      "venue" : "Proc. CVPR, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "Proc. CVPR, 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollár", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt" ],
      "venue" : "Proc. CVPR, 2015.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell" ],
      "venue" : "Proc. CVPR, 2015.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning visually grounded meaning representations",
      "author" : [ "C.H. Silberer" ],
      "venue" : "Ph.D. dissertation, The University of Edinburgh, 2015.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Automatic description generation from images: A survey of models, datasets, and evaluation measures",
      "author" : [ "R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler- Cinbis", "F. Keller", "A. Muscat", "B. Plank" ],
      "venue" : "J. Artif. Intell. Res., vol. 55, pp. 409–442, 2016.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Ambient sound provides supervision for visual learning",
      "author" : [ "A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba" ],
      "venue" : "Proc. ECCV, 2016.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Soundnet: Learning sound representations from unlabeled video",
      "author" : [ "Y. Aytar", "C. Vondrick", "A. Torralba" ],
      "venue" : "Proc. NIPS, 2016, pp. 892–900.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Sound- Word2Vec: Learning word representations grounded in sounds",
      "author" : [ "A.K. Vijayakumar", "R. Vedantam", "D. Parikh" ],
      "venue" : "arXiv preprint arXiv:1703.01720, 2017.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning",
      "author" : [ "L. Gelderloos", "G. Chrupała" ],
      "venue" : "Proc. COLING, 2016.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep convolutional acoustic word embeddings using word-pair side information",
      "author" : [ "H. Kamper", "W. Wang", "K. Livescu" ],
      "venue" : "Proc. ICASSP, 2016.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Matching words and pictures",
      "author" : [ "K. Barnard", "P. Duygulu", "D. Forsyth", "N. de Freitas", "D.M. Blei", "M.I. Jordan" ],
      "venue" : "J. Mach. Learn. Res., vol. 3, pp. 1107–1135, 2003.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation",
      "author" : [ "M. Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid" ],
      "venue" : "Proc. ICCV, 2009.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier" ],
      "venue" : "Trans. ACL, vol. 2, pp. 67–78, 2014.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "Proc. CVPR, 2009.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556, 2014.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep multimodal semantic embeddings for speech and images",
      "author" : [ "D. Harwath", "J. Glass" ],
      "venue" : "Proc. ASRU, 2015.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "M. Abadi" ],
      "venue" : "2015. [Online]. Available: http: //tensorflow.org/",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980, 2014.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Query-by-example spoken term detection using phonetic posteriorgram templates",
      "author" : [ "T.J. Hazen", "W. Shen", "C. White" ],
      "venue" : "Proc. ASRU, 2009.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams",
      "author" : [ "Y. Zhang", "J.R. Glass" ],
      "venue" : "Proc. ASRU, 2009.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1–5].",
      "startOffset" : 138,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1–5].",
      "startOffset" : 138,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1–5].",
      "startOffset" : 138,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1–5].",
      "startOffset" : 138,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1–5].",
      "startOffset" : 138,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "for endangered or unwritten languages [6]; grounding speech using co-occurring visual contexts could be a way to train systems in such lowresource scenarios [7].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "for endangered or unwritten languages [6]; grounding speech using co-occurring visual contexts could be a way to train systems in such lowresource scenarios [7].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8–10], and for understanding language acquisition in humans, who have access to visual cues for grounding [11–14].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8–10], and for understanding language acquisition in humans, who have access to visual cues for grounding [11–14].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8–10], and for understanding language acquisition in humans, who have access to visual cues for grounding [11–14].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8–10], and for understanding language acquisition in humans, who have access to visual cues for grounding [11–14].",
      "startOffset" : 223,
      "endOffset" : 230
    }, {
      "referenceID" : 11,
      "context" : "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8–10], and for understanding language acquisition in humans, who have access to visual cues for grounding [11–14].",
      "startOffset" : 223,
      "endOffset" : 230
    }, {
      "referenceID" : 12,
      "context" : "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8–10], and for understanding language acquisition in humans, who have access to visual cues for grounding [11–14].",
      "startOffset" : 223,
      "endOffset" : 230
    }, {
      "referenceID" : 13,
      "context" : "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8–10], and for understanding language acquisition in humans, who have access to visual cues for grounding [11–14].",
      "startOffset" : 223,
      "endOffset" : 230
    }, {
      "referenceID" : 14,
      "context" : "Specifically, we are interested in the setting considered in [15, 16], where natural images of scenes are paired with spoken descriptions, and neither the images nor speech are labelled.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "Specifically, we are interested in the setting considered in [15, 16], where natural images of scenes are paired with spoken descriptions, and neither the images nor speech are labelled.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "Both [15] and [16] used paired neural networks to map images and speech into a common semantic space where matched images and spoken captions are close to each other.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 15,
      "context" : "Both [15] and [16] used paired neural networks to map images and speech into a common semantic space where matched images and spoken captions are close to each other.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 16,
      "context" : "The same task was also considered in earlier work on tagging mobile phone images with spoken descriptions [17, 18].",
      "startOffset" : 106,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "The same task was also considered in earlier work on tagging mobile phone images with spoken descriptions [17, 18].",
      "startOffset" : 106,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "Despite the practical relevance, and interesting extensions in follow-on work [7, 19], this joint mapping approach does not give an explicit grounding of speech in terms of textual labels.",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "Despite the practical relevance, and interesting extensions in follow-on work [7, 19], this joint mapping approach does not give an explicit grounding of speech in terms of textual labels.",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "Recent studies have shown that using extra visual features from the scene in which the speech occurs can improve conventional ASR [20, 21].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "Recent studies have shown that using extra visual features from the scene in which the speech occurs can improve conventional ASR [20, 21].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 21,
      "context" : "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22–26].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22–26].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22–26].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22–26].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 25,
      "context" : "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22–26].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 25,
      "context" : "The study of [26] particularly influenced our approach, since they build a speech system using textual BoW labels (§3.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27–30].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 27,
      "context" : "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27–30].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 28,
      "context" : "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27–30].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 29,
      "context" : "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27–30].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 30,
      "context" : "In natural language processing, images have also been used to capture aspects of meaning (semantics) of written language; see [31, 32] for reviews.",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 31,
      "context" : "In natural language processing, images have also been used to capture aspects of meaning (semantics) of written language; see [31, 32] for reviews.",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 32,
      "context" : "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33–35], and phonemes with images [36].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 33,
      "context" : "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33–35], and phonemes with images [36].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 34,
      "context" : "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33–35], and phonemes with images [36].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 35,
      "context" : "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33–35], and phonemes with images [36].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 25,
      "context" : "In [26], transcriptions were used to obtain this type of ideal BoW supervision.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "We use a multi-label visual classifier (with parameters γ) which, instead of binary indicators, produces soft targets yvis ∈ [0, 1] , with yvis,w = P (w|I,γ) the probability of wordw being present given image I .",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Note that f(X) is not a distribution over the vocabulary, since any number of terms in the vocabulary can be present in an utterance; rather, each dimension fw(X) can have any value in [0, 1].",
      "startOffset" : 185,
      "endOffset" : 191
    }, {
      "referenceID" : 15,
      "context" : "It is a CNN based on [16, 37], consisting of several convolution X logsumexp",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 36,
      "context" : "It is a CNN based on [16, 37], consisting of several convolution X logsumexp",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 25,
      "context" : "Figure 2: A two-layer Palaz, Synnaeve and Collobert (PSC) network [26].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 25,
      "context" : "The second architecture is the one from Palaz, Synnaeve and Collobert [26], referred to as PSC.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "note that this intermediate method improves PSC’s location prediction capability (refer to [26]).",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "In image captioning, the goal is to produce a natural language description of a scene [27–30].",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 27,
      "context" : "In image captioning, the goal is to produce a natural language description of a scene [27–30].",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 28,
      "context" : "In image captioning, the goal is to produce a natural language description of a scene [27–30].",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 29,
      "context" : "In image captioning, the goal is to produce a natural language description of a scene [27–30].",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 37,
      "context" : "In contrast, rather than a fluent sentence, here we want a vision tagging system [38–40] that predicts an unordered set of words (nouns, adjectives, verbs) that accurately describe aspects of the scene (Figure 1, left).",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 38,
      "context" : "In contrast, rather than a fluent sentence, here we want a vision tagging system [38–40] that predicts an unordered set of words (nouns, adjectives, verbs) that accurately describe aspects of the scene (Figure 1, left).",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 39,
      "context" : "We train our vision tagging system on the Flickr30k data set [41], which contains 30k images, each with a set of 5 captions, which we convert into a BoW after removing stop words.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 40,
      "context" : "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 41,
      "context" : "Specifically, we use VGG-16 [43], but replace the final",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 42,
      "context" : "We train our word prediction model on the data set of parallel images and spoken captions of [44], containing 8000 images with 5 spoken captions each.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "We arrived at these two structures starting from those in [16] and [26], respectively, and then tuned them on our development data.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "We arrived at these two structures starting from those in [16] and [26], respectively, and then tuned them on our development data.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 43,
      "context" : "All models were implemented in TensorFlow [45].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 44,
      "context" : "Based on development tuning, they are trained using Adam [46] for 25 epochs with a minibatch of 128 and a learning rate of 0.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "Given input X , our model gives a score fw(X) ∈ [0, 1] for every word w in its vocabulary, and",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 45,
      "context" : "For evaluation, we use three metrics [47, 48]: P@10 is the average precision (across keywords, in %) of the 10 highest-scoring proposals; P@N is the average precision of the top N proposals, with N the number of true occurrences of the keyword; and equal error rate (EER) is the average error rate at which the false acceptance and false rejection rates are equal.",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 46,
      "context" : "For evaluation, we use three metrics [47, 48]: P@10 is the average precision (across keywords, in %) of the 10 highest-scoring proposals; P@N is the average precision of the top N proposals, with N the number of true occurrences of the keyword; and equal error rate (EER) is the average error rate at which the false acceptance and false rejection rates are equal.",
      "startOffset" : 37,
      "endOffset" : 45
    } ],
    "year" : 2017,
    "abstractText" : "During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance—acting as a spoken bag-of-words classifier—without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. “man” and “person”, making it even more effective as a semantic keyword spotter.",
    "creator" : "LaTeX with hyperref package"
  }
}