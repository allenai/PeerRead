{
  "name" : "1502.07976.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Error-Correcting Factorization",
    "authors" : [ "Miguel Angel Bautista", "Oriol Pujol", "Fernando De la Torre", "Sergio Escalera" ],
    "emails" : [ "pujol}@ub.edu", "ftorre@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Error-Correcting Output Codes, Multi-class learning, Matrix Factorization"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "In the last decade datasets have experimented an exponential growth rate, generating vast collections of data that need to automatically be analyzed. In particular, multimedia datasets have experienced an explosion on data availability, thanks to the almost negligible cost of gathering multi-media data from Internet. Therefore, there is a pushing need for efficient algorithms that are able to automatize knowledge extraction processes on those datasets. One of the classic problems in Pattern Recognition and Machine Intelligence is to perform automatic classification, i.e., automatically attributing a label to each sample of the dataset. In this sense, the classification process is often considered as first step for higher order representations or knowledge extractions. In multi-class classification problems the goal is to find a function f : Rn → K, that maps samples to a finite discrete set K of labels with |K| > 2. While there exists a large set of approaches to estimate f all of them can be grouped in two different categories: Single-Machine/Single-Loss approaches and Divide and Conquer\n• Miguel Angel Bautista, Oriol Pujol and Sergio Escalera are with the Department of Applied Mathematics and Analisis, University of Barcelona, Barcelona, 08007, Spain and the Computer Vision Center, Autonomous University of Barcelona, 08193, Bellaterra (Cerdanyola), Barcelona, Spain E-mail: {mbautista,sescalera,oriol pujol}@ub.edu • Fernando De la Torre is in the Robotics Institute at Carnegie Mellon University, Pittsburgh, 15216, PA. E-mail: ftorre@cs.cmu.edu\napproaches. The formers attempt to approximate a single f for the complete multi-class problem, while the latter decouple f into a set of binary sub-functions (binary classifiers) that are potentially easier to estimate and aggregate the results.\nIn this sense, Error-Correcting Output Codes (ECOC) is a divide and conquer approach that has proven to be very effective in many different multi-class contexts. The core property within an ECOC is its capability to correct errors in binary classifiers by using redundancy. However, existing literature represents the error-correcting capability of an ECOC as an scalar, hindering a deeper the analysis of error-correction and redundancy on class pairs. Furthermore, classical divide and conquer approaches that have been included in the ECOC framework like One vs. All [48] or Random [2] approaches ignore the data distribution, thus not taking profit of allocating the error-correcting capabilities of ECOCs in a problemdependent fashion. In addition, recent problem-dependent ECOC designs have focused on designing the binary subfunctions rather than analyzing the core error-correcting property. In order to overcome this limitations, our proposal builds an ECOC matrix by factorizing a design matrix D that encodes the desired ’correction properties’ between classes (i.e a design matrix which can be obtained directly from data or be set by experts on the problem domain). The proposed method finds the ECOC coding that yields the closest configuration to the design matrix. We cast the task of designing an ECOC as a matrix factorization problem with binary constraints. A visual example is shown in Figure 1. ar X iv :1\n50 2.\n07 97\n6v 2\n[ cs\n.C V\n] 5\nM ar\n2 01\n5"
    }, {
      "heading" : "2 RELATED WORK",
      "text" : ""
    }, {
      "heading" : "2.1 Single-machine/Single-loss Approaches",
      "text" : "The multi-class problem can be directly treated by some methods that exhibit a multi-class behaviour off the shelf (i.e Nearest Neighbours [22], Decision Trees [30], Random Forests [6]). However, some of the most powerful methods for binary classification like Support Vector Machines (SVM) or Adaptive Boosting (AdaBoost) can not be directly extended to the multiclass case and further development is required. In this sense, literature is prolific on single-loss strategies to estimate f . One of the most well know approaches are the extensions of SVMs [7] to the multi-class case. For instance, the work of Weston and Watkins [55] presents a single-machine extension of the SVM method to cope with the multi-class case, in which k predictor functions are trained, constrained with k−1 slack variables per sample. However, a more recent adaptation of [14] reduces the number of constraints per samples to one, paying only for the second largest classification score among the k predictors. To solve the optimization problem a dual decomposition algorithm is derived, which iteratively solves the quadratic programming problem associated with each training sample. Despite these efforts, single-machine approaches to estimate f scale poorly with the number of classes and are often outperformed by simple decompositions [48], [52]. In recent years various works that extended the classical Adaptive Boosting method [20] to the multi-class setting have been presented [51], [43]. In [62] the authors directly extend the AdaBoost algorithm to the multi-class case without reducing it to multiple binary problems, that is estimating a single f for the whole multi-class problem. This algorithm is based on an exponential loss function for multi-class classification which is optimized on a forward stage-wise additive model. Furthermore, the work of Saberian and Vasconcenlos [50] presents a derivation of a new margin loss function for multi-class classification altogether with the set of real class codewords that maximize the presented multi-class margin, yielding boundaries with max margin. However, though these methods are consistently derived and supported with strong theoretical results, methodologies that jointly optimize a multiclass loss function present some limitations: • They scale linearly with k, rendering them unsuitable for\nproblems with a large k. • Due to their single-loss architecture the exploitation of par-\nallelization on modern multi-core processors is difficult. • They can not recover from classification errors on the class\npredictors."
    }, {
      "heading" : "2.2 Divide and Conquer Approaches",
      "text" : "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28]. In this sense, instead of developing a method to cope with the multi-class case, divide and conquer approaches decouple f into a set of l binary problems which are treated separately. Once the responses of binary classifiers are obtained a committee strategy is used to find the final output. In this trend one can find three main lines of research: flat strategies, hierarchical classification, and ECOC. Flat strategies like One vs. One [52] and One vs. All [48] are those that use a predefined problem partition scheme followed by a committee strategy to aggregate the binary classifier outputs. On the other hand, hierarchical classification relies on a similarity metric distance\namong classes to build a binary tree in which nodes correspond to different problem partitions [23], [40], [28]. Finally, the ECOC framework consists of two steps: In the coding step, a set of binary partitions of the original problem are encoded in a matrix of discrete codewords [16] (univocally defined, one code per class) (see Figure 2). At the decoding step a final decision is obtained by comparing the test codeword resulting of the union of the binary classifier responses with every class codeword and choosing the class codeword at minimum distance [17], [61]. The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58]. Predefined codings like One vs. All or One vs. One are directly embeddable in the ECOC framework. In [2], the authors propose the Dense and Sparse Random coding designs with a fixed code length of {10, 15} log2(K), respectively. In [2] the authors encourage to generate a set of 104 random matrices and select the one that maximizes the minimum distance between rows, thus showing the highest correction capability. However, the selection of a suitable code length l still remains an open problem."
    }, {
      "heading" : "2.3 Problem-dependent Strategies",
      "text" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46]. A common trend of these works is to exploit information of the multi-class data distribution obtained a priori in order to design a decomposition into binary problems that are easily separable. In that sense, [57] computes a spectral decomposition of the graph laplacian associated to the multi-class problem. The expected most separable partitions correspond to the thresholded eigenvectors of the laplacian. However, this approach does not provide any warranties on defining unequivocal codewords (which is a core property of the ECOC coding framework) or obtaining a suitable code length l. In [24], Gao and Koller propose a method which adaptively learns an ECOC coding by optimizing a novel multi-class hinge loss function sequentially. On an update of their earlier work, Gao and Koller propose in [23] a joint optimization process to learn a hierarchy of classifiers in which each node corresponds to a binary subproblem that is optimized to find easily separable subproblems. Nonetheless, although the hierarchical configuration speeds up the testing step, it is highly prone to error propagation since node mis-classifications can not be recovered. Finally, the work of Zhao et. al [58] proposes a dual projected gradient method embedded on a constrained concave-convex procedure to optimize an objective composed of a measure of expected problem separability, codeword correlation and regularization terms. In the light of these results, a general trend of recent works is to optimize a measure of binary problem separability in order to induce easily separable sub-problems. This assumption leads to ECOC coding matrices that boost the boundaries of easily separable classes while modeling with low redundancy the ones with most confusion."
    }, {
      "heading" : "2.4 Our approach",
      "text" : "In this paper we present the Error-Correcting Factorization (ECF) method for factorizing a design matrix of desired ’errorcorrecting properties’ between classes into a discrete ECOC matrix. The proposed ECF method is a general framework for the ECOC coding step since the design matrix is a flexible\ntool for error-correction analysis. In this sense, the problem of designing the ECOC matrix is reduced to defining the design matrix, where higher level reasoning may be used. For example, following recent state-of-the-art works one could build a design matrix following a ”hard classes are left behind” spirit, boosting the boundaries of easily separable classes and disregarding the classes that are not easily separable. An alternative for building the design matrix is the ”no class is left behind” criteria, where we may boost those classes that are prone to be confused in the hope of recovering more errors. Note that the design matrix could also directly encode knowledge of domain experts on the problem, providing a great flexibility on the design of the ECOC coding matrix. Figure 2 shows different coding schemes and the real boundaries learned by binary classifiers (SVM with RBF kernel) for a Toy problem of 14 classes (see section 5 for further details on the dataset). We can see how the binary problems induced by ECF in Fig. 2(a) boost the boundaries of classes that are prone to be confused, while other approaches that use equal or higher number of classifiers like Dense Random [2] in Fig. 2(b), or classic One vs. All designs in Fig. 2(c) fail in this task. The paper is organized as follows: Section 3 introduces the ECOC properties and derives ECF, where we cast the problem of finding an ECOC matrix that follows a certain distribution of correction as a discrete optimization problem. Section 4 presents a discussion of the method addressing important issues from the point of view of the ECOC framework. Concretely, we derive the optimal problem-dependent code length for ECOCs obtained by means of ECF, which to the best of our knowledge is the first time this question is tackled in the extended ECOC literature. In addition, we show how ECF converges to a solution with negligible objective value when the design matrix follows certain constraints. Section 5 shows how ECF yields ECOC coding matrices that obtain higher classification performances than state-of-the-art methods with comparable or lower computational complexity. Finally, Section 6 concludes the paper."
    }, {
      "heading" : "3 METHODOLOGY",
      "text" : "In this section, we review existing properties of the ECOC framework and propose to cast the ECOC coding matrix optimization as a Matrix Factorization problem that can be solved efficiently using a constrained coordinate descent approach."
    }, {
      "heading" : "3.1 Error-Correcting Output Codes",
      "text" : "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61]. At the coding step an ECOC coding matrix X ∈ {−1,+1}k×l (see notation1) is constructed, where k denotes the number of classes in the problem and l the number of bi-partitions (also known as dichotomies) to be learnt. In the coding matrix, the rows (xi’s, also known as codewords) are unequivocally defined, since these are the identifiers of each category in the multi-class problem. On the other hand, the columns of X (xj ’s) denote the bi-partitions to be learnt by base classifiers (also known as dichotomizer). Therefore, for a certain column a dichotomizer learns the boundary between classes valued +1 and classes valued −1. However, [2] introduced a third value, defining ternary valued coding matrices. X ∈ {−1,+1, 0}k×l. In this case, for any given dichotomy categories can be valued as +1 or −1 depending on the meta-class they belong to, or 0 if they are ignored by the dichotomizer. This new value allows the inclusion of wellknown decomposition techniques into the ECOC framework, such as One vs. One [52].\nAt the decoding step a data sample s is classified among the {c1, . . . , ck} possible categories. In order to perform the classification task, each dichotomizer predicts a binary value for s whether it belongs to one of the bi-partitions defined by the correspondent dichotomy. Once the set of predictions y ∈ {−1,+1}l is obtained, it is compared to the rows of X using a distance function δ, known as the decoding function. Usual decoding techniques are based on well-known distance measures such as the l1 or Euclidean distance. These measures are proved to be effective for X ∈ {+1,−1}k×l. Nevertheless, it is not until the work of [17] that decoding functions took into account the meaning of the 0 value at the decoding step. Generally, the final prediction for s is given by the class ci, where arg\ni min δ(xi,y), i ∈ {1, . . . , k}.\n1. Bold capital letters denote matrices (e.g. X), bold lower-case letters represent vectors (e.g., x). All non-bold letters denote scalar variables. xi is the i−th row of the matrix X. xj is the j−th column of the matrix X. 1 is a matrix or vector of all ones of the appropriate size. xij denotes the scalar in the i−th row and j−th column of X. ‖X‖F = tr(X>X) denotes the Frobenius norm. ‖·‖p is used to denote the Lp-norm. x⊕y is an operator which concatenates vectors x and y . rank(X) denotes the rank of X. X ≤ 0 denotes the point-wise inequality"
    }, {
      "heading" : "3.2 Good practices in ECOC",
      "text" : "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:\n1) Correction capability: let H ∈ Rk×k denote a symmetric matrix of hamming distances among all pairs of rows in X, the correction capability is expressed as bmin(H)−1\n2 c 2,\nconsidering only off-diagonal values of H. In this sense, if min(H) = 3, ECOC will be able to recover the correct multi-class prediction even if b 3−1\n2 c = 1 binary classifier\nmisses its prediction.3\n2) Uncorrelated binary sub-problems: the induced binary problems should be as uncorrelated as possible for X to recover binary classifier errors. 3) Use of powerful binary classifiers: since the final class prediction consists of the aggregation of bit predictors, accurate binary classifiers are also required to obtain accurate multi-class predictions."
    }, {
      "heading" : "3.3 From global to pair-wise correction capability",
      "text" : "In literature, correction capability has been a core objective of problem-dependent designs of X. In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25]. Hence, min(H) is expected to be large in order for X to recover from as many binary classifier errors as possible. However, since H expresses the hamming distance between rows of X, one can alternatively express the correction capability in a pair-wise fashion [5], allowing for a deeper understanding of how correction is distributed among codewords. Figure 3 shows an example of global and pair-wise correction capabilities calculation. Recall that the ⊕ operator between two vectors denotes its concatenation. Thus, the pairwise correction capability is defined as follows:\n4) The pair-wise correction capability of codewords xi\nand xj is expressed as: bmin(h i⊕hj)−1 2\nc, where we only consider off-diagonal values of H. This means that a sample of class ci is correctly discriminated from class cj even if bmin(h i⊕hj))−1 2\nc binary classifiers miss their predictions.\nNote that though in Figure 3 the global correction capability of X is 0, there are pairs of codewords with a higher correction, e.g. x2 and x8. In this case the global correction capability as defined in literature is overlooking ECOC coding characteristics that can potentially be exploited. This novel way of expressing the correction capability of an ECOC matrix enables a better understanding of how ECOC coding matrices distribute their correction capability, and gives an insight on how to design coding matrices. In this sense, it is straightforward to demand the correction capabilities of the ECOC matrix to be allocated according to those classes that are more prone to error, in order for them to have better recovery behavior (i.e. following a ”no class is left behind” criteria). However, recent works [57], [23], [58] have focused on designing a matrix X where binary problems are easily separable. This assumption leads to a matrix X where classes that are not easily separable\n2. In the case of ternary codes this correction capability can be easily adapted.\n3. Note that for X to be valid all off-diagonal elements of H should be greater or equal than one.\nshow a small hamming distance on their respective codewords (i.e. following a ”hard classes are left behind” scheme).\nIn addition to the proposal of a general method for ECOC coding by means of the definition of a design matrix, we explore the effect of focusing the learning effort of our method in those classes that have complex boundaries (i.e. those which show a small inter-class margin). It is important to take into account that though it is natural to estimate the design matrix from training data, it is not a limitation of ECF. In this sense, the design matrix can also code information of experts or any other distance measure directly set by the user. Formally, let X ∈ {−1,+1}k×l be a coding matrix, let H be a symmetric matrix of pair-wise l1 distances between rows of X and let D ∈ Rk×k be a design matrix (e.g. pair-wise distance measure between class codewords). It is natural to see that the ordinal properties of the distance should hold in H and D. Thus, if distance between codewords xk and xl (dkl) is required to be larger than the distance between codewords xi and xj (dij), this order should be maintained in H. Then we want to find a configuration of X such that hij < hkl ⇐⇒ dij < dkl∀i,j,k,l.\nNote that the l1 distances in H can be seen as a function of the dot product of the codewords ‖xi − xj‖1 = −(x ixj > )+l\n2 ,\nwhere x ∈ {−1,+1} . Therefore, instead of directly requiring H to match D, we can equivalently require the product XX> to match D [54]. This implies that we can cast the problem of finding X into a Matrix Factorization problem, where we find an X so that the matrix of inner products XX> is closest to D under a given norm."
    }, {
      "heading" : "3.4 Error-Correcting Factorization",
      "text" : "This section describes the objective function and the optimization strategy for the ECF algorithm."
    }, {
      "heading" : "3.4.1 Objective",
      "text" : "Our goal is to find an ECOC coding matrix that encodes the properties denoted by the design matrix D. In this sense, ECF seeks a factorization of the design matrix D ∈ Rk×k into a discrete ECOC matrix X. This factorization is formulated as the quadratic form XX> that reconstructs D with minimal Frobenius distance under several constraints, as shown in\nEquation (1) 4.\nminimize X\n‖D−XX>‖2F (1)\nsubject to X ∈ {−1,+1}k×l (2) XX> −P ≤ 0 (3) X>X− 1(l − 1) ≤ 0 (4) −X>X− 1(l − 1) ≤ 0 (5)\nThe component X∗ ∈ {−1,+1}k×l that solves this optimization problem generates the inner product of discrete vectors that is closest to D under the Frobenius norm. In order for X to be a valid matrix under the ECOC framework we constraint X in Equations (2)-(5). Equation (2) ensures that each binary problem classes will belong to one of the two possible metaclasses. In addition, to avoid the case of having two or more equivalent rows in X, the constraints in 3 ensure that the correlation between rows of X less or equal than a certain user-defined matrix −1l ≤ P ≤ 1l (recall that 1 denotes a matrix or vector of all 1s of the appropriate size when used), where P encodes the minimum distance between any pair of codewords. P is a symmetric matrix with pii = l ∀i. Thus, by setting the off diagonal values in P we can control the minimum inter-class correction capability. Hence, if we want the correction capability of rows xi and xj to be b c−1\n2 c, we set\npi = pj = 1(l − c). Finally, constraints in Equations (4) and (5) ensure the induced binary problems are not equivalent. Similar constraints have been studied thoroughly in literature [16], [36], [25] defining methods that rely on diversity measures for binary problems to obtain a coding matrix X. Equations (4) and (5) can be considered as soft-constraints since its violation does not imply violating the ECOC properties in terms of row distance. This is easy to show since a coding matrix X ∈ {−1,+1}k×l that induces some equivalent binary problems but ensures that XX> ≤ 1(l− 1), ∀i, j : i 6= j will define a matrix whose rows are unequivocally defined. In this sense, a coding matrix X can be easily projected on the set defined by constraints (4) and (5) by eliminating repeated columns, X = xj : xj 6= xi∀j 6= i. Thus, constraints in 4 and 5 ensure that uncorrelated binary sub-problems will be defined in our coding matrix X. The discrete constraint in Equation 2 on the variable elevates the optimization problem to the NP-Hard class. To overcome this issue and following [13], [58], [8] we relax the discrete constraint in 2 an replace it by X ∈ [−1,+1]k×l in Equation 7."
    }, {
      "heading" : "3.4.2 Optimization",
      "text" : "In this section, we detail the process for optimizing X. The minimization problem posed in Equation (1) with the relaxation of the boolean constraint in Equation (2) is non-convex, thus, X∗ is not guaranteed to be a global minimum. In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15]. Coordinate Descent techniques have been widely applied in Nonnegative Matrix Factorization obtaining satisfying results in terms of efficiency [34], [31]. In addition, it has\n4. Recall that the l1 distance is a function of the dot product ‖xi − xj‖1 = −(x ixj > )+l\n2 .\nbeen proved that if each of the coordinate sub-problems can be solved exactly, Coordinate Descent converges to a stationary point [29], [53]. Using this result, we decouple the problem in Equation (1) into a set of linear least-squares problems (one for each coordinate). Therefore, if the problem in Equation (1) is going to be minimized along the i−th coordinate of X, we fix all rows of X except of xi and we substitute X\nwith [ xi\nX′i\n] in Equations (1) and (3), where X′i denotes matrix\nX after removing the i−th row. In addition, we substitute D with [ l di\ndi T D′i′i\n] , where D′i′i denotes the matrix D after re-\nmoving the i−th row and column. Equivalently, we substitute\nP =\n[ l pi\npi T P′i′i\n] , obtaining the following block decomposition:\nminimize xi\n∥∥∥∥∥ [ l di di T D′i′i ] − [ xixi T X′ixi X′ixi T X′iX′i > ]∥∥∥∥∥ 2\nF\n(6)\nsubject to xi ∈ [−1,+1]l (7)[ xixi T X′ixi\nX′ixi T X′iX′i >\n] − [ l pi\npi T P′i′i\n] ≤ 0. (8)\nAnalyzing the block decomposition in Equation (6) we can see that the only terms involving free variables are xixi>, X′ixi and X′ixi>. Thus, since D and XX> are symmetric by definition, the minimizer xi∗ of Equation (6) is the solution to the linear least-squares problem shown in Equation (9):\nminimize xi\n∥∥∥X′ixi − di∥∥∥2 2\n(9)\nsubject to −1 ≤ xi ≤ +1 (10) X′ixi − pi ≤ 0, (11)\nwhere constraint (10) is the relaxation of the discrete constraint (2). In addition, constraint (11) ensures the correlation of xi with the rest of the rows of X is below a certain value pi. Algorithm 1 shows the complete optimization process.\nAlgorithm 1: Error-Correcting Factorization Algorithm.\nData: D̃ ∈ Rk×k,P ∈ Nk×k, l Result: X ∈ {−1,+1}k×l begin\nrepeat foreach i ∈ {1, 2, . . . , k} do\nxi ← minimize xi∈Rl\n∥∥X′ixi − di∥∥2 2 , subject to :\n−1 ≤ xi ≤ +1, X′ixi − pi ≤ 0; X← -suboptimal(X); X = {xj : xj 6= xi∀j 6= i}; // Projection step to remove duplicate columns\nuntil convergence;\nTo solve the minimization problem in Algorithm 1 we use the Active Set method described in [26], which finds an initial feasible solution by first solving a linear programming problem. Once ECF converges to a solution X∗ with objective value fobj(X∗) we obtain a discretized -suboptimal solution X ∈ {−1,+1} with objective value fobj(X) by sampling 1000 points that split the interval [−1,+1] and choosing the point that minimizes ‖fobj(X∗)−fobj(X)‖2. Finally, we discard\nrepeated columns if any appear 5."
    }, {
      "heading" : "3.5 Connections to Singular Value Decomposition, Nearest Correlation Matrix and Discrete Basis problems",
      "text" : "Similar objective functions to the one defined in the ECF problem in Equation (1) are found in other contexts, for example, in the Singular Value Decomposition problem (SVD). The SVD uses the same objective function as ECF subjected to the constraint XX> = I. However, the solution of SVD yields an orthogonal basis, disagreeing with the objective defined in Equation (1) which ensures different correlations between the xi’s. In addition, we can also find a common ground with the Nearest Correlation Matrix (NMC) Problem [32], [9], [39]. However, the NMC solution does not yield a discrete factor X, instead it seeks directly for the Gramian XX> where X is not discrete, as in Equation (12).\nminimize X\n‖X−D‖2F (12)\nsubject to X 0 (13) cXc> = b (14)\nIn addition, the ECF has similarities with the Discrete Basis Problem (DBP) [42], since the factors are X discrete valued. Nevertheless, DBP factorizes D ∈ {0, 1}k×k instead of D ∈ Rk×k, as show in Equation (15).\nminimize X,Y\n‖X ◦Y −D‖1 (15)\nsubject to X,Y,D ∈ {0, 1} (16)"
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "In this section we discuss how to ensure that the design matrix D is valid, as well as how to automatically estimate the code length for each problem given D. Furthermore, we analyze the convergence of ECF in relation to the order of updating the coordinates. Finally we show that under certain conditions of D ECF converges to a solution with almost negligible objective value."
    }, {
      "heading" : "4.1 Ensuring a representable design matrix",
      "text" : "An alternative interpretation for ECF is that it seeks for a discrete matrix X whose Gramian is closest to D under the Frobenius norm. However, since D can be directly set by the user we need to guarantee that D is a correlation matrix that is realizable in the Rk×k space, that is, D has to be symmetric and positive semi-definite. In particular, we would like to find the correlation matrix D̃ ∈ Rk×k that is closest to D under the Frobenius norm. This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach. However, for this particular case in addition to be in the Positive Semidefinite (PSD) Cone and symmetric we also require D to be scaled in the [−l,+l] range, with δ̃ii = l∀i. In this sense, to find D̃ we follow an alternating projections algorithm, similar as [32], which is shown in Algorithm 2. We first project D into the PSD cone by computing its eigenvectors and recovering D = V diag(λ+)V\n>, where λ+ are the non-negative eigenvalues of D. Then, we scale D in the range [−l,+l] and set δii = l∀i.\n5. In all our runs of ECF this situation happened with a chance of less than 10−5%.\nAlgorithm 2: Projecting D into the PSD cone with additional constraints.\nData: D ∈ Rk×k Result: D̃ ∈ Rk×k begin\nrepeat D← V diag(λ+)V>; D← D ∈ [−l,+l]k×k; D← dii = l∀i; until convergence;"
    }, {
      "heading" : "4.2 Defining a code length with representation guarantees",
      "text" : "The definition of a problem-dependent ECOC code length l, that is, choosing the number of binary partitions for a given multi-class task is a problem that has been overlooked in literature. For example, predefined coding designs like One vs. All or One vs. One have fixed code length. On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively. These values are arbitrary and unjustified. Additionally, to build a Dense or Sparse Random ECOC matrix one has to generate a set of 1000 matrices and chose the one that maximizes min(H). Consider the Dense Random Coding design, of length l = d10 log2(k)e, the ECOC matrix will have in the best case a correction capability of b 10−1\n2 c = 4, independently of the distribution of the multi-class data. In addition, the effect of maximizing min(H) leads to an equi-distribution of the correction capability over the classes. Other approaches, like Spectral ECOC [57] search for the code length by looking at the best performance on a validation set. Nevertheless, recent works have shown that the code length can be reduced to of l = log2(k) with very small loss in performance if the ECOC coding design is carefully chosen [38] and classifiers are strong. In this paper, instead of fixing the code length or optimizing it on a validation subset, we derive the optimal length according to matrix rank properties. Consider the rank of a factorization of D into XX>, there are three different possibilities:\n1) If rank(XX>) = rank(D), we obtain rank factorization algorithm that should be able to factorize D with minimal error. 2) In the case when rank(XX>) < rank(D) we obtain a low-rank factorization method that cannot guarantee to represent D with 0 error, but reconstructs the components of D with higher information. 3) If rank(XX>) > rank(D), the system is overdetermined and many possible solutions exist.\nIn general we would like to reconstruct D with minimal error, and since rank(X) ≤ min(k, l) and k (the number of classes) is fixed, we only have to set the number of columns of X to control the rank. Hence, by setting rank(X) = l = rank(D), ECF will be able to factorize D with minimal error. Figure 4 shows visual results for the ECF method applied on the Traffic and ARFace datasets. Note how, for the Traffic (36 classes) and ARFaces (50 classes) datasets the required code length for ECF to full rank factorization is l = 6 and l = 8, respectively as shown in Figures 4(e)(f)."
    }, {
      "heading" : "4.3 Order of Coordinate Updates",
      "text" : "Coordinate Descent has been applied in a wide span of problems obtaining satisfying results. However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33]. In particular, [44] derives a convergence rate which is faster when coordinates are chosen uniformly at random rather than on a cyclic fashion. Hence, choosing coordinates at random its a suitable choice when the problem shows some of the following characteristics [47]:\n• Not all data is available at all times. • A randomized strategy is able to avoid worst-case order\nof coordinates, and hence might be preferable. • Recent efforts suggest that randomization can improve the\nconvergence rate [44].\nHowever, the structure of ECF is different and calls for a different analysis. In particular, we remark the following points. (i) At each coordinate update of ECF, information about the rest of coordinates is available. (ii) Since our coordinate updates are solved uniquely, a repetition on a coordinate update does not change the objective function. (iii) The descent on the objective value when updating a coordinate is maximal when all other coordinates have been updated. These reasons leads us to choose a cyclic update scheme for ECF. In addition in Figure 5 we show a couple of examples in which the cyclic\norder of coordinates converges faster than the random order for two problems: Vowel and ARFace (refer to Section 5 for further information on the datasets). This behavior is common for all datasets. In particular, note how the cyclic order of coordinates reduces the standard deviation on the objective function, which is denoted by the narrower blue shaded area in Figure 5."
    }, {
      "heading" : "4.4 Approximation Errors and Convergence results when",
      "text" : ""
    }, {
      "heading" : "D is an inner product of binary data",
      "text" : "The optimization problem posed by ECF in Equation (1) is nonconvex due to the quadratic term XX>, even if the discrete constraint is relaxed. This implies that we cannot guarantee that the algorithm converges to the global optima. Recall that ECF seeks for the term XX> that is closest to D under the Frobenius norm. Hence, the error in the approximation can be measured by ‖X∗X∗>−D‖2F ≥ 0, where X∗ is the local optimal point to which ECF converges. In this sense, we introduce DB which is the matrix of inner products of discrete vectors that is closest to D under the Frobenious norm. Thus, we expand the norm as in the following equation:\n‖X∗X∗> −D‖2F = ‖X∗X∗> −DB + DB −D‖2F = (17) = ‖X∗X∗> −DB‖2F + ‖D−DB‖2F − (18) −2 tr((X∗X∗> −DB)(D−DB)). (19)\n• The optimization error εo: measured as the distance between the local optimum where ECF converges and DB\ndenoted by εo = ‖X∗X∗> −DB‖2F , which is expressed as the first term in Equation (18). • The discretization error εd: computed as, εd = ‖D−DB‖2F , that is, the distance between D and the closest inner product of discrete vectors DB , expressed as the second term in Equation (18).\nIn order to better understand how ECF works we analyze both components separately. Then, to analyze if ECF converges to a good solution in terms of Frobenius norm we set εd = 0 by generating a matrix D = DB which is the inner product matrix of random discrete vectors, and thus, all the terms except of ‖X∗X∗> −DB‖2F are zero. By doing that, we can empirically observe the magnitude of the optimization error εo. In order to do that we run ECF 30 times on 100 different DB matrices\nof different sizes and calculate the average ε̄o. Figure 6 shows examples for different DG matrices of size 10× 10, 100× 100, and 500 × 500. In Figure 6 we can see how ECF converges to a solution with almost negligible optimization error after 15 iterations. In fact, the average objective value for all 3000 runs of ECF on different DB ’s after 15 update cycles (coordinate updates for all xi’s) is ε̄o < 10−10. This implies, that ECF converges in average to a point with almost negligible objective value, and when applied to D’s which are not computed from binary components the main source of the approximation error is the discretization error εd. Since ECF seeks to find a discrete decomposition of D this discretization error is unavoidable, and as we have seen empirically, ECF converges in average to a solution with almost negligible objective value."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "In this section we present the experimental results of the proposed Error-Correcting Factorization method. In order to do so, we first present the data, methods and settings."
    }, {
      "heading" : "5.1 Data",
      "text" : "The proposed Error-Correcting Factorization method was applied to a total of 8 datasets. In order to provide a deep analysis and understanding of the method, we synthetically generated a Toy problem consisting of k = 14 classes, where each class contained 100 two dimensional points sampled from a Gaussian distribution with same standard deviation but different means. Figure 6(d) shows the synthetic multi-class generated data, where each color corresponds to a different category. We selected 5 well-known UCI datasets: Glass, Segmentation, Ecoli, Yeast and Vowel that range in complexity and number of classes. Finally, we apply the classification methodology in two challenging computer vision categorization problems. First, we test the methods in a real traffic sign categorization problem consisting of 36 traffic sign classes. Second, 50 classes\nfrom the ARFaces [41] dataset are classified using the present methodology. These datasets are public upon request to the authors. Table 1 shows the characteristics of the different datasets. •Traffic sign categorization: We test ECF on a real traffic sign categorization problem, of 36 classes [10]. The dataset contains a total of 3481 samples of size 32×32, filtered using the Weickert anisotropic filter, masked to exclude the background pixels, and equalized to prevent the effects of illumination changes. These feature vectors are then projected into a 100 feature vector by means of PCA. A visual sample is show in Figure 7(a). •ARFaces classification: The ARFace database [41] is composed of 26 face images from 126 different subjects (from which 50 are selected), portraying different expressions and complements. An example is shown in Figure 7(b)."
    }, {
      "heading" : "5.2 Methods and settings",
      "text" : "We compared the proposed Error-Correcting Factorization method, with the standard predefined One vs. All (OVA) and One vs. One (OVO) approaches [48], [52]. In addition, we introduce two random designs for ECOC matrices. In the first one, we generated random ECOC coding matrices fixing the general correction capability to a certain value (RAND). In the second, we generate a Dense Random coding matrix [3] (DENSE). These comparisons enable us to analyze the effect of reorganizing the inter-class correcting capabilities of an ECOC matrix. Finally, in order to compare our proposal with state-of-the-art methods, we also used the Spectral ECOC (SECOC) method [57] and the Relaxed Hierarchy [23] (R-H) . Finally we propose two different flavors of ECF, ECF-H and ECF-E. In ECF-H we compute the design matrix D in order to allocate the correction capabilities on those classes that are hard to discriminate. On the other hand, for ECF-E we compute D allocating correction to those classes that are easy to discriminate. D is computed as the Mahalanobis distance between each pair of classes. Although, there exist a number of approaches to define D from data [23], [58], [57], i.e. the margin between each pair of classes (after training a One vs. One SVM classifier), we experimentally observed that the Mahalanobis distance provides good generalization and leverages the computational cost of training a One vs. One SVM classifier. All the reported classification accuracies are the mean of a stratified 5−fold cross-validation on the aforementioned datasets. For all methods we used an SVM classifier with RBF kernel. The parameters C and γ were tunned by cross-validation on a\nvalidation subset of the data using an inner 2−fold crossvalidation. The parameter C was tunned on a grid-search on a log sampling in the range [0, 1010], and the γ parameter was equivalently tuned on a equidistant linear sampling in the range [0, 1], we used the libsvm implementation available at [12]. For both ECF-H and ECF-E we run the factorization forcing different minimum distance between classes by setting P ∈ 1 · {1, 3, 5, 7, 10} . For the Relaxed Hierarchy method [23] we used values for ρ ∈ {0.1, 0.4, 0.7, 0.9}. In all the compared methods that use a decoding function (e.g all tested methods but the one in [23]) we used both the Hamming Decoding (HD) and the Loss-Weighted decoding (LWD) [46]."
    }, {
      "heading" : "5.3 Experimental Results",
      "text" : "In Figure 8 we show the multi-class classification accuracy as a function of the relative computational complexity for all datasets using both Hamming decoding (HD) and LossWeighted Decoding (LWD). We used non-linear SVM classifiers and we define the relative computational complexity as the number of unique Support Vectors (SVs) yielded for each method, as in [23]. For visualization purposes we use an exponential scale and normalize the number of SVs by the maximum number of SVs obtained by a method in that particular dataset. In addition, although the code length cannot be considered as an accurate measure of complexity when using non-linear classifiers in the feature space, it is the only measure of complexity that is available prior to learning the binary problems and designing the coding matrix. In this sense, we show in Figure 9 the classification results for all datasets as a function of the code length l, using both Hamming decoding (HD) and Loss-Weighted Decoding (LWD). Figures 8 and 9 and show how the proposed ECF-H obtains in most of the cases better performance than state-of-the-art approaches even with reduced computational complexity. In addition, in most datasets the ECF-H is able to boost the boundaries of those classes prone to error, the effect of this is that it attains higher classification accuracies than the rest of methods paying the prize of an small increase on the relative computational complexity. Specifically, we can see how on Glass dataset, Vowel, Yeast, Segmentation and Traffic datasets (Figs. 8(e)-(f) and 9(e)-(f), respectively), the proposed method outperforms the rest of the approaches while yielding a comparable or even lower computational complexity, independently of the decoding function used. We also can see that the RAND and ECF-E methods present erratic behaviours. This is expected for the random coding design, since incrementing the number of SVs or dichotomies does not imply an increase in performance if the dichotomies are not carefully selected. On the other hand, the reason why ECF-E is not stable is not completely straightforward. ECF-E focus its design in dichotomies that are very easy to learn, allocating correction to those classes that are separable. We hypothesize that when these dichotomies become harder (there exists a limited number of easy separable partitions) to learn the addition of a difficult dichotomy harms the performance by adding confusion to previously learned dichotomies until proper error-correction is allocated. On the other hand, we can see how ECF-H usually shows a more stable behaviour since it focuses on categories that are prone to be confused. In this sense, we expect that the addition of dichotomies will increase the correction. Finally, it is worth noting that the Spectral ECOC method yields a code length of l = k − 1, corresponding to the full eigendecomposition.\nOur proposal defines coding matrices which ensure to follow the design denoted by D, fulfilling ECOC properties.\nAs a summary, we show in Figure 10 a comparison in terms of classification accuracy for different methods over all datasets. We compare the classification accuracy of a selected method for both decodings (at different operating complexities if available) versus the best performing method in a range of ±5% of the operative complexity. For consistency we show the comparison using both the number of SVs and the number of dichotomies as the computational complexity. If the compared method dominates in most of the datasets it will be found above the diagonal. In Figures 10(a) and 10(d) we compare ECF-H with the best performant of the rest of the methods and see that ECF-H outperforms the rest of the methods 62%−70% of the times depending on the complexity measure. This implies that ECF-H dominates most of the methods in terms of performance by focusing on those classes that are more prone to error regardless of the complexity measure used (number of SVs or number of dichotomies). In addition, when repeating the comparison for ECF-E in Figures 10(b) and 10(e) we see that the majority of the datasets are clearly below the diagonal (ECF-E is the most suitable choice 10%−17% of times). Finally, Figures 10(c) and 10(f) show the comparison for OVA, which is a standard method often defended by its simplicity [48]. We clearly see how it never outperforms any method and it is not the recommended choice for almost any dataset. In Table 2 we show the percentage of wins for all methods6, in increasing order of complexity averaged over all datasets. Note how, ECFH denoted by H in the table although being the third less complex method outperforms by far the rest of the methods with an improvement of at least 12% − 20% in the worst case. In conclusion, the experimental results show that ECF-H yields ECOC coding matrices which obtain comparable or even better results than state-of-the-art methods with similar relative complexity. Furthermore, by a allowing a small increase in the computational complexity when compared to state-of-theart methods, ECF is able to obtain better classification results by boosting the boundaries of classes that are prone to be confused."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "We presented the Error-Correcting Factorization method for multi-class learning which is based on the Error-Correcting Output Codes framework. The proposed method factorizes a design matrix of desired correction properties into a discrete Error-Correcting component consistent with the design matrix. ECF is a general method for building an ECOC multi-class classifier with desired properties, which can be either directly\n6. The R-H method [23] is far less complex than the compared methods, however we compare it to the to the closest operating complexity for each of the rest of the methods.\nset by the user or obtained from data using a priori inter-class distances. We note that the proposed approach is not a replacement for ECOC codings, but a generalized framework to build ECOC matrices that follow a certain error-correcting criterion design. The Error-Correcting Factorization is formulated as a minimization problem which is optimized using a constrained Coordinate Descent, where the minimizer of each coordinate is the solution to a least-squares problem with box and linear constraints that can be efficiently solved. By analyzing the approximation error, we empirically show that although ECF is a non-convex optimization problem, the optimization is very efficient. We performed experiments using ECF to build ECOC matrices following the common trend in state-of-theart works, in which the design matrix priorized the most separable classes. In addition, we hypothesized and showed that a more beneficial situation is to allocate the correction capability of the ECOC to those categories which are more prone to confusion. Experiments show that when ECF is used to allocate the correction capabilities to those classes which are prone to confusion we obtain higher accuracies than state of the art methods with efficient models in terms of the number of Support Vectors and dichotomies.\nFinally, there still exists open questions that require a deeper analysis for future work. The results obtained raise a fair doubt regarding the right allocation of error correcting power in several methods found in literature where ECOC designs are based on the premise of boosting the classes which are easily\nseparable. In the light of these results, we may conjecture that a careful allocation of error correction must be made in such a way that balances two aspects: on one hand, simple to classify boundaries must be handled properly. On the other hand, the error correction must be allocated on difficult classes for the ensemble to correct possible mistakes. In addition, it would be interesting to study which are the parameters that affect the suitability of the no class is left behind and the hard classes are left behind one. Finally we could consider ternary matrices and further regularizations."
    } ],
    "references" : [ {
      "title" : "Generalized non-metric multidimensional scaling",
      "author" : [ "Sameer Agarwal", "Josh Wills", "Lawrence Cayton", "Gert Lanckriet", "David J Kriegman", "Serge Belongie" ],
      "venue" : "In ICAIS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Reducing multiclass to binary: A unifying approach for margin classifiers",
      "author" : [ "E. Allwein", "R. Schapire", "Y. Singer" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Reducing multiclass to binary: A unifying approach for margin classifiers",
      "author" : [ "Erin L. Allwein", "Robert E. Schapire", "Yoram Singer", "Pack Kaelbling" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2000
    }, {
      "title" : "On the design of an ecoc-compliant genetic algorithm",
      "author" : [ "Miguel Ángel Bautista", "Sergio Escalera", "Xavier Bar", "Oriol Pujol" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Introducing the separability matrix for error correcting output codes coding",
      "author" : [ "Miguel Bautista", "Oriol Pujol", "Xavier Baró", "Sergio Escalera" ],
      "venue" : "MCS, pages 227–236,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Image classification using random forests and ferns",
      "author" : [ "Anna Bosch", "Andrew Zisserman", "Xavier Muoz" ],
      "venue" : "ICCV",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "A training algorithm for optimal margin classifiers",
      "author" : [ "Bernhard E Boser", "Isabelle M Guyon", "Vladimir N Vapnik" ],
      "venue" : "In COLT,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1992
    }, {
      "title" : "Least-squares covariance matrix adjustment",
      "author" : [ "Stephen Boyd", "Lin Xiao" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "On the accuracy and performance of the GeoMobil system",
      "author" : [ "J. Casacuberta", "J. Miranda", "M. Pla", "S. Sanchez", "A.Serra", "J.Talaya" ],
      "venue" : "In International Society for Photogrammetry and Remote Sensing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Robust euclidean embedding",
      "author" : [ "Lawrence Cayton", "Sanjoy Dasgupta" ],
      "venue" : "In ICML,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Improved output coding for classification using continuous relaxation",
      "author" : [ "Koby Crammer", "Yoram Singer" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "Koby Crammer", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "A least-squares framework for component analysis",
      "author" : [ "Fernando De la Torre" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Solving multiclass learning problems via error-correcting output codes",
      "author" : [ "T. Dietterich", "G. Bakiri" ],
      "venue" : "In Journal of Artificial Intelligence Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1995
    }, {
      "title" : "On the decoding process in ternary error-correcting output codes",
      "author" : [ "S. Escalera", "O. Pujol", "P.Radeva" ],
      "venue" : "Transactions in Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Subclass problem-dependent design for errorcorrecting output codes",
      "author" : [ "Sergio Escalera", "David MJ Tax", "Oriol Pujol", "Petia Radeva", "Robert PW Duin" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Describing objects by their attributes",
      "author" : [ "Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E Schapire" ],
      "venue" : "In COLT,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1995
    }, {
      "title" : "Regularization paths for generalized linear models via coordinate descent",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Rob Tibshirani" ],
      "venue" : "Journal of statistical software,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "An optimal global nearest neighbor metric",
      "author" : [ "Keinosuke Fukunaga", "Thomas E Flick" ],
      "venue" : "Pattern Anaylsis and Machine Intelligence, Transactions on,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1984
    }, {
      "title" : "Discriminative learning of relaxed hierarchy for large-scale visual recognition",
      "author" : [ "Tianshi Gao", "Daphne Koller" ],
      "venue" : "In Computer Vision (ICCV),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Multiclass boosting with hinge loss based on output coding",
      "author" : [ "Tianshi Gao", "Daphne Koller" ],
      "venue" : "In ICML,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Evolving output codes for multiclass problems",
      "author" : [ "N. Garcia-Pedrajas", "C. Fyfe" ],
      "venue" : "Evolutionary Computation, IEEE Transactions on,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Numerical linear algebra and optimization",
      "author" : [ "Phil Gill" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "Euclidean embedding of co-occurrence data",
      "author" : [ "Amir Globerson", "Gal Chechik", "Fernando Pereira", "Naftali Tishby" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2007
    }, {
      "title" : "Learning and using taxonomies for fast visual categorization",
      "author" : [ "Gregory Griffin", "Pietro Perona" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2008
    }, {
      "title" : "On the convergence of the block nonlinear gaussseidel method under convex constraints",
      "author" : [ "L. Grippo", "M. Sciandrone" ],
      "venue" : "Operations Research Letters,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2000
    }, {
      "title" : "Application of a multilayer decision tree in computer recognition of chinese characters",
      "author" : [ "YX Gu", "Qing Ren Wang", "Ching Y Suen" ],
      "venue" : "Pattern Anaylsis and Machine Intelligence, Transactions on,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1983
    }, {
      "title" : "Nenmf: an optimal gradient method for nonnegative matrix factorization",
      "author" : [ "Naiyang Guan", "Dacheng Tao", "Zhigang Luo", "Bo Yuan" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2012
    }, {
      "title" : "Computing the nearest correlation matrixa problem from finance",
      "author" : [ "Nicholas J Higham" ],
      "venue" : "IMA journal of Numerical Analysis,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2002
    }, {
      "title" : "Fast coordinate descent methods with variable selection for non-negative matrix factorization",
      "author" : [ "Cho-Jui Hsieh", "Inderjit S Dhillon" ],
      "venue" : "In ACM SIGKDD,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method",
      "author" : [ "Hyunsoo Kim", "Haesun Park" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2008
    }, {
      "title" : "Nonmetric multidimensional scaling: a numerical method",
      "author" : [ "Joseph B Kruskal" ],
      "venue" : "Psychometrika, 29(2):115–129,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1964
    }, {
      "title" : "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy",
      "author" : [ "Ludmila I Kuncheva", "Christopher J Whitaker" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2003
    }, {
      "title" : "Projected gradient methods for nonnegative matrix factorization",
      "author" : [ "Chih-Jen Lin" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2007
    }, {
      "title" : "Evolutionary design of multiclass support vector machines",
      "author" : [ "Ana C. Lorena", "André C.P.L.F. Carvalho" ],
      "venue" : "Journal of Intelligent Fuzzy Systems,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2007
    }, {
      "title" : "A dual approach to semidefinite least-squares problems",
      "author" : [ "Jérôme Malick" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2004
    }, {
      "title" : "Constructing category hierarchies for visual recognition",
      "author" : [ "Marcin Marszalek", "Cordelia Schmid" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2008
    }, {
      "title" : "The AR face database",
      "author" : [ "A. Martinez", "R. Benavente" ],
      "venue" : "In Computer Vision Center Technical Report",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1998
    }, {
      "title" : "The discrete basis problem. Knowledge and Data Engineering",
      "author" : [ "Pauli Miettinen", "Taneli Mielikainen", "Aristides Gionis", "Gautam Das", "Heikki Mannila" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2008
    }, {
      "title" : "A theory of multiclass boosting",
      "author" : [ "Indraneel Mukherjee", "Robert E Schapire" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2013
    }, {
      "title" : "Efficiency of coordinate descent methods on hugescale optimization problems",
      "author" : [ "Yu Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2012
    }, {
      "title" : "Relative attributes",
      "author" : [ "Devi Parikh", "Kristen Grauman" ],
      "venue" : "In ICCV, pages 503–510",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2011
    }, {
      "title" : "Discriminant ECOC: A heuristic method for application dependent design of error correcting output codes",
      "author" : [ "O. Pujol", "P. Radeva", "J. Vitrià" ],
      "venue" : "In Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2006
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "Peter Richtárik", "Martin Takávc" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2014
    }, {
      "title" : "In defense of one-vs-all classification",
      "author" : [ "Ryan Rifkin", "Aldebaro Klautau" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2004
    }, {
      "title" : "Methods for binary multidimensional scaling",
      "author" : [ "Douglas LT Rohde" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2002
    }, {
      "title" : "Multiclass boosting: Theory and algorithms",
      "author" : [ "Mohammad J Saberian", "Nuno Vasconcelos" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2011
    }, {
      "title" : "Using output codes to boost multiclass learning problems",
      "author" : [ "Robert E Schapire" ],
      "venue" : "In ICML,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1997
    }, {
      "title" : "Convergence of a block coordinate descent method for nondifferentiable minimization",
      "author" : [ "Paul Tseng" ],
      "venue" : "Journal of optimization theory and applications,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2001
    }, {
      "title" : "Multidimensional spectral hashing",
      "author" : [ "Yair Weiss", "Rob Fergus", "Antonio Torralba" ],
      "venue" : "ECCV",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2012
    }, {
      "title" : "Support vector machines for multi-class pattern recognition",
      "author" : [ "Jason Weston", "Chris Watkins" ],
      "venue" : "In ESANN,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 1999
    }, {
      "title" : "Designing category-level attributes for discriminative visual recognition",
      "author" : [ "Felix X Yu", "Liangliang Cao", "Rogerio S Feris", "John R Smith", "Shih-Fu Chang" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2013
    }, {
      "title" : "Spectral error correcting output codes for efficient multiclass recognition",
      "author" : [ "Xiao Zhang", "Lin Liang", "Heung-Yeung Shum" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2009
    }, {
      "title" : "Sparse output coding for large-scale visual recognition",
      "author" : [ "Bin Zhao", "Eric P Xing" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2013
    }, {
      "title" : "Adaptive errorcorrecting output codes. In IJCAI, pages 1932–1938",
      "author" : [ "Guoqiang Zhong", "Mohamed Cheriet" ],
      "venue" : null,
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2013
    }, {
      "title" : "Joint learning of error-correcting output codes and dichotomizers from data",
      "author" : [ "Guoqiang Zhong", "Kaizhu Huang", "Cheng-Lin Liu" ],
      "venue" : "Neural Computing and Applications,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2012
    }, {
      "title" : "Decoding design based on posterior probabilities in ternary error-correcting output codes",
      "author" : [ "Jin Deng Zhou", "Xiao Dan Wang", "Hong Jian Zhou", "Jie Ming Zhang", "Ning Jia" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 46,
      "context" : "All [48] or Random [2] approaches ignore the data distribution, thus not taking profit of allocating the error-correcting capabilities of ECOCs in a problemdependent fashion.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "All [48] or Random [2] approaches ignore the data distribution, thus not taking profit of allocating the error-correcting capabilities of ECOCs in a problemdependent fashion.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 20,
      "context" : "e Nearest Neighbours [22], Decision Trees [30], Random Forests [6]).",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 28,
      "context" : "e Nearest Neighbours [22], Decision Trees [30], Random Forests [6]).",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "e Nearest Neighbours [22], Decision Trees [30], Random Forests [6]).",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "One of the most well know approaches are the extensions of SVMs [7] to the multi-class case.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 52,
      "context" : "For instance, the work of Weston and Watkins [55] presents a single-machine extension of the SVM method to cope with the multi-class case, in which k predictor functions are trained, constrained with k−1 slack variables per sample.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "However, a more recent adaptation of [14] reduces the number of constraints per samples to one, paying only for the second largest classification score among the k predictors.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 46,
      "context" : "Despite these efforts, single-machine approaches to estimate f scale poorly with the number of classes and are often outperformed by simple decompositions [48], [52].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 18,
      "context" : "In recent years various works that extended the classical Adaptive Boosting method [20] to the multi-class setting have been presented [51], [43].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 49,
      "context" : "In recent years various works that extended the classical Adaptive Boosting method [20] to the multi-class setting have been presented [51], [43].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 41,
      "context" : "In recent years various works that extended the classical Adaptive Boosting method [20] to the multi-class setting have been presented [51], [43].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 48,
      "context" : "Furthermore, the work of Saberian and Vasconcenlos [50] presents a derivation of a new margin loss function for multi-class classification altogether with the set of real class codewords that maximize the presented multi-class margin, yielding boundaries with max margin.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 46,
      "context" : "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 16,
      "context" : "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 44,
      "context" : "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 38,
      "context" : "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 26,
      "context" : "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 46,
      "context" : "All [48] are those that use a predefined problem partition scheme followed by a committee strategy to aggregate the binary classifier outputs.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 21,
      "context" : "On the other hand, hierarchical classification relies on a similarity metric distance among classes to build a binary tree in which nodes correspond to different problem partitions [23], [40], [28].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 38,
      "context" : "On the other hand, hierarchical classification relies on a similarity metric distance among classes to build a binary tree in which nodes correspond to different problem partitions [23], [40], [28].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 26,
      "context" : "On the other hand, hierarchical classification relies on a similarity metric distance among classes to build a binary tree in which nodes correspond to different problem partitions [23], [40], [28].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 14,
      "context" : "Finally, the ECOC framework consists of two steps: In the coding step, a set of binary partitions of the original problem are encoded in a matrix of discrete codewords [16] (univocally defined, one code per class) (see Figure 2).",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : "At the decoding step a final decision is obtained by comparing the test codeword resulting of the union of the binary classifier responses with every class codeword and choosing the class codeword at minimum distance [17], [61].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 58,
      "context" : "At the decoding step a final decision is obtained by comparing the test codeword resulting of the union of the binary classifier responses with every class codeword and choosing the class codeword at minimum distance [17], [61].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 46,
      "context" : "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 44,
      "context" : "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 54,
      "context" : "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 22,
      "context" : "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 55,
      "context" : "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : "In [2], the authors propose the Dense and Sparse Random coding designs with a fixed code length of {10, 15} log2(K), respectively.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "In [2] the authors encourage to generate a set of 10 random matrices and select the one that maximizes the minimum distance between rows, thus showing the highest correction capability.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 54,
      "context" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 55,
      "context" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 57,
      "context" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 56,
      "context" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 44,
      "context" : "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 54,
      "context" : "In that sense, [57] computes a spectral decomposition of the graph laplacian associated to the multi-class problem.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 22,
      "context" : "In [24], Gao and Koller propose a method which adaptively learns an ECOC coding by optimizing a novel multi-class hinge loss function sequentially.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "On an update of their earlier work, Gao and Koller propose in [23] a joint optimization process to learn a hierarchy of classifiers in which each node corresponds to a binary subproblem that is optimized to find easily separable subproblems.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 55,
      "context" : "al [58] proposes a dual projected gradient method embedded on a constrained concave-convex procedure to optimize an objective composed of a measure of expected problem separability, codeword correlation and regularization terms.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "2(a) boost the boundaries of classes that are prone to be confused, while other approaches that use equal or higher number of classifiers like Dense Random [2] in Fig.",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 14,
      "context" : "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 1,
      "context" : "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 58,
      "context" : "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 1,
      "context" : "However, [2] introduced a third value, defining ternary valued coding matrices.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 15,
      "context" : "Nevertheless, it is not until the work of [17] that decoding functions took into account the meaning of the 0 value at the decoding step.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 34,
      "context" : "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 54,
      "context" : "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 34,
      "context" : "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 54,
      "context" : "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 21,
      "context" : "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 23,
      "context" : "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "However, since H expresses the hamming distance between rows of X, one can alternatively express the correction capability in a pair-wise fashion [5], allowing for a deeper understanding of how correction is distributed among codewords.",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 54,
      "context" : "However, recent works [57], [23], [58] have focused on designing a matrix X where binary problems are easily separable.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "However, recent works [57], [23], [58] have focused on designing a matrix X where binary problems are easily separable.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 55,
      "context" : "However, recent works [57], [23], [58] have focused on designing a matrix X where binary problems are easily separable.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 51,
      "context" : "Therefore, instead of directly requiring H to match D, we can equivalently require the product XX> to match D [54].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "Similar constraints have been studied thoroughly in literature [16], [36], [25] defining methods that rely on diversity measures for binary problems to obtain a coding matrix X.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 34,
      "context" : "Similar constraints have been studied thoroughly in literature [16], [36], [25] defining methods that rely on diversity measures for binary problems to obtain a coding matrix X.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "Similar constraints have been studied thoroughly in literature [16], [36], [25] defining methods that rely on diversity measures for binary problems to obtain a coding matrix X.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "To overcome this issue and following [13], [58], [8] we relax the discrete constraint in 2 an replace it by X ∈ [−1,+1]k×l in Equation 7.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 55,
      "context" : "To overcome this issue and following [13], [58], [8] we relax the discrete constraint in 2 an replace it by X ∈ [−1,+1]k×l in Equation 7.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 47,
      "context" : "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 33,
      "context" : "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 35,
      "context" : "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].",
      "startOffset" : 321,
      "endOffset" : 325
    }, {
      "referenceID" : 13,
      "context" : "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].",
      "startOffset" : 327,
      "endOffset" : 331
    }, {
      "referenceID" : 32,
      "context" : "Coordinate Descent techniques have been widely applied in Nonnegative Matrix Factorization obtaining satisfying results in terms of efficiency [34], [31].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 29,
      "context" : "Coordinate Descent techniques have been widely applied in Nonnegative Matrix Factorization obtaining satisfying results in terms of efficiency [34], [31].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 27,
      "context" : "been proved that if each of the coordinate sub-problems can be solved exactly, Coordinate Descent converges to a stationary point [29], [53].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 50,
      "context" : "been proved that if each of the coordinate sub-problems can be solved exactly, Coordinate Descent converges to a stationary point [29], [53].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : "To solve the minimization problem in Algorithm 1 we use the Active Set method described in [26], which finds an initial feasible solution by first solving a linear programming problem.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 30,
      "context" : "In addition, we can also find a common ground with the Nearest Correlation Matrix (NMC) Problem [32], [9], [39].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "In addition, we can also find a common ground with the Nearest Correlation Matrix (NMC) Problem [32], [9], [39].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 37,
      "context" : "In addition, we can also find a common ground with the Nearest Correlation Matrix (NMC) Problem [32], [9], [39].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 40,
      "context" : "In addition, the ECF has similarities with the Discrete Basis Problem (DBP) [42], since the factors are X discrete valued.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "In this sense, to find D̃ we follow an alternating projections algorithm, similar as [32], which is shown in Algorithm 2.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 54,
      "context" : "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 55,
      "context" : "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 54,
      "context" : "Other approaches, like Spectral ECOC [57] search for the code length by looking at the best performance on a validation set.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 36,
      "context" : "Nevertheless, recent works have shown that the code length can be reduced to of l = log2(k) with very small loss in performance if the ECOC coding design is carefully chosen [38] and classifiers are strong.",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 45,
      "context" : "However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 50,
      "context" : "However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 31,
      "context" : "However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 42,
      "context" : "In particular, [44] derives a convergence rate which is faster when coordinates are chosen uniformly at random rather than on a cyclic fashion.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 45,
      "context" : "Hence, choosing coordinates at random its a suitable choice when the problem shows some of the following characteristics [47]:",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 42,
      "context" : "• Recent efforts suggest that randomization can improve the convergence rate [44].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 39,
      "context" : "from the ARFaces [41] dataset are classified using the present methodology.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "•Traffic sign categorization: We test ECF on a real traffic sign categorization problem, of 36 classes [10].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 39,
      "context" : "•ARFaces classification: The ARFace database [41] is composed of 26 face images from 126 different subjects (from which 50 are selected), portraying different expressions and complements.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 46,
      "context" : "One (OVO) approaches [48], [52].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "In the second, we generate a Dense Random coding matrix [3] (DENSE).",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 54,
      "context" : "Finally, in order to compare our proposal with state-of-the-art methods, we also used the Spectral ECOC (SECOC) method [57] and the Relaxed Hierarchy [23] (R-H) .",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 21,
      "context" : "Finally, in order to compare our proposal with state-of-the-art methods, we also used the Spectral ECOC (SECOC) method [57] and the Relaxed Hierarchy [23] (R-H) .",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 21,
      "context" : "Although, there exist a number of approaches to define D from data [23], [58], [57], i.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 55,
      "context" : "Although, there exist a number of approaches to define D from data [23], [58], [57], i.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 54,
      "context" : "Although, there exist a number of approaches to define D from data [23], [58], [57], i.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "The parameter C was tunned on a grid-search on a log sampling in the range [0, 10], and the γ parameter was equivalently tuned on a equidistant linear sampling in the range [0, 1], we used the libsvm implementation available at [12].",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "The parameter C was tunned on a grid-search on a log sampling in the range [0, 10], and the γ parameter was equivalently tuned on a equidistant linear sampling in the range [0, 1], we used the libsvm implementation available at [12].",
      "startOffset" : 173,
      "endOffset" : 179
    }, {
      "referenceID" : 10,
      "context" : "The parameter C was tunned on a grid-search on a log sampling in the range [0, 10], and the γ parameter was equivalently tuned on a equidistant linear sampling in the range [0, 1], we used the libsvm implementation available at [12].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 21,
      "context" : "For the Relaxed Hierarchy method [23] we used values for ρ ∈ {0.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "g all tested methods but the one in [23]) we used both the Hamming Decoding (HD) and the Loss-Weighted decoding (LWD) [46].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 44,
      "context" : "g all tested methods but the one in [23]) we used both the Hamming Decoding (HD) and the Loss-Weighted decoding (LWD) [46].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "We used non-linear SVM classifiers and we define the relative computational complexity as the number of unique Support Vectors (SVs) yielded for each method, as in [23].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 46,
      "context" : "Finally, Figures 10(c) and 10(f) show the comparison for OVA, which is a standard method often defended by its simplicity [48].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : "The R-H method [23] is far less complex than the compared methods, however we compare it to the to the closest operating complexity for each of the rest of the methods.",
      "startOffset" : 15,
      "endOffset" : 19
    } ],
    "year" : 2015,
    "abstractText" : "Error Correcting Output Codes (ECOC) is a successful technique in multi-class classification, which is a core problem in Pattern Recognition and Machine Learning. A major advantage of ECOC over other methods is that the multi-class problem is decoupled into a set of binary problems that are solved independently. However, literature defines a general error-correcting capability for ECOCs without analyzing how it distributes among classes, hindering a deeper analysis of pairwise error-correction. To address these limitations this paper proposes an Error-Correcting Factorization (ECF) method, our contribution is three fold: (I) We propose a novel representation of the error-correction capability, called the design matrix, that enables us to build an ECOC on the basis of allocating correction to pairs of classes. (II) We derive the optimal code length of an ECOC using rank properties of the design matrix. (III) ECF is formulated as a discrete optimization problem, and a relaxed solution is found using an efficient constrained block coordinate descent approach. (IV) Enabled by the flexibility introduced with the design matrix we propose to allocate the error-correction on classes that are prone to confusion. Experimental results in several databases show that when allocating the error-correction to confusable classes ECF outperforms state-of-the-art approaches.",
    "creator" : "LaTeX with hyperref package"
  }
}