{
  "name" : "1512.00408.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reinforcement Learning Applied to an Electric Water Heater: From Theory to Practice",
    "authors" : [ "F. Ruelens", "B. J. Claessens", "S. Quaiyum", "B. De Schutter", "R. Belmans" ],
    "emails" : [ "(frederik.ruelens@esat.kuleuven.be).", "(bert.claessens@vito.be)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Auto-encoder network, demand response, electric water heater, fitted Q-iteration, reinforcement learning.\nI. INTRODUCTION\nTHE share of renewable energy sources is expected toreach 25% of the global power generation portfolio by 2020 [1]. The intermittent and stochastic nature of most renewable energy sources, however, makes it challenging to integrate these sources into the power grid. Successful integration of these sources requires flexibility on the demand side through demand response programs. Demand response programs enable end users with flexible loads to adapt their consumption profile in response to an external signal. A prominent example of flexible loads are electric water heaters with a hot water storage tank [2], [3]. These loads have the ability to store energy in their water buffer without impacting the comfort level of the end user. In addition to having significant flexibility, electric water heaters can consume about 2 MWh per year for a household with a daily hot water demand of 100 liters [4]. As a result, electric water heaters are a prime candidate for residential demand response programs.\nF. Ruelens and R. Belmans are with the Department of Electrical Engineering, KU Leuven/EnergyVille, Belgium (frederik.ruelens@esat.kuleuven.be).\nB. J. Claessens is with the Energy Department of Vito/EnergyVille, Belgium (bert.claessens@vito.be).\nS. Quaiyum is with the Department of Electrical Engineering, Uppsala University, Sweden.\nB. De Schutter and R. Babuška are with the Delft Center for Systems and Control, Delft University of Technology, The Netherlands.\nPreviously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8]. Amongst others, two prominent control paradigms in the demand response literature on electric water heaters are model-based approaches and reinforcement learning.\nPerhaps the most researched control paradigm applied to demand response are model-based approaches, such as Model Predictive Control (MPC) [7], [9], [8]. Most MPC strategies use a gray-box model, based on general expert knowledge of the underlying system dynamics, requiring a system identification step. Given this mathematical model, an optimal control action can be found by solving a receding horizon problem [10]. In general, the implementation of MPC consists of several critical steps, namely, selecting accurate models, estimating the model parameters, estimating the state of the system, and forecasting of the exogenous variables. All these steps make MPC an expensive technique, the cost of which needs to be balanced out by the possible financial gains [11]. Moreover, possible model errors resulting from an inaccurate model or forecast, can effect the stability of the MPC controller [12], [13].\nIn contrast to MPC, Reinforcement Learning (RL) techniques [14] do not require expert knowledge and consider their environment as a black-box. RL techniques enable an agent to learn a control policy by interacting with its environment, without the need to use modeling and system identification techniques. In [15], Ernst et al. state that the trade-off in applying MPC and RL mainly depends on the quality of the expert knowledge about the system dynamics that could be exploited in the context of system identification. In most residential demand response applications, however, expert knowledge about the system dynamics or future disturbances might be unavailable or might be too expensive to obtain relative to the expected financial gain. In this context, RL techniques are an excellent candidate to build a general purpose agent that can be applied to any demand response application.\nThis paper proposes a learning agent that minimizes the cost of energy consumption of an electric water heater. The agent measures the state of its environment through a set of sensors that are connected along the water buffer of the electric water heater. However, learning in a high-dimension state space can significantly impact the learning rate of the RL algorithm. This is known as the “curse of dimensionality”. A popular approach to mitigate its effects is to reduce the dimensionality of the state space during a pre-processing step. Inspired by the work of [16], this paper applies an autoencoder network to reduce the dimensionality of the state\nar X\niv :1\n51 2.\n00 40\n8v 1\n[ cs\n.L G\n] 2\n9 N\nov 2\n01 5\nvector. By so doing, this paper makes following contributions: (1) we demonstrate how a well-established RL technique, fitted Q-iteration, can be combined with an auto-encoder network to minimize the cost of energy consumption of an electric water heater; (2) in a simulation-based experiment, we assess the performance of different state representations and batch sizes; (3) we successfully apply an RL agent to an electric water heater in the lab (Fig. 1).\nThe remainder of this paper is organized as follows. Section II gives a non-exhaustive literature overview of RL related to demand response. Section III maps the considered demand response problem to a Markov decision process. Section IV describes how an auto-encoder network can be used to find a low-dimensional state representation, followed by a description of the fitted Q-iteration algorithm in Section V. Section VI presents the results of the simulation-based experiments and Section VII presents the results of the lab experiment. Finally, Section VIII draws conclusions and discusses future work."
    }, {
      "heading" : "II. REINFORCEMENT LEARNING",
      "text" : "This section gives a non-exhaustive overview of recent developments related to Reinforcement Learning (RL) and demand response. Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20]. After each interaction with the environment, Q-learning uses temporal difference learning [14] to update its state-action value function or Qfunction. A major drawback of Q-learning is that the given observation is discarded after each update. As a result, more interactions are needed to spread already known information through the state space. This inefficient use of information limits the application of Q-learning to real-world applications.\nIn contrast to Q-learning, batch RL techniques [21], [22], [23] are more data efficient, since they store and reuse past interactions. As a result, batch RL techniques require less interactions with their environment, which makes them more practical for real-world applications, such as demand response. Perhaps the most popular batch RL technique which has been applied to a wide range of applications [16], [24], [25] is fitted Q-iteration developed by Ernst et al. [21]. Fitted Q-iteration iteratively estimates the Q-function given a fixed batch of past interactions. An online version that uses a neural network, neural fitted Q-iteration, has been proposed by Riedmiller et al. in [22]. Finally, an interesting alternative is to combine experience replay to an incremental RL technique such as Qlearning or SARSA [26]. In [27], the authors demonstrate how fitted Q-iteration can be used to control a cluster of electric water heaters. The results indicate that fitted Q-iteration was able to reduce the cost of energy consumption of a cluster of 100 electric water heaters after a learning period of 40 days. In addition, [28] shows how fitted Q-iteration can be extended to reduce the cost of energy consumption of a heat-pump thermostat given that a forecast of the outside temperature is provided.\nA promising alternative to the previously mentioned modelfree techniques are model-based or model-assisted RL techniques. For example, the authors of [29] present a model-based\npolicy search method that learns a Gaussian process to model uncertainties. In addition, inspired by [30], the authors of [31] demonstrate how a model-assisted batch RL technique can be applied to control a building heating system."
    }, {
      "heading" : "III. PROBLEM FORMULATION",
      "text" : "The aim of this work is to develop a controller or agent that minimizes the cost of energy consumption of an electric water heater, given an external price profile. This price profile is provided to the agent at the start of each day. The agent can measure the temperature of the water buffer through a set of temperature sensors that are connected along the hull of the buffer tank. Following the approach presented in [28], the electric water heater is equipped with a backup controller that overrules the control action from the agent when the safety or comfort constraints of the end user are violated. A challenge in developing such an agent is that the dynamics of the electric water heater, the future hot water demand and the settings of the backup controller are unknown to the agent. To overcome this challenge, this paper leverages on the previous work of [16], [21], [28] and applies techniques from RL."
    }, {
      "heading" : "A. Markov decision process framework",
      "text" : "To apply RL, this paper formulates the underlying sequential decision-making problem of the learning agent as a Markov decision process formulation. The Markov decision process formulation is defined by its d-dimensional state space X ⊂ Rd, its action space U ⊂ R, its stochastic discrete-time transition function f , and its cost function ρ. The optimization horizon is considered finite, comprising T ∈ N \\ {0} steps, where at each discrete time step k, the state evolves following:\nxk+1 = f(xk, uk,wk) ∀k ∈ {1, ..., T − 1}, (1)\nwhere a random disturbance wk is drawn from a conditional distribution pW(·|xk), uk ∈ U is the control action and xk ∈ X the state. Associated with each state transition, a cost ck is given by:\nck = ρ(xk, uk,wk) ∀k ∈ {1, ..., T}. (2)\nThe goal of the learning agent is to find an optimal control policy h∗ : X → U that minimizes the expected T -stage return for any state in the state space. The expected T -stage return JhT starting from x1 and following a policy h is defined as follows:\nJhT (x1) = E pW(·|xk) [ T∑ k=1 ρ(xk, h(xk),wk) ] , (3)\nwhere E denotes the expectation operator over all possible stochastic realizations.\nA more convenient way to characterize a policy is by using a state-action value function or Q-function:\nQh(x, u) = E pW(·|x)\n[ ρ(x, u,w) + JhT (f(x, h(x),w)) ] , (4)\nwhich indicates the cumulative return starting from state x and by taking action u and following h thereafter.\nThe optimal Q-function corresponds the best Q-function that can be obtained by any policy:\nQ∗(x, u) = min h Qh(x, u). (5)\nStarting from an optimal Q-function for every state-action pair, the optimal policy h∗ is calculated as follows:\nh∗(x) ∈ arg min u∈U Q∗(x, u), (6)\nwhere Q∗ satisfies the Bellman optimality equation [32]:\nQ∗(x, u) = E w∼pW(·|x)\n[ ρ(x, u,w) + min\nu′∈U Q∗(f(x, u,w), u′)\n] .\n(7)\nFollowing the notation introduced in [28], the next three paragraphs give a description of the state, the action, and the cost function tailored to an electric water heater."
    }, {
      "heading" : "B. Observable state vector",
      "text" : "The observable state vector of an electric water heater contains a time-related component and a controllable component. The time-related component xt describes the part of the state related to timing, which is relevant for the dynamics of the system. Specifically, the tap water demand of the end user is considered to have diurnal and weekly patterns. As such, the time-related component contains the day of the week and the quarter in the day. The controllable component xph represents physical state information that is measured locally and is influenced by the control action. The controllable component contains the temperature measurements of the ns sensors that are connected along the hull of the storage tank. The observable state vector is given by:\nxk = ( d, t︸︷︷︸ xtk , T 1k , . . . , T i k, . . . , T ns k︸ ︷︷ ︸\nxphk\n), (8)\nwhere d ∈ {1, . . . , 7} is the current day of the week, t ∈ {1, . . . , 96} is the quarter in the day and T ik denotes the temperature measurements of sensor i at time step k."
    }, {
      "heading" : "C. Control action",
      "text" : "The learning agent can control the heating element of the electric water heater with a binary control action uk ∈ {0, 1}, where 0 indicates off and 1 on. However, the backup mechanism, which enacts the comfort and safety constraints of the end user, can overrule this control action of the learning agent. The function B : X × U → Uph maps the control action uk ∈ U to a physical action uphk ∈ Uph according to:\nuphk = B(xk, uk,θ), (9)\nwhere the vector θ defines the safety and user-defined comfort settings of the backup controller. In order to have a generic approach we assume that the logic of the backup controller is unknown to the learning agent. However, the learning agent can measure the physical action uphk enforced by the backup controller (see Fig. 2), which is required to calculate the cost.\nThe logic of the backup controller of the electric water heater is defined as:\nB(x, u,θ) =  P e if xsoc(x,θ) ≤ xsoc(θ)\nuP e if xsoc(θ) < xsoc(x,θ) < x̄soc(θ), 0 if xsoc(x,θ) ≥ x̄soc(θ)\n(10)\nwhere P e is the electrical power rating of the heating element, xsoc(x,θ) is the current state of charge and xsoc(θ) and x̄soc(θ) are the upper and lower bounds for the state of charge. A detailed description of how the state of charge is calculated can by found in [3]."
    }, {
      "heading" : "D. Cost function",
      "text" : "At the start of each optimization period T∆t, the learning agent receives a price vector λ = {λk}Tk=1 for the next T time steps. At each time step, the agent receives a cost ck according to:\nck = u ph k λk∆t, (11)\nwhere λk is the electricity price during time step k, and ∆t the length of one control period."
    }, {
      "heading" : "IV. BATCH OF FOUR-TUPLES",
      "text" : "Generally, batch RL techniques estimate the Q-function based on a batch of four-tuples (xl, ul,x′l, cl). This paper, however, considers the following batch of four-tuples:\nF = {(xl, ul,x′l, u ph l ), l = 1, . . . ,#F}, (12)\nwhere for each l, the next state x′l, and the physical action u ph l have been obtained as a result of taking control action ul in the state xl. Note that, F does not include the observed cost cl, since the cost depends on the price vector that is provided to the learning agent at the start of each day.\nAs defined by (8), xl contains all temperature measurements of the sensors connected to the hull of the water buffer. Learning in a high-dimensional state space requires more observations from the environment to estimate the Q-function, as more tuples are needed to cover the state-action space. This is known as the “curse of dimensionality”. This curse becomes even more pronounced in practical applications,\nAlgorithm 1 Fitted Q-iteration [21] using feature vectors. Require: R = {\n(zl, ul, z ′ l, u ph l ), l = 1, . . . ,#R } , {λt}Tt=1\n1: initialize Q̂0 to zero 2: for N = 1, . . . , T do 3: for l = 1, . . . ,#R do 4: cl ← uphl λt B where t is to the quarter in the day of the time-related component xtl = (d, t) of state zl 5: QN,l ← cl + min\nu∈U Q̂N−1(z\n′ l, u)\n6: end for 7: use a regression technique to obtain Q̂N from Treg = {( (zl, ul) , QN,l ) , l = 1, . . . ,#R } .\n8: end for Ensure: Q̂∗ = Q̂T\nwhere each observation corresponds to a “real” interation with the environment.\nA pre-processing step can be used to find a compact and more efficient representation of the state space and can help to converge to a good policy much faster [33]. A popular technique to find a compact representation is to use a handcrafted feature vector based on insights of the considered control problem [34]. Alternative approaches that do not require prior knowledge are unsupervised feature learning algorithms, such as auto-encoders [16] or a principal component analysis [33].\nAs illustrated in Fig. 2, this paper demonstrates how an auto-encoder can be used to find a compact representation of the sensory input data. An auto-encoder network is a neural network that maps its output back to its input. By selecting a lower number of neurons in the middle hidden layer than in the input layer p < d, the auto-encoder can be used to reduce the dimensionality of the input data.\nThe reduced feature vector zl ∈ Z ⊂ Rp can be calculated as follows:\nzl = (x t l,Φenc(x ph l ,w, b)), (13)\nwhere w and b denote the weights and the biases that connect the input layer with the middle hidden layer of the autoencoder network. The function Φenc : X → Z is an encoder function and maps the observed state vector xl to the feature vector zl. To train the weights of the auto-encoder, a conjugate gradient descent algorithm is used as presented in [35].\nIn the next section, fitted Q-iteration is used to find a policy h : Z → U that maps every feature vector to a control action using batch R:\nR = {(zl, ul, z′l, u ph l ), l = 1, . . . ,#R}, (14)\nwhich consists of feature vectors with a dimensionality p. Since we apply the auto-encoder on the input data of the supervised learning algorithm, we assume that all input data is equally important. As such, it is possible that we ignore low-variance yet potentially useful components during the learning process. A possible route of future work would be to add a regularization term to the regression algorithm of the supervised learning algorithm to prevent the risk of overfitting without the risk of ignoring potentially important data."
    }, {
      "heading" : "V. FITTED Q-ITERATION",
      "text" : "This section describes the learning algorithm and the exploration strategy of the agent based on the batch of feature vectors R presented in the previous section."
    }, {
      "heading" : "A. Fitted Q-iteration",
      "text" : "Fitted Q-iteration iteratively builds a training set Treg with all state-action pairs (z, u) in R as the input. The target values consist of the corresponding Q-values, based on the approximation of the Q-function of the previous iteration. In the first iteration (N = 1), the Q-values approximate the expected cost (line 5 in Algorithm 1). In the subsequent iterations, Q-values are updated using the Q-function of the previous iteration. As a result, Algorithm 1 needs T iterations until the Q-function contains all information about the future costs. Note that, the cost corresponding to each tuple is recalculated using price vector λ that is provided at the start of the day (line 4 in Algorithm 1). As a result, the algorithm can reuse past experiences to find a control policy for the next day. Following [21], Algorithm 1 applies an ensemble of extremely randomized trees as a regression algorithm to estimate the Qfunction. An empirical study of the accuracy and convergence properties of extremely randomized trees can be found in [21]. However, in principle, any regression algorithm, such as neural networks [23], [24], can be used to estimate the Q-function."
    }, {
      "heading" : "B. Boltzmann exploration",
      "text" : "During the day, the learning agent uses a Boltzmann exploration strategy [36] and selects an action with the following probability:\nP (u|z) = e Q̂∗(z,u)/τd\nΣu′∈UeQ̂ ∗(z,u′)/τd\n, (15)\nwhere τd is the Boltzmann temperature at day d, Q̂∗ is the Qfunction from Algorithm 1 and z is the current feature vector measured by the learning agent. If τd → 0, the exploration will decrease and the policy will become greedier. Thus by starting with a high τd the exploration starts completely random,\nhowever as τd decreases the policy directs itself to the most interesting state-action pairs. In the evaluation experiments, Q̂∗ in (15) is linearly scaled between [0, 100] and the τ1 is set to 100 at the start of the experiment, which will result in an equal probability for all actions. The Boltzmann temperature is updated as follows τd = τd−1 − ∆τ , which increases the probability of selecting higher valued actions."
    }, {
      "heading" : "VI. SIMULATION-BASED RESULTS",
      "text" : "This section describes the results of the simulation-based experiments, which use a non-linear stratified tank model with 50 temperature layers. A detailed description of the stratified tank model can be found in [3]. The specifications of the electric water heater are chosen in correspondence with the electric water heater used during the lab experiment (see Section VII). The simulated electric water heater has a power rating of 2.36kW and has a water buffer of 200 liter. The experiments use realistic hot water profiles with a mean daily consumption of 120 liter [37] and use price information from the Belgian day-ahead [38] and balancing market [39]. The learning agent can measure the temperature of the 50 temperature layers obtained with the simulation model.\nThe aim of the first simulation-based experiment is to find a compact state representation using an auto-encoder network and to assess the impact of the state representation on the performance of fitted Q-iteration. The second simulation-based experiment compares the result of fitted Q-iteration with the default thermostat controller."
    }, {
      "heading" : "A. Step 1: feature selection",
      "text" : "This experiment compares the performance of fitted Qiteration combined with different feature representations for different fixed batch sizes. An auto-encoder (AE) network\nthat reduces the original sensory input vector (50 temperature sensors) to 5 dimensions is denoted by AE 5. The simulations are repeated for 100 simulation days. The average cost of energy consumption of these 100 simulations is depicted in Fig. 3. As can be seen in Fig. 3, the performance of fitted Qiteration combined with a specific state representation depends on the number of tuples in the batch. For example for a batch size of 10 days, AE 3 results in a lower cost than AE 15, while after 75 days, AE 15 will result in a lower cost than AE 3. In addition, as can be seen from Fig. 3, AE 1 resulted in a relatively bad policy, independent of the batch size.\nIn general, it can be concluded that for a batch of limited size, fitted Q-iteration with a low-dimensional feature vector will outperform fitted Q-iteration using the full state information, i.e. 50 temperature measurements. By learning in a low-dimensional state space, it is possible to learn with a smaller and more efficient representation. As a result, the agent requires less observations to converge to a better control policy than when the full state information is used. In addition, as more observations will result in a more efficient coverage of the state-action space, it can be seen from Fig. 3 that the result of fitted Q-iteration with the full state improves significantly as the batch size increases. In the following subsection, we present the results of AE 5 in more detail. A method for selecting an appropriate feature representation during the learning process will be part of our future work (Section VIII)."
    }, {
      "heading" : "B. Step 2: evaluation",
      "text" : "Fig. 4 compares the total cost of energy consumption using fitted Q-iteration combined with AE 5 against the default thermostat controller for two relevant price profiles, i.e. dayahead prices (top plot) and imbalance prices (bottom plot). The default thermostat controller enables the heating element\nwhen the state-of-charge drops below its minimum threshold and stays enabled until the state-of-charge reaches 100%. Note that in contrast to the learning agent, the default controller is agnostic about the price profile.\nThe experiment starts with an empty batch and the tuples of the current day are added to the given batch at the end of each day. At the start of each day, the auto-encoder is trained to find a batch of compact feature vectors, which are then used by fitted Q-iteration to estimate the Q-function for the next day. Online, the learning agent uses a Boltzmann exploration strategy with ∆τ set to 10, which results in 10 days of exploration.\nThe results of the experiment indicate that fitted Q-iteration was able to reduce the total cost of energy consumption by 24% for the day-head prices and by 34% for the imbalance prices compared to the default strategy. Note that the imbalance prices are generally more volatile than the day-ahead prices, as they reflect real-time imbalances due to forecasting errors of renewable energy sources, such as wind and solar, which were not foreseen in the day-ahead market.\nThe temperature profiles of the simulation layers and power profiles of a “mature” learning agent (batch size of 100 days) for the day-ahead and imbalance prices are depicted in Fig. 5 and Fig. 6. It can be seen in the bottom plot of both figures that the mature learning agent reduces the cost of energy consumption by consuming energy during low price moments."
    }, {
      "heading" : "VII. LAB RESULTS",
      "text" : "The aim of our lab experiment is to demonstrate that fitted Q-iteration can be successfully applied to minimize the cost of energy consumption of a real-world electric water heater."
    }, {
      "heading" : "A. Lab setup",
      "text" : "The setup used in the lab experiment was part of a pilot project on residential demand response in Belgium [40], where\na cluster of 10 electric water heaters was used for direct load control. Fig. 1 shows the electric water heater used during the lab experiment. The electric water heater is a standard unit that was equipped with eight temperature sensors and a controllable power relay. A controllable valve connected to the outlet of the buffer tank is used to simulate the hot water demand of a household with a mean daily flow volume of 120 liter [37]. An Arduino prototyping platform with a JSON/RPC 2.0 interface is used to communicate with a computer in the lab1, which runs the learning agent that uses fitted Qiteration. Fitted Q-iteration is implemented in Python and Scikit-learn [41] is used to estimate the Q-function, using an ensemble of extremely randomized trees [21].\nSimilar as in the previous simulation-based experiments, it is assumed that the learning agent is provided with a deterministic external price profile for the following day. The learning agent uses a Boltzmann exploration strategy with ∆τ set to 10, which results in 10 days of exploration. In order to compare the performance of the lab experiment with the performance of the simulation-based experiments, we used both day-ahead prices and imbalance prices."
    }, {
      "heading" : "B. Evaluation",
      "text" : "The performance of the learning agent was evaluated using different feature vectors as presented in Section VI. The best performance, however, was obtained by including the eight temperature measurements in the observable state vector.\nUsing identical price profiles and tap water demands, Fig. 7 and Fig. 8 show the temperature measurements and the power profiles of the mature learning agent using imbalance prices and day-ahead prices. As can be seen, the learning agent was able to successfully minimize the cost of energy consumption by consuming during low price moments.\n1Intel Core i5 - 4GB Memory\nFig. 9 depicts the experimental results, spanning 40 days, of fitted Q-iteration and the default thermostat controller. The top plot of this figure indicates the cumulative costs of energy consumption and the bottom plot indicates the daily costs of energy consumption. After 40 days, fitted Q-iteration was able to reduce the cost of energy consumption by 15% compared to the default thermostat controller. Furthermore, by excluding the first ten exploration days, this reduction increases to 28%."
    }, {
      "heading" : "VIII. CONCLUSIONS AND FUTURE WORK",
      "text" : "This paper has demonstrated how an auto-encoder network can be used in combination with a well-established batch reinforcement learning algorithm, called fitted Q-iteration, to reduce the cost of energy consumption of an electric water heater. The auto-encoder network was used to find a compact representation of the state vector. In a series of simulation-based experiments using an electric water heater with 50 temperature sensors, the proposed method was able to converge to good policies much faster than when using the full state information. Compared to a default thermostat controller, the presented approach has reduced the cost of energy consumption by 24% using day-ahead prices and by 34% using imbalance prices.\nIn a lab experiment, fitted Q-iteration has been successfully applied to an electric water heater with eight temperature sensors. A reduction of the state vector did not improve the performance of fitted Q-iteration. Compared to the thermostat controller, fitted Q-iteration was able to reduce the total cost of energy consumption by 15% within 40 days of operation.\nBased on the results of both experiments the following four conclusions can be drawn: (1) learning in a compact feature space can improve the quality of the control policy when the number of observations is relatively small (25 days); (2) when the number of observations increases it is advisable to switch to higher state-space representation; (3) when only a limited number of temperature sensors is available, i.e. 1-10 sensors, it\nis recommended to use the full state vector; (4) when applied to a real-world stetting, fitted Q-iteration was able to obtain good control policies within 20 days of operation.\nIn our future research, we aim at developing a method for selecting an appropriate state representation during the learning process. A promising route is to construct experts, where each expert combines a learning algorithm with a different feature representation. A metric based on the performance of each expert, as presented in [42], could then be used to select the expert with the highest metric as described in [43]."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "The authors would like to thank Davy Geysen, Geert Jacobs, Koen Vanthournout, and Jef Verbeeck from Vito for providing us with the lab setup. This work was supported by a Ph.D.\ngrant of the Institute for the Promotion of Innovation through Science and Technology in Flanders (IWT-Vlaanderen) and by Stable MultI-agent LEarnIng for neTworks (SMILE-IT)."
    } ],
    "references" : [ {
      "title" : "World Energy Outlook 2013: Renewable Energy Outlook, An annual report released by the International Energy Agency",
      "author" : [ "F. Birol" ],
      "venue" : "http://www.worldenergyoutlook.org/media/weowebsite/ 2013, Paris, France, [Online: accessed July 21, 2015].",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Ten years of operating experience with a remote controlled water heater load management system at detroit edison",
      "author" : [ "B. Hastings" ],
      "venue" : "IEEE Trans. on Power Apparatus and Syst., no. 4, pp. 1437–1441, 1980.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "A smart domestic hot water buffer",
      "author" : [ "K. Vanthournout", "R. D’hulst", "D. Geysen", "G. Jacobs" ],
      "venue" : "IEEE Trans. on Smart Grid, vol. 3, no. 4, pp. 2121–2127, Dec. 2012.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Energy cost calculator for electric and gas water heaters",
      "author" : [ "U.S. Department of Energy" ],
      "venue" : "http://energy.gov/eere/femp/ energy-cost-calculator-electric-and-gas-water-heaters-0{#}output, [Online: accessed November 10, 2015].",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Electric water heater modeling and control strategies for demand response",
      "author" : [ "R. Diao", "S. Lu", "M. Elizondo", "E. Mayhorn", "Y. Zhang", "N. Samaan" ],
      "venue" : "Proc. 2012 IEEE Power and Energy Society General Meeting,, pp. 1–8.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Distributed voltage control mechanism in low-voltage distribution grid field test",
      "author" : [ "S. Iacovella", "K. Lemkens", "F. Geth", "P. Vingerhoets", "G. Deconinck", "R. D’Hulst", "K. Vanthournout" ],
      "venue" : "Proc. 4th IEEE PES Innov. Smart Grid Technol. Conf. (ISGT Europe), Oct 2013, pp. 1–5.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Modeling and control of aggregated heterogeneous thermostatically controlled loads for ancillary services",
      "author" : [ "S. Koch", "J.L. Mathieu", "D.S. Callaway" ],
      "venue" : "Proc. 17th IEEE Power Sys. Comput. Conf. (PSCC), Stockholm, Sweden, Aug. 2011, pp. 1–7.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "State estimation and control of heterogeneous thermostatically controlled loads for load following",
      "author" : [ "J. Mathieu", "D. Callaway" ],
      "venue" : "Proc. 45th Hawaii Int. Conf. on System Science (HICSS), Maui, HI, Jan. 2012, pp. 2002–2011.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Scheduling of domestic water heater power demand for maximizing PV self-consumption using model predictive control",
      "author" : [ "F. Sossan", "A.M. Kosek", "S. Martinenas", "M. Marinelli", "H. Bindner" ],
      "venue" : "Proc. 4th IEEE PES Innov. Smart Grid Technol. Conf. (ISGT Europe), Oct 2013, pp. 1–5.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Model Predictive Control, 2nd ed",
      "author" : [ "E.F. Camacho", "C. Bordons" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Beyond theory: the challenge of implementing model predictive control in buildings",
      "author" : [ "J. Cigler", "D. Gyalistras", "J. Širokỳ", "V. Tiet", "L. Ferkl" ],
      "venue" : "Proc. 11th REHVA World Congress (CLIMA), Czech Republic, Prague, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Model predictive control for energy efficient buildings",
      "author" : [ "Y. Ma" ],
      "venue" : "Ph.D. dissertation, University of California Berkeley, Mechanical Engineering, Berkeley, CA, 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Selecting building predictive control based on model uncertainty",
      "author" : [ "M. Maasoumy", "M. Razmara", "M. Shahbakhti", "A. Sangiovanni Vincentelli" ],
      "venue" : "Proc. American Control Conference (ACC), Portland, OR, June 2014, pp. 404–411.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "Reinforcement learning versus model predictive control: a comparison on a power system problem",
      "author" : [ "D. Ernst", "M. Glavic", "F. Capitanescu", "L. Wehenkel" ],
      "venue" : "IEEE Trans. Syst., Man, Cybern., Syst., vol. 39, no. 2, pp. 517–529, 2009.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deep auto-encoder neural networks in reinforcement learning",
      "author" : [ "S. Lange", "M. Riedmiller" ],
      "venue" : "Proc. IEEE 2010 Int. Joint Conf. on Neural Networks (IJCNN), Barcelona, Spain, July 2010, pp. 1–8.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Using smart devices for system-level management and control in the smart grid: A reinforcement learning framework",
      "author" : [ "E.C. Kara", "M. Berges", "B. Krogh", "S. Kar" ],
      "venue" : "Proc. 3rd IEEE Int. Conf. on Smart Grid Commun. (SmartGridComm), Tainan, Taiwan, Nov. 2012, pp. 85–90.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Evaluation of reinforcement learning control for thermal energy storage systems",
      "author" : [ "G.P. Henze", "J. Schoenmann" ],
      "venue" : "HVAC&R Research, vol. 9, no. 3, pp. 259–275, 2003.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Optimal demand response using device-based reinforcement learning",
      "author" : [ "Z. Wen", "D. O’Neill", "H. Maei" ],
      "venue" : "IEEE Trans. on Smart Grid, vol. 6, no. 5, pp. 2312–2324, Sept 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Optimal bidding of plug-in electric vehicles in a market-based control setup",
      "author" : [ "M. González", "R. Luis Briones", "G. Andersson" ],
      "venue" : "Proc. 18th IEEE Power Sys. Comput. Conf. (PSCC), Wroclaw, Poland, 2014, pp. 1–7.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Tree-based batch mode reinforcement learning",
      "author" : [ "D. Ernst", "P. Geurts", "L. Wehenkel" ],
      "venue" : "Journal of Machine Learning Research, pp. 503–556, 2005.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Neural fitted Q-iteration–first experiences with a data efficient neural reinforcement learning method",
      "author" : [ "M. Riedmiller" ],
      "venue" : "Proc. 16th European Conference on Machine Learning (ECML), vol. 3720. Porto, Portugal: Springer, Oct. 2005, p. 317.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : "Nature, vol. 518, no. 7540, pp. 529–533, 2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning for robot soccer",
      "author" : [ "M. Riedmiller", "T. Gabel", "R. Hafner", "S. Lange" ],
      "venue" : "Autonomous Robots, vol. 27, no. 1, pp. 55–73, 2009.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Variable selection for dynamic treatment regimes: a reinforcement learning approach",
      "author" : [ "R. Fonteneau", "L. Wehenkel", "D. Ernst" ],
      "venue" : "Proc. European Workshop on Reinforcement Learning (EWRL), Villeneuve d’Ascq, France, 2008.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Experience replay for realtime reinforcement learning control",
      "author" : [ "S. Adam", "L. Busoniu", "R. Babuška" ],
      "venue" : "IEEE Trans. on Syst., Man, and Cybern., Part C: Applications and Reviews, vol. 42, no. 2, pp. 201–212, 2012.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Demand response of a heterogeneous cluster of electric water heaters using batch reinforcement learning",
      "author" : [ "F. Ruelens", "B. Claessens", "S. Vandael", "S. Iacovella", "P. Vingerhoets", "R. Belmans" ],
      "venue" : "Proc. 18th IEEE Power Sys. Comput. Conf. (PSCC), Wrocław, Poland, Aug. 2014, pp. 1–8.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Residential Demand Response Applications Using Batch Reinforcement Learning",
      "author" : [ "F. Ruelens", "B. Claessens", "S. Vandael", "B. De Schutter", "R. Babuška", "R. Belmans" ],
      "venue" : "Submitted to IEEE Trans. on Smart Grid (http://arxiv.org/pdf/1504.02125.pdf), Apr. 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "PILCO: A model-based and data-efficient approach to policy search",
      "author" : [ "M. Deisenroth", "C.E. Rasmussen" ],
      "venue" : "Proceedings of the 28th International Conference on machine learning (ICML-11), 2011, pp. 465–472.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Approximate model-assisted neural fitted q-iteration",
      "author" : [ "T. Lampe", "M. Riedmiller" ],
      "venue" : "2014 International Joint Conference on Neural Networks (IJCNN), July 2014, pp. 2698–2704.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Experimental analysis of data-driven control for a building heating system",
      "author" : [ "G.T. Costanzo", "S. Iacovella", "F. Ruelens", "T. Leurs", "B. Claessens" ],
      "venue" : "CoRR, vol. abs/1507.03638, 2015. [Online]. Available: http://arxiv.org/abs/1507.03638",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Using PCA to efficiently represent state spaces",
      "author" : [ "W. Curran", "T. Brys", "M. Taylor", "W. Smart" ],
      "venue" : "The 12th European Workshop on Reinforcement Learning (EWRL 2015), Lille, France, 2015.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.W. Moore" ],
      "venue" : "Journal of Artificial Intelligence Research, pp. 237– 285, 1996.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Realistic domestic hot-water profiles in different time scales: Report for the international energy agency, solar heating and cooling task (IEA-SHC)",
      "author" : [ "U. Jordan", "K. Vajen" ],
      "venue" : "Universität Marburg, Marburg, Germany, Tech. Rep., 2001.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "LINEAR breakthrough project: Large-scale implementation of smart grid technologies in distribution grids",
      "author" : [ "B. Dupont", "P. Vingerhoets", "P. Tant", "K. Vanthournout", "W. Cardinaels", "T. De Rybel", "E. Peeters", "R. Belmans" ],
      "venue" : "Proc. 3rd IEEE PES Innov. Smart Grid Technol. Conf. (ISGT Europe), Berlin, Germany, Oct. 2012, pp. 1–8.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg" ],
      "venue" : "The Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Batch mode reinforcement learning based on the synthesis of artificial trajectories",
      "author" : [ "R. Fonteneau", "S.A. Murphy", "L. Wehenkel", "D. Ernst" ],
      "venue" : "Annals of Operations Research, vol. 208, no. 1, pp. 383–416, 2013.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Forecasting electricity consumption by aggregating specialized experts",
      "author" : [ "M. Devaine", "P. Gaillard", "Y. Goude", "G. Stoltz" ],
      "venue" : "Machine Learning, vol. 90, no. 2, pp. 231–260, 2013.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "THE share of renewable energy sources is expected to reach 25% of the global power generation portfolio by 2020 [1].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "A prominent example of flexible loads are electric water heaters with a hot water storage tank [2], [3].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "A prominent example of flexible loads are electric water heaters with a hot water storage tank [2], [3].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "In addition to having significant flexibility, electric water heaters can consume about 2 MWh per year for a household with a daily hot water demand of 100 liters [4].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "Previously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "Previously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "Previously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "Previously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "Perhaps the most researched control paradigm applied to demand response are model-based approaches, such as Model Predictive Control (MPC) [7], [9], [8].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "Perhaps the most researched control paradigm applied to demand response are model-based approaches, such as Model Predictive Control (MPC) [7], [9], [8].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "Perhaps the most researched control paradigm applied to demand response are model-based approaches, such as Model Predictive Control (MPC) [7], [9], [8].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "Given this mathematical model, an optimal control action can be found by solving a receding horizon problem [10].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "All these steps make MPC an expensive technique, the cost of which needs to be balanced out by the possible financial gains [11].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "Moreover, possible model errors resulting from an inaccurate model or forecast, can effect the stability of the MPC controller [12], [13].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "Moreover, possible model errors resulting from an inaccurate model or forecast, can effect the stability of the MPC controller [12], [13].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "In contrast to MPC, Reinforcement Learning (RL) techniques [14] do not require expert knowledge and consider their environment as a black-box.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "In [15], Ernst et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "Inspired by the work of [16], this paper applies an autoencoder network to reduce the dimensionality of the state ar X iv :1 51 2.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "After each interaction with the environment, Q-learning uses temporal difference learning [14] to update its state-action value function or Qfunction.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "In contrast to Q-learning, batch RL techniques [21], [22], [23] are more data efficient, since they store and reuse past interactions.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : "In contrast to Q-learning, batch RL techniques [21], [22], [23] are more data efficient, since they store and reuse past interactions.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "In contrast to Q-learning, batch RL techniques [21], [22], [23] are more data efficient, since they store and reuse past interactions.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "Perhaps the most popular batch RL technique which has been applied to a wide range of applications [16], [24], [25] is fitted Q-iteration developed by Ernst et al.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "Perhaps the most popular batch RL technique which has been applied to a wide range of applications [16], [24], [25] is fitted Q-iteration developed by Ernst et al.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "Perhaps the most popular batch RL technique which has been applied to a wide range of applications [16], [24], [25] is fitted Q-iteration developed by Ernst et al.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "[21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "in [22].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 25,
      "context" : "Finally, an interesting alternative is to combine experience replay to an incremental RL technique such as Qlearning or SARSA [26].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 26,
      "context" : "In [27], the authors demonstrate how fitted Q-iteration can be used to control a cluster of electric water heaters.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 27,
      "context" : "In addition, [28] shows how fitted Q-iteration can be extended to reduce the cost of energy consumption of a heat-pump thermostat given that a forecast of the outside temperature is provided.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 28,
      "context" : "For example, the authors of [29] present a model-based Fig.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : "In addition, inspired by [30], the authors of [31] demonstrate how a model-assisted batch RL technique can be applied to control a building heating system.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 30,
      "context" : "In addition, inspired by [30], the authors of [31] demonstrate how a model-assisted batch RL technique can be applied to control a building heating system.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : "Following the approach presented in [28], the electric water heater is equipped with a backup controller that overrules the control action from the agent when the safety or comfort constraints of the end user are violated.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : "To overcome this challenge, this paper leverages on the previous work of [16], [21], [28] and applies techniques from RL.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "To overcome this challenge, this paper leverages on the previous work of [16], [21], [28] and applies techniques from RL.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 27,
      "context" : "To overcome this challenge, this paper leverages on the previous work of [16], [21], [28] and applies techniques from RL.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 27,
      "context" : "Following the notation introduced in [28], the next three paragraphs give a description of the state, the action, and the cost function tailored to an electric water heater.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "A detailed description of how the state of charge is calculated can by found in [3].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "Algorithm 1 Fitted Q-iteration [21] using feature vectors.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 31,
      "context" : "A pre-processing step can be used to find a compact and more efficient representation of the state space and can help to converge to a good policy much faster [33].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 15,
      "context" : "Alternative approaches that do not require prior knowledge are unsupervised feature learning algorithms, such as auto-encoders [16] or a principal component analysis [33].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 31,
      "context" : "Alternative approaches that do not require prior knowledge are unsupervised feature learning algorithms, such as auto-encoders [16] or a principal component analysis [33].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : "Following [21], Algorithm 1 applies an ensemble of extremely randomized trees as a regression algorithm to estimate the Qfunction.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 20,
      "context" : "An empirical study of the accuracy and convergence properties of extremely randomized trees can be found in [21].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : "However, in principle, any regression algorithm, such as neural networks [23], [24], can be used to estimate the Q-function.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 23,
      "context" : "However, in principle, any regression algorithm, such as neural networks [23], [24], can be used to estimate the Q-function.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : "During the day, the learning agent uses a Boltzmann exploration strategy [36] and selects an action with the following probability:",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "A detailed description of the stratified tank model can be found in [3].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : "The experiments use realistic hot water profiles with a mean daily consumption of 120 liter [37] and use price information from the Belgian day-ahead [38] and balancing market [39].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 34,
      "context" : "The setup used in the lab experiment was part of a pilot project on residential demand response in Belgium [40], where 0 48 96 144 192 240 288 10 20 30 40 50 60 70",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 33,
      "context" : "A controllable valve connected to the outlet of the buffer tank is used to simulate the hot water demand of a household with a mean daily flow volume of 120 liter [37].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 35,
      "context" : "Fitted Q-iteration is implemented in Python and Scikit-learn [41] is used to estimate the Q-function, using an ensemble of extremely randomized trees [21].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "Fitted Q-iteration is implemented in Python and Scikit-learn [41] is used to estimate the Q-function, using an ensemble of extremely randomized trees [21].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 36,
      "context" : "A metric based on the performance of each expert, as presented in [42], could then be used to select the expert with the highest metric as described in [43].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 37,
      "context" : "A metric based on the performance of each expert, as presented in [42], could then be used to select the expert with the highest metric as described in [43].",
      "startOffset" : 152,
      "endOffset" : 156
    } ],
    "year" : 2015,
    "abstractText" : "Electric water heaters have the ability to store energy in their water buffer without impacting the comfort of the end user. This feature makes them a prime candidate for residential demand response. However, the stochastic and nonlinear dynamics of electric water heaters, makes it challenging to harness their flexibility. Driven by this challenge, this paper formulates the underlying sequential decision-making problem as a Markov decision process and uses techniques from reinforcement learning. Specifically, we apply an auto-encoder network to find a compact feature representation of the sensor measurements, which helps to mitigate the curse of dimensionality. A wellknown batch reinforcement learning technique, fitted Q-iteration, is used to find a control policy, given this feature representation. In a simulation-based experiment using an electric water heater with 50 temperature sensors, the proposed method was able to achieve good policies much faster than when using the full state information. In a lab experiment, we apply fitted Q-iteration to an electric water heater with eight temperature sensors. Further reducing the state vector did not improve the results of fitted Q-iteration. The results of the lab experiment, spanning 40 days, indicate that compared to a thermostat controller, the presented approach was able to reduce the total cost of energy consumption of the electric water heater by 15%.",
    "creator" : "LaTeX with hyperref package"
  }
}