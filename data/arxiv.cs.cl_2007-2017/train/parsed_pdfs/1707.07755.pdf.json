{
  "name" : "1707.07755.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "miguel.ballesteros@ibm.com,", "onaizan@us.ibm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 7.\n07 75\n5v 2\n[ cs\n.C L\n] 2\nA ug\n2 01\n7\nWe present a transition-based AMR parser that directly generates AMR parses from plain text. We use Stack-LSTMs to represent our parser state and make decisions greedily. In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further."
    }, {
      "heading" : "1 Introduction",
      "text" : "Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstractmeaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016).\nAMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al.,\n1Check (Banarescu et al., 2013) for a complete description of AMR graphs.\n2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia).\nInspired by Wang et al. (2015b,a); Goodman et al. (2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shiftreduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al. (2016) presented transition-based tree-to-graph transducers that traverse a dependency tree and transforms it to an AMR graph. Damonte et al. (2016)’s input is a sentence and it is therefore more similar (with a different parsing algorithm) to our approach, but their parser relies on external tools, such as dependency parsing, semantic role labeling or named entity recognition.\nThe input of our parser is plain text sentences and, through rich word representations, it predicts all actions (in a single algorithm) needed to generate an AMR graph representation for an input sentence; it handles the detection and annotation of named entities, word sense disambiguation and it makes connections between the nodes detected towards building a predicate argument structure. Even though the system that runs with just words is very competitive, we further improve the results incorporating POS tags and dependency trees into our model.\nStack-LSTMs2 have proven to be useful in tasks related to syntactic and semantic parsing (Dyer et al., 2015, 2016; Swayamdipta et al., 2016) and named entity recognition (Lample et al., 2016). In this paper, we demonstrate that they can be effectively used for AMR parsing as well.\n2We use the dynamic framework of Neubig et al. (2017) to implement our parser."
    }, {
      "heading" : "2 Parsing Algorithm",
      "text" : "Our parsing algorithm makes use of a STACK (that stores AMR nodes and/or words) and a BUFFER that contains the words that have yet to be processed. The parsing algorithm is inspired from the semantic actions presented by Henderson et al. (2013), the transition-based NER algorithm by Lample et al. (2016) and the arc-standard algorithm (Nivre, 2004). As in (Ballesteros and Nivre, 2013) the buffer starts with the root symbol at the end of the sequence. Figure 2 shows a running example. The transition inventory is the following:\n• SHIFT: pops the front of the BUFFER and push it to the STACK.\n• CONFIRM: calls a subroutine that predicts the AMR node corresponding to the top of\nthe STACK. It then pops the word from the STACK and pushes the AMR node to the STACK. An example is the prediction of a propbank sense: From occured to occur-01.\n• REDUCE: pops the top of the STACK. It occurs when the word/node at the top of the\nstack is complete (no more actions can be applied to it). Note that it can also be applied to words that do not appear in the final output graph, and thus they are directly discarded.\n• MERGE: pops the two nodes at the top of the STACK and then it merges them, it\nthen pushes the resulting node to the top of STACK. Note that this can be applied recursively. This action serves to get multiword named entities (e.g. New York City).\n• ENTITY(label): labels the node at the top of the STACK with an entity label. This action\nserves to label named entities, such as New York City or Madrid and it is normally run after MERGE when it is a multi-word named entity, or after SHIFT if it is a single-word named entity.\n• DEPENDENT(label,node): creates a new node in the AMR graph that is dependent on the\nnode at the top of the STACK. An example is the introduction of a negative polarity to a given node: From illegal to (legal, polarity -).\n• LA(label) and RA(label): create a left/right arc with the top two nodes at the top of the\nSTACK. They keep both the head and the dependent in the stack to allow reentrancies (multiple incoming edges). The head is now a composition of the head and the dependent. They are enriched with the AMR label.\n• SWAP: pops the two top items at the top of the STACK, pushes the second node to the\nfront of the BUFFER, and pushes the first one back into the STACK. This action allows nonprojective arcs as in (Nivre, 2009) but it also helps to introduce reentrancies. At oracle time, SWAP is produced when the word at the top of the stack is blocking actions that may happen between the second element at the top of the stack and any of the words in the buffer.\nFigure 1 shows the parser actions and the effect on the parser state (contents of the stack, buffer) and how the graph is changed after applying the actions.\nWe implemented an oracle that produces the sequence of actions that leads to the gold (or close to gold) AMR graph. In order to map words in the sentences to nodes in the AMR graph we need to align them. We use the JAMR aligner provided by Flanigan et al. (2014).3 It is important to mention that even though the aligner is quite accurate, it is not perfect, producing a F1 score of around 0.90. This means that most sentences have at least one alignment error which implies that our oracle is not capable of perfectly reproducing all AMR graphs. This has a direct impact on the accuracy of the parser described in the next section since it is trained on sequences of actions that are not perfect. The oracle achieves 0.895 F1 Smatch score (Cai and Knight, 2013) when it is run on the development set of the LDC2014T12.\nThe algorithm allows a set of different constraints that varies from the basic ones (not allowing impossible actions such as SHIFT when the buffer is empty or not generating arcs when the words have not yet been CONFIRMed and thus transformed to nodes) to more complicated ones based on the propbank candidates and number of arguments. We choose to constrain the parser to the basic ones and let it learn the more complicated ones.\n3We used the latest version of the aligner (Flanigan et al., 2016)"
    }, {
      "heading" : "3 Parsing Model",
      "text" : "In this section, we revisit Stack-LSTMs, our parsing model and our word representations."
    }, {
      "heading" : "3.1 Stack-LSTMs",
      "text" : "The stack LSTM is an augmented LSTM (Hochreiter and Schmidhuber, 1997; Graves, 2013) that allows adding new inputs in the same way as LSTMs but it also provides a POP operation that moves a pointer to the previous element. The output vector of the LSTM will consider the stack pointer instead of the rightmost position of\nthe sequence.4"
    }, {
      "heading" : "3.2 Representing the State and Making Parsing Decisions",
      "text" : "The state of the algorithm presented in Section 2 is represented by the contents of the STACK, BUFFER and a list with the history of actions (which are encoded as Stack-LSTMs).5 All of this forms the vector st that represents the state which s calculated as follows:\nst = max {0,W[stt;bt;at] + d} ,\nwhereW is a learned parameter matrix, d is a bias term and stt, bt,at represent the output vector of the Stack-LSTMs at time t.\nPredicting the Actions: Our model then uses the vector st for each timestep t to compute the probability of the next action as:\np(zt | st) = exp\n(\ng⊤ zt st + qzt\n)\n∑ z′∈A exp ( g⊤ z′ st + qz′\n) (1)\nwhere gz is a column vector representing the (output) embedding of the action z, and qz is a bias term for action z. The set A represents the actions listed in Section 2. Note that due to parsing constraints the set of possible actions may vary. The total number of actions (in the LDC2014T12 dataset) is 478; note that they include all possible labels (in the case of LA and RA ) and the different dependent nodes for the DEPENDENT action\n4We refer interested readers to (Dyer et al., 2015) for further details.\n5 Word representations, input and hidden representations have 100 dimensions, action and label representations are of size 20.\nPredicting the Nodes: When the model selects the action CONFIRM, the model needs to decide the AMR node6 that corresponds to the word at the top of the STACK, by using st, as follows:\np(et | st) = exp\n(\ng⊤ et st + qet\n)\n∑ e′∈N exp ( g⊤ e′ st + qe′\n) (2)\nwhere N is the set of possible candidate nodes for the word at the top of the STACK. ge is a column vector representing the (output) embedding of the node e, and qe is a bias term for the node e. It is important to mention that this implies finding a propbank sense or a lemma. For that, we rely entirely on the AMR training set instead of using additional resources.\nGiven that the system runs two softmax operations, one to predict the action to take and the second one to predict the corresponding AMR node, and they both share LSTMs to make predictions, we include an additional layer with a tanh nonlinearity after st for each softmax."
    }, {
      "heading" : "3.3 Word Representations",
      "text" : "We use character-based representations of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). They learn representations for words that are orthographically similar. Note that they are updated with the updates to the model. Ballesteros et al. (2015) and Lample et al. (2016) demonstrated that it is possible to achieve high results in syntactic parsing and named entity recognition by just using character-based word representations (not even POS tags, in fact, in some cases the results with just character-based representations outperform those that used explicit POS tags since they provide similar vectors for words with similar/same morphosyntactic tag (Ballesteros et al., 2015)); in this paper we show a similar result given that both syntactic parsing and named-entity recognition play a central role in AMR parsing.\nThese are concatenated with pretrained word embeddings. We use a variant of the skip n-gram model provided by Ling et al. (2015a) with the LDC English Gigaword corpus (version 5). These embeddings encode the syntactic behavior of the words (see (Ling et al., 2015a)).\nMore formally, to represent each input token, we concatenate two vectors: a learned character-\n6When the word at the top of stack is an out of vocabulary word, the system directly outputs the word itself as AMR node.\nbased representation (w̃C); and a fixed vector representation from a neural language model (w̃LM). A linear map (V) is applied to the resulting vector and passed through a component-wise ReLU,\nx = max {0,V[w̃C; w̃LM] + b} .\nwhere V is a learned parameter matrix, b is a bias term and wC is the character-based learned representation for each word, w̃LM is the pretrained word representation."
    }, {
      "heading" : "3.4 POS Tagging and Dependency Parsing",
      "text" : "We may include preprocessed POS tags or dependency parses to incorporate more information into our model. For the POS tags we use the Stanford tagger (Toutanova et al., 2003) while we use the Dyer et al. (2015)’s Stack-LSTM parser trained on the English CoNLL 2009 dataset (Hajič et al., 2009) to get the dependencies.\nPOS tags: The POS tags are preprocessed and a learned representation tag is concatenated with the word representations. This is the same setting as (Dyer et al., 2015).\nDependency Trees: We use them in the same way as POS tags by concatenating a learned representation dep of the dependency label to the parent with the word representation. Additionally, we enrich the state representation st, presented in Section 3.2. If the two words at the top of the STACK have a dependency between them, st is enriched with a learned representation that indicates that and the direction; otherwise st remains unchanged. st is calculated as follows:\nst = max {0,W[stt;bt;at;dept] + d} ,\nwhere dep t is the learned vector that represents that there is an arc between the two top words at the top of the stack."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "We use the LDC2014T12 dataset7 for our experiments. Table 1 shows results, including comparison with prior work that are also evaluated on the same dataset.8\n7This dataset is a standard for comparison and has been used for evaluation in recent papers like (Wang et al., 2015a; Goodman et al., 2016; Zhou et al., 2016). We use the standard training/development/test split: 10,312 sentences for training, 1,368 sentences for development and 1,371 sen-\nOur model achieves 0.68 F1 in the newswire section of the test set just by using character-based representations of words and pretrained word embeddings. All prior work uses lemmatizers, POS taggers, dependency parsers, named entity recognizers and semantic role labelers that use additional training data while we achieve competitive scores without that. Pust et al. (2015) reports 0.66 F1 in the full test by using WordNet for concept identification, but their performance drops to 0.61 without WordNet. It is worth noting that we\ntences held-out for testing. 8The first entry for Damonte et al. is calculated using a pretrained LDC2015 model, available at https://github.com/mdtux89/amr-eager, but evaluated on the LDC2014 dataset. This means that the score is not directly comparable with the rest. The second entry (0.64) for Damonte et al. is calculated by training their parser with the LDC2014 training set which makes it directly comparable with the rest of the parsers.\nachieved 0.64 in the same test set without WordNet. Wang et al. (2015b,a) without SRL (via Propbank) achieves only 0.63 in the newswire test set while we achieved 0.69 without SRL (and 0.68 without dependency trees).\nIn order to see whether pretrained word embeddings and character-based embeddings are useful we carried out an ablation study by showing the results of our parser with and without character-based representations (replaced by standard lookup table learned embeddings) and with and without pretrained word embeddings. By looking at the results of the parser without character-based embeddings but with pretrained word embeddings we observe that the characterbased representation of words are useful since they help to achieve 2 points better in the Newswire dataset and 1 point more in the full test set. The parser with character-based embeddings but without pretrained word embeddings, the parser has more difficulty to learn and only achieves 0.61 in the full test set. Finally, the model that does not use neither character-based embeddings nor pretrained word embeddings is the worst achieving only 0.59 in the full test set, note that this model has no explicity way of getting any syntactic information through the word embeddings nor a smart way to handle out of vocabulary words.\nAll the systems marked with * require that the input is a dependency tree, which means that they solve a transduction task between a dependency tree and an AMR graph. Even though our parser starts from plain text sentences when we incorporate more information into our model, we achieve further improvements. POS tags provide small improvements (0.6801 without POS tags vs 0.6822 for the model that runs with POS tags). Dependency trees help a bit more achieving 0.6920."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "We present a new transition-based algorithm for AMR parsing and we implement it using StackLSTMS and a greedy decoder. We present competitive results, without any additional resources and external tools. Just by looking at the words, we achieve 0.68 F1 (and 0.69 by preprocessing dependency trees) in the standard dataset used for evaluation."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Marco Damonte, Shay Cohen and Giorgio Satta for running and evaluating their parser in the LDC2014T12 dataset. We also thank Chuan Wang for useful discussions."
    } ],
    "references" : [ {
      "title" : "Broad-coverage ccg semantic parsing with amr",
      "author" : [ "Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1699–1710, Lisbon, Portugal. Association for Com-",
      "citeRegEx" : "Artzi et al\\.,? 2015",
      "shortCiteRegEx" : "Artzi et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved transition-based parsing by modeling characters instead of words with lstms",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–",
      "citeRegEx" : "Ballesteros et al\\.,? 2015",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "Going to the roots of dependency parsing",
      "author" : [ "Miguel Ballesteros", "Joakim Nivre." ],
      "venue" : "Computational Linguistics, 39(1).",
      "citeRegEx" : "Ballesteros and Nivre.,? 2013",
      "shortCiteRegEx" : "Ballesteros and Nivre.",
      "year" : 2013
    }, {
      "title" : "Abstract meaning representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the 7th Linguis-",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Smatch: an evaluation metric for semantic feature structures",
      "author" : [ "Shu Cai", "Kevin Knight." ],
      "venue" : "ACL (2), pages 748–752.",
      "citeRegEx" : "Cai and Knight.,? 2013",
      "shortCiteRegEx" : "Cai and Knight.",
      "year" : 2013
    }, {
      "title" : "An incremental parser for abstract meaning representation",
      "author" : [ "Marco Damonte", "Shay B. Cohen", "Giorgio Satta." ],
      "venue" : "CoRR, abs/1608.06111.",
      "citeRegEx" : "Damonte et al\\.,? 2016",
      "shortCiteRegEx" : "Damonte et al\\.",
      "year" : 2016
    }, {
      "title" : "Transitionbased dependency parsing with stack long shortterm memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah Smith." ],
      "venue" : "Proceedings of NAACL-HLT 2016.",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Cmu at semeval-2016 task 8: Graph-based amr parsing with infinite ramp loss",
      "author" : [ "Jeffrey Flanigan", "Chris Dyer", "Noah A Smith", "Jaime Carbonell." ],
      "venue" : "Proceedings of SemEval, pages 1202–1206.",
      "citeRegEx" : "Flanigan et al\\.,? 2016",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2016
    }, {
      "title" : "A discriminative graph-based parser for the abstract meaning representation",
      "author" : [ "Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Flanigan et al\\.,? 2014",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2014
    }, {
      "title" : "Noise reduction and targeted exploration in imitation learning for abstract meaning representation parsing",
      "author" : [ "James Goodman", "Andreas Vlachos", "Jason Naradowsky." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Goodman et al\\.,? 2016",
      "shortCiteRegEx" : "Goodman et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "CoRR, abs/1308.0850.",
      "citeRegEx" : "Graves.,? 2013",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model",
      "author" : [ "James Henderson", "Paola Merlo", "Ivan Titov", "Gabriele Musillo." ],
      "venue" : "Computational Linguistics, 39(4):949–998.",
      "citeRegEx" : "Henderson et al\\.,? 2013",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Kazuya Kawakami", "Sandeep Subramanian", "Chris Dyer." ],
      "venue" : "Proceedings of NAACL-HLT 2016.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Two/too simple adaptations of word2vec for syntax problems",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Ling et al\\.,? 2015a",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Tiago Luı́s", "Luı́s Marujo", "Ramón Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural shift-reduce ccg semantic parsing",
      "author" : [ "Dipendra Kumar Misra", "Yoav Artzi." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1775– 1786, Austin, Texas. Association for Computational",
      "citeRegEx" : "Misra and Artzi.,? 2016",
      "shortCiteRegEx" : "Misra and Artzi.",
      "year" : 2016
    }, {
      "title" : "Dynet: The dynamic neural network toolkit",
      "author" : [ "Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin." ],
      "venue" : "arXiv preprint arXiv:1701.03980.",
      "citeRegEx" : "Oda et al\\.,? 2017",
      "shortCiteRegEx" : "Oda et al\\.",
      "year" : 2017
    }, {
      "title" : "An Efficient Algorithm for Projective Dependency Parsing",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160.",
      "citeRegEx" : "Nivre.,? 2003",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2003
    }, {
      "title" : "Incrementality in deterministic dependency parsing",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together.",
      "citeRegEx" : "Nivre.,? 2004",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2004
    }, {
      "title" : "Algorithms for deterministic incremental dependency parsing",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Computational Linguistics, 34:4:513–553. MIT Press.",
      "citeRegEx" : "Nivre.,? 2008",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2008
    }, {
      "title" : "Non-projective dependency parsing in expected linear time",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Vol-",
      "citeRegEx" : "Nivre.,? 2009",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2009
    }, {
      "title" : "Parsing english into abstract meaning representation using syntaxbased machine translation",
      "author" : [ "Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May." ],
      "venue" : "Proc. EMNLP, Lisbon, Portugal.",
      "citeRegEx" : "Pust et al\\.,? 2015",
      "shortCiteRegEx" : "Pust et al\\.",
      "year" : 2015
    }, {
      "title" : "Greedy, joint syntacticsemantic parsing with stack lstms",
      "author" : [ "Swabha Swayamdipta", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin,",
      "citeRegEx" : "Swayamdipta et al\\.,? 2016",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2016
    }, {
      "title" : "Feature-rich part-ofspeech tagging with a cyclic dependency network",
      "author" : [ "Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer." ],
      "venue" : "Proceedings NAACL.",
      "citeRegEx" : "Toutanova et al\\.,? 2003",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2003
    }, {
      "title" : "Boosting transition-based AMR parsing with refined actions and auxiliary analyzers",
      "author" : [ "Chuan Wang", "Nianwen Xue", "Sameer Pradhan." ],
      "venue" : "Proc. of , ACL 2015, pages 857–862.",
      "citeRegEx" : "Wang et al\\.,? 2015a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "A transition-based algorithm for amr parsing",
      "author" : [ "Chuan Wang", "Nianwen Xue", "Sameer Pradhan." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Wang et al\\.,? 2015b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust subgraph generation improves abstract meaning representation parsing",
      "author" : [ "Keenon Werling", "Gabor Angeli", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
      "citeRegEx" : "Werling et al\\.,? 2015",
      "shortCiteRegEx" : "Werling et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical dependency analysis with support vector machines",
      "author" : [ "Hiroyasu Yamada", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 195–206.",
      "citeRegEx" : "Yamada and Matsumoto.,? 2003",
      "shortCiteRegEx" : "Yamada and Matsumoto.",
      "year" : 2003
    }, {
      "title" : "Syntactic processing using the generalized perceptron and beam search",
      "author" : [ "Yue Zhang", "Stephen Clark." ],
      "venue" : "Computational Linguistics, 37(1):105–151.",
      "citeRegEx" : "Zhang and Clark.,? 2011",
      "shortCiteRegEx" : "Zhang and Clark.",
      "year" : 2011
    }, {
      "title" : "Amr parsing with an incremental joint model",
      "author" : [ "Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang QU", "Ran Li", "Yanhui Gu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 680–689, Austin,",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack.",
      "startOffset" : 57,
      "endOffset" : 110
    }, {
      "referenceID" : 30,
      "context" : "Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al.",
      "startOffset" : 154,
      "endOffset" : 196
    }, {
      "referenceID" : 7,
      "context" : "Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al.",
      "startOffset" : 154,
      "endOffset" : 196
    }, {
      "referenceID" : 14,
      "context" : ", 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : ", 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al.",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : ", 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstractmeaning representation parsing (Wang et al.",
      "startOffset" : 82,
      "endOffset" : 132
    }, {
      "referenceID" : 24,
      "context" : ", 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstractmeaning representation parsing (Wang et al.",
      "startOffset" : 82,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : ", 2016) and even abstractmeaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Check (Banarescu et al., 2013) for a complete description of AMR graphs.",
      "startOffset" : 6,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "(2015b,a); Goodman et al. (2016); Damonte et al.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "(2016); Damonte et al. (2016) and Dyer et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "(2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shiftreduce algorithm that produces AMR graphs directly from plain text.",
      "startOffset" : 8,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "(2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shiftreduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al.",
      "startOffset" : 8,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "(2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shiftreduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al. (2016) presented transition-based tree-to-graph transducers that traverse a dependency tree and transforms it to an AMR graph.",
      "startOffset" : 8,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "(2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shiftreduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al. (2016) presented transition-based tree-to-graph transducers that traverse a dependency tree and transforms it to an AMR graph. Damonte et al. (2016)’s input is a sentence and it is therefore more similar (with a different parsing algorithm) to our approach, but their parser relies on external tools, such as dependency parsing, semantic role labeling or named entity recognition.",
      "startOffset" : 8,
      "endOffset" : 347
    }, {
      "referenceID" : 24,
      "context" : "Stack-LSTMs have proven to be useful in tasks related to syntactic and semantic parsing (Dyer et al., 2015, 2016; Swayamdipta et al., 2016) and named entity recognition (Lample et al.",
      "startOffset" : 88,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : ", 2016) and named entity recognition (Lample et al., 2016).",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "(2016) and the arc-standard algorithm (Nivre, 2004).",
      "startOffset" : 38,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "As in (Ballesteros and Nivre, 2013) the buffer starts with the root symbol at the end of the sequence.",
      "startOffset" : 6,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "The parsing algorithm is inspired from the semantic actions presented by Henderson et al. (2013), the transition-based NER algorithm by Lample et al.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "The parsing algorithm is inspired from the semantic actions presented by Henderson et al. (2013), the transition-based NER algorithm by Lample et al. (2016) and the arc-standard algorithm (Nivre, 2004).",
      "startOffset" : 73,
      "endOffset" : 157
    }, {
      "referenceID" : 22,
      "context" : "This action allows nonprojective arcs as in (Nivre, 2009) but it also helps to introduce reentrancies.",
      "startOffset" : 44,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "We use the JAMR aligner provided by Flanigan et al. (2014). It is important to mention that even though the aligner is quite accurate, it is not perfect, producing a F1 score of around 0.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "895 F1 Smatch score (Cai and Knight, 2013) when it is run on the development set of the LDC2014T12.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "We used the latest version of the aligner (Flanigan et al., 2016)",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "The stack LSTM is an augmented LSTM (Hochreiter and Schmidhuber, 1997; Graves, 2013) that allows adding new inputs in the same way as LSTMs but it also provides a POP operation that moves a pointer to the previous element.",
      "startOffset" : 36,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "The stack LSTM is an augmented LSTM (Hochreiter and Schmidhuber, 1997; Graves, 2013) that allows adding new inputs in the same way as LSTMs but it also provides a POP operation that moves a pointer to the previous element.",
      "startOffset" : 36,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "We refer interested readers to (Dyer et al., 2015) for further details.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "We use character-based representations of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "(2016) demonstrated that it is possible to achieve high results in syntactic parsing and named entity recognition by just using character-based word representations (not even POS tags, in fact, in some cases the results with just character-based representations outperform those that used explicit POS tags since they provide similar vectors for words with similar/same morphosyntactic tag (Ballesteros et al., 2015)); in this paper we show a similar result given that both syntactic parsing",
      "startOffset" : 390,
      "endOffset" : 416
    }, {
      "referenceID" : 1,
      "context" : ", 2015b; Ballesteros et al., 2015). They learn representations for words that are orthographically similar. Note that they are updated with the updates to the model. Ballesteros et al. (2015) and Lample et al.",
      "startOffset" : 9,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : ", 2015b; Ballesteros et al., 2015). They learn representations for words that are orthographically similar. Note that they are updated with the updates to the model. Ballesteros et al. (2015) and Lample et al. (2016) demonstrated that it is possible to achieve high results in syntactic parsing and named entity recognition by just using character-based word representations (not even POS tags, in fact, in some cases the results with just character-based representations outperform those that used explicit POS tags since they provide similar vectors for words with similar/same morphosyntactic tag (Ballesteros et al.",
      "startOffset" : 9,
      "endOffset" : 217
    }, {
      "referenceID" : 15,
      "context" : "These embeddings encode the syntactic behavior of the words (see (Ling et al., 2015a)).",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "We use a variant of the skip n-gram model provided by Ling et al. (2015a) with the LDC English Gigaword corpus (version 5).",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "For the POS tags we use the Stanford tagger (Toutanova et al., 2003) while we use the Dyer et al.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : ", 2003) while we use the Dyer et al. (2015)’s Stack-LSTM parser trained on the English CoNLL 2009 dataset (Hajič et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "This is the same setting as (Dyer et al., 2015).",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "This dataset is a standard for comparison and has been used for evaluation in recent papers like (Wang et al., 2015a; Goodman et al., 2016; Zhou et al., 2016).",
      "startOffset" : 97,
      "endOffset" : 158
    }, {
      "referenceID" : 10,
      "context" : "This dataset is a standard for comparison and has been used for evaluation in recent papers like (Wang et al., 2015a; Goodman et al., 2016; Zhou et al., 2016).",
      "startOffset" : 97,
      "endOffset" : 158
    }, {
      "referenceID" : 31,
      "context" : "This dataset is a standard for comparison and has been used for evaluation in recent papers like (Wang et al., 2015a; Goodman et al., 2016; Zhou et al., 2016).",
      "startOffset" : 97,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "Model F1(Newswire) F1(ALL) Flanigan et al. (2014)* (POS, DEP) 0.",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Model F1(Newswire) F1(ALL) Flanigan et al. (2014)* (POS, DEP) 0.59 0.58 Flanigan et al. (2016)* (POS, DEP, NER) – 0.",
      "startOffset" : 27,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Model F1(Newswire) F1(ALL) Flanigan et al. (2014)* (POS, DEP) 0.59 0.58 Flanigan et al. (2016)* (POS, DEP, NER) – 0.66 Werling et al. (2015)* (POS, DEP, NER) 0.",
      "startOffset" : 27,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "62 – Damonte et al. (2016)(POS, DEP, NER, SRL) – 0.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "62 – Damonte et al. (2016)(POS, DEP, NER, SRL) – 0.61 Damonte et al. (2016)(POS, DEP, NER, SRL) – 0.",
      "startOffset" : 5,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "64 Artzi et al. (2015) (POS, CCG) 0.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "64 Artzi et al. (2015) (POS, CCG) 0.66 – Goodman et al. (2016)* (POS, DEP, NER) 0.",
      "startOffset" : 3,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "64 Artzi et al. (2015) (POS, CCG) 0.66 – Goodman et al. (2016)* (POS, DEP, NER) 0.70 – Zhou et al. (2016)* (POS, DEP, NER, SRL) 0.",
      "startOffset" : 3,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "64 Artzi et al. (2015) (POS, CCG) 0.66 – Goodman et al. (2016)* (POS, DEP, NER) 0.70 – Zhou et al. (2016)* (POS, DEP, NER, SRL) 0.71 0.66 Pust et al. (2015) (LM, NER) – 0.",
      "startOffset" : 3,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "64 Artzi et al. (2015) (POS, CCG) 0.66 – Goodman et al. (2016)* (POS, DEP, NER) 0.70 – Zhou et al. (2016)* (POS, DEP, NER, SRL) 0.71 0.66 Pust et al. (2015) (LM, NER) – 0.61 Pust et al. (2015) (Wordnet, LM, NER) – 0.",
      "startOffset" : 3,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "64 Artzi et al. (2015) (POS, CCG) 0.66 – Goodman et al. (2016)* (POS, DEP, NER) 0.70 – Zhou et al. (2016)* (POS, DEP, NER, SRL) 0.71 0.66 Pust et al. (2015) (LM, NER) – 0.61 Pust et al. (2015) (Wordnet, LM, NER) – 0.66 Wang et al. (2015b)* (POS, DEP, NER) 0.",
      "startOffset" : 3,
      "endOffset" : 239
    }, {
      "referenceID" : 0,
      "context" : "64 Artzi et al. (2015) (POS, CCG) 0.66 – Goodman et al. (2016)* (POS, DEP, NER) 0.70 – Zhou et al. (2016)* (POS, DEP, NER, SRL) 0.71 0.66 Pust et al. (2015) (LM, NER) – 0.61 Pust et al. (2015) (Wordnet, LM, NER) – 0.66 Wang et al. (2015b)* (POS, DEP, NER) 0.63 0.59 Wang et al. (2015a)* (POS, DEP, NER, SRL) 0.",
      "startOffset" : 3,
      "endOffset" : 286
    }, {
      "referenceID" : 23,
      "context" : "Pust et al. (2015) reports 0.",
      "startOffset" : 0,
      "endOffset" : 19
    } ],
    "year" : 2017,
    "abstractText" : "We present a transition-based AMR parser that directly generates AMR parses from plain text. We use Stack-LSTMs to represent our parser state and make decisions greedily. In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further.",
    "creator" : "LaTeX with hyperref package"
  }
}