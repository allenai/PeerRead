{
  "name" : "1303.5713.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Symbolic Probabilistic Inference with Evidence Potential",
    "authors" : [ "Kuo-Chu Chang", "Robert EUng" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Recent research on the Symbolic Probabilis tic Inference (SPI) algorithm[;:] has focused attention on the importance of resolving general queries in Bayesian networks. SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. In response to this research we have extended the evidence potential algorithm [3] with the same fea tures. We call the extension symbolic evi dence potential inference (SEPI). SEPI like SPI can handle generic queries and is incre mental with respect to queries and observa tions. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence poten tial algorithm [3] is the basic framework for recursive query processing.\nIn this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network - the standard procedure of the evidence potential algorithm. With the clique tree, various probability distribu tions are computed and stored in each clique. This is the \"pre-processing\" step of SEPI. Once this step is done, the query can then be computed. To process a query, a recursive process similar to the SPI algorithm is used. The queries are directed to the root clique and decomposed into queries for the clique's subtrees until a particular query can be an swered at the clique at which it is directed. The algorithm and the computation are sim ple. The SEPI algorithm will be presented in this paper along with several examples.\n1 Introduction\nThe Bayesian networks technology provides a rep resentation language for uncertain beliefs and infer ence algorithms for drawing sound conclusions from such representations. Bayesian Network is a directed, acyclic graph in which the nodes represent random variables, and the arcs between the nodes represent possible probabilistic dependence between the vari ables. The success of the representation is mainly due to the development of many probabilistic infer ence algorithms [3, 4, 5, 6]. While most of the algo rithms can efficiently perform simple queries such as the marginal probability of each node given evidence, they have not efficiently addressed the problem of more general queries such as joint or conditional probabili ties of any combination of nodes.\nThe recent work of Symbolic Probabilistic Inference (SPI) [1, 2] has made a significant step in this direc tion. SPI is a goal-driven method which is incremen tal with respect to both queries and observations. In response to this research we have extended the evi dence potential (EP) algorithm [3] with the same fea tures. We call the extension symbolic evidence poten tial inference (SEPI). Unlike traditional Bayesian Net inferencing algorithms, both SPI and SEPI are goal directed, performing only those calculations that are required to respond to queries. While in SPI, opera tions are done on a search tree constructed from the original network, in SEPI, a clique-tree structure ob tained from the EP algorithm is the basic framework for recursive query processing.\nIn SEPI, the EP algorithm [3] is used as the \"pre processing\" step in which various probabilities such as \"set-chain\" conditional [3) and marginal probabilities of each clique are computed based on the clique tree. The second step in SEPI is to process the query with a recursive mechanism similar to the SPI algorithm. A query is directed to the root clique and decomposed into queries for the clique's subtrees. This recursive process continues until a particular query can be an swered at the clique at which it is directed. The answer\nSymbolic Probabilistic Inference with Evidence Potential 83\nis then computed and returned to the next higher level in the clique tree. Once a clique has responses from all of its subtrees it can compute its own response to its predecessor clique. This process terminates when the root clique processes all the responses from its sub trees.\nWith similar mechanisms for caching and incorporat ing evidence as in SPI, the calculation in SEPI is also incremental with respect to both query and evidences. However, since all the necessary probability distribu tions are stored in the \"pre-processing\" step, the SEPI algorithm is more efficient.\nThe paper is organized as follows. Section 2 briefly de scribes the EP algorithm which includes the construc tion of the clique tree. Section 3 describes the SEPI algorithm. A systematic recursive query and caching procedure will be presented. Some illustrative exam ples are given in Section 4, followed by the concluding remarks in Section 5."
    }, {
      "heading" : "2 Evidence Potential Algorithm",
      "text" : "In this section, we will briefly review the evidence po tential (EP) algorithm [3]. The algorithm first orga nizes the original network into clique tree, where each clique is a group of nodes not necessary mutually ex clusive. It then performs inference by passing messages between cliques in a similar way to the distributed al gorithm [4] .\nThe first part of the algorithm is to form a clique tree. This part consists of five steps\n1. Marry Parents: link predecessors of a node to gether\n2. Remove Arc Directions: remove directions of all arcs\n3. Fill in: generate new arcs between nodes when ever necessary to form a \"perfect\" graph\n4. Find Cliques: form node clusters/ cliques 5. Order Cliques, and Find Residuals and Separa\ntors: form cluster tree\nAfter the clique tree is formed, the second part of the algorithm is to calculate the marginal probability of each node. Before this can be done, the \"evidence potential\" and \"separator potential\" likelihoods [3] are calculated for each cluster.\nThe second part of the algorithm consists of the fol lowing:\n1. Calculating Evidence Potentials and Separator potentials: they are calculated from the prior node conditionals in each clique.\n2. Calculate Set-Chain Conditionals: namely, the conditional probability of the residuals given the\nTo illustrate this algorithm, the example given in [3] is shown in Figure 1. The corresponding clique tree and the set-chain conditional of each clique is shown in Figure 2. It is clear that the joint probability of the whole network can be obtained by multiplying all the set-chain conditionals together with the marginal probabilities of all the root cliques. Any query can then be obtained from the joint probability. The basic idea of EP algorithm is to decompose and factor the original formulae so that only minimum operations are required to answer the queries.\n3 Symbolic Inference with Evidence Potential\nThe procedure described in the previous section can be considered as the pre-processing step for the generic query algorithm to be described. We call this new al gorithm symbolic evidence potential inference (SEPI). In this algorithm, the goal is to calculate the results of arbitrary queries. The idea is to derive an efficient in ference algorithm which takes advantage of the clique tree structure of the EP algorithm.\nThe SEPI algorithm consists of several major process ing steps. The first step is to organize the nodes of a Bayesian network into a clique tree structure and\ncalculate and store the various probability functions as described in the previous section (e.g., set-chain conditional and joint probability distribution). In the second step, queries from the user are directed to the root clique of the tree. The query is decomposed into queries for the clique's subtrees. This recursive pro cedure continues until a particular query can be an swered.\nThe general format for a query received by SEPI is as a conditional probability, namely, P{XIY}, where X and Y are sets of nodes in the network. This query is first transformed into joint distribution format P(Z):::: P{X, Y} and directed to the root clique. In order to answer the query, it would be sufficient to calculate P(Z \\ CoiC0), where Co is the root clique. This is because we can calculate the query by\nP(Z) = L P(Z \\ CaiCo)P(Co) (1) Co\\Z\nwhere the prior probability P( Co) is available at the clique Co. According to the EP algorithm, the clique tree is organized in a way that the separators are the overlapping nodes between the successor and prede cessor cliques. Denote the separators between the root clique and the child clique C; as S;, then\nP(Z \\ CaiCo) = P(Z \\Co IS;) (2)\nDefine T( Ci) as all the nodes in the subtree rooted from C;, then the new request to be sent to each child C; is\nP((Z \\Co) n T(C;)IS;) (3)\nNote that if the successor clique has nothing to do with the query, i.e., (Z \\Co) n T(C;) = 0, then no query will be sent to that clique.\nAt the clique C;, when the request arrives for a prob ability distribution represented by P{XIS;}, if such a distribution had already been computed earlier and cached, it can be returned immediately. However, usu ally it will be necessary to send requests to the clique's successors in order to compute the response. Since it can be easily shown that\nP{XIS;} = Lfl P(XnT(C,.i)IS;j)P(Rc,IS;) (4) Rei j\nwhere Rc, is the residual nodes of C;, C;j is the j- th child clique of C;, and S;j is the separators between C; and C;j, the request to each child C;j will be\nP(XnT(C;i)IS;j)· (5) The recursive process continues until it reaches the leaf node or the request can be answered from the cached results.\nTo handle the evidence, just substitute the observed values into all the clique distributions involving the observed node. This operation is very simple in which the particular dimension of the observed value is sim ply eliminated and the rest of the distribution remain the same. The substitution needs to be done for all the distributions including the cached results stored in each clique which involves the observed node. Af ter the substitution, all the other operations can be applied on distributions with the reduced dimensions.\nAs in the SPI algorithm, three major operations are needed in the SEPI algorithm: multiplication, sum mation and substitution. Multiplication calculates the product of two distributions, summation calculates the sum of a distribution over a set of variables, and substi tution calculates the result of substituting an observed value for a node into a distribution.\n4 Examples\nWith the network given in Figure 1, we will now il lustrate the SEPI algorithm with several query exam ples. First, assuming the query we are interested is P(AX S), the recursive algorithm works as follows.\n• The query P(AXS) is received at the root clique (AT), based on eqn. (2) and (3), a new query P(X SIT) is generated and sent to the successor clique (T LE)\n• The query P(X SIT) is received at the clique (TLE); similarly, a new query P(XSILE) is gen erated and sent to the successor clique ( LE B)\n• The query P(X SILE) is received at the clique (LEB), based on (5), new queries P(SIBL) and P( X IE B) are generated and sent to the successors (BLS) and (EBD) respectively.\nSymbolic Probabilistic Inference with Evidence Potential 85\n- The query P(SIBL) is received at the clique ( B LS) which is available in the cache due to the pre-processing.\n- The query P(XIEB) is received at clique (EBD), a new query P(XIE) is generated and sent to the successor clique (EX)\n- The query P(XIE) is received at clique (EX) which is available.\n• At clique (EBD), compute the query P(XIEB) by\nP(XIEB) = 2::: P(XIE)P(DIBE) (6) D\n• At clique (LEB), compute the query P(X SILE) by\nP(X SILE) = 2::: P(SIBL)P(XIEB)P(BILE) B\n(7) • At clique (T LE), compute the query P(X SIT) by\nP(X SIT) = 2::: P(XSILE)P(LEIT) (8) LE\n• At root clique (AT), compute the query P(AXS) by\nP(AX S) = 2::: P(X SIT)P(AT) (9) T\nAssume in the second example that the node E is observed and the observed value is E•. To calcu late the posterior probability of the same query, we first substitute the observed value into all the distri butions in the cliques related to the observed node. These include P(DIBE), P(BILE), and P(LEIT) in the cliques (TLE), (LEB), and (EBD) respectively. The substitution operation simply eliminates the par ticular dimension corresponding to the observed value in the distributions. Then the same procedure as de scribed above to calculate the query can be applied using the distributions with new reduced dimensions. The result is therefore,\nP(AXSIE = E•) = LT LL [LB [P(SIBL) LD [p(XIE.)P(DIBE•)J\nP(BILE•)J P(LE.IT)] P(AT) (10)\n5 Conclusion\nSPI algorithm [1, 2] is the latest inference algorithms in which the emphasis is on the efficient generic query. The main goal of these algorithms is to respond to arbitrary queries in an efficient manner. In these al gorithms the network is first converted into a search tree and the probabilities are manipulated by symbol ically decomposing or factoring the formulae. These methods are incremental with respect to queries and\nevidence and have good potential for parallel process ing.\nIn this paper, we develop a similar query algorithm based on the combination of evidence potential algo rithm and the SPI inference mechanism. Rather than converting the network into a SPI search tree, we con struct a \"clique tree\" based on the evidence potential algorithm. Additionally, the evidence potential algo rithm is used as the pre-processing step where all the necessary probability distributions for answering the query are computed and stored in each clique.\nSimilar to the SPI algorithm, queries are directed to the root clique of the tree. They are decomposed into queries for the clique's subtrees. This recursive pro cedure continues until a particular query can be an swered. The answer is then computed and returned to the next higher level. The algorithm and the computa tion are simple. With a similar mechanism for caching and incorporating eviden.ce as in the SPI algorithm, the calculation is also incremental with respect to both query and evidence. However, the SEPI algorithm is more efficient since all the necessary probability distri butions are stored in the pre-processing step. A ver sion of the SEPI algorithm as well as the SPI algorithm have been implemented, preliminary results from sev eral examples show that with the prep-processing step, the query process of the SEPI algorithm is faster than the SPI algorithm.\nReferences\n[1] B. D'Ambrosio. Symbolic probabilistic inference in belief nets. 1990.\n[2] R. Shachter A. Del Favero and B. D'Ambrosio. Symbolic probabilistic inference: A probabilistic perspective. Proceeding of AAAI, 1990.\n[3] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical struc tures and their application in expert systems. Jour nal Royal Statistical Society B, 50, 1988.\n[4] Judea Pearl. Fusion, propagation, and structuring in belief networks. Artificial Intelligence, 29, 1986.\n[5] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, 1988.\n[6] Ross D. Shachter. Intelligent probabilistic infer ence. In L.N. Kana! and J.F. Lemmer, editors, Uncertainty in Artificial Intelligence. Amsterdam: North-Holland, 1986."
    } ],
    "references" : [ {
      "title" : "Symbolic probabilistic inference in belief nets",
      "author" : [ "B. D'Ambrosio" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1990
    }, {
      "title" : "Symbolic probabilistic inference: A probabilistic perspective",
      "author" : [ "R. Shachter A. Del Favero", "B. D'Ambrosio" ],
      "venue" : "Proceeding of AAAI,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1990
    }, {
      "title" : "Local computations with probabilities on graphical struc­ tures and their application in expert systems",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Jour­ nal Royal Statistical Society  B,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1988
    }, {
      "title" : "Fusion, propagation, and structuring in belief networks",
      "author" : [ "Judea Pearl" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1986
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "Judea Pearl" ],
      "venue" : "Morgan Kaufmann Publishers,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1988
    }, {
      "title" : "Intelligent probabilistic infer­ ence",
      "author" : [ "Ross D. Shachter" ],
      "venue" : "Uncertainty in Artificial Intelligence. Amsterdam: North-Holland,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1986
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "In response to this research we have extended the evidence potential algorithm [3] with the same fea­ tures.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence poten­ tial algorithm [3] is the basic framework for recursive query processing.",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "The success of the representation is mainly due to the development of many probabilistic infer­ ence algorithms [3, 4, 5, 6].",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "The success of the representation is mainly due to the development of many probabilistic infer­ ence algorithms [3, 4, 5, 6].",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "The success of the representation is mainly due to the development of many probabilistic infer­ ence algorithms [3, 4, 5, 6].",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "The success of the representation is mainly due to the development of many probabilistic infer­ ence algorithms [3, 4, 5, 6].",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "The recent work of Symbolic Probabilistic Inference (SPI) [1, 2] has made a significant step in this direc­ tion.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "The recent work of Symbolic Probabilistic Inference (SPI) [1, 2] has made a significant step in this direc­ tion.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "In response to this research we have extended the evi­ dence potential (EP) algorithm [3] with the same fea­ tures.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "In SEPI, the EP algorithm [3] is used as the \"pre­ processing\" step in which various probabilities such as \"set-chain\" conditional [3) and marginal probabilities of each clique are computed based on the clique tree.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "In this section, we will briefly review the evidence po­ tential (EP) algorithm [3].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "It then performs inference by passing messages between cliques in a similar way to the distributed al­ gorithm [4] .",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Before this can be done, the \"evidence potential\" and \"separator potential\" likelihoods [3] are calculated for each cluster.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "To illustrate this algorithm, the example given in [3] is shown in Figure 1.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "SPI algorithm [1, 2] is the latest inference algorithms in which the emphasis is on the efficient generic query.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "SPI algorithm [1, 2] is the latest inference algorithms in which the emphasis is on the efficient generic query.",
      "startOffset" : 14,
      "endOffset" : 20
    } ],
    "year" : 2011,
    "abstractText" : "Recent research on the Symbolic Probabilis­ tic Inference (SPI) algorithm[;:] has focused attention on the importance of resolving general queries in Bayesian networks. SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. In response to this research we have extended the evidence potential algorithm [3] with the same fea­ tures. We call the extension symbolic evi­ dence potential inference (SEPI). SEPI like SPI can handle generic queries and is incre­ mental with respect to queries and observa­ tions. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence poten­ tial algorithm [3] is the basic framework for recursive query processing. In this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network the standard procedure of the evidence potential algorithm. With the clique tree, various probability distribu­ tions are computed and stored in each clique. This is the \"pre-processing\" step of SEPI. Once this step is done, the query can then be computed. To process a query, a recursive process similar to the SPI algorithm is used. The queries are directed to the root clique and decomposed into queries for the clique's subtrees until a particular query can be an­ swered at the clique at which it is directed. The algorithm and the computation are sim­ ple. The SEPI algorithm will be presented in this paper along with several examples.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}