{
  "name" : "1605.02669.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The GPU-based Parallel Ant Colony System",
    "authors" : [ "Rafał Skinderowicza" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The Ant Colony System (ACS) is, next to Ant Colony Optimization (ACO) and the MAX-MIN Ant System (MMAS), one of the most efficient metaheuristic algorithms inspired by the behavior of ants. In this article we present three novel parallel versions of the ACS for the graphics processing units (GPUs). To the best of our knowledge, this is the first such work on the ACS which shares many key elements of the ACO and the MMAS, but differences in the process of building solutions and updating the pheromone trails make obtaining an efficient parallel version for the GPUs a difficult task. The proposed parallel versions of the ACS differ mainly in their implementations of the pheromone memory. The first two use the standard pheromone matrix, and the third uses a novel selective pheromone memory. Computational experiments conducted on several Travelling Salesman Problem (TSP) instances of sizes ranging from 198 to 2392 cities showed that the parallel ACS on Nvidia Kepler GK104 GPU (1536 CUDA cores) is able to obtain a speedup up to 24.29x vs the sequential ACS running on a single core of Intel Xeon E5-2670 CPU. The parallel ACS with the selective pheromone memory achieved speedups up to 16.85x, but in most cases the obtained solutions were of significantly better quality than for the sequential ACS.\nKeywords: parallel Ant Colony System, CUDA, GPU, selective pheromone memory, parallel metaheuristic"
    }, {
      "heading" : "1. Introduction",
      "text" : "The possibility of using graphics processing units (GPUs) to perform general purpose computations appeared recently and quickly became a subject of intense research. GPUs offer attractive performance to energy consumption and the cost of purchase ratio, and allow to perform many types of computations more quickly while maintaining the same cost in relation to the CPUs [34]. The usefulness of GPUs is evidenced by the fact they are used in approx. 10% of the fastest supercomputers in the world [25]. On the other hand, due to significant differences from the CPU architecture, the use of GPUs often requires significant changes in the algorithm. GPUs contain a large number of relatively simple computing units (processing elements), and therefore taking full advantage of their performance requires an appropriate division of calculations into subtasks. At the same time, the number of high latency operations should be minimized, particularly involving global memory access [19].\nMetaheuristic algorithms allow to find approximate solutions of good quality to many difficult optimization problems in a relatively short period of time. Good examples of efficient metaheuristic algorithms are algorithms inspired by the foraging behavior of ants, including the Ant Colony Algorithm (ACO), the Ant Colony System (ACS) and the MAX-MIN Ant System (MMAS) [13].\nSimilarly to other metaheuristic algorithms, ant colony algorithms are computationally demanding, therefore much research effort was put into developing efficient parallel versions for multi-processor computers [20]. In this article we present novel parallel versions of the Ant Colony System dedicated to the GPU. To the best of our knowledge, this is the first such work, although GPU versions of some ant colony algorithms, including the ACO and the MMAS, were proposed in the literature [10, 20]. The ACS algorithm is very similar to the ACO and the MMAS, however, there are important differences which make efficient parallelization of the algorithm more complicated. Specifically, we focus on the local update of the pheromone memory, which has\nEmail address: rafal.skinderowicz@us.edu.pl (Rafał Skinderowicz)\nPreprint submitted to Elsevier May 10, 2016\nar X\niv :1\n60 5.\n02 66\n9v 1\n[ cs\n.D C\n] 9\nM ay\n2 01\n6\na significant impact on both the algorithm runtime and the quality of the solutions obtained. We use the Travelling Salesman Problem (TSP) to compare the performance of the algorithms.\nBased on the parallel versions for the GPU of the ACO and the MMAS as described in the literature, we propose two parallel versions of the ACS which differ in the implementation of the pheromone memory update which affects both the algorithm runtime and the quality of the results. We also present a third parallel version of the ACS which includes a new version of the selective pheromone memory as inspired by an earlier work [23, 24].\nThe structure of the article is as follows. Section 2 contains a review of the literature on parallel versions of ant colony algorithms with focus on the implementations dedicated to the GPU. Section 3 describes the sequential ACS and the proposed parallel GPU versions of the ACS. Section 4 shows results and analysis of the computational experiments carried on a few selected TSP instances, focusing on the comparison between the proposed parallel versions of the ACS. Section 5 contains a brief summary and possible directions for further research."
    }, {
      "heading" : "2. Related work",
      "text" : "In the ACO algorithm (and related algorithms), a population of ants (agents) work simultaneously on solutions to the problem that is being tackled. In nature, some species of ants use chemical substances called pheromones as a method of an indirect communication with other members of the colony. If an ant finds a food source, it deposits small amounts of the pheromone on the path leading from the nest to the food source. This pheromone trail attracts other ants and leads them to the food source. Similarly, in the ACO algorithm a number of ants iteratively construct solutions to the problem based on additional knowledge about the problem and on virtual pheromone trails that are deposited on the solution components. The pheromone trails play the essential role of a collective memory and are usually referred to as a pheromone memory.\nDespite the parallel nature of the calculations, the parallelization of ant algorithms is not an easy task and can be done in many ways. An extensive overview of the different approaches to parallelization of ant algorithms can be found in [20]. The main criterion for dividing the parallel implementations of ant algorithms is the number of ant colonies used, i.e. one or multiple. Another criterion is the existence of cooperation between the individuals in a colony and between the colonies. Cooperation most often involves an exchange of the best solutions found to date. A single colony can be seen as a cell model in which the ants belong to overlapping neighborhoods which define network solutions exchange. In this model there are many pheromone matrices that are updated by ants assigned to respective neighborhoods. Another more popular approach is the master–slave model, in which a designated process (master) supervises the work of the slave processes (threads), including gathering knowledge on the best solution found so far.\nMost of the parallel ACO versions dedicated to the CPU apply the multi-colony approach [20]. In this case a single processor (or core) deals with the calculations for a single colony and the speedup is obtained thanks to simultaneous calculations for a number of colonies. In most cases, periodic exchange of the best solution found to date is performed between the colonies. The exchange is usually performed according to a predefined communication topology. In [29], a comparison can be found of the various communication topologies (policies) on the convergence of the MMAS algorithm solving the TSP. The benefits of the communication strongly depend on whether a local search was used and on the number of iterations respective to the problem size. For small problems and a large number of iterations, the benefits of communication were small or even had a negative effect. In the case of a smaller number of iterations, an exchange of solutions often improved the quality of the results. Similar conclusions can be found in the work of Chen et al. [7], in which proof of the convergence (at infinity) of the parallel ACO is given.\nCecilia et al. [5] proposed an efficient parallel (GPU) version of the ACO for the TSP in which an efficient parallelization scheme (data-parallel) was applied of the solution construction process. The authors also proposed an efficient parallel version of the probabilistic roulette wheel selection method used by an ant when looking for the next node (solution component) to append to a current partial solution. The new method was called I-Roulette. Combined, the innovations allowed to obtain a speedup of 20x as compared to the sequential version of the algorithm. The parallel version also improved the quality of the solutions found in a few cases. Uchida et al. [31] proposed a GPU version of the Ant System (AS) for the TSP. A speedup of 22x was achieved by replacing the sequential roulette wheel selection method with a parallel method called\nthe stochastic trial based on the I-Roulette method. The resulting speedups increased along with the size of the problem, from 4.51x for the d198 TSP instance to 22.11x for the pr1002 instance. However, for the largest instance, i.e. pcb3038, the speedup was only 11.87x.\nDelevacq et al. [10] studied a parallel implementation of the MMAS for the TSP on the GPU. The authors suggested two approaches - the first with a single ant colony and the second with multiple ant colonies. Two versions were investigated under the first approach. In the first version, ANTthread, each ant was assigned to a separate thread, and in the second version, ANTblock, the calculations for a single ant were performed by a block of threads. The latter turned out to be much faster, with speedups reaching 19.47x relative to the sequential MMAS. If using the local search (3-opt heuristic), the maximum speedup was 8.03x. In [32], based on the algorithm presented by Cecilia et al. [5], Wei et al. proposed an optimized ACO transition rule in which the maximum value was used instead of calculating the exact value of the sum of the probabilities. This resulted in further acceleration of this phase of the algorithm. However, no information was provided about the absolute runtime of the algorithm.\nThe TSP was used in most of the work on the parallel ACO for GPUs because of its simple definition and a straightforward graph representation. However, other problems were also considered; for example, in the work of Youness et al. [35], the parallel MMAS for the Satisfiability Problem (SAT) was presented. The authors used the data parallel approach in which a block of threads worked on a single ant’s solution. Also, the evaluation of the solutions and the evaporation of the pheromone were parallelized, thus giving a maximum speedup of 21x relative to the sequential implementation. Lower speedups were noted for the smallest problems.\nThe parallel ACO for the GPU to accelerate bi-directional pedestrian movement simulation was described in [14]. A speedup of 18x was achieved while maintaining the simulation quality comparable to the sequential version. Cano et al. [4] presented Parallel multi-objective Ant Programming for the classification algorithm using GPUs. Very large speedup values were achieved, i.e. up to 834x relative to the sequential version. Such large values probably stem from the nature of the computations, in which most of the time was spent on the evaluation of the solutions that was easy to parallelize. Not without significance is the fact that the authors implemented the algorithm in Java rather than in C, which suggests that the sequential version could be sped up, thus resulting in smaller relative speedup values [19]. The classification task was also addressed in [33], in which a parallel version of the AntMiner algorithm for the GPU was about 100x faster than the sequential version.\nFingler et al. [15] used the GPU to accelerate calculations of the ACO for the knapsack problem. The parallel ACO with multiple colonies was proposed, in which each thread block was responsible for the computations of one ant. The resulting solutions were of lower quality than generated by the best sequential heuristic, however, they were found in a much shorter period of time (more than 500 times faster in some cases).\nThe multi–colony approach was also used by Bai et al. [2] to speedup the MMAS using the GPUs. The GPU accelerated ACO was successfully used by Cekmez et al. [6] to solve the path planning problem of an Unmanned Aerial Vehicle (UAV). A survey of recent advances in applying the GPUs to speedup other metaheuristic algorithms can be found in [18, 22, 26].\nEffective use of GPU computing power requires taking into account the distinct differences between the CPU and GPU architectures, i.e. the parallel ACO algorithms for the GPU differ from the parallel implementations dedicated to the CPU. The CPU version usually uses a number of threads that is equal to the number of processors, while the effective use of the GPU computing power usually requires that hundreds or thousands of threads be used. In this paper we present an approach based on the solutions proposed in [8, 9, 10, 30], which use a single colony with a single pheromone memory matrix. According to the classification given in [20], this is a coarse-grained master-slave approach. One of the advantages of this approach is that the solution search process remains relatively faithful to that of the sequential ACS."
    }, {
      "heading" : "2.1. Characteristics of GPU computing",
      "text" : "The use of graphics processors for general purpose computing requires taking into account the significant differences between the GPU and CPU architectures [19]. CPUs are designed to achieve high-speed processing of a single instruction stream (or multiple in the case of multi-core chips). To achieve this goal, CPUs are equipped with complex circuits for branch prediction, thus performing calculations out-of-order and doing vector computations efficiently (e.g. SSE, AVX). Moreover, they are also equipped with multi-level cache\nmemories of a size up to a few megabytes per core. All of these solutions are designed to hide global memory access latency and to maximize the use of the available computing power. GPUs, on the other hand, are designed to efficiently perform a large number of independent parallel computations required in the process of graphics generation. The relative slowness of the computations for a single pixel is balanced by the high throughput resulting from a significant degree of data-parallelism.\nFigure 1 shows a simplified diagram of a GPU. Usually, the GPU contains a large number of stream processors (or CUDA cores in the case of Nvidia GPUs), each belonging to one of several streaming multiprocessors (SM). At any given moment a single core performs calculations for a single thread. Threads are grouped into blocks, with each block assigned to a single SM. Cores belonging to the same SM share, among others, the register file, local memory, instruction fetch and decoding, and load/store units. By sharing various auxiliary units, more computing cores can be packed into a single SM at the expense of limited flexibility of calculations of individual cores.\nThe long delays, often at an order of hundreds of cycles, in accessing the global (main) memory are one of the main obstacles to efficient parallel computations on GPUs [17, 19]. The GPU memory bus is wider than the memory bus of the CPU and has a relatively large bandwidth (often hundreds of GB/s), but it is often still not enough to provide data for all of the cores of the GPU. For this reason, the GPU programming model assumes the use of a large number of threads, e.g. often tens or hundreds of thousands, between which switching is fast or even free (zero-overhead scheduling). While a group of threads is waiting for the completion of data transfer to or from the global memory, it is possible to perform calculations by threads for which data have already been transferred. Summarizing, a large number of the GPU processing elements allows to obtain high speedups, provided that the computations are largely independent and enough data is transferred in time [19]."
    }, {
      "heading" : "3. Ant Colony System",
      "text" : "The Travelling Salesman Problem (TSP) is one of the most often studied combinatorial optimization problems [28]. It is NP-hard but has a relatively simple definition, which allows one to focus one’s attention on the algorithm, hence the TSP has become the de facto standard problem used to test heuristic algorithms. In this paper we also chose the TSP (symmetric version) as a tool to test the proposed algorithms, but the conclusions drawn should be useful in the context of other problems that can be solved by the ACS.\nIn the TSP the salesman has to visit each city from a given set exactly once and return to the starting city by taking into account that the trip between any pair of cities is associated with a given cost. The solution to the problem is a route with the minimum total cost. TSP can be defined by using a complete graph G = (V,A), where V = {1, 2, . . . , n}, is a set of nodes (cities), whereas A = {(i, j)|i, j ∈ V, i 6= j} is a set of edges connecting the nodes. A cost dij is defined for each edge (i, j) ∈ A.\nThe TSP was one of the first problems used to test the performance of algorithms as inspired by the behavior of ants, including the Ant System and its enhanced version - the Ant Colony System [11, 12]. In\neach iteration of the ACS, each ant builds a complete solution to the problem as follows: each ant starts building the solution from a randomly selected node (city). In each successive step the ant extends its current partial solution with one of the previously unvisited neighboring nodes of the current node (in the case of the TSP each node is adjacent to all other nodes). Choosing a node in graph G = (V,A) is equivalent to choosing a corresponding edge because only a single (unique) edge exists between each pair of nodes. The choice of the next node j of an ant k located in node i is carried out according to the formula:\nj =\n{ arg maxl∈Jik [τil] · [ηil]\nβ , if q ≤ q0 J, if q > q0,\n(1)\nwhere ηil is the cost associated with the selected edge (i, l), τil is the value of pheromone on the edge (i, l), J ik is the set of unvisited (candidate) nodes of ant k and q0 is a parameter. J is a node (city) selected with the probability defined by:\nP (J |i) = [τiJ ] · [ηiJ ] β∑\nl∈Jik [τil] · [ηil] β . (2)\nThe choice defined by formula (1) is controlled by parameter q0 (q0 ∈ [0, 1]), whose value is compared with q chosen randomly from uniform distribution. If q < q0, the choice is greedy, i.e. the edge (i, j) with the highest product of the pheromone trail, τij , and the additional (heuristic) information about the problem, ηij , is selected. Otherwise q > q0, and the selection is made randomly with the probability as defined by Eq. 2 derived from the Ant System [12]. In the first case we can talk about exploitation of knowledge accumulated by the algorithm in the pheromone trail values. In the second case we can talk about exploration of the solution space [11].\nUsually, a relatively high value of q0 is used, e.g. 0.9, so the ACS can find good quality solutions in less time than the ACO. In order to further reduce the computation time required to evaluate Eq. 1, where q < q0, the choice is narrowed down to the so-called candidate set containing only a subset of the closest unvisited nodes of the current node. The size of the candidate set, cl, is typically 10 to 25 [11, 13]. The use of the candidate set is justified by the insight that solutions of good quality are comprised mainly of edges connecting nodes in the direct vicinity. If all elements of the candidate set are already elements of the partial solution, the next element is selected from the remaining nodes (cities). The candidate set for each node (city) is typically calculated at the beginning of the algorithm and does not change. It is worth mentioning that Randall and Montgomery [21] proposed dynamic update schemes for candidate sets for the TSP and the Quadratic Assignment Problem. The dynamic version of the candidate sets allowed to slightly improve the quality of the solutions relative to the static version, but at the expense of a significantly increased computation time of the algorithm.\nTwo types of pheromone updates are used in the ACS. The first, called the local pheromone update, involves \"evaporating\" a small amount of the virtual pheromone from the pheromone trail every time the corresponding edge, (a, b), is selected by an ant. The update is according to the formula:\nτab ← (1− ρ) · τab + ρ · τ0 , (3)\nwhere ρ ∈ (0, 1) is a parameter and τ0 is the initial pheromone trail value. The local pheromone update reduces the likelihood of the same edge being chosen by subsequent ants, and thus improves exploration of the solution space. After all the ants have built complete solutions, a global pheromone update is performed according to the formula:\nτab ← (1− α) · τab + α ·∆τab , (4)\nwhere\n∆τab =\n{ (Lgb)\n−1, if (a, b) ∈ global_best_tour , 0, otherwise ,\nwhere α ∈ (0, 1) is the coefficient of evaporation (decay) of the pheromone and Lgb is the length of the globally best solution found so far. The global pheromone update places emphasis on exploitation of the search space around the best solutions found to date, hence improving the convergence of the algorithm [13].\nFigure 2 presents a pseudocode for the ACS. Each ant starts building a solution from a randomly selected node (line 4). In the subsequent steps the ant extends the partial solution (lines 6–12) with nodes selected"
    }, {
      "heading" : "12 end",
      "text" : "in accordance with formula (1). The complexity of the algorithm equals O(#iterations ·#ants ·#nodes2) as the number of iterations needed to select the next node (line 8) is on the order of O(#nodes). It is often assumed that #ants = O(#nodes), thus the complexity of the algorithm is O(#iterations ·#nodes3). The pseudocode for the procedure of the next node selection, in accordance with formulas (1) and (2) is shown in Fig. 3. In the first place a node is selected from the candidate set containing cl nearest neighbors (lines 5–9) of the current node. Depending on the value of parameter q0, the selection is made in a deterministic manner (line 13) or randomly with the probability defined by formula (2). If all of the nodes in the candidate set are already a part of the solution, the next node is chosen greedily from the remaining nodes (line 18)."
    }, {
      "heading" : "3.1. Parallel ACS for the GPU",
      "text" : "When designing the algorithms presented in this work we were guided by the conclusions and comments presented in the literature on the parallel ACO and the MMAS for the GPU. Efficient implementation of the ACS for the GPU, however, requires that one take into account the significant differences in relation to the ACO and the MMAS. Based on the results and conclusions presented, among others, in [5, 8, 10], we applied a model of parallelization in which each ant corresponds to a single thread block. The stream processors (cores) in modern GPUs are grouped into so-called warps working in the SIMT mode (single instruction, multiple threads) [34]. This means that all threads in the warp execute the same instruction at the same time. In the case of Nvidia GPUs, the warp size is typically 32. Each divergence (branch) in the control flow between threads in the warp, e.g. due to a conditional statement, requires that all the threads execute the instruction for both paths, which may negatively affect the computation time. This is one of the reasons why the task parallelism model with a single ant per thread is not well suited for the GPU [5]. For the same reason, the block size should be a multiple of the size of the warp, and such a rule was adopted in our implementation. The warp size also affects the size of the candidate set. Following [8], we used the value cl = 32. As a result, all threads in the warp perform computations required to select the next node according to formulas (1) and (2)."
    }, {
      "heading" : "3.1.1. Pseudo-random proportional rule",
      "text" : "In the ACS, the solution construction process is carried out according to the pseudo-random proportional rule as defined by formulas (1) and (2). In most implementations, only the elements of the candidate set are\nconsidered when applying the rule, and only if the set is empty are the remaining nodes considered. To construct a candidate set with cl = 32 elements (Fig. 3 lines 5–8), one needs to check whether the nodes belonging to the list of cl nearest neighbors of the current node are already a part of the solution. The use of an array of boolean values indicating the membership of a given node in the partial solution allows us to perform this step in parallel by all warp threads in a constant time O(1). If the candidate set is not empty, a random value q ∈ [0, 1) is drawn (we used the XORWOW pseudo-random number generator from the cuRAND library).\nIf q ≤ q0 then the current solution is extended with a node to which an edge with the maximum value of the product of the pheromone trail and the value resulting from the heuristic knowledge about the problem leads. The selection of the maximum value is, of course, an example of a reduction operation that can be performed in O(log cl) steps. Nvidia GPUs starting with Kepler architecture offer instructions that enable direct data exchange between the threads of a warp. These instructions give the threads access to values stored in the registers of other threads in the warp without the usage of shared memory [34]. This allows, among others, for more effective implementation of the reduction algorithm [1]. In our implementation of the reduction algorithm we applied the variants of the __shfl() instruction, which is available in the Nvidia GPUs starting from Fermi architecture (compute capability 3.0 and up).\nIf q > q0 then one of the nodes belonging to the candidate set is chosen with the probability defined by formula (2), also known as the roulette wheel. Calculation of the selection probabilities for the elements of the candidate set requires calculation of the products of the corresponding pheromone trails and the heuristic knowledge values. Obviously, this can be calculated by using the reduction algorithm. Figure 4 shows an example of the parallel element selection according to the roulette wheel method. Because in the ACS the selection is limited to elements of the candidate set, which in our case had a maximum size of 32, there was no need for a more complex algorithm, e.g. I-Roulette proposed in [5]. Limiting the size of the candidate set to 32 also provides other advantages. First, there is no need for synchronization barriers because the threads in a warp work synchronously (SIMT mode). Second, by limiting the size to 32 elements we could use the efficient intra-warp data exchange functions introduced in the Fermi architecture which enable threads to access each other’s registers without the need for shared memory. In addition to the __shfl, we used the __ballot function. The result of the function is an N bit integer, with the N -th bit equal to 1 if the predicate parameter of the N -th thread has a non-zero value. Because registers are the fastest type of memory available, their use can have a positive impact on the computation time, as was confirmed by the\nresults presented in [8]. In the case of an empty candidate set, the next node is one of the remaining nodes to which the edge with the highest product of the pheromone trail and the heuristic knowledge values leads. This step can be done in O(n/p+ log p), where n is the number of the remaining nodes and p is the number of threads in the block. This is due to the fact that each of the p threads sequentially selects its own maximum out of the n/p elements, and next the parallel reduction is used to choose the global maximum from the p thread local maxima. Summarizing, parallel selection of the next node in the ACS can be done in O(n/p+ log p), where n is the size of the problem and p is the number of threads in a block."
    }, {
      "heading" : "3.1.2. Pheromone update",
      "text" : "Two kinds of pheromone memory updates are performed in the ACS. These are global and local. The global pheromone update involves depositing an additional amount of pheromone on the edges belonging to the best solution found so far. This is relatively easy to parallelize. In our implementation it is performed in a separate kernel by a block of 128 threads. Based on the preliminary computational experiments, we found that a greater number of threads did not improve the algorithm runtime. This can be explained by the fact that most of the time in the ACS is spent on constructing the solutions and on local pheromone memory updates.\nThe second type of pheromone update in the ACS is called local and is performed after the \"transition\" of an ant over an edge and after the corresponding element has been included in the partial solution. It is possible that the pheromone trail on the edge will be updated more than once in the same iteration of the algorithm if it is selected by any of the remaining ants. Thus the local pheromone update is inherent to the solution construction process, and in our implementation it is carried out in the kernel responsible for the transition rule.\nIt should be noted, however, that some differences between the sequential and parallel search processes could arise. The obvious difference comes from the fact that in the parallel version a given number of ants simultaneously construct solutions to the problem and the relative order of their execution may vary between successive algorithm runs, while in the sequential version the ants make their decisions in the same order. If in the parallel implementation a single kernel execution is responsible for the extension of the ants’ partial solutions with single elements, the basic difference between the sequential and parallel version boils down to a different relative order of the pheromone memory updates. For example, suppose two ants (with indices 0 and 1) are located at node a. In the sequential version the ant with index 0 will select the next node and will perform the local pheromone update on the chosen edge before the ant with index 1 starts, thus the action of the former ant could have an effect on the actions of the latter ant. In the case of the parallel execution, the ant with index 1 could take action before the ant with index 0 due to slightly different timings of the threads corresponding to both ants. Moreover, even if the order of execution of the operations on the GPU is the same as in the sequential version, it is still not certain that ant 1 will see a new pheromone value written by ant 0. This could result from the weak memory model used in modern GPUs which does not guarantee that\nchanges made in the global memory by one thread are immediately visible to the other threads and appear in the order they were made [34]. Actually, the pheromone update operation is a read-modify-write type of instruction and as such should be performed atomically. The CUDA provides a set of atomic instructions, among them atomicCAS or the compare-and-swap, but their use entails additional overhead and should be limited.\nFor the purpose of this study, two parallel versions of the ACS for the GPU were designed (plus one with a selective pheromone memory described in the next section). In the first implementation, named ACS-GPU, execution of the solution construction process is close to the execution of the sequential version, i.e. each ant expands its current solution with a new node and then performs the local pheromone update. The next step begins after each ant has extended its partial solutions with single elements. In this version, each step corresponds to a single GPU kernel execution. The local pheromone update is implemented by using the atomicCAS instruction to prevent any memory inconsistencies.\nThe second implementation, denoted ACS-GPU-Alt, aims to achieve a maximum speedup, perhaps at the expense of the quality of the solutions found. In this version the entire solution construction process and the local pheromone update are performed in a single kernel execution. In this way the number of kernel calls is reduced from O(n) to O(1), where n is the size of the problem. Although the computational overhead associated with the kernel call is quite small (at the order of µs), it could become significant if the total algorithm execution time is also small. Another important difference in relation to the ACS-GPU version is resignation from the use of costly atomic instructions in the local pheromone update process. It may therefore happen, due to the simultaneous global memory reads and writes, that some of the updated pheromone values will be lost. Obviously, this could influence the probability of an edge being selected by subsequent ants, but it should not lead to a construction of invalid solutions. It is also worth noting that if the complete solution is built during a single kernel execution, it is possible that some ants will build their complete solutions before the other ants even start. This stems from the fact that there is no enforced synchronization between the successive steps of the solution construction process, as in the ACSGPU version. This is especially likely if the number of ants is much greater than the number of physically available GPU stream processors (CUDA cores), which is typical in the case of large TSP instances (with thousands of cities). A detailed list of the proposed algorithms’ kernels and blocks/threads configurations is given in Tab. 1."
    }, {
      "heading" : "3.2. Selective pheromone memory",
      "text" : "Based on our earlier ideas [23, 24], we developed a parallel version of the ACS for the GPU in which the pheromone matrix was replaced with a selective pheromone memory of a smaller size. In the ACS, the n × n pheromone matrix holds the pheromone values for every edge (u, v) of graph G(V,A), whose nodes represent cities and the edges correspond to roads between the cities (in the case of the TSP). Every node u has n− 1 edges connecting it with neighboring nodes, and thus for every node there are n− 1 pheromone values stored. In the proposed selective pheromone memory model, for every node only a limited number, s < n, of edges can have a non-minimum value, while the rest are assumed to have a minimum value, i.e. τmin. In the previous work [23, 24], a slightly different selective pheromone memory model was proposed in\nwhich, similarly, the total number of pheromone values was limited, but no limit was put on the number of pheromone values for particular nodes.\nThe selective memory model is associated with several decisions. It is necessary to select the edges for which the pheromone values should be stored. Intuitively, the pheromone values should be stored for the \"important\" edges, i.e. for edges which are necessary in order to build solutions of good quality. Generally, it is difficult to know a priori which edges are important for good quality solutions to be found, hence we applied a dynamic selection criterion. It is applied each time a pheromone trail is updated, therefore, it should be relatively quick to compute in order not to significantly increase the total algorithm runtime. For this reason we adopted a simple algorithm working as follows. If the pheromone update applies to an edge for which the pheromone value is already stored in the (selective) pheromone memory, this value is modified similarly to the ACS with a standard pheromone matrix. Otherwise, a new pheromone trail value is calculated by using the ACS rules but assuming that the current value of the pheromone trail equals the minimum pheromone level, i.e. τmin. At the same time, if the maximum number of pheromone trails, s, per node is exceeded, the new trail overrides the least recently added pheromone trail. Hence the algorithm resembles the least recently used (LRU) heuristic used in the CPU cache memory controller implementations.\nThe pseudocode of the selective pheromone memory update algorithm is shown in Fig 5. The selective pheromone memory data are stored as a list of records, one for every node u. A record for a node u consists of a pair of vectors and the variable tail. The first vector of the pair is an index vector and serves to identify the indices of the nodes which are connected with node u. The second vector stores the values of the pheromone trails on the edges identified by the first vector. The variable tail stores the index of the most recently added pheromone trail. Knowing the tail value and the size of the vector s allows us to easily calculate the index of the least recently added value. As the maximum number of pheromone trails is limited, the pheromone update process resembles that of a circular buffer. Both the pheromone read and the update operations require, in the worst case scenario, checking the contents of the entire index vector, hence their complexity both in the average and in the worst case scenario, is O(s). Assuming that size s of the vectors used in the selective memory is a (small) constant, the complexity becomes O(1). In the parallel implementation for the GPU, the search for a value in the index vector could be efficiently done by using the warp voting functions, i.e. __ballot and __shfl.\nThe selective pheromone memory consists of n records, each containing two vectors of size s. Previous studies have shown that it is enough to remember only a small number of pheromone trail values to obtain good quality results [23, 24]. This is largely consistent with the observations of the convergence of PopulationBased ACO (PACO), in which the pheromone matrix is defined on the basis of a population of solutions, often of a very small size [16]. Intuitively, it is sufficient that only the edges belonging to the optimal solution (assuming there is only one) have pheromone trail values that are different from the minimum value of τmin. Of course, the optimal solution is not known a priori, therefore the selective memory size, i.e. n× s, should be at least several times larger than the size of the problem, n. On the other hand, the larger the memory size, the higher the computational cost associated with the pheromone read and update operations. Based on the preliminary experiments, we chose a value of s = 8 for which most of the pheromone trails corresponding to the edges traversed (selected) by ants fit into the memory as shown in Fig. 6. Hence, at any time no more\nthan 8 edges for every node could have pheromone trail values different from τmin. Because s is constant, the final memory complexity of the selective pheromone memory equals O(ns) = O(n), versus O(n2) for the standard pheromone matrix."
    }, {
      "heading" : "4. Experiments",
      "text" : "The experiments consist of several parts. We started with a focus on the parallel ACS implementation by using the standard pheromone matrix for the GPU, then a performance-oriented investigation of the parameters’ values, and we ended by focusing on the parallel ACS with the selective pheromone memory."
    }, {
      "heading" : "4.1. Parallel vs sequential ACS",
      "text" : "The first part of the experiments was focused on the ACS algorithm’s performance on the GPU. Three versions of the ACS algorithm were considered. The first, labeled ACS-SEQ, was the reference sequential implementation of the ACS in C language by Thomas Stützle [27]. The second version was the abovementioned ACS-GPU, which is a parallel version of the ACS for the GPU; it is a version in which the process of solution construction by the ants is closest to the sequential version of the ACS. The third version, ACS-GPU-Alt, is an alternative implementation for the GPU in which the main emphasis is put on the performance. In this version the complete solution is built in a single kernel execution and the pheromone memory updates are performed without the use of atomic instructions, such as CAS.\nThe algorithms were implemented in C++ with the CUDA library (version 6.5) and compiled using GCC v. 4.8 with the -O2 optimization switch. The CUDA was used because of the relative ease of programming the Nvidia GPU that was used in the experiments. Pseudo-random numbers were generated using the XORWOW generator from the cuRAND library. The calculations were performed on a computer equipped with a CPU and GPU, whose specifications are given in Tab. 2.\nThe ACS algorithm, similarly to the ACO, requires setting the values of several parameters. On the basis of the guidelines presented in the literature and on preliminary computations, the following values of\nthe parameters were used in the experiments. The number of ants m equals the size of the problem, i.e. m = n (unless stated otherwise); β = 3; α = 0.2 (global pheromone evaporation coefficient); ρ = 0.01 (local pheromone evaporation coefficient). For the efficiency of the ACS, the value of the parameter q0 is very important because it impacts the balance between the exploitation and the exploration of the solution space. Based on the preliminary experiments, we found that a value of q = (N −20)/n leads to good quality results in the case of the TSP. It is worth noting that the high q0 value improves convergence, however, it may lead to getting stuck in a local optimum. The computations were repeated 30 times for every combination of an algorithm and set of parameter values.\nWe agree with Delevacq et al. [10] that a reliable evaluation of a parallel algorithm should take into account the time (speedup) as well as the quality of the results. The speedup values were calculated in relation to the sequential implementation of the ACS algorithm from the ACOTSP software by Stützle [27]. A similar solution was adopted in the literature [8, 9] in order to make the comparisons more reliable and reproducible. The sequential version takes advantage of only a single CPU core, which makes the GPU vs CPU comparison not entirely fair, as was pointed out in [19], however, such a convention was adopted in most of the articles on the parallel GPU versions of the ACO and MMAS algorithms as presented in the literature, including [5, 10].\nIn the first part of the experiments the algorithms ACS-SEQ, ACS-GPU and ACS-GPU-Alt were run on a set of 8 TSP instances selected from the TSPLIB repository: d197, A280, lin318, pcb442, rat783, pr1002, nrw1379, pr2392. The number of iterations of the algorithm was equal to 1000, so the total number of generated solutions was equal to 1000 · n, where n is the size of the problem. Table 3 shows the mean timings and speedups obtained for the investigated algorithms relative to the sequential ACS implementation (ACS-SEQ). As can be seen, the highest speedup was obtained for the ACS-GPU-Alt, which, depending on the TSP instance, obtained speedups from 12.8 to 17.4 for d198 and pcb442 instances, respectively. For comparison, the ACS-GPU achieved a minimum speedup of 1.97 for the d198 instance and a maximum of 6.4 for the pcb442 instance. Small speedup values obtained for the ACS-GPU can be explained by the fact that the pheromone updates were performed using atomic operations, which are more expensive than plain read/write operations. The penalty of many more CUDA kernel invocations in the ACS-GPU is not without significance because each kernel invocation was responsible for extending the ants’ solutions only by a single element. In contrast, in the ACS-GPU-Alt, complete solutions were constructed during a single kernel execution. Moreover, no expensive atomic instructions were used in the ACS-GPU-Alt pheromone update implementation.\nThe question arises as to what extent this \"relaxed\" model of the ACS execution affects the quality of the solutions in the case of the ACS-GPU-Alt. Figure 7 shows the box-plot of the mean distance to the best solution obtained for the algorithms investigated here. As can be observed, the ACS-SEQ and ACSGPU achieved a similar quality of solutions for most of the TSP instances, except for a280, for which the sequential algorithm was clearly better. Much more interesting is the plot for the ACS-GPU-Alt algorithm, which found solutions of much better quality for larger instances, i.e. rat783, pr1002, nrw1379, pr2392. This can be explained by the differences in the implementation of the local pheromone update. In the ACSGPU-Alt, the local pheromone updates are not performed atomically, so some pheromone updates can be\nlost. Because these updates result in a reduction (evaporation) of the pheromone on the edges selected by the ants, the pheromone trail values remain higher than they should. Limited evaporation of the pheromone causes the solutions built to be less diverse and more centered around the best solution found so far. We can state that the process of solution construction in the ACS-GPU-Alt is more exploitation-oriented, which allows us to obtain better results, especially in the case of larger TSP instances."
    }, {
      "heading" : "4.2. Limiting the number of local pheromone updates",
      "text" : "In order to better investigate the effects of the observed phenomena, the ACS-GPU-Alt was run with a limited number of local pheromone updates. More precisely, the local pheromone update was performed for every k-th edge selected by an ant. The following values of k were used: 1, 2, 4, 8 and 16. Table 4 contains the mean times of the solution construction depending on the period of the local pheromone updates k. The relative speedup values are also shown. As can be observed, the less frequent the updates, the shorter the algorithm runtime was and thus the higher the speedups. This confirms earlier observations that the time spent on the execution of local pheromone updates has a significant impact on the algorithm runtime; for example, for k = 16 and instance pcb442, a speedup of 21.9 was obtained in contrast to 17.7 if the update was performed with the standard period (k = 1). Of course, the greatest speedup could be obtained by completely removing the local pheromone updates. However, the local pheromone update is essential to the performance of the ACS in terms of solution quality.\nTable 5 shows the mean relative error of the obtained solutions depending on the period of the local pheromone update. As can be observed, a change in the period of the local pheromone update has a significant\nACS-SEQ 16046 (1.7%) 2579 (0.0%) 42404 (0.9%) 51695 (1.8%) 9672 (9.8%) 290818 (12.3%) 65018 (14.8%) 434362 (14.9%) ACS-GPU-Alt 15919 (0.9%) 2579 (0.0%) 42203 (0.4%) 51511 (1.4%) 8939 (1.5%) 266416 (2.8%) 58350 (3.0%) 391691 (3.6%)\nimpact on the quality of the solutions, however, whether the impact is positive or negative depends on the size of the problem. For the smallest instances, i.e. d198 and a280, any increase in the period k of the local pheromone update impairs the quality of the solutions. For the pcb442 instance, significantly better solutions were found for k = 8, and in the case of the rat783 TSP instance, the best results were observed for k = 2. For the two largest instances, i.e. nrw1379 and pr2392, significantly better results were obtained for all k ≥ 2. However, the best quality results for the nrw1379 instance were found for k = 4, and for the pr2392 instance the best value was k = 8. These results can be explained by the fact that the local pheromone update is intended to increase exploration of the problem solution space. If the local pheromone update is performed less often, the search process becomes more exploitative, i.e. the solutions constructed are close to the best solutions found so far. This is beneficial for larger problems, particularly if the available computational budget is small. On the other hand, increased exploitation does not improve the quality of the solutions for the smaller problems, and may even lead to getting stuck in local optima, which results in worse quality solutions.\nSummarizing, a less frequent local pheromone update reduces the computation time of the parallel version of the ACS for the GPU, however, it affects the quality of the solutions. For the smaller problems this is disadvantageous, but for problems of a larger size (> 1000 cities) it significantly improves the quality of the results. Table 6 shows the best solutions obtained both for the CPU and the GPU versions of the ACS. As can be observed, in almost every case the GPU version found better quality solutions and its dominance increased along with the size of the problem."
    }, {
      "heading" : "4.3. Manipulating the number of ants",
      "text" : "An interesting question is what impact the number of ants m has both on the runtime and on the quality of the solutions of parallel ACS for the GPU. To measure this effect, in the following experiments a constant computational budget, b, was set and measured in the total number of solutions built. In a single iteration of the algorithm, each ant builds one complete solution to the problem, hence if the number of ants is m and the total number of solutions to build is b, then the number of algorithm iterations equals b/m. It is worth recalling that in the previous experiments we used the number of ants m equal to the size of the TSP instance solved, as is often recommended in the literature [5, 8, 10, 13]. The ACS-GPU-Alt algorithm was run for the two largest instances, nrw1379 and pr2392, with the number of ants m ∈ {128, 256, 512, 1024, n}, where n was the size of the problem. The smallest number of ants was set to 128 because in order to make good use of the hundreds (in our case 1536) of GPU CUDA cores, the number of active threads should be large [19, 34]. In our approach a single ant solution was built by a block of 32 threads, hence the minimum number of threads was equal to 128 ·m.\nTable 7 shows the results. For both problems the highest mean speedup and the lowest mean solution error were obtained when the number of ants was equal to m = 256. Please note that for both instances, i.e. nrw1379 and pr2392, the smallest speedups and the largest mean solution error were obtained for the number of ants equal to the size of the problem, m = n. For example, in the case of the pr2392 instance, the mean error was 10.67% for m = n, while for m = 256 the mean error was only 5.29%.\nSummarizing, both the period of the local pheromone update and the number of ants have a significant impact on the quality of the results. Now the question is whether combining the less frequent pheromone update with a smaller number of ants makes it possible to further improve the quality of the solutions and/or the speedups. In order to answer this question, the ACS-GPU-Alt algorithm was run for the nrw1379 and pr2392 instances with the number of ants equal to 256 and the period of local pheromone update k equal to 2, 4, 8 and 16. Table 8 shows the results of the computations. A larger update period resulted in greater speedups for both TSP instances. The highest speedups were obtained for k = 16 and were equal to 20.058x and 18.785x for the nrw1379 and pr2392 instances, respectively. At the same time, increasing the period of the local pheromone update caused a deterioration in the quality of the results. Taking into account the previous results, we can conclude that both a lower number of ants and less frequent local pheromone updates can reduce the computation time, but at the expense of the quality of solutions."
    }, {
      "heading" : "4.4. Parallel ACS with a selective pheromone memory",
      "text" : "The last part of the experiments was focused on the ACS-GPU-SPM in which the pheromone matrix was replaced with a selective pheromone memory. The number of ants and the period of the local pheromone update have the largest effect on selective memory performance. Based on the results of the previous experiments, the ACS-GPU-SPM was run with the number of ants m equal to 128 and 256, and the period of the local pheromone update ∈ {1, 2, 4, 8, 16}. Figure 8 shows the plot of the mean speedup for the ACS-GPU-Alt and ACS-GPU-SPM algorithms. As can be observed, the speedups for the algorithm with\nthe selective pheromone memory were lower than for the ACS-GPU-Alt algorithm with the full pheromone matrix. This is due to the fact that both the reading and writing operations for the selective pheromone memory are more costly than for the standard pheromone matrix implemented using a plain array. The largest differences can be observed when the local pheromone update was performed in every step of the algorithm (k = 1). For both instances, the speedups obtained for the ACS-GPU-SPM are about 30% less than those for the ACS-GPU-Alt. With the increase of the local pheromone update period k, the differences in the algorithms’ runtimes decreased. For k = 16, the speedup of the ACS-GPU-SPM was approx. 25% worse than the speedup of the ACS-GPU-Alt. Although the use of a selective pheromone memory had a negative impact on the algorithm runtime, it improved the quality of the results, which can be observed in Fig. 9. In all cases, the mean solutions error for the ACS-GPU-SPM was much smaller than the error for the ACS-GPU-Alt algorithm. The largest differences can be seen for the larger of the two problems, i.e. pr2392, and with the increase of the local pheromone update period. In particular, the ACS-GPU-SPM with the local pheromone update period equal to 16 obtained a mean error of 3.72% and 4.66% for the nrw1379 and pr2392 TSP instances, respectively. For the same values of parameters the ACS-GPU-Alt obtained mean error values equal to 4.72% for nrw1379 and 8.4% for the pr2392 instance. This is a typical example of a compromise between the performance and quality of the results, however, the selective pheromone memory requires much less space in the global memory of the GPU.\nFigure 10 shows a comparison of the mean speedups for the parallel algorithms investigated here. The ACS-GPU algorithm was the slowest, for which the average speedup amounted to a maximum of 6.48x for\ninstance pcb442. the The ACS-GPU-Alt algorithm was much faster, with a mean speedup of approx. 15x and up to 17.72x obtained for the pcb442 instance. The largest speedup value of 24.29x was obtained by the ACS-GPU-Alt* algorithm for the lin318 TSP instance. The ACS-GPU-Alt* version differed from the ACS-GPU-Alt by different values of parameters: the number of ants m = 256 and the period of the local pheromone update k = 4. These values were chosen based on previous experiments as providing the best compromise between the speed of execution and the quality of the results obtained. The ACS-GPU-SPM* version, also with the same values of parameters as in the ACS-GPU-Alt* version, obtained a speedup that was similar to the ACS-GPU-Alt, reaching a maximum value of 16.85x for the pcb442 instance.\nIn the next part of the experiments, both algorithms, i.e. the ACS-GPU-Alt and the ACS-GPU-SPM, were run with a fixed time limit, with the number of ants m = 256 and the period of the local pheromone update k = 4. Setting the same time limit for both algorithms allowed for an easy comparison in terms of the quality of the solutions. The limit was set for each instance separately based on how much time it took the ACS-GPU-Alt algorithm to perform 1000 iterations – however, with the \"base\" values of the parameters, i.e. m = n and k = 1. Table 9 presents a comparison of the quality of the results. For all instances, except lin318, the ACS-GPU-SPM produced results of significantly better quality than the ACS-GPU-Alt, according to the non-parametric Wilcoxon rank sum test (the level of significance was 0.05). Moreover, in all cases the best solution found by the ACS-GPU-SPM was of better quality than the solution found by the second algorithm. In conclusion, despite the fact that the selective pheromone memory is slower than the pheromone matrix, the improvement in the quality of the results is so large that it can get ahead of the faster algorithm.\nIn order to further investigate the performance of the ACS-GPU-SPM algorithm a few larger TSP instances were selected with the size ranging from 3038 to 14051 cities. Table 10 shows the best results out of 5 runs for each instance. As can be seen, the quality of the results degrades as the size of the problem grows. It follows from the fact that the size of the solution search space grows exponentially and the linear increase in the computation time is not sufficient to keep the quality of the results on a similar level. The quality of the results could be significantly improved if a local search heuristic was applied [10]. Nevertheless, the GPU processing power allows the ACS-GPU-SPM algorithm to generate and evaluate over 300000 solutions per second for the TSP instance with 14051 cities."
    }, {
      "heading" : "5. Summary",
      "text" : "In the paper, three novel, data-parallel versions of the ACS algorithm for the GPU are proposed, namely the ACS-GPU, ACS-GPU-Alt and ACS-GPU-SPM. All three algorithms were implemented by using the Nvidia CUDA framework and experimentally compared with the reference sequential implementation (ACOTSP) by Thomas Stützle [27]. A series of numerical experiments was conducted using a set of TSP instances of sizes ranging from 198 to 14051 cities chosen from the TSPLIB repository.\nGenerally, the solution construction process of the ACS is relatively easy to parallelize. However, the implementation of the local pheromone trail update is of key importance to the efficiency of the parallel ACS. The results showed that the ACS-GPU is closest to the sequential version in terms of solution quality. It is also the slowest of the three algorithms, with speedups up to 6.43x, mainly due to the use of the expensive atomic instructions (on the global GPU memory) during the local pheromone update. The ACS-GPU-Alt algorithm is the fastest, in which no expensive atomic instructions are used and complete solutions are built during a single kernel execution, hence diminishing the overhead associated with the GPU kernel call. The ACS-GPU-Alt achieved a maximum speedup of 17.4x. A further speedup can be obtained by performing the local pheromone update less frequently and by using a smaller number of ants. With the number of ants set to m = 256 and the local pheromone update performed every 4 steps, the ACS-GPU-Alt algorithm achieved a maximum speedup of 24.29x for the lin318 instance.\nIn the third algorithm, denoted as ACS-GPU-SPM, the pheromone matrix was replaced with a novel version of the selective pheromone memory. In the pheromone matrix, for every node u there exists a row of pheromone trails values, one for each edge starting at node u. In the selective pheromone memory, for every node u only a small, constant number, s, of pheromone trails values is stored, i.e. for a selected subset of the edges starting at node u, while for the rest the minimum value, τmin, is presumed. A set of the selected pheromone trails may change as a result of performing local or global pheromone updates. If the pheromone trail for the edge is in the selective memory, its value is updated, otherwise the new pheromone trail completely overwrites the least recently added pheromone trail to keep the size constant. On the one hand, the selective pheromone memory causes the solutions that are built to consist primarily of the selected edges and thus improves the exploitation of the solutions search space. On the other hand, the rotation of the trails resulting from the pheromone updates improves the exploration. The parallel implementation of the selective pheromone memory for the GPU is more complex than the standard pheromone matrix and turns out to be approx. 30% slower.\nIt should be noted, however, that if the ACS-GPU-Alt and the ACS-GPU-SPM were run with the same time limit, the latter would allow to obtain results of better quality. Moreover, both the ACS-GPU-Alt and the ACS-GPU-SPM allow to obtain results of quality better than the sequential ACS; for example, for the pr2392 instance, the sequential ACS was able to find a solution with a mean error of 16.34% (relative to the optimum) in 2412 seconds, while the parallel ACS-GPU-SPM found a solution with a mean error of 4.05% in 174 seconds.\nIn conclusion, it is possible to achieve an efficient parallel version of the ACS for the GPU. One of the key elements influencing the algorithm runtime is the implementation of the pheromone memory, and in particular of the local pheromone update. Abandoning full compliance with the modus operandi of the sequential ACS can significantly speed up the algorithm and even improve the quality of the results."
    }, {
      "heading" : "5.1. Further research",
      "text" : "As a part of further research, the algorithm should be tested by using a newer generation of GPUs, and it should be run simultaneously on multiple GPUs. Perhaps concurrent kernels execution may be used to speed up even further the computation for multiple ants and multiple colonies [3]. It would also be interesting to see how convergence of the algorithm changes if a local search heuristic is used, e.g. 3-Opt, as presented in [10]. Acknowledgments: This research was supported in part by PL-Grid Infrastructure. [1] NVIDIA: CUDA C Programming Guide. http://docs.nvidia.com/cuda/cuda-c-programming-guide/. Accessed: 2014-\n06-18.\n[2] Hongtao Bai, Dantong OuYang, Ximing Li, Lili He, and Haihong Yu. Max-min ant system on gpu with cuda. In Innovative Computing, Information and Control (ICICIC), 2009 Fourth International Conference on, pages 801–804. IEEE, 2009.\n[3] Alberto Cano, José María Luna, and Sebastián Ventura. High performance evaluation of evolutionary-mined association rules on gpus. The Journal of Supercomputing, 66(3):1438–1461, 2013.\n[4] Alberto Cano, Juan Luis Olmo, and Sebastián Ventura. Parallel multi-objective ant programming for classification using GPUs. J. Parallel Distrib. Comput., 73(6):713–728, 2013.\n[5] José M. Cecilia, José M. García, Andy Nisbet, Martyn Amos, and Manuel Ujaldon. Enhancing data parallelism for ant colony optimization on gpus. J. Parallel Distrib. Comput., 73(1):42–51, 2013.\n[6] U. Cekmez, M. Ozsiginan, and O.K. Sahingoz. A uav path planning with parallel aco algorithm on cuda platform. In Unmanned Aircraft Systems (ICUAS), 2014 International Conference on, pages 347–354, May 2014.\n[7] Ling Chen, Hai-Ying Sun, and Shu Wang. A parallel ant colony algorithm on massively parallel processors and its convergence analysis for the travelling salesman problem. Inf. Sci., 199:31–42, 2012.\n[8] Laurence Dawson and Iain A. Stewart. Candidate set parallelization strategies for ant colony optimization on the GPU. In Joanna Kolodziej, Beniamino Di Martino, Domenico Talia, and Kaiqi Xiong, editors, Algorithms and Architectures for Parallel Processing - 13th International Conference, ICA3PP 2013, Vietri sul Mare, Italy, December 18-20, 2013, Proceedings, Part I, volume 8285 of Lecture Notes in Computer Science, pages 216–225. Springer, 2013.\n[9] Laurence Dawson and Iain A. Stewart. Improving ant colony optimization performance on the GPU using CUDA. In Proceedings of the IEEE Congress on Evolutionary Computation, CEC 2013, Cancun, Mexico, June 20-23, 2013, pages 1901–1908. IEEE, 2013.\n[10] Audrey Delevacq, Pierre Delisle, Marc Gravel, and Michaël Krajecki. Parallel ant colony optimization on graphics processing units. J. Parallel Distrib. Comput., 73(1):52–61, 2013.\n[11] Marco Dorigo and Luca Maria Gambardella. Ant colony system: a cooperative learning approach to the traveling salesman problem. IEEE Trans. Evolutionary Computation, 1(1):53–66, 1997.\n[12] Marco Dorigo, Vittorio Maniezzo, and Alberto Colorni. Ant system: optimization by a colony of cooperating agents. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 26(1):29–41, 1996.\n[13] Marco Dorigo and Thomas Stützle. Ant colony optimization. MIT Press, 2004.\n[14] Sankha Baran Dutta, Robert D. McLeod, and Marcia R. Friesen. GPU accelerated nature inspired methods for modelling large scale bi-directional pedestrian movement. In 2014 IEEE International Parallel & Distributed Processing Symposium Workshops, Phoenix, AZ, USA, May 19-23, 2014, pages 448–456. IEEE, 2014.\n[15] Henrique Fingler, Edson Norberto Cáceres, Henrique Mongelli, and Siang W. Song. A CUDA based solution to the multidimensional knapsack problem using the ant colony optimization. In David Abramson, Michael Lees, Valeria V. Krzhizhanovskaya, Jack Dongarra, and Peter M. A. Sloot, editors, Proceedings of the International Conference on Computational Science, ICCS 2014, Cairns, Queensland, Australia, 10-12 June, 2014, volume 29 of Procedia Computer Science, pages 84–94. Elsevier, 2014.\n[16] Michael Guntsch and Martin Middendorf. A population based approach for ACO. In Stefano Cagnoni, Jens Gottlieb, Emma Hart, Martin Middendorf, and Günther R. Raidl, editors, Applications of Evolutionary Computing, EvoWorkshops 2002: EvoCOP, EvoIASP, EvoSTIM/EvoPLAN, Kinsale, Ireland, April 3-4, 2002, Proceedings, volume 2279 of Lecture Notes in Computer Science, pages 72–81. Springer, 2002.\n[17] Byunghyun Jang, Dana Schaa, Perhaad Mistry, and David R. Kaeli. Exploiting memory access patterns to improve memory performance in data-parallel architectures. IEEE Trans. Parallel Distrib. Syst., 22(1):105–118, 2011.\n[18] Pavel Krömer, Jan Platos, and Václav Snásel. Nature-inspired meta-heuristics on modern gpus: State of the art and brief survey of selected algorithms. International Journal of Parallel Programming, 42(5):681–709, 2014.\n[19] Victor W. Lee, Changkyu Kim, Jatin Chhugani, Michael Deisher, Daehyun Kim, Anthony D. Nguyen, Nadathur Satish, Mikhail Smelyanskiy, Srinivas Chennupaty, Per Hammarlund, Ronak Singhal, and Pradeep Dubey. Debunking the 100x GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU. In André Seznec, Uri C. Weiser, and Ronny Ronen, editors, 37th International Symposium on Computer Architecture (ISCA 2010), June 19-23, 2010, Saint-Malo, France, pages 451–460. ACM, 2010.\n[20] Martín Pedemonte, Sergio Nesmachnow, and Héctor Cancela. A survey on parallel ant colony optimization. Appl. Soft Comput., 11(8):5181–5197, 2011.\n[21] Marcus Randall and James Montgomery. Candidate set strategies for ant colony optimisation. In Marco Dorigo, Gianni Di Caro, and Michael Sampels, editors, Ant Algorithms, Third International Workshop, ANTS 2002, Brussels, Belgium, September 12-14, 2002, Proceedings, volume 2463 of Lecture Notes in Computer Science, pages 243–249. Springer, 2002.\n[22] Christian Schulz, Geir Hasle, André R Brodtkorb, and Trond R Hagen. Gpu computing in discrete optimization. part ii: Survey focused on routing problems. EURO Journal on Transportation and Logistics, 2(1-2):159–186, 2013.\n[23] Rafał Skinderowicz. Ant colony system with selective pheromone memory for TSP. In Ngoc Thanh Nguyen, Kiem Hoang, and Piotr Jedrzejowicz, editors, Computational Collective Intelligence. Technologies and Applications - 4th International Conference, ICCCI 2012, Ho Chi Minh City, Vietnam, November 28-30, 2012, Proceedings, Part II, volume 7654 of Lecture Notes in Computer Science, pages 483–492. Springer, 2012.\n[24] Rafał Skinderowicz. Ant colony system with selective pheromone memory for SOP. In Costin Badica, Ngoc Thanh Nguyen, and Marius Brezovan, editors, Computational Collective Intelligence. Technologies and Applications - 5th International Conference, ICCCI 2013, Craiova, Romania, September 11-13, 2013, Proceedings, volume 8083 of Lecture Notes in Computer Science, pages 711–720. Springer, 2013.\n[25] Erich Strohmaier, Jack Dongarra, Horst Simon, and Martin Meuer. TOP500 Supercomputer Site.\n[26] Y. Tan and K. Ding. A survey on gpu-based implementation of swarm intelligence algorithms. Cybernetics, IEEE Transactions on, PP(99):1–14, 2015.\n[27] Stützle Thomas. Ant colony optimization - public software. http://iridia.ulb.ac.be/~mdorigo/ACO/aco-code/ public-software.html. Accessed: 2014-06-18.\n[28] Michael Trick. David l. applegate, robert e. bixby, vasek chvátal , william j. cook. the traveling salesman problem: A computational study, princeton university press, princeton, 2007, ISBN-13: 978-0-691-12993-8, 606 pp. Oper. Res. Lett., 36(2):276–277, 2008.\n[29] Colin Twomey, Thomas Stützle, Marco Dorigo, Max Manfrin, and Mauro Birattari. An analysis of communication policies for homogeneous multi-colony ACO algorithms. Inf. Sci., 180(12):2390–2404, 2010.\n[30] Akihiro Uchida, Yasuaki Ito, and Koji Nakano. An efficient GPU implementation of ant colony optimization for the traveling salesman problem. In ICNC, pages 94–102, 2012.\n[31] Akihiro Uchida, Yasuaki Ito, and Koji Nakano. Accelerating ant colony optimisation for the travelling salesman problem on the gpu. International Journal of Parallel, Emergent and Distributed Systems, 29(4):401–420, 2014.\n[32] Kai-Cheng Wei, Chao-Chin Wu, and Chien-Ju Wu. Using CUDA GPU to accelerate the ant colony optimization algorithm. In Shi-Jinn Horng, editor, International Conference on Parallel and Distributed Computing, Applications and Technologies, PDCAT 2013, Taipei, Taiwan, December 16-18, 2013, pages 90–95. IEEE, 2013.\n[33] Robin M. Weiss. GPU-Accelerated Ant Colony Optimization. In Wen-mei W. Hwu, editor, GPU Computing Gems Emerald Edition, Applications of GPU Computing Series, pages 325 – 340. Morgan Kaufmann, Boston, 2011.\n[34] Nicholas Wilt. The cuda handbook: A comprehensive guide to gpu programming. Pearson Education, 2013.\n[35] Hassan A. Youness, Aziza Ibraheim, Mohammed Moness, and Muhammad Osama. An efficient implementation of ant colony optimization on GPU for the satisfiability problem. In Masoud Daneshtalab, Marco Aldinucci, Ville Leppänen, Johan Lilius, and Mats Brorsson, editors, 23rd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, PDP 2015, Turku, Finland, March 4-6, 2015, pages 230–235. IEEE, 2015."
    } ],
    "references" : [ {
      "title" : "Max-min ant system on gpu with cuda",
      "author" : [ "Hongtao Bai", "Dantong OuYang", "Ximing Li", "Lili He", "Haihong Yu" ],
      "venue" : "In Innovative Computing, Information and Control (ICICIC),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "High performance evaluation of evolutionary-mined association rules on gpus",
      "author" : [ "Alberto Cano", "José María Luna", "Sebastián Ventura" ],
      "venue" : "The Journal of Supercomputing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Parallel multi-objective ant programming for classification using GPUs",
      "author" : [ "Alberto Cano", "Juan Luis Olmo", "Sebastián Ventura" ],
      "venue" : "J. Parallel Distrib. Comput.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Enhancing data parallelism for ant colony optimization on gpus",
      "author" : [ "José M. Cecilia", "José M. García", "Andy Nisbet", "Martyn Amos", "Manuel Ujaldon" ],
      "venue" : "J. Parallel Distrib. Comput.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "A uav path planning with parallel aco algorithm on cuda platform",
      "author" : [ "U. Cekmez", "M. Ozsiginan", "O.K. Sahingoz" ],
      "venue" : "In Unmanned Aircraft Systems (ICUAS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "A parallel ant colony algorithm on massively parallel processors and its convergence analysis for the travelling salesman problem",
      "author" : [ "Ling Chen", "Hai-Ying Sun", "Shu Wang" ],
      "venue" : "Inf. Sci.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Candidate set parallelization strategies for ant colony optimization on the GPU",
      "author" : [ "Laurence Dawson", "Iain A. Stewart" ],
      "venue" : "Algorithms and Architectures for Parallel Processing - 13th International Conference,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Improving ant colony optimization performance on the GPU using CUDA",
      "author" : [ "Laurence Dawson", "Iain A. Stewart" ],
      "venue" : "In Proceedings of the IEEE Congress on Evolutionary Computation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Parallel ant colony optimization on graphics processing units",
      "author" : [ "Audrey Delevacq", "Pierre Delisle", "Marc Gravel", "Michaël Krajecki" ],
      "venue" : "J. Parallel Distrib. Comput.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Ant colony system: a cooperative learning approach to the traveling salesman problem",
      "author" : [ "Marco Dorigo", "Luca Maria Gambardella" ],
      "venue" : "IEEE Trans. Evolutionary Computation,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1997
    }, {
      "title" : "Ant system: optimization by a colony of cooperating agents",
      "author" : [ "Marco Dorigo", "Vittorio Maniezzo", "Alberto Colorni" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "Ant colony optimization",
      "author" : [ "Marco Dorigo", "Thomas Stützle" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "GPU accelerated nature inspired methods for modelling large scale bi-directional pedestrian movement",
      "author" : [ "Sankha Baran Dutta", "Robert D. McLeod", "Marcia R. Friesen" ],
      "venue" : "In 2014 IEEE International Parallel & Distributed Processing Symposium Workshops,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "A CUDA based solution to the multidimensional knapsack problem using the ant colony optimization",
      "author" : [ "Henrique Fingler", "Edson Norberto Cáceres", "Henrique Mongelli", "Siang W. Song" ],
      "venue" : "Proceedings of the International Conference on Computational Science,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "A population based approach for ACO",
      "author" : [ "Michael Guntsch", "Martin Middendorf" ],
      "venue" : "editors, Applications of Evolutionary Computing, EvoWorkshops 2002: EvoCOP,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Exploiting memory access patterns to improve memory performance in data-parallel architectures",
      "author" : [ "Byunghyun Jang", "Dana Schaa", "Perhaad Mistry", "David R. Kaeli" ],
      "venue" : "IEEE Trans. Parallel Distrib. Syst.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Nature-inspired meta-heuristics on modern gpus: State of the art and brief survey of selected algorithms",
      "author" : [ "Pavel Krömer", "Jan Platos", "Václav Snásel" ],
      "venue" : "International Journal of Parallel Programming,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Debunking the 100x GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU",
      "author" : [ "Victor W. Lee", "Changkyu Kim", "Jatin Chhugani", "Michael Deisher", "Daehyun Kim", "Anthony D. Nguyen", "Nadathur Satish", "Mikhail Smelyanskiy", "Srinivas Chennupaty", "Per Hammarlund", "Ronak Singhal", "Pradeep Dubey" ],
      "venue" : "37th International Symposium on Computer Architecture (ISCA",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "A survey on parallel ant colony optimization",
      "author" : [ "Martín Pedemonte", "Sergio Nesmachnow", "Héctor Cancela" ],
      "venue" : "Appl. Soft Comput.,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Candidate set strategies for ant colony optimisation",
      "author" : [ "Marcus Randall", "James Montgomery" ],
      "venue" : "Ant Algorithms, Third International Workshop,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "Gpu computing in discrete optimization. part ii: Survey focused on routing problems",
      "author" : [ "Christian Schulz", "Geir Hasle", "André R Brodtkorb", "Trond R Hagen" ],
      "venue" : "EURO Journal on Transportation and Logistics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Ant colony system with selective pheromone memory for TSP",
      "author" : [ "Rafał Skinderowicz" ],
      "venue" : "Computational Collective Intelligence. Technologies and Applications - 4th International Conference,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Ant colony system with selective pheromone memory for SOP",
      "author" : [ "Rafał Skinderowicz" ],
      "venue" : "Computational Collective Intelligence. Technologies and Applications - 5th International Conference,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "A survey on gpu-based implementation of swarm intelligence",
      "author" : [ "Y. Tan", "K. Ding" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Ant colony optimization - public software. http://iridia.ulb.ac.be/~mdorigo/ACO/aco-code/ public-software.html",
      "author" : [ "Stützle Thomas" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "cook. the traveling salesman problem: A computational study, princeton university",
      "author" : [ "robert e. bixby", "vasek chvátal", "william j" ],
      "venue" : "press, princeton,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    }, {
      "title" : "An analysis of communication policies for homogeneous multi-colony ACO algorithms",
      "author" : [ "Colin Twomey", "Thomas Stützle", "Marco Dorigo", "Max Manfrin", "Mauro Birattari" ],
      "venue" : "Inf. Sci.,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "An efficient GPU implementation of ant colony optimization for the traveling salesman problem",
      "author" : [ "Akihiro Uchida", "Yasuaki Ito", "Koji Nakano" ],
      "venue" : "In ICNC,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "Accelerating ant colony optimisation for the travelling salesman problem on the gpu",
      "author" : [ "Akihiro Uchida", "Yasuaki Ito", "Koji Nakano" ],
      "venue" : "International Journal of Parallel, Emergent and Distributed Systems,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Using CUDA GPU to accelerate the ant colony optimization algorithm",
      "author" : [ "Kai-Cheng Wei", "Chao-Chin Wu", "Chien-Ju Wu" ],
      "venue" : "International Conference on Parallel and Distributed Computing, Applications and Technologies,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "GPU-Accelerated Ant Colony Optimization",
      "author" : [ "Robin M. Weiss" ],
      "venue" : "GPU Computing Gems Emerald Edition, Applications of GPU Computing Series,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "The cuda handbook: A comprehensive guide to gpu programming",
      "author" : [ "Nicholas Wilt" ],
      "venue" : "Pearson Education,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2013
    }, {
      "title" : "An efficient implementation of ant colony optimization on GPU for the satisfiability problem",
      "author" : [ "Hassan A. Youness", "Aziza Ibraheim", "Mohammed Moness", "Muhammad Osama" ],
      "venue" : "23rd Euromicro International Conference on Parallel,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "GPUs offer attractive performance to energy consumption and the cost of purchase ratio, and allow to perform many types of computations more quickly while maintaining the same cost in relation to the CPUs [34].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 17,
      "context" : "At the same time, the number of high latency operations should be minimized, particularly involving global memory access [19].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "Good examples of efficient metaheuristic algorithms are algorithms inspired by the foraging behavior of ants, including the Ant Colony Algorithm (ACO), the Ant Colony System (ACS) and the MAX-MIN Ant System (MMAS) [13].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 18,
      "context" : "Similarly to other metaheuristic algorithms, ant colony algorithms are computationally demanding, therefore much research effort was put into developing efficient parallel versions for multi-processor computers [20].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : "To the best of our knowledge, this is the first such work, although GPU versions of some ant colony algorithms, including the ACO and the MMAS, were proposed in the literature [10, 20].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 18,
      "context" : "To the best of our knowledge, this is the first such work, although GPU versions of some ant colony algorithms, including the ACO and the MMAS, were proposed in the literature [10, 20].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 21,
      "context" : "We also present a third parallel version of the ACS which includes a new version of the selective pheromone memory as inspired by an earlier work [23, 24].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "We also present a third parallel version of the ACS which includes a new version of the selective pheromone memory as inspired by an earlier work [23, 24].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 18,
      "context" : "An extensive overview of the different approaches to parallelization of ant algorithms can be found in [20].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "Most of the parallel ACO versions dedicated to the CPU apply the multi-colony approach [20].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "In [29], a comparison can be found of the various communication topologies (policies) on the convergence of the MMAS algorithm solving the TSP.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 5,
      "context" : "[7], in which proof of the convergence (at infinity) of the parallel ACO is given.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] proposed an efficient parallel (GPU) version of the ACO for the TSP in which an efficient parallelization scheme (data-parallel) was applied of the solution construction process.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 28,
      "context" : "[31] proposed a GPU version of the Ant System (AS) for the TSP.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[10] studied a parallel implementation of the MMAS for the TSP on the GPU.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "In [32], based on the algorithm presented by Cecilia et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "[5], Wei et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 32,
      "context" : "[35], the parallel MMAS for the Satisfiability Problem (SAT) was presented.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "The parallel ACO for the GPU to accelerate bi-directional pedestrian movement simulation was described in [14].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "[4] presented Parallel multi-objective Ant Programming for the classification algorithm using GPUs.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "Not without significance is the fact that the authors implemented the algorithm in Java rather than in C, which suggests that the sequential version could be sped up, thus resulting in smaller relative speedup values [19].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 30,
      "context" : "The classification task was also addressed in [33], in which a parallel version of the AntMiner algorithm for the GPU was about 100x faster than the sequential version.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "[15] used the GPU to accelerate calculations of the ACO for the knapsack problem.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[2] to speedup the MMAS using the GPUs.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] to solve the path planning problem of an Unmanned Aerial Vehicle (UAV).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "A survey of recent advances in applying the GPUs to speedup other metaheuristic algorithms can be found in [18, 22, 26].",
      "startOffset" : 107,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "A survey of recent advances in applying the GPUs to speedup other metaheuristic algorithms can be found in [18, 22, 26].",
      "startOffset" : 107,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "A survey of recent advances in applying the GPUs to speedup other metaheuristic algorithms can be found in [18, 22, 26].",
      "startOffset" : 107,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "In this paper we present an approach based on the solutions proposed in [8, 9, 10, 30], which use a single colony with a single pheromone memory matrix.",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "In this paper we present an approach based on the solutions proposed in [8, 9, 10, 30], which use a single colony with a single pheromone memory matrix.",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "In this paper we present an approach based on the solutions proposed in [8, 9, 10, 30], which use a single colony with a single pheromone memory matrix.",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 27,
      "context" : "In this paper we present an approach based on the solutions proposed in [8, 9, 10, 30], which use a single colony with a single pheromone memory matrix.",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : "According to the classification given in [20], this is a coarse-grained master-slave approach.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "Characteristics of GPU computing The use of graphics processors for general purpose computing requires taking into account the significant differences between the GPU and CPU architectures [19].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 15,
      "context" : "The long delays, often at an order of hundreds of cycles, in accessing the global (main) memory are one of the main obstacles to efficient parallel computations on GPUs [17, 19].",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "The long delays, often at an order of hundreds of cycles, in accessing the global (main) memory are one of the main obstacles to efficient parallel computations on GPUs [17, 19].",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "Summarizing, a large number of the GPU processing elements allows to obtain high speedups, provided that the computations are largely independent and enough data is transferred in time [19].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 25,
      "context" : "The Travelling Salesman Problem (TSP) is one of the most often studied combinatorial optimization problems [28].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "The TSP was one of the first problems used to test the performance of algorithms as inspired by the behavior of ants, including the Ant System and its enhanced version - the Ant Colony System [11, 12].",
      "startOffset" : 192,
      "endOffset" : 200
    }, {
      "referenceID" : 10,
      "context" : "The TSP was one of the first problems used to test the performance of algorithms as inspired by the behavior of ants, including the Ant System and its enhanced version - the Ant Colony System [11, 12].",
      "startOffset" : 192,
      "endOffset" : 200
    }, {
      "referenceID" : 10,
      "context" : "2 derived from the Ant System [12].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "In the second case we can talk about exploration of the solution space [11].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "The size of the candidate set, cl, is typically 10 to 25 [11, 13].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "The size of the candidate set, cl, is typically 10 to 25 [11, 13].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 19,
      "context" : "It is worth mentioning that Randall and Montgomery [21] proposed dynamic update schemes for candidate sets for the TSP and the Quadratic Assignment Problem.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "The global pheromone update places emphasis on exploitation of the search space around the best solutions found to date, hence improving the convergence of the algorithm [13].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "Based on the results and conclusions presented, among others, in [5, 8, 10], we applied a model of parallelization in which each ant corresponds to a single thread block.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "Based on the results and conclusions presented, among others, in [5, 8, 10], we applied a model of parallelization in which each ant corresponds to a single thread block.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "Based on the results and conclusions presented, among others, in [5, 8, 10], we applied a model of parallelization in which each ant corresponds to a single thread block.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 31,
      "context" : "The stream processors (cores) in modern GPUs are grouped into so-called warps working in the SIMT mode (single instruction, multiple threads) [34].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "This is one of the reasons why the task parallelism model with a single ant per thread is not well suited for the GPU [5].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "Following [8], we used the value cl = 32.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 31,
      "context" : "These instructions give the threads access to values stored in the registers of other threads in the warp without the usage of shared memory [34].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "I-Roulette proposed in [5].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "results presented in [8].",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 31,
      "context" : "changes made in the global memory by one thread are immediately visible to the other threads and appear in the order they were made [34].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 21,
      "context" : "Selective pheromone memory Based on our earlier ideas [23, 24], we developed a parallel version of the ACS for the GPU in which the pheromone matrix was replaced with a selective pheromone memory of a smaller size.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "Selective pheromone memory Based on our earlier ideas [23, 24], we developed a parallel version of the ACS for the GPU in which the pheromone matrix was replaced with a selective pheromone memory of a smaller size.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "In the previous work [23, 24], a slightly different selective pheromone memory model was proposed in",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 22,
      "context" : "In the previous work [23, 24], a slightly different selective pheromone memory model was proposed in",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "Previous studies have shown that it is enough to remember only a small number of pheromone trail values to obtain good quality results [23, 24].",
      "startOffset" : 135,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : "Previous studies have shown that it is enough to remember only a small number of pheromone trail values to obtain good quality results [23, 24].",
      "startOffset" : 135,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "This is largely consistent with the observations of the convergence of PopulationBased ACO (PACO), in which the pheromone matrix is defined on the basis of a population of solutions, often of a very small size [16].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 24,
      "context" : "The first, labeled ACS-SEQ, was the reference sequential implementation of the ACS in C language by Thomas Stützle [27].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "[10] that a reliable evaluation of a parallel algorithm should take into account the time (speedup) as well as the quality of the results.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "The speedup values were calculated in relation to the sequential implementation of the ACS algorithm from the ACOTSP software by Stützle [27].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "A similar solution was adopted in the literature [8, 9] in order to make the comparisons more reliable and reproducible.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "A similar solution was adopted in the literature [8, 9] in order to make the comparisons more reliable and reproducible.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "The sequential version takes advantage of only a single CPU core, which makes the GPU vs CPU comparison not entirely fair, as was pointed out in [19], however, such a convention was adopted in most of the articles on the parallel GPU versions of the ACO and MMAS algorithms as presented in the literature, including [5, 10].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "The sequential version takes advantage of only a single CPU core, which makes the GPU vs CPU comparison not entirely fair, as was pointed out in [19], however, such a convention was adopted in most of the articles on the parallel GPU versions of the ACO and MMAS algorithms as presented in the literature, including [5, 10].",
      "startOffset" : 316,
      "endOffset" : 323
    }, {
      "referenceID" : 8,
      "context" : "The sequential version takes advantage of only a single CPU core, which makes the GPU vs CPU comparison not entirely fair, as was pointed out in [19], however, such a convention was adopted in most of the articles on the parallel GPU versions of the ACO and MMAS algorithms as presented in the literature, including [5, 10].",
      "startOffset" : 316,
      "endOffset" : 323
    }, {
      "referenceID" : 3,
      "context" : "It is worth recalling that in the previous experiments we used the number of ants m equal to the size of the TSP instance solved, as is often recommended in the literature [5, 8, 10, 13].",
      "startOffset" : 172,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "It is worth recalling that in the previous experiments we used the number of ants m equal to the size of the TSP instance solved, as is often recommended in the literature [5, 8, 10, 13].",
      "startOffset" : 172,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : "It is worth recalling that in the previous experiments we used the number of ants m equal to the size of the TSP instance solved, as is often recommended in the literature [5, 8, 10, 13].",
      "startOffset" : 172,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : "It is worth recalling that in the previous experiments we used the number of ants m equal to the size of the TSP instance solved, as is often recommended in the literature [5, 8, 10, 13].",
      "startOffset" : 172,
      "endOffset" : 186
    }, {
      "referenceID" : 17,
      "context" : "The smallest number of ants was set to 128 because in order to make good use of the hundreds (in our case 1536) of GPU CUDA cores, the number of active threads should be large [19, 34].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 31,
      "context" : "The smallest number of ants was set to 128 because in order to make good use of the hundreds (in our case 1536) of GPU CUDA cores, the number of active threads should be large [19, 34].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 8,
      "context" : "The quality of the results could be significantly improved if a local search heuristic was applied [10].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "All three algorithms were implemented by using the Nvidia CUDA framework and experimentally compared with the reference sequential implementation (ACOTSP) by Thomas Stützle [27].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "Perhaps concurrent kernels execution may be used to speed up even further the computation for multiple ants and multiple colonies [3].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "3-Opt, as presented in [10].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "[2] Hongtao Bai, Dantong OuYang, Ximing Li, Lili He, and Haihong Yu.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[3] Alberto Cano, José María Luna, and Sebastián Ventura.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[4] Alberto Cano, Juan Luis Olmo, and Sebastián Ventura.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] José M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] U.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7] Ling Chen, Hai-Ying Sun, and Shu Wang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[8] Laurence Dawson and Iain A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[9] Laurence Dawson and Iain A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[10] Audrey Delevacq, Pierre Delisle, Marc Gravel, and Michaël Krajecki.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[11] Marco Dorigo and Luca Maria Gambardella.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] Marco Dorigo, Vittorio Maniezzo, and Alberto Colorni.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] Marco Dorigo and Thomas Stützle.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] Sankha Baran Dutta, Robert D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] Henrique Fingler, Edson Norberto Cáceres, Henrique Mongelli, and Siang W.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] Michael Guntsch and Martin Middendorf.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] Byunghyun Jang, Dana Schaa, Perhaad Mistry, and David R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] Pavel Krömer, Jan Platos, and Václav Snásel.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[19] Victor W.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[20] Martín Pedemonte, Sergio Nesmachnow, and Héctor Cancela.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] Marcus Randall and James Montgomery.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] Christian Schulz, Geir Hasle, André R Brodtkorb, and Trond R Hagen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] Rafał Skinderowicz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] Rafał Skinderowicz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[26] Y.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[27] Stützle Thomas.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[28] Michael Trick.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[29] Colin Twomey, Thomas Stützle, Marco Dorigo, Max Manfrin, and Mauro Birattari.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[30] Akihiro Uchida, Yasuaki Ito, and Koji Nakano.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[31] Akihiro Uchida, Yasuaki Ito, and Koji Nakano.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[32] Kai-Cheng Wei, Chao-Chin Wu, and Chien-Ju Wu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[33] Robin M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[34] Nicholas Wilt.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[35] Hassan A.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "The Ant Colony System (ACS) is, next to Ant Colony Optimization (ACO) and the MAX-MIN Ant System (MMAS), one of the most efficient metaheuristic algorithms inspired by the behavior of ants. In this article we present three novel parallel versions of the ACS for the graphics processing units (GPUs). To the best of our knowledge, this is the first such work on the ACS which shares many key elements of the ACO and the MMAS, but differences in the process of building solutions and updating the pheromone trails make obtaining an efficient parallel version for the GPUs a difficult task. The proposed parallel versions of the ACS differ mainly in their implementations of the pheromone memory. The first two use the standard pheromone matrix, and the third uses a novel selective pheromone memory. Computational experiments conducted on several Travelling Salesman Problem (TSP) instances of sizes ranging from 198 to 2392 cities showed that the parallel ACS on Nvidia Kepler GK104 GPU (1536 CUDA cores) is able to obtain a speedup up to 24.29x vs the sequential ACS running on a single core of Intel Xeon E5-2670 CPU. The parallel ACS with the selective pheromone memory achieved speedups up to 16.85x, but in most cases the obtained solutions were of significantly better quality than for the sequential ACS.",
    "creator" : "LaTeX with hyperref package"
  }
}