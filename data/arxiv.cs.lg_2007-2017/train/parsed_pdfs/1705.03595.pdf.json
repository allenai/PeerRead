{
  "name" : "1705.03595.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Hirokatsu Kataoka", "Yutaka Satoh", "Kaori Abe", "Akio Nakamura" ],
    "emails" : [ "yu.satou}@aist.go.jp", "abe.keroko@aist.go.jp", "nkmr-a@cck.dendai.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n03 59\n5v 1\n[ cs\n.C V\n] 1\nThe paper presents a novel concept for collaborative descriptors between deeply learned and hand-crafted features. To achieve this concept, we apply convolutional maps for pre-processing, namely the convovlutional maps are used as input of hand-crafted features. We recorded an increase in the performance rate of +17.06% (multiclass object recognition) and +24.71% (car detection) from grayscale input to convolutional maps. Although the framework is straight-forward, the concept should be inherited for an improved representation."
    }, {
      "heading" : "1. Introduction",
      "text" : "Neural networks computationally mirror the architecture of the human brain. Recent architectures have been made deeper in order to provide higher-level representations of object. Especially in computer vision, deep convolutional neural networks (DCNN) are becoming increasingly widely used for image recognition and object detection. Representative deep models such as VGGNet [5] have been proposed in the field of computer vision.\nThe use of neural net and hand-crafted feature is repeated in the computer vision field (see Figure 1). The suggestive knowledge motivates us to study an advanced handcrafted feature after the DCNN in the recent 3rd AI. We anticipate that the hand-crafted feature should collaborate with deeply learned parameters since an outstanding performance is achieved with the automatic feature learning.\nIn the paper, we present a novel concept for collaborative descriptors which jointly work both DCNN and handcrafted feature. We apply convolutional maps for preprocessing as a first implementation. The framework is simple, but it is effective for recognition tasks. We di-\nrectly access the convolutional maps in order to improve the performance rate of hand-crafted features. We try to evaluate several hand-crafted features such as scale invariant feature transform (SIFT) and bag-of-words (BoW) [2] (object recognition) and higher-order local autocorrelation (HLAC) [4] (car detection). Then, the recognition rates for all hand-crafted features are compared using convolutional maps and grayscale images that have the same settings for the image description."
    }, {
      "heading" : "2. Convolutional maps for preprocessing",
      "text" : "Here, we introduce a framework for combining convolutional maps and hand-crafted features. Figure 2 shows the process for using convolutional maps as a preprocessor. We employ hand-crafted features, namely SIFT+BoW [2] and HLAC [4].\nGiven a convolutional mapMk with VGG [5] for kernel k, our goal is to extract a feature vector V from the map and define a classification label L as a function of SVM, L = fsvm(V ). We use VGGNet to create convolutional maps.\nThe 2nd max-pooling layer is used as a convolutional map that contains the 56 × 56 × 128 feature map. We can handle 128 maps with dimensions of 56 × 56 [pixel]. The patch and kernel sizes are suitable for balanced feature ex-\ntraction.\nWe can obtain a convolutional map M conv and maxpoolingmapMmp for kernel k from input mapM as below:\nM convk = X∑\nx\nY∑\ny\nfkxyMxy + bkxy (1)\nM mp k = max ij M convijk (2)\nwhere f is a filter and b is a bias in CNN. x, y, i, j are the indices of the two-dimensional map. Hereafter, the 2nd maxpooling layerMmp is the input image."
    }, {
      "heading" : "3. Results",
      "text" : "Comparisons of the hand-crafted features from convolutional maps and grayscale images are shown in Figure 3 and Table 1, respectively. We used a support vector machine (SVM) as a two- or multi-class classifier.\nSIFT+BoW was employed for the Caltech 101 dataset [3], since it is a kind of object categorization. According to Figure 3, the convolutional maps allow us to clearly distinguish between object classes. In the case of SIFT+BoW with convolutional maps, the codeword dictionary was shared across all 128 kernels. The dimension of the vector was 1,000, even when using convolutional maps. SIFT+BoW with convolutional maps (rate: 58.28%) was +17.06% better than the SIFT+BoW for the grayscale image (rate: 41.22%) on the Caltech 101 dataset. The results show that the use of preprocessing gives a perspective for the learning-based preprocessing with CNN. Although the CNN architecture is based on the ImageNet pre-trained model, a more sophisticated model such as fine tuning with objective data may improve preprocessing.\nHLAC was applied to the UIUC cars dataset [1]. The HLAC setting in convolutional maps was used to extract 25 dimension features per learned kernel, therefore we can extract a 25 [dim]× 128 [kernel] = 3,200 [dim] HLAC feature from an image. Table 1 shows the proposed setting (98.82%) performed better than the conventional grayscale setting (74.11%). We get a +24.71% improvement through using the convolutional maps. The preprocessing of HLAC usually requires binarization; however, we can extend this to a combination of convolution and binarization with convolutional maps.\nThe convolutional maps allow us to improve the expressiveness of the feature vector. The next step will make use of convolutional maps with well-organized approaches."
    }, {
      "heading" : "4. Conclusion",
      "text" : "The paper presented a novel concept of collaborative descriptors which combine deep convolutional neural network (DCNN) into hand-crafted features. We assigned convolutional maps as a preprocessing. We recorded an increase in the performance rate of +17.06% and +24.71% for the Caltech101 and UIUC cars datasets, respectively.\nIn the future, we must adjust the parameters of the convolutional maps, hand-crafted features and their integration. This paper only considered old-fashioned features and datasets; however, the use of convolutional maps will likely result in even greater improvements in the performance rate when used in conjunction with more sophisticated features."
    } ],
    "references" : [ {
      "title" : "Learning to detect objects in images via a sparse, part-based representation",
      "author" : [ "S. Agarwal", "A. Awan", "D. Roth" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Visual categorization with bags of keypoints",
      "author" : [ "G. Csurka", "C.R. Dance", "L. Fan", "J. Willamowski", "C. Bray" ],
      "venue" : "ECCVW,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "P. Perona" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "A new scheme for practical flexible and intelligent vision systems",
      "author" : [ "N. Otsu", "T. Kurita" ],
      "venue" : "IAPR Workshop on Computer Vision,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1988
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "International Conference on Learning Representation (ICLR),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Representative deep models such as VGGNet [5] have been proposed in the field of computer vision.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "We try to evaluate several hand-crafted features such as scale invariant feature transform (SIFT) and bag-of-words (BoW) [2] (object recognition) and higher-order local autocorrelation (HLAC) [4] (car detection).",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "We try to evaluate several hand-crafted features such as scale invariant feature transform (SIFT) and bag-of-words (BoW) [2] (object recognition) and higher-order local autocorrelation (HLAC) [4] (car detection).",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : "We employ hand-crafted features, namely SIFT+BoW [2] and HLAC [4].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "We employ hand-crafted features, namely SIFT+BoW [2] and HLAC [4].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "Given a convolutional mapMk with VGG [5] for kernel k, our goal is to extract a feature vector V from the map and define a classification label L as a function of SVM, L = fsvm(V ).",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "SIFT+BoW was employed for the Caltech 101 dataset [3], since it is a kind of object categorization.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "HLAC was applied to the UIUC cars dataset [1].",
      "startOffset" : 42,
      "endOffset" : 45
    } ],
    "year" : 2017,
    "abstractText" : "The paper presents a novel concept for collaborative descriptors between deeply learned and hand-crafted features. To achieve this concept, we apply convolutional maps for pre-processing, namely the convovlutional maps are used as input of hand-crafted features. We recorded an increase in the performance rate of +17.06% (multiclass object recognition) and +24.71% (car detection) from grayscale input to convolutional maps. Although the framework is straight-forward, the concept should be inherited for an improved representation.",
    "creator" : "LaTeX with hyperref package"
  }
}