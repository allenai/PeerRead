{
  "name" : "1603.04259.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Item2Vec: Neural Item Embedding for Collaborative Filtering",
    "authors" : [ "Oren Barkan", "Noam Koenigstein" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze itemitem relations in order to produce item similarities. Recently, several works in the field of Natural Language Processing suggested to learn a latent representation of words using neural embedding algorithms. Among them, the Skipgram with Negative Sampling (SGNS), also known as Word2Vec, was shown to provide stateof-the-art results on various linguistics tasks. In this paper, we show that item-based CF can be cast in the same framework of neural word embedding. Inspired by SGNS, we describe a method we name Item2Vec for item-based CF that produces embedding for items in a latent space. The method is capable of inferring item-toitem relations even when user information is not available. We present experimental results on large scale datasets that demonstrate the effectiveness of the proposed method and show it provides a similarity measure that is competitive with SVD.\nIndex terms – skip-gram, word2vec, neural word embedding, collaborative filtering, item similarity, recommendation systems."
    }, {
      "heading" : "1. Introduction",
      "text" : "Computing item similarities is a key building block in modern Recommender Systems. While many recommendation algorithms are focused on learning a low dimensional embedding of users and items simultaneously [1, 2, 3], computing item similarities is an end in itself. Items similarities are extensively used by online retailers for many different recommendation tasks. This papers deals with the overlooked task of learning item similarities by embedding items in a low dimensional space regardless of the users.\nItem-based similarities are used by online retailers for recommendations based on a single item. For example, in the Windows 10 App Store, the details page of each app or game includes a list of other similar apps titled “People also like”. This list can be extended to a full page recommendation list of items similar to the original app as shown in Fig.1. Similar recommendation lists which are based merely on similarities to a single item exist in most online stores e.g., Amazon, Netflix, Google Play, iTunes store and many others.\nThe single item recommendations are different than the more “traditional” user-to-item recommendations because they are usually shown in the context of an explicit user interest in a specific item and in the context of an explicit user intent to purchase. Therefore, single item recommendations based on item\nsimilarities often have higher Click-Through Rates (CTR) than user-to-item recommendations and consequently responsible for a larger share of sales or revenue.\nSingle item recommendations based on item similarities are used also for a variety of other recommendation tasks: In “candy rank” recommendations a similar item (usually of lower price) is suggested at the check-out page right before the payment. In “bundle” recommendations a set of several items are grouped and recommended together. Finally, item similarities are used in online stores for better exploration and discovery and improve the overall user experience. It is unlikely that a user-item CF method, that learns the connections between items implicitly by defining slack variables for users, would produce better item representations than a method that is optimized to learn the item relations directly.\nItem similarities are also at the heart of item-based CF algorithms aim at learning the representation directly from the item-item relations [4, 5]. There are several scenarios where item-based CF methods are desired: in a large scale dataset, when the number of users is significantly larger than the number of items, the computational complexity of methods that model items solely is significantly lower than methods that model both users and items simultaneously. For example, online music services may have hundreds of millions of enrolled users with just tens of thousands of artists (items).\nIn certain scenarios, the user-item relations are not available. For instance, a significant portion of today’s online shopping is done without an explicit user identification process. Instead, the available information is per session. Treating these sessions as “users” would be prohibitively expensive as well as less informative.\nRecent progress in neural embedding methods for linguistic tasks have dramatically advanced state-ofthe-art natural language processing (NLP) capabilities [6, 7, 8, 9]. These methods attempt to map words and phrases to a low dimensional vector space that captures semantic relations between the words. Specifically, Skip-gram with Negative Sampling (SGNS), known also as Word2Vec [8], set new records in various NLP tasks and its applications have been extended to other domains beyond NLP [10, 11].\nIn this paper, we propose to apply SGNS to itembased CF. Motivated by its great success in other domains, we suggest that SGNS with minor modifications may capture the relations between different items in collaborative filtering datasets. To this end, we propose a modified version of SGNS named Item2Vec. We show that Item2Vec can induce a\nsimilarity measure that is competitive with an itembased CF using SVD. It is important to clarify that we do not claim to achieve the state-of-the-art, but merely to show another successful application of SGNS for item-based CF. Therefore, we choose to compare our method to a SVD-based model and leave the comparison to other more complex method to a future research.\nThe rest of the paper is organized as follows: Section 2 overviews related work in context of SGNS, in Section 3 we describe how to apply SGNS for itembased CF, in Section 4 we describe the experimental setup and present empirical qualitative and quantitative results."
    }, {
      "heading" : "2. Skip-gram with negative sampling",
      "text" : "(SGNS) – Related work\nSGNS is a neural word embedding method that was introduced by Mikolov et. al in [8]. The method aims at finding words representation that captures the relation between a word to its surrounding words in a sentence. In the rest of this section, we provide a brief overview of the SGNS method.\nGiven a sequence of words 1( ) K i i w = from a finite\nvocabulary 1{ } W i i W w == , the Skip-gram objective aims at maximizing the following term:\n1 , 0\n1 log ( | )\nK\ni j i\ni c j c j\np w w K + = − ≤ ≤ ≠ ∑ ∑ (1)\nwhere c is half of the context window size (that may\ndepend on i w ) and ( | ) j i p w w is the softmax function:\nexp( )\n( | ) exp( )\nW\nT i j\nj i T T\ni k\nk I\nu v p w w\nu v ∈\n= ∑\n(2)\nwhere ( )m i u U∈ ⊂ ℝ and ( )m i v V∈ ⊂ ℝ are latent vectors that correspond to the target and context\nrepresentations for the word i w W∈ , respectively,\n{1,..., } W I W≜ and the parameter m is chosen\nempirically and according to the size of the dataset. Using Eq. (2) is impractical due to the computational\ncomplexity of ( | ) j i p w w∇ , which is a linear function of\nthe vocabulary size W (that for is usually in size of"
    }, {
      "heading" : "5 610 10− .",
      "text" : "Negative sampling comes to alleviate the above computational problem by the replacement of the softmax function from Eq. (2) with\n1\n( | ) ( ) ( ) N T T\nj i i j i k\nk p w w u v u vσ σ = = −∏\nwhere ( ) 1/1 exp( )x xσ = + − , N is a parameter that\ndetermines the number of negative examples to be\ndrawn per a positive example. A negative word i w is sampled from the unigram distribution raised to the 3/4rd power (this distribution was found to significantly outperform the unigram distribution, empirically [8]).\nIn order to overcome the imbalance between rare and frequent words the following subsampling procedure is proposed [8]: Given the input word sequence, we discard each word w with a probability\n( | ) 1 ( ) p discard w f w\nρ = − where ( )f w is the\nfrequency of the word w and ρ is a prescribed\nthreshold. This procedure was reported to accelerate the learning process and to improve the representation of rare words significantly [8]. In our experiments, we observed the same improvements when applying subsampling as well.\nThe latent vectors are then estimated by applying a stochastic optimization with respect to the objective in Eq. (1)."
    }, {
      "heading" : "3. Item2Vec – SGNS for item-based CF",
      "text" : "In the context of CF data, the items are given as user generated sets. Note that the information about the relation between user and a set of items is not always available. For example, we might be supplied with a dataset that is generated from orders that a store received, without any information about which identity sent the order. In other words, there are scenarios where multiple sets of items might belong to the same user, but this information is not provided. In Section 4 we show that our method handles with these scenarios as well.\nWe propose to apply SGNS to item-based CF. The application of SGNS to CF data is straightforward once we observed that a sequence of words is equivalent to a set or basket of items. Therefore, from now on, we will use the terms “word” and “item” interchangeably.\nBy moving from sequences to sets, the spatial / time information is lost. We chose to discard this information since in this paper we assume a static environment, where items that belong to the same set are considered similar, no matter in what order / time they were generated by the user. This assumption may not hold in other scenarios, but we keep the treatment\nof these scenarios out of scope of this paper.\nSince we ignore the spatial information, we treat each pair of items in a set as a positive example. This implies a window size that is determined from the set size. Specifically, for a given set of items, the objective from Eq. (1) is modified as follows:\n1\n1 log ( | )\nK K\nj i\ni j i p w w K = ≠ ∑∑\nThe rest of the process remains identical to the algorithm described in Section 2. Finally, we compute the affinity between a pair of items by the application cosine similarity.\nIn this work we used i u as the final representation\nfor the i -th item. Other options are to use i v , the\nadditive composition, i i u v+ or the concatenation T\nT T\ni i u v  . Note that the last two options sometimes\nproduce superior representation. We name the described method “Item2Vec”."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "In this section we provide an empirical evaluation of the proposed method. We provide both qualitative and quantitative results depending whether a metadata about the items exists. As a baseline item-based CF algorithm we used item-item SVD."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate our method on two different types of datasets, both private.\nThe first dataset is user-artist data that is retrieved from the Microsoft XBOX Music service. This dataset consist of 9M events. Each event consists of a userartist relation, which means the user played a song by the specific artist. The dataset contains 732K users and 49K distinct artists.\nThe second dataset contains physical goods orders from Microsoft Store. An order is given by a basket of items without any information about the user that made it. Therefore, the information in this dataset is weaker in the sense that we cannot bind between users and items. The dataset consist of 379K orders (that contains more than a single item) and 1706 distinct items."
    }, {
      "heading" : "4.2 Systems and parameters",
      "text" : "We applied Item2Vec to both datasets. The optimization is done by stochastic gradient decent. We ran the algorithm for 20 epochs. We set the negative sampling value to 15N = for both datasets. The\ndimension parameter m was set to 100 and 40 for the\nMusic and Store datasets, respectively. We also employed subsampling with ρ values of 510− and 310−\nfor the Music and Store datasets, respectively. The reason we set different parameter values is due to different sizes of the datasets.\nWe compare our method to a SVD based item-item similarity system. To this end, we apply SVD to a square matrix in size of number of items, where the\n( , )i j entry contains the number of times ( , ) i j w w\nappears as a positive pair in the dataset. Then, we normalize each entry according to the square root of the product of its row and column sums. Finally, the latent representation is given by the rows of 1/2US ,\nwhere S is a diagonal matrix that its diagonal is the\ntop m singular values and U is a matrix that contains\nthe corresponding left singular vectors as columns. The affinity between items is computed by cosine similarity. Throughout this section we name this method “SVD”."
    }, {
      "heading" : "4.3 Experiments and results",
      "text" : "The music dataset does not provide genre metadata. Therefore, for each artist we retrieved the genre metadata from the internet and used this information in order to visualize the relation between the learnt representation and the genres. This is motivated by the\nassumption that a useful representation would cluster artists according to their genre. To this end, we generated a subset that contains the top 100 popular artists per genre for the following distinct genres: 'R&B / Soul', 'Kids', 'Classical', 'Country', 'Electronic / Dance', 'Jazz', 'Latin', 'Hip Hop', 'Reggae / Dancehall', 'Rock', 'World', 'Christian / Gospel' and 'Blues / Folk'. We applied t-SNE [12] with a cosine kernel to reduce the dimensionality of the item vectors to 2. Then, we colored each artist point according to its genre.\nFigures 2(a) and 1(b) present the 2D embedding, produced by t-SNE, for Item2Vec and SVD, respectively. As we can see, Item2Vec provides a better clustering. We further observe that some of the relatively homogenous areas in Fig. 2(a) are contaminated with items that are colored differently. We found out that many of these cases originate in artists that were mislabeled in the web or have a mixed genre.\nTable 1 presents several examples, where the genre associated with a given artist (according to metadata we retrieved from the web) was inaccurate or at least inconsistent with Wikipedia. Therefore, we conclude that usage based models such as Item2Vec may be useful for the detection of mislabeled data and even provide a suggestion for the correct label using a simple k nearest neighbor (KNN) classifier.\nIn order to quantify the similarity quality, we tested\nthe genre consistency between an item and its nearest neighbors. We do that by iterating over the top q\npopular items (for various values of q ) and check\nwhether their genre is consistent with the genres of the\nk nearest items that surround them. This is done by a\nsimple majority voting. We ran the same experiment for\ndifferent neighborhood sizes ( k = 6, 8, 10, 12 and 16)\nand no significant change in the results was observed.\nTable 2 presents the results obtained for 8k = . We\nobserve that Item2Vec is consistently better than the SVD model, where the gap keeps growing as q\nincreases. This might imply that Item2Vec produces a better representation for less popular items than the one produced by SVD, which is unsurprising since\nItem2Vec subsamples popular items and samples the negative examples according to their popularity.\nWe further validate this hypothesis by applying the same ‘genre consistency’ test to a subset of 10K unpopular items (the last row in Table 2). We define an unpopular item in case it has less than 15 users that played its corresponding artist. The accuracy obtained by Item2Vec was 68%, compared to 58.4% by SVD.\nQualitative comparisons between the Item2Vec and SVD are presented in Tables 3-4 for Music and Store datasets, respectively. The tables present seed items and their 5 nearest neighbors. The main advantage of this comparison is that it enables the inspection of item similarities in higher resolutions than genres. Moreover, since the Store dataset lacks any informative tags / labels a qualitative evaluation is inevitable. We observe that, for both datasets, Item2Vec provides lists that are more related to the seed item than SVD. Furthermore, we see that even though the Store dataset contains weaker information, Item2Vec manages to infer about the relations between items quite well."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we proposed Item2Vec - a neural embedding algorithm for item-based collaborative filtering. Item2Vec is based on SGNS with minor modifications. We present both quantitative and qualitative evaluations that demonstrate the effectiveness of Item2Vec when compared to a SVDbased item similarity model. We observed that Item2Vec produces a better representation for items than the one obtained by the baseline SVD-based model, where the gap between the two becomes more significant for unpopular items. We explain this by the fact that Item2Vec employs negative sampling together with subsampling of popular items.\nIn future we plan to investigate more complex CF models such as [1, 2, 3] and compare between them and Item2Vec."
    }, {
      "heading" : "A COMPARISON BETWEEN SVD AND ITEM2VEC",
      "text" : ""
    }, {
      "heading" : "ON GENRE CLASSIFICATION TASK FOR VARIOUS",
      "text" : "SIZES OF TOP POPULAR ARTIST SETS\nTop (q) popular\nartists\nSVD\nAccuracy\nItem2Vec Accuracy\n2.5K 85% 86.4% 5K 83.4% 84.2% 10K 80.2% 82% 15K 76.8% 79.5% 20K 73.8% 77.9% 10K unpopular (see text) 58.4% 68%\n[4] Sarwar B, Karypis G, Konstan J, Riedl J. Item-based collaborative filtering recommendation algorithms. InProceedings of the 10th international conference on World Wide Web 2001 Apr 1 (pp. 285-295). ACM.\n[5] Linden G, Smith B, York J. Amazon.com recommendations: Item-to-item collaborative filtering. Internet Computing, IEEE. 2003 Jan;7(1):76-80.\n[6] Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask learning. InProceedings of the 25th international conference on Machine learning 2008 Jul 5 (pp. 160-167). ACM.\n[7] Mnih A, Hinton GE. A scalable hierarchical distributed language model. InAdvances in neural information processing systems 2009 (pp. 1081-1088).\n[8] Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems 2013 (pp. 3111-3119).\n[9] Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. 2013 Jan 16.\n[10] Frome A, Corrado GS, Shlens J, Bengio S, Dean J, Mikolov T. Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems 2013 (pp. 2121-2129).\n[11] Lazaridou A, Pham NT, Baroni M. Combining language and vision with a multimodal skip-gram model. arXiv preprint arXiv:1501.02598. 2015 Jan 12.\n[12] Van der Maaten, L., & Hinton, G. Visualizing data using t-SNE. Journal of Machine Learning Research, (2008) 9(2579-2605), 85"
    } ],
    "references" : [ {
      "title" : "One-class collaborative filtering with random graphs",
      "author" : [ "U. Paquet", "Koenigstein", "May" ],
      "venue" : "In Proceedings of the 22nd international conference on World Wide Web (pp. 999-1008)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Y Koren", "R Bell", "C. Volinsky" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Item-based collaborative filtering recommendation algorithms",
      "author" : [ "B Sarwar", "G Karypis", "J Konstan", "J. Riedl" ],
      "venue" : "InProceedings of the 10th international conference on World Wide Web 2001 Apr",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Amazon.com recommendations: Item-to-item collaborative filtering",
      "author" : [ "G Linden", "B Smith", "J. York" ],
      "venue" : "Internet Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "R Collobert", "J. Weston" ],
      "venue" : "InProceedings of the 25th international conference on Machine learning 2008 Jul",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "A scalable hierarchical distributed language model. InAdvances in neural information processing systems",
      "author" : [ "Mnih A", "Hinton GE" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems",
      "author" : [ "T Mikolov", "I Sutskever", "K Chen", "GS Corrado", "J. Dean" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "T Mikolov", "K Chen", "G Corrado", "J. Dean" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Devise: A deep visual-semantic embedding model",
      "author" : [ "A Frome", "GS Corrado", "J Shlens", "S Bengio", "J Dean", "T. Mikolov" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Combining language and vision with a multimodal skip-gram model",
      "author" : [ "A Lazaridou", "NT Pham", "M. Baroni" ],
      "venue" : "arXiv preprint arXiv:1501.02598",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "L. Van der Maaten", "G. Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "While many recommendation algorithms are focused on learning a low dimensional embedding of users and items simultaneously [1, 2, 3], computing item similarities is an end in itself.",
      "startOffset" : 123,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "While many recommendation algorithms are focused on learning a low dimensional embedding of users and items simultaneously [1, 2, 3], computing item similarities is an end in itself.",
      "startOffset" : 123,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "Item similarities are also at the heart of item-based CF algorithms aim at learning the representation directly from the item-item relations [4, 5].",
      "startOffset" : 141,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "Item similarities are also at the heart of item-based CF algorithms aim at learning the representation directly from the item-item relations [4, 5].",
      "startOffset" : 141,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Recent progress in neural embedding methods for linguistic tasks have dramatically advanced state-ofthe-art natural language processing (NLP) capabilities [6, 7, 8, 9].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : "Recent progress in neural embedding methods for linguistic tasks have dramatically advanced state-ofthe-art natural language processing (NLP) capabilities [6, 7, 8, 9].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 6,
      "context" : "Recent progress in neural embedding methods for linguistic tasks have dramatically advanced state-ofthe-art natural language processing (NLP) capabilities [6, 7, 8, 9].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "Recent progress in neural embedding methods for linguistic tasks have dramatically advanced state-ofthe-art natural language processing (NLP) capabilities [6, 7, 8, 9].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 6,
      "context" : "Specifically, Skip-gram with Negative Sampling (SGNS), known also as Word2Vec [8], set new records in various NLP tasks and its applications have been extended to other domains beyond NLP [10, 11].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "Specifically, Skip-gram with Negative Sampling (SGNS), known also as Word2Vec [8], set new records in various NLP tasks and its applications have been extended to other domains beyond NLP [10, 11].",
      "startOffset" : 188,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "Specifically, Skip-gram with Negative Sampling (SGNS), known also as Word2Vec [8], set new records in various NLP tasks and its applications have been extended to other domains beyond NLP [10, 11].",
      "startOffset" : 188,
      "endOffset" : 196
    }, {
      "referenceID" : 6,
      "context" : "al in [8].",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "sampled from the unigram distribution raised to the 3/4rd power (this distribution was found to significantly outperform the unigram distribution, empirically [8]).",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : "In order to overcome the imbalance between rare and frequent words the following subsampling procedure is proposed [8]: Given the input word sequence, we discard each word w with a probability",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "This procedure was reported to accelerate the learning process and to improve the representation of rare words significantly [8].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "We applied t-SNE [12] with a cosine kernel to reduce the dimensionality of the item vectors to 2.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "In future we plan to investigate more complex CF models such as [1, 2, 3] and compare between them and Item2Vec.",
      "startOffset" : 64,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "In future we plan to investigate more complex CF models such as [1, 2, 3] and compare between them and Item2Vec.",
      "startOffset" : 64,
      "endOffset" : 73
    } ],
    "year" : 2016,
    "abstractText" : "Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze itemitem relations in order to produce item similarities. Recently, several works in the field of Natural Language Processing suggested to learn a latent representation of words using neural embedding algorithms. Among them, the Skipgram with Negative Sampling (SGNS), also known as Word2Vec, was shown to provide stateof-the-art results on various linguistics tasks. In this paper, we show that item-based CF can be cast in the same framework of neural word embedding. Inspired by SGNS, we describe a method we name Item2Vec for item-based CF that produces embedding for items in a latent space. The method is capable of inferring item-toitem relations even when user information is not available. We present experimental results on large scale datasets that demonstrate the effectiveness of the proposed method and show it provides a similarity measure that is competitive with SVD.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}