{
  "name" : "1701.04024.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue",
    "authors" : [ "Mihail Eric", "Christopher D. Manning" ],
    "emails" : [ "meric@cs.stanford.edu", "manning@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Effective task-oriented dialogue systems are becoming important as society progresses toward using voice for interacting with devices and performing everyday tasks such as scheduling. To that end, research efforts have focused on using machine learning methods to train agents using dialogue corpora. One line of work has tackled the problem using partially observable Markov decision processes and reinforcement learning with carefully designed action spaces (Young et al., 2013). However, the large, hand-designed action and state spaces make this class of models brittle, and in practice most deployed dialogue system remain hand-written rule-based systems.\nRecently, neural network models have achieved success on a variety of natural language process-\ning tasks (Bahdanau et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015b), due to their ability to implicitly learn powerful distributed representations from data in an end-to-end trainable fashion. This paper extends recent work examining the utility of distributed state representations for taskoriented dialogue agents, without providing rules or manually tuning features.\nOne prominent line of recent neural dialogue work has continued to build systems with modularly-connected representation, belief state, and generation components (Wen et al., 2016b). These models must learn to explicitly represent user intent through intermediate supervision, and hence suffer from not being truly end-to-end trainable. Other work stores dialogue context in a memory module and repeatedly queries and reasons about this context to select an adequate system response (Bordes and Weston, 2016). While reasoning over memory is appealing, these models simply choose among a set of utterances rather than generating text and also must have temporal dialogue features explicitly encoded.\nWe aim to fill a gap in the literature by systematically building increasingly complex models of text generation, starting with a vanilla sequenceto-sequence recurrent architecture. The result is a simple, intuitive, and highly competitive model, which outperforms the more complex model of Bordes and Weston (2016) by 6.9%. Our contributions are as follows: 1) We perform a systematic, empirical analysis of increasingly complex sequence-to-sequence models for task-oriented dialogue, and 2) we develop a recurrent neural dialogue architecture augmented with an attentionbased copy mechanism that is able to significantly outperform more complex models on a variety of metrics on realistic data. ar X\niv :1\n70 1.\n04 02\n4v 1\n[ cs\n.C L\n] 1\n5 Ja\nn 20\n17"
    }, {
      "heading" : "2 Architecture",
      "text" : "We use neural encoder-decoder architectures to frame dialogue as a sequence-to-sequence learning problem. Given a dialogue between a user (u) and a system (s), we represent the dialogue utterances as {(u1, s1), (u2, s2), . . . , (uk, sk)} where k denotes the number of turns in the dialogue. At the ith turn, we encode the aggregated dialogue context composed of the tokens of (u1, s1, . . . , si−1, ui). Letting x1, . . . , xm denote these tokens, we first embed these tokens using a trained embedding function φemb that maps each token to a fixed-dimensional vector. These mappings are fed into the encoder to produce contextsensitive hidden representations h1, . . . , hm.\nThe vanilla Seq2Seq decoder predict the tokens of the ith system response si, which we denote as y1, . . . , yn with a recurrent model. We extend that with an attention-based model (Bahdanau et al., 2014; Luong et al., 2015a), where, at every time step j of the decoding, an attention score eji is computed for each hidden state hi of the encoder, using the attention mechanism of (Vinyals et al., 2015b). These scores are used to form a normalized linear combination of the encoder hidden representations h1, . . . , hm which is linearly combined with the current hidden state h̃j of the decoder into a fixed-dimensional vector oj . In the basic attention model, this oj is used to compute logits over the tokens of the output vocabulary V , and the next token yj is predicted by maximizing the log-likelihood.\nAn effective task-oriented dialogue system must have powerful language modelling capabilities and be able to pick up on relevant entities of an underlying knowledge base. We augment the attention encoder-decoder model with an attentionbased copy mechanism in the style of (Jia and Liang, 2016). During decoding, we define õj = [oj , ej ] where ej is the combined attention scores of the encoder hidden states, and a new logits vector d ∈ R|V |+m is computed as Uõj where U is an appropriately-dimensioned trainable matrix. Thus the model either predicts a token yj from V or softly copies a token xi from the encoder input context, via the attention score eji. Rather than copy over any token mentioned in the encoder dialogue context, our model is trained to only copy over entities of the knowledge base mentioned, as this provides a conceptually intuitive goal for the model’s predictive learning: as training progresses\nit will learn to either predict a token from the standard vocabulary of the language model thereby ensuring well-formed natural language utterances, or to copy over the relevant entities from the input context, thereby learning to extract important dialogue context.\nIn our best performing model, we augment the inputs to the encoder by adding entity type features. Classes present in the knowledge base of the dataset are encoded as one-hot vectors. Whenever an entity token is seen during encoding, we append the appropriate one-hot vector to the token’s word embedding before it is fed into the recurrent cell.\nAll of our architectures use an LSTM cell as the recurrent unit (Hochreiter and Schmidhuber, 1997) with a bias of 1 added to the forget gate in the style of (Zaremba et al., 2015)."
    }, {
      "heading" : "2.1 Training",
      "text" : "We train using a cross-entropy loss and the Adam optimizer (Kingma and Ba, 2015), applying dropout (Hinton et al., 2012) as a regularizer to the input and output of the LSTM. We identified hyperparameters by random search, evaluating on a held-out validation subset of the data. Dropout keep rates ranged from 0.75 to 0.95. We used word embeddings with size 300, and hidden layer and cell sizes were set to 353, identified through our search. We applied gradient clipping with a clipvalue of 10 to avoid gradient explosions during training. The attention, output parameters, word embeddings, and LSTM weights are randomly initialized from a uniform unit-scaled distribution in the style of (Sussillo and Abbott, 2015)."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Data",
      "text" : "For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014), a restaurant reservation system dataset. While the goal of the original challenge was building a system for inferring dialogue state, for our study, we use the version of the data from Bordes and Weston (2016), which ignores the dialogue state annotations, using only the raw text of the dialogues, while adding system commands. Thus, the raw text includes user and system utterances as well as the API calls the system would make to the underlying KB in response to the user’s queries. We use the train/validation/test splits from this mod-\nified version of the dataset. The dataset is appealing for a number of reasons: 1) It is derived from a real-world system so it presents the kind of linguistic diversity and conversational abilities we would hope for in an effective dialogue agent. 2) It is grounded via an underlying knowledge base of restaurant entities and their attributes. 3) Previous results have been reported on it so we can directly compare our model performance. We include statistics of the dataset in Table 1."
    }, {
      "heading" : "3.2 Metrics",
      "text" : "Evaluation of dialogue systems is known to be difficult (Liu et al., 2016). We employ several metrics for assessing specific aspects of our model, drawn from previous work:\n• Per-Response Accuracy: Bordes and Weston (2016) report a per-turn response accuracy, which tests their model’s ability to select the system response at a certain timestep. Their system does a multiclass classification over a predefined candidate set of responses, which was created by aggregating all system responses seen in the training, validation, and test sets. Our model actually generates each individual token of the response, and we consider a prediction to be correct if every token of the model output matches the corresponding token in the gold response. Evaluating using this metric on our model is therefore significantly more stringent.\n• Per-Dialogue Accuracy: Bordes and Weston (2016) also report a per-dialogue accuracy, which assesses their model’s ability to classify every system turn of the dialogue correctly. We calculate a similar value of dialogue accuracy, though again our model generates every token of every response.\n• BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating\ndialogue systems (Ritter et al., 2011; Li et al., 2015). We calculate average BLEU score over all responses generated by the system, and primarily report these scores to gauge our model’s ability to accurately generate the language patterns seen in DSTC2.\n• Entity F1: We also report entity F1 averaged over all responses, to evaluate the model’s ability to generate relevant entities from the underlying knowledge base and to capture the semantics of the user-initiated dialogue flow.\nOur experiments show that sometimes our model generates a response to a given input that is perfectly reasonable, but is penalized because our evaluation metrics involve direct comparison to the gold system output. For example, given a user request for an australian restaurant, the gold system output is you are looking for an australian restaurant right? whereas our system outputs what part of town do you have in mind?, which is a more directed follow-up intended to narrow down the search space of candidate restaurants the system should propose. This issue, which recurs with evaluation of dialogue or other generative systems, could be alleviated through more forgiving evaluation procedures based on beam search decoding."
    }, {
      "heading" : "3.3 Results",
      "text" : "In Table 2, we include the results of our models compared to the reported performance of the best performing model of (Bordes and Weston, 2016), which is a variant of an end-to-end memory network (Sukhbaatar et al., 2015). Their model is referred to as MemNN. We also include the model of (Liu and Perez, 2016), referred to as GMemNN, and the model of (Seo et al., 2016), referred to as QRN, which currently is state-of-the-art. In the table, Seq2Seq refers to our vanilla encoder-decoder architecture with (1), (2), and (3) LSTM layers respectively. +Attn refers to a 1-layer Seq2Seq with attention-based decoding. +Copy refers to +Attn with our copy-mechanism added. +EntType refers to +Copy with entity class features added to encoder inputs.\nWe see that a 1-layer vanilla encoder-decoder is already able to significantly outperform MemNN in both per-response and per-dialogue accuracies, despite our more stringent setting. Adding layers to Seq2Seq leads to a drop in performance, suggesting an overly powerful model for the small\ndataset size. Adding an attention-based decoding to the vanilla model increases BLEU although per-response and per-dialogue accuracies suffer a bit. Adding our attention-based entity copy mechanism achieves large increases in per-response accuracies and entity F1. Adding entity class features to +Copy achieves our best-performing model, in terms of per-response accuracy and entity F1. This model achieves a 6.9% increase in per-response accuracy on DSTC2 over MemNN, including +1.5% per-dialogue accuracy, and is on par with the performance of GMemNN, including beating its per-dialogue accuracy. It also achieves the highest entity F1."
    }, {
      "heading" : "4 Discussion and Conclusion",
      "text" : "We have iteratively built out a class of neural models for task-oriented dialogue that is able to outperform other more intricately designed neural architectures on a number of metrics. The model incorporates in a simple way abilities that we believe are essential to building good task-oriented dialogue agents, namely maintaining dialogue state and being able to extract and use relevant entities in its responses, without requiring intermediate supervision of dialogue state or belief tracker modules. We attribute the large gains in per-response accuracy and entity F1 demonstrated by +EntType to its ability to pick out the relevant KB entities from the dialogue context fed into the encoder. In Figure 1, we see the attention-based copy weights of the model, indicating that the model is able to learn the relevant entities it should focus on in the input context. The powerful language modelling\nabilities of the Seq2Seq backbone allow smooth integration of these extracted entities into both system-generated API calls and natural language responses as shown in the figure.\nThe appeal of our model comes from the simplicity and effectiveness of framing system response generation as a sequence-to-sequence mapping with a soft copy mechanism over relevant context. Unlike the task-oriented dialogue agents of Wen et. al (2016b), our architecture does not explicitly model belief states or KB slot-value trackers, and we preserve full end-to-end-trainability. Further, in contrast to other referenced work on DSTC2, our model offers more linguistic versatility due to its generative nature while still remaining highly competitive and outperforming other models. We hope this simple and effective architecture can be a strong baseline for future research efforts on task-oriented dialogue."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning end-to-end goal-oriented dialog. arXiv preprint arXiv:1605.07683",
      "author" : [ "Bordes", "Weston2016] A. Bordes", "J. Weston" ],
      "venue" : null,
      "citeRegEx" : "Bordes et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2016
    }, {
      "title" : "The second dialog state tracking challenge",
      "author" : [ "B. Thomson", "J. Williams" ],
      "venue" : "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue,",
      "citeRegEx" : "Henderson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580",
      "author" : [ "Hinton et al.2012] G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Data recombination for neural semantic parsing. Association for Computational Linguistics",
      "author" : [ "Jia", "Liang2016] R. Jia", "P. Liang" ],
      "venue" : null,
      "citeRegEx" : "Jia et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: a method for stochastic optimization. International Conference for Learning Representations",
      "author" : [ "Kingma", "Ba2015] D. Kingma", "J. Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055",
      "author" : [ "J. Li", "M. Galley", "C. Brockett", "J. Gao", "W.B. Dolan" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "2016. Gates end-to-end memory networks. arXiv preprint arXiv:1610.04211",
      "author" : [ "Liu", "Perez2016] F. Liu", "J. Perez" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "How not to evaluate your dialogue system: an empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "C.-W. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Luong et al.2015a] M. Luong", "H. Pham", "C.D. Manning" ],
      "venue" : "Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine",
      "author" : [ "Papineni et al.2002] K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu" ],
      "venue" : null,
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Data-driven response generation in social media",
      "author" : [ "Ritter et al.2011] A. Ritter", "C. Cherry", "W.B. Dolan" ],
      "venue" : "Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Ritter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2011
    }, {
      "title" : "Query-reduction networks for question answering",
      "author" : [ "Seo et al.2016] M. Seo", "S. Min", "A. Farhadi", "H. Hajishirzi" ],
      "venue" : "arXiv preprint arXiv:1606.04582",
      "citeRegEx" : "Seo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2016
    }, {
      "title" : "Random walk initialization for training very deep feed forward networks. arXiv preprint arXiv:1412.6558",
      "author" : [ "Sussillo", "Abbott2015] D. Sussillo", "L.F. Abbott" ],
      "venue" : null,
      "citeRegEx" : "Sussillo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sussillo et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "O. Vinyals", "Q.V. Le" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "2015b. Grammar as a foreign language",
      "author" : [ "Vinyals et al.2015b] O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "2016b. A network-based end-toend trainable task-oriented dialogue system",
      "author" : [ "Wen et al.2016b] T.H. Wen", "M. Gasic", "N. Mrksic", "L.M.R.-B", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young" ],
      "venue" : "arXiv preprint arXiv:1604.04562",
      "citeRegEx" : "Wen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "POMDP-based statistical spoken dialog systems: a review",
      "author" : [ "Young et al.2013] S. Young", "M. Gasic", "B. Thomson", "J.D. Williams" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Young et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Zaremba et al.2015] W. Zaremba", "I. Sutskever", "O. Vinyals" ],
      "venue" : "arXiv preprint arXiv:1409.2329",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "One line of work has tackled the problem using partially observable Markov decision processes and reinforcement learning with carefully designed action spaces (Young et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 179
    }, {
      "referenceID" : 19,
      "context" : "the recurrent unit (Hochreiter and Schmidhuber, 1997) with a bias of 1 added to the forget gate in the style of (Zaremba et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "We train using a cross-entropy loss and the Adam optimizer (Kingma and Ba, 2015), applying dropout (Hinton et al., 2012) as a regularizer to",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014), a restaurant reservation system dataset.",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014), a restaurant reservation system dataset. While the goal of the original challenge was building a system for inferring dialogue state, for our study, we use the version of the data from Bordes and Weston (2016), which ignores the dialogue state annotations, using only the raw text of the dialogues,",
      "startOffset" : 103,
      "endOffset" : 338
    }, {
      "referenceID" : 8,
      "context" : "ficult (Liu et al., 2016).",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "• BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al.",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : ", 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; Li et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : ", 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; Li et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "We also include the model of (Liu and Perez, 2016), referred to as GMemNN, and the model of (Seo et al., 2016), referred to as QRN, which currently is state-of-the-art.",
      "startOffset" : 92,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "Task-oriented dialogue focuses on conversational agents that participate in userinitiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an explicit representation by depending on a latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. We complement recent work by showing the effectiveness of simple sequence-to-sequence neural architectures with a copy mechanism. Our model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on DSTC2.",
    "creator" : "LaTeX with hyperref package"
  }
}