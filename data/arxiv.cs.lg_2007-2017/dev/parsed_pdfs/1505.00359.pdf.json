{
  "name" : "1505.00359.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Can deep learning help you find the perfect match?",
    "authors" : [ "Harm de Vries", "Jason Yosinski" ],
    "emails" : [ "mail@harmdevries.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Online dating has become the dominant way to seek romantic and sexual partners. Big dating services, such as OKCupid.com and Match.com, have an enormous amount of users which makes it important to quickly filter the user base by your preferences. The aim of dating systems is to help you in this process by presenting the most promising profiles. The traditional way to recommend profiles is to calculate match scores that are based on social and physical attributes e.g. body type and education level. The most popular dating app to date, Tinder1, employs an alternative matching strategy. Profile pictures2 of geographically nearby users are one by one presented, and a user can quickly decide to like or dislike the profile by swiping right or left, respectively. If both users like each other, they are a match and have the ability to chat with each other to possibly arrange an offline date.\nThe success of these apps indicates the importance of visual appearance in the search for the ideal partner, and highlights that attribute based matching algorithms are missing important information. However, extracting visual information, like attractiveness and personality type, from a profile picture is a challenging task. Recently proposed matching algorithms [12, 4, 1] sidestep this problem by using collaborative filtering. Instead of calculating matching scores based on the content of a profile, such systems recommend profiles that are high ranked by “similar” users. One of the drawbacks of collaborative filtering is that it suffers from the so-called cold start problem: when a new user enters the system it cannot make recommendations due to the lack of information. Understanding the content of the profile pictures could partially solve this cold start problem; We still can not recommend profiles to a new user, but we can recommend his/her profile to existing users.\nThe 2012 winning entry [11] of the ImageNet competition[5] has rapidly changed the field of computer vision. Their convolutional network (convnet) trained on a large labeled image database significantly outperformed all classical computer vision techniques on a challenging object recognition\n1Available in 24 languages, and an estimated user base of 50 million 2Also your mutual interests and friends are shown, but most emphasis is put on pictures\nar X\niv :1\n50 5.\n00 35\n9v 1\n[ cs\n.L G\n] 2\nM ay\ntask. The key ingredient of the success of convnets and other deep learning models [3] is that they learn multiple layers of representations as opposed to hand-crafted or shallow features. Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].\nMotivated by these recent advances, we investigate in this paper whether we can successfully train such deep learning models to predict attractiveness from a profile picture. To this end, the author of this paper collected and labeled more than 9K profile pictures from dating app Tinder. We found, however, that the dataset was still too small to successfully train a convolutional network directly. We overcome this problem by using transfer learning i.e. extracting features representation from another neural network. Several studies[6, 14] have demonstrated that high layer activations from top-performing ImageNet networks serve as excellent features for recognition tasks for which the network was not trained. The introduced attractiveness prediction task is defined over a very specific image distribution, namely profile pictures, possibly making transferability of ImageNet features rather limited. We therefore investigate if we can transfer from a task that’s much more related to attractiveness prediction: gender prediction from profile pictures."
    }, {
      "heading" : "2 The task and the data",
      "text" : "The aim of this project is to investigate whether we can predict preferences for potential partners solely from a profile picture. I have taken myself as the object of study. Although the results of one person can never be statistically significant, we consider it as a first step to study the feasibility of modern computer vision techniques to grasp a subtle concept such as attractiveness. Note that predicting beauty is a related but different concept: one can be beautiful but nevertheless not be the type you’re attracted to."
    }, {
      "heading" : "2.1 Attractiveness dataset",
      "text" : "In order to extract my preferences, I labeled 9364 profile pictures from Tinder as either a like or dislike. Quite surprisingly, the dataset is fairly balanced with 53% likes and 47% dislikes. This might lead you to conclude that I’m not very picky. However, we found that Tinder does not provide unbiased samples from the population, but gives you popular profiles more frequently3. Note that Tinder profiles contain up to five pictures, but I only judged and labeled the first one. The collected pictures were judged on a scale of 360x360, but later rescaled to 250x250 when training the convnet model for computational reasons.\nDuring labeling I experienced the disadvantage of a binary classification problem. Some of the profile pictures are just on the border of like and dislike, and basically your mood4 determines whether you like or dislike the picture. Unfortunately, this makes the attractiveness labelling quite noisy, and thus harder to learn for any model. In order to quantify how much noise entered the labelling process, I performed another experiment a couple of weeks after the original labeling. This period was long enough to not remember/recognize the profile pictures. I classified 100 random pictures from the dataset, and compared them with my original labelling. I made 12 errors out of 100, meaning that I achieved 88% accuracy on my original labeling. If we assume that these errors come from a 50/50 guess, we estimate that roughly a quarter of the profile pictures are not consistently labeled.\nAnother useful question to ask at this point is: how difficult is it for other people to predict my preferences? We investigate this question by setting up a small experiment with the second author of this paper. He studied 100 images and their corresponding labels with the goal to achieve high accuracy on 100 unseen profile pictures. During the training procedure he first looked at all 50 dislike and 50 like pictures side by side, scrolling through them all a few times. Then the training set was shuffled and he went through the pictures one a time, predicting the label for each. The correct label was shown after each image, so that he could learn from his mistakes. He obtained 86%, 82%, 88%, 88% accuracy during the four iterations. Memorizing his last mistakes could\n3We speculate that Tinder does so to keep you greedy: the hope to match with pretty girls keeps you using the application. Another explanation is that mostly attractive people are using the application.\n4I found that it also matters which profile pictures you have seen before: after a series of likes there is a tendency to keep liking.\ndefinitely improve this performance, but probably would not lead to better test set accuracy. The test performance was measured on the same 100 random pictures as the consistency experiment to compare the relative accuracy. He made 24 errors on the same images, and achieved 76% accuracy.\nThe results of this simple experiment gives us a rough indication of the hardness of the task, although we have to be careful to interpret these numbers. On the one hand, the preferences of the second author could be well-aligned with the first author, giving us a too optimistic measure of the learned prediction performance. On the other hand, he only studied a 100 pictures, and increasing the number examples might increase his performance.\nAs a final note we stress that the collected profile pictures have much variation in viewpoints and personality types. In contrast to standard image recognition benchmarks, faces are not aligned and persons are not always in the center of the image. As we show in Section 3.1, this makes it difficult to train convnet directly on the small dataset."
    }, {
      "heading" : "2.2 Gender dataset",
      "text" : "As we describe in Section 3.1, we found that the collected dataset is too small for a convolutional network to work well directly. We therefore collected another 418452 profile pictures from the OKCupid dating site with labeled gender and age. To make training of this neural network straightforward, we created this dataset such that we have an equal number of male and female profile pictures. We discard age information in the following, because we found that the signal was too noisy5. Our strategy is to train a convnet for gender prediction, and then transfer the learned feature representations to attractiveness prediction.\nThe dataset was collected from a real-world dating site which raises questions about the quality of the provided labels. For example, some pictures might be wrongly labeled, or even impossible to discriminate for humans. It was too time consuming to clean up the full dataset, so we estimated the quality of the labels as follows. We randomly sampled 1000 images from the gender dataset, and categorized them as either one of the following:\n5Most OKCupid users are between 20 and 35 which makes it even hard for humans to predict age. Not to mention that people tend to lie about their age on online dating websites.\nClean: If the gender is clearly recognizable from the picture. Unknown: If there isn’t a person in the picture. Note that it is sometimes possible to infer gender\nfrom other objects in the picture. For instance, a car in the picture most likely corresponds to a male.\nMixed: If both males and females appear in the picture. It’s sometimes possible to infer the gender by looking at the leading person in the picture.\nNo face: There is no face visible in the picture; only some body parts. For instance, if a picture is taken from far away and only the back is visible.\nHalf-face: If most part of the face is not visible. For example, a close-up of the eye only.\nWe provide examples of the categories in Figure 1. The resulting numbers per category are presented in Table 1. We conclude that almost 90% of the pictures is clean, and the remaining 10% are at least difficult. We simply estimate the human performance around 95%, and consider the remaining errors due to uninteresting factors. Moreover, our primary task is attractiveness prediction, thus learning the subtle uninteresting factors might not lead to better transferable features.\nAs usual in prediction tasks, we randomly split the attractiveness and gender dataset in a training, validation and test set. For the attractiveness dataset we used 90%, 5% and 5% of the data for the training, validation and test set, respectively. Since we have more data for gender prediction, we make a 80%, 10%and 10% split."
    }, {
      "heading" : "3 Experiments",
      "text" : "In Section 3.1 we first train a convnet to predict attractiveness from the small labeled dataset. Section 3.2 presents the details of training a convnet for gender prediction. We then investigate in Section 3.3 how well the features of this network transfer to attractiveness prediction. We compare against features obtained from VGGNet, one of the top performing convnets on ImageNet."
    }, {
      "heading" : "3.1 Attractiveness prediction",
      "text" : "After collecting the data, our first attempt was to train a convnet on the attractiveness dataset. Our architecture is inspired by VGGNet [14], and follows the latest trends in architecture design to have very deep networks and small filter sizes. We use five convolutional layers, all with 3x3 filter sizes and rectified linear activation functions. Each layer is followed with non-overlapping max pooling of size 2x2. We start with 8 feature maps in the first layer and gradually increase it to 32 in the last convolutional layer. There are two fully connected layers on top of respectively 32 and 16 units. The network has in the order of 870K parameters. The details of the architecture are shown in Table 2 (a).\nThe only preprocessing step that is applied is subtracting the training set mean from all images. We regularize the network by applying dropout [8] with probability 0.5 on the fully connected layers, and include L2 weight decay with coefficient 0.001. The convnet is trained with Stochastic Gradient Descent (SGD) to minimize the negative log likelihood, and optimized for 50 epochs with a learning rate of 0.001 and 0.9 momentum.\nFigure 2 (a) shows the training and validation misclassification rate during optimization. We can see that even this very small network with strong regularization immediately overfits. We think that there is simply too much variation in the profile pictures for the convnet to learn the regularities\nfrom the raw profile pictures. Hence, we decide not to explore further regularization techniques, but instead focus on transfer learning. In the next sections we investigate if a convnet trained for gender prediction results in good representations for attractiveness prediction."
    }, {
      "heading" : "3.2 Gender prediction",
      "text" : "The gender dataset with over 400K images is much bigger than the attractiveness dataset. Therefore, we can afford to train a much bigger network without the risk of over fitting. The proposed convnet architecture is similar in spirit to the attractiveness network presented in the previous section. We decide to use nine convolutional layers with 3x3 filter sizes and rectified linear activation functions. We further apply 2x2 max pooling after two convolutional layer, except for the first layer where we directly apply pooling after one layer. We follow the rule of thumb introduced in [14] and double the number of feature maps after each pooling layer, except for the last pooling layer where we kept the number of feature maps the same. The biases (in contrast to the weights) in the convolutional layers are untied i.e. each location in a feature map has its own bias parameter. The final 12-layer architecture is shown in Table 2 (b), and has over 28 million parameters.\nWe tried several small modifications on this architecture: decreasing the number of feature maps (starting from 32), using tied biases, and adding an extra pooling after the two final convolutional layers. However, we obtained the best performance with the network described above.\nWe also apply dropout with probability 0.5 on the fully connected layers, and include L2 weight decay with coefficient 0.0001. The weights are initialized from U(−0.06, 0.06), while the biases are initially set to zero. We again train with Stochastic Gradient Descent (SGD) to minimize the negative log likelihood. We optimized for 13 epochs with a learning rate of 0.001 and momentum coefficient 0.9. The models were implemented in Theano[2] and took about 3 days to train on a GeForce GTX Titan Black. The misclassification rates during training are shown in Figure 2 (b). Note that in this figure the training error is aggregated over mini-batches, and only gives us a rough estimate of the true training error.\nThe final model was selected by early stopping, and achieved 7.4% and 7.5% error on the validation and test set, respectively. In Section 2.2 we established that approximately 10% of the dataset is difficult. Hence, we consider 92.5% accuracy as very decent performance that approaches human level performance."
    }, {
      "heading" : "3.3 Transfer learning",
      "text" : "We compare two transfer learning strategies: one from the gender net and the other from VGGNet, one of the top-performing ImageNet convnets."
    }, {
      "heading" : "3.3.1 Gender",
      "text" : "After training the gender network we explore if the features are helpful to predict attractiveness. The gender network has in the order of 28 million parameters, and the available attractiveness dataset is relatively small, so training the full network probably leads to overfitting. We therefore decide to train only the last layers of the gender network. We compare training the last, the last two and the last three layers, which have 1026, 525K and 8.9M parameters, respectively. We do not apply dropout when training these last layers, but we do use the same L2 regularization as in the gender network. We train with SGD for 50 epochs with a learning rate of 0.001 and 0.9 momentum.\nThe training and validation curves are shown in Figure 3 (a-c). Note that the transfer performance is pretty poor. Only training the last layer barely decreases the training error, and significantly underfits. On the other hand, training all fully connected layers does decrease the training error very quickly, but doesn’t carry over to the validation error. With early stopping on the validation error, we achieved the best performance of 61.3% accuracy on the test set by only training the last two layers. The results"
    }, {
      "heading" : "3.3.2 ImageNet",
      "text" : "The features extracted from ImageNet networks are known to achieve excellent transfer performance [6]. We decide to use VGGNet [14], one of the top performing ImageNet convnets, and use Caffe[10] to extract the features. In order to feed the images to VGGNet, we resize all images to 224x224. We extract 4096 dimensional features from the highest layer (called FC7) of the 19-layer VGGNet. We put a logistic regression with weight decay on top of the extracted representation. After finetuning the hyperparameters, we obtained the best results with a L2 regularization coefficient of 0.8, a learning rate of 0.0001, and momentum of 0.9. Note that relatively strong weight decay is still necessary to prevent overfitting. The error curves during training are presented in Figure 3 (d). We again apply early stopping on the validation error. Our best model obtains an validation and test accuracy of 66.9% and 68.1%, respectively."
    }, {
      "heading" : "4 Discussion and Conclusion",
      "text" : "The VGGNet features clearly outperform the features obtained from the gender prediction task. Our findings confirm that ImageNet activations are excellent image features for a wide variety tasks. However, we did not expect them to outperform the features from the gender prediction task since that network was trained on a similar set of images. One possible explanation for the poor transfer is that the gender network learns features that are invariant to female characteristics, and are therefore not appropriate to discriminate between profile pictures. Another reason could be that the gender network only has two classes, which does not force the network to learn very discriminative features. One possible direction for future research is to investigate if adding an extra class of non-profile pictures leads to better transferable features.\nFurther studies could also investigate other ways to deal with the huge variability in the profile pictures. For example, face extraction could be a good way to reduce variability, while keeping the most important aspect of attractiveness. We believe that the most promising avenue is to collect a bigger and cleaner dataset from which we can learn a good feature representation for attractiveness prediction. It remains, however, an open question what kind of labeled information is necessary to learn a good image representation of attractiveness."
    }, {
      "heading" : "5 Acknowledgement",
      "text" : "We thank Mehdi Mirza for extracting the VGGNet features. We also thank the developers of Theano[2] and Blocks and the computational resources provided by Compute Canada and Calcul Québec. We are grateful to all members of the LISA lab for helpful discussions, in particular to Yoshua Bengio, Aaron Courville, Roland Memisevic, Kyung Hyun Cho, Yann Dauphin, Laurent Dinh, Kyle Kastner, Junyoung Chung, Julian Serban, Alexandre de Brébison, César Laurent and Christopher Olah."
    } ],
    "references" : [ {
      "title" : "Ccr-a contentcollaborative reciprocal recommender for online dating",
      "author" : [ "J. Akehurst", "I. Koprinska", "K. Yacef", "L.A.S. Pizzato", "J. Kay", "T. Rej" ],
      "venue" : "IJCAI, pages 2199–2204,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Y. Bengio" ],
      "venue" : "Found. Trends Mach. Learn., 2(1):1–127, Jan.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Recommender system for online dating service",
      "author" : [ "L. Brozovsky", "V. Petricek" ],
      "venue" : "CoRR, abs/cs/0703042,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L. jia Li", "K. Li", "L. Fei-fei" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "CoRR, abs/1310.1531,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "CoRR, abs/1502.01852,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "CoRR, abs/1502.03167,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "Proceedings of the ACM International Conference on Multimedia, pages 675–678. ACM,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Collaborative filtering for people-to-people recommendation in online dating: Data analysis and user trial",
      "author" : [ "A. Krzywicki", "W. Wobcke", "Y. Kim", "X. Cai", "M. Bain", "A. Mahidadia", "P. Compton" ],
      "venue" : "International Journal of Human-Computer Studies, 76(0):50 – 66,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "CoRR, abs/1312.6229,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "CoRR, abs/1409.4842,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "We resort to transfer learning and compare feature representations transferred from VGGNet[14] and a self-trained gender prediction network.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "Recently proposed matching algorithms [12, 4, 1] sidestep this problem by using collaborative filtering.",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "Recently proposed matching algorithms [12, 4, 1] sidestep this problem by using collaborative filtering.",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "Recently proposed matching algorithms [12, 4, 1] sidestep this problem by using collaborative filtering.",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "The 2012 winning entry [11] of the ImageNet competition[5] has rapidly changed the field of computer vision.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "The 2012 winning entry [11] of the ImageNet competition[5] has rapidly changed the field of computer vision.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "The key ingredient of the success of convnets and other deep learning models [3] is that they learn multiple layers of representations as opposed to hand-crafted or shallow features.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 8,
      "context" : "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "Several studies[6, 14] have demonstrated that high layer activations from top-performing ImageNet networks serve as excellent features for recognition tasks for which the network was not trained.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "Several studies[6, 14] have demonstrated that high layer activations from top-performing ImageNet networks serve as excellent features for recognition tasks for which the network was not trained.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "Our architecture is inspired by VGGNet [14], and follows the latest trends in architecture design to have very deep networks and small filter sizes.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "We regularize the network by applying dropout [8] with probability 0.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "We follow the rule of thumb introduced in [14] and double the number of feature maps after each pooling layer, except for the last pooling layer where we kept the number of feature maps the same.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "The models were implemented in Theano[2] and took about 3 days to train on a GeForce GTX Titan Black.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "The features extracted from ImageNet networks are known to achieve excellent transfer performance [6].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "We decide to use VGGNet [14], one of the top performing ImageNet convnets, and use Caffe[10] to extract the features.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "We decide to use VGGNet [14], one of the top performing ImageNet convnets, and use Caffe[10] to extract the features.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "We also thank the developers of Theano[2] and Blocks and the computational resources provided by Compute Canada and Calcul Québec.",
      "startOffset" : 38,
      "endOffset" : 41
    } ],
    "year" : 2017,
    "abstractText" : "Is he/she attractive or not? We can often answer this question in a split of a second, and this ability is one of the main reasons behind the success of recent dating apps. In this paper we explore if we can predict attractiveness from profile pictures with convolutional networks. We argue that the introduced task is difficult due to i) the large number of variations in profile pictures and ii) the noise in attractiveness labels. We find that our self-labeled dataset of 9364 pictures is too small to apply a convolutional network directly. We resort to transfer learning and compare feature representations transferred from VGGNet[14] and a self-trained gender prediction network. Our findings show that VGGNet features transfer better and we conclude that our best model, achieving 68.1% accuracy on the test set, is moderately successful at predicting attractiveness.",
    "creator" : "LaTeX with hyperref package"
  }
}