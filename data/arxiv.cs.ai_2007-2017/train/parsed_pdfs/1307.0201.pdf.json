{
  "name" : "1307.0201.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Simulating Ability: Representing Skills in Games",
    "authors" : [ "Magnus Lie Hetland" ],
    "emails" : [ "mlh@idi.ntnu.no" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: games, characters, skills, task resolution, simulation, psychometrics"
    }, {
      "heading" : "1 Introduction",
      "text" : "One of the most fundamental concept in games is the representation of agents, entities acting on behalf of a player, and the simulation of their abilities. Consider, for example, the different pieces of chess, and the moves they are capable of; or the troops in the game of Risk [1], and their offensive and defensive strengths, represented by the number of dice to roll. In earlier games such as these, the game mechanics tend to treat abilities in a rather abstract and simplified manner, but as games have become more true to life, so has the simulation of skills. One important line of development in this area started with the tabletop roleplaying games, such as the original Dungeons & Dragons [2], and has continued to the present day, with computer games requiring skill systems of high sophistication. Yet little has been published about such skill systems, or what constitutes realistic simulation of skills and abilities. This paper is an attempt at drawing parallels between skill simulation in games, on the one hand, and skill modeling in psychometrics on the other, arguing that a simple log-odds model is an attractive alternative to many of the ad hoc systems that have been used so far."
    }, {
      "heading" : "2 Skills and Abilities in Games",
      "text" : "In this section, I briefly discuss some existing approaches to skill modeling. I start by examining a variety of models that have been used in tabletop roleplaying games. This is motivated in part by the fact that descriptions of these models are\nar X\niv :1\n30 7.\n02 01\nv2 [\ncs .G\nT ]\n2 6\nJu l 2\nmuch more readily available than the ones embedded in proprietary game code, and in part by the fact that “Most [computer roleplaying games] use a system based on an old paper [roleplaying game] to handle their game mechanic” [3, p. 358]. After outlining the main models, I describe some flaws in the most commonly used one, and outline an alternative."
    }, {
      "heading" : "2.1 Some Classic Approaches",
      "text" : "A survey of published roleplaying games indicates that a few mechanisms are by far the most common. There is signiticant variation in the details, but even so, a few main models may be distilled.1\nUniform scale. The most popular mechanic is the uniform scale. While it has different implementations in terms of die rolls, the underlying model is this: Skill and difficulty are both represented on the same scale, and the probability of success is a linear function of their difference. One common implementation is treating the skill as a fixed probability (or a fixed value on a uniformly random scale), and the difficulty as an additive modifier. A uniformly random value is then generated using dice, and if this value is equal to or less than the modified skill, the action is a success. Another common implementation is to treat the skill as an additive modifier to the uniform die roll, trying to roll a sum that exceeds a difficulty level. Various variations are of course possible here, but they amount to the same thing, that is, a linear mapping from the difference skill − difficulty to the probability of success, capped at 0 and 100 %. Alternatively, it can be viewed as simulating the outcome as being uniformly distributed, usually symmetrically around the skill level. The difficulty is then the outcome quality required to succeed.∗\nSum of dice. This simply means that you roll a fixed number of identical dice to generate your distribution. The uniform scale is the special case you get with a single die; with even two dice, the distribution becomes decidedly more bell-curved. This mechanism is used in about one in five games, with both the number and type of dice varying quite a bit. The two main implementations discussed for the uniform scale (roll under skill or roll over difficulty) are both used here as well.\nBinomial die pool. The simplest version of this is flipping a number of coins equal to the skill level, and counting the number of heads. More generally, a number of dice representing the skill level are rolled, and each die exceeding a given threshold value counts as a success, usually with multiple successes representing a better outcome. In some games, difficulty is represented by the target number, while in others, the target is fixed, and difficulty is represented as the required number of successes.\n∗ Note that this is not necessarily how outcome quality is simulated in these games.\nGeneral die pool. This is a broader class of systems that includes games where the ability is represented as a number of dice, and the outcome is the sum of those dice. This is, of course, a generalization of the binomial case, except that the distribution for each die is fixed (as opposed to the Bernoulli trials in the binomial case).\nStep dice. This is similar to the uniform case, as the outcome variable is, indeed, uniform. However, instead of modeling skill as an additive offset, it is treated as multiplicative; or, rather, the skill is mapped to the type of die, which dictates the range of outcomes.\nMax-die pool. This is similar to the die pool mechanic, but rather than adding the dice, the maximum is used. That is, the skill indicates the number of dice to roll, and the highest die roll indicates the outcome.\nThere are, of course, other mechanics, but most games fall into one of these categories, and of them, the uniform scale is clearly the most commonly used, accounting for about half the games surveyed."
    }, {
      "heading" : "2.2 The Problem With Percentiles",
      "text" : "The uniform distribution certainly has an intuitive appeal. A superficial analysis seems to indicate that, as opposed to for the bell curve, a fixed modifier such as +10 % means the same whether your starting probability (your skill) is high or low. This analysis quickly breaks down when looking at the edge cases. A bonus of +10 % is clearly more useful to anyone with a skill of 90 % or less than to those more skilled.\nWe might, instead, consider an unlimited scale, where +10 really does always mean +10, for example.∗ If we ignore randomness, and assume that skill level is all that matters for the outcome, a skill level of a will always beat a skill level (or difficulty level) of b, if a > b. This is in a sense the assumption underlying so-called Guttman scales, and the model discussed in the following sections is an extension to this, to account for random variations of various kinds.\nWright highlights the problems with the uniform scale (in the context of test equating) as follows [4]. Consider two tasks of the same kind (involving the same skill), one easy and one hard. Let’s say two persons, A and B, attempt both tasks. In general, we’d expect them to succeed more often on the easy task than on the hard task, except if they’re at the extremes (succeeding 0 % of the time on the easy task, or 100 % on the hard one). We could also assume that A is more skilled B, and should therefore succeed more often than B on both tasks. As shown in Fig. 1, our assumptions lead to the need for a non-linear mapping between the two; the uniform, linear model gives us some problematic thresholding effects. Figure 2 illustrates this point by comparing the piecewise linear curve resulting from the uniform model with a smooth, logistic curve. The latter, I will argue in the following, is a much better choice for skill modeling in general.\n∗ Exactly what this means will be explained in the following."
    }, {
      "heading" : "2.3 The Outline of a Model",
      "text" : "The following brief line of reasoning is based on Rasch’s motivation for his eponymous probabilistic model of skill levels and difficulty [5, pp. 72–75]. While it does not cover all the details we’ll be investigating, it does give us an outline of the form our skill model should take.2\nWhat we’re seeking is a function f that takes a skill level a and a difficulty level x and produces a probability of success, f(a, x). If we assume that both a and x are measured on a multiplicative scale, we’d expect a person that is twice as skilled to be able to deal with tasks that are twice as hard, or, in general, that f(a, x) = f(ka, kx), for any positive constant k. Now, consider person A and person B, with respective skill levels of a and b, and problems X and Y with difficulty levels of x and y. Assume that A is k times as skilled as B, that is, a = kb, and problem X is k times as hard as problem Y, x = ky. This, of course, is equivalent to a/x = b/y. In any such case, we’d want f(a, x) = f(b, y). In other words, we don’t need to consider skill and difficulty separately, only their ratio, and we can write f(a/x) rather than f(a, x). For problems that are way too hard (low ratio) we’d expect a probability near zero, and for problems that are just too easy (high ratio), we’d want to get close to a 100 % chance of success. If, for simplicity (and without loss of generality), we assumed that skill and difficulty are on the same scale, we would also have f(1) = 50 %. If skill and difficulty are to be interchangeable (as, for example, when the skills of two persons are pitted against each other), we would also require the probability to be symmetric, that is,\nf(a/x) = 1− f(x/a) .\nEven given these desired properties, we have some leeway in choosing the exact form of f . One function satisfying the requirements is shown in Fig. 3. The details of, and motivation for, this specific function are discussed in the following."
    }, {
      "heading" : "3 Skills as Evidence: the 1PL",
      "text" : "In this section, I show that a rather natural interpretation of skill and difficulty as evidence for and against a positive outcome yields a model that conforms to all the desired properties outlined so far."
    }, {
      "heading" : "3.1 Bayes Factors and Odds Ratios",
      "text" : "We have seen that the probability of success should be computed as a function of the skill-to-difficulty ratio a/x, and that this function must have some basic properties that amount to getting a sigmoid curve when plotting it with a logarithmic horizontal axis, as in Fig. 3. It would seem that we can choose whichever function we want, as long as it looks sort of like Fig. 3, and that the various functions may all fit different situations with a varying degree of accuracy. If we want one unifying model, however, there is a particular function that stands out:\nf(a, x) = f(a/x) = a/x\n1 + a/x =\na\na + x . (1)\nThis is the function Rasch used, because it was the simplest function he knew of that had the desired properties [5, p. 74].∗ Simplicity is not the only reason to favor this function, however. For one thing, it admits of a very intuitive interpretation, namely that the odds of success are a :x. The same idea was used already by Zermelo [6], in modeling chess players. In his model, the odds of player A with skill level a winning over B, with skill b, were simply a : b. So if player A was ten times as skilled as B, she would also win ten times as often. But beyond its simplicity and intuitive appeal, this model follows very naturally from the interpretation of skill and difficulty as evidence, that is, as facts that influence the probability of the outcome.\nFirst, let’s look at the core rule for combining new evidence with existing knowledge: Bayes’ theorem. When formulated in terms of odds, it can be written as follows.\nR = PL\nHere, P is the prior odds, L is the likelihood ratio (or Bayes factor), and R is the revised odds. In other words, P and R represent your degrees of belief (as odds) in a given hypothesis before and after being presented by a given piece of evidence, and L tells you how much more likely it would be to observe that evidence if the hypothesis were true.\nIf we (for simplicity) assume even prior odds,† and a series of factors whose strengths (likelihood ratios) are L1 . . .Ln, the revised odds are\nR = L1 × · · · × Ln . ∗ That is, it was the simplest function he knew of that increases from 0 to 1 as a/x\ngoes from 0 to ∞. † We could also let the innate task difficulty be represented as the inital odds, for\nexample; it makes no difference to the calculations.\nAny factor with strength L for an outcome automatically has the strength 1/L against that same outcome. So if we let skill level and difficulty be the two only factors, acting for and against success, respectively, we end up with Rasch’s formulation. Similarly, if we let two people’s skills be the only two factors, acting against each other, we end up with Zermelo’s formulation.\nThe previous arguments give us a plausible interpretation of Rasch’s probability function for the skill-to-difficulty ratio. But how much leeway do we have in our modeling here? If we wish to interpret factors such as skill and difficulty as pieces of evidence, modeled by their weight, whatever that might mean, how much freedom do we have in choosing the specific mapping? As it turns out, not much. In fact, Good [7, 8] makes a convincing case that this is the only formalization possible. In fact, the model follows from the following three requirements:3\n1. The weight of one piece of evidence should only depend on how likely the evidence is to have been present given success, and given failure. 2. The odds of success should only depend on the prior odds and the combined weight of evidence for and against it. 3. The weight of independent pieces of evidence is combined by multiplication.\nIn Good’s derivation, the combination in the third requirement is actually by addition, rather than multiplication, which simply entails using a logarithmic transform, measuring everything in log-odds units, or logits. This also aligns quite well with the grades of evidence described by Jeffreys [9, p. 432], as shown in Table 1. These grades are also based on the Bayes factor L, and follow a logarithmic progression.\nThe resulting probability function is the so-called logistic function, which is shown compared with the uniform model in Fig. 2, and is what the right panel in Fig. 3 would have depicted, had the horizontal axis been linear."
    }, {
      "heading" : "3.2 IRT and Rasch Models",
      "text" : "The model we have derived by viewing skill and difficulty levels as strength of evidence is, in fact, the one commonly known as the one-parameter logistic\nmodel, or 1PL, in the field of psychometrics.4 This model was initially introduced by Rasch [5], but has since become a foundation for both so-called invariant measurement in the social sciences [10] and in the item-response theory of psychological testing [11].\nThe requirements of invariant measurement constrain us to using a sigmoid probability function on an additive scale, as discussed in the introduction. It is possible to use, say, a probit scale, but the logit model is vastly more common. While a very important reason for this is its ease of computation, as we have seen in the preceding section, there are also philosophical and methodological reasons for favoring it.\nThis use of a logistic distribution can be found elsewhere as well. For example, the initial Elo model for rating chess players used a normal distribution, and this is still used by FIDE, the World Chess Federation. However, investigations by the United States Chess Federation have found that a logistic curve is a better match to real-world outcomes, and so they have switched to a logit-based Elo model.∗ There are other player skill estimation systems, such as Microsoft’s TrueSkill ranking and matching system,† that use a normal distribution.\nFor our goals of approximate simulation the difference may be of little practical importance though, given that the greatest absolute difference between the two cumulative distributions is less than 1 % [12, p. 120] (see Fig. 4).\nAt this point it might be worth pointing out that even though the most commonly used uniform model is a poor match for the kind of sigmoid we’re looking for, the second-most popular sum-of-dice mechanism is a rather good fit. See, for example, Fig. 5, which compares the distribution for the sum of three six-sided dice (3d6) alongside a moment-matched logistic curve."
    }, {
      "heading" : "4 The 2PL, 3PL and 4PL",
      "text" : "The 1PL nicely captures skill and difficulty as (additive) levels of evidence for a successful outcome. There are aspects of task resolution that are still not captured by this model, though, but that can be addressed by more flexible psychometric models: the two-, three- and four-parameter models (2PL, 3PL and 4PL, respectively).\nFirst, consider the choice of scale. We have chosen to use a single scale for measuring skills and difficulties, but perhaps this is too restrictive? Consider the following generalization of Bayes’ theorem, described by Zlotnick [13]:\nR = PLr\nHere, r ∈ [0, 1] is a reliability rating of the evidence represented by the Bayes factor L. Zlotnick describes r as the probability of the reported evidence being real, as opposed to pure fabrication. For the purpose of skill modeling, we could\n∗ See, e.g., “Arpad Elo and the Elo Rating System” in the December 16, 2007 issue of Chess News 〈http://en.chessbase.com/home/TabId/211/PostId/4004326〉. † 〈http://research.microsoft.com/en-us/projects/trueskill〉\nview r as a degree of relevance between the modified skill level and the actual outcome. The lower the relevance, the less impact the skill level will have, and the more random the outcome. In our logit model, r would simply be a factor, modifying the units of the logit scale. If we relax the restriction r ∈ [0, 1], we end up with the 2PL, where the scale or slope of the logistic curve is a separate parameter.∗ Taking Zermelo’s model of chess as an example, we might want to apply an r > 1 for the game of Go, and an r < 1 for the game of Ludo, representing the variability or degree of randomness inherent in the games. For a game of “heads-or-tails,” we would set r = 0, as the level of gambling skill would be completely irrelevant, and the outcome is purely random.\nSome skill models (such as Microsoft’s TrueSkill system) assume that the variance in performance varies from person to person. In other words, the r could be linked to the task at hand, to the person attempting it, or to the relationship between the two (e.g., degree of relevance of the applied skill to the task).\nThe 2PL is still a bit limited in that, unless we set r = 0, the probability of success for low and high modified skill still converges to 0 and 100 %, respectively. This is an issue that has been tackled in psychometrics as well, in the context of guessing in multiple-choice tests. The aim here is estimating the skill level of the students (in logits) based on their test result, given that random guessing will give you the correct answer to a question, say, 25 % of the time. In this case, for lower skill levels, the probability of success should converge on 25 %, not 0 %. This requirement gives rise to the three-parameter logistic model (3PL) of Birnbaum [14]. Generalizing to the natural case where the upper asymptote may differ from 1 (that is, we can have random failure), we end up with the four-parameter logistic model (4PL) [15]. This, then, is the model I propose for skill simulation in games: that the probability of success for a skill level a and difficulty x be given by\nf(a, x; r, `, u) = ` + (u− `) · 1 1 + e−r(a−x) , (2)\nwhere a and x are (additive) ability and difficulty, as before, r is the slope (indicating the reliability or relevance of the modified skill), and ` and u are the lower and upper asymptotes, respectively. In other words, ` is the probability of failing randomly, regardless of skill, while 1− u is the probability of succeeding randomly, regardless of skill."
    }, {
      "heading" : "5 Discussion and Future Work",
      "text" : "In this paper I have argued that a four-parameter logistic model is a natural choice when simulating skills in games. It matches simple intuitions of bellcurve outcomes, it is backed up by viewing skills as evidence for success, and it is a model used in psychometric practice, for estimating skill levels based on\n∗ In psychometrics, r is commonly known as the discrimination parameter, describing the ability of a given task to discriminate between high and low skill.\nsuccesses and failures. Other, similar models are certainly possible. For example, a Gaussian rather than logistic distribution could be used, although the logistic distribution approximates the Gaussian very closely, is much easier to work with, computationally, and leads to the satisfying interpretation of skill levels and difficulty as weight of evidence.\nI have also argued for going beyond the basic Rasch model (1PL), parametrizing the slope and asymptotes of the probability function. There is some philosophical controversy about such additional parametrization in psychometrics, related to the concept of invariant measurement, but these don’t really apply to the problem of skill simulation.\nFuture work on the model might include examining its suitability for modeling outcome quality, rather than simply success and failure. This could be relevant, for example, to modeling damage in combat simulations. To what extent does it make sense to use the common die mechanic of simply generating a value from the (logistic) probability distribution, adding it to the modified skill, and viewing the result as an outcome quality? It might very well be that the outcome measures don’t map directly to the logistic scale; this is, of course, an empirical question [see, e.g., 16, 17].\nOne issue that has not been addressed in this work is the modeling of player skill, as opposed to character skill. Models such as Elo and TrueSkill are routinely used to match players of comparable skill levels. The logistic model of character skill proposed in this paper is also used in psychometric estimation. For games that employ both player and character skill, a hybrid approach could be employed, where player skill is estimated, and opposing characters are simulated with similar parameters, tuned to the proper level of challenge.\nAnother important issue is skill development and learning. For games that involve character development, it would be crucial to know how a skill, measured in logits, would increase with repeated use and practice. It seems, for example, that for many domains, the time to perform a task, as well as the number of errors, follows a power law as a function of the number of trials, the so-called “power law of practice” [18]. The number of errors is, of course, directly linked to skill level in the logistic model. This is also an area where player skill estimation could be useful, for evaluating learning and skill gain over time.\nAcknowledgements. The author would like to thank Ole Edsberg for fruitful discussions on the topic of this paper, including the role of discernment and degree of randomness in task resolution, and for input on existing player skill estimation systems.\nNotes\n1. This survey was based on the encyclopedic listing of published roleplaying games (currently 1524 games) maintained by Kim [19]. As the focus of the survey was die mechanics, games using cards, tables, pebbles, roulette wheels or other randomness generators, as well as diceless games (that is, ones without\nrandomness) were eliminated. Certain die mechanics that strayed too far from the independent simulation of a single action (for example, ones including dice that are spent and recuperated over time, or games that mix the use of dice with various other narrative mechanics) were also removed. This initial elimination accounted for 129 games (or about 8 %). Games whose die mechanic was not unambiguously described or readily available were also eliminated (another 652 games, or 42 %). There is no clear reason to assume that the elimination of these latter games represents any strong bias, and although the criteria used required some amount of subjective judgement, it is assumed that the remaining 729 games (48 %) form a sample that is representative enough for the current purposes.\n2. A more thorough discussion of this topic, with the requirements of so-called invariant measurement spelled out in detail, can be found in the recent book on this topic by Engelhard, Jr. [10, p. 13–17].\n3. The first requirement means that we can have a function W (H :E) for the weight of the evidence in favor of H provided by E, and that this would be a function of the likelihoods P (E |H) and P (E | ¬H), that is, W (H :E) = f(P (E |H), P (E | ¬H)), for some function f . The second requirement means that, for some function g, we can write P (H |E) = g(W (H :E), P (H)). Combining the two, we get P (H |E) = g(f(P (E |H), P (H | ¬H)), P (H)). For simplicitiy, we let x = P (H), y = P (E), and z = P (H |E), and get\nz = g ( f ( y · z\nx , y · 1− z 1− x\n) , x ) .\nFrom this, we see that g is mathematically independent of x, and must therefore depend on the value of f . Therefore, f must be mathematically independent of y, so it necessarily depends only on the ratio of its arguments, that is, of the likelihood ratio L = P (E |H)/P (E | ¬H). Also, f must be strictly monotonic (obviously increasing), because otherwise it would have the same value for two different values of z, which would violate the previous equation. If we add the third requirement (combination through multiplication), the odds ratio model follows.\n4. There are some philosophical differences between the Rasch model and the 1PL [11, p. 19], but they are mathematically equivalent, and the philosophical differences apply when viewed in the context of measurement, as opposed to the current issue of simulation."
    } ],
    "references" : [ {
      "title" : "Dungeons & Dragons : Rules for fantastic medieval wargames campaigns playable with paper and pencil and miniature figures",
      "author" : [ "G. Gygax", "D. Arneson" ],
      "venue" : "Tactical Studies Rules",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Andrew Rollings and Ernest Adams on Game Design",
      "author" : [ "A. Rollings", "E. Adams" ],
      "venue" : "New Riders",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Thinking with raw scores",
      "author" : [ "B.D. Wright" ],
      "venue" : "Rasch Measurement Transactions, 7(2):299–300",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Probabilistic Models for Some Intelligence and Attainment Tests",
      "author" : [ "G. Rasch" ],
      "venue" : "The University of Chicago Press, Chicago, expanded edition",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeitsrechnung",
      "author" : [ "E. Zermelo" ],
      "venue" : "Mathematische Zeitschrift, 29(1):436–460",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1929
    }, {
      "title" : "Corroboration",
      "author" : [ "I.J. Good" ],
      "venue" : "explanation, evolving probability, simplicity and a sharpened razor. British Journal for the Philosophy of Science, 19:123–143",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Weight of evidence: A brief survey",
      "author" : [ "I.J. Good" ],
      "venue" : "Bayesian Statistics, 2: 249–270",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "The Theory of Probability",
      "author" : [ "H. Jeffreys" ],
      "venue" : "Clarendon Press, Oxford, third edition",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Invariant Measurement: Using Rasch Models in the Social, Behavioral, and Health Sciences",
      "author" : [ "G. Engelhard", "Jr." ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "The Theory and Practice of Item Response Theory",
      "author" : [ "R.J. de Ayala" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "A logistic approximation to the cumulative normal distribution",
      "author" : [ "S.R. Bowling", "M.T. Khasawneh" ],
      "venue" : "Journal of Industrial Engineering and Management, 2(1):114–127",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A theorem for prediction",
      "author" : [ "J. Zlotnick" ],
      "venue" : "Studies in Intelligence, 11(4)",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Some latent trait models and their use in inferring an examinee’s ability",
      "author" : [ "A. Birnbaum" ],
      "venue" : "F. M. Lord and M. R. Novick, editors, Statistical theories of mental test scores, pages 395–470. Addison-Wesley",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "The four-parameter logistic item response theory model as a robust method of estimating ability despite aberrant responses",
      "author" : [ "W. Liao", "R. Ho", "Y. Yen" ],
      "venue" : "Social Behavior and Personality, 40(10):1679–1694",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Skill and chance in ball games",
      "author" : [ "C. Reep", "R. Pollard", "B. Benjamin" ],
      "venue" : "Journal of the Royal Statistical Society. Series A (General), 134(4):623–629",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Are soccer matches badly designed experiments",
      "author" : [ "G.K. Skinner", "G.H. Freeman" ],
      "venue" : "Journal of Applied Statistics",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Mechanisms of skill acquisition and the law of practice",
      "author" : [ "A. Newell", "P.S. Rosenbloom" ],
      "venue" : "Paper 2387, Carnegie Mellon University, Computer Science Department",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "An encyclopedia of role-playing games",
      "author" : [ "J.H. Kim" ],
      "venue" : "Published at http: //darkshire.net/jhkim/rpg/encyclopedia, November 2012. ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One important line of development in this area started with the tabletop roleplaying games, such as the original Dungeons & Dragons [2], and has continued to the present day, with computer games requiring skill systems of high sophistication.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "Wright highlights the problems with the uniform scale (in the context of test equating) as follows [4].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "The same idea was used already by Zermelo [6], in modeling chess players.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "In fact, Good [7, 8] makes a convincing case that this is the only formalization possible.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "In fact, Good [7, 8] makes a convincing case that this is the only formalization possible.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "This model was initially introduced by Rasch [5], but has since become a foundation for both so-called invariant measurement in the social sciences [10] and in the item-response theory of psychological testing [11].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "This model was initially introduced by Rasch [5], but has since become a foundation for both so-called invariant measurement in the social sciences [10] and in the item-response theory of psychological testing [11].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "This model was initially introduced by Rasch [5], but has since become a foundation for both so-called invariant measurement in the social sciences [10] and in the item-response theory of psychological testing [11].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 11,
      "context" : "We have chosen to use a single scale for measuring skills and difficulties, but perhaps this is too restrictive? Consider the following generalization of Bayes’ theorem, described by Zlotnick [13]:",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 12,
      "context" : "This requirement gives rise to the three-parameter logistic model (3PL) of Birnbaum [14].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "Generalizing to the natural case where the upper asymptote may differ from 1 (that is, we can have random failure), we end up with the four-parameter logistic model (4PL) [15].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 16,
      "context" : "It seems, for example, that for many domains, the time to perform a task, as well as the number of errors, follows a power law as a function of the number of trials, the so-called “power law of practice” [18].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 17,
      "context" : "This survey was based on the encyclopedic listing of published roleplaying games (currently 1524 games) maintained by Kim [19].",
      "startOffset" : 122,
      "endOffset" : 126
    } ],
    "year" : 2013,
    "abstractText" : "Throughout the history of games, representing the abilities of the various agents acting on behalf of the players has been a central concern. With increasingly sophisticated games emerging, these simulations have become more realistic, but the underlying mechanisms are still, to a large extent, of an ad hoc nature. This paper proposes using a logistic model from psychometrics as a unified mechanism for task resolution in simulation-oriented games.",
    "creator" : "LaTeX with hyperref package"
  }
}