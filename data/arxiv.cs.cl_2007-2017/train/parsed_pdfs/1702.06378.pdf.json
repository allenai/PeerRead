{
  "name" : "1702.06378.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-task Learning with CTC and Segmental CRF for Speech Recognition",
    "authors" : [ "Liang Lu", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith" ],
    "emails" : [ "llu@ttic.edu,", "lingpenk@cs.cmu.edu,", "cdyer@google.com,", "nasmith@cs.washington.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "State-of-the-art speech recognition accuracy has been significantly improved over the past few years since the application of deep neural networks [1, 2]. Recently, it has been shown that with the application of both neural network acoustic model and language model, the automatic speech recognizer has approached the human-level accuracy on switchboard conversational speech recognition benchmark using around 2000 hours of transcribed data [3]. While the progress is mainly driven by well engineered neural network architectures and a large amount of training data, the hidden Markov model (HMM) that has been the backbone for speech recognition for decades is still playing the central role in the deep learning revolution for speech processing. Though tremendously successful for the problem of speech recognition, the HMM-based pipeline factorizes the whole system into several components, and building these components separately may be computationally less efficient when developing a large scale system using thousands to hundred of thousands of data, e.g. [4].\nAlong with the mainstream study of using the hybrid HMM/NN framework for speech recognition, recently, there has been an increasing interest in end-to-end training approaches for this task. The key idea is to directly map the input acoustic frames to output characters or words without the intermediate alignment to context-dependent phones used by HMMs. In particular, three architectures have been proposed for the goal of end-to-end learning, i.e., connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with\nattention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13]. These end-to-end models significantly simplify the pipeline of speech recognition. They do not require intermediate alignment or segmentation as HMMs, instead, the alignment or segmentation is marginalized out during training for CTC and SCRF or inferred by the attention mechanism. In terms of the recognition accuracy, however, the end-to-end models are usually left behind the HMM-based counterpart. Though CTC has been shown to outperform HMM systems[14], the improvement is based on the use of context dependent phone targets and very large amount of training data. Therefore, it has almost the same system complexity as HMM acoustic models in that case. When the training data is less abundant, it has been shown that the accuracy of CTC systems degrade significantly [15].\nHowever, the end-to-end models have the flexibility to be combined together to mitigate the weakness of each other. For instance, Kim et al. [16] proposed a multitask learning approach to train a joint attention and CTC model using a shared encoder. It is shown that the CTC auxiliary task can help the attention model to overcome the misalignment problem in the initial few epochs, and speed up the convergence of the attention model. Another nice property of the multi-task learning approach is that the joint model can still be trained end-to-end. Inspired by this work, we study the end-to-end training of the joint CTC and SCRF model using an interpolated loss function. The key difference of our study from [16] is that the two loss functions of CTC and attention model are locally normalized for each output token and they are both trained using the cross entropy criterion. However, the SCRF loss function is normalized at the sequence-level, which is similar to the sequence discriminative training objective function for HMMs. From this perspective, the interpolation of CTC and SCRF loss functions is analogous to the sequence discriminative training of HMMs with CE regularization to overcome overfitting, where a sequence-level loss is also interpolated with a frame-level loss, eg. [17]. Similar to the observations in [16], we demonstrate that the joint training approach improves the recognition accuracies of both CTC and SCRF acoustic models. Besides, we also show that CTC can be used to pretrain the neural network feature extractor to speed up the convergence of the joint model. Experiments were performed on the TIMIT database."
    }, {
      "heading" : "2. Segmental Conditional Random Fields",
      "text" : "SCRF is a variant of the linear-chain CRF model where each output token corresponds to a segment of input tokens instead of a single input instance. In the context of speech recognition, given a sequence of input vectors of T frames X = {x1, · · · ,xT } and its corresponding sequence of output labels\nar X\niv :1\n70 2.\n06 37\n8v 1\n[ cs\n.C L\n] 2\n1 Fe\nb 20\n17\ny = {y1, · · · , yJ}, the zero-order linear-chain CRF defines the sequence-level conditional probability as\nP (y |X) = 1 Z(X) T∏ t=1 exp f (yt,xt) , (1)\nwhere Z(X) denotes the normalization term, and T = J . Extension to higher order models is straightforward, but it is usually computationally much more expensive. The model defined as Eq (1) requires the length of X and y to be equal, which makes it infeasible for speech recognition directly because the lengths of the input and output sequences are not equal. For the case that T ≥ J as in speech recognition, SCRF defines the sequence-level conditional probability with the auxiliary segment labels E = {e1, · · · , eJ} as\nP (y,E |X) = 1 Z(X) J∏ j=1 exp f (yj , ej , x̄j) , (2)\nwhere ej = 〈sj , nj〉 is a tuple of the beginning (sj) and the end (nj) time tag for the segment of yj , and nj > sj while nj , sj ∈ [1, T ]; yj ∈ Y and Y denotes the vocabulary set; x̄j is the embedding vector of the segment corresponding to the token yj ; In this case, Z(X) sums over all the possible (y,E) pairs, i.e.,\nZ(X) = ∑ y,E J∏ j=1 exp f (yj , ej , x̄j) . (3)\nSimilar to other CRF-based models, the function f(·) is defined as\nf (yj , ej , x̄t) = w >Φ(yj , ej , x̄j), (4)\nwhere Φ(·) denotes the feature function, and w is the weight vector. Most of conventional approaches for SCRF-based acoustic models use manually defined feature function Φ(·), where the features and segment boundary information are provided by an auxiliary system [18, 19]. In [20, 13], we proposed an end-to-end training approach for SCRFs, where Φ(·) was defined with neural networks, and the segmental level features were leaned by RNNs. The model was referred as segmental RNN (SRNN), and it will used as the implementation of the SCRF acoustic model for multi-task learning in this study."
    }, {
      "heading" : "2.1. Feature Function and Acoustic Embedding",
      "text" : "SRNN uses an RNN to learn segmental level acoustic embeddings. Given the input sequence X = {x1, · · · ,xT }, and we need to compute the embedding vector x̄j in Eq. (4) corresponding to the segment ej = 〈sj , nj〉. Since the segment boundaries are known, it is straightforward to employ an RNN to map the segment into a vector as\nhsj hsj+1 ... hnj\n =  RNN(h0,xsj ) RNN(hsj ,xsj+1)\n... RNN(hnj−1,xnj )  (5) where h0 denotes the initial hidden state, which is initialized to be zero. RNN(·) denotes the nonlinear recurrence operation used in an RNN, which takes the previous hidden state and the feature vector at the current timestep as inputs, and produce an updated hidden state vector. Given the recurrent hidden states,\nthe embedding vector can be simply defined as x̄j = hnj as in our previous work [13]. However, the drawback of this implementation is the large memory cost, as we need to store the array of hidden states {hsj , · · · ,hnj} for all the possible segments 〈sj , nj〉. If we denote H as the dimension of an RNN hidden state, the memory cost will be in the order of O(T 2H), where T is the length of X . It is particularly a problem for the joint model as the CTC model requires additional memory space. In this work, we adopt another approach that requires much less memory. In this approach, we use an RNN to read the whole input sequence as\nh1 h2 ... hT\n =  RNN(h0,x1) RNN(h1,x2)\n... RNN(hT−1,xT )  (6) and we define the embedding vector for segment e = 〈k, t〉 as\nx̄j = [ hsj hnj ] (7)\nIn this case, we only provide the context information for the feature function Φ(·) to extract segmental features. We refer this approach as context aware embedding. Since we only need to read the input sequence once, the memory cost is in the order of O(TH), which is much smaller. The cost, however, is the slightly degradation of the recognition accuracy. This model is illustrated by Figure 1.\nThe feature function Φ(·) also requires a vector representation of the label yj . This embedding vector can be obtained using a linear embedding matrix as the common practice in RNN language models. More specifically, yj is firstly represented as a one-hot vector vj , and it is then mapped into a continuous space by a linear embedding matrixM as\nuj = Mvj (8)\nGiven the acoustic embedding x̄j and label embedding uj , the feature function Φ(·) can be represented as\nΦ(yj , ej , x̄j) = σ(W1uj +W2x̄j + b), (9)\nwhere σ denotes a non-linear activation function such as sigmoid or tanh; W1,W2 and b are weight matrices and a bias vector. corresponds to one layer or multiple layers of linear or non-linear transformation. In fact, it is straightforward to stack multiple nonlinear layers in the feature function."
    }, {
      "heading" : "2.2. Loss Function",
      "text" : "For speech recognition, the segmentation labels E are usually unknown in the training set. In this case, we cannot train the model directly by maximizing the conditional probability as Eq. (2). However, the problem can be addressed by marginalizing out the segmentation variable as\nLscrf = − logP (y |X) = − log ∑ E P (y,E |X)\n= − log ∑ E ∏ j\nexp f (yj , ej , x̄j)︸ ︷︷ ︸ ≡Z(X,y) + logZ(X), (10)\nwhere Z(X,y) denotes the summation over all the possible segmentations when only y is observed. To simplify notations, the objective function Lscrf is define with only one training utterance.\nHowever, the number of possible segmentations is exponential with the length of X , which makes the naive computation of both Z(X,y) and Z(X) impractical. To address this problem, a dynamic programming algorithm can be applied, which can reduce the computational complexity to O(T 2 · |Y|) [21]. The computational cost can be further reduced by limiting the maximum length of all the possible segments. The readers are referred to [13] for further details including the decoding algorithm."
    }, {
      "heading" : "3. Connectionist Temporal Classification",
      "text" : "CTC also directly computes the conditional probability of P (y | X), with the key difference from SCRF in that it normalizes the probabilistic distribution at the per-frame level. To address the problem of length mismatch between the input and output sequences, CTC allows repetitions of output labels and introduces a special blank token (−), which represents the probability of not emitting any valid label at a particular time step. The conditional probability is then obtained by summing over all the probabilities of all the paths that corresponding to y after merging the repeated labels and removing the blank tokens, i.e.,\nP (y |X) = ∑\nπ∈Φ(y)\nP (π |X), (11)\nwhere Φ(y) denotes the set of all the possible paths that correspond to y after repetitions of labels and insertions of the blank token. Now the length of π is the same as X , the probability P (π | X) is then simply approximated by the independence assumption as\nP (π |X) ≈ T∏\nt=1\nP (πt | xt), (12)\nwhere πt ∈ Y ∪ {−}, and P (πt | xt) can be computed using the softmax function. The training criterion for CTC is to maximize the conditional probability of the ground truth labels,\nwhich is equivalent to minimize the negative log likelihood as\nLctc = − logP (y |X), (13)\nwhich can be reformulated as the CE criterion. More details regarding the computation of the loss and the backpropagation algorithm to train CTC models can be found in [22]."
    }, {
      "heading" : "4. Joint Training Loss",
      "text" : "Training the two models jointly is trivial. We can simply interpolate the CTC and SCRF loss functions as\nL = λLctc + (1− λ)Lscrf , (14)\nwhere λ ∈ [0, 1] is the interpolation weight. The two models share the same neural network for feature extraction. In this work, we focus on the RNN with long short-term memory (LSTM) [23] units for feature extraction. Other type of neural architecture, e.g., convolutional neural network (CNN) or combinations of CNN and RNN, may be considered for future work."
    }, {
      "heading" : "5. Experiments",
      "text" : "Our experiments were performed on the TIMIT database, and both the SRNN and CTC models were implemented using the DyNet toolkit [24]. We followed the standard protocol of the TIMIT dataset, and our experiments were based on the Kaldi recipe [25]. We used the core test set as our evaluation set, which has 192 utterances. Our models were trained with 48 phonemes, and their predictions were converted to 39 phonemes before scoring. The dimension of uj was fixed to be 64, and the dimension of w in Eq. (4) is also 64. We set the initial SGD learning rate to be 0.1, and we exponentially decay the learning rate by 0.75 when the validation error stopped decreasing. We also subsampled the acoustic sequence by a factor of 4 using the hierarchical RNN as in [13]. Our models were trained with dropout regularization [26], using an specific implementation for recurrent networks [27]. The dropout rate was 0.2 unless specified otherwise. Our models were randomly initialized with the same random seed."
    }, {
      "heading" : "5.1. Baseline Results",
      "text" : "Table 1 shows the baseline results of SRNN and CTC models using two different kinds of features. The FBANK features are 120-dimensional with delta and delta-delta coefficients, and the fMLLR features are 40-dimensional, which were obtained from a Kaldi baseline system. We used a 3-layer bidirectional LSTMs for feature extraction, and we used the greedy best path decoding algorithm for both models. Our SRNN and CTC achieved comparable phone error rate (PER) for both kinds of\nfeatures. However, for the CTC system, Graves et al. [28] obtained better result using about the same size of neural network (3 hidden layers with 250 hidden units of bidirectional LSTMs) than ours (18.6% vs.19.9%). Apart from the implementation difference of using different code bases, Graves et al. [28] applied the prefix decoding with beam search, which may have lower search error than our best path decoding algorithm."
    }, {
      "heading" : "5.2. Multi-task Learning Results",
      "text" : "Table 2 shows results of multi-task learning for CTC and SRNN using the interpolated loss as Eq. (14). We only show results of using LSTMs with 250 dimensional hidden states. The interpolation weight was set to be 0.5. In our experiments, tuning the interpolation weight did not further improve the recognition accuracy. From Table 2, we can see from the table that multitask learning improve recognition accuracies of both SRNN and CTC acoustic models, which may due to the regularization effect of the joint training loss. The improvement for FBANK feature is relatively much larger than fMLLR features. In particular, with multi-task learning, the recognition accuracy of our CTC system with best path decoding is comparable to the results obtained by Graves et al. [28] with beam search decoding.\nOne of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3 - 4 times faster than the SRNN model which uses the same RNN encoder. The joint model by multi-task learning is slightly more expensive than the standalone SRNN model. To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training\nof the joint model. This is analogous to sequence training of HMM acoustic models, where the network is usually pretrained by the frame-level CE criterion. Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We investigated multi-task learning with CTC and SCRF for speech recognition in this paper. Using an RNN encoder for feature extraction, both CTC and SCRF can be trained end-to-end, and the two model can be trained together by interpolating the two loss functions. From experiments on the TIMIT dataset, the multi-task learning approach improved the recognition accuracies of both CTC and SCRF acoustic models. We also showed that CTC can be used to pretrain the RNN encoder, and speed up the training of the joint model. In future, we will study the multi-task learning approach for larger scale speech recognition tasks, where the CTC pretraining approach may be more helpful to overcome the problem of high computational cost."
    }, {
      "heading" : "7. Acknowledgements",
      "text" : "We thank the NVIDIA Corporation for the donation of a Titan X GPU."
    }, {
      "heading" : "8. References",
      "text" : "[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82–97, 2012.\n[2] F. Seide, G. Li, and D. Yu, “Conversational speech transcription using context-dependent deep neural networks.” in Interspeech, 2011, pp. 437–440.\n[3] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “Achieving human parity in conversational speech recognition,” arXiv preprint arXiv:1610.05256, 2016.\n[4] H. Soltau, H. Liao, and H. Sak, “Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition,” arXiv preprint arXiv:1610.09975, 2016.\n[5] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in Proc. ICML, 2014, pp. 1764– 1772.\n[6] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger et al., “Deep Speech: Scaling up end-to-end speech recognition,” in arXiv preprint arXiv:1412.5567, 2014.\n[7] H. Sak, A. Senior, K. Rao, O. Irsoy, A. Graves, F. Beaufays, and J. Schalkwyk, “Learning acoustic frame labeling for speech recognition with recurrent neural networks,” in Proc. ICASSP. IEEE, 2015, pp. 4280–4284.\n[8] Y. Miao, M. Gowayyed, and F. Metze, “Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,” in Proc. ASRU. IEEE, 2015, pp. 167–174.\n[9] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Advances in Neural Information Processing Systems, 2015, pp. 577– 585.\n[10] L. Lu, X. Zhang, K. Cho, and S. Renals, “A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition,” in Proc. Interspeech, 2015.\n[11] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. ICASSP. IEEE, 2016, pp. 4960– 4964.\n[12] O. Abdel-Hamid, L. Deng, D. Yu, and H. Jiang, “Deep segmental neural networks for speech recognition.” in Proc. INTERSPEECH, 2013, pp. 1849–1853.\n[13] L. Lu, L. Kong, C. Dyer, N. Smith, and S. Renals, “Segmental recurrent neural networks for end-to-end speech recognition,” in Proc. INTERSPEECH, 2016.\n[14] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and accurate recurrent neural network acoustic models for speech recognition,” in Proc. INTERSPEECH, 2015.\n[15] G. Pundak and T. N. Sainath, “Lower frame rate neural network acoustic models,” in Proc. INTERSPEECH, 2016.\n[16] S. Kim, T. Hori, and S. Watanabe, “Joint ctc-attention based endto-end speech recognition using multi-task learning,” in Proc. ICASSP, 2017.\n[17] H. Su, G. Li, D. Yu, and F. Seide, “Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription,” in Proc. ICASSP. IEEE, 2013, pp. 6664–6668.\n[18] G. Zweig, P. Nguyen, D. Van Compernolle, K. Demuynck, L. Atlas, P. Clark, G. Sell, M. Wang, F. Sha, H. Hermansky et al., “Speech recognitionwith segmental conditional random fields: A summary of the jhu clsp 2010 summer workshop,” in Proc. ICASSP. IEEE, 2011, pp. 5044–5047.\n[19] E. Fosler-Lussier, Y. He, P. Jyothi, and R. Prabhavalkar, “Conditional random fields in speech, audio, and language processing,” Proceedings of the IEEE, vol. 101, no. 5, pp. 1054–1075, 2013.\n[20] L. Kong, C. Dyer, and N. A. Smith, “Segmental recurrent neural networks,” in Proc. ICLR, 2016.\n[21] S. Sarawagi and W. W. Cohen, “Semi-markov conditional random fields for information extraction.” in Proc. NIPS, vol. 17, 2004, pp. 1185–1192.\n[22] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML. ACM, 2006, pp. 369–376.\n[23] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[24] G. Neubig, C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anastasopoulos, M. Ballesteros, D. Chiang, D. Clothiaux, T. Cohn et al., “DyNet: The Dynamic Neural Network Toolkit,” arXiv preprint arXiv:1701.03980, 2017.\n[25] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlıcek, Y. Qian, P. Schwarz, J. Silovský, G. Semmer, and K. Veselý, “The Kaldi speech recognition toolkit,” in Proc. ASRU, 2011.\n[26] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n[27] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural network regularization,” arXiv preprint arXiv:1409.2329, 2014.\n[28] A. Graves, A.-R. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. ICASSP. IEEE, 2013, pp. 6645–6649."
    } ],
    "references" : [ {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury" ],
      "venue" : "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82–97, 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Conversational speech transcription using context-dependent deep neural networks.",
      "author" : [ "F. Seide", "G. Li", "D. Yu" ],
      "venue" : "in Interspeech,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Achieving human parity in conversational speech recognition",
      "author" : [ "W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig" ],
      "venue" : "arXiv preprint arXiv:1610.05256, 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition",
      "author" : [ "H. Soltau", "H. Liao", "H. Sak" ],
      "venue" : "arXiv preprint arXiv:1610.09975, 2016.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards end-to-end speech recognition with recurrent neural networks",
      "author" : [ "A. Graves", "N. Jaitly" ],
      "venue" : "Proc. ICML, 2014, pp. 1764– 1772.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep Speech: Scaling up end-to-end speech recognition",
      "author" : [ "A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger" ],
      "venue" : "arXiv preprint arXiv:1412.5567, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning acoustic frame labeling for speech recognition with recurrent neural networks",
      "author" : [ "H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk" ],
      "venue" : "Proc. ICASSP. IEEE, 2015, pp. 4280–4284.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding",
      "author" : [ "Y. Miao", "M. Gowayyed", "F. Metze" ],
      "venue" : "Proc. ASRU. IEEE, 2015, pp. 167–174.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015, pp. 577– 585.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition",
      "author" : [ "L. Lu", "X. Zhang", "K. Cho", "S. Renals" ],
      "venue" : "Proc. Interspeech, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
      "author" : [ "W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals" ],
      "venue" : "Proc. ICASSP. IEEE, 2016, pp. 4960– 4964.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep segmental neural networks for speech recognition.",
      "author" : [ "O. Abdel-Hamid", "L. Deng", "D. Yu", "H. Jiang" ],
      "venue" : "in Proc. INTER- SPEECH,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Segmental recurrent neural networks for end-to-end speech recognition",
      "author" : [ "L. Lu", "L. Kong", "C. Dyer", "N. Smith", "S. Renals" ],
      "venue" : "Proc. INTERSPEECH, 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Fast and accurate recurrent neural network acoustic models for speech recognition",
      "author" : [ "H. Sak", "A. Senior", "K. Rao", "F. Beaufays" ],
      "venue" : "Proc. INTERSPEECH, 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Lower frame rate neural network acoustic models",
      "author" : [ "G. Pundak", "T.N. Sainath" ],
      "venue" : "Proc. INTERSPEECH, 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Joint ctc-attention based endto-end speech recognition using multi-task learning",
      "author" : [ "S. Kim", "T. Hori", "S. Watanabe" ],
      "venue" : "Proc. ICASSP, 2017.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription",
      "author" : [ "H. Su", "G. Li", "D. Yu", "F. Seide" ],
      "venue" : "Proc. ICASSP. IEEE, 2013, pp. 6664–6668.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Speech recognitionwith segmental conditional random fields: A summary of the jhu clsp 2010 summer workshop",
      "author" : [ "G. Zweig", "P. Nguyen", "D. Van Compernolle", "K. Demuynck", "L. Atlas", "P. Clark", "G. Sell", "M. Wang", "F. Sha", "H. Hermansky" ],
      "venue" : "Proc. ICASSP. IEEE, 2011, pp. 5044–5047.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Conditional random fields in speech, audio, and language processing",
      "author" : [ "E. Fosler-Lussier", "Y. He", "P. Jyothi", "R. Prabhavalkar" ],
      "venue" : "Proceedings of the IEEE, vol. 101, no. 5, pp. 1054–1075, 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Segmental recurrent neural networks",
      "author" : [ "L. Kong", "C. Dyer", "N.A. Smith" ],
      "venue" : "Proc. ICLR, 2016.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Semi-markov conditional random fields for information extraction.",
      "author" : [ "S. Sarawagi", "W.W. Cohen" ],
      "venue" : "in Proc. NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2004
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "A. Graves", "S. Fernández", "F. Gomez", "J. Schmidhuber" ],
      "venue" : "Proc. ICML. ACM, 2006, pp. 369–376.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "DyNet: The Dynamic Neural Network Toolkit",
      "author" : [ "G. Neubig", "C. Dyer", "Y. Goldberg", "A. Matthews", "W. Ammar", "A. Anastasopoulos", "M. Ballesteros", "D. Chiang", "D. Clothiaux", "T. Cohn" ],
      "venue" : "arXiv preprint arXiv:1701.03980, 2017.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "The Kaldi speech recognition toolkit",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlıcek", "Y. Qian", "P. Schwarz", "J. Silovský", "G. Semmer", "K. Veselý" ],
      "venue" : "Proc. ASRU, 2011.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1929
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "W. Zaremba", "I. Sutskever", "O. Vinyals" ],
      "venue" : "arXiv preprint arXiv:1409.2329, 2014.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "A. Graves", "A.-R. Mohamed", "G. Hinton" ],
      "venue" : "Proc. ICASSP. IEEE, 2013, pp. 6645–6649.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "State-of-the-art speech recognition accuracy has been significantly improved over the past few years since the application of deep neural networks [1, 2].",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "State-of-the-art speech recognition accuracy has been significantly improved over the past few years since the application of deep neural networks [1, 2].",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "Recently, it has been shown that with the application of both neural network acoustic model and language model, the automatic speech recognizer has approached the human-level accuracy on switchboard conversational speech recognition benchmark using around 2000 hours of transcribed data [3].",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 3,
      "context" : "[4].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 46,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 46,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 46,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 46,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 102,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 102,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 102,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 12,
      "context" : ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 13,
      "context" : "Though CTC has been shown to outperform HMM systems[14], the improvement is based on the use of context dependent phone targets and very large amount of training data.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "When the training data is less abundant, it has been shown that the accuracy of CTC systems degrade significantly [15].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "[16] proposed a multitask learning approach to train a joint attention and CTC model using a shared encoder.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "The key difference of our study from [16] is that the two loss functions of CTC and attention model are locally normalized for each output token and they are both trained using the cross entropy criterion.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "[17].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Similar to the observations in [16], we demonstrate that the joint training approach improves the recognition accuracies of both CTC and SCRF acoustic models.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "Most of conventional approaches for SCRF-based acoustic models use manually defined feature function Φ(·), where the features and segment boundary information are provided by an auxiliary system [18, 19].",
      "startOffset" : 195,
      "endOffset" : 203
    }, {
      "referenceID" : 18,
      "context" : "Most of conventional approaches for SCRF-based acoustic models use manually defined feature function Φ(·), where the features and segment boundary information are provided by an auxiliary system [18, 19].",
      "startOffset" : 195,
      "endOffset" : 203
    }, {
      "referenceID" : 19,
      "context" : "In [20, 13], we proposed an end-to-end training approach for SCRFs, where Φ(·) was defined with neural networks, and the segmental level features were leaned by RNNs.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 12,
      "context" : "In [20, 13], we proposed an end-to-end training approach for SCRFs, where Φ(·) was defined with neural networks, and the segmental level features were leaned by RNNs.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 12,
      "context" : "Given the recurrent hidden states, the embedding vector can be simply defined as x̄j = hnj as in our previous work [13].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "To address this problem, a dynamic programming algorithm can be applied, which can reduce the computational complexity to O(T 2 · |Y|) [21].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 12,
      "context" : "The readers are referred to [13] for further details including the decoding algorithm.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "More details regarding the computation of the loss and the backpropagation algorithm to train CTC models can be found in [22].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "where λ ∈ [0, 1] is the interpolation weight.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 22,
      "context" : "In this work, we focus on the RNN with long short-term memory (LSTM) [23] units for feature extraction.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "Our experiments were performed on the TIMIT database, and both the SRNN and CTC models were implemented using the DyNet toolkit [24].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 24,
      "context" : "We followed the standard protocol of the TIMIT dataset, and our experiments were based on the Kaldi recipe [25].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "We also subsampled the acoustic sequence by a factor of 4 using the hierarchical RNN as in [13].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 25,
      "context" : "Our models were trained with dropout regularization [26], using an specific implementation for recurrent networks [27].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "Our models were trained with dropout regularization [26], using an specific implementation for recurrent networks [27].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "[28] obtained better result using about the same size of neural network (3 hidden layers with 250 hidden units of bidirectional LSTMs) than ours (18.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[28] applied the prefix decoding with beam search, which may have lower search error than our best path decoding algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[28] with beam search decoding.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling objectives used for end-to-end training of speech recognition models. Both models define the transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a “continuation” of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder used for feature extraction for both outputs. We find that this multi-task objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model.",
    "creator" : "LaTeX with hyperref package"
  }
}