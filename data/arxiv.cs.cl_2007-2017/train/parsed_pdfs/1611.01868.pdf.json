{
  "name" : "1611.01868.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Truth Discovery with Memory Network",
    "authors" : [ "Luyang Li", "Bing Qin", "Wenjing Ren", "Ting Liu" ],
    "emails" : [ "tliu}@ir.hit.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the age of information abundance, conflicts commonly exists in multiple-source statements about the same object. For example, different booking sites provide different boarding time about the same flight on the same date. The phenomenon seriously affects people’s life. Truth discovery task is to find the most credible statement to resolve the confliction (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010). The development of the research benefit other natural language processing task like knowledge management (Poston and Speier, 2005), question answering (Banerjee and Han, 2009), information retrieval (Olteanu et al., 2013) and so on.\nPrevious methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010). They consider the mutual effect between the reliability of sources and the credibility of statements; however, they mostly ignore the mutual effect among the credibility of statements. It is necessary to incorporate the effect from other statements of the same object when computing the credibility of the current statement. Another challenge is that there are multiple types of data in the real world, like categorical data and continuous data, which are both helpful in evaluating the reliability of sources. Few approaches jointly use multiple types of data to learn source reliability and predict the truth. CRH method first presente a framework to joint two types of data and is the state-of-the-art method. However it treats two types of data equally during the reliability evaluating which is intuitively not.\nWe propose memory network based models to learn source reliability and predict the credibility of statements. Given an object, the statements from multiple sources with different reliability can be treated as context to each other. We use feedforward memory network (FFMN) and feedback memory network (FBMN) to incorporate the effect of the credibility between statements. The two models both use memory component to store and update the reliability of sources. We propose a revised framework to utilize multiple types data and assign different weights based on the effect of different types data in model learning. The proposed methods have been verified effectiveness through the experiments on public gold standard datasets. FFMN and FBMN outperform the state-of-the-art method.\nar X\niv :1\n61 1.\n01 86\n8v 1\n[ cs\n.C L\n] 7\nN ov\n2 01\n6\nWe make the following contributions:\n• We propose memory network based models to resolve truth discovery problem. We convert the network-structure data into vectorial representation and input multiple-source statements of one object as context to predict the truth. The reliability of sources are stored as memories in FFMN and FBMN to help in computing the credibility of statements.\n• For better learning the reliability of sources, we proposed a framework to jointly utilize categorical data and continuous data. We weightily combine loss functions of two types of data, in which the weights are fine-tuned automatically through model optimization.\n• Through experiments, we verify that memory network based models outperform the state-of-the-art method, and specifically FFMN works better than other neural network models."
    }, {
      "heading" : "2 Methodology",
      "text" : "We first make a formulation of the problem. Then we introduce the framework and central functions."
    }, {
      "heading" : "2.1 Problem Formulation",
      "text" : "In the real world, the statement is a naturally existing information unit which is the description of a thing of an event. We can always draw an object and a property from the statement, which compose an entry (Li et al., 2014b). The observation is the value of the entry. According to the same entry, observations from different sources may contain conflicting values. The goal of the prediction model is to find the correct observation of the entry.\nEXAMPLE 1. “The flight AA-2446-LAX-DFW will take off at 01:19 PM” is a statement. “The flight AA-2446-LAX-DFW” is an object and “takeoff time” is a property. “The takeoff time of flight AA-2446-LAX-DFW” is an entry. The observation of the entry “the takeoff time of flight AA-2446-LAX-DFW” is “01:19 PM”.\nThe object may have more than one property in a statement, and we separate it into multiple entries. In other words, we take an entry as the basic unit in the prediction model, and the credibility of the observations drew from same statement will be evaluated separately.\nEXAMPLE 2. “The flight AA-2446-LAX-DFW will take off at 01:19 PM at gate 42B” has two entries, “the takeoff time of AA-2446-LAX-DFW” and “the gate of AA-2446-LAX-DFW”.\nActually, there are two data types in the observations which are categorical data and continuous data. The categorical data is class-style data. The continuous data is a number, or can be converted into real number. By prediction model, we want to predict the correct category for categorical type data and predict the closest value to the true value for continuous type data.\nEXAMPLE 3. The “gate 42B” is categorical data, and “01:19 PM” can be treated as continuous data by converting into minutes.\nSuppose there are N entries, each of which has K observations offered by K sources {s1, s2, ..., sK}. Given an entry ei, the observations are the value set Vi = {vi1, vi2, ..., viK}."
    }, {
      "heading" : "2.2 Basic Framework",
      "text" : "In the direction of truth discovery, the lack of gold standard data is a challenge. CRH method (Li et al., 2014b) uses an unsupervised framework to resolve the problem. Li et al. think reliable sources offer trustworthy observations which must be close to the truth. We cautiously think more trustworthy observations should be closer to the truth. Given an entry ei, D is the function to compute the distance between the truth ti and the observation value vik. Truth finding task is treated as an optimization problem. The optimization objective is to minimize the following loss function. The rik stands for the credibility of the observation provided by source Sk. We compute the credibility of the observation by using memory network based model, which is introduced in the Section 3.\nfloss = N∑ i=1 K∑ k=1 rik ∗D(ti, vik) (1)\nThe distance functions of categorical type data and continuous data are different. When entry ei belongs to the categorical data Ucate, the distance function dcate is as following.\ndcate(ti, vik) = { 1, vik 6= ti 0, vik = ti\n(2)\nIf the observations of entry ei belong to continuous data Ucon, the distance function dcon is as following. The denominator of the function is the mean square error of value set Vi = {vi1, vi2, ..., vik, ...viK} according to the entry ei. ṽi stands for the mean value of the value set Vi.\ndcon(ti, vik) = |ti − vik|√\n(vi1 − ṽi)2 + (vi2 − ṽi)2 + ...+ (viK − ṽi)2 (3)\nThe truth ti which can minimize the overall weighted absolute distance is the weighted median (Li et al., 2014b). Given the observation set {vi1, vi2, ..., vik, ...viK} with the credibility set {ri1, ri2, ..., rik, ...riK}, the weighted median of the set is vim which satisfies the following condition.\n∑ k:vik<vim rik < 1 2 K∑ k=1 rik & ∑ k:vik>vim rik ≤ 1 2 K∑ k=1 rik (4)\nThe loss function in our model is as following, which is the sum of the loss functions of categorical data and continuous data respectively. Specifically, the penalty values α and β are learnt automatically through the model learning.\nfloss = α ∑\ni∈Ucate K∑ k=1 rik ∗ dcate(ti, vik) + β ∑ j∈Ucon K∑ k=1 rjk ∗ dcon(tj , vjk) (5)\nInitialization. We use voting approach to compute the initial value of categorical data,and use averaging approach for continuous data. Although, the results would be not effected on condition that the optimization is convex (Li et al., 2014b)."
    }, {
      "heading" : "3 Memory Network based Model for Truth Discovery",
      "text" : "We would like to briefly introduce the memory network. Memory network is a framework with a longterm memory as inference component (Weston et al., 2014; Sukhbaatar et al., 2015; Tang et al., 2016). It consists of a memorym with four components I , G, O andR. The memorym can be read and written to store the long-term information which is useful in prediction. I component learns the representation of inputs. G component generates new memories based on a new input. O component produces an output based on the new input and current memories. R component converts the output into the response format.\nIn our problem, source reliability is long-term information to be used through predicting truth. Source reliability should be updated during learning a model by inputing each sample and be combined with input to generate a new output. We use memory M = {m1,m2, ...,mK} to store the reliability of K sources and updating the memories based on the input observations and current derivative of back propagation. The inputs are the vectorized observations and response of the model are the credibility of observations. We learn the representation of the input data with memories by two types of models, feedforward neural network and feedback neural network. We call them Feedforward Memory Network (FFMN) and Feedback Memory Network (FBMN).\nPre-processing. The truth discovery data has network structure, and multiple observations of the same object have relationships. We think embedding algorithms can learn the latent law of the structure and interaction among data. We learn word embedding of data to vectorized every information from dataset. There are three kinds of data which are objects, properties and values. We use Word2Vec (Mikolov et\nal., 2013) to obtain the vector of each data. Specifically, we take a source and its observation of an entry (i.e., object, property and value) as a term of context in learning word embedding. We think the relevance between data can be learnt based on the context-based word embedding learning algorithm."
    }, {
      "heading" : "3.1 Feedforward Memory Network",
      "text" : "Feedforward neural network is like a directed acyclic graph which has no feedback through the network. Feedforward memory network is feedforward neural network with memory network mechanism. The architecture of FFMN is shown in Figure 1. The input {x1, x2, ..., xj , ..., xK} of each iteration in I component is the vectors of observations according to a same entry. The observation vector is the concatenation of the vectors of object data, property data and value data. M storesK memory vectors which stand for the reliability of sources. The memories and inputs are computed through element-wise multiplication. And σ : R → [0, 1] is softmax function. The response of the FFMN is {r1, r2, ..., rj , ..., rK} which is computed like in the following formula.\nrj = σ(mj xj) (6)"
    }, {
      "heading" : "3.2 Feedback Memory Network",
      "text" : "Feedback neural network is a kind of network whose neurons feedback output to other neurons as input after a time step. Long short-term memory (LSTM) is a typical feedback neural network. LSTM uses hidden layerH to store “short term memory” and uses cell unit C to store “long term memory”. It adopts input gate xt , forget gate ft to control the updating of “long term memory” ct, and adopts output gate ot to compute hidden vector at current timestep.\nOur feedback memory network (FBMN) is shown in Figure 2. We add a memory component M into LSTM to store the reliability of sources. The memory component M plays a different role and also has a different operation by comparing with the cell C in LSTM. The input of the model is a series of values from different sources according to a same entry. The time step is the order of input values. The response of the model is the result of softmax operation of the hidden vector of last time step. The updating of memories in M is based on the response through the back-propagation of derivative.\nThe hidden vector ht is computed based on memory mt, hidden vector on last time step ht−1 and the current input xt. Memory mt store reliability of the tth source. The formula are shown in the following, where σ is the sigmoid function and is the element-wise multiplication. The detailed operating mechanism is shown on the right part of the Figure2.\nit = σ(Wixxt +Wihht−1 +Wicct−1 +Wimmt + bi) (7)\nft = σ(Wfxxt +Wfhht−1 +Wfcct−1 +Wfmmt + bf ) (8)\nct = ft ct−1 + it tanh(Wcxxt +Wchht−1 +Wcmmt + bc) (9)\not = σ(Woxxt +Wohht−1 +Wocct−1 +Wommt + bo) (10)\nht = ot tanh(ct) (11)"
    }, {
      "heading" : "4 Experiment and Analysis",
      "text" : "We present the experiment results to show the effectiveness of our memory network based models. We first introduce the dataset and evaluation criterions, and present the results and make an analysis. We also discuss the influence of parameters on the results."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We use two public datasets (Li et al., 2012) 1 to demonstrate the effectiveness of the proposed method. The statistics of datasets are listed in Table 32. In view of existence of redundancy in datasets, which specifically causes multiple different values according to a same entry in the ground truth set, we eliminate the redundancy through data pre-processing. The entries contained in ground truths are part of whole entries in the dataset. The ground truths are used only in the evaluation.\nStock Dataset. Xian Li et al. (Li et al., 2012) collected stock data from 55 sources on every work data in July 2011. There are 1000 stock symbols with 16 properties. The ground truths contain NASDAQ100 stocks and other 100 randomly selected stocks. These stock data are acquired by taking the majority of the values provided by five sources, nasdaq.com, yahoo finance, google finance, bloomberg and MSN finance. In order to verify the capacity of utilizing both categorical data and continuous data, Qi Li et al. (Li et al., 2014b) treat the properties Volume, Shares Outstanding and Market Cap are considered as continuous type, and treat other properties as categorical type (Li et al., 2014b). We keep pace with them in the experiments.\nFlight Dataset. The flight dataset is collected from 38 sources over one month period (December 2011). It consists of 1200 flights with 6 properties. We treat departure gate and arrival gate as categorical type, and treat other properties as continuous type. We treat time data as real number by converting into minutes. The ground truths are about 100 randomly selected flights.\nStock Data Flight Data Observations 12,056,684 2,703,448 Entries 335,975 204,414 Ground Truths 29,207 16,276"
    }, {
      "heading" : "4.2 Evaluation Criteria",
      "text" : "We use two evaluation criteria, which are Error Rate and Mean Normalized Absolute Distance (MNAD) (Li et al., 2014b), to evaluate our methods. They are used to evaluate two types of data separately. The lower these two criteria are, the better the results are.\nError Rate: Error rate is to compute the percentage of the wrong prediction of categorical data. If the output of the method is different from the ground truth, it is a wrong prediction. This evaluation criterion reflects the capability of methods to predict on categorical data.\nMNAD: MNAD is to evaluate the closeness between the prediction output and the ground truths on the continuous data. Because the values of different entries share different scales. The absolute distance between the prediction output oi and ground truth ti is normalized by the mean square error of the entry by given an entry ei,.\nMNAD = 1\nM M∑ i=1 |ti − oi|√ (vi1 − ṽi)2 + (vi2 − ṽi)2 + ...+ (viK − ṽi)2\n(12)"
    }, {
      "heading" : "4.3 Baseline Methods",
      "text" : "We use the following baseline methods to make a comparison with our methods. Mean: Mean method averages all values of the same entry as the prediction which is used on continuous data. Median:: Median method finds the median value of all values of the same entry as the prediction which is used on continuous data. GTM (Zhao and Han, 2012): Gaussian Truth Model (GTM) is a Bayesian probabilistic method which only work on continuous data. Note that this method only uses continuous data to learn the model and make a prediction, and insufficient data may lead to a lower performance in GTM compared with others.\nVoting: The method takes the value with the highest occurrence as predicted value. Investment (Pasternack and Roth, 2011): In this approach, a source invests” its reliability uniformly on the observations it provides, and collects credits back from the credibility of those observations. PooledInvestment (Pasternack and Roth, 2011): PooledInvestment linearly scale the credibility of observations which is different from Investment method. 2-Estimates (Galland et al., 2010): This method is proposed based on the assumption that there is one and only one true value for each entry”. If a source provides an observation for an entry, 2-Estimates assumes that this source votes against different observations on this entry.\n3-Estimates (Galland et al., 2010): 3-Estimates improves 2-Estimates by considering the difficulty of getting the truth for each entry, the estimation of which will affect the sources weight.\nTruthFinder (Yin et al., 2008): TruthFinder adopts Bayesian analysis, in which for each observation, its confidence is calculated as the product of its providers reliability degrees. Similarity function is used to adjust the vote of a value by considering the influences between facts.\nAccuSim (Dong et al., 2009): AccuSim also applies Bayesian analysis and it also adopts the usage of the similarity function. Meanwhile, it considers complement vote which is adopted by 2-Estimates and 3-Estimates.\nCRH (Li et al., 2014b): The method is the state-of-the-art method on the current datasets which take both categorical type and continuous type data into consideration through iteratively computing reliability of sources.\nBi-LSTM: Bidirectional LSTM is a variant of LSTM which is verified effective or even the state-ofthe-art method in many NLP tasks."
    }, {
      "heading" : "4.4 Truth Discovery Experiment",
      "text" : "We compare our FFMN and FBMN models with baseline methods. The experiment results in Table 2 verify the effectiveness of our models.\nFrom the results, we can see our memory network based models outperform the previous methods. On stock data, FFMN has the best prediction capacity on categorical data which has a lowest error rate,\nand LSTM based models which are Bi-LSTM and FBMN perform better on continuous data which have lowest MNAD. On flight data, FFMN performs best both on categorical and continuous data. Among previous methods, CRH has the similar framework with our methods. The results between the two kinds of methods verify that the effectiveness of using neural network based model to resolve the truth discovery problem.\nWe make a comparison among neural network based methods and find memory network based models gain best results. FBMN has good MNAD results on continuous data. Specifically, FFMN makes an impression with a simple architecture, however, outperform other more complex models."
    }, {
      "heading" : "4.5 Parameter Setting",
      "text" : "We have done a series of experiments to analyze the effect of parameters to the results of truth discovery. We make an analysis of the following parameters, embeddings learnt by different algorithms, different embedding length, different learning rates.\nEmbedding: We adopt LINE (Tang et al., 2015) word embedding learning algorithm to compare with Word2Vec (Mikolov et al., 2013) in truth discovery experiment. LINE can learn embedding from network-structure information which preserves both first-order proximity and second-order proximity. We use embedding in FFMN model and run experiments on flight data. The results show that embedding learnt by Word2Vec is more suitable for our problem. We also can see that LINE second order gains better than LINE first order.\nEmbedding Length: We try multiple numbers of dimensionality from 50 to 300. We find the dimensionality has almost no effect on the results. Thus, the embedding length is set to 50 in final experiment.\nLearning Rate: We set the learning rate as 0.03, which is also verified with little impact on the results by trying from 0.0003 to 0.3."
    }, {
      "heading" : "5 Related Work",
      "text" : "The truth discovery problem is to find the most credible statement. Most methods take a voting and similarity mechanism. More popular statements are more likely to be true. Similar statements have similar credibility. The statements from sources with similar reliability have similar credibility. The existing methods also take some important features into account, such as the reliability of sources, number of sources which post the same statements, difficulty of the statement, uncertainty in the information extraction (Pasternack and Roth, 2011), similarity between statements and copying relationship, etc. Li et al. categorize the previous methods (Li et al., 2012). The basic method is using voting strategy. The web-link based methods share the similar strategy that the credibility of statements are computed based on the links between statements and sources. Corresponding methods are HUB (Kleinberg, 1999), AvgLog (Pasternack and Dan, 2010), Investment (Pasternack and Dan, 2010) and PoolInvestment (Pasternack and Dan, 2010). Specifically, Investment method takes the idea that the source “invests” its reliability uniformly on its statements. The credibilities of statements are computed based on assessed reliability of sources. PoolInvestment method adds linear scaling on each entry through computing the credibility of statements. The IR based methods are inspired by the similarity computing approach in information retrieval. Given a value of the object, the credibility is computed both based on the supporting sources and against sources. Corresponding methods are Cosine (Galland et al., 2010), 2-estimates (Galland et al., 2010) and 3-estimates (Galland et al., 2010). Specifically, 3-estimates method take the likehood of correctness of voting on the value into account.\nThe Bayesian based methods apply Bayesian analysis, which are to predict the probability of a statement to be a truth based on observed information. The corresponding methods include TruthFinder (Yin et al., 2008), AccuPr (Dong et al., 2009), AccuSim (Dong et al., 2009), AccuFormat (Dong et al., 2009), LCA (Pasternack and Dan, 2013) and CRH (Li et al., 2014b). Specially, TruthFinder method considers similarity between statements, and AccuPr method consider that different statements on the same entry should be disjoint. LCA method is a probabilistic model which analyze latent credibility factors by using them as parameters to find the maximum a posteriori (MAP). CRH method (Li et al., 2014b) use heterogeneous types of data which consists categorical data and continuous data to estimate the reliability of sources and predict truth. The copying affected method discounts votes from copied observations through computing credibility, such as AccuCopy (Dong et al., 2009)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Truth discovery is a fundamental research problem in natural language processing and data mining. Previous approaches mostly consider the mutual effect between the reliability of sources and the credibility of observations. The mutual effect among the credibility of observations to the same entry is also important, however, has not yet been appreciated. We use memory network based models to learn the latent relationship among observations of the same entry. Specifically, the proposed model adopts memory mechanism to learn the reliability of sources and incorporate it into the representing of observation credibility. We utilize multiple types of data and take their different contributions in the truth discovery by assigning weights automatically in the loss function. The experiments show that our methods much outperform the state-of-the-art method on two public gold standard datasets. The feedforward memory network has the best performance."
    } ],
    "references" : [ {
      "title" : "Answer credibility: A language modeling approach to answer validation",
      "author" : [ "Banerjee", "Han2009] Protima Banerjee", "Hyoil Han" ],
      "venue" : "In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, May 31 - June",
      "citeRegEx" : "Banerjee et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2009
    }, {
      "title" : "Integrating conflicting data: The role of source dependence",
      "author" : [ "Dong et al.2009] Xin Luna Dong", "Laure Berti-Equille", "Divesh Srivastava" ],
      "venue" : "Proceedings of the Vldb Endowment,",
      "citeRegEx" : "Dong et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2009
    }, {
      "title" : "Less is more: selecting sources wisely for integration",
      "author" : [ "Dong et al.2012] Xin Luna Dong", "Barna Saha", "Divesh Srivastava" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "Dong et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2012
    }, {
      "title" : "Corroborating information from disagreeing views",
      "author" : [ "Serge Abiteboul", "Am Marian", "Pierre Senellart" ],
      "venue" : "In ACM International Conference on Web Search & Data Mining,",
      "citeRegEx" : "Galland et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Galland et al\\.",
      "year" : 2010
    }, {
      "title" : "Authoritative sources in a hyperlinked environment",
      "author" : [ "Jon M. Kleinberg" ],
      "venue" : "Journal of the Acm,",
      "citeRegEx" : "Kleinberg.,? \\Q1999\\E",
      "shortCiteRegEx" : "Kleinberg.",
      "year" : 1999
    }, {
      "title" : "Truth finding on the deep web: is the problem solved",
      "author" : [ "Li et al.2012] Xian Li", "Xin Luna Dong", "Kenneth Lyons", "Weiyi Meng", "Divesh Srivastava" ],
      "venue" : "Proceedings of the Vldb Endowment,",
      "citeRegEx" : "Li et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2012
    }, {
      "title" : "A confidence-aware approach for truth discovery on long-tail data",
      "author" : [ "Li et al.2014a] Qi Li", "Yaliang Li", "Jing Gao", "Lu Su", "Bo Zhao", "Murat Demirbas", "Wei Fan", "Jiawei Han" ],
      "venue" : "Proceedings of the Vldb Endowment,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Resolving conflicts in heterogeneous data by truth discovery and source reliability estimation",
      "author" : [ "Li et al.2014b] Qi Li", "Yaliang Li", "Jing Gao", "Bo Zhao", "Wei Fan", "Jiawei Han" ],
      "venue" : "In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "A survey on truth discovery",
      "author" : [ "Li et al.2016] Yaliang Li", "Jing Gao", "Chuishi Meng", "Qi Li", "Lu Su", "Bo Zhao", "Wei Fan", "Jiawei Han" ],
      "venue" : "Acm Sigkdd Explorations Newsletter,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "Computer Science",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Web credibility: Features exploration and credibility prediction",
      "author" : [ "Stanislav Peshterliev", "Xin Liu", "Karl Aberer" ],
      "venue" : "In European Conference on Advances in Information Retrieval,",
      "citeRegEx" : "Olteanu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Olteanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Knowing what to believe(when you already know something)",
      "author" : [ "Pasternack", "Dan2010] Jeff Pasternack", "Roth Dan" ],
      "venue" : "In COLING 2010, International Conference on Computational Linguistics, Proceedings of the Conference,",
      "citeRegEx" : "Pasternack et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pasternack et al\\.",
      "year" : 2010
    }, {
      "title" : "Latent credibility analysis",
      "author" : [ "Pasternack", "Dan2013] Jeff Pasternack", "Roth Dan" ],
      "venue" : "In International Conference on World Wide Web,",
      "citeRegEx" : "Pasternack et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pasternack et al\\.",
      "year" : 2013
    }, {
      "title" : "Making better informed trust decisions with generalized fact-finding",
      "author" : [ "Pasternack", "Roth2011] Jeff Pasternack", "Dan Roth" ],
      "venue" : "In International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Pasternack et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pasternack et al\\.",
      "year" : 2011
    }, {
      "title" : "Effective use of knowledge management systems: a process model of content ratings and credibility indicators",
      "author" : [ "Poston", "Speier2005] Robin S. Poston", "Cheri Speier" ],
      "venue" : null,
      "citeRegEx" : "Poston et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Poston et al\\.",
      "year" : 2005
    }, {
      "title" : "Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks",
      "author" : [ "Snow et al.2008] Rion Snow", "Brendan O’Connor", "Daniel Jurafsky", "Andrew Y. Ng" ],
      "venue" : null,
      "citeRegEx" : "Snow et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2008
    }, {
      "title" : "Line: Large-scale information network embedding",
      "author" : [ "Tang et al.2015] Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei" ],
      "venue" : "In International Conference on World Wide Web",
      "citeRegEx" : "Tang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Aspect level sentiment classification with deep memory",
      "author" : [ "Tang et al.2016] Duyu Tang", "Bing Qin", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "Tang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "On truth discovery in social sensing: A maximum likelihood estimation approach",
      "author" : [ "Wang et al.2012] Dong Wang", "Lance Kaplan", "Hieu Le", "Tarek Abdelzaher" ],
      "venue" : "In ACM/IEEE International Conference on Information Processing in Sensor Networks,",
      "citeRegEx" : "Wang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2012
    }, {
      "title" : "Truth discovery with multiple conflicting information providers on the web",
      "author" : [ "Yin et al.2008] Xiaoxin Yin", "Jiawei Han", "Philip S. Yu" ],
      "venue" : "IEEE Transactions on Knowledge & Data Engineering,",
      "citeRegEx" : "Yin et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2008
    }, {
      "title" : "A probabilistic model for estimating real-valued truth from conflicting sources",
      "author" : [ "Zhao", "Han2012] Bo Zhao", "Jiawei Han" ],
      "venue" : "Proc.of Intl.workshop on Quality in Databases",
      "citeRegEx" : "Zhao et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2012
    }, {
      "title" : "A bayesian approach to discovering truth from conflicting sources for data integration",
      "author" : [ "Zhao et al.2012] Bo Zhao", "Benjamin I.P. Rubinstein", "Jim Gemmell", "Jiawei Han" ],
      "venue" : "Proceedings of the Vldb Endowment,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Truth discovery task is to find the most credible statement to resolve the confliction (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010).",
      "startOffset" : 87,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "Truth discovery task is to find the most credible statement to resolve the confliction (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010).",
      "startOffset" : 87,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "Truth discovery task is to find the most credible statement to resolve the confliction (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010).",
      "startOffset" : 87,
      "endOffset" : 146
    }, {
      "referenceID" : 10,
      "context" : "The development of the research benefit other natural language processing task like knowledge management (Poston and Speier, 2005), question answering (Banerjee and Han, 2009), information retrieval (Olteanu et al., 2013) and so on.",
      "startOffset" : 199,
      "endOffset" : 221
    }, {
      "referenceID" : 19,
      "context" : "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).",
      "startOffset" : 75,
      "endOffset" : 271
    }, {
      "referenceID" : 15,
      "context" : "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).",
      "startOffset" : 75,
      "endOffset" : 271
    }, {
      "referenceID" : 1,
      "context" : "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).",
      "startOffset" : 75,
      "endOffset" : 271
    }, {
      "referenceID" : 3,
      "context" : "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).",
      "startOffset" : 75,
      "endOffset" : 271
    }, {
      "referenceID" : 2,
      "context" : "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).",
      "startOffset" : 75,
      "endOffset" : 271
    }, {
      "referenceID" : 18,
      "context" : "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).",
      "startOffset" : 75,
      "endOffset" : 271
    }, {
      "referenceID" : 20,
      "context" : "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).",
      "startOffset" : 75,
      "endOffset" : 271
    }, {
      "referenceID" : 8,
      "context" : "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).",
      "startOffset" : 75,
      "endOffset" : 271
    }, {
      "referenceID" : 17,
      "context" : "Memory network is a framework with a longterm memory as inference component (Weston et al., 2014; Sukhbaatar et al., 2015; Tang et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "1 Datasets We use two public datasets (Li et al., 2012) 1 to demonstrate the effectiveness of the proposed method.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "(Li et al., 2012) collected stock data from 55 sources on every work data in July 2011.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "2-Estimates (Galland et al., 2010): This method is proposed based on the assumption that there is one and only one true value for each entry”.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "3-Estimates (Galland et al., 2010): 3-Estimates improves 2-Estimates by considering the difficulty of getting the truth for each entry, the estimation of which will affect the sources weight.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "TruthFinder (Yin et al., 2008): TruthFinder adopts Bayesian analysis, in which for each observation, its confidence is calculated as the product of its providers reliability degrees.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "AccuSim (Dong et al., 2009): AccuSim also applies Bayesian analysis and it also adopts the usage of the similarity function.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "Embedding: We adopt LINE (Tang et al., 2015) word embedding learning algorithm to compare with Word2Vec (Mikolov et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : ", 2015) word embedding learning algorithm to compare with Word2Vec (Mikolov et al., 2013) in truth discovery experiment.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "categorize the previous methods (Li et al., 2012).",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "Corresponding methods are HUB (Kleinberg, 1999), AvgLog (Pasternack and Dan, 2010), Investment (Pasternack and Dan, 2010) and PoolInvestment (Pasternack and Dan, 2010).",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "Corresponding methods are Cosine (Galland et al., 2010), 2-estimates (Galland et al.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : ", 2010), 2-estimates (Galland et al., 2010) and 3-estimates (Galland et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : ", 2010) and 3-estimates (Galland et al., 2010).",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "The corresponding methods include TruthFinder (Yin et al., 2008), AccuPr (Dong et al.",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : ", 2008), AccuPr (Dong et al., 2009), AccuSim (Dong et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : ", 2009), AccuSim (Dong et al., 2009), AccuFormat (Dong et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : ", 2009), AccuFormat (Dong et al., 2009), LCA (Pasternack and Dan, 2013) and CRH (Li et al.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "The copying affected method discounts votes from copied observations through computing credibility, such as AccuCopy (Dong et al., 2009).",
      "startOffset" : 117,
      "endOffset" : 136
    } ],
    "year" : 2016,
    "abstractText" : "Truth discovery is to resolve conflicts and find the truth from multiple-source statements. Conventional methods mostly research based on the mutual effect between the reliability of sources and the credibility of statements, however, pay no attention to the mutual effect among the credibility of statements about the same object. We propose memory network based models to incorporate these two ideas to do the truth discovery. We use feedforward memory network and feedback memory network to learn the representation of the credibility of statements which are about the same object. Specially, we adopt memory mechanism to learn source reliability and use it through truth prediction. During learning models, we use multiple types of data (categorical data and continuous data) by assigning different weights automatically in the loss function based on their own effect on truth discovery prediction. The experiment results show that the memory network based models much outperform the state-of-the-art method and other baseline methods.",
    "creator" : "LaTeX with hyperref package"
  }
}