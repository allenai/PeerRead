{
  "name" : "1606.02785.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Network-Based Abstract Generation for Opinions and Arguments",
    "authors" : [ "Lu Wang", "Wang Ling" ],
    "emails" : [ "luwang@ccs.neu.edu", "lingwang@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Collecting opinions from others is an integral part of our daily activities. Discovering what other people think can help us navigate through different aspects of life, ranging from making decisions on regular tasks to judging fundamental societal issues and forming personal ideology. To efficiently absorb the massive amount of opinionated information, there is a pressing need for automated systems that can generate concise and fluent opinion summary about an entity or a topic. In spite of substantial researches in opinion summarization, the most prominent approaches mainly rely on extractive summarization methods, where phrases or sentences from the original documents are selected for inclusion in the summary (Hu and Liu, 2004; Lerman et al., 2009). One of the problems that extractive methods suffer from\nis that they unavoidably include secondary or redundant information. On the contrary, abstractive summarization methods, which are able to generate text beyond the original input, can produce more coherent and concise summaries.\nIn this paper, we present an attention-based neural network model for generating abstractive summaries of opinionated text. Our system takes as input a set of text units containing opinions about the same topic (e.g. reviews for a movie, or arguments\nar X\niv :1\n60 6.\n02 78\n5v 1\n[ cs\n.C L\n] 9\nJ un\n2 01\nfor a controversial social issue), and then outputs a one-sentence abstractive summary that describes the opinion consensus of the input.\nSpecifically, we investigate our abstract generation model on two types of opinionated text: movie reviews and arguments on controversial topics. Examples are displayed in Figure 1. The first example contains a set of professional reviews (or critics) about movie “The Martian” and an opinion consensus written by an editor. It would be more useful to automatically generate fluent opinion consensus rather than simply extracting features (e.g. plot, music, etc) and opinion phrases as done in previous summarization work (Zhuang et al., 2006; Li et al., 2010). The second example lists a set of arguments on “death penalty”, where each argument supports the central claim “death penalty deters crime”. Arguments, as a special type of opinionated text, contain reasons to persuade or inform people on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation.\nExisting abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010). Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions.\nTo address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and fluent opinion summaries. Our method is based on the recently proposed framework of neural encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a), which translates a sentence in a source language into a target language. Different from previous work, our summarization system is designed to support multiple input text units. An attention-based model (Bahdanau et al., 2014) is deployed to al-\nlow the encoder to automatically search for salient information within context. Furthermore, we propose an importance-based sampling method so that the encoder can integrate information from an important subset of input text. The importance score of a text unit is estimated from a novel regression model with pairwise preference-based regularizer. With importance-based sampling, our model can be trained within manageable time, and is still able to learn from diversified input.\nWe demonstrate the effectiveness of our model on two newly collected datasets for movie reviews and arguments. Automatic evaluation by BLEU (Papineni et al., 2002) indicates that our system outperforms the state-of-the-art extract-based and abstractbased methods on both tasks. For example, we achieved a BLEU score of 24.88 on Rotten Tomatoes movie reviews, compared to 19.72 by an abstractive opinion summarization system from Ganesan et al. (2010). ROUGE evaluation (Lin and Hovy, 2003) also indicates that our system summaries have reasonable information coverage. Human judges further rated our summaries to be more informative and grammatical than compared systems."
    }, {
      "heading" : "2 Data Collection",
      "text" : "We collected two datasets for movie reviews and arguments on controversial topics with goldstandard abstracts.1 Rotten Tomatoes (www. rottentomatoes.com) is a movie review website that aggregates both professional critics and user-generated reviews (henceforth RottenTomatoes). For each movie, a one-sentence critic consensus is constructed by an editor to summarize the opinions in professional critics. We crawled 246,164 critics and their opinion consensus for 3,731 movies (i.e. around 66 reviews per movie on average). We select 2,458 movies for training, 536 movies for validation and 737 movies for testing. The opinion consensus is treated as the gold-standard summary.\nWe also collect an argumentation dataset from idebate.org (henceforth Idebate), which is a Wikipedia-style website for gathering pro and con arguments on controversial issues. The arguments under each debate (or topic) are organized into dif-\n1The datasets can be downloaded from http://www. ccs.neu.edu/home/luwang/.\nferent “for” and “against” points. Each point contains a one-sentence central claim constructed by the editors to summarize the corresponding arguments, and is treated as the gold-standard. For instance, on a debate about “death penalty”, one claim is “the death penalty deters crime” with an argument “enacting the death penalty may save lives by reducing the rate of violent crime” (Figure 1). We crawled 676 debates with 2,259 claims. We treat each sentence as an argument, which results in 17,359 arguments in total. 450 debates are used for training, 67 debates for validation, and 150 debates for testing."
    }, {
      "heading" : "3 The Neural Network-Based Abstract Generation Model",
      "text" : "In this section, we first define our problem in Section 3.1, followed by model description. In general, we utilize a Long Short-Term Memory network for generating abstracts (Section 3.2) from a latent representation computed by an attention-based encoder (Section 3.3). The encoder is designed to search for relevant information from input to better inform the abstract generation process. We also discuss an importance-based sampling method to allow encoder to integrate information from an important subset of input (Sections 3.4 and 3.5). Postprocessing (Section 3.6) is conducted to re-rank the generations and pick the best one as the final summary."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "In summarization, the goal is to generate a summary y, composed by the sequence of words y1, ..., |y|. Unlike previous neural encoder-decoder approaches which decode from only one input, our input consists of an arbitrary number of reviews or arguments (henceforth text units wherever there is no ambiguity), denoted as x = {x1, ..., xM}. Each text unit xk is composed by a sequence of words xk1, ..., x k |xk|. Each word takes the form of a representation vector, which is initialized randomly or by pre-trained embeddings (Mikolov et al., 2013), and updated during training. The summarization task is defined as finding ŷ, which is the most likely sequence of words ŷ1, ..., ŷN such that:\nŷ = argmaxy logP (y|x) (1)\nwhere logP (y|x) denotes the conditional loglikelihood of the output sequence y, given the input text units x. In the next sections, we describe the attention model used to model logP (y|x)."
    }, {
      "heading" : "3.2 Decoder",
      "text" : "Similar as previous work (Sutskever et al., 2014b; Bahdanau et al., 2014), we decompose logP (y|x) into a sequence of word-level predictions:\nlogP (y|x) = ∑\nj=1,...,|y|\nlogP (yj |y1, ..., yj−1, x) (2)\nwhere each word yj is predicted conditional on the previously generated y1, ..., yj−1 and input x. The probability is estimated by standard word softmax:\np(yj |y1, ..., yj−1, x) = softmax(hj) (3)\nhj is the Recurrent Neural Networks (RNNs) state variable at timestamp j, which is modeled as:\nhj = g(yj−1,hj−1, s) (4)\nHere g is a recurrent update function for generating the new state hj from the representation of previously generated word yj−1 (obtained from a word lookup table), the previous state hj−1, and the input text representation s (see Section 3.3).\nIn this work, we implement g using a Long ShortTerm Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), which has been shown to be effective at capturing long range dependencies. Here we summarize the update rules for LSTM cells, and refer readers to the original work (Hochreiter and Schmidhuber, 1997) for more details. Given an arbitrary input vector uj at timestamp j − 1 and the previous state hj−1, a typical LSTM defines the following update rules:\nij = σ(Wiuuj +Wihhj−1 +Wiccj−1 + bi)\nfj = σ(Wfuuj +Wfhhj−1 +Wfccj−1 + bf )\ncj = fj cj−1 + ij tanh(Wcuuj +Wchhj−1 + bc) oj = σ(Wouuj +Wohhj−1 +Woccj + bo)\nhj = oj tanh(cj) (5)\nσ is component-wise logistic sigmoid function, and denotes Hadamard product. Projection matrices\nW∗∗ and biases b∗ are parameters to be learned during training.\nLong range dependencies are captured by the cell memory cj , which is updated linearly to avoid the vanishing gradient problem. It is accomplished by predicting two vectors ij and fj , which determine what to keep and what to forget from the current timestamp. Vector oj then decides on what information from the new cell memory cj can be passed to the new state hj . Finally, the model concatenates the representation of previous output word yj−1 and the input representation s (see Section 3.3) as uj , which serves as the input at each timestamp."
    }, {
      "heading" : "3.3 Encoder",
      "text" : "The representation of input text units s is computed using an attention model (Bahdanau et al., 2014). Given a single text unit x1, ..., x|x| and the previous state hj, the model generates s as a weighted sum:∑\ni=1,...,|x|\naibi (6)\nwhere ai is the attention coefficient obtained for word xi, and bi is the context dependent representation of xi. In our work, we construct bi by building a bidirectional LSTM over the whole input sequence x1, ..., x|x| and then combining the forward and backward states. Formally, we use the LSTM formulation from Eq. 5 to generate the forward states hf1 , ...,h f |x| by setting uj = xj (the projection word xj using a word lookup table). Likewise, the backward states hb|x|, ...,h b 1 are generated using a backward LSTM by feeding the input in the reverse order, that is, uj = x|x|−j+1. The coefficients ai are computed with a softmax over all input:\nai = softmax(v(bi,hj−1)) (7)\nwhere function v computes the affinity of each word xi and the current output context hj−1 — how likely the input word is to be used to generate the next word in summary. We set v(bi,hj−1) = Ws · tanh(Wcgbi+Whghj−1), where W∗ and W∗∗ are parameters to be learned."
    }, {
      "heading" : "3.4 Attention Over Multiple Inputs",
      "text" : "A key distinction between our model and existing sequence-to-sequence models (Sutskever et al., 2014b; Bahdanau et al., 2014) is that\nour input consists of multiple separate text units. Given an input of N text units, i.e. {xk1, ..., xk|xk|} N k=1, a simple extension would be to concatenate them into one sequence as z = x11, ..., x 1 |x1|, SEG, x 2 1, ..., x 2 |x2|, SEG, x N 1 , ..., x N |xN |, where SEG is a special token that delimits inputs. However, there are two problems with this approach. Firstly, the model is sensitive to the order of text units. Moreover, z may contain thousands of words. This will become a bottleneck for our model with a training time of O(N |z|), since attention coefficients must be computed for all input words to generate each output word.\nWe address these two problems by sub-sampling from the input. The intuition is that even though the number of input text units is large, many of them are redundant or contain secondary information. As our task is to emphasize the main points made in the input, some of them can be removed without losing too much information. Therefore, we define an importance score f(xk) ∈ [0, 1] for each document xk (see Section 3.5). During training, K candidates are sampled from a multinomial distribution which is constructed by normalizing f(xk) for input text units. Notice that the training process goes over the training set multiple times, and our model is still able to learn from more than K text units. For testing, top-K candidates with the highest importance scores are collapsed in descending order into z."
    }, {
      "heading" : "3.5 Importance Estimation",
      "text" : "We now describe the importance estimation model, which outputs importance scores for text units. In general, we start with a ridge regression model, and add a regularizer to enforce the separation of summary-worthy text units from others.\nGiven a cluster of text units {x1, ..., xM} and their summary y, we compute the number of overlapping content words between each text unit and summary y as its gold-standard importance score. The scores are uniformly normalized to [0, 1]. Each text unit xk is represented as an d−dimensional feature vector rk ∈ Rd, with label lk. Text units in the training data are thus denoted with a feature matrix R̃ and a label vector L̃. We aim at learning f(xk) = rk ·w by minimizing ||R̃w − L̃||22 + β · ||w||22. This is a standard formulation for ridge regression, and we use fea-\ntures in Table 1. Furthermore, pairwise preference constraints have been utilized for learning ranking models (Joachims, 2002). We then consider adding a pairwise preference-based regularizing constraint to incorporate a bias towards summary-worthy text units: λ · ∑ T ∑ xp,xq∈T ,lp>0,lq=0 ||(rp− rq) ·w−1|| 2 2, where T is a cluster of text units to be summarized. Term (rp − rq) · w enforces the separation of summary-worthy text from the others. We further construct R̃′ to contain all the pairwise differences (rp − rq). L̃′ is a vector of the same size as R̃′ with each element as 1. The objective function becomes:\nJ(w) = ||R̃w−L̃||22+λ · ||R̃′w−L̃′||22+β · ||w||22 (8)\nλ, β are tuned on development set. With β̃ = β · Id and λ̃ = λ · I|R′|, closed-form solution for ŵ is:\nŵ = (R̃TR̃+ R̃′Tλ̃R̃′+ β̃)−1(R̃TL̃+ R̃′Tλ̃L̃′) (9)"
    }, {
      "heading" : "3.6 Post-processing",
      "text" : "For testing phase, we re-rank the n-best summaries according to their cosine similarity with the input text units. The one with the highest similarity is included in the final summary. Uses of more sophisticated re-ranking methods (Charniak and Johnson, 2005; Konstas and Lapata, 2012) will be investigated in future work."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Data Pre-processing. We pre-process the datasets with Stanford CoreNLP (Manning et al., 2014) for tokenization and extracting POS tags and dependency relations. For RottenTomatoes dataset, we replace movie titles with a generic label in training, and substitute it with the movie name if there is any generic label generated in testing.\nPre-trained Embeddings and Features. The size of word representation is set to 300, both for input and output words. These can be initialized randomly or using pre-trained embeddings learned from Google news (Mikolov et al., 2013). We also extend our model with additional features described in Table 2. Discrete features, such as POS tags, are mapped into word representation via lookup tables. For continuous features (e.g TF-IDF scores), they are attached to word vectors as additional values.\nHyper-parameters and Stop Criterion. The LSTMs (Equation 5) for the decoder and encoders are defined with states and cells of 150 dimensions. The attention of each input word and state pair is computed by being projected into a vector of 100 dimensions (Equation 6).\nTraining is performed via Adagrad (Duchi et al., 2011). It terminates when performance does not improve on the development set. We use BLEU (up to 4-grams) (Papineni et al., 2002) as evaluation metric, which computes the precision of n-grams in generated summaries with gold-standard abstracts as the reference. Finally, the importance-based sampling rate (K) is set to 5 for experiments in Sections 5.2 and 5.3.\nDecoding is performed by beam search with a beam size of 20, i.e. we keep 20 most probable output sequences in stack at each step. Outputs with end of sentence token are also considered for re-ranking. Decoding stops when every beam in stack generates the end of sentence token."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Importance Estimation Evaluation",
      "text" : "We first evaluate the importance estimation component described in Section 3.5. We compare with Support Vector Regression (SVR) (Smola and Vapnik, 1997) and two baselines: (1) a length baseline that ranks text units based on their length, and (2) a centroid baseline that ranks text units according\nto their centroidness, which is computed as the cosine similarity between a text unit and centroid of the cluster to be summarized (Erkan and Radev, 2004).\nWe evaluate using mean reciprocal rank (MRR), and normalized discounted cumulative gain at top 3 and 5 returned results (NDCG@3). Text units are considered relevant if they have at least one overlapping content word with the gold-standard summary. From Figure 2, we can see that our importance estimation model produces uniformly better ranking performance on both datasets."
    }, {
      "heading" : "5.2 Automatic Summary Evaluation",
      "text" : "For automatic summary evaluation, we consider three popular metrics. ROUGE (Lin and Hovy, 2003) is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as reference. ROUGE-SU4 (measures unigram and skipbigrams separated by up to four words) is reported. We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014). We further consider METEOR (Denkowski and Lavie, 2014). As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases.\nFor comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset. Ganesan et al. (2010) utilize a graph-based algorithm to remove repetitive information, and merge opinionated expressions based on syntactic struc-\ntures of product reviews.2 For both datasets, we consider two extractive summarization approaches: (1) LEXRANK (Erkan and Radev, 2004) is an unsupervised method that computes text centrality based on PageRank algorithm; (2) Sipos et al. (2012) propose a supervised SUBMODULAR summarization model which is trained with Support Vector Machines. In addition, LONGEST sentence is picked up as a baseline.\nFour variations of our system are tested. One uses randomly initialized word embeddings. The rest of them use pre-trained word embeddings, additional features in Table 2, and their combination. For all systems, we generate a one-sentence summary.\nResults are displayed in Table 3. Our system with pre-trained word embeddings and additional features achieves the best BLEU scores on both datasets (in boldface) with statistical significance (two-tailed Wilcoxon signed rank test, p < 0.05). Notice that our system summaries are conciser (i.e. shorter on average), which lead to higher scores on precision based-metrics, e.g. BLEU, and lower scores on recall-based metrics, e.g. METEOR and ROUGE. On RottenTomatoes dataset, where summaries generated by different systems are similar in length, our system still outperforms other methods in METEOR and ROUGE in addition to their significantly better BLEU scores. This is not true on Idebate, since the length of summaries by extract-based systems is significantly longer. But the BLEU scores of our system are considerably higher. Among our four systems, models with pre-trained word embeddings in general achieve better scores. Though additional features do not always improve the performance, we find that they help our systems converge faster."
    }, {
      "heading" : "5.3 Human Evaluation on Summary Quality",
      "text" : "For human evaluation, we consider three aspects: informativeness that indicates how much salient information is contained in the summary, grammaticality that measures whether a summary is grammatical, and compactness that denotes whether a summary contains unnecessary information. Each aspect is rated on a 1 to 5 scale (5 is the best). The judges are\n2We do not run this model on Idebate because it relies on high redundancy to detect repetitive expressions, which is not observed on Idebate.\nalso asked to give a ranking on all summary variations according to their overall quality.\nWe randomly sampled 40 movies from RottenTomatoes test set, each of which was evaluated by 5 distinct human judges. We hired 10 proficient English speakers for evaluation. Three system summaries (LexRank, Opinosis, and our system) and human-written abstract along with 20 representative reviews were displayed for each movie. Reviews with the highest gold-standard importance scores were selected.\nResults are reported in Table 4. As it can be seen, our system outperforms the abstract-based system OPINOSIS in all aspects, and also achieves better informativeness and grammaticality scores than LEXRANK, which extracts sentences in their original form. Our system summaries are ranked as the best in 18% of the evaluations, and has an average ranking of 2.3, which is higher than both OPINOSIS and LEXRANK on average. An inter-rater agreement of Krippendorff’s α of 0.71 is achieved for\noverall ranking. This implies that our attentionbased abstract generation model can produce summaries of better quality than existing summarization systems. We also find that our system summaries are constructed in a style closer to human abstracts than others. Sample summaries are displayed in Figure 3."
    }, {
      "heading" : "5.4 Sampling Effect",
      "text" : "We further investigate whether taking inputs sampled from distributions estimated by importance scores trains models with better performance than the ones learned from fixed input or uniformlysampled input. Recall that we sample K text units based on their importance scores (Importance-Based Sampling). Here we consider two other setups: one is sampling K text units uniformly from the input (Uniform Sampling), another is picking K text units with the highest scores (Top K). We try various K values. Results in Figure 4 demonstrates that Importance-Based Sampling can produce comparable BLEU scores to Top K methods, while both of them outperform Uniform Sampling. For METEOR score, Importance-Based Sampling uniformly outperforms the other two methods3."
    }, {
      "heading" : "5.5 Further Discussion",
      "text" : "Finally, we discuss some other observations and potential improvements. First, applying the re-ranking component after the model generates n-best abstracts leads to better performance. Preliminary experiments show that simply picking the top-1 gener-\n3We observe similar results on the Idebate dataset\nations produces inferior results than re-ranking them with simple heuristics. This suggests that the current models are oblivious to some task specific issues, such as informativeness. Post-processing is needed to make better use of the summary candidates. For example, future work can study other sophisticated re-ranking algorithms (Charniak and Johnson, 2005; Konstas and Lapata, 2012).\nFurthermore, we also look at the difficult cases where our summaries are evaluated to have lower informativeness. They are often much shorter than the gold-standard human abstracts, thus the information coverage is limited. In other cases, some generations contain incorrect information on domain-dependent facts, e.g. named entities, numbers, etc. For instance, a summary “a poignant coming-of-age tale marked by a breakout lead performance from Cate Shortland” is generated for movie “Lore”. This summary contains “Cate Shortland” which is the director of the movie instead of actor. It would require semantic features to handle this issue, which has yet to be attempted."
    }, {
      "heading" : "6 Related Work",
      "text" : "Our work belongs to the area of opinion summarization. Constructing fluent natural language opinion summaries has mainly considered product reviews (Hu and Liu, 2004; Lerman et al., 2009), community question answering (Wang et al., 2014), and editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corresponding feature. Our model instead utilizes abstract generation techniques to construct natural language summaries. As far as we know, we are also\nthe first to study claim generation for arguments. Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews (Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014). Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010). Nevertheless, the output summaries are not guaranteed to be grammatical. Gerani et al. (2014) then design a set of manually-constructed realization templates for producing grammatical sentences that serve different discourse functions. Our approach does not require any human-annotated rules, and can be applied in various domains.\nOur task is closely related to recent advances in neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a). Based on the sequence-to-sequence paradigm, RNNs-based models have been investigated for compression (Filippova et al., 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level. Built on the attention-based translation model in Bahdanau et al. (2014), Rush et al. (2015) study the problem of constructing abstract for a single sentence. Our task differs from the models presented above in that our model carries out abstractive decoding from multiple sentences instead of a single sentence."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we presented a neural approach to generate abstractive summaries for opinionated text. We employed an attention-based method that finds salient information from different input text units to generate an informative and concise summary. To cope with the large number of input text, we deploy an importance-based sampling mechanism for model training. Experiments showed that our system obtained state-of-the-art results using both automatic evaluation and human evaluation."
    } ],
    "references" : [ {
      "title" : "A simple domain-independent probabilistic approach to generation",
      "author" : [ "Angeli et al.2010] Gabor Angeli", "Percy Liang", "Dan Klein" ],
      "venue" : null,
      "citeRegEx" : "Angeli et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Angeli et al\\.",
      "year" : 2010
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Abstractive multi-document summarization via phrase selection and merging",
      "author" : [ "Bing et al.2015] Lidong Bing", "Piji Li", "Yi Liao", "Wai Lam", "Weiwei Guo", "Rebecca Passonneau" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association",
      "citeRegEx" : "Bing et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bing et al\\.",
      "year" : 2015
    }, {
      "title" : "Coarse-to-fine n-best parsing and maxent discriminative reranking",
      "author" : [ "Charniak", "Johnson2005] Eugene Charniak", "Mark Johnson" ],
      "venue" : "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Charniak et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Charniak et al\\.",
      "year" : 2005
    }, {
      "title" : "A hierarchical phrase-based model for statistical machine translation",
      "author" : [ "David Chiang" ],
      "venue" : "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Chiang.,? \\Q2005\\E",
      "shortCiteRegEx" : "Chiang.",
      "year" : 2005
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie" ],
      "venue" : "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation",
      "citeRegEx" : "Denkowski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Denkowski et al\\.",
      "year" : 2014
    }, {
      "title" : "A hybrid approach to multi-document summarization of opinions in reviews",
      "author" : [ "Amanda J Stent", "Robert Gaizauskas" ],
      "venue" : "INLG",
      "citeRegEx" : "Fabbrizio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fabbrizio et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Erkan", "Radev2004] Günes Erkan", "Dragomir R. Radev" ],
      "venue" : "J. Artif. Int. Res.,",
      "citeRegEx" : "Erkan et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Erkan et al\\.",
      "year" : 2004
    }, {
      "title" : "Sentence compression by deletion with lstms",
      "author" : [ "Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Filippova et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Filippova et al\\.",
      "year" : 2015
    }, {
      "title" : "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions",
      "author" : [ "ChengXiang Zhai", "Jiawei Han" ],
      "venue" : "In Proceedings of the 23rd international conference on computational linguistics,",
      "citeRegEx" : "Ganesan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ganesan et al\\.",
      "year" : 2010
    }, {
      "title" : "Abstractive summarization of product reviews using discourse structure",
      "author" : [ "Gerani et al.2014] Shima Gerani", "Yashar Mehdad", "Giuseppe Carenini", "Raymond T. Ng", "Bita Nejat" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Gerani et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gerani et al\\.",
      "year" : 2014
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Tomás Kociský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Hu", "Liu2004] Minqing Hu", "Bing Liu" ],
      "venue" : "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "Hu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2004
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "Thorsten Joachims" ],
      "venue" : "In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "Joachims.,? \\Q2002\\E",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2002
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Karpathy", "Fei-Fei2014] Andrej Karpathy", "Li FeiFei" ],
      "venue" : "arXiv preprint arXiv:1412.2306",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2014
    }, {
      "title" : "Concept-to-text generation via discriminative reranking",
      "author" : [ "Konstas", "Lapata2012] Ioannis Konstas", "Mirella Lapata" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Konstas et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Konstas et al\\.",
      "year" : 2012
    }, {
      "title" : "Sentiment summarization: Evaluating and learning user preferences",
      "author" : [ "Lerman et al.2009] Kevin Lerman", "Sasha BlairGoldensohn", "Ryan McDonald" ],
      "venue" : "In Proceedings of the 12th Conference of the European Chapter of the Association",
      "citeRegEx" : "Lerman et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lerman et al\\.",
      "year" : 2009
    }, {
      "title" : "Structure-aware review mining and summarization",
      "author" : [ "Li et al.2010] Fangtao Li", "Chao Han", "Minlie Huang", "Xiaoyan Zhu", "Ying-Ju Xia", "Shu Zhang", "Hao Yu" ],
      "venue" : "In Proceedings of the 23rd International Conference on Computational Linguistics,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Automatic evaluation of summaries using ngram co-occurrence statistics",
      "author" : [ "Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy" ],
      "venue" : "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human",
      "citeRegEx" : "Lin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2003
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky" ],
      "venue" : "In Proceedings of 52nd Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Manning et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "In Proceedings of the 40th annual meeting on association for computational linguistics,",
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Summarizing contrastive viewpoints in opinionated text",
      "author" : [ "Paul et al.2010] Michael J. Paul", "ChengXiang Zhai", "Roxana Girju" ],
      "venue" : "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Paul et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Paul et al\\.",
      "year" : 2010
    }, {
      "title" : "Experiments in single and multidocument summarization using mead",
      "author" : [ "Dragomir R. Radev" ],
      "venue" : "First Document Understanding Conference",
      "citeRegEx" : "Radev.,? \\Q2001\\E",
      "shortCiteRegEx" : "Radev.",
      "year" : 2001
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Rush et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Large-margin learning of submodular summarization models",
      "author" : [ "Sipos et al.2012] Ruben Sipos", "Pannaga Shivaswamy", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 13th Conference of the European Chapter",
      "citeRegEx" : "Sipos et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sipos et al\\.",
      "year" : 2012
    }, {
      "title" : "Support vector regression machines. Advances in neural information processing",
      "author" : [ "Smola", "Vapnik1997] Alex Smola", "Vladimir Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Smola et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Smola et al\\.",
      "year" : 1997
    }, {
      "title" : "The General Inquirer: A Computer Approach to Content Analysis",
      "author" : [ "Dexter C. Dunphy", "Marshall S. Smith", "Daniel M. Ogilvie" ],
      "venue" : null,
      "citeRegEx" : "Stone et al\\.,? \\Q1966\\E",
      "shortCiteRegEx" : "Stone et al\\.",
      "year" : 1966
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks. CoRR, abs/1409.3215",
      "author" : [ "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Domain-independent abstract generation for focused meeting summarization",
      "author" : [ "Wang", "Cardie2013] Lu Wang", "Claire Cardie" ],
      "venue" : "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Query-focused opinion summarization for user-generated content",
      "author" : [ "Wang et al.2014] Lu Wang", "Hema Raghavan", "Claire Cardie", "Vittorio Castelli" ],
      "venue" : "In Proceedings of COLING",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Recognizing contextual polarity in phrase-level sentiment analysis",
      "author" : [ "Janyce Wiebe", "Paul Hoffmann" ],
      "venue" : "In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Pro-",
      "citeRegEx" : "Wilson et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2005
    }, {
      "title" : "Movie review mining and summarization",
      "author" : [ "Zhuang et al.2006] Li Zhuang", "Feng Jing", "Xiao-Yan Zhu" ],
      "venue" : "In Proceedings of the 15th ACM International Conference on Information and Knowledge Management,",
      "citeRegEx" : "Zhuang et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "methods, where phrases or sentences from the original documents are selected for inclusion in the summary (Hu and Liu, 2004; Lerman et al., 2009).",
      "startOffset" : 106,
      "endOffset" : 145
    }, {
      "referenceID" : 36,
      "context" : "summarization work (Zhuang et al., 2006; Li et al., 2010).",
      "startOffset" : 19,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : "summarization work (Zhuang et al., 2006; Li et al., 2010).",
      "startOffset" : 19,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010).",
      "startOffset" : 157,
      "endOffset" : 198
    }, {
      "referenceID" : 10,
      "context" : "Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010).",
      "startOffset" : 157,
      "endOffset" : 198
    }, {
      "referenceID" : 11,
      "context" : "For example, Gerani et al. (2014)",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "An attention-based model (Bahdanau et al., 2014) is deployed to allow the encoder to automatically search for salient information within context.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "Automatic evaluation by BLEU (Papineni et al., 2002) indicates that our system outperforms the state-of-the-art extract-based and abstractbased methods on both tasks.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "72 by an abstractive opinion summarization system from Ganesan et al. (2010). ROUGE evaluation (Lin and Hovy, 2003) also indicates that our system summaries have reasonable information coverage.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 23,
      "context" : "Each word takes the form of a representation vector, which is initialized randomly or by pre-trained embeddings (Mikolov et al., 2013), and updated during training.",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "Similar as previous work (Sutskever et al., 2014b; Bahdanau et al., 2014), we decompose logP (y|x)",
      "startOffset" : 25,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "The representation of input text units s is computed using an attention model (Bahdanau et al., 2014).",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "isting sequence-to-sequence models (Sutskever et al., 2014b; Bahdanau et al., 2014) is that our input consists of multiple separate text units.",
      "startOffset" : 35,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "Furthermore, pairwise preference constraints have been utilized for learning ranking models (Joachims, 2002).",
      "startOffset" : 92,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : "- num of words - category in General Inquirer - unigram (Stone et al., 1966) - num of POS tags - num of positive/negative/neutral - num of named entities words (General Inquirer, - centroidness (Radev, 2001) MPQA (Wilson et al.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : ", 1966) - num of POS tags - num of positive/negative/neutral - num of named entities words (General Inquirer, - centroidness (Radev, 2001) MPQA (Wilson et al.",
      "startOffset" : 125,
      "endOffset" : 138
    }, {
      "referenceID" : 35,
      "context" : ", 1966) - num of POS tags - num of positive/negative/neutral - num of named entities words (General Inquirer, - centroidness (Radev, 2001) MPQA (Wilson et al., 2005)) - avg/max TF-IDF scores",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : "We pre-process the datasets with Stanford CoreNLP (Manning et al., 2014) for tokenization and extracting POS tags and dependency relations.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 23,
      "context" : "from Google news (Mikolov et al., 2013).",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "Training is performed via Adagrad (Duchi et al., 2011).",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : "We use BLEU (up to 4-grams) (Papineni et al., 2002) as evaluation met-",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : "generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014).",
      "startOffset" : 19,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014).",
      "startOffset" : 19,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "For comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 28,
      "context" : "PageRank algorithm; (2) Sipos et al. (2012) propose a supervised SUBMODULAR summarization model which is trained with Support Vector Machines.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "Constructing fluent natural language opinion summaries has mainly considered product reviews (Hu and Liu, 2004; Lerman et al., 2009), community question answering (Wang et al.",
      "startOffset" : 93,
      "endOffset" : 132
    }, {
      "referenceID" : 34,
      "context" : ", 2009), community question answering (Wang et al., 2014), and",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "editorials (Paul et al., 2010).",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corre-",
      "startOffset" : 12,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews (Ganesan et al.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010).",
      "startOffset" : 105,
      "endOffset" : 146
    }, {
      "referenceID" : 10,
      "context" : "Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010).",
      "startOffset" : 105,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010). Nevertheless, the output summaries are not guaranteed to be grammatical. Gerani et al. (2014)",
      "startOffset" : 106,
      "endOffset" : 242
    }, {
      "referenceID" : 9,
      "context" : "sequence-to-sequence paradigm, RNNs-based models have been investigated for compression (Filippova et al., 2015) and summarization (Filippova et al.",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : ", 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level.",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 27,
      "context" : ", 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level.",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : ", 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level.",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "Built on the attention-based translation model in Bahdanau et al. (2014), Rush et al.",
      "startOffset" : 50,
      "endOffset" : 73
    } ],
    "year" : 2016,
    "abstractText" : "We study the problem of generating abstractive summaries for opinionated text. We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and fluent summaries. An importance-based sampling method is designed to allow the encoder to integrate information from an important subset of input. Automatic evaluation indicates that our system outperforms state-ofthe-art abstractive and extractive summarization systems on two newly collected datasets of movie reviews and arguments. Our system summaries are also rated as more informative and grammatical in human evaluation.",
    "creator" : "LaTeX with hyperref package"
  }
}