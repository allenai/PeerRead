{
  "name" : "1611.02361.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents",
    "authors" : [ "Rui Zhang", "Honglak Lee", "Dragomir Radev" ],
    "emails" : [ "ryanzh@umich.edu", "honglak@eecs.umich.edu", "radev@umich.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Sentence and document modeling systems are important for many Natural Language Processing (NLP) applications. The challenge for textual modeling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents). As a traditional method, the bag-of-words model treats\nsentences and documents as unordered collections of words. In this way, however, the bag-of-words model fails to encode word orders and syntactic structures.\nBy contrast, order-sensitive models based on neural networks are becoming increasingly popular thanks to their ability to capture word order information. Many prevalent order-sensitive neural models can be categorized into two classes: Recursive models and Convolutional Neural Networks (CNN) models. Recursive models can be considered as generalizations of traditional sequence-modeling neural networks to tree structures. For example, (Socher et al., 2013) uses Recursive Neural Networks to build representations of phrases and sentences by combining neighboring constituents based on the parse tree. In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached. CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors. Therefore, the model actually splits the sentence locally into n-grams by sliding windows.\nHowever, despite their ability to account for word orders, order-sensitive models based on neural networks still suffer from several disadvantages. First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains (Iyyer et al., 2015; Ma et al., 2015). Besides, since tree-structured neural networks are vulnerable to the vanishing gradient problem (Iyyer et al., 2015), recursive models require heavy label-\nar X\niv :1\n61 1.\n02 36\n1v 1\n[ cs\n.C L\n] 8\nN ov\ning on phrases to add supervisions on internal nodes. Furthermore, parsing is restricted to sentences and it is unclear how to model paragraphs and documents using recursive neural networks. In CNN models, convolutional operators process word vectors sequentially using small windows. Thus sentences are essentially treated as a bag of n-grams, and the long dependency information spanning sliding windows is lost.\nThese observations motivate us to construct a textual modeling architecture that captures long-term dependencies without relying on parsing for both sentence and document inputs. Specifically, we propose Dependency Sensitive Convolutional Neural Networks (DSCNN), an end-to-end classification system that hierarchically builds textual representations with only root-level labels.\nDSCNN consists of a convolutional layer built on top of Long Short-Term Memory (LSTM) networks. DSCNN takes slightly different forms depending on its input. For a single sentence (Figure 1), the LSTM network processes the sequence of word embeddings to capture long-distance dependencies within the sentence. The hidden states of the LSTM are extracted to form the low-level representation, and a convolutional layer with variable-size filters and max-pooling operators follows to extract task-specific features for classification purposes. As for document modeling (Figure 2), DSCNN first applies independent LSTM networks to each subsentence. Then a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences.\nWe evaluate DSCNN on several sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art. In particular, our method achieves highest accuracies on MR sentiment analysis (Pang and Lee, 2005), TREC question classification (Li and Roth, 2002), and subjectivity classification task SUBJ (Pang and Lee, 2004) compared with several competitive baselines.\nThe remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec-\ntures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5."
    }, {
      "heading" : "2 Related Work",
      "text" : "The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding. In this formulation, instead of using one-hot vectors by indexing words into a vocabulary, word embeddings are learned by projecting words onto a low dimensional and dense vector space that encodes both semantic and syntactic features of words.\nGiven word embeddings, different models have been proposed to learn the composition of words to build up phrase and sentence representations. Most methods fall into three types: unordered models, sequence models, and Convolutional Neural Networks models.\nIn unordered models, textual representations are independent of the word order. Specifically, ignoring the token order in the phrase and sentence, the bag-of-words model produces the representation by averaging the constituting word embeddings (Landauer and Dumais, 1997). Besides, a neural-bagof-words model described in (Kalchbrenner et al., 2014) adds an additional hidden layer on top of the averaged word embeddings before the softmax layer for classification purposes.\nIn contrast, sequence models, such as standard Recurrent Neural Networks (RNN) and Long ShortTerm Memory networks, construct phrase and sentence representations in an order-sensitive way. For example, thanks to its ability to capture longdistance dependencies, LSTM has re-emerged as a popular choice for many sequence-modeling tasks, including machine translation (Bahdanau et al., 2014), image caption generation (Vinyals et al., 2014), and natural language generation (Wen et al., 2015). Besides, RNN and LSTM can be both converted to tree-structured networks by using parsing information. For example, (Socher et al., 2013) applied Recursive Neural Networks as a variant of the standard RNN structured by syntactic trees to the\nsentiment analysis task. (Tai et al., 2015) also generalizes LSTM to Tree-LSTM where each LSTM unit combines information from its children units.\nRecently, CNN-based models have demonstrated remarkable performances on sentence modeling and classification tasks. Leveraging convolution operators, these models can extract features from variablelength phrases corresponding to different filters. For example, DCNN in (Kalchbrenner et al., 2014) constructs hierarchical features of sentences by onedimensional convolution and dynamic k-max pooling. (Yin and Schütze, 2015) further utilizes multichannel embeddings and unsupervised pretraining to improve classification results."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "In this section, we describe two building blocks for our system. We first discuss Long Short-Term Memory as a powerful network for modeling sequence data, and then formulate convolution and max-overtime pooling operators for the feature extraction over sequence inputs."
    }, {
      "heading" : "3.1 Long Short-Term Memory",
      "text" : "Recurrent Neural Network (RNN) is a class of models to process arbitrary-length input sequences by recursively constructing hidden state vectors ht. At each time step t, the hidden state ht is an affine function of the input vector xt at time t and its previous hidden state ht−1, followed by a non-linearity such as the hyperbolic tangent function:\nht = tanh(Wxt +Uht−1 + b) (1)\nwhere W, U and b are parameters of the model. However, traditional RNN suffers from the exploding or vanishing gradient problems, where the gradient vectors can grow or decay exponentially as they propagate to earlier time steps. This problem makes it difficult to train RNN to capture longdistance dependencies in a sequence (Bengio et al., 1994; Hochreiter, 1998).\nTo address this problem of capturing long-term relations, Long Short-Term Memory (LSTM) networks, proposed by (Hochreiter and Schmidhuber, 1997) introduce a vector of memory cells and a set of gates to control how the information flows through the network. We thus have the input gate it, the forget gate ft, the output gate ot, the memory cell ct,\nthe input at the current step t as xt, and the hidden state ht, which are all in Rd. Denote the sigmoid function as σ, and the element-wise multiplication as . At each time step t, the LSTM unit manipulates a collection of vectors described by the following equations:\nit = σ ( W(i)xt +U (i)ht−1 + b (i) )\nft = σ ( W(f)xt +U (f)ht−1 + b (f) )\not = σ ( W(o)xt +U (o)ht−1 + b (o) )\nut = tanh ( W(u)xt +U (u)ht−1 + b (u) )\nct = it ut + ft ct−1 ht = ot tanh(ct)\n(2)\nNote that the gates it, ft, ot ∈ [0, 1]d and they control at time step t how the input is updated, how much the previous memory cell is forgotten, and the exposure of the memory to form the hidden state vector respectively."
    }, {
      "heading" : "3.2 Convolution and Max-over-time Pooling",
      "text" : "Convolution operators have been extensively used in object recognition (LeCun et al., 1998), phoneme recognition (Waibel et al., 1989), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), and other traditional NLP tasks (Collobert and Weston, 2008). Given an input sentence of length s: [w1, w2, ..., ws], convolution operators apply a number of filters to extract local features of the sentence.\nIn this work, we employ one-dimensional wide convolution described in (Kalchbrenner et al., 2014). Let ht ∈ Rd denote the representation of wt, and F ∈ Rd×l be a filter where l is the window size. One-dimensional wide convolution computes the feature map c of length (s+ l − 1)\nc = [c1, c2, ..., cs+l−1] (3)\nfor the input sentence. Specifically, in wide convolution, we stack ht column by column, and add (l−1) zero vectors to both ends of the sentence respectively. This formulates an input feature map X ∈ Rd×(s+2l−2). Thereafter, one-dimensional convolution applies the filter F to each set of consecutive l columns in X to produce\n(s − l − 1) activations. The k-th activation is produced by\nck = f b+∑ i,j (F Xk:k+l−1)i,j  (4) where Xk:k+l−1 ∈ Rd×l is the k-th sliding window in X, and b is the bias term. performs elementwise multiplications and f is an nonlinear function such as Rectified Linear Units (ReLU) or the hyperbolic tangent.\nThen, the max-over-time pooling selects the maximum value in the feature map\ncF = max(c) (5)\nas the feature corresponding to the filter F. In practice, we apply many filters with different window sizes l to capture features encoded in llength windows of the input."
    }, {
      "heading" : "4 Model Architectures",
      "text" : "Convolutional Neural Networks have demonstrated state-of-the-art performances in sentence modeling and classification. Despite the fact that CNN is an order-sensitive model, traditional convolution operators extract local features from each possible window of words through filters with predefined sizes. Therefore, sentences are effectively processed like a bag of n-grams, and long-distance dependencies can be only captured if we have long enough filters.\nTo capture long-distance dependencies, much recent effort has been dedicated to building treestructured models from the syntactic parsing information. However, we observe that these methods suffer from three problems. First, they require an external parser and are vulnerable to parsing errors (Iyyer et al., 2015). Besides, tree-structured models need heavy supervisions to overcome vanishing gradient problems. For example, in (Socher et al., 2013), input sentences are labeled for each subphrase, and softmax layers are applied at each internal node. Finally, tree-structured models are restricted to sentence level, and cannot be generalized to model documents.\nIn this work, we propose a novel architecture to address these three problems. Our model hierarchically builds text representations from input words\nwithout parsing information. Only labels at the root level are required at the top softmax layer, so there is no need for labeling subphrases in the text. The system is not restricted to sentence-level inputs: the architecture can be restructured based on the sentence tokenization for modeling documents."
    }, {
      "heading" : "4.1 Sentence Modeling",
      "text" : "Let the input of our model be a sentence of length s: [w1, w2, ..., ws], and c be the total number of word embedding versions. Different versions come from pre-trained word vectors such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014).\nThe first layer of our model consists of LSTM networks processing multiple versions of word embedding. For each version of word embedding, we construct an LSTM network where the input xt ∈ Rd is the d-dimensional word embedding vector for wt. As described in the previous section, the LSTM layer will produce a hidden state representation ht ∈ Rd at each time step. We collect hidden state representations as the output of LSTM layers:\nh(i) = [h (i) 1 ,h (i) 2 , ...,h (i) t , ...,h (i) s ] (6)\nfor i = 1, 2, ..., c.\nA convolution neural network follows as the second layer. To deal with multiple word embeddings, we use filter F ∈ Rc×d×l, where l is the window size. Each hidden state sequence h(i) produced by the i-th version of word embeddings forms one channel of the feature map. These feature maps are stacked as c-channel feature maps X ∈ Rc×d×(s+2(l−1)).\nSimilar to the single channel case, activations are computed as a slight modification of equation 4:\nck = f b+∑ i,j,r (F Xk:k+l−1)i,j,r  (7) A max-over-time pooling layer is then added on top of the convolution neural network. Finally, the pooled features are used in a softmax layer for classification. A sentence modeling example is illustrated in Figure 1."
    }, {
      "heading" : "4.2 Document Modeling",
      "text" : "Our model is not restricted to sentences; it can be restructured to model documents. The intuition comes from the fact that as the composition of words\nbuilds up the semantic meaning for sentences, the composition of sentences establishes the semantic meaning for documents (Li et al., 2015).\nNow suppose that the input of our model is a document consisting of n subsentences: [s1, s2, ..., sn]. Subsentences can be obtained by splitting the document using punctuation (comma, period, question mark, and exclamation point) as delimiters.\nWe employ independent LSTM networks for each subsentence in the same way as the first layer of the sentence modeling architecture. For each subsentence we feed-forward the hidden states of the corresponding LSTM network to the average pooling layer. Take the first sentence of the document as an example,\nh (i) s1 =\n1\nlen(s1) len(s1)∑ j=1 h (i) s1,j (8)\nwhere h(i)s1,j is the hidden state of the first sentence at time step j, and len(s1) denotes the length of the first sentence. In this way, after the averaging pooling layers, we have a representation sequence consisting of averaged hidden states for subsentences,\nh(i) = [h (i) s1 ,h (i) s2 , ...,h (i) sn ] (9)\nfor i = 1, 2, ..., c. Thereafter, a high-level LSTM network comes into play to capture the joint meaning created by the sentences.\nSimilar as sentence modeling, a convolutional layer is placed on top of the high-level LSTM for feature extraction. Finally, a max-over-time pooling layer and a softmax layer follow to pool features and perform the classification task. Figure 2 gives the schematic for the hierarchy."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "Movie Review Data (MR) proposed by (Pang and Lee, 2005) is a dataset for sentiment analysis of movie reviews. The dataset consists of 5,331 positive and 5,331 negative reviews, mostly in one sentence. We follow the practice of using 10-fold cross validation to report results.\nStanford Sentiment Treebank (SST) is another popular sentiment classification dataset introduced\nby (Socher et al., 2013). The sentences are labeled in a fine-grained way (SST-5): {very negative, negative, neutral, positive, very positive}. The dataset has been split into 8,544 training, 1,101 validation, and 2,210 testing sentences. Without neutral sentences, SST can also be used in binary mode (SST2), where the split is 6,920 training, 872 validation, and 1,821 testing.\nFurthermore, we apply DSCNN on question type classification task on TREC dataset (Li and Roth, 2002), where sentences are questions in the following 6 classes: {abbreviation, entity, description, location, numeric}. The entire dataset consists of 5,452 training examples and 500 testing examples.\nWe also benchmark our system on the subjectivity classification dataset (SUBJ) released by (Pang and Lee, 2004). The dataset contains 5,000 subjective sentences and 5,000 objective sentences. We report 10-fold cross validation results as the baseline does.\nFor document-level dataset, we use Large Movie Review (IMDB) created by (Maas et al., 2011). There are 25,000 training and 25,000 testing examples with binary sentiment polarity labels, and 50,000 unlabeled examples. Different from Stanford Sentiment Treebank and Movie Review dataset, every example in this dataset has several sentences."
    }, {
      "heading" : "5.2 Training Details and Implementation",
      "text" : "We use two sets of 300-dimensional pre-trained embeddings, word2vec1 and GloVe2, forming two channels for our network. For all datasets, we use 100 convolution filters each for window sizes of 3, 4, 5. Rectified Linear Units (ReLU) is chosen as the nonlinear function in the convolutional layer.\nFor regularization, before the softmax layers, we employ Dropout operation (Hinton et al., 2012) with dropout rate 0.5, and we do not perform any l2 constraints over the parameters. We use the gradientbased optimizer Adadelta (Zeiler, 2012) to minimize cross-entropy loss between the predicted and true distributions, and the training is early stopped when the accuracy on validation set starts to drop.\nAs for training cost, our system processes around 4000 tokens per second on a single GTX 670 GPU. As an example, this amounts to 1 minute per epoch\n1https://code.google.com/p/word2vec/ 2http://nlp.stanford.edu/projects/glove/\non the TREC dataset, converging within 50 epochs."
    }, {
      "heading" : "5.3 Pretraining of LSTM",
      "text" : "We experiment with two variants of parameter initialization of sentence level LSTMs. The first variant (DSCNN in Table 1) initializes the weight matrices in LSTMs as random orthogonal matrices. In the second variant (DSCNN-Pretrain in Table 1), we first train sequence autoencoders (Dai and Le, 2015) which read input sentences at the encoder and reconstruct the input at the decoder. We pretrain separately on each task based on the same train/valid/test splits. The pretrained encoders are used to be the start points of LSTM layers for later supervised classification tasks."
    }, {
      "heading" : "5.4 Results and Discussions",
      "text" : "Table 1 reports the results of DSCNN on different datasets, demonstrating its effectiveness in comparison with other state-of-the-art methods."
    }, {
      "heading" : "5.4.1 Sentence Modeling",
      "text" : "For sentence modeling tasks, DSCNN beats all baselines on MR and TREC, and achieves the same best result on SUBJ as MVCNN. In SST-2, DSCNN only reports a slightly lower accuracy than MVCNN. In MVCNN, however, the author uses more resources including five versions of word embeddings. For SST-5, DSCNN is second only to\nTree-LSTM, which nonetheless relies on parsers to build tree-structured neural models.\nThe benefit of DSCNN is illustrated by its consistently better results over the sequential CNN models including DCNN and CNN-MC. The superiority of DSCNN is mainly attributed to its ability to maintain long-term dependencies. Figure 3 depicts the correlation between the dependency length and the classification accuracy. While CNN-MC and DSCNN are similar when the sum of dependency arc lengths is below 15, DSCNN gains obvious ad-\nvantages when dependency lengths grow for long and complex sentences. Dep-CNN is also more robust than CNN-MC, but it relies on the dependency parser and predefined patterns to model longer linguistic structures.\nFigure 4 gives some examples where DSCNN makes correct predictions while CNN-MC fails. In the first example, CNN-MC classifies the question as entity due to its focus on the noun phrase “worn or outdated flags”, while DSCNN captures the long dependency between “done with” and “flags”, and\nassigns the correct label description. Similarly in the second case, due to “Nile”, CNN-MC labels the question as location, while the dependency between “depth of” and “river” is ignored. As for the third example, the question involves a complicated and long attributive clause for the subject “artery”. CNN-MC gets easily confused and predicts the type as location due to words “from” and “to”, while DSCNN keeps correct. Finally, “Lindbergh” in the last example make CNN-MC bias to human.\nWe also sample some misclassified examples of DSCNN in Figure 5. Example (a) fails because the numeric meaning of “point” is not captured by the word embedding. Similarly, in the second example, the error is due to the out-of-vocabulary word “TMJ” and it is thus apparently difficult for DSCNN to figure out that it is an abbreviation. Example (c) is likely to be an ambiguous or mistaken annotation. The finding here agrees with the discussion in DepCNN work (Ma et al., 2015)."
    }, {
      "heading" : "5.4.2 Document Modeling",
      "text" : "For document modeling, the result of DSCNN on IMDB against other baselines is listed on the last column of Table 1. Documents in IMDB consist of several sentences and thus very long: the average length is 241 tokens per document and the maximum length is 2526 words (Dai and Le, 2015). As a result, there is no result reported using CNN-based models due to prohibited computation time, and most previous works are unordered models including variations of bag-of-words.\nDSCNN outperforms bag-of-words model (Maas et al., 2011), Deep Averaging Network (Iyyer et al., 2015), and word representation Restricted Boltzmann Machine model combined with bag-of-words features (Dahl et al., 2012). The key weakness of bag-of-words prevents those models from capturing long-term dependencies.\nBesides, Paragraph Vector (Le and Mikolov, 2014) and SA-LSTM (Dai and Le, 2015) achieve better results than DSCNN. It is worth mentioning that both methods, as unsupervised learning algorithms, can gain much positive effects from unlabeled data (they are using 50,000 unlabeled examples in IMDB). For example in (Dai and Le, 2015), with additional data from Amazon reviews, the error rate of SA-LSTM on MR dataset drops by 3.6%."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we present DSCNN, Dependency Sensitive Convolutional Neural Networks for purpose of text modeling at both sentence and document levels. DSCNN captures long-term inter-sentence and intra-sentence dependencies by processing word vectors through layers of LSTM networks, and extracts features by convolutional operators for classification. Experiments show that DSCNN consistently outperforms traditional CNNs, and achieves state-of-the-art results on several sentiment analysis, question type classification and subjectivity classification datasets."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank anonymous reviewers for their constructive comments. This work was supported by a University of Michigan EECS department fellowship and NSF CAREER grant IIS-1453651."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Bengio et al.2003] Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Guidelines for the clear style constituent to dependency conversion",
      "author" : [ "Choi", "Palmer2012] Jinho D Choi", "Martha Palmer" ],
      "venue" : "Technical report, Technical Report 01-12,",
      "citeRegEx" : "Choi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2012
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Collobert", "Weston2008] Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2008
    }, {
      "title" : "Training restricted boltzmann machines on word observations",
      "author" : [ "Dahl et al.2012] George E Dahl", "Ryan P Adams", "Hugo Larochelle" ],
      "venue" : "arXiv preprint arXiv:1202.5695",
      "citeRegEx" : "Dahl et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dahl et al\\.",
      "year" : 2012
    }, {
      "title" : "Semi-supervised sequence learning. arXiv preprint arXiv:1511.01432",
      "author" : [ "Dai", "Le2015] Andrew M Dai", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "Dai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580",
      "author" : [ "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107–116",
      "author" : [ "Sepp Hochreiter" ],
      "venue" : null,
      "citeRegEx" : "Hochreiter.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1998
    }, {
      "title" : "Deep recursive neural networks for compositionality in language",
      "author" : [ "Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Irsoy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Irsoy et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III" ],
      "venue" : "In Proceedings of ACL-IJCNLP,",
      "citeRegEx" : "Iyyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Edward Grefenstette", "Phil Blunsom" ],
      "venue" : "arXiv preprint arXiv:1404.2188",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In Proceedings of EMNLP,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
      "author" : [ "Landauer", "Dumais1997] Thomas K Landauer", "Susan T Dumais" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "Landauer et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Landauer et al\\.",
      "year" : 1997
    }, {
      "title" : "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053",
      "author" : [ "Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2014
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun et al.1998] Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning question classifiers",
      "author" : [ "Li", "Roth2002] X. Li", "D. Roth" ],
      "venue" : "In COLING,",
      "citeRegEx" : "Li et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2002
    }, {
      "title" : "A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057",
      "author" : [ "Li et al.2015] Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Dependency-based convolutional neural networks for sentence embedding",
      "author" : [ "Ma et al.2015] Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou" ],
      "venue" : "In Proceedings of ACL-IJCNLP,",
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Christopher Potts." ],
      "venue" : "Proceedings of ACL-HLT, pages 142–150, Portland, Oregon, USA, June.",
      "citeRegEx" : "Potts.,? 2011",
      "shortCiteRegEx" : "Potts.",
      "year" : 2011
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Pang", "Lee2004] Bo Pang", "Lillian Lee" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Pang et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2004
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Pang", "Lee2005] Bo Pang", "Lillian Lee" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Pang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2005
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Richard Socher", "Christopher D Manning" ],
      "venue" : "Proceedings of EMNLP,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "From symbolic to sub-symbolic information in question classification",
      "author" : [ "Silva et al.2011] Joao Silva", "Luı́sa Coheur", "Ana Cristina Mendes", "Andreas Wichert" ],
      "venue" : "Artificial Intelligence Review,",
      "citeRegEx" : "Silva et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Silva et al\\.",
      "year" : 2011
    }, {
      "title" : "Semantic compositionality through recursive matrixvector spaces",
      "author" : [ "Brody Huval", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In Proceedings of EMNLP-CoNLL,",
      "citeRegEx" : "Socher et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "Proceedings of EMNLP,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075",
      "author" : [ "Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning" ],
      "venue" : null,
      "citeRegEx" : "Tai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "arXiv preprint arXiv:1411.4555",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2014
    }, {
      "title" : "Phoneme recognition using time-delay",
      "author" : [ "Toshiyuki Hanazawa", "Geoffrey Hinton", "Kiyohiro Shikano", "Kevin J Lang" ],
      "venue" : null,
      "citeRegEx" : "Waibel et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Waibel et al\\.",
      "year" : 1989
    }, {
      "title" : "Semantically conditioned lstm-based natural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745",
      "author" : [ "Wen et al.2015] Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young" ],
      "venue" : null,
      "citeRegEx" : "Wen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Multichannel variable-size convolution for sentence classification",
      "author" : [ "Yin", "Schütze2015] Wenpeng Yin", "Hinrich Schütze" ],
      "venue" : "In Proceedings of CoNLL,",
      "citeRegEx" : "Yin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2015
    }, {
      "title" : "Adadelta: An adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "For example, (Socher et al., 2013) uses Recursive Neural Networks to build",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors.",
      "startOffset" : 98,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors.",
      "startOffset" : 98,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains (Iyyer et al., 2015; Ma et al., 2015).",
      "startOffset" : 118,
      "endOffset" : 155
    }, {
      "referenceID" : 19,
      "context" : "First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains (Iyyer et al., 2015; Ma et al., 2015).",
      "startOffset" : 118,
      "endOffset" : 155
    }, {
      "referenceID" : 11,
      "context" : "Besides, since tree-structured neural networks are vulnerable to the vanishing gradient problem (Iyyer et al., 2015), recursive models require heavy labelar X iv :1 61 1.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding.",
      "startOffset" : 152,
      "endOffset" : 220
    }, {
      "referenceID" : 21,
      "context" : "The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding.",
      "startOffset" : 152,
      "endOffset" : 220
    }, {
      "referenceID" : 24,
      "context" : "The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding.",
      "startOffset" : 152,
      "endOffset" : 220
    }, {
      "referenceID" : 12,
      "context" : "Besides, a neural-bagof-words model described in (Kalchbrenner et al., 2014) adds an additional hidden layer on top of the",
      "startOffset" : 49,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "For example, thanks to its ability to capture longdistance dependencies, LSTM has re-emerged as a popular choice for many sequence-modeling tasks, including machine translation (Bahdanau et al., 2014), image caption generation (Vinyals et al.",
      "startOffset" : 177,
      "endOffset" : 200
    }, {
      "referenceID" : 29,
      "context" : ", 2014), image caption generation (Vinyals et al., 2014), and natural language generation (Wen et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 31,
      "context" : ", 2014), and natural language generation (Wen et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "For example, (Socher et al., 2013) applied Recursive Neural Networks as a variant of the",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 28,
      "context" : "(Tai et al., 2015) also generalizes LSTM to Tree-LSTM where each LSTM unit combines information from its children units.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "For example, DCNN in (Kalchbrenner et al., 2014) constructs hierarchical features of sentences by onedimensional convolution and dynamic k-max pooling.",
      "startOffset" : 21,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "This problem makes it difficult to train RNN to capture longdistance dependencies in a sequence (Bengio et al., 1994; Hochreiter, 1998).",
      "startOffset" : 96,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "This problem makes it difficult to train RNN to capture longdistance dependencies in a sequence (Bengio et al., 1994; Hochreiter, 1998).",
      "startOffset" : 96,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : "Convolution operators have been extensively used in object recognition (LeCun et al., 1998), phoneme recognition (Waibel et al.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 30,
      "context" : ", 1998), phoneme recognition (Waibel et al., 1989), sentence modeling and classification (Kalchbrenner et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 12,
      "context" : ", 1989), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), and other traditional NLP tasks (Col-",
      "startOffset" : 46,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : ", 1989), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), and other traditional NLP tasks (Col-",
      "startOffset" : 46,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "In this work, we employ one-dimensional wide convolution described in (Kalchbrenner et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "First, they require an external parser and are vulnerable to parsing errors (Iyyer et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 27,
      "context" : "For example, in (Socher et al., 2013), input sentences are labeled for each sub-",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "Different versions come from pre-trained word vectors such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : ", 2013) and GloVe (Pennington et al., 2014).",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 18,
      "context" : "The intuition comes from the fact that as the composition of words builds up the semantic meaning for sentences, the composition of sentences establishes the semantic meaning for documents (Li et al., 2015).",
      "startOffset" : 189,
      "endOffset" : 206
    }, {
      "referenceID" : 27,
      "context" : "Method MR SST-2 SST-5 TREC SUBJ IMDB SVM (Socher et al., 2013) — 79.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "7 — — — NB (Socher et al., 2013) — 81.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "2 SVMS (Silva et al., 2011) — — — 95.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "0 — — Standard-RNN (Socher et al., 2013) — 82.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 26,
      "context" : "2 — — — MV-RNN (Socher et al., 2012) 79.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 27,
      "context" : "4 — — — RNTN (Socher et al., 2013) — 85.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 28,
      "context" : "8 — — — Standard-LSTM (Tai et al., 2015) — 86.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 28,
      "context" : "8 — — — bi-LSTM (Tai et al., 2015) — 86.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 28,
      "context" : "1 — — — Tree-LSTM (Tai et al., 2015) — 88.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "8 DCNN (Kalchbrenner et al., 2014) — 86.",
      "startOffset" : 7,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "0 — — CNN-MC (Kim, 2014) 81.",
      "startOffset" : 13,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "9 — Dep-CNN (Ma et al., 2015) 81.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "4 — — Neural-BoW (Kalchbrenner et al., 2014) — 80.",
      "startOffset" : 17,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "2 — — DAN (Iyyer et al., 2015) 80.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "6 WRRBM+BoW(bnc) (Dahl et al., 2012) — — — — — 89.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 27,
      "context" : "SVM: Support Vector Machines with unigram features (Socher et al., 2013) NB: Naive Bayes with unigram features(Socher et al.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 27,
      "context" : ", 2013) NB: Naive Bayes with unigram features(Socher et al., 2013) NBSVM-bi: Naive Bayes SVM and Multinomial Naive Bayes with bigrams (Wang and Man-",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "ning, 2012) SVMS : SVM with features including uni-bi-trigrams, POS, parser, and 60 hand-coded rules (Silva et al., 2011) Standard-RNN: Standard Recursive Neural Network (Socher et al.",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 27,
      "context" : ", 2011) Standard-RNN: Standard Recursive Neural Network (Socher et al., 2013) MV-RNN: Matrix-Vector Recursive Neural Network (Socher et al.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : ", 2013) MV-RNN: Matrix-Vector Recursive Neural Network (Socher et al., 2012) RNTN:Recursive Neural Tensor Network (Socher et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : ", 2012) RNTN:Recursive Neural Tensor Network (Socher et al., 2013) DRNN: Deep Recursive Neural Network (Irsoy and Cardie, 2014) Standard-LSTM: Standard Long Short-Term Memory Network (Tai et al.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : ", 2013) DRNN: Deep Recursive Neural Network (Irsoy and Cardie, 2014) Standard-LSTM: Standard Long Short-Term Memory Network (Tai et al., 2015) bi-LSTM: Bidirectional LSTM (Tai et al.",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 28,
      "context" : ", 2015) bi-LSTM: Bidirectional LSTM (Tai et al., 2015) Tree-LSTM: Tree-Structured LSTM (Tai et al.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 28,
      "context" : ", 2015) Tree-LSTM: Tree-Structured LSTM (Tai et al., 2015) SA-LSTM: Sequence Autoencoder LSTM (Dai and Le, 2015).",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "DCNN: Dynamic Convolutional Neural Network with k-max pooling (Kalchbrenner et al., 2014) CNN-MC: Convolutional Neural Network with static pretrained and fine-tuned pretrained word-embeddings (Kim, 2014) MVCNN: Multichannel Variable-Size Convolution Neural Network (Yin and Schütze, 2015) Dep-CNN: Dependency-based Convolutional Neural Network (Ma et al.",
      "startOffset" : 62,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : ", 2014) CNN-MC: Convolutional Neural Network with static pretrained and fine-tuned pretrained word-embeddings (Kim, 2014) MVCNN: Multichannel Variable-Size Convolution Neural Network (Yin and Schütze, 2015) Dep-CNN: Dependency-based Convolutional Neural Network (Ma et al.",
      "startOffset" : 110,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "Neural-BoW : Neural Bag-of-Words Models (Kalchbrenner et al., 2014) DAN: Deep Averaging Network (Iyyer et al.",
      "startOffset" : 40,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : ", 2014) DAN: Deep Averaging Network (Iyyer et al., 2015) Paragraph-Vector: Logistic Regression on Paragraph-Vector (Le and Mikolov, 2014) WRRBM+BoW(bnc): word representation Restricted Boltzmann Machine combined with bag-of-words features (Dahl et al.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : ", 2015) Paragraph-Vector: Logistic Regression on Paragraph-Vector (Le and Mikolov, 2014) WRRBM+BoW(bnc): word representation Restricted Boltzmann Machine combined with bag-of-words features (Dahl et al., 2012) Full+Unlabeled+BoW(bnc):word vector based model capturing both semantic and sentiment, trained on unlabeled examples, and with bag-of-words features concatenated (Maas et al.",
      "startOffset" : 190,
      "endOffset" : 209
    }, {
      "referenceID" : 27,
      "context" : "by (Socher et al., 2013).",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "For regularization, before the softmax layers, we employ Dropout operation (Hinton et al., 2012) with dropout rate 0.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 33,
      "context" : "We use the gradientbased optimizer Adadelta (Zeiler, 2012) to minimize cross-entropy loss between the predicted and true distributions, and the training is early stopped when the accuracy on validation set starts to drop.",
      "startOffset" : 44,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "The finding here agrees with the discussion in DepCNN work (Ma et al., 2015).",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : ", 2011), Deep Averaging Network (Iyyer et al., 2015), and word representation Restricted Boltzmann Machine model combined with bag-of-words features (Dahl et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : ", 2015), and word representation Restricted Boltzmann Machine model combined with bag-of-words features (Dahl et al., 2012).",
      "startOffset" : 104,
      "endOffset" : 123
    } ],
    "year" : 2016,
    "abstractText" : "The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a generalpurpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long ShortTerm Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentencelevel tasks. Moreover, unlike other CNNbased models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-ofthe-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification.",
    "creator" : "LaTeX with hyperref package"
  }
}