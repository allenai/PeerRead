{
  "name" : "1408.4245.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards Crowdsourcing and Cooperation in Linguistic Resources",
    "authors" : [ "Dmitry Ustalov" ],
    "emails" : [ "dau@imm.uran.ru" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: games with a purpose, mechanized labor, wisdom of the crowd, gamification, crowdsourcing, cooperation, linguistic resources."
    }, {
      "heading" : "1 Introduction",
      "text" : "Crowdsourcing has become a mainstream and well-suited approach for solving many linguistic data gathering problems such as sense inventory creation [1], corpus annotation [2], information extraction [3], etc. However, its most effective use still remains a problem because human annotators’ motivation and availability are tantalizingly constrained and it is crucial to get the most of performance from the effort interested people can make.\nAnother extremely popular term nowadays is gamification. The origin of the gamification concept is, of course, video game industry. The idea of gamification is in embedding interactive and game-based techniques into application to strengthen user engagement and increase the time spent annotating. Due to the insufficiency of exploration, gamification is more rarely used in academia when compared to the industry.\nCooperation is a major, if not principal, element of today’s video games, which is confirmed by the presentations made in recent years at E3 — the largest video game exposition and event. Initially, multiplayer mode in video games was focused on player versus player competitions, but a few years ago the focus has changed to cooperated human players versus AI and guild versus guild games.\nar X\niv :1\n40 8.\n42 45\nv2 [\ncs .S\nI] 2\n4 A\npr 2\n01 7\nThe work, as described in this paper, makes the following contributions: 1) it presents a survey on crowdsourcing taxonomies and cooperation in linguistic resources, 2) makes recommendations on using cooperation in existent genres of crowdsourcing, and 3) provides an evidence of the efficiency of cooperation represented by a popular Russian linguistic resource created through crowdsourcing.\nThe rest of this paper is organized as follows. Section 2 focuses on related work towards crowdsourcing genres and cooperation in linguistic resources. Section 3 is devoted to the cooperative aspect of crowdsourcing. Section 4 discusses cooperation using OpenCorpora as the example, which is a sufficiently popular Russian linguistic resource created through crowdsourcing. Section 5 interprets and explains the obtained results. Section 6 concludes with final remarks and directions for the future work."
    }, {
      "heading" : "2 Crowdsourcing Genres & Activities",
      "text" : "Early studies on crowdsourcing genres in their wide definition were conducted in 2009. Quinn & Bederson in their technical report [4] proposed the term of distributed human computation along with the taxonomy of seven different genres of these computations such as games with a purpose, mechanized labor, wisdom of crowds, crowdsourcing, dual-purpose work, grand search, human-based genetic algorithms, and knowledge collection from volunteer contributions.\nIn the same year Yuen et al. also presented [5] another taxonomy of five crowdsourcing genres: initiatory human computation, distributed human computation, social game-based human computation with volunteers, paid engineers and online players, which is similar to the previously mentioned.\nMany studies following the early ones are focused on classification of whether a crowdsourced project belongs to a specific class of the given taxonomy. For instance, Sabou et al. study of correlation between crowdsourcing genres [6], quality assessment [7], and guidelines on corpus annotation through crowdsourcing [2] align various best practices among the established genres.\nThere are other attempts to create a taxonomy of crowdsourcing genres. Zwass investigated the phenomena of co-creation [8] and proposed a taxonomy of user-created digital content which includes the following: knowledge compendia, consumer reviews, multimedia content, blogs, mashups, virtual worlds. The resulted taxonomy appears to be too general and, since it was not intended, does not fit the natural language processing field perfectly.\nErickson presented four quadrant model [9] composed of two orthogonal dichotomies to classify crowdsourcing projects: “same place–different places” and “same time–different times”. The resulted taxonomy tends to assign all the mentioned above crowdsourced projects to the “different places–different times” quadrant also called Global Crowdsourcing.\nSome studies propose much narrower dichotomies. This is the case of the research conducted by Suendermann & Pieraccini [10], which introduces a concept of private crowd being a trade-off between two extremes: an inexpensive, highly available yet uncontrolled public crowd such as the Amazon’s one, and the\nexpensive to hire, high-quality and professional expert annotators. The private crowd term can be referred to as controlled crowd."
    }, {
      "heading" : "2.1 Three Genres of Crowdsourcing",
      "text" : "In 2013, Wang et al. aggregated most of the previous studies in their very welldone survey. The mentioned work emphasizes three intuitive and well-separated genres of crowdsourcing [11]:\nGames with a purpose (GWAPs), when a player without any special knowledge is put into a gaming environment and have to make right decisions to win the game under the pressure of time or any game mechanics’ constraints. Phrase Detectives4 and JeuxDeMots5 can be considered as good examples of such games. Mechanized labor (MLab), when an annotator who meet the preliminary requirements is asked to answer a questionnaire on a centralized platform and is rewarded for their work by micropayments. The most well-recognized examples of MLab are Amazon Mechanical Turk6 and CrowdFlower7. Wisdom of the crowd (WotC), when motivated volunteers share their knowledge on the given topic in the free form in order to answer some question, to explain something to other people, and so on. The obvious examples of WotC are Wikipedia8 and Yahoo! Answers9.\nObservations reveal that research papers often do not specify the exact crowdsourcing genre and treat the crowdsourcing term as a synonym to MLab due to extreme popularity of the Amazon’s product."
    }, {
      "heading" : "2.2 Cooperation in Linguistic Resources",
      "text" : "Cooperation, derived from to cooperate, is to work actively with rather than against others [12, p. 435]. Unfortunately, cooperative crowdsourcing in linguistic resources is less explored in the literature, but present studies show that considering the concept of cooperation in crowdsourcing is a trending topic deserving attention.\nAn early study of Wikipedia and its quality by Wilkinson & Huberman [13] found a statistically significant correlation between page edits, talkpage conversations and the quality of these pages. The study revealed the fact that pages with more intense discussion activity often have better quality than less discussed ones.\n4 https://anawiki.essex.ac.uk/phrasedetectives/ 5 http://www.jeuxdemots.org/ 6 http://mturk.com/ 7 http://www.crowdflower.com/ 8 http://wikipedia.org/ 9 https://answers.yahoo.com/\nA study by Arazy & Nov [14] pays a special attention to local inequality — inequality of editors’ contribution in a particular article, and global inequality — inequality in overall Wikipedia activity for the same set of editors. As a result, they found that global inequality has an impact on local inequality, which influences editors’ coordination in a positive way, which in its turn contributes to quality.\nBudzise-Weaver et al. [15] consider several cases of multilingual digital libraries and their collaboration both with state institutions and crowdsourced projects in order to provide multilingual information access for users. The paper does not describe how exactly crowdsourcing can help digital libraries in doing their job, but does demonstrate significant interest to crowdsourcing from an interdisciplinary point of view.\nRanj Bar & Maheswaran [16] in their case study on Wikipedia concluded that new mechanisms are needed to coordinate the activities in crowdsourcing due to the fact that high quality articles are controlled by small groups of permanent editors, and supporting these articles is a huge burden for the editors."
    }, {
      "heading" : "3 Crowdsourcing Genres and Cooperation",
      "text" : "Each of the three crowdsourcing genres has its own identities; and the principle of paritipants’ cooperation changes with each particular crowdsourcing instance. However, it seems possible to denote three common points:\n– attractiveness, the degree of how a participant can find a crowdsourcing process attractive, – usefulness, the degree of how a participant can find his activity results useful to their own purposes, – difficulty, the degree of how it is difficult to embed cooperative elements into a process.\nWhen specific case studies are available, the correspondent details are provided."
    }, {
      "heading" : "3.1 Games with a purpose",
      "text" : "The main advantage of GWAPs is their high attractiveness, because people love video games and it is easier to get new participants than in other genres of crowdsourcing. One may find low usefulness in these games, but the more attractive the game is, the less other factors are becoming important.\nIt is necessary to mention that video games are a very costly kind of software and producing GWAPs requires not only creating a game, but also designing innovative game mechanics allowing a player to both enjoy the game and to implicitly produce valuable data. Thus, games with a purpose have high difficulty to be realized.\nAuthors of Phrase Detectives say that the cost of data gathering using their means is lower than using other approaches [17], but they did not consider the\ntotal cost of the game design and development. Elements of real-time players’ cooperation may enhance GWAPs attractiveness even more. The evidence of this is the fact that modern cooperative multiplayer video games like Dota 2 or Destiny have substituted traditional free for all (deathmatch) multiplayer games."
    }, {
      "heading" : "3.2 Mechanized labor",
      "text" : "Since MLab projects are often deployed on specialized platforms available on the World Wide Web, the main advantage of MLab is its low difficulty : cooperative elements may be embedded supplementarily to the annotation process through allowing annotators to join teams and making them participate in the teambased activity.\nIn order to cover as much domains as possible, platforms’ owners provide only very utilitarian and generic interfaces allowing one to answer a questionnaire without exposing them to any domain-specific features.\nSince MLab participants are often rewarded for their work that may be or may not be interesting for them, the mechanized labor projects have medium usefulness and usually low attractiveness."
    }, {
      "heading" : "3.3 Wisdom of the crowd",
      "text" : "The strong side of WotC projects is, indeed, high usefulness due to the fundamental principle of such a genre, when volunteers make efforts to make their resource better for everyone. WotC have low attractiveness for the same reasons, however it depends on every particular instance.\nThe above mentioned study by Arazy & Nov also touches upon a typical regulation problem called “edit warring” in Wikipedia [14], when “editors who disagree about the content of a page repeatedly override each other’s contributions, rather than trying to resolve the disagreement through discussion”.\nThe phenomena of “edit warring” was later studied by Yasseri et. al [18]. Such a problem may be partially resolved by using the controlled crowd instead of the public one when volunteers have a mentor and responsibility for their actions [19]. Therefore, such projects have medium difficulty."
    }, {
      "heading" : "4 Evidence",
      "text" : "An evidence that cooperation does work and really stimulates participants to do more assignments is the case of OpenCorpora, which is a project focused on creation of a large annotated Russian corpus through crowdsourcing [20].\nCurrently, OpenCorpora participants have to annotate morphologically ambiguous examples in the MLab manner. One can annotate examples individually, but has an opportunity to join teams and annotate examples in cooperation with their teammates. A team can be created and joined by everyone, and teams\nchallenge each other by means of active collaborators, annotated examples, and error rates.\nAs according to the full-scale pilot study conducted on one of the largest Russian information technologies’ websites10, volunteers were very positive about their participation in the cooperative annotation. The study was followed by the creation of the largest team uniting 170 participants. The team got the 2nd place in the total rank11 based on the number of the annotated examples.\nThe possible explanation of such a result would be found in what have driven the participants’ motivation. It was not only their altruism and readiness to help, but the possibility for their team to get the leading places in the total rank, as well as their personal participation being one of the keys to the team’s possible success."
    }, {
      "heading" : "4.1 “Is there a relationship?”",
      "text" : "To make it possible to study the present result more thoroughly, the OpenCorpora team has kindly provided us with the dataset consisted of user ID, the group’s name, and various activity information including total number of the annotated examples per user. Hereafter participants who joined a team are referred to as teammates, and those who did not join a team are referred to as individuals.\n10 http://habrahabr.ru/post/152799/#comment_5315923 11 http://opencorpora.org/?page=stats\nThe initial dataset contains information on 2642 users: 2219 of them are individuals and 423 are teammates. The distributions’ densities are depicted at Fig. 1 and seem to be right-skewed. In order to remove outliers from the dataset, users who annotated less than 50 examples or more than 350 examples have been excluded. As a result, the dataset has been reduced to 579 individuals and 195 teammates, 71 of those are the members of the largest team.\nIn general, the individuals annotated 801 531 examples and the teammates annotated 970 650, while in the dataset the individuals annotated 71 150 examples and the teammates annotated only 29 049 examples.\nHence, the research question is “Is there a relationship between being a team member and the number of annotated examples for a regular OpenCorpora user?”"
    }, {
      "heading" : "4.2 Inference",
      "text" : "Since the dataset is right-skewed and such hypothesis tests as t-test may be unreliable, a randomization test was implemented in the R programming language and executed for 25 000 times under the significance level of α = .05 in order to estimate the unbiased p-value.\nThe true difference in means of the numbers of annotated examples among the teammates (µT ) and the individuals (µI) has been examined. The following hypothesis H was evaluated in order to find a relationship between being a team member and the number of annotated examples:\nH0 : µT −µI = 0, the teammates and the individuals on average have no difference in their annotation activity,\nHA : µT − µI > 0, the teammates tend to annotate more examples on average than the individuals.\nThe density of differences in the number of annotated examples is demonstrated at Fig 2(a): the observed difference in means for this one-tailed test is x̄T − x̄I = 26.085, and the p-value is p = 0. Thus, p < α and the null hypothesis H0 has been rejected, suggesting that µT > µI : the teammates tend to annotate more examples than the individuals."
    }, {
      "heading" : "5 Discussion",
      "text" : "The obtained result can also be explained by a teammate being more loyal and attached to the resource than an individual. Therefore, it is reasonable to study the performance of a particular team."
    }, {
      "heading" : "5.1 The Largest Team vs. The Individuals",
      "text" : "In order to compare the behavior of the individuals and the largest team members instead of all the teammates, the true difference in means of the numbers of annotated examples among the teammates of the largest team (µH) and the individuals (µI) was examined. The following hypothesis H\n′ was evaluated in the similar way as the previous one:\nH ′0 : µH − µI = 0, the teammates of the largest team and the individuals on average have no difference in their annotation activity, H ′A : µH − µI > 0, the teammates of the largest team annotate more examples on average than the individuals.\nThe simulation results for this one-tailed test are presented at Fig. 2(b): the observed difference in means is x̄H − x̄I = 29.552 and the p-value is p = .001. Since p < α, the null hypothesis H ′0 has been rejected, suggesting that µH > µI : the teammates of the largest team annotate more examples than the individuals.\nThis result agrees well with the H0 hypothesis and can be explained by the fact that the largest team is still relatively small and consists of only 170 teammates who were highly motivated for a short time due to news rotation on the website where they came from. Their activity decreased significantly when the announcement of the OpenCorpora disappeared from the news headline. Their team took the 2nd place on the leaderboard; they lost to the the leading team as the latter had annotated approximately seven times more examples (501 963 versus 76 559)."
    }, {
      "heading" : "5.2 The Largest Team vs. Other Teams",
      "text" : "Statistical testing of teams’ performance based on comparison of their impact is complicated due to lack of participants in other teams. For instance, the 2nd largest team is comprised of 36 users only, the 3rd largest — 24, the 4th —\n13, which is insufficient for any meaningful test. However, it is indeed possible to compare the performance of the largest team with the performance of other teams considered together.\nThe true difference in means of the numbers of annotated examples among the teammates of the largest team (µH) and other teams (µR) was examined, and the following hypothesis H ′′ has been evaluated:\nH ′′0 : µH − µR = 0, the teammates of the largest team and other teammates on average have no difference in their annotation activity, H ′′A : µH − µR 6= 0, the teammates of the largest team and other teammates on average have the difference in their annotation activity.\nThe simulation results for this two-tailed test are presented at Fig. 2(c): the observed difference in means is x̄H−x̄R = 5.453 and the p-value is p = .629. Since p > α, the null hypothesis H ′0 has not been rejected, suggesting that µH = µR: the teammates of the largest team annotate the same number of examples as other teammates do."
    }, {
      "heading" : "6 Conclusion",
      "text" : "According to the obtained results, there is a correlation between being a team member and the number of annotated examples for a regular OpenCorpora user. The use of team-based cooperation can improve the user activity on crowdsourced linguistic resources. However, since the study is observational, it was impossible to establish causal relationships between the variables.\nWhen organized in teams, users do provide more annotations comparing with those who are not organized in teams. Thus, it is highly recommended for a crowdsourced resource to provide users with the opportunity to join teams and annotate examples in cooperation with their teammates.\nThe statistical hypotheses have been evaluated with use of the randomization test with the significance level of .05. The present dataset is available12 in an depersonalized form under the Creative Commons Attribution-ShareAlike 3.0 license. The source code of the above mentioned simulation program is included under the MIT License.\nFurther work may be focused on assessing the quality of team-based cooperation results and on studying the patterns of cooperation and the efficiency of their use in other popular linguistic resources created through crowdsourcing.\nAcknowledgments. This work is supported by the Russian Foundation for the Humanities, project no. 13-04-12020 “New Open Electronic Thesaurus for Russian”, and by the Program of Government of the Russian Federation 02.A03.21.0006 on 27.08.2013.\n12 http://ustalov.imm.uran.ru/pub/opencorpora-cooperation.tar.gz\nThe author would like to thank Dmitry Granovsky for the extended statistical information collected from http://opencorpora.org/. The author is also grateful to the anonymous referees who offered very useful comments on the present paper."
    } ],
    "references" : [ {
      "title" : "Creating a system for lexical substitutions from scratch using crowdsourcing",
      "author" : [ "C. Biemann" ],
      "venue" : "Language Resources and Evaluation 47(1)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Corpus Annotation through Crowdsourcing: Towards Best Practice Guidelines",
      "author" : [ "M. Sabou", "K. Bontcheva", "L. Derczynski", "A. Scharl" ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), European Language Resources Association (ELRA)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Information Extraction Meets Crowdsourcing: A Promising Couple",
      "author" : [ "C. Lofi", "J. Selke", "W.T. Balke" ],
      "venue" : "Datenbank-Spektrum 12(2)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A Taxonomy of Distributed Human Computation",
      "author" : [ "A.J. Quinn", "B.B. Bederson" ],
      "venue" : "Human-Computer Interaction Lab Tech Report, University of Maryland",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A Survey of Human Computation Systems",
      "author" : [ "M.C. Yuen", "L.J. Chen", "I. King" ],
      "venue" : "International Conference on Computational Science and Engineering, 2009. CSE ’09. Volume 4., IEEE",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Crowdsourcing Research Opportunities: Lessons from Natural Language Processing",
      "author" : [ "M. Sabou", "K. Bontcheva", "A. Scharl" ],
      "venue" : "Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technologies, ACM",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Crowdsourced Knowledge Acquisition: Towards Hybrid-Genre Workflows",
      "author" : [ "M. Sabou", "A. Scharl", "F. Michael" ],
      "venue" : "International Journal on Semantic Web and Information Systems 9(3)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Co-Creation: Toward a Taxonomy and an Integrated Research Perspective",
      "author" : [ "V. Zwass" ],
      "venue" : "International Journal of Electronic Commerce 15(1)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Some Thoughts on a Framework for Crowdsourcing",
      "author" : [ "T. Erickson" ],
      "venue" : "CHI 2011 Workshop on Crowdsourcing and Human Computation.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Crowdsourcing for Industrial Spoken Dialog Systems",
      "author" : [ "D. Suendermann", "R. Pieraccini" ],
      "venue" : "In Eskénazi, M., Levow, G.A., Meng, H., Parent, G., Suendermann, D., eds.: Crowdsourcing for Speech Processing: Applications to Data Collection, Transcription and Assessment. John Wiley & Sons, Ltd",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Perspectives on crowdsourcing annotations for natural language processing",
      "author" : [ "A. Wang", "C.D.V. Hoang", "M.Y. Kan" ],
      "venue" : "Language Resources and Evaluation 47(1)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "No Contest: A Case Against Competition",
      "author" : [ "A. Kohn" ],
      "venue" : "Houghton Mifflin Harcourt",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Cooperation and Quality in Wikipedia",
      "author" : [ "D.M. Wilkinson", "B.A. Huberman" ],
      "venue" : "Proceedings of the 2007 International Symposium on Wikis, ACM",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Determinants of Wikipedia Quality: The Roles of Global and Local Contribution Inequality",
      "author" : [ "O. Arazy", "O. Nov" ],
      "venue" : "Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work, ACM",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Collaboration and Crowdsourcing: The Cases of Multilingual Digital Libraries",
      "author" : [ "T. Budzise-Weaver", "J. Chen", "M. Mitchell" ],
      "venue" : "Electronic Library, The 30(2)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Case Study: Integrity of Wikipedia Articles",
      "author" : [ "A. Ranj Bar", "M. Maheswaran" ],
      "venue" : "Confidentiality and Integrity in Crowdsourcing Systems. SpringerBriefs in Applied Sciences and Technology. Springer International Publishing",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Phrase Detectives: Utilizing Collective Intelligence for Internet-scale Language Resource Creation",
      "author" : [ "M. Poesio", "J. Chamberlain", "U. Kruschwitz", "L. Robaldo", "L. Ducceschi" ],
      "venue" : "ACM Trans. Interact. Intell. Syst. 3(1)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dynamics of Conflicts in Wikipedia",
      "author" : [ "T. Yasseri", "R. Sumi", "A. Rung", "A. Kornai", "J. Kertész" ],
      "venue" : "PLOS ONE 7(6)",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A Spinning Wheel for YARN: User Interface for a Crowdsourced Thesaurus",
      "author" : [ "P. Braslavski", "D. Ustalov", "M. Mukhin" ],
      "venue" : "Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Crowdsourcing morphological annotation",
      "author" : [ "V. Bocharov", "S. Alexeeva", "D. Granovsky", "E. Protopopova", "M. Stepanova", "A. Surikov" ],
      "venue" : "Computational Linguistics and Intellectual Technologies: papers from the Annual conference “Dialogue”, RGGU",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Crowdsourcing has become a mainstream and well-suited approach for solving many linguistic data gathering problems such as sense inventory creation [1], corpus annotation [2], information extraction [3], etc.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "Crowdsourcing has become a mainstream and well-suited approach for solving many linguistic data gathering problems such as sense inventory creation [1], corpus annotation [2], information extraction [3], etc.",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 2,
      "context" : "Crowdsourcing has become a mainstream and well-suited approach for solving many linguistic data gathering problems such as sense inventory creation [1], corpus annotation [2], information extraction [3], etc.",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 3,
      "context" : "Quinn & Bederson in their technical report [4] proposed the term of distributed human computation along with the taxonomy of seven different genres of these computations such as games with a purpose, mechanized labor, wisdom of crowds, crowdsourcing, dual-purpose work, grand search, human-based genetic algorithms, and knowledge collection from volunteer contributions.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "also presented [5] another taxonomy of five crowdsourcing genres: initiatory human computation, distributed human computation, social game-based human computation with volunteers, paid engineers and online players, which is similar to the previously mentioned.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "study of correlation between crowdsourcing genres [6], quality assessment [7], and guidelines on corpus annotation through crowdsourcing [2] align various best practices among the established genres.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "study of correlation between crowdsourcing genres [6], quality assessment [7], and guidelines on corpus annotation through crowdsourcing [2] align various best practices among the established genres.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "study of correlation between crowdsourcing genres [6], quality assessment [7], and guidelines on corpus annotation through crowdsourcing [2] align various best practices among the established genres.",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "Zwass investigated the phenomena of co-creation [8] and proposed a taxonomy of user-created digital content which includes the following: knowledge compendia, consumer reviews, multimedia content, blogs, mashups, virtual worlds.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "Erickson presented four quadrant model [9] composed of two orthogonal dichotomies to classify crowdsourcing projects: “same place–different places” and “same time–different times”.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "This is the case of the research conducted by Suendermann & Pieraccini [10], which introduces a concept of private crowd being a trade-off between two extremes: an inexpensive, highly available yet uncontrolled public crowd such as the Amazon’s one, and the",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "The mentioned work emphasizes three intuitive and well-separated genres of crowdsourcing [11]:",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "An early study of Wikipedia and its quality by Wilkinson & Huberman [13] found a statistically significant correlation between page edits, talkpage conversations and the quality of these pages.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "A study by Arazy & Nov [14] pays a special attention to local inequality — inequality of editors’ contribution in a particular article, and global inequality — inequality in overall Wikipedia activity for the same set of editors.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "[15] consider several cases of multilingual digital libraries and their collaboration both with state institutions and crowdsourced projects in order to provide multilingual information access for users.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Ranj Bar & Maheswaran [16] in their case study on Wikipedia concluded that new mechanisms are needed to coordinate the activities in crowdsourcing due to the fact that high quality articles are controlled by small groups of permanent editors, and supporting these articles is a huge burden for the editors.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : "Authors of Phrase Detectives say that the cost of data gathering using their means is lower than using other approaches [17], but they did not consider the",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "The above mentioned study by Arazy & Nov also touches upon a typical regulation problem called “edit warring” in Wikipedia [14], when “editors who disagree about the content of a page repeatedly override each other’s contributions, rather than trying to resolve the disagreement through discussion”.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "al [18].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "Such a problem may be partially resolved by using the controlled crowd instead of the public one when volunteers have a mentor and responsibility for their actions [19].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 19,
      "context" : "An evidence that cooperation does work and really stimulates participants to do more assignments is the case of OpenCorpora, which is a project focused on creation of a large annotated Russian corpus through crowdsourcing [20].",
      "startOffset" : 222,
      "endOffset" : 226
    } ],
    "year" : 2017,
    "abstractText" : "Linguistic resources can be populated with data through the use of such approaches as crowdsourcing and gamification when motivated people are involved. However, current crowdsourcing genre taxonomies lack the concept of cooperation, which is the principal element of modern video games and may potentially drive the annotators’ interest. This survey on crowdsourcing taxonomies and cooperation in linguistic resources provides recommendations on using cooperation in existent genres of crowdsourcing and an evidence of the efficiency of cooperation using a popular Russian linguistic resource created through crowdsourcing as an example.",
    "creator" : "TeX"
  }
}