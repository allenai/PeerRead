{
  "name" : "1708.09032.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Plausibility and probability in deductive reasoning",
    "authors" : [ "Andrew MacFie" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 8.\n09 03\n2v 1\n[ cs\n.A I]\n2 9\nA ug\n2 01\n7\nContents"
    }, {
      "heading" : "1 A natural problem: Quantified deductive uncertainty 2",
      "text" : "1.1 The phenomenon . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 The (modeling) problem . . . . . . . . . . . . . . . . . . . . . 3"
    }, {
      "heading" : "2 Solutions 4",
      "text" : "2.1 Formal representation of plausibilities . . . . . . . . . . . . . . 4\n2.1.1 Plausibility functions . . . . . . . . . . . . . . . . . . . 4 2.1.2 Languages . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.1.3 Epistemic quality vs. computation costs . . . . . . . . 5 2.1.4 Conditional plausibility . . . . . . . . . . . . . . . . . . . 7\n2.2 Rational plausibilities . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2.1 On bets and scoring rules . . . . . . . . . . . . . . . . . 7 2.2.2 Epistemic quality . . . . . . . . . . . . . . . . . . . . . 8"
    }, {
      "heading" : "3 (Potential) applications and extensions 11",
      "text" : "3.1 Foundations of “semi-rigorous proofs” . . . . . . . . . . . . . . . 11 3.2 Logical omniscience in Bayesian epistemology . . . . . . . . . 12 3.3 Decision theory . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nReferences 13\n∗School of Mathematics and Statistics, Carleton University, Ottawa, Canada"
    }, {
      "heading" : "1 A natural problem: Quantified deductive",
      "text" : "uncertainty"
    }, {
      "heading" : "1.1 The phenomenon",
      "text" : "Epistemic uncertainty is usually defined as uncertainty among physical states due to lack of data or information. By information we mean facts which we observe from the external world. For example, whether it rains tomorrow is a piece of information we have not observed, so we are uncertain about its truth value. However, we also may have uncertainty about purely deductive statements, which are completely determined by the information we have, due to limited reasoning ability. That is, before we have proved or refuted a mathematical statement, we have some deductive uncertainty about whether there is a proof or refutation.\nUnder deductive uncertainty, there is a familiar process of appraising a degree of belief one way or the other, saying a statement has high or low plausibility. We may express rough confidence levels in notable open conjectures such as P 6= NP [21] or the Goldbach conjecture, and we also deal with plausibility in everyday mathematical reasoning. Sometimes general patterns show up across problems and we extrapolate them to new ones. If we have an algorithm and are told it runs in time O(n lgn), we usually assume that this implies good practical performance because this is a commonly observed co-occurrence. So the plausibility of the running time being 1010!n⌈lg n⌉ is considered particularly low. Any mathematical result seen as “surprising” must have been a priori implausible. Et cetera. Many more examples of plausibility in mathematics may be found in [28, 33].\nIn some instances it may be natural to quantify deductive uncertainty, and perhaps speak of “probabilities”. For example, let d be the 10100th decimal digit of π. If we have not computed d and all we know is that, say, d is odd, it feels like d has a uniform probability distribution over {1, 3, 5, 7, 9}. Mazur [28, Sec. 2] would describe this as an application of the principle of insufficient reason. We use the same “symmetry” argument to state the probability that a given number n is prime via the prime number theorem or Fermat primality test. Probabilities also show up in enumerative induction, where confidence in a universal quantification increases as individual instances are verified. The four color theorem is one of many cases where only positive instances could be found and eventually a proof was given. Furthermore, to this theorem and other similar claims there are relevant 0-1 laws [3] which state that the “conditional probability” of a uniform random instance being a counterexample, given that counterexamples exist, goes to 1 asymptotically. With this fact one can use individual instances to “update” a Bayesian probability on the universal statement. Bayesianism in practical mathematics has been discussed previously in [5].\nNotwithstanding the examples above, mathematicians generally leave their uncertainty unquantified. This may be due to haziness about, for example, what\na “60% chance” means, and how probability should be used, in the context of deductive reasoning. One point to emphasize is that we are referring to subjective uncertainty rather than any due to inherent “randomness” of mathematics. Of course there is nothing random about whether a proof exists of a given mathematical statement. Frege, speaking on mathematical reasoning, appears to note this lack of randomness as a problem: “the ground [is] unfavorable for induction; for here there is none of that uniformity which in other fields can give the method a high degree of reliability” [11]. However, we only require subjective uniformity in order to have an analogy to other forms of uncertainty."
    }, {
      "heading" : "1.2 The (modeling) problem",
      "text" : "Gödel mentions deductive probabilities in a discussion of empirical methods in mathematics [14]:\nIt is easy to give examples of general propositions about integers where the probability can be estimated even now. For example, the probability of the proposition which states that for each n there is at least one digit 6= 0 between the n-th and n2-th digits of the decimal expansion of π converges toward 1 as one goes on verifying it for greater and greater n.\nIn commentary, Boolos naturally asks how such probabilities would be computed [4]:\nOne may, however, be uncertain whether it makes sense to ask what the probability is of that general statement, given that it has not been falsified below n = 1000000, or to ask for which n the probability would exceed .999.\nWith Boolos, we want to know, how would subjective deductive probabilities work in general? Are there right and wrong ways to assign these probabilities? Do they even make sense? These questions have both positive and normative versions; we focus on the normative.\nBayesianism is the theory of probabilities for physical uncertainty [41]. It gives an interpretation of probability, where we take a probability space, and interpret the probability measure as assigning subjective degrees of belief to events which represent expressible physical states. Looking from the reverse direction, Bayesianism argues from the nature of physical uncertainty to reach a conclusion that degrees of belief should form a probability space. In this second sense we can think of Bayesianism as a proof, where the premise is physical uncertainty, the inferences are rationality arguments, and the conclusion is probability theory. There are two ways to make use of a proof to learn something new. First, we can apply the theorem if we are able to satisfy the premise. Here this would mean reducing deductive uncertainty to physical uncertainty by defining virtual information states. This general problem of deductive probabilities has received some attention in the literature (the\nsample space is taken to consist of complete formal theories) as we mention in later sections. But what if there is no valid way to define virtual information for deductive uncertainty, i.e. what if probability theory is not appropriate for deductive uncertainty in this manner? What if something else is? The second way to learn from a proof is to imitate the proof technique. Here we would start with the premise of deductive uncertainty, proceed using analogous but adapted rationality arguments, and reach a conclusion which is a set of mathematical rules possibly different from probability theory. We focus on this approach.\nThe first step is to fix a concrete and unambiguous way to quantify uncertainty. If we assign a number to our belief in a statement, what does that mean? And is it always possible for us to do so? In Bayesianism, uncertainty is quantified by a single real number from [0, 1] and a prominent operational definition of this quantification is based on fair bets [36, Ch. 13, 41, Sec. 2.2.1]. This operationalization appears to work equally well for deductive uncertainty. That is, anyone can express their uncertainty about a mathematical statement φ using a number in [0, 1] which encodes the betting payoffs they consider acceptable if they were to bet on φ. We call these values plausibilities. (This is not to be confused with other usages such as “plausibility measures” [18, Sec. 2.8]). We assume this operationalization is meaningful and understood; for further details and justifications see Sec. 2.2.1.\nIn the context of physical uncertainty, Bayesianism adds constraints on what plausibilities/probabilities should be for a rational agent, namely coherence (so plausibilities are probabilities; see [31] for a modern asset pricing perspective), conditionalization, regularity and other guidance for selecting priors [41]. Also, probabilistic forecasts may be rated on accuracy using loss functions [26].\nHowever, the assumptions of Bayesianism on which these constraints are based do not necessarily still apply to deductive plausibilities and indeed we may have additional or different requirements. Thus the precise question to answer is, what constraints should be put on deductive plausibilities and what mathematical structure results?"
    }, {
      "heading" : "2 Solutions",
      "text" : ""
    }, {
      "heading" : "2.1 Formal representation of plausibilities",
      "text" : ""
    }, {
      "heading" : "2.1.1 Plausibility functions",
      "text" : "Fix an encoding of deductive statements into finite strings so that there is a decision problem Π ⊆ {0, 1}∗ corresponding to the true statements. We take an association of plausibilities to encoded statements as a function p : {0, 1}∗ → [0, 1]. We call p a plausibility function. A plausibility function represents an agent’s uncertainty about Π. One could also have defined a plausibility function to take a finite sequence of input strings and return a\nfinite sequence of plausibilities, that is, working at the level of bet systems instead of bets."
    }, {
      "heading" : "2.1.2 Languages",
      "text" : "Finding proofs is a matter of computation, so our reasoning abilities are equivalent to our computational resources; and generally we will experience deductive uncertainty when faced with any intractable computational problem. Importantly, we cannot meaningfully talk about problems with only one input, since obviously the best output is the actual truth value. So we must talk of uncertainty about an entire set of inputs simultaneously.\nProbability spaces consist of a measurable space and a probability measure. In Bayesianism, the measurable space may be substituted by a “sentence space” which is closed under logical operations. In the deductive case, any nontrivial problem Π has an input set that is trivially closed under logical operations, since any input is logically equivalent to “true” or “false”. We conclude that the problem Π need not have any particular syntactic structure and we may consider standard problems from theoretical computer science.\nThere is a line of research on deductive uncertainty where the inputs come from a first-order language. Typically this work aims at finding composites of logic and probability theory, and there is less focus on practicality. The most recent work is by Garrabrant et al. [13] and [13, Sec. 1.2] reviews previous literature. In the present work we instead restrict our attention to decidable problems. We do this because inconsistent logics do not make sense in terms of betting. So first-order logics are problematic due to Gödel’s first and second incompleteness theorems."
    }, {
      "heading" : "2.1.3 Epistemic quality vs. computation costs",
      "text" : "For a given problem Π ⊆ {0, 1}∗ we seek an epistemic improvement relation ≺Π on plausibility functions, where q ≺Π p iff p is a strictly better uncertainty assignment for Π than q, ignoring any computational costs of the functions. For example, if we decide to require regularity, we would say that for all functions p and q, if p is regular and q is not then q ≺Π p. Guidance for selecting plausibility functions is then based on weighing ≺Π against computational costs. If we are given a probability distribution on inputs, we take the distributional decision problem (Π,D) and consider a distribution-specific relation ≺(Π,D). We refer to the relation as an order but it is not necessarily total.\nImprovement relations can be found in other modeling contexts. For algorithm running time sequences we use asymptotic relations such as big O or polynomial growth vs. non-polynomial growth. Fine-grained relations may be used if the model of computation is fixed. The Nash equilibrium represents a kind of ordering which expresses that a strategy profile can be improved from one player’s perspective. Pareto optimality is similar but for group rationality.\nAnother example is the Marshall/Kaldor-Hicks social welfare improvement relation in economics [9, 12]. This last relation is useful even though it cannot be defined to be both transitive and antisymmetric.\nIn general we have a tradeoff between epistemic quality (whatever we determine that to be) and computational complexity. A theory of deductive uncertainty must not only define gradients of epistemic quality but dictate how to make this tradeoff. If we allow arbitrary computations, the problem immediately disappears. E.g. there is a temptation to look into Solomonoff induction [27] as a model of inductive reasoning applied to mathematical knowledge. This would be an attempt to formalize, e.g. Pólya’s patterns of plausible reasoning [28, 33], such as “A analogous to B, B more credible =⇒ A somewhat more credible”. However we must be cautious, because an incomputable prior cannot be the correct tradeoff between quality and efficiency.\nComputation costs may or may not be measured asymptotically. Asymptotic means no finite set of inputs can make a difference. If we use asymptotic complexity this forces us to define ≺Π so that it is compatible, i.e. also asymptotic. As an example, utility in game theory is generally not compatible with asymptotic computation costs. There are, however, game models which trade off running time and other considerations in a non-trivial way, for example using discounted utility. In economics and game theory, the concept of “bounded rationality” refers to decision making with limitations on reasoning/optimization power, such as imperfect recall, time constraints, etc. [35]. We note some economic models which incorporate bounded computation: game-playing Turing machines with bounded state set [29], automata models [32], machines as strategies and utility affected by computation cost [10, 19], information assymetry in finance [1]. Rubinstein [35] said in 1998, “I have the impression that many of us feel that the attempts to model bounded rationality have yet to find the right track.” Perhaps progress has been made since then. If a game model uses a practically awkward criterion for algorithm performance, simple models of computation may be used, or equilibria may be reasoned about without analyzing a particular algorithm.\nA simple approach to the tradeoff is to fix a resource bound and consider as “feasible” only functions that can be computed within the bound. Then, given ≺Π, we optimize over the subset of feasible plausibility functions. This is the method we focus on in the remainder. E.g. we may assume the CobhamEdmonds thesis and consider ≺Π restricted to polynomial-time-computable functions.\nWe make the assumption that we, as modelers, are always capable of analyzing given plausibility functions however is necessary to evaluate ≺Π and analyze computational complexity. This is of course not true, as discussed in [2, 30] which consider bounded rationality in economic systems. However this is a worthy assumption since building meta-uncertainty into the model creates a regress which would add significant complexity. Thus we can say that optimizing ≺Π is the rational way to select a plausibility function even if we are not currently able to do so constructively. Particularly, when we ana-\nlyze functions according to an input distribution, the business of defining the distribution is that of the unbounded analyst. In practice, e.g. approximation algorithms are analyzed, even if the problem they attempt to approximate is intractable."
    }, {
      "heading" : "2.1.4 Conditional plausibility",
      "text" : "If we select plausibility functions by optimizing ≺Π over feasible functions, the definition of feasibility could change over time or simply vary across contexts, so in general we speak of conditional plausibility functions p(·|S), where S is an oracle or complexity class or other representation of the resources available to compute the function. Another interpretation is that, over time, computation costs may effectively come down, enlarging the budget set of feasible functions. This notation and terminology indicate an analogy where knowledge, in the computational sense (roughly that of [15, Sec. 9.2.3, 16, Sec. 7.2]), takes the place of information.\nIn full generality, an agent may have an internal state which affects the plausibility function computed. Specifically, over the life of the agent it may perform background computations which add to a growing “knowledge base” so that later plausibility functions are informed by a greater knowledge base than previous ones. The background processes themselves are outside the scope of our analysis; their performance is determined by such things as expected times when queries arrive which would add complication. However, if the background processes are assumed to run for a long time, they may generate knowledge inaccessible to an efficient algorithm serving a query. Different knowledge may be (pre-)computed at different times, i.e. if all processes are dovetailed and have different sequences of times when a certain input length is ready. Alternatively, an agent may gain access to an external oracle. We assume that any distribution on inputs stays fixed.\nIn Bayesianism, conditionalization is the process by which updates on new information must occur. I.e. after observing A, our new probability of B is P (B|A) = P (A ∩ B)/P (A). We note that conditionalization is a uniform process in that there is a finite rule that performs updates for all events A at all times. If there is an infinite set of possible S, we could restrict to uniform-updating plausibility functions, i.e. those which take a representation of S as a parameter. In, for example, Garrabrant’s model [13], the plausibility function takes an additional parameter n, which is the number of stages to run. However this level of analysis is beyond our scope."
    }, {
      "heading" : "2.2 Rational plausibilities",
      "text" : ""
    }, {
      "heading" : "2.2.1 On bets and scoring rules",
      "text" : "We give some further clarifying comments on betting situations. With fair bets we implicitly assume linear utility in money but we can discharge the\nassumption: First find the agent’s utility of money with lotteries then scale units of money to have a linear relationship with utility. Bet decisions have to include the chance to buy, sell short or do nothing; only choosing between buy and nothing allows p ≡ 0 to mean no bets at all. Wagers must be finite, otherwise analysis is trivial. It is possible that a bet event itself includes new knowledge on which we should update before betting. This would be the case if the party offering the bet is known to have access to a powerful oracle. For operationalization purposes we consider situations where we do not update on the bet itself; for example, if the offering party is known to be stupid or not selfinterested. Bets on mathematical facts may not have predetermined resolution times. For some propositions neither a positive nor a negative resolution are likely in a reasonable timeframe. And when the truth finally does come out, an agent’s circumstances will have changed. Agents will put at least some probability on resolution by any given time, so wagers can be scaled up so that agents at least consider it worth their time to make a bet.\nOther methods of eliciting quantified uncertainty are equivalent to betting. For example, fair bet odds are equivalent to asking “what is an objective chance p̄ such that your uncertainty of this event is equivalent to that of an event with objective chance p̄?”. The betting elicitation is also equivalent to strictly proper scoring rules, which go back to de Finetti and beyond [6]. This refers to a situation where the agent is asked for a number x ∈ [0, 1] representing uncertainty about a proposition φ, and then the agent receives a score of B([φ], x). Here we use Iverson brackets where [φ] is 1 if φ is true, and 0 otherwise. (The word “score” is a bit of a misnomer because lower scores are better.) The scoring function B is strictly proper iff\nargmin x yB(1, x) + (1− y)B(0, x) = y,\ni.e. if [φ] is a Bernoulli(y)-distributed random variable, we obtain the optimal expected score by choosing x = E[φ] = y. Situations in which proper scoring rules are being applied are subject to considerations similar to those above for betting. We note that with both betting and scoring, the agent is presented with a simple decision where the only computational difficulty comes from the one instance of some decision problem. This allows us to conclude that their equivalence holds in the computational setting as well."
    }, {
      "heading" : "2.2.2 Epistemic quality",
      "text" : "It is desirable to be calibrated in terms of frequencies, which means\n|{ω : ω ∈ Π ∩ Ω, p(ω) ≈ c}|\n|{ω : ω ∈ Ω, p(ω) ≈ c}| ≈ c,\nfor all c ∈ [0, 1], where Ω is some set of inputs. We would expect this, for example, if plausibilities are compatible with objective chances. More generally, let D = (Dn) be a distribution ensemble and let F be a set of “simple” functions. If the members of F are easy to compute, our plausibilities should make use\nof any knowledge contained in these simple functions. We say a plausibility function p is calibrated with respect to F if for all f ∈ F ,\n|CovDn(f, 1Π)− CovDn(f, p)| ≤ ǫ,\nfor small ǫ. If p fails to be calibrated for some f , then we are leaving easy knowledge on the table. Formally we could say if p is calibrated and q is not, then q ≺(Π,D) p.\nBeyond calibration, we look at utility considerations. Since plausibilities encode how we make simple decisions, more desirable plausibility values are those that lead to better outcomes from decisions. Dutch book arguments justifying Bayesianism are essentially saying if you can avoid losing money in a bet, you should. On the other hand, scoring rules are generally considered to index epistemic quality. In fact, the concepts of betting and scoring rules are essentially the same. It is shown in [34] that Dutch books exist iff an agent’s forecasts are strictly dominated, where domination means there is another plausibility assignment that obtains a better score in all outcomes, according to a continuous proper scoring rule. Conditionalization also follows from optimizing a proper scoring rule. We take this scoring rule characterization of Bayesianism (which led to probability theory in that case) and apply it to deductive plausibilities via analysis of plausibility functions.\nProper scoring rules conveniently associate a real number to each of our plausibilities. There are three possible approaches for chosing a rule. First, we may simply say q ≺Π p if p performs no worse than q according to all proper scoring rules, and p performs strictly better according to at least one; or alternatively, if p performs strictly better for all proper scoring rules. Second, we may put a distribution on proper scoring rules, and consider the expected score over all rules. Third, we may use a single rule (technically a degenerate distribution). The third option is simplest, although there are various proper scoring rules and they may generally give different results. The Brier score has some desirable properties [37]. The logarithmic score also has desirable properties and is closely related to information-theoretic entropy. However, given any proper scoring rule, one can always construct a decision environment such that performance in the environment is precisely performance according to the scoring rule.\nWorst-case scoring of plausibility functions leads to trivialities since p ≡ 1/2 is optimal unless the problem can be exactly solved. An alternative in some cases would be to consider inputs of length ≤ n rather than n. We focus on the standard practice of considering inputs of the same length, but we employ average-case analysis, i.e. using expected scores. We break into two cases depending on whether the environment (input distribution) is fixed or may be a function of p. We say arbitrage of the first type exists if p scores suboptimally for a fixed input distribution. Arbitrage of the second type exists if there is some computationally bounded agent which looks at p, then generates an input distribution and scores better on that distribution than p does.\nIf inputs are distributed according to an ensemble D, we may say that q ≺(Π,D) p if the expected score of p is less than that of q. This is the model used in\n[25]. A very similar view is considering p as an unnormalized distribution on strings of length n, and finding the statistical distance between the normalized distribution and the normalized distribution corresponding to 1Π. There we would require at least that E(p) = E(1Π).\nIf the input distribution is not fixed, we are in a situation where an adversary may chose the distribution, based on p. If I prove that u ∈ Π =⇒ v ∈ Π but p(u) > p(v) I have found an inconsistency but also if I know w ∈ Π and p(w) = 0, this is an inconsistency as well. Adversaries may exploit both kinds of inconsistencies if they can find the flawed plausibilities, dominate them, and produce an input distribution which highly weights those inputs. Technically, if we know we are at risk to be exploited by an adversary offering a bet, we will update rather than bet. But we want to do this as little as possible. We still want to maximize the situations where we do bet, because those are the ones where we stand to win.\nWe focus on worst-case adversaries because these are the ones most likely to “find” us (thinking of zero-sum games). We must talk about bets asymptotically since there will always be individual bets that we lose. “Bets” here are the adversary selling us the difference between our score and the adversary’s score, on the input distribution generated by the adversary. For each input length n, the bettor takes as input 1n and outputs the bet. An “implicit” adversarial bettor would randomly generate a set of inputs and the bettor’s plausibilities for those inputs. An “explicit” bettor deterministically outputs a small set of inputs, weights on those inputs, and the bettor’s plausibilities for the inputs. Small sets of weighted inputs could be made into a single bet, where we are sold the difference of expected scores. The adversary’s output distribution, or the weights, determine D.\nWe can consider the case of an infinite loss over infinite time, or finite loss in finite time. Fix a scoring rule B. For infinite loss, a function p is exploited by a complexity class C if there is a Cp-samplable distribution D and Cpcomputable plausibility function pC such that\n∑\nn\nEDnB(1Π, p)−EDnB(1Π, pC) = ∞\nand the terms are eventually nonnegative. Here Cp-samplable and Cpcomputable mean samplable/computable by a machine with resources associated with a complexity class C, and oracle access to p. So we say q ≺Π p if for some complexity class C, the plausibility function q is exploited by C and p is not. In the case of finite loss, we say a function p is exploited by a complexity class C if there is a Cp-samplable distribution D and Cp-computable plausibility function pC such that\nEDnB(1Π, p) ≫ EDnB(1Π, pC), n → ∞."
    }, {
      "heading" : "3 (Potential) applications and extensions",
      "text" : ""
    }, {
      "heading" : "3.1 Foundations of “semi-rigorous proofs”",
      "text" : "Zeilberger presented the concept of a “semi-rigorous proof” and predicted that it might become acceptable by the mathematical community: “I can envision an abstract of a paper, c. 2100, that reads: ‘We show, in a certain precise sense, that the Goldbach conjecture is true with probability larger than 0.99999’” [42]. In order for such a result to be useful, there must be cases where plausibilities are objective.\nAre plausibilities objective or subjective? Should we expect people to agree on plausibilities? There are various sources of subjectivity: how to embed individual questions in problems, first-order logic issues, and problems Π which are computationally hard to estimate well (these are readily constructible [17]) and/or where ≺Π has no unique optimum. For example, take Gödel’s π problem from Sec. 1.2: Let φ represent the sentence, for each n there is at least one digit 6= 0 between the n-th and n2-th digits of the decimal expansion of π. First, there may not be a proof or refutation of φ, in which case betting outcomes are undefined. Second, if φ is decideable, its truth value is a single bit, so the optimal plausibility is simply the truth value. We would need to embed φ in a class of problem instances to avoid this triviality, and there are different ways of doing so. If the problem is too easy, the answer is still trivial. If it is hard, we do not learn much from the answer. To condition on observed digits of π, we can allow access to an oracle that checks φ for large ranges of the digits of π, but again we have to define the oracle not to be too weak or too powerful.\nWe stated at the end of Sec. 1.1 that mathematicians may be uncomfortable with putting too much focus on plausibilities. Gödel says, “I admit that every mathematician has an inborn abhorrence to giving more than heuristic significance to such inductive arguments” [14]. Also, Corfield notes, “Pólya himself had the intuition that two mathematicians with apparently similar expertise in a field might have different degrees of belief in the truth of a result and treat evidence for that result differently” [5]. However, the physics community has had notable success using non-rigorous mathematical methods.\nOne practical issue with publishing probabilities for mathematical problems is error amplification. If we take the conjunction of two “independent” uncertain statements we end up with uncertainty greater than that of either of the original statements, which means confidence erodes over time. In mathematics this is undesirable since we are used to taking arbitrarily long sequences of conjunctions and implications with no loss of validity.\nMore on probabilistic proofs is found in [7]. The potential for models of uncertainty in mathematics to explain the use of large computations to increase confidence in conjectures is noted in [5]."
    }, {
      "heading" : "3.2 Logical omniscience in Bayesian epistemology",
      "text" : "Even within traditional Bayesianism, the assumption of computational unboundedness can be undesirable; this is known as the problem of logical omniscience [20, 22, 40]. Some work has been done on formal models for logical nonomniscience, including a resource-bounded AIXI [23] and resource-bounded Solomonoff prior [27]. Even approximate inference in Bayesian networks is NP-hard although there is research on methods such as belief propagation which succeed in special cases. The general problem remains, at least in the following form. Solomonoff’s recursion-theoretic notion of extrapolation [24, 27] gives an ideal family of “universal” prior distributions m suitable for general agents. However, it lacks a useful improvement relation on approximations since there is only an ideal, which is incomputable, and no guidance on how to approximate it optimally. This sort of problem is why Ben Goertzel said in 2011, “the theory of rationality under limited computing resources is just not that clear”.\nPerhaps we can generalize some of the above modeling ideas to get a concept of optimal feasible approximation to a probability measure. In the above we could view our task as approximating trivial probability measures, with 0 for false events, 1 for true, and then consider extending to other measures such as m. We would have the rule: The epistemically normative feasible measure is the optimal feasible approximation to the normative infeasible measure. For measures such as m which are defined on {0, 1}∗, we may define normalized probability distributions mn on strings of length n, or Bernoulli distributions for each individual input. For each n, we can compare mn or m and the approximating function using statistical distance to get a real sequence of error values that can be used to define the quality of the approximation. This would involve generalizing from proper scoring rules to, for example, the KullbackLeibler divergence. In the case of comparing distributions on {0, 1}n we at least require the approximating function to define a proper probability distribution for each n to avoid trivialities."
    }, {
      "heading" : "3.3 Decision theory",
      "text" : "Our approach has been to focus on an epistemic problem and simple decision contexts, and not consider more complex decision making. It is not clear what relationship plausibilities should have with a more general decision theory. Also, in either physical or deductive uncertainty, expected utility maximization must be modified in the case of bounded agents because tasks involved other than plausibility assignment can be computationally hard as well. The usual assumption of consistency allows simple decisions about bets and utilities to uniquely determine arbitrarily complex decisions. In the face of computational problems which can only be approximated by an actual agent, such as integration or optimization, this kind of consistency is infeasible. Thus lifting a theory of bets to a theory of general decisions involves additionally considering computational solutions to the various problems that are involved in general\ndecisions. Our approach here may already include important pieces of and concepts for a more general theory.\nModels of deductive uncertainty based on formal logic have been connected to applications in highly reliable artificial intelligence [38], updateless decision theory, reflective oracles [8], and logical counterfactuals [39]. It is a possibility that the present models have uses there too.\nAcknowledgements. I thank Zhicheng Gao and Corey Yanofsky for useful pointers, and Boaz Barak and Vadim Kosoy for crucial extended discussions."
    } ],
    "references" : [ {
      "title" : "Computational complexity and information asymmetry in financial products",
      "author" : [ "S. Arora" ],
      "venue" : "ICS",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Musings on information and knowledge",
      "author" : [ "R.J. Aumann" ],
      "venue" : "Econ Journal Watch",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Submaps of maps. I. General 0–1 laws",
      "author" : [ "E.A. Bender", "Z.-C. Gao", "L.B. Richmond" ],
      "venue" : "Journal of Combinatorial Theory, Series B",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1992
    }, {
      "title" : "Introductory note to *1951",
      "author" : [ "G. Boolos" ],
      "venue" : "Kurt Gödel Collected Works,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "Towards a Philosophy of Real Mathematics",
      "author" : [ "D. Corfield" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "The role of ‘Dutch books’ and of ‘proper scoring rules’",
      "author" : [ "B. de Finetti" ],
      "venue" : "The British Journal for the Philosophy of Science",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1981
    }, {
      "title" : "Probabilistic Proofs and Transferability",
      "author" : [ "K. Easwaran" ],
      "venue" : "Philosophia Mathematica",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Reflective oracles: A foundation for game theory in artificial intelligence",
      "author" : [ "B. Fallenstein", "J. Taylor", "P.F. Christiano" ],
      "venue" : "In: International Workshop on Logic, Rationality and Interaction. Springer",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Kaldor-Hicks Compensation",
      "author" : [ "A.M. Feldman" ],
      "venue" : "The New Palgrave Dictionary of Economics and the Law",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Bounding rationality by discounting time",
      "author" : [ "L. Fortnow", "R. Santhanam" ],
      "venue" : "arXiv preprint arXiv:0911.3162",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "The foundations of arithmetic. Die Grundlagen der Arithmetik. A logicomathematical enquiry into the concept of number. Eine logisch mathematische Untersuchung über den Begriff der Zahl, Containing a 1974 translation by J. L. Austin and a reprinting of the 1884 German original, Third reprinting of the second revised edition",
      "author" : [ "G. Frege" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1953
    }, {
      "title" : "Price Theory: An Intermediate Text",
      "author" : [ "D. Friedman" ],
      "venue" : "South-Western,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1986
    }, {
      "title" : "Logical Induction",
      "author" : [ "S. Garrabrant" ],
      "venue" : "CoRR abs/1609.03543",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Some basic theorems on the foundations of mathematics and their implications (*1951)",
      "author" : [ "K. Gödel" ],
      "venue" : "Kurt Gödel Collected Works,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1995
    }, {
      "title" : "Computational complexity. A conceptual perspective",
      "author" : [ "O. Goldreich" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "The knowledge complexity of interactive proof systems",
      "author" : [ "S. Goldwasser", "S. Micali", "C. Rackoff" ],
      "venue" : "SIAM Journal on computing",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1989
    }, {
      "title" : "If NP languages are hard on the worstcase, then it is easy to find their hard instances",
      "author" : [ "D. Gutfreund", "R. Shaltiel", "A. Ta-Shma" ],
      "venue" : "Computational Complexity",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Reasoning about uncertainty",
      "author" : [ "J.Y. Halpern" ],
      "venue" : "MIT press,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Algorithmic rationality: Game theory with costly computation",
      "author" : [ "J.Y. Halpern", "R. Pass" ],
      "venue" : "Journal of Economic Theory",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Dealing with logical omniscience: Expressiveness and pragmatics",
      "author" : [ "J.Y. Halpern", "R. Pucella" ],
      "venue" : "Artificial intelligence",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Complexity Theory Column 36",
      "author" : [ "L.A. Hemaspaandra" ],
      "venue" : "SIGACT News 33.2",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "Epistemic Logic",
      "author" : [ "V. Hendricks", "J. Symons" ],
      "venue" : "The Stanford Encyclopedia of Philosophy",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability",
      "author" : [ "M. Hutter" ],
      "venue" : "pages, http://www.hutter1.net/ai/uaibook.htm. Berlin: Springer,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "No better ways to generate hard NP instances than picking uniformly at random",
      "author" : [ "R. Impagliazzo", "L. LA" ],
      "venue" : "Foundations of Computer Science,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1990
    }, {
      "title" : "Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm",
      "author" : [ "V. Kosoy" ],
      "venue" : "CoRR abs/1608.04112",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Evaluating probability forecasts",
      "author" : [ "T.L. Lai", "S.T. Gross", "D.B. Shen" ],
      "venue" : "The Annals of Statistics",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "An introduction to Kolmogorov Complexity and its Applications",
      "author" : [ "M. Li", "P. Vitányi" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "Is it plausible?",
      "author" : [ "B. Mazur" ],
      "venue" : "Mathematical Intelligencer",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "On play by means of computing machines: preliminary version",
      "author" : [ "N. Megiddo", "A. Wigderson" ],
      "venue" : "Proceedings of the 1986 Conference on Theoretical aspects of reasoning about knowledge",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1986
    }, {
      "title" : "Methodological Foundations for Bounded Rationality as a Primary Framework",
      "author" : [ "S. Modarres-Mousavi" ],
      "venue" : "PhD thesis. Virginia Tech,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2002
    }, {
      "title" : "De Finetti was right: probability does not exist",
      "author" : [ "R.F. Nau" ],
      "venue" : "In: Theory and Decision",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2001
    }, {
      "title" : "On Complexity As Bounded Rationality (Extended Abstract)",
      "author" : [ "C.H. Papadimitriou", "M. Yannakakis" ],
      "venue" : "Proceedings of the Twenty-sixth Annual ACM Symposium on Theory of Computing. STOC ’94",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1994
    }, {
      "title" : "Mathematics and Plausible Reasoning: Patterns of Plausible Inference",
      "author" : [ "G. Pólya" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1968
    }, {
      "title" : "Probabilistic coherence and proper scoring rules",
      "author" : [ "J.B. Predd" ],
      "venue" : "IEEE Transactions on Information Theory",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2009
    }, {
      "title" : "Modeling Bounded Rationality",
      "author" : [ "A. Rubinstein" ],
      "venue" : "MIT press,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1998
    }, {
      "title" : "Artificial Intelligence: A Modern Approach. 3rd",
      "author" : [ "S. Russell", "P. Norvig" ],
      "venue" : "Upper Saddle River, NJ,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2009
    }, {
      "title" : "Axiomatic characterization of the quadratic scoring rule",
      "author" : [ "R. Selten" ],
      "venue" : "Experimental Economics",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1998
    }, {
      "title" : "Aligning Superintelligence with Human Interests: A Technical Research Agenda",
      "author" : [ "N. Soares", "B. Fallenstein" ],
      "venue" : "Tech. rep. MIRI,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2014
    }, {
      "title" : "Toward Idealized Decision Theory",
      "author" : [ "N. Soares", "B. Fallenstein" ],
      "venue" : "CoRR abs/1507.01986",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2015
    }, {
      "title" : "Bayesian Epistemology",
      "author" : [ "W. Talbott" ],
      "venue" : "The Stanford Encyclopedia of Philosophy",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2016
    }, {
      "title" : "Varieties of Bayesianism",
      "author" : [ "J. Weisberg" ],
      "venue" : "Handbook of the History of Logic",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2011
    }, {
      "title" : "Theorems for a price: tomorrow’s semi-rigorous mathematical culture",
      "author" : [ "D. Zeilberger" ],
      "venue" : "In: Notices of the American Mathematical Society",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "We may express rough confidence levels in notable open conjectures such as P 6= NP [21] or the Goldbach conjecture, and we also deal with plausibility in everyday mathematical reasoning.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : "Many more examples of plausibility in mathematics may be found in [28, 33].",
      "startOffset" : 66,
      "endOffset" : 74
    }, {
      "referenceID" : 32,
      "context" : "Many more examples of plausibility in mathematics may be found in [28, 33].",
      "startOffset" : 66,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, to this theorem and other similar claims there are relevant 0-1 laws [3] which state that the “conditional probability” of a uniform random instance being a counterexample, given that counterexamples exist, goes to 1 asymptotically.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "Bayesianism in practical mathematics has been discussed previously in [5].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "Frege, speaking on mathematical reasoning, appears to note this lack of randomness as a problem: “the ground [is] unfavorable for induction; for here there is none of that uniformity which in other fields can give the method a high degree of reliability” [11].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 13,
      "context" : "Gödel mentions deductive probabilities in a discussion of empirical methods in mathematics [14]:",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "In commentary, Boolos naturally asks how such probabilities would be computed [4]:",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 40,
      "context" : "Bayesianism is the theory of probabilities for physical uncertainty [41].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "If we assign a number to our belief in a statement, what does that mean? And is it always possible for us to do so? In Bayesianism, uncertainty is quantified by a single real number from [0, 1] and a prominent operational definition of this quantification is based on fair bets [36, Ch.",
      "startOffset" : 187,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "That is, anyone can express their uncertainty about a mathematical statement φ using a number in [0, 1] which encodes the betting payoffs they consider acceptable if they were to bet on φ.",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 30,
      "context" : "In the context of physical uncertainty, Bayesianism adds constraints on what plausibilities/probabilities should be for a rational agent, namely coherence (so plausibilities are probabilities; see [31] for a modern asset pricing perspective), conditionalization, regularity and other guidance for selecting priors [41].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 40,
      "context" : "In the context of physical uncertainty, Bayesianism adds constraints on what plausibilities/probabilities should be for a rational agent, namely coherence (so plausibilities are probabilities; see [31] for a modern asset pricing perspective), conditionalization, regularity and other guidance for selecting priors [41].",
      "startOffset" : 314,
      "endOffset" : 318
    }, {
      "referenceID" : 25,
      "context" : "Also, probabilistic forecasts may be rated on accuracy using loss functions [26].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "We take an association of plausibilities to encoded statements as a function p : {0, 1}∗ → [0, 1].",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "[13] and [13, Sec.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "Another example is the Marshall/Kaldor-Hicks social welfare improvement relation in economics [9, 12].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "Another example is the Marshall/Kaldor-Hicks social welfare improvement relation in economics [9, 12].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 26,
      "context" : "there is a temptation to look into Solomonoff induction [27] as a model of inductive reasoning applied to mathematical knowledge.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 27,
      "context" : "Pólya’s patterns of plausible reasoning [28, 33], such as “A analogous to B, B more credible =⇒ A somewhat more credible”.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 32,
      "context" : "Pólya’s patterns of plausible reasoning [28, 33], such as “A analogous to B, B more credible =⇒ A somewhat more credible”.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 34,
      "context" : "[35].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "We note some economic models which incorporate bounded computation: game-playing Turing machines with bounded state set [29], automata models [32], machines as strategies and utility affected by computation cost [10, 19], information assymetry in finance [1].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 31,
      "context" : "We note some economic models which incorporate bounded computation: game-playing Turing machines with bounded state set [29], automata models [32], machines as strategies and utility affected by computation cost [10, 19], information assymetry in finance [1].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "We note some economic models which incorporate bounded computation: game-playing Turing machines with bounded state set [29], automata models [32], machines as strategies and utility affected by computation cost [10, 19], information assymetry in finance [1].",
      "startOffset" : 212,
      "endOffset" : 220
    }, {
      "referenceID" : 18,
      "context" : "We note some economic models which incorporate bounded computation: game-playing Turing machines with bounded state set [29], automata models [32], machines as strategies and utility affected by computation cost [10, 19], information assymetry in finance [1].",
      "startOffset" : 212,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : "We note some economic models which incorporate bounded computation: game-playing Turing machines with bounded state set [29], automata models [32], machines as strategies and utility affected by computation cost [10, 19], information assymetry in finance [1].",
      "startOffset" : 255,
      "endOffset" : 258
    }, {
      "referenceID" : 34,
      "context" : "Rubinstein [35] said in 1998, “I have the impression that many of us feel that the attempts to model bounded rationality have yet to find the right track.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "This is of course not true, as discussed in [2, 30] which consider bounded rationality in economic systems.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : "This is of course not true, as discussed in [2, 30] which consider bounded rationality in economic systems.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : "In, for example, Garrabrant’s model [13], the plausibility function takes an additional parameter n, which is the number of stages to run.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "The betting elicitation is also equivalent to strictly proper scoring rules, which go back to de Finetti and beyond [6].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "This refers to a situation where the agent is asked for a number x ∈ [0, 1] representing uncertainty about a proposition φ, and then the agent receives a score of B([φ], x).",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "for all c ∈ [0, 1], where Ω is some set of inputs.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 33,
      "context" : "It is shown in [34] that Dutch books exist iff an agent’s forecasts are strictly dominated, where domination means there is another plausibility assignment that obtains a better score in all outcomes, according to a continuous proper scoring rule.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 36,
      "context" : "The Brier score has some desirable properties [37].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 24,
      "context" : "[25].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "99999’” [42].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 16,
      "context" : "Are plausibilities objective or subjective? Should we expect people to agree on plausibilities? There are various sources of subjectivity: how to embed individual questions in problems, first-order logic issues, and problems Π which are computationally hard to estimate well (these are readily constructible [17]) and/or where ≺Π has no unique optimum.",
      "startOffset" : 308,
      "endOffset" : 312
    }, {
      "referenceID" : 13,
      "context" : "Gödel says, “I admit that every mathematician has an inborn abhorrence to giving more than heuristic significance to such inductive arguments” [14].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Also, Corfield notes, “Pólya himself had the intuition that two mathematicians with apparently similar expertise in a field might have different degrees of belief in the truth of a result and treat evidence for that result differently” [5].",
      "startOffset" : 236,
      "endOffset" : 239
    }, {
      "referenceID" : 6,
      "context" : "More on probabilistic proofs is found in [7].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "The potential for models of uncertainty in mathematics to explain the use of large computations to increase confidence in conjectures is noted in [5].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "Even within traditional Bayesianism, the assumption of computational unboundedness can be undesirable; this is known as the problem of logical omniscience [20, 22, 40].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "Even within traditional Bayesianism, the assumption of computational unboundedness can be undesirable; this is known as the problem of logical omniscience [20, 22, 40].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 39,
      "context" : "Even within traditional Bayesianism, the assumption of computational unboundedness can be undesirable; this is known as the problem of logical omniscience [20, 22, 40].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "Some work has been done on formal models for logical nonomniscience, including a resource-bounded AIXI [23] and resource-bounded Solomonoff prior [27].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 26,
      "context" : "Some work has been done on formal models for logical nonomniscience, including a resource-bounded AIXI [23] and resource-bounded Solomonoff prior [27].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 23,
      "context" : "Solomonoff’s recursion-theoretic notion of extrapolation [24, 27] gives an ideal family of “universal” prior distributions m suitable for general agents.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 26,
      "context" : "Solomonoff’s recursion-theoretic notion of extrapolation [24, 27] gives an ideal family of “universal” prior distributions m suitable for general agents.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 37,
      "context" : "Models of deductive uncertainty based on formal logic have been connected to applications in highly reliable artificial intelligence [38], updateless decision theory, reflective oracles [8], and logical counterfactuals [39].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "Models of deductive uncertainty based on formal logic have been connected to applications in highly reliable artificial intelligence [38], updateless decision theory, reflective oracles [8], and logical counterfactuals [39].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 38,
      "context" : "Models of deductive uncertainty based on formal logic have been connected to applications in highly reliable artificial intelligence [38], updateless decision theory, reflective oracles [8], and logical counterfactuals [39].",
      "startOffset" : 219,
      "endOffset" : 223
    } ],
    "year" : 2017,
    "abstractText" : "We consider the problem of rational uncertainty about unproven mathematical statements, which Gödel and others have remarked on. Using Bayesian-inspired arguments we build a normative model of fair bets under deductive uncertainty which diverges from probability theory per se but draws on the theory of algorithms. We comment on connections to Zeilberger’s notion of “semi-rigorous proofs”, particularly that inherent subjectivity is an obstacle.",
    "creator" : "LaTeX with hyperref package"
  }
}