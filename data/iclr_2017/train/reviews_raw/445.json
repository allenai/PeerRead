{"conference": "ICLR 2017 conference submission", "title": "Dialogue Learning With Human-in-the-Loop", "abstract": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes.  Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime.  Finally, real experiments with Mechanical Turk validate the approach.", "reviews": [{"is_meta_review": true, "comments": "SUMMARY: This paper describes a set of experiments evaluating techniques for\ntraining a dialogue agent via reinforcement learning. A\nstandard memory network architecture is trained on both bAbI and a version of\nthe WikiMovies dataset (as in Weston 2016, which this work extends). Numerous\nexperiments are performed comparing the behavior of different training\nalgorithms under various experimental conditions.\n\nSTRENGTHS: The experimentation is comprehensive. I agree with the authors that\nthese results provide additional useful insight into the performance of the\nmodel in the 2016 paper (henceforth W16).\n\nWEAKNESSES: This is essentially an appendix to the earlier paper. There is no\nnew machine learning content. Secondarily, the paper seems to confuse the\ndistinction between \"training with an adaptive sampling procedure\" and \"training\nin interactive environments\" more generally. In particular, no comparisons are\npresented to the to the experiments with a static exploration policy presented\nin W16, when the two training can & should be evaluated side-by-side.\nThe only meaningful changes between this work and W16 involve simple\n(and already well-studied) changes to the form of this exploration policy.\n\nMy primary concern remains about novelty: the extra data introduced here is\nwelcome enough, but probably belongs in a *ACL short paper or a technical\nreport. This work does not stand on its own, and an ICLR submission is not an\nappropriate vehicle for presenting it.\n\n\"REINFORCEMENT LEARNING\"\n\n[Update: concerns in this section have been addressed by the authors.]\n\nThis paper attempts to make a hard distinction between the reinforcement\nlearning condition considered here and the (\"non-RL\") condition considered in\nW16. I don't think this distinction is nearly as sharp as it's\nmade out to be. \n\nAs already noted in Weston 2016, the RBI objective is a special case of vanilla\npolicy gradient with a zero baseline and off-policy samples. In this sense the\nversion of RBI considered in this paper is the same as in W16, but with a\ndifferent exploration policy; REINFORCE is the same objective with a nontrivial\nbaseline. Similarly, the change in FP is only a change to the sampling policy.\nThe fixed dataset / online learning distinction is not especially meaningful\nwhen the fixed dataset consists of endless synthetic data.\n\nIt should be noted that some variants of the exploration policy in W16 provide a\nstronger training signal than is available in the RL \"from scratch\" setting\nhere: in particular, when $\\pi_acc = 0.5$ the training samples will feature much\ndenser reward. However, if I correctly understand Figures 3 and 4 in this paper,\nthe completely random initial policy achieves an average reward of ~0.3 on bAbI\nand ~0.1 on movies---as good or better than the other exploration policies in\nW16!\n\nI think this paper would be a lot clearer if the delta from W16 were expressed\ndirectly in terms of their different exploration policies, rather than trying to\ncast all of the previous work as \"not RL\" when it can be straightforwardly\naccommodated in the RL framework.\n\nI was quite confused by the fact that no direct comparisons are made to the\ntraining conditions in the earlier work. I think this is a symptom of the\nproblem discussed above: once this paper adopts the position that this work is\nabout RL and the previous work is not, it becomes possible to declare that the\ntwo training scenarios are incomparable. I really think this is a mistake---to\nthe extent that the off-policy sample generators used in the previous paper are\nworse than chance, it is always possible to compare to them fairly here.\nEvaluating everything in the \"online\" setting and presenting side-by-side\nexperiments would provide a much more informative picture of the comparative\nbehavior of the various training objectives.\n\nON-POLICY VS OFF-POLICY\n\nVanilla policy gradient methods like the ones here typically can't use\noff-policy samples without a little extra hand-holding (importance sampling,\ntrust region methods, etc.). They seem to work out of the box for a few of the\nexperiments in this paper, which is an interesting result on its own. It would\nbe nice to have some discussion of why that might be the case.\n\nOTHER NOTES\n\n- The claim that \"batch size is related to off-policy learning\" is a little\nodd. There are lots of on-policy algorithms that require the agent to collect a\nlarge batch of transitions from the current policy before performing an \n(on-policy) update.\n\n- I think the experiments on fine-tuning to human workers are the most exciting\npart of this work, and I would have preferred to see these discussed (and\nexplored with) in much more detail rather than being relegated to the\npenultimate paragraphs.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "pros:\n - demonstration that using teacher's feedback to improve performance in a dialogue system can be made to work \n  in a real-world setting\n - comprehensive experiments\n \n cons:\n - lack of technical novelty due to prior work\n - not all agree with the RL vs not-RL (pre-built datasets) distinction suggested in the paper with respect to the previous work\n \n Overall, the paper makes a number of practical contributions and evaluation, rather than theoretical novelty.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Dialogue Learning With Human-in-the-Loop", "is_meta_review": false, "comments": "As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle.\nDespite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting.\n\nseveral points were raised that were in turn addressed by the authors:\n1. formalisation of the task (learning dialogue) is not precise. when can we declare success? \nThe answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision.\n\n2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning.\nThe authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges.\n\n3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching \u2026 or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?))\nThe authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal.\n\n4. relation to prior work Weston\u201916 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston\u201916 - and not replacing it. In this case Weston\u201916 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn\u2019t find this in the experiments.\nThe authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing.\n\nThere is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston'16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Review", "is_meta_review": false, "comments": "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model\u2019s answer, which does not always come with a positive reward. Thus, the model must learn to use the teacher\u2019s feedback to significantly improve performance.\n\nOverall, the paper is written clearly, and several interesting models are tested. It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult.\n\nMy main concern is with the paper`s novelty. In the words of the authors, this paper has two primary differences with the work of Weston:\n\n\u201c(i) That earlier work did not use the natural reinforcement learning/online setting, but \u201ccheated\u201d with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.\u201d\n\nPoint (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper. Thus, the main point of the paper is that \u201cthe model also works if we collect the data online (i.e. the agent\u2019s policy is used to collect data rather than a fixed policy beforehand)\u201d. While this is a step in the right direction, I\u2019m not sure if it\u2019s significant enough for an ICLR paper. Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration. Thus, the paper is borderline accept/reject. \n\nEDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 16 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "TITLE": "Review", "is_meta_review": false, "comments": "\nSUMMARY: This paper describes a set of experiments evaluating techniques for\ntraining a dialogue agent via reinforcement learning. A\nstandard memory network architecture is trained on both bAbI and a version of\nthe WikiMovies dataset (as in Weston 2016, which this work extends). Numerous\nexperiments are performed comparing the behavior of different training\nalgorithms under various experimental conditions.\n\nSTRENGTHS: The experimentation is comprehensive. I agree with the authors that\nthese results provide additional useful insight into the performance of the\nmodel in the 2016 paper (henceforth W16).\n\nWEAKNESSES: This is essentially an appendix to the earlier paper. There is no\nnew machine learning content. Secondarily, the paper seems to confuse the\ndistinction between \"training with an adaptive sampling procedure\" and \"training\nin interactive environments\" more generally. In particular, no comparisons are\npresented to the to the experiments with a static exploration policy presented\nin W16, when the two training can & should be evaluated side-by-side.\nThe only meaningful changes between this work and W16 involve simple\n(and already well-studied) changes to the form of this exploration policy.\n\nMy primary concern remains about novelty: the extra data introduced here is\nwelcome enough, but probably belongs in a *ACL short paper or a technical\nreport. This work does not stand on its own, and an ICLR submission is not an\nappropriate vehicle for presenting it.\n\n\"REINFORCEMENT LEARNING\"\n\n[Update: concerns in this section have been addressed by the authors.]\n\nThis paper attempts to make a hard distinction between the reinforcement\nlearning condition considered here and the (\"non-RL\") condition considered in\nW16. I don't think this distinction is nearly as sharp as it's\nmade out to be. \n\nAs already noted in Weston 2016, the RBI objective is a special case of vanilla\npolicy gradient with a zero baseline and off-policy samples. In this sense the\nversion of RBI considered in this paper is the same as in W16, but with a\ndifferent exploration policy; REINFORCE is the same objective with a nontrivial\nbaseline. Similarly, the change in FP is only a change to the sampling policy.\nThe fixed dataset / online learning distinction is not especially meaningful\nwhen the fixed dataset consists of endless synthetic data.\n\nIt should be noted that some variants of the exploration policy in W16 provide a\nstronger training signal than is available in the RL \"from scratch\" setting\nhere: in particular, when $\\pi_acc = 0.5$ the training samples will feature much\ndenser reward. However, if I correctly understand Figures 3 and 4 in this paper,\nthe completely random initial policy achieves an average reward of ~0.3 on bAbI\nand ~0.1 on movies---as good or better than the other exploration policies in\nW16!\n\nI think this paper would be a lot clearer if the delta from W16 were expressed\ndirectly in terms of their different exploration policies, rather than trying to\ncast all of the previous work as \"not RL\" when it can be straightforwardly\naccommodated in the RL framework.\n\nI was quite confused by the fact that no direct comparisons are made to the\ntraining conditions in the earlier work. I think this is a symptom of the\nproblem discussed above: once this paper adopts the position that this work is\nabout RL and the previous work is not, it becomes possible to declare that the\ntwo training scenarios are incomparable. I really think this is a mistake---to\nthe extent that the off-policy sample generators used in the previous paper are\nworse than chance, it is always possible to compare to them fairly here.\nEvaluating everything in the \"online\" setting and presenting side-by-side\nexperiments would provide a much more informative picture of the comparative\nbehavior of the various training objectives.\n\nON-POLICY VS OFF-POLICY\n\nVanilla policy gradient methods like the ones here typically can't use\noff-policy samples without a little extra hand-holding (importance sampling,\ntrust region methods, etc.). They seem to work out of the box for a few of the\nexperiments in this paper, which is an interesting result on its own. It would\nbe nice to have some discussion of why that might be the case.\n\nOTHER NOTES\n\n- The claim that \"batch size is related to off-policy learning\" is a little\nodd. There are lots of on-policy algorithms that require the agent to collect a\nlarge batch of transitions from the current policy before performing an \n(on-policy) update.\n\n- I think the experiments on fine-tuning to human workers are the most exciting\npart of this work, and I would have preferred to see these discussed (and\nexplored with) in much more detail rather than being relegated to the\npenultimate paragraphs.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Code/data release, paper update", "OTHER_KEYS": "Jason E Weston", "comments": "We have released the data, code, and simulator described in the paper at ", "IS_META_REVIEW": false, "DATE": "15 Dec 2016", "is_meta_review": false}, {"TITLE": "clarifications w/r/t \"dialogue\" and technical contribution", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "pre-review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "why not use supervised learning", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016 (modified: 04 Dec 2016)", "is_meta_review": false}], "SCORE": 5, "authors": "Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, Jason Weston", "KEYWORDS": "we explore a reinforcement learning setting for dialogue where the bot improves its abilities using reward-based or textual feedback", "accepted": true, "id": ""}
