{
  "name" : "1703.01507.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma",
    "authors" : [ "Mieczysław A. Kłopotek" ],
    "emails" : [ "(klopotek@ipipan.waw.pl)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation f of the data points into lower dimensional space such that all of them fall into predefined error range δ.\nWe formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability 1− all of them fall into predefined error range δ for any user-predefined failure probability .\nThis result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma. In particular, we take a closer look at the k-means algorithm and prove that a good solution in the projected space is also a good solution in the original space. Furthermore, under proper assumptions local optima in the original space are also ones in the projected space. We define also conditions for which clusterability property of the original space is transmitted to the projected space, so that special case algorithms for the original space are also applicable in the projected space.\nKeywords: Johnson-Lindenstrauss Lemma, random projection, sample distortion, dimensionality reduction, linear JL transform, k-means algorithm, clusterability retention,\nar X\niv :1\n70 3.\n01 50\n7v 4\n[ cs\n.D S]\n1 8"
    }, {
      "heading" : "1 Introduction",
      "text" : "Dimensionality reduction plays an important role in many areas of data processing, and especially in machine learning (cluster analysis, classifier learning, model validation, data visualisation etc.).\nUsually it is associated with manifold learning, that is a belief that the data lie in fact in a low dimensional subspace that needs to be identified and the data projected onto it so that the number of degrees of freedom is reduced and as a consequence also sample sizes can be smaller without loss of reliability. Techniques like reduced k-means [17], PCA (Principal Component Analysis), Kernel PCA, LLE (Locally Linear Embedding), LEM (Laplacian Eigenmaps), MDS (Metric Multidimensional Scaling), Isomap, SDE (Semidefinite Embedding), just to mention a few.\nBut there exists still another possibility of approaching the dimensionality reduction problems, in particular when such intrinsic subspace where data is located cannot be identified. The problem of choice of the subspace has been surpassed by several authors by so-called random projection, applicable in particularly highly dimensional spaces (tens of thousands of dimensions) and correspondingly large data sets (of at least hundreds of points).\nThe starting point here is the Johnson-Lindenstrauss Lemma [13]. Roughly speaking it states that there exists a linear1 mapping from a higher dimensional space into a sufficiently high dimensional subspace that will preserve approximately the distances between points, as needed e.g. by k-means algorithm [4].\nTo be more formal consider a set Q of m objects Q = {1, . . . ,m}. An object i ∈ Q may have a representation xi ∈ Rn. Then the set of these representations will be denoted by Q. An object i ∈ Q may have a representation x′i ∈ Rn ′ , in a different space. Then the set of these representations will be denoted by Q′. With this notation let us state:\nTheorem 1. (Johnson-Lindenstrauss) Let δ ∈ (0, 1 2 ). Let Q be a set of m objects and Q - a set of points representing them in Rn, and let n′ ≥ C lnm δ2\n, where C is a sufficiently large constant (e.g.20). There exists a Lipschitz mapping f : Rn → Rn′ such that for all u,v ∈ Q\n(1− δ)‖u− v‖2 ≤ ‖f(u)− f(v)‖2 ≤ (1 + δ)‖u− v‖2 (1)\nA number of proofs and applications of this theorem have been proposed which in fact do not prove the theorem as such but rather create a prob1JL Lemma speaks about a general transformation, but many researchers look just for linear ones.\nabilistic version of it, like e.g. [11, 1, 3, 12, 14, 10]. For an overview of Johnson-Lindenstrauss Lemma variants see e.g. [15].\nEssentially the idea behind these probabilistic proofs is as follows: It is proven that the probability of reconstructing the length of a random vector from a projection onto a subspace within a reasonable error boundaries is high.\nOne then inverts the thinking and states that the probability of reconstructing the length of a given vector from a projection onto a (uniformly selected) random subspace within a reasonable error boundaries is high.\nBut uniform sampling of high dimensional subspaces is a hard task. So instead n′ vectors with random coordinates are sampled from the original n-dimensional space and one uses them as a coordinate system in the n′dimensional subspace which is a much simpler process. One hopes that the sampled vectors will be orthogonal (and hence the coordinate system will be orthogonal) which in case of vectors with thousands of coordinates is reasonable. That means we create a matrix M of n′ rows and n columns as follows: for each row i we sample n numbers from N (0, 1) forming a row vector aTi . We normalize it obtaining the row vector b T i = a T i · (aTi ai)−1/2. This becomes the ith row of the matrix M . Then for any data point x in the original space its random projection is obtained as x′ =Mx.\nThen the mapping we seek is the projection multiplied by a suitable factor.\nIt is claimed afterwards that this mapping is distance-preserving not only for a single vector, but also for large sets of points with some, usually very small probability, as Dasgupta and Gupta [11] maintain. Via applying the above process many times one can finally get the mapping f that is needed. That is each time we sample a subspace from the space of subspaces and check if condition expressed by equation (1) holds for all the points, and if not, we sample again, while we have the reasonable hope that we will get the subspace of interest after a finite number of steps with probability that we assume.\nIn this paper we explore the following flaw of the mentioned approach: If we want to apply for example a k-means clustering algorithm, we are in fact not interested in resampling the subspaces in order to find a convenient one so that the distances are sufficiently preserved. Computation over and over again of m2/2 distances between the points in the projected space may turn out to be much more expensive than computing O(mk) distances during k-means clustering (if m k) in the original space. In fact we are primarily interested in clustering data. But we do not have any criterion for the kmeans algorithm that would say that this particular subspace is the right one via e.g. minimization of k-means criterion (and in fact for any other\nclustering algorithm). Therefore, we rather seek a scheme that will allow us to say that by a certain random sampling we have already found the subspace that we sought with a sufficiently high probability. As far as we know, this is the first time such a problem has been posed.\nTo formulate claims concerning k-means, we need to introduce additional notation. Let us denote with C a partition of Q into k clusters {C1, . . . , Ck}. For any i ∈ Q let C(i) denote the cluster Cj to which i belongs. For any set of objects Cj let µ(Cj) = 1|Cj | ∑ i∈Cj xi and µ ′(Cj) = 1 |Cj | ∑ i∈Cj x ′ i.\nUnder this notation the k-means cost function may be written as J(Q,C) = ∑ i∈Q ‖xi − µ(C(i))‖2 (2)\nJ(Q′,C) = ∑ i∈Q ‖x′i − µ′(C(i)‖2 (3)\nfor the sets Q,Q′. Our contribution is as follows:\n• We formulate and prove a set version of JL Lemma - see Theorem 6.\n• Based on it we demonstrate that a good solution to k-means problem in the projected space is also a good one in the original space - see Theorem 2.\n• We show that local k-means minima in the original and the projected spaces match under proper conditions - see Theorems 3, 4.\n• We demonstrate that a perfect k-means algorithm in the projected space is a constant factor approximation of the global optimum in the original space - see Theorem 5\n• We prove that the projection preserves several clusterability properties - see Theorems 9, 7, 8. 10 and 11.\nFor k-means in particular we make the following claim:\nTheorem 2. Let Q be a set of m representatives of objects from Q in an n-dimensional orthogonal coordinate system Cn. Let δ ∈ (0, 12), ∈ (0, 1). and let\nn′ ≥ 2− ln + 2 ln(m) − ln(1 + δ) + δ\n(4)\nLet Cn′ be a randomly selected (via sampling from a normal distribution) n′dimensional orthogonal coordinate system. Let the set Q′ consist of m objects\nsuch that for each i ∈ Q, x′i ∈ Q′ is a projection of xi ∈ Q onto Cn′. If C is a partition of Q, then\n(1− δ)J(Q,C) ≤ n n′ J(Q′,C) ≤ (1 + δ)J(Q,C) (5)\nholds with probability of at least 1− .\nNote that the inequality (5) can be rewitten as( 1− δ\n1 + δ\n) J(Q′,C) ≤ n ′\nn J(Q,C) ≤\n( 1 + δ\n1− δ\n) J(Q′,C)\nFurthermore\nTheorem 3. Under the assumptions and notation of Theorem 2, if the partition C∗ constitutes a local minimum of J(Q,C) over C (in the original space) and if for any two clusters g = 2(1 − α) times half of the distance between their centres is the gap between these clusters, where α ∈ [0, 1), and\nδ ≤ 1−\n( 1− g\n2 )2( 1− g\n2\n)2 + (1 + 2p)\n(6)\n(p to be defined later by inequality (14)) then this same partition is (in the projected space) also a local minimum of J(Q′,C) over C, with probability of at least 1− .\nTheorem 4. Under the assumptions and notation of Theorem 2, if the clustering C′∗ constitutes a local minimum of J(Q′,C) over C (in the projected space) and if for any two clusters 1 − α times the distance between their centres is the gap between these clusters, where α ∈ [0, 1), and\nδ 1− δ ≤ 1− α 2 (1 + 2p) + α2 (7)\nthen the very same partition C′∗ is also (in the original space) a local minimum of J(Q,C) over C, with probability of at least 1− .\nTheorem 5. Under the assumptions and notation of Theorem 2, if CG denotes the clustering reaching the global optimum in the original space, and\nC′G denotes the clustering reaching the global optimum in the projected space, then\nn n′ J(Q′,C′G) ≤ (1 + δ)J(Q,CG) (8)\nwith probability of at least 1− . That is the perfect k-means algorithm in the projected space is a constant factor approximation of k-means optimum in the original space.\nWe postpone the proof of the theorems 2-5 till section 3, as we need first to derive the basic theorem 6 in section 2 which is essentially based on the results reported by Dasgupta and Gupta [11].\nLet us however stress at this point the significance of these theorems. Earlier forms of JL lemma required sampling of the coordinates over and over again2, with quite a low success rate until a mapping is found fitting the error constraints. In our theorems, we need only one sampling in order to achieve the required success probability of selecting a suitable subspace to perform k-means. In Section 5 we illustrate this advantage by some numerical simulation results, showing at the same time the impact of various parameters of Jonson-Lindenstrauss Lemma on the dimensionality of the projected space. In Section 6 we recall the corresponding results of other authors.\nIn Section 4 we demonstrate an additional advantage of our version of JL lemma consisting in preservation of various clusterability criteria.\nSection 7 contains some concluding remarks."
    }, {
      "heading" : "2 Derivation of the Set-Friendly",
      "text" : "Johnson-Lindenstrauss Lemma\nLet us present the process of seeking the mapping f from Theorem 1 in a more detailed manner, so that we can then switch to our target of selecting the size of the subspace guaranteeing that the projected distances preserve their proportionality in the required range.\nLet us consider first a single vector x = (x1, .x2, ..., xn) of n independent random variables drawn from the normal distribution N (0, 1) with mean 0 and variance 1. Let x′ = (x1, .x2, ..., xn′), where n′ < n, be its projection onto the first n′ coordinates.\nDasgupta and Gupta [11] in their Lemma 2.2 demonstrated that for a positive β\n2 Though in passing a similar result is claimed in Lemma 5.3 http://math.mit.edu/ ~bandeira/2015_18.S096_5_Johnson_Lindenstrauss.pdf, though without an explicit proof.\n• if β < 1 then\nPr(‖x′‖2 ≤ βn ′\nn ‖x‖2) ≤ β\nn′ 2 ( 1 +\nn′(1− β) n− n′\n)n−n′ 2\n(9)\n• if β > 1 then\nPr(‖x′‖2 ≥ βn ′\nn ‖x‖2) ≤ β\nn′ 2 ( 1 +\nn′(1− β) n− n′\n)n−n′ 2\n(10)\nNow imagine we want to keep the error of squared length of x bounded within a range of ±δ (relative error) upon projection, where δ ∈ (0, 1). Then we get the probability\nPr ( (1− δ)‖x‖2 ≤ n\nn′ ‖x′‖2 ≤ (1 + δ)‖x‖2 ) ≥ 1− (1− δ) n′ 2 ( 1 + n′δ\nn− n′\n)n−n′ 2\n− (1 + δ) n′ 2 ( 1− n ′δ\nn− n′\n)n−n′ 2\nThis implies\nPr ( (1− δ)‖x‖2 ≤ n\nn′ ‖x′‖2 ≤ (1 + δ)‖x‖2 ) ≥ 1− 2max (1− δ)n′2 (1 + n′δ n− n′ )n−n′ 2 ,\n(1 + δ) n′ 2 ( 1− n ′δ\nn− n′\n)n−n′ 2  = 1− 2 max\nδ∗∈{−δ,+δ} (1− δ∗)n′2 (1 + δ∗n′ n− n′ )n−n′ 2  The same holds if we scale the vector x.\nNow if we have a sample consisting of m points in space, without however a guarantee that coordinates are independent between the vectors then we\nwant that the probability that squared distances between all of them lie within the relative range ±δ is higher than\n1− ≤ 1− ( m\n2\n)( 1− Pr ( (1− δ)‖x‖2 ≤ n\nn′ ‖x′‖2 ≤ (1 + δ)‖x‖2\n)) (11)\nfor some failure probability3 term ∈ (0, 1). To achieve this, it is sufficient that the following holds:\n≥ 2 ( m\n2\n) max\nδ∗∈{−δ,+δ} (1− δ∗)n′2 (1 + δ∗n′ n− n′ )n−n′ 2  Taking logarithm\nln ≥ ln(m(m− 1))\n+ max δ∗∈{−δ,+δ}\n( n′\n2 ln(1− δ∗) + (n− n ′) 2 ln\n( 1 + δ∗n′\nn− n′\n))\nln − ln(m(m− 1))\n≥ max δ∗∈{−δ,+δ}\n( n′\n2 ln(1− δ∗) + (n− n ′) 2 ln\n( 1 + δ∗n′\nn− n′ )) We know4 that ln(1 + x) < x for x > −1 and x 6= 0, hence the above\nholds if\nln − ln(m(m− 1)) ≥ max δ∗∈{−δ,+δ}\n( n′\n2 ln(1− δ∗) + (n− n ′) 2 δ∗n′ n− n′\n)\nln −ln(m(m−1)) ≥ max δ∗∈{−δ,+δ}\n( n′\n2 ln(1− δ∗) + 1 2 (δ∗)n′\n) = n′\n2 max δ∗∈{−δ,+δ} (ln(1− δ∗) + δ∗)\nRecall that also we have ln(1− x) + x < 0 for x < 1 and x 6= 0, threfore\nmax δ∗∈{−δ,+δ}\n( 2 ln − ln(m(m− 1))\nln(1− δ∗) + δ∗\n) ≤ n′\n3 We speak about a success if all the projected data points lie within the range defined by formula (1). Otherwise we speak about failure (even if only one data point lies outside this range). 4Please recall at this point the Taylor expansion ln(1+x) = x−x2/2+x3/3−x5/5 . . . which converges in the range (-1,1) and hence implies ln(1+x) < x for x ∈ (−1, 0)∪ (0, 1) as we will refer to it discussing difference to JL theorems of other authors.\nSo finally, realizing that − ln(1− δ)− δ ≥ − ln(1 + δ) + δ > 0, and that ln(m(m− 1)) < 2 ln(m) we get as sufficient condition5\nn′ ≥ 2− ln + 2 ln(m) − ln(1 + δ) + δ\nNote that this expression does not depend on n that is the number of dimensions in the projection is chosen independently of the original number of dimensions6.\nSo we are ready to formulate our major finding of this paper\nTheorem 6. Let δ ∈ (0, 1 2 ), ∈ (0, 1). Let Q ⊂ Rn be a set of m points in an n-dimensional orthogonal coordinate system Cn and let (as in formula (4))\nn′ ≥ 2− ln + 2 ln(m) − ln(1 + δ) + δ\nLet Cn′ be a randomly selected (via sampling from a normal distribution) n′-dimensional orthogonal coordinate system. For each v ∈ Q let v′ be its projection onto Cn′. Then for all pairs u,v ∈ Q\n(1− δ)‖u− v‖2 ≤ n n′ ‖u′ − v′‖2 ≤ (1 + δ)‖u− v‖2 (12)\nholds with probability of at least 1−"
    }, {
      "heading" : "3 Proofs of theorems 2-5",
      "text" : "The permissible error δ will surely depend on the target application. Let us consider the context of k-means. First we claim for k-means, that the JL Lemma applies not only to data points but also to cluster centres.\n5 We substituted the denominator with a smaller positive number and the nominator with a larger positive number so that the fraction value increases so that a higher n′ will be required than actually needed. 6 Though in passing a similar result is claimed in Lemma 5.3 http://math.mit.edu/ ~bandeira/2015_18.S096_5_Johnson_Lindenstrauss.pdf, though without an explicit proof. They propose that\nn′ ≥ (2 + r) 2 ln(m) − ln(1 + δ) + δ\nin order to get a failure rate below m−r. In fact when we substitute = m−r, both formulas are the same. However, usage of alows for control of failure rate in the other theorems in this paper, while r does not make this possibility obvious. Also fixing r versus fixing impacts disadvantageously the growth rate of n′ with m.\nLemma 1. Let δ ∈ (0, 1 2 ), ∈ (0, 1). Let Q ⊂ Rn be a set of m representatives of elements of Q in an n-dimensional orthogonal coordinate system Cn and let the inequality (4) hold. Let Cn′ be a randomly selected (via sampling from a normal distribution) n′-dimensional orthogonal coordinate system. For each xi ∈ Q let x′i ∈ Q′ be its projection onto Cn′. Let C be a partition of Q. Then for all data points xi ∈ Q\n(1− δ)‖xi − µ(C(i))‖2 ≤ n\nn′ ‖x′i − µ′(C(i))‖2 ≤ (1 + δ)‖xi − µ(C(i))‖2\n(13)\nhold with probability of at least 1− ,\nProof. As we know, data points under k-means are assigned to clusters having the closest cluster centre. On the other hand the cluster centre µ is the average of all the data point representatives in the cluster.\nHence the cluster element i has the squared distance to its cluster centre µ(C(i)) amounting to\n‖xi − µ(C(i))‖2 = 1 |C(i)| ∑ j∈C(i) ‖xi − xj‖2\nBut according to Theorem 6\n(1− δ) ∑ j∈C(i) ‖xi − xj‖2 ≤ n n′ ∑ j∈C(i) ‖x′i − x′j‖2 ≤ (1 + δ) ∑ j∈C(i) ‖xi − xj‖2\nHence\n(1− δ)‖xi − µ(C(i))‖2 ≤ n\nn′ ‖x′i − µ′(C(i))‖2 ≤ (1 + δ)‖xi − µ(C(i))‖2\nNote that here µ′(C(i)) is not the projective image of µ(C(i)), but rather the centre of projected images of cluster elements.\nThe Lemma 1 permits us to prove Theorem 2\nProof. (Theorem 2) According to formula (13):\n(1− δ)‖xi − µ(C(i))‖2 ≤ n\nn′ ‖x′i − µ′(C(i))‖2 ≤ (1 + δ)‖xi − µ(C(i))‖2\nHence∑ i∈Q (1−δ)‖xi−µ(C(i))‖2 ≤ ∑ i∈Q n n′ ‖x′i−µ′(C(i))‖2 ≤ ∑ i∈Q (1+δ)‖xi−µ(C(i))‖2\n(1−δ) ∑ i∈Q ‖xi−µ(C(i))‖2 ≤ ∑ i∈Q n n′ ‖x′i−µ′(C(i))‖2 ≤ (1+δ) ∑ i∈Q ‖xi−µ(C(i))‖2\nBased on defining equations (2) and (3) we get the formula (5)\n(1− δ)J(Q,C) ≤ n n′ J(Q′,C) ≤ (1 + δ)J(Q,C)\nLet us now investigate the distance between centres of two clusters, say C1, C2. Let their cardinalities amount to m1,m2 respectively. Denote C12 = C1 ∪C2. Consequently m12 = |C12| = m1 +m2. For a set Cj let V AR(Cj) = 1 |Cj | ∑ i∈Cj ‖xi − µ(Cj)‖ 2 and V AR′(Cj) = 1|Cj | ∑ i∈Cj ‖x ′ i − µ′(Cj)‖2.\nTherefore V AR(C12) = 1 |C12| ∑ i∈C12 ‖xi − µ(C12)‖2\n= 1\n|C12| ((∑ i∈C1 ‖xi − µ(C12)‖2 ) + (∑ i∈C2 ‖xi − µ(C12)‖2 ))\nBy inserting a zero\n= 1\n|C12| ((∑ i∈C1 ‖xi − µ(C1) + µ(C1)− µ(C12)‖2 ) + (∑ i∈C2 ‖xi − µ(C12)‖2 ))\n= 1\n|C12| ((∑ i∈C1 ( (xi − µ(C1))2 + (µ(C1)− µ(C12))2 + 2(xi − µ(C1))(µ(C1)− µ(C12)) ))\n+ (∑ i∈C2 ‖xi − µ(C12)‖2 ))\n= 1\n|C12| (((∑ i∈C1 (xi − µ(C1))2 ) + (∑ i∈C1 (µ(C1)− µ(C12))2 )\n+2( ∑ i∈C1 xi − ∑ i∈C1 µ(C1))(µ(C1)− µ(C12))\n)) + (∑ i∈C2 ‖xi − µ(C2) + µ(C2)− µ(C12)‖2 )\n= 1\n|C12| (((∑ i∈C1 (xi − µ(C1))2 ) + |C1|(µ(C1)− µ(C12))2\n+2(|C1|µ(C1)− |C1|µ(C1))(µ(C1)− µ(C12))) + (∑ i∈C2 ‖xi − µ(C12)‖2 ))\n= 1\n|C12|\n(( V AR(C1)|C1|+ |C1|(µ(C1)− µ(C12))2 ) + (∑ i∈C2 ‖xi − µ(C12)‖2 ))\nVia the same reasonig we get:\n= 1 |C12| (( V AR(C1)|C1|+ |C1|(µ(C1)− µ(C12))2 ) + ( V AR(C2)|C2|+ |C2|(µ(C2)− µ(C12))2\n)) = 1\n|C12| (V AR(C1)|C1|+ V AR(C2)|C2|\n+|C1|(µ(C1)− µ(C12))2 + |C2|(µ(C2)− µ(C12))2 )\nAs Apparently µ(C12) = 1|C12| ∑ i∈C12 xi = 1 |C12| ( ( ∑ i∈C1 xi) + ( ∑ i∈C2 xi) ) =\n1 |C12| (|C1|µ(C1) + |C1|µ(C1)) that is µ(C12) = |C1| |C12|µ(C1)+ |C2| |C12|µ(C2, we get\n= 1\n|C12|\n( V AR(C1)|C1|+ V AR(C2)|C2|+ |C1| ( µ(C1)−\n|C1| |C12| µ(C1)− |C2| |C12| µ(C2\n)2\n+|C2| ( µ(C2)−\n|C1| |C12| µ(C1)− |C2| |C12| µ(C2)\n)2)\n= 1\n|C12|\n( V AR(C1)|C1|+ V AR(C2)|C2|+ |C1| ( |C2| |C12| µ(C1)− |C2| |C12| µ(C2) )2\n+|C2| ( − |C1| |C12| µ(C1) + |C1| |C12| µ(C2) )2)\n= 1\n|C12|\n( V AR(C1)|C1|+ V AR(C2)|C2|+\n|C1||C2|2 + |C1|2|C2| |C12|2\n(µ(C1)− µ(C2))2 )\nhence\nV AR(C12) == 1\n|C12|\n( V AR(C1)|C1|+ V AR(C2)|C2|+\n|C1||C2| |C12|\n(µ(C1)− µ(C2))2 )\nThis leads immediately to\nV AR(C12)·m12 = V AR(C1)·m1+V AR(C2)·m2+m1·m2/m12·‖µ(C1)−µ(C2)‖2\nwhich implies\nV AR(C12) · m212\nm1 ·m2 = V AR(C1) · m12 m2 +V AR(C2) · m12 m1 +‖µ(C1)−µ(C2)‖2\nAccording to Lemma 1, applied to the set C12 as a cluster,\n(1− δ) ( V AR(C1) ·m12/m2 + V AR(C2) ·m12/m1 + ‖µ(C1)− µ(C2)‖2 ) ≤ n n′ ( V AR′(C1) ·m12/m2 + V AR′(C2) ·m12/m1 + ‖µ′(C1)− µ′(C2)‖2\n) ≤ (1 + δ) ( V AR(C1) ·m12/m2 + V AR(C2) ·m12/m1 + ‖µ(C1)− µ(C2)‖2\n) and with respect to C1, C2 combined\n(1− δ) (V AR(C1) ·m12/m2 + V AR(C2) ·m12/m1)\n≤ n n′ (V AR′(C1) ·m12/m2 + V AR′(C2) ·m12/m1)\n≤ (1 + δ) (V AR(C1) ·m12/m2 + V AR(C2) ·m12/m1) These two last equations mean that\n−2δ (V AR(C1) ·m12/m2 + V AR(C2) ·m12/m1) + (1− δ)‖µ(C1)− µ(C2)‖2 ≤ n n′ ( ‖µ′(C1)− µ′(C2)‖2 ) ≤ 2δ (V AR(C1) ·m12/m2 + V AR(C2) ·m12/m1) + (1 + δ)‖µ(C1)− µ(C2)‖2\nLet us assume that the quotient\nV AR(C1) ·m12/m2 + V AR(C2) ·m12/m1 ‖µ(C1)− µ(C2)‖2 ≤ p (14)\nwhere p is some positive number. So we have in effect\n(1−δ(1+2p))‖µ(C1)−µ(C2)‖2 ≤ n n′ ( ‖µ′(C1)− µ′(C2)‖2 ) ≤ (1+δ(1+2p))‖µ(C1)−µ(C2)‖2\nUnder balanced ball-shaped clusters p does not exceed 1. So we have shown the lemma\nLemma 2. Under the assumptions of preceding lemmas for any two clusters C1, C2\n(1−δ(1+2p))‖µ(C1)−µ(C2)‖2 ≤ n n′ ( ‖µ′(C1)− µ′(C2)‖2 ) ≤ (1+δ(1+2p))‖µ(C1)−µ(C2)‖2 (15) where p depends on degree of balance between clusters and cluster shape, holds with probability at least 1− .\nNow let us consider the choice of δ in such a way that with high probability no data point will be classified into some other cluster. We claim the following\nLemma 3. Consider two clusters C1, C2. Let δ ∈ (0, 12), ∈ (0, 1). Let Q ⊂ Rn be a set of m points in an n-dimensional orthogonal coordinate system Cn and let the inequality (4) hold. Let Cn′ be a randomly selected (via sampling from a normal distribution) n′-dimensional orthogonal coordinate system. For each xi ∈ Q let v′ be its projection onto Cn′. For two clusters C1, C2, obtained via k-means, in the original space let µ1,µ2 be their centres and µ′1,µ ′ 2 be centres to the correspondings sets of projected cluster members. Furthermore let d be the distance of the first cluster centre to the common border of both clusters and let the closest point of the first cluster to this border be at the distance of αd from its cluster centre as projected on the line connecting both cluster centres, where α ∈ (0, 1). Then all projected points of the first cluster are (each) closer to the centre of the set of projected points of the first than to the centre of the set of projected points of the second if\nδ ≤ 1−\n( 1− g\n2 )2( 1− g\n2\n)2 + (1 + 2p) = 1− α2 (1 + 2p) + α2 (16)\nwhere g = 2(1− α), with probability of at least 1− .\nProof. Consider a data point x ”close” to the border between the two neighbouring clusters, on the line connecting the cluster centres, belonging to the first cluster, at a distance αd from its cluster centre, where d is the distance of the first cluster centre to the border and α ∈ (0, 1). The squared distance between cluster centres, under projection, can be ”reduced” by the factor 1 − δ, (beside the factor n\nn′ which is common to all the points) whereas the\nsquared distance of x to its cluster centre may be ”increased” by the factor 1 + δ. This implies a relationship between the factor α and the error δ.\nIf x′ should not cross the border between the clusters, the following needs to hold:\n‖x′ − µ′1‖ ≤ 1\n2 ‖µ′2 − µ′1‖ (17)\nwhich implies: n\nn′ ‖x′ − µ′1‖2 ≤\nn n′ 1 4 ‖µ′2 − µ′1‖2\nAs (see Lemma 1)\nn n′ ‖x′ − µ′1‖2 ≤ (1 + δ)‖x− µ1‖2 = (1 + δ)(αd)2\nand (see Lemma 2)\nn n′ 1 4 ‖µ′2 − µ′1‖2 ≥ (1− δ(1 + 2p)) 1 4 ‖µ2 − µ1‖2 = (1− δ(1 + 2p))d2\nwe know that, for inequality (17) to hold, it is sufficient that:\n(1 + δ)(αd)2 ≤ (1− δ(1 + 2p))d2\nthat is\nα ≤ √\n1− δ(1 + 2p) 1 + δ\nBut 2(1−α)d or 2(1−α) can be viewed as absolute or relative gap between clusters. So if we expect a relative gap g = 2(1 − α) between clusters, we have to choose δ in such a way that\n1− g 2 ≤ √ 1− δ(1 + 2p) 1 + δ\nTherefore\nδ ≤ 1−\n( 1− g\n2 )2( 1− g\n2\n)2 + (1 + 2p)\n(18)\nSo we see that the decision on the permitted error depends on the size of the gap between clusters that we hope to observe.\nThe Lemma 3 allows us to prove Theorem 3 in a straight forward manner.\nProof. (Theorem 3) Observe that in this theorem we impose the condition of this lemma on each cluster. So all projected points are closer to their set centres than to any other centre. So the k-means algorithm would get stuck at this clustering and hence we get at a local minimum.\nLemma 4. Let δ ∈ (0, 1 2 ), ∈ (0, 1). Let Q ⊂ Rn be a set of m points in an n-dimensional orthogonal coordinate system Cn and let the inequility (4) hold. Let Cn′ be a randomly selected (via sampling from a normal distribution) n′-dimensional orthogonal coordinate system. For each xi ∈ Q let v′ be its projection onto Cn′. For any two k-means clusters C1, C2 in the projected space let µ′1,µ ′ 2 be their centres in the projected space and µ1,µ2 be centres to the corresponding sets of cluster members in the original space. Furthermore let d be the distance of the first cluster centre to the common border of both clusters in the projected space and let the closest point of the first cluster to this border in that space be at the distances of αd from its cluster centre, where α ∈ [0, 1). Then all points of the first cluster in the original space are (each) closer to the centre of the set of points of the first than to the centre of the set of points of the second cluster in the original space if\nδ ≤ 1−\n( 1− (2−2α)\n2 )2 ( 1− (2−2α)\n2\n)2 + (1 + 2p) = 1− α2 (1 + 2p) + α2 (19)\nwith probability of at least 1− .\nProof. Consider a data point x′ ”close” to the border between the two neighbouring clusters in the projected space, on the line connecting the cluster centres, belonging to the first cluster, at a distance αd from its cluster centre, where d is the distance of the first cluster centre to the border and α ∈ (0, 1). The squared distance between cluster centres, in original space, can be ”reduced” by the factor (1 + δ)−1 (beside the factor n ′\nn which is common to all\nthe points), whereas the squared distance of x to its cluster centre may be ”increased” by the factor (1− δ)−1 . This implies a relationship between the factor α and the error δ.\nIf x (in the original space) should not cross the border between the clusters, the following needs to hold:\n‖x− µ1‖ ≤ 1\n2 ‖µ2 − µ1‖ (20)\nwhich implies: n′\nn ‖x− µ1‖2 ≤\nn′\nn\n1 4 ‖µ2 − µ1‖2\nAs (see Lemma 1)\nn′ n ‖x− µ1‖2 ≤ (1− δ)−1‖x′ − µ′1‖2 = (1− δ)−1(αd)2\nand (see Lemma 2)\nn′\nn\n1 4 ‖µ2 − µ1‖2 ≥ (1 + δ(1 + 2p))−1 1 4 ‖µ′2 − µ′1‖2 = (1 + δ(1 + 2p))−1d2\nThus, we know that, for inequality (20) to hold, it is sufficient that:\n(1− δ)−1(αd)2 ≤ (1 + δ(1 + 2p))−1d2\nthat is\nα ≤\n√ 1− δ\n1 + δ(1 + 2p)\nBut 2(1−α)d or 2(1−α) can be viewed as absolute or relative gap between clusters. So if we want to have a relative gap g = 2(1− α) between clusters, we have to choose δ in such a way that\n1− g 2 ≤\n√ 1− δ\n1 + δ(1 + 2p)\nTherefore\nδ ≤ 1−\n( 1− g\n2 )2( 1− g\n2\n)2 + (1 + 2p)\n(21)\nThe Lemma 4 allows us to prove Theorem 4 in a straight forward manner.\nProof. (Theorem 4) Observe that in this theorem we impose the condition of this lemma on each cluster. So all original space points are closer to their set centres than to any other centre. So the k-means algorithm would get stuck at this clustering and hence we get at a local minimum.\nHaving these results, we can go over to the proof of the Theorem 5.\nProof. (Theorem 5) Let CG denote the clustering reaching the global optimum in the original space. Let C′G denote the clustering reaching the global optimum in the projected space. From the Theorem 2 we have that\n(1− δ)J(Q,CG) ≤ n\nn′ J(Q′,CG) ≤ (1 + δ)J(Q,CG)\nOn the other hand\n(1 + δ)−1J(Q′,C′G) ≤ n′\nn J(Q,C′G) ≤ (1− δ)−1J(Q′,C′G)\nAs C′G is the global minimum in the projected space, hence\nJ(Q′,C′G) ≤ J(Q′,CG)\nSo n\nn′ J(Q′,C′G) ≤\nn n′ J(Q′,CG) ≤ (1 + δ)J(Q,CG)\nSo n\nn′ J(Q′,C′G) ≤ (1 + δ)J(Q,CG)\nNote that analogously, CG is the global minimum in the original space, hence\nJ(Q,CG) ≤ J(Q,C′G)\nn′ n J(Q,CG) ≤ n′ n J(Q,C′G) ≤ (1− δ)−1J(Q′,C′G) (22)"
    }, {
      "heading" : "4 Clusterability and the dimensionality re-",
      "text" : "duction\nIn the literature a number of notions of so-called clusterability have been introduced. Under these notions of clusterability algorithms have been developed clustering the data nearly optimally in polynomial times, when some constraints are matched by the clusterability parameters.\nIt seems therefore worth to have a look at the issue if the aforementioned projection technique would affect the clusterability property of the data sets.\nLet us consider, as representatives, the following notions of clusterability, present in the literature:\n• Perturbation Robustness meaning that small perturbations of distances / positions in space of set elements do not result in a change of the optimal clustering for that data set. Two brands may be distinguished: additive [2] and multiplicative ones [9] (the limit of perturbation is upper-bounded either by an absolute value or by a coefficient). The s-Multiplicative Perturbation Robustness (0 < s < 1) holds for a data set with d1 being its distance function if the following holds. Let C be an optimal clustering of data points for this distance. Let d2 be any distance function over the same set of points such that for any two points u,v, s · d1(u,v) < d2(u,v) < 1s · d1(u,v). Then the same clustering C is optimal under the distance function d2.\nThe s-Additive Perturbation Robustness (0 < s < 1) holds for a data set with d1 being its distance function if the following holds. Let C be an optimal clustering of data points for this distance. Let d2 be any distance function over the same set of points such that for any two points u,v, d1(u,v) − s < d2(u,v) < ·d1(u,v) + s. Then the same clustering C is optimal under the distance function d2. Subsequently we are interested only in the multiplicative version.\n• σ-Separatedness [16] meaning that the cost J(Q,Ck) of optimal clustering Ck of the data set Q into k clusters is less than σ2 (0 < σ < 1) times the cost J(Q,Ck−1) of optimal clustering Ck−1 into k− 1 clusters J(Q,Ck−1) < σ 2J(Q,Ck−1)\n• (c, σ)-Approximation-Stability [7] meaning that if the cost function values of two partitions Ca,Cb differ by at most the factor c > 1 (that is c ·J(Q,Ca) ≥ J(Q,Cb) and c ·J(Q,Cb) ≥ J(Q,Ca)), then the distance (in some space) between the partitions is at most σ (d(Ca,Cb) < σ for some distance function d between partiitions). As Ben-David [8] recalls, this implies the uniqueness of optimal solution.\n• β-Centre Stability [6] meaning, for any centric clustering, that the distance of an element to its cluster centre is β > 1 times smaller than the distance to any other cluster centre under optimal clustering.\n• (1 + β) Weak Deletion Stability [5] (β > 0) meaning that given an optimal cost function value OPT for k centric clusters, the cost function of a clustering obtained by deleting one of the cluster centres and assigning elements of that cluster to one of the remaining clusters should be bigger than (1 + β) ·OPT .\nLet us first have a look at the σ-Separatedness. Let CG,k denote an optimal clustering into k clusters in the original space and C′G,k in the projected space. From properties of k-means we know that J(Q,CG,k) ≤ J(Q,CG,k−1) and J(Q′,C′G,k) ≤ J(Q′,C′G,k−1). From theorem 5 we know that\nn n′ J(Q′,C′G,k) ≤ (1 + δ)J(Q,CG,k)\nand J(Q,CG,k−1) ≤ n\nn′ (1− δ)−1J(Q′,C′G,k−1)\nσ-Separatedness implies that\nσ2 ≥ J(Q,CG,k) J(Q,CG,k−1)\n≥ n n′ (1 + δ)−1J(Q′,CG,k)\nJ(Q,CG′,k−1)\n≥ n n′ (1 + δ)−1J(Q′,C′G,k)\nn n′ (1− δ)−1J(Q′,C′G,k−1) = (1− δ)J(Q′,C′G,k) (1 + δ)J(Q′,C′G,k−1)\nThis implies\nσ2 1 + δ\n1− δ ≥\nJ(Q′,C′G,k)\nJ(Q′,C′G,k−1)\nSo we claim\nTheorem 7. Under the assumptions and notation of Theorem 2, If the data set Q has the property of σ-Separatedness in the original space, then with probability at least 1 − it has the property of σ √\n1+δ 1−δ -Separatedness in the\nprojected space.\nThe fact that this Separatedness increases is of course a defficiency, because clustring algorithms require as low Separatedness as possible (because the clusters are then better separated).\nLet us turn to the (c, σ)-Approximation-Stability. We can reformulate it as follows: if the distance (in some space) between the partitions is more than σ then the cost function values of two partitions differ by at least the factor c > 1. Consider now two partitions C1, C2, with distance over σ in some abstract partition space, not related to the embedding spaces. Then in the original space the following must hold.\nJ(Q,C1) ≥ c · J(Q,C2)\nUnder the projection we get\n(1− δ)−1 n n′ J(Q′,C1) ≥ c · (1 + δ)−1 n n′ J(Q′,C2)\nJ(Q′,C1) ≥ c · 1− δ 1 + δ J(Q′,C2)\nThis result means that\nTheorem 8. Under the assumptions and notation of Theorem 2, if the data set Q has the property of (c, σ)-Approximation- Stability in the original space, then with probability at least 1 − it has the property of (c · 1−δ\n1+δ , σ-\nApproximation Stability property in the projected space.\nLet us now consider s-Multiplicative Perturbation Stability. We claim that\nLemma 5. If the data set Q has the property of s-Multiplicative Perturbation Robustness under the distance d1, and the set Qp is its perturbation with distance d2 such that νd1 ≤ d2 ≤ 1νd1, and s = ν · sp, where 0 < ν, sp < 1, then set Qp has the property of sp-Multiplicative Perturbation Robustness\nProof. Apparently Qp is a perturbation of Q such that both share same optimal clustering. Let Qq be a perturbation of Qp, with distance d3, such that spd2 ≤ d3 ≤ 1spd2. Then sd1 = spνd1 ≤ spd2 ≤ d3 ≤ 1 sp d2 ≤ 1spνd1 = 1 s d1 that isQq is a perturbation ofQ such that both share same optimal clustering. So Qp and Qq share common optimal clustering, hence Qp has the property of sp-Multiplicative Perturbation Robustness\nWe claim that\nLemma 6. Under the assumptions and notation of Theorem 2, if the data set Q has the property of s-Multiplicative Perturbation Robustness with s < 1−δ, and if CG is the global optimum of k-means in Q, then it is also the global optimum in Q′ with probability at least 1−\nProof. Assume the contrary that is that in Q′ some other clustering C′G is the global optimum. Let us define the distance d1(i, j) = ‖xi − xj‖ and d2(i, j) = n n′ ‖x′i−x′j‖. The distance d2 is a realistic distance in the coordinate system C as we assume n > n′. As the k-means optimum does not change under rescaling, so C′G is also an optimal solution for clustering task under d2. But\nsd1(i, j) < (1−δ)d1(i, j) ≤ d2(i, j) ≤ (1+δ)d1(i, j) < (1−δ)−1d1(i, j) < s−1d1(i, j)\nhence the distance d2 is a perturbation of d1 and hence CG should be optimal under d2 also. We get a contradiction. So the claim of the lemma must be true.\nThis implies that\nTheorem 9. Under the assumptions and notation of Theorem 2, if the data set Q has the property of s-Multiplicative Perturbation Robustness with factor s < spν (1−δ)2 1+δ\n(0 < sp, ν < 1) in the original space, then with probability at least 1 − 2 it has the property of sp-Multiplicative Perturbation Robustness in the projected space.\nProof. The Lemma 6 implies that the global optima of the original and projected spaces are identical. So assume that in the original space for the distance d1(i, j) = ‖xi − xj‖ CG is the optimal clustering. Then under projection d′1(i, j) = ‖x′i − x′j‖ we have the same optimal clustering.\nFor a perturbation with factor sp in the projected space define the distance d′2(i, j) = ‖y′i−y′j‖ where for any i let y′i be a perturbation of x′i. We will be done if we can demonstrate that d′2 yields the same optimum in the projected space as d′1 does. For any i let yi be some point in the original spacve such that y′i is its projection to the projected space. We will treat yi as an image of xi and will subsequently show that the set of these points yi can be treated as a perturbation of xi with the factor s.\nFor each counterpart d2(i, j) = ‖yi − yj‖ of d′2 in original space (1 + δ)−1 n\nn′ d′2(i, j) ≤ d2(i, j) ≤ (1− δ)−1 nn′d ′ 2(i, j) holds. As spd ′ 1(i, j) ≤ d′2(i, j) ≤\n(sp) −1d′1(i, j) and (1− δ)d1(i, j) ≤ nn′d ′ 1(i, j) ≤ (1 + δ)d1(i, j) we obtain\nsd1(i, j) < (1+δ) −1sp(1−δ)d1(i, j) ≤ (1+δ)−1sp\nn n′ d′1(i, j) ≤ (1+δ)−1 n n′ d′2(i, j)\n≤ d2(i, j) ≤ (1− δ)−1 n\nn′ d′2(i, j)\n≤ (1− δ)−1s−1p n\nn′ d′1(i, j) ≤ (1− δ)−1s−1p (1 + δ)d1(i, j) <\n1 s d1(i, j)\nSo d2 is a perturbation of d1 with the factor s. d1 is s-multiplicative perturbation robust, therefore both have the same optimal solution CG. Furthermore d2 has the property of ν(1− δ)-Multiplicative Robustness (see Lemma 5). Therefore its counterpart d′2 has the same optimum clustering CG as d2 (see Lemma 6), hence as d1, hence as d′1.\nRecall that d′2 was selected as any perturbation of d ′ 1 with factor sp. And it turned out that it yields the same optimal solution as d′1. So with high probability (factor 2 is taken as we deal with two data sets, comprising points xi and yi) d′1 possesses sp-Multiplicative Perturbation Robustness in the projected space.\nWe claim\nTheorem 10. Under the assumptions and notation of Theorem 2, if the data set Q has both the property of β-Centre Stability and s-Multiplicative Perturbation Robustness with s < 1 − δ in the original space, then with probability at least 1− it has the property of β √ 1−δ 1+δ\n-Centre Stability in the projected space.\nProof. The s-Multiplicative Perturbation Robustness ensures that both the original and the projected space share same optimal clustering C. Consider a data point xi and a cluster C ∈ C not containing i. Then xi, µ(C) and µ(C ∪ {i}) are colinear. So are x′i, µ′(C) and µ′(C ∪ {i}), that is the respective (linear) projections. Furthermore ‖xi−µ(C∪{i})‖‖µ(C)−µ(C∪{i})‖ = |C| 1 , hence\n‖xi − µ(C)‖ = |C|+1|C| ‖xi − µ(C ∪ {i})‖. Likewise ‖x′i−µ ′(C∪{i})‖ ‖µ′(C)−µ′(C∪{i})‖ = |C| 1\n. Upon projection the distance to own cluster centre can increase relatively by √ 1 + δ and to the C ∪ {i} centre can decrease by √ 1− δ, see Lemma 1. That means ‖x′i − µ′(C(i))‖2 ≤ (1 + δ)n ′ n ‖xi − µ(C(i))‖2 and (1− δ)−1‖x′i − µ′(C ∪ {i})‖2 ≥ n′ n ‖xi − µ(C ∪ {i})‖2. Due to the aforementioned relations ‖x′i − µ′(C)‖2 ≥ (1 − δ)n ′ n ‖xi − µ(C)‖2. Due to β-Centre-Stability in the original space we had: β2‖xi − µ(C(i))‖2 < ‖xi − µ(C)‖2. Due to the aforementioned relations we have\n‖x′i − µ′(C)‖2 ≥ (1− δ) n′\nn ‖xi − µ(C)‖2\n> β2(1− δ)n ′\nn ‖xi − µ(C(i))‖2 ≥ β2 1− δ 1 + δ ‖x′i − µ′(C(i))‖2\nThat is ‖x′i−µ′(C)‖ > β √ 1−δ 1+δ ‖x′i−µ′(C(i))‖ Hence the data centre stability\ncan drop to β √\n1−δ 1+δ .\nWe claim\nTheorem 11. Under the assumptions and notation of Theorem 2, if the data set Q has both the property of (1 + β) Weak Deletion Stability and sMultiplicative Perturbation Robustness with s < 1 − δ in the original space, then with probability at least 1 − it has the property of (1 + β)1−δ\n1+δ Weak\nDeletion Stability in the projected space.\nProof. The s-Multiplicative Perturbation Robustness ensures that both original and the projected space share same optimal clustering. Let this optimal clustering be called Co. By C denote any clustering obtained from Co by deletion of one cluster centre and assigning cluster elements to one of the remaining clusters. By the assumption of (1+β)-Weak Deletion stability (1 + β)J(Q,Co) ≤ J(Q,C). Theorem 2 implies that (1− δ)n′\nn J(Q,C) ≤ J(Q′,C) and (1+ δ)−1J(Q′,Co) ≤\nn′ n J(Q,Co). Therefore J(Q′,C) ≥ (1 − δ)n′ n J(Q,C) ≥ (1 + β)(1 − δ)n′ n J(Q,Co) ≥ (1 + β)(1− δ)(1 + δ)−1J(Q′,Co) which implies the claim."
    }, {
      "heading" : "5 Numerical Experiments on Some Aspects",
      "text" : "of Our Approach\nNote that we have two formulas for computing the reduced space dimensionality n′, the formula (11) and (4). The latter does not engage the original\nDependence of reduced dimensionality n' on failure prob. epsilon\nDependence of reduced dimensionality n' on error range delta\nDependence of reduced dimensionality n' on original dimensionality n\nDiscrepancy between original and projected squared distances\ndimensionality n, while it is explicit in n′. The value of n′ in the former depends on n, however n′ can be only computed iteratively.\nLet us investigate the differences between n′ computation in both cases. Let us check the impact of the following parameters: n - the original dimensionality (see table 4 and figure 4), δ - the limitation of deviation of the distances between data points in the original and the reduced space (see table 3 and figure 3), m - the sample size (see table 1 and figure 1), as well as - the maximum failure probability of the ”JL” transformation (see table 2 and figure 2). Note that in all figures the X-axis is on log scale.\nAs visible in figure 4 the value of n′ from the explicit formula does not depend on the original dimensionality n. The value computed from the implicit formula approaches the explicit value quite quickly with the growing dimensionality n.\nOn the other hand, the implicit n′ departs from the explicit one with growing sample size m, as visible in fig. 1. Both grow with increasing m.\nIn fig. 2 we see that when we increase the acceptable failure rate , the requested dimensionality n′ drops, whereby the implicit one approaches the explicit one.\nFig. 3 shows that the requested dimensionality drops quite quickly with increased relative error range δ till a kind of saturation is achieved. At extreme ends of δ implicit and explicit n′ formulas converge to one another.\nThe behaviour of explicit n′ is not surprising, as it is visible directly from the formula (4). The important insight here is however the required dimensionality of the projected data, of hundreds of thousands for realistic , δ. So the random projection via the Johnson-Lindenstrauss Lemma is not yet another dimensionality reduction technique. It is suitable for cases where techniques like PCA are not feasible computationally.\nThe behaviour of implicit n′ for the case of increasing original dimensionality n is as expected - the explicit n′ reflects the ”in the limit” behaviour of the implicit formulation. The convergence for extreme values of δ is intriguing. The discrepancy for and the divergence for growing m indicate that there is still space for better explicit formulas on n′. Especially it is worth investigating for increasing m as the processing becomes more expensive in the original space when m is increasing.\nIn order to give an impression how effective the random projection is, see fig. 5. It illustrates the distribution of discrepancies between squared distances in the projected and in the original spaces. The discrepancies are expressed as\n‖f(u)− f(v)‖2\n‖u− v‖2\nError margin versus gap between clusters\nOne can see that they correspond quite well to the imposed constraints. As the application for k-means clustering, we see in fig. 6 that the bigger the relative gap between clusters, the larger the error value δ is permitted, if class membership shall not be distorted by the projection."
    }, {
      "heading" : "6 Previous work",
      "text" : "Note that if we would set (close) to 1, and expand by Taylor method the ln function in denominator of the inequality (4) to up to three terms then we get the value of n′ from equation (2.1) from the paper [11]:\nn′ ≥ 4 lnm δ2 − δ3\nNote, however, that setting to a value close to 1 does not make sense as\nwe want to keep rare the event that the data does not fit the interval we are imposing.\nThough one may be tempted to view our results as formally similar to those of Dasgupta and Gupta, there is one major difference. Let us first recall that the original proof of Johnson and Lindenstrauss [13] is probabilistic, showing that projecting the m -point subset onto a random subspace of O(lnm/ 2) dimensions only changes the (squared) distances between points by at most 1 − δ with positive probability. Dasgupta and Gupta showed that this probability is at least 1/m, which is not much indeed. In order to get failure probability below say 0.05%, one needs to repeat the random projection and checking of distances r times, with such r that > (1− 1\nm )r.\nIn case of m = 1, 000 this means over r = 2, 995 repetitions, and with m = 1, 000, 000 - over r = 2, 995, 000 repetitions,\nIn this paper we have shown that this success probability can be raised to 1− for an given in advance. Hereby the increase of target dimensionality is small enough compared to Dasgupta and Gupta formula, that our random projection method is orders of magnitude more efficient. A detailed comparison is contained in the tables 5, 6, 7, 8. We present in these tables n′ computed using our formulas with those proposed by Dasgupta and Gupta as well as we present the required number of repetition of projection onto sampled subspaces in order to obtain a faithful distance discrepancies with reasonable probability. Dasgupta and Gupta generally obtain several times lower number of dimensions. However, as stated in the introduction, the number of repeated samplings annihilates this advantage and in fact a much\nhigher burden when clustering is to be expected. Note that the choice of n′ has been estimated by [1]\nn′ ≥ (4 + 2γ) lnm δ2 − δ3\nwhere γ is some positive number. They propose a projection based on two or three discrete values randomly assigned instead of ones from normal distribution. With the quantity γ they control the probability that a single element of the set Q leaves the predefined interval ±δ. They do not bother about controlling the probability that none of the elements leaves the interval of interest. Rather, they derive expected values of various moments.\nLarsen and Nelson [14] concentrate on finding the highest value of n′ for which Johnson-Lindenstrauss Lemma does not hold demonstrating that the value they found is the tightest even for non-linear mappings f . Though not directly related to our research, they discuss the other side of the coin, that is the dimensionality below which at least one point of the data set has to violate the constraints."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper we investigated a novel aspect of the well known and widely explored and exploited Johnson-Lindenstrauss lemma on the possibility of dimensionality reduction by projection onto a random subspace.\nThe original formulation means in practice that we have to check whether or not we have found a proper transformation f leading to error bounds within required range for all pairs of points, and if necessary (and it is theoretically necessary very frequently), to repeat the random projection process over and over again.\nWe have shown here that it is possible to determine in advance the choice of dimensionality in the random projection process as to assure with desired certainty that none of the points of the data set violates restrictions on error bounds. This new formulation can be of importance for many data mining applications, like clustering, where the distortion of distances influences the results in a subtle way (e.g. k-means clustering).\nVia some numerical examples we have pointed at the real application areas of this kind of projections, that is problems with high number of dimensions, starting with dozens of thousands and hundreds of thousands of dimensions.\nAdditionally, our reformulation of the JL Lemma permits to preserve some well-known clusterability properties at the projection."
    } ],
    "references" : [ {
      "title" : "Database-friendly random projections: Johnsonlindenstrauss with binary coins",
      "author" : [ "Dimitris Achlioptas" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Clusterability: A theoretical study",
      "author" : [ "Margareta Ackerman", "Shai Ben-David" ],
      "venue" : "Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Approximate nearest neighbors and the fast johnson-lindenstrauss transform",
      "author" : [ "Nir Ailon", "Bernard Chazelle" ],
      "venue" : "In Proceedings of the Thirtyeighth Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "k-means++: the advantages of careful seeding",
      "author" : [ "D. Arthur", "S. Vassilvitskii" ],
      "venue" : "Proc. of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Stability yields a ptas for k-median and k-means clustering",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Center-based clustering under perturbation stability",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "Inf. Process. Lett.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Approximate clustering without the approximation",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta" ],
      "venue" : "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Computational feasibility of clustering under clusterability",
      "author" : [ "Shai Ben-David" ],
      "venue" : "assumptions. https://arxiv.org/abs/1501.00437,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Are stable instances easy",
      "author" : [ "Yonatan Bilu", "Nathan Linial" ],
      "venue" : "Comb. Probab. Comput.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Random projection estimation of discrete-choice models with large choice sets, arxiv:1604.06036, 2016",
      "author" : [ "Khai X. Chiong", "Matthew Shum" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "An elementary proof of a theorem of johnson and lindenstrauss",
      "author" : [ "Sanjoy Dasgupta", "Anupam Gupta" ],
      "venue" : "Random Struct. Algorithms,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "Nearest-neighbor-preserving embeddings",
      "author" : [ "Piotr Indyk", "Assaf Naor" ],
      "venue" : "ACM Trans. Algorithms,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Extensions of lipschitz mappings into a hilbert space. In Conference in modern analysis and probability (New Haven, Conn., 1982)",
      "author" : [ "W.B. Johnson", "J. Lindenstrauss" ],
      "venue" : "Also appeared in volume 26 of Contemp. Math.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1984
    }, {
      "title" : "Optimality of the johnsonlindenstrauss",
      "author" : [ "Kasper Green Larsen", "Jelani Nelson" ],
      "venue" : "lemma. CoRR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "On variants of the johnson-lindenstrauss lemma",
      "author" : [ "Jiŕı Matousek" ],
      "venue" : "Random Struct. Algorithms,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "The effectiveness of lloyd-type methods for the k-means problem",
      "author" : [ "Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Strong consistency of reduced k-means clustering",
      "author" : [ "Yoshikazu Terada" ],
      "venue" : "Scand. J. Stat.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Techniques like reduced k-means [17], PCA (Principal Component Analysis), Kernel PCA, LLE (Locally Linear Embedding), LEM (Laplacian Eigenmaps), MDS (Metric Multidimensional Scaling), Isomap, SDE (Semidefinite Embedding), just to mention a few.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "The starting point here is the Johnson-Lindenstrauss Lemma [13].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "by k-means algorithm [4].",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "[11, 1, 3, 12, 14, 10].",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "[11, 1, 3, 12, 14, 10].",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "[11, 1, 3, 12, 14, 10].",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "[11, 1, 3, 12, 14, 10].",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "[11, 1, 3, 12, 14, 10].",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "[11, 1, 3, 12, 14, 10].",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "[15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "It is claimed afterwards that this mapping is distance-preserving not only for a single vector, but also for large sets of points with some, usually very small probability, as Dasgupta and Gupta [11] maintain.",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 10,
      "context" : "We postpone the proof of the theorems 2-5 till section 3, as we need first to derive the basic theorem 6 in section 2 which is essentially based on the results reported by Dasgupta and Gupta [11].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 10,
      "context" : "Dasgupta and Gupta [11] in their Lemma 2.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Two brands may be distinguished: additive [2] and multiplicative ones [9] (the limit of perturbation is upper-bounded either by an absolute value or by a coefficient).",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "Two brands may be distinguished: additive [2] and multiplicative ones [9] (the limit of perturbation is upper-bounded either by an absolute value or by a coefficient).",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "• σ-Separatedness [16] meaning that the cost J(Q,Ck) of optimal clustering Ck of the data set Q into k clusters is less than σ (0 < σ < 1) times the cost J(Q,Ck−1) of optimal clustering Ck−1 into k− 1 clusters J(Q,Ck−1) < σ J(Q,Ck−1) • (c, σ)-Approximation-Stability [7] meaning that if the cost function values of two partitions Ca,Cb differ by at most the factor c > 1 (that is c ·J(Q,Ca) ≥ J(Q,Cb) and c ·J(Q,Cb) ≥ J(Q,Ca)), then the distance (in some space) between the partitions is at most σ (d(Ca,Cb) < σ for some distance function d between partiitions).",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "• σ-Separatedness [16] meaning that the cost J(Q,Ck) of optimal clustering Ck of the data set Q into k clusters is less than σ (0 < σ < 1) times the cost J(Q,Ck−1) of optimal clustering Ck−1 into k− 1 clusters J(Q,Ck−1) < σ J(Q,Ck−1) • (c, σ)-Approximation-Stability [7] meaning that if the cost function values of two partitions Ca,Cb differ by at most the factor c > 1 (that is c ·J(Q,Ca) ≥ J(Q,Cb) and c ·J(Q,Cb) ≥ J(Q,Ca)), then the distance (in some space) between the partitions is at most σ (d(Ca,Cb) < σ for some distance function d between partiitions).",
      "startOffset" : 267,
      "endOffset" : 270
    }, {
      "referenceID" : 7,
      "context" : "As Ben-David [8] recalls, this implies the uniqueness of optimal solution.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "• β-Centre Stability [6] meaning, for any centric clustering, that the distance of an element to its cluster centre is β > 1 times smaller than the distance to any other cluster centre under optimal clustering.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "• (1 + β) Weak Deletion Stability [5] (β > 0) meaning that given an optimal cost function value OPT for k centric clusters, the cost function of a clustering obtained by deleting one of the cluster centres and assigning elements of that cluster to one of the remaining clusters should be bigger than (1 + β) ·OPT .",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "1) from the paper [11]:",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "Table 5: Comparison of effort needed for k-means under our dimensionality reduction approach and that of Dasgupta and Gupta [11], depending on sample size m .",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Table 6: Comparison of effort needed for k-means under our dimensionality reduction approach and that of Dasgupta and Gupta [11], depending on failure prob.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Table 7: Comparison of effort needed for k-means under our dimensionality reduction approach and that of Dasgupta and Gupta [11], depending on error range δ .",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Table 8: Comparison of effort needed for k-means under our dimensionality reduction approach and that of Dasgupta and Gupta [11], depending on original dimensionality n .",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 12,
      "context" : "Let us first recall that the original proof of Johnson and Lindenstrauss [13] is probabilistic, showing that projecting the m -point subset onto a random subspace of O(lnm/ ) dimensions only changes the (squared) distances between points by at most 1 − δ with positive probability.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "Note that the choice of n′ has been estimated by [1] n′ ≥ (4 + 2γ) lnm δ2 − δ3 where γ is some positive number.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "Larsen and Nelson [14] concentrate on finding the highest value of n′ for which Johnson-Lindenstrauss Lemma does not hold demonstrating that the value they found is the tightest even for non-linear mappings f .",
      "startOffset" : 18,
      "endOffset" : 22
    } ],
    "year" : 2017,
    "abstractText" : "In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation f of the data points into lower dimensional space such that all of them fall into predefined error range δ. We formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability 1− all of them fall into predefined error range δ for any user-predefined failure probability . This result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma. In particular, we take a closer look at the k-means algorithm and prove that a good solution in the projected space is also a good solution in the original space. Furthermore, under proper assumptions local optima in the original space are also ones in the projected space. We define also conditions for which clusterability property of the original space is transmitted to the projected space, so that special case algorithms for the original space are also applicable in the projected space.",
    "creator" : "LaTeX with hyperref package"
  }
}