{
  "name" : "1605.03072.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-Supervised Representation Learning based on Probabilistic Labeling",
    "authors" : [ "Ershad Banijamali", "Ali Ghodsi" ],
    "emails" : [ "aghodsib}@uwaterloo.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nAs the amount of data grows rapidly, the process of extracting meaningful information becomes more and more challenging. Among these challenges, having no access to the data categories is very crucial. In the real world, the amount of labeled data compared to unlabeled data is almost negligible. On the other hand, determining data categories, or acquiring labels, is expensive for many reasons, e.g. it is extremely time-consuming for large datasets and usually needs human supervision. The importance of the methods that can benefit from this fast growing amount of unlabeled data has significantly increased.\nSemi-supervised learning is the area of utilizing unlabeled data combined with, usually very smaller set of, labeled data to gain better data representation or classification accuracy. There are wide range of applications for semi-supervised learning including, but not limited to, text classification [1], [14], genetics and medical research [4], [21], and object detection [16]."
    }, {
      "heading" : "A. Related Works",
      "text" : "In recent years, semi-supervised learning has attracted attention from many researchers and several algorithms have been designed for semi-supervised learning that can relate to the present work.\nGraph-based algorithms, which usually define a loss function for labeled data and use unlabeled as a regularizer, are important classes of semi-supervised learning methods. Example of this class are [5], [26] that try to convey the labels to the unlabeled data over the edges of the graph. Label propagation has been tried in many other articles including [22] which, inspired by the idea of locally linear embedding (LLE) [17], assumes the labels of data points can be linearly constructed by the labels of their adjacent samples in an sparse neighborhood and [25], which tries to propagate the labels over pairs of data points. Transductive support vector machines (TSVM) is another class of algorithms, used by [7], in which the goal is to maximize the margin for both unlabeled and labeled points.\nHowever, an important point in semi-supervised learning is that there exists no guarantee that the use of unlabeled data will help us to achieve a better representation of the data. In an excellent work by Cozman et. al [8], the important question ”Do Unlabeled Data Improve or Degrade Classification Performance?” was addressed and it was shown that, not only unlabeled data can be useless in learning a new representation, but also it can degrade the performance of the algorithm in many cases. To reduce the likelihood of having destructive unlabeled samples, there is a set of assumptions about the structure of the underlying distribution of data, including smoothness assumption, clustering assumption, and manifold assumption, that researchers usually make one of them."
    }, {
      "heading" : "B. Contribution",
      "text" : "Most of the semi-supervised algorithms include two objective functions for labeled and unlabeled data points, which are optimized jointly. In this paper, we also start with deriving two separate objective functions. For the labeled points, we look for a mapping which maximizes the dependency of the transformed points and their labels, and for the unlabeled points we look for a mapping that keeps them near their labeled neighbors. However, by some manipulations, we then combine these two functions and solve the problem by optimizing a single objective function. Further investigations show that the objective function can also be obtained by a specific assignment of labels to the points. We call this probabilistic labeling. This labeling not only provides the objective function\nar X\niv :1\n60 5.\n03 07\n2v 3\n[ cs\n.L G\n] 1\n5 Se\np 20\n16\nof our problem much faster and easier, but also enables us to obtain a bound on the performance of the algorithm based on probability of classification error in the original space. This bound shows the maximum deviation of the objective function value from its optimal value, when we know the true label of all data points in our dataset.\nWe will also derive the kernelized version of the algorithm, which is very helpful when the linear transformation does not provide a good representation of data in the target space. The results of applying the algorithm on real and synthetic datasets will be presented."
    }, {
      "heading" : "II. BACKGROUND: HILBER-SCHMIDT INDEPENDENCE CRITERION (HSIC)",
      "text" : "The Hilbert-Schmidt Independence Criterion (HSIC) is a very useful tool in statistics to measure the dependence between two random variables [11]. We use HSIC in our proposed method. Following is a short description about this measure.\nDefinition 1. Suppose X and Y are two domain sets. Let φ and ψ be two mappings that map X and Y to their corresponding Reproducing Kernel Hilbert Space (RKHS) F and G. The Borel probability measure over X ×Y is denoted by pxy . Then HSIC is defined as the following:\nHSIC(pxy,F ,G) =‖ Ex,y[φ(x)⊗ ψ(y)]− µx ⊗ µy ‖2HS (1)\nwhere µx and µy are mean of φ(x) and ψ(y), respectively, and ⊗ is the tensor product. ‖ . ‖HS is also the Hilbert-Schmidt norm.\nThe following theorem by [11] shows the relation between HSIC and independence of x and y, when (x, y) is drawn from pxy .\nTheorem 1. Suppose k and l are reproducing kernels of RKHS’s F and G on the compact domains X and Y . Assume, without loss of generality, ‖ f ‖∞≤ 1 and ‖ g ‖∞≤ 1 for all f ∈ F and g ∈ G. Then, HSIC(pxy,F ,G) is zero, if and only if, x and y are independent.\n1) Empirical HSIC: The empirical HSIC was also defined in [11] to show that HSIC is, in fact, a practical criterion.\nDefinition 2. Let Z = {(x1, y1), (x2, y2), ..., (xm, ym) ⊆ X× Y be a series of m independent observation drawn from pxy . An estimation of HSIC is given by:\nHSIC(Z,F ,G) = 1\n(m− 1)2 tr(KHmLHm) (2)\nwhere K and L are matrices containing the evaluation of the reproducing kernel of F and G respectively, and Hm is the centering matrix of size m, Hm = I − 1m11 >."
    }, {
      "heading" : "III. ALGORITHM SSRL-PL",
      "text" : "Let X be a unit ball in d-dimensional space and X contain n observations from X in form of a d × n matrix, i.e.\nX = [x1,x2, ...,xn] where each xi ∈ Rd is a column vector. According to this definition, ‖ xi ‖2≤ 1 ∀i = {1, .., n}, where ‖ . ‖2 is the L-2 norm of the vector.\nSuppose from n samples, l of them have labels and the rest u = n − l are unlabeled. Let d × l matrix XL and d × u matrix XU contain the set of labeled and unlabeld samples, respectively. Without loss of generality, assume X is ordered such that the first l samples are labeled, i.e. X = [XL, XU ].\nSuppose there are also C classes of data points {1, 2, ..., C}. Variable yi denotes the label of data point xi in XL, which indicate the class to whixh xi belongs. For data points xj in XU , yj is unknown.\nThe goal of our algorithm is to map the data to a pdimensional space by finding a linear transformation, that we denote it by V . V is a d×p matrix while d can be much larger than p. Let zi be the low-dimensional representation of data point xi. Then: zi = V >xi, where V > is the transposed of V . Accordingly, we can obtain the matrix Z, which contains the low-dimensional data points as follows:\nZ = V >X (3)\nConsidering the dataset as a graph with n nodes, the transformation V transforms labeled and unlabeled points as follows.\nLabeled Data: For the labeled data, we try to find a mapping that maximizes the dependency between low-dimensional data points and the labels, based on the HSIC measure. Therefore, we will have the following objective:\narg max V\n1\n(l − 1)2 tr(Z>LZLHlKlHl)\n= arg max V\n1\n(l − 1)2 tr(X>L V V >XLHlKlHl)\n(4)\nwhere we used linear kernel for the data points in pdimensional space and Kl is a kernel over labels. A kernel commonly used for labels is the delta kernel. Entry (i, j) of a delta kernel is 1 if xi and xj have the same label and is 0 otherwise. Delta kernel can be interpreted as the adjacency matrix of a graph whose nodes are labeled data points. Regardless of the relative position of the nodes in this graph, there is an edge between two nodes if they have the same label. All points have also a self-loop. We will use this kernel for labels throughout this paper.\nIf we do not impose any constraint on V , the function can be unbound. A good choice for the constraint which also guarantees the orthonormality of the basis of the p-dimensional space is V >V = I , where I is the identity matrix. By adding this constraint and also re-arranging the matrices inside the trace, we will have:\narg max V\n1\n(l − 1)2 tr(V >XLHlKlHlX>L V )\nsubject to V >V = I (5)\nFor the sake of simplicity, we do not write the V >V = I in the next expressions. However, we always consider this constraint in defining objective functions.\nThe objective function in (5) can be recast using X (all data points, labeled and unlabeled) and a kernel Kn defined over X. Kn is an n×n matrix with all zero entries except the first l × l block, which is equal to Kl. Then, we will have:\narg max V\n1\n(n− 1)2 tr(V >XHnKnHnX>V ) (6)\nThe solution to (6) is the eigenvectors corresponding to the p largest eigenvalues of matrix XHnKnHnX>.\nUnlabeled Data: In the previous part we defined an objective function based on the relation of labeled points with each other. Now we want to define another objective function based the relation of the unlabeled points with the rest of the of the points. The goal here is to find a transformation that preserves the neighborhood between unlabeled data points and their labeled neighbors. We want the unlabeled points to have high similarity with their labeled neighbors in the pdimensional space. This is a rational choice, as a common assumption in semi-supervised learning is that close points in original space are likely to have same labels.\nIf unlabeled data point xi and labeled data point xj are neighbors in d-dimensional space, then zi and zj should have high similarity. Similarity can be defined in different ways but we measure the similarity between two points zi and zj as < z̄i, z̄j >, where < ., . > is the dot product and z̄i is the centered version of zi. Hence, we can define the following objective function, which measures the similarity of neighboring points:\nmax ∑ ij wij < z̄i, z̄j >= max ∑ ij wij z̄ > i z̄j (7)\nwhere 0 ≤ wij ≤ 1 determines the strength of neighborhood between xi and xj . Note that if both of these points are labeled then wij = 0, as we have already taken care of labeled points in Kn. Clearly maximizing this objective function forces points with strong neighborhood (large wij) to have large similarity.\nThe value of wij between two unlabeled points depend on their similarity in term of their neighborhood. That is, their connections to the labeled nodes. For example, if two unlabeled points have strong neighborhood with labeled points from similar classs, then wij is high.\nWe define an n × n matrix W that contains wij’s. Based on our definitions here, the first l × l block of this matrix is all zeros. The objective function in (7) can be written in the following matrix form:∑\nij\nwij z̄ > i z̄j = tr(Z̄>Z̄W ) = tr(HnZ>ZHnW )\n= tr(V >XHnWHnX>V ) (8)\nTherefore, we can also write this objective function similar\nto (4) by multiplying the trace function to the normalization factor 1/(n− 1)2 and adding a constraint on V .\narg max V\n1\n(n− 1)2 tr(V >XHnWHnX>V ) (9)\nCombining (6) and (9), we should find mapping V such that the following objective is maximized.\narg max V\n1\n(n− 1)2 tr(V >XHn(Kn +W )HnX>V ) (10)\nThe inner matrix, Kn + W , is the matrix we needed. The elements in Kn as stated above, define the edge of the corresponding graph globally, i.e. the local position of the labeled nodes (being each other’s neighbors) does not have effect on the edge between them. To be consistent with this global structure of the graph, we extend the edges defined in W , i.e. if unlabeled node xi is connected to the labeled node xj with an edge of weight wij , we also connect xi to all other labeled nodes from the same class as xj by edges of weight wij . This will clearly affect the weight between unlabeled points too. Therefore, the structure of the graph remains global, but connection between unlabeled data points and the rest of the graph is determined based on their local positions.\nElements of Kn + W show our certainty in connecting different nodes in the graph. For labeled nodes, we have 0 and 1 which indicates absolute certainty. For unlabeled nodes, we have 0 ≤ wij ≤ 1, which is an indicator of our uncertainty. To capture these properties, we define a C-dimensional label vector for each data point. For the data point xi, the label vector is denoted by yi. If xi, is labeled then yi is an all zero vector except in position yi, which gets value 1 and it determines the class of xi. If xi is unlabeled, then the cth element of yi, which we denote it by yci , is the probability that xi belongs to class c, and ∑C c=1 y c i = 1. To assign this label probabilities, we look at the set of the k nearest labeled neighbors of the unlabeled points xi. Let us denote this set by Li,k. Then:\nyci = f ci C∑ c=1 f ci\nwhere f ci = ∑\nxj∈Li,k ycj=1\nS(xi,xj) (11)\nwhere S(., .) is a measure of similarity. As nearby unlabeled points are sharing similar labeled points, they are more likely to have similar label probability vectors as well.\nNow lets look at the dot product of label probability vectors of two points xi and xj , i.e. yiy>j (yi’s are defined as row vectors). If xi and xi are labeled, this dot product builds elements of Delta kernel matrix, and if one of the points is unlabeled, the dot product builds elements of W . Therefore, we can build Kn +W simply by Y Y > where Y is an n×C label matrix. The ith row of Y is yi, the label vector of xi. A graph with an adjacency matrix of Y Y > will satisfy all the conditions we wanted for Kn + W . Based on the ordering,\nwe defined for the data points, the first l rows of Y will be corresponding to the labeled points and rest of the rows will be corresponding to the unlabeled data.\nBased on the above descriptions, the objective function in (12), is equal to:\narg max V\n1\n(n− 1)2 tr(V >XHnY Y >HnX>V ) (12)\nThis is the objective we use to find the d× p mapping matrix V . Similar to (6), the columns of the mapping matrix are the eigenvectors corresponding to the top p eigenvalues of XHnY Y >HnX >.\nSuppose Xts is a d × nts matrix that contains nts test samples. It is clear that the test points can be mapped to lowdimensional space simply by: Zts = V >Xts"
    }, {
      "heading" : "IV. KERNELIZED VERSION",
      "text" : "The advantage of a linear transformation is that it explicitly states the basis of new space as a linear combination of the basis of original space. However, in many applications, a linear transformation is not capable of yielding a good representation of the data in the new space. Kernel trick is a useful method in these situations, by which, we first implicitly take the data points to a high dimensional RKHS using a non-linear function and then find the low-dimensional representation. An important aspect of our algorithm is its ability to be stated in the kernelized form.\nBased on the representer theorem, the matrix V , which we find from (12) can be constructed by a linear combination of functions of data points in the Hilbert space. Let φ be the function in the Hilbert space. Then V = φ(X)β. By plugging this in (12) and replacing φ(X)>φ(X) by the kernel matrix KX , we will have:\narg max β\n1\n(n− 1)2 tr(β>KXHnY Y >HnKXβ)\nsubject to β>KXβ = I (13)\nehere β is a n×p transformation matrix. Again, suppose Q = KXHnY Y\n>HnKX . The solution to (13) that determines β is the eigenvectors corresponding to the top p eigenvalues of the generalized eigenvalue problem: Qβ = λKXβ. The pdimensional representation of the data is obtained by: Z = β>KX . A popular kernel, which also works very well in our experiments, is the RBF kernel.\nFor the test data, we should first compute the kernel similarity between test and training samples. Suppose the entries of the n×nts matrix Kts stores the similarities between each pair of training and test data points. Then the p- dimensional test data is: Zts = β>Kts."
    }, {
      "heading" : "V. EXPERIMENT RESULTS",
      "text" : "In this section, the evaluation of applying the above algorithm on different synthetic and real datasets is presented. The\nparameter of the algorithm for each experiment is obtained by leave-one-out cross-validation. We also use RBF kernel similarity in (11)."
    }, {
      "heading" : "A. Toy Example",
      "text" : "First, to demonstrate the capabilities of the SSRL-PL algorithm, we apply it on a toy dataset. The two-moon dataset is a well-known for illustrating the effectiveness of an algorithm on a small set of points. The dataset has 200 samples in two almost balanced classes. Here in Fig. 1, the results of applying the SSRL-PL algorithm on the dataset is demonstrated, for both kernelized and non-kernelized versions. The number of labeled points in each class is 4, i.e. 0.04 of all points. As it can be easily seen, the algorithm is able to identify the correct labels based the label probability assignments. In the kernelized version, the new representation also provides the ability to classify the points using a linear discriminant."
    }, {
      "heading" : "B. Demonstration and Benchmarks",
      "text" : "Here, we present the results of applying the algorithm on more challenging datasets. The USPS dataset is used to show the generalizabilty of the algorithm and some other datasets from UCI repository are used to show the effectiveness of the algorithm in finding a good representation of data that is suitable for classification, despite the fact the dimensionality of the projected space is much lower than the dimensionality of the original space.\n1) USPS: USPS hand-written digit dataset consists of 11000 data points in 10 classes. The classes are balanced and each of them has 1100 images of size 16 × 16 from handwritten digits 0 to 9. Therefore, the dimensionality of samples is 256. In this experiments, we randomly chose 2000 samples from them for training and the rest is only used for the testing. The training set is divided into labeled and unlabeled sets. In fact, 10% of the data is labeled. The models is trained by the training set and the obtained transformation matrix, V , is applied on both training and test sets. Figure 2 shows the result of applying kernelized SSRL-PL, with RBF kernel, on the dataset. The data is mapped into a three-dimensional space. The left-hand side plot shows the result for only labeled samples of the training set and the right-hand side plot shows the result for both the unlabeled samples of the training set and the test set. We can easily see from this plot that the algorithm is generalizable as its performance on the training set and the large unseen test set is the same.\n2) Benchmark datasets: In [6], multiple benchmarks for the task of semi-supervised learning have been introduced for a fair comparison between algorithms. Datasets can be accessed publicly at http://olivier.chapelle.cc/ssl-book/benc hmarks.html. The sets we have used among them are g241c, g241d, and BCI. g241c and g241d both have 1500 data points and 241 dimensions, while BCI has 400 points and 117 dimensions. For each dataset, 12 different splits exist, which divide the data into labeled and unlabeled sets. The number of labeled points based on these splits can be either 10 or 100. Therefore, the average error rate can be easily reported on these benchmarks. The table below shows the results of applying SSRL-PL on these datasets, according to the provided splits. For comparison, the results of some other algorithms are also reported in the table. These algorithms are LapSVM, LapSVMp[13], and Semi-KSC[2]. The first column of the table, which is titled by l, indicates the number of labeled points in the set."
    }, {
      "heading" : "C. Real-world datasets",
      "text" : "Now we examine the performance of the algorithm on six real-world datasets. MNIST-10K is a set of 10000 images of hand-written digits, which are randomly selected from the MNIST dataset. USPS is also set of images of handwritten digits. UMIST a face recognition dataset. The COIL20 and SBdata are sets of images of different objects. Reuters dataset [10], contains 810000 English news stories in different categories. We followed the same procedure in [19] to obtain 10000 samples from this set in 4 categories. Other statistics of the datasets are mentioned in table III.\nWe compare the performance of the algorithm by multiple dictionary learning algorithms. Discriminative K-SVD (DKSVD)[23], Fisher Discrimination Dictionary Learning (FDDL)[20], and Label Consistent K-SVD (LCKSVD)[12] are three supervised dictionary learning algorithms. Also two important semi-supervised dictionary learning algorithm, i.e. OSSDL [24] and S2D2 [18].\nWe first divide the datasets in two parts, 50% for training and 50% for test. Among the training points we choose l points as labeled and the rest unlabeled such that there is at least one labeled point in each class. We repeat this process 10 times. Results in table I show the mean and standard deviation of the classification error on the test set. As we can see, the proposed method in this work outperform the other methods . The two other semi-supervised learning algorithms also perform very well. We also include the dimensionality of the target space in the table, which shows that the reduction in dimensionality is significant.\nFor MNIST-10K and COIL-20 we performed another experiment. Again we first divide the datasets in half. Then for different number of labeled points we apply the SSRL-\nPL algorithm to the resulting training data, for 10 random splits. We compare the performance of the algorithm with two other scenarios. 1) When only use labeled points to find the mapping V , using kernelized version of (5). 2) When we use all the labels of the training data and find the mapping V , using kernelized version of (6). Figure 3 shows the results of these experiments. As we can see the SSRL-PL performs close to the case when we know all the labels, which shows that the algorithm could convey the label information very well. The fluctuation in the whole labeled results is due to the first random split of dataset to test and train sets."
    }, {
      "heading" : "VI. BOUND ON THE PERFORMANCE OF THE SSRL-PL ALGORITHM",
      "text" : "In this section, we derive a bound on the performance of the algorithm. The bound is dependent on the way we assign the probabilities to the unlabeled data points. Of course, there are situations in which the distribution of the observed unlabeled data points in the space can make them useless, and sometimes destructive, in deriving the final mapping. However, this can happen for any semi-supervised algorithm.\nLet us assume a special case of the SSRL-PL which we call winner take all, or WTA for short. In fact, for any label vector we set the element with the highest probability to one and rest of the elements to zero. Therefore, the u bottom rows of the label matrix Y will also have only 0 and 1. Also suppose X is the data matrix which contains n data points.\nConsider the objective in (12). We define the following function:\nfX(V, Y ) 4 =\n1\n(n− 1)2 tr(V >XHnY Y >HnX>V ) (14)\nLet V † be the solution to (12) when there is l labeled points and u = n− l unlabeled data point in the dataset. Assume Yp denotes the label matrix in this situation. In addition, consider another situation in which labels of all data points in X are known. In fact, a completely supervised problem. Let us denote by Yn the label matrix in this scenario. Suppose V ∗ is the optimal mapping for the supervised problem. So:\nV † = arg max V fX(V, Yp) where V † > V † = I V ∗ = arg max V fX(V, Yn) where V ∗>V ∗ = I\nOur goal is to bound the following expression:\nfX(V ∗, Yn)− fX(V †, Yn) (15)\nIn fact, we want to see how much deviation exists between the transformation by V ∗ and the transformation by V †. As fX(V, Y ) is a measure of similarity between the labels and the low-dimensional data points, the bound in (15) shows the extent to which the low-dimensional representation of the data by V † is similar to the real labels of the data points. Note that since V ∗ is optimal solution for fX(V, Yn), the quantity in (15) is always non-negative.\nLemma 2. Suppose X is a d × n matrix of data points and Y is a n×C matrix of labels. Based on the definition in (14)\nfX(V, Y ) = n2\n(n− 1)2 ‖ V >(XY − X̄Ȳ ) ‖2F (16)\nwhere ‖ . ‖F is the Frobenius norm of matrix. X̄d×1 and Ȳ1×C are average of data points and label vectors, respectively, and columns of XY d×C are the weighted average of data points, where weights are columns of Y .\nProof: In Appendix.\nBased on the above lemma, we can conclude that:\narg max V fX(V, Y ) = arg max V\n√ fX(V, Y ) (17)\nV † and V ∗ are still the maximizers of √ fX(V, Yp) and√\nfX(V, Yn), respectively. As we have bounded V by the constraint V >V = I , the values of fX(V, Y ), and subsequently√ fX(V, Y ), are also bounded. Therefore, instead of bounding (15), we can bound the square root of the functions, i.e.:√ fX(V ∗, Yn)− √ fX(V †, Yn) (18)\nWe do this to be able to use the properties of the Frobenius norm (‖ . ‖F is a norm, ‖ . ‖2F is not).\nThe following theorem states the bound on (18). Some intermediate steps go to the appendix.\nTheorem 3. Suppose X is a unit ball in Rd. For n samples drawn iid, according to some probability measure, from X , where the label of only l of them is known and the rest u points are unlabeled, the mapping learned by SSRL-PL algorithm causes at most the following deviation from the mapping that maximizes the HSIC similarity measure between data points and all their revealed real labels.√\nfX(V ∗, Yn)− √ fX(V †, Yn) ≤ 2(2 + √ 2)u\nn− 1 PWTAe\nwhere PWTAe is the error of WTA classifier.\nProof: In Appendix.\nAs we can see from this theorem, the gap between the two functions vanishes when u is reduced, which shows the consistency of the derived bound. Another important obser-\nvation about this bound is its independence to dimensionality of original and target space. Therefore, it can be extended to the kernel version as well. Furthermore, suppose that"
    }, {
      "heading" : "V̂ = arg max",
      "text" : "V lim n →∞ fX(V, Yn). In [3], it has been shown that\nthe deviation of the fX(V, Yn) under V̂ and V ∗ is of order O(1/ √ n). This, together with the results of Theorem 3 can yield a generalization bound on SSRL-PL."
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "We proposed a new algorithm for learning a representation of data when the label information is available for a small portion of the dataset. The algorithm tries to maximize the similarity between the new representation of data and label set, where the label set for unlabeled data is assigned probabilistically and the similarity measure is HSIC. The effectiveness of the proposed algorithm was evaluated on different datasets. We also derived a bound for the proposed algorithm which can be helpful for seeing if the presence of unlabeled data is constructive or destructive.\nIn terms of time complexity, the proposed algorithm is equivalent to a standard eigenvalue decomposition problem for symmetric matrices. This problem can be solved efficiently, for example, by singular value decomposition (SVD) methods. However, for faster implementation, using deep autoencoders that are able to estimate eigenvector of their input would be interesting in the future. Alternatively, one can train a network that maximizes the dependency between data points and label vector by optimizing HSIC as its objective function and stochastic gradient descent algorithm."
    }, {
      "heading" : "VIII. APPENDIX",
      "text" : "Proof of Lemma 2: It is known that: √\ntr(AA>) =‖ A ‖F . We denote the ith column of Y by yi.\nfX(V, Y ) = 1\n(n− 1)2 tr(\nA︷ ︸︸ ︷ V >XHnY A>︷ ︸︸ ︷ Y >HnX >V )\n= 1\n(n− 1)2 ||\nA︷ ︸︸ ︷ V >XHnY ‖2F\n= 1\n(n− 1)2 ‖ V > ([ n∑ i=1 (xi − X̄)y1i n∑ i=1 (xi − X̄)y2i\n... n∑ i=1 (xi − X̄)yCi ]) ‖2F\n= 1\n(n− 1)2 ‖ nV > ( 1 n [ n∑ i=1 xiy 1 i n∑ i=1 xiy 2 i ... n∑ i=1 xiy C i ] − 1\nn [ n∑ i=1 X̄y1i n∑ i=1 X̄y2i ... n∑ i=1 X̄yCi ] ) ‖2F\n= n2\n(n− 1)2 ‖ V >\n([ Xy1 Xy2 ... XyC ] − X̄Ȳ ]) ‖2F\n= n2\n(n− 1)2 ‖ V >(XY − X̄Ȳ ) ‖2F\nObtaining the final result for the theorem, needs bounding both ‖ Ȳn − Ȳp ‖2 and ‖ XYn −XYp ‖F . Lets denote by ei the difference between the real label vector and the assigned label vector of point xi, ei = yni−ypi. For the labeled points ei is an all zero vector, for the unlabeled points, if an error happens, the length of ei is √ 2. So:\n‖ Ȳn − Ȳp ‖2=‖ ē ‖2≤ √ 2u\nn PWTAe (19)\nLet E = Yn − Yp and ec be its cth column. Let also nec be the number of errors for class c. Note that whether a point in class c misclassified as another class or a point in another class misclassfied as c, nec increases by one. The bound for ‖ XYn −XYp ‖F is then the following:\n‖ XYn −XYp ‖F =‖ XE ‖F ≤ C∑ c=1 ‖ Xec ‖2\n= C∑ c=1 ‖ 1 n n∑ i=1 xi(yn c i − ypci ) ‖2\n≤ 1\nn C∑ c=1 nec max i ‖ xi ‖2 ≤ 1 n C∑ c=1 nec ≤ 2u n PWTAe\n(20)\nProof of Theorem 3: Suppose 1 = XYn − XYp and\n2 = Ȳn − Ȳp. According to (18):√ fX(V ∗, Yn)− √ fX(V †, Yn) = n\nn− 1 ×\n( ‖ V ∗>(XYn − X̄Ȳn) ‖F − ‖ V † > (XYn − X̄Ȳn) ‖F ) = n\nn− 1\n( ‖ V ∗>(XYp + 1 − X̄(Ȳp + 2)) ‖F\n− ‖ V †t(XYp + 1 − X̄(Ȳp + 2)) ‖F )\n(a) ≤ n\nn− 1\n( ‖ V ∗>(XYp − X̄Ȳp) ‖F − ‖ V † t (XYp − X̄Ȳp) ‖F\n+ ‖ V ∗> 1 ‖F + ‖ V † > 1 ‖F\n+ ‖ V ∗>X̄ 2 ‖2 + ‖ V † > X̄ 2 ‖2 ) (b) ≤ n\nn− 1\n( ‖ V ∗> 1 ‖F + ‖ V † > 1 ‖F\n+ ‖ V ∗>X̄ 2 ‖2 + ‖ V † > X̄ 2 ‖2 ) (c) ≤ n\nn− 1\n( ‖ 1 ‖F + ‖ 1 ‖F + ‖ 2 ‖2 + ‖ 2 ‖2 ) (d) ≤ 2(2 + √ 2)u\nn− 1 PWTAe\nwhere inequality (a) comes from triangle inequality, (b) from the fact that V † is the maximizer of the ‖ V >(XYp−X̄Ȳp) ‖F , (c) from norm properties, and also the fact that orthonormal transformation does not increase the vector length, and finally (d) from (19) and (20).\nA special case of WTA algorithms is 1-NN. In [9], [15], a bound on the performance of 1-NN was proposed which can be very helpful for our analysis. Given the underlying classconditional distribution function is Lipschitz, the probability of error of 1-NN classifier which uses m points as the training is:\nP 1-NNe ≤ 2P ∗e − C\nC − 1 P ∗e 2 + δ(m) (21)\nwhere P ∗e is the error of Bayesian classifier, C is the number of classes, and δ is a penalty factor as a function of number of training points which vanishes as m→∞. Using (21) we can further bound the algorithm performance which will be independent of the way we assign label to the unlabled data points."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this paper, we present a new algorithm for semi-<lb>supervised representation learning. In this algorithm, we first find<lb>a vector representation for the labels of the data points based<lb>on their local positions in the space. Then, we map the data to<lb>lower-dimensional space using a linear transformation such that<lb>the dependency between the transformed data and the assigned<lb>labels is maximized. In fact, we try to find a mapping that is as<lb>discriminative as possible. The approach will use Hilber-Schmidt<lb>Independence Criterion (HSIC) as the dependence measure.<lb>We also present a kernelized version of the algorithm, which<lb>allows non-linear transformations and provides more flexibility<lb>in finding the appropriate mapping. Use of unlabeled data<lb>for learning new representation is not always beneficial and<lb>there is no algorithm that can deterministically guarantee the<lb>improvement of the performance by exploiting unlabeled data.<lb>Therefore, we also propose a bound on the performance of the<lb>algorithm, which can be used to determine the effectiveness of<lb>using the unlabeled data in the algorithm. We demonstrate the<lb>ability of the algorithm in finding the transformation using both<lb>toy examples and real-world datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}