{
  "name" : "1502.08033.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SciRecSys: a Recommendation System for Scientific Publication by Discovering Keyword Relationships",
    "authors" : [ "Vu Le Anh", "Hai Vo Hoang", "Hung Nghiep Tran", "Jason J. Jung", "Nguyen Tat Thanh" ],
    "emails" : [ "lavu@ntt.edu.vn,", "vohoanghai2@gmail.com,", "nghiepth@uit.edu.vn,", "j2jung@gmail.com", "vohoanghai2@gmail.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Keyword ranking, Keyword similarity, Keyword inference, Scientific Recommendation System, Bibliographical corpus"
    }, {
      "heading" : "1 Introduction",
      "text" : "Keyword-based search engines (Google, Bing Search, and Yahoo) have emerged and dominated the Internet. The success of these search engines are based on the study of keyword relationships and keyword indexes. The task of measuring keywords’ relationships is the basic operation for building the related network of abstract objects which are applied in many problems and applications, such as document clustering, synonym extraction, plagiarism detection problem, taxonomy, search engine optimization, recommendation system, etc. In this work, we focus on two problems. First, we study the rank, inference and similarity of keywords over scientific publications on assuming that the keywords belong to a virtual ontology of keywords. The inference relationship will help us find parents, children, the similarity relationship will help us find siblings of a given keyword and the rank of keywords will determine how important they are. Second, we apply these relationship in our scientific recommendation system, SciRecSys. The first problem is not easy since we have to consider four main factors:\ni- Content factor. The meaning of the keyword should be considered in the context of its paper. The paper itself is determined by its keywords;\n? Corresponding Author: Hai Vo Hoang, Information Technology College, Ho Chi Minh, Vietnam. Phone/Fax: +84 938 642 717. Email: vohoanghai2@gmail.com.\nar X\niv :1\n50 2.\n08 03\n3v 1\n[ cs\n.D L\n] 2\n7 Fe\nii- Publicity factor. The hotness of a topic (or keyword) depends on how popular it is. People always find the hot topic for reading.\niii- Impact factor. In the world of scientific publications the citation is a very important factor. People often follow the citations of a scientific paper for finding the necessary information; iv- Randomness factor. Randomness is very important factor in many real complex systems. Readers sometimes find something for reading quite randomly.\nSciRecSys recommendation system is designed to be a search engine for scientific publications. We want to apply the rank, inference and similarity of keywords to solve three following problems: (i) Ranking papers matched to the given keyword; (ii) Recommending additional keywords related to a given keyword to help the reader navigating the corpus effectively; (iii) Suggesting papers for “Reading more” function in the context the reader is reading a given topic."
    }, {
      "heading" : "2 Related works",
      "text" : "The similarity of keywords is often used as a crucial feature to reveal the relation between objects and many methods to measure it have been proposed. One baseline for similarity measures is using distance functions such as squared Euclidean distance, cosine similarity, Jaccard coefficient, Pearsons correlation coefficient, and relative entropy. Anna Huang et al. [6] has compared and analyzed the effectiveness of these measures in text clustering problem. She represent a document as an m-dimensional vector of the frequency of terms and uses a weighting scheme to reflect their importance through frequencies tf/idf. Singthongchai et al. [12] make keywords search more practical by calculating keyword similarity by combining Jaccard’s, N-Gram and Vector Space. Probabilistic models for similarity measure has been studied in language speech [4,3]. Ido Dagan et al. have proposed a bigram similarity model and used the relative entropy to compute the similarity of keywords [4]. Sung-Hyuk Cha has conducted a comprehensive survey on probability density functions for similarity measures [3]. Ontology-based methods are also exploited to measure the similarity of keywords [2,11,10]. Bollegala et al. [2] use the Wordnet database - ontology of words to measure keywords’ relatedness by extracting lexico-syntactic patterns that indicate various aspects of semantic similarity and modifying four popular co-occurrence measures, including Jaccard, Overlap (Simpson), Dice, and Pointwise mutual information (PMI). Vincent Schickel-Zuber and Boi Falting [11] present a novel similarity measure for hierarchical ontologies called Ontology Structure based Similarity (OSS) that allows similarities to be asymmetric. Snchez et al. [10] presents an ontology-based method relying on the exploitation of taxonomic features available in an ontology.\nMarkov Chain model which properties are studied in [7,8] is used for computing the similarity and ranking [5,1]. Fouss et al.[5] use a stochastic random-walk model to compute similarities between nodes of a graph for recommendation. Vu et al. [1] introduce an N-star model, and demonstrate it in ranking conference and journal problems. Finally, Lops et al. [9] do a thorough review on state-of-the-art and trends of contentbased recommender systems."
    }, {
      "heading" : "3 Backgrounds",
      "text" : ""
    }, {
      "heading" : "3.1 Basic definitions",
      "text" : "Suppose K, P are the sets of keywords and papers respectively. p ∈ P is a paper and A ∈ K is a keyword. K(p) ⊆ K is the set of keywords belonging to p. P (A) = {q ∈ P|A ∈ K(q)} is the set of papers containingA. We assume thatK(p) 6= ∅∧P (A) 6= ∅. Finally, C(p) ⊆ P is the set of papers cited by p. In the case p has no citing, we assume that C(p) = P . It guarantees that C(p) 6= ∅.\nA couple (A, R) is called a ranking system if: (i) A = {a1, . . . , an} is a finite set, and (ii) R is a non-negative function onA (i.e., R : A → [0,+∞)). A ranking score on A can be represented asR = (R(a1), . . . , R(an))T . Furthermore, (A, R) is normalized if RT is normalized (‖ RT ‖= 1).\nSuppose A and B are two events. The inference and similarity of two events A and B are denoted by I(A,B) and S(A,B), respectively. They can be computed as follows:\nI(A,B) = Pr(A and B)\nPr(A) S(A,B) =\nPr(A and B)\nPr(A or B) (1)\nWe have S(A,B) ≤ 1, I(A,B) ≤ 1. S(A,B) is symmetric. I(A,B) = Pr(B|A)."
    }, {
      "heading" : "3.2 Relationships of keywords based on the occurrences",
      "text" : "Let us introduce two normalized ranking scores Rc, Rp on set of keywords, K, based on the occurrences. Rc (c stands for counting) is based on counting the documents containing the given keyword.\nRc(A) = |P (A)|\nΣB∈K|P (B)| (A ∈ K) (2)\nRp (p stands for probability) is based on the probability of the occurrence of the given keyword in the papers.\nRp(A) = 1\n|P| Σp∈P (A)\n1\n|K(p)| (A ∈ K) (3)\n1 |P| is the probability of choosing a paper from the corpus. 1 |K(p)| is the probability of choosing keyword A from the paper p containing |K(p)| keywords. We propose following formulas for measuring the inference and similarity of two keywords A, B based on the occurrences:\nIc(A,B) = |P (A) ∩ P (B)| |P (A)| Sc(A,B) = |P (A) ∩ P (B)| |P (A) ∪ P (B)| (A,B ∈ K) (4)\nP (A)∩P (B) is the set of papers containing both keywords A and B. P (A)∪P (B) is the set of papers containing keywords A or B."
    }, {
      "heading" : "4 Relationships of keywords based on graph of keywords",
      "text" : ""
    }, {
      "heading" : "4.1 Markov Chain model of the reading process",
      "text" : "We propose a Markov Chain model to simulate the reading process which can combine four factors: content, publicity, impact and randomness. We assume that the reader reads\na topic (keyword) A in some paper p at any time (A ∈ K(p)). S = {(A, p) ∈ K × P|A ∈ K(p)} is the set of states. From current state θ = (A, p), the reader will move to new state ξ = (B, q) ∈ S with the conditional probability Pr(θ → ξ) by applying 4 following actions:\n– A1. Same paper - some topic. The reader is interested in current paper, and choose randomly a topic in the same paper with the probability equal to α1. Thus, p = q.\nPr(θ →A1 ξ) = if(p = q, α1 |K(p)| , 0) (5)\n– A2. Same topic - some paper. The reader is interested in current topic, and choose randomly another paper which have the same topic with the probability equal to α2. Thus, A = B.\nPr(θ →A2 ξ) = if(A = B, α2 |P (A)| , 0) (6)\n– A3. Some topic - cited paper. The reader is interested in some cited paper with the probability equal to α3. First, he choose randomly a cited paper and then choose randomly a new topic belonging to the chosen paper. Thus, q ∈ C(p).\nPr(θ →A3 ξ) = if(q ∈ C(p), α3\n|C(p)||K(q)| , 0) (7)\n– A4. Some paper - some topic. The reader stop reading the current paper and choose randomly a new paper with the probability equal to α4.\nPr(θ →A4 ξ) = α4\n|P||K(q)| (8)\nαi > 0 are constants and Σ4i=1αi = 1. From the assumptions, we have:\nPr(θ → ξ) = Σ4i=1Pr(θ →Ai ξ)\nThe necessary condition of the Markov Chain model is that total of the output conditional probability from any state is equal to 1. Here is the formula of the conditions:\nO(θ) = ΣξPr(θ → ξ) = 1\nOur readers can check it by apply the formula 5, 6, 7 and 8. The probability of a reader in state θ = (A, p) is denoted by Pr(θ). We have:\nPr(θ) = Σξ∈SPr(ξ)× Pr(ξ → θ) (9)\nProposition 1. There exists a unique stationary score {Pr(θ)}θ∈S satisfying (9).\nProof. Since a state θ can jump to any state (and itself too) by apply action A4, the Markov Chain is irreducible and aperiodic (see more [7,8]). The Perron-Frobenius theorem [7] states that there exists a unique stationary score {Pr(θ)}θ∈S . {Pr(θ)}θ∈S is determined by following algorithm:\nAlgorithm : Computing Stationary probability {Pr(θ)}θ∈S\n1. begin\n2. k = 0 3. Foreach θ ∈ S do Pr(θ)(0) = 1|S| 4. repeat"
    }, {
      "heading" : "5. k = k + 1 stop = true",
      "text" : ""
    }, {
      "heading" : "6. Foreach θ ∈ S do",
      "text" : "7. Pr(θ)(k) = 0 8. Foreach ξ ∈ S do Pr(θ)(k)+ = Pr(ξ)(k-1)Pr(ξ → θ) 9. if |Pr(θ)(k) − Pr(θ)(k)| > then stop = false 10. until stop 11. Foreach θ ∈ S do Pr(θ) = Pr(k)(θ)"
    }, {
      "heading" : "12. end",
      "text" : ""
    }, {
      "heading" : "4.2 Ranking, Inference and Similarity of keywords",
      "text" : "The rank score of keywordA based on the transition graph,Rg(A) (g stands for graph), is equal to the probability of user reads keyword A. We have:\nRg(A) = Σp∈K(A)Pr(θ) (θ = (A, p) ∈ S) (10)\nLet n0 be a positive integer. For each sequence s = θ1θ2 . . . θn0 ∈ Sn0 , let Pr(s) is the probability of s occurs in Sn0 generated by the Markov Chain model. F ⊆ Sn0 , we denote Pr(F ) = Σs∈FPr(s). For each keyword A, let\nP g(A) = {s = θ1θ2 . . . θn0 ∈ Sn0 |∃i ∈ {1, 2, . . . , n0}, p ∈ P : θi = (A, p)}\nThe formulas for measuring the inference and similarity of two keywords A, B based on the Markov Chain model:\nIg(A,B) = Pr(P g(A) ∩ P g(B))\nPr(P g(A)) Sg(A,B) = Pr(P g(A) ∩ P g(B)) Pr(P g(A) ∪ P g(B))\n(11)\nWe will apply Monte Carlo method for Markov Chain to compute the formulas (11). First, we generate a large enough number of n0-length sequences of states by our Markov Chain model. Then we apply counting techniques for approximating the probabilities and computing the formulas."
    }, {
      "heading" : "5 SciRecSys - Recommendation System",
      "text" : "In this paper, we want to present three scenarios by using SciRecSys.\nUniversal ranking vs. keyword based ranking The reader chooses a keyword A to find some related papers belonging to P (A). Question is “What is the order for sorting P (A)?”. There are two ways for ranking: (i) All results are sorted by only one universal ranking, {Rgu(p)}p∈P or (ii) The order of the results depend on A with the keyword based ranking, {RgA(p)}p∈P (A). We propose Rgu(p) is equal to the probability of user reads p. Hence,\nRgu(p) = ΣA∈K(p),θ=(A,p)Pr(θ) (12) We propose RgA(p) is equal to the probability of state (A, p). Hence,\nRgA(p) = Pr(θ) (θ = (A, p), p ∈ P (A)) (13)\nRecommending keywords The user is in keyword (topic)A. What are the next recommended topics for following situations: (i) similar topics, Sibling(A)? (ii) more detail topics, Child(A)? (iii) more general topics, Parent(A)? Let mc, mp, ms be the parameters of the system. We denote: Child(A) = {B ∈ K|Ig(B,A) > mc}; Parent(A) = {B ∈ K|Ig(A,B) > mf}; Sibling(A) = {B ∈ K|Sg(A,B) > mb}. We remind our readers that keyword A may have many parents or his parent can be his sibling or his child. Recommending papers The user is in paper p. The user can choose the most interesting keyword A ∈ K(p) to read more. What are the next recommended papers for him? The system will request the user to refine recommended papers by choosing a keyword A and then give the recommendation based on the conditional probability change from state θ = (A, p) to the papers by applying actions A2 and A3. Suppose:\nRgθ(q) = ΣB∈K(q),ξ=(B,q)(Pr(θ →A2 ξ) + Pr(θ →A3 ξ)) (14) Finally, the recommended papers are chosen based on Rgθ ranking function."
    }, {
      "heading" : "6 Experiments",
      "text" : "We collect data from DBLP6 and Microsoft Academic Search7 (MAS) to conduct experiments. We choose three datasets for experiments from three different domains: (i) D1 is the publications of ICRA - International Conference on Robotics and Automation; (ii)D2 is the publications of ICDE - International Conference on Data Engineering (iii)D3 is the publications of GI-Jahrestagung - Germany Conference in Computer Science. ICRA and ICDE conferences are one of the most famous and biggest ones in their area. They are chosen for rich publications and citations. GI-Jahrestagung is a smaller conference but its topic is quite various."
    }, {
      "heading" : "6.1 Experiments for rank scores",
      "text" : "We do the experiments for three different ranking scores Rc, Rp and Rg . The values on {αi}4i=1 are chosen for testing different contexts. The rank scores are scaled with the same rate for the convenience. For each two ranking scores i and j, we do examine: (i) the Spearman’s rank correlation coefficient (denote ρij) for monotone checking; (ii) the differences of values: ∆ij(A) = Ri(A)− Rj(A), %∆ij(A) = ∆ij(A)Rj(A) . Here are some interesting observations:\n6 http://dblp.uni-trier.de accessed on December 2013 7 http://academic.research.microsoft.com/ accessed on December 2013\n– The rank scores are monotone but quite different to each others. It comes from that ρij are close to 1 and the average values of %∆ij are very high. For instance with dataset D1, we have: ρcg = 0.97, ρpg = 0.99, ρcp = 0.97. The average of |%∆cg|, |%∆pg|, |%∆cp| are 47.13%, 30.06% and 51.76% respectively."
    }, {
      "heading" : "6.2 Experiments for Inference and Similarity",
      "text" : "We do experiments to compare (Ic, Sc) vs. (Ig ,Sg). For both inference and similarity, we find out some amazing results:\n– Ig & Sg exploits the citation network and the results are domain dependent. Our approach seems to give “hot” results. Let us see Tab. 3(a), which shows similar keywords to Real Time over dataset D1. Sg generates “hot” keyword Humanoid Robot instead of Indexing Terms. This phenomenon also occurs in dataset D2. We assume the cause is Ig & Sg exploit citation network and the paper–keyword relationships to generate more attractive keywords. Additionally, similar keywords to Real Time are quite different between two datasets D1 and D2, so we can say that these methods are domain dependent. – Ig & Sg could help overcome the missing co-occurrence problem. The traditional methods based on occurrences of keywords could not work when there is no cooccurrence in the dataset. For instance, Tab. 4(b) shows that Sc generates only one similar keyword to Sensor Network in small dataset D3. In contrast, Sg could help us find more acceptable similar keywords.\nWe also find out some interesting observations about inference particularly:\n– The inference between two keywords is asymmetric and Ig exploits the citation network to generate a clear inference.\n8 Vu et al.\nUnlike similarity between two keywords, inference is asymmetric, i.e., the inference between A and B differs from the one between B and A. Let us see Tab. 5, the inference from Robot Hand and Robot Arm are almost different. More interestingly, Robot Hand could infer Robot Arm but Robot Arm does not infer Robot Hand, so there is a flow of inference here. Further examination shows that, High Speed is repeated at top 1 on two inference lists generated by Ic. Whereas, High Speed only appears on Ig’s Robot Arm inference list, not on Robot Hand inference list. So, we have a clear inference flow from Robot Hand to Robot Arm then to High Speed with Ig , not with Ic. To explain this difference, we notice that when we infer from Robot Hand, High Speed shares more common publications than Robot Arm , 4 vs. 2. However, Robot Hand has more citations from Robot Arm than High Speed, 9 vs. 5. So we assume that our approach exploits the citation network, which is an asymmetric structure, to generate a clearer inference."
    }, {
      "heading" : "6.3 SciRecSys Recommendation System",
      "text" : "We compare universal ranking scoresRgu(p) and local ranking scoreR g A(p) and conduct experimental models for recommending keywords. We get some interesting results:\n– Universal ranking prefers most popular papers, whereas local ranking prefers papers being not only popular but also focused on a small set of topics. Let us see Tab. 6 (a). Top recommended papers returned by Universal ranking shows a large number of citing/cited papers. Whereas, top recommended papers returned by Local ranking have less citing/cited papers but their keywords are more specific. The average number of keywords in each recommended papers of Local ranking and Universal ranking are 2.25 and 6.75, respectively, except the common\ntop ranked paper with ID = 395393. These numbers in D2 are 3.00 and 10.12. We notice that the average number of keywords in one paper is around 2 and 3 for those two datasets."
    }, {
      "heading" : "7 Conclusion and future works",
      "text" : "We have introduced and studied a new approach on discovering various relationships among keywords from scientific publications. The proposed Markov Chain model combined four main factors of the problem: content, publicity, impact and randomness. The stationary probability of the states in the model helps us ranking and measuring the inference and similarity of keywords.\nWe have proposed SciRecSys recommendation system for navigating the scientific publications efficiently. By applying the relationships among keywords, we have suggested the solutions for the ranking results of a given keyword, related keywords and recommended papers. The experiments have shown that our methods can reflect how\nhot keyword is and overcome the missing co-occurrence problem. Moreover, our approach can exploit the latent information of citation network effectively.\nAs future work, we are planning to i) do experiment on big dataset to upgrade the quality of our ranking and measuring system, ii) study how to combine inference relationship by stationary probability graph with a given ranking systems, iii) investigate the time series in the keyword relationship and the trend prediction problem, and iv) apply model of recommendation systems in various problems, e.g., product recommendation, event recommendation, and so on."
    } ],
    "references" : [ {
      "title" : "A general model for mutual ranking systems",
      "author" : [ "V.L. Anh", "H.V. Hoang", "K.L. Trung", "H.L. Trung", "J.J. Jung" ],
      "venue" : "Nguyen, N.T., Attachoo, B., Trawinski, B., Somboonviwat, K. (eds.) Proceedings of the 6th Asian Conference on Intelligent Information and Database Systems (ACIIDS 2014), Bangkok, Thailand, April 7-9. Lecture Notes in Computer Science, vol. 8397, pp. 211–220. Springer",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Measuring semantic similarity between words using web search engines",
      "author" : [ "D. Bollegala", "Y. Matsuo", "M. Ishizuka" ],
      "venue" : "Williamson, C.L., Zurko, M.E., Patel-Schneider, P.F., Shenoy, P.J. (eds.) Proceedings of the 16th International Conference on World Wide Web (WWW 2007), Banff, Alberta, Canada, May 8-12. pp. 757–766. ACM",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Comprehensive survey on distance/similarity measures between probability density functions",
      "author" : [ "S.H. Cha" ],
      "venue" : "International Journal of Mathematical Models and Methods in Applied Sciences 1(4), 300–307",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Similarity-based models of word cooccurrence probabilities",
      "author" : [ "I. Dagan", "L. Lee", "F.C.N. Pereira" ],
      "venue" : "Machine Learning 34(1-3), 43–69",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation",
      "author" : [ "F. Fouss", "A. Pirotte", "J.M. Renders", "M. Saerens" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering 19(3), 355–369",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Similarity measures for text document clustering",
      "author" : [ "A. Huang" ],
      "venue" : "Proceedings of 6th In New Zealand Computer Science Research Student Conference, Christchurch, New Zealand. pp. 49–56",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The perron-frobenius theorem and the ranking of football teams",
      "author" : [ "J.P. Keener" ],
      "venue" : "SIAM review 35(1), 80–93",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Mpagerank: The stability of web graph",
      "author" : [ "L.T. Kien", "L.T. Hieu", "T.L. Hung", "L.A. Vu" ],
      "venue" : "Vietnam Journal of Mathematics 37(4), 475–489",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Content-based recommender systems: State of the art and trends",
      "author" : [ "P. Lops", "M. de Gemmis", "G. Semeraro" ],
      "venue" : "Ricci, F., Rokach, L., Shapira, B., Kantor, P. (eds.) Recommender Systems Handbook, chap. 3, pp. 73–105. Springer",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Ontology-based semantic similarity: A new feature-based approach",
      "author" : [ "D. Sánchez", "M. Batet", "D. Isern", "A. Valls" ],
      "venue" : "Expert Systems with Applications 39(9), 7718–7728",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Oss: A semantic similarity function based on hierarchical ontologies",
      "author" : [ "V. Schickel-Zuber", "B. Faltings" ],
      "venue" : "Veloso, M.M. (ed.) Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI 2007), Hyderabad, India, January 6-12. pp. 551–556",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A method for measuring keywords similarity by applying jaccard’s, n-gram and vector space",
      "author" : [ "J. Singthongchai", "S. Niwattanakul" ],
      "venue" : "Lecture Notes on Information Theory 1(4), 159–164",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "[6] has compared and analyzed the effectiveness of these measures in text clustering problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[12] make keywords search more practical by calculating keyword similarity by combining Jaccard’s, N-Gram and Vector Space.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "Probabilistic models for similarity measure has been studied in language speech [4,3].",
      "startOffset" : 80,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Probabilistic models for similarity measure has been studied in language speech [4,3].",
      "startOffset" : 80,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "have proposed a bigram similarity model and used the relative entropy to compute the similarity of keywords [4].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Sung-Hyuk Cha has conducted a comprehensive survey on probability density functions for similarity measures [3].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "Ontology-based methods are also exploited to measure the similarity of keywords [2,11,10].",
      "startOffset" : 80,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "Ontology-based methods are also exploited to measure the similarity of keywords [2,11,10].",
      "startOffset" : 80,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Ontology-based methods are also exploited to measure the similarity of keywords [2,11,10].",
      "startOffset" : 80,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "[2] use the Wordnet database - ontology of words to measure keywords’ relatedness by extracting lexico-syntactic patterns that indicate various aspects of semantic similarity and modifying four popular co-occurrence measures, including Jaccard, Overlap (Simpson), Dice, and Pointwise mutual information (PMI).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "Vincent Schickel-Zuber and Boi Falting [11] present a novel similarity measure for hierarchical ontologies called Ontology Structure based Similarity (OSS) that allows similarities to be asymmetric.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "[10] presents an ontology-based method relying on the exploitation of taxonomic features available in an ontology.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "Markov Chain model which properties are studied in [7,8] is used for computing the similarity and ranking [5,1].",
      "startOffset" : 51,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "Markov Chain model which properties are studied in [7,8] is used for computing the similarity and ranking [5,1].",
      "startOffset" : 51,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Markov Chain model which properties are studied in [7,8] is used for computing the similarity and ranking [5,1].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "Markov Chain model which properties are studied in [7,8] is used for computing the similarity and ranking [5,1].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "[5] use a stochastic random-walk model to compute similarities between nodes of a graph for recommendation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] introduce an N-star model, and demonstrate it in ranking conference and journal problems.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] do a thorough review on state-of-the-art and trends of contentbased recommender systems.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "Since a state θ can jump to any state (and itself too) by apply action A4, the Markov Chain is irreducible and aperiodic (see more [7,8]).",
      "startOffset" : 131,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "Since a state θ can jump to any state (and itself too) by apply action A4, the Markov Chain is irreducible and aperiodic (see more [7,8]).",
      "startOffset" : 131,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "The Perron-Frobenius theorem [7] states that there exists a unique stationary score {Pr(θ)}θ∈S .",
      "startOffset" : 29,
      "endOffset" : 32
    } ],
    "year" : 2015,
    "abstractText" : "In this work, we propose a new approach for discovering various relationships among keywords over the scientific publications based on a Markov Chain model. It is an important problem since keywords are the basic elements for representing abstract objects such as documents, user profiles, topics and many things else. Our model is very effective since it combines four important factors in scientific publications: content, publicity, impact and randomness. Particularly, a recommendation system (called SciRecSys) has been presented to support users to efficiently find out relevant articles.",
    "creator" : "Hung Nghiep Tran"
  }
}