{
  "name" : "1603.08575.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
    "authors" : [ "S. M. Ali Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "Koray Kavukcuoglu", "Geoffrey E. Hinton" ],
    "emails" : [ "AESLAMI@GOOGLE.COM", "HEESS@GOOGLE.COM", "THEOPHANE@GOOGLE.COM", "TASSA@GOOGLE.COM", "KORAYK@GOOGLE.COM", "GEOFFHINTON@GOOGLE.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "“Our knowledge springs from two fundamental sources of the mind; the first is the capacity of receiving representations, [...] the second is the power of knowing an object through these representations.” (Kant, 1781)\nThe human percept of a visual scene is highly structured. Scenes like those in Fig. 1 naturally decompose into objects that are arranged in space, have visual and physical properties, and are in functional relationships with each other. Artificial systems that interpret images in this way are desirable, as accurate detection of objects and inference of\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nx\nz\nx\nz1 z2 z3\nn = 2\nFigure 1. Left: The latent random variable z (the plate’s appearance) produces the observation x (the image). The relationship between z and x is specified by a model (red arrow). Inference is the task of computing likely values of z given x (black arrow). Right: For most images of interest, multiple latent variables give rise to the image. We wish to recover their attributes, e.g., positions and appearances. We propose an iterative, recurrent, variable-length inference procedure (black arrows) that attends to one object at a time, and train it end-to-end via gradient descent.\ntheir attributes is thought to be fundamental for many problems of interest. Consider a robot whose task is to prepare a meal with the ingredients in Fig. 1. To plan a course of action it will need to determine which objects are present, which category each object belongs to, and where each one is located in the scene.\nThe notion of using interpretable probabilistic models for image understanding has a long history, however in practice it has been difficult to define models that are: (a) expressive enough to capture the complexity of natural scenes, and (b) amenable to tractable inference. Here we develop a principled framework for efficient inference in rich, structured models of images. This framework achieves scene interpretation via probabilistic inference (‘vision as inverse graphics’, e.g., Grenander 1976) and imposes structure\nar X\niv :1\n60 3.\n08 57\n5v 1\n[ cs\n.C V\n] 2\n8 M\nar 2\non the representation through appropriate partly- or fullyspecified generative models, rather than supervision from labels. Crucially, our framework allows for reasoning about the complexity of a given scene (the dimensionality of its latent space). We demonstrate that via a Bayesian Occam’s razor type effect, it is possible to discover the underlying causes of a dataset of images in an unsupervised manner. For instance, the model structure will enforce that a scene is formed by a variable number of entities that appear in different locations, but the process of learning will identify what these scene elements look like and where they appear in any given image. The framework naturally combines high-dimensional distributed representations (e.g., to model object appearances) with directly interpretable latent variables (e.g., object pose) and knowledge about the generative process (e.g., how pose affects image pixels). This combination makes it easier to avoid the pitfalls of representational spaces that are too unconstrained (leading to data-hungry learning) or too rigid (leading to model failure via mis-specification).\nThe main contributions of the paper are as follows. First, in Sec. 2 we formalize a scheme for efficient variational inference in latent spaces of variable dimensionality. The key idea is to treat inference as an iterative process, implemented as a recurrent neural network that attends to one object at a time, and learns to use an appropriate number of inference steps for each image. This approach allows for visual understanding in a way that is scalable with regards to scene complexity, due to its iterative nature, and scalable with regards to model complexity, due to the recurrent implementation of the inference network. The iterative formulation naturally captures the dependencies between latent variables in the posterior, for instance accounting for the fact that parts of the scene have already been explained. This is critical for accurate inference, and hence also for model learning. We call the proposed framework AttendInfer-Repeat (AIR). End-to-end learning is enabled by recent advances in amortized variational inference, e.g., combining gradient based optimization for continuous latent variables with black-box optimization for discrete ones.\nSecond, in Sec. 3 we show that AIR allows for learning of generative models that decompose multi-object scenes into their underlying causes, e.g., the constituent objects, in an unsupervised manner. We demonstrate these capabilities on MNIST digits (Sec. 3.1) and show that the model also discovers stroke-like components in the Omniglot dataset (Lake et al. 2015, Sec. 3.2). Finally, in Sec. 3.3 we demonstrate how our inference framework can be used to perform fast inference for a 3D rendering engine, recovering the counts, identities and 3D poses of objects in scenes containing complex meshes with significant occlusion in a single forward pass of a neural network, providing a fast and scalable approach to ‘vision as inverse graphics’."
    }, {
      "heading" : "2. Approach",
      "text" : "In this paper we take a Bayesian perspective of scene interpretation, namely that of treating this task as inference in a generative model. Thus given an image x and a model pxθ (x|z)pzθ(z) parameterized by θ we wish to recover the underlying scene description z by computing the posterior p(z|x) = pxθ (x|z)pzθ(z)/p(x). In this view, the prior pzθ(z) captures our assumptions about the underlying scene, and the likelihood pxθ (x|z) is our model of how a scene description is rendered to form an image. Both can take various forms depending on the problem at hand and we will describe particular instances in Sec. 3. Together, they define the language that we use to describe a scene.\nMany real-world scenes naturally decompose into objects. We therefore make the modeling assumption that the scene description is structured into groups of variables zi, where each group describes the attributes of one of the objects in the scene, e.g., its type, appearance, and pose. Since the number of objects will vary from scene to scene, we assume models of the following form:\npθ(x) = N∑ n=1 pN (n) ∫ pzθ(z|n)pxθ (x|z)dz. (1)\nThis can be interpreted as follows. We first sample the number of objects n from a suitable prior (for instance a Binomial distribution) with maximum value N . The latent, variable length, scene descriptor z = (z1, z2, . . . , zn) is then sampled from a scene model z ∼ pzθ(·|n). Finally, we render the image according to x ∼ pxθ (·|z). Since the indexing of objects is arbitrary, pzθ(·) is exchangeable and pxθ (x|·) is permutation invariant, and therefore the posterior over z is exchangeable.\nThe prior and likelihood terms can take different forms. We consider two scenarios: For 2D scenes (Sec. 3.1), each object is characterized in terms of a continuous 3-dimensional variable for its pose (position and scale), and a learned distributed continuous representation for its shape. For 3D scenes (Sec. 3.3) objects are defined in terms of their position and rotation, and a categorical variable that characterizes their identity, e.g., sphere, cube or cylinder.\nWe refer to the two kinds of variables for each object i in both scenarios as ziwhat and z i where respectively, bearing in mind that their meaning (e.g., position and scale in pixel space vs. position and orientation in 3D space) and their data type (continuous vs. discrete) will vary. We further assume that zi are independent under the prior, i.e., pzθ(z|n) = ∏n i=1 p z θ(z\ni), but non-independent priors, such as a distribution over hierarchical scene graphs (e.g., Zhu & Mumford 2006), can also be accommodated. Furthermore, while the number of objects is bounded as per Eq. 1, it is relatively straightforward to relax this assumption."
    }, {
      "heading" : "2.1. Inference",
      "text" : "Despite their natural appeal, inference for most models in the form of Eq. 1 is intractable. We therefore employ an amortized variational approximation to the true posterior by learning a distribution qφ(z, n|x) parameterized by φ that minimizes the divergence KL [qφ(z, n|x)||pzθ(z, n|x)]. While amortized variational approximations have recently been used successfully in a variety of works (Rezende et al., 2014; Kingma & Ba, 2014; Mnih & Gregor, 2014) the specific form our model poses two additional difficulties. Trans-dimensionality: As a challenging departure from classical latent space models, the size of the the latent space n (i.e., the number of objects) is a random variable itself, which necessitates evaluating pN (n|x) = ∫ pzθ(z, n|x)dz, for all n = 1...N . Symmetry: There are strong symmetries that arise, for instance, from alternative assignments of objects appearing in an image x to latent variables zi.\nWe address these challenges by formulating inference as an iterative process implemented as a recurrent neural network, which infers the attributes of one object at a time. The network is run for N steps and in each step explains one object in the scene, conditioned on the image and on its knowledge of previously explained objects (see Fig. 2).\nTo simplify sequential reasoning about the number of objects, we parameterize n as a variable length latent vector zpres using a unary code: for a given value n, zpres is the vector formed of n ones followed by one zero. Note that the two representations are equivalent. The posterior takes the following form:\nqφ(z, zpres|x) = n∏ i=1 qφ(z i, zipres = 1|x, z1:i−1)\n×qφ(zn+1pres = 0|z1:n, x). (2)\nqφ is implemented as a neural network that, in each step, outputs the parameters of the sampling distributions over the latent variables, e.g., the mean and standard deviation\nof a Gaussian distribution for continuous variables. zpres can be understood as an interruption variable: at each time step, if the network outputs zpres = 1, it describes at least one more more object and goes on to the next time step, but if it outputs zpres = 0, no more objects are described, and inference terminates for that particular datapoint.\nNote that conditioning of zi|x, z1:i−1 is critical to capture dependencies between the latent variables zi, e.g., to avoid explaining the same object twice. The specifics of the networks that achieve this depend on the particularities of the models and we will describe them in detail in Sec. 3."
    }, {
      "heading" : "2.2. Learning",
      "text" : "We can jointly optimize the parameters θ of the model and φ of the inference network by maximizing the lower bound on the marginal likelihood of an image under the model:\nlog pθ(x) ≥ L(θ, φ) = Eqφ [ log pθ(x, z, n)\nqφ(z, n, |x)\n] (3)\nwith respect θ and φ. L is called the negative free energy. We provide an outline of how to construct an unbiased estimator of the gradient of Eq. 3 below, for more details see Schulman et al. (2015).\n2.2.1. PARAMETERS OF THE MODEL θ\nComputing a Monte Carlo estimate of ∂∂θL is relatively straightforward: given a sample from the approximate posterior (z, zpres) ∼ qφ(·|x) (i.e., when the latent variables have been ‘filled in’) we can readily compute ∂ ∂θ log pθ(x, z, n) provided p is differentiable in θ. This is effectively a partial M-step in a generalized EM scheme.\n2.2.2. PARAMETERS OF THE INFERENCE NETWORK φ\nComputing a Monte Carlo estimate of ∂∂φL is more involved. As discussed above, the RNN that implements qφ produces the parameters of the sampling distributions for the scene variables z and presence variables zpres. For a time step i, denote with ωi all the parameters of the sampling distributions of variables in (zipres, z\ni). We parameterize the dependence of this distribution on z1:i−1 and x using a recurrent function Rφ(·) implemented as a neural network such that (ωi,hi) = Rφ(x,hi−1) with hidden variables h. The full gradient is obtained via chain rule:\n∂L ∂φ = ∑ i ∂L ∂ωi ∂ωi ∂φ . (4)\nBelow we explain how to compute ∂L/∂ωi. We first rewrite our cost function as follows:\nL(θ, φ) = Eqφ [`(θ, φ, z, n)] , (5)\nwhere `(θ, φ, z, n) is defined as log pθ(x,z,n)qφ(z,n,|x) . Let z i be an arbitrary element of the vector (zi, zipres) of type {what, where, pres}. How to proceed depends on whether zi is continuous or discrete.\nContinuous variables: Suppose zi is a continuous variable. We use the path-wise estimator (also known as ‘re-parameterization trick’, e.g., Kingma & Welling 2013; Schulman et al. 2015), which allows us to ‘back-propagate’ through the random variable zi. For many continuous variables (in fact, without loss of generality), zi can be sampled as h(ξ, ωi), where h is a deterministic transformation function, and ξ a random variable from a fixed noise distribution p(ξ). We then obtain a gradient estimate:\n∂L ∂ωi ≈ ∂`(θ, φ, z, n) ∂zi ∂h ∂ωi . (6)\nDiscrete parameters: For discrete scene variables (e.g. zipres) we cannot compute the gradient ∂L/∂ωij by backpropagation. Instead we use the likelihood ratio estimator (Mnih & Gregor, 2014; Schulman et al., 2015). Given a posterior sample (z, n) ∼ qφ(·|x) we can obtain a Monte Carlo estimate of the gradient as follows:\n∂L ∂ωi ≈ ∂ log q(z i|ωi) ∂ωi `(θ, φ, z, n). (7)\nIn the raw form presented here this gradient estimate is likely to have high variance (see appendix for details). We reduce its variance using appropriately structured baselines (Mnih & Gregor, 2014) that are functions of the image and the latent variables produced so far."
    }, {
      "heading" : "3. Models and Experiments",
      "text" : "We first apply AIR to a dataset of multiple MNIST digits, and show that it can reliably learn to detect and generate the constituent digits from scratch (Sec. 3.1). We then demonstrate the model’s capabilities on the Omniglot dataset (Sec. 3.2), where the model learns to represent each character using elements that resemble strokes. Finally, we apply AIR to a setting where a 3D renderer is specified in advance. We show that AIR learns to use the renderer to infer the counts, identities and poses of multiple objects in a 3D table-top scene (Sec. 3.3).\nThe structure of the AIR model and networks used in the 2D experiments are best described visually, see Fig. 3.\nFor the dataset of MNIST digits, we also investigate the behavior of a variant, difference-AIR (DAIR), which employs a slightly different recurrent architecture for the inference network (see Fig. 13 in appendix). As opposed to AIR which computes zi via hi and x, DAIR reconstructs at every time step i a partial reconstruction xi of the data x. The partial reconstruction is set as the mean of the distribution pxθ (x|z1, z2, . . . , zi−1). We then create an error canvas ∆xi = xi − x. The DAIR inference equation Rφ is then specified as (ωi,hi) = Rφ(∆xi,hi−1)."
    }, {
      "heading" : "3.1. Multi-MNIST",
      "text" : "We begin with a 50×50 dataset of multi-MNIST digits. Each image contains zero, one or two non-overlapping random MNIST digits with equal probability (see Fig. 4a). The desired goal is to train a network that produces sensible explanations for each of the images. We train AIR with N = 3 on 60,000 such images from scratch, i.e., without\na curriculum or any form of supervision by maximizing L with respect to the parameters of the inference network and the generative model. Upon completion of training we inspect the model’s inferences (see Fig. 4b). We draw the reader’s attention to the following observations. First, the model identifies the number of digits correctly, due to the opposing pressures of (a) wanting to explain the scene, and (b) the cost that arises from instantiating an object under the prior. This is indicated by the number of attention windows in each image; we also plot the accuracy of count inference over the course of training (Fig. 5, left). Second, it locates the digits accurately. Third, the recurrent network learns a suitable scanning policy to ensure that different time-steps account for different digits (Fig. 5, right). Note that we did not have to specify any such policy in advance, nor did we have to build in a constraint to prevent two time-steps from explaining the same part of the image. Finally, that the network learns to not use the second time-step when the image contains only a single digit, and to never use the third time-step (images contain a maximum of two digits). This allows for the inference network to stop upon encountering the first zipres equaling 0, leading to potential savings in computation during inference.\nIt is informative to inspect how the model’s inferences evolve over time. In Fig. 6 we show reconstructions of a\nfixed set of test images at various points during training. The model’s reconstructions are at first very poor. It then gradually learns to reconstruct well, however it makes use of all available time-steps. It is only towards the end of training that it learns to use its time-steps more sparingly, leading it to perform correct inference of object counts.\nOwing to the structure and nature of the networks used in AIR, inference under a learned model is almost instantaneous in contrast to classical inference techniques e.g., direct optimization or Markov chain Monte Carlo. To demonstrate this and to better understand the learned model, we implement a graphical user interface for real-time inference and reconstruction. A video showing its use can be found here: https://youtu.be/4tc84kKdpY4."
    }, {
      "heading" : "3.1.1. STRONG GENERALIZATION",
      "text" : "Since the model learns the concept of a digit independently of the positions or numbers of times it appears in each image, one would hope that it would be able to generalize, e.g., by demonstrating an understanding of scenes that have structural differences to training scenes. We probe this behavior with the following scenarios: (a) Extrapolation: training on images each containing 0, 1 or 2 digits and then testing on images containing 3 digits, and (b) Interpolation: training on images containing 0, 1 or 3 digits and testing on images containing 2 digits. The result of this experiment is shown in Fig. 7. An AIR model trained on up to 2 digits is effectively unable to infer the correct count when presented with an image of 3 digits. We believe this to be caused by the LSTM which learns during training never to expect more than 2 digits. AIR’s generalization performance is improved somewhat when considering the interpolation task. DAIR by contrast generalizes well in both tasks (and finds interpolation to be slightly easier than extrapolation). A closely related baseline is the Deep Recur-\n0 50 100 150 200\nEpochs (×103 )\n400\n450\n500\n550\n600\nV a ri\na ti\no n a l B\no u n d\nAIR - 0,1,2 to 3 AIR - 0,1,3 to 2 DAIR - 0,1,2 to 3 DAIR - 0,1,3 to 2\n0 50 100 150 200\nEpochs (×103 )\n0\n20\n40\n60\n80\n100\nG e n .\nA cc\nu ra\ncy (\n% )\nrent Attentive Writer (DRAW, Gregor et al. 2015), which like AIR, generates data sequentially. However, DRAW has a fixed and large number of steps (40 in our experiments). As a consequence generative steps do not correspond to easily interpretable entities, complex scenes are drawn faster and simpler ones slower. We show DRAW’s reconstructions in Fig. 7. Interestingly, DRAW learns to ignore precisely one digit in the image (see appendix for further details)."
    }, {
      "heading" : "3.1.2. REPRESENTATIONAL POWER",
      "text" : "A second motivation for the use of structured generative models is that their inferences about the structure of a scene provides useful representations for downstream tasks. We examine this ability by first training an AIR model on 0, 1 or 2 digits and then produce inferences for a separate collection of images that contains precisely 2 digits. We split this data into training and test and consider two tasks: (a) predicting the sum of the two digits (as was done in Ba et al., 2015), and (b) determining if the digits appear in an ascending order. We compare with a CNN trained from the raw pixels (Fig. 8). AIR achieves high accuracy using only a fraction of the labeled data (see appendix for details)."
    }, {
      "heading" : "3.2. Omniglot",
      "text" : "We also investigate the behavior of AIR on the Omniglot dataset (Lake et al., 2015) which contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon’s Mechanical Turk by 20 people. This means that the data was produced according a process (pen strokes) that is not directly reflected in the structure of our generative model. It is therefore interesting to examine the outcome of learning under mis-specification. We train the model from the previous section, this time allowing for a maximum of up to 4 inference time-steps per image. Fig. 9 shows that by using different numbers of time-steps to describe characters of varying complexity, AIR discovers a representation consisting of spatially coherent elements resembling strokes, despite not exploiting stroke labels in the data or building in the physics of strokes, in contrast with Lake et al. (2015). Further results can be found in the supplementary video."
    }, {
      "heading" : "3.3. 3D Scenes",
      "text" : "The experiments above demonstrate learning of inference and generative networks in models where we impose structure in the form of a variable-sized representation and spatial attention mechanisms. We now consider an additional\nAttend, Infer, Repeat: Fast Scene Understanding with Generative Models (a )D at\na (b )A IR\n(c )S\nup .\n(d )O\npt .\nFigure 10. 3D objects: The task is to infer the identity and pose of a 3D object. (a) Images from the dataset. (b) Reconstructions produced by re-rendering the inference made by an AIR network trained on the data without supervision. (c) Reconstructions produced by an AIR network trained with ground-truth labels. Note poor performance on cubes due to their symmetry. (d) Reconstructions obtained by performing direct gradient descent on the scene representation to minimize reconstruction error. This approach is less stable and much more susceptible to local minima.\nway of imparting knowledge to the system: we specify the generative model via a 3D renderer, i.e., we completely specify how any scene representation is transformed to produce the pixels in an image. Therefore the task is to learn to infer the counts, identities and poses of several objects, given different images containing these objects and an implementation of a 3D renderer from which we can draw new samples. This formulation of computer vision is often called ‘vision as inverse graphics’ (see e.g., Grenander 1976; Loper & Black 2014; Jampani et al. 2015).\nThe primary challenge in this view of computer vision is that of inference. While it is relatively easy to specify highquality generative models in the form of probabilistic renderers, performing posterior inference is either extremely computationally expensive or prone to getting stuck in local minima (e.g., via optimization or Markov chain Monte Carlo). Therefore it would be highly desirable amortize this cost over training in the form of an inference network. In addition, probabilistic renderers (and in particular 3D renderers) typically are not capable of providing gradients with respect to their inputs, and 3D scene representations often involve discrete variables, e.g., mesh identities. We address these challenges by using finite-differencing to obtain a gradient through the renderer, using the score function estimator to get gradients with respect to discrete variables, and using an AIR inference architecture to handle correlated posteriors and variable-length representations.\nWe demonstrate the capabilities of this approach by first considering scene consisting of only one of three objects: a red cube, a blue sphere, and a textured cylinder (see Fig. 10a). Since the scenes only consist of single objects,\n(a )D\nat a\n(b )R\nec on\n.\nFigure 11. 3D scenes: AIR can learn to recover the counts, identities and poses of multiple objects in a 3D table-top scene. (a) Images from the dataset. (b) Inference using AIR produces a scene description which we visualize using the specified renderer. AIR does occasionally make mistakes, e.g., image 5.\nthe task is only to infer the identity (cube, sphere, cylinder) and pose (position and rotation) of the object present in the image. We train a single-step (N = 1) AIR inference network for this task. The network is only provided with unlabeled images and is trained to maximize the likelihood of those images under the model specified by the renderer. The quality of the scene representations produced by the learned inference network can be visually inspected in Fig. 10b. The network accurately and reliably infers the identity and pose of the object present in the scene. In contrast, an identical network trained to predict the groundtruth identity and pose values of the training data (in a similar style to Kulkarni et al. 2015a) has much more difficulty in accurately determining the cube’s orientation (Fig. 10c). The supervised loss forces the network to predict the exact angle of rotation. However this is not identifiable from the image due to the rotational symmetries of some of the objects, which leads to conditional probabilities that are multi-modal and difficult to represent using standard network architectures. We also compare with direct optimization of the likelihood from scratch for every test image (Fig. 10d), and observe that this method is slower, less stable and more susceptible to local minima. So not only does amortization reduce the cost of inference, but it also overcomes the pitfalls of independent gradient optimization.\nWe finally consider a more complex setup, where we infer the counts, identities and positions of a variable number of crockery items in a table-top scene (Fig. 11a and Fig. 12). This would be of critical importance to a robot, say, which is in the process of interacting with the objects and the table. The goal is to learn to achieve this task with as little supervision as possible, and indeed we observe that with AIR it is possible to do so with no supervision other than a specification of the renderer. This setting can be extended to include additional scene variables, such as the camera position, as we demonstrate in appendix H (Fig. 19). We show reconstructions of AIR’s inferences in Fig. 11b and Fig. 12, which are for the most part robust and accurate. We provide a quantitative comparison of AIR’s inference robust-\nness and accuracy with that of a fully supervised network in Fig. 12. We consider two scenarios: one where each object type only appears exactly once, and one where objects can repeat in the scene. A naive supervised setup struggles greatly with object repetitions or when an arbitrary ordering of the objects is imposed by the labels, however training is more straightforward when there are no repetitions. AIR achieves equivalent error and competitive count accuracy despite the added difficulty of object repetitions."
    }, {
      "heading" : "4. Related Work",
      "text" : "Deep neural networks have had great success in learning to predict various quantities from images, e.g., object classes (Krizhevsky et al., 2012), camera positions (Kendall et al., 2015) and actions (Mnih et al., 2015). These methods work best when large labeled datasets are available for training.\nAt the other end of the spectrum, e.g., in ‘vision as inverse graphics’, only a generative model is specified in advance and prediction is treated as an inference problem, which is then solved using MCMC or message passing at testtime. These models range from highly specified (Milch et al., 2005; Mansinghka et al., 2013), to partially specified (Zhu & Mumford, 2006; Roux et al., 2011; Heess et al., 2011; Eslami & Williams, 2014; Tang et al., 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al., 2012). Inference is very challenging and almost always the bottle-neck in model design.\nHinton et al. (1995); Tu & Zhu (2002); Kulkarni et al. (2015a); Jampani et al. (2015); Wu et al. (2015) exploit data-driven predictions to empower the ‘vision as inverse graphics’ paradigm. For instance, in PICTURE, Kulkarni et al. (2015a) use a deep network to distill the results of slow MCMC, speeding up predictions at test-time.\nVariational auto-encoders (Rezende et al., 2014; Kingma & Ba, 2014) and their discrete counterparts (Mnih & Gregor, 2014) made the important contribution of showing how the gradient computations for learning of amortized inference\nand generative models could be interleaved, allowing both to be learned simultaneously in an end-to-end fashion (see also Schulman et al. 2015). Works like that of Hinton et al. (2011); Kulkarni et al. (2015b) aim to learn disentangled representations in an auto-encoding framework using special network structures and / or careful training schemes.\nIt is also worth noting that attention mechanisms in neural networks have been studied in discriminative and generative settings, e.g. by Mnih et al. (2014); Ba et al. (2015); Jaderberg et al. (2015) and Gregor et al. (2015).\nAIR draws upon, extends and links these ideas. Similar to our work is also Huang & Murphy (2015), however they assume a fixed number of objects. By its nature AIR is also related to the following problems: counting (Lempitsky & Zisserman, 2010; Zhang et al., 2015), trans-dimensionality (Graves, 2016), sparsity (Bengio et al., 2009) and gradient estimation through renderers (Loper & Black, 2014). It is the combination of these elements that unlocks the full capabilities of the proposed approach."
    }, {
      "heading" : "5. Discussion",
      "text" : "We presented several principled models that not only learn to count, locate, classify and reconstruct the elements of a scene, but do so in a fraction of a second at test-time. The main ingredients are (a) building in meaning using appropriately structured models, (b) amortized inference that is attentive, iterative and variable-length, and (c) end-to-end learning. Learning is most successful when the variance of the gradients is low and the likelihood is well suited to the data. It will be of interest to examine the scaling of variance with the number of objects and more sophisticated likelihoods (e.g., occlusion). It is straightforward to extend the framework to semi- or fully-supervised settings. Furthermore, the framework admits a plug-and-play approach where existing state-of-the-art detectors, classifiers and renderers are used as sub-components of an AIR inference network. We plan to investigate these lines of research in future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers, Shakir Mohamed, Andrew Zisserman, Peter Dayan, Peter Battaglia, Tejas Kulkarni, Taco Cohen, Tom Erez, Thore Graepel, Jonathan Hunt and Vlodomyr Mnih for insightful discussions, feedback and suggestions."
    }, {
      "heading" : "A. Stochastic Gradient Estimators",
      "text" : "In this section, we give further details behind equations Eq. 6 and Eq. 7. We simplify notation by not referencing the model parameters θ and considering a single latent z at a time. Assume we have a function `(z) and distribution qφ(z); we wish to estimate∇φE[`(z)].\nA.1. Reparameterization trick\nAs per the main body, we supposed the existence of a differentiable function h and random variable ξ with fixed noise distribution pξ(·) such that h(ξ, φ) ∼ qφ(·). It follows that:\n∂\n∂φ Ez∼qφ [`(z)] =\n∂\n∂φ Eξ∼pξ [`(h(ξ, φ)] =Eξ∼pξ [ ∂\n∂φ `(h(ξ, φ) ] =Eξ∼pξ [ ∂`\n∂z\n∂h\n∂φ ] =Ez∼qφ [ ∂`\n∂z\n∂h\n∂φ ] ≈∂`(z)\n∂z\n∂h(ξ, φ)\n∂φ . (8)\nIn other words, an estimate of the gradient can be recovered by forwarding sampling the model by using the reparameterization given by h, and backpropagating normally through h.\nA.2. Likelihood ratio estimator\nThe likelihood ratio method simply uses the equality:\n∂ log qφ(z)\n∂φ =\n∂qφ(z)\n∂φ\nqφ(z) (9)\nto rewrite an integral as an expectation. Assuming that ∂qφ(z) ∂φ exists and is continuous, we have:\n∂\n∂φ\n∫ qφ(z)`(z)∂z = ∫ z ∂qφ(z) ∂φ qφ(z)dz\n= ∫ z ∂ log `φ(z) ∂θ `φ(z)`(z)dz\n= Eqφ(z) [ ∂ log qφ(z)\n∂φ `(z) ] ≈ ∂ log qφ(z)\n∂φ `(z). (10)\nNote that if `(z) is a constant with respect to z, then the expression is clearly 0, since the integral evaluates to the same constant."
    }, {
      "heading" : "B. Prior for unary encoding",
      "text" : "Recall that we can encode the number of objects n as a variable length unary code vector zpres defined by zipres = 1 for i ≤ n, and zn+1pres = 0 (more generally, it can be useful to implicitly define zjpres = 0, for j > n). Consider an arbitrary distribution p(·) over n, and denote µ≥n =∑ k≥n p(k) the probability that there are at least n objects. We define a joint probability distribution for zpres and show it is consistent with p(n).\nLet p(zipres = 1|zi−1pres ) = zi−1pres µ≥i µ≥(i−1) for i ≥ 2, and p(z1pres) = µ≥1. Note that if z i pres = 0 for any i, it follows immediately that zjpres = 0 for j ≥ i. The sampled vector is therefore a correct unary code. Furthermore,\nP (max{i : zipres = 1} = n) = P (z1pres = 1, z 2 pres = 1, . . . , z n pres = 1, z n+1 pres = 0)\n= ( n∏ i=1 P (zipres = 1|zi−1pres = 1) ) P (zn+1pres = 0|znpres = 1)\n= µ≥1 × µ≥2 µ≥1 × µ≥3 µ≥2 . . . µ≥n µ≥(n−1) × ( 1− µ≥(n+1) µ≥n ) = µ≥n − µ≥(n+1) = p(n)\nIt follows that for zpres following the distribution specified above, the corresponding maximum index is distributed according to p(n) as desired."
    }, {
      "heading" : "C. Details of 2D Experiments",
      "text" : "All experiments were performed using the Adam optimizer (Kingma & Ba, 2014) with a batch size of 64. Inference networks and decoders were trained using a learning rate of 10−4 and baselines were trained using a higher learning rate of 10−3. LSTMs had 256 cell units and object appearances were coded with 50 units. Images were normalized to hold values between 0 and 1 and the likelihood function was a Gaussian with fixed standard deviation equal to 0.3. The prior p(n) was fixed to a geometric distribution which favors sparse reconstructions."
    }, {
      "heading" : "D. DAIR network",
      "text" : "We assume that the renderer likelihood px(x|z1, z2, . . . , zi) has a link function I which maps a sufficient statistic hi to the mean; hi can be iteratively updated from hi−1 and zi−1. this is the case for instance for Gaussian and Bernoulli distributions (where hi is respectively taken to be the mean and log-odds of the distribution). In DAIR, we use the error ∆xi between the partial reconstruction I(hi−1) and the data x as inputs to a feed-forward neural network which predicts zi, zipres.\nDAIR can be thought of as a special case of AIR with additional structure; namely, the recurrent aspect of AIR is fixed to become a canvas-reconstruction network; see Fig. 13 for more details."
    }, {
      "heading" : "E. Details of AIR vs. CNN Experiments",
      "text" : "AIR’s inferences are fed through a 4-layer network (each with 512 hidden units) to produce the 19-way prediction of the sum or a 2-way prediction of the order, and the convolution network uses a 64×(5×5)-64×(5×5)-64×(5×5)-512 architecture."
    }, {
      "heading" : "F. DRAW Comparisons",
      "text" : "We compare AIR and DAIR to a state of the art DRAW network with 40 drawing steps with 4 latent units per time step, 400 LSTM hidden units, spatial transformer (Jaderberg et al., 2015) attention module, and single read and write heads of size 16 × 16. We report free energy on two test sets: a test dataset with 0, 1 or 2 digits, and another with images with precisely 3 digits. The likelihood model was in all cases Gaussian with fixed standard deviation of 0.3. DRAW outperforms AIR and DAIR on the 0/1/2 dataset; this is likely due to the fact that DRAW uses many more drawing steps (40) than AIR and thus has an\nexcellent statistical model of single digits. DRAW however does not conceptually understand them as distinct units, as evidenced by its poor generalization on the 3-digits dataset, where DAIR has both better score, and more meaningful reconstruction: DAIR partially generalizes to a number of digit never seen (Fig. 7), while DRAW interestingly learns to perfectly ignore exactly one digit in the image (see Fig. 7). More generally, the VAE subroutine present in AIR could be replaced by a DRAW network, thus leading to a ‘best of both worlds’ model with excellent single digit model and understanding of a scene in terms of its constituent parts."
    }, {
      "heading" : "G. Sprites Experiments",
      "text" : "We also consider a 50×50 dataset of sprites: red circles, green squares and blue diamonds. Each image in the dataset contains zero, one or two sprites (see Fig. 14a). The images are composed additively (sprites do not occlude each other). We use the exact same model structure as for the multi-MNIST dataset.\nAt the end of unsupervised training, AIR successfully learns about the underlying causes of the scenes (namely, the sprites), as well as their counts and locations, and also produces convincing reconstructions (see Fig. 14b). Note that the inference network correctly detects the correct number of sprites even when two overlapping sprites of the same type and color appear in the same image (Fig. 14a,b, images 1 and 3). Also note that the reconstructions are accurate, meaning that the inference network successfully produces the codes for each sprite despite the presence of the other sprites in its field of view. Fig. 14c displays a collection of samples from the model after training. We display quantitative evaluation of the network’s counting accuracy in Fig. 15, reconstructions over the course of learning in Fig. 16, and a visualization of its scanning policy in Fig. 17.\nNote that these tasks can only be successfully achieved once the inference network has learned a sensible policy for scanning the image, e.g., one in which every object is attended to only once. However the network must break\nmultiple symmetries to achieve this, e.g., it does not matter which object it explains first. In Fig. 17 we visualize the learned scanning policy for 3 different runs of training (only differing in the random seed). In each case a unique policy is learned, and the policy appears to be spatial (as opposed to one that is based on digit identity or size)."
    }, {
      "heading" : "H. Details of 3D Scene Experiments",
      "text" : "The experiments in section 3.3 were performed using the rendering capabilities of the MuJoCo physics simulator (Todorov et al., 2012).\nH.1. Gradient estimation\nDifferentiation of MuJoCo’s graphics engine was performed using forward finite-differencing (with a constant = 10−4) with respect to the scene configuration. This is a generic procedure which would work for any graphics engine; we chose MuJoCo because it is fast (using only the fixed functionality of OpenGL) and because scenes are conveniently parameterized. Interestingly, despite the coarse 8-bit output of OpenGL, quantization errors appeared to average out reasonably well over the pixels.\nH.2. Scene generation\nSingle object scenes: For the results shown in Fig. 10 we created a scene that contained a MuJoCo box geom representing the table, 3 ‘objects’ (also in the form of MuJoCo geoms; cube, sphere, textured cylinder), and a fixed camera. The objects could be moved in the plane of the table and rotated along the axis orthogonal to it (i.e. 3 degrees of freedom per object). We created random scenes containing at most one object by randomly sampling position, rotation angle, object presence (visibility) and object type. (Geoms were made invisible by moving them out of the field of view of the camera.) An illustration is shown in Fig. 18.\nTabletop scenes: For the results shown in Fig. 11 we used scenes with a box geom for the table, and nine mesh geoms for the crockery items. The cup, pan, and plate were each replicated three times to allow for arbitrary threeobjects scenes. Each geom had three degrees of freedom (position in the table plane and rotation). Random scenes with up to N = 3 objects were created by randomly sampling position, rotation angle, object presence, and object type three times. As for the single objects were rendered\ninvisible by moving them outside of the field of view of the camera.\nWe experimented with two versions of the scene: one with a fixed camera (Fig. 11), and one version where the camera could be moved in an orbit around the table (i.e. one degree of freedom). We discuss the experiment with the fixed camera in the main text. For the latter set of scenes, the camera position was also chosen randomly and the image was rendered from the random camera position. Camera movement was restricted to ± 40 degrees from the central position. In this experiment the model had to learn to infer the camera position in addition to the objects on the table. The montage in Fig. 12 in the main text shows a ground truth scene (with camera) and the inferred identities and positions of the objects as well as the inferred position of the camera. We show several examples of random scenes with variable camera and the associated inferences in Fig. 19. For the most part the network infers all scene parameters reliably.\nImage preprocessing: We rendered all scene images at 128× 128 pixels. We down-sampled scene images to 32× 32 pixels for input to the network.\nH.3. Model\nWe trained a network to perform inference in the following fixed generative model:\np(x, z1:Npres , z 1:N where, z 1:N what) = (11)\np(x|z1:Npres , z1:Nwhere, z1:Nwhat) N∏ i=1 p(zipres)p(z i what)p(z i where),\nwhere zipres is the visibility indicator: z i pres ∼ Bernoulli(α) for object i; zwhere ∈ R3 indicates position and rotation\nangle: ziwhere ∼ N (0,Σwhere); and ziwhat is a three-valued discrete variable indicating the object type (mesh / geom type): ziwhat ∼ Discrete(β).\nThe marginal distribution over scenes under this model is the same as the marginal distribution under a model of form described in Section 2 in the main text where p(n) = Binomial(N,α) and n = ∑N i=1 zi.\nFor the variable camera scenes the model included an additional random variable zcam ∈ R where zcam ∼ N (0, σ2cam).\nTo evaluate the likelihood term p(x|z) we (1) render the scene description using the MuJoCo rendering engine to produce a high-resolution image y; (2) blur the resulting image y as well as x using a fixed-with blur kernel; (3) compute N (x|y, Iσ2x).\nH.4. Network\nThe AIR inference network for our experiments is a standard recurrent network (no LSTM) that is run for a fixed number of steps (N = 1 or N = 3). In each step the network computes:\n(ωipres, ω i what, ω i where,h i) = R(x, zi−1pres , z i−1 what, z i−1 where,h i−1),\nwhere the ωi represent the parameters of the sampling distributions for the random variables: Bernoulli for zpres; Discrete for zwhat; and Gaussian for zwhere. For the experiments with random camera angle we use a separate network that computes ωcam = F (x) and we provide the sampled camera angle as additional input to R at each time step.\nH.5. Supervised learning\nFor the baselines trained in a supervised manner we use the ground truth scene variables z1:Npres , z 1:N where, z 1:N what that underly the training scene images as labels and train a network of the same form as the inference network to maximize the conditional log likelihood of the ground truth scene variables given the image."
    } ],
    "references" : [ {
      "title" : "Multiple Object Recognition with Visual Attention",
      "author" : [ "Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Ba et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2015
    }, {
      "title" : "Group Sparse Coding",
      "author" : [ "Bengio", "Samy", "Pereira", "Fernando", "Singer", "Yoram", "Strelow", "Dennis" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bengio et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "A Generative Model for Parts-based Object Segmentation",
      "author" : [ "Eslami", "S.M. Ali", "Williams", "Christopher K. I" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Eslami et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Eslami et al\\.",
      "year" : 2014
    }, {
      "title" : "The Shape Boltzmann Machine: a Strong Model of Object Shape",
      "author" : [ "Eslami", "S.M. Ali", "Heess", "Nicolas", "Winn", "John" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Eslami et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Eslami et al\\.",
      "year" : 2012
    }, {
      "title" : "Adaptive Computation Time",
      "author" : [ "Graves", "Alex" ],
      "venue" : null,
      "citeRegEx" : "Graves and Alex.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graves and Alex.",
      "year" : 2016
    }, {
      "title" : "DRAW: A Recurrent Neural Network For Image Generation",
      "author" : [ "Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo", "Wierstra", "Daan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Pattern Synthesis: Lectures in Pattern Theory",
      "author" : [ "Grenander", "Ulf" ],
      "venue" : "Applied Mathematical Sciences",
      "citeRegEx" : "Grenander and Ulf.,? \\Q1976\\E",
      "shortCiteRegEx" : "Grenander and Ulf.",
      "year" : 1976
    }, {
      "title" : "Weakly Supervised Learning of ForegroundBackground Segmentation Using Masked RBMs",
      "author" : [ "Heess", "Nicolas", "Roux", "Nicolas Le", "Winn", "John M" ],
      "venue" : "In International Conference on Artificial Neural Networks,",
      "citeRegEx" : "Heess et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Heess et al\\.",
      "year" : 2011
    }, {
      "title" : "Training Products of Experts by Minimizing Contrastive Divergence",
      "author" : [ "Hinton", "Geoffrey E" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton and E.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hinton and E.",
      "year" : 2002
    }, {
      "title" : "The ”wake-sleep” algorithm for unsupervised neural networks",
      "author" : [ "Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Randford M" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 1995
    }, {
      "title" : "Transforming Auto-encoders",
      "author" : [ "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Wang", "Sida D" ],
      "venue" : "In International Conference on Artificial Neural Networks,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient inference in occlusion-aware generative models of images",
      "author" : [ "Huang", "Jonathan", "Murphy", "Kevin" ],
      "venue" : "CoRR, abs/1511.06362,",
      "citeRegEx" : "Huang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "PoseNet: A Convolutional Network for Real-Time 6DOF Camera Relocalization",
      "author" : [ "Kendall", "Alex", "Grimes", "Matthew", "Cipolla", "Roberto" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "Kendall et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kendall et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Kingma", "Diederik P", "Ba", "Jimmy" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2013
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Picture: A probabilistic programming language for scene perception",
      "author" : [ "Kulkarni", "Tejas D", "Kohli", "Pushmeet", "Tenenbaum", "Joshua B", "Mansinghka", "Vikash K" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep Convolutional Inverse Graphics Network",
      "author" : [ "Kulkarni", "Tejas D", "Whitney", "William F", "Kohli", "Pushmeet", "Tenenbaum", "Josh" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Human-level concept learning through probabilistic program induction",
      "author" : [ "Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B" ],
      "venue" : "Science, 350(6266),",
      "citeRegEx" : "Lake et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning To Count Objects in Images",
      "author" : [ "Lempitsky", "Victor", "Zisserman", "Andrew" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Lempitsky et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lempitsky et al\\.",
      "year" : 2010
    }, {
      "title" : "OpenDR: An Approximate Differentiable Renderer",
      "author" : [ "Loper", "Matthew M", "Black", "Michael J" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Loper et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Loper et al\\.",
      "year" : 2014
    }, {
      "title" : "Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs",
      "author" : [ "Mansinghka", "Vikash", "Kulkarni", "Tejas D", "Perov", "Yura N", "Tenenbaum", "Josh" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mansinghka et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mansinghka et al\\.",
      "year" : 2013
    }, {
      "title" : "BLOG: Probabilistic Models with Unknown Objects",
      "author" : [ "Milch", "Brian", "Marthi", "Bhaskara", "Russell", "Stuart", "Sontag", "David", "Ong", "Daniel L", "Kolobov", "Andrey" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Milch et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2005
    }, {
      "title" : "Neural Variational Inference and Learning in Belief Networks",
      "author" : [ "Mnih", "Andriy", "Gregor", "Karol" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent Models of Visual Attention",
      "author" : [ "Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "Kavukcuoglu", "Koray" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis" ],
      "venue" : "Nature, 518,",
      "citeRegEx" : "Dharshan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dharshan et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
      "author" : [ "Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning a generative model of images by factoring appearance and shape",
      "author" : [ "Roux", "Nicolas Le", "Heess", "Nicolas", "Shotton", "Jamie", "Winn", "John M" ],
      "venue" : "In International Conference on Artificial Neural Networks,",
      "citeRegEx" : "Roux et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep Boltzmann Machines",
      "author" : [ "Salakhutdinov", "Ruslan", "Hinton", "Geoffrey" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2009
    }, {
      "title" : "Gradient Estimation Using Stochastic Computation Graphs",
      "author" : [ "Schulman", "John", "Heess", "Nicolas", "Weber", "Theophane", "Abbeel", "Pieter" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Tensor Analyzers",
      "author" : [ "Tang", "Yichuan", "Salakhutdinov", "Ruslan", "Hinton", "Geoffrey" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Tang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning Generative Models With Visual Attention",
      "author" : [ "Tang", "Yichuan", "Srivastava", "Nitish", "Salakhutdinov", "Ruslan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Tang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2014
    }, {
      "title" : "MuJoCo: A physics engine for model-based control",
      "author" : [ "Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval" ],
      "venue" : "In International Conference on Intelligent Robots and Systems,",
      "citeRegEx" : "Todorov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Todorov et al\\.",
      "year" : 2012
    }, {
      "title" : "Image Segmentation by Data-Driven Markov Chain Monte Carlo",
      "author" : [ "Tu", "Zhuowen", "Zhu", "Song-Chun" ],
      "venue" : "Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Tu et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2002
    }, {
      "title" : "Salient Object Subitizing",
      "author" : [ "Zhang", "Jianming", "Ma", "Shuga", "Sameki", "Mehrnoosh", "Sclaroff", "Stan", "Betke", "Margrit", "Lin", "Zhe", "Shen", "Xiaohui", "Price", "Brian", "Mĕch", "Radomı́r" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "A Stochastic Grammar of Images",
      "author" : [ "Zhu", "Song-Chun", "Mumford", "David" ],
      "venue" : "Foundations and Trends in Computer Graphics and Vision,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "While amortized variational approximations have recently been used successfully in a variety of works (Rezende et al., 2014; Kingma & Ba, 2014; Mnih & Gregor, 2014) the specific form our model poses two additional difficulties.",
      "startOffset" : 102,
      "endOffset" : 164
    }, {
      "referenceID" : 29,
      "context" : "3 below, for more details see Schulman et al. (2015).",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 29,
      "context" : "We use the path-wise estimator (also known as ‘re-parameterization trick’, e.g., Kingma & Welling 2013; Schulman et al. 2015), which allows us to ‘back-propagate’ through the random variable z.",
      "startOffset" : 31,
      "endOffset" : 125
    }, {
      "referenceID" : 29,
      "context" : "Instead we use the likelihood ratio estimator (Mnih & Gregor, 2014; Schulman et al., 2015).",
      "startOffset" : 46,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "We also investigate the behavior of AIR on the Omniglot dataset (Lake et al., 2015) which contains 1623 different handwritten characters from 50 different alphabets.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 18,
      "context" : "We also investigate the behavior of AIR on the Omniglot dataset (Lake et al., 2015) which contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon’s Mechanical Turk by 20 people. This means that the data was produced according a process (pen strokes) that is not directly reflected in the structure of our generative model. It is therefore interesting to examine the outcome of learning under mis-specification. We train the model from the previous section, this time allowing for a maximum of up to 4 inference time-steps per image. Fig. 9 shows that by using different numbers of time-steps to describe characters of varying complexity, AIR discovers a representation consisting of spatially coherent elements resembling strokes, despite not exploiting stroke labels in the data or building in the physics of strokes, in contrast with Lake et al. (2015). Further results can be found in the supplementary video.",
      "startOffset" : 65,
      "endOffset" : 931
    }, {
      "referenceID" : 15,
      "context" : ", object classes (Krizhevsky et al., 2012), camera positions (Kendall et al.",
      "startOffset" : 17,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : ", 2012), camera positions (Kendall et al., 2015) and actions (Mnih et al.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 22,
      "context" : "These models range from highly specified (Milch et al., 2005; Mansinghka et al., 2013), to partially specified (Zhu & Mumford, 2006; Roux et al.",
      "startOffset" : 41,
      "endOffset" : 86
    }, {
      "referenceID" : 21,
      "context" : "These models range from highly specified (Milch et al., 2005; Mansinghka et al., 2013), to partially specified (Zhu & Mumford, 2006; Roux et al.",
      "startOffset" : 41,
      "endOffset" : 86
    }, {
      "referenceID" : 27,
      "context" : ", 2013), to partially specified (Zhu & Mumford, 2006; Roux et al., 2011; Heess et al., 2011; Eslami & Williams, 2014; Tang et al., 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al.",
      "startOffset" : 32,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : ", 2013), to partially specified (Zhu & Mumford, 2006; Roux et al., 2011; Heess et al., 2011; Eslami & Williams, 2014; Tang et al., 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al.",
      "startOffset" : 32,
      "endOffset" : 142
    }, {
      "referenceID" : 30,
      "context" : ", 2013), to partially specified (Zhu & Mumford, 2006; Roux et al., 2011; Heess et al., 2011; Eslami & Williams, 2014; Tang et al., 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al.",
      "startOffset" : 32,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : ", 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al., 2012).",
      "startOffset" : 38,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : ", 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al., 2012). Inference is very challenging and almost always the bottle-neck in model design. Hinton et al. (1995); Tu & Zhu (2002); Kulkarni et al.",
      "startOffset" : 83,
      "endOffset" : 207
    }, {
      "referenceID" : 2,
      "context" : ", 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al., 2012). Inference is very challenging and almost always the bottle-neck in model design. Hinton et al. (1995); Tu & Zhu (2002); Kulkarni et al.",
      "startOffset" : 83,
      "endOffset" : 224
    }, {
      "referenceID" : 2,
      "context" : ", 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al., 2012). Inference is very challenging and almost always the bottle-neck in model design. Hinton et al. (1995); Tu & Zhu (2002); Kulkarni et al. (2015a); Jampani et al.",
      "startOffset" : 83,
      "endOffset" : 249
    }, {
      "referenceID" : 2,
      "context" : ", 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al., 2012). Inference is very challenging and almost always the bottle-neck in model design. Hinton et al. (1995); Tu & Zhu (2002); Kulkarni et al. (2015a); Jampani et al. (2015); Wu et al.",
      "startOffset" : 83,
      "endOffset" : 272
    }, {
      "referenceID" : 2,
      "context" : ", 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al., 2012). Inference is very challenging and almost always the bottle-neck in model design. Hinton et al. (1995); Tu & Zhu (2002); Kulkarni et al. (2015a); Jampani et al. (2015); Wu et al. (2015) exploit data-driven predictions to empower the ‘vision as inverse graphics’ paradigm.",
      "startOffset" : 83,
      "endOffset" : 290
    }, {
      "referenceID" : 2,
      "context" : ", 2013; 2014), to largely unspecified (Hinton, 2002; Salakhutdinov & Hinton, 2009; Eslami et al., 2012). Inference is very challenging and almost always the bottle-neck in model design. Hinton et al. (1995); Tu & Zhu (2002); Kulkarni et al. (2015a); Jampani et al. (2015); Wu et al. (2015) exploit data-driven predictions to empower the ‘vision as inverse graphics’ paradigm. For instance, in PICTURE, Kulkarni et al. (2015a) use a deep network to distill the results of slow MCMC, speeding up predictions at test-time.",
      "startOffset" : 83,
      "endOffset" : 426
    }, {
      "referenceID" : 26,
      "context" : "Variational auto-encoders (Rezende et al., 2014; Kingma & Ba, 2014) and their discrete counterparts (Mnih & Gregor, 2014) made the important contribution of showing how the gradient computations for learning of amortized inference and generative models could be interleaved, allowing both to be learned simultaneously in an end-to-end fashion (see also Schulman et al.",
      "startOffset" : 26,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "Works like that of Hinton et al. (2011); Kulkarni et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "Works like that of Hinton et al. (2011); Kulkarni et al. (2015b) aim to learn disentangled representations in an auto-encoding framework using special network structures and / or careful training schemes.",
      "startOffset" : 19,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "by Mnih et al. (2014); Ba et al.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "(2014); Ba et al. (2015); Jaderberg et al.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "(2014); Ba et al. (2015); Jaderberg et al. (2015) and Gregor et al.",
      "startOffset" : 8,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "(2014); Ba et al. (2015); Jaderberg et al. (2015) and Gregor et al. (2015).",
      "startOffset" : 8,
      "endOffset" : 75
    }, {
      "referenceID" : 34,
      "context" : "By its nature AIR is also related to the following problems: counting (Lempitsky & Zisserman, 2010; Zhang et al., 2015), trans-dimensionality (Graves, 2016), sparsity (Bengio et al.",
      "startOffset" : 70,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : ", 2015), trans-dimensionality (Graves, 2016), sparsity (Bengio et al., 2009) and gradient estimation through renderers (Loper & Black, 2014).",
      "startOffset" : 55,
      "endOffset" : 76
    } ],
    "year" : 2016,
    "abstractText" : "We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects – counting, locating and classifying the elements of a scene – without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",
    "creator" : "LaTeX with hyperref package"
  }
}