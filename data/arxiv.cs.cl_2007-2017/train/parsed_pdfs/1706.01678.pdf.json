{
  "name" : "1706.01678.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Text Summarization using Abstract Meaning Representation",
    "authors" : [ "Shibhansh Dohare", "Harish Karnick" ],
    "emails" : [ "sdohare@cse.iitk.ac.in", "hk@cse.iitk.ac.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Text Summarization using Abstract Meaning Representation\nShibhansh Dohare Department of CSE\nIndian Institute of Technology, Kanpur sdohare@cse.iitk.ac.in\nHarish Karnick Department of CSE\nIndian Institute of Technology, Kanpur hk@cse.iitk.ac.in\nAbstract\nSummarization of large texts is still an open problem in language processing. In this work we develop a full fledged pipeline to generate summaries of news articles using the Abstract Meaning Representation(AMR). We first generate the AMR graphs of stories then extract summary graphs from the story graphs and finally generate sentences from the summary graph. For extracting summary AMRs from the story AMRs we use a two step process. First, we find important sentences from the text and then extract the summary AMRs from those selected sentences. We outperform the previous methods using AMR for summarization by more that 3 ROGUE-1 points. On the CNN-Dailymail corpus we achieve results competitive with the strong lead-3 baseline till summary graph extraction step."
    }, {
      "heading" : "1 Introduction",
      "text" : "Automatic summary generation is an important problem for natural language understanding and with the ever increasing size of text present on the Internet the importance of automatic summarization is increasing. It has significant application for summarizing large texts like stories, journal papers, news articles and even larger texts like books when people just don’t have the time to read the full text.\nExisting methods for summarization can be broadly categorized into two categories Extractive and Abstractive. Extractive methods picks up words and sometimes directly sentences from the text. These methods are inherently limited in the sense that they can never generate human level summaries for large and complicated documents\nwhich require rephrasing sentences and incorporating information from full text to generate summaries. Most of the work done on summarization has been extractive.\nOn the other hand Abstractive methods take advantages of the recent developments in deep learning. Specifically the recent success of the sequence to sequence learning models where recurrent networks read the text, encodes it and then generates target text. Though these methods have recently shown to be competitive with the extractive methods they are still far away from reaching human level quality in summary generation. The bigger problem with these methods is that like most machine learning algorithms they tend perform to poorly on the text outside of the domain of the given corpus.\nAbstract meaning Representation (AMR) as introduced by Banarescu et al. (2013). It focuses on capturing the meaning of the text, by giving a specific meaning representation to the text. AMR tries to capture the ”who is doing what to whom” in a sentence. The formalism aims to give same representation to sentences which have the same underlying meaning. For example ’He likes apple’ and ’Apples are liked by him’ should be assigned the same AMR.\nThe work on summarization using ARMs was started by Liu et al. (2015). The idea is to generate AMR representation of the story sentences. Extract important sub graphs from the story graph and finally generate summary from these extracted graphs.\nOur contributions in this work are three folds. First we complete the pipeline to generate summary sentences from the story sentences using AMR as an intermediate representation. Second, we develop a novel algorithm to extract summary AMR graphs from the document AMR graphs which outperforms Liu et al. (2015)’s method to\nar X\niv :1\n70 6.\n01 67\n8v 1\n[ cs\n.C L\n] 6\nJ un\n2 01\n7\ngenerate summary AMR graphs and finally we explain the problems with the the ROGUE evaluation metric, the CNN-Dailymail corpus Nallapati et al. (2016) and existing methods used for its summarization.\nWe use a two step process for the task. First is to extract summary AMRs from the story sentences and then generating summary sentences using already available generators. We use the idea that there are only few sentences that are important from the point of view of summary, i.e. most of the information contained in the summary is present in very few sentences and they can be used to generate the summary.\nThe rest of the paper is divided as follows section 2 contains introduction to AMRs, their parsing and generation, section 3 and 4 contains the data sets used the algorithm used for summary generation respectively, section 5 contains the results of summary generation using AMR and finally in section 6 we discuss the problems with the current dataset and evaluation metric and the possible directions of future work."
    }, {
      "heading" : "2 AMR: Parsing and generation",
      "text" : "AMR was introduced with the aim to induce work on statistical natural Language understanding and generation. AMR graphs are rooted, directed, edge and vertex labeled graphs. Consider for example the sentence ”I looked carefully all around me”. The graphic representation as generated using ARMICA Saphra and Lopez (2015) of the AMR parse of this sentence generated by JAMR parser is given above in Figure 1. The nodes in the AMR are labeled with concepts as in the above example animal represents a concept. Edges con-\ntains the information regarding the relations between the concepts. Here topic is the relation between the node boa and picture. AMR heavily relies on Propbank for semantic relations (edge labels). Concepts can also be of the form run-01 where the index 01 represents the first sense of the word run. Further details about the AMR can be found in the AMR guidelines Banarescu et al. (2015).\nA lot of work has gone into parsing sentences to their AMR. There are three main approaches to parsing. There is Alignment based parsing Flanigan et al. (2014) (JAMR), Zhou et al. (2016) which uses graph based algorithms for concept and relation identification. Grammar based parsers like Wang et al. (2016) (CAMR) generate output by performing shift reduce transformations on output of a dependency parser. Neural parsing Konstas et al. (2017); Peng et al. (2017) is based on using seq2seq models for parsing, the main problem in using neural methods for parsing is the absence of a huge corpus of human generated AMRs. Peng et al. (2017) reduced the vocabulary size to tackle this problem while Konstas et al. (2017) used larger external corpus of external sentences.\nRecently some has been done on producing meaningful sentences form AMRs. Flanigan et al. (2014) used a number of tree to string conversion rules for generating sentences. Song et al. (2016) reformed the problem as a traveling salesman problem. Konstas et al. (2017) used seq2seq learning methods."
    }, {
      "heading" : "3 Datasets",
      "text" : "We used two datasets for the task. First is the AMR Corpora - Knight et al. (2014) DEFT Phase 2 AMR Annotation R1 AMR Bank. We use the proxy report section as it’s the only one that is relevant for the task, it contains the gold-standard AMRs for the news articles selected from the gigaword corpus and their summary points. The stories contains 17.5 sentences on an average and summary size is 1 or 2 sentences. The training and test sets contain 298 and 33 summary document pairs respectively.\nSecond is the CNN-Dailymail corpus. This dataset is more better suited for the task as we’ve to generate larger summaries of around 3 or 4 sentences. It has around 300k document summary pairs and the stories have 39 sentences on average.\nThe problem with this dataset is that these are just sentences and we don’t have their gold-standard AMRs. We use automatic parsers to get the AMRs but it’ll effect the quality of final generation so we compare the results of using gold-standard and automatically generated AMRs on the gold-standard dataset to get an idea of the error introduced by using automatic parsers."
    }, {
      "heading" : "4 Pipeline",
      "text" : "The pipeline consists of three steps, first convert all the given story sentences to there summary representations followed by extracting summary graph from the story graph and finally generating sentences from these extracted summaries. In the following subsections we explain each of the methods in greater detail."
    }, {
      "heading" : "4.1 Story to AMR",
      "text" : "As the first step we convert the story sentences to their Abstract Meaning Representations. We use JAMR version 2 Flanigan et al. (2014) as its openly available and has a performance close to the state of the art parsers for the task. For the AMR-bank we’ve the gold-standard AMR parses but we still parse the input stories to study the effect of using JAMR instead of the gold-standard AMRs.\nThe cost of parsing the whole CNN/Dailymail corpus is enormous. Time taken per 500 document summary pairs was around 36 hours. The machine we used had 24 CPU cores and 128 GB RAM. We could run only 8 processes in parallel on this machine (no. of CPU cores was the bottleneck). We parsed in mini batches of size 10 at a time this helped in reducing the parsing time of each batch."
    }, {
      "heading" : "4.2 Story AMRs to Summary AMRs",
      "text" : "After parsing (Step 1) we’ve the AMR graphs for the story sentences. In this step we need the AMR graphs of the summary sentences. We divide this task in two parts. First is the finding the important sentences from the story and then extracting the key information from those sentence AMR graphs."
    }, {
      "heading" : "4.2.1 Selecting Important sentences",
      "text" : "Our algorithm is based on the idea that only few sentences are important from the point of view of summary. i.e. there are only a few sentences which contain most of the important information\nand from these sentences we can generate the summary. Below we give a justification to this hypothesis that we can use a few sentences to generate the summary for news articles.\nThe story sentences (avg length 28.1 tokens) in the CNN-Dailymail corpus tend to be larger than the summary sentences (avg length 19.4 tokens) and most of the information corresponding to a summary sentence can can be found in only one sentence from the story.\nTo test this hypothesis we find for each summary sentence which sentence from the story contains maximum information of this summary sentence. We use ROGUE-1 Lin (2004) Recall scores (measures the ratio of words in the target summary that are contained in the predicted summary to the total number of words in the target summary) as the metric for the information contained in the story sentence. The results that we obtained for 5000 randomly chosen document summary pairs from the CNN-Dailymail corpus are given in figure 2. The average recall score that we obtained is around 79%. Upon manual inspection of the summary sentence and the corresponding best sentence from the story we realized, when this score is more than 0.5 or 0.6, almost always the information in the summary sentence is extracted from this chosen story sentence. The score is not perfectly 1 because of stop words and different verb forms. Around 80% of summary sentences have score above 0.5. So, our hypothesis seems to be\ncorrect for most of the summary sentences. This also suggests the highly extractive nature of the summary in the corpus.\nNow the task in hand is to select the important sentences. To find which are the important sentences from the summary we use the following methods.\nTopic modeling at sentence level We consider each sentence as a document. Then we use Latent Dirichlet Allocation (LDA) Blei et al. (2003) to find the key feature words. To measure the importance of each sentence we count the occurrence of feature words in the sentence. The sentences with large count values are chosen as the important sentences.\nWe also take advantage of the fact that these are news articles and thus most of the important information is contained in first few sentences. So we also test by choosing the sentence with the first occurrence of the top two most referred entities in the story and the first story sentence as well."
    }, {
      "heading" : "4.2.2 Extracting summary graph",
      "text" : "To extract summary graphs from the selected sentences we exploit the structure of AMR graphs. We observe that in an Abstract Meaning Representation graph the order of attributes (children) of a node is important and that only first few attributes are important. This allows us to drop the later coming attributes thus helping in extracting the key information. We use this feature of AMR to extract the key information from the sentence. Finally, we use this extracted graph in the generation step. Look at Appendix for more detail"
    }, {
      "heading" : "4.3 Summary Generation",
      "text" : "To generate sentences from the extracted AMR graphs we use already available generators Konstas et al. (2017) Flanigan et al. (2016). We use Flanigan et al. (2016). Generators can significantly effect the results, we’ll analyze the effect of using generator in the next section.\nTo analyze the effectiveness of each of the step we need some method to evaluate summary quality before generation. We use the ideas used by Liu et al. (2015) for the evaluation of the summary generation quality from the extracted AMR graphs. We use the alignments provided in the AMR Bank. Alignments simply map the words in the original sentence with the node or edge in the AMR graph. To generate the summary we find the words aligned with the sentence in the selected\ngraph and output them in no particular order as the predicted summary. Though this does not generate valid sentences as the ordering is not there, we can use the ROGUE 1 metric as it’s based on comparing uni-grams between target and predicted summaries to evaluate the quality of extracting graphs from full sentence AMRs.\nIn the next section we give the results produced by the full pipeline on the gold standard section. To compare our method with Liu et al. (2015) method we compare our results after step 2 of the pipeline, we’ve extracted the AMR from the sentence AMRs. To check the effect of generator we reproduce the full sentence from the full AMR and compare them with the original, this gives us an idea of how much effect do generators have on the final summary quality. Then we test how effective our extraction from the sentence AMR method really is ? and finally we give the results on the CNN-Dailymail corpus."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "We use the standard ROGUE metric for evaluating summaries. We report the recall, precision and F1 scores for ROGUE-1, and only F1 scores for ROGUE-2 and ROGUE-L. ROGUE-1 Recall and precision are measured for unigram overlap in reference and predicted summary. ROGUE-2 uses bigram overlap and ROGUE-L uses longest common sequence between target and predicted summaries for evaluation."
    }, {
      "heading" : "5.1 Results on the Proxy report section",
      "text" : "In table 1 we report the results of using the pipeline upto step 2 on the proxy report section of the AMR Bank. We compare our various methods with Liu et al. (2015)’s methods using the Rogue 1 Recall, Precision and F1 scores. For AMR extraction we’ve dropped the children of the root node which are neither among the two children nor connected with the root with an edge labeled :time.In LDA based sentence selection we use top 5 features to select the sentence. Our first co-occurrence based sentence selection plus first the sentence outperforms Liu et al. (2015)’s method by around three and a half ROGUE-1 points.\nThe results after the final generation step are given in table 2. It’s clear from the results that there is a significant drop in final results because of generator especially in ROGUE-1 Precision. We look at the effect of using generator in the next\nsubsection"
    }, {
      "heading" : "5.2 Effectiveness of generator",
      "text" : "Consider the Rogue scores of the best story sentence (on ROGUE-1 Recall scores, as in section 4.3) as summary for the two cases, first when we directly used these sentence as summary and second when we use the sentences generated from the gold-standard AMR graphs of these sentence as the summary. It’s clear from the results that there is a significant drop of around 15 rogue points in the R1 F1 results whole process.\nThere may be multiple possible reasons for so much error in the generation step. We think of some possible reasons for generation error. First and probably ”the reason”, generators might never be able to achieve scores equal to the original sentences on ROGUE metric. AMR formalism tries to assign same AMR graphs to the sentences that have same meaning so there is a one to many mapping from AMR graphs to sentences. Thus the automatic generators that we’re using might not be trying to generate the original sentence but some other sentence that has the same underlying meaning. This can also explain the low ROGUE-2 and\nROGUE-L scores. As the sentences might be getting rephrased, where they’ll loose most of the bi and tri grams from the original sentence resulting in low ROGUE-2 and ROGUE-L scores. Second, the generators currently are far from generating grammatically and logically perfect sentences, this source of error might be reduced with the ongoing work on generators."
    }, {
      "heading" : "5.3 Analyzing the effectiveness of extraction from AMR",
      "text" : "To extract the summary AMR graph form the sentence AMR graph we used the idea that important information with any AMR tree is situated in some initial sub trees connected to the root.\nMeasuring the effectiveness of extracting summary graphs is not straight forward. We employ the method used by Liu et al. (2015) to measure the quality of generated summary after AMR extraction (refer to section 4.3 for details of evaluation at this step). The aim of extracting summary graphs from the sentence graphs is to drop out the not so important information from the sentences. If we are able to achieve this perfectly the ROGUE-1 recall scores that we’re getting should remain almost the same (as we’re not add any new information) and the ROGUE-1 precision should go up (as we’ve thrown out some useless information) and thus effectively improving the overall ROGUE-1 F1 score. In table 4 we compared the ROGUE-1, recall, precision and F1 scores of the extracted AMRs from the leading 3 sentences of the CNN-Dailymail test set with the original AMRs of these sentences.\nFrom the results we can conclude that the extracting algorithm that we’ve proposed increases the ROGUE-1 precision, this indicates that our ex-\ntraction method is working up-to some extent and is at-least better than random extraction where the precision would’ve remained the same. The drop in ROGUE-1 recall scores suggests that we’ve dropped too much information and this reduction in recall scores overwhelms the increment in precision to reduce the final ROGUE-1 F1 scores."
    }, {
      "heading" : "5.4 Results on the CNN-Dailymail corpus",
      "text" : "In table 4 we report the results on the CNNDailymail corpus. We present results up-to step 2 of the pipeline using the leading sentences as important sentences, we’ve used the nonanonymized dataset. Consider the scores of the original lead-3 baseline and the scores till step 2 of the Lead 3 (full AMR), there is a drop of around 5 ROGUE-1 points in the results, reason for such a significant drop is the absence of stop words in the summary produced after step 2. ROGUE considers stop words like ’a’, ’the’, ’is’ etc. for summary evaluation while these words are not mapped during the AMR generation. We further discuss the problems with ROGUE in 6.2. The results we achieve after extraction from the leading 4 sentences are competitive with the lead-3 baseline at step 2 of the pipeline."
    }, {
      "heading" : "6 Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Comparing various abstractive and extractive methods with our approach",
      "text" : "Most of the extractive methods tend to choose some sentences and directly use them as the summary. This is not how the target summary is written in most cases. For the CNN-Dailymail corpus the extractive current state of the art Nallapati et al. (2017)(SummaRunNer) directly picks sentences from the story for the summary. The story sentences in the corpus are around 1.5 times the\nlength of the sentences in the story so the story sentences are not true representatives of the summary sentences and thus the extractive methods are not producing summary similar to the target summary. In contrast to these methods our method produce summary sentences of the size of target summaries.\nThe abstractive methods for summarization suffer from the lack of generalization like all other supervised learning methods in the sense that they are not able to perform well on the text outside of the domain of the given corpus. In our method as we’re not using any learning method, so we should be able to produce similar results on text which is very different than news stories (under the constraint that the summary sentences are generated mainly from one sentence)."
    }, {
      "heading" : "6.2 Need of an new dataset and evaluation metric",
      "text" : "ROGUE metric, by it’s design has lots of properties that make it unsuitable for evaluating abstractive summaries. For starters, ROGUE matches exact words and not the stems of the words, it also considers stop words for evaluation. One of the reasons why ROGUE like metrics might never become suitable for evaluating abstractive summaries is its incapabilities of knowing if the sentences have been restructured. A good evaluation metric should be one where we compare the meaning of the sentence and not the exact words.\nWe now show why the CNN-Dailymail corpus is not suitable for Abstractive summarization. The nature of summary points in the corpus is highly extractive (Section 4.2.1 for details) where most of the summary points are simply picked up from some sentences in the story. Tough, this is a good enough reason to start searching for better dataset,\nit’s not the biggest problem with the dataset. The dataset has the property that a lot of important information is in the first few sentences and most of the summary points are directly pick from these sentences. The extractive methods based on sentence selection like SummaRunNer are not actually performing well, the results they’ve got are only slightly better than the lead 3 baseline. The work doesn’t show how much of the selected sentences are among the first few and it might be the case that the sentences selected by the extractive methods are mostly among the first few sentences, the same can be the problem with the abstractive methods, where the output might be getting copied from the initial few sentences. This is only a possibility and it can only be tested if compare the summary generated by these methods with the lead3 baseline using the ROGUE metric.\nThese problems with this corpus evoke the need to have another corpus where we don’t have so much concentration of important information at any location but rather the information is more spread out and the summaries are more abstractive in nature.\nAs the point raised by See et al. (2017) the choice of the reference summaries is highly subjective, there are multiple possible ways to choose the summary points and the summary points in the corpus might not be the best possible summary. The only apparent solution to this problem is to either have multiple reference summaries that can cover most of the possible summary points or have humans judges for the generated summary."
    }, {
      "heading" : "6.3 Possible future directions",
      "text" : "As this proposed algorithm is a step be step process we can focus on improving each step to produce better results. The parser and generator are not perfect and thus introduce error in the pipeline. The JAMR parser is close to the state of the art parsers while the generator that we’re using is not close to the state of the art. A lot of recent work has gone into developing better generators specifically Konstas et al. (2017) (Neural AMR) using better generators should directly improve the quality of the final generated summary.\nThe most exciting improvements can be done in the summary graph extraction methods. It is the step which is completely novel and not a lot of work has been done to extract AMRs for summarization. One of the possibilities can be to use\nseq2seq learning methods to learn linearized summary AMRs from the linearized story AMRs. In order to make this pipeline generalizable for any sort of text, we need to get rid of the hypothesis that the summary is being extracted exactly from one sentence. So, the natural direction seems to be joining AMRs of multiple sentences that are similar and then extracting the summary AMR from that large graph. It’ll be like clustering similar sentences and then extracting a summary graph from each of these cluster."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work we’ve completed a pipeline for summary generation using AMR. We show that our AMR extraction methods are better than the one used by Liu et al. (2015). We also observed the problems with the existing long text dataset and the evaluation metric for summarization."
    }, {
      "heading" : "A Supplemental Material",
      "text" : "The following appendix contains a detailed description of the AMR extraction step (Subsection 4.2.2)) Figure 3, shows the AMR parse of the gold-standard AMR graph of the sentence ’Absurd as it might seem to me , a thousand miles from any human habitation and in danger of death , I took out of my pocket a sheet of paper and my fountain - pen’ taken from the The Little Prince Corpus. Figure 4, shows how we truncate the graph, here we’ve kept only the first two arguments of the root node and dropped the remaining subtrees"
    } ],
    "references" : [ {
      "title" : "Abstract meaning representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Martha Palmer", "Philipp Koehn", "Nathan Schneider." ],
      "venue" : "Pro-",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "Journal of Machine Learning Research, 3:9931022. http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Generation from abstract meaning representation using tree transducers",
      "author" : [ "Jeffrey Flanigan", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of",
      "citeRegEx" : "Flanigan et al\\.,? 2016",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2016
    }, {
      "title" : "A discriminative graph-based parser for the abstract meaning representation",
      "author" : [ "Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Flanigan et al\\.,? 2014",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2014
    }, {
      "title" : "Deft phase 2 amr annotation r1 ldc2015e86",
      "author" : [ "Kevin Knight", "Laura Baranescu", "Claire Bonial", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Daniel Marcu", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "philadelphia: Linguistic data consor-",
      "citeRegEx" : "Knight et al\\.,? 2014",
      "shortCiteRegEx" : "Knight et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural amr: Sequence-to-sequence models for parsing and generation",
      "author" : [ "Ioannis Konstas", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Konstas et al\\.,? 2017",
      "shortCiteRegEx" : "Konstas et al\\.",
      "year" : 2017
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "C. Lin." ],
      "venue" : "Text Summarization Branches Out, Post-Conference Workshop of ACL 2004. Barcelona, Spain.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Toward abstractive summarization using semantic representations",
      "author" : [ "Fei Liu", "Flanigan Jeffrey", "Thomson Sam", "Sadeh Norman", "Smith Noah A." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Compu-",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
      "author" : [ "Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou." ],
      "venue" : "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17).",
      "citeRegEx" : "Nallapati et al\\.,? 2017",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2017
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence rnns and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "a glar G ulehre", "Bing Xiang" ],
      "venue" : "In Computational Natural Language Learning",
      "citeRegEx" : "Nallapati et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Addressing the data sparsity issue in neural amr parsing",
      "author" : [ "Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. Association for",
      "citeRegEx" : "Peng et al\\.,? 2017",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2017
    }, {
      "title" : "Amrica: an amr inspector for cross-language alignments",
      "author" : [ "Naomi Saphra", "Adam Lopez." ],
      "venue" : "System Demonstrations of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Saphra and Lopez.,? 2015",
      "shortCiteRegEx" : "Saphra and Lopez.",
      "year" : 2015
    }, {
      "title" : "Get to the point: Summarization with pointer-generator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "http://aclweb.org/anthology/P16-1001.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Amr-to-text generation as a traveling salesman problem",
      "author" : [ "Linfeng Song", "Yue Zhang", "Xiaochang Peng", "Zhiguo Wang", "Daniel Gildea." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Song et al\\.,? 2016",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2016
    }, {
      "title" : "Camr at semeval-2016 task 8: An extended transition-based amr parser",
      "author" : [ "Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Compuational",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Amr parsing with an incremental joint model",
      "author" : [ "Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang QU", "Ran Li", "Yanhui Gu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Compu-",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Abstract meaning Representation (AMR) as introduced by Banarescu et al. (2013). It focuses on capturing the meaning of the text, by giving a specific meaning representation to the text.",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "The work on summarization using ARMs was started by Liu et al. (2015). The idea is to generate AMR representation of the story sentences.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "First we complete the pipeline to generate summary sentences from the story sentences using AMR as an intermediate representation. Second, we develop a novel algorithm to extract summary AMR graphs from the document AMR graphs which outperforms Liu et al. (2015)’s method to ar X iv :1 70 6.",
      "startOffset" : 26,
      "endOffset" : 263
    }, {
      "referenceID" : 8,
      "context" : "generate summary AMR graphs and finally we explain the problems with the the ROGUE evaluation metric, the CNN-Dailymail corpus Nallapati et al. (2016) and existing methods used for its summarization.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "The graphic representation as generated using ARMICA Saphra and Lopez (2015) of the AMR parse of this sentence generated by JAMR parser is given above in Figure 1.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "Further details about the AMR can be found in the AMR guidelines Banarescu et al. (2015).",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "There is Alignment based parsing Flanigan et al. (2014) (JAMR), Zhou et al.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "There is Alignment based parsing Flanigan et al. (2014) (JAMR), Zhou et al. (2016) which uses graph based algorithms for concept and relation identification.",
      "startOffset" : 33,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "There is Alignment based parsing Flanigan et al. (2014) (JAMR), Zhou et al. (2016) which uses graph based algorithms for concept and relation identification. Grammar based parsers like Wang et al. (2016) (CAMR) generate output by performing shift reduce transformations on output of a dependency parser.",
      "startOffset" : 33,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "parsing Konstas et al. (2017); Peng et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "parsing Konstas et al. (2017); Peng et al. (2017) is based on using seq2seq models for parsing, the main problem in using neural methods for parsing is the absence of a huge corpus of human generated AMRs.",
      "startOffset" : 8,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "parsing Konstas et al. (2017); Peng et al. (2017) is based on using seq2seq models for parsing, the main problem in using neural methods for parsing is the absence of a huge corpus of human generated AMRs. Peng et al. (2017) reduced the vo-",
      "startOffset" : 8,
      "endOffset" : 225
    }, {
      "referenceID" : 5,
      "context" : "cabulary size to tackle this problem while Konstas et al. (2017) used larger external corpus of external sentences.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "Song et al. (2016) reformed the problem as a traveling salesman problem.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "Konstas et al. (2017) used seq2seq learning methods.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "First is the AMR Corpora - Knight et al. (2014) DEFT Phase 2 AMR Annotation R1 AMR Bank.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "We use JAMR version 2 Flanigan et al. (2014) as its openly available and has a performance close to",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "We use ROGUE-1 Lin (2004) Recall scores (measures the ratio of words in the target summary that are contained in the predicted summary to the total number of words in the target summary) as the metric for the information contained in the story sentence.",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "Then we use Latent Dirichlet Allocation (LDA) Blei et al. (2003) to find the key feature words.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "To generate sentences from the extracted AMR graphs we use already available generators Konstas et al. (2017) Flanigan et al.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "(2017) Flanigan et al. (2016). We use",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "We use the ideas used by Liu et al. (2015) for the evaluation of the summary generation quality from the extracted AMR graphs.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "In the next section we give the results produced by the full pipeline on the gold standard section. To compare our method with Liu et al. (2015) method we compare our results after step 2 of the pipeline, we’ve extracted the AMR from the sentence AMRs.",
      "startOffset" : 65,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : "In table 1 we report the results of using the pipeline upto step 2 on the proxy report section of the AMR Bank. We compare our various methods with Liu et al. (2015)’s methods using the Rogue 1 Recall, Precision and F1 scores.",
      "startOffset" : 50,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "Our first co-occurrence based sentence selection plus first the sentence outperforms Liu et al. (2015)’s method by around three and a half ROGUE-1 points.",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "Method Rogue-1 Recall Rogue-1 Precision Rogue-1 F1 Liu et al. (2015) 51.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "We employ the method used by Liu et al. (2015) to measure the quality of generated summary after AMR extraction (refer to section 4.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "abstractive model (Nallapati et al., 2016) - 35.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "65 pointer-generator + coverage (See et al., 2017) - 39.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 12,
      "context" : "lead-3 baseline (original) (See et al., 2017) - 40.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "57 lead-3 baseline (anonymized) (Nallapati et al., 2017)* - 39.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "5 SummaRuNner (Nallapati et al., 2017)* - 39.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "For the CNN-Dailymail corpus the extractive current state of the art Nallapati et al. (2017)(SummaRunNer) directly picks sentences from the story for the summary.",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "As the point raised by See et al. (2017) the choice of the reference summaries is highly subjective, there are multiple possible ways to choose the summary points and the summary points in the corpus might not be the best possible summary.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "A lot of recent work has gone into developing better generators specifically Konstas et al. (2017) (Neural AMR) using better generators should directly improve the quality of the final generated summary.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "used by Liu et al. (2015). We also observed the problems with the existing long text dataset and the evaluation metric for summarization.",
      "startOffset" : 8,
      "endOffset" : 26
    } ],
    "year" : 2017,
    "abstractText" : "Summarization of large texts is still an open problem in language processing. In this work we develop a full fledged pipeline to generate summaries of news articles using the Abstract Meaning Representation(AMR). We first generate the AMR graphs of stories then extract summary graphs from the story graphs and finally generate sentences from the summary graph. For extracting summary AMRs from the story AMRs we use a two step process. First, we find important sentences from the text and then extract the summary AMRs from those selected sentences. We outperform the previous methods using AMR for summarization by more that 3 ROGUE-1 points. On the CNN-Dailymail corpus we achieve results competitive with the strong lead-3 baseline till summary graph extraction step.",
    "creator" : "LaTeX with hyperref package"
  }
}