{
  "name" : "1611.04684.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Knowledge Enhanced Hybrid Neural Network for Text Matching",
    "authors" : [ "Yu Wu", "Wei Wu", "Zhoujun Li", "Ming Zhou" ],
    "emails" : [ "wuyu@buaa.edu.cn", "lizj@buaa.edu.cn", "wuwei@microsoft.com", "mingzhou@microsoft.com" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Semantic matching is a fundamental problem in many NLP tasks such as question answering (QA) (Voorhees and others 1999), conversation (Wang et al. 2013), and paraphrase identification (Dolan, Quirk, and Brockett 2004). Take questionanswering as an example. Given a question and an answer passage, one can employ a matching function to measure their matching degree. The matching degree reflects how likely the passage can be used as an answer to the question.\nThe challenge of text matching lies in semantic gaps between natural language sentences. Existing work tackles the challenge by representing sentences or their semantic and syntactic relations from different levels of abstractions with neural networks (Hu et al. 2014; Socher et al. 2011). These models only rely on the text within a pair to perform matching, whereas we find that sentences in a pair could have very complicated semantic and syntactic structures, and it is difficult for the-state-of-the-art neural models to extract useful features from such sentences to bridge the semantic gaps in the text pair. Table 1 gives an example from community QA\n∗The work was done when the first author was an intern in Microsoft Research Asia.\nto illustrate the challenge. The answer is very long1 and contains a lot of information that well compare the two schools but semantically far from the question (e.g., “horse riding” and “lances swords”). The information makes the answer a high quality one, but hinders the existing models from establishing the semantic relations between the question and the answer in matching. Similarly, when questions become long, matching also becomes difficult. In practice, such long text is not rare. For example, in a public QA data set, 54.8% question answer pairs are longer than 60 words (question length plus answer length). More seriously, the-state-of-theart model can only achieves 74.2% matching accuracy on pairs longer than 60 words compared to its performance 78.8% on pairs shorter than 30 words. These evidence indicates us that improving matching performance on pairs with long text is important but challenging, because the semantic gap is even bigger in such pairs.\nWe study semantic matching in text pairs, and particularly, we aim to improve matching accuracy on long text. Our idea is that since it is difficult to establish the matching relations for pairs with long text only by themselves, we consider incorporating prior knowledge into the matching process. The prior knowledge could be topics, tags, and entities related to the text pair, and represents a kind of global context obtained elsewhere compared to local context such as phrases, syntactic elements obtained within the text in the pair. In matching, the global context can help filter out noise, and highlight parts that are important to matching. For instance, if we have a tag “family” indicating the category of\n1The original answer has 149 words.\nar X\niv :1\n61 1.\n04 68\n4v 1\n[ cs\n.C L\n] 1\n5 N\nov 2\n01 6\nthe question in Table 1 in community QA, we can use the tag to enhance the matching between the question and the answer. “Family” reflects the global semantics of the question. It strengthens the effect of its semantically similar words like “kids”,“child” and “activity” in QA matching, and at the same time reduce the influence of “horse riding” and “lances swords” to matching. With the tag as a bridge, the semantic relation between the question and the answer can be identified, which is difficult to achieve only by themselves.\nWe propose a knowledge enhanced hybrid neural network (KEHNN) to leverage the prior knowledge in matching. Given a text pair, KEHNN exploits a knowledge gate to fuse the semantic information carried by the prior knowledge into the representation of words and generates a knowledge enhanced representation for each word. The knowledge gate is a non-linear unit and controls how much information from the word is kept in the new representation and how much information from the prior knowledge flows to the representation. By this means, noise from the irrelevant words is filtered out, and useful information from the relevant words is strengthened. The model then forms three channels to perform matching from multiple perspectives. Each channel models the interaction of two pieces of text in a pair by a similarity matrix. The first channel matches text pairs on words. It calculates the similarity matrix by word embeddings. The second channel conducts matching on local structures of sentences. It captures sequential structures of sentences in the pair by a Bidirectional Recurrent Neural Network with Gated units (BiGRU) (Bahdanau, Cho, and Bengio 2014), and constructs the similarity matrix with the hidden vectors given by BiGRU. In the last channel, the knowledge enhanced representations, after processed by another BiGRU to further capture the sequential structures, are utilized to construct the similarity matrix. Since the prior knowledge represents global semantics of the text pair, the channel performs matching from a global context perspective. The three channels then exploit a convolutional neural networks (CNN) to extract compositional relations of the matching elements in the matrices as high level features for matching. The features are finally synthesized as a matching score by a multilayer perceptron (MLP). The matching architecture lets two objects meet at the beginning, and measures their matching degree from multiple perspectives, thus the interaction of the two objects are sufficiently modeled.\nWe conduct experiments on public data sets for QA and conversation. Evaluation results show that KEHNN can significantly outperform the-state-of-the-art matching methods, and particularly improve the matching accuracy on long text.\nOur contributions in this paper are three-folds: 1) proposal of leveraging prior knowledge to improve matching on long text; 2) proposal of a knowledge enhanced hybrid neural network which incorporates prior knowledge into matching in a general way and conducts matching on multiple levels; 3) empirical verification of the effectiveness of the proposed method on two public data sets."
    }, {
      "heading" : "Related Work",
      "text" : "Early work on semantic matching is based on bag of words (Ramos 2003) and employs statistical techniques like LDA\n(Blei, Ng, and Jordan 2003) and translation models (Koehn, Och, and Marcu 2003) to overcome the semantic gaps. Recently, neural networks have proven more effective on capturing semantics in text pairs. Existing methods can be categorized into two groups. The first group follows a paradigm that matching is conducted by first representing sentences as vectors. Typical models in this group include DSSM (Huang et al. 2013), NTN (Socher et al. 2013), CDSSM (Shen et al. 2014), Arc1 (Hu et al. 2014), CNTN (Qiu and Huang 2015), and LSTMs (Tan, Xiang, and Zhou 2015). These methods, however, lose useful information in sentence representation, and leads to the emergence of methods in the second group. The second group matches text pairs by an interaction representation of sentences which allows them to meet at the first step. For example, MV-LSTM (Wan et al. 2015) generates the interaction representation by LSTMs and neural tensors, and then uses k-max pooling and a multi-layer perceptron to compute a matching score. MatchPymid (Pang et al. 2016) employs CNN to extract features from a word similarity matrix. More effort along this line includes DeepMatchtopic (Lu and Li 2013), MultiGranCNN (Yin and Schütze 2015), ABCNN (Yin et al. 2015), Arc2 (Hu et al. 2014), MatchSRNN (Wan et al. 2016), and Coupled-LSTM (Liu, Qiu, and Huang 2016). Our method falls into the second group, and extends the existing methods by introducing prior knowledge into matching and conducting matching with multiple channels."
    }, {
      "heading" : "Approach",
      "text" : ""
    }, {
      "heading" : "Problem Formalization",
      "text" : "Suppose that we have a data set D = {(li, Sx,i, Sy,i)}Ni=1, where Sx,i = (w0, . . . , wj , . . . , wI) and Sy,i = (w ′ 0, . . . , w ′ j , . . . , w ′ J) are two pieces of text, and wj and w ′ j represent the j-th word of Sx,i and Sy,i respectively, and N is the number of instances. li ∈ {1, . . . , C} is a label indicating the matching degree between Sx,i and Sy,i. In addition to D, we have prior knowledge for Sx,i and Sy,i denoted as kx,i and ky,i respectively. Our goal is to learn a matching model g(·, ·) with D and { ∪Ni=1kx,i,∪Ni=1ky,i } . Given a new pair (Sx, Sy) with prior knowledge (kx,ky), g(Sx, Sy) predicts the matching degree between Sx and Sy .\nTo learn g(·, ·), we need to answer two questions: 1) how to use prior knowledge in matching; 2) how to perform matching with both text pairs and prior knowledge. In the following sections, we first describe our method on incorporating prior knowledge into matching, then we show details of our model."
    }, {
      "heading" : "Knowledge Gate",
      "text" : "Inspired by the powerful gate mechanism (Hochreiter and Schmidhuber 1997; Chung et al. 2014) which controls information in and out when processing sequential data with recurrent neural networks (RNN), we propose using knowledge gates to incorporate prior knowledge into matching. The underlying motivation is that we want to use the prior knowledge to filter out noise and highlight the useful information to matching in a piece of text. Formally, let ew ∈ Rd denote the embedding of a word w in text Sx and kx ∈ Rn\ndenote the representation of the prior knowledge of Sx. Knowledge gate kw is defined as\nkw = σ(Wkew + Ukkx), (1)\nwhere σ is a sigmoid function, andWk ∈ Rd×d, Uk ∈ Rd×n are parameters. With kw, we define a knowledge enhanced representation for w as\nẽw = kw ew + (1− kw) kx, (2) where is an element-wise multiplication operation. Equation (2) means that prior knowledge is fused into matching by a combination of the word representation and the knowledge representation. In the combination, the knowledge gate element-wisely controls how much information from word w is preserved, and how much information from prior knowledge kx flows in. The advantage of the elementwise operation is that it offers a way to precisely control the contributions of prior knowledge and words in matching. Entries of kw lie in [0, 1]. The larger an entry of kw is, the more information from the corresponding entry of ew will be kept in ẽw. In contrast, the smaller an entry of kw is, the more information from the corresponding entry of kx will flow into ẽw. Since kw is determined by both ew and kx and learned from training data, it will keep the useful parts in the representations of w and the prior knowledge and at the same time filter out noise from them."
    }, {
      "heading" : "Matching with Multiple Channels",
      "text" : "With the knowledge enhanced representations, we propose a knowledge enhanced hybrid neural network (KEHNN) which conducts matching with multiple channels. Figure 1 gives the architecture of our model. Given a pair (Sx, Sy), the model looks up an embedding table and represents Sx and Sy as Sx = [ex,0, . . . , ex,i, . . . , ex,I ] and Sy = [ey,0, . . . , ey,i, . . . , ey,J ] respectively, where ex,i, ey,i ∈ Rd are the embeddings of the i-th word of Sx and Sy respectively. Sx and Sy are used to create three similarity matrices, each of which is regarded as an input channel of a convolutional neural network (CNN). CNN extracts high level features from the similarity matrices. All features are finally concatenated and synthesized by a multilayer perceptron (MLP) to form a matching score.\nSpecifically, in channel one, ∀i, j, element e1,i,j in similarity matrix M1 is calculated by\ne1,i,j = h(e ᵀ x,i · ey,j), (3)\nwhere h(·) could be ReLU or tanh. M1 matches Sx and Sy on words.\nIn channel two, we employ bidirectional gated recurrent units (BiGRU) (Chung et al. 2014) to encode Sx and Sy into hidden vectors. A BiGRU consists of a forward RNN and a backward RNN. The forward RNN processes Sx as it is ordered (i.e., from ex,1 to ex,I ), and generates a sequence of hidden states ( −→ h 1, . . . , −→ h I). The backward RNN reads the sentence in its reverse order (i.e., from ex,I to ex,1) and generates a sequence of backward hidden states ( ←− h 1, . . . , ←− h I). BiGRU then forms the hidden vectors of Sx as {hx,i = [ −→ h i, ←− h i]}Ii=1 by concatenating the forward and\nthe backward hidden states. More specifically, ∀i, −→ h i ∈ Rm is calculated by\nzi = σ(Wzex,i + Uz −→ h i−1) (4) ri = σ(Wrex,i + Ur −→ h i−1) (5) h̃i = tanh(Whex,i + Uh(ri −→ h i−1)) (6) −→ h i = zi h̃i + (1− zi) −→ h i−1, (7)\nwhere zi and ri are an update gate and a reset gate respectively, and Wz , Wh, Wr, Uz , Ur,Uh are parameters. The backward hidden state ←− h i ∈ Rm is obtained in a similar way. Following the same procedure, we get {hy,i}Ji=1 as the hidden vectors of Sy . With the hidden vectors, ∀i, j, we calculate element e2,i,j in similarity matrix M2 by\ne2,i,j = h(h ᵀ x,iW2hy,j + b2), (8)\nwhere W2 ∈ R2m×2m and b2 ∈ R are parameters. Since BiGRU encodes sequential information of sentences into hidden vectors, M2 matches Sx and Sy on local structures (i.e., sequential structures) of sentences.\nIn the last channel, we employ another BiGRU to process the sequences of Sx and Sy which consists of the knowledge enhanced representations in Equation (2), and obtain the knowledge enhanced hidden states khx = (khx,1, . . . , khx,I) and khy = (khy,1, . . . , khy,J) for Sx and Sy respectively. Similar to channel two, ∀i, j, element e3,i,j in similarity matrix M3 is given by\ne3,i,j = h(kh ᵀ x,i ·W3 · khy,j + b3), (9)\nwhere W3 ∈ R2m×2m and b3 ∈ R are parameters. Prior knowledge represents a kind of global semantics of Sx and Sy , and therefore M3 matches Sx and Sy on global context of sentences.\nThe similarity matrices are then processed by a CNN to abstract high level features. ∀i = 1, 2, 3, CNN regards a similarity matrix as an input channel, and alternates convolution and max-pooling operations. Suppose that z(l,f) =[ z (l,f) i,j ] I(l,f)×J(l,f) denotes the output of feature maps of type-f on layer-l, where z(0,f) = Mf , ∀f = 1, 2, 3. On convolution layers (i.e. ∀l = 1, 3, 5, . . . ,), we employ a 2D convolution operation with a window size r(l,f)w × r(l,f)h , and define z(l,f)i,j as\nz (l,f) i,j = σ( Fl−1∑ f ′=0 r (l,f) w∑ s=0 r (l,f) h∑ t=0 w (l,f) s,t · z (l−1,f ′) i+s,j+t + b l,k), (10)\nwhere σ(·) is a ReLU, and w(l,f) ∈ Rr(l,f)w ×r (l,f) h and bl,k are parameters of the f -th feature map on the l-th layer, and Fl−1 is the number of feature maps on the (l − 1)-th layer. An max pooling operation follows a convolution operation and can be formulated as z (l,f) i,j = max\np (l,f) w >s≥0 max p (l,f) h >t≥0 zi+s,j+t, ∀l = 2, 4, 6, . . . , (11)\nwhere p(l,f)w and p (l,f) h are the width and the height of the 2D pooling respectively.\nThe output of the final feature maps are concatenated as a vector v and fed to a two-layer feed-forward neural network (i.e., MLP) to calculate a matching score g(Sx, Sy):\ng(Sx, Sy) = σ1 (w ᵀ 2 · σ2 (w ᵀ 1v + b4) + b5) , (12)\nwhere w1, w2, b4, and b5 are parameters. σ1(·) is softmax and σ2(·) is tanh.\nKEHNN inherits the advantage of 2D CNN (Pang et al. 2016; Wan et al. 2015) that matching two objects by letting them meet at the beginning. Moreover, it constructs interaction matrices by considering multiple matching features. Therefore semantic relations between the two objects can be sufficiently modeled and leveraged in building the matching function. Our model extends the existing models (Hu et al. 2014) by fusing extra knowledge into matching and conducting matching with multiple channels.\nWe learn g(·, ·) by minimizing cross entropy (Levin and Fleisher 1988) withD and { ∪Ni=1kx,i,∪Ni=1ky,i } . Let Θ denote the parameters of our model. Then the objective function of learning can be formulated as\nL(D; Θ) = − N∑ i=1 C∑ c=1 P gc (li) · log(Pc(g(Sx,i, Sy,i)), (13)\nwhere N in the number of instances inD, and C is the number of values of labels in D. Pc(g(Sx,i, Sy,i) returns the cth element from the C-dimensional vector g(Sx,i, Sy,i), and P gc (li) is 1 or 0, indicating whether li equals to c or not. We optimize the objective function using back-propagation and the parameters are updated by stochastic gradient descent with Adam algorithm (Kingma and Ba 2014). As regularization, we employ early-stopping (Lawrence and Giles 2000) and dropout (Srivastava et al. 2014) with rate of 0.5. We set the initial learning rate and the batch size as 0.01 and 50 respectively."
    }, {
      "heading" : "Prior Knowledge Acquisition",
      "text" : "Prior knowledge plays a key role to the success of our model. As described above, in learning, we expect prior knowledge to represent global context of input. In practice, we can use\ntags, keywords, topics, or entities that are related to the input as instantiation of the prior knowledge. Such prior knowledge could be obtained either from the metadata of the input, or from extra algorithms, and represent a summarization of the overall semantics of the input. Algorithms include tag recommendation (Wu et al. 2016), keyword extraction (Wu et al. 2015), topic modeling (Blei, Ng, and Jordan 2003) and entity linking (Han, Sun, and Zhao 2011) can be utilized to extract the prior knowledge from multiple resources like web documents, social media and knowledge base.\nIn our experiments, we use question categories as the prior knowledge in the QA task, because the categories assigned by the askers can reflect the question intention. For conversation task, we pre-trained a Twitter LDA model (Zhao et al. 2011) on external large social media data, as the topics learning from social media could help us group text with similar meaning in a better way. Both the categories and the topics represent a high level abstraction from human or an automatic algorithm to the QA pairs or the message-response pairs, and therefore, they can reflect the global semantics of the input of the two tasks. As a consequence, our knowledge gate can learn a better representation for matching with the prior knowledge."
    }, {
      "heading" : "Experiments",
      "text" : "We tested our model on two matching tasks: answer selection for question answering and response selection for conversation."
    }, {
      "heading" : "Baseline",
      "text" : "We considered the following models as baselines:\nMulti-layer perceptron (MLP): each sentence is represented as a vector by averaging its word vectors. The two vectors were fed to a two-layer feedforward neural network to calculate a matching score. MLP shared the embedding tables with our model.\nDeepMatchtopic: the matching model proposed in (Lu and Li 2013) which only used topic information to perform matching.\nCNNs: the Arc1 model and the Arc2 model proposed by Hu et al. (2014).\nCNTN: the convolution neural tensor network (Qiu and Huang 2015) proposed for community question answering.\nMatchPyramid: the model proposed by Pang et al. (Pang et al. 2016) who match two sentences using an approach of image recognition. The model is a special case of our model with only channel one.\nLSTMs: sentence vectors are generated by the last hidden state of LSTM (Lowe et al. 2015), or the attentive pooling result of all hidden states (Tan, Xiang, and Zhou 2015). We denote the two models as LSTM and LSTMa.\nMV-LSTM: the model (Wan et al. 2015) generates an interaction vector by combining hidden states of two sentences given by a shared BiLSTM. Then the interaction vector is fed to an MLP to compute the matching score.\nWe implemented all baselines and KEHNN by an opensource deep learning framework Theano (Theano Development Team 2016) . For all baselines and our model, we set the dimension of word embedding (i.e.,d) as 100 and the maximum text length (i.e., I and J) as 200. In LSTMs, MVLSTM, and BiGRU in our model, we set the dimension of hidden states as 100 (i.e., m). We only used one convolution layer and one max-pooling layer in all CNN based models, because we found that the performance of the models did not get better with the number of layers increased. For Arc2, MatchPyramid, and KEHNN, we tuned the window size in convolution and pooling in {(2, 2), (3, 3)(4, 4)}, and found that (3, 3) is the best choice. The number of feature maps is 8. For Arc1 and CNTN, we selected the window size from {2, 3, 4} and set it as 3. The number of feature maps is 200. In MLP, we tuned the dimension of the hidden layer in {50, 200, 400, 800} and set it as 50. Sx and Sy in KEHNN shared word embeddings, knowledge embeddings, parameters of BiGRUs, and parameters of the knowledge gates. All tuning was conducted on validation sets. The activation functions in baselines are the same as those in our model."
    }, {
      "heading" : "Answer Selection",
      "text" : "The goal of answer selection is to recognize high quality answers in answer candidates of a question. We used a public data set of answer selection in SemEval 2015 (AlessandroMoschitti, Glass, and Randeree 2015), which collects question-answer pairs from Qatar Living Forum2 and requires to classify the answers into 3 categories (i.e. C = 3 in our model) including good, potential and bad. The ratio of the three categories is 51 : 10 : 39. The statistics of the data set is summarized in Table 2. We used classification accuracy as an evaluation metric.\n2http://www.qatarliving.com/forum\nSpecific Setting In this task, we regarded question categories tagged by askers as prior knowledge (both kx and ky). There are 27 categories in the Qatar Living data. Knowledge vector k was initialized by averaging the embeddings of words in the category. For all baselines and our model, the word embedding and the topic model (in DeepMatchtopic) were trained on a Qatar living raw text provided by SemEval-2015 3. We fixed the word embedding during the training process, and set h in Equation (3), (8), (9) as ReLU.\nResults JAIST, the champion of the task in SemEval15, used 12 features and an SVM classifier and achieved an accuracy of 0.725. From Table 3, we can see that advanced neural networks, such as CNTN, MV-LSTM, LSTMa and KEHNN, outperform JAIST’s model, indicating that handcrafted features are less powerful than deep learning methods. Models that match text pairs by interaction representations like Arc2 and MatchPyramid are not better than models that perform matching with sentence embeddings like Arc1. This is because the training data is small and we fixed the word embedding in learning. LSTM based models in general performs better than CNN based models, because they can capture sequential information in sentences. KEHNN leverages both the sequential information and the prior knowledge from categories in matching by a CNN with multiple channels. Therefore, it outperforms all other methods, and the improvement is statistically significant (t-test with p-value ≤ 0.05). It is worthy to note that the gap between different methods is not big. This is because answers labeled as ”potential” only cover 10% of the data and are hard to predict."
    }, {
      "heading" : "Response Selection",
      "text" : "Response selection is important for building retrieval-based chatbots (Wang et al. 2013). The goal of the task is to select a proper response for a message from a candidate pool to realize human-machine conversation. We used a public English conversation data set, the Ubuntu Corpus (Lowe et al. 2015), to conduct the experiment. The corpus consists of a large number of human-human dialogue about Ubuntu technique. Each dialogue contains at least 3 turns, and we only kept\n3http://alt.qcri.org/semeval2015/task3/ index.php?id=data-and-tools\nthe last two utterances as we study text pair matching and ignore context information. We used the data pre-processed by Xu et al. (Xu et al. 2016)4, in which all urls and numbers were replaced by “ url ” and “ number ” respectively to alleviate the sparsity issue. The training set contains 1 million message-response pairs with a ratio 1 : 1 between positive and negative responses, and both the validation set and the test set have 0.5 million message-response pairs with a ratio 1 : 9 between positive and negative responses. We followed Lowe et al. (Lowe et al. 2015) and employed recall at position k in n candidates as evaluation metrics and denoted the metrics as Rn@k. Rn@k indicates if the correct response is in the top k results from n candidates.\nSpecific Setting In this task, we trained a topic model to generate topics for both messages and responses as prior knowledge. We crawled 8 million questions (question and description) from the ”Computers & Internet” category in Yahoo! Answers, and utilized these data to train a Twitter LDA model (Zhao et al. 2011) with 100 topics. In order to construct kx,i and ky,i, we separately assigned a topic to a message and a response by the inference algorithm of Twitter LDA. Then we transformed the topic to a vector by averaging the embeddings of top 20 words under the topic. Word embedding tables were initialized using the public word vectors available at http://nlp.stanford. edu/projects/glove (trained on Twitter) and updated in learning. Tanh is used as h in Equation (3), (8), (9).\nResults Table 4 reports the evaluation results on response selection. Our method outperforms baseline models on all metrics, and the improvement is statistically significant (ttest with p-value≤ 0.01). In the data set, as the training data becomes large and we updated word embedding in learning, Arc2 and MatchPyraimd are much better than Arc1. LSTM based models perform better than CNN based models, which is consistent with the results in the QA task."
    }, {
      "heading" : "Discussions",
      "text" : "We first investigate the performance of KEHNN in terms of text length, as shown in Table 5. We compared our model with 2 typical matching models: LSTM and MV-LSTM. We binned the text pairs into 4 buckets, according to the length\n4https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0\nof the concatenation of the two pieces of text. #Pair represents the number of pairs that fall into the bucket. From the results, we can see that on relatively short text (i.e., length in [0, 30) ), KEHNN performs comparably well with MVLSTM, while on long text, KEHNN significantly improves the matching accuracy. The results verified our claim that matching with multiple channels and prior knowledge can enhance accuracy on long text. Note that on the Ubuntu data, all models perform worse on short text than them on long text. This is because we ignored context for short messageresponse pairs, while long pairs are usually independent with context and have complete semantics.\nFurthermore, we also report the contributions of different channels of our model in Table 6. We can see that channel two is the most powerful one on the conversation data, while channel three is the best one on the QA data. This is because the prior knowledge in the conversation data is automatically generated rather than obtained from meta-data like that in the QA data. The automatically generated prior knowledge contains noise which hurts the performance of channel three. The full model outperforms all single channels consistently, demonstrating that matching with multiple channels could leverage the three types of features and sufficiently model the semantic relations in text pairs."
    }, {
      "heading" : "Conclusion",
      "text" : "This paper proposed KEHNN that can leverages prior knowledge in semantic matching. Experimental results show that our model can significantly outperform state-of-the-art matching models on two matching tasks."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Long text brings a big challenge to semantic matching due to<lb>their complicated semantic and syntactic structures. To tackle<lb>the challenge, we consider using prior knowledge to help<lb>identify useful information and filter out noise to matching in<lb>long text. To this end, we propose a knowledge enhanced hy-<lb>brid neural network (KEHNN). The model fuses prior knowl-<lb>edge into word representations by knowledge gates and estab-<lb>lishes three matching channels with words, sequential struc-<lb>tures of sentences given by Gated Recurrent Units (GRU),<lb>and knowledge enhanced representations. The three channels<lb>are processed by a convolutional neural network to generate<lb>high level features for matching, and the features are synthe-<lb>sized as a matching score by a multilayer perceptron. The<lb>model extends the existing methods by conducting matching<lb>on words, local structures of sentences, and global context<lb>of sentences. Evaluation results from extensive experiments<lb>on public data sets for question answering and conversation<lb>show that KEHNN can significantly outperform the-state-of-<lb>the-art matching models and particularly improve the perfor-<lb>mance on pairs with long text.",
    "creator" : "LaTeX with hyperref package"
  }
}