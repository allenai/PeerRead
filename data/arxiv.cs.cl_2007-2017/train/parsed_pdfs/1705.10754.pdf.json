{
  "name" : "1705.10754.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Low Dimensionality Representation for Language Variety Identification",
    "authors" : [ "Francisco Rangel", "Marc Franco-Salvador", "Paolo Rosso" ],
    "emails" : [ "francisco.rangel@autoritas.es,", "mfranco@prhlt.upv.es,", "prosso@dsic.upv.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: low dimensionality representation; language variety identification; similar languages discrimination; author profiling; big data; social media"
    }, {
      "heading" : "1 Introduction",
      "text" : "Language variety identification aims at labelling texts in a native language (e.g. Spanish, Portuguese, English) with their specific variation (e.g. Argentina, Chile, Mexico, Peru, Spain; Brazil, Portugal; UK, US). Although at first sight language variety identification may seem a classical text classification problem, cultural idiosyncrasies may influence the way users construct their discourse, the kind of sentences they build, the expressions they use or their particular choice of words. Due to that, we can consider language variety identification as a double problem of text classification and author profiling, where information about how language is shared by people may help to discriminate among classes of authors depending on their language variety. ? The work of the first author was in the framework of ECOPORTUNITY IPT-2012-1220-\n430000. The work of the last two authors was in the framework of the SomEMBED MINECO TIN2015-71147-C2-1-P research project. This work has been also supported by the SomEMBED TIN2015-71147-C2-1-P MINECO research project and by the Generalitat Valenciana under the grant ALMAPATER (PrometeoII/2014/030)\nar X\niv :1\n70 5.\n10 75\n4v 1\n[ cs\n.C L\n] 3\n0 M\nay 2\n01 7\nThis task is specially important in social media. Despite the vastness and accessibility of the Internet destroyed frontiers among regions or traits, companies are still very interested in author profiling segmentation. For example, when a new product is launched to the market, knowing the geographical distribution of opinions may help to improve marketing campaigns. Or given a security threat, knowing the possible cultural idiosyncrasies of the author may help to better understand who could have written the message.\nLanguage variety identification is a popular research topic of natural language processing. In the last years, several tasks and workshops have been organized: the Workshop on Language Technology for Closely Related Languages and Language Variants @ EMNLP 20141; the VarDial Workshop @ COLING 2014 - Applying NLP Tools to Similar Languages, Varieties and Dialects2; and the LT4VarDial - Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialect3 @ RANLP [14][12]. We can find also several works focused on the task. In [10] the authors addressed the problem of identifying Arabic varieties in blogs and social fora. They used character n-gram features to discriminate between six different varieties and obtained accuracies between 70%-80%. Similarly, [13] collected 1,000 news articles of two varieties of Portuguese. They applied different features such as word and character n-grams and reported accuracies over 90%. With respect to the Spanish language, [6] focused on varieties from Argentina, Chile, Colombia, Mexico and Spain in Twitter. They used meta-learning and combined four types of features: i) character n-gram frequency profiles, ii) character n-gram language models, iii) Lempel-Ziv-Welch compression and iv) syllable-based language models. They obtained an interesting 60%- 70% accuracy of classification.\nWe are interested in discovering which kind of features capture higher differences among varieties. Our hypothesis is that language varieties differ mainly in lexicographic clues. We show an example in Table 1.\nIn this work we focus on the Spanish language variety identification. We differentiate from the previous works as follows: i) instead of n-gram based representations, we propose a low dimensionality representation that is helpful when dealing with big data in social media; ii) in order to reduce the possible over-fitting, our training and test partitions do not share any author of instance between them4; and iii) in contrast to the Twitter dataset of [6], we will make available our dataset to the research community.\n1 http://alt.qcri.org/LT4CloseLang/index.html 2 http://corporavm.uni-koeln.de/vardial/sharedtask.html 3 http://ttg.uni-saarland.de/lt4vardial2015/dsl.html 4 It is important to highlight the importance of this aspect from an evaluation perspective in an\nauthor profiling scenario. In fact, if texts from the same authors are both part of the training"
    }, {
      "heading" : "2 Low Dimensionality Representation",
      "text" : "The key aspect of the low dimensionality representation (LDR) is the use of weights to represent the probability of each term to belong to each one of the different language varieties. We assume that the distribution of weights for a given document should be closer to the weights of its corresponding language variety. Formally, the LDR is estimated as follows:\nTerm-frequency - inverse document frequency (tf-idf) matrix creation. First, we apply the tf-idf [11] weighting for the terms of the documents of the training set D. As result we obtain the following matrix:\n∆ =  w11 w12 ... w1m δ(d1) w21 w22 ... w2m δ(d2) ... ... ... ... wn1 wn2 ... wnm δ(dn)  , (1) where each row in the matrix ∆ represents a document d, each column represents a vocabulary term t, wij represents its tf-idf, and δ(di) represents the assigned class c of the document i, that is, the language variety actually assigned to this document.\nClass-dependent term weighting. Using the matrix ∆, we obtain the class-dependent term weight matrix β. This matrix contains the weights of each term t for each language variety C on the basis of Eq. 2:\nW (t, c) = ∑ d∈D/c=δ(d) wdt∑\nd∈D wdt ,∀d ∈ D, c ∈ C (2)\nBasically, the term weightW (t, c) is the ratio between the weights of the documents belonging to a concrete language variety c and the total distribution of weights for that term t.\nClass-dependent document representation. We employ the class-dependent term weights β to obtain the final representation of the documents as follows:\nd = {F (c1), F (c2), ..., F (cn)} ∼ ∀c ∈ C, (3)\nF (ci) = {avg, std,min,max, prob, prop} (4)\nwhere each F (ci) contains the set of features showed in Eq. 4 and described in Table 2. As we can see, our class-dependent weights β are employed to extract a small5 — but\nand test sets, their particular style and vocabulary choice may contribute at training time to learn the profile of the authors. In consequence, over-fitting would be biasing the results. 5 Our hypothesis is that the distribution of weights for a given document should be closer to the weights of its corresponding language variety, therefore, we use the most common descriptive statistics to measure this variability among language varieties.\nvery discriminant — number of features for each language variety.6 We note that the same process can be followed in order to represent a test document d′ ∈ D′. We just need to use the β matrix obtained with D to index the document d′ by means of Eq. 3."
    }, {
      "heading" : "3 Evaluation Framework",
      "text" : "In this section, we describe the corpus and the alternative representations that we employ in this work."
    }, {
      "heading" : "3.1 HispaBlogs Corpus",
      "text" : "We have created the HispaBlogs dataset7 by collecting posts from Spanish blogs from five different countries: Argentina, Chile, Mexico, Peru and Spain. For each country, there are 450 and 200 blogs respectively for training and test, ensuring that each author appears only in one set. Each blog contains at least 10 posts. The total number of blogs is 2,250 and 1,000 respectively. Statistics of the number of words are shown in Table 3."
    }, {
      "heading" : "3.2 Alternative representations",
      "text" : "We are interested in investigating the impact of the proposed representation and compare its performance with state-of-the-art representations based on n-grams and with two approaches based on the recent and popular distributed representations of words by means of the continuous Skip-gram model [1].\n6 Using the LDR a document is represented by a total set of features equal to 6 multiplied by the number of categories (the 5 language varieties), in our case 30 features. This is a considerable dimensionality reduction that may be helpful to deal with big data environments. 7 The HispaBlogs dataset was collected by experts on social media from the Autoritas Consulting company (http://www.autoritas.net). Autoritas experts in the different countries selected popular bloggers related to politics, online marketing, technology or trends. The HispaBlogs dataset is publicly available at: https://github.com/autoritas/RD-Lab/tree/master/data/ HispaBlogs\nState-of-the art representations State-of-the-art representations are mainly based on n-grams models, hence we tested character and word based ones, besides word with tf-idf weights. For each of them, we iterated n from 1 to 10 and selected 1,000, 5,000 and 10,000 most frequent grams. The best results were obtained with the 10,000 most frequent BOW, character 4-grams and tf-idf 2-grams. Therefore, we will use them in the evaluation.\nDistributed representations Due to the increasing popularity of the distributed representations [4], we used the continuous Skip-gram model to generate distributed representations of words (e.g. n-dimensional vectors), with further refinements in order to use them with documents. The continuous Skip-gram model [7,8] is an iterative algorithm which attempts to maximize the classification of the context surrounding a word. Formally, given a wordw(t), and its surrounding wordsw(t−c), w(t−c+1), ..., w(t+ c) inside a window of size 2c + 1, the training objective is to maximize the average of the log probability shown in Equation 5:\n1\nT T∑ t=1 ∑ −c≤j≤c,j 6=0 log p(wt+j |wt) (5)\nTo estimate p(wt+j |wt) we used negative sampling [8] that is a simplified version of the Noise Contrastive Estimation (NCE) [3,9] which is only concerned with preserving vector quality in the context of Skip-gram learning. The basic idea is to use logistic regression to distinguish the target word WO from draws from a noise distribution Pn(w), having k negative samples for each word. Formally, the negative sampling estimates p(wO|wI) following Equation 6:\nlog σ(v′wO T vwI ) + k∑ i=1 Ewi ∼ Pn(w) [ log σ(−v′wi T vwI ) ] (6)\nwhere σ(x) = 1/(1+exp(−x)). The experimental results in [8] show that this function obtains better results at the semantic level than hierarchical softmax [2] and NCE.\nIn order to combine the word vectors to represent a complete sentence we used two approaches. First, given a list of word vectors (w1, w2, ..., wn) belonging to a document, we generated a vector representation v of its content by estimating the average of their\ndimensions: v = n−1 ∑n i=1 wi. We call this representation Skip-gram in the evaluation. In addition, we used Sentence vectors (SenVec) [5], a variant that follows Skip-gram architecture to train a special vector sv representing the sentence. Basically, before each context window movement, SenVec uses a special vector sv in place of w(t) with the objective of maximizing the classification of the surrounding words. In consequence, sv will be a distributed vector of the complete sentence.\nFollowing state-of-the-art approach [5], in the evaluation we used a logistic classifier for both SenVec and Skip-gram approaches.8"
    }, {
      "heading" : "4 Experimental Results",
      "text" : "In this section we show experimental results obtained with the machine learning algorithms that best solve the problem with the proposed representation, the impact of the preprocessing on the performance, the obtained results in comparison with the ones obtained with state-of-the-art and distributed representations, the error analysis that provides useful insights to better understand differences among languages, a depth analysis on the contribution of the different features and a cost analysis that highlights the suitability of LDR for a big data scenario."
    }, {
      "heading" : "4.1 Machine learning algorithms comparison",
      "text" : "We tested several machine learning algorithms9 with the aim at selecting the one that best solves the task. As can be seen in Table 4, Multiclass Classifier10 obtains the best result (results in the rest of the paper refer to Multiclass Classifier). We carried out a statistical test of significance with respect to the next two systems with the highest performance: SVM (z0.050, 880 < 1, 960) and LogitBoost (z0.05 = 1, 983 > 1, 960).\n8 We used 300-dimensional vectors, context windows of size 10, and 20 negative words for each sample. We preprocessed the text with word lowercase, tokenization, removing the words of length one, and with phrase detection using word2vec tools: https://code.google.com/p/word2vec/\n9 http://www.cs.waikato.ac.nz/ml/weka/ 10 We used SVM with default parameters and exhaustive correction code to transform the multi-\nclass problem into a binary one."
    }, {
      "heading" : "4.2 Preprocessing impact",
      "text" : "The proposed representation aims at using the whole vocabulary to obtain the weights of its terms. Social media texts may have noise and inadequately written words. Moreover, some of these words may be used only by few authors. With the aim at investigating their effect in the classification, we carried out a preprocessing step to remove words that appear less than n times in the corpus, iterating n between 1 and 100. In Figure 1 the corresponding accuracies are shown. In the left part of the figure (a), results for n between 1 and 10 are shown in a continuous scale. In the right part (b), values from 10 to 100 are shown in a non-continuous scale. As can be seen, the best result was obtained with n equal to 5, with an accuracy of 71.1%. As it was expected, the proposed representation takes advantage from the whole vocabulary, although it is recommendable to remove words with very few occurrences that may alter the results. We show examples of those infrequent words in Table 5.\nIn Figure 2, when analysing the evolution of the number of remaining words in function of the value of n, we can see a high number of words with very low frequency\nof occurrence. These words may introduce a high amount of noise in our LDR weight estimation. In addition, removing these words may be also beneficial in order to reduce the processing time needed to obtain the representation. This fact has special relevance for improving the performance in big data environments."
    }, {
      "heading" : "4.3 Language variety identification results",
      "text" : "In Table 6 we show the results obtained by the described representations employing the Multiclass Classifier. As can be appreciated, the proposed low dimensionality representation improves more than 35% the results obtained with the state-of-the-art representations. BOW obtains slightly better results than character 4-grams, and both of them improve significantly the ones obtained with tf-idf 2-grams. Instead of selecting the most frequent n-grams, our approach takes advantage from the whole vocabulary and assigns higher weights to the most discriminative words for the different language varieties as shown in Equation 2.\nWe highlight that our LDR obtains competitive results compared with the use of distributed representations. Concretely, there is no significant difference among them (Skip-gram z0.05 = 0, 5457 < 1, 960 and SenVecz0.05 = 0, 7095 < 1, 960). In addition, our proposal reduces considerably the dimensionality of one order of magnitude as shown in Table 6."
    }, {
      "heading" : "4.4 Error analysis",
      "text" : "We aim at analysing the error of LDR to better understand which varieties are the most difficult to discriminate. As can be seen in Table 7, the Spanish variety is the easiest to discriminate. However, one of the highest confusions occurs from Argentinian to Spanish. Mexican and Spanish were considerably confused with Argentinian too. Finally, the highest confusion occurs from Peruvian to Chilean, although the lowest average confusion occurs with Peruvian. In general, Latin American varieties are closer to each other and it is more difficult to differentiate among them. Language evolves over time. It is logical that language varieties of nearby countries — as the Latin American ones — evolved in a more similar manner that the Spanish variety. It is also logical that even more language variety similarities are shared across neighbour countries, e.g. Chilean compared with Peruvian and Argentinian.\nFig. 3. F1 values for identification as the corresponding language variety vs. others.\nIn Figure 3 we show the precision and recall values for the identification of each variety. As can be seen, Spain and Chile have the highest recall so that texts written in these varieties may have less probability to be misclassified as other varieties. Nevertheless, the highest precisions are obtained for Mexico and Peru, implying that texts written in such varieties may be easier to discriminate."
    }, {
      "heading" : "4.5 Most discriminating features",
      "text" : "In Table 8 we show the most discriminant features. The features are sorted by their information gain (IG). As can be seen, the highest gain is obtained by average, maximum and minimum, and standard deviation. On the other hand, probability and proportionality features has low information gain.\nWe experimented with different sets of features and show the results in Figure 4. As may be expected, average-based features obtain high accuracies (67.0%). However, although features based on standard deviation have not the highest information gain, they obtained the highest results individually (69.2%), as well as their combination with average ones (70,8%). Features based on minimum and maximum obtain low results (48.3% and 54.7% respectively), but in combination they obtain a significant increase (61.1%). The combination of the previous features obtains almost the highest accuracy\n(71.0%), equivalent to the accuracy obtained with probability and proportionality features (71.1%)."
    }, {
      "heading" : "4.6 Cost analysis",
      "text" : "We analyse the cost from two perspectives: i) the complexity to the features; and ii) the number of features needed to represent a document. Defining l as the number of different language varieties, and n the number of terms of the document to be classified, the cost of obtaining the features of Table 2 (average, minimum, maximum, probability and proportionality) isO(l ·n). Definingm as the number of terms in the document that coincides with some term in the vocabulary, the cost of obtaining the standard deviation is O(l ·m). As the average is needed previously to the standard deviation calculation, the total cost isO(l ·n)+O(l ·m) that is equal toO(max(l ·n, l ·m)) = O(l ·n). Since the number of terms in the vocabulary will always be equal or greater than the number of coincident terms (n ≥ m), and as the number of terms in the document will always be much higher than the number of language varieties (l << n), we can determine the cost as lineal with respect to the number of terms in the document O(n). With respect to the number of features needed to represent a document, we showed in Table 6 the considerable reduction of the proposed low dimensionality representation."
    }, {
      "heading" : "4.7 Robustness",
      "text" : "In order to analyse the robustness of the low dimensionality representation to different languages, we experimented with the development set of the DSLCC corpus11 from the Discriminating between Similar Languages task [12]. The corpus consists of 2,000 sentences per language or variety, with between 20 and 100 tokens per sentence, obtained from news headers. In Table 9 we show the results obtained with the proposed representation and the two distributed representations, Skip-gram and SenVec. It is important to notice that, in general, when a particular representation improves for one language is at cost of the other one. We can conclude that the three representations obtained comparative results and support the robustness of the low dimensionality representation."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this work, we proposed the LDR low dimensionality representation for language variety identification. Experimental results outperformed traditional state-of-the-art representations and obtained competitive results compared with two distributed representationbased approaches that employed the popular continuous Skip-gram model. The dimensionality reduction obtained by means of LDR is from thousands to only 6 features per language variety. This allows to deal with large collections in big data environments such as social media. Recently, we have applied LDR to the age and gender identification task obtaining competitive results with the best performing teams in the author profiling task at the PAN12 Lab at CLEF.13 As a future work, we plan to apply LDR to other author profiling tasks such as personality recognition. 11 http://ttg.uni-saarland.de/lt4vardial2015/dsl.html 12 http://pan.webis.de 13 http://www.clef-innitiative.org"
    } ],
    "references" : [ {
      "title" : "Classes for fast maximum entropy training",
      "author" : [ "Goodman", "Joshua" ],
      "venue" : "Proceedings of the Acoustics, Speech, and Signal Processing (ICASSP’01) vol. 1 pp. 561–564",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
      "author" : [ "Gutmann", "Michael U.", "Hyvärinen", "Aapo" ],
      "venue" : "The Journal of Machine Learning Research vol. 13 pp. 307–361",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Distributed representations, Parallel distributed processing: explorations in the microstructure of cognition, vol",
      "author" : [ "Hinton", "Geoffrey E.", "Mcclelland", "James L.", "Rumelhart", "David E." ],
      "venue" : "1: foundations. In: MIT Press, Cambridge, MA",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Le", "Quoc V.", "Mikolov", "Tomas" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning (ICML’14) vol. 32",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Language variety identification in Spanish tweets",
      "author" : [ "Maier", "Wolfgang", "Gómez-Rodrı́guez", "Carlos" ],
      "venue" : "Workshop on Language Technology for Closely Related Languages and Language Variants (EMNLP’14) pp. 25–35",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "Proceedings of Workshop at International Conference on Learning Representations (ICLR’13)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S.", "Dean", "Jeff" ],
      "venue" : "Advances in Neural Information Processing Systems pp. 3111–3119",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A fast and simple algorithm for training neural probabilistic language models",
      "author" : [ "Mnih", "Andriy", "Teh", "Yee Whye" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning (ICML’12) pp. 1751–1758",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Automatic identification of Arabic language varieties and dialects in social media",
      "author" : [ "Sadat", "Fatiha", "Kazemi", "Farmazeh", "Farzindar", "Atefeh" ],
      "venue" : "1st. International Workshop on Social Media Retrieval and Analysis (SoMeRa’14)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Term-weighting approaches in automatic text retrieval",
      "author" : [ "Salton", "Gerard", "Buckley", "Christopher" ],
      "venue" : "Information processing & management, vol. 24(5), pp. 513–523",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Merging comparable data sources for the discrimination of similar languages: The DSL corpus collection",
      "author" : [ "Tan", "Liling", "Zampieri", "Marcos", "Ljubešic", "Nicola", "Tiedemann", "Jörg" ],
      "venue" : "7th Workshop on Building and Using Comparable Corpora Building Resources for Machine Translation Research (BUCC’14) pp. 6–10",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatic identification of language varieties: The case of portuguese",
      "author" : [ "Zampieri", "Marcos", "Gebrekidan-Gebre", "Binyam" ],
      "venue" : "Proceedings of the 11th Conference on Natural Language Processing (KONVENS’12) pp. 233–237",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A report on the DSL shared task 2014",
      "author" : [ "Zampieri", "Marcos", "Tan", "Liling", "Ljubeši", "Nicola", "Tiedemann", "Jörg" ],
      "venue" : "Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial’14) pp. 58–67",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "In the last years, several tasks and workshops have been organized: the Workshop on Language Technology for Closely Related Languages and Language Variants @ EMNLP 20141; the VarDial Workshop @ COLING 2014 - Applying NLP Tools to Similar Languages, Varieties and Dialects2; and the LT4VarDial - Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialect3 @ RANLP [14][12].",
      "startOffset" : 395,
      "endOffset" : 399
    }, {
      "referenceID" : 10,
      "context" : "In the last years, several tasks and workshops have been organized: the Workshop on Language Technology for Closely Related Languages and Language Variants @ EMNLP 20141; the VarDial Workshop @ COLING 2014 - Applying NLP Tools to Similar Languages, Varieties and Dialects2; and the LT4VarDial - Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialect3 @ RANLP [14][12].",
      "startOffset" : 399,
      "endOffset" : 403
    }, {
      "referenceID" : 8,
      "context" : "In [10] the authors addressed the problem of identifying Arabic varieties in blogs and social fora.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "Similarly, [13] collected 1,000 news articles of two varieties of Portuguese.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "With respect to the Spanish language, [6] focused on varieties from Argentina, Chile, Colombia, Mexico and Spain in Twitter.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "We differentiate from the previous works as follows: i) instead of n-gram based representations, we propose a low dimensionality representation that is helpful when dealing with big data in social media; ii) in order to reduce the possible over-fitting, our training and test partitions do not share any author of instance between them4; and iii) in contrast to the Twitter dataset of [6], we will make available our dataset to the research community.",
      "startOffset" : 385,
      "endOffset" : 388
    }, {
      "referenceID" : 9,
      "context" : "First, we apply the tf-idf [11] weighting for the terms of the documents of the training set D.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "Distributed representations Due to the increasing popularity of the distributed representations [4], we used the continuous Skip-gram model to generate distributed representations of words (e.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "The continuous Skip-gram model [7,8] is an iterative algorithm which attempts to maximize the classification of the context surrounding a word.",
      "startOffset" : 31,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "The continuous Skip-gram model [7,8] is an iterative algorithm which attempts to maximize the classification of the context surrounding a word.",
      "startOffset" : 31,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "To estimate p(wt+j |wt) we used negative sampling [8] that is a simplified version of the Noise Contrastive Estimation (NCE) [3,9] which is only concerned with preserving vector quality in the context of Skip-gram learning.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "To estimate p(wt+j |wt) we used negative sampling [8] that is a simplified version of the Noise Contrastive Estimation (NCE) [3,9] which is only concerned with preserving vector quality in the context of Skip-gram learning.",
      "startOffset" : 125,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "To estimate p(wt+j |wt) we used negative sampling [8] that is a simplified version of the Noise Contrastive Estimation (NCE) [3,9] which is only concerned with preserving vector quality in the context of Skip-gram learning.",
      "startOffset" : 125,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "The experimental results in [8] show that this function obtains better results at the semantic level than hierarchical softmax [2] and NCE.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "The experimental results in [8] show that this function obtains better results at the semantic level than hierarchical softmax [2] and NCE.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "In addition, we used Sentence vectors (SenVec) [5], a variant that follows Skip-gram architecture to train a special vector sv representing the sentence.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "Following state-of-the-art approach [5], in the evaluation we used a logistic classifier for both SenVec and Skip-gram approaches.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "In order to analyse the robustness of the low dimensionality representation to different languages, we experimented with the development set of the DSLCC corpus11 from the Discriminating between Similar Languages task [12].",
      "startOffset" : 218,
      "endOffset" : 222
    } ],
    "year" : 2017,
    "abstractText" : "Language variety identification aims at labelling texts in a native language (e.g. Spanish, Portuguese, English) with its specific variation (e.g. Argentina, Chile, Mexico, Peru, Spain; Brazil, Portugal; UK, US). In this work we propose a low dimensionality representation (LDR) to address this task with five different varieties of Spanish: Argentina, Chile, Mexico, Peru and Spain. We compare our LDR method with common state-of-the-art representations and show an increase in accuracy of ∼35%. Furthermore, we compare LDR with two reference distributed representation models. Experimental results show competitive performance while dramatically reducing the dimensionality — and increasing the big data suitability — to only 6 features per variety. Additionally, we analyse the behaviour of the employed machine learning algorithms and the most discriminating features. Finally, we employ an alternative dataset to test the robustness of our low dimensionality representation with another set of similar languages.",
    "creator" : "LaTeX with hyperref package"
  }
}