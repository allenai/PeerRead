{"conference": "ICLR 2017 conference submission", "title": "Sample Importance in Training Deep Neural Networks", "abstract": "The contribution of each sample during model training varies across training iterations and the model's parameters. We define the concept of sample importance as the change in parameters induced by a sample. In this paper, we explored the sample importance in training deep neural networks using stochastic gradient descent. We found that \"easy\" samples -- samples that are correctly and confidently classified at the end of the training -- shape parameters closer to the output, while the \"hard\" samples impact parameters closer to the input to the network. Further, \"easy\" samples are relevant in the early training stages, and \"hard\" in the late training stage. Further, we show that constructing batches which contain samples of comparable difficulties tends to be a poor strategy compared to maintaining a mix of both hard and easy samples in all of the batches. Interestingly, this contradicts some of the results on curriculum learning which suggest that ordering training examples in terms of difficulty can lead to better performance.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers provided detailed, confident reviews and there was significant discussion between the parties. \n \n Reviewer 2 and 3 felt quite strongly that the paper was a clear reject. Reviewer 1 thought the paper should be accepted.\n \n I was concerned with the two points raised by R3 and don't feel they were adequately addressed by the author's comments:\n \n - Dependence of the criteria on the learning rate (this does not make sense to me); and\n - Really really poor results on CIFAR-10 (and this is not being too picky, like asking them to be state-of-the-art; they are just way off)\n \n I engaged R1 to see how they felt about this. In reflection, I side with the majority opinion that the paper needs re-work to meet the ICLR acceptance bar.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "a few interesting ideas but very big issues", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term \\phi^t_{i,j} is never defined, only \\phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).\n\nThe paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)\n\nThe study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions\n - \u201cthe overall sample importance is different under different epochs\u201d => yes the norm of the gradient is expected to vary\n - \u201cOutput layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops\u201d => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables\n\nThe question of Figure 4 is absurd \u201cIs Sample Importance the same as Negative log-likelihood of a sample?\u201d. Of course not.\n\nThe results are very bad on CIFAR which discredits the applicability of those results.\n\nOn Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%\n\nDespite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented.", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "22 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "This paper examines how the gradient of each individual data sample influences the various aspect of SGD learning for neural networks.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper examines the so called \"Sample Importance\" of each sample of a training data set, and its effect to the overall learning process.\n\nThe paper shows empirical results that shows different training cases induces bigger gradients at different stages of learning and different layers.\nThe paper shows some interesting results contrary to the common curriculum learning ideas of using easy training samples first. However, it is unclear how one should define \"Easy\" training cases.\n\nIn addition, the experiments demonstrating ordering either NLL or SI is worse than mixed or random batch construction to be insightful.\n\nPossible Improvements:\nIt would be nice to factor out the magnitudes of the gradients to the contribution of \"sample importance\". Higher gradient (as a function of a particular weight vector) can be affected weight/initialization, thereby introducing noise to the model.\n\nIt would also be interesting if improvements based on \"Sample importance\" could be made to the batch selection algorithm to beat the baseline of random batch selection.\n\n\nOverall this paper is a good paper with various experiments examining how various samples in SGD influences the various aspect of training.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "Batch Selection reference is missing", "IS_META_REVIEW": false, "comments": "Your work is closely related to batch selection: \"Online batch selection for faster training of neural networks\" from ICLR Workshops 2016 where the authors provided \"an initial study of possible online batch selection strategies\". In your work, batch selection is denoted as \"batch construction\" and you study a broader set of selection strategies. \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "04 Dec 2016", "TITLE": "Clarification on \"NLL is not predictive of the SI\"", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "confirm graph interpretation, ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Overall importance", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers provided detailed, confident reviews and there was significant discussion between the parties. \n \n Reviewer 2 and 3 felt quite strongly that the paper was a clear reject. Reviewer 1 thought the paper should be accepted.\n \n I was concerned with the two points raised by R3 and don't feel they were adequately addressed by the author's comments:\n \n - Dependence of the criteria on the learning rate (this does not make sense to me); and\n - Really really poor results on CIFAR-10 (and this is not being too picky, like asking them to be state-of-the-art; they are just way off)\n \n I engaged R1 to see how they felt about this. In reflection, I side with the majority opinion that the paper needs re-work to meet the ICLR acceptance bar.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "a few interesting ideas but very big issues", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term \\phi^t_{i,j} is never defined, only \\phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).\n\nThe paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)\n\nThe study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions\n - \u201cthe overall sample importance is different under different epochs\u201d => yes the norm of the gradient is expected to vary\n - \u201cOutput layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops\u201d => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables\n\nThe question of Figure 4 is absurd \u201cIs Sample Importance the same as Negative log-likelihood of a sample?\u201d. Of course not.\n\nThe results are very bad on CIFAR which discredits the applicability of those results.\n\nOn Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%\n\nDespite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented.", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "22 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "This paper examines how the gradient of each individual data sample influences the various aspect of SGD learning for neural networks.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper examines the so called \"Sample Importance\" of each sample of a training data set, and its effect to the overall learning process.\n\nThe paper shows empirical results that shows different training cases induces bigger gradients at different stages of learning and different layers.\nThe paper shows some interesting results contrary to the common curriculum learning ideas of using easy training samples first. However, it is unclear how one should define \"Easy\" training cases.\n\nIn addition, the experiments demonstrating ordering either NLL or SI is worse than mixed or random batch construction to be insightful.\n\nPossible Improvements:\nIt would be nice to factor out the magnitudes of the gradients to the contribution of \"sample importance\". Higher gradient (as a function of a particular weight vector) can be affected weight/initialization, thereby introducing noise to the model.\n\nIt would also be interesting if improvements based on \"Sample importance\" could be made to the batch selection algorithm to beat the baseline of random batch selection.\n\n\nOverall this paper is a good paper with various experiments examining how various samples in SGD influences the various aspect of training.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "14 Dec 2016", "TITLE": "Batch Selection reference is missing", "IS_META_REVIEW": false, "comments": "Your work is closely related to batch selection: \"Online batch selection for faster training of neural networks\" from ICLR Workshops 2016 where the authors provided \"an initial study of possible online batch selection strategies\". In your work, batch selection is denoted as \"batch construction\" and you study a broader set of selection strategies. \n", "OTHER_KEYS": "(anonymous)"}, {"DATE": "04 Dec 2016", "TITLE": "Clarification on \"NLL is not predictive of the SI\"", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "03 Dec 2016", "TITLE": "confirm graph interpretation, ", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "02 Dec 2016", "TITLE": "Overall importance", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "Tianxiang Gao, Vladimir Jojic", "accepted": false, "id": "591"}