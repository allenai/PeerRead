{
  "name" : "1307.3337.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unsupervised Gene Expression Data using Enhanced Clustering Method",
    "authors" : [ ],
    "emails" : [ "ch_ansekh80@rediffmail.com", "drktvel@gmail.com", "elayarajaphd.e@gmail.com", "en.sathishkumar@yahoo.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "monitor the expression profiles of thousands of genes under various experimental conditions. Identification of co-expressed genes and coherent patterns is the central goal in microarray or gene expression data analysis and is an important task in bioinformatics research. Feature selection is a process to select features which are more informative. It is one of the important steps in knowledge discovery. The problem is that not all features are important. Some of the features may be redundant, and others may be irrelevant and noisy. In this work the unsupervised Gene selection method and Enhanced Center Initialization Algorithm (ECIA) with K-Means algorithms have been applied for clustering of Gene Expression Data. This proposed clustering algorithm overcomes the drawbacks in terms of specifying the optimal number of clusters and initialization of good cluster centroids. Gene Expression Data show that could identify compact clusters with performs well in terms of the Silhouette Coefficients cluster measure.\nKeywords—Clustering; Unsupervised Feature Selection; ECIA;\nK-Means; Gene expression data\nI. INTRODUCTION\nMining microarray gene expression data is an important research topic in bioinformatics with broad applications. Microarray technologies are powerful techniques for simultaneously monitoring the expression of thousands of genes under different sets of conditions. Gene expression data can be analyzed in two ways: unsupervised and supervised analysis. In supervised analysis, information about the structure/groupings of the object is assumed known or at least partially known. This prior knowledge is used in analysis process. In unsupervised analysis, prior knowledge is unknown.\nClustering of gene expression data can be divided into two main categories: Gene-based clustering and Sample-based clustering. In gene based clustering, genes are treated as objects and samples are features or attributes for clustering. The goal of gene-based clustering is to identify differentially expressed genes and sets of genes or conditions with similar expression pattern or profiles, and to generate a list of expression measurements. In Sample based clustering, samples are treated as objects and genes are features for clustering. Sample based clustering can be used to reveal the phenotype structure or substructure of samples. Applying the conventional clustering methods to cluster samples using all the genes as features may degrade the quality and reliability of clustering results.\nClustering has been used in a number of applications such as engineering, biology, medicine and data mining. Cluster analysis of gene expression data has proved to be a useful tool for identifying co-expressed genes. DNA microarrays are emerged as the leading technology to measure gene expression levels primarily, because of their high throughput. Results from these experiments are usually presented in the form of a data matrix in which rows represent genes and columns represent conditions or samples [13]. Each entry in the matrix is a measure of the expression level of a particular gene under a specific condition. Analysis of these data sets reveals genes of unknown functions and the discovery of functional relationships between genes [19]. Co-expressed genes can be grouped into clusters based on their expression patterns of gene based clustering and Sample based clustering. In gene based clustering, the genes are treated as the objects, while the samples are the features. In sample based clustering, the samples can be partitioned into homogeneous groups where the genes are regarded as features and the samples as objects [20].\nDiscriminant analysis is now widely used in bioinformatics, such as distinguishing cancer tissues from normal tissues or one cancer subtype vs. another [4]. A critical issue in discriminant analysis is feature selection: instead of using all available variables (features or attributes) in the data, one selectively chooses a subset of features to be used in the discriminant system. There are number of advantages of feature selection: (1) dimension reduction to reduce the computational cost; (2) reduction of noise to improve the classification accuracy; (3) more interpretable features or characteristics that can help identify and monitor the target diseases or function types. These advantages are typified in DNA microarray gene expression profiles. Of the tens of thousands of genes in experiments, only a smaller number of them show strong correlation with the targeted phenotypes [4, 17].\nThe most popular clustering algorithms in microarray gene expression analysis are Hierarchical clustering, K-Means clustering [3], and SOM [9]. Of these K-Means clustering is very simple and fast efficient. Numerous methods have been proposed to solve clustering problem. The most popular clustering methods are K-Means clustering algorithm which is developed by Mac Queen [7]. The K-Means algorithm is effective in producing clusters for many practical applications. But the computational complexity of the original K-Means algorithm is very high, especially for large data sets. The KMeans clustering algorithm is a partitioning clustering method\nthat separates data into K groups. For the real life problems, the suitable number of clusters cannot be predicted. To overcome the above drawback the current research focused on developing the clustering algorithms without giving the initial number of clusters [2, 3, 5].\nThis paper is organized as follows. Section 2 presents and overview of Gene based clustering Techniques. Section 3 describes the Unsupervised Clustering algorithms. Section 4 describes performance of Experimental analysis and discussion. Section 5 conclusion and future work.\nII. GENE BASED CLUSTERING TECHNIQUES\nThe purpose of clustering gene expression data is to reveal the natural structure inherent data and extracting useful information from noisy data. The two class cancer subtype classification problem, 50 informative genes are usually sufficient. There are studies suggesting that only a few genes are sufficient [4]. Thus, computation is reduced while prediction accuracy is increased via effective feature selection. When a small number of genes are selected, their biological relationship with the target diseases is more easily identified. These \"marker\" genes provide additional scientific understanding of the problem [8]. Selecting an effective and more representative feature set is the subject of this paper."
    }, {
      "heading" : "A. Analysis the Data",
      "text" : "The gene expression data is min-max normalization by setting min 0 and max 1. Min-max normalization performs a linear transformation on the original data. Suppose that minA and maxA are the minimum and maximum values of an attribute A. Min-max normalization maps a value of A to \uD835\uDC63 in the range [new_minA, new_ maxA] by computing\n\uD835\uDC63 ′ = \uD835\uDC63 − \uD835\uDC5A\uD835\uDC56\uD835\uDC5B\uD835\uDC34\n\uD835\uDC5A\uD835\uDC4E\uD835\uDC65\uD835\uDC34 −\uD835\uDC5A\uD835\uDC56\uD835\uDC5B\uD835\uDC34 \uD835\uDC5B\uD835\uDC52\uD835\uDC64_\uD835\uDC5A\uD835\uDC4E\uD835\uDC65\uD835\uDC34 − \uD835\uDC5B\uD835\uDC52\uD835\uDC64_\uD835\uDC5A\uD835\uDC56\uD835\uDC5B\uD835\uDC34 + \uD835\uDC5B\uD835\uDC52\uD835\uDC64_\uD835\uDC5A\uD835\uDC56\uD835\uDC5B\uD835\uDC34\nMin-max normalization preserves the relationships among the original data values. It will encounter an “out-of-bounds” error if a future input case for normalization falls outside of the original data range for A. After the normalization of discretized value of gene gi at condition, tj is given the Gene Expression Data are normalizes then run on the discretized data. Discretization is then performed on this normalized expression data. The discretization is done as follows [4].\ni. The discretized value of gene gi at condition, t1 (i.e., the first condition)\n g\uD835\uDC56 ,\uD835\uDC611=\n1 \uD835\uDC56\uD835\uDC53 \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC61\uD835\uDC57 > 0 0 \uD835\uDC56\uD835\uDC53 \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC611 = 0\n−1 \uD835\uDC56\uD835\uDC53 \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC611 < 0\nii. The discretized values of gene gi at conditions tj (j = 1, 2, …, T − 1) i.e., at the rest of the conditions (T − {t1})\n g\uD835\uDC56 ,\uD835\uDC61\uD835\uDC57+1= 1 \uD835\uDC56\uD835\uDC53 \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC61\uD835\uDC57 < \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC61\uD835\uDC57+1 0 \uD835\uDC56\uD835\uDC53 \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC61\uD835\uDC57 = \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC61\uD835\uDC57+1 −1 \uD835\uDC56\uD835\uDC53 \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC61\uD835\uDC57 > \uD835\uDF00g\uD835\uDC56 ,\uD835\uDC61\uD835\uDC57+1\nWhere gi, tj is the discretized value of gene gi at conditions tj (j = 1, 2, …, T − 1). The expression value of gene gi at condition tj is given by gi, tj. We see in the above computation that the first condition, t1, is treated as a special case and its discretized value is directly based on gi, t1 i.e., the expression value at condition t1. For the rest of the conditions the discretized value is calculated by comparing its expression value with that of the previous value. This helps in finding whether the gene is up 1 or down -1 regulated at that particular condition. Each gene will now have a regulation pattern of 0, 1, and -1 across the conditions or time points. This pattern is represented as a string."
    }, {
      "heading" : "B. Unsupervised Gene Selection",
      "text" : "Gene selection is an important problem in the research of Gene expression data analysis. In some cases, too many redundant or missing values are there in gene expression data. In this section, an existing works of Velayutham and Thangavel et al proposed an algorithm of unsupervised feature selection (Unsupervised Quick Reduct algorithms) [21] is applied. This method is based on dependency measure using rough set theory.\nRough set theory can be regarded as a new mathematical tool for imperfect data analysis. The theory has found applications in many domains. Objects characterized by the same information are indiscernible (similar) in view of the available information about them. The indiscernibility relation generated in this way is the mathematical basis of rough set theory. Any set of all indiscernible (similar) objects is called an elementary set, and forms a basic granule (atom) of knowledge about the universe [17, 24]. Any union of some elementary sets is referred to as a crisp (precise) set – otherwise the set is rough (imprecise, vague). Rough set is a pair of precise sets, called the lower and the upper approximation of the rough set, and is associated. The lower approximation consists of all objects which surely belong to the set and the upper approximation contains all objects which possibly belong to the set. The difference between the upper and the lower approximation constitutes the boundary region of the rough set. From a data table are called columns of which are labeled by attributes, rows – by objects of interest and entries of the table are attribute values.\nIn data mining applications, decision class labels are often unknown or incomplete. In this situation the unsupervised feature selection is play vital role to select features."
    }, {
      "heading" : "C. Unsupervised Quick Reduct Algorithm[21]",
      "text" : "The USQR algorithm attempts to calculate a reduct without exhaustively generating all possible subsets. It starts off with an empty set and adds in turn, one at a time, those attributes that result in the greatest increase in the rough set dependency metric, until this produces its maximum possible value for the dataset. According to the algorithm, the mean dependency of each attribute subset is calculated and the best candidate is chosen:\nγ \uD835\uDC45 \uD835\uDC4E =\n\uD835\uDC43\uD835\uDC42\uD835\uDC46\uD835\uDC45(\uD835\uDC4E)\n\uD835\uDC48 ,∀\uD835\uDC4E ∈ \uD835\uDC34\nWhere R is a reduct if and only if K R ({a})=K c ({a}) and ∀\uD835\uDC65 \uD835\uDC45, \uD835\uDC3E\uD835\uDC65 \uD835\uDC4E \uD835\uDC3E\uD835\uDC50( \uD835\uDC4E )\nAlgorithm 1: The USQR algorithm\nUSQR (C) C, the set of all conditional features;\n(1) R ← {} (2) do (3) T← R (4) ∀ x ∈ (C-R) (5) ∀ y ∈ C (6) γR∪{x} y = POS R∪{x }(y)\nU\n(7) if γR∪{x} y , ∀ y ∈ C > γT y , ∀ y ∈ C (8) T ← R∪{x} (9) R ← T\n(10) until γR y , ∀ y ∈ C = γC y , ∀ y ∈ C (11) return R\nIII. UNSUPERVISED CLUSTERING ALGORITHMS\nThe reduced data selected by the gene selection algorithm USQR are clustered by using the K-Means algorithms."
    }, {
      "heading" : "A. K-Means Clustering",
      "text" : "The main objective in cluster analysis is to group objects that are similar in one cluster and separate objects that are dissimilar by assigning them to different clusters. One of the most popular clustering methods is K-Means clustering algorithm [3, 9, 12]. It is classifies objects to a pre-defined number of clusters, which is given by the user (assume K clusters). The idea is to choose random cluster centers, one for each cluster. These centers are preferred to be as far as possible from each other. In this algorithm mostly Euclidean distance is used to find distance between data points and centroids [6, 13, 22]. The Euclidean distance between two multi-dimensional data points X = (x1, x2, x3, ..., xm) and Y = (y1, y2, y3, ..., ym) is described as follows\nD(X, Y) = (\uD835\uDC651 − \uD835\uDC661) 2 + (\uD835\uDC652 − \uD835\uDC662) 2 +⋯+ (\uD835\uDC65\uD835\uDC5A − \uD835\uDC66\uD835\uDC5A) 2\nThe K-Means method aims to minimize the sum of squared distances between all points and the cluster centre. This procedure consists of the following steps, as described below.\nAlgorithm 2: K-Means clustering algorithm [18]\nRequire: D = {d1, d2, d3, ..., dn } // Set of n data points.\nK - Number of desired clusters\nEnsure: A set of K clusters.\nSteps: 1. Arbitrarily choose k data points from D as initial centroids;\n2. Repeat\nAssign each point di to the cluster which has the closest centroid; Calculate the new mean for each cluster;\nUntil convergence criteria is met.\nThough the K-Means algorithm is simple, it has some drawbacks of quality of the final clustering, since it highly depends on the arbitrary selection of the initial centroids[1, 14]."
    }, {
      "heading" : "B. The Enhanced method",
      "text" : "Performance of iterative clustering algorithms which converges to numerous local minima depends highly on initial cluster centers. Generally initial cluster centers are selected randomly. In this section, the cluster centre initialization algorithm is studied to improve the performance of the KMeans algorithm.\nAlgorithm 3: Enhanced Centre Initialization Algorithm\n(ECIA) [25]\nRequire: D = {d1, d2, d3,..., di,..., dn } // Set of n data\npoints. di = { x1, x2, x3,..., xi,..., xm } // Set of attributes of one data point. k // Number of desired clusters.\nEnsure: A set of k clusters.\nSteps: 1. In the given data set D, if the data points contains the both\npositive and negative attribute values then go to Step 2, otherwise go to step 4.\n2. Find the minimum attribute value in the given data set D. 3. For each data point attribute, subtract with the minimum\nattribute value.\n4. For each data point calculate the distance from origin. 5. Sort the distances obtained in step 4. Sort the data points\naccordance with the distances.\n6. Partition the sorted data points into k equal sets. 7. In each set, take the middle point as the initial centroid. 8. Compute the distance between each data point di (1 ≤ i ≤\nn) to all the initial centroids cj (1 ≤ j ≤ k). 9. Repeat 10. For each data point di, find the closest centroid cj and\nassign di to cluster j.\n11. Set ClusterId[i]=j. // j:Id of the closest cluster. 12. Set NearestDist[i]= d(di, cj). 13. For each cluster j (1 ≤ j ≤ k), recalculate the centroids. 14. For each data point di, 14.1 Compute its distance from the centroid of the present\nnearest cluster.\n14.2 If this distance is less than or equal to the present nearest\ndistance, the data point stays in the same cluster.\nElse 14.2.1 For every centroid cj (1≤ j ≤ k) compute the distance\nd(di, cj). End for;\nUntil convergence criteria is met.\nIV. EXPERIMENTAL ANALYSIS AND DISCUSSION\nIn this section, we describe the data sets used to analyze the methods studied in sections 2 and 3, which are arranged for the listed in Table 1, number of features/genes are in column wise, and number of items/samples are in row wise [23].\nIt can be downloaded from: http://www.sciencemag.org/\nfeature/ data/ 984559.shl and corresponds to the selection of\n517 genes whose expression varies in response to serum\nconcentration inhuman fibroblasts [11].\n2) Yeast data: This data set is downloaded from Gene\nExpression Omnibus-databases. The Yeast cell cycle dataset\ncontains 2884 genes and 17 conditions. To avoid distortion or\nbiases arising from the presence of missing values in the data\nmatrix we removal all the genes that had any missing value.\nThis step results in a matrix of size 2882 * 17.\n3) Simulated data: It is downloaded from http://\nwww.igbmc.ustrasbg.fr/ projects/fcm/y3c.txt. The set contains\n300 Genes [3].\n4) Leukemia data: It is downloaded from the website:\nhttp://datam.i2r.a-star.edu.sg/datasets/krbd/. The set contains\n7129*34."
    }, {
      "heading" : "A. Comparative Analysis",
      "text" : "The ECIA with K-Means used to clustering in the all data\nsets after the section 2 as Table 2, the distance measure used here is the Euclidean distance. To access the quality of the clusters, we used the silhouette measure proposed by Rousseeuw [11, 15, 16]. In this method the Initial centroid values taken 7 then run as 10 times running to ECIA with Kmeans clusters data into 7 groups as Table 3.\nTABLE II. COMPARATIVE ANALYSIS TO FILTER OF GENE EXPRESSION DATA\nData sets Before Unsupervised\nGene Selection\nAfter Unsupervised\nGene selection\nSerum 517*17 49*17\nYeast 2884*17 118*17\nSimulated 300*17 67*17\nLeukemia 7129*34 142*34\nTABLE III. COMPARATIVE ANALYSIS OF GENE EXPRESSION DATA\nData sets\nClust\ner\nSerum (49*17)\nYeast\n(118*17)\nSimulated\n(67*17)\nLeukemia\n(142*34)\nNo.\nof\nGen\ne\nMeas\nure\nvalue\nNo.\nof\nGen\ne\nMeas\nure\nvalue\nNo.\nof Ge ne\nMeas\nure\nvalue\nNo.\nof\nGen\ne\nMeas\nure\nvalue\nC1 6 0.629 13 0.621 10 0.314 19 0.391 C2 7 0.532 20 0.597 9 0.428 21 0.301 C3 6 0.439 21 0.584 12 0.278 15 0.401 C4 9 0.347 19 0.471 9 0.291 25 0.321\nC5 7 0.301 16 0.421 11 0.281 19 0.491 C6 6 0.397 17 0.327 8 0.325 22 0.336 C7 8 0.309 12 0.697 8 0.391 21 0.418\nTotal Gene 49 118 67 142\nIn table 3, the comparative analysis of gene expression data\nrepresent genes and measure value of four data sets. The Simulated data set produce high measure value comparing to\nother three data sets."
    }, {
      "heading" : "B. Performance Comparison",
      "text" : "In this various clusters performance is analysis to best cluster such as compact genes as Fig.1 and table 4. Among four data sets, each one produce compact cluster and provide\nbest genes.\nIn this work, unsupervised gene selections method USQR was studied and applies to avoid too many redundant or missing values in Microarray gene expression data. In this unsupervised gene selection method is based on unsupervised Feature selection using Rough set methods. These methods are used to get minimum number of random gene data sets and then we use K-Means clustering technique to improve the quality of clusters. One of the demerits of K-Means algorithm is random selection of initial seed point of desired clusters. This was overcome with ECIA for finding the initial centroids to avoid the random selection of initial values. Therefore, the ECIA algorithm is not depending upon any choice of the\nnumber of cluster and automatic evaluation of initial seed centroids and it produces different better results. Both the algorithms were tested with gene expression data and analysis the performance of cluster values using silhouette measurement. Therefore, finding solution to select the different centroids as clusters seed points and various measures are used to improve the cluster performance is our\nfuture endeavor."
    } ],
    "references" : [ {
      "title" : "Initializing K-Means using Genetic Algorithms",
      "author" : [ "Bashar Al-Shboul", "Sung-Hyon Myaeng" ],
      "venue" : "World Academy of Science, Engineering and Technology",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "K-Means Clustering Algorithm with Improved Initial center",
      "author" : [ "Chen Zhang", "Shixiong Xia" ],
      "venue" : "Second International Workshop on Knowledge Discovery and Data Mining (WKDD), pp. 790-792, 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Mininmum Redundancy Feature Selection from Microarray Gene Expression Data”, proceedings of the International Bioinformatic Conference",
      "author" : [ "Chris Ding", "Hanchuna Peng" ],
      "venue" : "Date on 11-14,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "A New Algorithm to Get the Initial Centroids",
      "author" : [ "F. Yuan", "Z.H. Meng", "H.X. Zhangz", "C.R. Dong" ],
      "venue" : "proceedings of the 3rdInternational Conference on Machine Learning and Cybernetics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "K - Modes clustering",
      "author" : [ "A Chaturvedi J.C.", "P Green" ],
      "venue" : "Journals of Classification, (18):35–55, 2001.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Fuzzy C means method for clustering microarray data",
      "author" : [ "Doulaye Dembele", "Philippe Kastner" ],
      "venue" : "Bioinformatics, vol.19,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "An Interactive Approach to mining Gene Expression Data",
      "author" : [ "Daxin Jiang", "Jian Pei", "Aidong Zhang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Network constrained clustering for gene microarray Data”, doi:10.1093 /bioinformatics",
      "author" : [ "Dongxiao Zhu", "Alfred O Hero", "Hong Cheng", "Ritu Khanna", "Anand Swaroop" ],
      "venue" : "bti 655,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "An Efficient enhanced K-Means clustering algorithm",
      "author" : [ "A.M Fahim", "M Salem A", "A Torkey", "A Ramadan M" ],
      "venue" : "Journal of Zhejiang University,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Automatic Generation of Merge Factor for Clustering Microarray Data",
      "author" : [ "K Karteeka Pavan", "Allam Appa Rao", "A V Dattatreya Rao", "GR Sridhar" ],
      "venue" : "IJCSNS International Journal of Computer Science and Network Security,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Divisive Correlation Clustering Algorithm (DCCA) for grouping of genes: detecting varying Patterns in expression profiles",
      "author" : [ "K.R De", "A. Bhattacharya" ],
      "venue" : "bioinformatics, Vol. 24, pp. 1359-1366, 2008.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Improving the accuracy and efficiency of the K-Means clustering algorithm”, international Conference on Data Mining and Knowledge Engineering (ICDMKE)",
      "author" : [ "K.A. Abdul Nazeer", "M.P. Sebastian" ],
      "venue" : "Proceedings of the World Congress on Engineering (WCE- 2009), London, UK",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Selecting variables for K-Means cluster analysis by using a genetic algorithm that optimises the silhouettes",
      "author" : [ "R. Lletí", "M.C. Ortiz", "L.A. Sarabia", "M.S. Sánchez" ],
      "venue" : "Analytica Chimica Acta,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "An Efficient Approach for Computing Silhouette Coefficients",
      "author" : [ "Moh'd Belal Al- Zoubi", "Mohammad al Rawi" ],
      "venue" : "Journal of Computer Science",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Means Clustering Algorithm with Improved Initial Center",
      "author" : [ "Madhu Yedla", "Srinivasa Rao Pathakota", "T M Srinivasa", "“Enhancing K" ],
      "venue" : "Madhu Yedla et al. / (IJCSIT) International Journal of Computer Science and Information Technologies,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Microarray biochip technology",
      "author" : [ "Sunnyvale", "Schena M" ],
      "venue" : "CA: Eaton Publishing;",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2000
    }, {
      "title" : "An Effective Technique for Clustering Incremental Gene expression data",
      "author" : [ "Sauravjoyti Sarmah", "Dhruba K. Bhattacharyya" ],
      "venue" : "IJCSI International Journal of Computer Science Issues,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Improved K-Means Clustering Algorithm for Exploring Local Protein Sequence Motifs Representing Common Structural Property",
      "author" : [ "Wei Zhong", "Gulsah Altun", "Robert Harrison", "Phang C. Tai", "Yi Pan" ],
      "venue" : "IEEE transactions on nano bio science,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Cancer Classification Using Single Genes",
      "author" : [ "Xiaosheng Wang", "Osamu Gotoh" ],
      "venue" : "20th international conference on Genome informatics(GIW 2009),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Incremental Genetic K-Means Algorithm and its Application in Gene Expression Data Analysis",
      "author" : [ "Y. Lu", "S. Lu", "F. Fotouhi", "Y. Deng", "S. Brown" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Means Clustering Algorithm with Improved Initial Center",
      "author" : [ "Y. Madhu", "Srinivasa Rao Pathakota", "T M Srinivasa", "“Enhancing K" ],
      "venue" : "Madhu Yedla et al. / (IJCSIT) International Journal of Computer Science and Information Technologies,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Results from these experiments are usually presented in the form of a data matrix in which rows represent genes and columns represent conditions or samples [13].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "Analysis of these data sets reveals genes of unknown functions and the discovery of functional relationships between genes [19].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "In sample based clustering, the samples can be partitioned into homogeneous groups where the genes are regarded as features and the samples as objects [20].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "another [4].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 2,
      "context" : "Of the tens of thousands of genes in experiments, only a smaller number of them show strong correlation with the targeted phenotypes [4, 17].",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "The most popular clustering algorithms in microarray gene expression analysis are Hierarchical clustering, K-Means clustering [3], and SOM [9].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "The most popular clustering algorithms in microarray gene expression analysis are Hierarchical clustering, K-Means clustering [3], and SOM [9].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "The most popular clustering methods are K-Means clustering algorithm which is developed by Mac Queen [7].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "To overcome the above drawback the current research focused on developing the clustering algorithms without giving the initial number of clusters [2, 3, 5].",
      "startOffset" : 146,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "To overcome the above drawback the current research focused on developing the clustering algorithms without giving the initial number of clusters [2, 3, 5].",
      "startOffset" : 146,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "To overcome the above drawback the current research focused on developing the clustering algorithms without giving the initial number of clusters [2, 3, 5].",
      "startOffset" : 146,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "There are studies suggesting that only a few genes are sufficient [4].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "These \"marker\" genes provide additional scientific understanding of the problem [8].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "The discretization is done as follows [4].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "Any set of all indiscernible (similar) objects is called an elementary set, and forms a basic granule (atom) of knowledge about the universe [17, 24].",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "One of the most popular clustering methods is K-Means clustering algorithm [3, 9, 12].",
      "startOffset" : 75,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "One of the most popular clustering methods is K-Means clustering algorithm [3, 9, 12].",
      "startOffset" : 75,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "In this algorithm mostly Euclidean distance is used to find distance between data points and centroids [6, 13, 22].",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "In this algorithm mostly Euclidean distance is used to find distance between data points and centroids [6, 13, 22].",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "In this algorithm mostly Euclidean distance is used to find distance between data points and centroids [6, 13, 22].",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "Algorithm 2: K-Means clustering algorithm [18]",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "Though the K-Means algorithm is simple, it has some drawbacks of quality of the final clustering, since it highly depends on the arbitrary selection of the initial centroids[1, 14].",
      "startOffset" : 173,
      "endOffset" : 180
    }, {
      "referenceID" : 20,
      "context" : "Algorithm 3: Enhanced Centre Initialization Algorithm (ECIA) [25]",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "In this section, we describe the data sets used to analyze the methods studied in sections 2 and 3, which are arranged for the listed in Table 1, number of features/genes are in column wise, and number of items/samples are in row wise [23].",
      "startOffset" : 235,
      "endOffset" : 239
    }, {
      "referenceID" : 8,
      "context" : "1) Serum data: This data set is described and used in [10].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "shl and corresponds to the selection of 517 genes whose expression varies in response to serum concentration inhuman fibroblasts [11].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "The set contains 300 Genes [3].",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "To access the quality of the clusters, we used the silhouette measure proposed by Rousseeuw [11, 15, 16].",
      "startOffset" : 92,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "To access the quality of the clusters, we used the silhouette measure proposed by Rousseeuw [11, 15, 16].",
      "startOffset" : 92,
      "endOffset" : 104
    }, {
      "referenceID" : 13,
      "context" : "To access the quality of the clusters, we used the silhouette measure proposed by Rousseeuw [11, 15, 16].",
      "startOffset" : 92,
      "endOffset" : 104
    } ],
    "year" : 2013,
    "abstractText" : "Microarrays are made it possible to simultaneously monitor the expression profiles of thousands of genes under various experimental conditions. Identification of co-expressed genes and coherent patterns is the central goal in microarray or gene expression data analysis and is an important task in bioinformatics research. Feature selection is a process to select features which are more informative. It is one of the important steps in knowledge discovery. The problem is that not all features are important. Some of the features may be redundant, and others may be irrelevant and noisy. In this work the unsupervised Gene selection method and Enhanced Center Initialization Algorithm (ECIA) with K-Means algorithms have been applied for clustering of Gene Expression Data. This proposed clustering algorithm overcomes the drawbacks in terms of specifying the optimal number of clusters and initialization of good cluster centroids. Gene Expression Data show that could identify compact clusters with performs well in terms of the Silhouette Coefficients cluster measure. Keywords—Clustering; Unsupervised Feature Selection; ECIA; K-Means; Gene expression data",
    "creator" : "Microsoft® Office Word 2007"
  }
}