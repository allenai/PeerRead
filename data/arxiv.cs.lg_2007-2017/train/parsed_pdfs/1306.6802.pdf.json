{
  "name" : "1306.6802.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evaluation Measures for Hierarchical Classification: a unified view and novel approaches",
    "authors" : [ "Aris Kosmopoulos", "Ioannis Partalas", "Eric Gaussier", "Georgios Paliouras", "Ion Androutsopoulos" ],
    "emails" : [ "akosmo@iit.demokritos.gr", "paliourg@iit.demokritos.gr", "ioannis.partalas@imag.fr,", "eric.gaussier@imag.fr,", "ion@aueb.gr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n68 02\nHierarchical classification addresses the problem of classifying items into a hierarchy of classes. An important issue in hierarchical classification is the evaluation of different classification algorithms, which is complicated by the hierarchical relations among the classes. Several evaluation measures have been proposed for hierarchical classification using the hierarchy in different ways. This paper studies the problem of evaluation in hierarchical classification by analyzing and abstracting the key components of the existing performance measures. It also proposes two alternative generic views of hierarchical evaluation and introduces two corresponding novel measures. The proposed measures, along with the state-of-the-art ones, are empirically tested on three large datasets from the domain of text classification. The empirical results illustrate the undesirable behavior of existing approaches and how the proposed methods overcome most of these methods across a range of cases."
    }, {
      "heading" : "1 Introduction",
      "text" : "Hierarchical classification addresses the problem of classifying items into a hierarchy of classes. In past years mainstream classification research did not place enough emphasis on the presence of relations between the classes, in our cases hierarchical relations. This is gradually changing and more effort is put into\nhierarchical classification in particular, partly because many real-world knowledge systems and services use a hierarchical scheme to organize their data (e.g. Yahoo, Wikipedia). Research in hierarchical classification has become important, because flat classification algorithms are ill-equipped to address large scale problems with hundreds of thousands of hierarchically related classes. Promising initial results on large-scale problems show that hierarchical classifiers can be effective in improving information retrieval (Kosmopoulos et al., 2010).\nMany research questions in hierarchical classification remain open. An important issue is how to properly evaluate hierarchical classification algorithms. While standard flat classification problems have benefited from established measures such as precision and recall, there are no established evaluation measures for hierarchical classification tasks, where the assessment of an algorithm becomes more complicated due to the relations among the classes. For example, classification errors in the upper levels of the hierarchy (e.g. when wrongly classifying a document of the class music into the class food) are more severe than those in deeper levels (e.g. when classifying a document from progressive rock as alternative rock). Several evaluation measures have been proposed for hierarchical classification (HC) (Costa et al., 2007; Sokolova and Guy, 2009) using the hierarchy in different ways. Nevertheless, none of them is widely adopted, making it very difficult to compare the performance of different HC algorithms.\nA number of comparative studies of HC performance measures have been published in the literature. An early study can be found in (Sun et al., 2003), which is limited to a particular type of graph-distance measures. A review of HC measures is presented in (Costa et al., 2007), focusing on single-label tasks and without providing any empirical results; in multi-label tasks each object can be assigned to more than one classes, e.g. a newspaper article may belong to both politics and economics. In (Nowak et al., 2010) many multi-label evaluation measures are compared, but the role of the hierarchy is not emphasized. Finally, Brucker et al. (2011) provide a comprehensive empirical analysis of HC performance measures, but they focus on the evaluation of clustering methods rather than classification ones. While these studies provide interesting insights, they all miss important aspects of the problem of evaluating HC algorithms. In particular, they do not abstract the problem in order to describe existing evaluation measures within a common framework.\nThe work presented here addresses these issues by analyzing and abstracting the key components of existing HC performance measures. More specifically:\n1. It groups existing HC evaluation measures under two main types and provides a generic framework for each type, based on flow networks and set theory.\n2. It provides a critical overview of the existing HC performance measures using the proposed framework.\n3. It introduces two new HC evaluation measures that address important deficiencies of state-of-the-art measures.\n4. It provides comparative empirical results on large HC datasets from text classification with a variety of HC algorithms.\nThe remainder of this paper is organized as follows. Section 2 introduces the problem of HC, presents general requirements for HC measures and the proposed frameworks. Furthermore, it presents existing HC evaluation measures using the proposed frameworks and introduces two new measures that address problems the state-of-the-art measures have. Section 3 presents a case study comparison and analysis of the proposed measures and the existing ones. Section 4 describes the empirical setting and data of the empirical analysis of the measures and Section 5 presents and discusses the empirical results. Finally, Section 6 concludes and summarizes remaining open issues."
    }, {
      "heading" : "2 A Framework of Hierarchical Classification Per-",
      "text" : "formance Measures\nThis section presents a new framework within which HC performance measures can be described and characterized. Firstly, supporting notation is defined and then the general requirements for the evaluation are presented and discussed, based on interesting problems that appear in hierarchical classification. We then proceed with the presentation of the proposed framework, which is used in further sections to describe and analyze the measures."
    }, {
      "heading" : "2.1 Notation",
      "text" : "In classification tasks the training set is typically denoted as S = { (xi,yi) }n\ni=1 ,\nwhere xi is the feature vector of instance i and yi ⊆ C is the set of classes in which the instance belongs, where C = {c1, . . . , cK}.\nIn contrast to flat classification, where the classes are considered unrelated, in HC the classes are organized in taxonomies. The taxonomies are usually either trees, in which case nodes (classes) have a single parent each, or directed acyclic graphs (DAGs), in which case nodes can have multiple parents; see Figures 1(a) and 1(b) respectively. In some cases, hierarchies may also be cyclic graphs. In all cases the hierarchy imposes a parent-child relation among the classes, which implies that an instance belonging in a specific class, also belongs in all its ancestor classes. A taxonomy is thus usually defined as a pair (C,≺), where C is the set of all classes (Silla and Freitas, 2011) and ≺ is the subclass-of relationship with the following properties:1\n• Asymmetry: if ci ≺ cj then cj ⊀ ci for every ci, cj ∈ C.\n• Anti-reflexivity: ci ⊀ ci for every ci ∈ C. 1Without loss of generality, we assume a subclass-of relationship among the classes, but in some cases a different relationship may hold, for example part-of. We assume however, that the three properties always hold for the relationship."
    }, {
      "heading" : "1.1 1.2 2.1 3.1 3.2 3.3",
      "text" : ""
    }, {
      "heading" : "1.1 1.2 2.1 3.1 3.2 3.3",
      "text" : "• Transitivity: if ci ≺ cj and cj ≺ ck, then ci ≺ ck for every ci, cj , ck ∈ C.\nIn graphs with cycles, only the transitivity property holds. In this article we consider only hierarchies without cycles and we denote the descendants and ancestors of a class c ∈ C as De(c) and An(c), respectively. The parents of a class c are denoted as Pa(c). Finally, we assume that an instance can be classified in any class of the hierarchy, and not only in the leaf classes."
    }, {
      "heading" : "2.2 General Problems in Hierarchical Classification Evaluation",
      "text" : "The commonly used measures of precision, recall, F-measure, accuracy etc. are not appropriate for HC, due to the relations that exist among the classes. A hierarchical performance measure should use the class hierarchy in order to evaluate properly HC algorithms. In particular, one must account for several different types of error according to the hierarchy. For example, consider the tree hierarchy in Figure 1(a). Assume that the true class for a test instance is 3.1 and that two different classification systems output 3 and 1 as the predicted classes. Using flat evaluation measures, both systems are punished equally, but the error of the second system is more severe as it makes a prediction in a different and unrelated sub-tree.\nIn order to measure the severity of an error in hierarchical classification, there are several interesting issues that need to be addressed. Figure 2 presents five cases that require special handling. In all cases, the nodes surrounded by\ncircles are the true classes, while the nodes surrounded by rectangles are the predicted ones. These cases can be sub-grouped in a) pairing problems (Figures 2(d) and 2(e)) where one must select which pairs of predicted and true classes to take into account for the calculation of the error, and b) distance-measuring problems (Figures 2(c), 2(a) and 2(b)) which concern the way that the error will be calculated for a pair of predicted and true classes.\nFigure 2(a) presents an over-specialization error where the predicted class is a descendant of the true class. Figure 2(b) depicts an under-specialization error, where an ancestor of the true class is selected. In both these cases the desired behavior of the measure would be to reduce the penalty to the classification system, according to the distance between the true class and the predicted one.\nThe third case (Figure 2(c)), called alternative paths, presents a scenario where there are two different ways to reach the true class starting from a predicted class. In this case, a measure could use one of the two paths or both in order to evaluate the performance of the classification system. Selecting the path that minimizes the distance between the two classes and using that as a measure of error seems reasonable. In Figure 2(c) the predicted class is an ancestor of the true class, but an alternative paths case may also involve multiple paths from an ancestor to a descendant predicted class.\nFigure 2(d) presents a scenario which is common in multi-label data. In this case one must decide, before even measuring the error, which pairs of true and predicted classes should be compared. For example, node A (true class) could be compared to B (predicted) and D to C; or node A could be compared to both B and C, and node D to none; other pairings are also possible. Depending on the pairings, the score assigned to the classifier will be different. It seems reasonable to use the pairings that minimize the classification error. For example, in Figure 2(d) it could be argued that the prediction of B and C are based on evidence about A and thus both B and C should be compared to A.\nFinally, Figure 2(e) presents a case where the predicted class should probably not be matched to any true class. This is typically the case when the predicted class and the true class are too distant which is why we call this case the long distance problem."
    }, {
      "heading" : "2.3 Pair-based Measures",
      "text" : "Pair-based measures assign costs to pairs of predicted and true classes. For example, in Figure 2(d) class B could be paired with A and class C with D, and then the sum of the corresponding costs would give the total misclassification error.\nLet Ŷ = {ŷi|i = 1 . . .M} and Y = {yj|j = 1 . . .N} be the sets of the predicted and true classes respectively, for a single test instance (the index of the instance is omitted due to simplicity). The sets Y and Ŷ are augmented with a default predicted and a default true class, respectively corresponding to ŷM+1 and yN+1. These classes are used when a predicted class cannot or should not be paired to any true class and vice-versa. For example, when the distances between a predicted class ŷi and all the true classes yi exceed a predefined\nthreshold (see the long distance problem in Figure 2(e)), the predicted class ŷi may be paired with the default true class.\nAdditionally, let κij be the cost of predicting class ŷi instead of the true class yj. The matrix K = [κij ]i=1...M+1,j=1...N+1, κij ≥ 0, ∀i, j contains the costs of all possible pairs of predicted and true classes, including the default classes.\nPair-based measures typically calculate the cost κij of a pair of a predicted class ŷi and a true class yi as the minimum distance of ŷi and yi in the hierarchy, e.g. as the number of edges between the classes along the shortest path that connects them. The intuition is that the closer the two classes are in the hierarchy, the more similar they are, and therefore the less severe the error. More elaborate cost measures may assign weights to the hierarchy’s edges, and the weights may decrease when moving from the top to the bottom (Blockeel et al., 2002; Holden and Freitas, 2006). The distance to the default classes is usually set to a fixed large value.\nIn a spirit of fairness (minimum penalty), the aim of an evaluation mea-\nsure is to pair the classes returned by a system and the true classes in a way that minimizes the overall classification error. This can be formulated as the following optimization problem:\nProblem 1. \n       \n        \nmin xij ,\n1≤i≤M+1, 1≤j≤N+1\n∑\ni=1...(N+1), j=1...(M+1)\nκijxij\nsubject to: (i) ∀i = 1 . . .M, ∀j = 1 . . .N, xij ∈ {0; 1}; x(M+1)(N+1) = 0 (ii) αp ≤ ∑N+1 j=1 xij ≤ βp, ∀i = 1 . . .M\n(iii) αt ≤ ∑M+1 i=1 xij ≤ βt, ∀j = 1 . . .N\nConstraint (i) states that xij , which denotes the alignment between classes, is either 0 (classes ŷi and yj are not paired) or 1 (classes ŷi and yj are paired); it furthermore states that the default predicted and true classes cannot be aligned (these default classes are solely used to “collect” those predicted and true classes with no counterpart). The parameters αp, βp ∈ N (constraint (ii)) are the lower and upper bounds of the allowed number of true classes that a predicted class can be paired with. For example, setting αp = βp = 1 requires each predicted class to be paired with exactly one true class. Similarly, the parameters αt, βt ∈ N (constraint (iii)) limit the number of predicted labels that a true class can be paired with. The above constraints directly imply2 that ∀i = 1 . . .M, xiN+1 ≤ βp and ∀j = 1 . . .N, xM+1j ≤ βt, meaning that the default true class can be aligned to at most βpM predicted classes and the default predicted class to at most βtN true classes.\nThe above problem corresponds to a best pairing problem in a bipartite graph, the nodes of which being respectively the predicted and true classes. It is important to note here that the pairing we are looking for is not a matching, since the same node can be paired with several nodes. We opt to approach this problem as a graph pairing one rather than a linear optimization one, for two reasons: first because there exist polynomial solutions to pairing problems in graphs, and second because the graph framework allows one to easily illustrate how the different cost-based measures proposed so far relate to each other. In particular, we model it as a cost flow minimization problem (Ahuja et al., 1993)."
    }, {
      "heading" : "2.3.1 A Flow Network Model for Class Pairing",
      "text" : "A flow network is a directed graph G = (V,E) with m edges, where each edge u ∈ E is associated with a lower and an upper capacity denoted bu and cu respectively. The flow along an edge u is denoted as φu and bu ≤ φu ≤ cu. The flow of the network is a vector φ = (φ1, φ2, . . . , φm)\nT ∈ Rm. For each vertex 2Indeed, in the worst case, i.e. when all xij but xiN+1 are 0, constraint (ii) yields xiN+1 =\nβp; the reasoning is similar for constraint (iii).\ni ∈ V , the flow conservation property holds: ∑\nu∈ω+(i)\nφu = ∑\nu∈ω−(i)\nφu\nwhere ω+(i) and ω−(i) denote the set of edges entering and leaving vertex i respectively. Each edge u is also associated with a cost γu which represents the cost of using this edge. The total cost of a flow φ is:\nγT × φ = ∑\nu∈E\nγuφu,\nwhere γ = (γ1, γ2, . . . , γm) T ∈ Rm. The minimum cost flow is the one that minimizes γTφ while satisfying the capacity and flow conservation constraints. The quantity to be minimized in flow networks is the same as the one in Problem 1, the constraints in this latter problem corresponding to capacity constraints, as explained below. Furthermore, the following integrality theorem states that when the bounds of capacity intervals are integers, there exists a minimal cost flow such that the quantity of flow on each edge is also an integer:\nIntegrality Theorem. If a flow network has capacities which are all integer valued and there exists some feasible flow in the network, then there is a minimum cost feasible flow with an integer valued flow on every arc.\nFurthermore, all standard algorithms for finding minimal cost flows guarantee to find this particular flow (Ahuja et al., 1993).\nPairing problems in bipartite graphs are represented with flow networks by adding two nodes, a source and a sink, and edges from the source to the first set of nodes, from the second set of nodes to the sink, and from the sink to the source. These extra nodes and edges ensure that the flow conservation constraints are satisfied. For pair-based measures, one thus obtains the following flow network framework G(V,E) (see also Figure 3):\n• V includes a source, a sink, the predicted classes, the true classes, a default true class and a default predicted class;\n• E includes edges from the source to all the predicted classes (including the default predicted class), from every predicted class to every true class (including the default true class), from every true class to the sink and from the sink to the source.\nNo edges exist between the default predicted and default true class, as required by constraint (i) above.\nIn our setting the capacity interval [bu; cu] of an edge u expresses the possible number of pairs that each predicted or true class can participate in. The interval between each pair of predicted and true classes restricts the flow on that network which indicates whether this pair will be considered in the calculation of the evaluation measure. Put it differently, in the solved flow network the flow values\nwill reflect the specific evaluation measure as they show the pairs that make up the solution with the minimum cost. The intervals between the source and the predicted classes as well as between the true classes and the sink also affect the way that the pairing will be performed.\nDue to the constraints in Problem 1, the capacity intervals are defined as follows:\n• From each predicted class ŷj to each true class yi, excluding the default class, the capacity interval is [0;1]; the integrality theorem here implies that the flow value between predicted and true classes will be either 0 or 1, i.e. a predicted and a true class either be paired (1) or not paired (0). The capacity bounds here correspond to the xij values of Problem 1 (constraint (i));\n• From the source to a (non-default) predicted class, the capacity interval is [αp;βp] meaning that a predicted class is aligned with at least αp (and at most βp) true classes;\n• Similarly, from a (non-default) true class to the sink, the capacity intervals is [αt;βt] meaning that a true class is aligned with at least αt (and at most βt) predicted classes;\n• From each predicted class ŷj to the default true class the capacity interval is [0;βp] and from the default predicted class to each true class the capacity interval is [0;βt]; from the source (resp. sink) to the default predicted (resp. true) class, the capacity interval is [0;βtN ] (resp. [0;βpM ]), as mentioned in footnote 2;\n• Lastly, from the sink to the source, the capacity interval is [0;βtN+βpM ], which corresponds to a loose setting compatible with the intervals given above (this last capacity interval does not impose any constraint but is necessary to ensure flow conservation)."
    }, {
      "heading" : "2.3.2 Existing Pair-based Measures",
      "text" : "The majority of the existing pair-based measures deals only with tree hierarchies and single-label problems. Under these conditions the pairing problem becomes simple, because a single path exists between the predicted and the true classes. The complexity of the problem increases when the hierarchy is a DAG or when the problem is multi-labeled; current measures cannot handle the majority of the phenomena presented in Section 2.2.\nIn the simplest case of pair-based measures (Dekel et al., 2004; Holden and Freitas, 2006), the measure trivially pairs the single prediction with the single true label (M = N = 1), so that αp = βp = αt = βt = 1. Note that no default classes exist in this measure, or equivalently the corresponding costs are equal to infinity, γ(DP, T ) = γ(DT,P ) = +∞\nM N\nFor a pair (ŷj , yi), of a predicted and a true class, depicted as P and T respectively in Figure 4, γ(ŷj, yi) = γ(P, T ) = κij is taken to be the distance between yi and ŷj:\nκij = ∑\ne∈E(i,j)\nwe, (1)\nwhere E(i, j) is the set of edges along the path from yi to ŷj in the hierarchy and we is the weight of edge e. For we = 1, we get what Dekel et al. (2004) call tree induced error.\nIn (Sun and Lim, 2001) two cost measures are proposed for multi-label problems in tree hierarchies, where all possible pairs of the predicted and true classes are used in the calculation. In this case, αp = βp = N and αt = βt = M . Again, no default classes are used and so the corresponding costs are: γ(DP, yi) = γ(DT, ŷj) = +∞, i = 1 . . .N, j = 1 . . .M . Note that this is an extreme case, where all pairs of predicted and true labels are used. The weights we are calculated in two alternative ways: a) as the similarity (e.g., cosine similarity) between the classes of the predicted and true ones, and b) using the distances of the hierarchy as in Equation 1.\nA measure dubbed Graph Induced Error (GIE) was proposed and used during the second Large Scale Hierarchical Text classification challenge (LSHTC)3. GIE is based on the best matching pairs of predicted and true classes and can handle multi-label (and single-labeled) classification with both tree and DAG class hierarchies. For a particular instance being classified, each predicted class is paired either with one true class or with the default true class; multiple predicted classes can be paired with the default true class (Figure 5). Similarly, each true class is paired with exactly one predicted class or with the default predicted class, and several true classes can be paired with the default predicted class. Hence, αp = βp = αt = βt = 1. The cost κij is computed as in Equation 1, with we = 1, ∀e. If the hierarchy is a DAG, multiple hierarchy paths may link each predicted class ŷj to its paired true class yi; then E(i, j) is taken to be the shortest of these paths. The cost of pairing a class (predicted or true) with a default one is set to a positive value Dmax. Figure 5 presents the corresponding flow network.\nIn multi-label classification GIE’s concept of “best” matching fails to address the pairing problem of Section 2.2. For example, if a predicted class has two true classes as children, as in Figure 2(d), then only one of them would be paired with its parent. The other one would either be penalized with Dmax or would be paired with another distant class."
    }, {
      "heading" : "2.3.3 Multi-label Graph Induced Accuracy",
      "text" : "We propose here a straightforward extension of GIE called Multi-label Graph Induced Accuracy (MGIA), in which each class is allowed to participate in more than one pair. This extension makes the method more suitable to the pairing problem. Figure 6 presents the MGIA flow network, in which αp = αt = 1,\n3http://lshtc.iit.demokritos.gr/\nβp = N , βt = M . The cost of pairing a class (predicted or true) with a default one is set as in GIE. Solving the flow network optimization problem is easy since the only constraints are that the default predicted class cannot be paired with the default true class and that categories of the same set (predicted or true) cannot be paired to each other. Thus each pairing can be solved separately from the others by pairing a class with either the default class of the other set, or the nearest class of the other set.\nAs in the previous pair-based measures, after the solution of the problem an error is calculated on the solved network. Instead of using directly this error for evaluation we define an accuracy based measure as follows:\n1− fnerror|P ∪ T \\ P ∩ T | ∗Dmax where fnerror is the value provided by the solved flow network.\nThe above measure is bounded in [0,1] and the better a system is the closer it will be to 1. Note that in the case where all predicted classes and all true classes\nare paired with the respective default classes, fnerror will reach its maximum value |P ∪T |∗Dmax and will be equal to the denominator as P ∩T = ∅ resulting in a value of 0. Essentially, the advantage of the proposed measure over other pair-based measures is that it takes into account the correct predictions of the classification system (that is the true positives, P ∩ T )."
    }, {
      "heading" : "2.4 Set-based Measures",
      "text" : "The performance measures of this category are based on operations on the entire sets of predicted and true classes, possibly including also their ancestors and descendants, as opposed to pair-based measures, which consider only pairs of predicted and true classes.\nSet-based measures have two distinct phases:\n1. The augmentation of Y and Ŷ with information about the hierarchy.\n2. The calculation of a cost measure based on the augmented sets.\nThe augmentation of Y and Ŷ is a crucial step, attempting to capture the hierarchical relations of the classes. For example, the sets may be augmented with the ancestors of the true and predicted classes as follows:\nYaug = Y ∪ An(y1) ∪ . . . ∪ An(yN ) (2)\nŶaug = Ŷ ∪ An(ŷ1) ∪ . . . ∪ An(ŷM ) (3) Using the augmented sets of predicted and true classes, two approaches have mainly been adopted to calculate the misclassification cost: a) symmetric difference loss and b) hierarchical precision and recall.\nSymmetric difference loss is calculated as follows, where |S| the cardinality of a set S:\nl∆(Yaug, Ŷaug) = |(Ŷaug \\ Yaug) ∪ (Yaug \\ Ŷaug)|\nIf we use the initial Ŷ and Y sets instead of Ŷaug, Yaug , the measure becomes the standard symmetric difference for flat multi-label classification. Also, note that the two quantities of the symmetric loss difference express the false positive and false negative rates respectively.\nOn the other hand, hierarchical precision and recall are defined as follows:\nPH = |Ŷaug ∩ Yaug |\n|Ŷaug|\nRH = |Ŷaug ∩ Yaug|\n|Yaug| The nominator of these measures expresses the true positive rate and can be written as follows:\n|Ŷaug ∩ Yaug| = |Ŷaug ∪ Yaug| − l∆((Yaug , Ŷaug)\nwhere we note that the symmetric loss is a substractive term. Set-based measures are not affected by the pairing problem of Figure 2(d) and the long distance problem of Figure 2(e), as they do not rely on pairing of true and predicted classes."
    }, {
      "heading" : "2.4.1 Existing Set-based Measures",
      "text" : "Different measures differ mainly in the way the sets of predicted and true classes are augmented. In (Kiritchenko et al., 2005; Struyf et al., 2005; Cai and Hofmann, 2007) the ancestors of the predicted and true classes are added to Yaug and Ŷaug, as in Equations 2 and 3 above. Alternatively, in Ipeirotis et al. (2001) the descendants of the true and predicted classes are added:\nYaug = Y ∪De(y1) ∪ . . . ∪De(yN)\nŶaug = Ŷ ∪De(ŷ1) ∪ . . . ∪De(ŷM )\nIn the latter approach, when the true and predicted classes are in different sub-graphs of the hierarchy (different sub-trees, if the hierarchy is a tree), a maximum penalty will be given, even when several ancestors have been correctly predicted.\nIn (Cesa-Bianchi et al., 2006), the approach that adds the ancestors is adopted (Equations 2 and 3) but the augmented sets are then altered as follows:\nYaug ← Yaug \\ { yk\n∣ ∣ ∣ ∣ yk ∈ Yaug ∧ yk /∈ Ŷaug ∧ Pa(yk) /∈ Ŷaug }\n(4)\nŶaug ← Ŷaug \\ { ŷk\n∣ ∣ ∣ ∣ ŷk ∈ Ŷaug ∧ ŷk /∈ Yaug ∧ Pa(ŷk) /∈ Yaug }\n(5)\nEquation 5 introduces some tolerance to over-specialization. Consider, for example, Figure 7(a) where we assume that the only true class is A and the only predicted class is C. According to Equation 3 we add class B (and class A) to Ŷaug. Based on Equation 5 we then remove C from Ŷaug to avoid penalizing the classification method for B and C. Similarly, with Equation 4 we tolerate underclassification. In Figure 7(b) the only true class is C and the only predicted class is A. According to Equation 2 class B (and A) are added to Yaug. Based on Equation 4 then we remove C from Ŷaug to avoid penalizing the classification method for both B and C. The drawback of this measure is that it tends to favor category systems that stop their predictions early in the hierarchy."
    }, {
      "heading" : "2.4.2 Lowest Common Ancestor Precision, Recall and F1 Measures",
      "text" : "The approach proposed in this paper is based on the hierarchical versions of precision, recall and F1, which add all the ancestors of the predicted and true classes to Yaug and Ŷaug. Adding all the ancestors has the undesirable effect of over-penalizing errors that happen to nodes with many ancestors.\nIn an attempt to address this issue, we propose the Lowest Common Ancestor Precision (PLCA), Recall (RLCA) and F1 (FLCA) measures. These measures use the concept of the lowest common ancestor (LCA) as defined in graph theory (Aho et al., 1973).\nDefinition 1. The lowest common ancestor LCA(n1, n2) of two nodes n1 and n2 of a tree T is defined as the lowest node in T (furthest from the root) that is an ancestor of both n1 and n2.\nFor example, in Figure 8(a) LCA(3.1, 3.2.2) = 3. In the case of a DAG the definition of LCA changes. LCA(n1, n2) is a set of nodes (instead of a single node), since it is possible for two nodes to have more than one LCA. Furthermore, the LCA may not necessarily be the node that is furthest from the root. In order to define the LCA between two DAG nodes, we use the concept of the shortest path between them.\nDefinition 2. Given a set Pall(n1, n2) containing all paths that connect nodes n1 and n2, we define pathsmin(n1, n2) ⊆ Pall(n1, n2) as the set for which: ∀p ∈ pathsmin(n1, n2); ∄p′ ∈ Pall(n1, n2) \\ pathsmin(n1, n2) : cost(p) ≤ cost(p′)\nwhere the cost of a path corresponds to its length, when the edges of the hierarchy are unweighted.\nFor example in Figure 8(b):\n• pathmin(2.1, 3.1) = {2.1, 3, 3.1}\n• pathmin(2.1, 3.2.2) = {2.1, 3, 3.2.2}\n• pathmin(3.2.2, 3.2.1) = {3.2.2, 3.2, 3.2.1}\nIt is worth noting that in the general case pathsmin(n1, n2) is a set of paths; not a single one.\nIn multi-label classification, we would like to extend the definition of LCA to compare a node n (e.g. a true class) against a set of nodes S (e.g. the predicted classes).\nDefinition 3. The LCA(n, S) of a node n and a set of nodes S is the set of all the lowest common ancestors LCA(n, i) for each i ∈ Sbest(n, S) ⊆ S, where Sbest(n, S) = {i ∈ S : ∄j ∈ S, j 6= i∧ cost(pathmin(n, i)) > cost(pathmin(n, j))} For example, in Figure 8(b) Sbest(3.1, {2.1, 3.3, 3.2.1}) = {2.1, 3.3} and LCA(3.1, {2.1, 3.3, 3.2.1}) is {3}.\nGiven this definition and sets, Y being the true and Ŷ the predicted classes of an instance, we compute the LCA(y, Ŷ ) of each element y of Y . Similarly for each element ŷ of Ŷ by computing LCA(ŷ, Y ). Using Figure 8(b), let Y = {2.1, 3.2.1, 3.3} and Ŷ = {3.1, 3.2.1, 3.2.2}. Then\n• LCA(2.1, Ŷ ) = {3}, connecting 2.1 with either 3.1 using pathmin(2.1, 3.1) or 3.2.2 using pathmin(2.1, 3.2.2).\n• LCA(3.3, Ŷ ) = {3}, connecting 3.3 with either 3.1 using pathmin(3.3, 3.1) or 3.2.2 using pathmin(3.3, 3.2.2).\n• LCA(3.2.1, Ŷ ) = {3.2.1}, connecting 3.2.1 with itself.\n• LCA(3.2.1, Y ) = {3.2.1}, connecting 3.2.1 with itself.\n• LCA(3.1, Y ) = {3}, connecting 3.1 with either 2.1 using pathmin(3.1, 2.1) or 3.3 using pathmin(3.1, 3.3).\n• LCA(3.2.2, Y ) = {3.2, 3}, the first connecting 3.2.2 with 3.2.1 using pathmin(3.2.2, 3.2.1) and the second connecting 3.2.2 with either 2.1 using pathmin(3.2.2, 2.1) or 3.3 using pathmin(3.2.2, 3.3).\nAdditionally, we are interested in the sets containing all the LCA of each of the two sets.\nDefinition 4. Given a set of true classes (nodes) Y and a set of predicted classes (nodes) Ŷ , we define LCAall(Y, Ŷ ) as the set containing all LCA(y, Ŷ ) for all y ∈ Y . Similarly we define LCAall(Ŷ , Y ) as the set containing all LCA(ŷ, Y ) for all ŷ ∈ Ŷ . In the above example LCAall(Y, Ŷ ) ={3, 3.2.1}, LCAall(Ŷ , Y ) ={3, 3.2, 3.2.1}. Definition 5. Given a set of true classes (nodes) Y , a set of predicted classes (nodes) Ŷ and a set of LCAall, we define G ex t (Y, Ŷ ) as the graph that contains:\n• all pathsmin(y, a): y ∈ Y ∧ a ∈ LCA(y, Ŷ )\n• all pathsmin(y, a) subpaths of pathsmin(ŷ, y) : ŷ ∈ Ŷ ∧y ∈ Sbest(ŷ, Y )∧a ∈ LCA(ŷ, Y )\nSimilarly Gexp (Y, Ŷ ) is the graph that contains:\n• all pathsmin(ŷ, b): ŷ ∈ Ŷ ∧ b ∈ LCA(ŷ, Y )\n• all pathsmin(ŷ, b) subpaths of pathsmin(y, ŷ) : y ∈ Y ∧ ŷ ∈ Sbest(y, Ŷ )∧b ∈ LCA(y, Ŷ )\nFor example, for the Y and Ŷ of figure 8(b) we get the Gext (Y, Ŷ ) and G ex p (Y, Ŷ ) graphs of figure 9(a) and 9(b), respectively. Based on these graphs the true and predicted sets of classes are augmented, in order for the set-based measures to be calculated. In the case of Figure 9, Yaug = {3, 2.1, 3.2, 3.3, 3.2.1} and Ŷaug = {3, 3.1, 3.2, 3.2.1, 3.2.2}. The next step is to calculate cost measures based on these two sets which in our case are the following:\nPLCA = |Ŷaug ∩ Yaug|\n|Ŷaug |\nRLCA = |Ŷaug ∩ Yaug|\n|Yaug|\nFLCA = 2PLCARLCA PLCA +RLCA\nIn the example of Figure 9 all three measures, PLCA, RLCA and FLCA, between sets Yaug and Ŷaug, are 0.6. We prefer this approach over the symmetric difference loss, since it takes into account the TP in addition to FP and FN. Ignoring TP leads systems to prefer predicting fewer categories, since missing a single FP usually costs more than the gain of finding an extra TP. This behavior is also observed in the results of real systems, (see section 4) and is considered undesirable.\nThe two graphs Gext (Y, Ŷ ) and G ex p (Y, Ŷ ) were created using all nodes of LCAall(Y, Ŷ ) and LCAall(Ŷ , Y ) and all corresponding paths. However, subgraphs of the two graphs Gt(Y, Ŷ ) ⊆ Gext (Y, Ŷ ) and Gp(Y, Ŷ ) ⊆ Gexp (Y, Ŷ ), could be selected that would connect each node of Y ∪ Ŷ with an LCA and vice versa. For example, in Figure 8(b) node 3.2.2 has two LCAs, node 3.2 and 3. Node 3.2 could be removed from Gext (Y, Ŷ ) and G ex p (Y, Ŷ ), without breaking the condition of any node in Y ∪ Ŷ with an LCA or vice versa. We would then get graphs Gt(Y, Ŷ ) and Gp(Y, Ŷ ) of Figure 10. PLCA, RLCA and FLCA, between the reduced sets Yaug and Ŷaug of Figure 10, are 0.5 instead of 0.6 (Figure 9).\nIn other words graphs Gt(Y, Ŷ ) and Gp(Y, Ŷ ) should comprise the nodes necessary for connecting the nodes of the two sets, through their LCAs. Redundant nodes can lead to fluctuations in PLCA, RLCA and FLCA, and should be removed. In order to obtain the minimal LCA graphs, we have to solve the following maximization problem:\nProblem 2. Minimal LCA graph extension. \n                               \n                               \nargmax (Gt(Y,Ŷ )⊆G ex t (Y,Ŷ ), Gp(Y,Ŷ )⊆G ex p (Y,Ŷ )) FLCA(Gt(Y, Ŷ ), Gp(Y, Ŷ )) subject to:\n(i) ∀y ∈ Y ; y ∈ Gt(Y, Ŷ ) and ∀ŷ ∈ Ŷ ; ŷ ∈ Gp(Y, Ŷ ) (ii) ∀y ∈ Y ; ∃a : a ∈ Gt(Y, Ŷ ) ∧ a ∈ Gp(Y, Ŷ ) ∧ a ∈ LCA(y, Ŷ )\n∀ŷ ∈ Ŷ ; ∃b : b ∈ Gt(Y, Ŷ ) ∧ b ∈ Gp(Y, Ŷ ) ∧ b ∈ LCA(ŷ, Y ) (iii) 6 ∃G′t(Y, Ŷ ) subject to constraints (i) and (ii) :\n|{a : a ∈ Gt(Y, Ŷ ) ∧ a ∈ LCAall(Y, Ŷ )}| > |{a′ : a′ ∈ G′t(Y, Ŷ ) ∧ a′ ∈ LCAall(Ŷ , Y )}|\n6 ∃G′p(Y, Ŷ ) subject to constraints (i) and (ii) : |{b : b ∈ Gp(Y, Ŷ ) ∧ b ∈ LCAall(Y, Ŷ )}| > |{b′ : b′ ∈ G′p(Y, Ŷ ) ∧ b′ ∈ LCAall(Ŷ , Y )}|\n(iv) ∀y ∈ Y ; ∃py ∈ Gt(Y, Ŷ ) : py ∈ pathsmin(y, a) ∧ a ∈ LCAall(Y, Ŷ ) ∀ŷ ∈ Ŷ ; ∃pŷ ∈ Gp(Y, Ŷ ) : pŷ ∈ pathsmin(ŷ, b) ∧ b ∈ LCAall(Ŷ , Y ) (v) ∀a ∈ Gt(Y, Ŷ ) ∩ LCAall(Ŷ , Y ); ∃py ∈ Gt(Y, Ŷ ) : py ∈ pathsmin(y, a) ∧ y ∈ Y\n∀b ∈ Gp(Y, Ŷ ) ∩ LCAall(Y, Ŷ ); ∃pŷ ∈ Gp(Y, Ŷ ) : pŷ ∈ pathmin(ŷ, b) ∧ ŷ ∈ Ŷ\nThe maximization of FLCA(Gt(Y, Ŷ ), Gp(Y, Ŷ )) is subject to a set of constraints: Constraint (i) requires all class nodes of an initial set (Y or Ŷ ) to be included in the final subgraphs. Constraint (ii) enforces the existence of at\nleast one LCA for each node of Y ∪ Ŷ , in the subgraphs. Constraint (iii) limits the total number of LCAs used to the minimum required in order to be able to satisfy constraints (i) and (ii). Constraint (iv) implies the existence of at least one path connecting each class node of each subgraph to one of its LCAs, while constraint (v) implies the inverse, i.e. that each LCA of the subgraphs is connected with at least one class node of each subgraph.\nOne way to solve this maximization problem would be to create all possible Gt(Y, Ŷ ) and Gp(Y, Ŷ ) graphs, which respect the above constraints and choose the ones leading to the highest FLCA. This procedure is very computationally expensive and for this reason we devised the approximation presented in algorithm 1.\nAlgorithm 1 Approximation algorithm for computing Gt(Y, Ŷ ) and Gp(Y, Ŷ ), subgraphs of Gext (Y, Ŷ ) and G ex p (Y, Ŷ )\n1: procedure GetSubgraphs(Gext (Y, Ŷ ), G ex p (Y, Ŷ )) 2: LCAall ← getLCAsFrom(Gext (Y, Ŷ ), Gexp (Y, Ŷ )) 3: bestLCAs ← GetBestLCAs(LCAall, Gext (Y, Ŷ ), Gexp (Y, Ŷ )) 4: finalPaths ← GetBestPaths(bestLCAs,Gext (Y, Ŷ ), Gexp (Y, Ŷ )) 5: Gt(Y, Ŷ ) ← all paths from finalPaths containing a node ∈ Y 6: Gp(Y, Ŷ ) ← all paths from finalPaths containing a node ∈ Ŷ 7: end procedure 8: procedure GetBestLCAs(LCAs, Gt, Gp) 9: sortedLCAs ← sort(LCAs) ⊲ sort LCAs, by the number of Y and Ŷ that they connect, in descending order\n10: bestLCAs ← {}, i ← 1 11: repeat 12: bestLCAs ← bestLCAs ∪ sortedLCAsi 13: i ← i+ 1 14: until Satisfied(bestLCAs, Gt(Y, Ŷ ), Gp(Y, Ŷ )) ⊲ procedure\nSATISFIED checks whether constraints (i), (ii) and (iii) of the optimization problem are satisfied 15: for i ← 1 to sizeof(bestLCAs) do ⊲ top-down redundancy removal 16: if Satisfied(bestLCAs \\ bestLCAsi, Gt(Y, Ŷ ), Gp(Y, Ŷ )) then 17: bestLCAs ← bestLCAs \\ bestLCAsi 18: end if 19: end for 20: for i ← sizeof(bestLCAs) to 1 do ⊲ bottom-up redundancy removal 21: if Satisfied(bestLCAs \\ bestLCAsi, Gt(Y, Ŷ ), Gp(Y, Ŷ )) then 22: bestLCAs ← bestLCAs \\ bestLCAsi 23: end if 24: end for 25: return bestLCAs 26: end procedure\nAlgorithm 2 Part 2 of Algorithm 1\n27: procedure GetBestPaths(bestLCAs, Gt(Y, Ŷ ), Gp(Y, Ŷ )) 28: bestY ← {}, bestŶ ← {} 29: for i ← 1 to sizeof(bestLCAs) do 30: for j ← 1, sizeof(Y ) do ⊲ find nodes Yj , for which bestLCAi is an\nLCA 31: if bestLCAsi ∈ LCAs(Yj , Ŷ ) then 32: bestY ← bestY ∩ bestPath(bestLCAsi, Yj) ⊲ BestPath\nselects the path of pathmin(bestLCAi, Yj) that shares most common nodes with all other selected paths\n33: end if 34: end for 35: for j ← 1 to sizeof(Ŷ ) do 36: if bestLCAsi ∈ LCAs(Ŷj , Y ) then 37: bestŶ ← bestŶ ∩BestPath(bestLCAsi, Ŷj) 38: end if 39: end for 40: end for 41: return bestY ∪ bestŶ 42: end procedure\nThe main procedure is decomposed into three subprocedures. Procedure GetBestLCAs returns an approximation of the minimum amount of LCAs needed in order to satisfy constraints (i), (ii) and (iii) of the maximization problem. This is achieved by initially sorting, in descending order, all LCAs by the number of Y and Ŷ that they connect. On this list, we perform two passes, first top-down and then bottom-up removing all redundant LCAs, i.e LCAs of nodes for which other LCAs are already included in the list. In the final step of the algorithm, GetBestPaths selects the minimum paths that satisfy constraints (iv) and (v). In case two or more paths exist that connect the same node with an LCA we choose the one which leads to the smallest possible subgraphs.\nAn interesting issue arises when a class and one of its ancestors co-exist in the predicted or the true class sets. Assume for example a system A predicting that an instance belongs to node X , while another system B assigns it also to one of the ancestors of X . Each extra ancestor of X would lead to higher F1 score since it would increase the size of the Yaug∩Ŷaug set. This happens because all the ancestors of an LCA(n1, x2) are also ancestors of nodes n1 and n2. We address this issue by removing from set Y any node y for which ∃y′ ∈ Y : y′ is a descendant of y. We then do the same for set Ŷ by removing each node ŷ for which ∃ŷ′ ∈ Ŷ : ŷ′ is a descendant of ŷ.\nLCA precision, recall and F1 are not purely set-based measures, but are actually a bridge between pair-based and set-based measures. They are based on augmented sets of predicted and true nodes and calulate scores, based on the relation of those two sets. However the use of the LCA(n,S) leads to a pairing\nof predicted and true nodes. So these measures could also be characterized as a hybrid, combining the advantages of both types of measure."
    }, {
      "heading" : "3 Case Studies",
      "text" : "In this section we apply various measures to selected cases in order to demonstrate their pros and cons. As a representative of the pair-based measures we chose the Graph Induced Error (GIE), while for set-based ones we selected the hierarchical versions of precision (PH), recall (RH), F1-measure (FH =\n2·PH ·RH PH+RH )\nand Symmetric Difference Loss (l∆(Yaug , Ŷaug)), using all the ancestors of the predicted (Ŷ ) and true (Y ) labels in order to augment the sets of classes. We also use our proposed pair-based measure MGIA and the set-based LCA versions of precision (PLCA), recall (RLCA), F1-measure (FLCA) in order to illustrate their advantages and limitations, as well as the differences between the two types of measure. Regarding MGIA we also provide in parenthesis the fnerror, before the transformation that we propose in subsection 2.3.3, in order to be easier comparable with GIE. All the above measures are implemented in a fast and easy to use tool written in C++ that is open source and available for download.4\nLike all pair-based methods GIE and MGIA require a maximum distance threshold, above which nodes are paired with a default one. In the cases that we study here, this threshold is set to 5.\nBased on the situations presented in section 2.2, which highlight important challenges in hierarchical evaluation, the case studies here correspond to specific examples where these situations appear. The list of cases here is not exhaustive, but it is sufficient to motivate the use of the proposed measures. Additionally, in the Section 4 we present results on real datasets and classification systems."
    }, {
      "heading" : "3.1 Handling the Pairing Problem",
      "text" : "Case 1 captures the situation where the number of true T and predicted P labels (classes) differ, while being at the same level of the hierarchy. In this elementary case, Figures 11(a) and 11(b) are two symmetric variants leading to different, but symmetric hierarchical precision (PH) and recall (RH) scores, as shown in Table 1. The same is true for our proposed LCA versions of precision and recall (PLCA and RLCA). The results between the hierarchical version and the LCA versions differ because the LCA versions ignore the graph above node B which is the lowest common ancestor of nodes T 1, P1 and P2. The hierarchical versions on the other hand also take into account node A and in that way they give higher results. This behavior is undesirable, since each node above B would increase the results of the hierarchical measures but would not affect the LCA versions.\nAll other measures give the same result in both cases. l∆(Yaug, Ŷaug) always computes the symmetric difference between the two augmented sets. The sym-\n4The tool is available from http://nlp.cs.aueb.gr/software and datasets/HEMKit.zip\nmetric difference takes into account only the sum of false possitives and false negatives which are the same in both cases. Among the pair-based measures, GIE seems inappropriate for this problem. It matches T1 with one of P1 and P2 in Fig 11(a) and penalizes the unmatched predicted class with the maximum cost, ignoring the fact that the misclasification is in the proximity of the correct category. Similarly in Figure 11(b) MGIA provides a more suitable evaluation, as it allows multiple categories to match to the nearest one.\nCase 2 in Figure 12 is an example showing that taking into account all the ancestors is undesirable compared to our proposed LCA approach for set based measures. The hierarchy is still a tree and the classification is multilabel. TP is a node which was predicted correctly, while P1 is misclassified. Although the mistake in Figure 12(b) is worse than that of Figure 12(a), since it is further from the true class TP , all set-based measures except our proposed LCA measures continue to give the same results. This is because l∆(Yaug , Ŷaug) and the hierarchical versions of precision, recall and F1 take into account all the ancestors of the predicted and true labels, while the LCA versions, which are hybrid measures, uses the augmented graphs Gt(Y, Ŷ ) and Gp(Y, Ŷ ) which were created using only the least common ancestors (LCAs). LCA measures do some kind of pairing in order to pair each node with the closest node of the other set and in that way take into account the distance between predicted and true nodes, which the other set-based measure ignore.\nThus in Figure 12(a) the augmented sets of the LCA are {TP, D} and {TP, D, P1} while for all other set-based measures are {TP, D, B, A} and {TP, P1,\nD, B, A}. In Figure 12(b) the augmented sets of LCA become {TP, D, B} and {TP, B, P1} while for all other set-based measures they remain the same. The differentiation between such cases is an advantage of LCA-SDL over existing set-based measures."
    }, {
      "heading" : "3.2 Handling alternative paths",
      "text" : "In Figure 13(a) we assume a single-label classification task, where the hierarchy is a DAG. In this DAG there are two paths from the root (node A) to node P1. It is worth noting that this is the simplest case, where both paths {A, B, P1} and {A, C, P1} have the same length. In this case, pair-based measures (Table 3) remain unaffected compared to Figure 13(b) which is a tree, a desirable behavior that is due to the use of the shortest path from T1 to P1, while existing set-based measures are affected. In particular, they calculate a misclassification error that takes into account both of the two alternative paths to P1. On the other hand LCA measures behave like the pair-based measures, due to the use of the lowest common ancestor, having again an advantage over existing set-based measures."
    }, {
      "heading" : "3.3 Combining elementary cases",
      "text" : "In this subsection we study cases where the combination of the above mentioned problems leads to variable behavior of the set-based evaluation measures. Figure 14 presents such a case, where although GIE is affected by the matching problem discussed in case 1, the set-based methods give similar results to each other according to Table 4. The multiple paths phenomenon does not affect the hierarchical versions of precision, recall and F1 because all nodes above the lowest common ancestors B and A of T1, P1 and P2 are also shared by all classes (true and predicted). With this example we wish to show that there are certain cases in which existing set-based methods give the same results as the LCA ones, but we have not identified cases in which the behavior of an existing set-based measure would be more desirable than that of the LCA versions.\nThe case shown in Figure 15 differs from the previous case, since the right sub-graph now connects to the left one only through node A and also P1 con-\nnects to C through two different nodes D and E. This is the reason why now hierarchical versions of precision, recall and F1 differ from the LCA versions, as shown in table 5. LCA versions, due to its nearest common ancestor approach, ignores node D while all other set-based measures count both D and E and thus over-penalize the error of P1."
    }, {
      "heading" : "3.4 Multiple path counting",
      "text" : "Due to comparison of pairs of true and predicted classes, pair-based methods will often count the same path more than once. This multiple counting increases the error estimated by these methods. Figure 16, illustrates such a case. In Figure 16(a) the edge between T1 and B will be counted for the length of both {T1, B, P1} and {T1, B, A, C, D, P2} paths. As a result, pair-based measures in this case tend to overestimate the errors, in comparison to set-based ones. For each extra predicted node that we add as a descendant of B pair-based error estimates increase by at least 2 while the size of Yaug of set-based measures increase by 1 which seems more reasonable for such a change in the hierarchy.\nFigure 16(b) presents a similar example. According to Table 6, the error of MGIA before the proposed transformation is increased from 7 to 10, while l∆(Yaug , Ŷaug) increases from 5 to 6 and FLCA decreases from 0.44 to 0.25. This is because the whole path form T1 to D is counted twice by the pair-based method. However double counting seems desirable in this case, as the error in 16(b) is more severe than that in 16(a). While both measures penalize the error in 16(b) more than in 16(a), the extra penalization of MGIA is roughly proportional to the distance between T1 and P1, while that of the set based measures is not. All set based measures would give the same result even if P1 was a child of C, which is a less severe error than when it is a child of D. Therefore, counting more than once the common paths, may be an advantage of the pair-based measures in some cases."
    }, {
      "heading" : "3.5 Very distant predictions",
      "text" : "The aim of this case (Figure 17) is to show how each of the two types of measure handle very large distances between predicted and true labels. Pair-based measures compute the distance between each pair of predicted and true nodes and if this distance is above a certain threshold, a standard maximum distance is assigned. Set-based measures can use a threshold on the number of ancestors of the predicted and true nodes that will be used in the augmented sets. Using this threshold, we impose a common ancestor to be used in order to connect at least one predicted with one true node, at a distance equal to the threshold.\nFor example, in the case shown in Figure 17(a), if a maximum distance of 4 is used for pair-based measures, then for set-based measures it would be reasonable to request a lowest common ancestor at distance 2. By adding the distance of the lowest common ancestor to both the true and the predicted labels, a distance of 4 between them is reached.\nThis operation on the hierarchy of Figure 17(a) leads to the hierarchy of Figure 17(b), where an artificial node 0 was used in order to directly connect nodes B and E. The results of each measure are presented in Table 7. In\nthis example we see that all the measures can be run with a maximum distance threshold that might be necessary for computational reasons or due to these long distance problem discussed in section 2.2. Pair-based measures are affected more by the threshold than set-based ones, as shown in the example. l∆(Yaug , Ŷaug) decreased by 2 points, while MGIA decreased by 4. This is due to multiple counting of paths, which most of the times is undesirable as discussed in section 3.2."
    }, {
      "heading" : "3.6 Over and under-specialization",
      "text" : "In all the previous cases the predicted and the true categories were leaves of the hierarchies, but this is not always the case. Figure 18 presents simple examples of an inner node being either a true (Figure 18(a)) or a predicted (Figure 18(b)) category. As shown in Table 8 the two cases receive the same scores.\nFigure 18(a) shows a case of over-specialization. As described in section 2.2, different evaluation measures treat this type of error differently. One could even argue that since P1 is predicted, T1 is also predicted as a direct ancestor of it. This is not the case here and as shown in Table 8 all measures treat it as a misclassification error.\nRegarding under-specialization, the simplest example is shown in Figure 18(b). It is also considered an error that is more severe the further the true category is from the predicted. For example in Figure 18(c) the predicted node is an ancestor of the predicted node of Figure 18(b). All measures lead to a higher error estimate in this case than in 18(b). A similar example for overspecialization would lead to the same observations."
    }, {
      "heading" : "3.7 Summary",
      "text" : "Table 9 presents the advantages and disadvantages of each measure from the scope of the cases presented in Section 2.2. The pair-based measures can handle alternative paths, while, from the set-based measures, only the proposed LCA measures are able to deal with them efficiently. All measures can handle overspecialization and under-specialization in some way. All set-based measures can deal with the pairing problem since they produce augmented sets, while GIE cannot handle it efficiently and this is why MGIA was proposed. Considering the long distance problem only pair-based measures can handle it by definition, while set-based measures cannot handle it without using the threshold modification\nthat we proposes in Section 3.5. Finally, multi-path counting is a special feature of the pair-based measures which although most of the times is undesirable, it could make them behave better than the set-based measures in certain cases.\nAs a general conclusion the proposed measures always behave better or at least as well as the existing measures of their category. Therefore, if one wishes to use a pair-based or a set-based measure we suggest using the ones proposed in this paper, instead of the existing ones. Furthermore, in most cases one should choose the LCA measures over MGIA, due to the multiple counting of paths discussed in section 3.5. Multiple counting of paths is most of the times undesirable since it leads to over-penalization. Additionally, these cases could also serve as benchmarks, in order to observe the behaviour of newly proposed hierarchical evaluation measures. In this way, we conclude the discussion regarding the behavior of the measures in benchmark cases in order to observe them using real data and systems in the following section."
    }, {
      "heading" : "4 Empirical Study",
      "text" : "In this section we apply various evaluation measures to the predictions of the systems that participated in the Large Scale Hierarchical Text Classification Pascal Challenges of 2011 (LSHTC2) and 2012 (LSHTC3). The goal of this section is to study using real data and systems, the extent to which the performance ranking of systems is affected by the choice between flat and hierarchical evaluation measures and also by the type of hierarchical measure used. In the first subsection we present the datasets that we used, in the second subsection we discuss the evaluation measures included in the comparison and in the final subsection we discuss the results of the study.\nIn section 3 we demonstrated that, in certain cases, some measures behave more desirably than others. In this section we show that the differences among the methods also affect the rankings of real systems in practice."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "In LSHTC2 three different datasets were provided as three separate tasks. Each participant could participate in any or all of them with the same or with a different system. The first dataset (DMOZ) was based on pages crawled from the Open Directory Project (ODP), a human-edited hierarchical directory of the Web.5 The hierarchy of this dataset was transformed into a tree and all instances deeper than level five of the hierarchy were transferred to the fifth level, thus leading to a hierarchy with a maximum depth of 5. This dataset was the smallest of the three, regarding the number of categories and instances.\nThe other two datasets of LSHTC2, also used in LSHTC3, are based on DBpedia.6 They are called DBpedia Large and DBPedia Small, respectively. The largest of the two datasets, DBPedia Large, contains almost all abstracts of the DBpedia, as instances to be used for training and classification, with the exception of some non-English abstracts. Therefore this dataset comprises many more categories than the DMOZ one and goes to a larger depth. DBpedia Small is a subset of DBpedia Large, selected in a way that led to a dataset of similar size to the DMOZ, while maximizing the ratio of instances per node. This process has resulted in a much easier classification task.The hierarchy of the DBpedia Small dataset has been transformed into a DAG, by removing cycles, while cycles still appear in DBpedia Large.\nAll three datasets were pre-processed in the same way. All the words of the abstracts were stemmed and each stem was mapped to a feature id. The categories (classes) were also mapped to category ids. Each instance was represented in sparse vector format as a collection of category ids and a collection of feature ids accompanied by their frequencies in the instance. The mapping between ids, categories and stems was different for each dataset. Only leaves of each hierarchy were used as valid classification nodes for LSHTC2, while in LSHTC3 participants were also allowed to classify instances in inner nodes. For each inner node of the hierarchy that was assigned instances however, a dummy leaf was created for evaluation purposes as a direct child and all the instances were transferred to the child.\nTable 10 presents basic statistics of the three datasets. The first two datasets are almost of the same size, but DBpedia Small is more multi-labeled and has a deeper, less ballanced hierarchy than DMOZ. However the ratio of training instances to categories is comparable in the two datasets (14.16 for DMOZ and 12.5 for DBpedia Small). DBpedia Large is very different in this respect, having a ratio of training instances to categories equal to 7.2. Accounting for multilabelling, this ratio becomes similar in the two DBpedia datasets (23.27 for Small and 23.73 for Large) and much smaller for DMOZ (14.5). DBpedia Large is also much larger than the other two datasets in terms of training and testing instances.\n5http://www.dmoz.org/ 6http://dbpedia.org/About"
    }, {
      "heading" : "4.2 Evaluation Measures and Statistical tests",
      "text" : "The evaluation measures that were used in this study were the ones presented in Section 3. Accuracy and GIE are reproduced here as reported during the challenge. Using these evaluation measures, different rankings of the participating systems are created. In order to measure the correlation between these rankings, we used Kendall’s rank correlation (Kendall (1938)).\nIn the LSHTC2 challenge, statistical significance tests were only used for the flat evaluation measures. To the best of our knowledge, the literature does not provide special statistical significance tests for hierarchical measures. In this paper, as well as in LSHTC3, we performed a micro sign test (s-test) similar to that used in (Yang and Liu (1999)). Each of the hierarchical measures provides a score for each instance and this score is always averaged over the number of instances. Assuming that:\n• ai is the performance of system a for instance i, according to an evaluation measure,\n• bi is the performance of system b for instance i, according to the same evaluation measure,\n• n is the number of times that ai and bi differ over all i, • k is the number of times that ai performs better than bi over all i, the null hypothesis (H0) is that k has a bionomial distribution Bin(n, p), where p = 0.5. H1 is that p > 0.5, meaning that system a is better than system b. According to Yang and Liu (1999), if n is greater than 12, which is always the case in these large scale problems, then the p-value can be approximately computed using the standard normal distribution for:\nZ = k − 0.5n 0.5 √ n\n(6)\nIt is worth stressing that the s-test only takes into account which system performs better at each instance, ignoring how much better it performs. Alternatively, the Wilcoxon signed-rank test Wilcoxon (1945) could take into account the difference in performance at each instance. For reasons of simplicity however, in this paper we used the s-test."
    }, {
      "heading" : "4.3 Results",
      "text" : "In this subsection we present the results for each dataset and discuss the behavior of each measure. Table 11 presents the results on the DMOZ dataset for all the systems that participated in the LSHTC2 challenge. Recall that DMOZ has a tree hierarchy and is the least multi-labeled dataset of the three. Systems are evaluated by each measure and are ranked in descending order. The number in brackets indicates the system’s rank using the corresponding measure. If two systems have the same rank, it means that there is no statistically significant difference between their results according to our statistical significance tests. Table 12 presents the Kendall rank correlation between each pair of rankings.\nThe first observation is that the ranking of the flat accuracy is different from that of the other hierarchical measures. This shows that flat and hierarchical measures treat the problem differently. Another interesting observation is that the rankings also differ between hierarchical measures.\nThe handling of multiple labels per instance is an important aspect the classification methods. Table 13 presents the average number of predictions per instance for each system. Since most instances of the dataset are single-labeled, most of the participants treated the task as a single-label one. As discussed in previous sections the treatment of multi-labeling by different measures, greatly affects their behavior, but since multi-labeling is rare in this dataset, this decision did not affect much the hierarchical measures. However there are some examples of systems, such as M and J, which assign multiple labels and perform better according to hierarchical measures than according to accuracy. D and E, on the other hand, perform worse using some hierarchical measures than with accuracy. The more multi-labeled a result is, the greater the opportunity is for a hierarchical measure to reward or penalize the systems for its decision.\nAs discussed in previous sections, hierarchical measures vary in the way they handle multi-labeling and DAG hierarchies. Being a tree hierarchy and almost single-labeled, the DMOZ dataset does not reveal a lot of these differences. The tree hierarchy is the main reason why, according to Table 12, l∆(Yaug , Ŷaug) and FH are very highly correlated with FLCA, since their main difference is in the way they treat multiple ancestors, something possible only in DAGs and not in trees. FLCA and FH are more correlated with each other than with l∆(Yaug , Ŷaug), as expected, since they differ in the calculations they perform on the augmented sets. We also observe a correlation between GIE and MGIA, although the main reasons here are not the hierarchy, but the limited multilabeling that provides fewer opportunities for dealing with the pairing problem and the proposed tranformation of the error that our MGIA performs.\nTables 14 and 15 present the LSHTC2 results on DBpedia Small, which\nis a more multi-labeled dataset with a DAG hierarchy. As expected, these characteristics greatly affect the behavior of the measures. The most important observation is that the two hierarchical measures (GIE and l∆(Yaug, Ŷaug)) that measure only the error, without performing a transformation to it, have very low correlation with Acc and the other hierarchical measures. Furthermore, taking into consideration the average predictions per instance of each system (Table 16), we observe a relation between the rankings of these two measures. By computing only the error these two measures take into account only FP and FN, without counting the TP. For this reason they tend to penalize systems with higher average predictions per instance, since they are more likely to make more mistakes. The way the rest of the set based measures handle the augmented true and predicted sets of classes and the transformation that MGIA performs to the error is much closer to the idea of doing calculations with TP, FP, TN, FN, as accuracy does and for this reason these measures are more correlated with it. These measures also penalize less the systems that have a higher average\npredictions per instance, if they manage to have some extra TPs by doing so.\nTables 17 and 18 present the results on the same dataset (DBpedia Small), but with the systems of LSHTC3. The number of systems participating in LSHTC3 is much larger than LSHTC2 (17 instead of 7). This is not only important for statistical reasons (more experiments lead to safer conclusions), but also because according to Table 19 we now have more systems with higher average number of predictions per instance, something which affects the behavior of the measures. Another important difference is that in LSHTC3 systems were allowed to classify to inner nodes, even if these nodes did not have any training instance directly belonging to them.\nFH and FLCA are the hierarchical measure that are most correlated with flat accuracy, although the correlation is much lower in this case where we have many more systems and inner node classification is treated as a mistake by\naccuracy. The correlation between GIE and MGIA is much higher than that of LSHTC2, but they are not fully correlated. A high correlation also continues to be observed between GIE and l∆(Yaug, Ŷaug) for the reason explained previously in LSHTC2.\nA very interesting case is that of system X2, which predicts many categories (labels) per instance, 10.649 labels per instance, when the average true labels per instance is 1.8550. This means that a large number of predicted labels is wrong, while the predicted labels could still be in the vicinity of the correct ones. As expected, flat accuracy penalizes this behavior giving the lowest rank to this system. GIE and MGIA also penalize this system, although MGIA less severely. On the other hand, the set-based measures do not punish X2 that much (6 for FH , 4 for l∆(Yaug, Ŷaug) and 9 for FLCA. A closer look at the system shows that it is in fact not that bad. However, it returns all the nodes of a path from the root to leaf as predicted labels, instead of just the leaf. Set-based measures still penalize it when the leaf and its ancestors are wrong predictions, but they do not over-penalize it, unlike pair-based measures. This is a nice example, of the difference between set-based and pair-based measures. We can also observe that MGIA and FLCA are the less extreme measures of their categories to a point where the ranks are very close 10 and 9 respectively. This is because these measures can be seen as hybrid measures since the first also conducts a set operation (although it is a pair-based measure) and the second does some kind of matching between true and predicted nodes in order to create the augmented sets. These characteristics help them overcome the weaknesses of the measures of their respective categories and in that way their behavior is more desirable.\nOn the third dataset (DBpedia Large) we faced some computational issues with the hierarchical measures. The problem originated from the very large scale of the dataset’s hierarchy, which is a DAG (in reality it contains circles but we remove them). To avoid the computational problems, we run the evaluation measures with a maximum path threshold of 2 and 4. This means that all nodes are forced to have a lowest common ancestor at a depth of 1 and 2 respectively (if they do not have one we create a dummy one). Although this seems restrictive, it is very similar to the idea behind the Long Distance problem of Figure 2(e) discussed in section 2. In the Long Distance problem we used dummy nodes in order to link nodes that were further than a threshold from each other, in order to avoid overpenalization. The same dummy nodes are used here for computational reasons.\nTables 20 and 21 present the results for a distance threshold of 4 for the systems of LSHTC2. Since this dataset has the most complex hierarchy and\nit is the most multi-labeled one, it should be treated very differently by each measure. Interestingly the rankings of FH , MGIA and FLCA remain highly correlated with accuracy compared to the other measures, although the number of systems is not high enough (only 5 systems) in order to make safe conclusions.\nAnother interesting observation is the disagreement of GIE and MGIA about systems C3 and E3. As shown in Table 22, E3 predicts fewer categories per instance than C3. Since most of the times the predicted categories (labels) are fewer than the true ones and GIE over-penalizes all the unmatched true categories, it is natural for GIE to penalize system C3 more than E3. This problem is fixed by MGIA, which allows multi-pairing and this is why it instead ranks C3 as a better system than E3. We also notice that this difficult hierarchy affects the performance of l∆(Yaug , Ŷaug) and its ranks become less correlated with FLCA. A more interesting observation is that FH , MGIA and FLCA are completely correlated with each other and not correlated with the average number\nof predictions per instance (Table 22).\nTables 23 and 24 present the results for a maximum path threshold of 2. The main purpose of this experiment is to show that the measures remain largely unaffected by this parameter. Indeed most rankings do not seem to be affected compared to Tables 20 and 21. Nevertheless, general advice is to keep the maximum paths parameter as large as possible.\nTables 25 and 26 present the results on the same dataset (DBpedia Large), but with the systems of LSHTC3. The main difference is that in LSHTC3, systems were allowed to classify to inner nodes. Table 27 shows that the average number of predictions per instance is similar to that of LSHTC2. The most interesting observation is that while system F3 is ranked once again high according to Accuracy and all the other hierarchical measures except GIE and l∆(Yaug , Ŷaug) which rank it as one of the worst systems. It is even more interesting that, according to Table 27, system F3 provides the most labels per\ninstance. The assignement of many labels is penalized heavily by measures based only on FP and FN as we mentioned before.\nAnother interesting observation is that MGIA and FLCA are not fully correlated anymore. In fact MGIA is more correlated with FH than with FLCA. As the hierarchy becomes more complicated and the results more multi-labeled our two proposed measures behave more differently. Finally system G3, which predicts the smallest number of instances per document, is one ofthe best systems according to all measures except MGIA which ranks it as one of worst. This is because although G3 has the highest PH and PLCA, it also has a very low RH and RLCA compared to other systems. The computation of F1 seems more sutable in this case compared to the transformation that we proposed for MGIA, one extra reason why we propose FLCA over MGIA.\nSimilar experiments with a maximum path threshold of 2 were also conducted with systems of LHSTC3, but the results were similar to the ones of LSHTC2 and thus we omit them here, in the interest of space.\nThe experiments presented in this section illustrated, with the use of real systems and datasets, that hierarchical measures treat the competing systems differently than flat measures. This was shown by presenting the differences in the rankings of the systems across the three datasets. Flat evaluation measures, which are commonly used, often provide a false indication of which system performs better by ignoring hierarchical dependencies of classes and treating all errors equally. As a result, their use guides research away from the methods that incorporate the hierarchy in the classification process. We also showed that different variants of hierarchical measures give different rankings under different conditions. The goal was not to choose the best measure, but to show that different hierarchical evaluation measures give different results, not only in absolute values but also in the ranking of the systems. Finally we showed that the scale of the task is also an issue which requires attention."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this work we studied the problem of evaluating the performance of hierarchical classification methods. Specifically, this work abstracted and presented the key points of existing performance measures. We proposed a grouping of the methods into a) pair-based and b) set-based. Measures in the former group attempt to match each prediction to a true class and measure their distance. In contrast set-based measures use the hierarchical relations in order to augment the sets of predicted and true labels, and then use set operations, like symmetric difference and intersection, on the augmented label sets.\nIn order to model pair-based measures, we introduced a novel generic framework based on flow networks, while for set-based measures we provided a framework based on set operations. Thus, salient features of these measures are stressed and presented under a common formalism.\nAnother contribution of this paper was the proposal of two measures (one for each group) that address several deficiencies of existing measures. The proposed measures, along with existing ones were assessed in two ways. First, we applied them to selected cases, in order to demonstrate their pros and cons. Second, we studied them empirically on three large datasets based on DMOZ and Wikipedia with different characteristics (single-label, multi-label, tree and DAG hierarchies). The analysis of the results showed that the hierarchical measures behave differently, especially in cases of multi-label data and DAG hierarchies.\nAlso, the two proposed measures have shown a more robust behavior compared to their counterparts. Finally, the results supported our initial premise that flat measures are not adequate for evaluating hierarchical categorization systems.\nOur analysis showed that although in certain rare cases pair-based measures may behave more desirably, in most cases the set-based method proposed in this paper (FLCA) exhibits the most desirable behavior than that of our proposed pair-based measure (MGIA), since it is actually a hybrid measure because of the pairing way that the LCAs are selected. This why we propose the use of FLCA instead of all other hierarchical measures, although it is still an open-issue to propose a measure that combines all the pros of both our proposed MGIA and FLCA."
    } ],
    "references" : [ {
      "title" : "On finding lowest common ancestors in trees",
      "author" : [ "Alfred Aho", "John Hopcroft", "Jeffrey Ullman" ],
      "venue" : "In Proc. 5th ACM Symp. Theory of Computing (STOC),",
      "citeRegEx" : "Aho et al\\.,? \\Q1973\\E",
      "shortCiteRegEx" : "Aho et al\\.",
      "year" : 1973
    }, {
      "title" : "Network Flows: Theory, Algorithms, and Applications",
      "author" : [ "Ravindra K. Ahuja", "Thomas L. Magnanti", "James B. Orlin" ],
      "venue" : null,
      "citeRegEx" : "Ahuja et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Ahuja et al\\.",
      "year" : 1993
    }, {
      "title" : "Hierarchical multi-classification",
      "author" : [ "Hendrik Blockeel", "Maurice Bruynooghe", "Saso Dzeroski", "Jan Ramon", "Jan Struyf" ],
      "venue" : "In ACM SIGKDD 2002 Workshop on Multi-Relational Data Mining,",
      "citeRegEx" : "Blockeel et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Blockeel et al\\.",
      "year" : 2002
    }, {
      "title" : "An empirical comparison of flat and hierarchical performance measures for multi-label classification with hierarchy extraction",
      "author" : [ "Florian Brucker", "Fernando Benites", "Elena Sapozhnikova" ],
      "venue" : "In Proceedings of the 15th international conference on Knowledge-based and intelligent information and engineering systems - Volume Part I,",
      "citeRegEx" : "Brucker et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Brucker et al\\.",
      "year" : 2011
    }, {
      "title" : "Exploiting known taxonomies in learning overlapping concepts",
      "author" : [ "Lijuan Cai", "Thomas Hofmann" ],
      "venue" : "In International Joint Conferences on Artificial Intelligence,",
      "citeRegEx" : "Cai and Hofmann.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cai and Hofmann.",
      "year" : 2007
    }, {
      "title" : "Incremental algorithms for hierarchical classification",
      "author" : [ "Nicolò Cesa-Bianchi", "Claudio Gentile", "Luca Zaniboni" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2006
    }, {
      "title" : "A review of performance evaluation measures for hierarchical classifiers",
      "author" : [ "E.P. Costa", "A.C. Lorena", "Carvalho", "A.A. Freitas" ],
      "venue" : null,
      "citeRegEx" : "Costa et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Costa et al\\.",
      "year" : 2007
    }, {
      "title" : "Large margin hierarchical classification",
      "author" : [ "Ofer Dekel", "Joseph Keshet", "Yoram Singer" ],
      "venue" : "In Proceedings of the twenty-first international conference on Machine learning,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2004
    }, {
      "title" : "Hierarchical classification of g-protein-coupled receptors with a pso/aco algorithm",
      "author" : [ "N. Holden", "A.A. Freitas" ],
      "venue" : "In IEEE Swarm Intelligence Symposium (SIS-06),",
      "citeRegEx" : "Holden and Freitas.,? \\Q2006\\E",
      "shortCiteRegEx" : "Holden and Freitas.",
      "year" : 2006
    }, {
      "title" : "Probe, count, and classify: categorizing hidden web databases",
      "author" : [ "Panagiotis G. Ipeirotis", "Luis Gravano", "Mehran Sahami" ],
      "venue" : "In ACM SIGMOD international conference on Management of data,",
      "citeRegEx" : "Ipeirotis et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Ipeirotis et al\\.",
      "year" : 2001
    }, {
      "title" : "A new measure of rank correlation",
      "author" : [ "M.G. Kendall" ],
      "venue" : "In Biometrica,",
      "citeRegEx" : "Kendall.,? \\Q1938\\E",
      "shortCiteRegEx" : "Kendall.",
      "year" : 1938
    }, {
      "title" : "Functional annotation of genes using hierarchical text categorization",
      "author" : [ "Svetlana Kiritchenko", "Stan Matwin", "A. Fazel Famili" ],
      "venue" : "In ACL workshop on linking biological literature, ontologies and databases: mining biological semantics,",
      "citeRegEx" : "Kiritchenko et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kiritchenko et al\\.",
      "year" : 2005
    }, {
      "title" : "The ecir 2010 large scale hierarchical classification workshop",
      "author" : [ "Aris Kosmopoulos", "Eric Gaussier", "George Paliouras", "Sujeevan Aseervatham" ],
      "venue" : "SIGIR Forum,",
      "citeRegEx" : "Kosmopoulos et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kosmopoulos et al\\.",
      "year" : 2010
    }, {
      "title" : "Performance measures for multilabel evaluation: a case study in the area of image classification",
      "author" : [ "Stefanie Nowak", "Hanna Lukashevich", "Peter Dunker", "Stefan Rüger" ],
      "venue" : "In Proceedings of the international conference on Multimedia information retrieval,",
      "citeRegEx" : "Nowak et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nowak et al\\.",
      "year" : 2010
    }, {
      "title" : "A survey of hierarchical classification across different application domains",
      "author" : [ "Carlos N. Silla", "Jr.", "Alex A. Freitas" ],
      "venue" : "Data Mining Knowledge Discovery,",
      "citeRegEx" : "Silla et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Silla et al\\.",
      "year" : 2011
    }, {
      "title" : "A systematic analysis of performance measures for classification tasks",
      "author" : [ "Marina Sokolova", "Lapal Guy" ],
      "venue" : "Information Processing Management,",
      "citeRegEx" : "Sokolova and Guy.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sokolova and Guy.",
      "year" : 2009
    }, {
      "title" : "Hierarchical multi-classification with predictive clustering trees in functional genomics",
      "author" : [ "Jan Struyf", "Saso Dzeroski", "Hendrik Blockeel", "Amanda Clare" ],
      "venue" : "Progress in Artificial Intelligence,",
      "citeRegEx" : "Struyf et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Struyf et al\\.",
      "year" : 2005
    }, {
      "title" : "Hierarchical text classification and evaluation",
      "author" : [ "Aixin Sun", "Ee-Peng Lim" ],
      "venue" : "In IEEE International Conference on Data Mining,",
      "citeRegEx" : "Sun and Lim.,? \\Q2001\\E",
      "shortCiteRegEx" : "Sun and Lim.",
      "year" : 2001
    }, {
      "title" : "Performance measurement framework for hierarchical text classification",
      "author" : [ "Aixin Sun", "Ee-Peng Lim", "Wee-Keong Ng" ],
      "venue" : "Journal of the American Society for Information Science and Technology,",
      "citeRegEx" : "Sun et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2003
    }, {
      "title" : "Individual comparisons by ranking methods",
      "author" : [ "F. Wilcoxon" ],
      "venue" : "Biometrics Bulletin,",
      "citeRegEx" : "Wilcoxon.,? \\Q1945\\E",
      "shortCiteRegEx" : "Wilcoxon.",
      "year" : 1945
    }, {
      "title" : "A re-examination of text categorization methods",
      "author" : [ "Y. Yang", "X. Liu" ],
      "venue" : "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "Yang and Liu.,? \\Q1999\\E",
      "shortCiteRegEx" : "Yang and Liu.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Promising initial results on large-scale problems show that hierarchical classifiers can be effective in improving information retrieval (Kosmopoulos et al., 2010).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "Several evaluation measures have been proposed for hierarchical classification (HC) (Costa et al., 2007; Sokolova and Guy, 2009) using the hierarchy in different ways.",
      "startOffset" : 84,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "Several evaluation measures have been proposed for hierarchical classification (HC) (Costa et al., 2007; Sokolova and Guy, 2009) using the hierarchy in different ways.",
      "startOffset" : 84,
      "endOffset" : 128
    }, {
      "referenceID" : 18,
      "context" : "An early study can be found in (Sun et al., 2003), which is limited to a particular type of graph-distance measures.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "A review of HC measures is presented in (Costa et al., 2007), focusing on single-label tasks and without providing any empirical results; in multi-label tasks each object can be assigned to more than one classes, e.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "In (Nowak et al., 2010) many multi-label evaluation measures are compared, but the role of the hierarchy is not emphasized.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "Finally, Brucker et al. (2011) provide a comprehensive empirical analysis of HC performance measures, but they focus on the evaluation of clustering methods rather than classification ones.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "More elaborate cost measures may assign weights to the hierarchy’s edges, and the weights may decrease when moving from the top to the bottom (Blockeel et al., 2002; Holden and Freitas, 2006).",
      "startOffset" : 142,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "More elaborate cost measures may assign weights to the hierarchy’s edges, and the weights may decrease when moving from the top to the bottom (Blockeel et al., 2002; Holden and Freitas, 2006).",
      "startOffset" : 142,
      "endOffset" : 191
    }, {
      "referenceID" : 1,
      "context" : "In particular, we model it as a cost flow minimization problem (Ahuja et al., 1993).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, all standard algorithms for finding minimal cost flows guarantee to find this particular flow (Ahuja et al., 1993).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "In the simplest case of pair-based measures (Dekel et al., 2004; Holden and Freitas, 2006), the measure trivially pairs the single prediction with the single true label (M = N = 1), so that αp = βp = αt = βt = 1.",
      "startOffset" : 44,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "In the simplest case of pair-based measures (Dekel et al., 2004; Holden and Freitas, 2006), the measure trivially pairs the single prediction with the single true label (M = N = 1), so that αp = βp = αt = βt = 1.",
      "startOffset" : 44,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "In (Sun and Lim, 2001) two cost measures are proposed for multi-label problems in tree hierarchies, where all possible pairs of the predicted and true classes are used in the calculation.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "For we = 1, we get what Dekel et al. (2004) call tree induced error.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "In (Kiritchenko et al., 2005; Struyf et al., 2005; Cai and Hofmann, 2007) the ancestors of the predicted and true classes are added to Yaug and Ŷaug, as in Equations 2 and 3 above.",
      "startOffset" : 3,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "In (Kiritchenko et al., 2005; Struyf et al., 2005; Cai and Hofmann, 2007) the ancestors of the predicted and true classes are added to Yaug and Ŷaug, as in Equations 2 and 3 above.",
      "startOffset" : 3,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "In (Kiritchenko et al., 2005; Struyf et al., 2005; Cai and Hofmann, 2007) the ancestors of the predicted and true classes are added to Yaug and Ŷaug, as in Equations 2 and 3 above.",
      "startOffset" : 3,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : ", 2005; Cai and Hofmann, 2007) the ancestors of the predicted and true classes are added to Yaug and Ŷaug, as in Equations 2 and 3 above. Alternatively, in Ipeirotis et al. (2001) the descendants of the true and predicted classes are added: Yaug = Y ∪De(y1) ∪ .",
      "startOffset" : 8,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "In (Cesa-Bianchi et al., 2006), the approach that adds the ancestors is adopted (Equations 2 and 3) but the augmented sets are then altered as follows: Yaug ← Yaug \\ {",
      "startOffset" : 3,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "These measures use the concept of the lowest common ancestor (LCA) as defined in graph theory (Aho et al., 1973).",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "In order to measure the correlation between these rankings, we used Kendall’s rank correlation (Kendall (1938)).",
      "startOffset" : 68,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "In order to measure the correlation between these rankings, we used Kendall’s rank correlation (Kendall (1938)). In the LSHTC2 challenge, statistical significance tests were only used for the flat evaluation measures. To the best of our knowledge, the literature does not provide special statistical significance tests for hierarchical measures. In this paper, as well as in LSHTC3, we performed a micro sign test (s-test) similar to that used in (Yang and Liu (1999)).",
      "startOffset" : 68,
      "endOffset" : 468
    }, {
      "referenceID" : 10,
      "context" : "In order to measure the correlation between these rankings, we used Kendall’s rank correlation (Kendall (1938)). In the LSHTC2 challenge, statistical significance tests were only used for the flat evaluation measures. To the best of our knowledge, the literature does not provide special statistical significance tests for hierarchical measures. In this paper, as well as in LSHTC3, we performed a micro sign test (s-test) similar to that used in (Yang and Liu (1999)). Each of the hierarchical measures provides a score for each instance and this score is always averaged over the number of instances. Assuming that: • ai is the performance of system a for instance i, according to an evaluation measure, • bi is the performance of system b for instance i, according to the same evaluation measure, • n is the number of times that ai and bi differ over all i, • k is the number of times that ai performs better than bi over all i, the null hypothesis (H0) is that k has a bionomial distribution Bin(n, p), where p = 0.5. H1 is that p > 0.5, meaning that system a is better than system b. According to Yang and Liu (1999), if n is greater than 12, which is always the case in these large scale problems, then the p-value can be approximately computed using the standard normal distribution for: Z = k − 0.",
      "startOffset" : 68,
      "endOffset" : 1122
    }, {
      "referenceID" : 19,
      "context" : "Alternatively, the Wilcoxon signed-rank test Wilcoxon (1945) could take into account the difference in performance at each instance.",
      "startOffset" : 19,
      "endOffset" : 61
    } ],
    "year" : 2013,
    "abstractText" : "Hierarchical classification addresses the problem of classifying items into a hierarchy of classes. An important issue in hierarchical classification is the evaluation of different classification algorithms, which is complicated by the hierarchical relations among the classes. Several evaluation measures have been proposed for hierarchical classification using the hierarchy in different ways. This paper studies the problem of evaluation in hierarchical classification by analyzing and abstracting the key components of the existing performance measures. It also proposes two alternative generic views of hierarchical evaluation and introduces two corresponding novel measures. The proposed measures, along with the state-of-the-art ones, are empirically tested on three large datasets from the domain of text classification. The empirical results illustrate the undesirable behavior of existing approaches and how the proposed methods overcome most of these methods across a range of cases.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}