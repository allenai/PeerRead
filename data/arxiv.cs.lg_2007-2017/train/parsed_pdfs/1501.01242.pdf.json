{
  "name" : "1501.01242.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Online Relative Comparison Kernel Learning",
    "authors" : [ "Eric Heim", "Matthew Berger", "Lee M. Seversky", "Milos Hauskrecht" ],
    "emails" : [ "milos}@cs.pitt.edu", "lee.seversky}@us.af.mil" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords Online Learning, Kernel Learning, Relative Comparisons."
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning a similarity model over a set of objects from human feedback is important to many applications in collaborative filtering, document and multimedia retrieval, and visualization. It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30]. In this work we focus on learning a similarity model from human feedback through relative comparisons. More specifically, we focus on the relative comparison kernel learning (RCKL) problem, in which the goal is to learn a positive semidefinite (PSD) kernel matrix from relative comparisons given by humans. Kernels are used for modeling object relationships in many learning techniques [23], and hence are applicable to many methods that utilize kernels for these applications.\n∗University of Pittsburgh, Department. of Computer Science {eric, milos}@cs.pitt.edu †Air Force Research Laboratory, Information Directorate {matthew.berger.1, lee.seversky}@us.af.mil\nIn learning a kernel from human supervision, it is important to obtain feedback which is intuitive for the user to provide and informative for a learning algorithm to use. For instance, naive forms of supervision such as numerical judgments between pairs of objects have been shown to be very noisy [24]. A relative comparison, the response to a query of the form “Is object A more similar to object B or C?”, is well known as an intuitive mechanism for soliciting human feedback and an effective way of learning similarity [11]. Recent works addressing fine-grained categorization [27] and perceptual visualization design [6] have shown the practicality and benefit of learning kernels from relative comparisons.\nMany RCKL methods [1, 26] learn a kernel by solving a semidefinite program (SDP) in batch, where all obtained relative comparisons are required to learn the kernel. However, in many practical applications, a batch approach is not appropriate due to the online and dynamic nature of the application. For example in crowdsourcing, it is often of interest to minimize the number of dispatched tasks and thus the cost of the crowd by leveraging active learning techniques [25, 9] to adaptively select the most informative relative comparison query. The success of these techniques depends on maintaining an up to date model so as to ensure the most informative query is selected, as well as an efficient learning method to quickly update the model so that no crowd participant is idle. Likewise, recommendation systems for online marketplaces obtain continuous feedback in the form of click-through data via user interaction. In order for the learned kernel to be up to date and reflect the latest user feedback, the learning method must be able to quickly incorporate feedback as it is received.\nThese scenarios motivate the need for an efficient and online method for learning from large-scale relative comparison data. Batch methods poorly scale for large object collections primarily because they must ensure their solutions are PSD. Without any prior assumptions on the data this operation is of O(n3) time complexity for n objects, which for large n is prohibitively slow for the aforementioned applications.\nThis work introduces a novel online RCKL framework called Efficient online Relative comparison Kernel LEarning (ERKLE) that sequentially updates a kernel one query response at a time in O(n2) complexity. ERKLE employs stochastic gradient descent [3] for RCKL, taking advantage of the sparse and low-rank structure of the RCKL gradient over a single comparison to devise fast updates that only require finding the smallest eigenvector and eigenvalue of a\nar X\niv :1\n50 1.\n01 24\n2v 2\n[ cs\n.L G\n] 1\n2 Ja\nn 20\n15\nsuitable matrix. We show that the gradient structure, which enables such an efficient update, generalizes several wellknown convex RCKL methods [1, 26]. The structure of the gradient also reveals a simple way to bound the smallest eigenvalue after each gradient step, which often allows updates to be performed in constant time. Motivated by work in online learning [5], we also derive a passive-aggressive version of ERKLE to ensure learned kernels model the most recently obtained relative comparisons without over-fitting. In summary, our main contributions are:\n1. An online RCKL framework for large-scale similarity learning that generalizes many current RCKL methods. 2. An efficient kernel update method with O(n2) time complexity that exploits the unique structure of RCKL stochastic gradients when stochastic gradient steps may result in a non-PSD matrix. 3. A passive-aggressive update procedure for online relative comparison kernel learning 4. An experimental evaluation that shows ERKLE has both improved performance and faster run times compared to batch RCKL methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "The problem of learning a kernel matrix, driven by relative comparison feedback, has been the focus of much recent work. Most recent techniques primarily differ by the choice of loss function. For instance, Generalized Non-metric Multidimensional Scaling [1] employs hinge loss, Crowd Kernel Learning [25] uses a scale-invariant loss, and Stochastic Triplet Embedding [26] uses a logistic loss function.\nThe aforementioned RCKL methods can be viewed as solving a kernelized special case of the classic non-metric multidimensional scaling problem [14], where the goal is to find an embedding of objects in Rd such that they satisfy given Euclidean distance constraints. In contrast to many of the kernel-learning formulations, their analogous embeddinglearning counterparts are non-convex optimization problems, which only guarantee convergence to a local minimum. In the typical non-convex batch setting, multiple solutions are found with different initializations and the best is chosen among them. This strategy is poorly suited for the online setting where triplets are being observed sequentially, and which solution is best may change as feedback is received.\nIn this work we consider the online RCKL problem, where one is sequentially acquiring relative comparisons among a large collection of objects. Stochastic gradient descent techniques [21] are a popular class of methods for online learning of high-dimensional data for a very general class of functions, where recent techniques [29, 22] have demonstrated competitive performance with batch techniques. In particular, recent methods [8, 16] have developed efficient methods to solve SDPs in an online fashion. The work of [4]\nshows how to devise efficient update schemes for solving SDPs when the gradient of the objective function is low-rank. We build upon and improve the efficiency of this work, by taking advantage of the sparse and low-rank structure of the gradient common in convex RCKL formulations.\nOur passive-aggressive step size procedure is similar to that which is introduced in [5] for other online learning problems. In their work, the authors create a passiveaggressive online update rule for classic SVM formulations used in problems such as binary/multi-class classification and regression. In deriving such an update for different RCKL loss functions, we relate how different methods can be utilized under a common passive-aggressive framework. To our knowledge, such an update for RCKL problems and the associated analysis of RCKL methods has not been done."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "In this section, we formally define RCKL and provide a brief overview of RCKL methods. Let Sn+ be the set of n × n PSD matrices, and Mab be the entry at row a column b of a matrix M. The goal of RCKL is to learn a PSD kernel matrix K ∈ Sn+ over n objects, given a set T of triplets:\n(3.1) T = {(a, b, c) | a is more similar to b than c}\nsuch that squared distance constraints are satisfied:\n(3.2) ∀(a,b,c)∈T : d2K(a, b) < d2K(a, c)\nwhere d2K(a, b) = K aa + Kbb − 2Kab.\nWe say a kernel K satisfies a triplet ti = (ai, bi, ci) ∈ T if the constraint in (3.2) corresponding to ti is satisfied.\nIn this work, we consider triplets that are answers to relative comparison queries posed to one or more people. We define a query q to have three components, a “head” object h to be compared with two objects o1 and o2. A query q = ( h, { o1, o2 }) can be answered by either the triplet(\nh, o1, o2 ) or ( h, o2, o1 ) , indicating that h is more similar to o1 than o2 or h is more similar to o2 than o1, respectively. It is desirable to learn a kernel that not only satisfies observed triplets, but also that generalizes to unseen triplets, leading to a learned kernel that models a more complete notion of the desired human similarity space.\n3.1 RCKL Formulation Many RCKL methods can be generalized by the following SDP:\n(3.3) min K L (K, T ) + τTrace(K) s.t. K 0.\nThe objective function is composed of two terms. The first term is a function L measuring how much loss K incurs for not satisfying triplets in T . The second term is a trace regularization on K weighted by a hyperparameter τ . Trace\nregularization is used as a convex approximation of the nonconvex rank function. Higher values of τ enforce that (3.3) produces lower-complexity similarity models. Finally, K is constrained to be PSD.\nThe loss function in the objective can be decomposed into the sum of losses over individual triplets:\n(3.4) L (K, T ) = ∑ t∈T l (K, t) .\nExisting RCKL methods differ in the choice of the loss function l. The Stochastic Triplet Embedding (STE) approach of [26] defines l (K, t) = − log pKt as the loss function, where pKt is the probability that a triplet is satisfied:\n(3.5) pKt=(a,b,c) = exp(−d2K(a, b))\nexp(−d2K(a, b)) + exp(−d2K(a, c)) .\nGeneralized Nonmetric Multidimensional Scaling (GNMDS) [1] uses a hinge loss, where l (K, t = (a, b, c)) is defined as:\n(3.6) max(0, d2K(a, b)− d2K(a, c) + 1).\nFor either loss function l, (3.3) is a convex optimization problem and the globally optimal solution is found by performing projected gradient descent, which consists of two update steps. The first step is a simple descending step along the gradient of the objective:\n(3.7) K′i = Ki−1 − δi (∇L (Ki−1, T ) + τI) ,\nwhere i denotes the current iteration, δi is the learning rate. The second step projects the result of the first gradient step onto the PSD cone:\n(3.8) Ki = ΠS+ (K ′ i) .\nThese steps are iterated until convergence."
    }, {
      "heading" : "4 Efficient Online Relative Comparison Kernel Learning (ERKLE)",
      "text" : "The main computational bottleneck of traditional RCKL methods is the projection onto the PSD cone, ΠS+ . This projection is commonly found by first taking the eigendecomposition of K′i = VΛV\nT and setting all negative eigenvalues to 0, i.e. Ki = V[Λ]+Vt, where [·]+ is defined entry-wise as [Λii]+ = max(0,Λ\nii). Absent of any prior knowledge on the structure of K′i, its full eigendecomposition is necessary for the projection. Since this is an O(n3) operation, the projection step renders batch methods computationally prohibitive for learning the similarity of a large number of objects in an online manner.\n4.1 Stochastic Gradient Step To create an efficient and online framework for RCKL – ERKLE – we leverage\nstochastic gradient descent techniques [3]. As shown in (3.4), the loss function L naturally decomposes into the sum over losses l defined on individual observations (triplets in our case). From this decomposition, ERKLE first performs the following stochastic gradient step:\n(4.9) K′j ← Kj−1 − δj∇l (Kj−1, tj) ,\nwhere triplets t1, ..., tj−1 have been observed, Kj−1 is the online solution after observing the j − 1 triplet,\nPerforming a stochastic optimization gives ERKLE an advantage over current RCKL methods that perform batch optimizations. Batch methods attempt to minimize a loss function over a training set. This is known to minimize empirical risk with respect to the particular training samples, which is used as an estimate of expected risk over the ground truth distribution over all samples. Obtaining triplets in an online fashion from a source can be viewed as sampling triplets from a ground truth distribution at random. As such, taking stochastic steps over samples directly minimizes expected risk with respect to the ground truth distribution of triplets, not empirical risk with respect to the training instances. The practical impact of this characteristic is that stochastic methods tend to generalize better to unobserved samples. For more discussion on this characteristic of stochastic methods see [3].\nNote that our online formulation does not include trace regularization. Although this may impact our method in generalizing to unseen triplets, our online formulation achieves good generalization through carefully constructed, data-dependent step sizes δj , as detailed in Section 4.3.\n4.2 Efficient Projection In order to retain positive semidefiniteness, after taking a stochastic gradient step the resulting matrix K′j must be projected onto the PSD cone. Following the procedure of ΠS+ is prohibitively expensive for our online setting. Instead, for RCKL methods we can take advantage of the sparse and low-rank nature of the gradient to devise an efficient projection scheme. To this end, we introduce a canonical gradient matrix G over a triplet t = (a, b, c)), where the entries are defined as:\n(4.10) Gij =  −2 if i = a, j = b or i = b, j = a 2 if i = a, j = c or i = c, j = a 1 if i = b, j = b −1 if i = c, j = c 0 otherwise.\nNow consider the following choice for the stochastic step:\n(4.11) ∇l (K, t) = f (K, t) G,\nwhere f is a real-valued function defined below. With (4.11) as the gradient in (4.9), Kj−1 is updated by increasing entries\ncorresponding to the similarity between objects a and b and decreasing the similarity between a and c by a factor of f(Kj−1, tj).\nThe function f can be defined such that we recover the gradients of l for different convex RCKL formulations. The stochastic gradient for STE can be obtained by defining f as:\n(4.12) f (K, t) = 1− pKt\nSimilarly by defining f to be: (4.13) f (K, t) = { 1 if d2K(a, b) + 1 < d 2 K(a, c)\n0 otherwise\nthe stochastic gradient for GNMDS is obtained. Note, that this not only generalizes these two methods for use in our online framework but also suggests a simple way to create new online RCKL methods by designing a function f that weighs the contribution of individual triplets.\nDecomposing the online updates in such a way reveals a key insight into how to perform efficient projections onto the PSD cone after the stochastic step. Algorithm 1 outlines the procedure for efficient projection in ERKLE. Here, λ↓ and v↓ are the smallest eigenvalue and eigenvector of matrix K, respectively. This procedure has a time complexity O(n2) due to finding λ↓ and v↓. To show that Algorithm 1 does indeed perform a projection onto the PSD cone, we prove the following theorem:\nTHEOREM 4.1. Algorithm 1 results in a PSD matrix Kj that is closest to K′j in terms of Frobenius distance.\nProof. Let K0 ∈ Sn+ (i.e. identity). We use this as our base case and show inductively that after each iteration of the main loop, Kj remains PSD. Let γj = δjf (Kj−1, tj) be the magnitude of an update. By (4.11), the update in Equation (4.9) can be written as Kj−1 − γjG. The only nonzero eigenvalues of −γjG are λ1 = 3γj and λ2 = −3γj . It follows from Weyl’s inequality that the matrix K′j = Kj−1 − γjG has at most one negative eigenvalue. If K′j has no negative eigenvalues, then it is PSD (line 6 of Algorithm (1)). If K′j has one negative eigenvalue, line 4 of Algorithm 1 results in a PSD matrix Kj that is closest to K′j in terms of Frobenius distance by Case 2 of Theorem 4 in [4].\nThe important implication of Thm. 4.1 is that ERKLE can incorporate a triplet into a kernel in O(n2) time by performing the efficient projection outlined in Algorithm 1. Furthermore, if a step is sufficiently small, then no projection is needed at all. Let λ0j be the smallest eigenvalue of Kj . By Weyl’s inequality, if λ0j − 3γj ≥ 0, then all eigenvalues of K′j+1 are greater than or equal to 0. This can be used to skip the projection step when the update is known to result in a PSD matrix. In our algorithm, we lower bound the smallest eigenvalue by maintaining a conservative estimate\nAlgorithm 1 Efficient PSD Projection 1: procedure Π1+(K) 2: Find λ↓ and v↓ from K 3: if λ↓ < 0 then 4: return K− λ↓v↓vT↓ 5: else 6: return K 7: end if 8: end procedure\nλ̂0j . Initially, λ̂ 0 0 ← λ00. It is updated each iteration with it’s lower bound λ̂0j ← λ̂0j−1 − 3γj . If λ̂0j < 0, then Alg. 1 is used to project onto the PSD cone and λ̂0j ← max (0, λ↓). Otherwise, no projection is performed. In the case where λ00 >> −3γj , this simple lower-bounding procedure can save many eigenvalue/eigenvector computations until a projection may be necessary.\n4.3 Passive-Aggressive Updates A key difference between the batch and stochastic RCKL updates is the magnitude of the updates. For both methods the magnitude of the updates with respect to a single triplet t is a function of a learning rate and how well the previous solution satisfies t. In the previous section we denoted the magnitude of an ERKLE update as γj . In the batch setting, the same learning rate δi is used for all triplets in a given step. In contrast, stochastic methods typically use different learning rates δj for different triplets tj , which can result in faster convergence rates. To take advantage of faster convergence, the learning rates must satisfy certain conditions. Early work [3] on the topic of learning rates suggest that δj should satisfy two constraints:∑∞ j=1 δ 2 j < ∞ and ∑∞ j=1 δj = ∞. For example δj = 1/j satisfies these constraints. Later work [18] suggests a more aggressive setting of δj = 1/ √ j.\nHowever, in the online setting there is no reason to believe that a triplet should have less influence on the kernel than those obtained before it. On the other hand, we do not wish to over-fit to the most recently obtained triplets. It is this observation that motivates Passive-Aggressive (PA) Online Learning [5]. In the RCKL setting, the general idea is that if the previous solution Kj−1 satisfies a newly obtained triplet tj = (a, b, c) by a margin of 1, then do not update the kernel (passive). Otherwise, update the kernel so that the kernel is changed the minimal amount, but tj is satisfied by a margin of 1 (aggressive). A fortunate side effect of choosing minimally sized updates is that updates are less likely to result in nonPSD matrices than larger steps, thus potentially reducing the number of projections onto the PSD cone via our conservative eigenvalue estimate (Section 4.2).\nTo derive a passive-aggressive update for ERKLE, we wish to learn a magnitude of a stochastic step γj = δjf(Kj−1, tj) with passive-aggressive properties. f as de-\nfined by GNMDS in (4.13) is inherently passive, but if Kj−1 does not satisfy the margin constraint, it takes a step independent of how close the previous solution is to satisfying tj . As such, we wish to find a δj that takes an aggressive step. We do this by solving the following optimization problem:\n(4.14) min δj\nδ2j\ns.t. d2K′j (a, b) + 1 ≤ d2K′j (a, c), δj ≥ 0\nBy (4.11) and (4.13), the first constraint can be rewritten as:\n(4.15) d2Kj−1(a, b)− d 2 Kj−1(a, c)− 10δj + 1 ≤ 0\nWith the assumption that the triplet is not satisfied by a margin of one in Kj−1, no update is required; otherwise, only a positive value of δj can satisfy (4.15), making the positive constraint on δj redundant. Also, the smallest δj that satisfies (4.15) is the one that makes the left hand side exactly zero. As a result, the inequality constraint can be handled as equality. To find the optimum we first write the Lagrangian L (δj , α):\n(4.16) δ2j + α ( d2Kj−1(a, b)− d 2 Kj−1(a, c)− 10δj + 1 ) Taking the partial derivative of (4.16) with respect to δj , setting it to 0, and solving for δj results in δj = 5α. Substituting this back into (4.16) makes the Lagrangian:\n(4.17) − 25α2 + α ( d2Kj−1(a, b)− d 2 Kj−1(a, c) + 1 ) Taking the partial derivative of (4.17) with respect to α, setting it to 0, solving for α and then substituting this back into δj = 5α results in the minimum step size that satisfies the margin constraint:\n(4.18) δj = d2Kj−1(a, b)− d 2 Kj−1 (a, c) + 1\n10\nA similar passive-aggressive update can be derived using the probability of a triplet being satisfied in STE. Consider the following optimization:\n(4.19) min δj\nδ2j\ns.t. p K′j tj ≥ P, δj ≥ 0\nIn (4.19) the minimal step size is chosen such that the probability that a triplet is satisfied after the update is greater than or equal to a given probability P ∈ (0.5, 1). Using (4.19), we derive the following step size:\n(4.20) δj = d2Kj−1(a, b)− d 2 Kj−1 (a, c) + κ\n10\nwhere κ = log (P )−log (1− P ). The full derivation is given in Sec. A. Both derivations reveal that passive-aggressive\nupdates using STE and GNMDS are very similar. Setting P = e1+e in (4.20) recovers the GNMDS passive-aggressive step in (4.18), and changing the margin in (4.18) recovers different settings of P .\nNote that using (4.18) as a step size results in a K′j with the intended passive-aggressive property, not necessarily the kernel Kj after the projection. We choose to find a passiveaggressive step size instead of a full update for computational efficiency Finding a true passive-aggressive step size with respect to Kj would require iteratively projecting onto the PSD cone, which is computationally prohibitive in the online setting. In practice, d2K′j is a good approximation to d 2 Kj\n, as their difference is dependent on the magnitude of the (potentially) negative eigenvalue of K′j , which tends to be quite small.\nEven for a proper setting of δj , it has been shown that stochastic methods perform best when multiple rounds of updates or passes are performed on the observed samples [2, 20, 28]. For our problem setting, this indicates that ERKLE may benefit from revisiting triplets that were previously used to update the kernel. In our experiments we perform a simple multi-pass scheme where for each new triplet, ERKLE not only steps over the most recently obtained triplet, but also a number of randomly sampled triplets from the set of previously obtained triplets. We denote the number of “passes” ERKLE performs each time a new triplet is observed as β. Algorithm 2 in Sec. B describes this process in more detail. This simple approach is sufficient for maintaining high accuracy while still ensuring computational efficiency for the online setting."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we evaluate ERKLE by comparing it to batch RCKL methods. Batch methods are not truly applicable to the online learning setting, but can be applied in what is often called “mini-batches”. In the mini-batch learning setting, every time a new batch of m triplets are received, batch RCKL is run on all obtained triplets so far. Thus, we compare ERKLE to running their batch counterparts in mini-batches.\nWe evaluate each method on four different data sets, each with its own challenges. First, we start with a small-scale synthetic experiment to evaluate how the methods perform in an idealized setting. Second, a large-scale synthetic experiment is run to show how ERKLE and batch compare in terms of practical run time. Third, a data set of triplets over popular music artists is used to evaluate how the methods perform in a real-world setting with moderate triplet noise. Finally, ERKLE and batch RCKL are evaluated on a data set of triplets over scene images, which consist of a small number of triplets, thus focusing on the performance of these methods with very little feedback.\nFor these experiments, we wish to see how the learned kernels generalize to held out triplets as triplets are obtained.\nThis is important in real-world applications where the goal is to accurately model all the relationships among objects, not just the observed ones. Because of this, one of our main evaluation metrics is normalized test error, which is defined as the total number of unsatisfied test triplets by a learned kernel divided by the total number of test triplets.\nUnless otherwise noted, the experiments were run with the following specifications. Each method started with an initial kernel set to identity in order to give no method an advantage (all methods initially satisfy no triplets). All batch methods were terminated after a maximum of 1000 iterations or when the change in objective between iterations was less than 10−7. We denote the batch methods with the suffix “-Batch” (e.g. STE-Batch) and the ERKLE variants with “- ERKLE” (e.g. STE-ERKLE). We denote passive-aggressive ERKLE as PA-ERKLE, and use the step size that satisfies the margin by 1 as in (4.18). The mini-batch size is 100, and all methods are evaluated every 100 observed triplets.\nWe used the batch STE, GNMDS, and CKL (Crowd Kernel Learning [25]) MATLAB implementations specified by [26] in which the eig MATLAB function is used to find the all eigenvalues and eigenvectors for projection onto the PSD cone. ERKLE was also implemented in MATLAB, where the eigs function is used to find a single eigenvalue/eigenvector pair with the smallest eigenvalue for the projections. The τ hyperparameter was chosen to be the best performing setting over ten varying options. The timed experiments were performed on an Intel Core i5-4670K CPU @ 3.4 GHz with 16 GB of RAM and the single thread option enabled. Each experiment was performed with ten trials, each with different, randomly chosen test, train and validation sets. The error bars in the graphs represent the 95% confidence interval.\n5.1 Small-Scale Synthetic Data Our first experiment is to test each method on an ideal, small-scale, synthetic data set. We created the synthetic data set by first generating 100 data points (n = 100) in R50 from N (0, 1). Using the distances\nbetween points, we answered all possible relative comparison queries which resulted in 485100 triplets. 10000 triplets were used as the train set and the rest were used as the test set.\nDiscussion: Figure 1a shows the effect that the learning rate parameter δj has on the performance of ERKLE as more triplets are observed in an online fashion. For a setting of 1/j, the learning rate decays too rapidly to improve performance significantly after j = 3000. The learning rate 1/ √ j performs better, but still levels off, quicker than the final two methods. The last two methods have learning rates that are independent of the number of observed triplets. STEERKLE with a constant learning rate and PA-ERKLE take steps solely based on how well the current solution satisfies the observed triplet, and vastly outperform the alternative learning rates based on number of observations. This result indicates that reducing the influence of a triplet because it was observed later has an adverse effect on the ability of a learned kernel to generalize to unobserved triplets.\nFigure 1b shows the performance of STE-ERKLE (with δj set to 1), and PA-ERKLE compared to three batch RCKL methods. The τ hyperparameter was chosen by selecting the best setting over choices as evaluated on the test set. With a single pass over the data (β = 1), both ERKLE methods outperformed all batch methods slightly. With ten passes over the data, the ERKLE methods outperformed the batch methods by a large margin. In addition, the batch methods level off more quickly than the ERKLE methods, indicating that if more triplets were obtained, the ERKLE methods would further outperform even the batch methods. We believe that these results show that by minimizing the expected risk directly, ERKLE is able to learn a more general kernel than batch methods that minimize empirical risk.\nFigure 1c shows the performance of two ERKLE methods and two batch RCKL methods as a function of how many effective “passes” each method performed on the data. For ERKLE, this amounts to the setting of the β parameter. For the batch RCKL methods, this is the number of full gradient\nsteps it takes. Each method was run over all training triplets with the step size δj validated on the test set for the batch methods. This effectively measures training cost as a function of passes through the data, and thus, is independent of implementation. Clearly, if only few passes through the data can be performed, then ERKLE is the better choice.\n5.2 Large-Scale Synthetic Data Next, we evaluated how PA-ERKLE compared to batch GNMDS in terms of practical run time on a large scale experiment. For this experiment, we generated 5000 data points in the same manner in which the small-scale synthetic data was generated. 10000 randomly generated triplets were used as the train set and 50000 were used as the test set. The batch methods were run in mini-batches of 500 triplets due to time constraints. The hyperparameter τ and the step size δi were chosen as the settings that best performed on the test set. This experiment was run over 5 trials, each with a different train and test set.\nDiscussion: Figure 2a shows the cumulative run time of one pass of PA-ERKLE, and 1 and 2 steps of batch GNMDS. The times shown for the batch methods are for the best chosen τ and not for the total time it took to find it. The figure shows that a single pass of PA-ERKLE is often significantly faster than a single gradient step of batch GNMDS. Two steps of GNMDS takes even longer. ERKLE can perform online updates much faster due to the efficient projection procedure as well as the ability to skip certain projections by estimating the lower bound. In this experiment, the mean number of eigenvalue/eigenvector computations over the 5 trials was 724.2 with a standard deviation of 3.7. Hence PA-ERKLE was able to skip the projection step roughly 93% of the time.\nFigure 2b depicts the test errors of each method. Initially, the batch methods perform better, but around 2500 triplets, PA-ERKLE outperforms the batch methods. This experiment indicates that PA-ERKLE can achieve competitive results with batch methods in a single pass over the data, and produce truly online solutions instead of mini-batch solutions while\nhaving faster run time.\n5.3 Music Artist Similarity For the last two experiments we performed evaluations on real-world data sets. First, we performed an experiment using relative comparisons among popular music artists gathered from a web survey. The aset400 data set [7] contains 16,385 relative comparisons over 412 artists. We randomly chose 10000 triplets as the train set, 1000 as the validation set for the τ parameter, and the rest were used as the test set. The aset400 data set presents a challenge not present in the synthetic data: It has a moderate amount of conflicting triplets, thus methods used in the evaluation must deal with noise within the triplets.\nDiscussion: Figure 3a shows how ERKLE and batch RCKL methods generalize to the test set. STE-ERKLE performs considerably worse than the other methods, most likely due to the noise in the observed triplets. The probability pKt used in STE-ERKLE decays rapidly. Thus, triplets that are in agreement with previously obtained triplets do not influence the learned kernel greatly. However, a conflicting triplet will make STE-ERKLE perform a relatively more drastic update. PA-ERKLE, however, is much more robust to noise due to the minimal step size taken to satisfy a triplet. Because of this, PA-ERKLE performs as well as the batch methods and often better when multiple passes are taken.\nFigure 3b shows the training errors of each method. This figure highlights how well each method fits to the observed triplets. The STE-ERKLE models are greatly effected by the presence of conflicts in that they do not learn a kernel that fits to a large number of the observed triplets. PA-ERKLE, on the other hand, is able to fit better to the set of observed triplets, thus resulting in better test accuracy, as well.\nAs previously discussed, dissimilar from batch methods ERKLE does not use trace regularization. Experimentally, however, we nevertheless find that our method outperforms batch methods that use trace regularization, in either producing low-rank or high-rank kernels. To demonstrate this, in\nFigure 3c we plot the ranks of the kernels learned by the batch methods. In our experiments, the range of potential τ values was set so that the batch methods never chose either the upper or lower bound. We did this to ensure that the range of regularization options were sufficiently strict or lenient. We observe that the batch methods generally produce low-rank kernels under a small number of triplets, but as the number of triplets are observed the rank increases. Our method is able to better generalize without using trace regularization, regardless of the preferred rank, due to the PA updates only satisfying triplets to the necessary extent.\n5.4 Outdoor Scene Similarity Our final experiment used triplets over 200 randomly chosen images of scenes from the Outdoor Scene Recognition (OSR) data set [19]. Relative comparison queries were posed to 20 people via an online system. After an initial 1200 randomly chosen queries were answered (every object appeared as the head of a triplet 6 times), 20 “rounds” of 200 triplets were adaptively chosen according to the adaptive selection criterion in [25], resulting in 3600 total triplets. For each trial of this experiment, 1000 triplets were randomly chosen as the test set, 1000 as the train set, and 600 were used as the validation set for the τ parameter. This experiment is especially challenging for two reasons. First, this is the smallest experiment in terms of triplets, highlighting how the methods perform with little feedback. In addition, the adaptive selection algorithm chooses relative comparison queries with the highest information gain, meaning, the triplets are intentionally chosen to give disparate information about the relationships among objects.\nDiscussion: Figure 4a depicts test errors on each method. We observe that STE-ERKLE consistently outperforms STE-Batch, and in particular STE-ERKLE performs well under a small number of triplets relative to all other methods. PA-ERKLE is comparable or outperforms its batch counterpart in GNMDS-Batch, given enough triplets (at least 500). However, PA-ERKLE performs quite well in training\nerror compared to all other methods, indicating that even in such a challenging scenario, the passive-aggressive update scheme minimally interferes with previously obtained triplets."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this work, we developed a method to learn a PSD kernel matrix from relative comparisons given in an online fashion. By taking advantage of the sparse and low-rank structure of the online formulation, we show how to take stochastic gradient descent updates of complexity O(n2). We show how passive-aggressive online learning benefits our method in terms of generalizing to unseen triplets, and in conjunction with the stochastic gradient structure, enables us to perform a small number of necessary PSD projections in practice. Experimentally, we show on synthetic and real-world data that our method learns kernels that generalize as well and often better to held out relative comparisons than batch methods, while demonstrating improved run-time performance.\nFor future work, we wish to improve online RCKL in three ways. First, will explore the use of online trace regularization. If trace regularization is naively applied to the stochastic gradient in (4.9), the update becomes fullrank and our efficient projection procedure cannot be used. However, an efficient update scheme should be possible if the kernel itself is low-rank. We will investigate novel methods for appropriately weighting the trace in an online manner, so that we are consistent with the parameter-free property of PA-ERKLE. Second, PA-ERKLE performed well in our experiments with moderate triplet noise, however, it could be beneficial to explicitly handle conflicting triplets when they are observed. This can be done out of model using a denoising method [17], or in model using a threshold on the passive-aggressive learning rate. Finally, one of the main benefits of having an online learning algorithm is the natural application of active learning methods. Prior work has proposed an adaptive selection scheme which operates in mini-batches [25]; however, such a scheme is too expensive\nto be applied online. We will investigate novel adaptive triplet selection methods which are both efficient and informative."
    }, {
      "heading" : "A Derivation of STE Passive-Aggressive Step Size",
      "text" : "To derive the STE version of the passive-aggressive step size we wish to solve the following optimization (4.19):\nmin δj\nδ2j\ns.t. p K′j tj ≥ P, δj ≥ 0\nAs with the GNMDS derivation with the assumption that the triplet is not satisfied by a probability greater than or equal to P , only a positive value of δj can satisfy the first constraint, making the positive constraint on δj redundant. In addition, the smallest δj that satisfies the remaining constraint is the one that makes the left hand side exactly zero. As a result, the inequality constraint can be handled as equality. Next, we take the Lagrangian:\nδ2j +α (log (P )− log (1− P )) +α ( dKj−1(a, b)− dKj−1(a, c) ) − 10δjα\nTaking the partial derivative of the Lagrangian with respect to δj , setting it to 0, and solving for δj results in δj = 5α. Substituting this back into the Lagrangian makes it:\n−25α2 +α (log (P )− log (1− P )) +α ( dKj−1(a, b)− dKj−1(a, c) ) Taking the partial derivative of the Lagrangian with respect to α, setting it to 0, and solving for α results in:\nα = log (P )− log (1− P ) + dKj−1(a, b)− dKj−1(a, c)\n50\nSubstituting this into the solution for δj gives us:\nδj = log (P )− log (1− P ) + dKj−1(a, b)− dKj−1(a, c)\n10\nThis is what is given in (4.20)."
    }, {
      "heading" : "B ERKLE with Multiple Passes",
      "text" : "Algorithm 2 ERKLE with Multiple Passes Input: β : # of triplets stepped over\n1: K0 ← I 2: for j = 1, 2, ... do 3: K′j ← Kj−1 − δj∇l (Kj−1, tj) 4: Kj ← Π1S+ ( K′j ) 5: if j > 2β then 6: for k = 1, 2, ..., β − 1 do 7: Randomly select t′ from {t1, t2, ..., tj} 8: K′j ← Kj − δj+k∇l (Kj−1, t′) 9: Kj ← Π1S+ ( K′j )\n10: end for 11: end if 12: end for\nAlgorithm 1 is much like the original ERKLE algorithm. Here, after a sufficient number of triplets have been obtained (in our experiments, we chose 2β), β − 1 triplets are selected every iteration from all previously observed triplets (for a total of β updates per iteration). These triplets are stepped over as done in the original ERKLE algorithm. For our random selection used in our experiments, we simply selected uniformly at random with replacement from the obtained triplets. More sophisticated random selection procedures may be used in order ensure triplets obtained initially do not get selected drastically more times than those obtained later. For instance, when a triplet gets chosen on line 7, one could reduce the probability of that triplet being chosen subsequently."
    } ],
    "references" : [ {
      "title" : "Generalized non-metric multidimensional scaling",
      "author" : [ "S. Agarwal", "J. Wills", "L. Cayton", "G. Lanckriet", "D.J. Kriegman", "S.J. Belongie" ],
      "venue" : "AISTATS",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Sequence labelling svms trained in one pass",
      "author" : [ "A. Bordes", "N. Usunier", "L. Bottou" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pages 146–161. Springer",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Online learning and stochastic approximations",
      "author" : [ "L. Bottou" ],
      "venue" : "On-line learning in neural networks, 17:9",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Efficient low-rank stochastic gradient descent methods for solving semidefinite programs",
      "author" : [ "J. Chen", "T. Yang", "S. Zhu" ],
      "venue" : "AISTATS",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Online passive-aggressive algorithms",
      "author" : [ "K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer" ],
      "venue" : "JMLR, 7:551– 585",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning perceptual kernels for visualization design",
      "author" : [ "C. Demiralp", "M.S. Bernstein", "J. Heer" ],
      "venue" : "Infovis",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The quest for ground truth in musical artist similarity",
      "author" : [ "D.P.W. Ellis", "B. Whitman", "A. Berenzweig", "S. Lawrence" ],
      "venue" : "ISMIR",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Projection-free online learning",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "ICML",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Low-dimensional embedding using adaptively selected ordinal data",
      "author" : [ "K.G. Jamieson", "R.D. Nowak" ],
      "venue" : "Allerton",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "T. Joachims" ],
      "venue" : "SIGKDD",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Ordrec: an ordinal model for predicting personalized item rating distributions",
      "author" : [ "Y. Koren", "J. Sill" ],
      "venue" : "RecSys",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Whittlesearch: Image search with relative attribute feedback",
      "author" : [ "A. Kovashka", "D. Parikh", "K. Grauman" ],
      "venue" : "CVPR",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Nonmetric multidimensional scaling: a numerical method",
      "author" : [ "J.B. Kruskal" ],
      "venue" : "Psychometrika, 29(2):115–129",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Music information retrieval using social tags and audio",
      "author" : [ "M. Levy", "M. Sandler" ],
      "venue" : "Multimedia, IEEE Transactions on, 11(3):383–395",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Stochastic gradient descent with only one projection",
      "author" : [ "M. Mahdavi", "T. Yang", "R. Jin", "S. Zhu", "J. Yi" ],
      "venue" : "NIPS",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning multi-modal similarity",
      "author" : [ "B. McFee", "G. Lanckriet" ],
      "venue" : "JMLR, 12:491–523",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
      "author" : [ "E. Moulines", "F.R. Bach" ],
      "venue" : "NIPS",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Modeling the shape of the scene: A holistic representation of the spatial envelope",
      "author" : [ "A. Oliva", "A. Torralba" ],
      "venue" : "IJCV, 42(3):145–175",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "B. Recht", "C. Re", "S. Wright", "F. Niu" ],
      "venue" : "NIPS, pages 693–701",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "The annals of mathematical statistics, pages 400–407",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1951
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "Nicolas L. Roux", "M. Schmidt", "F.R. Bach" ],
      "venue" : "In NIPS",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Learning with kernels: support vector machines",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : "regularization, optimization, and beyond. MIT press",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Absolute identification by relative judgment",
      "author" : [ "N. Stewart", "G.D.A. Brown", "N. Chater" ],
      "venue" : "Psychological review, 112(4):881",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Adaptively learning the crowd kernel",
      "author" : [ "O. Tamuz", "C. Liu", "O. Shamir", "A. Kalai", "S.J. Belongie" ],
      "venue" : "ICML",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Stochastic triplet embedding",
      "author" : [ "L. Van Der Maaten", "K. Weinberger" ],
      "venue" : "In MLSP,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Similarity comparisons for interactive fine-grained categorization",
      "author" : [ "C. Wah", "G. Van Horn", "S. Branson", "S. Maji", "P. Perona", "S.J. Belongie" ],
      "venue" : "CVPR",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Breaking the curse of kernelization: Budgeted stochastic gradient descent for largescale svm training",
      "author" : [ "Z. Wang", "K. Crammer", "S. Vucetic" ],
      "venue" : "JMLR, 13(1):3103–3131",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Dual averaging method for regularized stochastic learning and online optimization",
      "author" : [ "L. Xiao" ],
      "venue" : "NIPS",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and R",
      "author" : [ "E. Zudilova-Seinstra", "T. Adriaansen" ],
      "venue" : "van Liere. Trends in interactive visualization: state-of-the-art survey. Springer",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 11,
      "context" : "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 28,
      "context" : "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 21,
      "context" : "Kernels are used for modeling object relationships in many learning techniques [23], and hence are applicable to many methods that utilize kernels for these applications.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "For instance, naive forms of supervision such as numerical judgments between pairs of objects have been shown to be very noisy [24].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "Recent works addressing fine-grained categorization [27] and perceptual visualization design [6] have shown the practicality and benefit of learning kernels from relative comparisons.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Recent works addressing fine-grained categorization [27] and perceptual visualization design [6] have shown the practicality and benefit of learning kernels from relative comparisons.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Many RCKL methods [1, 26] learn a kernel by solving a semidefinite program (SDP) in batch, where all obtained relative comparisons are required to learn the kernel.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 24,
      "context" : "Many RCKL methods [1, 26] learn a kernel by solving a semidefinite program (SDP) in batch, where all obtained relative comparisons are required to learn the kernel.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "For example in crowdsourcing, it is often of interest to minimize the number of dispatched tasks and thus the cost of the crowd by leveraging active learning techniques [25, 9] to adaptively select the most informative relative comparison query.",
      "startOffset" : 169,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : "For example in crowdsourcing, it is often of interest to minimize the number of dispatched tasks and thus the cost of the crowd by leveraging active learning techniques [25, 9] to adaptively select the most informative relative comparison query.",
      "startOffset" : 169,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : "ERKLE employs stochastic gradient descent [3] for RCKL, taking advantage of the sparse and low-rank structure of the RCKL gradient over a single comparison to devise fast updates that only require finding the smallest eigenvector and eigenvalue of a ar X iv :1 50 1.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "We show that the gradient structure, which enables such an efficient update, generalizes several wellknown convex RCKL methods [1, 26].",
      "startOffset" : 127,
      "endOffset" : 134
    }, {
      "referenceID" : 24,
      "context" : "We show that the gradient structure, which enables such an efficient update, generalizes several wellknown convex RCKL methods [1, 26].",
      "startOffset" : 127,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "Motivated by work in online learning [5], we also derive a passive-aggressive version of ERKLE to ensure learned kernels model the most recently obtained relative comparisons without over-fitting.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "For instance, Generalized Non-metric Multidimensional Scaling [1] employs hinge loss, Crowd Kernel Learning [25] uses a scale-invariant loss, and Stochastic Triplet Embedding [26] uses a logistic loss function.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "For instance, Generalized Non-metric Multidimensional Scaling [1] employs hinge loss, Crowd Kernel Learning [25] uses a scale-invariant loss, and Stochastic Triplet Embedding [26] uses a logistic loss function.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : "For instance, Generalized Non-metric Multidimensional Scaling [1] employs hinge loss, Crowd Kernel Learning [25] uses a scale-invariant loss, and Stochastic Triplet Embedding [26] uses a logistic loss function.",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 12,
      "context" : "The aforementioned RCKL methods can be viewed as solving a kernelized special case of the classic non-metric multidimensional scaling problem [14], where the goal is to find an embedding of objects in R such that they satisfy given Euclidean distance constraints.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "Stochastic gradient descent techniques [21] are a popular class of methods for online learning of high-dimensional data for a very general class of functions, where recent techniques [29, 22] have demonstrated competitive performance with batch techniques.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "Stochastic gradient descent techniques [21] are a popular class of methods for online learning of high-dimensional data for a very general class of functions, where recent techniques [29, 22] have demonstrated competitive performance with batch techniques.",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 20,
      "context" : "Stochastic gradient descent techniques [21] are a popular class of methods for online learning of high-dimensional data for a very general class of functions, where recent techniques [29, 22] have demonstrated competitive performance with batch techniques.",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "In particular, recent methods [8, 16] have developed efficient methods to solve SDPs in an online fashion.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "In particular, recent methods [8, 16] have developed efficient methods to solve SDPs in an online fashion.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "The work of [4] shows how to devise efficient update schemes for solving SDPs when the gradient of the objective function is low-rank.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "Our passive-aggressive step size procedure is similar to that which is introduced in [5] for other online learning problems.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "The Stochastic Triplet Embedding (STE) approach of [26] defines l (K, t) = − log pt as the loss function, where pt is the probability that a triplet is satisfied:",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Generalized Nonmetric Multidimensional Scaling (GNMDS) [1] uses a hinge loss, where l (K, t = (a, b, c)) is defined as:",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "1 Stochastic Gradient Step To create an efficient and online framework for RCKL – ERKLE – we leverage stochastic gradient descent techniques [3].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "For more discussion on this characteristic of stochastic methods see [3].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "If Kj has one negative eigenvalue, line 4 of Algorithm 1 results in a PSD matrix Kj that is closest to Kj in terms of Frobenius distance by Case 2 of Theorem 4 in [4].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "Early work [3] on the topic of learning rates suggest that δj should satisfy two constraints: ∑∞ j=1 δ 2 j < ∞ and ∑∞ j=1 δj = ∞.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 16,
      "context" : "Later work [18] suggests a more aggressive setting of δj = 1/ √ j.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "It is this observation that motivates Passive-Aggressive (PA) Online Learning [5].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Even for a proper setting of δj , it has been shown that stochastic methods perform best when multiple rounds of updates or passes are performed on the observed samples [2, 20, 28].",
      "startOffset" : 169,
      "endOffset" : 180
    }, {
      "referenceID" : 18,
      "context" : "Even for a proper setting of δj , it has been shown that stochastic methods perform best when multiple rounds of updates or passes are performed on the observed samples [2, 20, 28].",
      "startOffset" : 169,
      "endOffset" : 180
    }, {
      "referenceID" : 26,
      "context" : "Even for a proper setting of δj , it has been shown that stochastic methods perform best when multiple rounds of updates or passes are performed on the observed samples [2, 20, 28].",
      "startOffset" : 169,
      "endOffset" : 180
    }, {
      "referenceID" : 23,
      "context" : "We used the batch STE, GNMDS, and CKL (Crowd Kernel Learning [25]) MATLAB implementations specified by [26] in which the eig MATLAB function is used to find the all eigenvalues and eigenvectors for projection onto the PSD cone.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "We used the batch STE, GNMDS, and CKL (Crowd Kernel Learning [25]) MATLAB implementations specified by [26] in which the eig MATLAB function is used to find the all eigenvalues and eigenvectors for projection onto the PSD cone.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "The aset400 data set [7] contains 16,385 relative comparisons over 412 artists.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "4 Outdoor Scene Similarity Our final experiment used triplets over 200 randomly chosen images of scenes from the Outdoor Scene Recognition (OSR) data set [19].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "After an initial 1200 randomly chosen queries were answered (every object appeared as the head of a triplet 6 times), 20 “rounds” of 200 triplets were adaptively chosen according to the adaptive selection criterion in [25], resulting in 3600 total triplets.",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 15,
      "context" : "This can be done out of model using a denoising method [17], or in model using a threshold on the passive-aggressive learning rate.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "Prior work has proposed an adaptive selection scheme which operates in mini-batches [25]; however, such a scheme is too expensive",
      "startOffset" : 84,
      "endOffset" : 88
    } ],
    "year" : 2015,
    "abstractText" : "Learning a kernel matrix from relative comparison human feedback is an important problem with applications in collaborative filtering, object retrieval, and search. For learning a kernel over a large number of objects, existing methods face significant scalability issues inhibiting the application of these methods to settings where a kernel is learned in an online and timely fashion. In this paper we propose a novel framework called Efficient online Relative comparison Kernel LEarning (ERKLE), for efficiently learning the similarity of a large set of objects in an online manner. We learn a kernel from relative comparisons via stochastic gradient descent, one query response at a time, by taking advantage of the sparse and low-rank properties of the gradient to efficiently restrict the kernel to lie in the space of positive semidefinite matrices. In addition, we derive a passive-aggressive online update for minimally satisfying new relative comparisons as to not disrupt the influence of previously obtained comparisons. Experimentally, we demonstrate a considerable improvement in speed while obtaining improved or comparable accuracy compared to current methods in the online learning setting.",
    "creator" : "LaTeX with hyperref package"
  }
}