{
  "name" : "1606.09600.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exploring Prediction Uncertainty in Machine Translation Quality Estimation",
    "authors" : [ "Daniel Beck", "Lucia Specia", "Trevor Cohn" ],
    "emails" : [ "debeck1@sheffield.ac.uk,", "l.specia@sheffield.ac.uk,", "t.cohn@unimelb.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Quality Estimation (QE) (Blatz et al., 2004; Specia et al., 2009) models aim at predicting the quality of automatically translated text segments. Traditionally, these models provide point estimates and are evaluated using metrics like Mean Absolute Error (MAE), Root-Mean-Square Error (RMSE) and Pearson’s r correlation coefficient. However, in practice QE models are built for use in decision making in large workflows involving Machine Translation (MT). In these settings, relying on point estimates would mean that only very accurate prediction models can be useful in practice.\nA way to improve decision making based on quality predictions is to explore uncertainty estimates. Consider for example a post-editing scenario where professional translators use MT in an effort to speed-up the translation process. A QE\nmodel can be used to determine if an MT segment is good enough for post-editing or should be discarded and translated from scratch. But since QE models are not perfect they can end up allowing bad MT segments to go through for postediting because of a prediction error. In such a scenario, having an uncertainty estimate for the prediction can provide additional information for the filtering decision. For instance, in order to ensure good user experience for the human translator and maximise translation productivity, an MT segment could be forwarded for post-editing only if a QE model assigns a high quality score with low uncertainty (high confidence). Such a decision process is not possible with point estimates only.\nGood uncertainty estimates can be acquired from well-calibrated probability distributions over the quality predictions. In QE, arguably the most successful probabilistic models are Gaussian Processes (GPs) since they considered the state-ofthe-art for regression (Cohn and Specia, 2013; Hensman et al., 2013), especially in the low-data regimes typical for this task. We focus our analysis in this paper on GPs since other common models used in QE can only provide point estimates as predictions. Another reason why we focus on probabilistic models is because this lets us employ the ideas proposed by Quiñonero-Candela et al. (2006), which defined new evaluation metrics that take into account probability distributions over predictions.\nThe remaining of this paper is organised as follows:\n• In Section 2 we further motivate the use of GPs for uncertainty modelling in QE and revisit their underlying theory. We also propose some model extensions previously developed in the GP literature and argue they are more appropriate for the task.\nar X\niv :1\n60 6.\n09 60\n0v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n16\n• We intrinsically evaluate our proposed models in terms of their posterior distributions on training and test data in Section 3. Specifically, we show that differences in uncertainty modelling are not captured by the usual point estimate metrics commonly used for this task.\n• As an example of an application for predicitive distributions, in Section 4 we show how they can be useful in scenarios with asymmetric risk and how the proposed models can provide better performance in this case.\nWe discuss related work in Section 5 and give conclusions and avenues for future work in Section 6.\nWhile we focus on QE as application, the methods we explore in this paper can be applied to any text regression task where modelling predictive uncertainty is useful, either in human decision making or by propagating this information for further computational processing."
    }, {
      "heading" : "2 Probabilistic Models for QE",
      "text" : "Traditionally, QE is treated as a regression task with hand-crafted features. Kernel methods are arguably the state-of-the-art in QE since they can easily model non-linearities in the data. Furthermore, the scalability issues that arise in kernel methods do not tend to affect QE in practice since the datasets are usually small, in the order of thousands of instances.\nThe most popular method for QE is Support Vector Regression (SVR), as shown in the multiple instances of the WMT QE shared tasks (Callisonburch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). While SVR models can generate competitive predictions for this task, they lack a probabilistic interpretation, which makes it hard to extract uncertainty estimates using them. Bootstrapping approaches like bagging (Abe and Mamitsuka, 1998) can be applied, but this requires setting and optimising hyperparameters like bag size and number of bootstraps. There is also no guarantee these estimates come from a well-calibrated probabilistic distribution.\nGaussian Processes (GPs) (Rasmussen and Williams, 2006) is an alternative kernel-based framework that gives competitive results for point estimates (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2014b). Unlike SVR, they explicitly model uncertainty in the data and in the predictions. This makes GPs very applicable when\nwell-calibrated uncertainty estimates are required. Furthermore, they are very flexible in terms of modelling decisions by allowing the use of a variety of kernels and likelihoods while providing efficient ways of doing model selection. Therefore, in this work we focus on GPs for probabilistic modelling of QE. In what follows we briefly describe the GPs framework for regression."
    }, {
      "heading" : "2.1 Gaussian Process Regression",
      "text" : "Here we follow closely the definition of GPs given by Rasmussen and Williams (2006). Let X = {(x1, y1), (x2, y2), . . . , (xn, yn)} be our data, where each x ∈ RD is a D-dimensional input and y is its corresponding response variable. A GP is defined as a stochastic model over the latent function f that generates the data X :\nf(x) ∼ GP(m(x), k(x,x′)),\nwhere m(x) is the mean function, which is usually the 0 constant, and k(x,x′) is the kernel or covariance function, which describes the covariance between values of f at the different locations of x and x′.\nThe prior is combined with a likelihood via Bayes’ rule to obtain a posterior over the latent function:\np(f |X ) = p(y|X, f)p(f) p(y|X) ,\nwhere X and y are the training inputs and response variables, respectively. For regression, we assume that each yi = f(xi) + η, where η ∼ N (0, σ2n) is added white noise. Having a Gaussian likelihood results in a closed form solution for the posterior.\nTraining a GP involves the optimisation of model hyperparameters, which is done by maximising the marginal likelihood p(y|X) via gradient ascent. Predictive posteriors for unseen x∗ are obtained by integrating over the latent function evaluations at x∗.\nGPs can be extended in many different ways by applying different kernels, likelihoods and modifying the posterior, for instance. In the next Sections, we explain in detail some sensible modelling choices in applying GPs for QE."
    }, {
      "heading" : "2.2 Matèrn Kernels",
      "text" : "Choosing an appropriate kernel is a crucial step in defining a GP model (and any other kernel\nmethod). A common choice is to employ the exponentiated quadratic (EQ) kernel1:\nkEQ(x,x ′) = σv exp(−\nr2\n2 ) ,\nwhere r2 = D∑ i=1 (xi − x′i)2 l2i\nis the scaled distance between the two inputs, σv is a scale hyperparameter and l is a vector of lengthscales. Most kernel methods tie all lengthscale to a single value, resulting in an isotropic kernel. However, since in GPs hyperparameter optimisation can be done efficiently, it is common to employ one lengthscale per feature, a method called Automatic Relevance Determination (ARD).\nThe EQ kernel allows the modelling of nonlinearities between the inputs and the response variables but it makes a strong assumption: it generates smooth, infinitely differentiable functions. This assumption can be too strong for noisy data. An alternative is the Matèrn class of kernels, which relax the smoothness assumption by modelling functions which are ν-times differentiable only. Common values for ν are the half-integers 3/2 and 5/2, resulting in the following Matèrn kernels:\nkM32 = σv(1 + √ 3r2) exp(− √ 3r2)\nkM52 = σv\n( 1 + √ 5r2 + 5r2\n3\n) exp(− √ 5r2) ,\nwhere we have omitted the dependence of kM32 and kM52 on the inputs (x,x′) for brevity. Higher values for ν are usually not very useful since the resulting behaviour is hard to distinguish from limit case ν → ∞, which retrieves the EQ kernel (Rasmussen and Williams, 2006, Sec. 4.2).\nThe relaxed smoothness assumptions from the Matèrn kernels makes them promising candidates for QE datasets, which tend to be very noisy. We expect that employing them will result in a better models for this application."
    }, {
      "heading" : "2.3 Warped Gaussian Processes",
      "text" : "The Gaussian likelihood of standard GPs has support over the entire real number line. However, common quality scores are strictly positive values, which means that the Gaussian assumption\n1Also known as Radial Basis Function (RBF) kernel.\nis not ideal. A usual way to deal with this problem is model the logarithm of the response variables, since this transformation maps strictly positive values to the real line. However, there is no reason to believe this is the best possible mapping: a better idea would be to learn it from the data.\nWarped GPs (Snelson et al., 2004) are an extension of GPs that allows the learning of arbitrary mappings. It does that by placing a monotonic warping function over the observations and modelling the warped values inside a standard GP. The posterior distribution is obtained by applying a change of variables:\np(y∗|x∗) = f ′(y∗)√ 2πσ2∗ exp\n( f(y∗)− µ∗\n2σ∗\n) ,\nwhere µ∗ and σ∗ are the mean and standard deviation of the latent (warped) response variable and f and f ′ are the warping function and its derivative.\nPoint predictions from this model depend on the loss function to be minimised. For absolute error, the median is the optimal value while for squared error it is the mean of the posterior. In standard GPs, since the posterior is Gaussian the median and mean coincide but this in general is not the case for a Warped GP posterior. The median can be easily obtained by applying the inverse warping function to the latent median:\nymed∗ = f −1(µ∗).\nWhile the inverse of the warping function is usually not available in closed form, we can use its gradient to have a numerical estimate.\nThe mean is obtained by integrating y∗ over the latent density:\nE[y∗] = ∫ f−1(z)Nz(µ∗, σ2∗)dz,\nwhere z is the latent variable. This can be easily approximated using Gauss-Hermite quadrature since it is a one dimensional integral over a Gaussian density.\nThe warping function should be flexible enough to allow the learning of complex mappings, but it needs to be monotonic. Snelson et al. (2004) proposes a parametric form composed of a sum of tanh functions, similar to a neural network layer:\nf(y) = y + I∑\ni=1\nai tanh(bi(y + ci)),\nwhere I is the number of tanh terms and a,b and c are treated as model hyperparameters and optimised jointly with the kernel and likelihood hyperparameters. Large values for I allow more complex mappings to be learned but raise the risk of overfitting.\nWarped GPs provide an easy and elegant way to model response variables with non-Gaussian behaviour within the GP framework. In our experiments we explore models employing warping functions with up to 3 terms, which is the value recommended by Snelson et al. (2004). We also report results using the f(y) = log(y) warping function."
    }, {
      "heading" : "3 Intrinsic Uncertainty Evaluation",
      "text" : "Given a set of different probabilistic QE models, we are interested in evaluating the performance of these models, while also taking their uncertainty into account, particularly to distinguish among models with seemingly same or similar performance. A straightforward way to measure the performance of a probabilistic model is to inspect its negative (log) marginal likelihood. This measure, however, does not capture if a model overfit the training data.\nWe can have a better generalisation measure by calculating the likelihood on test data instead. This was proposed in previous work and it is called Negative Log Predictive Density (NLPD) (Quiñonero-Candela et al., 2006):\nNLPD(ŷ,y) = − 1 n n∑ i=1 log p(ŷi = yi|xi).\nwhere ŷ is a set of test predictions, y is the set of true labels and n is the test set size. This metric has since been largely adopted by the ML community when evaluating GPs and other probabilistic models for regression (see Section 5 for some examples).\nAs with other error metrics, lower values are better. Intuitively, if two models produce equally incorrect predictions but they have different uncertainty estimates, NLPD will penalise the overconfident model more than the underconfident one. On the other hand, if predictions are close to the true value then NLPD will penalise the underconfident model instead.\nIn our first set of experiments we evaluate models proposed in Section 2 according to their negative log likelihood (NLL) and the NLPD on test\ndata. We also report two point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson’s r, which has recently proposed by Graham (2015) as a more robust alternative."
    }, {
      "heading" : "3.1 Experimental Settings",
      "text" : "Our experiments comprise datasets containing three different language pairs, where the label to predict is post-editing time:\nEnglish-Spanish (en-es) This dataset was used in the WMT14 QE shared task (Bojar et al., 2014). It contains 858 sentences translated by one MT system and post-edited by a professional translator.\nFrench-English (fr-en) Described in (Specia, 2011), this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator.\nEnglish-German (en-de) This dataset is part of the WMT16 QE shared task2. It was translated by one MT system for consistency we use a subset of 2, 828 instances post-edited by a single professional translator.\nAs part of the process of creating these datasets, post-editing time was logged on an sentence basis for all datasets. Following common practice, we normalise the post-editing time by the length of the machine translated sentence to obtain postediting rates and use these as our response variables.\nTechnically our approach could be used with any other numeric quality labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006). Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012). Additionally, time is more directly applicable in real translation environments – where uncertainty estimates could be useful, as it relates directly to productivity measures.\nFor model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015). These features are used in the strong baseline models provided by the WMT\n2www.statmt.org/wmt16\nQE shared tasks. While the best performing systems in the shared tasks use larger feature sets, these are mostly resource-intensive and languagedependent, and therefore not equally applicable to all our language pairs. Moreover, our goal is to compare probabilistic QE models through the predictive uncertainty perspective, rather than improving the state-of-the-art in terms of point predictions. We perform 10-fold cross validation instead of using a single train/test splits and report averaged metric scores.\nThe model hyperparameters were optimised by maximising the likelihood on the training data. We perform a two-pass procedure similar to that in (Cohn and Specia, 2013): first we employ an isotropic kernel and optimise all hyperparameters using 10 random restarts; then we move to an ARD equivalent kernel and perform a final optimisation step to fine tune feature lengthscales. Point predictions were fixed as the median of the distribution."
    }, {
      "heading" : "3.2 Results and Discussion",
      "text" : "Table 1 shows the results obtained for all datasets. The first two columns shows an interesting finding in terms of model learning: using a warping function drastically decreases both NLL and NLPD. The main reason behind this is that standard GPs distribute probability mass over negative values, while the warped models do not. For the fr-en and en-de datasets, NLL and NLPD follow similar trends. This means that we can trust NLL as a measure of uncertainty for these datasets. However, this is not observed in the en-es dataset. Since this dataset is considerably smaller than the others, we believe this is evidence of overfitting, thus showing that NLL is not a reliable metric for small datasets.\nIn terms of different warping functions, using the parametric tanh function with 3 terms performs better than the log for the fr-en and en-de datasets. This is not the case of the en-es dataset, where the log function tends to perform better. We believe that this is again due to the smaller dataset size. The gains from using a Matèrn kernel over EQ are less conclusive. While they tend to perform better for fr-en, there does not seem to be any difference in the other datasets. Different kernels can be more appropriate depending on the language pair, but more experiments are needed to verify this, which we leave for future work.\nThe differences in uncertainty modelling are by and large not captured by the point estimate metrics. While MAE does show gains from standard to Warped GPs, it does not reflect the difference found between warping functions for fr-en. Pearson’s r is also quite inconclusive in this sense, except for some observed gains for en-es. This shows that NLPD indeed should be preferred as a evaluation metric when proper prediction uncertainty estimates are required by a QE model."
    }, {
      "heading" : "3.3 Qualitative Analysis",
      "text" : "To obtain more insights about the performance in uncertainty modelling we inspected the predictive distributions for two sentence pairs in the fr-en dataset. We show the distributions for a standard GP and a Warped GP with a tanh3 function in Figure 1. In the first case, where both models give accurate predictions, we see that the Warped GP distribution is peaked around the predicted value, as it should be. It also gives more probability mass to positive values, showing that the model is able to learn that the label is non-negative. In the second case we analyse the distributions when both models make inaccurate predictions. We can see that the Warped GP is able to give a broader distribution in this case, while still keeping most of the mass outside the negative range.\nWe also report above each plot in Figure 1 the NLPD for each prediction. Comparing only the Warped GP predictions, we can see that their values reflect the fact that we prefer sharp distributions when predictions are accurate and broader ones when predictions are not accurate. However, it is interesting to see that the metric also penalises predictions when their distributions are too broad, as it is the case with the standard GPs since they can not discriminate between positive and negative values as well as the Warped GPs.\nInspecting the resulting warping functions can bring additional modelling insights. In Figure 2 we show instances of tanh3 warping functions learned from the three datasets and compare them with the log warping function. We can see that the parametric tanh3 model is able to learn nontrivial mappings. For instance, in the en-es case the learned function is roughly logarithmic in the low scales but it switches to a linear mapping after y = 4. Notice also the difference in the scales, which means that the optimal model uses a latent Gaussian with a larger variance."
    }, {
      "heading" : "4 Asymmetric Risk Scenarios",
      "text" : "Evaluation metrics for QE, including those used in the WMT QE shared tasks, are assumed to be symmetric, i.e., they penalise over and underestimates equally. This assumption is however too simplistic for many possible applications of QE. For example:\n• In a post-editing scenario, a project manager may have translators with limited expertise in post-editing. In this case, automatic translations should not be provided to the translator unless they are highly likely to have very good quality. This can be enforced this by increasing the penalisation weight for underestimates. We call this the pessimistic scenario.\n• In a gisting scenario, a company wants to automatically translate their product reviews so\nthat they can be published in a foreign language without human intervention. The company would prefer to publish only the reviews translated well enough, but having more reviews published will increase the chances of selling products. In this case, having better recall is more important and thus only reviews with very poor translation quality should be discarded. We can accomplish this by heavier penalisation on overestimates, a scenario we call optimistic.\nIn this Section we show how these scenarios can be addressed by well-calibrated predictive distributions and by employing asymmetric loss functions. An example of such a function is the asymmetric linear (henceforth, AL) loss, which is a generalisation of the absolute error:\nL(ŷ, y) = { w(ŷ − y) if ŷ > y y − ŷ if ŷ ≤ y,\nwhere w > 0 is the weight given to overestimates. If w > 1 we have the pessimistic scenario, and the optimistic one can be obtained using 0 < w < 1. For w = 1 we retrieve the original absolute error loss.\nAnother asymmetric loss is the linear exponential or linex loss (Zellner, 1986):\nL(ŷ, y) = exp[w(ŷ − y)]− (ŷ − y)− 1\nwhere w ∈ R is the weight. This loss attempts to keep a linear penalty in lesser risk regions, while\nimposing an exponential penalty in the higher risk ones. Negative values for w will result in a pessimistic setting, while positive values will result in the optimistic one. For w = 0, the loss approximates a squared error loss. Usual values for w tend to be close to 1 or−1 since for higher weights the loss can quickly reach very large scores. Both losses are shown on Figure 3."
    }, {
      "heading" : "4.1 Bayes Risk for Asymmetric Losses",
      "text" : "The losses introduced above can be incorporated directly into learning algorithms to obtain models for a given scenario. In the context of the AL loss this is called quantile regression (Koenker, 2005), since optimal estimators for this loss are posterior quantiles. However, in a production environment the loss can change over time. For instance, in the gisting scenario discussed above the parameter w could be changed based on feedback from indicators of sales revenue or user experience. If the loss is attached to the underlying learning algorithms, a change in w would require full model retraining, which can be costly.\nInstead of retraining the model every time there is a different loss, we can train a single probabilistic model and derive Bayes risk estimators for the loss we are interested in. This allows estimates to be obtained without having to retrain models when the loss changes. Additionally, this allows different losses/scenarios to be employed at the same time using the same model.\nMinimum Bayes risk estimators for asymmetric losses were proposed by Christoffersen and\nDiebold (1997) and we follow their derivations in our experiments. The best estimator for the AL loss is equivalent to the ww+1 quantile of the predictive distribution. Note that we retrieve the median when w = 1, as expected. The best estimator for the linex loss can be easily derived and results in:\nŷ = µy − wσ2y 2\nwhere µy and σ2y are the mean and the variance of the predictive posterior."
    }, {
      "heading" : "4.2 Experimental Settings",
      "text" : "Here we assess the models and datasets used in Section 3.1 in terms of their performance in the asymmetric setting. Following the explanation in the previous Section, we do not perform any retraining: we collect the predictions obtained using the 10-fold cross-validation protocol and apply different Bayes estimators corresponding to the asymmetric losses. Evaluation is performed using the same loss employed in the estimator (for instance, when using the linex estimator with w = 0.75 we report the results using the linex loss with same w) and averaged over the 10 folds.\nTo simulate both pessimistic and optimistic scenarios, we use w ∈ {3, 1/3} for the AL loss and w ∈ {−0.75, 0.75} for the linex loss. The only exception is the en-de dataset, where we report results for w ∈ −0.25, 0.75 for linex3. We also report results only for models using the Matèrn52 kernel. While we did experiment with different kernels and weighting schemes4 our findings showed similar trends so we omit them for the sake of clarity."
    }, {
      "heading" : "4.3 Results and Discussion",
      "text" : "Results are shown on Table 2. In the optimistic scenario the tanh-based warped GP models give consistently better results than standard GPs. The log-based models also gives good results for AL but for linex the results are mixed except for en-es. This is probably again related to the larger sizes of the fr-en and en-de datasets, which allows the tanh-based models to learn richer representations.\n3Using w = −0.75 in this case resulted in loss values on the order of 107. In fact, as it will be discussed in the next Section, the results for the linex loss in the pessimistic scenario were inconclusive. However, we report results using a higher w in this case for completeness and to clarify the inconclusive trends we found.\n4We also tried w ∈ {1/9, 1/7, 1/5, 5, 7, 9} for the AL loss and w ∈ {−0.5,−0.25, 0.25, 0.5} for the linex loss.\nThe pessimistic scenario shows interesting trends. While the results for AL follow a similar pattern when compared to the optimistic setting, the results for linex are consistently worse than the standard GP baseline. A key difference between AL and linex is that the latter depends on the variance of the predictive distribution. Since the warped models tend to have less variance, we believe the estimator is not being “pushed” towards the positive tails as much as in the standard GPs. This turns the resulting predictions not conservative enough (i.e. the post-editing time predictions are lower) and this is heavily (exponentially) pe-\nnalised by the loss. This might be a case where a standard GP is preferred but can also indicate that this loss is biased towards models with high variance, even if it does that by assigning probability mass to nonsensical values (like negative time). We leave further investigation of this phenomenon for future work."
    }, {
      "heading" : "5 Related Work",
      "text" : "Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models.\nThe NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs (Snelson et al., 2004), but also others like Lázaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to evaluate recent work on uncertainty propagation methods for neural networks (Hernández-Lobato and Adams, 2015).\nAsymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others. Besides the AL and the linex, another well studied loss is the asymmetric quadratic, which in turn relates to the concept of expectiles (Newey and Powell, 1987). This loss generalises the commonly used squared error loss. In terms of applications, Cain and Janssen (1995) gives an example in real estate assessment, where the consequences of under- and over-assessment are usually different depending on the specific scenario. An engineering example is given by Zellner (1986) in the context of dam construction, where an underestimate of peak water level is much more serious than an overestimate. Such real-world applications guided many developments in this field: we believe that translation and other language processing scenarios which rely on NLP technologies can heavily benefit from these advancements."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This work explored new probabilistic models for machine translation QE that allow better uncertainty estimates. We proposed the use of NLPD, which can capture information on the whole predictive distribution, unlike usual point estimatebased metrics. By assessing models using NLPD we can make better informed decisions about which model to employ for different settings. Furthermore, we showed how information in the predictive distribution can be used in asymmetric loss scenarios and how the proposed models can be beneficial in these settings.\nUncertainty estimates can be useful in many other settings beyond the ones explored in this work. Active Learning can benefit from variance information in their query methods and it has shown to be useful for QE (Beck et al., 2013). Exploratory analysis is another avenue for future work, where error bars can provide further insights about the task, as shown in recent work (Nguyen and O’Connor, 2015). This kind of analysis can be useful for tracking post-editor behaviour and assessing cost estimates for translation projects, for instance.\nOur main goal in this paper was to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Daniel Beck was supported by funding from CNPq/Brazil (No. 237999/2012-9). Lucia Specia was supported by the QT21 project (H2020 No. 645452). Trevor Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105). The authors would like to thank James Hensman for his advice on Warped GPs and the three anonymous reviewers for their comments."
    } ],
    "references" : [ {
      "title" : "Query learning strategies",
      "author" : [ "Abe", "Mamitsuka1998] Naoki Abe", "Hiroshi Mamitsuka" ],
      "venue" : null,
      "citeRegEx" : "Abe et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Abe et al\\.",
      "year" : 1998
    }, {
      "title" : "Reducing Annotation Effort for Quality Estimation via Active Learning",
      "author" : [ "Beck et al.2013] Daniel Beck", "Lucia Specia", "Trevor Cohn" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Beck et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2013
    }, {
      "title" : "Joint Emotion Analysis via Multi-task Gaussian Processes",
      "author" : [ "Beck et al.2014a] Daniel Beck", "Trevor Cohn", "Lucia Specia" ],
      "venue" : "In Proceedings of EMNLP,",
      "citeRegEx" : "Beck et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2014
    }, {
      "title" : "SHEF-Lite 2.0 : Sparse Multitask Gaussian Processes for Translation Quality Estimation",
      "author" : [ "Beck et al.2014b] Daniel Beck", "Kashif Shah", "Lucia Specia" ],
      "venue" : "In Proceedings of WMT14,",
      "citeRegEx" : "Beck et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2014
    }, {
      "title" : "Non-Linear Text Regression with a Deep Convolutional Neural Network",
      "author" : [ "Bitvai", "Cohn2015] Zsolt Bitvai", "Trevor Cohn" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Bitvai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bitvai et al\\.",
      "year" : 2015
    }, {
      "title" : "Confidence estimation for machine translation",
      "author" : [ "Blatz et al.2004] John Blatz", "Erin Fitzgerald", "George Foster" ],
      "venue" : "In Proceedings of the 20th Conference on Computational Linguistics,",
      "citeRegEx" : "Blatz et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Blatz et al\\.",
      "year" : 2004
    }, {
      "title" : "Findings of the 2015 Workshop on Statistical Machine Translation",
      "author" : [ "Turchi." ],
      "venue" : "Proceedings of WMT15, pages 22–64.",
      "citeRegEx" : "Turchi.,? 2015",
      "shortCiteRegEx" : "Turchi.",
      "year" : 2015
    }, {
      "title" : "Real Estate Price Prediction under Asymmetric Loss",
      "author" : [ "Cain", "Janssen1995] Michael Cain", "Christian Janssen" ],
      "venue" : "Annals of the Institute of Statististical Mathematics,",
      "citeRegEx" : "Cain et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Cain et al\\.",
      "year" : 1995
    }, {
      "title" : "A Framework for Evaluating Approximation Methods for Gaussian Process Regression",
      "author" : [ "Christopher K.I. Williams", "Iain Murray" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chalupka et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chalupka et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimal Prediction Under Asymmetric Loss",
      "author" : [ "Christoffersen", "Francis X. Diebold" ],
      "venue" : null,
      "citeRegEx" : "Christoffersen et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Christoffersen et al\\.",
      "year" : 1997
    }, {
      "title" : "Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation",
      "author" : [ "Cohn", "Specia2013] Trevor Cohn", "Lucia Specia" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Cohn et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cohn et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving Evaluation of Machine Translation Quality Estimation",
      "author" : [ "Yvette Graham" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Graham.,? \\Q2015\\E",
      "shortCiteRegEx" : "Graham.",
      "year" : 2015
    }, {
      "title" : "Gaussian Processes for Big Data",
      "author" : [ "Nicolò Fusi", "Neil D. Lawrence" ],
      "venue" : "In Proceedings of UAI,",
      "citeRegEx" : "Hensman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hensman et al\\.",
      "year" : 2013
    }, {
      "title" : "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks",
      "author" : [ "Hernández-Lobato", "Ryan P. Adams" ],
      "venue" : "In Proceedings of ICML",
      "citeRegEx" : "Hernández.Lobato et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hernández.Lobato et al\\.",
      "year" : 2015
    }, {
      "title" : "Movie Reviews and Revenues: An Experiment in Text Regression",
      "author" : [ "Joshi et al.2010] Mahesh Joshi", "Dipanjan Das", "Kevin Gimpel", "Noah A. Smith" ],
      "venue" : "In Proceedings of NAACL",
      "citeRegEx" : "Joshi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2010
    }, {
      "title" : "Quantile Regression",
      "author" : [ "Roger Koenker" ],
      "venue" : null,
      "citeRegEx" : "Koenker.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koenker.",
      "year" : 2005
    }, {
      "title" : "Postediting time as a measure of cognitive effort",
      "author" : [ "Wilker Aziz", "Luciana Ramos", "Lucia Specia" ],
      "venue" : "In Proceedings of WPTP",
      "citeRegEx" : "Koponen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Koponen et al\\.",
      "year" : 2012
    }, {
      "title" : "Bayesian Warped Gaussian Processes",
      "author" : [ "Miguel Lázaro-Gredilla" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Lázaro.Gredilla.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lázaro.Gredilla.",
      "year" : 2012
    }, {
      "title" : "Lyrics, Music, and Emotions",
      "author" : [ "Mihalcea", "Strapparava2012] Rada Mihalcea", "Carlo Strapparava" ],
      "venue" : "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "Mihalcea et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mihalcea et al\\.",
      "year" : 2012
    }, {
      "title" : "Asymmetric Least Squares Estimation and Testing",
      "author" : [ "Newey", "Powell1987] Whitney K. Newey", "James L. Powell" ],
      "venue" : null,
      "citeRegEx" : "Newey et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Newey et al\\.",
      "year" : 1987
    }, {
      "title" : "Posterior Calibration and Exploratory Analysis for Natural Language Processing Models",
      "author" : [ "Nguyen", "O’Connor2015] Khanh Nguyen", "Brendan O’Connor" ],
      "venue" : "In Proceedings of EMNLP, number September,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Evaluating Predictive Uncertainty Challenge",
      "author" : [ "Carl Edward Rasmussen", "Fabian Sinz", "Olivier Bousquet", "Bernhard Schölkopf" ],
      "venue" : "MLCW",
      "citeRegEx" : "QuiñoneroCandela et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "QuiñoneroCandela et al\\.",
      "year" : 2006
    }, {
      "title" : "Gaussian processes for machine learning, volume 1",
      "author" : [ "Rasmussen", "Christopher K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen et al\\.",
      "year" : 2006
    }, {
      "title" : "An Investigation on the Effectiveness of Features for Translation Quality Estimation",
      "author" : [ "Shah et al.2013] Kashif Shah", "Trevor Cohn", "Lucia Specia" ],
      "venue" : "In Proceedings of MT Summit XIV",
      "citeRegEx" : "Shah et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2013
    }, {
      "title" : "A study of translation edit rate with targeted human annotation",
      "author" : [ "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul" ],
      "venue" : "Proceedings of AMTA",
      "citeRegEx" : "Snover et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "Estimating the sentence-level quality of machine translation systems",
      "author" : [ "Specia et al.2009] Lucia Specia", "Nicola Cancedda", "Marc Dymetman", "Marco Turchi", "Nello Cristianini" ],
      "venue" : "In Proceedings of EAMT,",
      "citeRegEx" : "Specia et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Specia et al\\.",
      "year" : 2009
    }, {
      "title" : "Multi-level Translation Quality Prediction with QUEST++",
      "author" : [ "Specia et al.2015] Lucia Specia", "Gustavo Henrique Paetzold", "Carolina Scarton" ],
      "venue" : "In Proceedings of ACL Demo Session,",
      "citeRegEx" : "Specia et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Specia et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploiting Objective Annotations for Measuring Translation Postediting Effort",
      "author" : [ "Lucia Specia" ],
      "venue" : "In Proceedings of EAMT,",
      "citeRegEx" : "Specia.,? \\Q2011\\E",
      "shortCiteRegEx" : "Specia.",
      "year" : 2011
    }, {
      "title" : "Learning to identify emotions in text",
      "author" : [ "Strapparava", "Mihalcea2008] Carlo Strapparava", "Rada Mihalcea" ],
      "venue" : "In Proceedings of the 2008 ACM Symposium on Applied Computing,",
      "citeRegEx" : "Strapparava et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Strapparava et al\\.",
      "year" : 2008
    }, {
      "title" : "Bayesian Estimation and Prediction Using Asymmetric Loss Functions",
      "author" : [ "Arnold Zellner" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Zellner.,? \\Q1986\\E",
      "shortCiteRegEx" : "Zellner.",
      "year" : 1986
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Quality Estimation (QE) (Blatz et al., 2004; Specia et al., 2009) models aim at predicting the quality of automatically translated text segments.",
      "startOffset" : 24,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "Quality Estimation (QE) (Blatz et al., 2004; Specia et al., 2009) models aim at predicting the quality of automatically translated text segments.",
      "startOffset" : 24,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "cesses (GPs) since they considered the state-ofthe-art for regression (Cohn and Specia, 2013; Hensman et al., 2013), especially in the low-data regimes typical for this task.",
      "startOffset" : 70,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "cesses (GPs) since they considered the state-ofthe-art for regression (Cohn and Specia, 2013; Hensman et al., 2013), especially in the low-data regimes typical for this task. We focus our analysis in this paper on GPs since other common models used in QE can only provide point estimates as predictions. Another reason why we focus on probabilistic models is because this lets us employ the ideas proposed by Quiñonero-Candela et al. (2006), which defined new evaluation metrics that take into account probability distributions over predictions.",
      "startOffset" : 94,
      "endOffset" : 441
    }, {
      "referenceID" : 23,
      "context" : "Gaussian Processes (GPs) (Rasmussen and Williams, 2006) is an alternative kernel-based framework that gives competitive results for point estimates (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2014b).",
      "startOffset" : 148,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "We also report two point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson’s r, which has recently proposed by Graham (2015) as a more robust alternative.",
      "startOffset" : 179,
      "endOffset" : 193
    }, {
      "referenceID" : 27,
      "context" : "French-English (fr-en) Described in (Specia, 2011), this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator.",
      "startOffset" : 36,
      "endOffset" : 50
    }, {
      "referenceID" : 24,
      "context" : "Technically our approach could be used with any other numeric quality labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006).",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 16,
      "context" : "Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012).",
      "startOffset" : 199,
      "endOffset" : 221
    }, {
      "referenceID" : 26,
      "context" : "For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 29,
      "context" : "Another asymmetric loss is the linear exponential or linex loss (Zellner, 1986):",
      "startOffset" : 64,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "In the context of the AL loss this is called quantile regression (Koenker, 2005), since optimal estimators for this loss are posterior quantiles.",
      "startOffset" : 65,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "views (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al.",
      "startOffset" : 6,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : ", 2004), but also others like Lázaro-Gredilla (2012) and Chalupka et",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 28,
      "context" : "Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others.",
      "startOffset" : 88,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others.",
      "startOffset" : 107,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : "Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others. Besides the AL and the linex, another well studied loss is the asymmetric quadratic, which in turn relates to the concept of expectiles (Newey and Powell, 1987). This loss generalises the commonly used squared error loss. In terms of applications, Cain and Janssen (1995) gives an example in real estate assessment, where the consequences of under- and over-assessment are usually different depending on the specific scenario.",
      "startOffset" : 107,
      "endOffset" : 409
    }, {
      "referenceID" : 15,
      "context" : "Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others. Besides the AL and the linex, another well studied loss is the asymmetric quadratic, which in turn relates to the concept of expectiles (Newey and Powell, 1987). This loss generalises the commonly used squared error loss. In terms of applications, Cain and Janssen (1995) gives an example in real estate assessment, where the consequences of under- and over-assessment are usually different depending on the specific scenario. An engineering example is given by Zellner (1986) in the context of dam construction, where an underestimate of peak water level is much more serious than an overestimate.",
      "startOffset" : 107,
      "endOffset" : 614
    }, {
      "referenceID" : 1,
      "context" : "Active Learning can benefit from variance information in their query methods and it has shown to be useful for QE (Beck et al., 2013).",
      "startOffset" : 114,
      "endOffset" : 133
    } ],
    "year" : 2016,
    "abstractText" : "Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows.",
    "creator" : "LaTeX with hyperref package"
  }
}