{
  "name" : "1705.08440.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "michalew@plearn.bitnet" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n08 44\n0v 1"
    }, {
      "heading" : "1 Introduction",
      "text" : "A statistical database is a collection of aggregated information at a given time. The purpose of such database is to derive or infer facts; statistics is helpful in extracting patterns from quantative (and qualitative) data [6]. Data are analysed to extract information and the result of such analysis must be validated to increase our knowledge about modeled domain.\nSeveral statistical packages (e.g. MINITAB SAS) are used for such statistical analysis of data. However, they do not inform the user about underlying assumptions, seldom warn against obvious misuse, do not provide guidance in the process of analysis, and hardly assist in the interpretation of the results [8]. Clearly, there is a need for statistical expert systems [11], [14], [15], [16], which would include the statistical expertise.\nIn [17] the authors provide a general characteristic of statistical expert systems. They wrote:\n“As statistical expert system is a computer program which can act in the role of expert statistical consultant. That is, it can give expert advice on how to design a study, what data should be collected to answer the research question, and how to analyse the data. Moreover, since the fundamental tool of statistical analysis is the computer, on which the expert system will be run, it is normally envisaged that the expert system and data analysis routines will be an integrated whole. thus, not only does the system advise on the analysis, but it also actually carries it out, discussing the results and the direction of further analyses with the user.”\nIn this paper we deal with some aspects of creative data analysis. In particular we are interested in applications of AI techniques for development of a software environment which would allow to process both data and knowledge; such software should provide an easy interface for formulating hypotheses and their statistical verification.\nThe paper is organized as follows. The next section discusses the architecture of a creative data analysis system. Section 3 presents knowledge acquisition strategies. Section 4 introduces some aspects of new version of SEAD System, based on belief network approach and DST. Section 5 is devoted to DS methodology: after describing the nature of evidential reasoning we present mathematics of evidential resoning and next details of the message propagation algorithm is introduced and its application to belief revision is presented. In Section 6 a user interface to belief network is described."
    }, {
      "heading" : "2 Data Analysis System",
      "text" : "Information systems are designed for storing and manipulating data (from some domain) and for drawing conclusions when a problem from that domain is formulated. Clearly, this is of value insofar as the data are meaningful and the domain formalization is sufficiently adequate.\nThere are at least two main categories of such systems, termed tentatively standard decision aid tools (like statistical systems, pattern recognition programs, etc. and knowledge based systems . Although proven to be useful, systems of both categories suffer from some drawbacks limiting their applicability.\nThe main disadvantage of systems from the first category is that a user is forced to (1) formulate his (her) problem in a formal language, (2) check the input data validity, and (3) translate solutions produced by the system to the language appropriate for a given domain. In fact, to solve real-life problems the user must cooperate with professional task analyzer.\nWhen developing knowledge based systems an important obstacle concerning knowledge acquisition emerges. Knowledge bases generated by experts in different areas remain incomplete, subjective and even contradictory. Additional difficulties arise in cases where there is a need for processing data and knowledge simultanously.\nIn both categories of systems the process of hypotheses generation is left to the user.\nThe above approaches, considered separately, are especially inefficient when formalizing poorly recognized domains or domains with large amount of vagueness. In case of medical diagnosis, for instance, although the characteristics of certain diseases are fixed, the formal relationships among symptoms and diseases are hard to determine and serious difficulties in formulating scientific hypotheses may occur. To formalize such domains one must combine data, partial knowledge and tentative hypotheses.\nIn summary, an effective information system must provide the following features: data storing, knowledge acquisition and manipulation, automated generation of hypotheses, performing standard statistical analyses, including tests of research hypotheses, and easy communication between the knowledge base and database.\nSuch experimental information system (called SEAD for Statistical and Expert system for Analysis of Data) was recently implemented in the Institute of Computer Science, Polish Academy of Sciences [26].\nThe system architecture reflects a typical understanding of the notion of science according which science is viewed as a progressive development of theoretical generalizations of observations and experiments leading to new observations and new experiments. So, scientific research means permanent moving within the cycle “observation – generalization – theory – application – observation”.\nVarious tools have been developed to support a researcher in different parts of his cycle, including tools supporting storing observations and experiments (data base systems) and storing the acquired theoretical generalizations (knowledge base systems). Also the tools supporting the knowledge acquisition are included, such as the ability to generalize over a data base (statistical data analysis systems and numerous “learning from examples” systems), tools allowing verification against new incoming experiments results in the data base (statistical analysis systems), or against new single cases (expert systems). These tools are placed in specialized submodules, which are the main components of the system: data base system, knowledge base system, expert system, knowledge acquisition and verification tools.\nThe main feature of this system is implementation of all four components into one\norganism, whose heart is DB pumping and absorbing data. Such a solution (based on unified data formats and unified access methods) results in a research tool of quite new and unique quality. The user can also employ each system’s component as an independent unit or (s)he can add his (her) own modules."
    }, {
      "heading" : "3 Knowledge Acquisition Strategies",
      "text" : "The completion of knowledge base by human experts is a bottleneck in the process developing an expert systems. Usually, experts engaged in the process are excessively cautious (especially when dealing with poorly recognized domains), they use different semantics (often with vague and ambiguous notions) that results in a extremely long completion period without any warrant that the set of rules is complete and that it doesn’t contain contradictory assertions.\nAnother question concerns rules validity. According to methodological considerations, to refine a rule we search for so-called idealizing law (being the heart of the rule) first, and next we release systematically simplifying assumptions. Of course, such a process must be stopped at a reasonable time, and since some assumptions still remain hidden, the rule in its symbolic version is valid to some extent only. Hence we may address two questions:\n– were the most important assumptions embodied into the rule?, and – how to estimate the degree of validity of such rule?\nTo answer these questions and to optimize the knowledge acquisition process a number of tools have been developed in the SEAD system. These tools can be divided into two categories described in sections 3.1 and 3.2."
    }, {
      "heading" : "3.1 Passive user",
      "text" : "If certain aspect of the problem under the considerations is either ill-recognized or the knowledge about it is too general, some data preprocessing might be necessary. the aim of such preprocessing is to generate tentative rules directly from an existing data base. At this stage the user plays a passive role contrary to the further stages realized with his active collaboration. Here the user has a few possibilities:\n– Discretization of a continuous variable under a criterion of diagnosis accuracy maximization. – Some optimization but for a set of discrete variables. – Identification of irrelevant variables. – Generation of elementary hypotheses for specific values of variables. – Generation of rules."
    }, {
      "heading" : "3.2 Active user",
      "text" : "By an active knowledge acquisition we mean the simultaneous utilization of the DB and the KB, and the statistical verification of the partial knowledge. The following tools are helpful in these tasks.\n– Tools for rules identification. These the search for rules with given condition or conclusion, the identification of rules contexually analogical to a given rule, etc. Any hypothetical rule can be therefore compared by the user with those existing in the knowledge base. – Search for data in the DB relevant to a given hypothesis; particularly search for data contradicting given rule. these operations are realized through queries to the DB. A specialized interface allow simultaneous access to the KB and the DB. – Rules formation from incomplete schemata (called suppositions). This is the active strategy attempting to find a rule (or possibly a set of rules) satisfying constraints imposed by the user. The constraints concern values of some attributes as well as degrees of truth which should be attached to the rules. – KB verification with respect to the contents of the DB. If the DB is sufficiently large we are interested in how accurate the rules are, i.e. how strong is their expressive power (measured in terms of misclassification error). To answer these questions we employ the Statistical Module, which assists the user in his research (search for the strongest relationships in the set of variables for a given group of objects, marginal analysis for each variable, graphics for pairs of variables, et.)."
    }, {
      "heading" : "3.3 Problems of Automatic Rule Generation",
      "text" : "In very large statistical databases the generation of hypotheses (classification rules) is done on the basis of small samples of data. However, all known methods for automatic generation of classification rules aim at the precise partition of the attributevalued space.They are create “long” conjunctions of the pairs (attribute, value), which precisely classify all data sets [28]. Such approaches provide reasonable results for (large) databases, where the classification problems are well defined. However, for databases where classification is done on the basis of small samples of data (typical case for “soft sciences”), this is not the case. In such databases there is a need for preprocessing data, i.e. for generating a first iteration of hypotheses which are true with a high probability. This first iteration of hypotheses should yield simple formulae expressed in a language understandable for a user. The precise, “long” classification rules (hypotheses) would be useless: they are precise only for a small sample of data and don’t provide a guidance for the next iterations of hypotheses. Because of this, the development of new classification methods for a single attribute and small sets(i.e. 2 or 3) of attributes are of\nparticular interest. Summarizing, notice, that there are several problems connected with automatic rule generation in small databases (these include also large databases, where a data sampling was performed):\n– the set of data is not representative for drawing conclusions strictly dividing the attribute/value space, – the generated rules are hardly interpretable, if at all possible; rules are detailed and concerned with only several cases, – a new sample (or a new set of data) can easily provide a contradiction, – the set of rules usually is too large; this might be appropriate for an expert system\nor diagnostic automata, but not for data analysis, – in many methods the domains of attributes must be discrete; the process of dis-\ncretization is usually left to the expert (user).\nTaking into account the above drawbacks, we decided to construct a rule generating algorithm which would start from a single attribute (“bottom up” approach). We present the main idea of the method for creating the reasoning rules. This is based on (1) an optimization process defined on the set of discrete values of a given attribute and (2) a process of dividing a continous domain of an attribute into subsets; such optimal divisions are converted into the set of classification rules [25]"
    }, {
      "heading" : "4 SEAD.2 – the new version",
      "text" : "Some experiences of exploitation of SEAD system caused several significant modifications, which were entered into the next version of SEAD:"
    }, {
      "heading" : "4.1 Data Base",
      "text" : "Data base, built for SEAD 1 by our group, compatible to dBase family, appeared to be ineffective. Especially much difficulties was noticed when cooperation of data base with expert and statistical parts (written in C language) was needed. We decided to accommodate CODEBASE, which includes both libraries of procedures of access to data bases like dBase IV, Fox Base or CLIPPER written in C and libraries for user interfaces near to dBase IV or CLIPPER for MS DOS and MS WINDOWS."
    }, {
      "heading" : "4.2 Knowledge Base",
      "text" : "Knowledge base now is the bayesian network with uncertainty propagation based on Dempster-Schafer model, instead of rule-based knowledge base with uncertainty propagation near to MYCIN [37] used in former version of SEAD. This changes was caused by following drawbacks of rule approaches:\n– independence of variables is assumed, several appearances of the same variable in chain causes some computation complications (each appearance of concrete variable is treated as appearance of separate variable) – independence of rules is assumed; data base appears as a rubbish where rules are entered de facto without any control process. – Shortliffe’s approach assumed the use certainty factor CF [37], nature of which was defined not clearly enough. Even if rule’s CF was interpreted in terms of probability, combination of rules lead very fast to loss of interpretability.\nMeanwhile the belief networks has the following advantages:\n– there exists well defined relationship between variables (conditional or not) – these relationships are to be tested very simply, what can be used for new rules\ngeneration for example – one cannot add/remove rules without retribution, because it can break relationships\nbetween variables\nApproach based on belief networks and Dempster-Schafer methodology generates many theoretical and practical problems. Some of them are showed below in section 5"
    }, {
      "heading" : "4.3 Rule preceiving of bayesian knowledge base",
      "text" : "It is easy to notice, that rule approach has several significant advantages. The most important of them are following: simplicity, rule interpretability and naturalness and comparatively easiness of introducing some knowledge acquisition mechanisms.\nThat was the reason of introducing to new version of SEAD systems some special methods, which make possible for the user to preceive bayesian knowledge base as a set of rules. These methods are described below in section 6."
    }, {
      "heading" : "5 DS Approach to Knowledge Representation and Manipulation",
      "text" : ""
    }, {
      "heading" : "5.1 The Nature of Evidential Reasoning",
      "text" : "The term evidential reasoning coined by Lowrance and Garvey in [23] covers a set of techniques designed for processing and reasoning from row data (evidence) which are currently accessible to an agent. It is based upon the Dempster-Shafer theory (DST) of belief functions. This theory was founded by G. Shafer in his monograph [32] and was highly influenced by the works of A. P. Dempster [9, 10]. DST is based on two principal ideas: “the idea of obtaining degrees of belief for one question from subjective probabilities for a related question and Dempster’s rule for combining such degrees of belief when they are based on independent items of evidence” [34] p. 474.\nTo give a concise rationale for this theory (see [29]) for deeper discussion) we should note that the information required to understand the current state of the world, or to solve a real problem, comes from multiple sources, like real-time sensor data, accumulated domain knowledge and current contextual information. Information coming from sensors, called evidence, provides a support for certain conclusions. The very nature of evidence can be summarized as follows:(1) it is uncertain, i.e it allows for multiple possible explanations, (2) it is incomplete as the sensors rarely has a full view of situation, (3) it may be completely or partial incorrect .\nBecause of its nature, evidence is not readily represented either by logical or by classical probabilistic formalism. The ad hoc proposed methods for handling uncertain information (like e.g. CF factors) cause difficulties in interpretation and in extending the capabilities of such methodologies. DST, in contrast, provides a natural representation for evidential information, a formal basis for drawing conclusions from evidence, and a representation for belief.\nIn evidential reasoning a knowledge source is allowed to express probabilistic opinions about the (partial) truth or falsity of statements composed of subsets of propositions from a space of distinct, exhaustive possibilities (called the frame of discernment). The beliefs can be assigned to the individual (elementary) propositions in the space or to disjunctions of these propositions (i.e. to compound propositions) or both. When belief is assigned to a disjunction, a knowledge source is explicitly stating that it does not has enough information to distribute this belief more precisely. Hence a knowledge source can distribute its belief to statements whose granularity is appropriate to its state of knowledge. Further, the statements to which belief is assigned are not necessarily distinct from one another. The distribution of beliefs over a frame of discernment is called a body of evidence.\nThe easiest way to formally represent a body of evidence is to define a mass function m being a probability distribution over a set of subsets of a frame of discernment Θ (as such it must satisfy the obvious condition that ∑\n{m(A) | A ⊆ Θ} = 1). Now, if the mass function is positive on singletons ofΘ only then our model reduces to standard probability distribution. On the other extreme, a mass function can take the form m(A) = 1 for some A ⊆ Θ. When A = Θ then such a function represents total ignorance about current state of the world. When A ⊂ Θ, then the mass function describes our certainty that the truth lies in A; when Θ is a Cartesian product of frames describing related questions then m(A) = 1 defines a logical belief function. Thus, Boolean logic is another special case of belief functions.\nDST is therefore a generalization of both Boolean logic and probability calculus. This generalization lies deep in the representation of belief functions and has effects on many aspects of the theory. The heart of this theory is Dempster’s Rule of Combination, or DRC for short, being a formal tool for fusing different bodies of evidence. The result is a\nnew body of evidence representing the consensus among the original bodies of evidence. In special cases this rule behaves like Bayes’ rule and in other cases like set intersection. Thus belief function provide a framework in which propositional logic and probabilistic statements can be freely mixed and processed under common formalism.\nBelief functions, however, provide more general models. Particularly, random sets are a generalization of the fixed sets of logical models and upper (plausibility functions) and lower (belief functions) bound intervals are a generalization of point probabilities.\nCurrent automated reasoning systems are most effective when domain knowledge can be modeled as a set of loosely interconnected concepts (propositions) what justifies an incremental approach to updating beliefs. Applying DST formalism, independent opinions are expressed by independent bodies of evidence and dependent opinions can either be expressed by a single body of evidence or by a network describing the interrelationships among several bodies of evidence (so called belief networks). Updating the belief (by using Dempster’s Rule of combination) in one proposition affects the entire body of evidence. This allows to involve whole the knowledge already accumulated in the system (contrary to rule-based systems where only fragmentary knowledge is used during the process).\nIn this paper we assume some familiarity with the DST, although the appropriate definitions are included in the next section. In the sequel we will focus on the mathematics of evidential reasoning, mainly on so-called belief revision."
    }, {
      "heading" : "5.2 Mathematics of Evidential Reasoning",
      "text" : "In this section we briefly review a mathematical formalization of the evidential reasoning. It consists of two main parts. In static part we simply represent our knowledge about a problem in terms of belief functions and dynamic part allows to express knowledge evolution.\nBasic notions\nX = {X1, . . . , Xn} is a set of variables and Θi = Dom(Xi) is a discrete set of possible values of i-th variable. (The case of continuous variable is considered by [33] or [44].)\nBy A, B, C,. . . we shall denote subsets of the set X .Θ will denote the configurations space or frame of discernment, Θ = ×{Θ | i = 1 . . . n}, and Θ.A = ∏\n{Θ | Xi ∈ A} will denote the marginal frame. If x is a configuration from Θ then x.A stands for the projection of x onto Θ.A; x.A is obtained by dropping from x the components being members of Θi for all Xi ∈ X \\A.\nKnowledge representation\nAssume (Ω,B, P ) is a probability space, Ξ is a discrete space and X is a mapping which\nassigns subsets of Ξ to points of Ω, i.e. X(ω) ⊆ Ξ for each ω ∈ Ω. Treating Ξ as the set of elementary hypotheses we identify each subset of Ξ as a compound hypothesis. If Ω is viewed as a set of observations then X(ω) = A can be interpreted as a rule “If observation = ω then hypothesis = A”. According to classical approach the mapping X induces a measure µ defined on POW - the power set of the set of all compound hypotheses:\nµ(A) = P ({ω ∈ Ω | X(ω) ∈ A}), A ∈ POW (1)\nNote that when X is a function, i.e. X(ω) ∈ Ξ for all ω ∈ Ω then POW can be reduced to the power set of Ξ and (1) reduces to the definition of the random variable. Under the general setting X constitutes so-called random set [47].\nOf special importance are the next set functions derived from (1) and defined for all A ⊆ Ξ, A 6= ∅:\nm(A) = P ({ω ∈ Ω | X(ω) = A}), (2)\nBel(A) = P ({ω ∈ Ω | X(ω) ⊆ A, X(ω) 6= ∅}) (3)\nPl(A) = P ({ω ∈ Ω | (X(ω) ∩A) 6= ∅}), (4)\nAssuming that all observations are relevant, i.e. X(ω) 6= ∅ for all ω ∈ Ω, we easily recover the main properties of the basic probability assignment, m, i.e. m(∅) = 0, and ∑\n{m(A) | A ⊆ Ξ} = 1. In general case, when m(∅) > 0, the mass function should be modified according to the assignment m(A) := m(A)/(1 −m(∅) and m(∅) = 0. The m-function plays the role analogical to the probability distribution P (X = x), x ∈ ΘX ; particularly when m(A) is positive on the singletons only then m is just a probability distribution. The belief and plausibility functions can be computed from m by means of the identities\nBel(A) = ∑ {m(B) | B ⊆ A}, P l(A) = ∑ {m(B) | B ∩ A 6= ∅} (5)\nSimilarly plausibility function can be deduced from a belief function by means of the equation Pl(A) = 1−Bel(Ξ −A), A ∈ Ξ.\nShafer [32] proposes further formulas enabling to express one set function in terms of other set function and fast algorithms for doing this task are reported in [18].\nIn the sequel we shall use multivariate belief functions, i.e. belief function defined on the family of subsets of the space Θ introduced at the beginning of this section.\nSpecial class of multivariate belief functions form so called logical belief functions satisfying the condition m(A) = 1 for some A ⊂ Θ. (When A = Θ then the mass function represents vacuous belief function; it models total ignorance about the possible location of X inΘ.) To be more illustrative we present four elementary situations:\n(a) the logical expression “X1 = x1 and X2 = x2 . . . and Xn = xn” is represented by the element θ =< x1, x2, . . . , x >∈ Θ, (b) the logical expression “X1 = x1 or X2 = x2 . . . or Xn = xn”is represented by the subset of the form ⋃n\ni=1({xi} ×Θ.(X − {Xi})), (c) the logical expression “not (X1 = x1)” is represented by the subset of the form\n(Θ1 − {x1})×Θ.(X − {X1}). (d) the logical expression “if ((X1 = x1) or . . . or (Xn−1 = xn−1) then (Xn = xn)” is\nrepresented by the subset of the form {x1, x2, . . . , xn}∪ (Θ1 −{x1})× (Θ2 −{x2})× . . .× (Θn−1 − {xn−1})×Θn .\nWhen the logical assertions cannot be stated with full certainty we convert them to so called simple support function by setting m(A) = α and m(Θ) = 1−α, where α ∈ (0, 1) and A is a subset of Θ representing appropriate assertion.\nKnowledge evolution\nUp to this moment we have assumed that all the belief functions are defined on the common space Θ. Suppose we have two belief functions defined on the frames Θ.A and Θ.B, respectively. How to represent beliefs expressed by means of the attributes (variables) from the set A in terms of the attributes from the set B? We answer these questions now.\nMinimal extension. Suppose that to describe a domain we used attributes from the set A ⊂ X and as a result we obtained belief function BelA defined over the space Θ.A. After a time we decided to use larger set of attributes B ⊃ A. To translate the function BelA into the belief function BelB defined over the space Θ.B observe that using the attributes from the set A we are able to create m = ∏\ni∈A |Θi| (where |Θi| stands for the cardinality of the set Θi ) elementary propositions q of the form “Xi1 = xi1 and Xi2 = xi2 and . . . and Xir = xir” where Xij ∈ A, xij ∈ Θij and |A|= r. In other words the θ’s are the only atomic propositions of the language LA. Since the values of the attributes form the set B−A are unknown to a subject using the language LA then each atomic proposition θ ∈ Θ.A translates into the compound proposition θ × Θ.(B −A) ⊂ Θ.B. Similarly each A ⊆ Θ.A translates into the subset A×Θ.(B−A) of Θ.B. Thus the mass function mA characterizing the belief function BelA translates into the mass function mB of the form\nmB(B) =\n{\nmA(a) if B = A×Θ.(B−A), A ⊆ θ.A 0 otherwise\n(6)\nIn the sequel we shall denote the function mB derived from mA by means of the above procedure as m↑B A .\nMarginalization. Assume now that we reduce the set B of attributes to its subset A. Obviously each atomic proposition θ ∈ Θ.B points to a proposition ξ ∈ Θ.A if its projection onto Θ.A (defined as “dropping coordinates form B−A”) equals to ξ. Thus the mass function mB translates to a mass mA via the equation\nmA(A) = ∑ {mB(B) | B ⊆ Θ.B, proj(B) = A} (7)\nIn the sequel we shall denote the function mA derived from mB by means of the above procedure as m↓A B\n. Fusing evidence. When m1 and m2 are two separate and independent bodies of evidence defined on a common frame Θ, to arrive to the final conclusions we combine them by means of Dempster’s Rule of Combination (DRC for short). That is we crate new mass function\nm(A) = (m1 ⊕m2)(A) = κ · ∑ {m1(A ∪B) ·m1(A ∪ C) | A,B ⊆ (Θ −A), A ∩B = ∅} (8)\nwhere κ is a normalizing constants defined as κ−1 = 1−m(∅). Note that when m1 and m2 are categorical mass functions, i.e. m1(A) = 1 and m1(B) = 1 for some A,B ⊆ Θ then the rule produces categorical mass function with m(A ∩B) = 1. Hence DRC generalizes classical sets intersection.\nTo illustrate the notions introduced in this section assume that p and q are two propositional variables, and their frames Ξ and Λ are such that Ξ = Λ = {t, f} where t and f stand respectively for “true” and “false”. The logical implication can be described by the mass function mp→q on Θ = Ξ × Λ with one focal (i.e. positive) element mp→q({{f, f}, {f, t}, {t, t}}) = 1. Similarly the fact “p is true” is represented by the categorical mass function mp on the frame Ξ, that is mp({t}) = 1. To combine these two functions we must extend mp to the bpa on Θ: m ↑Θ p ({t, f}, {t, t}}) = 1. Now, combining mp→q with m ↑Θ p we obtain the bpa m of the form m({t, t}) = 1. Projecting this m onto the space Λ we state that m↓Λ({t}) = 1 what means that “q is true” what agrees with the standard logical inference pattern Modus Ponens . Note that if we combine mp→q with m-function representing statement “p is false” then the resulting function (after projection on the frame Λ) has the form m({{f}, {t}}), i.e. nothing can be said about the truth of the proposition q (what again agrees with classical logic). Similarly combining mp→q with the m function stating that “q is false” we obtain a counterpart of the Modus Tollens pattern.\nThis example shows that the formalism of belief functions perfectly agrees with the propositional calculus. In general, reasoning with categorical belief functions corresponds to the idea of using set valued description of variables to estimation and testing hypotheses - cf [31].\nConditioning. When Bel is a belief function over Θ and Belo is a categorical belief function focused on a subset A of Θ then the combination of Bel with Belo results in the conditional belief function Bel(· | A). Its properties are described in [32], [46] and [38]."
    }, {
      "heading" : "5.3 Reasoning under Uncertainty",
      "text" : "With the DRC the problem of reasoning can be stated as follows: Given a collection of unrelated pieces of evidence ε1, ε2, . . . , εm (each of which is translated to the appropriate belief function) we turn them into a single body of the form ε1∧ε2∧· · ·∧εm characterized by a belief function being the orthogonal sum the appropriate belief functions. Adding to the resulting body some findings which are possibly certain, we obtain a conditional belief function. This closely corresponds to the probabilistic reasoning where the bodies of evidence are represented by the (conditional) probabilities from which the joint probability distribution is derived; having this distribution we are looking for a distribution conditioned on a subset of Θ.\nTo formalize the problem observe that each body εi is represented as a belief function over a subset Hi ⊆ X . Denoting H the family of subsets Hi, i = 1 . . .m we are interested in a belief function\nBelX(· | e) = (⊕{Bel|H ∈ H}) ↓{X} (9)\nwhere e is a subset of Θ representing current findings. This corresponds to deriving a conditional probability distribution P (X | e) from the joint probability distribution P (X1, . . . , Xm) = ∏\n{P (Xi | Π(Xi) | i = 1 . . .m} given the set of observations (instantiated variables) e; here Π(Xi) is the set of immediate causes for Xi – see [27]. Note that under probabilistic setting the subsets Hi are defined as the sum of {Xi} and Π(Xi); further the operator ⊕ is replaced by the multiplication and the projection operator ↓{X} corresponds to the summation over all variables form the set X − {X}. Hence the equation (9) nicely summarizes both the probabilistic and strictly evidential problem of reasoning.\nBecause immediate derivation of BelX(· | e) is numerically very expensive, a number of procedures has been designed to this problem - cf. [27], [22], [36].\nThe simplest and more attractive procedure seems to be that of Shafer and Shenoy. It relies upon the observation that deleting a single variable Y from the set X results in a belief function (here X ′ = X − {Y })\nBel↓X ′ = (⊕{BelH | H ∈ H, Y 6∈ H})⊕ (⊕BelH | H ∈ H, Y ∈ H) ↓X ′\nand follows from the fact that (1) the operator ⊕ is commutative and associative, and (2) if BelA and BelB are two belief function defined over Θ.A and Θ.B, respectively, then (BelA ⊕ BelB) ↓A = BelA ⊕ Bel ↓(A∩B) B . Note that after performing the above\ncomputations the original family H reduces to new family H′ consisting of all H ’s such that Y 6∈ H , and the new set H ′ = ( ⋃\n{H | Y ∈ H}) − {Y }. In its very nature the procedure is closely related to the approach used by [2] in solving nonserial dynamic problems and it says that to find a margin over Θ.{X} it suffices to delete one by one variables from the set X −{X}, each time combining belief functions over relatively small frame being the sum of these H ’s which contain the currently removed variable. This last operation is allowed because the projection operator is such that (3) if I ⊂ J are two subsets of X then Bel↓I = (Bel↓J)↓I . The three properties (1), (2) and (3) can be viewed as the necessary and sufficient conditions for local computations described below- cf. [36].\nObviously, the numerical complexity of this new procedure hardly depends on the order in which the variables from the set X ′ are removed - see [2] for details. Shafer and Shenoy (1986) observed that an optimal order can be recovered if we arrange the original family H into a join tree – consult [24] – (Markov tree in their nomenclature) which has a nice property stating that when L ∈ H is a leaf node in the tree and P ∈ H is its parent node then the variables from the set L − P occurs only in the set L. (Of course when H is not an acyclic hypergraph it must be embedded in such a hypergraph first – see [48] for an appropriate algorithm).\nNow the procedure of finding Bel↓X ′\ncan be summarized as follows. Assume that H is organized into a join tree rooted on the node corresponding to the variable X . Each node in the tree can be imagined as a separate processor equipped with local information in the form of a component belief function (when a node was added to H in the process of the tree formation its local information has the form of vacuous belief function). Each node communicates with its neighbors by passing messages (belief functions) to the neighbors. There are two kinds op operations: a propagation operation which describes how messages are passed from node to node so that all of the local information is globally distributed, and a fusion rule which describes how the messages incoming to a node are combined to make marginal belief functions and outgoing messages. In other words propagation takes place along the edges of the tree and fusion takes place within the nodes. Both the operations can be formalized as follows. Let N(H ; T ) denote the set of all neighbors of the node H in the join tree T . The message which the node H sends to a neighbor H ′ equals to\nMH→H ′ = (BelH ⊕ (⊕{M G→H | G ∈ (N(H ; T )− {H ′})}))↓(H∩H ′) (10)\nThe operation in the parentheses corresponds to fusion and (·)↓(H∩H ′) represents propagation. Note that (10) allows for parallel belief updating. When a node received messages from all its neighbors, a marginal belief is computed\nBel↓H = BelH ⊕ (⊕{M H′→H | H ′ ∈ N(H ; T )}) (11)\nThis procedure with some modifications reducing the number of summations⊕, described in [47]., has been successfully implemented in our system."
    }, {
      "heading" : "5.4 Belief Revision",
      "text" : "The aim of belief revision is, according to [27] Ch. 5 to identify a composite set of propositions (one from each variable) which “best” explains the evidence at hand. Under probabilistic context this formalizes as follows. Given e, the set of instantiated variables (evidence), let x stand for an assignment of values to the variables in X consistent with e; such an x is said to be explanation, interpretation or extension of e. The most probable explanation (MPE) of the evidence at hand is such an extension xe which maximizes the conditional probability P (X = x | e). This task can be performed locally by letting each variable X in X compute the function\nβ(x | e) = max x’ {P (x, x’) | e} (12)\nwhere x’ stands for an explanation projected on the space Θ(X − {X}). That 12 can be computed locally follows from the fact that the pair (·,max) satisfies the requirements (1)-(3) specified in section 5.3. Note that in 12 the operation of summation – requested when computing conditional probabilities P (X | e) – was replaced by the maximum operator.\nWhen we use belief function the maximum operator appears both in the ⊕-summation and in the projection operator. Hence to fit the rule (9) to the formula (12) we must redefine both the operators. First, instead ⊕-summation we must define new ⊕ − max operator\n(m1⊕maxm2)(A) = max{m1(A∪C)·m2(A∪D) | C,D ⊆ (Θ−A), C∩D = ∅}(13)\nHere, contrary to the⊕-summation, normalization is unnecessary as the relative strengths are more important than the absolute values of commitment to a given set of propositions. Similarly, the projection operator takes now the form (we assume that A ⊂ B)\nm↓maxB A = max{mB(B) | B ⊆ Θ.B, proj(B) = A} (14)\nUsing these operators in equation (10) we obtain a message passing algorithm for belief updating. This algorithm can be used in two main modes described below.\nExplanatory mode. Rooting the join tree on a variable X we are searching for the instantiations of other variables which explain computed value ox X at best.\nHypothesizing. Again we root the tree on a variable X and we assume that X takes a value xo. Our algorithm allow to find the best explanation for such an assignment (i.e. we are searching an answer for the question “which settings of the remaining variables explain the condition “X = xo” at best?”)\nConditioning. We choose a group of variables, say ε, and we instantiate them to the values e ∈ Θ.ε. Now our algorithm allows to answer the question “what will happen if ε = e?”."
    }, {
      "heading" : "6 Viewing Belief Network as a Set of Rules",
      "text" : "A natural phenomenon of human expression of knowledge is a widespread usage of implications: most mathematical theorems are of the form ”if < premise > then < conclusion >”, though obviously they are equivalent to for example a CNF or DNF representation. Furthermore, humans use implication-like statements even if described phenomena fail to be deterministic, e.g. in describing course of chemical reactions, laws in social sciences etc. The term ”rule” for implication-like expression of knowledge found a wide-spread use in artificial intelligence, e.g. in MYCIN-like expert systems.\nTaking into account this phenomenon, also our group made an effort to find a way for describing the contents of a belief-network based knowledge bases (which is very different in nature from modular production rules based knowledge bases) in terms of rule-like constructs.\nThis transcription should support the user by helping him:\n– to understand the contents of the knowledge base – to formulate queries to the knowledge base – to enter and update the knowledge base and – to understand the justifications of the system for results of its reasoning\nThe last item has just been subject of the preceding sections. To understand better the the nature of the rule-like transcription let us first give the\ngeneral definition of a belief network:"
    }, {
      "heading" : "6.1 The Concept of a Belief Network",
      "text" : "We generalize here the definition of belief network from [12] (bayesian networks), while using the denotation of [36].\nDefinition 1. We define a mapping ⊙ : V V × V V → V V called decombination such that: if BEL12 = BEL1⊙BEL2 then BEL1 = BEL2 ⊙BEL12.\nIn case of probabilities, decombination means memberwise division:\nPr12(A) = Pr1(A)/Pr2(A).\nIn case of DS belief functions, let us remaind that the commonality function Q is defined as Q(A) = ∑\nB;A⊆B m(B). Hence for Bel12 = Bel1⊕Bel2 then Q12(A) = c·Q1(A)·Q2(A) (c - normalizing factor). Let us remaind also that by a pseudo-belief function a function over powerset is understood which differs from the proper belief function by the fact,\nthat also negative mass function (m) values are allowed, but only to such extent that the commonality function Q remains non-negative. Then let us define the operator ⊖ as yieldiing a DS pseudo-belief function such that: whenever Bel12 = Bel1 ⊖Bel2 then\nQ12(A) = c ·Q1(A)/Q2(A)\n(c - normalizing constant) for non-zero Q2(A). It is easy to check that such a function always exists. Obviously, Bel1 = Bel1 ⊕ Bel12. The operator ⊖ means then the decombination of two DS belief functions. Both for probabilities and for DS belief functions decombination may be not uniquely determined. Moreover, for DS belief functions not always a decombined DS belief function will exist. Hence we extend the domain to DS pseudo-belief functions which is closed under this operator. We claim here without a proof (which is simple) that DS pseudo-belief functions fit the axiomatic framework of Shenoy/Shafer. Also, we claim that if an (ordinary) DS belief function is represented by a factorization in DS pseudo-belief functions, then any propagation of uncertainty yields the very same results as when it would have been factorized into ordinary DS belief functions. Let us define now the concept of pseudo-conditioning.\nDefinition 2. By pseudo-conditioning | of a belief function BEL on a set of variables h we understand the transformation: BEL|h = BEL⊙BEL↓h.\nNotably, pseudo-conditioning means in case of probability functions proper conditioning. In case of DS pseudo-belief functions the operator | has meaning entirely different from traditionally used notion of conditionality - pseudo-conditioning is a technical term used exclusively for valuation of nodes in belief networks. Notice: some other authors e.g. [3] recognized also the necessity of introduction of two different notions in the context of the Shenoy/Shafer axiomatic framework (compare a priori and a posteriori conditionals in [3]). [3] introduces 3 additional axioms governing the ’a priori’ conditionality to enable propagation with them. Our pseudo-conditionality is bound only to the assumption of executability of the ⊙ operation and does not assume any further properties of it. We will discuss the consequences of this difference elsewhere. Let us define now the general notion of belief networks:\nDefinition 3. A belief network is a pair (D,BEL) where D is a dag (directed acyclic graph) and BEL is a belief distribution called the underlying distribution. Each node i in D corresponds to a variable Xi in BEL, a set of nodes I corresponds to a set of variables XI and xi, xI denote values drawn from the domain of Xi and from the (cross product) domain of XI respectively. Each node in the network is regarded as a storage cell for any distribution BEL↓{Xi}∪Xπ(i)|Xπ(i) where Xπ(i) is a set of nodes corresponding to the parent nodes π(i) of i. The underlying distribution represented by a belief network is\ncomputed via:\nBEL = n ⊙\ni=1\nBEL↓{Xi}∪Xπ(i)|Xπ(i)\nThe notion of belief network just introduced possesses several important characteristics:\n– The relationship between the global belief function BEL and its constituting factors BEL↓{Xi}∪Xπ(i)|Xπ(i) is local: this means that having an empirical model of BEL and knowing the structure D of he distribution BEL we can estimate factors BEL↓{Xi}∪Xπ(i)|Xπ(i) separately for each one by projection of the empirical model onto the subset {Xi} ∪Xπ(i) of variables. – Removal of a leave node from D (and hence the respective factor from the above ”product”) is equivalent to projection of BEL onto the space spanned by variables associated with the remaining odes: this enables to apply the technique of uncertainty propagation by edge reversals. – A belief network reflects causal dependencies among variables (by directions of arrows) – We can reason about conditional independence of disjunctive sets of variables given another set of variables using only graphical properties of dag-representation D and without referring to numerical properties of the underlying distribution.\nIn case of probabilistic distributions the first to properties are obvious. The last property has been studied very carefully for probabilistic belief networks by Geiger, Verma and Pearl [12]. It is easily checked that the defining formula for the underlying distribution in probabilistic case reduces to:\nP (x1, ..., xn) = n ∏\ni=1\nP (xi|xπ(i))\nIn their paper they introduce the notion of d-separation (implying conditional independence) as follows:\nDefinition 4. [12] A trail in a dag is a sequence of links that form a path in the underlying undirected graph. A node β is called a head-to-head node with respect to a trail t if there are two consecutive links α → β and β ← γ on that t.\nDefinition 5. [12] A trail t connecting nodes α and β is said to be active given a set of nodes L, if (1) every head-to-head-node wrt t either is or has a descendent in L and (2) every other node on t is outside L. Otherwise t is said to be blocked (given L).\nDefinition 6. [12] If J,K and L are three disjoint sets of nodes in a dag D, then L is said to d-separate J from K, denoted I(J,K|L)D iff no trail between a node in J and a node in K is active given L.\nIt has been shown in [13] that\nTheorem7. [13] Let L be a set of nodes in a dag D, and let α, β /∈ L be two additional nodes in D. Then α and β are connected via an active trail (given L) iff α and β are connected via a simple (i.e. not possessing cycles in the underlying undirected graph) active trail (given L).\nDefinition 8. [12] If XJ , XK , XL are three disjoint sets of variables of a distribution P, thenXJ , XK are said to be conditionally independent givenXL (denoted I(XJ , XK |XL)P iff P (xJ , xK |xL) = P (xJ |xL).P (xK |xL) for all possible values of XJ , XK , XL for which P (xL) > 0. I(XJ , XK |XL)P is called a (conditional independence) statement\nTheorem9. [12] Let PD = {P |(D,P) is a Bayesian network}. Then:\nI(J,K|L)D iff I(XJ , XK |XL)P for all P ∈ PD.\nThe ”only if” part (soundness) states that whenever I(J,K|L)D holds in D, it must represent an independence that holds in every underlying distribution.\nThe ”if” part (completeness) asserts that any independence that is not detected by d-separation cannot be shared by all distributions in PD and hence cannot be revealed by non-numeric methods.\nIn case of DS belief functions, our defining equation for the underlying distribution of a belief network has the form:\nBel = n ⊕\ni=1\nBel↓{Xi}∪Xπ(i)|Xπ(i)\nIt is easily seen that the first two properties hold for DS belief networks as defined in this paper. The notion of d-separation and its relationship with conditional independence are easily transferred onto DS belief networks.This definition differs significantly from what is generally considered to be a DS belief network (e.g. in [36]). Usually, a DS belief network is considered to have the defining equation of the form:\nBel =\nn ⊕\ni=1\nBeli\nwhere Beli is a DS belief function in some subset of variables of Bel without any assumption of the nature of Beli. The deviation from traditional approach seems to be significant one and requires some explanation. First of all the traditional DS belief network has in general none of the above-mentioned three properties hold. The advantages of\nour DS belief network definition are obvious. The question remains whether there are any disadvantages. Ones connected with uncertainty propagation within the Shenoy/Shafer axiomatic framework [36] is of particular interest for implementation of this expert system.\nShenoy and Shafer [36] consider it unimportant whether or not the factorization should refer to conditional probabilities in case of probabilistic belief networks. We shall make at this point the remark that for expert system inference engine it is of primary importance how the contents of the knowledge base should be understood by the user as beside computation an expert system is expected at least to justify its conclusions and it can do so only referring to elements of the knowledge base. So if a belief network (or a hypergraph) is to be used as the knowledge base, as much elements as possible have to refer to experience of the user.\nIn our opinion, the major reason for this remark of Shenoy and Shafer is that in fact the Dempster-Shafer belief function cannot be decomposed in terms of the traditional conditional belief function as defined in the previous section 5. This can only be done if a pseudo-conditioning like ours is introduced. But an intriguing question remains whether the traditional DS belief networks extend essentially the class of DS belief functions suitable for Shenoy/Shafer propagation of uncertainty. The sad result that really\nTheorem 10. [20] Traditional DS belief network induced hypergraph (as considered by Shenoy and Shafer [36]) may for a given joint belief distribution have simpler structure than (be properly covered by) the closest hypergraph induced by a new DS belief network (as defined above.)\nThis fact, however, is compensated completely by another one. Shenoy/Shafer propagation does not run in hypergraphs, but in hypertrees. And:\nTheorem 11. [20] No traditional DS belief network induced hypertree (as considered by Shenoy and Shafer [36]) may for a given joint belief distribution have simpler structure than (be properly covered by) the closest hypertree induced by a new DS belief network (as defined above.)\nThis fully justifies, in our opinion, the usage of the new belief network definition."
    }, {
      "heading" : "6.2 Understanding a Belief Network in Terms of Rules",
      "text" : "A dag structure of a belief network was presented in Fig.1. Let us consider a fragment of a belief network in Fig.2 consisting of a node (and the associated variable, or attribute, or feature) Y and all of its parents: X and Z. Let the values of them range for Y: {y1, y2, y3},\nfor X:{x1, x2}, for Z:{z1, z2, z3, z4}, resp.\nThen a table of valuations will be associated with the node Y. In probabilistic case the valuation will take the form given in table 1.\nIn general, it will be an exhaustive table of conditional probabilities of each value of a node given each value combination of its parents. In case of Dempster-Shafer valuation, only some (that is focal point) conditional valuations will be given, e.g. as in table 2.\nEach table of this form may be displayed for presentation purposes as a ”beam” of rules: in probabilistic case:\nIF X = x1 AND Z = z1 THEN Y = y1 WITH p111 IF X = x1 AND Z = z1 THEN Y = y2 WITH p112 IF X = x1 AND Z = z1 THEN Y = y3 WITH p113 IF X = x2 AND Z = z1 THEN Y = y1 WITH p211 . . . . . . . . . IF X = x2 AND Z = z4 THEN Y = y3 WITH p243 and in Dempster-Shafer case for the valuation in table 2: IF X=x1 AND Z=z1 THEN Y=y1 WITH Q1 IF X=x1 AND Z=z1 THEN Y=y2 AND IF X=x1 AND Z=z3 THEN Y=y2 WITH Q2 IF X=x2 AND Z=z2 THEN Y=y3 WITH Q3 IF X=x1 AND Z=z4 THEN Y=y1 AND IF X=x1 AND Z=z4 THEN Y=y3 AND IF X=x2 AND Z=z4 THEN Y=y1 AND IF X=x2 AND Z=z4 THEN Y=y3 WITH Q4\nwhile measures Q (commonality) are calculated according to the principles of the Dempster-Shafer Theory [32].\nFirst of all notice that we are talking always about a ”beam of rules” and not of a single rule - this was rarely considered in production rule systems.\nIn probabilistic case, it should be clear from the very beginning what is meant by a single rule (of the beam): the certainty factor after the word ”with” expresses the proportion of cases fitting the ”rule” among those meeting the premise. At the same time, the above examples exhaust the richness of the language needed for presenting contents of the knowledge base to the user.\nThe Dempster-Shafer case is more subtle. Here, all the variables (X,Y,Z) are treated as potentially set-valued, and the equality sign in the expressions should be rather treated as ”is containing” and not as set equality sign. The certainty factor after the word ”with” expresses again the proportion of cases fitting the ”rule” among those meeting the premise. At the same time, the above examples exhaust the richness of the language needed for presenting contents of the knowledge base to the user.\nThe representation language for knowledge base has not only syntactic restrictions,\nbut also semantica ones: within a rule, only direct predecessors of a given node may occur in the premise part of a rule."
    }, {
      "heading" : "6.3 Rule-oriented Queries to the Knowledge Base",
      "text" : "A knowledge base in belief network notation means something more than just the sum of rules. The built-in system of uncertainty propagation can for the given network give the level of probability (belief) for more general formulae without any restrictions provided by network topology:\n– for logical expressions (with operators AND, OR, NOT) in atomic formulae of the form <variable> = <value>. - the system may deduce what is the unconditional probability/belief of such a formulae to hold, - the same question may be also answered conditionally: what is the probability/belief of the expression given some of the variables of the network have restricted value ranges, - the same question may be also answered conditionally: what is the probability/belief of the expression given some constraints expressed in terms of logical expressions are imposed onto the network,\n– for beams of rules of the type IF attr1=? AND attr2=? AND attr3=? ... THEN attr=? - the complete beam can be inferred unconditionally, or conditionally or for a given set of constraints as described for logical expressions.\nFor probabilistic case, the methods of implementation of the above query-answering is described in [27]. Implementation for Dempster-Shafer Theory runs along the same lines. Let us consider as an example the network in Fig.1. Let us assume that each of the nodes p1,...,p8 takes only one of two values: ’y’ and ’n’.\nWe intend to ask the question: p5 = ′ y′.or.p1 = ′ n′. The belief network is to be amended temporarily by a node x1, taking values ’y’ i ’n’, which we connect with p5 and p1 (see Fig.3). With this node, we associate the rules: IF p5 = ′ t′ AND p1 = ′ n′ THEN x1 =\n′ t′WITH 1. IF p5 = ′ t′ AND p1 = ′ t′ THEN x1 =\n′ t′WITH 1. IF p5 = ′ n′ AND p1 = ′ n′ THEN x1 =\n′ t′WITH 1. IF p5 = ′ n′ AND p1 = ′ t′ THEN x1 =\n′ n′WITH 1. and for the remaining rules the probability is equal 0.\nThe answer is the final valuation of the node x1 after uncertainty propagation.\nNow let’s turn to checking validity of the query of the form:\nx3 : if (p5 = ′ t′ .or. p1 = ′ n′) .and. p3 = ′ n′ then p6 =′ t′\nwith respect to contents of the knowledge base. Node x1 connected with p5 and p1, node x2 connected with p3 and p1 and node x3 connected with p6 and p1 (see Fig.4). Let nodes x1 and x2 take values ’y’ or ’n’, and the node x3 - ’y’, ’n’, ’?’. Node x1 is associated with a beam of rules as mentioned above, and x2 - with: IF p3 = ′ t′ AND x1 = ′ t′ THEN x2 =\n′ t′WITH 1. IF p3 = ′ t′ AND x1 = ′ n′ THEN x2 =\n′ n′WITH 1. IF p3 = ′ n′ AND x1 = ′ t′ THEN x2 =\n′ n′WITH 1. IF p3 = ′ n′ AND x1 = ′ n′ THEN x2 =\n′ n′WITH 1. with the remaining rule probabilities equal 0. Node x3 is associated with rule beam: IF p6 = ′ t′ AND x2 = ′ t′ THEN x3 =\n′ t′WITH 1. IF p6 = ′ t′ AND x2 = ′ n′ THEN x3 =\n′ n′WITH 1. IF p6 = ′ n′ AND x2 = ′ t′ THEN x3 =\n′?′WITH 1. IF p6 = ′ n′ AND x2 = ′ n′ THEN x3 = ′?′WITH 1.\nwith the remaining rule probabilities equal 0.\nThe final answer is the valuation of the node x3 after uncertainty propagation. The probability of x3 =\n′ t′ indicates how frequently the rule fires and is correct,probability of the event x3 =\n′?′ indicates how often the rule did not fire (and only for this reason is considered by logicians to be true). probability of x3 =\n′ n′ indicates, how frequently the rule fires, but is in error. (The respective 3-valued logic is considered in [19]."
    }, {
      "heading" : "6.4 Belief Network Construction",
      "text" : "To construct a belief network correctly, one should:\n– properly uncover the causal structure governing the attribute, and thereafter – calculate the valuations of each of the node of the network.\nFor the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].\nNode valuation can be also calculated from data [5] as relative conditional frequency. The Dempster-Shafer case is more difficult as clear relationship between data and the joint belief distribution is still subject of disputes. Set this issue aside, proper Q-value quotients may be exploited.\nHowever, the knowledge base may be only partially reflected by data. Thenm there emerges the necessity to manipulate manually the knowledge base. The user is then requested to enter a consistent description of the network structure. Given this, the entrance of the valuations may be run precisely following the vary same pattern as the contents of the knowledge base are presented to the user."
    } ],
    "references" : [ {
      "title" : "Learning with CASTLE, Symbolic and Quantitative Approaches",
      "author" : [ "S. Acid", "L.M. deCampos", "A. Gonzales", "R. Molina", "N. Perez de la Blanca" ],
      "venue" : "Lecture Notes In Computer Science",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1991
    }, {
      "title" : "Nonserial Dynamic Programming",
      "author" : [ "U. Bertelè", "F. Brioschi" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1972
    }, {
      "title" : "An axiomatic framework for propagating uncertainty in directed acyclic networks, International",
      "author" : [ "J. Cano", "M. Delgado", "S. Moral" ],
      "venue" : "Journal of Approximate Reasoning",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1993
    }, {
      "title" : "Approximating discrete probability distributions with dependence trees",
      "author" : [ "C.K. Chow", "C.N. Liu" ],
      "venue" : "IEEE Transactions on Information Theory , Vol. IT-14,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1968
    }, {
      "title" : "Consistency of an estimate of tree-dependent probability distribution",
      "author" : [ "C.K. Chow", "T.J. Wagner" ],
      "venue" : "IEEE Transactions on Information Theory ,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1973
    }, {
      "title" : "Statistical Expert Systems — A Special Application Area for KnowledgeBased Computer Methodology",
      "author" : [ "S.I. Chowdhury" ],
      "venue" : "Technical Report No. 104,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1987
    }, {
      "title" : "A Bayesian method for the induction of probabilistic networks from data, Machine Learning",
      "author" : [ "G.F. Cooper", "E. Herskovits" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1992
    }, {
      "title" : "Expert Systems and Statistics, Proceedings SEAS Spring Meeting 1984, Vol.II, pp.529-538",
      "author" : [ "P. Darius" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1984
    }, {
      "title" : "New methods for reasoning towards posterior distributions based on sample data",
      "author" : [ "A.P. Dempster" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1966
    }, {
      "title" : "Upper and lower probabilities induced by a multivalued mapping",
      "author" : [ "A.P. Dempster" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1967
    }, {
      "title" : "d-Separation: From theorems to algorithms, M.Henrion, R.D.Shachter, L.N.Kamal, J.F.Lemmer (Eds): Uncertainty in Artificial Intelligence",
      "author" : [ "D. Geiger", "T Verma", "J. Pearl" ],
      "venue" : "Elsevier Science Publishers B.V. (North-Holland),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1990
    }, {
      "title" : "Expert Systems in Statistics, The Knowledge Engineering",
      "author" : [ "D.J. Hand" ],
      "venue" : "Review, 1,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1986
    }, {
      "title" : "A Statistical Knowledge Enhancement System",
      "author" : [ "D.J. Hand" ],
      "venue" : "Journal of the Royal Statistical Society, Series A, 150,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1987
    }, {
      "title" : "Emergent Themes in Statistical Expert Systems, in Knowledge, Data, and Computer Assisted Decisions, M",
      "author" : [ "D.J. Hand" ],
      "venue" : "Schader and W. Gaul (Editors), Springer-Verlag, Berln,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1990
    }, {
      "title" : "Statistical Expert Systems, in Statistical and Scientific Databases",
      "author" : [ "D.J. Hand", "R. Cubitt" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1991
    }, {
      "title" : "Computational aspects of the Mobius transform of a graph",
      "author" : [ "R. Kennes" ],
      "venue" : "Technical Report TR/IRIDIA/90-13, IRIDIA,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1990
    }, {
      "title" : "Beliefs in Markov Trees - From Local Computations to Local Valuation",
      "author" : [ "M.A. K lopotek" ],
      "venue" : "World Scientific Singapore,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1992
    }, {
      "title" : "Multivariate belief functions and graphical models",
      "author" : [ "A. Kong" ],
      "venue" : "Doctoral Dissertation,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1986
    }, {
      "title" : "Fast manipulation of probabilities with local representations - with applications to expert systems (with discussion)",
      "author" : [ "D.J. Lauritzen", "S.L. Spiegelhalter" ],
      "venue" : "J. Roy. Statist. Soc.,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1988
    }, {
      "title" : "Evidential reasoning: An implementation for multisensor integration",
      "author" : [ "J.D. Lowrance", "T.D. Garvey" ],
      "venue" : "Tech. Rep. 307,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1982
    }, {
      "title" : "The Theory of Relational Databases",
      "author" : [ "D. Maier" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1983
    }, {
      "title" : "Automated Rules Generation",
      "author" : [ "M. Michalewicz", "Z. Michalewicz" ],
      "venue" : "Proceedings of the Sixth International Symposium on Methodologies for Intelligent Systems, Charlotte,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1991
    }, {
      "title" : "Statistical-Expert System for Data Analysis and Knowledge Acquisition, in Statistical and Scientific Databases",
      "author" : [ "M. Michalewicz", "S.T. Wierzchoń", "E. Syropiatko", "A. Pacan", "A. Matuszewski", "M. K lopotek" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1991
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Influence",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1988
    }, {
      "title" : "Induction of Decision Trees",
      "author" : [ "J.R. Quinlann" ],
      "venue" : "Machine Learning, Vol.1,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1986
    }, {
      "title" : "Understanding evidential reasoning",
      "author" : [ "Ruspini", "E.H" ],
      "venue" : "Intn. J. Approximate Reasoning,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1992
    }, {
      "title" : "The recovery of causal poly-trees from statistical data, w Uncertainty in Artificial Intelligence",
      "author" : [ "G. Rebane", "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1989
    }, {
      "title" : "Uncertain Dynamic Systems. Prentice-Hall, New Jersey",
      "author" : [ "F.C. Schweppe" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1973
    }, {
      "title" : "A Mathematical Theory of Evidence",
      "author" : [ "G. Shafer" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1976
    }, {
      "title" : "Axioms for probability and belief function propagation",
      "author" : [ "G. Shafer", "P. Shenoy" ],
      "venue" : "Uncertainty in Artificial Intelligence",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1990
    }, {
      "title" : "Axioms for probability and belief-function propagation, [w",
      "author" : [ "P.P. Shenoy", "G. Shafer" ],
      "venue" : "Uncertainty in Artificial Intelligence",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1990
    }, {
      "title" : "Non-Standard Logics for Automated Reasoning",
      "author" : [ "P. Smets" ],
      "venue" : "Belief functions",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1988
    }, {
      "title" : "Causality from probability, [w:] G.McKee (Ed.):Evolving knowledge in natural and artificial intelligence",
      "author" : [ "P. Spirtes", "C. Glymour", "R. Scheines" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1990
    }, {
      "title" : "Automated construction of sparse Bayesian networks from unstructured probabilistic models and domain information, Henrion M., Shachter R.D.,Kanal L.N",
      "author" : [ "S. Srinivas", "S. Russel", "A. Agogino" ],
      "venue" : "Lemmer J.F.: Uncertainty in Artificial Intelligence",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1990
    }, {
      "title" : "OommenB.J.: On using the chi-squared statistics for determining statistic dependence",
      "author" : [ "R.S. Valiveti" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1992
    }, {
      "title" : "Equivalence and synthesis of causal models",
      "author" : [ "T.S. Verna", "J. Pearl" ],
      "venue" : "Proc. of the Conference on Uncertainty in Artificial Intelligence (pp. 220-227),",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1990
    }, {
      "title" : "Prior envelopes based on belief functions",
      "author" : [ "L.A. Wasserman" ],
      "venue" : "Ann. Stat.,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1990
    }, {
      "title" : "An Inference rule based on",
      "author" : [ "S.T. Wierzchon" ],
      "venue" : "Sugeno measure. In: J.C. Bezdek (ed.) Analysis of Fuzzy Information, CRC Press,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 1987
    }, {
      "title" : "Representation and processing of uncertain information in the Dempster-Shafer framework",
      "author" : [ "S.T. Wierzchon" ],
      "venue" : null,
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1993
    }, {
      "title" : "Constraint propagation over restricted space of configurations",
      "author" : [ "S.T. Wierzchon" ],
      "venue" : "Advances in the Dempster-Shafer Theory of Evidence,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "The purpose of such database is to derive or infer facts; statistics is helpful in extracting patterns from quantative (and qualitative) data [6].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : "However, they do not inform the user about underlying assumptions, seldom warn against obvious misuse, do not provide guidance in the process of analysis, and hardly assist in the interpretation of the results [8].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 11,
      "context" : "Clearly, there is a need for statistical expert systems [11], [14], [15], [16], which would include the statistical expertise.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "Clearly, there is a need for statistical expert systems [11], [14], [15], [16], which would include the statistical expertise.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "Clearly, there is a need for statistical expert systems [11], [14], [15], [16], which would include the statistical expertise.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "In [17] the authors provide a general characteristic of statistical expert systems.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "Such experimental information system (called SEAD for Statistical and Expert system for Analysis of Data) was recently implemented in the Institute of Computer Science, Polish Academy of Sciences [26].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 24,
      "context" : "They are create “long” conjunctions of the pairs (attribute, value), which precisely classify all data sets [28].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "This is based on (1) an optimization process defined on the set of discrete values of a given attribute and (2) a process of dividing a continous domain of an attribute into subsets; such optimal divisions are converted into the set of classification rules [25]",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 19,
      "context" : "The term evidential reasoning coined by Lowrance and Garvey in [23] covers a set of techniques designed for processing and reasoning from row data (evidence) which are currently accessible to an agent.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "Shafer in his monograph [32] and was highly influenced by the works of A.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "Dempster [9, 10].",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "Dempster [9, 10].",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 25,
      "context" : "To give a concise rationale for this theory (see [29]) for deeper discussion) we should note that the information required to understand the current state of the world, or to solve a real problem, comes from multiple sources, like real-time sensor data, accumulated domain knowledge and current contextual information.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 36,
      "context" : "(The case of continuous variable is considered by [33] or [44].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 38,
      "context" : "Under the general setting X constitutes so-called random set [47].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 28,
      "context" : "Shafer [32] proposes further formulas enabling to express one set function in terms of other set function and fast algorithms for doing this task are reported in [18].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "Shafer [32] proposes further formulas enabling to express one set function in terms of other set function and fast algorithms for doing this task are reported in [18].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 27,
      "context" : "In general, reasoning with categorical belief functions corresponds to the idea of using set valued description of variables to estimation and testing hypotheses - cf [31].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 28,
      "context" : "Its properties are described in [32], [46] and [38].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 37,
      "context" : "Its properties are described in [32], [46] and [38].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 31,
      "context" : "Its properties are described in [32], [46] and [38].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 23,
      "context" : "m} given the set of observations (instantiated variables) e; here Π(Xi) is the set of immediate causes for Xi – see [27].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 23,
      "context" : "[27], [22], [36].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[27], [22], [36].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 30,
      "context" : "[27], [22], [36].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 1,
      "context" : "In its very nature the procedure is closely related to the approach used by [2] in solving nonserial dynamic problems and it says that to find a margin over Θ.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "[36].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "Obviously, the numerical complexity of this new procedure hardly depends on the order in which the variables from the set X ′ are removed - see [2] for details.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "Shafer and Shenoy (1986) observed that an optimal order can be recovered if we arrange the original family H into a join tree – consult [24] – (Markov tree in their nomenclature) which has a nice property stating that when L ∈ H is a leaf node in the tree and P ∈ H is its parent node then the variables from the set L − P occurs only in the set L.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 39,
      "context" : "(Of course when H is not an acyclic hypergraph it must be embedded in such a hypergraph first – see [48] for an appropriate algorithm).",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 38,
      "context" : "This procedure with some modifications reducing the number of summations⊕, described in [47].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 23,
      "context" : "The aim of belief revision is, according to [27] Ch.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "We generalize here the definition of belief network from [12] (bayesian networks), while using the denotation of [36].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 30,
      "context" : "We generalize here the definition of belief network from [12] (bayesian networks), while using the denotation of [36].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "[3] recognized also the necessity of introduction of two different notions in the context of the Shenoy/Shafer axiomatic framework (compare a priori and a posteriori conditionals in [3]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] recognized also the necessity of introduction of two different notions in the context of the Shenoy/Shafer axiomatic framework (compare a priori and a posteriori conditionals in [3]).",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "[3] introduces 3 additional axioms governing the ’a priori’ conditionality to enable propagation with them.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "The last property has been studied very carefully for probabilistic belief networks by Geiger, Verma and Pearl [12].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "[12] A trail in a dag is a sequence of links that form a path in the underlying undirected graph.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] A trail t connecting nodes α and β is said to be active given a set of nodes L, if (1) every head-to-head-node wrt t either is or has a descendent in L and (2) every other node on t is outside L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] If J,K and L are three disjoint sets of nodes in a dag D, then L is said to d-separate J from K, denoted I(J,K|L)D iff no trail between a node in J and a node in K is active given L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] If XJ , XK , XL are three disjoint sets of variables of a distribution P, thenXJ , XK are said to be conditionally independent givenXL (denoted I(XJ , XK |XL)P iff P (xJ , xK |xL) = P (xJ |xL).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] Let PD = {P |(D,P) is a Bayesian network}.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "in [36]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "Ones connected with uncertainty propagation within the Shenoy/Shafer axiomatic framework [36] is of particular interest for implementation of this expert system.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 30,
      "context" : "Shenoy and Shafer [36] consider it unimportant whether or not the factorization should refer to conditional probabilities in case of probabilistic belief networks.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "[20] Traditional DS belief network induced hypergraph (as considered by Shenoy and Shafer [36]) may for a given joint belief distribution have simpler structure than (be properly covered by) the closest hypergraph induced by a new DS belief network (as defined above.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[20] Traditional DS belief network induced hypergraph (as considered by Shenoy and Shafer [36]) may for a given joint belief distribution have simpler structure than (be properly covered by) the closest hypergraph induced by a new DS belief network (as defined above.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : "[20] No traditional DS belief network induced hypertree (as considered by Shenoy and Shafer [36]) may for a given joint belief distribution have simpler structure than (be properly covered by) the closest hypertree induced by a new DS belief network (as defined above.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[20] No traditional DS belief network induced hypertree (as considered by Shenoy and Shafer [36]) may for a given joint belief distribution have simpler structure than (be properly covered by) the closest hypertree induced by a new DS belief network (as defined above.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 28,
      "context" : "while measures Q (commonality) are calculated according to the principles of the Dempster-Shafer Theory [32].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : "For probabilistic case, the methods of implementation of the above query-answering is described in [27].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 26,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 32,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 33,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 34,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 35,
      "context" : "For the probabilistic case, a number of respective causal-structure-from-data reconstruction algorithms have been elaborated [1], [4], [5], [7], [13], [30], [39], [40], [41], [42], [43], [45].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "Node valuation can be also calculated from data [5] as relative conditional frequency.",
      "startOffset" : 48,
      "endOffset" : 51
    } ],
    "year" : 2017,
    "abstractText" : "In this paper we present a methodology and discuss some implementation issues for a project on statistical/expert approach to data analysis and knowledge acquisition. We discuss some general assumptions underlying the project. Further, the requirements for a user-friendly computer assistant are specified along with the nature of tools aiding the researcher. Next we show some aspects of belief network approach and Dempster-Shafer (DST) methodology introduced in practice to system SEAD. Specifically we present the application of DS methodology to belief revision problem. Further a concept of an interface to probabilistic and DS belief networks enabling a user to understand the communication with a belief network based reasoning system is presented",
    "creator" : "dvips(k) 5.996 Copyright 2016 Radical Eye Software"
  }
}