{
  "name" : "1611.10228.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Behavior-Based Machine-Learning: A Hybrid Approach for Predicting Human Decision Making",
    "authors" : [ "Gali Noti", "Effi Levi", "Yoav Kolumbus", "Amit Daniely" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n10 22\n8v 1\n[ cs\n.L G\n] 3\n0 N\nIn this paper we present our approach for predicting human decision behavior: we suggest to use machine learning algorithms with features that are based on well-established behavioral theories. The basic idea is that these psychological features are essential for the representation of the data and are important for the success of the learning process. We implement a vanilla model in which we train SVM models using behavioral features that rely on the psychological properties underlying the competition baseline model. We show that this basic model captures the 14 choice biases and outperforms all the other learning-based models in the competition. The preliminary results suggest that such hybrid models can significantly improve the prediction of human decision making, and are a promising direction for future research.\n∗Rachel & Selim Benin School of Computer Science & Engineering and Federmann Center for the Study of Rationality, The Hebrew University of Jerusalem, Israel.\n†Rachel & Selim Benin School of Computer Science & Engineering, The Hebrew University of Jerusalem, Israel.\n‡Racah Institute of Physics, The Hebrew University of Jerusalem, Israel. §Rachel & Selim Benin School of Computer Science & Engineering, The Hebrew Uni-\nversity of Jerusalem, Israel; and Google."
    }, {
      "heading" : "1 Introduction",
      "text" : "It is well known that the classic economic theory, which considers perfectly rational decision makers, generally fails to predict human decisions. A large body of work in psychology and behavioral economics attempts to capture the way humans make decisions. These attempts typically address behavior patterns that consistently emerge in a specific setting, but often these patterns are different, or even contradicting, when the setting is changed. For instance, in risky choice problems, when the decision is based solely on the description of the problem people tend to overweight rare events [5], while in decisions based on experience they tend to underweight rare events (see review of the experience-description gap in [4]).\nRecently, researchers were challenged in the Choice Prediction Competition (CPC) 2015 [2] to propose a model that will be general enough to predict human decisions for a wide space of problems. The competition focused on 14 well-studied choice anomalies, which included the four biases captured by prospect theory [5] (including overweighting of rare events), and additional 10 biases that were documented in studies of decisions with and without feedback (including underweighting of rare events in repeated settings with feedback). The organizers, Erev et al. (2015), defined a problem space for which they showed that these anomalies emerge (in a series of experiments). The challenge for the competitors was to develop a model that would best predict the decisions made by humans in any problem in this problem space. For the full description of the competition see [2] and the competition website. Next we briefly describe the main details. The Problem Space: The organizers defined an 11-dimensional problem space of choice tasks that is wide enough to give rise to the 14 choice anomalies discussed above. Denote the problem space by PS = LA×HA× pHA×LB×HB×pHB×LotNum×LotShape×Corr×Amb×FB. Each problem x ∈ PS is a choice between two gamble options A and B. Option A provides the high outcome HA with probability pHA, and the low outcome LA otherwise. Option B provides with probability pHB, an outcome of a lottery with an expected value HB, or LB otherwise. LotNum defines the number of possible outcomes in the lottery and LotShape defines whether the lottery’s distribution is symmetric, right-skewed, or left-skewed around its mean (or is undefined if LotNum = 1). Corr determines the sign of the correlation between the payoffs of the two options.\nThe 10th dimension, Ambiguity (Amb), defines the precision of the initial information the decision maker receives regarding the probabilities of the outcomes in Option B. The competition focused on the two extreme cases: Amb = 0 implies complete information (i.e., no ambiguity), and Amb =\n1 implies that the decision maker does not receive any initial information regarding option B’s probabilities. The 11th dimension is the feedback to the decision maker and was studied within a problem: subjects repeatedly faced each problem, first without any feedback and then with complete feedback. Data: Prior to the competition announcement, the organizers performed two preliminary experiments. In each experiment, human participants were presented with a set of decision problems drawn from the problem space. Participants faced each problem for 25 trials. In the first five trials they did not receive any feedback, while following each of the remaining 20 trials they received full feedback of the obtained and forgone outcomes. The 25 trials were divided to five blocks, each consisting of five consecutive trials, and the average portion of option B choosers (the B-rate) was recorded for each block. In the first experiment, 30 hand-tailored problems in the problem space were used for replicating the emergence of the 14 choice anomalies mentioned above. The second experiment used 60 problems that were randomly generated from the problem space, to show the robustness of the anomalies. The Challenge: Given the parameters and B-rates for these 90 decision problems (the estimation set), the competitors were asked to provide a model (in the form of a program), which receives a problem represented by the first 10 parameters described above1 and outputs five B-rates, one for each block. The model must capture all 14 choice anomalies (represented by a published script containing 14 tests which the model should pass), and should be easy and straight-forward to implement (from a description of at most 1500 words).\nTo evaluate the models, the organizers ran a third experiment using 60 new randomly generated decision problems. The results of this experiment were used for evaluating the competitors’ models (the test set), using the Mean Squared Deviation (MSD) score. For a set of observations OBSj and a set of predictions Bj for problems j = 1..N :\nMSD(OBS,B) = 100 · 1\nN\n∑\nj\n(OBSj − Bj) 2 (1)\nThe final evaluation metric is the MSD averaged over all five blocks. Our Approach: The common approach towards solving this problem in behavioral fields is to derive patterns from behavior observed in experiments (in some setting), and then define a model which employs these patterns to make a prediction, all generally based on well-established literature (see, for example, the competition baseline model [2]). These approaches put the\n1The 11th parameter is tested within a problem.\nemphasis on characterizing or describing the observed behavior, but generally use simple, hand-tuned methods to make the actual prediction.\nWe choose to take a different approach towards modeling human decision making. We use the behavioral theory based patterns to compute a feature representation for the decision problem, to be used as input to powerful machine learning algorithms. In this prospect, the computational field of machine learning has much to offer, from sophisticated learning algorithms to efficient ways for feature selection and reduction, as well as reliable validation techniques.\nIn this work we implement a vanilla version of a hybrid model that combines Support Vector Machines (SVM) models with behavioral-based features and adjustments. The behavioral features are computed based on an effective (i.e., biased) form of the target problem. Our hybrid model was ranked lower than most of the variants of the baseline model, but outperformed all the learning-based models in the competition, demonstrating that these psychological features are important for the success of the learning process.\nIn the next section we present the details of our hybrid model and in Section 3 we present the competition results and conclude with future research directions."
    }, {
      "heading" : "2 The Hybrid Model",
      "text" : ""
    }, {
      "heading" : "2.1 Feature representation",
      "text" : "Each sample representing a problem p is given by 10 parameters: LA,HA, pHA, LB,HB, pHB,LotNum,LotShape, Corr, Amb (the first 10 coordinates of the problem space PS). For each block i of consecutive five choice trials, we compute a feature representation f ip that is consisted of (24+i-1) features, as follows.\nFirst, we generate a problem pieff by converting p’s probabilities – pHa and pHb – to their effectiveform at block i. Specifically, set pHa = effective(pHa, i) and pHb = effective(pHb, i), by:\neffective(p, i) =\n\n \n \np i if p ≤ 0.02 p+ (1− 1 i ) · (1− p) if p ≥ 0.98 p otherwise\nThat is, the effective form of a problem reflects the tendency to underweight rare probabilities as i increases.\nNext, we compute a list of 16 behavior-based features from the parameters of pieff . These features were designed to capture salient properties of the\nproblem, which play a role when people face the problem of choosing between options A and B, in our setting of small decisions under risk and ambiguity. Most of these features are derived from the baseline model provided by [2]. The features include properties of a single option as well as properties that refer to the problem as a whole.\nThe features make use of the full distributions of options A and B. Let us annotate these distributions by DistA = (oa1, p a 1; o a 2, p a 2; ...; o a sa , pasa) and DistB = (ob1, p b 1; o b 2, p b 2; ...; o b sb , pbsb), i.e., the full outcomes and probabilities, where the outcomes o∗j appear in ascending order. We proceed to describe all the features.\n1. IsGain: equals 1 if the portion of non-negative outcomes is greater than 0.65 and at least one outcome is strictly positive, or 0 otherwise.\n2. IsLoss: equals 1 if the portion of non-positive outcomes is greater than 0.65 and at least one outcome is strictly negative, or 0 otherwise.\n3. EvA: The expected value of option A. Specifically, EvA = ∑sa j=1 o a j ·p a j .\n4. BevB: The best estimation of the expected value of option B, as follows:\nBevB =\n{\n0.48 · ∑sb j=1 1 sb obj + 0.48 · EvA+ 0.04 · o b 1 if i = 1 and Amb = 1 ∑sb j=1 o b j · p b j otherwise\nFor ambiguous problems in the first round, BevB is a pessimistic estimation of the expectation assuming pHB is uniform, and biased towards option A’s expected value. Otherwise, it is the actual expected value of option B.\n5. BevA pt: The expected value of option A, using prospect theory valuation and weighting functions. Specifically,\nBevA pt = V (oa1)π(p a 1) + ...+ V (o a sa )π(pasa)\nwhere\nV (oi) =\n{\noαi if oi ≥ 0 −λ|oi| β otherwise\nand\nπ(pi) =\n\n\n\np γ i\n(pγi +(1−pi) γ)\n1 γ\nif oi ≥ 0\npδi\n(pδi+(1−pi) δ)\n1 δ\notherwise\nwe used α = 0.77, β = 0.9, γ = 0.79, δ = 0.87, λ = 1.0023.\n6. BevB pt: Same asBevA pt, computed for DistB with one small change: for ambiguous problems in the first round, we take DistB to be\npb1 = bevB − 1 sb ∑sb j=2 o b j\nob1 − 1 sb ∑sb j=2 o b j\nand\npb2 = p b 3 = ... = p b sb = 1− pb1 sb − 1\n7. SignMax: The sign of the maximal outcome.\n8. RatioMin:\nRatioMin =\n{\nmin{|oa 1 |,|ob 1 |} max{|oa 1 |,|ob 1 |} if sign(oa1) = sign(o b 1) 0 otherwise\n9. BevBminA: The difference between the expected value of option B and option A. That is,\nBevBminA = BevB − EvA\n10. BevBminA pt: The difference between the prospect-theory expected value of option B and option A. That is,\nBevBminA pt = BevB pt− EvA pt\n11. BevBminA unif : The difference between the expected value of option B and option A, assuming the outcomes are distributed uniformly in both options. Specifically,\nBevBminA unif =\nsb ∑\nj=1\n1 sb obj −\nsa ∑\nj=1\n1\nsa oaj\n12. BevBminA sign: The difference between the expected value of option B and of option A, assuming that positive outcomes are replaced by R and negative outcomes are replace by −R, where R is the payoff range. That is,\nBevBminA sign =\nsb ∑\nj=1\nR · sgn(obj) · p b j −\nsa ∑\nj=1\nR · sgn(oaj ) · p a j\nwhere R = max{oa1, ..., o a sa , ob1, ..., o b sb } −min{oa1, ..., o a sa , ob1, ..., o b sb }.\n13. BisBetter: The probability that B will give a better outcome, computed as:\nBisBetter =\nsb ∑\ni=1\n\npib · ∑\nj:oaj<o b j\npaj\n\n\n14. SCPT : The decision made by the Stochastic Cumulative Prospect Theory model described in [3].\n15. V arBminA: The difference in variance between option B and option A.\n16. EntBestMinA: The difference in entropy between option B and option A, where the entropy for a distribution p = p1, ..., ps is computed as\nentropy(p) = −\ns ∑\ni=1\npi log pi\nFinally, the feature representation f ip for a problem p in block i is a vector which is obtained by\n• Setting the first 8 entries to p’s values: (Ha, pHa, La,Hb, pHb, Lb, Amb, Corr).\n• Setting entries 9-24 to the features listed above based on pieff .\n• Setting the next i−1 entries to the B-rates predicted in the i−1 previous blocks (B1, ..., Bi−1) (or to the B-rates observed in the estimation experiments2 when computed for the training set).\nWe use the standardized vector, where each of the first 24 entries is standardized to have zero mean and unit variance based on randomly generated 100,000 problems (from the problem space described in Section 1), and the Bis are standardized based on the 90 estimation problems."
    }, {
      "heading" : "2.2 Learning Algorithm",
      "text" : "For learning the problem we chose to use Support Vector Machines (SVM) [1], which is a versatile and powerful learning method. We experimented with various choices of kernels, and the best results were obtained with a polynomial kernel of the third degree, using N-fold (for N = 10) cross validation over the 90 estimation problems. For each block i ∈ {1, ..., 5} we learned a separate SVM model over the feature representation at block i as described above.\n2Experiment 1 and 2, data in Tables 2 and 3, in [2], respectively."
    }, {
      "heading" : "2.3 Prediction",
      "text" : "Given a target problem p represented by a feature vector x, the hybrid model predicts the B-rates for x sequentially: first predicting B1, the average Brates in the first block of “from description” trials, then predicting every later Bi at block i ∈ {2, ..., 5} of “from feedback” trials, relying on the predictions so far (B1, ..., Bi−1). The prediction for each block i is done using the SVM model learned for block i followed by adjustments based on the problem properties.\nSpecifically, for each block i ∈ {1, ..., 5}, the hybrid model predicts Brates Bi by carrying the following procedure:\n1. Set z to be the standardized feature representation of the target problem x.\n2. Apply the SVM model at block i to z to obtain Bi.\n3. Adjust Bi in the following two cases, taking into account the effective form of the problem pieff , and the features BevB, EvA and SCPT of xeff (see Section 2.1).\n(a) If pieff is trivial, i.e., if one of the options dominates the other, then average Bi with the trivial choice.\n(b) If i = 1, and at least one of the conditions |Bi − 0.5| < 0.025 or BevB = EvA holds, then set Bi = 0.7 ∗Bi + 0.3 ∗ SCPT (where SCPT is the feature calculated in Section 2.1).\nWe therefore push the prediction in the direction suggested by Stochastic Cumulative Prospect Theory (SCPT) when the B-rate prediction in the first block is not clear enough."
    }, {
      "heading" : "3 Results and Conclusion",
      "text" : ""
    }, {
      "heading" : "3.1 Results",
      "text" : "The hybrid model passed the tests provided by [2], indicating success in capturing all 14 choice anomalies. The MSD score on the estimation set (the 90 problems initially supplied to the competitors) was 0.74 compared to 0.71 achieved by the baseline model, with an average of 0.94 and standard deviation 0.74 over the 25 models submitted to the competition. The MSD score on the test set (60 new problems) was 1.24 compared to 0.98 achieved by the baseline model, with an average of 1.47 and standard deviation 0.89.\nOut of the 25 models submitted to the competition, 14 were variants of the baseline model and the remaining were based on statistics, machine learning or other methods. Our model ranked 14th, where ranks 1-13 were all taken by variants of the baseline model."
    }, {
      "heading" : "3.2 Conclusion",
      "text" : "Our hybrid model outperformed all the models which were not variants of the baseline model. In particular, it performed better than all of the learningbased models that did not make use of behavioral theories. These results suggest that learning methods alone are not sufficient for predicting human decisions, and that representing and manipulating the data based on behavioral knowledge is important for the success of these methods, supporting our approach of integrating behavioral features and adjustments with the powerful machine learning tools to improve their performance.\nHowever, the fact that the hybrid model was ranked lower than the baseline model and its variants, indicates that this integration should be further studied and improved. We believe that a first major improvement to the preliminary results shown in this paper can be obtained by further experimenting with the full range of parameters available for SVM, as well as trying other machine learning methods. A second significant improvement may be obtained by learning from a larger training set. Running large scale experiments is becoming more applicable with the growing popularity of online labor markets such as Amazon Mechanical Turk. Obviously, a further study of such hybrid models for predicting human decision making is called for."
    } ],
    "references" : [ {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine Learning, 20(3):273–297",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "From anomalies to forecasts: A choice prediction competition for decisions under risk and ambiguity",
      "author" : [ "I. Erev", "E. Ert", "O. Plonsky" ],
      "venue" : "Mimeo, pages 1–56",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Combining a theoretical prediction with experimental evidence to yield a new prediction: An experimental design with a random sample of tasks",
      "author" : [ "I. Erev", "A.E. Roth", "R.L. Slonim", "G. Barron" ],
      "venue" : "Unpublished manuscript, Columbia University and Faculty of Industrial Engineering and Management, Techion, Haifa, Israel",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The description–experience gap in risky choice",
      "author" : [ "R. Hertwig", "I. Erev" ],
      "venue" : "Trends in cognitive sciences, 13(12):517–523",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Prospect theory: An analysis of decision under risk",
      "author" : [ "D. Kahneman", "A. Tversky" ],
      "venue" : "Econometrica: Journal of the econometric society, pages 263– 291",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1979
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "For instance, in risky choice problems, when the decision is based solely on the description of the problem people tend to overweight rare events [5], while in decisions based on experience they tend to underweight rare events (see review of the experience-description gap in [4]).",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "For instance, in risky choice problems, when the decision is based solely on the description of the problem people tend to overweight rare events [5], while in decisions based on experience they tend to underweight rare events (see review of the experience-description gap in [4]).",
      "startOffset" : 276,
      "endOffset" : 279
    }, {
      "referenceID" : 1,
      "context" : "Recently, researchers were challenged in the Choice Prediction Competition (CPC) 2015 [2] to propose a model that will be general enough to predict human decisions for a wide space of problems.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "The competition focused on 14 well-studied choice anomalies, which included the four biases captured by prospect theory [5] (including overweighting of rare events), and additional 10 biases that were documented in studies of decisions with and without feedback (including underweighting of rare events in repeated settings with feedback).",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "For the full description of the competition see [2] and the competition website.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Our Approach: The common approach towards solving this problem in behavioral fields is to derive patterns from behavior observed in experiments (in some setting), and then define a model which employs these patterns to make a prediction, all generally based on well-established literature (see, for example, the competition baseline model [2]).",
      "startOffset" : 339,
      "endOffset" : 342
    }, {
      "referenceID" : 1,
      "context" : "Most of these features are derived from the baseline model provided by [2].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "SCPT : The decision made by the Stochastic Cumulative Prospect Theory model described in [3].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "2 Learning Algorithm For learning the problem we chose to use Support Vector Machines (SVM) [1], which is a versatile and powerful learning method.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "Experiment 1 and 2, data in Tables 2 and 3, in [2], respectively.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "1 Results The hybrid model passed the tests provided by [2], indicating success in capturing all 14 choice anomalies.",
      "startOffset" : 56,
      "endOffset" : 59
    } ],
    "year" : 2016,
    "abstractText" : "A large body of work in behavioral fields attempts to develop models that describe the way people, as opposed to rational agents, make decisions. A recent Choice Prediction Competition (2015) challenged researchers to suggest a model that captures 14 classic choice biases and can predict human decisions under risk and ambiguity. The competition focused on simple decision problems, in which human subjects were asked to repeatedly choose between two gamble options. In this paper we present our approach for predicting human decision behavior: we suggest to use machine learning algorithms with features that are based on well-established behavioral theories. The basic idea is that these psychological features are essential for the representation of the data and are important for the success of the learning process. We implement a vanilla model in which we train SVM models using behavioral features that rely on the psychological properties underlying the competition baseline model. We show that this basic model captures the 14 choice biases and outperforms all the other learning-based models in the competition. The preliminary results suggest that such hybrid models can significantly improve the prediction of human decision making, and are a promising direction for future research. Rachel & Selim Benin School of Computer Science & Engineering and Federmann Center for the Study of Rationality, The Hebrew University of Jerusalem, Israel. Rachel & Selim Benin School of Computer Science & Engineering, The Hebrew University of Jerusalem, Israel. Racah Institute of Physics, The Hebrew University of Jerusalem, Israel. Rachel & Selim Benin School of Computer Science & Engineering, The Hebrew University of Jerusalem, Israel; and Google.",
    "creator" : "LaTeX with hyperref package"
  }
}