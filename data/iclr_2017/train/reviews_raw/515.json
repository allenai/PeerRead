{"conference": "ICLR 2017 conference submission", "title": "Exponential Machines", "abstract": "Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.", "reviews": [{"is_meta_review": true, "comments": "This paper introduces a polynomial linear model for supervised classification tasks. The model is based on a combination of the Tensor Train (TT) tensor decomposition method and a form of stochastic Riemannian  optimization. A few empirical experiments are performed that demonstrate the good performance of the proposed model relative to appropriate baselines.\n\nFrom a theoretical standpoint, I think the approach is interesting and elegant. The main machinery underlying this work are the TT decomposition and the geometric structure of the manifold of tensors with fixed TT-rank, which have been established in prior work. The novelty of this paper is in the combination of this machinery to form an efficient polynomial linear model. As such, I would have hoped that the paper mainly focused on the efficacy of this combination and how it is superior to obvious alternatives. For example, I would have really appreciated seeing how FMs performed when optimized over the manifold of positive definite matrices, as another reviewer mentioned. Instead, there is a bit too much effort devoted to explaining prior work.\n\nI think the empirical analysis could be substantially improved. I am particularly puzzled by the significant performance boost obtained from initializing with the ordinary logistic regression solution. I would have liked some further analysis of this effect, especially whether or not it is possible to obtain a similar performance boost with other models. Regarding the synthetic data, I think an important baseline would be against a vanilla feed forward neural network, which would help readers understand how complicated the interactions are and how difficult the dataset is to model. I agree with the previous reviewer regarding a variety of other possible improvements to the experimental section.\n\nA few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'.\n\nOverall, I am on the fence regarding this paper. The main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "Modeling nonlinear interactions between variables by imposing Tensor-train structure on parameter matrices is certainly an elegant and novel idea. All reviewers acknowledge this to be the case. But they are also in agreement that the experimental section of this paper is somewhat weak relative to the rest of the paper, making this paper borderline. A revised version of this paper that takes reviewer feedback into account is invited to the workshop track.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Update", "OTHER_KEYS": "Alexander Novikov", "comments": "We would like to thank all the reviewers and commenters for their time, feedback, and ultimately for making our paper better.\n\nWe updated the paper to address the questions raised in the reviews.\n\nWe found out that the backtracking algorithm is beneficial only for the small-scale setting when the mini-batch size is of the order of the full dataset (e.g. UCI experiments). Otherwise, backtracking is (locally) tuning the learning rate too well and overfits to each mini-batch, which stagnates the convergence process.\nFor this reason, we removed the backtracking from the paper text and from all the experiments.\nWe also found out that after removing the backtracking, the dropout becomes unnecessary, and we removed it as well.\n\nOther changes:\n1) We added the complexity of each step of the Riemannian gradient descent.\n2) Investigated the effect of the proper initialization on the model and proposed a random initialization that works on par with the initialization from the solution of the linear model.\n3) Added the validation loss for the comparison of optimizers on the UCI datasets.\n4) Added the convergence plots to compare Riemannian optimization vs SGD baseline on the synthetic dataset.\n5) Added 3 suggested papers to the related work section.\n6) Plotted the TT-rank vs. accuracy graph on the MovieLens 100K dataset.\n7) Restructured the experiments section to make it easier to assess different aspects of the model on different datasets.\n8) Added a feedforward neural network baseline for the synthetic dataset.\n9) Fixed typos and made a few clarifications. \n", "IS_META_REVIEW": false, "DATE": "17 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Nice idea, but experiments are very preliminary", "is_meta_review": false, "comments": "The paper presents an application of a tensor factorization to linear models, which allows to consider higher-order interactions between variables in classification (and regression) problems, and that maintains computational feasibility, being linear in the dimension. The factorization employed is based on the TT format, first proposed by Oseledests (2011). The authors also propose the adoption of a Riemannian optimization scheme to explicit consider the geometry of the tensor manifold, and thus speed up convergence.\n\nThe paper in general is well written, it presents an interesting application of the TT tensor format for linear models (together with an application of Riemannian optimization), which in my opinion is quite interesting since it has a wide range of possible applications in different algorithms in machine learning.\n\nOn the other side, I have some concerns are about the experimental part, which I consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets. Moreover the authors do not take into account explicitly the problem of the choice of the rank to be used in the experiments. In general the experimental section seems a collection of preliminary experiments where different aspects have been tested by not in a organic way.\n\nI think the paper is close to a weak acceptance / weak rejection, I don't rate it as a full acceptance paper, mainly due to the non-satisfactory experiment setting. In case of extra experiments confirming the goodness of the approach, I believe the paper could have much better scores.\n\nSome minor comments:\n-formula 2: Obvious comment: learning the parameters of the model in (1) can be done as in (2), but also in other ways, depending on the approach you are using.\n-the fact that the rank is bounded by 2r, before formula 9, is explained in Lubich et al., 2015?\n-after formula 10: why the N projections in total they cost O(dr^2(r+N)), it should be O(Ndr^2(r+1)), no? since each of the elements of the summation has rank 1, and the cost for each of them is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z)=1. Am I wrong?\n-section 6.2: can you explain why the random initialization freezes the convergence? This seems interesting but not motivated. Any guess?\n-section 6.3: you adopt dropout: can you comment in particular on the advantages it gives in the context of the exponential machines? did you use it on real datasets?\n-how do you choose r_0 in you experiments? with a validation set?\n-in section 7: why you don't have x_1 x_2 among the variables?\n-section 8: there is a typo in \"experiments\"\n-section 8.1: \"We simplicity, we binarized\" I think there's a problem with the English language in this sentence\n-section 8.3: \"we report that dropout helps\".. this is quite general statement, only tested on a synthetic dataset\n-section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time? or test wrt other algorithms?", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "04 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Interesting idea, analysis could be improved", "is_meta_review": false, "comments": "This paper introduces a polynomial linear model for supervised classification tasks. The model is based on a combination of the Tensor Train (TT) tensor decomposition method and a form of stochastic Riemannian  optimization. A few empirical experiments are performed that demonstrate the good performance of the proposed model relative to appropriate baselines.\n\nFrom a theoretical standpoint, I think the approach is interesting and elegant. The main machinery underlying this work are the TT decomposition and the geometric structure of the manifold of tensors with fixed TT-rank, which have been established in prior work. The novelty of this paper is in the combination of this machinery to form an efficient polynomial linear model. As such, I would have hoped that the paper mainly focused on the efficacy of this combination and how it is superior to obvious alternatives. For example, I would have really appreciated seeing how FMs performed when optimized over the manifold of positive definite matrices, as another reviewer mentioned. Instead, there is a bit too much effort devoted to explaining prior work.\n\nI think the empirical analysis could be substantially improved. I am particularly puzzled by the significant performance boost obtained from initializing with the ordinary logistic regression solution. I would have liked some further analysis of this effect, especially whether or not it is possible to obtain a similar performance boost with other models. Regarding the synthetic data, I think an important baseline would be against a vanilla feed forward neural network, which would help readers understand how complicated the interactions are and how difficult the dataset is to model. I agree with the previous reviewer regarding a variety of other possible improvements to the experimental section.\n\nA few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'.\n\nOverall, I am on the fence regarding this paper. The main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "03 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Some comments and related works", "OTHER_KEYS": "Mathieu Blondel", "comments": "Hi Alexander,\n\nYour paper is an interesting addition to the literature on low-rank polynomial models. Good work!\n\nI agree with a previous comment that it would be worth adding more comments on the difference between FMs and EMs in Section 9. For instance, EMs model all d-combinations while FMs model combinations up to some degree. It would also be informative to compare the total model size of both models.\n\nI am not sure I agree that \"TT-format allows for Riemannian optimization\" is an advantage of tensor trains. For example, it should be possible to train FMs over the positive definite matrix manifold.\n\nAdmittedly, the literature is fairly recent but here are a few relevant prior works:\n\n- \"On the Computational Efficiency of Training Neural Networks\" by Livni et al. ", "IS_META_REVIEW": false, "DATE": "22 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "A good paper ", "is_meta_review": false, "comments": "This paper proposes to use the tensor train (TT) decomposition to represent the full polynomial linear model. The TT form can reduce the computation complexity in both of inference and model training. A stochastic gradient over a Riemann Manifold has been proposed to solve the TT based formulation. The empirical experiments validate the proposed method.\n\nThe proposed approach is very interesting and novel for me. I would like to vote acceptance on this paper. My only suggestion is to include the computational complexity per iteration.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "is_meta_review": false, "comments": "The paper describes how to use a tensor factorization method called Tensor Train for modeling the interactions between features for supervised classification tasks. Tensor Train approximates tensors of any dimensions using low rank products of matrices. The rank is used as a parameter for controlling the complexity of the approximation. Experiments are performed on different datasets for binary classification problems.\n\nThe core of the paper consists in demonstrating how the TT formalism developed by one of the authors could be adapted for modeling interactions between features. Another contribution is a gradient algorithm that exploits the geometrical structure of the factorization. These ideas are probably new in Machine learning. The algorithm itself is of reasonable complexity for the inference and could probably be adapted to large size problems although this is not the case for the experiments here.\n\nThe experimental section is not well structured, it is incomplete and could be improved. We miss a description of the datasets characteristics. The performance on the different datasets are not provided. Each dataset has been used for illustrating one aspect of the model, but you could also provide classification performance and a comparison with baselines for all the experiments. The experiments on the 2 UCI datasets show optimization performance on the training set, you could provide the same curves on test sets to show how the algorithm generalizes. The comparison to other approaches (section 8.4) is only performed on artificial data, which are designed with interacting features and are not representative of diverse situations. The same holds for the role of Dropout. The comparison on the Movielens dataset is incomplete. Besides, all the tests are performed on small size problems.\n\nOverall, there are original contributions which could be worth a publication. The experiments are incomplete and not conclusive. A more detailed comparison with competing methods, like Factorization Machines, could also improve the paper. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Revision", "OTHER_KEYS": "Alexander Novikov", "comments": "Yesterday I updated the pdf. The only thing that changed is that I added a paragraph at the end of the related works section to credit the papers suggested by the comment below.", "IS_META_REVIEW": false, "DATE": "17 Dec 2016", "is_meta_review": false}, {"TITLE": "questions and related works", "OTHER_KEYS": "(anonymous)", "comments": "From the reported experiments on MovieLens and synthetic data, it seems that higher-order FMs give better accuracy at lower cost than ExMs. Is there experimental evidence that potentially modeling all the feature interactions gives better performances on some problems (on real data)?\n\nOn a related note, it'd be interesting to know what capacity your model has as a function of the TT-rank: does the generalization error depend in a nice way on the TT-rank, or does it blow up?\n\nTwo related works are:\n- \"Tensor machines for learning target-specific polynomial features\" which learns a CP factorization for polynomial regression and proves generalization error bounds; the proof is probably adaptable to your setting.\n- \"Learning multidimensional Fourier series with tensor trains\", which uses TT-decomposition to learn 'random' Fourier features.", "IS_META_REVIEW": false, "DATE": "15 Dec 2016", "is_meta_review": false}, {"TITLE": "tensor decomposition", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Alexander Novikov, Mikhail Trofimov, Ivan Oseledets", "KEYWORDS": "A supervised machine learning algorithm with a polynomial decision function (like SVM with a polynomial kernel) that models exponentially many polynomial terms by factorizing the tensor of the parameters.", "accepted": false, "id": ""}
