{
  "name" : "1212.6276.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Echo State Queueing Network: a new reservoir computing learning tool",
    "authors" : [ "Sebastián Basterrech", "Gerardo Rubino" ],
    "emails" : [ "Sebastian.Basterrech@inria.fr", "Gerardo.Rubino@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we propose a new RC model called Echo State Queueing Network (ESQN), where we use ideas coming from RandNNs for the design of the reservoir. ESQNs consist in ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The paper positions ESQNs in the global Machine Learning area, and provides examples of their use and performances. We show on largely used benchmarks that ESQNs are very accurate tools, and we illustrate how they compare with standard ESNs.\nIndex Terms - Reservoir Computing, Echo State Network, Random Neural Network, Queueing Network, Machine Learning\nI. INTRODUCTION Artificial Neural Networks (ANNs) are a class of computational models which have been proven to be very powerful as statistical learning tools to solve complicated engineering tasks as well as many theoretical issues. Several types of ANNs have been designed, some of them originating in the field of Machine Learning while others coming from biophysics and neuroscience. The Random Neural Network (RandNN) proposed by E. Gelenbe in 1989 [1], is a mathematical object inspired by biological neuronal behavior which merges features of Spiking Neural Networks and Queueing Networks. In the literature, actually two different interpretations of exactly the same mathematical model are proposed. One is a type of spiking neuron and the associated network which is called RandNNs. The other one is a new type of queue and networks of queues, respectively called G-queues and G-networks. The RandNN is a connectionist model where spikes circulate among the interconnected neurons. A discrete state-space is used to represent the internal state (potential) of each neuron. The firing times of the spikes are modeled as Poisson processes. The potential of each neuron is represented by a positive integer that increases when a spike arrives or decreases after the neuron fires. In order to use RandNNs in\nsupervised learning problems, a gradient descent algorithm has been described in [2], and Quasi-Newton methods have been proposed in [3], [4]. Additionally, the function approximation properties of the model were studied in [5], [6]. The structure of the model leads to efficient numerical evaluation procedures, to good performance in learning algorithms and to easy hardware implementations. Consequently, since its introduction the model has been applied in a variety of scientific fields. Nevertheless, the RandNNs model suffers from limitations. Some of them are related to the use of a feedforward topology (see [7]). The original acronym to refer the model was RNN. In this work to avoid a conflict of notation, we use RandNN for Random Neural Networks, due to the use of RNNs in Machine Learning literature for Recurrent Neural Networks.\nConcerning models with recurrences (circuits) in their topologies, they are recognized as powerful tools for a number of tasks in Machine Learning (both traditional ANNs and RandNNs). However, they have a main limitation which comes from the difficulty in implementing efficient training algorithms. The main drawbacks related to learning algorithms are the following: convergence is not always guaranteed, many algorithmic parameters are involved, sometimes long training times are required [8], [9]. For all those reasons learning using recurrent neural networks is principally feasible for relatively small networks.\nRecently, a new paradigm called Reservoir Computing (RC) has been developed which overcome the main drawbacks of learning algorithms applied to networks with cyclic topologies. About ten years ago two main RC models were proposed: Echo State Networks (ESNs) [10] and Liquid State Machines (LSMs) [11]. Both models describe the possibility of using recurrent neural networks without adapting the weight connections involved in recurrences. The network outputs are generated using very simple learning methods such as classification or regression models. The RC approach have been successfully applied in many machine learning tasks achieving goods results, specially in temporal learning tasks [8], [11], [12].\nIn this paper we introduce a new type of RC method which uses some ideas from RandNNs. The paper is organized as follows: we begin by describing the RandNN model in Section II. In Section III, we introduce the two funding\nar X\niv :1\n21 2.\n62 76\nv1 [\ncs .N\nE ]\n2 6\nD ec\n2 01\n2\n2 RC models. Section IV discusses the contribution of this paper, a new RC model similar to the ESN, but using also ideas inspired by queuing theory. Finally, we present some experimental results and we end with some conclusions as well as a discussion regarding future lines of research."
    }, {
      "heading" : "II. DESCRIPTION OF THE RANDOM NEURAL NETWORK MODEL",
      "text" : "A Random Neural Network (RandNN) is a specific queuing network proposed in [1] which merges concepts from spiking neural networks and queuing theory. Depending on the context, its nodes are seen as queues or as spiking neurons. Each of these neurons receives spikes (pulses) from outside, which are of one out of two disjoint types, called excitatory (or positive) and inhibitory (or negative). Associated with a neuron there is an integer variable called the neuron’s potential. Each time a neuron receives an excitatory spike, its potential is increased by one. If a neuron receives an inhibitory spike and its potential was strictly positive, it decreases by one; if it was equal to 0, it remains at that value. As far as the neuron’s potential is strictly positive, the neuron sends excitatory spikes to outside. When the neuron’s potential is strictly positive, we say that the neuron is excited or active. After numbering the neurons in an arbitrary order, let’s denote by Su(t) the potential of neuron u at time t. During the periods when the neuron is active, it produces excitatory spikes with some rate ru > 0. In other words, the output process of the pulses coming out of an active neuron is a Poisson process. Then, a spike produced by neuron u is transferred to the environment with probability du. For each synapse between neuron u and v an excitatory spike (respectively inhibitory spike) produced by u is switched to neuron v with probability p+v,u (respectively p − v,u). In the literature related to RandNNs, the probability that a pulse generated at neuron u goes to neuron v is usually denoted by p+/−u,v . This is different from the notation used in the standard ANNs literature, where a direct connection between u and v is often denoted as (v, u), that is, in the reverse order. In this paper we follow the latter notation. This routing procedure is performed independently of anything else happening in the network, including previous or future switches at the same neuron or at any other one. Observe that for any neuron u we have\ndu + N∑ v=1 p+v,u + N∑ v=1 p−v,u = 1,\nwhere N is the number of neurons in the network. The weight connection between any two neurons u and v (u sending spikes to v) is defined as: w+v,u = rup + v,u and w − v,u = rup − v,u.\nLet us assume that the external (i.e. from the environment) arrival process of positive (respectively negative) spikes to neuron u is Poisson with rate λ+u (respectively with rate λ − u ). Some of these rates can be 0, meaning that no spike of the considered type arrives at the given neuron coming from the network’s environment. In order to avoid the trivial case where nothing happens, we also must assume that ∑N u=1 λ + u > 0\n(otherwise, the network is composed of neurons that are inactive at all times). Last, the usual independence assumptions between all the considered Poisson and routing processes in the model are assumed.\nWe call S(t) = (S1(t), · · · , SN (t)) the state of the network at time t. Observe that S is a continuous time Markov process over the state space NN . We will assume that S is irreducible and ergodic. We are interested in the network’s behavior in steady-state, so, let us assume that S is in equilibrium (that is, assume S is stationary). Let %u be the probability (in equilibrium) that neuron u is excited,\n%u = lim t→∞ P(Su(t) > 0).\nThis parameter is called the activity rate of neuron u. Since process S is ergodic, for all neron u we have 0 < %u < 1. Gelenbe in [1], [13] shows that in an equilibrium situation the %us satisfy the following non-linear system of equations:\nfor each node u, %u = T+u\nru + T − u , (1)\nfor each node u, T+u = λ + u + N∑ v=1 %vw + u,v, (2)\nfor each node u, T−u = λ − u + N∑ v=1 %vw − u,v, (3)\nwith the supplementary condition that, for all neuron u, we have %u < 1. In other words, under the assumption of irreducibility, if the system of equations (1), (2) and (3) has a solution (%1, · · · , %N ) such that we have %u < 1, for all neuron u, then the solution is unique and the Markov process is ergodic. Moreover, its stationary distribution is given by the product of the marginal probabilities of the neuron’s potential. For more details and proofs, see [1], [5].\nAs a learning tool used to learn some unknown function, we map the function’s variables to the external arrival rates, the λ+u s and λ − u s numbers (however, usually we set set λ − u = 0 for all input neuron u, so we map the function’s variables to the λ+u s only). The network’s output is the set of loads. The learning parameters are the set of weights in the model. An appropriate optimization method (such as Gradient Descent) is used to find weights such that when the arrival rate (of positive spikes) equals the input data, the network output matches (with small error) the corresponding known output data values. The model has been widely used in fields such as: combinatorial optimization, machine learning problems, communication networks and computer systems [14]–[18].\nIII. RESERVOIR COMPUTING METHODS\nRecurrent Neural Networks are a large class of computational models used in several applications of Machine Learning and in neurosciences. The main characteristic of this type of ANNs is the existence of at least one feedback loop among the connections, that is, a circuit of connections.The cyclic topology causes that the non-linear transformation of\n3 the input history can be stored in internal states. Hence recurrent neural networks are a powerful tool for forecasting and time series processing applications. They are also very useful for building associative memories, in data compression and for static pattern classification [8]. However, in spite of these important abilities and of the fact that we have efficient algorithms for training neural networks without recurrences, no efficient algorithms exist for the case where recurrences are present.\nSince the early 2000s, Reservoir Computing has gained prominence in the ANN community. In the two basic forms of the model described before, ESNs and LSMs, at least three well-differenced structures can be identified: the input layer, where neurons receive information from the environment; the reservoir (in ESNs) or liquid (in LSMs), a nonlinear “expansion” function implemented using a recurrent neuronal network; the readout, which is usually a linear function or a neural network without recurrences, producing the desired output.\nThe weight connections among neurons in the reservoir and the connections between input and reservoir neurons are fixed during the learning process, only the weights between input neurons and readout units, and between reservoir and readout units, are the object of the training process. The reservoir with its recurrences or circuits, allows a kind of “expansion” of the input and possibly of history data into a larger space. From this point of view, the reservoir idea is similar to the expansion function used in Kernel Methods, for example in the Support Vector Machine [19]. The projection can enhance the linear separability of the data [12]. On the other hand, the readout layer is built to be performant in learning, specially to be robust and fast in this process. The RC approach is based on the empirical observation that under certain assumptions, training only a linear readout is often sufficient to achieve good performance in many learning tasks [8]. For instance, the ESN model has the best known learning performance on the Mackey–Glass times series prediction task [20], [21].\nThe topology of a RC model consists of an input layer with Na units sending pulses to the reservoir (and possibly also to the readout), a recurrent neural network with Nx units, where Na Nx, and a layer with Nb readout neurons having adjustable connections from the reservoir (and possibly from the input) layer(s).\nThe main difference between LSMs and ESNs consists in the type of nodes included in the reservoir. In the original LSM model the liquid was built using a model derived from Hodgkin-Huxley’s work, called Leaky Integrate and Fire (LFI) neurons. In the standard ESN model, the activation function of the units is most often tanh(·). An ESN is basically a threelayered NN where only the hidden layer has recurrences, but allowing connections from input to readout (and, again, where learning is concentrated in the readout only). Our training data consists of K pairs (a(k),b(k)), k = 1, . . . ,K, of inputoutput values of some unknown function f , where a(k) ∈ RNa , b(k) ∈ RNb and b(k) = f(a(k)). The weights matrices\nare win (connections from input to reservoir), wr (connections inside the reservoir) and wout (connections between input or reservoir and readout), of dimensions Nx×(1+Na), Nx×Nx and Nb × (1 +Na +Nx), respectively. The first rows of win and wout contain ones corresponding to the bias terms.\nEach neuron j of the reservoir has a real state xj . When the input a arrives to the ESN, the reservoir first updates its state x = (x1, . . . , xNx) by executing\nx := tanh ( win[1;a] + wrx ) , (4)\nand then, the ESN computes its outputs\ny := wout[1;a;x], (5)\nwhere [·; ·] is the vertical vector concatenation. If we think of the ESN has a dynamical system receiving a time series of inputs a(1),a(2), . . . and producing a series of outputs y(1),y(2), . . ., the corresponding series of state values evolves according to\nx(t) = tanh ( win[1;a(t)] + wrx(t− 1) ) ,\nwith the output at t computed by y(t) := wout[1;a(t);x(t)]. To ensure good properties in the reservoir, the wr matrix is usually scaled to control its spectral radius (to have ρ(wr) < 1) [10]. The role of the spectral radius is more complex when the reservoir is built with spiking neurons (in the LSM model) [12], [22].\nSeveral extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.\nIV. A NEW RESERVOIR COMPUTING METHOD: ECHO STATE QUEUING NETWORKS\nIn this paper, we propose to reach the objective of simultaneously keeping the good properties of the two models previously described. For this purpose, we introduce the Echo State Queuing Network (ESQN), a new RC model where the reservoir dynamics is based on a specific type of queuing network (RandNN) behavior in steady-state.\nThe architecture of an ESQN consists of an input layer, a reservoir and a readout layer. The input layer is composed of Na random neural units which send spikes toward the reservoir or toward the readout nodes. The reservoir dynamics is designed inspired by the equations of recurrent RandNNs (see below). Let us index the input neurons from 1 and Na, and the reservoir neurons from Na + 1 to Na +Nx.\nWhen an input a is offered to the network, we first identify the rates of the external positive spikes with that input, that is: λ+u = au, and, as it is traditionally done in RandNNs, λ−u = 0, for all u = 1, . . . , Na. In a standard RandNN, the neuron’s loads are computed solving the expressions (1), (2) and (3). More precisely, input neurons behave as a M/M/1 queues. The load or activity rate of neuron u, u = 1, . . . , Na is, in the stable case (au < ru), simply %u = au/ru. For reservoir units, the loads are computed solving the non-linear\n4\nsystem composed of equations (1), (2) and (3). The network is stable if all obtained loads are < 1.\nIn our ESQN model, we do the same for input neurons, but for the reservoir, we introduce the concept of state. The state is simply the vector of loads %. When we need the network output corresponding to a new input a, we first compute a new state by using\n%u :=\nNa∑ v=1 av rv w+u,v + Na+Nx∑ v=Na+1 %vw + u,v\nru + Na∑ v=1 av rv w−u,v + Na+Nx∑ v=Na+1 %vw − u,v , (6)\nfor all u ∈ [Na+1, Na+Nx]. When this is seen as a dynamical system, on the left we have the loads at t, and on the r.h.s. the loads at t− 1.\nThe readout part is computed by a parametric function g(wout,a, %) (or g(wout,a(t), %(t)) when this is used as a dynamical prediction system. In this paper we present the simple case of computing the readout using a linear regression. It is easy to change this by another type of function g, due to the independent structure between the reservoir and readout layers. Thus, the network output y(t) = [y1(t), . . . , yNb(t)] is computed for any m ∈ [1, Nb] using expression (5) and it can be written as follows, where we use the temporal version at time t:\nym(t) = w out m0 + Na∑ i=1 woutmiai(t) + Na+Nx∑ i=1+Na woutmi%i(t). (7)\nThe output weights wout can be computed using some of the traditional algorithms to solve regressions such as the “ridge regression” or the least mean square algorithms [27]."
    }, {
      "heading" : "V. EXPERIMENTAL RESULTS",
      "text" : "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30]. To evaluate the models’ accuracy, we use the Normalized Mean Square Error (NMSE):\nNMSE =\n∑K k=1 ∑Nb j=1 ( b (k) j − y (k) j )2∑K k=1 ∑Nb j=1 ( b (k) j − b̄j\n)2 , (8) where b̄j is the empirical mean, and where we use the same notation as before for the data. The positive and negative weights of the ESQN model and the initial reservoir state were randomly initialized in the intervals [0, 0.2] and [0, 1], respectively. As usual, the training performance can depend on the choice of the starting weights. To take this into account, we experiment with 20 different random initial weights and we calculate their average performance. The preprocessing data step consisted in rescaling the data in the interval [0, 1]. The learning method used was offline ridge regression [31]. This algorithm contains a regularization parameter which is\nadjusted for each data set. The time series data considered were:\n1) Fixed 10th order nonlinear autoregressive moving average (NARMA) system. The series is generated by the following expression: b(t+ 1) = 0.3 b(t) + 0.05 b(t) ∑9 i=0 b(t− i)\n+ 1.5 s(t − 9) s(t) + 0.1, where s(t) ∼ Unif[0, 0.5]. We generated a training data with 1990 samples and a validation set with 390 samples. 2) Traffic data from an Internet Service Provider (ISP) working in 11 European cities. The original data is in bits and was collected every 5 minutes. We rescaled it in [0, 1]. The size of the training data is 9848 and the size of validation set is 4924. The input neurons (Na = 7) are mapped to the last 7 points of the past data, that is with values from t − 6 up to time t. This configuration was suggested in [29] where the authors discuss different neural network topologies taking into account seasonal traits of the data. 3) Traffic data from United Kingdom Education and Research Networking Association (UKERNA). The Internet traffic was collected every day. The network input at any time t is the triple composed of the traffic at times t, t − 6 and t − 7, as studied in [29]. This small data set has 47 training pairs and 15 validation samples.\nThe NARMA series data was studied in deep in [12], [21], [28], [32], [33]. For the last two data sets the performance using NN, ARIMA and Holt-Winters methods can be seen in [29]. A typical ESN model consists in a reservoir with the following characteristics: random topology, Nx large enough, sparsely connected (roughly between 15% and 20% of their weights are non-zeros) [8]. The specific ESN used has a sparsity of 15% and spectral radius of 0.95 in its reservoir matrix. In [12], the authors obtained the best performance in the NARMA data problem when the spectral radius was close to 1.\nThe ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34]. Both models have 80 units in the reservoir for the NARMA data and 40 units for the other two data sets. In this paper, in order to compare the performance of the ESQN\n5 200 210 220 230 240 250 260 270 280 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Example of ESQN estimation of the 80 validation instances of Narma series data F u n ct io n a l v a lu e\nTime steps\nTarget ESQN\nFigure 1. Example of ESQN prediction for 80 time steps of fixed 10th NARMA validation data set. The reservoir was randomly initialized and it had 80 units.\n4900 4902 4904 4906 4908 4910 4912 4914 4916 4918 4920 0.66\n0.68\n0.7\n0.72\n0.74\n0.76\n0.78\n0.8\nTime steps\nFu nc\ntio na\nl v al\nue\nExample of ESQN estimation of 20 validation instances of ISP data\nTarget ESQN\nFigure 2. Example of ESQN prediction for 20 instances in the validation set of the European ISP traffic data. The instances correspond to time steps between 4900 and 4920. The reservoir was randomly initialized and it had 40 neurons.\nand ESN models we use the standard versions of each of them. Table I presents the accuracy of the ESQN and ESN models. In the last column we give a 95% confidence interval obtained from 20 independent runs. We can see that for the 10th order NARMA and UKERNA data the performance obtained with ESQN is better than with ESN (even if in the NARMA case, the confidence intervals have a “slight” nonempty intersection). In the case of the European ISP data, ESN shows a significant better performance. Observe that in all cases the accuracy obtained with ESQN was very good. Also observe that we are using some years of cumulated knowledge about ESNs in our implementation, which we are comparing with our first versions of our new largely unexplored ESQN model.\nFigure 3 shows that the reservoir size is an important parameter affecting the performance of the ESQN. This also\n0 50 100 150 200 250 300 350 400 450 500 0.09\n0.1\n0.11\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17 The ESQN model accuracy for different reservoir sizes for NARMA series data\nN M\nS E\na ve\nra g\ne o\nf 2\n0 r\nu n\ns −\nV a\nlid a\ntio n\ns e\nt\nReservoir size\nFigure 3. The ESQN model performance for different reservoir sizes which are computed for 10th NARMA validation data set. The reservoir weights were randomly initialized. Figure shows the NMSE average achieved in 20 runs with different ESQN initial weights.\n5 6 7 8 9 10 11 12 13 14 15 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1 Example of ESQN estimation for the last 10 instances of the validation UKERNA data\nF u\nn ct\nio n\na l v\na lu\ne\nTime steps\nTarget ESQN\nFigure 4. ESQN estimation for UKERNA validation data set. The reservoir weights were randomly initialized. The reservoir size is 40.\nhappens with the ESN model: in general, a larger reservoir enriches the learning abilities of the model. The sparsity and density of the reservoir in the ESQN model was not studied in this work. It is left for future efforts. NARMA is an interesting time series data where the outputs depend on both the input and previous outputs. The modeling problem is difficult to solve due to the non-linearity of the data and the necessity of having some kind of long memory. Figure 1 illustrates an estimation of ESQN with 80 units in the reservoir which are randomly chosen in [0, 0.2]. Figures 2 and 4 show the prediction values for an interval of validation data. Figure 2 shows the prediction of 20 instances beginning at time 4900 of the validation set. The main difficulty to model UKERNA data (using day scale) is that the training set is small. In spite of this, Figure 4 illustrates the good performance of the ESQN model. This figure shows the estimation of the last 10\n6 instances in the validation data."
    }, {
      "heading" : "VI. CONCLUSIONS",
      "text" : "In this contribution, we have presented a new type of Reservoir Computing model which we call Echo State Queuing Network (ESQN). It combines ideas from queueing and neural networks. It is based on two computational models: the Echo State Network (ESN) and the Random Neural Network. Both methods have been successfully used in forecasting and machine learning problems. Particularly, ESNs have been applied in many temporal learning tasks. Our model was used to predict three time series data which are widely used in the machine learning literature. In all cases tested, the performance results have been very good. We empirically investigated the relation between the reservoir size and the ESQN performance. We found that the reservoir size has a significant impact on the accuracy. Another positive property of ESQNs is their simplicity, since reservoir units are just counter functions. Last, our tool is very easy to implement, both in software and in hardware.\nThere are still several aspects of the model to be studied in future work. Some examples are the impact of the sparsity of the reservoir weights, the weight initialization methods used, the scaling of reservoir weights and the utilization of leaky integrators."
    } ],
    "references" : [ {
      "title" : "Random Neural Networks with Negative and Positive Signals and Product Form Solution",
      "author" : [ "E. Gelenbe" ],
      "venue" : "Neural Computation, vol. 1, no. 4, pp. 502–510, 1989.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Learning in the recurrent random neural network",
      "author" : [ "——" ],
      "venue" : "Neural Computation, vol. 5, pp. 154–164, 1993.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Training the Random Neural Network using Quasi-Newton Methods",
      "author" : [ "A. Likas", "A. Stafylopatis" ],
      "venue" : "Eur.J.Oper.Res, vol. 126, pp. 331–339, 2000.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Levenberg-Marquardt Training Algorithms for Random Neural Networks",
      "author" : [ "S. Basterrech", "S. Mohammed", "G. Rubino", "M. Soliman" ],
      "venue" : "Computer Journal, vol. 54, no. 1, pp. 125–135, January 2011. [Online]. Available: http://dx.doi.org/10.1093/comjnl/bxp101",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The Spiked Random Neural Network: Nonlinearity, Learning and Approximation",
      "author" : [ "E. Gelenbe" ],
      "venue" : "Proc. Fifth IEEE International Workshop on Cellular Neural Networks and Their Applications, London, England, april 1998, pp. 14–19.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Function Approximation by Random Neural Networks with a Bounded Number of Layers",
      "author" : [ "E. Gelenbe", "Z. Mao", "Y. Da-Li" ],
      "venue" : "Journal of Differential Equations and Dynamical Systems, vol. 12, pp. 143–170, 2004.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning in the feedforward random neural network: A critical review",
      "author" : [ "M. Georgiopoulos", "C. Li", "T. Koçak" ],
      "venue" : "Performance Evaluation, vol. 68, no. 4, pp. 361 – 384, 2011. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0166531610000970",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Reservoir computing approaches to recurrent neural network training",
      "author" : [ "M. Lukos̆evic̆ius", "H. Jaeger" ],
      "venue" : "Computer Science Review, pp. 127–149, 2009. [Online]. Available: http://dx.doi.org/10.1016/j. cosrev2009.03.005",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Bifurcations in the learning of Recurrent Neural Networks",
      "author" : [ "K. Doya" ],
      "venue" : "IEEE International Symposium on Circuits and Systems, 1992, pp. 2777–2780.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "The “echo state” approach to analysing and training recurrent neural networks",
      "author" : [ "H. Jaeger" ],
      "venue" : "German National Research Center for Information Technology, Tech. Rep. 148, 2001.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Real-time computing without stable states: a new framework for a neural computation based on perturbations",
      "author" : [ "W. Maass", "T. Natschläger", "H. Markram" ],
      "venue" : "Neural Computation, pp. 2531–2560, november 2002.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "An experimental unification of reservoir computing methods",
      "author" : [ "D. Verstraeten", "B. Schrauwen", "M. D’Haene", "D. Stroobandt" ],
      "venue" : "Neural Networks, no. 3, pp. 287–289, 2007.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Product-Form Queueing Networks with Negative and Positive Customers",
      "author" : [ "E. Gelenbe" ],
      "venue" : "Journal of Applied Probability, vol. 28, no. 3, pp. 656–663, September 1991.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Cognitive Packet Networks",
      "author" : [ "E. Gelenbe", "Z. Xu", "E. Seref" ],
      "venue" : "11th IEEE International Conference on Tools with Artificial Intelligence (ICTAI’99), 1999, pp. 47–54.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The Cognitive Packet Network: A Survey",
      "author" : [ "G. Sakellari" ],
      "venue" : "The Computer Journal, vol. 53, no. 3, pp. 268–279, 2010. [Online]. Available: http://comjnl.oxfordjournals.org/cgi/content/abstract/bxp053v1",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Quantifying the Quality of Audio and Video Transmissions over the Internet: the PSQA Approach",
      "author" : [ "G. Rubino" ],
      "venue" : "Design and Operations of Communication Networks: A Review of Wired and Wireless Modelling and Management Challenges, ser. Edited by J. Barria. Imperial College Press, 2005.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Minimum cost graph covering with the random neural network",
      "author" : [ "E. Gelenbe", "F. Batty" ],
      "venue" : "Computer Science and Operations Research. New York: Pergamon, 1992, pp. 139–147.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "A GRASP algorithm with RNN based local search for designing a WAN access network",
      "author" : [ "H. Cancela", "F. Robledo", "G. Rubino" ],
      "venue" : "Electronic Notes in Discrete Mathematics, vol. 18, pp. 59–65, 2004. [Online]. Available: http://dx.doi.org/10.1016/j.endm.2004.06.010",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Support-Vector Networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Mach. Learn., vol. 20, no. 3, pp. 273–297, Sep. 1995. [Online]. Available: http://dx.doi.org/10.1023/A:1022627411411",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Training Recurrent Networks by Evolino",
      "author" : [ "J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F. Gomez" ],
      "venue" : "Neural Computation, vol. 19, no. 3, pp. 757–779, Mar. 2007. [Online]. Available: http://dx.doi.org/10.1162/ neco.2007.19.3.757",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication",
      "author" : [ "H. Jaeger", "H. Haas" ],
      "venue" : "Science, vol. 304, no. 5667, pp. 78–80, 2004. [Online]. Available: http://www.sciencemag.org/content/304/5667/78.abstract",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Improving reservoirs using Intrinsic Plasticity",
      "author" : [ "B. Schrauwen", "M. Wardermann", "D. Verstraeten", "J.J. Steil", "D. Stroobandt" ],
      "venue" : "Neurocomputing, vol. 71, pp. 1159–1171, March 2007.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Backpropagation-Decorrelation: online recurrent learning with O(n) complexity",
      "author" : [ "J.J. Steil" ],
      "venue" : "Proceedings of IJCNN’04, vol. 1, 2004.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Decoupled Echo State Networks with lateral inhibition",
      "author" : [ "Y. Xue", "L. Yang", "S. Haykin" ],
      "venue" : "Neural Networks, no. 3, pp. 365–376, 2007.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Optimization and applications of Echo State Networks with leaky-integrator neurons",
      "author" : [ "H. Jaeger", "M. Lukos̆evic̆ius", "D. Popovici", "U. Siewert" ],
      "venue" : "Neural Networks, no. 3, pp. 335–352, 2007.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Numerical Recipes in C, 2nd ed",
      "author" : [ "W. Press", "S. Teukolsky", "W. Vetterling", "B. Flannery" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1992
    }, {
      "title" : "Minimum Complexity Echo State Network",
      "author" : [ "A. Rodan", "P. Tin̆o" ],
      "venue" : "IEEE Transactions on Neural Networks, pp. 131–144, 2011. [Online]. Available: http://dx.doi.org/10.1109/TNN.2010.2089641",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multiscale Internet traffic forecasting using neural networks and time series methods",
      "author" : [ "P. Cortez", "M. Rio", "M. Rocha", "P. Sousa" ],
      "venue" : "Expert Systems, 2012. [Online]. Available: http://dx.doi.org/10.1111/j. 1468-0394.2010.00568.x",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Time Series Data Library",
      "author" : [ "R. Hyndman" ],
      "venue" : "Accessed on: August 31, 2012. [Online]. Available: http://robjhyndman.com/TSDL/ miscellaneous/",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Reservoir Computing Trends",
      "author" : [ "M. Lukos̆evic̆ius", "H. Jaeger", "B. Schrauwen" ],
      "venue" : "KI - Künstliche Intelligenz, pp. 1–7, 2012. [Online]. Available: http://dx.doi.org/10.1007/s13218-012-0204-5",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "New results on recurrent network training: unifying the algorithms and accelerating convergence",
      "author" : [ "A.F. Atiya", "A.G. Parlos" ],
      "venue" : "IEEE Trans. Neural Networks, vol. 11, pp. 697–709, 2000.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Self-Organizing Maps and Scale-Invariant Maps in Echo State Networks",
      "author" : [ "S. Basterrech", "C. Fyfe", "G. Rubino" ],
      "venue" : "Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on, nov. 2011, pp. 94 –99.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On self-organizing reservoirs and their hierarchies",
      "author" : [ "M. Lukos̆evic̆ius" ],
      "venue" : "Jacobs University, Bremen, Tech. Rep. 25, 2010.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Gelenbe in 1989 [1], is a mathematical object inspired by biological neuronal behavior which",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "In order to use RandNNs in supervised learning problems, a gradient descent algorithm has been described in [2], and Quasi-Newton methods have been proposed in [3], [4].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "In order to use RandNNs in supervised learning problems, a gradient descent algorithm has been described in [2], and Quasi-Newton methods have been proposed in [3], [4].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "In order to use RandNNs in supervised learning problems, a gradient descent algorithm has been described in [2], and Quasi-Newton methods have been proposed in [3], [4].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "Additionally, the function approximation properties of the model were studied in [5], [6].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "Additionally, the function approximation properties of the model were studied in [5], [6].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "Some of them are related to the use of a feedforward topology (see [7]).",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "The main drawbacks related to learning algorithms are the following: convergence is not always guaranteed, many algorithmic parameters are involved, sometimes long training times are required [8], [9].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "The main drawbacks related to learning algorithms are the following: convergence is not always guaranteed, many algorithmic parameters are involved, sometimes long training times are required [8], [9].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 9,
      "context" : "About ten years ago two main RC models were proposed: Echo State Networks (ESNs) [10] and Liquid State Machines (LSMs) [11].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "About ten years ago two main RC models were proposed: Echo State Networks (ESNs) [10] and Liquid State Machines (LSMs) [11].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "The RC approach have been successfully applied in many machine learning tasks achieving goods results, specially in temporal learning tasks [8], [11], [12].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "The RC approach have been successfully applied in many machine learning tasks achieving goods results, specially in temporal learning tasks [8], [11], [12].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 11,
      "context" : "The RC approach have been successfully applied in many machine learning tasks achieving goods results, specially in temporal learning tasks [8], [11], [12].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "A Random Neural Network (RandNN) is a specific queuing network proposed in [1] which merges concepts from spiking neural networks and queuing theory.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Gelenbe in [1], [13] shows that in an equilibrium situation the %us satisfy the following non-linear system of equations:",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : "Gelenbe in [1], [13] shows that in an equilibrium situation the %us satisfy the following non-linear system of equations:",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "For more details and proofs, see [1], [5].",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "For more details and proofs, see [1], [5].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "The model has been widely used in fields such as: combinatorial optimization, machine learning problems, communication networks and computer systems [14]–[18].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "The model has been widely used in fields such as: combinatorial optimization, machine learning problems, communication networks and computer systems [14]–[18].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "They are also very useful for building associative memories, in data compression and for static pattern classification [8].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 18,
      "context" : "From this point of view, the reservoir idea is similar to the expansion function used in Kernel Methods, for example in the Support Vector Machine [19].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "The projection can enhance the linear separability of the data [12].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "The RC approach is based on the empirical observation that under certain assumptions, training only a linear readout is often sufficient to achieve good performance in many learning tasks [8].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 19,
      "context" : "For instance, the ESN model has the best known learning performance on the Mackey–Glass times series prediction task [20], [21].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "For instance, the ESN model has the best known learning performance on the Mackey–Glass times series prediction task [20], [21].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "To ensure good properties in the reservoir, the w matrix is usually scaled to control its spectral radius (to have ρ(w) < 1) [10].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "The role of the spectral radius is more complex when the reservoir is built with spiking neurons (in the LSM model) [12], [22].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 23,
      "context" : "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 19,
      "context" : "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 25,
      "context" : "The output weights w can be computed using some of the traditional algorithms to solve regressions such as the “ridge regression” or the least mean square algorithms [27].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 26,
      "context" : "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 28,
      "context" : "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 0,
      "context" : "2] and [0, 1],",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "The preprocessing data step consisted in rescaling the data in the interval [0, 1].",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : "The learning method used was offline ridge regression [31].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "We rescaled it in [0, 1].",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 27,
      "context" : "This configuration was suggested in [29] where the authors discuss different neural network topologies taking into account seasonal traits of the data.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "The network input at any time t is the triple composed of the traffic at times t, t − 6 and t − 7, as studied in [29].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 30,
      "context" : "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 31,
      "context" : "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : "For the last two data sets the performance using NN, ARIMA and Holt-Winters methods can be seen in [29].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "A typical ESN model consists in a reservoir with the following characteristics: random topology, Nx large enough, sparsely connected (roughly between 15% and 20% of their weights are non-zeros) [8].",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 11,
      "context" : "In [12], the authors obtained the best performance in the NARMA data problem when the spectral radius was close to 1.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "The ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "The ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : "The ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 32,
      "context" : "The ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34].",
      "startOffset" : 177,
      "endOffset" : 181
    } ],
    "year" : 2012,
    "abstractText" : "In the last decade, a new computational paradigm was introduced in the field of Machine Learning, under the name of Reservoir Computing (RC). RC models are neural networks which a recurrent part (the reservoir) that does not participate in the learning process, and the rest of the system where no recurrence (no neural circuit) occurs. This approach has grown rapidly due to its success in solving learning tasks and other computational applications. Some success was also observed with another recently proposed neural network designed using Queueing Theory, the Random Neural Network (RandNN). Both approaches have good properties and identified drawbacks. In this paper, we propose a new RC model called Echo State Queueing Network (ESQN), where we use ideas coming from RandNNs for the design of the reservoir. ESQNs consist in ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The paper positions ESQNs in the global Machine Learning area, and provides examples of their use and performances. We show on largely used benchmarks that ESQNs are very accurate tools, and we illustrate how they compare with standard ESNs. Index Terms Reservoir Computing, Echo State Network, Random Neural Network, Queueing Network, Machine Learning",
    "creator" : "LaTeX with hyperref package"
  }
}