{
  "name" : "1407.1538.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Large-Scale Multi-Label Learning with Incomplete Label Assignments",
    "authors" : [ "Xiangnan Kong", "Zhaoming Wu", "Li-Jia Li", "Ruofei Zhang", "Philip S. Yu", "Hang Wu", "Wei Fan" ],
    "emails" : [ "xkong4@uic.edu", "lijiali@yahoo-inc.com", "psyu@cs.uic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Multi-label learning deals with the classification problems where each instance can be assigned with multiple labels simultaneously. Conventional multi-label learning approaches mainly focus on exploiting label correlations. It is usually assumed, explicitly or implicitly, that the label sets for training instances are fully labeled without any missing labels. However, in many real-world multi-label datasets, the label assignments for training instances can be incomplete. Some groundtruth labels can be missed by the labeler from the label set. This problem is especially typical when the number instances is very large, and the labeling cost is very high, which makes it almost impossible to get a fully labeled training set. In this paper, we study the problem of large-scale multi-label learning with incomplete label assignments. We propose an approach, called Mpu, based upon positive and unlabeled stochastic gradient descent and stacked models. Unlike prior works, our method can effectively and efficiently consider missing labels and label correlations simultaneously, and is very scalable, that has linear time complexities over the size of the data. Extensive experiments on two real-world multi-label datasets show that our Mpu model consistently outperform other commonly-used baselines."
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-label learning has drawn much attention in recent years, where each data example can be assigned with multiple labels simultaneously. Multi-label learning has a wide range of real-world applications. For example, in text categorization, one text document can belong to multiple categories, such as sports and entertainment ; Researchers in text mining are interested in learning a\n∗Computer Science Department, University of Illinois at Chicago, USA. xkong4@uic.edu †Computer Science Department, Tsinghua University, China ‡Yahoo! Research, USA. lijiali@yahoo-inc.com §Microsoft, USA. ¶Computer Science Department, University of Illinois at Chicago, USA. psyu@cs.uic.edu ‖Computer Science Department, Tsinghua University, China ∗∗Huawei Noah’s Ark Lab, Hong Kong, China.\nIn st\nan ce\nl s et\nmodel that can automatically predict a set of categories for each text document. Similarly, in image annotation tasks, one image can contain multiple objects or tags, and researchers in computer vision are interested in automatically predicting tags/objects for unlabeled image collections.\nConventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process. It is usually assumed, explicitly or implicitly, that all the label sets for training instances are fully labeled, i.e., all the labels within a label set of an instance should be completely annotated by a la-\nar X\niv :1\n40 7.\n15 38\nv1 [\ncs .L\nG ]\n6 J\nul 2\n01 4\nbeler without any missing labels. However, in many real-world multi-label learning tasks, it is very hard or expensive to get a fully labeled dataset, especially when the number of classes and/or instances are very large. It is usually much easier to get a set of partially labeled data, where some ground-truth labels for training instances can be missed by the labeler. For example, in Figure 1, we should an image that contain many tags in its ground-truth label set. It usually happens that the labeler may only want to annotate a small number of the labels for the image, instead of going through all possible labels in the vocabulary. In this example, many true labels of the image are missed by the labeler. If we directly use existing multi-label learning methods on such datasets, the missing labels in the training data will be treated as negative examples, and the performances of multi-label classification will degenerate greatly due to the simple treatment.\nIn this paper, we study the problem of large-scale multi-label learning with incomplete label assignments, as shown in Figure 2. Despite its value and significance, large-scale multi-label learning with incomplete label assignments is a much more challenging task due to the specific characteristics of the task. The reasons are listed as follows.\n• Incomplete Label Assignments. Most existing multi-label learning methods, such as Ml-knn [25] and Rank-svm [5], assume that the training data are fully labeled. However, in most real-world applications, the label assignments for training instances can be incomplete. Thus we cannot simply treat the missing labels as negative examples. A label that is not annotated by the labeler can still belong the ground-truth label set. We need to consider the incomplete label assignments explicitly while building our models on training data.\n• Label Correlations. Positive and Unlabeled learning methods [6, 16] can usually handle the cases when the label assignments are partially missing under single label settings. However, in multi-label learning problems, each instance can be assigned with multiple labels. Different class labels are correlated with each other, instead of being independent. We need to exploit the label correlations to facility the learning process of multi-label classification.\n• Scalability. Previous approaches on multi-label learning with incomplete label assignments [21, 2, 18] are mainly designed for small/moderatesized datasets. However, many real-world problems involve a large number of instances. In large-scale\ndatasets, it is even more typical to encounter the incomplete labeling issues due to the huge cost of labeling. In these cases, it is even more important that the learning method can handle large-scale datasets, with time complexities that are linear to both the number of instances and the number of classes.\nIn order to solve the above issues, we propose a novel solution, called Mpu, to learn from partially labeled training instances and can exploit label correlations to facilitate the learning process. Different from previous work, the proposed Mpu can scale to largescale problems with time complexity that is linear in both the number instances and number of classes. Empirical studies on real-world datasets show that the proposed method can significantly boost the performance of multi-label classification by considering missing labels and incorporating label correlations.\nThe rest of the paper is organized as follows. We start by introducing the preliminary concepts, giving the problem analysis and present the Mpu approach in Section 2. Then Section 3 reports the experiment results. We briefly review on related works of multi-label learning and learning from missing labels in Section 4. In Section 5, we conclude the paper."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "Given n data points of the form (xi,yi), where xi ∈ X ⊆ RD is a D dimensional vector denoting the features of the i-th instance. yi = (y 1 i , · · · , y q i ) > ∈ {−1, 1}q corresponds to its set of ground-truth labels within a fixed dictionary of q possible labels. yki = 1 if the k-th label is in the label set of instance i, otherwise yki = −1. In many real-world multi-label learning tasks, the training data are usually not fully labeled, where each instance can be labeled with a subset of the ground-truth labels. We call such settings as multilabel learning with incomplete label assignments or weak labeling problems, following the definition in [21]. Specially, in the training set, the ground-truth label set\nyi for each instance is not available. Instead, we only know a set of labels that are annotated by the labeler, denoted as si = (s 1 i , · · · , s q i ) > ∈ {−1, 1}q, where ski ≤ yki (∀1 ≤ i ≤ n, ∀1 ≤ k ≤ q). Thus, si only provides partial labels for instance xi. When s k i = 1, y k i = 1 is certain.\nPr ( ski = 1|xi, yki = −1 ) = 0, ∀i, k\nBut when ski = −1, either yki = −1 or yki = 1 can be true. In multi-label learning with incomplete label assignments, the training set is a set of partially labeled instances D = {(xi, si,yi)}ni=1, where yi’s are unknown. The learning task is to learn a prediction model f(·, ·) from D, such that for any unseen test data xi, the prediction f(xi, ·) should be close to the ground-truth, i.e., for any z ∈ {−1, 1}q, f(xi, z) = Pr(yi = z|xi) as close as possible.\nThe key issues of multi-label learning with incomplete label assignments are as follows:\n• How can we estimate the true label sets for the training data and use them to facilitate the training process of multi-label learning?\n• How can we exploit the correlations among multiple labels to improve the multi-label classification performances?\nIn the following sections, we will first introduce a model to estimate missing labels in the training set based upon PU (i.e., Positive and Unlabeled) stochastic gradient descent. Next we will describe our framework for incororating the correlations among labels based upon stack models."
    }, {
      "heading" : "2.1 Handling Missing Labels via PU Stochastic",
      "text" : "Gradient Descent In this subsection, we first address the problem of missing label assignments while assuming all labels are independent from each other. Then in the next subsection, we will show further how to extend the model to consider label correlations.\nOne simple solution for multi-label learning is to one-vs-all decomposition by treating a multi-label clas-\nsification problem as multiple binary classification problems (one for each label):\nPr(yi|xi) = q∏\nk=1\nPr ( yki = 1|xi ) Inspired by the positive and unlabeled learning in single-label classification [6], we propose a method, called PU Stochastic Gradient Descent, which can handle large-scale datasets with missing label assignments. Let {wk}qk=1 be a set of parameters of our classifier, where wk ∈ RD. According to the principle of maximum likelihood, we need to find the optimized w∗k to maximize the likelihood of yki ’s.\nwk ∗ = arg max\nwk\nlog\n( n∏\ni=1\nPr ( yki = 1|xi,wk\n))\nIn this method, we extend logistic regression to classification problems with incomplete label assignments as follows. Assume that yki satisfies a Bernouli distribution, and\nPr ( yki = 1|xi,wk ) =\n1\n1 + exp(−wk>xi)\nFollowing the assumption in [6], we assume that annotated labels are randomly sampled from the groundtruth label set with a constant rate c, where the sampling process is totally independent everything else, such as the feature of the instance. Assume that the probability that a label is not missing by the labeler is an unknown constant c.\nc = Pr ( ski = 1|yki = 1 ) = Pr ( ski = 1|yki = 1,xi,wk ) Here c can be directly estimated from the training set using cross validation process in [6]. With Bayes’ theorem, we have\nPr ( yki = 1|xi,wk ) =\nPr ( ski = 1|xi,wk ) Pr ( ski = 1|yki = 1,xi,wk )\nThen, we have\nPr ( ski = 1 | xi,wk ) = c · Pr ( yki = 1|xki ,wk ) = c\n1 + exp(−wk>xi)\n= c 1 + exp ( −ski wk>xi ) + (1− ski )(1− c) 2\nPr ( ski = −1 | xi,wk ) = 1− c · Pr ( yki = 1|xki ,wk ) = c\n1 + exp ( −ski wk>xi ) + (1− ski )(1− c) 2\nThus, we can get\nwk ∗ = arg max\nwk n∑ i=1 log Pr ( ski |xi,wk ) = arg max\nwk n∑ i=1 log ( c 1 + exp(−wk>xi) )\n= arg max wk n∑ i=1 log\n( c\n1 + exp ( −ski wk>xi ) + (1− ski )(1− c) 2\n)\nIn order to scale to large-scale problems, we use stochastic gradient descent to solve the above logistic regression problem efficiently. Different from conventional stochastic gradient descent approaches, which assume all the labels are availabel, we consider the incomplete label assignments and define the loss function as follows: l(wk,D) = − n∑\ni=1\nlog  c 1 + exp ( −ski wk > xi ) + (1− ski )(1− c) 2 "
    }, {
      "heading" : "2.2 Handling Label Correlations via Stacked",
      "text" : "Graphical Models In the previous subsection, we discussed how can we deal with incomplete label assignments in multi-label learning. Now we show how can we use the previous model to further consider label correlations.\nInspired by the collective classification methods in [15, 7] based on stacking, we proposed a multi-label learning method called Mpu. Mpu can consider the label correlations effectively using stacked graphical model, which does not rely on joint inference for all labels. Stacking [23] is one type of ensemble methods which build a chain of models. Each model in the stacking uses the outputs of previous models as the inputs.\nA stacked multi-label model allows inferences about one label to influence inferences about other labels but uses a different mechanism than other approaches to multilabel ensemble [4]. Rather than using joint inference to propagate inferences among labels, the stacked model uses one base model to predict the class labels for each label and uses those inferred labels as input to another stacked model. Learning: The learning algorithm to learn the stacked model is shown in Figure 3. First, we use the PU stochestic gradient descent method to learn a base model to predict the labels of the instances using the instance features. This base model is then used to infer labels for each of the instances. In order to avoid overfitting, or any bias from applying the base model on the same data from which it was trained, we use a cross-validation procedure (shown in Figure 4) to infer the estimated outputs of the base model as the inputs for the next stacked model.\nNext, we then construct an extended feature set to learn a stacked model using both features and estimated outputs of previous models as the inputs. In this way, we can build many levels of base models in a stacked model, where each subsequent base model uses the estimated predictions of class labels from the previous levels. In most cases, one single level of stacking is sufficient for multi-label learning, that can consider complex label correlations. Inference: During inference process, we take turns to apply the base models from different levels one by one. The outputs of the model in previous level is directly used as the inputs of the next level in the stacking. Then the base models in the last level produces the final predictions. Different from other multi-label ensemble methods that learn on true labels, which are not known at the inference time, we learn the\nstacked model on the inferred labels, where all input features are known at inference time. Such design can permit exact inference, while other ensemble methods require approximate inference techniques [4], such as classifier chains."
    }, {
      "heading" : "3 Experiments",
      "text" : "3.1 Data Collection In order to evaluate the performances of the proposed approach for multi-label classification with missing labels, we tested our algorithm on two datasets as summarized in Table 2.\n1) RCV1 Small (Topics Subset): The first dataset we used in this paper is a medium scale dataset, i.e., RCV1v2 Topics subset1, in order to test the performances of different multi-label learning methods on medium scale problems. The dataset consists of 6,000 news articles which are categorized into 101 classes. For each news article, the bag-of-words features are extracted resulting in 47,236 features.\n2) RCV1 Large (Topics): The second dataset we used is a large-scale dataset, i.e., RCV1v2 Topic full set, in order to test the scalability of different multi-label learning methods on large-scale problems. The dataset consists of 804,414 news articles which are categorized into 101 classes. The same number of bag-of-words features are extracted as the previous small dataset.\n3.2 Evaluation Metric In order to evaluate the performance of multi-label learning by the models, we follow previous works [9, 13, 17] by using Micro-F1 as the performance measure. Suppose that a multi-label dataset is D = {(xi,yi)}ni=1, which consists of n multilabel instances. yi ∈ {0, 1}q (i = 1, · · · , n). Let h(·) be a multi-label classifier, and its predicted label set for xi is h(xi).\nMicro-F1 is the harmonic mean of Micro-Precision and Micro-Recall. The Micro-Precision is the Micro-\n1http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/\ndatasets/multilabel.html\naverage of precision over all labels . Similarly, MicroRecall is the Micro-average of recall overal all possible labels.\nMicro-F1(h,D) = 2× ∑n i=1 ‖h(xi) ∩ yi‖1∑n\ni=1 ‖h(xi)‖1 + ∑n i=1 ‖yi‖1\nThe larger the value of Micro-F1, the better the performance of multi-label classification model.\nAll experiments are conducted on machines with Intel XeonTMQuad-Core CPUs of 2.26 GHz and 36 GB RAM.\n3.3 Comparative Methods In order to study the effectiveness of the proposed approach, we compare our method with different baseline methods, including a large scale multi-label learning method, a multi-label learning method with missing labels and a positive and an unlabeled learning method. The compared methods are summarized as follows:\n• Large-scale Multi-label Learning (M3L) [12]: The first baseline method is a multi-label classification method for large scale problems. We compared the M3L methods with two different kernels, i.e., Linear kernel (denoted as M3L Linear) and RBF kernel (denoted as M3L RBF), separately.\n• Positive and Unlabeled Learning (Elkan08) [6]: The second baseline method is a PU learning method which handle the cases where some positive instances can be missed by the labeler. This method is originally designed for binary classification problems. We use the binary decomposition method to solve multi-label classification problems by training one model over each class [1].\n• Multi-label Learning with Missing Labels (Well) [21]: we also compare with another baseline which\nare designed for multi-label learning with incomplete label assignments. The method can train a model on weakly labeled multi-label instances, and predict complete label sets on testing data. It can handle moderate-size datasets, but cannot scale to large-scale datasets. We use default parameter settings for this method.\n• Large-scale Multi-label Learning with Missing Labels (Mpu): the proposed method in this paper for large-scale multi-label learning with incomplete label assignments. Mpu can explicitly consider label correlations to facilitate multi-label learning process when a part of the ground-truth labels are missing in the label set. For simipicity, we set the number of stacking levels to the minium value 2.\n3.4 Performances on Small Dataset We first study the effectiveness of the proposed Mpu method on multi-label classification with incomplete label assignment. In our experiment, 10-fold cross validation is performed on the small data set to evaluate the multilabel classification performances. In each round of the cross validation, the instances are partitioned into two groups: 9 folds are used as training data, the remaining fold is used as testing data. In order to simulate the incomplete label assignments, we randomly sample and remove a subset of the labels from each of label sets in training data according to a ratio, called missing rate. For example, if the missing rate is 20%, we randomly sample 20% of the labels from the ground-truth labels of training instances and remove them from the train set. The higher the missing rate, the more ground-truth labels are missed by the labeler in the training set. We report the average results of 10-fold cross validation on the dataset.\nWe study the performance of the proposed Mpu method on multi-label classification with different number of missing rates: 0%, 10%, 20%, etc. When the missing rate is 0%, it represents the setting of conventional multi-label learning where the training instances are fully labeled. In real-world multi-label learning, we can usually only have a small number of labels annotated in the label set.\nThe results of all compared methods are shown in Figure 5. Firstly, we can observe that when the training set is fully labeled (i.e., missing rate is 0%), all multi-label learning methods outperform the singlelabel learning method, Elkan08. In general, these results support the importance of exploiting correlations among different class labels in multi-label learning problems. Because the method Elkan08 is based upon onevs-all decomposition, where different labels are assumed to be independent, thus it cannot work well in multilabel learning tasks when different labels are correlated.\nWe also observe that when the missing rate increases, the performances of M3L decrease very quickly. While all the other methods that can consider missing labels are relatively stable in their performances. This is because M3L is designed for supervised multi-label learning problems, which assumes all the training instances are fully labeled. When a label l is missed in the label set of a training instance, M3L will consider the instance as a negative example for the label l. This result can support the importance of considering missing labels in the training data, by assuming that some labels in the ground-truth label set can be missed by the labeler. Thus, for each single class label, the ‘negative’ examples are no longer pure negative examples, but are mixture of both positive examples and negative\nexamples. We further observe that our proposed method Mpu outperforms all the other methods on all missing rates. Mpu can estimate the missing rate of the training data automatically, and can utilize label correlations to improve the classification performances. This result can support the importance of considering both label correlations and missing labels at the same time.\n3.5 Performances on Large Dataset In our second experiment, we evaluate the efficiency of different multi-label learning methods on a large-scale dataset, which consists of around 800K instances. In this section, we compare both the Micro-F1 performances and running time in the training step. We reported the performances of Micro-F1 in Figure 6. We only showed the\nmethods that can finish running within a week. The methods show similar properties to those in the small datset. The M3L method initially has good performances when the training set is fully labeled. However when the missing rate increases, the performances of M3L drop very fast. The performances of Mpu method is quite stable. When the missing rate increases, the performances are still pretty good. Mpu can outperform M3L when the missing rate is greater than 30%.\nWe also show the training time of all methods in both dataset in Figure 7. We can observe that both Mpu and M3L can scale well to large-scale datasets. While the remaining methods such as Well cannot scale to datasets in this size. It is because the time complexity of Well is O(n2) in the number of training instances (n). Both Mpu and M3L are linear O(n) in the number of instances."
    }, {
      "heading" : "4 Related Work",
      "text" : "To the best of our knowledge, this paper is the first work addressing the problem of large-scale multi-label classification with incomplete label assignments. Our work is related to both multi-label learning techniques and positive and unlabeled learning. We briefly discuss both of them.\nMulti-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11]. The key challenge of multi-label learning is the large space of all label sets, i.e. the power set of all labels. In order to tackle this challenging task, many multi-label learning approaches focus on utilizing the labels correlations to facilitate the learning process. Conventional multi-label learning approaches can be roughly categorized as follows: (a) one-vs-all approaches: This type of approaches treat different labels as independent by converting the multi-label problem into multiple binary classification problems (one for each label) [1]. Zhang and Zhou[25] proposed Ml-knn, a binary method by extending the kNN algorithm to a multi-label problems using maximum a posteriori (MAP) principle to determine the label set predictions. (b) pairwise correlations: This type of approaches mainly use the pairwise relationships among different labels to facilitate the learning process [9]. Elisseeff and Weston [5] proposed Ranksvm, a kernel method by minimizing ranking loss to rank label pairs. (c) High-order correlations [24, 24]: This type of approaches can utilize higher order relationships among different labels. Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].\nThe work in [12] studied the large-scale multilabel leanring problems, and proposed an efficient ap-\nproach M3L. In addition, multi-label learning with incomplete label assignment has also been studied on small/moderate-size datasets [21, 2, 18]. The work in [21] proposed an approach Well to infer missing labels in multi-label learning under transductive settings.\nOur work is also related to another line of research, called positive and unlabeled learning, or PU learning [16, 6]. PU learning corresponds to the binary classification problems where some positive samples can be mislabeled. Thus in the training set, only positive and unlabeled examples are available, no reliable negative examples are given in the training set. Many previous works on PU learning focus on estimating reliable negative examples from the unlabeled dataset, and utilize the estimated labels to improve the classification performances, The work in [16] proposed a method based upon biased SVM. Initially all the unlabeled instances are treated as negative examples. But the cost of classifying an unlabeled example with positive label is lower than that of predicting positive examples with negative labels. Elkan and Noto [6] provided a statistic model for positive and unlabeled learning. It is assumed that the ground-truth labels are randomly sampled into the training set according to a constant factor. The random sampling process is assumed to be independent from everything else, such as features of the instances. The constant factor can be estimated using cross-validation process on the training data. Then most conventional classification models can be modified according to the factor to consider the missing labels."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have described and studied the problem of large-scale multi-label learning with incomplete label assignments. We have studied two real-world datasets, one small and one large to evaluate the performance of our proposed method. Different from previous works in multi-label learning, we consider missing labels in the training set and label correlations simultaneously. By explicitly consider the missing labels using positive and unlabeled learning model, and label correlations using stacking models, our method can effectively boost the performance of multi-label classification with partially labeled training data."
    }, {
      "heading" : "6 Acknowledgements",
      "text" : "This work is supported in part by NSF through grants CNS-1115234, DBI-0960443, and OISE-1129076, US Department of Army through grant W911NF-12-1-0066, and Huawei Grant."
    } ],
    "references" : [ {
      "title" : "Learning multi-label scene classification",
      "author" : [ "M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Multi-label learning with incomplete class assignments",
      "author" : [ "S. Bucak", "R. Jin", "A.K. Jain" ],
      "venue" : "In The 24th IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Combining instancebased learning and logistic regression for multilabel classification",
      "author" : [ "W. Cheng", "E. Hüllermeier" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Bayes optimal multilabel classification via probabilistic classifier chains",
      "author" : [ "K. Dembczyński", "W. Cheng", "E. Hüllermeier" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "A kernel method for multilabelled classification",
      "author" : [ "A. Elisseeff", "J. Weston" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Learning classifiers from only positive and unlabeled data",
      "author" : [ "C. Elkan", "K. Noto" ],
      "venue" : "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "Why stacked models perform effective collective classification",
      "author" : [ "A. Fast", "D. Jensen" ],
      "venue" : "In Proceedings of the 8th IEEE International Conference on Data Mining,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Random k-labelsets: An ensemble method for multilabel classification",
      "author" : [ "I.V.G. Tsoumakas" ],
      "venue" : "In Proceedings of 18th European Conference on Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Collective multi-label classification",
      "author" : [ "N. Ghamrawi", "A. McCallum" ],
      "venue" : "In Proceedings of ACM CIKM International Conference on Information and Knowledge Management,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Discriminative methods for multi-labeled classification",
      "author" : [ "S. Godbole", "S. Sarawagi" ],
      "venue" : "In Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Adaptive large margin training for multilabel classification",
      "author" : [ "Y. Guo", "D. Schuurmans" ],
      "venue" : "In Proceedings of the 25th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Large scale max-margin multi-label classificaiton with priors",
      "author" : [ "B. hariharan", "L. Zelnik-Manor", "S.V.N. Vishwanathan", "M. Varma" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Correlated label propagation with application to multi-label learning",
      "author" : [ "F. Kang", "R. Jin", "R. Sukthankar" ],
      "venue" : "IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Maximal margin labeling for multi-topic text categorization",
      "author" : [ "H. Kazawa", "T. Izumitani", "H. Taira", "E. Maeda" ],
      "venue" : "In NIPS",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "Stacked graphical models for ecient inference in markov random fields",
      "author" : [ "Z. Kou", "W. Cohen" ],
      "venue" : "In Proceedings of the 7th SIAM International Conference on Data",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Building text classifiers using positive and unlabeled examples",
      "author" : [ "B. Liu", "Y. Dai", "X. Li", "W.S. Lee", "P.S. Yu" ],
      "venue" : "In Proceedings of the 3rd IEEE International Conference on Data Mining,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Semi-supervised multilabel learning by constrained non-negative matrix factorization",
      "author" : [ "Y. Liu", "R. Jin", "L. Yang" ],
      "venue" : "In The 21st National Conference on Artificial Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "Mining partially annotated images",
      "author" : [ "Z. Qi", "M. Yang", "Z. Zhang" ],
      "venue" : "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Multi-label classification using ensembles of pruned sets",
      "author" : [ "J. Read", "B. Pfahringer", "G. Holmes" ],
      "venue" : "In Proceedings of the 8th IEEE International Conference on Data Mining,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2008
    }, {
      "title" : "Classifier chains for multi-label classification",
      "author" : [ "J. Read", "B. Pfahringer", "G. Holmes", "E. Frank" ],
      "venue" : "In The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Multi-label learning with weak label",
      "author" : [ "Y. Sun", "Y. Zhang", "Z. Zhou" ],
      "venue" : "In The 24th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Parametric mixture models for multi-labeled text. In NIPS, pages 721–728",
      "author" : [ "N. Ueda", "K. Saito" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2003
    }, {
      "title" : "Stacked generalization",
      "author" : [ "D. Wolpert" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1992
    }, {
      "title" : "Multi-label learning by exploiting label dependency",
      "author" : [ "M.-L. Zhang", "K. Zhang" ],
      "venue" : "In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Ml-knn: A lazy learning approach to multi-label learning",
      "author" : [ "M.-L. Zhang", "Z.-H. Zhou" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2007
    }, {
      "title" : "Multi-label learning by instance differentiation",
      "author" : [ "M.-L. Zhang", "Z.-H. Zhou" ],
      "venue" : "In Proceedings of the 21nd AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "Most existing multi-label learning methods, such as Ml-knn [25] and Rank-svm [5], assume that the training data are fully labeled.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "Most existing multi-label learning methods, such as Ml-knn [25] and Rank-svm [5], assume that the training data are fully labeled.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "Positive and Unlabeled learning methods [6, 16] can usually handle the cases when the label assignments are partially missing under single label settings.",
      "startOffset" : 40,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "Positive and Unlabeled learning methods [6, 16] can usually handle the cases when the label assignments are partially missing under single label settings.",
      "startOffset" : 40,
      "endOffset" : 47
    }, {
      "referenceID" : 20,
      "context" : "Previous approaches on multi-label learning with incomplete label assignments [21, 2, 18] are mainly designed for small/moderatesized datasets.",
      "startOffset" : 78,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "Previous approaches on multi-label learning with incomplete label assignments [21, 2, 18] are mainly designed for small/moderatesized datasets.",
      "startOffset" : 78,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "Previous approaches on multi-label learning with incomplete label assignments [21, 2, 18] are mainly designed for small/moderatesized datasets.",
      "startOffset" : 78,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "We call such settings as multilabel learning with incomplete label assignments or weak labeling problems, following the definition in [21].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "Inspired by the positive and unlabeled learning in single-label classification [6], we propose a method, called PU Stochastic Gradient Descent, which can handle large-scale datasets with missing label assignments.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "Following the assumption in [6], we assume that annotated labels are randomly sampled from the groundtruth label set with a constant rate c, where the sampling process is totally independent everything else, such as the feature of the instance.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "Here c can be directly estimated from the training set using cross validation process in [6].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "Inspired by the collective classification methods in [15, 7] based on stacking, we proposed a multi-label learning method called Mpu.",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Inspired by the collective classification methods in [15, 7] based on stacking, we proposed a multi-label learning method called Mpu.",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "Stacking [23] is one type of ensemble methods which build a chain of models.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "A stacked multi-label model allows inferences about one label to influence inferences about other labels but uses a different mechanism than other approaches to multilabel ensemble [4].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "Such design can permit exact inference, while other ensemble methods require approximate inference techniques [4], such as classifier chains.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "2 Evaluation Metric In order to evaluate the performance of multi-label learning by the models, we follow previous works [9, 13, 17] by using Micro-F1 as the performance measure.",
      "startOffset" : 121,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "2 Evaluation Metric In order to evaluate the performance of multi-label learning by the models, we follow previous works [9, 13, 17] by using Micro-F1 as the performance measure.",
      "startOffset" : 121,
      "endOffset" : 132
    }, {
      "referenceID" : 16,
      "context" : "2 Evaluation Metric In order to evaluate the performance of multi-label learning by the models, we follow previous works [9, 13, 17] by using Micro-F1 as the performance measure.",
      "startOffset" : 121,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "M3L Large-Scale Multi-Label Learning ¬ Label Correlations [12] ­ Large-Scale Data",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "Elkan08 Positive & Unlabeled Learning ® Missing Label Assignments [6]",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : "Well Multi-Label Learning with Missing Labels ¬ Label Correlations [21] ® Missing Label Assignments",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "• Large-scale Multi-label Learning (M3L) [12]: The first baseline method is a multi-label classification method for large scale problems.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "• Positive and Unlabeled Learning (Elkan08) [6]: The second baseline method is a PU learning method which handle the cases where some positive instances can be missed by the labeler.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "We use the binary decomposition method to solve multi-label classification problems by training one model over each class [1].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "• Multi-label Learning with Missing Labels (Well) [21]: we also compare with another baseline which 0.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "Conventional multi-label learning approaches can be roughly categorized as follows: (a) one-vs-all approaches: This type of approaches treat different labels as independent by converting the multi-label problem into multiple binary classification problems (one for each label) [1].",
      "startOffset" : 277,
      "endOffset" : 280
    }, {
      "referenceID" : 24,
      "context" : "Zhang and Zhou[25] proposed Ml-knn, a binary method by extending the kNN algorithm to a multi-label problems using maximum a posteriori (MAP) principle to determine the label set predictions.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "(b) pairwise correlations: This type of approaches mainly use the pairwise relationships among different labels to facilitate the learning process [9].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "Elisseeff and Weston [5] proposed Ranksvm, a kernel method by minimizing ranking loss to rank label pairs.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "(c) High-order correlations [24, 24]: This type of approaches can utilize higher order relationships among different labels.",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : "(c) High-order correlations [24, 24]: This type of approaches can utilize higher order relationships among different labels.",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "The work in [12] studied the large-scale multilabel leanring problems, and proposed an efficient approach M3L.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 20,
      "context" : "In addition, multi-label learning with incomplete label assignment has also been studied on small/moderate-size datasets [21, 2, 18].",
      "startOffset" : 121,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "In addition, multi-label learning with incomplete label assignment has also been studied on small/moderate-size datasets [21, 2, 18].",
      "startOffset" : 121,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : "In addition, multi-label learning with incomplete label assignment has also been studied on small/moderate-size datasets [21, 2, 18].",
      "startOffset" : 121,
      "endOffset" : 132
    }, {
      "referenceID" : 20,
      "context" : "The work in [21] proposed an approach Well to infer missing labels in multi-label learning under transductive settings.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "Our work is also related to another line of research, called positive and unlabeled learning, or PU learning [16, 6].",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "Our work is also related to another line of research, called positive and unlabeled learning, or PU learning [16, 6].",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "Many previous works on PU learning focus on estimating reliable negative examples from the unlabeled dataset, and utilize the estimated labels to improve the classification performances, The work in [16] proposed a method based upon biased SVM.",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 5,
      "context" : "Elkan and Noto [6] provided a statistic model for positive and unlabeled learning.",
      "startOffset" : 15,
      "endOffset" : 18
    } ],
    "year" : 2014,
    "abstractText" : "Multi-label learning deals with the classification problems where each instance can be assigned with multiple labels simultaneously. Conventional multi-label learning approaches mainly focus on exploiting label correlations. It is usually assumed, explicitly or implicitly, that the label sets for training instances are fully labeled without any missing labels. However, in many real-world multi-label datasets, the label assignments for training instances can be incomplete. Some groundtruth labels can be missed by the labeler from the label set. This problem is especially typical when the number instances is very large, and the labeling cost is very high, which makes it almost impossible to get a fully labeled training set. In this paper, we study the problem of large-scale multi-label learning with incomplete label assignments. We propose an approach, called Mpu, based upon positive and unlabeled stochastic gradient descent and stacked models. Unlike prior works, our method can effectively and efficiently consider missing labels and label correlations simultaneously, and is very scalable, that has linear time complexities over the size of the data. Extensive experiments on two real-world multi-label datasets show that our Mpu model consistently outperform other commonly-used baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}