{
  "name" : "1206.6385.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improved Estimation in Time Varying Models",
    "authors" : [ "Philip Bachman", "Doina Precup" ],
    "emails" : [ "PHIL.BACHMAN@GMAIL.COM", "DPRECUP@CS.MCGILL.CA" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Locally adapted parameterizations can produce flexible representations from relatively rigid components; locally weighted regression serves as a canonical example of this approach. Such models reduce bias but increase variance, due to reduced effective sample sizes used for each estimation. We tackle this problem using a natural machine learning idea: using a transformed (more restricted or simpler) space in which to find local parameterizations.\nA common approach to improving model efficacy in machine learning is to first transform the data into an alternate representation prior to model estimation, ideally in a way that amplifies useful information while attenuating noise. Algorithms exemplifying this approach include: PCA, ICA (Hyvärinen & Oja, 2000), nonlinear-dimension reduction, e.g. (Tenenbaum et al., 2000), and dimension reduction for regression (Fukumizu et al., 2004; Cook & Forzani, 2009).\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nAnother line of work considers transformations of the model used to describe the data, either by reducing the number of degrees of freedom, or by seeking a model form amenable to more powerful estimation procedures. Examples of the first approach include DiscLDA (Lacoste-Julien et al., 2008) and supervised dimensionality reduction using Bayesian mixture models (Sajama & Orlitsky, 2005), which seek useful linear reductions of the parameters of a generative model. The second approach includes the application of spectral methods to learning transformed representations of HMMs (Siddiqi et al., 2010) and PSRs (Boots & Gordon, 2011).\nIn this paper, we provide a different lens through which to view model transformations. In Sec. 2, we present a general formulation of the problem of estimating useful transformations of model parameters, which encompasses several of the previously mentioned methods for both data and model transformation. Our problem depends on the simultaneous estimation of a transformation of the parameter space of a model and of the parameters within the transformed space. We formulate the problem primarily for use with multiply parameterized models (such as locally weighted linear regression or mixture models), which distinguishes it from the spectral methods for HMM and PSR learning, which seek single transformed parameterizations of a given model. We illustrate our problem formulation in the context of familiar models (locally weighted regression and Gaussian mixtures) in Sec. 3. In Sec. 4 we present a novel algorithm for modeling time varying sparse network structures underlying sequential observations. In Sec. 5 and 6, we use synthetic data and data drawn from real-world BCI EEG experiments to showcase our algorithm."
    }, {
      "heading" : "2. A General Problem Formulation",
      "text" : "The problem investigated in this paper arises as a generalization of the following optimization:\nB∗ = arg min B [`(f,X,B)] (1)\nwhere the loss ` measures the “goodness” of fit of the model f to the data X = {(x1, y1), ..., (xm, ym)} given\na set of parameterizations B = {β1, ..., βm′} of f , and an optimal set of parameterizations B∗ is sought.\nThe idea of using multiple model parameterizations is not often explored in machine learning. As motivation for this view, we begin by expressing standard linear regression in the form of (1). In this case, f measures the residuals produced by a parameter vector:\nf((x, y), β) = βTx− y\nFor a set of parameter vectors βi, ` is proportional to the log-likelihood of observing the residuals assuming they are normally distributed with variance σ2:\n`(f,X,B) = 1 σ2 m′∑ i=1 m∑ j=1 f((xj , yj), βi)2 (2)\nWe usually think of the loss in this case as having m′ = 1. However, note that considering m′ > 1 does not modify the solution, as loss is measured equally over all (xj , yj), which implies that βi = βj ,∀βi, βj ∈ B∗ (i.e., there still is, in effect, one optimal parameter vector).\nUsing this view, we can transform standard linear regression into kernel weighted linear regression as follows:\nf((x, y), β) = βTx− y\n`(f,X,B) = 1 σ2 m′∑ i=1 m∑ j=1 k(βxi , xj)f((xj , yj), β w i ) 2 (3)\nwhere the kernel weighting function k(x, x′) measures similarity between locations in the input space, and each βi consists of two components: βi = (βwi , β x i ). The localization component βxi associates βi with a location in the input space and the coefficient component βwi associates βi with a set of regression coefficients.\nIntroducing the kernel k allows the βi in (3) to have local rather than global effect, which leads to different parameterizations at each location in observation space. However, there is no need to optimize jointly over B∗, as there is no constraint linking different elements in a parameter set. Allowing multiple local parameterizations of f is useful for increasing the power of simple models; estimation of time varying covariance matrices in financial modeling and estimation of time varying auto-regressions in econometrics are two well-studied examples of this idea.\nWhile locally weighted regression is typically thought of as a “non-parametric” method, in the context of our work it is more fruitfully viewed as an approach based on multiple parameterization, in which the implied infinite set B∗ can be queried “lazily” for specific parameter locations βxi , rather than computed monolithically.\nTo illustrate a problem in the form of (1) in which the elements of B∗ are not independent, for βi = (β µ i , β Σ i , β π i ), consider the following optimization:\nf((x, y), β) = βπp(x|βµ, βΣ)\n`(f,X,B) = − log  m∏ j=1  m′∑ i=1 f((xj , yj), βi)  , (4) in which 0 ≤ βπi ≤ 1,∀βi and p(x|βµ, βΣ) is the probability of observing x given a Gaussian distribution with mean βµ and covariance βΣ. Minimizing (4) corresponds to estimating a Gaussian mixture model for the data X = {x1, ..., xm}. Interdependence among the βi ∈ B∗ is induced by the negative log-likelihood loss, together with a constraint on the set of mixture weights: ∑ i β π i = 1. Note that in the last two examples, the estimation of B∗ may be subject to high variance. To tackle this problem, and to exploit possible structure in the parameterizations, we introduce a “generating” function g, which takes inputs β̂ ∈ Rp (with p chosen a priori) and transforms them into outputs β. This function can be used to express both regularities and restrictions in the space of parameterization. For instance, in the case of a time varying model, the optimal, temporally local parameterizations of f may lie on a low-dimensional manifold embedded in the full parameter space of f . The structure of such a manifold could be of interest, and restricting the estimation could significantly reduce variance in the resulting parameter estimates with only a small increase in bias.\nWe can now rephrase (1) as an optimization problem involving g. Given dimension p, a model f , a loss `, and a set of inputs X , our optimization becomes:\narg min g [ min B̂ [ `(f |g,X, B̂) ]] (5)\nin which B̂ = {β̂1, ..., β̂m′} is a set of inputs to g and f |g denotes the restriction of parameterizations of f to the output space of g.\nIf we define g(β̂) ≡ β̂, then (5) exactly reproduces (1). If we allow g to take an arbitrarily complex form, then we similarly recover the optimization in (1), as we can define g(β̂i) ≡ βi for each βi ∈ B∗. Thus, interesting cases of (5) arise when g is more carefully chosen. The next section illustrates some useful problems that arise from different definitions of g, f , and `."
    }, {
      "heading" : "3. Illustrations of the Problem Formulation",
      "text" : "As a first example, consider performing a locally weighted regression analogous to that in (3), but with the local parameterizations of f restricted to a linear subspace. Let\ng(β̂) = Aβ̂w, where A is the matrix of the parameters of g. We can re-write (5) as follows:\narg min A min B̂  m′∑ i=1 m∑ j=1 k(β̂xi , xj)(x T j Aβ̂ w i − yj)2  (6) in which we now split each β̂i into a localization subcomponent β̂xi and a coefficient subcomponent β̂ w i . If one views xTj Aβ̂ w i as a reduction of xj into the subspace spanned by the columns of A, followed by a linear regression in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002). However, minor modifications, like regularizing the β̂is via λ ∑ i ||β̂wi ||1, weaken this link.\nAs a second example, we restate the mixture of Gaussians model under the constraint that the means {βµ1 , ..., β µ m′} of the parameterizations {β1, ...βm′} lie within a linear subspace of the observation space, i.e. βi = (g(β̂ µ i ), β̂ Σ i , β̂ π i )\n1, with g defined as for (6). The resulting optimization can be written as follows:\narg min A min B̂ − log  m∏ j=1  m′∑ i=1 β̂πi p(xj |Aβ̂ µ i , β̂ Σ i )  (7) Performing the optimization in (7) was shown to be useful for classification tasks in (Sajama & Orlitsky, 2005).\nWe can similarly generate optimization problems in the form of (5) whose solutions correspond to PCA and sparse coding, which are left out due to space constraints."
    }, {
      "heading" : "4. Learning Compact Representations of",
      "text" : "Time Varying Network Structure\nIn this section, we use our new problem formulation to derive a novel algorithm for estimating time varying network structure, using a time-dependent sparse combination of learned basis structures. Through an analogy between our algorithm and sparse coding (Olshausen & Field, 1996), we then extend our algorithm to learning of task-driven basis structures, guided by the work in (Mairal et al., 2011). We begin by reviewing existing work on network structure estimation, before describing the new algorithms."
    }, {
      "heading" : "4.1. Sparse Network Structure Estimation",
      "text" : "In recent years, much effort has gone into developing effective methods for estimating sparsely structured Gaussian graphical models. A Gaussian graphical model (GGM) explains a set of m n-dimensional observations X =\n1Note that we have not transformed the covariances β̂Σi\n{x1, ..., xm}, xi ∈ Rn using a set of n vertices (each corresponding to one dimension) and a set of edges, each describing the strength of the relationship between its incident vertices. A GGM implies a covariance Σ and is equivalent to modeling X with a normal distribution N (~0,Σ). Typically, prior to estimating a GGM, the observations are standardized to have mean 0.\nMany existing methods addressing GGMs focus on estimating their structure, i.e. the pattern of zero/non-zero edges. These methods typically work with the precision matrix (i.e. Σ−1) implied by a GGM, as non-zero entries in Σ−1 correspond to non-zero edges in the GGM. Estimating the structure of Σ−1 is facilitated by the following relationship:\nρij = σ̃ij√ σ̃iiσ̃jj , (8)\nin which ρij indicates the partial correlation between the ith and jth dimension conditioned on the values of all other dimensions, and σ̃ij is the entry in the ith row and jth column of Σ−1. The relationship between partial correlations and GGM structure leads to efficient methods for GGM structure estimation, as partial correlations can be directly estimated by “self-regression”.\nThe use of self-regression for network structure estimation is based on the following results (Lauritzen, 1996):\nxit = ∑ j 6=i xjt ρ̃ij + i t, (9)\nin which xit represents the value of the i th dimension of the tth observation in X , ρ̃ij is a real-valued scalar, and it is uncorrelated with xit if and only if:\nρ̃ij = − σ̃ij σ̃ii = ρij √ σ̃jj σ̃ii , from which (10)\nρij = sign(ρ̃ij) √ ρ̃ij ρ̃ji. (11)\nHence, ρ̃ij can be efficiently estimated for any given i using linear regression of the response variables {xi1, ..., xim} on the covariates {x\\i1 , ...x \\i m}, in which x\\it indicates a vector including all dimensions except i; ρij can then be computed as well.\nMost existing methods for GGM structure estimation assume that Σ−1 is sparse. Value estimation methods estimate each entry in Σ−1 (Zhou et al., 2010), while structure estimation determines the pattern of zero/non-zero entries (Friedman et al., 2008; Song et al., 2009b; Kolar & Xing, 2011)). The sparsity assumption can be incorporated into the self-regression process by using sparsifying regression techniques, such as the well known Lasso (Tibshirani, 1996). Self-regression methods using sparsity have been shown to produce consistent estimates of structure in\nΣ−1 under suitable conditions (Meinshausen & Bühlmann, 2006; Wainwright et al., 2007; Kolar & Xing, 2011).\nA recent line of work focuses on extending methods for network structure estimation for the case when structures vary over time (Ahmed & Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar & Xing, 2011). We focus on the KELLER algorithm from (Song et al., 2009a), as our algorithm can be seen as its natural generalization using the problem formulation in (5). The KELLER algorithm is predicated on two assumptions: sparsity in the time varying network structure, and smoothness in the changes of these structures over time. This second assumption distinguishes KELLER from methods such as (Ahmed & Xing, 2009) and (Kolar et al., 2009), which assume abrupt changes in the network structure.\nTo estimate the structure of a network at time t, given a sequence of T observations X = {x1, .., xT |xi ∈ Rn}, KELLER performs a set of n independent `1-regularized locally weighted regressions, with the ith regression estimating the values ρ̃ij ,∀j 6= i as described above. By using locally weighted regression, these values are specifically adapted to the predominant network structure affecting the observation at time t. For time t, these regressions can be written compactly as follows:\nA∗t = arg min A∈Rn×n T∑ t′=1 k(t, t′) ||xt′ −Axt′ ||22 + λ||A||1, (12)\nin which k(t, t′) computes a kernel weight measuring temporal proximity, diagonal entries of A are fixed at 0, ||A||1 is the entry-wise matrix 1-norm (i.e. ∑ i ∑ j |Aij |), and λ controls the `1 regularization, which determines the sparsity of A. After estimating A∗ according to (12), KELLER performs a simple procedure to make the implied structure estimate coherent with the assumption of an undirected GGM (i.e. A∗ should be symmetric), which consists of inferring an edge between any pair of vertices (i, j) such that Aij 6= 0 or Aji 6= 0. An estimation similar to (12) is used in (Song et al., 2009b), without the additional symmetrization, for networks with directed edges.\nAs used in (12), the weighting kernel makes the estimate of A∗t at time t effectively independent from observations at times remote from t. This can lead to high variance, and ignores potential structure in the way in which the network structure changes over time. We will now state our algorithm, which addresses these problems."
    }, {
      "heading" : "4.2. Estimating Network Structures as Combinations of Basis Structures",
      "text" : "We reformulate the optimization in (12) similarly to the way in which we generalized locally weighted regression from (3) to the form (6). At each time t, the optimal A∗t\nis estimated as a linear combination of a set of k basis matrices Â = {A1, ..., Ak| diag(Ai) = 0}. Our proposed estimation procedure revolves around the following optimization:\nβ̂t = arg min β̂∈Rk T∑ t′=1 k(t, t′) ||xt′ − k∑ i=1 β̂iAixt′ ||22 + λ r(β̂)\n(13) in which β̂i is the ith element of β̂, r(β̂) is a regularization term, and λ controls the strength of regularization. Given β̂t, we estimateA∗t as ∑k i=1 β̂ i tA\ni. The optimization in (13) involves a fixed set of basis matrices Â, but what we really want is to jointly optimize the loss in (13) over all times 1 ≤ t ≤ T , with respect to both the β̂t ∈ B̂ = {β̂1, ...β̂T } and the Ai ∈ Â. By doing so, information extracted from the entire sequence is allowed to affect the estimation of each A∗t when each A ∗ t is constructed from the bases in Â, which helps mitigate problems with high variance.\nThe desired joint optimization over B̂ and Â is easy to express in the terms of (5). Let g(β̂) = ∑k i=1 β̂\niAi, where Ai ∈ Â and ||Ai||1 ≤ c. The constraint on the entry-wise 1-norm of each Ai enforces the structural sparsity assumption. Next, we define f(x, g(β̂)) = ||x − g(β̂)x||2. Then, we define `(f |g,X, B̂) as: `(f |g,X, B̂) = T∑ t=1 T∑ t′=1 k(t, t′) f(xt′ , g(β̂t))2+λ T∑ t=1 r(β̂t) (14) Finally, we express the full joint optimization as follows:\nÂ∗ = arg min Â min B̂ T∑ t=1 T∑ t′=1 k(t, t′) ||xt′ − k∑ i=1 β̂itA ixt′ ||22\n+λβ T∑ t=1 r(β̂t) + λA k∑ i=1 ||Ai||1 (15)\nin which we changed the entry-wise 1-norm constraint on each Ai for a functionally similar entry-wise 1-norm regularization term. Intuitively, our method produces a set of basis network structures, i.e. Â∗, with which the temporally local network structures can be effectively approximated.\nThe joint optimization in (15) is closely analogous to the following sparse coding objective:\nA∗ = arg min A∈Rn×k [ min B m∑ i=1 ( ||xi −Aβi||22 + λ||βi||1 )] ,\n(16) in whichB = {β1, ...βm |βi ∈ Rk}, λ controls the tradeoff between reconstruction accuracy and representational sparsity, and the columns of A are constrained to unit norm. We can emphasize this by introducing the concept of time varying pseudo-dictionaries Dt ∈ Rn×k, in which the ith\ncolumn of Dt is Aixt. Using pseudo-dictionaries, we can rewrite (15) as follows:\nÂ∗ = arg min Â\n(17)\nmin B̂ T∑ t=1 [( T∑ t′=1 k(t, t′) ||xt′ −Dt′ β̂t||22 ) + λβr(β̂t) ]\nin which we dropped the sparsifying penalty on Ai ∈ Â∗ for notational brevity. From (17), it can be seen that the inner optimization over B̂ in (15) can be addressed as a set of sparse coding problems. For our purposes, we set the regularization term λβr(β̂t) to:\nαλβ 2 ||β̂t||22 + (1− α)λβ ||β̂t||1; 0 ≤ α ≤ 1, (18)\nwhich corresponds to elastic-net regularization (Zou & Hastie, 2005). We use this form to meet the assumptions required for the task-driven dictionary learning described in (Mairal et al., 2011), used in the further extension of our algorithm.\nThe analogy between our method and sparse coding leads naturally to a method for effecting the joint optimization in (15). As in sparse coding, we can jointly optimize over Â and B̂ using an EM-like block coordinate descent process that alternates between optimizing B̂ while holding Â fixed and optimizing Â while holding B̂ fixed (each of these is a convex problem). When optimizing B̂ with Â held fixed, we compute the optimal β̂t for each t via elastic-net regressions solved with the publicly available, highly optimized glmnet package (Friedman et al., 2009). When optimizing Â with B̂ held fixed, given current estimates of each basis Ai ∈ Â, we compute the partial gradients of the objective in (17) w.r.t. the entries of each pseudo-dictionary Dt, and then backpropagate these partial gradients through the pseudo-dictionary formation process to get partial gradients w.r.t. each entry of each basis structure Ai. We symmetrize the partial gradient of (17) w.r.t. each Ai by setting ∂Aiuv = 1 2 (∂A i uv + ∂A i vu). We also set ∂A i uu = 0,∀u to maintain the zero-diagonal constraint on Ai ∈ Â. In the next subsection we refer to these (unsupervised) partial gradients as ∇Ai`u. Using the computed gradients, we then take a single gradient descent step to update each Ai.\nThe full joint optimization process iterates between updating the β̂i ∈ B̂ via the regression in (13) and performing a single gradient descent update of the entries in each Ai ∈ Â. We dynamically select the step size for gradient descent updates in each iteration by line search and iterate until convergence. We perform the iterative optimization using subsampled batches of the available observations, which yields a stochastic gradient descent approach to jointly optimizing (15)/(17)."
    }, {
      "heading" : "4.3. Supervised Basis Structure Learning",
      "text" : "We can adapt the work of (Mairal et al., 2011) to enable our algorithm to learn task-driven sets of basis network structures. We consider the task of minimizing differentiable supervised loss functions that can be written as:\nLs(X, B̂, w) = T∑ t=1 `s(ω>β̂t, yt) + ν 2 ||ω||22, (19)\nwhere ω ∈ Rk, yt is the target output at time t, and the β̂t ∈ B̂ were produced to minimize (17). This includes any differentiable linear function of the β̂t ∈ B̂. In this paper, we focus on classification tasks and thus use the binomial deviance loss of logistic regression, i.e. `s(ω>β̂t, yt) = log(1 + e−ytω >β̂t), yt ∈ {−1,+1}.\nThe crux of task-driven dictionary learning is converting the readily available gradients of `s w.r.t. the structure codes β̂t into gradients w.r.t. the pseudo-dictionaries Dt with which they were computed to minimize (17), as gradients w.r.t. theDt easily produce gradients w.r.t. theAi ∈ Â. Unfortunately, the optimization producing the β̂t makes the conversion∇β̂t → ∇Dt non-trivial. However, Mairal et al. (2011) show that if elastic-net regularization is used to produce each β̂t from the (xt, Dt), the gradient of the perinstance supervised loss `s w.r.t. Dt can be computed as follows:\n∇Dt`s(ωT β̂t, yt) = −Dtφtβ̂>t + (xt −Dtβ̂t)φ>t , (20)\nin which φt ∈ Rk is defined as follows:\nφtΛC = 0, φtΛ = (D > tΛDtΛ+αλβI) −1∇β̂tΛ`s(ω >β̂t, yt) (21) where Λ denotes the indices of non-zero entries in the sparse β̂t, ΛC indicates the complementary set of indices, and αλβ is the `2 regularization weight from (18). Once gradients of `s w.r.t. each Dt (i.e. ∇Dt`s) have been computed for each time t, they can be backpropagated through the pseudo-dictionary formation process and summed across time points to get gradients w.r.t. each Ai ∈ Â (i.e. ∇Ai`s).\nGiven unsupervised gradients ∇Ai`u, computed as described at the end of Sec. 4.2, and supervised gradients ∇Ai`s, we define the final gradients for stochastic descent optimization of the combined unsupervised/supervised objective as follows:\n∂Ai = γ∇Ai`u + (1− γ)∇Ai`s; 0 ≤ γ ≤ 1, (22)\nwhere γ is a mixing parameter controlling the tradeoff between supervised and unsupervised learning. As before, we enforce symmetry and zero-diagonal constraints prior to using the joint gradients for basis updates."
    }, {
      "heading" : "5. Synthetic Network Analysis",
      "text" : "This section presents tests based on simulated observation sequences which show the ability of our algorithm to recover recurring elements of time varying network structures. We generated each observation sequence by drawing the observation xt at time t from a normal distribution N (0,Σt), in which Σt was a convex combination of four covariance matrix bases: Σt = ∑4 i=1 α i tΣ\ni, with∑4 i=1 α i t = 1 and 0 ≤ αit ≤ 1. We generated smooth trajectories for the αit (an example set of trajectories is shown in Fig. 1). We generated each Σi by symmetrically removing two thirds of the off-diagonal entries (the ones with the smallest magnitude) from a random covariance matrix with eigenvalues uniformly distributed in (0, 1), and then rescaling diagonal entries to ensure positive definiteness. An example of the sparse basis structures used in our tests can be seen in the right panel of Fig. 2. The inputs ranged from 10-dimensional to 40-dimensional. For each tested dimensionality, we generated 25 sequences of 5000 observations, with the first 3000 reserved for training and the last 2000 reserved for testing; each sequence was based on different basis matrices Σi and different αit trajectories. Results are averaged over the 25 sequences.\nMethods based on (12) are much better suited for this task than methods expecting abrupt “change point” structure. Hence, we tested three methods for estimating time varying network structure in our sequences: locally weighted `1-\nregularized self-regression (as described in (12)), the same self-regression followed by projection of the inferred structures onto the principal components of structures estimated for each time point in the training set, and our iterative approach to learning task-driven basis structures.\nThe self-regression-based method used in our tests can be considered equivalent to KELLER (Song et al., 2009a). Using the principal components of the set of A∗t produced by this method is itself novel, and can be seen as an approximation to our method. When executing our method, we initialized the set Â using these principal structures. In our tests, we used six principal structures with the PCA-based method and learned six basis structures with our algorithm.\nWe measured test performance for a classification task in which the class of each xt was set as follows: yt = 1 if α1t + α 2 t ≥ α3t + α4t and yt = −1 otherwise. We also estimated a similarity score between the sets of estimated structures and the true precision matrices underlying each sequence, as explained below.\nClassification was performed using the parameterization produced by each method for a given xt (i.e. a matrix A∗t for the self-regression method, the same matrix projected onto a set of principal structures for the PCA method, and the inferred vector β̂t for our algorithm) as input features to a regularized logistic regression classifier, with the target class determined by yt. Fig. 3 presents the results. The basis structures learned by our method, and the codes they induce, offer an informative representation of regularities in time varying sparse network structure.\nWe measured similarity between learned bases and the true precision matrices using a form of pairwise matrix correlation. First we set the diagonal entries of each matrix to zero, then their off-diagonal entries to zero mean and unit norm, and finally “vectorize” each matrix and compute the\n660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714\n720 721 722 723 724 725 726 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769\nImproved Estimation in Time Varying Models\n0 500 1000 1500 100\n50\n0\n50\nAc cu\nm ul\nat ed\nE vi\nde nc\ne k6b\n0 500 1000 1500\n200\n100\n0\n100\n200\nk3b\n0 500 1000 1500\n80\n40\n0\n40\n80\nl1b\nFigure 4. Behavior of the evidence accumulation classifier learned using features produced by our algorithm, averaged over all trials for each subject. As indicated by Table 1, subject k6b proved difficult for both our method and RCSP. With the other two subjects, the discriminative capacity of our bases can be clearly seen in the rapid bifurcation of their induced classifier response after the cue time at x = 500. Lines trending upwards indicate left hand trials and lines trending downwards represent right hand trials.\nsubject during a set of test trials, given a labeled set of training trials. In each trial, a cue is given to the subject indicating a motor action, after which the subject visualizes that action for several seconds. Cortical activity during each trial was measured by a set of 60 electrodes placed on the scalp, taking measurements at 250Hz. Data collected from these electrodes was the subject of our analysis.\nWe used left hand and right hand trials from this dataset for the subjects l1b, k3b, and k6b. Several trials from each subject were discarded due to significant artifacts, as measured by deviation from a Gaussian model of the mean behavior of the joint set of trials for a subject. We also applied a whitening transform V D− 1 2 V T to each subject’s data prior to analysis, where D was a diagonal matrix containing the eigenvalues of the data and the columns of V were the corresponding eigenvectors. We set kernel widths and regularization weights for the optimization in (14) uniformly for all subjects and trials, following a brief manual search.\nWe learned a set of 20 sparse basis structures for each subject using our algorithm in an unsupervised fashion (i.e. γ = 1). Afterwards, we performed 20 rounds of randomized cross-validation in which we split the trials for each subject 4/1 into training/test sets. We trained three classifiers in each round of cross-validation: a classifier built on the β̂t inferred by our algorithm after a period of supervised basis updates (i.e. γ = 0.75) using the training set, a classifier built on the output of a set of 20 RCSP filters (Lotte & Guan, 2011), and a classifier built on the combination of both feature sets. The regularization parameter for RCSP was selected to maximize expected performance across all subjects.\nWe built our classifier by considering the β̂t and class labels for each time point in each training trial as inferred feature/label pairs for training an 2-regularized logistic regression classifier. Given the encoding of a particular trial in terms of a set of β̂t, an overall output for the trial was\ncomputed by accumulating (i.e. summing) the output of the learned single time-point classifier over the first three postcue seconds of the trial. After this evidence accumulation phase, the classification for each trial was determined by the sign of its overall output. RCSP filters were trained as described in (Lotte & Guan, 2011), after which the squared responses of these filters to the observations were used as input features to an 2-regularized logistic regression classifier, trained as above We also trained an analogous classifier using the combined features produced by our algorithm and the RCSP filters at each time point. Classification results for each subject are shown in Table 1, and a visual representation of the evidence accumulation process based on our features is shown in Fig. 4. Classifiers constructed in this fashion have the advantage of being amenable to “early exit”, in the spirit of drift-diffusion decision making.\nThese results show that our approach produces informative features in a real-world scenario, with the results for the combined features suggesting that our features supplement, rather than replace, the commonly used RCSP features.\n6. Conclusion We introduced a problem formulation in the context of multiply parameterized models. Using this formulation, we developed a novel algorithm for learning representations of sparse structure in time varying networks with recurring structural motifs. We used tests on synthetic data to show that our algorithm behaves as desired under suitable conditions, while an application to BCI EEG data showed the potential value of our algorithm in real world conditions.\nWe plan to investigate more in-depth the performance of our approach by applying it to other types of tasks, such as analysis of time varying weather and traffic patterns. Our algorithm is also readily extensible to the estimation of time varying structure in Dynamic Bayesian Networks. We also plan to look at alternative parameter transformation methods, beyond the linear transforms considered in this paper.\nFigure 4. Behavior of the evidence accumulation classifier learned using features produced by our algorithm, averaged over all trials for each subject. As indicated by Table 1, subject k6b proved difficult for both our method and RCSP. With the other two subjects, the discriminative capacity of our bases can be clearly seen in the rapid bifurcation of their induced classifier response after the cue time at x = 500. Lines trending upwards indicate left hand trials and lines trending downwards represent right hand trials.\ndot product betwe n the resulting vectors. This measure ranges from −1 to 1, with larg r magnitudes indicating greater similarity. For each sequence and each method, we found the best match to each (Σi)−1, as determined by the magnitude of our correlation score, among th set of bases produced by that method. We then averaged best match scores for each method over both true bases and sequences, to get a final score for each dimensionality. Fig. 3 shows the similarity scores achieved by the PCA-based method and our method, with the bases produced by our method consistently displaying greater similarity to the true bases than those produced by PCA alone. Fig. 2 shows a typical example of a best match produced by our method during these tests; as can be seen, the learned basis is qualitatively very similar to the true basis."
    }, {
      "heading" : "6. BCI EEG Analysis",
      "text" : "We applied our algorithm to the analysis of EEG data from a Brain Computer Interface (abbr. BCI) motor imagery experiment available as task 3a from BCI competition III (Schlögl et al., 2005; Blankertz et al., 2006). In this task, the objective is to infer the motor action visualized by a subject during a set of test trials, given a labeled set of training trials. In each trial, a cue is given to the subject indicating a motor action, after which the subject visualizes that action for several seconds. Cortical activity during each trial was measured by a set of 60 electrodes placed on the scalp, taking measurements at 250Hz. Data collected from these electrodes was the subject of our analysis.\nWe used left hand and right hand trials from this dataset for the subjects l1b, k3b, and k6b. Several trials from each subject were discarded due to significant artifacts, as measured by deviation from a Gaussian model of the mean behavior of the joint set of trials for a subject. We also applied a whitening transform V D− 1 2V T to each subject’s data prior to analysis, where D was a diagonal matrix containing the\neigenvalues of the data and the columns of V were the corresponding eigenvectors. We set kernel widths and regularization weights for the optimization in (15) uniformly for all subjects and trials, following a brief manual search.\nWe lear ed a set of 20 sparse ba is structures for each subject using our algorithm in an unsupervis d fashion (i.e. γ = 1). Afterwards, we performed 20 rounds of randomized cross-validation in which we split the trials for ach subject 4/1 into training/test sets. We trained three classifiers in each round of cross-validation: a classifier built on the β̂t inferred by our algorithm after a period of supervised basis updates (i.e. γ = 0.75) using the training set, a classifier built on the output of a set of 20 RCSP filters (Lotte & Guan, 2011), and a classifier built on the combination of both feature sets. The regularization parameter for RCSP was selected to maximize expected performance across all subjects.\nWe built our classifier by considering the β̂t and class labels for each time point in each training trial as inferred feature/label pairs for training an `2-regularized logistic regression classifier. Given the encoding of a particular trial in terms of a set of β̂t, an overall output for the trial was computed by accumulating (i.e. summing) the output of the learned single time-point classifier over the first three postcue seconds of the trial. After this evidence accumulation phase, the classification for each trial was determined by the sign of its overall output. RCSP filters were trained as described in (Lotte & Guan, 2011), after which the squared responses of these filters to the observations were used as input features to an `2-regularized logistic regression classifier, trained as for our algorithm. We also trained an analogous classifier using the combined features produced by our algorithm and the RCSP filters at each time point. Classification results for each subject are shown in Table 1, and a visual representation of the evidence accumulation process based on our features is shown in Fig. 4. Classifiers constructed in this fashion have the advantage of being amenable to “early exit”, in the spirit of drift-diffusion decision making.\nThese results show that our approach produces informative\nfeatures in a real-world scenario, with the results for the combined features suggesting that our features supplement, rather than replace, the commonly used RCSP features."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We introduced a problem formulation in the context of multiply parameterized models. Using this formulation, we developed a novel algorithm for learning representations of sparse structure in time varying networks with recurring structural motifs. We used tests on synthetic data to show that our algorithm behaves as desired under suitable conditions, while an application to BCI EEG data showed the potential value of our algorithm in real world conditions.\nWe plan to apply our approach to other types of tasks, such as analysis of time varying weather and traffic patterns, in addition to investigating alternative parameter transformation methods, beyond the linear transforms considered in this paper. Our algorithm is readily extensible to the estimation of time varying structure in Dynamic Bayesian Networks.\nAcknowledgements: Funded by NSERC and ONR."
    } ],
    "references" : [ {
      "title" : "Recovering time-varying networks of dependencies in social and biological studies",
      "author" : [ "A. Ahmed", "E.P. Xing" ],
      "venue" : null,
      "citeRegEx" : "Ahmed and Xing,? \\Q2009\\E",
      "shortCiteRegEx" : "Ahmed and Xing",
      "year" : 2009
    }, {
      "title" : "An online spectral learning algorithm for partially observable nonlinear dynamical systems",
      "author" : [ "B. Boots", "G. Gordon" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Boots and Gordon,? \\Q2011\\E",
      "shortCiteRegEx" : "Boots and Gordon",
      "year" : 2011
    }, {
      "title" : "Likelihood-based sufficient dimension reduction",
      "author" : [ "R.D. Cook", "L. Forzani" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Cook and Forzani,? \\Q2009\\E",
      "shortCiteRegEx" : "Cook and Forzani",
      "year" : 2009
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2008
    }, {
      "title" : "Regularization paths for generalized linear models via coordinate descent",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Technical report, Stanford University,",
      "citeRegEx" : "Friedman et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2009
    }, {
      "title" : "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces",
      "author" : [ "K. Fukumizu", "F.R. Bach", "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Fukumizu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2004
    }, {
      "title" : "Independent component analysis: algorithms and applications",
      "author" : [ "A. Hyvärinen", "E. Oja" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Hyvärinen and Oja,? \\Q2000\\E",
      "shortCiteRegEx" : "Hyvärinen and Oja",
      "year" : 2000
    }, {
      "title" : "On time varying undirected graphs",
      "author" : [ "M. Kolar", "E.P. Xing" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Kolar and Xing,? \\Q2011\\E",
      "shortCiteRegEx" : "Kolar and Xing",
      "year" : 2011
    }, {
      "title" : "Sparsistent Learning of Varying-coefficient Models with Structural Changes",
      "author" : [ "M. Kolar", "L. Song", "E.P. Xing" ],
      "venue" : "In NIPS 22,",
      "citeRegEx" : "Kolar et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolar et al\\.",
      "year" : 2009
    }, {
      "title" : "Disclda: Discriminative learning for dimensionality reduction and classification",
      "author" : [ "S. Lacoste-Julien", "F. Sha", "M.I. Jordan" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Lacoste.Julien et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lacoste.Julien et al\\.",
      "year" : 2008
    }, {
      "title" : "Graphical Models",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "Lauritzen,? \\Q1996\\E",
      "shortCiteRegEx" : "Lauritzen",
      "year" : 1996
    }, {
      "title" : "Regularizing common spatial patterns to improve bci designs: Unified theory and new algorithms",
      "author" : [ "F. Lotte", "C. Guan" ],
      "venue" : "IEEE Transactions on Biomedical Engineering,",
      "citeRegEx" : "Lotte and Guan,? \\Q2011\\E",
      "shortCiteRegEx" : "Lotte and Guan",
      "year" : 2011
    }, {
      "title" : "Task-driven dictionary learning",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Mairal et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mairal et al\\.",
      "year" : 2011
    }, {
      "title" : "High-dimensional graphs and variable selection with the lasso",
      "author" : [ "N Meinshausen", "P. Bühlmann" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Meinshausen and Bühlmann,? \\Q2006\\E",
      "shortCiteRegEx" : "Meinshausen and Bühlmann",
      "year" : 2006
    }, {
      "title" : "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "author" : [ "B.A. Olshausen", "D.J. Field" ],
      "venue" : null,
      "citeRegEx" : "Olshausen and Field,? \\Q1996\\E",
      "shortCiteRegEx" : "Olshausen and Field",
      "year" : 1996
    }, {
      "title" : "Supervised dimensionality reduction using mixture models",
      "author" : [ "Sajama", "A. Orlitsky" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sajama and Orlitsky,? \\Q2005\\E",
      "shortCiteRegEx" : "Sajama and Orlitsky",
      "year" : 2005
    }, {
      "title" : "Exploring regression structure using nonparametric functional estimation",
      "author" : [ "A.M. Samarov" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Samarov,? \\Q1993\\E",
      "shortCiteRegEx" : "Samarov",
      "year" : 1993
    }, {
      "title" : "Characterization of four-class motor-imagery eeg data for the bcicompetition",
      "author" : [ "A. Schlögl", "F. Lee", "H. Bischof", "G. Pfurtscheller" ],
      "venue" : "Journal of Neural Engineering,",
      "citeRegEx" : "Schlögl et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Schlögl et al\\.",
      "year" : 2005
    }, {
      "title" : "Reduced-rank hidden markov models",
      "author" : [ "S. Siddiqi", "B. Boots", "G. Gordon" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Siddiqi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Siddiqi et al\\.",
      "year" : 2010
    }, {
      "title" : "Keller: estimating timevarying interactions between genes",
      "author" : [ "L. Song", "M. Kolar", "E.P. Xing" ],
      "venue" : "Bioinformatics, 25:i128–",
      "citeRegEx" : "Song et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2009
    }, {
      "title" : "Time-varying dynamic bayesian networks",
      "author" : [ "L. Song", "M. Kolar", "E.P. Xing" ],
      "venue" : "In NIPS 22,",
      "citeRegEx" : "Song et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2009
    }, {
      "title" : "A global geometric framework for nonlinear dimensionality reduction",
      "author" : [ "J.B. Tenenbaum", "V. de Silva", "J.C. Langford" ],
      "venue" : "Science, 290:2319–2323,",
      "citeRegEx" : "Tenenbaum et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Tenenbaum et al\\.",
      "year" : 2000
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statiscal Society. Series B,",
      "citeRegEx" : "Tibshirani,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani",
      "year" : 1996
    }, {
      "title" : "Highdimensional graphical model selection using l1-regularized logistic regression",
      "author" : [ "M.J. Wainwright", "P. Ravikumar", "J. Lafferty" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Wainwright et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wainwright et al\\.",
      "year" : 2007
    }, {
      "title" : "An adaptive estimation of dimension reduction space",
      "author" : [ "Y. Xia", "H. Tong", "W.K. Li", "Zhu", "L-X" ],
      "venue" : "Journal of the Royal Statistical Society B,",
      "citeRegEx" : "Xia et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2002
    }, {
      "title" : "Time varying undirected graphs",
      "author" : [ "S. Zhou", "J. Lafferty", "L. Wasserman" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2010
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "H. Zou", "T. Hastie" ],
      "venue" : "Journal of the Royal Statistical Society B,",
      "citeRegEx" : "Zou and Hastie,? \\Q2005\\E",
      "shortCiteRegEx" : "Zou and Hastie",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "(Tenenbaum et al., 2000), and dimension reduction for regression (Fukumizu et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : ", 2000), and dimension reduction for regression (Fukumizu et al., 2004; Cook & Forzani, 2009).",
      "startOffset" : 48,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "Examples of the first approach include DiscLDA (Lacoste-Julien et al., 2008) and supervised dimensionality reduction using Bayesian mixture models (Sajama & Orlitsky, 2005), which seek useful linear reductions of the parameters of a generative model.",
      "startOffset" : 47,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "The second approach includes the application of spectral methods to learning transformed representations of HMMs (Siddiqi et al., 2010) and PSRs (Boots & Gordon, 2011).",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : "If one views xj Aβ̂ w i as a reduction of xj into the subspace spanned by the columns of A, followed by a linear regression in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002).",
      "startOffset" : 284,
      "endOffset" : 317
    }, {
      "referenceID" : 24,
      "context" : "If one views xj Aβ̂ w i as a reduction of xj into the subspace spanned by the columns of A, followed by a linear regression in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002).",
      "startOffset" : 284,
      "endOffset" : 317
    }, {
      "referenceID" : 12,
      "context" : "Through an analogy between our algorithm and sparse coding (Olshausen & Field, 1996), we then extend our algorithm to learning of task-driven basis structures, guided by the work in (Mairal et al., 2011).",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 10,
      "context" : "The use of self-regression for network structure estimation is based on the following results (Lauritzen, 1996):",
      "startOffset" : 94,
      "endOffset" : 111
    }, {
      "referenceID" : 25,
      "context" : "Value estimation methods estimate each entry in Σ−1 (Zhou et al., 2010), while structure estimation determines the pattern of zero/non-zero entries (Friedman et al.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : ", 2010), while structure estimation determines the pattern of zero/non-zero entries (Friedman et al., 2008; Song et al., 2009b; Kolar & Xing, 2011)).",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : "The sparsity assumption can be incorporated into the self-regression process by using sparsifying regression techniques, such as the well known Lasso (Tibshirani, 1996).",
      "startOffset" : 150,
      "endOffset" : 168
    }, {
      "referenceID" : 23,
      "context" : "Σ−1 under suitable conditions (Meinshausen & Bühlmann, 2006; Wainwright et al., 2007; Kolar & Xing, 2011).",
      "startOffset" : 30,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "A recent line of work focuses on extending methods for network structure estimation for the case when structures vary over time (Ahmed & Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar & Xing, 2011).",
      "startOffset" : 128,
      "endOffset" : 229
    }, {
      "referenceID" : 25,
      "context" : "A recent line of work focuses on extending methods for network structure estimation for the case when structures vary over time (Ahmed & Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar & Xing, 2011).",
      "startOffset" : 128,
      "endOffset" : 229
    }, {
      "referenceID" : 8,
      "context" : "This second assumption distinguishes KELLER from methods such as (Ahmed & Xing, 2009) and (Kolar et al., 2009), which assume abrupt changes in the network structure.",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "We use this form to meet the assumptions required for the task-driven dictionary learning described in (Mairal et al., 2011), used in the further extension of our algorithm.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "When optimizing B̂ with Â held fixed, we compute the optimal β̂t for each t via elastic-net regressions solved with the publicly available, highly optimized glmnet package (Friedman et al., 2009).",
      "startOffset" : 172,
      "endOffset" : 195
    }, {
      "referenceID" : 12,
      "context" : "We can adapt the work of (Mairal et al., 2011) to enable our algorithm to learn task-driven sets of basis network structures.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "However, Mairal et al. (2011) show that if elastic-net regularization is used to produce each β̂t from the (xt, Dt), the gradient of the perinstance supervised loss `s w.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 17,
      "context" : "BCI) motor imagery experiment available as task 3a from BCI competition III (Schlögl et al., 2005; Blankertz et al., 2006).",
      "startOffset" : 76,
      "endOffset" : 122
    } ],
    "year" : 2012,
    "abstractText" : "Locally adapted parameterizations of a model (such as locally weighted regression) are expressive but often suffer from high variance. We describe an approach for reducing this variance, based on the idea of estimating simultaneously a transformed space for the model and locally adapted parameterizations expressed in the new space. We present a new problem formulation that captures this idea and illustrate it in the important context of time varying models. We develop an algorithm for learning a set of bases for approximating a time varying sparse network; each learned basis constitutes an archetypal sparse network structure. We also provide an extension for learning task-specific bases. We present empirical results on synthetic data sets, as well as on a BCI EEG classification task.",
    "creator" : "LaTeX with hyperref package"
  }
}