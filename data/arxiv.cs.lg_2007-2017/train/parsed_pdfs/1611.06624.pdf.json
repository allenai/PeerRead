{
  "name" : "1611.06624.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Temporal Generative Adversarial Nets",
    "authors" : [ "Masaki Saito", "Eiichi Matsumoto" ],
    "emails" : [ "matsumoto}@preferred.jp" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Unsupervised learning of feature representations of large datasets is one of the most significant problems in computer vision. Using these representations learned by unlabelled datasets, one can leverage them to a variety of vision tasks such as classification, clustering, and generation.\nIn the field of computer vision, most studies regarding unsupervised learning are roughly classified into the following two categories: (i) studies that handle a collection of images, and (ii) those regarding a collection of videos. Although finding a good representation being capable of producing realistic samples has been treated as a difficult and challenging problem, in unsupervised learning of images, recent studies using Generative Adversarial Nets (GANs) [5] have shown impressive results, and succeeded to generate plausible images with a dataset that contains a large amount of natural images [2, 49]. In contrast, unsupervised learning of videos has been still regarded as a challenging problem. While recent studies have achieved remarkable progress [37, 28, 15] ∗Authors contributed equally\nin the frame prediction problem that predicts future frames, there still remains many challenges to be solved for the video generation problem without an input sequence. We believe it may arise from a difficulty of the video generation. To make a good model being capable of generating plausible videos, besides all frames in a generated video should be real, all adjacent frames must be smoothly connected and these perceived actions needs to be semantically consistent.\nTo tackle such a challenging task, in this paper we extend an existing GAN model and propose a Temporal Generative Adversarial Net (TGAN), that is able to produce realistic videos from an unlabelled video dataset. As with the existing GANs that yield images with a function called generator, our model generates realistic videos with a generator that returns a video from a latent variable drawn from a given distribution (e.g., uniform distribution). The main novelty of our study is a new network architecture that can efficiently learn a good representation of videos. Unlike an existing generator that generates videos with 3D deconvolutional layers [45], in our proposed model the generator consists of two sub networks called a temporal generator and an image generator (Fig.1). Specifically, the temporal generator first yields a set of latent variables, each of which corresponds to a latent variable of the image generator. Next, the image generator transforms these variables into the video frames.\nThere are several advantages of explicitly introducing two different latent spaces. The first is to simplify the generative process of videos. Using these two generators, we can decompose the whole training process into the training of the image generator and that of the temporal generator. In the image generator, we exploit the existing GAN model (c.f. [30]) that can learn correspondence between the latent variable and the image. In the temporal generator, we efficiently estimate the parameters by fixing nearly all of the parameters of the image generator.\nThe second is the applicability of a wider range of generation problems. In our representation, videos generated by our model are represented as a trajectory in the latent space. It enables us to not only achieve the frame interpolation of the generated video by simply interpolating the trajectory generated by the temporal generator, but also be able to ap-\n1\nar X\niv :1\n61 1.\n06 62\n4v 1\n[ cs\n.L G\n] 2\n1 N\nov 2\n01 6\npend new frames to the video by smoothly connecting two different trajectories, and speed-up the training by replacing the image generator with the existing pre-trained generator published on the web.\nThe rest of the paper is organized as follows. Section 2 summarizes related studies regarding our proposed method. In Section 3, we briefly explain the existing GAN model, and then propose our generative model for dealing with the videos. Section 4 describes three applications of our proposed method. Section 5 presents several experimental results. In Section 6 we summarize our study and describe our remaining tasks."
    }, {
      "heading" : "2. Related work",
      "text" : ""
    }, {
      "heading" : "2.1. Natural image generation",
      "text" : "Supervised learning with CNNs has recently shown outstanding performance in many tasks such as image classification [9, 11] and action recognition [14, 16, 35, 44], whereas unsupervised learning with CNNs has received less attention. A common approach for generating images is the use of undirected graphical models such as Boltzmann machines [34, 19, 4]. However, due to the difficulty of approximating gradients, it has been empirically observed that such deep graphical models frequently fail to find a good representation of natural images with a sufficient diversity. Both Gregor et al. [8] and Dosvotiskiy et al. [3] have proposed models that respectively use recurrent and deconvolutional networks, and successfully generated natural images. However, both models make use of supervised learning and require additional information such as labels.\nThe Generative Adversarial Network (GAN), which we have mainly employed in this study, is a model for unsupervised learning that finds good representations of samples by simultaneously training two different networks called the generator and the discriminator. In recent years, many extensions for GANs have been proposed. For example, conditional GANs have been proposed for modelling attributes of objects [24, 12]. Pathak et al. [29] adopted the adversarial\nnetwork to generate the contents of an image region conditioned on its surroundings. Li and Wand [20] employed the GAN model in order to efficiently synthesize texture. Denton et al. [2] proposed a Laplacian GAN that outputs a high-resolution image by iteratively generating images in coarse-to-fine manner.\nLater, Radford et al. [30] proposed a simple yet powerful model called Deep Convolutional GAN (DCGAN) for generating realistic images with a pair of convolutional and deconvolutional neural networks. Based on these results, Wang et al. [49] extended DCGAN by factorizing the image generation process into two ways, and proposed a new model called a Style and Structure GAN (S2-GAN) that exploits two types of generators."
    }, {
      "heading" : "2.2. Video recognition and unsupervised learning",
      "text" : "As recognizing videos is a challenging task which has received a lot of attention, many researchers have tackled such problems in various ways. In supervised learning of videos, while a common approach is to use dense trajectories [46, 33, 32], recent methods have employed CNNs so as to achieve state-of-the-art results [14, 16, 35, 44, 47]. Some studies are focused on extracting spatio-temporal feature vectors from a video in an unsupervised manner. Taylor et al. [41] proposed a method that extracts invariant features with Restricted Boltzmann Machines (RBMs). Temporal RBMs have also been proposed to explicitly capture the temporal correlations in videos [42, 40, 39]. Stavens and Thrun [38] dealt with this problem by using an optical flow and lowlevel features such as SIFT. Le et al. [18] use Independent Subspace Analysis (ISA) to extract spatio-temporal semantic features. Deep neural networks have also been applied to feature extraction from videos [51, 7, 48] in the same way as supervised learning.\nThere also exist several studies focusing on predicting video sequences from an input sequence with Recurrent Neural Networks (RNNs) represented by Long Short-Term Memory (LSTM) [10]. In particular, Ranzato et al. [31] proposed a Recurrent Neural Network (RNN) model that can\nlearn both spatial and temporal correlations. Srivastava et al. [37] also applied LSTMs and succeeded to predict the future sequence of a simple video. Zhou and Berg [50] proposed a network that creates depictions of objects at future times with LSTMs and DCGAN. Kalchbrenner et al. [15] also employed a convolutional LSTM model, and proposed Video Pixel Networks that directly learn the joint distribution of the raw pixel values. Oh et al. [28] proposed a deep autoencoder model conditioned on actions, and predicted next sequences of Atari games from a single screen shot and an action sent by a game pad. In order to deal with the problem that generated sequences are “blurry” compared to natural images, Mithieu et al. [23] replaced a standard mean squared error loss and improved the quality of predicted images. However, the above studies cannot directly be applied to the task of generating entire sequences from scratch since they require an initial sequence as an input.\nInterestingly, Vondrick et al. [45] recently proposed a generative model that yields a video sequence from scratch with DCGAN consisting of 3D deconvolutional layers. The main difference between their model and ours is model representation: while they simplified the video generation problem by assuming that a background in video sequence is always fixed and generate the video with 3D deconvolutions, we simplify this problem by decomposing the generation process of video into the 1D deconvolutional layers and the 2D deconvolutional layers."
    }, {
      "heading" : "3. Temporal Generative Adversarial Nets",
      "text" : ""
    }, {
      "heading" : "3.1. Generative Adversarial Nets",
      "text" : "Before we go into the details of TGAN, we briefly explain the existing GANs [5]. A GAN exploits two networks called the generator and the discriminator. The generator G1 : RK1 → RM is a function that generates samples x ∈ RM which look similar to a sample in the given dataset. The input is a latent variable z1 ∈ RK1 , where z1 is randomly drawn from a given distribution pG1(z1), e.g., an uniform distribution. The discriminator D1 : RM → [0, 1] is a classifier that discriminates whether a given sample is from the dataset or generated by G1.\nThe GAN simultaneously trains the two networks by playing a non-cooperative game; the generator wins if it generates an image that the discriminator misclassifies, whereas the discriminator wins if it correctly classifies the input samples. Such minimax game can be represented as\nmin θG1 max θD1 Ex∼pimage [lnD1(x)]\n+ Ez1∼pG1 [ln(1−D1(G1(z1)))], (1)\nwhere θG1 and θD1 are the parameters of the generator and the discriminator, respectively. pimage denotes the empirical data distribution of an image dataset. As the computation of\nthe exact solution of Eq.(1) is generally intractable, gradientbased methods such as Stochastic Gradient Descent (SGD) [27] are used to find a local equilibrium.\nThe performance of a GAN strongly depends on the network settings of the generator and the discriminator. While the original GAN [5] employed fully-connected and maxout [6] layers, the DCGAN [30] focused on the image generation problem and employed convolutional and deconvolutional layers to produce realistic images with better quality."
    }, {
      "heading" : "3.2. Temporal GAN",
      "text" : "Here we introduce our model based on the above discussion. Let T > 0 be the number of frames to be generated, and G0 : RK0 → RT×K1 be the temporal generator that gets another latent variable z0 ∈ RK0 as an argument and generates T latent variables each of which corresponds to the argument of G1. In our model, z0 is randomly drawn from a distribution pG0(z0). Note that the latent variable z1 is not randomly drawn from pG1 but generated from the temporal generator. That is, in order to generate a video with T frames, our model first generates T latent variables denoted by [z11 , . . . , z T 1 ], and then convert them into video frames with the image generator. These outputs are represented as [G1(z11), . . . , G1(z T 1 )]. Although our current model is capable of only generating a video with fixed T frames, it is also naturally extendable to the generator that yields a longer sequences by a simple modification (the details will be described later).\nAs with the generator we need to make a change so that the discriminator can take video frames as an argument. For the sake of simplicity we explicitly split the discriminator into the following two functions: the image discriminator D′1 and the temporal discriminator D0 (Fig.1). Specifically, D′1 : RM → RF first extracts a feature vector with F units for each video frame. After concatenating them into a single feature map, D0 : RT×F → [0, 1] discriminates whether the input is from the dataset or the generator.\nUsing these notations, the minimax game of Eq.(1) can be rewritten as\nmin θG0 ,θG1 max θD0 ,θD1\nE[x1,...,xT ]∼pvideo [lnD0([D ′ 1(x 1), . . . , D′1(x T )])]+ Ez0∼pG0 [ln ( 1−D0([D′1(x̂1), . . . , D′1(x̂T )])] ) , (2)\nwhere xt is the t-th frame of a video in a dataset, and x̂t is the t-th frame generated by zt1, i.e., x̂ t = G1(z t 1). θD0 and θG0 represent the parameter of D0 and that of G0, respectively. pvideo denotes the empirical data distribution of a video dataset."
    }, {
      "heading" : "3.3. Two-step training",
      "text" : "In the unsupervised learning of the image, the recently published DCGAN can successfully find a good correspon-\ndence between the latent space and the image space. Using this methodology, we decompose a complicated training process of the above two networks into the following two steps.\nFirstly, using a video dataset we train the image generator G1 and the image discriminator D1 to correctly learn the image space formed by the video dataset. As an image dataset we simply use all the images included in the video dataset. After that, we obtain θG1 and θD1 with Eq.(1). The training process of the two networks is the same as that of existing studies (e.g., [5, 30]).\nSecondly, using the trained two networks we train the temporal generator G0 and the temporal discriminator D0 under the condition that almost all of the parameters in G1 and D1 are fixed. In the image generator we can simply transfer all the learned parameters of G1 of Eq.(1) into those of Eq.(2). In the image discriminator D′1 in Eq.(2), we reuse all the layers of the trained D1 except for the loss layer to extract a feature vector from each video frame."
    }, {
      "heading" : "3.4. Network configuration",
      "text" : "This subsection describes the configuration of our four networks: the temporal generator, the image generator, the image discriminator, and the temporal discriminator. Table 1 shows a typical network setting. Note that the settings slightly differ depending on the dataset. Due to the limitation of space, we only explain the settings of the UCF-101 [36] and include the rest in the supplementary material.\nTemporal generator Unlike typical CNNs that perform two-dimensional convolutions in the spatial direction, the deconvolutional layers in the temporal generator perform a one-dimensional convolution in the temporal direction. For convenience of computation, we first regard z0 ∈ RK0 as a one-dimensional activation map of z0 ∈ R1×K0 , where the length and the number of channels are one and K0, respectively. A uniform distribution is used for the distribution of z0. Next, applying the deconvolutional layers we expand its length while reducing the number of channels. The settings for the deconvolutional layers are the same as those of the image generator except for the number of channels\nand one-dimensional deconvolution. Like the original image generator we insert a Batch Normalization (BN) layer [13] after deconvolution and use Rectified Linear Units (ReLU) [25] as activation functions.\nUnder an intuitive assumption that a latent trajectory generated by the temporal generator should be smooth, we apply the upsampling filter that performs bilinear interpolation (cf., [21]) to the outputs. In this study, the temporal generator yields the feature map with 8 frames with the deconvolutional layers, and expands them to 16 frames with the 2× upsampling layer.\nImage generator Although we follow the setting of the existing DCGAN (i.e., we used ReLU [25] and Batch Normalization layer [13]), several settings are different. In terms of the deconvolutional layers, instead of fractional-strided convolutions we set the kernel size, stride, and padding to 4, 2, and 2 respectively, except for the last deconvolutional layer. In the two-step training we use a uniform distribution for the distribution of z1.\nImage discriminator As with the image generator, we follow the existing DCGAN except for the convolutional layers. Although we used the linear layer and the softmax functions for the training the image discriminator, they are removed when we train the temporal generator and the temporal discriminator. Since the resolution of the video is 48× 48 pixel, the temporal discriminator finally yields a feature vector with 512 × 6 × 6 units. As activation functions, we use Leaky ReLUs [22] with a = 0.2.\nTemporal discriminator All the convolutional layers in the temporal discriminator convolve the feature map in the temporal direction. We first concatenate the feature vectors extracted from D′1, and consider the result as a feature map. Next, applying the convolutional layers we reduce its length while increasing the number of channels, and apply the softmax function. As with the original image discriminator we apply the batch normalization layers after the convolutions, and use Exponential Linear Units (ELUs) [1] with α = 1 as activation functions."
    }, {
      "heading" : "4. Applications",
      "text" : ""
    }, {
      "heading" : "4.1. Frame interpolation",
      "text" : "One of the advantages of our model is to be able to generate an intermediate frame between two adjacent frames. Since the video generation problem in our model is formulated as an estimation of a trajectory in the latent image space, our generator can easily yield long sequences in which an intermediate frame is naturally interpolated by just interpolating the trajectory.\nSpecifically, we interpolate the trajectory in the latent image space with the bilinear filter, which is the same in the last layer of the temporal generator (see Section 3.4)."
    }, {
      "heading" : "4.2. Generating videos of arbitrary length",
      "text" : "A second advantage is that our model can also append new additional frames that have different motions. That is, since the temporal generator does not contain any fullyconnected layers, the generator can also produce videos of arbitrary length by introducing additional latent variables to the temporal generator and regarding them as a feature map.\nTo this end, we redefine a latent variable as z0 = [z10 , . . . , z N 0 ] that consists of N latent variables. The size of each element is the same as the previous one, i.e., z0 ∈ RN×K0 . In this representation, we can also regard this latent variable as a feature map with K0 channels. As the temporal generator now consists of fully deconvolutional layers, it can also produce more than F frames.\nTo make a long consistent video, a previous motion and a next one in the generated frames should be semantically similar. Therefore, in the generation of the latent variables for the temporal generator, we employ a strategy that the next latent variable is drawn from a Gaussian distribution, whose mean is equivalent to the previous one, i.e.,\nzn+10 ∼ N (zn0 , σ2I), (3)\nwhere I and σ denote a K0-dimensional identity matrix and the standard deviation, respectively."
    }, {
      "heading" : "4.3. Pre-trained models",
      "text" : "A third advantage is that our model can employ the existing pre-trained generator and the discriminator learned on a different image dataset as the image generator and the discriminator in our model. While the benefit of using the pre-trained model is to speed-up the learning (i.e, to skip the first training step), it also affects generation of various “unknown” frames that do not contain a given video dataset. Through the experiments, we observed there are pros and cons to using pre-trained network models (see Section 5.3.1)."
    }, {
      "heading" : "5. Experiments",
      "text" : ""
    }, {
      "heading" : "5.1. Datasets",
      "text" : "We performed experiments with the following datasets.\n3D character We first train our models on a simple video dataset consisting of 5,184 videos with 75 frames. It means that in the training of the image generators we used 388,800 images. In this dataset, the 3D character called Hatsune Miku [26] takes 16 different short-term actions such as waving her hand and looking back. Since the number of motions is relatively small, we augmented the videos by uniformly changing angles of the camera, and finally got 324 videos from one motion. All the videos have a resolution of 64×64 pixels and were sampled at 12 frames per second.\nMoving MNIST To investigate the properties of our models, we also trained the models on the moving MNIST dataset [37], in which each video consists of two digits moving inside a 64×64 patch. In these videos, two digits move linearly and the direction and magnitude of the movement vectors are randomly chosen. If a digit approaches the edges of the frame, it bounces off the edge and its direction is changed while maintaining the speed.\nUCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories of human actions[36]. As the video generation from such a complex general dataset is a challenging problem, we first extracted the videos belonging to a specific category from the dataset and used them for training. Specifically, we extracted IceDancing and PlayingViolin from 101 classes, and made two (sub)datasets. To evaluate the representation by the discriminator, we also train the TGANs with the whole UCF-101 training dataset. To reduce the resolution we resized all the videos to 60 × 80 pixels, and cropped a square with 48 pixels from the center."
    }, {
      "heading" : "5.2. Training configuration",
      "text" : "All the parameters used in the optimizer are the same as those of the original GAN. Specifically, we use the Adam optimizer [17] with the learning rate of 0.0002 and the momentum term β1 of 0.5. All the weights are initialized from a zero-centered Gaussian distribution with standard deviation of 0.02. Note that in both training steps (i.e., training the image networks and the temporal networks), the above parameters are all the same. We used chainer [43] for both the implementation and the training.\nWe observed that the discriminator easily wins against the generator on the complicated datasets and the training cannot proceed. To avoid this, we added Gaussian noise to all layers of discriminators when using the Moving MNIST\nand UCF-101 datasets. All the scale parameters γ after the Batch Normalization layer are not used.\nIt should be noted that in the training of the temporal generator and the discriminator, we observed that not fixing scale and bias parameters in all the batch normalization layer of the image generator and the discriminator significantly improves the quality of the generated videos. We presume this alleviates the difference between the distribution of the activation by the image generator and those by the dataset."
    }, {
      "heading" : "5.3. Qualitative evaluation",
      "text" : "We trained our proposed model on the above datasets and visually confirmed the quality of the results. Fig.2 shows examples of generated videos by the generator trained on the 3D character dataset. Our model successfully generates plausible videos in which the character performes semantically meaningful actions. It should be noted that we manually verified that the generated frames in Fig.2 do not exist in the 3D character dataset.\nWe also generated smooth transitions between the two videos by linearly interpolating the two latent variables. The results are shown in Fig.3. Our model successfully generates plausible motions from the latent variables, and does not simply memorize and output the existing motions from the dataset.\nThe interesting property can be seen in Fig.4. As with Fig.2, we trained the model on the moving MNIST dataset and generated the frames from latent random variables. The generated frames are quite different from those of the existing model proposed by Srivastava et al. [37]. While the proposed model can produce relatively distinct frames, the predicted frames by the existing model tend to be blurry. In contrast, our model is capable of producing consistent frames in which each image is sharp, clear and easy to discriminate two digits.\nAnother interesting property is that although it correctly generates the frames in which each digit continues to move in a straight line, its velocity frequently changes and sometimes fails to bounce off the frame edge. Note that the existing models such as [37, 15] seem to generate more plausible frames that have those properties, however, these methods can not be directly compared with our method because the qualitative results the authors have shown are for “video prediction” that aims to generate future sequences from initial frames, whereas our results are generated from scratch.\nWe assumed that the main reason why such a result appears is related to a property of DCGANs. The existing DCGAN is capable of generating sharp images that contain realistic and semantically meaningful parts. However, it sometimes fails to correctly combine these parts and hence generates globally inconsistent image. We considered that the temporal generator exhibits a similar property: it can generate videos that consist of plausible short sequences, but\nsometimes fails to correctly combine those short sequences and generates an inconsistent video sequence.\nThe above properties are also confirmed in the experimental results using a specific category of UCF-101 (Fig.5). As the number of videos used for training was only about 100, it can be seen that the generated images by the image generator look very similar to those of the original dataset. However, the temporal generator tries to generate new videos by connecting similar motions from different videos.\nWe also observed that our model could correctly generate plausible videos even in a “dynamic” scene such as IceDancing where the background frequently changes due to the moving camera. This is a clear advantage of our model compared to existing models which assumes a fixed background such as [45]."
    }, {
      "heading" : "5.3.1 Three applications",
      "text" : "We performed the following experiments to illustrate the effectiveness of the three applications described in Section 4.1.\nTo show our model can be applied to frame interpolation, we generated intermediate frames by interpolating two adjacent latent variables of the image space. These results are shown in Fig.6. Since it can be seen that these frames are natural-looking and plausible, we verified that our generator has the ability to yield smooth videos with more frames per second than those in the original dataset.\nTo confirm whether our model is also capable of generating a long consistent sequence of length larger than sixteen frames, we also applied our method to the temporal generator trained on the 3D character dataset. In this case σ2 in Eq.(3) and the length of z0 were respectively set to 0.1 and 10. This means that our model generated 160 frames\nper video. We show the example result in Fig.7; we confirmed that the generated longer sequences are consistent. Due to limitations of space, we provide further results in the supplementary material.\nIn the experiments of using pre-trained models, we first used the existing generator and the discriminator provided in [30]. Next, we trained the parameters of the temporal generator and the discriminator with all of the training sets in UCF-101 while fixing all of the parameters of the pre-trained models except for batch normalization layers. The result is shown in Fig.8. It indicates that the generator managed to produce a wide variety of videos in which non-UCF101 elements, such as “dogs”, appear. However, we also observed that the number of “uninterpretable” videos which the generator outputs is higher than the original generator\ntrained from scratch."
    }, {
      "heading" : "5.4. Quantitative evaluation",
      "text" : "In order to investigate what kind of features are produced from each discriminator, we extracted two feature vectors from the image discriminator and the temporal discriminator, and applied them to the action recognition problem. Specifically, we used the activations in the last layer of the image discriminator and the first layer of the temporal discriminator, and constructed two classifiers that use a linear SVM with a leave-one-group-out threefold cross validation. For the evaluation of the image discriminator, we extracted 15 frames per clip at regular intervals and decided the result by the majority rule. As with the image discriminator, in the temporal discriminator we extracted 10 short clips having 16 frames per clip at regular intervals. We compare using the model proposed by Vondrick et al. [45].\nTable 2 shows quantitative results. It can be seen that\nthe image discriminator achieves a performance equal to or greater than that of the existing model despite our model only using 48× 48 pixels. Note that although the authors report an accuracy of 49.3% using a pair of a logistic regression and feature vectors extracted from their network, it is incommensurable with our model because they used another video dataset containing 5,000 hours videos for training. For comparison, we also extracted feature vectors with ResNet 50 Layers [9], and constructed a classifier in the same manner. Although its accuracy is higher than ours, we confirmed that our image discriminator successfully extracts some semantic information in an unsupervised manner.\nUnfortunately, the performance of the temporal discriminator is significantly worse than that of the image discriminator. It is clear that the temporal discriminator can extract semantic information about actions from the fact that its accuracy is above the chance rate, however, we hypothesize that the current temporal discriminator drops some useful features such as consistent actions."
    }, {
      "heading" : "6. Summary and future work",
      "text" : "We propose a generative model that learns semantic representations of videos and can generate realistic sequences. To tackle this challenging problem, i.e., unsupervised learning of videos, we formulate a generative process of videos as\na pair of (i) a function that generates a set of latent variables representing image space, and (ii) a function that converts them into a sequence. Using this representation, our model can not only generate plausible videos, but also perform frame interpolation of the generated video, and handle videos of arbitrary length. Through several experiments, we confirmed the usefulness of our models.\nDespite the above promising results, few problems still remain. The first is the end-to-end training. We speculate the quality of generated frames in TGAN could benefit from end-to-end training, however, we are yet to succeed training in such fashion. We also tried fine-tuning all the parameters in the networks after the two-step training, we have observed that it does not work well. An investigation of the end-to-end training and the fine tuning will be a part of future work.\nInvestigating the suitability of deeper models for our generators is also important. Although in this work we follow the existing studies of Radford et al. [30], we conjecture that our model’s quality could be significantly improved using recent networks such as ResNet [9] and DenseNet [11].\nAcknowledgements We would like to thank Brian Vogel, Jethro Tan, Tommi Kerola, and Zornitsa Kostadinova for helpful discussions."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this paper we propose a generative model, the Temporal Generative Adversarial Network (TGAN), which can learn a semantic representation of unlabelled videos, and is capable of generating consistent videos. Unlike an existing GAN that generates videos with a generator consisting of 3D deconvolutional layers, our model exploits two types of generators: a temporal generator and an image generator. The temporal generator consists of 1D deconvolutional layers and outputs a set of latent variables, each of which corresponds to a frame in the generated video, and the image generator transforms them into a video with 2D deconvolutional layers. This representation allows efficient training of the network parameters. Moreover, it can handle a wider range of applications including the generation of a long sequence, frame interpolation, and the use of pre-trained models. Experimental results demonstrate the effectiveness of our method.",
    "creator" : "LaTeX with hyperref package"
  }
}