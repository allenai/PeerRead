{
  "name" : "1705.06769.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning",
    "authors" : [ "Nat Dilokthanakul", "Christos Kaplanis" ],
    "emails" : [ "n.dilokthanakul14@imperial.ac.uk", "christos.kaplanis14@imperial.ac.uk", "n.pawlowski16@imperial.ac.uk", "m.shanahan@imperial.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning methods [28, 20] often struggle in environments where the rewards are sparsely encountered, and when their acquisition requires the coordination of temporally extended sequences of actions. In these types of environments, archetypally exemplified by the Atari game Montezuma’s revenge, the dearth of feedback the agent receives from the environment makes it very difficult to learn long sequences of actions, particularly when the timescale of the exploration strategy is short.\nHierarchical reinforcement learning (HRL) [3] is an approach that aims to deal with the reward sparsity problem by equipping the agent with temporally extended macro-actions, also known as options [29] or skills [16], which abstract over sequences of primitive actions. If useful options are established, then long sequences of primitive actions can be expressed by much shorter sequences of options, which are easier to learn as the agent can now employ temporally extended exploration in the option space. However, learning useful options is a difficult task in itself; one possibility is to incorporate prior knowledge about the task into their construction [3, 24, 9] but this can limit the generalisability of the algorithm to other tasks.\nIn this paper, we constrain this prior knowledge to the hypothesis that the ability to control features of its environment is an inherently useful skill for an agent to have for succeeding at a wide variety of tasks. By applying this concept in a deep HRL setting, we design an agent that is intrinsically motivated to control aspects of its environment via a set of options.\nar X\niv :1\n70 5.\n06 76\n9v 1\n[ cs\n.L G\n] 1\n8 M\nThe architecture of our agent is inspired by feudal reinforcement learning [8, 30] and the hierarchical deep reinforcement learning framework [17], whereby a meta-controller provides embedded subgoals to a sub-controller that interacts directly with the environment (see Figure 1a). In our agent, the metacontroller, which learns to maximise extrinsic reward from the environment, tells the sub-controller what feature of the environment it should control; the sub-controller receives intrinsic rewards for successfully changing the chosen feature as well as extrinsic rewards from the environment. We show that, when guided by this form of intrinsic motivation, the agent learns to perform better in tasks featuring sparse rewards.\nOur main contribution is in the design of discrete sets of subgoals available for the meta-controller to choose from and their corresponding intrinsic reward. By taking an existing idea of feature control [13, 6] and incorporating it into the subgoal design, we introduce a hierarchical agent with generically useful learnable options which we empirically evaluate in the Atari domain."
    }, {
      "heading" : "1.1 Related work",
      "text" : "The idea of embodying an agent with a form of intrinsic motivation, which in our case is the desire to be able to control aspects of its environment, is one that has been explored in several other works. Klyubin et al. introduced empowerment as an information theoretic measure of degrees of freedom that an agent has over an environment [15]. The concept of empowerment has recently gained interest in the context of intrinsically motivated reinforcement learning [22, 10]. In the other lines of work, intrinsic motivation is defined in the form of curiosity, which can be measured with model-learning progress [25, 12] or information gain [4].\nJaderberg et al. introduced the idea of off-policy training with auxiliary control tasks, such as pixel control or feature control, which can significantly speed up learning of the main task [13]. The rationale is that learning the auxiliary tasks gives the agent features that are useful for manipulating the environment. Drawing on this idea, we apply the idea of pixel and feature control to the HRL framework so that options are constructed with the explicit motive of altering given features or patches of pixels. By doing so, our agent is equipped with temporally-extended options, which can be used on-policy to explore the environment in a temporally-extended manner, thereby helping to address the problem of sparse reward.\nOur architecture also takes inspiration from recent work by Kulkarni et al. [17] and Vezhnevets et al. [30]. These works outline hierarchical architectures that comprise a subgoal-selecting metacontroller and a sub-controller that tries to achieve the subgoal. The main feature that sets our model apart is the design of the subgoals. Kulkarni et al. pre-define a set of discrete subgoals specific to the tasks at hand, while Vezhnevets et al. construct subgoals as a large continuous set of embedded states. We construct two discrete sets of subgoals, which are discussed in more detail in Section 2.1. One of them is fixed but is designed to be generically applicable in visual domains; the other can be automatically learned such that the subgoals are useful for solving the task at hand.\nThere are a large number of works on subgoal discovery [27, 19, 18], most of which are based on finding bottleneck states. Since finding bottleneck states requires global statistics of the environment, finding them can be difficult and hard to scale. Our work is in line with contemporary HRL, e.g. Option-Critic [2], which has moved towards end-to-end training where options and subgoals can automatically emerge from the optimisation of the system, with carefully designed architectures and objective functions."
    }, {
      "heading" : "2 The model",
      "text" : "We consider the standard reinforcement learning setting where an agent interacts with an environment by observing the state of the environment st and taking an action at at every discrete time step t. The environment provides an extrinsic reward to the agent rextt and then transitions to the next state st+1. The goal of the agent is to maximise the accumulated sum of extrinsic rewards over the finite horizon length of an episode.\nSpecifically, we consider a hierarchical agent with two components: a meta-controller and a subcontroller. The sub-controller is responsible for choosing the agent’s actions and directly interacts with the environment. The meta-controller operates on a longer time scale of c time steps and influences the behaviour of the sub-controller through a subgoal argument, gt. This influence is imposed by\ngiving gt as an input to the sub-controller in addition to st. Importantly, the meta-controller also gives intrinsic reward to the sub-controller, rintt , for successfully completing the subgoal gt and thus, by learning to associate gt and rintt , the sub-controller’s behaviour is biased to complete the subgoals. At the same time, the meta-controller learns to select sequences of gt such that the sub-controller trajectory maximises accumulated extrinsic reward."
    }, {
      "heading" : "2.1 Measure of control for intrinsic reward",
      "text" : "Here we detail two variants of our algorithm, corresponding to two ways to deliver the subgoal gt: (a) the pixel-control agent and (b) the feature-control agent. Both agents have the same architecture, which will be discussed in Section 2.2. The main difference between the two is the calculation of intrinsic reward, which is crucial for manipulation of the behaviour of the sub-controller.\nPixel control Following Jaderberg et al., we study the most basic form of controlling ability in the visual domain, which is the ability to control a given subset of pixels in the visual input [13]. We divide the pre-processed 84x84 input image into 36 pixel patches of size 14x14. We define the intrinsic reward as the squared difference between two consecutive frames of pixels in the patch, normalized by the squared difference of the whole image. The sub-controller is thus encouraged to maximise the change in values of pixels in the given (kth) patch relative to the entire screen. This can be written formally as\nrint(k) = η ||hk (st − st−1)||2\n||st − st−1||2 , (1)\nwhere hk is an 84x84 binary filter matrix with entries all equal to 0 apart from the kth pixel patch, which has entries all equal to 1. By applying this filter with element-wise multiplication , only the changes in the relevant part of the screen are taken into account. η is a scaling factor which controls the magnitude of the intrinsic reward per time step.1\nFeature control Jaderberg et al. introduced a notion of feature control which is defined as the ability to control the activations of specific neurons. Similarly, Bengio et al. introduced the notion of feature selectivity that measures how much a feature can be controlled, independently from other features [6]. We define intrinsic reward as Bengio et al.’s feature selectivity measure on the second convolutional layer of our network. To measure the selectivity of a feature, we take the difference between the mean activation of a selected feature map at consecutive time steps and normalize with\n1We choose η = 0.05 which gives a reasonable value of accumulated intrinsic reward over an episode at the start of the training. We leave the tuning of this parameter for future work. While we believe tuning this parameter is important, the more important parameter is the relative weight between extrinsic and intrinsic reward, see eq. 3.\nall feature maps. This can be written as\nrint(k) = η ||fk(st)− fk(st−1)||∑ k′ ||fk′(st)− fk′(st−1)|| , (2)\nwhere fk(.) denotes the mean over activation values in the kth feature map and ∑\nk′ denotes summation over all feature maps.\nIn contrast with the pixel-control agent, allowing the meta-controller to select a convolutional feature endows the agent with more flexible and abstract control of its environment. The instruction from the meta-controller is more abstract since a feature map can represent a complex function of the sensory inputs, and it is more flexible because the feature maps can be shaped during learning to encode aspects of the environment that are useful to control for the completion of the main task.\nShaped reward In addition to intrinsic reward, we also give extrinsic reward to the sub-controller, enabling it to learn fine-grained behaviour. We adjust the ratio between intrinsic and extrinsic reward with a parameter β, which results in the shaped reward,\nrt = βr ext t + (1− β)rintt . (3)\nThe sub-controller strictly follows the order of the meta-controller if β = 0. On the other hand, if β = 1, the meta-controller has little direct influence on the sub-controller. In this case, the subcontroller still receives the subgoal argument as an input but does not receive rewards for attaining the subgoal."
    }, {
      "heading" : "2.2 Architectural and optimisation details",
      "text" : "Our baseline model is a variant of the asynchronous advantage actor-critic algorithm (A3C) [21]. We adapted OpenAI’s A3C implementation2 3 to follow the architecture specified by Wang et al. [31]. The model consists of two parts: an encoding module and a Long-Short Term Memory layer (LSTM) [11]. The encoding module consists of two convolution layers and a fully connected layer. The first convolution has 16 8x8 filters with a stride length of 4 and the second layer has 32 4x4 filters with a stride length of 2. The second convolution is followed by a fully connected layer with 256 units. The output of the fully connected layer is then concatenated with the previous action and the previous reward, and then fed into an LSTM layer. The LSTM has 256 cells whose output linearly projects into the policy and value networks.\nOur hierarchical model extends the baseline model as follows (Figure 1). An additional LSTM layer is added to parameterise the meta-controller’s value and policy function. The input to the meta-controller’s LSTM includes the previous subgoal argument and extrinsic rewards accumulated from the previous meta-step. The sub-controller’s LSTM also has an additional input consisting of the current subgoal argument. The meta-controller operates every c = 100 time steps.\nThe subgoal argument is a one-hot vector, which specifies the index of the subgoal selected by the meta-controller. For the pixel-control agent, there are 37 subgoals corresponding to 36 patches of pixel plus a no-op which gives no intrinsic reward. The feature-control agent has 32 discrete subgoals, each corresponding to a feature map in the second convolutional layer.\nThe value and policy networks are optimised using the A3C loss function with 8 asynchronous agents, where the advantage is estimated with the generalized advantage estimator [26], using γ = 0.99 and λ = 1.0. We use the ADAM optimizer [14] with a learning rate of 0.0001 for all of our experiments. Our experiments use a backpropagation through time (BPTT) trajectory length of either 20 or 100 time steps for sub-controller and the baseline agent, and 20 meta-steps (2000 time steps) for the meta-controller. Finally, the gradients are scaled down when their L2-norms exceed 40."
    }, {
      "heading" : "3 Experimental results and discussion",
      "text" : "The objectives of our experiments are:\n2Our implementation is an adaptation of an open-source implementation of A3C, namely, “Universe-StarterAgent”. (https://github.com/openai/universe-starter-agent) which is written with Tensorflow [1]\n3The source code of our implementation will be made publicly available.\n1. To verify that the influence from the meta-controller is beneficial in sparse reward environments.\n2. To evaluate the performance of our agent in several environments in comparison to current state-of-the-art HRL systems.\n3. To study the behaviour of the agents under the influence of pixel-control and feature-control intrinsic motivation."
    }, {
      "heading" : "3.1 The environments",
      "text" : "We evaluated the model on Atari games in the OpenAI gym environment [7], a toolkit for comparing reinforcement learning algorithms that wraps the Arcade Learning Environment (ALE) [5] with a number of modifications. In our experiments, we make comparisons with the Feudal Network (FuN) [30] and Option-Critic [2] architectures, both evaluated on the ALE. 4 The environment provides the state as 210x160x3 RGB pixels. We pre-process the state by reshaping it into an 84x84x3 matrix, retaining the RGB channels. We also clip the extrinsic reward to the range of [-1, 1]. We used v0 setting for all the games, e.g., MontezumaRevenge-v0 for Montezuma’s Revenge."
    }, {
      "heading" : "3.2 Experiment 1: Influence of the meta-controller on performance",
      "text" : "To evaluate the effectiveness of the meta-controller, we ran our agent with different relative weights β ∈ {0.00, 0.25, 0.50, 0.75, 1.00} between extrinsic and intrinsic reward, and compared the performance with the baseline agent. We used BPTT = 20 for the sub-controller. First, we found that with β = 1.00 (no intrinsic reward) the agent’s performance was similar to the baseline. This result demonstrates that any significant gain or decline in performance using other values of β can be attributed to the intrinsic reward.\nAs shown in Figure 2, the feature-control agent with β = 0.75 outperforms other agents in Montezuma’s Revenge and Frostbite and is competitive with other agents in Q*bert and Private Eye. This result suggests that introducing a certain proportion of intrinsic reward in the sub-controller has a\n4However, it is important to note that they are not directly comparable. OpenAI’s gym adds stochasticity through the use of random frame-skips, while ALE is a deterministic environment. The standard evaluation protocol in ALE is to add a random number of no-op actions at the start of the episode to achieve some stochasticity.\npositive effect in sparse reward environments (such as Montezuma’s Revenge) without degrading the performance on dense reward environments (such as Q*bert).\nOur agents with β = 0 (no extrinsic reward) perform very poorly as expected. Since the sub-controller can only follow a limited number of subgoals from the meta-controller, its behaviours are also limited in this case. Giving extrinsic reward to the sub-controller is a way to allow fine-grained behaviours that are important for maximising extrinsic reward. Interestingly, agents with β = 0.25 also perform worse than baseline in Q*bert. This result shows that too much influence from the meta-controller can have negative effect in dense reward environments. Interestingly, the best value of β is consistent across all four games.\nWe observe that the pixel-control agent learns more quickly than the feature-control agent. However, the feature-control agent generally achieves better scores after 100 million frames of training. This is likely due to the fact that the feature-control agent needs time to learn useful features before the influence of the meta-controller becomes meaningful. Once the features have been learned, the subgoals obtained are of higher quality than the hard-coded ones in the pixel-control agent."
    }, {
      "heading" : "3.3 Experiment 2: Performance instability and the length of the BPTT roll-out",
      "text" : "In our initial experiments, we observed instability in the training curve of the feature-control agent, which comes in the form of catastrophic drops in performance. To alleviate this problem, we tried increasing the BPTT roll-out from 20 to 100 steps for the sub-controller. We reasoned that a longer unrolled sequence of BPTT could contribute to training stability in the following ways: (i) the updates are less frequent and give the agent more stable features, which are a crucial component in the calculation of the intrinsic reward, and (ii) it allows the gradient to be backpropagated further into the past, which potentially reduces bias in the update.\nIn Figure 3 we see that in Montezuma’s Revenge the agent attains a much higher score with BPTT = 100 than with BPTT = 20. In Frostbite, however, we observe the opposite effect. This could be because the gradient is already stable at BPTT = 20 and so increasing the BPTT length does not yield any positive effect; on the contrary, as a result of lowering the frequency of updates it can result in slower learning (see Figure 3)."
    }, {
      "heading" : "3.4 Experiment 3: Comparison to state-of-the-art hierarchical RL systems",
      "text" : "In this experiment, we evaluated our feature-control agent on Ms. Pac-Man, Asterix, Zaxxon and Montezuma’s Revenge. The aim was to show that the method is applicable to a broad range of games, and to compare our system to two state-of-the-art end-to-end HRL systems, namely the Option-Critic [2] and FuN [30] architectures.\nOur results are shown in Figure 4 and we note the following: (i) On Ms. Pac-Man, Asterix and Zaxxon we achieve better maximum scores than the Option-Critic network but worse maximum scores than the FuN Network; (ii) on Montezuma’s Revenge, our agent reaches approximately the same maximum score as the FuN network but it learns much more quickly, reaching this level of performance after fewer than a fifth of the number of observations. We anticipate being able to improve our agent’s\nperformance with a broader parameter search. For example, the discount parameter, γ, has been shown to have a significant impact on the performances of both A3C and FuN on different Atari games [30]. We did not compare with other state-of-the-art results in Montezuma’s Revenge, such as those obtained with the UNREAL agent [13], DQN-CTS [4] and DQN-PixelCNN [23], since these are not competing HRL methods and their advantageous features could easily be integrated into our agent."
    }, {
      "heading" : "3.5 Experiment 4: Visualisation of the agent’s behaviour under intrinsic motivation",
      "text" : "The influence of the intrinsic motivation provided by the meta-controller on the agent’s behaviour can be most easily visualised with the pixel-control agent. In Figure 5a, a sequence of screenshots from Montezuma’s Revenge is shown where we see the sub-controller moving the character to the patch selected by the meta-controller and causing it to jump around in the patch in order to generate intrinsic reward. Figure 5b shows another sequence where the meta-controller selects a patch over the ladder that must be climbed to collect the key. The character moves to the patch, but when the meta-controller then changes the location of the patch, the sub-controller ignores it and instead proceeds to collect the key, which results in extrinsic reward. This example highlights the importance of motivating the sub-controller with extrinsic as well as intrinsic reward, allowing the agent to be flexible and not completely at the mercy of the meta-controller.\nInterpreting the intrinsic motivation of the feature-control agent is much more difficult, since it involves understanding what is encoded in the selected convolutional feature map. In an attempt to visualise this, we upsampled the selected feature map and overlaid it with the raw state input5. The feature-control agent has to learn strategies to maximally change the activations of this feature map to gain intrinsic reward.\nWe present two scenarios with the feature-control agent in Figure 5c and d, which indicate how different types of features can evolve to form useful options for the agent. Figure 5c shows the agent collecting the key in the first room of Montezuma’s Revenge. In this scenario, the feature map is activated in front of the agent on the path towards the key. This implicitly encourages the agent to move towards the key, as it attempts to maximally alter the activations of the feature map. Figure 5d shows the agent collecting the sword in another room. In this scenario, the feature map is only activated when the agent completes the apparent sub-task (collecting the sword), as opposed to the first scenario where the entire path to completing the sub-task (collecting the key) is highlighted."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we presented an approach to tackling the reward sparsity problem in the form of a two-module deep hierarchical agent. In Montezuma’s Revenge, an Atari game with particularly sparse rewards, our agent learns several times faster than the current state-of-the-art HRL agents, reaching a similar final level of performance. We also show that our subgoal designs are generically applicable across visual tasks by evaluating the agent on several different games. Our agent almost\n5This approach of visualisation of attention features is used by Xu et al. (2015) [32].\nalways performs better than the baseline agent, which suggests that the ability to control aspects of the environment can be a generically useful subgoal.\nWe argue that part of the performance gain comes from the ability to perform temporally-abstracted exploration. By visualising the trajectories of the pixel-control agent, we observe that it successfully learns to move towards the patch selected by the meta-controller in order to maximise its intrinsic reward; the acquisition of this skill allows the meta-controller to motivate the agent to explore its environment in a broad and temporally extended manner. In the feature-control agent, the options are learned via the shaping of the convolutional features and, while the features are harder to interpret than the pixel patches, there is evidence from our visualisations that they are activated by the completion of intuitive subgoals, such as collection of the sword in Montezuma’s Revenge.\nAn important result from our experiments is that the best performances are achieved when the subcontroller is motivated by a combination of intrinsic and extrinsic reward. By leaking some extrinsic reward to the sub-controller, it frees us from the restriction that subgoals need to be complete or carefully designed, which can lead to brittle or sub-optimal solutions [9]. As long as the subgoals are useful for exploration, an agent equipped with such skills can learn faster, while still maintaining the ability to fine-tune its behaviour to maximise extrinsic reward.\nIn order to give our agent more flexibility, it would be interesting, in future work, to incorporate a termination condition to the options, which would allow the instruction from the meta-controller to be variable in length and thus more temporally precise. Additionally it would be interesting to quantify the extent to which our agents have learned to control their environment, perhaps by using the measure of empowerment [15]."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Marc Deisenroth for providing us with Azure credits from the Microsoft Azure Sponsorship for Teaching and Research. We would also like to thank Kyriacos Nikiforou, Hugh Salimbeni and Kai Arulkumaran for fruitful discussions. N.D. is supported by the DPST scholarship from the Thai government."
    } ],
    "references" : [ {
      "title" : "and X",
      "author" : [ "M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Mané", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Viégas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu" ],
      "venue" : "Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The option-critic architecture",
      "author" : [ "P.-L. Bacon", "J. Harb", "D. Precup" ],
      "venue" : "AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Recent advances in hierarchical reinforcement learning",
      "author" : [ "A.G. Barto", "S. Mahadevan" ],
      "venue" : "Discrete Event Dynamic Systems, 13(4):341–379",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Unifying count-based exploration and intrinsic motivation",
      "author" : [ "M. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1471–1479",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research, 47:253–279",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Independently Controllable Features",
      "author" : [ "E. Bengio", "V. Thomas", "J. Pineau", "D. Precup", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1703.07718",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Openai gym",
      "author" : [ "G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba" ],
      "venue" : "arXiv preprint arXiv:1606.01540",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Feudal reinforcement learning",
      "author" : [ "P. Dayan", "G.E. Hinton" ],
      "venue" : "Proceedings of the 5th International Conference on Neural Information Processing Systems, pages 271–278. Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Hierarchical reinforcement learning with the maxq value function decomposition",
      "author" : [ "T.G. Dietterich" ],
      "venue" : "Journal of Artificial Intelligence Research, 13:227–303",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Variational Intrinsic Control",
      "author" : [ "K. Gregor", "D. Jimenez Rezende", "D. Wierstra" ],
      "venue" : "International Conference on Learning Representations Workshop",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, 9(8): 1735–1780",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Vime: Variational information maximizing exploration",
      "author" : [ "R. Houthooft", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1109–1117",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Reinforcement learning with unsupervised auxiliary tasks",
      "author" : [ "M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "International Conference on Learning Representations",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "International Conference on Learning Representations",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Empowerment: A universal agent-centric measure of control",
      "author" : [ "A.S. Klyubin", "D. Polani", "C.L. Nehaniv" ],
      "venue" : "Evolutionary Computation, 2005. The 2005 IEEE Congress on, volume 1, pages 128–135. IEEE",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Skill discovery in continuous reinforcement learning domains using skill chaining",
      "author" : [ "G. Konidaris", "A.G. Barto" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1015–1023",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
      "author" : [ "T.D. Kulkarni", "K. Narasimhan", "A. Saeedi", "J. Tenenbaum" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3675–3683",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Automatic discovery of subgoals in reinforcement learning using diverse density",
      "author" : [ "A. McGovern", "A.G. Barto" ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning, pages 361–368. Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Q-cut—dynamic discovery of sub-goals in reinforcement learning",
      "author" : [ "I. Menache", "S. Mannor", "N. Shimkin" ],
      "venue" : "European Conference on Machine Learning, pages 295–306. Springer",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "et al",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : "Human-level control through deep reinforcement learning. Nature, 518(7540):529–533",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "International Conference on Machine Learning, pages 1928–1937",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Variational information maximisation for intrinsically motivated reinforcement learning",
      "author" : [ "S. Mohamed", "D.J. Rezende" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2125– 2133",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A",
      "author" : [ "G. Ostrovski", "M.G. Bellemare" ],
      "venue" : "van den Oord, and R. Munos. Count-based exploration with neural density models. In International Conference on Machine Learning",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Reinforcement learning with hierarchies of machines",
      "author" : [ "R. Parr", "S.J. Russell" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1043–1049",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Curious model-building control systems",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Networks, 1991. 1991 IEEE International Joint Conference on, pages 1458–1463. IEEE",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "High-dimensional continuous control using generalized advantage estimation",
      "author" : [ "J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel" ],
      "venue" : "International Conference on Learning Representations",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Using relative novelty to identify useful temporal abstractions in reinforcement learning",
      "author" : [ "Ö. Şimşek", "A.G. Barto" ],
      "venue" : "Proceedings of the twenty-first international conference on Machine learning, page 95. ACM",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press, Cambridge, MA, USA, 1st edition",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "R.S. Sutton", "D. Precup", "S. Singh" ],
      "venue" : "Artificial Intelligence, 112(1-2):181–211",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Feudal networks for hierarchical reinforcement learning",
      "author" : [ "A.S. Vezhnevets", "S. Osindero", "T. Schaul", "N. Heess", "M. Jaderberg", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "International Conference on Machine Learning",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Learning to reinforcement learn",
      "author" : [ "J.X. Wang", "Z. Kurth-Nelson", "D. Tirumala", "H. Soyer", "J.Z. Leibo", "R. Munos", "C. Blundell", "D. Kumaran", "M. Botvinick" ],
      "venue" : "arXiv preprint arXiv:1611.05763",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Show",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio" ],
      "venue" : "attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048–2057",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Reinforcement learning methods [28, 20] often struggle in environments where the rewards are sparsely encountered, and when their acquisition requires the coordination of temporally extended sequences of actions.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "Reinforcement learning methods [28, 20] often struggle in environments where the rewards are sparsely encountered, and when their acquisition requires the coordination of temporally extended sequences of actions.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "Hierarchical reinforcement learning (HRL) [3] is an approach that aims to deal with the reward sparsity problem by equipping the agent with temporally extended macro-actions, also known as options [29] or skills [16], which abstract over sequences of primitive actions.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "Hierarchical reinforcement learning (HRL) [3] is an approach that aims to deal with the reward sparsity problem by equipping the agent with temporally extended macro-actions, also known as options [29] or skills [16], which abstract over sequences of primitive actions.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 15,
      "context" : "Hierarchical reinforcement learning (HRL) [3] is an approach that aims to deal with the reward sparsity problem by equipping the agent with temporally extended macro-actions, also known as options [29] or skills [16], which abstract over sequences of primitive actions.",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 2,
      "context" : "However, learning useful options is a difficult task in itself; one possibility is to incorporate prior knowledge about the task into their construction [3, 24, 9] but this can limit the generalisability of the algorithm to other tasks.",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "However, learning useful options is a difficult task in itself; one possibility is to incorporate prior knowledge about the task into their construction [3, 24, 9] but this can limit the generalisability of the algorithm to other tasks.",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "However, learning useful options is a difficult task in itself; one possibility is to incorporate prior knowledge about the task into their construction [3, 24, 9] but this can limit the generalisability of the algorithm to other tasks.",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 7,
      "context" : "The architecture of our agent is inspired by feudal reinforcement learning [8, 30] and the hierarchical deep reinforcement learning framework [17], whereby a meta-controller provides embedded subgoals to a sub-controller that interacts directly with the environment (see Figure 1a).",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : "The architecture of our agent is inspired by feudal reinforcement learning [8, 30] and the hierarchical deep reinforcement learning framework [17], whereby a meta-controller provides embedded subgoals to a sub-controller that interacts directly with the environment (see Figure 1a).",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "The architecture of our agent is inspired by feudal reinforcement learning [8, 30] and the hierarchical deep reinforcement learning framework [17], whereby a meta-controller provides embedded subgoals to a sub-controller that interacts directly with the environment (see Figure 1a).",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "By taking an existing idea of feature control [13, 6] and incorporating it into the subgoal design, we introduce a hierarchical agent with generically useful learnable options which we empirically evaluate in the Atari domain.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "By taking an existing idea of feature control [13, 6] and incorporating it into the subgoal design, we introduce a hierarchical agent with generically useful learnable options which we empirically evaluate in the Atari domain.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "introduced empowerment as an information theoretic measure of degrees of freedom that an agent has over an environment [15].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 21,
      "context" : "The concept of empowerment has recently gained interest in the context of intrinsically motivated reinforcement learning [22, 10].",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "The concept of empowerment has recently gained interest in the context of intrinsically motivated reinforcement learning [22, 10].",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "In the other lines of work, intrinsic motivation is defined in the form of curiosity, which can be measured with model-learning progress [25, 12] or information gain [4].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 11,
      "context" : "In the other lines of work, intrinsic motivation is defined in the form of curiosity, which can be measured with model-learning progress [25, 12] or information gain [4].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "In the other lines of work, intrinsic motivation is defined in the form of curiosity, which can be measured with model-learning progress [25, 12] or information gain [4].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 12,
      "context" : "introduced the idea of off-policy training with auxiliary control tasks, such as pixel control or feature control, which can significantly speed up learning of the main task [13].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 16,
      "context" : "[17] and Vezhnevets et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[30].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "There are a large number of works on subgoal discovery [27, 19, 18], most of which are based on finding bottleneck states.",
      "startOffset" : 55,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "There are a large number of works on subgoal discovery [27, 19, 18], most of which are based on finding bottleneck states.",
      "startOffset" : 55,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "There are a large number of works on subgoal discovery [27, 19, 18], most of which are based on finding bottleneck states.",
      "startOffset" : 55,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "Option-Critic [2], which has moved towards end-to-end training where options and subgoals can automatically emerge from the optimisation of the system, with carefully designed architectures and objective functions.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : ", we study the most basic form of controlling ability in the visual domain, which is the ability to control a given subset of pixels in the visual input [13].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "introduced the notion of feature selectivity that measures how much a feature can be controlled, independently from other features [6].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "Our baseline model is a variant of the asynchronous advantage actor-critic algorithm (A3C) [21].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 30,
      "context" : "[31].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "The model consists of two parts: an encoding module and a Long-Short Term Memory layer (LSTM) [11].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "The value and policy networks are optimised using the A3C loss function with 8 asynchronous agents, where the advantage is estimated with the generalized advantage estimator [26], using γ = 0.",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 13,
      "context" : "We use the ADAM optimizer [14] with a learning rate of 0.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "com/openai/universe-starter-agent) which is written with Tensorflow [1] The source code of our implementation will be made publicly available.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "We evaluated the model on Atari games in the OpenAI gym environment [7], a toolkit for comparing reinforcement learning algorithms that wraps the Arcade Learning Environment (ALE) [5] with a number of modifications.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "We evaluated the model on Atari games in the OpenAI gym environment [7], a toolkit for comparing reinforcement learning algorithms that wraps the Arcade Learning Environment (ALE) [5] with a number of modifications.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 29,
      "context" : "In our experiments, we make comparisons with the Feudal Network (FuN) [30] and Option-Critic [2] architectures, both evaluated on the ALE.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "In our experiments, we make comparisons with the Feudal Network (FuN) [30] and Option-Critic [2] architectures, both evaluated on the ALE.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "We also clip the extrinsic reward to the range of [-1, 1].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "The aim was to show that the method is applicable to a broad range of games, and to compare our system to two state-of-the-art end-to-end HRL systems, namely the Option-Critic [2] and FuN [30] architectures.",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 29,
      "context" : "The aim was to show that the method is applicable to a broad range of games, and to compare our system to two state-of-the-art end-to-end HRL systems, namely the Option-Critic [2] and FuN [30] architectures.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 29,
      "context" : "For example, the discount parameter, γ, has been shown to have a significant impact on the performances of both A3C and FuN on different Atari games [30].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 12,
      "context" : "We did not compare with other state-of-the-art results in Montezuma’s Revenge, such as those obtained with the UNREAL agent [13], DQN-CTS [4] and DQN-PixelCNN [23], since these are not competing HRL methods and their advantageous features could easily be integrated into our agent.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "We did not compare with other state-of-the-art results in Montezuma’s Revenge, such as those obtained with the UNREAL agent [13], DQN-CTS [4] and DQN-PixelCNN [23], since these are not competing HRL methods and their advantageous features could easily be integrated into our agent.",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "We did not compare with other state-of-the-art results in Montezuma’s Revenge, such as those obtained with the UNREAL agent [13], DQN-CTS [4] and DQN-PixelCNN [23], since these are not competing HRL methods and their advantageous features could easily be integrated into our agent.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 31,
      "context" : "(2015) [32].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "By leaking some extrinsic reward to the sub-controller, it frees us from the restriction that subgoals need to be complete or carefully designed, which can lead to brittle or sub-optimal solutions [9].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 14,
      "context" : "Additionally it would be interesting to quantify the extent to which our agents have learned to control their environment, perhaps by using the measure of empowerment [15].",
      "startOffset" : 167,
      "endOffset" : 171
    } ],
    "year" : 2017,
    "abstractText" : "The problem of sparse rewards is one of the hardest challenges in contemporary reinforcement learning. Hierarchical reinforcement learning (HRL) tackles this problem by using a set of temporally-extended actions, or options, each of which has its own subgoal. These subgoals are normally handcrafted for specific tasks. Here, though, we introduce a generic class of subgoals with broad applicability in the visual domain. Underlying our approach (in common with work using “auxiliary tasks”) is the hypothesis that the ability to control aspects of the environment is an inherently useful skill to have. We incorporate such subgoals in an end-to-end hierarchical reinforcement learning system and test two variants of our algorithm on a number of games from the Atari suite. We highlight the advantage of our approach in one of the hardest games – Montezuma’s revenge – for which the ability to handle sparse rewards is key. Our agent learns several times faster than the current state-of-the-art HRL agent in this game, reaching a similar level of performance.",
    "creator" : "LaTeX with hyperref package"
  }
}