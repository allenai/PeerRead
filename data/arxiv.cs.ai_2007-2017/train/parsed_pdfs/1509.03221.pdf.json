{
  "name" : "1509.03221.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recurrent Neural Network Based Modeling of Gene Regulatory Network Using Bat Algorithm",
    "authors" : [ "Sudip Mandal", "Goutam Saha", "Rajat K. Pal" ],
    "emails" : [ "sudip.mandal007@gmail.com", "dr_goutamsaha@yahoo.com", "pal.rajatk@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "biologist and researchers. Several intelligent techniques and models were already proposed to identify the regulatory\nrelations among genes from the biological database like time series microarray data. Recurrent Neural Network (RNN) is\none of the most popular and simple approach to model the dynamics as well as to infer correct dependencies among genes."
    }, {
      "heading" : "In this paper, Bat Algorithm (BA) was applied to optimize the model parameters of RNN model of Gene Regulatory",
      "text" : "Network (GRN). Initially the proposed method is tested against small artificial network without any noise and the efficiency\nwas observed in term of number of iteration, number of population and BA optimization parameters. The model was also\nvalidated in presence of different level of random noise for the small artificial network and that proved its ability to infer the\ncorrect inferences in presence of noise like real world dataset. In the next phase of this research, BA based RNN is applied\nto real world benchmark time series microarray dataset of E. Coli. The results shown that it can able to identify the\nmaximum true positive regulation but also include some false positive regulations. Therefore, BA is very suitable for\nidentifying biological plausible GRN with the help RNN model.\nKeywords: Gene Regulatory Network; Recurrent Neural Network; Bat Algorithm, Microarray Data.\nI. INTRODUCTION\nMultiple genetic changes which lead to perturbations in specific metabolic pathways that control cell growth are the key reasons for Cancer. In the human body, these alterations often take place owing to mutations in cancer suppressor genes, which in turn leads to uncontrolled cell proliferation, survival and genomic instability. A gene regulatory network (GRN) represents the interactional interrelationships among a group of genes in a cell. The group of genes acts in a synergistic manner to carry out certain functions within the cell. A GRN is usually represented by a graph in which nodes represent genes and regulatory interactions between the genes are represented by directed edges (from the regulator to the regulated gene). The nature of the interaction is of two types, namely activation and repression. Thus, the study of GRNs appears to be very essential in order to discover the genetic causes of a particular disease and for the subsequent design of new and effective methods of treatment to control the disease causing genes with their mutual interactions [1].\nDNA microarrays [2], [3] are widely used now-a-days for the purpose of investigation of the mechanism that is responsible for Cancer. Microarrays contain the expression levels of thousands of genes those represent the cause and effect relationship of the species under investigation. This time series microarray database contains underlying information regarding the behaviour of genes in terms of changes in their expression in response to cancer causing mutations or any form of treatment at different time sample.\nnetworks and dynamics from the time series microarray data i.e. a reverse engineering problem. Boolean networks [4], [5] examine binary state transition matrices to search patterns in gene expression depending on a binary state function. A Dynamic Bayesian network [6], [7] makes conditional probabilistic transitions between network states that merge the features of Hidden Markov model to include the feedback. S-system [8], [9], [10], [11], [12] is also a popular model of Biochemical System Theory, represents a GRN as a set of differential equation with power law function. Neural Network [13], [14] along with GA was also proposed to infer GRN successfully.\nHowever, in this work, we have used Recurrent Neural Network (RNN) [15] which is closed loop Neural Network with a delayed feedback variable suitable to model genetic system dynamics from temporal data. Generally, RNN along with an optimization method is used to infer the GRN where objective function of optimization is the training error. Chiang et al. [16] proposed the hybridization of Genetic Algorithm (GA) and RNN for finding feed-forward regulated genes when some transcription factors were given to construct cancer-related regulatory modules in human cancer microarray data. Xu et al. [17] used Particle Swarm Optimization (PSO) to predict of dynamics and network structure of small artificial network and SOS network of E. Coli., but it sufferd from its accuracy of the model. Palafox et al. [18] have implemented K-means Population Based Incremental Learning (KPBIL) to optimize the parameters of the RNN and the model is tested against small real and artificial network. The results found to be reliable but it also included few unnecessary regulations in GRN. Noman et al. [19] proposed Decoupled Recurrent Neural Network which was trained Differential Evolution (DE) technique and introduced a penalty term or L1 regularizer in the objective function to balance between accuracy and network structure. The result had shown very good accuracy in finding all true regulation and dynamics for both small and large network. The main disadvantage of this process was inclusion of large number of false regulation. Rakshit et al. [20] have proposed weight matrix based Recurrent Fuzzy Neural Model using Invasive Weed and Artificial Bee Colony (ABC) Optimization technique along with a new penalty function. But results were very poor for both small artificial and real GRN. Kentzoglanakis et al. [21] hybridized PSO and Ant Colony Optimization (ACO) for reverse engineering problem of GRN where PSO was used to train the RNN parameters and ACO was introduced to find the biological plausible network structure. It was tested against small artificial, synthetic and real network with very good accuracy. But the process was very time consuming due to parallel implantation of two separate optimization technique.\nMost of these proposed methods, however, yet to accomplish an accurate inference of small scale real life genetic network. However, few of them were able to found all true regulations but they also detected some false regulations. Moreover, No Free Lunch (NFL) theorem [22] logically states that there is no single metaheuristic which is best suited for solving all kind of optimization problems. Therefore, inference of small genetic network using other optimization techniques is still an open problem to the researchers.\nThe Bat Algorithm (BA), which was first proposed by X.-S Yang [23], [24], was based on the use of echolocation of bats during their foraging. To the authors’ best knowledge, Bat Algorithm (BA) was successfully implemented in others field of engineering [25], [26] has yet to be incorporated for parameters optimization of S-System. As BA can be successfully implemented for continuous parameter optimization and multi-objective optimization [27], [28], it may be suitable for parameter optimization of S-system based model of GRN.\nIn this paper, BA is introduced for reconstruction of GRN using decoupled and regularized RNN model. The rest of the paper is organized as follows. The preliminary of RNN and BA are discussed in next section. The details of fitness function of BA for decoupled RNN along with penalty or pruning term and learning process for finding accurate structure of GRN are discussed in Section III. After that, the effectiveness of the proposed BA based RNN model is tested against artificial GRN (with and without presence of noise). Results are also\nvariation BA parameters to create best model for GRN. Next, decoupled RNN model with a known regularizer technique is proposed for reconstruction of real life bench mark SOS network for E. Coli. Conclusion is given in section V followed by references.\nII. THEORITICAL BACKGROUND\nBefore elaborating the methodology of optimizing the RNN for GRN using Bat Algorithm op, let’s revise the\nbasic concepts of RNN and BA."
    }, {
      "heading" : "A. Preliminary of RNN Model for GRN",
      "text" : "As the inputs of a classical Artificial Neural Network are supplied only from the training dataset, NNs are not suitable to model the dynamics of a system. However, RNN model is a closed loop NN with a delay variable between the outputs of each neuron in the output layer of the RNN to each of the neurons in the input layer that is appropriate to model temporal data. In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total \uD835\uDC41 neurons in the output unit \uD835\uDC52\uD835\uDC56(\uD835\uDC61 + ∆\uD835\uDC61) is a gene expression value of next time instant, and the neurons in the input units \uD835\uDC52\uD835\uDC56(\uD835\uDC61) are the gene expression of present state for same genes, thus they interacts with each and everyone.\n\uD835\uDC52\uD835\uDC56 \uD835\uDC61 + ∆\uD835\uDC61 = ∆\uD835\uDC61\n\uD835\uDF0F\uD835\uDC56 \uD835\uDC53 \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57\uD835\uDC52\uD835\uDC57\n\uD835\uDC41 \uD835\uDC57=1 \uD835\uDC61 + \uD835\uDEFD\uD835\uDC56 + 1 −\n∆\uD835\uDC61 \uD835\uDF0F\uD835\uDC56 \uD835\uDC52\uD835\uDC56 \uD835\uDC61 \uD835\uDC64ℎ\uD835\uDC52\uD835\uDC5F\uD835\uDC52 \uD835\uDC56 = 1, 2, . . ,\uD835\uDC41 (1)\nwhere, \uD835\uDC53() is a nonlinear function that acts as a classification function. Usually the sigmoid function is used for it where \uD835\uDC53 \uD835\uDC67 = 1/1 + \uD835\uDC52−\uD835\uDC67 . \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 represents the type and strength of the regulatory interaction from j-th gene towards i-th gene. The positive (negative) value of \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 represents activation (repression) control of gene-j on gene-i. \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 = 0 means gene-j has no regulatory control on gene-i. \uD835\uDEFD\uD835\uDC56 represents the basal expression level and \uD835\uDF0F\uD835\uDC56denotes the decay rate parameter of the i-th gene. ∆\uD835\uDC61 is incremental time instance, in this work it is set always as 1. So, in the discrete form the RNN model for GRN can be described by the following set of N(N+2) unknown parameters Ω = \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 , \uD835\uDEFD\uD835\uDC56 , \uD835\uDF0F\uD835\uDC56 where i, j = 1, 2,∙∙∙,N."
    }, {
      "heading" : "B. Preliminary of Bat Algorithm for training of RNN",
      "text" : "Bat Algorithm (BA), initially proposed by Yang [23], is inspired by echolocation behaviour of bats [30]. Echolocation is one kind of sonar which is used by bats to forage prey and also to avoid obstacles in their path. Bats transmit very loud and high frequency sound continuously and listen for the echo that reflects back from the surrounding objects. Thus a bat can compute direction and distance of the object from the transmitting and receiving wave. Moreover bats can discriminate between a prey and an obstacle easily even in complete darkness [30]. In order to convert these behaviours to Bat Algorithm, some rules are idealized by Yang [24].\n All bats use echolocation to measure distance or direction of objects, and they can also discriminate the\ndifference between food/prey and background obstacles by some magical way [24], [30], [31].\n Bats fly randomly with velocity vi at position xi with a minimum frequency fmin, varying wavelength λ and\nloudness A0 to search for prey. They can automatically adjust the wavelength (or frequency) of their emitted pulses and adjust the rate of pulse emission r ϵ [0, 1], depending on the proximity of their target [24], [30], [31].\n Although the loudness can vary in many ways, it is assumed that the loudness varies from a large\n(positive) A0 to a minimum constant value Amin [24], [30], [31].\nproduced from real-valued vectors with dimension jb and number of bats n, by taking into account lower and upper boundaries.\n\uD835\uDC65\uD835\uDC56\uD835\uDC4F ,\uD835\uDC57\uD835\uDC4F = \uD835\uDC65\uD835\uDC5A\uD835\uDC56\uD835\uDC5B ,\uD835\uDC57\uD835\uDC4F + \uD835\uDC5F\uD835\uDC4E\uD835\uDC5B\uD835\uDC51 0,1 ∗ (\uD835\uDC65\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 ,\uD835\uDC57\uD835\uDC4F − \uD835\uDC65\uD835\uDC5A\uD835\uDC56\uD835\uDC5B ,\uD835\uDC57\uD835\uDC4F ) (2) where ib=1, 2,…n and jb=1, 2,….d. \uD835\uDC65\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 ,\uD835\uDC57\uD835\uDC4F and \uD835\uDC65\uD835\uDC5A\uD835\uDC56\uD835\uDC5B ,\uD835\uDC57\uD835\uDC4F are lower and upper boundaries for dimension jb\nrespectively. rand is a function that generate random value within the limit [0,1].\nii. Update Process of Frequency, Velocity and Position: The frequency factor controls step size of a solution in BA [24], [30]. This factor is assigned to random value for each bat (solution) between lower and upper boundaries [Qmin, Qmax]. Velocity of a solution is proportional to frequency and new solution depends on its new velocity.\n\uD835\uDC44\uD835\uDC56\uD835\uDC4F = \uD835\uDC44\uD835\uDC5A\uD835\uDC56\uD835\uDC5B + \uD835\uDC44\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 − \uD835\uDC44\uD835\uDC5A\uD835\uDC56\uD835\uDC5B \uD835\uDEFD (3) \uD835\uDC63\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61 = \uD835\uDC63\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61−1 + \uD835\uDC65\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61 − \uD835\uDC65\uD835\uDC4F\uD835\uDC52\uD835\uDC60\uD835\uDC61 \uD835\uDC44\uD835\uDC56\uD835\uDC4F (4) \uD835\uDC65\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61 = \uD835\uDC65\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61−1 + \uD835\uDC63\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61 (5) Where nt denotes the iteration number, \uD835\uDC65\uD835\uDC4F\uD835\uDC52\uD835\uDC60\uD835\uDC61 denote the current global best solution so far. \uD835\uDEFD ϵ [0, 1] is random generated number to modify the frequency. For local search part of algorithm (exploitation) one solution is selected among the selected best solutions and random walk [24], [30] is applied.\n\uD835\uDC65\uD835\uDC5B\uD835\uDC52\uD835\uDC64 = \uD835\uDC65\uD835\uDC5C\uD835\uDC59\uD835\uDC51 + \uD835\uDF00 \uD835\uDC34 \uD835\uDC5B\uD835\uDC61 (6) Where \uD835\uDC34\uD835\uDC5B\uD835\uDC61 is average loudness of all bats and \uD835\uDF00 ϵ [0, 1] is random number which represents direction and\nintensity of random-walk [24], [30], [31].\niii. Loudness and Pulse Emission Rate Update Process: Loudness and pulse emission rate must be updated as iterations proceed [24], [30]. As a bat gets closer to its prey then loudness A usually decreases and pulse emission rate also. Loudness A and pulse emission rate r are updated by the following equations\n\uD835\uDC34\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61+1 = \uD835\uDEFC \uD835\uDC34\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61 (7) \uD835\uDC5F\uD835\uDC56\uD835\uDC4F \uD835\uDC5B\uD835\uDC61+1 = \uD835\uDC5F\uD835\uDC56\uD835\uDC4F 0 [1 − \uD835\uDC52−\uD835\uDEFE∗\uD835\uDC5B\uD835\uDC61 ] (8) where α and are loudness reduction and pulse rate increment constants [24], [30]. \uD835\uDC5F\uD835\uDC56\uD835\uDC4F 0 and \uD835\uDC34\uD835\uDC56\uD835\uDC4F 0 are initial\npulse rate and initial loudness which are random values between [0,1].\nIII. METHODOLOGY\nSo, the RNN model represents a set of \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 , \uD835\uDEFD\uD835\uDC56 , \uD835\uDF0F\uD835\uDC56 which are called as RNN model parameter. The inference method using the RNN model of genetic network is done by finding the optimum values of RNN parameter with the help of BA such that training error is minimized."
    }, {
      "heading" : "A. Estimation Criteria and Decoupled Recurrent Neural Network",
      "text" : "All optimization methods use an objective function or a fitness value to measure the goodness of a solution.\nMost commonly estimation criterion is squared error which is defined as follows\n\uD835\uDC53 = (\uD835\uDC52\uD835\uDC50\uD835\uDC4E\uD835\uDC59 ,\uD835\uDC58 ,\uD835\uDC56 ,\uD835\uDC61 − \uD835\uDC4B\uD835\uDC52\uD835\uDC52\uD835\uDC65\uD835\uDC5D ,\uD835\uDC58 ,\uD835\uDC56 ,\uD835\uDC61) 2\uD835\uDC47 \uD835\uDC61=1 \uD835\uDC41 \uD835\uDC56=1 \uD835\uDC40 \uD835\uDC58=1 (9)\nwhere N is the number of genes in the problem, T is the number of sampling instances of the observed gene expression data and M is number of training dataset. \uD835\uDC52\uD835\uDC50\uD835\uDC4E\uD835\uDC59 ,\uD835\uDC58 ,\uD835\uDC56 ,\uD835\uDC61 is numerically calculated gene expression value of k-th dataset at time t of i-th gene using the set of obtained parameter of RNN model. \uD835\uDC52\uD835\uDC52\uD835\uDC65\uD835\uDC5D ,\uD835\uDC58 ,\uD835\uDC56,\uD835\uDC61 is the actual gene expression level of k-th dataset at t-time of i-th gene. The f denotes total squared error between the calculated and the observed gene expression data gene expression. Therefore, RNN modelling is a non-linear function optimization problem to discover the optimal RNN parameter by minimizing the fitness function or mean square error so that calculated gene expression data fits with the observed gene expression data. Since for N genes, N(N+2) parameters must be determined to find solution of set of equations (1), thus the RNN model method search for the best RNN parameters values in N(N+2) dimensional space. This space is too large dimensional in\nnetwork inference problem is divided or decoupled into several sub-problems corresponds to each gene. Now, the objective function of the sub-problem corresponding to i-th gene is to find the values of decoupled RNN model parameters which minimizes error for only i-the gene fi and it is defined as follows\n\uD835\uDC53\uD835\uDC56 = (\uD835\uDC52\uD835\uDC50\uD835\uDC4E\uD835\uDC59 ,\uD835\uDC58 ,\uD835\uDC56,\uD835\uDC61 − \uD835\uDC52\uD835\uDC52\uD835\uDC65\uD835\uDC5D ,\uD835\uDC58 ,\uD835\uDC56 ,\uD835\uDC61) 2\uD835\uDC47 \uD835\uDC61=1 \uD835\uDC40 \uD835\uDC58=1 (10) \uD835\uDC52\uD835\uDC56 \uD835\uDC61 + ∆\uD835\uDC61 = ∆\uD835\uDC61\n\uD835\uDF0F\uD835\uDC56 \uD835\uDC53 \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57\uD835\uDC52\uD835\uDC57\n\uD835\uDC41 \uD835\uDC57=1 \uD835\uDC61 + \uD835\uDEFD\uD835\uDC56 + 1 −\n∆\uD835\uDC61 \uD835\uDF0F\uD835\uDC56 \uD835\uDC52\uD835\uDC56 \uD835\uDC61 (11)\nHence, to solve the differential equation (11), the number of RNN parameters needed to determine is only (N+2) parameters for i-th gene. Thus, this decoupling method divides a N(N+2)-dimensional problem space into (N+2)-dimensional sub problem space for each gene. By accumulating the (N+2) parameters of all N genes, overall structure of RNN can be achieved which in turn denotes the GRN."
    }, {
      "heading" : "B. Regularization as Penalty Term for Real Life Network",
      "text" : "However, real life genetic network are sparsely connected i.e. very few connectivity exist among genes and the measured data are also very noisy. So, RNN based GRN model may have different optimal solutions with very low error value depending on the different connectivity or structures among those genes of the network and the corresponding values of the kinetic parameters. This is known as over-fitting problem. So, to overcome this over-fitting problem for real life genetic network, the balance between the minimization of error and actual regulatory structure of GRN need to be achieved. So, a regularization term is introduced as penalty term along with error function to avoid over-fitting, to find out the original network by restricting regulator size and also to restrict reach space during optimization.\nTo generate the sparse solutions, the concept of in-degree or cardinality [19] of genes in error function was already introduced. Cardinality of gene is defined as the allowed number of regulations over the particular genes. In this paper, a penalty term based maximum cardinality I is added to fitness function for real life network reconstruction where it is assumed that out of N kinetic parameter values of each of the w, only I non-zero values are allowed within each w vectors , thus forcing the other (N-I) values to become zero. If any of these (N-I) elements achieved a non zero-value during optimization process, the solution will be penalized in the following way [19] for decoupled RNN system \uD835\uDC53\uD835\uDC56 = (\uD835\uDC52\uD835\uDC50\uD835\uDC4E\uD835\uDC59 ,\uD835\uDC58 ,\uD835\uDC56,\uD835\uDC61 − \uD835\uDC52\uD835\uDC52\uD835\uDC65\uD835\uDC5D ,\uD835\uDC58 ,\uD835\uDC56 ,\uD835\uDC61) 2 +\uD835\uDC47\uD835\uDC61=1 \uD835\uDC40 \uD835\uDC58=1 \uD835\uDC50 ( \uD835\uDC4A\uD835\uDC56,\uD835\uDC57 ) \uD835\uDC41−\uD835\uDC3C \uD835\uDC57=1 (12) where \uD835\uDC4A\uD835\uDC56,\uD835\uDC57 is the vector which contains the absolute values of \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 but sorted in ascending order. C is the weight constant that denote magnitude of penalty to balance between over fitting and actual network structure."
    }, {
      "heading" : "C. Learning Process",
      "text" : "Learning the optimal values of parameters that best fit with the training data is an optimization problem. So, to optimize the parameters of RNN, BA was introduced by minimize the error or fitness function fi. In Bat Algorithm, velocity and position of a bats is corresponds to a solution (i.e. a set of \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 , \uD835\uDEFD\uD835\uDC56 , \uD835\uDF0F\uD835\uDC56) in search space and it moves towards minimum error area gradually during iteration. Now the pseudo code [23], [24], [30] of the proposed method can be given as\n1. Objective function f(x)= fi (MSE error for Decoupled RNN with the penalty term and value of I) 2. For i=1: N 3. Generate initial population of Bat positions xib and velocity vib with the n set of [\uD835\uDC64\uD835\uDC56\uD835\uDC4F ,\uD835\uDC57\uD835\uDC4F , \uD835\uDEFD\uD835\uDC56\uD835\uDC4F , \uD835\uDF0F\uD835\uDC56\uD835\uDC4F ] (for ib =\n1, 2, ..., n and jb= 1,2,…,d) with proper range where n is the number of initial solutions or bats and d is the dimension of problem.\n4. Initialize pulse frequency Qib , pulse rate rib and loudness Aib at xib. 5. Find out the current best solution.\n7. Generate new solutions by adjusting frequency, and updating velocities and location/solutions. 8. if (rand > rib) 9. Select a solution among the best solutions 10. Generate a local solution around the selected best solution. 11. end if 12. if (rand< Aib and f(xib) < f(x best)) 13. Accept new solutions. 14. Increase rib, reduce Aib 15. end if 16. Ranks the bats and find current best xbest 17. end while 18. end for 19. Display results.\nIV. EXPERIMENTAL RESULTS\nIn this research work, the performance of the inference algorithm was evaluated over the both artificial and real networks and experimental results were compared with the others existing methods. Now, the performance of the modelling of GRN is measured by two processes.\nFirst most important performance criterion is measured from network structure or architecture point of view where inferred network is compared with the original network structure with respect to edge connectivity. Now, the two parameters, Sensitivity (Sn) and Specificity (SP) of the reconstructed network are define as follows [11], [18], [19]\n\uD835\uDC46\uD835\uDC5B = \uD835\uDC47\uD835\uDC43\n\uD835\uDC47\uD835\uDC43+\uD835\uDC39\uD835\uDC41 , \uD835\uDC46\uD835\uDC5D =\n\uD835\uDC47\uD835\uDC41\n\uD835\uDC47\uD835\uDC41+\uD835\uDC39\uD835\uDC43 (13)\nwhere TP (True Positive) denotes the number of correctly predicted regulations, TN (True Negative) represents the number of properly predicted non-regulations, FP (False Positive) denotes the number of incorrectly predicted regulations and FN (False Negative) represents the number of falsely predicted non-regulations by the inference algorithm.\nThe inferred values of the regulatory parameters are also bit of concern as its magnitude can affect the network connectivity. So, we define another performance measurement parameter as Inferred Parametric Error (IPE) which measures the deviation in magnitude of inferred parameters of RNN model from original one.\n\uD835\uDC3C\uD835\uDC43\uD835\uDC38 = \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 \uD835\uDC52\uD835\uDC65\uD835\uDC5D − \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 \uD835\uDC50\uD835\uDC4E\uD835\uDC59 \uD835\uDC41\uD835\uDC56,\uD835\uDC57=1 (14)\nWhere \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 \uD835\uDC52\uD835\uDC65\uD835\uDC5D is the actual value of weight parameter and \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 \uD835\uDC50\uD835\uDC4E\uD835\uDC59 are the calculated value of the same. Using these two types of performance parameter, we can estimate the efficiency of an inferred algorithm. All the experiments were performed using computer with an Intel© Dual Core processor, 2GB of RAM, Windows 7 platform and Matlab R2009b software tools.\nA. Inference for Small Artificial Network with Noise Free Data\nTo taste the effectiveness of the Bat Algorithm based RNN model of GRN, a known small artificial regulatory network was chosen which contain four genes with simple regulatory dynamics. Earlier approaches [17], [19], [20] already used this network to verify their proposed algorithm’s efficiency.\n1) Experimental Setup: First the proposed methodology applied to noiseless data using the parameters which are shown in the Table 1. If an insufficient amount of time-series data is given as training data, due to the high\ntime-series data is used to improve the chances of finding the correct interactions.\nSpecifically, here, 6 sets of noisefree time-series data were used, each consist of all 4 genes. The time-series data were generated by solving the set of differential equations (1) and the initial values of these sets were selected randomly. In real life, these time-series data could be obtained by different biolgical experiments. The number of sampling point was set as 15. For our work, there were total 90 datapoint for each of the genes. For each gene, 6 parameters need to be identified by using BA. The search space was selected as \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 ∈ [-30,30], \uD835\uDEFD\uD835\uDC56 ∈ [-10,10] and \uD835\uDF0F\uD835\uDC56 ∈ [0, 20] same as earlier work [19].\nCardinality I was set as 4 i.e. the maximum number of all types of possible regulations for a particular gene was 4 and value of C was set as 1. For BA, nt, maximum number of iteration was 2000 and initial number of solutions was 200. Value of α and was set as 0.1 and 0.1 respectively which were choosen after testing with some possible values that will be discused later. Boundary of frequency was intialized as [0,1] and step size during random walk was fixed by 0.001. The state of the rand function was reset to get the fixed output for all program execution.\n2) Result: Table II shows the inferred parameters for the first attempt of experiment. If the values of kinetic orders which have absolute value less than 0.05 were ignored, the BA based RNN model gave quite satisfactory results for noiseless data as it gave almost correct values of all parameters. Moreover, BA can also able to infer the correct signs and positions of regulations and non-regulations accurately. In spite of that, it can be concluded that if sufficient number of large time series data was available, BA can able to detect all correct values and directions of regulations very efficiently with negligible variations in numerical values.\nThe main advantages of this BA based RNN model were the reduction in data point, minimum IPR and faster convergence. It was observed that BA was converged within the 500 iteration for 200 bats to reach the stability or minimum error which is around 1 × 10−7 for all genes; while others evolutionary methods needed more number of iterations and dataset. The comparative study among existing methods to infer noise free small RNN based artificial network were shown in table II and III. It can be observed that the proposed RNN-BA method is much superior than RNN-IWO+ABC [20] and RNN-ACO+PSO [21] with respect to selectivity and sensitivity. Moreover from table III, it can be seen that RNN-BA model can reconstruct small artificial network with least number of training data sample point. IPR of RNN-BA model is also very smaller and better than RNN-IWO+ABC [20] and RNN-ACO+PSO [17] that proves the accuracy of the proposed model. However, only RNN-DE [19] process gave better performance than our RNN-BA model in term of IPR. In overall, RNN-BA is very promising method to model small and noiseless artificial network.\nProcess TP FP TN FN \uD835\uDC46\uD835\uDC5B \uD835\uDC46\uD835\uDC5D\nRNN-BA 8 0 8 0 1 1\nRNN-PSO [17] 8 0 8 0 1 1\nRNN-DE [19] 8 0 8 0 1 1\nRNN-IWO+ABC [20] 7 3 5 1 0.88 0.63\nRNN-ACO+PSO [21] 5 1 7 3 0.63 0.88\nB. Inference for Artificial System with Noisy Data\nIn the next phase of the experiment, the proposed method was tested against noisy artificial data to test whether the model was noise immune or not, as in real life data there are lots of measurement errors or noise. Here, the added noise was random in nature and different percentage of randomness was added to observe the performance with the increase of effect of noise in the data. The noise was added in the following way\n\uD835\uDC41\uD835\uDC5C\uD835\uDC56\uD835\uDC60\uD835\uDC66 \uD835\uDC51 = \uD835\uDC51 \uD835\uDC61 ∗ [ 1 − \uD835\uDC5B\uD835\uDC60\n100 + 2 ∗ \uD835\uDC5F\uD835\uDC4E\uD835\uDC5B\uD835\uDC51 ∗\n\uD835\uDC5B\uD835\uDC60\n100 ] (14)\nWhere d(t) is initial noiseless data, ns is percentage of random noise and rand is a function that generate random number between [0,1]. As ns increases, the Noisy(d) loses the originality from the actual data. In this paper, the results for ns = 5%, 10% random noises were observed. The optimization search parameters and the regularization parameters setting were same as in the previous noise-less case.\n1) Results: Table IV shows the inferred parameters value for noisy system of 4 genes artificial network. It was observed that adding 5% noise does not affect the accuracy of the structure; the value of actual nonzero regulatory parameters remained almost same as earlier noise-free case. But, due to noise we got two prominent false positive regulations whose values were greater than 1, else the magnitudes of others zero-valued non regulatory parameters were deviated with acceptable range. For 10% case, the inference was still successful to detect the all actual regulations and parameters which were still in acceptable range, but 3 FPs were added to the network. It is interesting to observe that the number of FPs increased with noise but still number of FN was zero. So, these results proved the robustness against noise of RNN modelling of GRN using BA.\n3 1.17 -8.17 12.21 0.39 -1.07 4.84 4 0.23 -0.30 8.02 -11.41 -0.27 4.77"
    }, {
      "heading" : "C. Model Selection or Parametric Sensitivity",
      "text" : "Next, parametric sensitivity of BA for 4 genes artificial network were observed by varying different\nparameters of it so that suitable model can be selected.\n1) Population: First, we tested the sensitivity of the population size where we observed the variation of Inferred Parametric Error (IPE) or deviation in parameters values with the different iteration number. From following figure, it can be observed that IPE was rather insensitive to iteration number but it decreased with initial population size. It is obvious that if number of population or different initial solutions is increased, the optimization technique will perform better. But it can be also observed that IPE was saturated after some value of population (here 200) which denotes minimum population required for the process. It also signifies that though we increase iteration number, IPE can’t go beyond a boundary level which is the limitation of BA. So, we have chosen minimum 200 populations for all other experiment.\n50 100 150 200 250\n0\n1000\n2000\n3000\n4000\n5000\n6000\nIP E\nNO OF BATS OR POPULATION\n500IT 1000IT 1500IT 2000IT 2500IT 3000IT\nFig. 1. Variation of IPE vs. No. of population\n2) Iteration Range: Now, we observed the change in fitness value with iteration number of the algorithm which is plotted in Fig. 2. It was observed, fitness value was initially decreasing with the iteration number and then saturated after some point. For all cases of different population, after 2000 iteration, the fitness value got saturated. But, when we used 50 populations, it was noticed that fitness value never went close to zero. So, it can be concluded that both number of iteration and initial population should be large enough for correct inference of RNN model.\n500 1000 1500 2000 2500 3000\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nF IT\nN E\nS S\nV A\nLU E\nITERATION\n50BAT 100BAT 150BAT 200BAT 250BAT\nFig. 2. Variation of Fitness value vs. No. of iteration\nstructure of the network. Fig. 3(a) and 3(b) have shown the variation of FP and TN with the increase of time series data for number of iteration 500. When we used a single time series data, the actual network structure could not be obtained where there was presence of extra regulations i.e. false positive. But as the number of time series data was increased there was sharp fall and increase in the number of FP and TN respectively. It was interesting to observed that in all cases of inference, BA always detected all actual regulations of the network i.e. TP is always constant (here it is 8) and consequently FN was always zero. So, sufficient number of time series with different initial sample point is necessary for correct inference of BA.\n4) Value of α and : For BA, α and are two important parameters that make balance between local and global search. Depending on the values of these parameters, convergence rate will be faster or slower. So, the selection of the value α and are very crucial for better performance of BA. Moreover, we observed in our experiment, that if the fitness value is less than equal to1 × 10−7, the BA give very good results in term of RNN model parameters values. So, we conducted our experiment in such way that program will be stopped if the fitness value become equal less than 1 × 10−7 and we repeated this experiment with different value of α and Following table showed the required iteration number with some fixed variation in α and It is found that for α=0.1 and gave best performance. Therefore, we select α = 0.1 and  for all other experiments.\nThe SOS network for E.Coli. [32] was first introduced by Uri Alon group [33] which is a benchmark in GRN problem to find out the effectiveness of the inference algorithm on real time dataset and network. In SOS network, 8 genes were considered (uvrD, lexA, umuD, recA, uvrA, uvrY, ruvA and polB). During their experiments, DNA of E.Coli. was irradiate with the UV light, which affected some gene, after that, the network would repair itself by suppressing others genes expression value (Fig. 4). They performed four experiments for different UV light intensities. Each experiment consists of 50 time steps spaced by 6 minutes for each of the eight genes. Since two of them (uvrY and ruvA) have trivial activity in comparison with the rest of the genes in the network, many researchers chosen rest of 6 genes only. But, in this work, all 8 genes were, considered for better transparency and dataset for experimental condition 1 [35] was chosen only for the validation of the proposed model as others dataset were processed under different experimental condition.\nFor each gene, 10 parameters are needed to be identified by using BA. The search space was selected as \uD835\uDC64\uD835\uDC56 ,\uD835\uDC57 ∈ [-10,10], \uD835\uDEFD\uD835\uDC56 ∈ [-10,10] and \uD835\uDF0F\uD835\uDC56 ∈ [0, 10] same as earlier work [19]. Cardinality I was set as 2 i.e. the maximum number of all types of possible regulations for a particular gene was 2 and value of C was set as 1. For BA, nt, maximum number of iteration was 2000 and initial number of solutions was 500. Value of α and was set as 0.1 and 0.1 respectively. The value of dt was 1. Due to the noise in SOS network for E.Coli. and the random intialization of metaheuristic, different results (i.e. values of RNN parameters) were obtained for each run. So, the program was executed 15 times and stastical analysis was performed for the inference GRN. Initially, individual standerd deviation (\uD835\uDF0E) and mean (\uD835\uDF07) were calculated for individiual weight. Then, probabilty of a regulation is calculated from conventional normal distribution by setting x = overlall mean of weights. From that, we find that our proposed method can found 6 true poistive reugalations among total 9 regulations. But there are lots of unknown and unexpected regulation were present due to side effect of noise. So, this model can also infer real life network.\nRecurrent neural network (RNN) model is a very popular candidate for inferring the GRN from microarray gene expression data in terms of biological plausibility and computational efficiency. In this work, we have implemented the decoupled RNN model where the regulatory parameters of each gene are calculated independently in separate search instances. We incorporate a new metaheuristic, called Bat Algorithm (BA), for inferring the regulators of RNN based GRN along with a regularizer or penalty term in objective function will help to avoid the FPs’ creation, while considering a sparse set of network parameters.\nTo prove the efficiency of this inference algorithm, it was applied to a benchmark problem of artificial network of 4 genes with and without noise. With the use of fewer data points, BA based RNN can infer the network with very high accuracy. But in presence of noise, the number of FPs increased significantly with increment of noise but still it can identify all TPs with good accuracy. It was also found that its noise robustness performance was better than few others existing methods.\nNext, this inference algorithm was applied to infer real life problem of SOS network and this method can detect the 6 correct regulations but it also includes many unnecessary regulation in the network. Different regularization technique may be employed to improve further accuracy and higher speed."
    } ],
    "references" : [ {
      "title" : "Identification of a Gene Expression Signature Common to Distinct Cancer Pathways",
      "author" : [ "N. Fankhauser", "I. Cima1", "P. Wild", "W. Krek" ],
      "venue" : "Cancer Informatics, Vol.11, pp. 139-146, 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Linking microarray data to the literature (2001)",
      "author" : [ "D.R. Masys" ],
      "venue" : "Nature Genetics, Vol. 28, pp. 9-10,2001.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Microarray data normalization and transformation",
      "author" : [ "J. Quackenbush" ],
      "venue" : "Nature Genetics., vol. 32, pp. 496–501, 2002.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Identification of Genetic Networks from a Small Number of Gene Expression Patterns under the Boolean Network Model",
      "author" : [ "T. Akutsu", "S. Miyano", "S. Kuhara" ],
      "venue" : "Pacific Symposium on Biocomputing 4, pp. 17-28, 1999.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Modeling Regulatory Networks with Weight Matrices",
      "author" : [ "D.C. Weaver", "C.T. Workman", "G.D. Stormo" ],
      "venue" : "Pacific Symposium on Biocomputing 4, pp.123, 1999.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Gene networks inference using dynamic Bayesian networks",
      "author" : [ "B.E. Perrin", "L. Ralaivola", "A. Mazurie", "S. Bottani", "J. Mallet", "F. D'Alche-Buc" ],
      "venue" : "Bioinformatics, vol. 19, Suppl 2, pp. II138-II148, 2003.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Comparative evaluation of reverse engineering gene regulatory networks with relevance networks, graphical Gaussian models and Bayesian networks",
      "author" : [ "A.V. Werhli", "M. Grzegorczyk", "D. Husmeier" ],
      "venue" : "Bioinformatics,vol. 22, no. 20, p. 2523, 2006.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Inference of S-system models of genetic networks using a cooperative coevolutionary algorithm",
      "author" : [ "S. Kimura", "K. Ide", "A. Kashihara", "M. Kano", "M. Hatakeyama", "R. Masui", "N. Nakagawa", "S. Yokoyama", "S. Kuramitsu", "A. Konagaya" ],
      "venue" : "Bioinformatics, vol. 21, no. 7, p. 1154, 2005.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Inference of Biological S-System Using the Separable Estimation Method and the Genetic Algorithm",
      "author" : [ "L.Z. Liu", "F.X. Wu", "W.J. Zhang" ],
      "venue" : "IEEE/ACM Transactions on Computational Biology and Bioinformatics, Vol. 9, No. 4, pp. 955-965, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Incorporating Time-Delays in S-System Model for Reverse Engineering Genetic Networks",
      "author" : [ "A.R. Chowdhury", "M. Chetty", "N.X. Vinh" ],
      "venue" : "BMC Bioinformatics, Vol. 14, pp. 1-22, 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reverse Engineering of Gene Regulatory Networks Using Dissipative Particle Swarm Optimization",
      "author" : [ "L. Palafox", "N. Noman", "H. Iba" ],
      "venue" : "IEEE Transactions on Evolutionary Computation, Vol. 17, No. 4, pp. 577-587, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "S-System Based Gene Regulatory Network Reconstruction Using Firefly Algorithm",
      "author" : [ "S. Mandal", "G. Saha", "R.K. Pal" ],
      "venue" : "Third International Conference on Computer, Communication, Control and Information Technology (C3IT-2015), pp.1-5, 2015, doi: 10.1109/C3IT.2015.7060217.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Discovering Gene Networks with a Neural-Genetic Hybrid",
      "author" : [ "E. Keedwell", "A. Narayanan" ],
      "venue" : "IEEE/ACM Transaction in Computational Biology and Bioinformatics, vol. 2, no. 3, pp. 231-242, 2005.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Modeling Genetic Regulatory Dynamics in Neural Development",
      "author" : [ "M.Wahde", "J.Hertz" ],
      "venue" : "Journal Computational Biology, vol. 8(4), pp.429–442, 2001.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A Field Guide to Dynamical Recurrent Networks",
      "author" : [ "J. Kolen", "S. Kremer" ],
      "venue" : "IEEE Press, 2001.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Modeling human cancer-related regulatory modules by GA-RNN hybrid algorithms",
      "author" : [ "J.H. Chiang", "S.Y. Chao" ],
      "venue" : "BMC Bioinformatics, Vol. 8(91), 2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Inference of genetic regulatory networks with recurrent neural network models using particle swarm optimization",
      "author" : [ "R. Xu", "I.I.D. Wunsch", "R. Frank" ],
      "venue" : "IEEE/ACM Trans. Computat. Biol. Bioinform., vol. 4, no.4, pp. 681–692, Oct.–Dec. 2007.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Study on the Use of Evolutionary Technique for Inference in Gene Regulatory Networks",
      "author" : [ "L. Palafox", "N. Noman", "H. Iba" ],
      "venue" : "Proceeding in Information and Communication Technology (PICT 6), pp. 82-92, 2013.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reconstruction of Gene Regulatory Networks from Gene Expression Data Using Decoupled Recurrent Neural Network Model",
      "author" : [ "N. Noman", "L. Palafox", "H. Iba" ],
      "venue" : "Proceeding in Information and Communication Technology (PICT 6), pp. 93-103, 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A Recurrent Fuzzy Neural Model of a Gene Reglatory Network for Knoledge Extraction Using Invasive Wee and Artificial Bee Colony Optimization Algorithm",
      "author" : [ "P. Rakshit", "P Das", "A. Konar", "M. Nasipuri", "R. Janarthanan" ],
      "venue" : "1 International Conference on Recent Advances in Information Technology (RAIT), 2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A Swarm Intelligence Framework for Reconstructing Gene Networks: Searchnig for Biologically Plausible Architecture",
      "author" : [ "K. Kentzoglannakis", "M. Poole" ],
      "venue" : "IEEE/ACM Trans. Computat. Biol. Bioinform., vol. 9, no. 2, pp. 358-372, 2012.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "No free lunch theorems for optimization,” IEEE Transaction on Evolutionary Computing",
      "author" : [ "D.H. Wolpert", "W.G. Macready" ],
      "venue" : "Vol. 1, pp.67–82, 1997.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A New Metaheuristic Bat-Inspired Algorithm",
      "author" : [ "X.-S. Yang" ],
      "venue" : "in: Nature Inspired Cooperative Strategies for Optimization (NISCO 2010) (Eds. J. R. Gonzalez et al.), Studies in Computational Intelligence, Springer Berlin, 284, Springer, 65-74, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Engineering Optimization: An Introduction with Metaheuristic Applications",
      "author" : [ "X.-S. Yang" ],
      "venue" : "Wiley , 2010.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Application of Bat Algorithm and Fuzzy Systems to Model Exergy Changes in a Gas Turbine",
      "author" : [ "A.L. Tamiru", "F.M. Hashim" ],
      "venue" : "Artificial Intelligence, Evolutionary Computing and Metaheuristics Studies in Computational Intelligence, Volume 427, pp 685-719, 2013.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Bat Algorithm for Topology Optimization in Microelectronic Applications",
      "author" : [ "X.-S. Yang", "M. Karamanoglu", "S. Fong" ],
      "venue" : "IEEE conference FGCT-2012,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Bat Algorithm for Multiobjective Optimization",
      "author" : [ "X.S. Yang" ],
      "venue" : "Int. J. Bio-Inspired Computation, Vol. 3, No. 5, pp.267-274, 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Bat algorithm: literature review and applications",
      "author" : [ "X.S. Yang" ],
      "venue" : "Int. J. Bio-Inspired Computation,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Neural network model of gene expression",
      "author" : [ "J.Vohradsk ́y" ],
      "venue" : "The FASEB Journal: official publication of the Federation of American Societies for Experimental Biology, vol. 15(3), pp. 846–854, 2001.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Improved Bat Algorithm (IBA) on Continuous Optimization Problems",
      "author" : [ "S. Yilmaz", "E.U. Kucuksille" ],
      "venue" : "Lecture Notes on Software Engineering Vol. 1, No. 3, pp. 279-283, August 2013.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Modified Bat Algorithm",
      "author" : [ "S. Yılmaz", "E. Ugur Kucuksille", "Y. Cengiz" ],
      "venue" : "Elektronika IR Elektrotechnika, vol. 20, no. 2, pp. 71-78, 2014.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Large-scale mapping and validation of Escherichia coli transcriptional regulation from a compendium of expression profiles.\" PLoS biology",
      "author" : [ "Faith", "J.Jeremiah" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2007
    }, {
      "title" : "Assigning numbers to the arrows: parameterizing a gene regulation network by using accurate expression kinetics",
      "author" : [ "M. Ronen", "R. Rosenberg", "B.I.Shraiman", "U. Alon" ],
      "venue" : "Proceedings of the National Academy of Sciences 99, 10555, 2002.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A General Recurrent Neural Network Approach to Model Genetic Regulatory Networks",
      "author" : [ "Xiao Hu", "A. Maglia", "D.C. Wunsch" ],
      "venue" : "Engineering in Medicine and Biology Society, 2005. IEEE-EMBS 2005. 27th Annual International Conference of the , vol., no., pp.4735-4738, 2006",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Thus, the study of GRNs appears to be very essential in order to discover the genetic causes of a particular disease and for the subsequent design of new and effective methods of treatment to control the disease causing genes with their mutual interactions [1].",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 1,
      "context" : "DNA microarrays [2], [3] are widely used now-a-days for the purpose of investigation of the mechanism that is responsible for Cancer.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "DNA microarrays [2], [3] are widely used now-a-days for the purpose of investigation of the mechanism that is responsible for Cancer.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Boolean networks [4], [5] examine binary state transition matrices to search patterns in gene expression depending on a binary state function.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "Boolean networks [4], [5] examine binary state transition matrices to search patterns in gene expression depending on a binary state function.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "A Dynamic Bayesian network [6], [7] makes conditional probabilistic transitions between network states that merge the features of Hidden Markov model to include the feedback.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "A Dynamic Bayesian network [6], [7] makes conditional probabilistic transitions between network states that merge the features of Hidden Markov model to include the feedback.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "S-system [8], [9], [10], [11], [12] is also a popular model of Biochemical System Theory, represents a GRN as a set of differential equation with power law function.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "S-system [8], [9], [10], [11], [12] is also a popular model of Biochemical System Theory, represents a GRN as a set of differential equation with power law function.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "S-system [8], [9], [10], [11], [12] is also a popular model of Biochemical System Theory, represents a GRN as a set of differential equation with power law function.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "S-system [8], [9], [10], [11], [12] is also a popular model of Biochemical System Theory, represents a GRN as a set of differential equation with power law function.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 11,
      "context" : "S-system [8], [9], [10], [11], [12] is also a popular model of Biochemical System Theory, represents a GRN as a set of differential equation with power law function.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : "Neural Network [13], [14] along with GA was also proposed to infer GRN successfully.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "Neural Network [13], [14] along with GA was also proposed to infer GRN successfully.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "However, in this work, we have used Recurrent Neural Network (RNN) [15] which is closed loop Neural Network with a delayed feedback variable suitable to model genetic system dynamics from temporal data.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "[16] proposed the hybridization of Genetic Algorithm (GA) and RNN for finding feed-forward regulated genes when some transcription factors were given to construct cancer-related regulatory modules in human cancer microarray data.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] used Particle Swarm Optimization (PSO) to predict of dynamics and network structure of small artificial network and SOS network of E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] have implemented K-means Population Based Incremental Learning (KPBIL) to optimize the parameters of the RNN and the model is tested against small real and artificial network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] proposed Decoupled Recurrent Neural Network which was trained Differential Evolution (DE) technique and introduced a penalty term or L1 regularizer in the objective function to balance between accuracy and network structure.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] have proposed weight matrix based Recurrent Fuzzy Neural Model using Invasive Weed and Artificial Bee Colony (ABC) Optimization technique along with a new penalty function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] hybridized PSO and Ant Colony Optimization (ACO) for reverse engineering problem of GRN where PSO was used to train the RNN parameters and ACO was introduced to find the biological plausible network structure.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "Moreover, No Free Lunch (NFL) theorem [22] logically states that there is no single metaheuristic which is best suited for solving all kind of optimization problems.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 22,
      "context" : "-S Yang [23], [24], was based on the use of echolocation of bats during their foraging.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 23,
      "context" : "-S Yang [23], [24], was based on the use of echolocation of bats during their foraging.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 24,
      "context" : "To the authors’ best knowledge, Bat Algorithm (BA) was successfully implemented in others field of engineering [25], [26] has yet to be incorporated for parameters optimization of S-System.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "To the authors’ best knowledge, Bat Algorithm (BA) was successfully implemented in others field of engineering [25], [26] has yet to be incorporated for parameters optimization of S-System.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : "As BA can be successfully implemented for continuous parameter optimization and multi-objective optimization [27], [28], it may be suitable for parameter optimization of S-system based model of GRN.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "As BA can be successfully implemented for continuous parameter optimization and multi-objective optimization [27], [28], it may be suitable for parameter optimization of S-system based model of GRN.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 28,
      "context" : "In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total N neurons in the output unit ei(t + ∆t) is a gene expression value of next time instant, and the neurons in the input units ei(t) are the gene expression of present state for same genes, thus they interacts with each and everyone.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total N neurons in the output unit ei(t + ∆t) is a gene expression value of next time instant, and the neurons in the input units ei(t) are the gene expression of present state for same genes, thus they interacts with each and everyone.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total N neurons in the output unit ei(t + ∆t) is a gene expression value of next time instant, and the neurons in the input units ei(t) are the gene expression of present state for same genes, thus they interacts with each and everyone.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total N neurons in the output unit ei(t + ∆t) is a gene expression value of next time instant, and the neurons in the input units ei(t) are the gene expression of present state for same genes, thus they interacts with each and everyone.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total N neurons in the output unit ei(t + ∆t) is a gene expression value of next time instant, and the neurons in the input units ei(t) are the gene expression of present state for same genes, thus they interacts with each and everyone.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total N neurons in the output unit ei(t + ∆t) is a gene expression value of next time instant, and the neurons in the input units ei(t) are the gene expression of present state for same genes, thus they interacts with each and everyone.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 19,
      "context" : "In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total N neurons in the output unit ei(t + ∆t) is a gene expression value of next time instant, and the neurons in the input units ei(t) are the gene expression of present state for same genes, thus they interacts with each and everyone.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 20,
      "context" : "In canonical RNN model [29], the gene’s regulations are expressed with the following tightly coupled architecture [15], [16], [17], [18], [19], [20], [21] where it is assumed that each of the total N neurons in the output unit ei(t + ∆t) is a gene expression value of next time instant, and the neurons in the input units ei(t) are the gene expression of present state for same genes, thus they interacts with each and everyone.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "Preliminary of Bat Algorithm for training of RNN Bat Algorithm (BA), initially proposed by Yang [23], is inspired by echolocation behaviour of bats [30].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "Preliminary of Bat Algorithm for training of RNN Bat Algorithm (BA), initially proposed by Yang [23], is inspired by echolocation behaviour of bats [30].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 29,
      "context" : "Moreover bats can discriminate between a prey and an obstacle easily even in complete darkness [30].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "In order to convert these behaviours to Bat Algorithm, some rules are idealized by Yang [24].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 23,
      "context" : " All bats use echolocation to measure distance or direction of objects, and they can also discriminate the difference between food/prey and background obstacles by some magical way [24], [30], [31].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 29,
      "context" : " All bats use echolocation to measure distance or direction of objects, and they can also discriminate the difference between food/prey and background obstacles by some magical way [24], [30], [31].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 30,
      "context" : " All bats use echolocation to measure distance or direction of objects, and they can also discriminate the difference between food/prey and background obstacles by some magical way [24], [30], [31].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "They can automatically adjust the wavelength (or frequency) of their emitted pulses and adjust the rate of pulse emission r ε [0, 1], depending on the proximity of their target [24], [30], [31].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "They can automatically adjust the wavelength (or frequency) of their emitted pulses and adjust the rate of pulse emission r ε [0, 1], depending on the proximity of their target [24], [30], [31].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 29,
      "context" : "They can automatically adjust the wavelength (or frequency) of their emitted pulses and adjust the rate of pulse emission r ε [0, 1], depending on the proximity of their target [24], [30], [31].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 30,
      "context" : "They can automatically adjust the wavelength (or frequency) of their emitted pulses and adjust the rate of pulse emission r ε [0, 1], depending on the proximity of their target [24], [30], [31].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 23,
      "context" : " Although the loudness can vary in many ways, it is assumed that the loudness varies from a large (positive) A0 to a minimum constant value Amin [24], [30], [31].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 29,
      "context" : " Although the loudness can vary in many ways, it is assumed that the loudness varies from a large (positive) A0 to a minimum constant value Amin [24], [30], [31].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 30,
      "context" : " Although the loudness can vary in many ways, it is assumed that the loudness varies from a large (positive) A0 to a minimum constant value Amin [24], [30], [31].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 23,
      "context" : "Initialization of parameters or solutions: Initial population [24], [30] of positions of bats are randomly produced from real-valued vectors with dimension jb and number of bats n, by taking into account lower and upper boundaries.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 29,
      "context" : "Initialization of parameters or solutions: Initial population [24], [30] of positions of bats are randomly produced from real-valued vectors with dimension jb and number of bats n, by taking into account lower and upper boundaries.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "rand is a function that generate random value within the limit [0,1].",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "Update Process of Frequency, Velocity and Position: The frequency factor controls step size of a solution in BA [24], [30].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 29,
      "context" : "Update Process of Frequency, Velocity and Position: The frequency factor controls step size of a solution in BA [24], [30].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "β ε [0, 1] is random generated number to modify the frequency.",
      "startOffset" : 4,
      "endOffset" : 10
    }, {
      "referenceID" : 23,
      "context" : "For local search part of algorithm (exploitation) one solution is selected among the selected best solutions and random walk [24], [30] is applied.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 29,
      "context" : "For local search part of algorithm (exploitation) one solution is selected among the selected best solutions and random walk [24], [30] is applied.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "xnew = xold + ε A nt (6) Where A is average loudness of all bats and ε ε [0, 1] is random number which represents direction and intensity of random-walk [24], [30], [31].",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "xnew = xold + ε A nt (6) Where A is average loudness of all bats and ε ε [0, 1] is random number which represents direction and intensity of random-walk [24], [30], [31].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 29,
      "context" : "xnew = xold + ε A nt (6) Where A is average loudness of all bats and ε ε [0, 1] is random number which represents direction and intensity of random-walk [24], [30], [31].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 30,
      "context" : "xnew = xold + ε A nt (6) Where A is average loudness of all bats and ε ε [0, 1] is random number which represents direction and intensity of random-walk [24], [30], [31].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 23,
      "context" : "Loudness and Pulse Emission Rate Update Process: Loudness and pulse emission rate must be updated as iterations proceed [24], [30].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 29,
      "context" : "Loudness and Pulse Emission Rate Update Process: Loudness and pulse emission rate must be updated as iterations proceed [24], [30].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 23,
      "context" : "Loudness A and pulse emission rate r are updated by the following equations Aib nt+1 = α Aib nt (7) rib nt+1 = rib 0 [1 − e ] (8) where α and are loudness reduction and pulse rate increment constants [24], [30].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 29,
      "context" : "Loudness A and pulse emission rate r are updated by the following equations Aib nt+1 = α Aib nt (7) rib nt+1 = rib 0 [1 − e ] (8) where α and are loudness reduction and pulse rate increment constants [24], [30].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 0,
      "context" : "rib 0 and Aib 0 are initial pulse rate and initial loudness which are random values between [0,1].",
      "startOffset" : 92,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "To generate the sparse solutions, the concept of in-degree or cardinality [19] of genes in error function was already introduced.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "If any of these (N-I) elements achieved a non zero-value during optimization process, the solution will be penalized in the following way [19] for decoupled RNN system fi = (ecal ,k ,i,t − eexp ,k ,i ,t) 2 + t=1 M k=1 c ( Wi,j ) N−I j=1 (12) where Wi,j is the vector which contains the absolute values of wi ,j but sorted in ascending order.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "Now the pseudo code [23], [24], [30] of the proposed method can be given as 1.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "Now the pseudo code [23], [24], [30] of the proposed method can be given as 1.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 29,
      "context" : "Now the pseudo code [23], [24], [30] of the proposed method can be given as 1.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "Now, the two parameters, Sensitivity (Sn) and Specificity (SP) of the reconstructed network are define as follows [11], [18], [19]",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "Now, the two parameters, Sensitivity (Sn) and Specificity (SP) of the reconstructed network are define as follows [11], [18], [19]",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "Now, the two parameters, Sensitivity (Sn) and Specificity (SP) of the reconstructed network are define as follows [11], [18], [19]",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "Earlier approaches [17], [19], [20] already used this network to verify their proposed algorithm’s efficiency.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "Earlier approaches [17], [19], [20] already used this network to verify their proposed algorithm’s efficiency.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 19,
      "context" : "Earlier approaches [17], [19], [20] already used this network to verify their proposed algorithm’s efficiency.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : "The search space was selected as wi ,j ∈ [-30,30], βi ∈ [-10,10] and τi ∈ [0, 20] same as earlier work [19].",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "The search space was selected as wi ,j ∈ [-30,30], βi ∈ [-10,10] and τi ∈ [0, 20] same as earlier work [19].",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "The search space was selected as wi ,j ∈ [-30,30], βi ∈ [-10,10] and τi ∈ [0, 20] same as earlier work [19].",
      "startOffset" : 74,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "The search space was selected as wi ,j ∈ [-30,30], βi ∈ [-10,10] and τi ∈ [0, 20] same as earlier work [19].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "Boundary of frequency was intialized as [0,1] and step size during random walk was fixed by 0.",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "It can be observed that the proposed RNN-BA method is much superior than RNN-IWO+ABC [20] and RNN-ACO+PSO [21] with respect to selectivity and sensitivity.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "It can be observed that the proposed RNN-BA method is much superior than RNN-IWO+ABC [20] and RNN-ACO+PSO [21] with respect to selectivity and sensitivity.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : "IPR of RNN-BA model is also very smaller and better than RNN-IWO+ABC [20] and RNN-ACO+PSO [17] that proves the accuracy of the proposed model.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "IPR of RNN-BA model is also very smaller and better than RNN-IWO+ABC [20] and RNN-ACO+PSO [17] that proves the accuracy of the proposed model.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "However, only RNN-DE [19] process gave better performance than our RNN-BA model in term of IPR.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "Process TP FP TN FN Sn Sp RNN-BA 8 0 8 0 1 1 RNN-PSO [17] 8 0 8 0 1 1 RNN-DE [19] 8 0 8 0 1 1 RNN-IWO+ABC [20] 7 3 5 1 0.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "Process TP FP TN FN Sn Sp RNN-BA 8 0 8 0 1 1 RNN-PSO [17] 8 0 8 0 1 1 RNN-DE [19] 8 0 8 0 1 1 RNN-IWO+ABC [20] 7 3 5 1 0.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : "Process TP FP TN FN Sn Sp RNN-BA 8 0 8 0 1 1 RNN-PSO [17] 8 0 8 0 1 1 RNN-DE [19] 8 0 8 0 1 1 RNN-IWO+ABC [20] 7 3 5 1 0.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "63 RNN-ACO+PSO [21] 5 1 7 3 0.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "RNN-PSO [17] 3 50 150 608.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 18,
      "context" : "RNN-DE [19] 10 50 500 ≅ 0",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 19,
      "context" : "RNN-IWO+ABC [20] 4 150 600 45.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 20,
      "context" : "RNN-ACO+PSO [21] 1 300 300 --",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "Where d(t) is initial noiseless data, ns is percentage of random noise and rand is a function that generate random number between [0,1].",
      "startOffset" : 130,
      "endOffset" : 135
    }, {
      "referenceID" : 31,
      "context" : "[32] was first introduced by Uri Alon group [33] which is a benchmark in GRN problem to find out the effectiveness of the inference algorithm on real time dataset and network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[32] was first introduced by Uri Alon group [33] which is a benchmark in GRN problem to find out the effectiveness of the inference algorithm on real time dataset and network.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 33,
      "context" : "[34]",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "The search space was selected as wi ,j ∈ [-10,10], βi ∈ [-10,10] and τi ∈ [0, 10] same as earlier work [19].",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "The search space was selected as wi ,j ∈ [-10,10], βi ∈ [-10,10] and τi ∈ [0, 10] same as earlier work [19].",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "The search space was selected as wi ,j ∈ [-10,10], βi ∈ [-10,10] and τi ∈ [0, 10] same as earlier work [19].",
      "startOffset" : 74,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "The search space was selected as wi ,j ∈ [-10,10], βi ∈ [-10,10] and τi ∈ [0, 10] same as earlier work [19].",
      "startOffset" : 103,
      "endOffset" : 107
    } ],
    "year" : 2015,
    "abstractText" : "Correct inference of genetic regulations inside a cell is one of the greatest challenges in post genomic era for the biologist and researchers. Several intelligent techniques and models were already proposed to identify the regulatory relations among genes from the biological database like time series microarray data. Recurrent Neural Network (RNN) is one of the most popular and simple approach to model the dynamics as well as to infer correct dependencies among genes. In this paper, Bat Algorithm (BA) was applied to optimize the model parameters of RNN model of Gene Regulatory Network (GRN). Initially the proposed method is tested against small artificial network without any noise and the efficiency was observed in term of number of iteration, number of population and BA optimization parameters. The model was also validated in presence of different level of random noise for the small artificial network and that proved its ability to infer the correct inferences in presence of noise like real world dataset. In the next phase of this research, BA based RNN is applied to real world benchmark time series microarray dataset of E. Coli. The results shown that it can able to identify the maximum true positive regulation but also include some false positive regulations. Therefore, BA is very suitable for identifying biological plausible GRN with the help RNN model.",
    "creator" : "Microsoft® Office Word 2007"
  }
}