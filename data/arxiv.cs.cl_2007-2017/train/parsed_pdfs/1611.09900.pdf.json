{
  "name" : "1611.09900.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Context-aware Natural Language Generation with Recurrent Neural Networks",
    "authors" : [ "Jian Tang", "Yifan Yang", "Sam Carton", "Ming Zhang", "Qiaozhu Mei" ],
    "emails" : [ "tangjianpku@gmail.com,", "yang1fan2@gmail.com,", "cs@pku.edu.cn,", "qmei}@umich.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Natural language generation is potentially useful in a variety of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al. 2015a; 2015b; Sordoni et al. 2015), text summarization (Rush, Chopra, and Weston 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and image caption (Xu et al. 2015). Traditional approaches usually generate languages according to some rules or templates designed by humans (Cheyer and Guzzoni 2014), which are specific for some tasks and domains and difficult to generalize to other tasks and domains. Besides, the languages generated according to these approaches are very similar, lacking the large variations of human languages. Therefore, it is a long shot of the community to develop automatic approaches that learn from data and generate languages as diverse as human languages.\nRecently, recurrent neural networks (RNNs) have been proved to very effective in natural language generation (Graves 2013; Sutskever, Martens, and Hinton 2011; Bowman et al. 2015). Comparing to the traditional approaches, RNNs directly model the generation process of\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ntext sequences, and the generating function can be automatically learned from a large amount of text data, providing an end-to-end solution. Though traditional RNNs suffer from the problem of gradient vanishing or exploding, the long-short term memory (LSTM) (Hochreiter and Schmidhuber 1997) unit effectively addresses this problem and is able to capture the long-range dependency in natural languages. RNNs with LSTM have shown very promising results on various data sets with different structures including Wikipedia articles (Graves 2013), linux source codes (Karpathy, Johnson, and Li 2015), scientific papers (Karpathy, Johnson, and Li 2015), NSF abstracts (Karpathy, Johnson, and Li 2015).\nMost of existing work focus on natural language generation only with their textual content while ignoring their contextual information. However, in reality natural languages are usually generated at/with particular contexts, e.g., time, locations, emotions or sentiments. Therefore, in this paper we study context-aware natural language generation. Our goal is to generate not only semantically and syntactically coherent sentences, but also sentences that are reasonable at particular contexts. Indeed, contexts have been proved to be very useful for various natural language processing tasks such as topic extraction (Mei et al. 2006), text classification (Cao et al. 2009) and language modelingmikolov2012context.\nWe proposed two novel approaches for context-aware natural language generation, which map a set of contexts to text sequences. Our first model C2S encodes a set of contexts into a continuous representation and then decode the representation into a text sequence through a recurrent neural network. The C2S model is able to generate semantically and syntactically very coherent sentences. However, one limitation is that when the sequences become very long, the information from the contexts may not be able to propagate to the words in distant positions. An intuitive approach to address this is to build the direct dependency between the contexts and the words in the sequences, allowing the information jump from the contexts to the words. However, not all the words may depend on the contexts, some of which may only depend on their preceding words. To resolve this, a gating mechanism is introduced to control when the information from the contexts are accessed. This is our second model: gated contexts to sequences (gC2S). ar X\niv :1\n61 1.\n09 90\n0v 1\n[ cs\n.C L\n] 2\n9 N\nov 2\n01 6\nWe evaluate our approaches on the user reviews from Amazon and TripAdvisor, where rich contexts are available. Two informative contexts are selected: sentiment rating and product id. Fig. presents two examples of reviews generated by gC2S, which are very difficult to tell from reviews written by real users. We choose the task of fake review detection to evaluate the effectiveness of our approach. Experimental results show that more than 50% of the fake reviews generated by our approach are misclassified as real reviews with human judges and more than 90% are misclassified by the existing state-of-the-art fake review detection algorithm."
    }, {
      "heading" : "Related Work",
      "text" : "The approaches of natural language generation can be roughly classified into two categories: the classical rulebased or template-based approaches, and recent approaches with recurrent neural networks, which automatically learn the natural language generator from the data. Classical approaches usually define some rules or templates (Cheyer and Guzzoni 2014) by humans, which are very brittle and hard to generalize to different tasks and domains. Though there are some recent approaches aiming to learn the template structures from large amounts of corpus (Oh and Rudnicky 2000), the training data is very expensive to obtain and the final generation process still requires additional human handcrafted features.\nComparing to the traditional rule-based approaches, the recurrent neural networks based approaches does not rely on any human handcrafted features and provide an end-toend solution. Our approach is also built on recurrent neural networks. (Graves 2013) studied sequence generation, including text, using recurrent neural networks (RNN) with long-short term memory unit. (Sutskever, Martens, and Hinton 2011) proposed a multiplicative RNN (MRNN) for text prediction and generation, in which different transformation functions between the hidden states are used for different input characters. (Bowman et al. 2015) investigated generating sentences from continuous semantic spaces with a variational auto-encoder, in which RNN is used for both the encoder and the encoder. These work have shown that RNNs are very effective for text generation on various data sets of different structures. However, all these work study natural language generation without contexts.\nThere are some recent work that investigate language modeling with context information. (Mikolov and Zweig 2012) studied language modeling by adding the topical features of preceding words as contexts. (Wang and Cho 2015) exploited the preceding words in larger windows for lan-\nguage modeling. These work focus on the task of language modeling and the preceding words are used as the contexts information while our task focuses on natural language generation and external contexts are studied. There are also some related work of response generation in conversation systems (Sordoni et al. 2015; Wen et al. 2015b), in which the conversation history are treated as contexts. Comparing to their work, our solutions are more general, application for a variety of context while their solution are specifically designed for contexts with specific structures."
    }, {
      "heading" : "Problem Definition",
      "text" : "Natural languages are usually associated with rich context information, e.g., time, location, which provide clues on how the natural languages are generated. In this paper, we study context-aware natural language generation. Given the context clues, we want to generate the corresponding natural languages. We first formally define the contexts as follows:\nDefinition 1 (Contexts.) The contexts of natural languages refer to the situations they are generated. Each context is defined as a high-dimensional vector.\nThe contexts of natural languages can be either discrete or continuous features. For example, the context can be a specific user or location; it can also be a continuous feature vectors generated from other sources. For discrete features, the context is usually represented with one-hot representations. Formally, we formulate our problem as follows:\nDefinition 2 (Context-aware natural language generation) Given a set of contexts C = {~ci}i=1,...,K , in which K is the total number of context types, our goal is to generate a sequence of words w1, w2, . . . , wn that are appropriate at the given contexts\nIn this paper, we take the user reviews as an example, where there exist abundant context information, e.g., user, time, sentiment, product. However, our proposed approaches are also general for other contextual data. Next, we introduce our approach for context-aware natural language generation."
    }, {
      "heading" : "Model",
      "text" : "In this section, we introduce our proposed approaches for generating natural language at particular contexts. We first introduce the recurrent neural networks (RNN), which are very effective models for text generation, and then introduce our proposed approaches, which map a set of contexts to a text sequence."
    }, {
      "heading" : "Recurrent Neural Network",
      "text" : "Recurrent neural network (RNN) models the generative process of sequence data, which summarizes the information into a hidden state ( a continuous representation) and then generate a new sample according to a probability distribution specified by the hidden state. Specifically, the hidden state ht from the sequence x1, x2, . . . , xt is recursively updated as:\nht = f(ht−1, xt), (1)\nwhere f(·, ·) is usually a nonlinear transformation, e.g., ht = tanh(Uht−1 + V xt)) (U, V are transformation matrices). The hidden state ht summarizes the information of the entire sequences x1, x2, . . . , xt, and the probability of generating next words p(xt+1|x≤t) is defined as\np(xt+1|x≤t) = p(xt+1|ht) ∝ exp(OTxt+1ht), (2)\nwhere Oxt+1 is the low-dimensional continuous representation of word xt+1.\nThe overall probability of a sequence ~x = x1, x2, . . . , xT is calculated as follows:\np(~x) = T∏ t=1 p(xt|ht−1), (3)\nTraining RNN can be done through maximizing the joint probability p(x) defined by Eqn. (3) and optimized through Back-propagation. However, training RNN with traditional state transition unit ht = tanh(Uxt + V ht−1) suffers from the problem of gradient vanishing or exploding, which is caused by the product of multiple non-linear transforming functions. (Hochreiter and Schmidhuber 1997) effectively addresses this problem through the long-short term memory (LSTM) unit. The core idea of LSTM is introducing the memory state and multiple gating functions to control the information written to the memory sate, reading from the memory state, and removed (or forgotten) from the memory state. Specifically, the detailed updating equations are listed below:\nzt = tanh(Wzxt +Whht−1 + bz)\nit = σ(Wixt +Whht−1 + bi)\nft = σ(Wfxt +Whht−1 + bf )\nct = ft ct−1 + it zt ot = σ(Woxt +Whht−1 + bo)\nht = ot tanh(ct),\n(4)\nwhere ct is the memory state, zt is the module that transform information from input space xt to the memory space, ht is the information read from the memory state, it, ft, ot are the input, forget, and output gates respectively. it controls the information from input zt to the memory state, ft controls the information in the memory state to be forgotten, ot controls the information read from the memory state. The memory state ct is updated through a linear combination of the input filtered by the input gate and the previous memory state filtered by the forget gate."
    }, {
      "heading" : "C2S: Contexts to Sequences",
      "text" : "RNN effectively the joint probability of natural languages p(~x). As mentioned previously, natural languages are usually generated at particular contexts.Therefore, instead of modeling the probability of observing a text sequence p(~x), we are more interested in the probability of observing x under some contexts C, i.e., the conditional probability p(~x|C). In this part, we introduce two generative models for modeling the conditional probability p(~x|C) based on recurrent neural networks. Encoder. Our framework is built on the encoder-decoder framework (Cho et al. 2014). The essential idea is to encode the contexts information into a continuous semantic representation, and then decode the semantic representation into a text sequence. We first introduce how to encode the contexts of natural languages into a semantic representation. We represent the contexts as a set C = {~ci}i=1,K , where ~ci is a type of context, K is the number of context types. Take the review as an example, each ~ci is a sentiment rating score (ranging from 1 to 5), a product id or a user id. For discrete contexts, each ~ci is a one hot-vector ~ci. The embedding of each context ~ci can be obtained through:\n~~ei = Ei~~ci, (5)\nwhere Ei ∈ Rd×|Ki|, Ki is the number of different context values of type i, and d is the dimension of context embedding. Once the embeddings of different contexts are obtained, they are concatenated into a long vector and followed by a non-linear transformation, formulated as follows:\nhC = tanh(W [~e1, ~e2, . . . , ~e|C|]) + b), (6)\nwhere W ∈ RKd×N , N is the size of hidden state of recurrent neural networks in the decoder. By Eqn. (5) and Eqn. 6, we are able to encoder the contexts into a semantic representation. Next we introduce how to decode it a text sequence.\nDecoder. We introduce two types of decoders. The first one is the vanilla recurrent neural networks with LSTM unit, and the initiate state of the RNN is set as the context embedding hC . We call this approach as C2S, and the whole encoder-decoder framework is presented in 2(b). The C2S have shown very promising results in the experiments. However, one limitation of the approach is that when the sequences become very long, the information from the contexts may not be able to propagate to the distant words. To resolve this, a natural solution would be to directly build the dependency between the contexts hC and each word, i.e., add the skip-connections between the contexts and the words. By doing this, when predicting the next word xt+1, it not only depends on the current hidden state ht, but also depends on the context representation hC . To combine the two sources of information, a simple way would be to take their summation or concatenate them. Here we use the way of taking their summation. However, simply summing the two representations which treats the two sources of information equally may be problematic as some words may depend on the context or others may not. Therefore, it would be a desirable to figure out an approach when the context information are required.\nWe achieve this through the gating mechanism. We introduce a gating function which depends on the current hidden state ht:\nmt = σ(V ht + b), (7)\nwhere V ∈ RN×N , σ(·) is the sigmoid function. The probability of next word p(xt+1|x≤t, C) will be calculated as follows:\np(xt+1|x≤t, C) ∝ exp(OTxt+1(ht +mt hC)), (8)\nwhere is the elementwise product. We call this model gC2S, and the whole framework is presented in 2(c). Generation. Once the models are trained, give a set of contexts, we can generate natural languages based on them. There are usually two types of approaches for natural language generation: beam search (Bahdanau, Cho, and Bengio 2014), which is widely used in neural machine translation, and random sample (Graves 2013). In our experiments, we tried both approaches. We find that the samples generated by the beam search are usually very trivial without much variation. Therefore, we adopt the approach of random sampling. During the sampling process, instead of using the standard softmax function, we also tried different values of temperatures. High temperatures will generate more diverse samples but making more mistakes while small temperatures tend to generate more conservative and confident samples. In the experiments, we empirically set the temperatures as 0.7."
    }, {
      "heading" : "Experiments",
      "text" : "In this section, we evaluate the effectiveness of the approach C2S and gC2S with the user review data. Different tasks are evaluated including language modeling, fake review detection with human judges or existing state-of-the-art fake review detection algorithm, sentiment classification on both the real and fake reviews. We first introduce the data sets to be used."
    }, {
      "heading" : "Data Sets",
      "text" : "We choose the user review data as it contains rich context information, e.g., users, time, sentiments, products. We select the sentiment ratings (ranging from 1 to 5) and product ids as the context information, which we believe are the most important factors that affect the review content. We use data from two websites: Amazon1 and TripAdvisor2 are used. For the Amazon data, we select three most popular domains including book, electronic and movie; for the TripAdvisor data, it is about the hotel domain. We select the most popular 20, 000 words as the vocabulary, and reviews containing unknown words, with length more than 100 words in the Amazon data and more than 300 words in the TripAdvisor data are all removed. The whole data are split into train, validation, test data according to the ratio 18:1:1. The statistics of the final data sets are summarized into Table 1. Training Details. All the models on trained on a single GPU. The batch size is set as 128. The weights are randomly initialized with the uniform distribution (−0.1, 0.1), and the biases are initialized with 0. The initial learning rate is set as 1, and the learning rate is halved if the perplexity of the current epoch on the validation data is not less than the last epoch. The gradient is clipped if the norm is larger than 5. Dropout is used from the input to hidden layer and from the hidden layer to output layer in the recurrent neural networks. Different values of hidden size is tried, and the results show that the larger, the better. Due to the limitation of GPU memory, we use 512 by default. For the number of layers of RNN, one layer is used by default as increasing the number of layers does not yield significantly better samples."
    }, {
      "heading" : "Language Modeling",
      "text" : "We start with the task of language modeling. Table 2 compares the performance of language modeling with the approach RNN, C2S and gC2S. First, both the C2S and gC2S with either the sentiment context, product context or their combination outperform the vanilla RNN without context information. This shows that contextual information are indeed helpful for natural language understanding and prediction. Second, the product context seems to be more informative than the sentiment context for language modeling, no matter with the C2S or gC2S model. This may be that there are many words in the reviews that are relevant to the product information. Comparing the C2S and gC2S model, the gC2S model consistently outperforms the C2S model no matter which contexts are used. As explained previously, this is because in the C2S model, the context information\n1Available at http://jmcauley.ucsd.edu/data/ amazon/links.html\n2Available at http://www.cs.cmu.edu/˜jiweil/ html/hotel-review.html\nmay not be able to affect the words that are far away from the beginning of the sequences while the gC2S effectively addresses this through adding the direct dependency between the contexts and the words in the sequences.\nTo further verify this, we compare the results of C2S and gC2S on different lengths of reviews. Fig. 4 presents the results. We can see that as the lengths of the reviews increase, the gC2S model outperforms the C2S model more significantly, showing its effectiveness for modeling lengthy reviews.\nFig. 3 presents several examples on the test data showing that which words are affected by the contexts. We mark the words with the largest gating values (the average of the gating vector is compared here). We can see that most of the words strongly affected by the contexts are words related to the products or sentiments.\nOverall, we can see that the gC2S model is indeed more effective than C2S. Therefore, in all the following experi-\nments, we only use gC2S."
    }, {
      "heading" : "Fake Review Detection",
      "text" : "To further evaluate the effectiveness of the gC2S model for natural language generation, we choose the task of fake review detection, which aims to classify whether the reviews are written by real users or generated by the gC2S model. Real reviews are treated as positive, and fake reviews are treated as negative. For the evaluation data, we randomly select some products which have at least two real reviews for each rating score in the Amazon data and one review for each rating score in the TripAdvisor data. For each real review, a fake review is generated with gC2S according to its contexts. Table 3 summarizes the number of reviews in each domain.\nHuman Evaluation. We use the Amazon Mechanical Turk to evaluate whether the reviews are fake not. We divide all\nthe data into different batches. Each batch contains twenty reviews about the same product. We show the urls of the products, the sentiment rating and the review content to users to ask the turkers to judge whether the reviews are written by real users or not. To control the quality of the results, some “gotcha” questions are inserted in the middle of the list of reviews. Only the results judged by users who answer the “gotcha” questions correctly are kept. The kappa score is .... We summarize the final results into Table 4.\nWe can see that in all the domains, more than 50% of the fake reviews generated by the gC2S model are misclassified by the Turkers, and around 80% of the real reviews are correctly classified. This shows that the reviews generated by the gC2S model are indeed very natural.\nAs more samples are generated by the RNN, more mistakes are likely to make by the model. Therefore, we want to see how the results of human evaluate change as the lengths of the reviews increase. Fig. 5 presents the human evaluation results w.r.t different lengths of reviews. We can see that for both the lengthy fake and real reviews, fewer percentages are misclassified by human judges. However, we can see that even for the fake reviews with more than 150 words, around 40% of them are still misclassified by the human judges.\nAutomatic Classification. Another way to evaluate the effectiveness of our approach is to see how well existing stateof-the-art fake review detection algorithm performs on our generated fake reviews. We adopt the approach in (Ott et al. 2011), which trains a classifier with 800 real reviews from TripAdvisor and 800 fake reviews written by the Amazon Mechanical Turkers3. Here we use the unigram and bigram features for classification, with which the results are very\n3The training data is available at http://myleott.com/ op_spam/.\nclose to the best results according to (Ott et al. 2011). Table (?) summarizes the results. We can see that more than 90% of the fake reviews generated by the gC2S model are misclassified as the real reviews by the classifier."
    }, {
      "heading" : "Sentiment Classification",
      "text" : "The above results show that given particular contexts, the gC2S model is able to generate very natural reviews that are indistinguishable from real reviews. But how well the generated reviews reflect the context information, e.g., how well the generated reviews express the sentiment polarity? Therefore, in this part we compare the results of sentiment classification on both the fake and real reviews. Two types of sentiment classification are conducted: finer granularity, i.e., sentiments with five different rating scores, and binary classification, in which reviews with 4 and 5 ratings are treated as positive and reviews with 1 and 2 ratings are treated as negative.\nTo conduct the classification, we randomly sample one 100,000 real reviews from the training data for training the sentiment classifier. As for the evaluation data, a fake review is generated for each test data according to its contexts. Table 6 and 7 summarize the results of fine granularity and binary sentiment classification respectively. We can see that the results on the real and fake reviews are very close to each other. On some domains (e.g., Book and Movie), the results on the fake reviews are even better than the real reviews, showing that the reviews generated by the gC2S model accurately reflect the sentiment polarity.\nFinally, we present some examples of fake reviews generated by the gC2S model (Table 8). We can see that the generated reviews are very natural, which are grammatically correct, accurately reflect the sentiment polarity and product information."
    }, {
      "heading" : "Conclusion",
      "text" : "This paper studied context-aware natural language generation. We proposed two approaches, C2S and gC2S, which encode the contexts into semantic representations and then decode the representations into text sequences. The gC2S model significantly outperforms the C2S model as it adds skip-connections between the context representations and the words in the sequences, allowing the information from the contexts to be able to directly affect the generation of words. We evaluated our approaches on the user reviews data. Experimental results show that more than 50% of the fake reviews generated by our approach are misclassified by human judges, and more than 90% of the reviews are misclassified by existing fake review detection algorithm.\nIn the future, we plan to integrate more context information, e.g., the user, the detailed descriptions of the products, the product prices, into our approaches and also evaluate our approaches in other scenarios, e.g., generating the titles of scientific papers based on the author, venue, and time information. It may be also beneficial to improve our model through the attention mechanical (Bahdanau, Cho, and Bengio 2014), i.e., attending to different types of contexts when generating words in different positions."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06349.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Context-aware query classification",
      "author" : [ "H. Cao", "D.H. Hu", "D. Shen", "D. Jiang", "J.-T. Sun", "E. Chen", "Q. Yang" ],
      "venue" : "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, 3–10. ACM.",
      "citeRegEx" : "Cao et al\\.,? 2009",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2009
    }, {
      "title" : "Method and apparatus for building an intelligent automated assistant",
      "author" : [ "A. Cheyer", "D. Guzzoni" ],
      "venue" : "US Patent 8,677,377.",
      "citeRegEx" : "Cheyer and Guzzoni,? 2014",
      "shortCiteRegEx" : "Cheyer and Guzzoni",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850.",
      "citeRegEx" : "Graves,? 2013",
      "shortCiteRegEx" : "Graves",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber",
      "year" : 1997
    }, {
      "title" : "Visualizing and understanding recurrent networks",
      "author" : [ "A. Karpathy", "J. Johnson", "F.-F. Li" ],
      "venue" : "arXiv preprint arXiv:1506.02078.",
      "citeRegEx" : "Karpathy et al\\.,? 2015",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2015
    }, {
      "title" : "A probabilistic approach to spatiotemporal theme pattern mining on weblogs",
      "author" : [ "Q. Mei", "C. Liu", "H. Su", "C. Zhai" ],
      "venue" : "Proceedings of the 15th international conference on World Wide Web, 533–542. ACM.",
      "citeRegEx" : "Mei et al\\.,? 2006",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2006
    }, {
      "title" : "Context dependent recurrent neural network language model",
      "author" : [ "T. Mikolov", "G. Zweig" ],
      "venue" : "SLT, 234–239.",
      "citeRegEx" : "Mikolov and Zweig,? 2012",
      "shortCiteRegEx" : "Mikolov and Zweig",
      "year" : 2012
    }, {
      "title" : "Stochastic language generation for spoken dialogue systems",
      "author" : [ "A.H. Oh", "A.I. Rudnicky" ],
      "venue" : "Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3, 27–32. Association for Computational Linguistics.",
      "citeRegEx" : "Oh and Rudnicky,? 2000",
      "shortCiteRegEx" : "Oh and Rudnicky",
      "year" : 2000
    }, {
      "title" : "Finding deceptive opinion spam by any stretch of the imagination",
      "author" : [ "M. Ott", "Y. Choi", "C. Cardie", "J.T. Hancock" ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, 309–319. Association for Compu-",
      "citeRegEx" : "Ott et al\\.,? 2011",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2011
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "A.M. Rush", "S. Chopra", "J. Weston" ],
      "venue" : "arXiv preprint arXiv:1509.00685.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan" ],
      "venue" : "arXiv preprint arXiv:1506.06714.",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "I. Sutskever", "J. Martens", "G.E. Hinton" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1017–1024.",
      "citeRegEx" : "Sutskever et al\\.,? 2011",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "Larger-context language modelling",
      "author" : [ "T. Wang", "K. Cho" ],
      "venue" : "arXiv preprint arXiv:1511.03729.",
      "citeRegEx" : "Wang and Cho,? 2015",
      "shortCiteRegEx" : "Wang and Cho",
      "year" : 2015
    }, {
      "title" : "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
      "author" : [ "T.-H. Wen", "M. Gasic", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young" ],
      "venue" : "arXiv preprint arXiv:1508.01745.",
      "citeRegEx" : "Wen et al\\.,? 2015b",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Chinese poetry generation with recurrent neural networks",
      "author" : [ "X. Zhang", "M. Lapata" ],
      "venue" : "EMNLP, 670–680.",
      "citeRegEx" : "Zhang and Lapata,? 2014",
      "shortCiteRegEx" : "Zhang and Lapata",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Natural language generation is potentially useful in a variety of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al.",
      "startOffset" : 118,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "Natural language generation is potentially useful in a variety of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al. 2015a; 2015b; Sordoni et al. 2015), text summarization (Rush, Chopra, and Weston 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and image caption (Xu et al.",
      "startOffset" : 173,
      "endOffset" : 219
    }, {
      "referenceID" : 16,
      "context" : "2015), text summarization (Rush, Chopra, and Weston 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and image caption (Xu et al. 2015).",
      "startOffset" : 130,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "Traditional approaches usually generate languages according to some rules or templates designed by humans (Cheyer and Guzzoni 2014), which are specific for some tasks and domains and difficult to generalize to other tasks and domains.",
      "startOffset" : 106,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "Recently, recurrent neural networks (RNNs) have been proved to very effective in natural language generation (Graves 2013; Sutskever, Martens, and Hinton 2011; Bowman et al. 2015).",
      "startOffset" : 109,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "Recently, recurrent neural networks (RNNs) have been proved to very effective in natural language generation (Graves 2013; Sutskever, Martens, and Hinton 2011; Bowman et al. 2015).",
      "startOffset" : 109,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "Though traditional RNNs suffer from the problem of gradient vanishing or exploding, the long-short term memory (LSTM) (Hochreiter and Schmidhuber 1997) unit effectively addresses this problem and is able to capture the long-range dependency in natural languages.",
      "startOffset" : 118,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "RNNs with LSTM have shown very promising results on various data sets with different structures including Wikipedia articles (Graves 2013), linux source codes (Karpathy, Johnson, and Li 2015), scientific papers (Karpathy, Johnson, and Li 2015), NSF abstracts (Karpathy, Johnson, and Li 2015).",
      "startOffset" : 125,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "Indeed, contexts have been proved to be very useful for various natural language processing tasks such as topic extraction (Mei et al. 2006), text classification (Cao et al.",
      "startOffset" : 123,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : "2006), text classification (Cao et al. 2009) and language modelingmikolov2012context.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Classical approaches usually define some rules or templates (Cheyer and Guzzoni 2014) by humans, which are very brittle and hard to generalize to different tasks and domains.",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "Though there are some recent approaches aiming to learn the template structures from large amounts of corpus (Oh and Rudnicky 2000), the training data is very expensive to obtain and the final generation process still requires additional human handcrafted features.",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "(Graves 2013) studied sequence generation, including text, using recurrent neural networks (RNN) with long-short term memory unit.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "(Bowman et al. 2015) investigated generating sentences from continuous semantic spaces with a variational auto-encoder, in which RNN is used for both the encoder and the encoder.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "(Mikolov and Zweig 2012) studied language modeling by adding the topical features of preceding words as contexts.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "(Wang and Cho 2015) exploited the preceding words in larger windows for language modeling.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "There are also some related work of response generation in conversation systems (Sordoni et al. 2015; Wen et al. 2015b), in which the conversation history are treated as contexts.",
      "startOffset" : 80,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "There are also some related work of response generation in conversation systems (Sordoni et al. 2015; Wen et al. 2015b), in which the conversation history are treated as contexts.",
      "startOffset" : 80,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "(Hochreiter and Schmidhuber 1997) effectively addresses this problem through the long-short term memory (LSTM) unit.",
      "startOffset" : 0,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "There are usually two types of approaches for natural language generation: beam search (Bahdanau, Cho, and Bengio 2014), which is widely used in neural machine translation, and random sample (Graves 2013).",
      "startOffset" : 191,
      "endOffset" : 204
    }, {
      "referenceID" : 10,
      "context" : "Table 5: Results of fake review detection in TripAdvisor with the approach in (Ott et al. 2011).",
      "startOffset" : 78,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "We adopt the approach in (Ott et al. 2011), which trains a classifier with 800 real reviews from TripAdvisor and 800 fake reviews written by the Amazon Mechanical Turkers3.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "close to the best results according to (Ott et al. 2011).",
      "startOffset" : 39,
      "endOffset" : 56
    } ],
    "year" : 2016,
    "abstractText" : "This paper studied generating natural languages at particular contexts or situations. We proposed two novel approaches which encode the contexts into a continuous semantic representation and then decode the semantic representation into text sequences with recurrent neural networks. During decoding, the context information are attended through a gating mechanism, addressing the problem of long-range dependency caused by lengthy sequences. We evaluate the effectiveness of the proposed approaches on user review data, in which rich contexts are available and two informative contexts, sentiments and products, are selected for evaluation. Experiments show that the fake reviews generated by our approaches are very natural. Results of fake review detection with human judges show that more than 50% of the fake reviews are misclassified as the real reviews, and more than 90% are misclassified by existing state-of-the-art fake review detection algorithm.",
    "creator" : "TeX"
  }
}