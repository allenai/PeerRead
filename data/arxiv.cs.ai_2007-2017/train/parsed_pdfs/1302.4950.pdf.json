{
  "name" : "1302.4950.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Moises Goldszmidt" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nOrder-of-magnitude probabilities (OMPs) and their formal relatives, E-semantics and kappa calculus, have had considerable impact in knowledge representa tion in the specific areas on nonmonotonic reason ing, belief revision, and the representation of uncer tainty [12, 15, 9, 18, 25]. The reasons for this suc cess are various: They allow the representation of be lief and uncertain knowledge as a set of if-then rules, they provide a well known mechanism for belief update which is linked to notions of conditioning, and finally the provide a link to probability theory [16], condi tional logics [3], and other calculi for uncertainty such as possibility theory [1]. Researchers in uncertainty in AI had hoped that this abstraction of numerical probabilities would also open the doors for faster algo rithms for belief update, in addition to better knowl edge representation methods. Although Goldszmidt and Pearl [16] introduced a semi-tractable algorithm\nfor OMPs, the algorithm did not run on distributions represented by belief networks, a favorite representa tion among Bayesian practitioners.\nIn this paper we introduce a polynomial algorithm for prediction tasks in networks quantified with order of-magnitude approximation probabilities. The algo rithm, which we call Predict has essentially the same complexity as the polytree algorithm [23], with the additional advantage that Predict is not bothered by undirected cycles in the network. Predict exhibits a conservative behavior in the sense that it is sound, but not complete. Soundness means that when the algorithm yields a believed value for a variable, a ma nipulation of the OMP according to their properties will yield the same result. Incompleteness means that sometimes the algorithm will fail to recognize a be lieved value for a particular variable. In this paper we characterize instances where Predict is complete, present a polynomial algorithm for testing sufficient conditions for completeness, and introduce a stratified procedure for computing a complete set of believed values for each variable.\nIn addition we study the role of Predict as a poly nomial estimator of probabilistic values, for deciding whether P(x) > f for a given real valued f, and il lustrate the use of Predict for speeding up two known algorithms for probabilistic belief update: bounded conditioning proposed by Horvitz, Suermondt, and Cooper [19], and the search-based algo�ithm proposed by Poole [24]. We also present expenmen�al results and describe an application to plan evaluatwn.\nThis paper is organized as follows. Section 2 reviews the main concepts behind OMPs and network repre sentations. Section 3 introduces Predict and its main properties, including theorems about soundness and completeness, and a stratified procedure for complete ness. Section 4 discusses Predict in the role of per forming approximate probabilisti� inference, show� its use in probabilistic update algonthms, and descnbes an application to plan evaluation including experimen tal results. Section 5 discusses Predict in the context of providing an algorithm for inference and represen tation of conditional logics of belief in networks, a new\nFast Belief Update Using Order-of-Magnitude Probabilities 209\nnotion of irrelevance, and finally summarizes the main results and future challenges.\n2 OMPs, Rankings, and Networks of Belief\nTo see the relation between K-ranking functions [26], €-semantics [12, 23), and probabilities, imagine an or dinary probability function p defined over a set n of possible worlds (or states of the world), and let the probability P(w) assigned to each world w be a poly nomial function of some small positive parameter f, for example, o:, {3t, /f2, • . . , and so on. Accordingly, the probabilities assigned to any proposition x, as well as all conditional probabilities P(x jy), will be ratio nal functions of L Now define the function K(xjy) as the lowest n such that limf_0 P(xjy)/tn is nonzero. In other words, K(x iy) = n is of the same order of magnitude as P(xjy).\nIt is easy to verify that when f is infinitesimally close to zero, the following properties of K can be derived from the analogous properties of P:\n1. K(x) = mini {K(wi)jw; f= x} 2. K(x) = 0 or K(.., x) = 0, or both\n3. K(x V y) min{K(x), K(y)}\n4. K(X 1\\ y) = K(X jy) + K(y)\nNote that these properties reflect the usual properties of probabilistic combinations (on a logarithmic scale) with min replacing addition, and addition replacing multiplication.\nIf we think of n for which P(w) = fn as measuring the degree to which the world w is disbelieved, then K(xjy) can be thought of as the degree of disbelief (or surprise) in x given that y is true. In particular, K(x) = 0 means that x is a serious possibility or plausible and K( -,x) > 0 translates to \"x is believed.\" 1 Note that in this case, P( -,x) will approach zero in the limit as f approaches zero and consequently P(x) will approach one. The algorithm we propose in this paper allows for an efficient computation of the plausible values of a variable x.\n2.1 Network Representations\nOne of the most efficient ways to representing and rea soning with a probability distribution is through the use of Bayesian networks. A network consist of a di rected acyclic graph r and a quantification Q. The nodes in r represent the variables of interest in the domain, and the edges represent direct (causal) influ ences. In this paper we will use lowercase letters of the alphabet, such as x, to represent both nodes and\n1 We are assuming that the variable x is proposi tional. The generalization to multiple valued variables is straightforward.\nvariables, and x will denote an instantiation of x (i.e., the value of X = x).2 If X is a node in f, then 1r(x) will denote the set of parents of x in r. i( x) will de note an instantiation of the parents of x, and when use inside P(xj1r(x)) or K(xj1r(x�), 1r(x) denotes the con junction of the parents of x. The quantification Q of r specifies the strength of the influence of the parents of x, 1r(x), on x. In probabilistic reasoning, this quan tification is done in terms of conditional probabilities P(xj1r(x)). Furthermore, the structure of the network encodes a set of conditional independence assumptions that translate into the following equation:\nIT P(xi j7r(x;)) l�i�n\n(1)\nwhere X - 1, ... , Xn are the nodes in r. The polytree algorithm [23) takes advantage of these independen cies to update a probability distribution in polynomial time when the underlying graph is a polytree, where r is a polytree iff r does not contain undirected cycles. If r contains undirected cycles the problem of updat ing the probability distribution in a Bayesian network is NP-hard [4) and all (known) exact algorithms are exponential.\nA network can be also quantified with K-rankings by providing local conditional rankings K(x j1r(x)), repre senting the degree of disbelief (or an order of magni tude probability) on the state of x given the values taken by its parents 7r(x) in the network r (15, 20). We can then draw a parallel to Eq. 1 in terms of kappa rankings K(x1 , ... , Xn) = Ll<i<n K(xi j7r(x;)). Furthermore, it was shown by Hunter [20) that the polytree algorithm is also valid in a network quanti fied with rankings. Thus, updating the ranking of a variable X can be done quite efficiently if f is a poly tree, yet the computation becomes exponential once more, if r contains undirected cycles. However, as the next section will show, we can compute the plausible values of each node Xi, that is those values Xi such that K(xi) = 0, quite efficiently.\n3 Predicting Belief Change\nThe algorithm, called Predict, is shown in Figure 1. It takes as input a network of belief, and computes the plausible values for each node Xi. We call this set of values the plausible set of x;. Both plausibility and the plausible set of a variable are defined next.\nDefinition 1 (Plausibility and Belief.} Given a ranking K, we say that x is plausible given y, de noted Pl(xjy), iff K(x jy) = 0. Similarly, we say that x is believed given y, denoted Bel(xjy), iff K(x'jy) > 0\n2Variables don't have to be propositional, yet, if x is a propositional variable -,x denotes the negation of x.\n3We will keep the usual notation in probabilities P(x, y) (�>(x, y)) to denote P(x 1\\ y) (�>(x 1\\ y)).\nfor every value x' different than x.4 The plausible set of x, written PlSet(xi) is defined as the set of val ues {x} such that Pl(x).\nThe algorithm traverses the nodes in the network fol lowing any topological sort of these nodes, and com putes PlSet(xi) based on the plausibility of x/s parents (see Eq. 2). Thus, the asymptotic complexity of the algorithm is determined by the number of nodes in the network and the number of edges (Step 2.2). Let E denote the number of edges in the network, N the number of nodes, and let L U represent a look-up op eration local to each family in the network.5 It is easy to verify that the complexity of Predict is given by the following theorem:\nTheorem 1 The complexity of Predict is O(E + N x LU).\nNote that this is the same complexity of the polytree algorithm [23], except that Predict also applies to net works containing undirected cycles. As described in Figure 1, Predict does not admit any evidence. A min imal change would allow the incorporation of evidence on roots, and the representation of actions and deci sions for evaluating plans. For the first case we sim ply take into account the evidence when computing Step 2.1 in Figure 1. For incorporating changes due to actions and decisions, we simply modify the graph and set the new roots to be the direct children (in the network) of the actions or decisions (8J.\nWe use well defined notions of soundness and com pleteness to characterize the output of Predict in terms of �e-rankings. The interplay with probabilities is char acterized in Section 4.\n4ln relation to infinitesimal probabilities, Bel(x jy) im plies that P(x ly) approaches one as t:: approaches zero.\n5Reca.ll that for each family in the network we must have the conditional ranking x:(x; jr(x;)).\nDefinition 2 (Soundness) Given a ranking �e rep resented by an OMP network. We say that a run of Predict is sound iff the fact that x is not in PlSet(x) implies that �e(x) > 0.\nTheorem 2 Predict is sound for any OMP network.\nThe notion of completeness establishes the opposite: If �e(xi) > 0, then we would like for x· fl. PlSet(xi)· In other words, if the ranking representeJ by the network establishes that a certain value of a variable is believed, then running Predict on this network should yield a plausible set for this variable containing a unique ele ment corresponding to this believed value.\nDefinition 3 (Completeness.) Given a network of belief we say that a run of Predict is complete iff the fact that K(x) > 0 implies that x is not in PlSet(x).\nUnfortunately, Predict is not complete in general. What incompleteness means is that sometimes Predict will regard more than one value of a node as plausible, even when there is enough information in the quan tification of the network to determine which of these values are not plausible. The source of the incomplete ness is Eq. 2 in Figure 1. This equation approximates the value of �e(xi) by assuming that x,'s parents are irrelevant to each other (see Def. 4). There are how ever, important classes of networks for which Predict computes a complete set of plausible values for each node. These cases, described in the next section, de pend on both the structure of the network, and its quantification.\n3.1 Conditions for Completeness\nThe computations of Predict will be complete precisely whenever the computations in Eq. 2 will yield the same set of plausible values than equation K(xjji(x,)) + �e(?r(x,)) for each node Xi· In this section we provide a set of sufficient conditions for completeness by focus ing on conditions for �e(?r(xi)) = Er;Sp$3 �e(xp) = 0 (where {xr, ... , x,} is the parent set of x,). We also present a polynomial O(E) algorithm for checking these conditions. A stratified method for refining an initial run of Predict until completeness is achieved is described in Section 3.2. Consider the following defi nition:\nD�finition 4 (Irrelevance.) Given a ranking \"'• a set of variables {xt. ... , x,..} are irrelevant to each other iff whenever �e(xl) = · · · = K(xn) = 0 then ��:(x1, • . • ,xn) = 0 {for all the instantiations of {xt, ... , x,. }).\nFast Belief Update Using Order-of-Magnitude Probabilities 211\nThis definition is by all means preliminary,6 but it serves its purpose for this paper, namely to provide a minimal language for establishing conditions for the completeness of Predict, and for describing a stratified algorithm for completeness in Section 3.2.\nAs Theorem 5 below points out, the assumption of ir relevance in Eq. 2 is true only under certain conditions. We need the definition of a backpath.\nDefinition 5 (Backpaths) Let x and y be two nodes in a network r. Let B P be an undirected path between x and y such that all nodes in BP are ancestors of either x or y. We call BP a backpath. We say that the backpath is blocked, iff there exists at least one node b in BP such that Bel(b) for some value b of b .\nNote that the definition of a blocked backpath not only depends on the structural configuration of the network, but also on the particular quantification of the network. As we will see, completeness will also de pend on the particular quantification (and sometimes on the evidence) in the network. The reason is that the quantification (and the evidence) will induce belief changes that can in turn produce blocked paths.\nTheorem 3 Let K- be a ranking represented by a net work with structure r, and let x and y be two nodes in the network such that all backpaths between x and y are blocked, then x and y are irrelevant in the sense of Def. 4-\nThus, if the structure of the network does not contain cycles, or if the quantification (or evidence) \"block\" all backpaths then the computation in Eq. 2 is always valid and Predict is complete.\nTheorem 4 If the network does not contain undi rected cycles, then Predict is complete.\nTheorem 5 If a run of Predict returns a set of be lieved nodes blocking all backpaths then the run of Pre dict is complete.\nThese results point to a procedure for checking whether a particular run of Predict is complete. This procedure takes as input the set of believed nodes. It removes their outgoing edges and then runs a breadth first search algorithm to construct a spanning forest. If no cross-edge is detected (i.e., there are no cycles) the run of Predict is complete. If a cycle is reported, then the run may be incomplete, since there are unblocked paths. Note however that unblocked paths do not im ply incompleteness. The complexity of this procedure is O(E).\n61t can be extended in various ways including condi tional irrelevance. In Section 5 we briefly discuss the poten tial of this definition in terms of plausibility and belief and the main difference with a definition of independence based on either probabilistic or OMP notions (see also [13]).\nFinally, there is a special quantification of the network that will guarantee completeness. This quantification is equivalent to the one studied by Poole [24].\nDefinition 6 Let r be a network and let Q be its quantification representing K--ranking. We say that Q is definite iff for every x; E r and every instantiation i(x;), there exists a unique instantiation x; such that K-{x;li(x;)) = 0.\nCorollary 1 If Q is definite then a run of Predict is complete.\n3.2 Stratified Completeness\nThe method that we propose in this section, called Scomplete, is based on \"artificially blocking\" the set of backpaths in a given network by assuming that the required nodes are believed. In essence the method is analogous to the algorithm of cutset-conditioning for updating probabilities in probabilistic Bayesian net works [23], except that it takes advantage of the sound ness properties of Predict to proceed in stages. The procedure uses the structure of the network to find a set of nodes CS = { c1, . . . , em} such that CS blocks all backpaths in the network. Then, for any given node x, x is in PlSet(x) (i.e., K-{x) = 0) iff there exists an instantiation of the nodes in CS such that K-(cl, . . . , en) = 0 and K-{xlcl, . . . , en) = 0. This is so since K-(x) =mines K-{x lcl, ... , en)+ K-{ c1 , ... , en).\nIn order to proceed in stages Scomplete starts with an initial set of nodes { c1 , . . . , Cn}, that blocks a subset of. all backpaths. Then, for each instance of these nodes, Scomplete calls Predict to compute a new PlSet(xi). In the next stage Scomplete increases the set of ini tial blocking nodes by adding {cn+l, . . . , em} , com putes again a new PlSet(xi), and so on. Note that at each stage the result of this computation is always sound and is at least as complete as the previous stage. Eventually Scomplete finds a set of nodes blocking of all backpaths in the graph and the result of the com putation will be a complete set of plausible values for each node in the network.\nScomplete is described in Figure 2. Steps 2 and 3 are essentially the procedure suggested in [27] for isolat ing loops in a network. If r' is empty in Step 3, then there are no loops, and the computation of PlSet(xi) is complete. Otherwise, all the nodes that remain in the modified graph f' are part of loops. 7 The new root nodes of the modified graph constitute a good set of blocking nodes for two reasons. First they are readily identifiable; second, they are irrelevant to each other given the set BSet (which contains believed nodes, as well as blocking nodes used in previous stages). Thus,\n7Moreover only those nodes that have more than one incoming arc and their descendants are source of incom pleteness. The reason is that only those nodes have par ents that were assumed to be irrelevant to one another and were not.\n21.2 Goldszmidt\nPROCEDURE Scomplete Input: A network r, and a set of believed nodes BSet. Output: A complete PlSet(x;) for each Xi in r.\n1. Let CS be an empty set of nodes, and let PlSet(x;) be empty for every node in r.\n2. Let f' be a copy of ; modify f' by removing nodes in BSet and any outgoing arcs from nodes in BSet\n3. While no longer removal is possible, find nodes in f' with single neighbor (only one incoming or outgoing arc); remove the nodes and arcs.\n4. If f' is empty STOP; else let 'R.. be the set of root nodes in r' and let cs = cs u 'R..\n5. For each instantiation of CS = { c1, . .. , en} such that ��:(cl,···•cn) = 0 compute PlSet'(x;) using Predict; let PlSet(x;) = PlSet(x;) U PlSet'(x;).\n6. Let BSet' be the set of nodes such that PlSet(x;) con tains one element; then let BSet = BSet U BSet' U CS; goto 2.\nEND Scomplete\nFigure 2: Computing a complete set of plausibility val ues for each node Xi in an OMP network.\nlet X be any instantiation of BSet, and let { r1, ... , rm} be the root nodes in f'. ��:(rl, ... , rm \\X) = 0 when ever ��:(ri\\X) = 0 for every r;, 1 � i � m. Step 4 simply augments the set of blocking nodes, and Step 5 invokes Predict in order to compute the PlSet(xi) of each node. There is some small amount of bookkeeping involved as Predict must be invoked for each instantiation of the blocking nodes, and the set PlSet(xi) must be assembled from the plausible sets in each iteration. Step 6 simply augments BSet for the next loop-isolation iteration in Steps 2 and 3.\n4 Approximate Probabilistic Inference\nIt is well known that, in general, probabilistic infer ence in Bayesian networks is NP-hard [4], and as our networks scale to meet the requirements of new and more challenging applications this inference will be come intractable. This fact emphasizes the impor tance of having a combination of anytime capabili ties [2] and approximation techniques. In this section we explore the use of Predict in two anytime proba bilistic algorithms: bounded-conditioning proposed by Horvitz, Suermondt, and Cooper [19], and the search based algorithm proposed by Poole [24]. Both these algorithms can improve their efficiency by using a fast procedure capable of estimating which events in the sampling space will have significant probabilities, and which events can be ignored in the computation. The idea is to take advantage of the connection between OMPs and probabilities through the parameter f, and use Predict to estimate, in polynomial time, whether P(x) >f.\nAs mentioned in Section 2, the properties of OMPs and\nthe properties of probability distributions are closely related when f is infinitesimally removed from zero. Yet, when used in conjunction with probabilistic al gorithms to estimate whether P( x) is above a given threshold represented by f, this f must take a real value. We introduce a formal definition of an f-OMP, one that results from the transformation of a numeri cal probability to a OMP based on a given f, where f is a real value. We then study the consequences of com puting with an f-OMP as iff where an infinitesimal quantity.\nDefinition 7 (f-OMP.) Let P be a probability dis tribution represented by a network r with nodes { x1, ... , Xn}, and let f be a real quantity bigger than zero. The f-OMP of P is defined as the OMP ��:(xn, ... , xt) = l::t<i<n ��:(x;\\7r(x;)),8 that results from the following transformation: ��:(x;\\7r(x;)) = K, where K is an integer such that fK +1 < P(x; \\1r(x;)) �\nfK.\nUsing Def. 7 we can, given a real valued f, abstract a probability distribution P to a particular f-OMP and use Predict to compute the plausible set of values for each variable.\nWhen f is an infinitesimal quantity, we can interpret the output of Predict as establishing that if x is in PlSet(x), then P(x) > f. Yet, when f is a real quan tity, and as f is bigger and farther removed from zero, we should expect an error with regards to the con nection to probabilistic values. This error appears be cause infinitesimal quantities do not accumulate but real quantities do. Thus, lower f-terms (i.e., those with an exponent bigger than K) may be significant for computations in the probabilistic domain, even though they are ignored in the OMP domain. As an exam ple consider a network f of nodes {x1, ... 1 Xn} in the form of a \"chain\" where Xn is the only root node, and the only parent of node x; is node Xi-1 for 2 � i � n. Consider a quantification where P(xt) = 1 - f, P(x;\\x;_I) = 1-f, and P(x;\\..,x;-1) = f, for 2 � i � n, for some real valued f. Then it is easy to verify that P(xn) > (1 -ft > 1 - nf. Using f, the f-OMP transformation of this distribution yields ��:(--,x1) = 1, ��:(x1) = 0, and ��:(x;\\x;_t) = ��:(--,x;\\..,x;-1) = 0, ��:(--,x;\\x;_t) = ��:(x;\\..,x;-1) = 1 for 2 � i � n, and ��:(xn) = 0. If the value off is such that f > 1 - m, ��:(xn) will still be equal to zero (namely plausible), yet P(xn) :f f. Note that this situation can also appear in a \"bushy\" network where node y is a functional \"AND\" of its parents { x1, ... , Xn} with P(x;) = 1-f for 1 � i � n, since P(y) > 1 - nf, given that\nP(y)= L P(y\\xl, . .. , xn) II P(x;).9 X1, ... 1Xn.\nThus, both the length of the largest path in a given network and the maximum number of parents in a\n8Such that for at least one instantiation of the nodes in r, ��:(xl, ... ,xn) = 0.\n9Thanks to K. Fertig and D. Koller for this example.\nFast Belief Update Using Order-of-Magnitude Probabilities 213\ngiven family influence the error between probabilities and OMPs. Further research is needed in order to pro vide a precise characterization of this error in terms of the structural properties of the network.\nSections 4.1 and 4.2 describe how to use predict to ap proximate probabilistic values with both the bounded conditioning algorithm and Poole's search-based al gorithm. Results and an application to plan evalua tion [14] are provided in Section 4.3.\n4.1 Predict and Bounded Conditioning\nThe method proposed by Horvitz, Suermondt and Cooper [19] is based on the cutset conditioning algo rithm for evaluating probabilistic Bayesian networks with cycles [23]. In cutset conditioning, dependency loops in a Bayesian network are \"broken\" by a set of nodes { c1, . . . , Cn} called cutset CS. An instantiation c1, ... , en of CS is called an instance of the cutset. The probability of a given node x is determined by the following equation\n(3)\nThe value of P( x Jc1, ... , en) for each instance is nor mally computed using a very efficient algorithm such as the polytree algorithm [23]. The term P( c1, ... , en) can also be computed using the polytree algorithm (see [27] for details). Note that the complexity of the computation is determined by the number of instances of the cutset, times the complexity of the polytree al gorithm. The number of instances of the cutset is de termined by product of the number of values of each node c; in CS. Clearly this number grows exponen tially as we add nodes to CS.\nThe idea behind bounded-conditioning is that in or der to establish whether the probability of interest is above certain value it may not be necessary to com plete the computation of Eq. 3. Each instance of CS can be considered as a separate subproblem, and the computation can be done subproblem by subproblem and then stopped as soon as the desired threshold is met. Let us use w; to denote an instance c1, ... , en of the cutset, where 1 � i � k, and let j denote the number of instances of Eq. 3 computed so far. Then the value of P( x) is bounded as shown in the follow ing equation [19]: L:l<i<j P(xiw;)P(w;) � P(x) � L:19\n�j P(xiw;)P(w;) +2:i+l�i�k P(w;). Note that as more subproblems are evaluated the closer we get to the real value of P( x ), hence the anytime character of the method.\nThe role of Predict is in providing an estimate of the values of P( w;), so that those instances with bigger probability values are evaluated first. This is of course specially significant in real time applications where computations must respond to time constraints. The problem with computing this ranking of the instances using conventional methods is, of course, that this\ncomputation will amount to updating the probabili ties of the network which is the problem in the first place.\nGiven an f: a run of Predict will return, in polyno mial time, those values of ci, where c; is a node in the cutset CS, for which P( ci) < L10 Since P( ci) > P( c1, ... , ci, ... , en) we know that we can eliminate from our initial computation of Eq. 3 all those in stances of CS where ci is participant.\nThe benefits in using Predict as described above will depend on the probability distribution over the set CS. A worst case will be when P( w;) = � for 1 � i � m, since depending on f: Predict will either eliminate all instances of P( w;) or will include all these instances. A best case is when P( w; ) is ordered in stratas of f:, f:2, (3, • • . , since by using different values off:, Pre dict will be able to rank these instances efficiently and allow the computation of P( x) using only a subset of the subproblems. Some experimental results are pro vided in Section 4.3.\n4.2 Predict and Poole's Search-Based Algorithm\nGiven a Bayesian network, this algorithm builds a search tree in which each leaf of the the tree repre sents a possible instantiation of all the variables of the probability distribution. The problem of belief update in the network is transformed into a search problem in the expansion of this tree. The role of Predict is in providing a lookahead estimator of which branches of the tree can be pruned (according to the threshold imposed by the parameter f:).\nConsider a Bayes network with nodes {x1, . . . , Xn}, where the indexes of these nodes are ordered consis tently with any topological sort of these nodes accord ing tor. The root of the search tree is labeled with<> and the children of a node labeled with < x1, . . . , Xj > are the nodes labeled with < x1, . . . , Xj, Xj+l > for each instantiation Xj+l of node Xj+I· The leaves of the tree are tuples of the form < x1, . . . , xn >. The probability of each node in the tree are given by the equation P(xl, . . . 'Xj) = nl�i�j P(xili(x;)). The algorithm to generate this tree proceeds as follows. At any point we have a queue Q with tuples repre senting partial instantiations < x1, . . . , Xj >. When a partial instantiation is selected, it is expanded with a new value xi. If i = n then we reached a leaf and the tuple is removed from the queue. Let us use w to denote a complete instantiation of the variables {x1, . . . , xn}. Suppose we want to compute P(g). At any stage during the algorithm we can divide the set of complete instantiations n as those that have been already generated W, and those that will be gener-\n10Subject to the approximation discussed in the previous section.\n214 Goldszmidt\nated from the queue. Let Pfv = LwEWAwl=u P(w) and P(g) = LwE(!Awl=u P(w). Then at anytime during the algo�ithm P(g) is bounded by P� ::; P(g) ::; PQ where PQ 1s the sum of the probability of the elements in the queue. Note that as the algorithm progresses PQ is a monotonically nondecreasing quantity and conse quently the algorithm can be claimed to be anytime.\nThe complexity of the algorithm is of course in di rect proportion with the number of worlds that need to be enumerated in order to get a good approxima tion of P(g). The role of Predict is precisely that of finding which worlds can be ignored in this compu tation. There are at least two strategies for using Predict to explore and prune the search space. The first strategy involves providing an f and then running Predict on the network to prune all those instances < x1, ... , Xn > such that P(xi) < f for 1 ::; i ::; n. Note that care has to be taken not to eliminate all worlds relevant to P(g), therefore it has to be the case that P(g) > f. The other strategy uses Predict to se lect which of the current instances in the queue can be ignored. For this strategy, given an f, we run Predict with a particular partial instantiation < x1, ... , Xj > to decide whether P(g lxl , ... , Xj) > c. Note that P(g, x1, ... , xj) = P(g lxl, . . . , xj)*P(xl, ... , xj), and as soon as either of these terms is less than f, the in stance can be ignored for all practical purposes. Both these strategies rely on the fact that Predict provides a \"lookahead\" estimate of the uncertainty in polynomial time.\n4.3 Experimental Results\nWe have explored the use of Predict as part of a prob abilistic algorithm in the context of an application to plan evaluation [14]. In this application the user rep resents an uncertain domain (and its dynamics) using Bayesian networks and then is interested in evaluating and ranking the performance of different plans. Plans can be sequences of actions, conditional actions, or policies as in Markov decision processes (MDPs) [10]. The user may also use the same techniques to monitor and forecast the progress of a given plan.\nMost of the time the user is interested in a lower bound of the probability of the variables of interest rather than in their exact value, specially if this approxima tion can be computed fast. In the context of this ap plication the user provides a value for f and then can use Predict and a version of the ideas discussed in Sec tion 4.1 combined with other strategies due to Dar wiche [7], to obtain different estimates about the de gree of belief of variables of interest in the domain (we are currently extending the capabilities of the tool to include the computation of utility values). Note that if f is small the values computed will be closer to the real value of P(x). Yet the time required in reaching this value will be longer since more subproblems (in terms of Eq. 3) must be considered. The bigger the f,\nthe faster the computation will go; yet, the poorer the approximation to P(x).\nThese networks are particularly difficult for conven tional probabilistic algorithms because they contain a large number of loops by virtue of the representation of time [11, 8]. The characteristics of the networks we experimented with in this application were as fol lows: The number of nodes was around the hundreds, the number of nodes required for a cutset was around twenty, and the number of states, that is the number of subproblems that need to be considered for an exact computation, was around 20 x 106. The big number of subproblems is due to the fact that some of the vari ables had big state spaces. The quantification of these networks contained extreme probabilities (.99), as well as more moderate ones (including .7, and .5).\nThe way we computed the error in the update due to the approximations, was to average the probabilis tic mass lost for each variable. Since we were missing some subproblems due to the pruning done by Predict, the probabilities of each variable in the domain did not add up to one. The difference represented the loss of mass, a factor we denote by LM. Thus if the algo rithm returned P' (x) = p, then p ::; P(x) ::; p + LM. Some of the results were: for f = 0.2, the algorithm computed an answer in approximately 2 minutes. Yet, the quality of the answer was not very good, on av erage, LM = .5. For f = 0.1 the computation took nearly 4 minutes, and the accuracy of the answers in creased considerably, with LM < .05. For, f = 0.01, the computation took approximately 6 minutes, with LM < .002. The exact computation, considering all the subproblems, using the dynamic conditioning algo rithm described in [7] took 9 hours. We remark that we do not intend to make any claims on the performance of exact algorithms from these timings. The objective is to show the pruning power of Predict, and illus trate the relation between f and LM for a particular case. Careful comparisons to other exact and approxi mate probabilistic algorithms such as clustering-based methods and stochastic simulation [22, 5] is the sub ject of future research (see Section. 5). The compu tations by Predict took approximately 50 milliseconds in all cases. The experiments were conducted on a Sparc-10, and all the algorithms were implemented in Common Lisp.\nWe performed further experiments with different net works to assess how many subproblems of the cutset conditioning we have to evaluate in order to get a reasonable estimate of the probabilities of each node. This is important since it provides a direct metric of how useful and effective is the estimation that Pre dict computes. The networks considered where a set of plan-evaluation networks such as the ones described above, the well known alarm network (see [19]), and the car network described in [9, 18]. We modified the car network and expanded it over time, and in addi tion we considered evidence for faulty conditions in order to flatten the distribution over the cutset cases.\nFast Belief Update Using Order·of·Magnitude Probabilities 215\nFor the plan-evaluation networks and the car network we found that with less than 5% of the total subprob lems the error in the probabilistic estimate will be on the third most significant digit (i.e. on average the LM � .001). For the plan-evaluation networks this required the evaluation of 4000 subproblems, and in the car network it required 32 subproblems. The total number of subproblems that need to be evaluated in the car network for an exact computation was approx imately 36 x 103. The complete set of data shows that in both these cases the distribution over the subprob lems is very favorable to the use of Predict. The alarm network in turn required approximately 60% of the subproblems (72 subproblems in absolute numbers). Yet, the total number of subproblems for the alarm network was 108, and an exact computation was fea sible in less than one minute.\n5 Discussion\nIn this paper we have mainly focused on formally in troducing Predict, proving its main properties, and describing its interaction with probabilistic inference. Yet, in addition to providing fast estimates of proba bilistic values, there are at least two other topics re lated to Predict that deserve further attention. The first is as providing a clear link between belief net works, conditional logics and formalisms for nonmono tonic reasoning, and the second is as introducing a con cept of irrelevance in which both the structure of the network and its quantification play a role. We briefly examine these in turn, summarize the main results and describe future work.\nBelief update, conditional logics, and networks. Even though Predict takes as input an OMP repre sented by a ranking K, it currently only distinguishes between K(x) = 0 and K(x) > 0, which correspond to Pl(x) and ·Pl(x). Indeed the whole algorithm and its output can be described in terms of two modal ities: belief and plausibility [13]. This fact has two important consequences: first it establishes a link be tween belief networks and conditional logics of be lief studied by Boutilier [3] and others in the con text of belief revision and nonmonotonic reasoning, and second, it provides an algorithm for prediction the consequences of actions in these formalisms. From Def. 1 we have the following properties of plausibil ity and belief: (1) If •Pl(•xiy) then Bel(xiy) and, (2) Pl(x 1\\ y) iff Pl(xjy) and Pl(y). Using these thee constraints embedded in the structure of the net work can be expressed in terms of plausibility as: Pl(x1, ... , Xn) iff Pl(xnl7r(xn)) and . . . and Pl(xd7r(x1)). Additional work is required in order to find compact and natural ways to specify these constraints in a log ical language, and to characterize the set of logical models that satisfy a theory expressed using networks of belief.\nBelief-based irrelevance and independence. As we mentioned in Section 3.1, Def. 4 is preliminary. We\ncan expand it by introducing the notion of a context and by expressing it in terms of plausibility and be lief: We say that x is irrelevant to y in the context of z whenever Pl(x 1\\ y) iff Pl(x) and Pl(y) whenever Bel(z). Note the difference between this definition and similar definitions for irrelevance and independence in terms of probabilities or OMPs, where context is es tablished by conditioning. In probabilities, for exam ple x is independent of y given z whenever P(x, yjz) = P(xlz) * P(yjz) {K(x, yjz) = K{x!z) + K(yjz)). The for mal properties of a belief-based notion of irrelevance are yet to be determined. The potential of exploring this definition is twofold: first, it will be instrumental, in conjunction with a network-based representation, in formalizing a notion of irrelevance for logics of belief in nonmonotonic reasoning, and second, it could led to extending Predict to deal with diagnostic reasoning, where evidence may be influential in the complexity of the computations.\nMore on Predict and probabilistic inference. Fur ther experiments are necessary in order to characterize the efficiency and power of an heuristic estimator such as Predict in the context of cutset-conditioning re lated algorithms. In addition, a comparison to stochas tic simulation methods [5] would provide good bench marks in terms of speed of computation. Similar to Predict these methods are anytime, and very effective for tasks of prediction. Note however that in contrast to stochastic simulation methods where errors can only be estimated through randomized approximations, the computations returned by Predict are guaranteed to be lower bounds of the probability function. We remark that for these comparisons to be meaningful, care must be taken to isolate factors related to particular imple mentations and platforms. Finally, we are also inves tigating the combination of Predict with other algo rithms for probabilistic inference including clustering based methods [22], 11 D'Ambrosio's SPI algorithm [6], and other search methods such as the one proposed by Henri on [17].\nOMPs and probabilities. The main idea behind Predict is based on ignoring dependencies in the com putation of belief. This strategy opens an intriguing question: to what extend can we ignore undirected cycles in a Bayesian network, and use the values com puted as heuristic estimates of the real probabilistic values? The analysis in this paper provides an answer in the context of prediction tasks in terms of OMPs. Yet, what is the meaning, if there is one, of ignor ing loops in the general case?12 A related problem is the study and investigation of the consequences of assuming a real valued E as opposed to an infinitesi mal quantity for an OMP. The examples presented in\n11 A first approach would be to follow the directions sug gested in (21].\n12B. D'Ambrosio has started experimenting with this technique in decision making problems related to di agnosis and repair with surprising results (personal communication) .\n216 Goldszmidt\nSection 4 only scratch the surface. They indicate that this approximation degrades in terms of the structural properties of the network. More work is needed in order to provide a precise characterization.\nSummary. With the exception of [16], previous work on OMPs, €-semantics, and x:-rankings have focused mainly on the representational benefits of these for malisms [15, 9, 18]. This paper shows the clear ben efits of using OMPs for computing in Bayesian net works. As an algorithm Predict takes advantage of both the structure of the network and its quantifica tion. We have also shown how can the relation be tween OMPs and probabilities, through the parame ter f, can be exploited to approximate and speed up probabilistic inferences. The applications of Predict that we are currently exploring include: plan evalua tion, plan monitoring, and forecasting for contingency planning in stochastic domains.\nAcknowledgments. Many thanks to P. Dagum, T. Dean, K. Fertig, H. Geffner, and M. Peot for useful discus sions on related topics, and to A. Darwiche, M. Druzdzel, N. Friedman and especially to D. Koller for comments on previous versions of this paper. This work was funded in part by ARPA contract # F30602-91-C-0031.\nReferences\n[1] S. Benferhat, D. Dubois, and H. Prade. Represent ing default rules in possibilistic logic. In Proc. of Principles of Knowledge Representation and Reason ing Conf. (KR-92}, pages 673-684, 1992.\n[2] M. Boddy and T. Dean. Decision-theoretic deliberation scheduling for problem solving in time constrained environments. Artificial Intelligence, 67(2):245-286, 1994.\n[3] C. Boutilier. Conditional logics of normality: A modal approach. Artificial Intelligence, 68:87-154, 1994.\n[4] G. Cooper. The computational complexity of proba bilistic inference using Bayesian belief networks. Ar tificial Intelligence, 42:393-405, 1990. Research note.\n[5] P. Dagum and E. Horvitz. Bayesian analysis of simu lation algorithms. Networks, 23:499-516, 1993.\n[6] B. D'Ambrosio. Local expression languages for prob abilistic dependence. In Proc. of the 7th Conf. on Uncertainty in AI, pages 95-102, 1991.\n[7] A. Darwiche. Conditioning algorithms for exact and approximate inference in causal networks. In Proc. of the 11th Conf. on Uncertainty in AI, 1995. This volume.\n[8] A. Darwiche and M. Goldszmidt. Action networks: A framework for reasoning about actions and change under uncertainty. In Proc. of the 10th Con/. on Un certainty in AI, pages 136-144, 1994.\n[9) A. Darwiche and M. Goldszmidt. On the relation be tween kappa calculus and probabilistic reasoning. In Proc. of the lOth Conf. on Uncertainty in AI, pages 145-153, 1994.\n(10] T. Dean, L. P. Kaebling, J. Kirman, and A. Nichol son. Planning with deadlines in stochastic domains. In Proc. of the American Association for Artificial In telligence Conj., pages 574-579, 1993.\n[11] T. Dean and K. Kanazawa. A model for reasoning about persistence and causation. Computational In telligence, 5(3):1442-150, 1989.\n[12] H. A. Geffner. Default Reasoning: Causal and Condi ti onal Theories. MIT Press, Cambridge, MA, 1992.\n[13] M. Goldszmidt. Belief-based irrelevance and networks: Toward faster algorithms for prediction. Working notes: AAAI Fall Symposium on Relevance, 1994.\n[14] M. Goldszmidt and A. Darwiche. Plan simulation us ing Bayesian networks. In 11th IEEE Conf. on Arti ficial Intelligence Applicati ons, pages 155-161, 1995.\n[15] M. Goldszmidt and J. Pearl. Rank-based systems: A simple approach to belief revision, belief update, and reasoning about evidence and actions. In Proc. of Principles of Know ledge Representation and Reason ing Conf. (KR-92}, pages 661-672, 1992.\n[16] M. Goldszmidt and J. PearL Reasoning with qualita tive probabilities can be tractable. In Proc. of the 8th Conf. on Uncertainty in AI, pages 112-120, 1992.\n[17] M. Henrion. Search-based methods to bound diag nostic probabilities in belief nets. In Proc. of the 7th Conf. on Uncertainty in AI, pages 142-150, 1991.\n[18] M. Henrion, G. Provan, B. del Favero, and G. Sanders. An experimental comparison of diagnostic perfor mance using infinitesimal and numerical Bayesian be lief networks. In Proc. of the 10th Conf. on Uncer tainty in AI, pages 319-326, 1994.\n[19] E. Horvitz, G. Cooper, and H. Suerdmont. Bounded conditioning: Flexible inference for decisions under scarce resources. In Proc. of the 5th Workshop on Uncertainty in AI, pages 182-193, 1989.\n[20] D. Hunter. Parallel belief revision. In Shachter, Levitt, Kanal, and Lemmer, editors, Uncertainty in Artificial Intelligence (Vol. 4}, pages 241-252. North-Holland, Amsterdam, 1990.\n[21] F. Jensen and S. Andersen. Approximations in Bayesian belief universes for knowledge-based sys tems. In Proc. of the 6th Conf. on Uncertainty in AI, pages 162-169, 1990.\n[22] F. Jensen, S. Lauritzen, and K. Olesen. Bayesian up dating in causal probabilistic networks by local com putations. Computational Statistics Quarterly, 4:269- 282, 1990.\n[23] J. Pearl. Probabilistic Reasoning in Intelligent Sys tems: Networks of Plausible Inference. Morgan Kauf mann, San Mateo, CA, 1988.\n[24] D. Poole. The use of conflicts in searching Bayesian networks. In Proc. of the 9th Conf. on Uncertainty in AI, pages 359-367, 1993.\n[25] P. Shenoy. On Spohn's rule for revision of be liefs. International Journal of Approximate Reason ing, 5(2):149-181, 1991.\n[26] W. Spohn. Ordinal conditional functions: A dy namic theory of epistemic states. In W. L. Harper and B. Skyrms, editors, Causation in Decision, Belief Change, and Statistics, pages 105-134. Reidel, Dor drecht, Netherlands, 1988.\n[27] H. Suermondt and G. Cooper. Probabilistic inference in multiply connected networks using loop cutsets. In ternational Journal of Approximate Reasoning, 4:283- 306, 1990."
    } ],
    "references" : [ {
      "title" : "Represent­ ing default rules in possibilistic logic",
      "author" : [ "S. Benferhat", "D. Dubois", "H. Prade" ],
      "venue" : "In Proc. of Principles of Knowledge Representation and Reason­ ing Conf",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1992
    }, {
      "title" : "Decision-theoretic deliberation scheduling for problem solving in time­ constrained environments",
      "author" : [ "M. Boddy", "T. Dean" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1994
    }, {
      "title" : "Conditional logics of normality: A modal approach",
      "author" : [ "C. Boutilier" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1994
    }, {
      "title" : "The computational complexity of proba­ bilistic inference using Bayesian belief networks",
      "author" : [ "G. Cooper" ],
      "venue" : "Ar­ tificial Intelligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1990
    }, {
      "title" : "Bayesian analysis of simu­",
      "author" : [ "P. Dagum", "E. Horvitz" ],
      "venue" : "lation algorithms. Networks,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1993
    }, {
      "title" : "Local expression languages for prob­ abilistic dependence",
      "author" : [ "B. D'Ambrosio" ],
      "venue" : "In Proc. of the 7th Conf. on Uncertainty in AI,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1991
    }, {
      "title" : "Conditioning algorithms for exact and approximate inference in causal networks",
      "author" : [ "A. Darwiche" ],
      "venue" : "In Proc. of the 11th Conf. on Uncertainty in AI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "The reasons for this suc­ cess are various: They allow the representation of be­ lief and uncertain knowledge as a set of if-then rules, they provide a well known mechanism for belief update which is linked to notions of conditioning, and finally the provide a link to probability theory [16], condi­ tional logics [3], and other calculi for uncertainty such as possibility theory [1].",
      "startOffset" : 315,
      "endOffset" : 318
    }, {
      "referenceID" : 0,
      "context" : "The reasons for this suc­ cess are various: They allow the representation of be­ lief and uncertain knowledge as a set of if-then rules, they provide a well known mechanism for belief update which is linked to notions of conditioning, and finally the provide a link to probability theory [16], condi­ tional logics [3], and other calculi for uncertainty such as possibility theory [1].",
      "startOffset" : 381,
      "endOffset" : 384
    }, {
      "referenceID" : 3,
      "context" : "It is well known that, in general, probabilistic infer­ ence in Bayesian networks is NP-hard [4], and as our networks scale to meet the requirements of new and more challenging applications this inference will be­ come intractable.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "This fact emphasizes the impor­ tance of having a combination of anytime capabili­ ties [2] and approximation techniques.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "1 combined with other strategies due to Dar­ wiche [7], to obtain different estimates about the de­ gree of belief of variables of interest in the domain (we are currently extending the capabilities of the tool to include the computation of utility values).",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "The exact computation, considering all the subproblems, using the dynamic conditioning algo­ rithm described in [7] took 9 hours.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "Careful comparisons to other exact and approxi­ mate probabilistic algorithms such as clustering-based methods and stochastic simulation [22, 5] is the sub­ ject of future research (see Section.",
      "startOffset" : 137,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "This fact has two important consequences: first it establishes a link be­ tween belief networks and conditional logics of be­ lief studied by Boutilier [3] and others in the con­ text of belief revision and nonmonotonic reasoning, and second, it provides an algorithm for prediction the consequences of actions in these formalisms.",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "In addition, a comparison to stochas­ tic simulation methods [5] would provide good bench­",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Finally, we are also inves­ tigating the combination of Predict with other algo­ rithms for probabilistic inference including clustering­ based methods [22], 11 D'Ambrosio's SPI algorithm [6], and other search methods such as the one proposed by Henri on [17].",
      "startOffset" : 188,
      "endOffset" : 191
    } ],
    "year" : 2011,
    "abstractText" : "We present an algorithm, called Predict, for updating beliefs in causal networks quantified with order-of-magnitude probabilities. The algorithm takes advantage of both the struc­ ture and the quantification of the network and presents a polynomial asymptotic com­ plexity. Predict exhibits a conservative be­ havior in that it is always sound but not al­ ways complete. We provide sufficient con­ ditions for completeness and present algo­ rithms for testing these conditions and for computing a complete set of plausible values. We propose Predict as an efficient method to estimate probabilistic values and illustrate its use in conjunction with two known al­ gorithms for probabilistic inference. Finally, we describe an application of Predict to plan evaluation, present experimental results, and discuss issues regarding its use with condi­ tional logics of belief, and in the characteri­ zation of irrelevance.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}