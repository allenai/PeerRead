{
  "name" : "1107.0050.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Ariel Felner", "Richard E. Korf" ],
    "emails" : [ "felner@bgu.ac.il", "korf@cs.ucla.edu", "sarit@eshcolot.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Journal of Arti cial Intelligence Research 22 (2004) 279-318 Submitted 04/04; published 11/04Additive Pattern Database HeuristicsAriel Felner felner@bgu.ac.ilDepartment of Information Systems Engineering,Ben-Gurion University of the Negev, Beer-Sheva, 85104, IsraelRichard E. Korf korf@cs.ucla.eduDepartment of Computer Science, University of California, Los Angeles, 90095Sarit Hanan sarit@eshcolot.comDepartment of Computer Science, Bar-Ilan UniversityRamat-Gan, Israel, 52900 AbstractWe explore a method for computing admissible heuristic evaluation functions for searchproblems. It utilizes pattern databases (Culberson & Schae er, 1998), which are precom-puted tables of the exact cost of solving various subproblems of an existing problem. Unlikestandard pattern database heuristics, however, we partition our problems into disjoint sub-problems, so that the costs of solving the di erent subproblems can be added togetherwithout overestimating the cost of solving the original problem. Previously (Korf & Fel-ner, 2002) we showed how to statically partition the sliding-tile puzzles into disjoint groupsof tiles to compute an admissible heuristic, using the same partition for each state andproblem instance. Here we extend the method and show that it applies to other domainsas well. We also present another method for additive heuristics which we call dynamicallypartitioned pattern databases. Here we partition the problem into disjoint subproblems foreach state of the search dynamically. We discuss the pros and cons of each of these methodsand apply both methods to three di erent problem domains: the sliding-tile puzzles, the4-peg Towers of Hanoi problem, and nding an optimal vertex cover of a graph. We ndthat in some problem domains, static partitioning is most e ective, while in others dynamicpartitioning is a better choice. In each of these problem domains, either statically parti-tioned or dynamically partitioned pattern database heuristics are the best known heuristicsfor the problem.1. Introduction and OverviewHeuristic search is a general problem-solving method in arti cial intelligence. The mostimportant heuristic search algorithms include A*(Hart, Nilsson, & Raphael, 1968), iterative-deepening-A* (IDA*)(Korf, 1985a), and depth- rst branch-and-bound (DFBnB). All ofthese algorithms make use of a heuristic evaluation function h(n), which takes a state nand e ciently computes an estimate of the cost of an optimal solution from node n to agoal state. If the heuristic function is \\admissible\", meaning that it never overestimates thecost of reaching a goal, then all the above algorithms are guaranteed to return an optimalsolution, if one exists. The most e ective way to improve the performance of a heuristicsearch algorithm is to improve the accuracy of the heuristic evaluation function. Developingmore accurate admissible heuristic functions is the goal of this paper.c 2004 AI Access Foundation. All rights reserved.\nFelner, Korf, & Hanan1.1 Sliding-Tile PuzzlesOne of the primary examples in this paper is the well-known family of sliding- tile puzzles(see Figure 1). The Fifteen Puzzle consists of fteen numbered tiles in a 4 4 square frame,with one empty position, called the \\blank\". A legal move is to move any tile horizontallyor vertically adjacent to the blank into the blank position. The task is to rearrange thetiles from some initial con guration into a particular desired goal con guration. An optimalsolution to the problem uses the fewest number of moves possible. The Twenty-Four Puzzleis the 5 5 version of this problem. 1 2 3 4\n5 6 7 8\n10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n91 2 3\n4 5 6 7 8 9 10 11\n12 13 14 15Figure 1: The Fifteen and Twenty-Four Puzzles in their Goal StatesThe classic evaluation function for the sliding-tile puzzles is called Manhattan distance.It is computed by counting the number of grid units that each tile is displaced from its goalposition, and summing these values over all tiles, excluding the blank. Since each tile mustmove at least its Manhattan distance to its goal position, and a legal move only moves onephysical tile, the Manhattan distance is a lower bound on the minimum number of movesneeded to solve a problem instance.Using the Manhattan distance heuristic, IDA* can nd optimal solutions to randomlygenerated Fifteen puzzle instances in less than a minute on current machines. However,optimally solving random instances of the Twenty-Four puzzle with Manhattan distance isnot practical on current machines. Thus, a more accurate heuristic function is needed.1.2 Subgoal InteractionsThe reason for the inaccuracy of the Manhattan distance function is that it assumes thateach tile can be moved along a shortest path to its goal location, without interference fromany other tiles. What makes the problem di cult is that the tiles do interfere with eachother. The key to developing a more accurate heuristic function is to account for some ofthose interactions in the heuristic.1.3 Overview of PaperThis paper is organized as follows. In section 2 we present previous work on designing moreaccurate heuristic functions, including pattern databases in general (Culberson & Schae er,1998; Korf, 1997) and statically-partitioned additive pattern databases in particular (Korf& Felner, 2002).In section 3 we present the idea of dynamically-partitioned additive pattern databases,initially in the context of the tile puzzles. This is another approach to additive pattern280\nAdditive Pattern Database Heuristicsdatabases where the partitioning into disjoint subproblems is done dynamically for eachstate of the search, rather then statically in advance for all the states of the search (Korf &Felner, 2002).Sections 4-6 present three applications of these ideas. Section 4 presents our imple-mentations and experiments on the tile puzzles. We then present two other applications ofadditive pattern databases, including the 4-peg Towers of Hanoi problem in Section 5, and nding a minimum vertex cover of a graph in Section 6. We discuss the method in generalin Section 7, and nally suggest further work and present our conclusions in Section 8.The basic idea of dynamically-partitioned pattern databases was developed indepen-dently by Gasser (Gasser, 1995) and by Korf and Taylor (Korf & Taylor, 1996) in thecontext of the sliding-tile puzzles. It was also brie y mentioned in the earlier work(Korf &Felner, 2002) where statically-partitioned databases were introduced. Much of the materialon previous work described below was taken from (Korf & Felner, 2002).There are two main contributions of this paper. The rst is that we show that additivepattern databases, which were rst developed and implemented for the sliding-tile puzzles,can be generalized and applied to other domains. The second is that we divide additivepattern databases into two methods, statically- and dynamically-partitioned databases, andcompare the two methods experimentally.2. Previous Work on Admissible Heuristic FunctionsIn this section, we describe previous work on admissible heuristics from the classic theoriesuntil the new idea of pattern databases.2.1 Heuristics as Optimal Solutions to Relaxed ProblemsIn general, admissible heuristic functions represent the cost of exact solutions to simpli edor relaxed versions of the original problem (Pearl, 1984). For example, in a sliding-tilepuzzle, to move a tile from position x to position y, x and y must be adjacent, and positiony must be empty. By ignoring the empty constraint, we get a simpli ed problem whereany tile can move to any adjacent position. We can solve any instance of this new problemoptimally by moving each tile along a shortest path to its goal position, counting the numberof moves made. The cost of such a solution is exactly the Manhattan distance from theinitial state to the goal state. Since we removed a constraint on the moves, any solution tothe original problem is also a solution to the simpli ed problem, and the cost of an optimalsolution to the simpli ed problem is a lower bound on the cost of an optimal solution tothe original problem. Thus, any heuristic derived in this way is admissible.2.2 Linear Con ictsThe rst signi cant improvement to Manhattan distance was the linear-con ict heuristic(Hansson, Mayer, & Yung, 1992). Assume that in the goal state, tile 1 is to the left of tile2 in the top row, but in some particular state, tile 2 is to the left of tile 1 in the top row.The Manhattan distance of these tiles counts the number of steps they must move in thetop row to reach their goal positions, but doesn't account for the fact that one of these tilesmust move out of the top row to allow the other to pass by, and then move back into the top281\nFelner, Korf, & Hananrow. Thus, the interaction between these two tiles allows us to add two moves to the sum oftheir Manhattan distances without violating admissibility. The full linear-con ict heuristic nds all tiles that are in their goal row and/or column, but reversed in order, computes thenumber of moves needed to resolve these con icts, and adds this to the Manhattan distanceheuristic.2.3 Non-Additive Pattern Databases2.3.1 Fifteen Puzzle(Culberson & Schae er, 1998) carried this idea much further. For any given state, theminimum number of moves needed to get any subset of the tiles to their goal positions,including moves of other tiles, is clearly a lower bound on the number of moves neededto solve the entire puzzle. They chose as a subset of the Fifteen Puzzle tiles those in thebottom row and those in the rightmost column, referring to these as the fringe tiles.The number of moves needed to solve the fringe tiles depends on the current positionsof the fringe tiles and the blank, but is independent of the positions of the other tiles. Wecan precompute a table of these values, called a pattern database, with a single breadth- rstsearch backward from the goal state. In this search, the unlabeled tiles are all equivalent,and a state is uniquely determined by the positions of the fringe tiles and the blank. Aseach con guration of these tiles is encountered for the rst time, the number of moves madeto reach it is stored in the corresponding entry of the table. Note that this table is onlycomputed once for a given goal state, and the cost of computing it can be amortized overthe solution of multiple problem instances with the same goal state.Once this table is computed, IDA* can be used to search for an optimal solution to aparticular problem instance. As each state is generated, the positions of the fringe tiles andthe blank are used to compute an index into the pattern database, and the correspondingentry is used as the heuristic value of that state.Using the fringe and another pattern database, and taking the maximum of the twodatabase values as the overall heuristic value, Culberson and Schae er reduced the numberof nodes generated to solve random Fifteen Puzzle instances by a factor of about a thousand,and reduced the running time by a factor of twelve, compared to Manhattan distance(Culberson & Schae er, 1998). Since these database values include moves of tiles that arenot part of the pattern, they are non-additive, and the only way to admissibly combine thevalues from two such databases is to take their maximum value.2.3.2 Rubik's CubeNon-additive pattern databases were also used to nd the rst optimal solutions to the3 3 3 Rubik's Cube (Korf, 1997). Three di erent pattern databases were precomputed.One stored the number of moves needed to solve the eight corner pieces, another containedthe moves needed for six of the twelve edge pieces, and the other covered the remaining sixedge pieces. Given a state in an IDA* search, we use the con gurations of each of the threegroups of pieces to compute indices into the corresponding pattern databases, and retrievethe resulting values. Given these three heuristic values, the best way to combine themwithout sacri cing admissibility is to take their maximum, since every twist of the cubemoves four edge pieces and four corner pieces, and moves that contribute to the solution282\nAdditive Pattern Database Heuristicsof pieces in one pattern database may also contribute to the solution of the others. Usingthis method, IDA* was able to nd optimal solutions to random instances of Rubik's Cube(Korf, 1997). The median optimal solution length is only 18 moves. With improvements byMichael Reid and Herbert Kociemba, larger pattern databases, and faster computers, moststates can now be solved in a matter of hours.2.3.3 Limitations of Non-Additive Pattern DatabasesOne of the limitations of non-additive pattern databases is that they don't scale up tolarger problems. For example, since the Twenty-Four puzzle contains 25 di erent positions,a pattern database covering n tiles and the blank would require 25!=(25 n 1)! entries. Adatabase of only six tiles and the blank would require over 2.4 billion entries. Furthermore,the values from a database of only six tiles would be smaller than the Manhattan distanceof all the tiles for almost all states.If we divide the tiles into several disjoint groups, the best way to combine them admis-sibly, given the above formalization, is to take the maximum of their values. The reasonis that non-additive pattern database values include all moves needed to solve the patterntiles, including moves of other tiles.Instead of taking the maximum of di erent pattern database values, we would like tobe able to sum their values, to get a more accurate heuristic, without violating admissi-bility. We present two ways to do this: statically-partitioned additive pattern databases,and dynamically-partitioned additive pattern databases. In this paper we study both thesemethods, present general conditions for their applicability, and compare them experimen-tally.2.4 Statically-Partitioned Additive Database HeuristicsWhen statically-partitioned additive pattern databases were introduced (Korf & Felner,2002), they were called disjoint pattern databases. We introduce the new terminology hereto more clearly distinguish them from dynamically-partitioned pattern databases that willbe presented below. To construct a statically-partitioned pattern database for the sliding-tile puzzles, we partition the tiles into disjoint groups, such that every tile is included ina group, and no tile belongs to more than one group. We precompute pattern databasesof the minimum number of moves of the tiles in each group that are required to get thosetiles to their goal positions. Then, given a particular state in the search, for each group oftiles, we use the positions of those tiles to compute an index into the corresponding patterndatabase, retrieve the number of moves required to solve the tiles in that group, and thenadd together the values for each group, to compute an overall admissible heuristic for thegiven state. This value will be at least as large as the Manhattan distance of the state, andusually larger, since it accounts for interactions between tiles in the same group. The term\\statically-partitioned\" refers to the fact that the same partition of the tiles is used for allstates of the search.The key di erence between additive databases and the non-additive databases describedin section 2.3 above is that the non-additive databases include all moves required to solvethe pattern tiles, including moves of tiles not in the pattern group. As a result, giventwo such database values, even if there is no overlap among their tiles, we can only take283\nFelner, Korf, & Hananthe maximum of the two values as an admissible heuristic, because moves counted in onedatabase may move tiles in the other database, and hence these moves would be countedtwice. In an additive pattern database, we only count moves of the tiles in the group.Manhattan distance is a trivial example of a set of additive pattern databases, whereeach group contains only a single tile. For each tile, we could use the breadth- rst searchdescribed above to automatically compute a table of the minimum number of moves of thattile needed to move it from any position to its goal position. Such a set of tables wouldcontain the Manhattan distance of each tile from each position to its goal position. Then,given a particular state, we simply look up the position of each tile in its correspondingtable and sum the resulting values, thus computing the sum of the Manhattan distances. Infact, an e cient implementation of Manhattan distance works in exactly this way, lookingup the position of each tile in a precomputed table of Manhattan distances, rather thancomputing it from x and y coordinates of the current and goal positions of the tile.In the earlier paper (Korf & Felner, 2002) we implemented statically-partitioned additivepattern databases for both the Fifteen Puzzle and the Twenty-Four puzzles. De ne anx y z partitioning to be a partition of the tiles into disjoint sets with cardinalities of x, yand z. For the Fifteen Puzzle we used a 7-8 partitioning and for the Twenty-Four Puzzle weused a 6-6-6-6 partitioning. Currently, these implementations are the best existing optimalsolvers for these puzzles.2.4.1 Limitations of Statically-Partitioned Database HeuristicsThe main limitation of the statically-partitioned pattern database heuristics is that theyfail to capture interactions between tiles in di erent groups of the partition. In this paperwe will try to address this limitation by adding a di erent approach to additive patterndatabases, namely dynamically-partitioned database heuristics. Throughout the paper wewill use both approaches for the various domains, and study the pros and cons of each.3. Dynamically-Partitioned Database HeuristicsThe main idea behind this section was developed independently by (Gasser, 1995) and(Korf & Taylor, 1996), in the context of the sliding-tile puzzles. Consider a table whichcontains for each pair of tiles, and each possible pair of positions they could occupy, thenumber of moves required of those two tiles to move them to their goal positions. Gasserrefers to this table as the 2-tile pattern database, while we call these values the pairwisedistances. For most pairs of tiles in most positions, their pairwise distance will equal thesum of their Manhattan distances. For some tiles in some positions however, such as twotiles in a linear con ict, their pairwise distance will exceed the sum of their Manhattandistances. Given n tiles, there are O(n4) entries in the complete pairwise distance table,but only those pairwise distances that exceed the sum of the Manhattan distances of thetwo tiles need be stored. For example, the full pairwise distance table for the Twenty-Fourpuzzle would contain (24 23=2) 25 24 = 165; 600 entries, but only about 3000 of theseexceed the sum of their Manhattan distances.284\nAdditive Pattern Database Heuristics3.1 Computing the Pairwise DistancesHow do we compute such tables? For each pair of tiles, we perform a single breadth- rstsearch, starting from the goal state. In this search, a state is uniquely determined by thepositions of the two tiles of interest, and the blank. All other tiles are indistinguishable.The rst time that each di erent state of the two tiles is reached, regardless of the positionof the blank, the number of moves made to reach this state is recorded in the database.The search continues until all states of the two tiles and the blank have been generated.We perform a separate search for each of the n(n 1)=2 di erent pairs of tiles, wheren is the number of tiles. Since each state of these searches is determined by the positionsof the two tiles and the blank, there are O(n3) di erent states in each search, for an overallcomplexity of O(n5) to compute all the pairwise-distance tables. Note that these tables areonly computed once for a given goal state, so their computational cost can be amortizedover all subsequent problem-solving trials with the same goal state.3.2 Computing the Heuristic Value of a StateGiven a 2-tile database, and a particular state of the puzzle, we can't simply sum thedatabase values for each pair of tiles to compute the heuristic, since each tile is pairedwith every other tile, and this sum will grossly overestimate the optimal solution length.Rather, we must partition the n tiles into n=2 non-overlapping pairs, and then sum thepairwise distances for each of the chosen pairs. With an odd number of tiles, one tilewill be left over, and simply contributes its Manhattan distance to the sum. To get themost accurate admissible heuristic, we want a partition that maximizes the sum of thepairwise distances. For each state of the search, this maximizing partition may be di erent,requiring the partitioning to be performed for each heuristic evaluation. Thus, we use theterm dynamically-partitioned additive pattern database heuristics.To compute the heuristic for a given state, de ne a graph where each tile is representedby a vertex, and there is an edge between each pair of vertices, labeled with the pairwisedistance of the corresponding pair of tiles in that state. We call this graph the mutual-costgraph. The task is to choose a set of edges from this graph so that no two chosen edgesare incident to the same vertex, such that the sum of the labels of the chosen edges ismaximized. This is called the maximum weighted matching problem, and can be solved inO(n3) time (Papadimitriou & Steiglitz, 1982), where n is the number of vertices, or tiles inthis case.3.3 Triple and Higher-Order DistancesThis idea can generalized to larger groups of tiles as follows. Let k be the size of thegroup. A k tile database includes a table for each of the nk di erent groups of k tiles.Each table includes the number of moves of these k tiles that are required to get them totheir goal positions from each possible set of k positions they could occupy. A degeneratecase is a set of 1 tile databases which is actually a lookup table with the Manhattandistances. The next case is the pairs database described above. Potentially there are(n+1)n(n 1)(n 2) : : : (n k+2) di erent entries in each table, since n is the number oftiles (such as 15) while n+1 is the number of positions (such as 16). However while building285\nFelner, Korf, & Hanana set of k tile databases we don't necessarily need all these values if we also have the setof the (k 1) tile databases. For example, suppose that we are creating a the set of the2 tile databases and that we already have all the Manhattan distances (1-tile databases).In that case we only need to store pairwise distances that are greater than the sum of theManhattan distances of the two tiles in the pairs. In general in a k tile database we onlyneed to store a value that exceeds the sum of any of the partitions of the k tiles to smallersize groups.The next step is to have 3 tile databases. A set of 3 tile databases contains, foreach triple of tiles and each possible set of three positions they could occupy, the numberof moves of the three tiles required to get them to their goal positions. We only need tostore those values that exceed the both the sum of the Manhattan distances, as well asthe value given to these three tiles by the pairwise heuristic of the corresponding tiles.With the addition of the 3 tile databases, the corresponding mutual-cost graph containsa vertex for each tile, an edge for each pairwise distance, and a hyperedge connecting threevertices for each triple distance. The task is to choose a set of edges and hyperedges thathave no vertices in common, so that the sum of the weights of the edges and hyperedgesis maximized. Unfortunately, the corresponding three-dimensional matching problem isNP-complete (Garey & Johnson, 1979), as is higher-order matching. For the tile puzzles,however, if we only include tiles whose pairwise or triple distances exceed the sum of theirManhattan distances, the mutual-cost graph becomes very sparse, and the correspondingmatching problem can be solved relatively e ciently, as presented below. For problemswhere the mutual-cost graph is not sparse, then we might not be able to optimally solvethe maximum-matching problem and will have to settle for a sub optimal matching whichis still an admissible heuristic to the original problem. For example, this is the case withthe vertex-cover domain of section 6.The main advantage of this approach is that it can capture more tile interactions,compared to a statically-partitioned pattern database. The disadvantage of this approachis that computing the heuristic value of each state requires solving a matching problem onthe mutual-cost graph, which is much more expensive than simply adding the values froma database for each group.We now consider these ideas for each of our problem domains in turn: sliding-tile puzzles,the 4-Peg Towers of Hanoi problem, and vertex cover. We compare static and dynamicpartitioning for each of these domains.4. Sliding-Tile PuzzlesIn this section we describe our experiments solving the Fifteen and Twenty-Four puzzlesusing statically- and dynamically-partitioned additive pattern database heuristics. We beginby describing a domain-speci c enhancement for computing an admissible heuristic for thesliding-tile puzzles for the dynamically-partitioned pattern databases which is more accuratethan the general maximum weighted matching technique described above.4.1 Weighted Vertex Cover HeuristicConsider a state of a sliding-tile puzzle in which three tiles are in their goal row, but inopposite order from their goal order. Each of these tiles is in a linear con ict with the286\nAdditive Pattern Database Heuristicsother two. We can represent this 3-way linear con ict with a triangle graph, shown inFigure 2, where each tile is represented by a vertex, and there is an edge between eachpair of vertices. In the mutual-cost graph, the label on each edge would be the sum of theManhattan distances of the two tiles represented by the vertices at each end of the edge,plus two moves to resolve the linear con ict between them. To simplify the problem, wesubtract the Manhattan distances from the edges, resulting in the con ict graph shown inFigure 2. What is the largest admissible heuristic for this situation? 2 2 2Figure 2: Con ict graph for 3-way linear con ictThe maximum matching of this graph can only contain one edge, with a cost of two,since any two edges will have a vertex in common. However, in this case we can actuallyadd four moves to the sum of the Manhattan distances of the three tiles, because two of thethree tiles will have to temporarily move out of the goal row. Thus, while the maximumweighted matching is clearly admissible, it doesn't always yield the largest possible admis-sible heuristic value. If the pairwise distance of tiles X and Y in a given state is a, therewill be an edge in the corresponding mutual-cost graph between nodes X and Y , weightedby a. If x is the number of moves of tile X in a solution, and y is the number of movesof tile Y , then their pairwise distance represents a constraint that x + y a. Each edgeof the mutual-cost graph represents a similar constraint on any solution. For simplicityand e ciency, we assume here that the Manhattan distance of each tile will be added tothe nal heuristic value, so the con ict graph will only represent moves in addition to theManhattan distance of each tile.The problem is to assign a number of moves to each vertex of the con ict graph such thatall the pairwise constraints are satis ed. Since the constraints are all lower bounds, assigninglarge values to each node will satisfy all the constraints. In order for the resulting heuristicto be admissible, however, the sum of the values of all vertices must be the minimum sumthat satis es all the constraints. This sum is then the maximum admissible heuristic forthe given state. Furthermore, each vertex must be assigned a non-negative integer. In thecase of the triangle con ict graph in Figure 2, assigning a value of one to each vertex ortile, for an overall heuristic of three, is the minimum value that will satisfy all the pairwiseconstraints.While this is true for the general case, we can take it further for the tile puzzle domain.For the special case of the tile puzzle, any path between any two locations must have thesame even-odd parity as the Manhattan distance between the two locations. Therefore ifthe pairwise distance of X and Y is larger than the sum of their Manhattan distance bytwo, then at least one of the tiles must move at least two moves more than its Manhattandistance in order to satisfy the pairwise con ict. Therefore, such an edge in the pairwisecon ict graph means not only that x+ y 2 but that x 2 or y 2.287\nFelner, Korf, & HananIn order to satisfy all these constraints, one of the vertices incident to each edge must beset to at least two. If all edges included in the con ict graph have cost two, then the minimalassignment is two times the number of vertices needed such that each edge is incident toone of these vertices. Such a set of vertices is called a vertex cover, and the smallest suchset of vertices is called a minimum vertex cover. A minimum vertex cover of the graphin Figure 2 must include two vertices, for an overall heuristic of four, plus the sum of theManhattan distances.The vertex-cover heuristic is easily generalized from pairwise edges to handle triples ofnodes. For example, each triple distance that is not captured by Manhattan or pairwisedistances introduces a hypergraph \\edge\" that connects three nodes. The correspondingconstraint is that the sum of the costs assigned to each of the three endpoints must begreater than or equal to the weight of the hyperedge. For the vertex cover, a hyperedge iscovered if any of its three endpoints are in the set.There are two possibilities to cover a pairwise edge (X;Y ) with a weight of two i.e.,assigning two moves either to X or to Y . However, some triple distances are four movesgreater than the sum of their Manhattan distances. Covering hyperedges with weights largerthan two is a little more complicated but the principle is the same and all possibilities areconsidered. See (Felner, 2001) for more details.The general problem here can be called \\weighted vertex cover\". Given a hypergraphwith integer-weighted edges, assign an integer value to each vertex, such that for eachhyperedge, the sum of the values assigned to each vertex incident to the hyperedge is atleast as large as the weight of the hyperedge. The \\minimum weighted vertex cover\" is aweighted vertex cover for which the sum of the vertex values is the lowest possible.It is easy to prove that the minimum weighted vertex cover problem is NP-complete.Clearly it is in NP, since we can check a possible assignment of values to the vertices in timelinear in the number of hyperedges. We can prove it is NP-complete by reducing standardvertex cover to it. Given a graph, the standard vertex-cover problem is to nd a smallest setof vertices such that every edge in graph is incident to at least one vertex in the set. Givena standard vertex-cover problem, create a corresponding weighted vertex-cover problem byassigning a weight of one to every edge. The solution to this weighted vertex-cover problemwill assign either zero or one to each vertex, such that at least one endpoint of each edge isassigned the value one. The original vertex-cover problem will have a solution of k verticesif and only if there is a solution to the weighted vertex-cover problem in which k verticesare assigned the value one.Since weighted vertex cover is NP-complete, and we have to solve this problem to com-pute the heuristic for each state of the search, it may seem that this is not a practicalapproach to computing heuristics. Our experimental results show that it is, for severalreasons. The rst is that by computing the Manhattan distance rst, and only includingthose edges for which the distance exceeds the sum of the Manhattan distances, the result-ing con ict graph is extremely sparse. For a random state of the Twenty-Four Puzzle, itincludes an average of only four pairs of nodes, and four triples of nodes, some of whichmay overlap. Secondly, since the graph is sparse, it is likely to be composed of two or moredisconnected components. To nd the minimum vertex cover of a disconnected graph, wesimply add together the vertices in the minimum vertex covers of each component. Finally,in the course of the search, we can compute the heuristic incrementally. Given the heuristic288\nAdditive Pattern Database Heuristicsvalue of a parent node, we compute the heuristic value of each child by focusing only on thedi erence between the parent and child nodes. In our case, only one tile moves between aparent and child node, so we incrementally compute the vertex cover based on this smallchange to the con ict graph.4.2 Experimental ResultsHere we compare our implementation of statically- and dynamically- partitioned additivepattern databases for both Fifteen and Twenty-Four puzzles. Note that the best statically-partitioned pattern databases were already reported in the earlier paper (Korf & Felner,2002) and are provided here for purposes of comparison.4.2.1 Mapping FunctionsFor the sliding-tile puzzles, there are two ways to store and index partial states in a patterndatabase, a sparse mapping and a compact mapping. For a pattern of k tiles, a sparsetable occupies a k-dimensional array with a total of 16k entries for the Fifteen Puzzle. Anindividual permutation is mapped into the table by taking the location of each tile of thepermutation as a separate index into the array. For example, if we have a pattern of threetiles (X,Y,Z), which are located in positions (2,1,3) they would be mapped to the arrayelement a[2][1][3]. While this makes the mapping function e cient, it is ine cient in termsof space, since it wastes those locations with two or more equal indices.Alternatively, a compact mapping occupies a single-dimensional array. A pattern ofthree tiles of the Fifteen puzzle needs an array of size 16 15 14. Each permutation ismapped to an index corresponding to its position in a lexicographic ordering of all per-mutations. For example, the permutation (2 1 3) is mapped to the third index, since it ispreceded by (1 2 3) and (1 3 2) in lexicographic order. This compact mapping doesn't wastespace, but computing the indices is more complex and therefore consumes more time.Note that in all cases below, except the 7-8 partitioning for the Fifteen Puzzle, a sparsemapping was used for the databases. For the 7-8 partitioning of the Fifteen Puzzle, we useda compact mapping. This was done to save memory as a sparse mapping for a pattern of 8tiles exceeds our memory capacity.4.2.2 Fifteen PuzzleIn section 2.4 under previous work, we mentioned a 7-8 statically-partitioned additive pat-tern database for the Fifteen Puzzle. Here we compare that static partition with severalothers, and with dynamically-partitioned additive pattern databases in order to study thebehaviors of the di erent methods (Korf & Felner, 2002)1.1. It is worth mentioning here that the way we treated the blank tile for the statically-partitioned patterndatabases is not trivial. For the backwards breadth- rst search we treated this tile as a distinct tile thatcan move to any adjacent location. If that location was occupied by a real tile then that real tile wasmoved to the former blank location and we add one to the length of the path to that node. However,for the pattern database tables we have only considered the locations of the real tiles as indexes into thetables. We did not preserve the location of the blank and stored the minimum among all possible blanklocations in order to save memory. In a sense, we have compressed the pattern databases accordingto the location of the blank (see (Felner, Meshulam, Holte, & Korf, 2004) about compressing patterndatabases). 289\nFelner, Korf, & Hanan A 7-8 partitioning A 5-5-5 partitioning A 6-6-3 partitioning Figure 3: Di erent Statically-partitioned Databases for Fifteen PuzzleHeuristic Function Value Nodes Sec. Nodes/sec MemoryManhattan 36.940 401,189,630 53 7,509,527 0Linear con ict 38.788 40,224,625 10 3,891,701 0Dynamic, MM: pairs 39.411 21,211,091 13 1,581,848 1,000Dynamic, MM: pairs+triples 41.801 2,877,328 8 351,173 2,300Dynamic, WVC: pairs 40.432 9,983,886 10 959,896 1,000Dynamic, WVC: pairs+triples 42.792 707,476 5 139,376 2,300DWVC: pairs+triples+quadruples 43.990 110,394 9 11,901 78,800Static: 5-5-5 41.560 3,090,405 .540 5,722,922 3,145Static: 6-6-3 42.924 617,555 .163 3,788,680 33,554Static: 7-8 45.630 36,710 .028 1,377,630 576,575Table 1: Experimental results on the Fifteen Puzzle.Table 1 presents the results of running IDA* with di erent heuristics averaged over 1000random solvable instances of the Fifteen Puzzle. The average optimal solution length was52.552 moves. Each row presents results of a di erent heuristic.The rst column indicates the heuristic function. The second column, Value showsthe average heuristic value of the thousand initial states. The Nodes column shows theaverage number of nodes generated to nd an optimal solution. The Seconds column givesthe average amount of CPU time that was needed to solve a problem on a 500 megahertzPC. The next column presents the speed of the algorithm in nodes per second. Finally, thelast column shows the amount of memory in kilobytes that was needed for the databases,if any 2.The rst row presents the results of running Manhattan distance. The second rowadds linear con icts to the Manhattan distance. The next two rows present the resultsof the dynamically partitioned additive databases when the heuristic was obtained by amaximum-matching (MM) on the con ict graph. The next three rows present the resultsof the dynamically partitioned additive databases with the weighted vertex-cover heuristic(WVC), which is more accurate than the maximum-matching heuristic for this problem.Thus, we include the maximum-matching results for the Fifteen Puzzle for comparison2. For simplicity, in this paper we assume that a kilobyte has 103 bytes and that a megabyte has 106 bytes290\nAdditive Pattern Database Heuristicspurposes, while for the Twenty-Four and Thirty-Five Puzzles below, we only used theweighted vertex-cover heuristics. We can view the weighted vertex-cover heuristic as animplementation of the dynamically partitioned database in this domain, since for each nodeof the search tree, we retrieve all the pairwise, triple distances etc, and nd the best wayto combine them in an admissible heuristic.By only storing those pairs of positions where the heuristic value exceeds the sum of theManhattan distances, the pairs databases only occupied about one megabyte of memory. Byonly storing those triples of positions that exceeded the sum of the Manhattan distances, andalso were not covered by the pairs database, the pairs and triples together only occupiedabout 2.3 megabytes of memory. IDA* with the pairs heuristic (with weighted vertex-cover) required an average of about ten seconds to solve each instance, and the pairs andtriples heuristic reduced this to about ve seconds each. We have also experimented witha dynamically partitioned heuristic that also considers quadruples of tiles. However, whileadding quadruples reduced the number of generated nodes, the constant time per nodeincreased, and the overall running time was longer than the heuristic that only used pairsand triples. We have found that the bottleneck here was keeping the con ict graph accuratefor each new node of the search tree. For every tile that moves we need to check all the pairs,triples and quadruples that include this tile to see whether edges were added or deleted fromthe con ict graph. This was expensive in time, and thus the system with pairs and triplesproduced the best results among the dynamically-partitioned heuristics.For the statically-partitioned database we report results for three di erent partitionsshown in Figure 3. The rst uses a 5-5-5 partitioning of the tiles. The second uses a 6-6-3partitioning. Finally, the third partition is the same 7-8 partitioning from (Korf & Felner,2002). As the partitions use larger patterns, the memory requirements increase. While the5-5-5 partitioning needed only 3 megabytes, the 7-8 partitioning needed 575 megabytes. Theresults for all the statically-partitioned databases were calculated by taking the maximumof the above partitions and the same partitions re ected about the main diagonal. Thesame tables were used for the re ected databases, simply by mapping the positions ofthe tiles to their re ected positions. Note that even the 5-5-5 partitioning is faster thanthe dynamically partitioned database while using the same amount of memory. The bestresults were obtained by the 7-8 partitioning and were already reported earlier (Korf &Felner, 2002). This heuristic reduced the average running time of IDA* on a thousandrandom solvable instance to 28 milliseconds per problem instance. This is over 2000 timesfaster than the 53 seconds required on average with the Manhattan distance heuristic onthe same machine.Thus, we conclude that if su cient memory is available, heuristics based on statically-partitioned pattern databases are more e ective for the Fifteen puzzle than those based ondynamically-partitioned pattern databases.4.2.3 Twenty-Four PuzzleIn our earlier paper (Korf & Felner, 2002) we implemented a statically-partitioned ad-ditive pattern database for the Twenty-Four puzzle, partitioning the tiles into four groupsof six tiles, as shown in Figure 4, along with its re ection about the main diagonal.291\nFelner, Korf, & Hanan Figure 4: Static 6-6-6-6 database for Twenty-Four Puzzle and its re ection about the maindiagonalProblem Dynamic Partitioning Static PartitioningNo Path Nodes Seconds Nodes Seconds1 95 306,958,148 1,757 2,031,102,635 1,4462 96 65,125,210,009 692,829 211,884,984,525 147,4933 97 52,906,797,645 524,603 21,148,144,928 14,9724 98 8,465,759,895 72,911 10,991,471,966 7,8095 100 715,535,336 3,922 2,899,007,625 2,0246 101 10,415,838,041 151,083 103,460,814,368 74,1007 104 46,196,984,340 717,454 106,321,592,792 76,5228 108 15,377,764,962 82,180 116,202,273,788 81,6439 113 135,129,533,132 747,443 1,818,055,616,606 3,831,04210 114 726,455,970,727 4,214,591 1,519,052,821,943 3,320,098Table 2: Twenty-Four Puzzle results for static vs. dynamic databasesHere we added experiments on the Twenty-Four Puzzle, using dynamically-partitionedpairwise and triple distances, and computing the heuristic as the minimum weighted vertexcover. These databases required about three megabytes of memory. On our 500 MegahertzPentium III PC, IDA* using this heuristic generated about 185,000 nodes per second. Weoptimally solved the rst ten randomly-generated problem instances from (Korf & Felner,2002), and compared the results to those with the statically-partitioned pattern databaseheuristic. This heuristic generated 1,306,000 nodes per second.Table 2 shows the results. The rst column gives the problem number, and the sec-ond the optimal solution length for that problem instance. The next two columns show thenumber of nodes generated and the running time in seconds for IDA* with the dynamically-partitioned pattern database heuristic, and the last two columns present the same informa-tion for IDA* with the statically-partitioned database heuristic.For the Twenty-Four puzzle, the dynamically-partitioned database heuristic usually re-sults in fewer node generations, but a longer runtime overall, compared to the statically-partitioned database heuristic. The reason is that computing the best partition for eachstate, which requires solving a weighted vertex-cover problem, is much slower than simply292\nAdditive Pattern Database Heuristicssumming values stored in the static pattern database. On the other hand, the dynamicdatabase requires only three megabytes of memory, compared to 244 megabytes for thestatic database we used. Problem 3 in the table is the only case where the static partitiongenerated a smaller number of nodes than the dynamic partition. Problem 9 is the onlycase where IDA* ran faster with the dynamically-partitioned heuristic than the statically-partitioned heuristic.Here again we added quadruples to the dynamically-partitioned database, and foundout that while the number of generated nodes decreased in all problems except problem 9,the overall running time increased, compared to using just pairwise and triple distances.4.2.4 Thirty-Five PuzzleSince the performance of the dynamic database relative to the static database improvedin going from the Fifteen Puzzle to the Twenty-Four Puzzle, an obvious question is whathappens with an even larger puzzle, such as the 6 6 Thirty-Five Puzzle. Unfortunately, nding optimal solutions to random instances of the Thirty-Five Puzzle is beyond the reachof current techniques and machines.However, we can predict the relative performance of IDA* with di erent heuristics,using the new theory that was introduced lately (Korf, Reid, & Edelkamp, 2001). Allthat is needed is the brute-force branching factor of the problem space, and a large randomsample of solvable states, along with their heuristic values. A rather surprising result of thistheory is that the rate of growth of the number of nodes expanded by IDA* with increasingdepth is exactly the brute-force branching factor. Di erent heuristics a ect the exponentin this exponential growth, but not the base. To predict the actual performance of IDA*with a given heuristic, we would need the average optimal solution depth as well, which isunknown for the Thirty-Five Puzzle. However, we can predict the relative performance ofdi erent heuristics without knowing the solution depth. See (Korf et al., 2001) for moredetails.We computed a dynamically-partitioned pairs and triples pattern database for theThirty-Five Puzzle, storing only those values that exceed the sum of the Manhattan dis-tances of the corresponding tiles. For each state, the heuristic is computed as the minimumweighted vertex cover of the con ict graph.We also computed several di erent statically-partitioned pattern database heuristics.They were all based on a partitioning of the problem into 7 pattern of 5 tiles each. We ranthem on random problems to limited depths, and compared the number of nodes generatedto those depths. The one that generated the fewest nodes in searching to the same depthis shown in Figure 5. The two diagrams are re ections of each other about the maindiagonal. Each group includes ve tiles, and hence the number of entries in each databaseis 36!=31! = 45; 239; 040. For speed of access, we stored each database in a table of size365 = 60; 466; 176. For the overall heuristic we used the maximum of the sum of the valuesfrom the seven databases, and the sum of the values from the seven databases re ectedabout the main diagonal.In order to approximate the distribution of values of each heuristic, we sampled tenbillion random solvable states of the problem, and computed both static and dynamically-partitioned heuristic values for each, as well as Manhattan distance. This information,293\nFelner, Korf, & Hanan Figure 5: Disjoint Databases for Thirty-Five Puzzlealong with the brute-force branching factor of the problem, which is approximately 2.36761(Korf et al., 2001), allows us to predict the relative average number of nodes that would begenerated by IDA* using each heuristic, when searching to the same depth. We computedthe speed of IDA* using each heuristic, in nodes per second by running the algorithms to alimited depth. Table 3 below shows the results.The rst column gives the heuristic used with IDA*. The second column gives the aver-age value of the heuristic over all ten billion random states. As expected, the dynamicallypartitioned heuristic has the highest average value, and Manhattan distance the lowest. Thethird column gives the number of nodes generated in searching to the same depth, relativeto the dynamically-partitioned heuristic, which generates the fewest nodes. As expected,Manhattan distance generates the most nodes, and the dynamically-partitioned heuristicthe fewest. The fourth column gives the speed in nodes per second of IDA* using eachof the di erent heuristics on a 1.7 megahertz PC. As expected, Manhattan distance is thefastest, and the dynamically-partitioned heuristic the slowest. The fth column gives thepredicted relative running time of IDA* with each heuristic, searching to the same depth.This is computed by dividing the relative nodes by the nodes per second. The results areexpressed relative to the dynamically predicted heuristic, which is the fastest overall. Inthis case, the slower speed in nodes per second of the dynamically-partitioned heuristic ismore than made up for by the fewer node generations, making it about 1.8 times faster thanthe statically-partitioned heuristic overall. By comparison, IDA* with Manhattan distanceis predicted to be almost 1000 times slower. The last column shows the memory requiredfor the di erent heuristics, in kilobytes.Heuristic Value Rel. Nodes Nodes/sec Rel. Time MemoryManhattan 135.02 31,000.0 20,500,000 987.5 0Static 139.82 11.5 4,138,000 1.8 404,000Dynamic 142.66 1.0 653,000 1.0 5,000Table 3: Predicted Performance results on the Thirty-Five Puzzle294\nAdditive Pattern Database Heuristics4.2.5 Discussion of Tile Puzzle ResultsThe relative advantage of the statically-partitioned database heuristics over the dynamically-partitioned heuristics appears to decrease as the problem size increases. When moving tothe Thirty-Five Puzzle, the predicted performance of the dynamically partitioned databaseis better than the statically-partitioned database. One possible explanation is that for theFifteen Puzzle, we can build and store databases of half the tiles. For the Twenty-FourPuzzle, we can only include a quarter of the tiles in a single database. For the Thirty-FivePuzzle, this decreases to only 1=7 of the tiles in one database. Note that for the Thirty-FivePuzzle, the memory needed by the dynamically-partitioned database is almost two ordersof magnitude less than that required by the statically-partitioned database.5. 4-Peg Towers of Hanoi ProblemWe now turn our attention to another domain, the Towers of Hanoi problem.5.1 Problem DomainThe well-known 3-peg Towers of Hanoi problem consists of three pegs that hold a numberof di erent-sized disks. Initially, the disks are all stacked on one peg, in decreasing order ofsize. The task is to transfer all the disks to another peg. The constraints are that only thetop disk on any peg can be moved at any time, and that a larger disk can never be placedon top of a smaller disk. For the 3-peg problem, there is a simple deterministic algorithmthat provably returns an optimal solution. The minimum number of moves is 2n 1 wheren is the number of disks. Figure 6: Five-disk four-peg Towers of Hanoi problemThe 4-peg Towers of Hanoi problem, known as Reve's puzzle (van de Liefvoort, 1992;Hinz, 1997) and shown in Figure 6, is more interesting. There exists a deterministic algo-rithm for nding a solution, and a conjecture that it generates an optimal solution, but theconjecture remains unproven (Frame, 1941; Stewart, 1941; Dunkel, 1941). Thus, systematicsearch is currently the only method guaranteed to nd optimal solutions, or to verify theconjecture for problems with a given number of disks.5.2 Search AlgorithmsOnce we eliminate the inverse of the last move made, the search tree for the sliding-tilepuzzles generates relatively few duplicate nodes representing the same state. As a result,295\nFelner, Korf, & Hananthe depth- rst algorithm iterative-deepening-A* (IDA*) is the algorithm of choice. This isnot the case with the Towers of Hanoi problem, however.5.2.1 Branching Factor of 4-Peg Towers of HanoiFor the 4-peg problem, there are between three and six legal moves in any given state,depending on how many pegs are occupied. For example, if all pegs hold at least one disk,the smallest disk can move to any of the other three pegs, the next smallest disk that is ontop of a peg can move to any of the two pegs that don't contain the smallest disk, and thethird smallest disk that is on top of a peg can move to the single peg with a larger disk ontop. We can reduce the number of duplicate nodes by never moving the same disk twicein a row, reducing the branching factor to between two and ve. The average branchingfactor among those states with all pegs occupied, which is the vast majority of the states,is approximately 3.7664.5.2.2 Depth-First vs. Breadth-First Search and A*Unfortunately, even with this reduction, there are many small cycles in the 4-peg Towersof Hanoi domain, meaning there are many di erent paths between the same pair of states.For example, if we move a disk from peg A to peg B, and then another disk from peg C topeg D, applying these two moves in the opposite order will generate the same state.Since it can't detect most duplicate nodes, a depth- rst search must generate everypath to a given depth. The number of such paths is the branching factor raised to thepower of the depth of the search. For example, the optimal solution depth for the 6-disk4-peg Towers of Hanoi problem is 17 moves. Thus, a brute-force depth- rst search to thisdepth would generate about 3:766417 6 109 nodes, making it the largest 4-peg problemthat is feasibly solvable by depth- rst search. By storing all generated nodes, however, abreadth- rst search wouldn't expand more than the number of unique states in the problemspace. Since any disk can be on any peg, and the disks on any peg must be in decreasingorder of size, there are only 4n states of the n-disk problem. For six disks, this is only46 = 4096 states. Thus, breadth- rst search is much more e cient than depth- rst searchon this problem.The heuristic analog of breadth- rst search is the well-known A* algorithm (Hart et al.,1968). A* maintains a Closed list of expanded nodes, and an Open list of those nodesgenerated but not yet expanded. Since it stores every node it generates on one of these twolists, A* is limited by the amount of available memory, typically exhausting memory in amatter of minutes on current machines.5.2.3 Frontier A* AlgorithmFrontier-A* (FA*) is a modi cation of A* designed to save memory (Korf, 1999; Korf &Zhang, 2000). Instead of saving both the Open and Closed lists, frontier-A* saves only theOpen list, and deletes nodes from memory once they have been expanded. Its name derivesfrom the fact that the Open list can be viewed as the frontier of the search space, whereasthe Closed list corresponds to the interior of the space. In order to keep from regeneratingclosed nodes, with each node on the Open list the algorithm stores those operators thatlead to closed nodes, and when expanding a node those operators are not applied. Once the296\nAdditive Pattern Database Heuristicsgoal is reached, we can't simply trace pointers back through the Closed list to construct thesolution path, since the Closed list is not saved. Rather, a divide-and-conquer technique isused to reconstruct the solution path in frontier search. Since our purpose here is simply tocompare di erent heuristic functions, in the experiments discussed below we only executedthe rst pass of the algorithm to reach the goal state and determine the solution depth,rather than reconstructing the solution path. For more details on frontier-A* and frontiersearch in general, the reader is referred to (Korf, 1999; Korf & Zhang, 2000).5.3 Admissible Heuristic Evaluation FunctionsBoth A* and frontier-A* require a heuristic function h(n) to estimate the distance fromnode n to a goal. If h(n) is admissible, or never overestimates actual cost, both algorithmsare guaranteed to nd an optimal path to the goal, if one exists.5.3.1 Heuristic Function Based on Infinite-Peg RelaxationThe simplest admissible heuristic for this problem is the number of disks not on the goalpeg. A more e ective admissible heuristic comes from a relaxation of the problem whichallows an in nite number of pegs, or equivalently, as many pegs as there are disks. In thisrelaxed problem, the disks on any non-goal peg can be moved to the goal peg by movingall but the bottom disk to their own temporary peg, moving the bottom disk to the goalpeg, and then moving each of the remaining disks from their temporary pegs to the goalpeg. Thus, the required number of moves of disks on each non-goal peg is twice the numberof disks on the peg minus one. For disks on the goal peg, we count down from the largestdisk until we nd one that is not on the goal peg. Each smaller disk on the goal peg mustmove twice, once to its temporary peg to allow the largest disk not on the goal peg tobe moved to the goal peg, and then from its temporary peg back to the goal peg. Thenumber of moves needed to optimally solve this relaxed problem is the sum of these valuesfor each peg. This is also an admissible heuristic for the original problem, which we call thein nite-peg heuristic.5.3.2 Precomputing Additive Pattern DatabasesWe can also compute additive pattern database heuristics for this problem. Consider a 15-disk problem, and assume we partition the disks into one group of 8 disks and another groupof 7 disks. For example, we could divide them into the 8 largest disks and the 7 smallestdisks, or any other disjoint partition we choose. Then, for each group of disks, we computea table with one entry for every di erent possible combination of pegs that those disks couldoccupy. The value of the entry is the minimum number of moves required to move all thedisks in the group to the goal peg, assuming there are no other disks in the problem. Sincethere are exactly 4n states of the n-disk problem, indexing this table is particularly easy,since each disk position can be represented by two bits, and any con guration of n disks canbe uniquely represented by a binary number 2n bits long. We can compute such a table foreach group of disks in a single breadth- rst search backward from the goal state, in whichall the disks in the group start on the goal peg, and there are no other disks. We store inthe table the number of moves made the rst time that each state is encountered in thisbreadth- rst search. 297\nFelner, Korf, & HananThere are two important simpli cations we can take advantage of in this domain. The rst is that for a given number of disks, the pattern database will contain exactly the samevalues regardless of which disks we choose. In other words, a pattern database for the eightlargest disks, or the eight smallest disks, or eight even-numbered disks will all be identical.The reason is that only the number of disks matter, and not their absolute sizes. Therefore,we only need a single pattern database for each number of disks.The second simpli cation is that a pattern database for n disks also contains a patterndatabase for m disks, if m < n. To look up a pattern of m disks, we simply assign then m largest disks to the goal peg, and then look up the resulting con guration in then-disk pattern database. The result is that we only need a single pattern database for thelargest number of disks of any group of our partition.5.3.3 Computing the Heuristic Values from the Pattern DatabaseThe most e ective heuristic is based on partitioning the disks into the largest groups possi-ble, and thus we want the largest pattern database that will t in memory. For example, a14-disk database occupies 414 entries or 256 megabytes, since the maximum possible valuein such a database is 113 moves, which ts in one byte. This is the largest pattern databasewe can use on a machine with a gigabyte of memory, since a 15-disk database would occupyall the memory of the machine at one byte per entry. The 14-disk database gives us theexact optimal solution cost for any problem with 14 or fewer disks. To solve the 15-diskproblem, we can partition the disks into 14 and 1, 13 and 2, 12 and 3, etc.Given a particular split, such as 12 and 3, the static partitioning approach will alwaysgroup the same 12 disks together, say for example, the 12 largest disks vs. the 3 smallestdisks. Given a particular state of the search, we use the positions of the 12 largest disksas an index into the database, and retrieve the value stored there. We then look up theposition of the 3 smallest disks, assigning all larger disks to the goal peg, and retrieve thatvalue. Finally, we add these two values for the nal heuristic value.For any given state, which disks we choose to group together will a ect the heuristicvalue. For example, consider a state of the 15-disk problem where the 3 largest disks areon the goal peg. If we group the 12 largest disks together, we won't get an exact heuristicvalue, because we won't capture the interactions with the 3 smallest disks. If instead wegroup the 12 smallest disks together, and put the 3 largest disks in the other group, ourheuristic value will be exact, because no moves are required of the 3 largest disks, and theydon't interact with any of the smaller disks once they are on the goal peg. As anotherexample, if we group the 3 smallest disks together and they all happen to be on the goalpeg, then no moves will be counted for them, whereas if they were grouped with at leastone larger disk not on the goal peg, the heuristic will add at least two moves for each of thesmaller disks on the goal peg.The dynamic partitioning approach potentially uses a di erent partition of the disks foreach state. Given a 12-3 split for example, we consider all 15 14 13=6 = 455 ways ofdividing the disks, compute the resulting heuristic value for each, and then use the maximumof these heuristic values. 298\nAdditive Pattern Database HeuristicsHeuristic h(s) Avg h Nodes Expanded Seconds Nodes StoredIn nite Peg 29 26.37 371,556,049 1198 27,590,243Static 12-3 86 62.63 9,212,163 32 1,778,813Static 13-2 100 74.23 2,205,206 7 525,684Static 14-1 114 87.79 158,639 2 82,550Dynamic 14-1 114 95.52 122,128 2 25,842Table 4: Results for the 15-disk problem5.4 Experimental ResultsFor each of the experiments below, we used the frontier-A* algorithm, since A* ran outof memory on the larger problems or with the weaker heuristics. The database we usedwas the complete 14-disk pattern database, which requires 256 megabytes of memory. Wereport experiments with the 15, 16, and 17 disk problems.5.4.1 15-Disk ProblemFor the 15-disk problem, we implemented ve di erent heuristic functions. The rst wasthe in nite-peg heuristic, described in section 5.3.1 above. This heuristic doesn't require adatabase.The next three heuristics were statically-partitioned heuristics, based on the 14-diskpattern database. We divided the disks into groups of 12 and 3, 13 and 2, and 14 and 1,respectively. For the 12-3 split, the tiles were divided into the 12 largest disks and the threesmallest disks, and similarly for the 13-2 and 14-1 splits.Finally, we used a dynamically-partitioned pattern database heuristic, dividing the disksinto a group of 14 and a singleton disk. For each state, we computed the heuristic value forall 15 di erent ways of dividing the disks into groups of 14 and one, and took the maximumof these as the heuristic value.In each case we solved the single problem instance in which all 15 disks start on a singlepeg, and the task is to transfer them all to a di erent goal peg. Ties among nodes of equalf(n) value were broken in favor of smaller h(n) values. The optimal solution path is 129moves long. Table 4 shows the results. The rst column lists the heuristic function used.The second column gives the heuristic value of the initial state. The third column shows theaverage value of the heuristic function over the entire problem space. The fourth columnindicates the number of nodes expanded, and the fth column presents the time in seconds,on a 450 megahertz Sun Ultra-10 workstation. The last column shows the maximum numberof nodes stored, which is an indication of the relative memory required by the algorithms,excluding the memory for the pattern database.The data show that the heuristic based on the in nite-peg relaxation is the weakest one,requiring the most time and storing the most nodes. It is also the slowest to compute pernode, since the position of each disk must be considered in turn. In contrast, the databaseheuristics can be calculated by using the bits of the state representation as an index intothe database, with shifting and masking operations to partition the bits.299\nFelner, Korf, & HananHeuristic h(s) Avg h Nodes Expanded Seconds Nodes StoredStatic 13-3 102 75.78 65,472,582 221 10,230,261Static 14-2 114 89.10 17,737,145 65 2,842,572Dynamic 14-2 114 95.52 6,242,949 96 1,018,132Table 5: Results for the 16-disk problemAmong the statically-partitioned heuristics, the data clearly show that the larger thegroups, the better the performance. In particular, splitting the 15 tiles into 14 and 1 ismuch more e ective than the 13-2 or 12-3 split.Finally, the dynamically-partitioned 14-1 split generated the smallest number of nodesand thus consumed the smallest amount of memory. This is because it computes the bestpartition of the disks for each state, in order to maximize the heuristic value. While thisheuristic takes longer to compute for each node, it is not a signi cant factor in the overalltime, and it makes up for it by expanding and storing fewer nodes.5.4.2 16-Disk ProblemFor the 16-disk problem, the optimal solution length is 161 moves. Without a patterndatabase, we could store approximately 34 million nodes on our one-gigabyte machine.Frontier-A* with the in nite-peg heuristic ran out of memory while expanding nodes whosef = g + h cost was 133.Storing the 14-disk pattern database, which takes 256 megabytes of memory reducedthe number of nodes we could store to about 23 million. This only allowed us to solve theproblem with a 13-3 or 14-2 split. We compared heuristics based on statically partitionedsplits of 13-3 and 14-2 against a dynamically-partitioned 14-2 heuristic. In the later case,there are 16 15=2 = 120 ways to partition the 16 disks into one group of 14 disks andanother group of 2 disks. For each state, we computed the heuristic for each di erentpartition, and used the maximum heuristic value.The results are shown in Table 5, in a format identical to that in Table 4 for the 15-diskproblem. Again, we nd that the 14-2 static split is much more e ective than the 13-3split. In this case, the dynamically-partitioned heuristic took longer to solve the problemthan the largest static partitioning, since the dynamically-partitioned heuristic is muchmore expensive to compute than the statically-partitioned heuristic. On the other hand,the more accurate heuristic resulted in expanding and storing fewer nodes, thus requiringless memory, which is the limiting resource for best- rst search algorithms such as A* orfrontier-A*.5.4.3 17-Disk ProblemFinally, we tested our heuristics on the 17-disk problem. The optimal solution length tothis problem is 193 moves. We were able to store only about 23 million nodes without apattern database, since the state representation requires more than a single 32-bit word inthis case. Frontier-A* with the in nite-peg heuristic ran out of memory while expanding300\nAdditive Pattern Database Heuristicsnodes of cost 131. With the 14-disk pattern database we were only able to store about 15million nodes. With static partitioning of the disks into the largest 14 and the smallest3, frontier-A* exhausted memory while expanding nodes of cost 183. The only way wewere able to solve this problem was with a dynamically-partitioned 14-3 heuristic. In thiscase, for each state we computed all 17 16 15=6 = 680 di erent ways of dividing thedisks, looked-up the database value for each, and returned the maximum of these as the nal heuristic value. This took two hours to run, expanded 101,052,900 nodes, and stored10,951,608 nodes.We conclude that with a gigabyte of memory, the dynamically-partitioned databasesare better than the statically-partitioned databases for the 17-disk problem. If we hadenough memory to solve a problem with both the statically- and dynamically-partitioneddatabases, using the statically partitioned database might be faster. However, the lim-iting resource for best- rst searches such as A* is memory rather than time. Since thedynamically-partitioned databases use much less memory than the statically-partitioneddatabases, dynamic partitioning is the method of choice for solving the largest problems.5.5 Time to Compute the Pattern DatabaseThe results above do not include the time to compute the pattern database. Computing the14-disk database takes about 263 seconds, or 4 minutes and 20 seconds. If we are solvingmultiple problem instances with the same goal state, but di erent initial states, then thiscost can be amortized over the multiple problem solving trials, and becomes less signi cantas the number of problem instances increases.The Towers of Hanoi problem traditionally uses a single initial state with all disksstarting on the same peg, however. If that is the only state we care to solve, then the costof generating the pattern database must be taken into account. For the 15-disk problem,this would add 263 seconds to each of the running times for the database heuristics in Table4. Even with this additional time cost, they all outperform the in nite-peg heuristic, by asmuch as a factor of 4.5. The 16- and 17-disk problems are too big to be solved on our onegigabyte machine without the pattern database.5.6 Capitalizing on the Symmetry of the SpaceFrontier-A* with dynamically-partitioned pattern-database heuristics is the best existingtechnique for solving arbitrary initial states of large 4-peg Towers-of-Hanoi problems. Ifwe are only interested in solving the standard initial state with all disks on a single peg,however, we can do better by capitalizing on the fact that the initial and goal states arecompletely symmetric in this case (Bode & Hinz, 1999).Consider moving all n disks from the initial to the goal peg. In order to be able to movethe largest disk to the goal peg, all the n 1 smaller disks must be removed from the initialpeg, cannot be on the goal peg, and hence must be on the remaining two intermediatepegs. Note that we don't know the optimal distribution of the n 1 smallest disks on theintermediate pegs, but we do know that such a state must exist on any solution path.Once we reach a state where the largest disk can be moved to the goal peg, we makethe move. If we then apply the moves we made to reach this state from the initial state inreverse order, and interchange the initial and goal pegs, instead of moving the n 1 smallest301\nFelner, Korf, & Hanandisks from the intermediate pegs back to the initial peg, we will move them to the goal peg,thus completing the solution of the problem. Thus, if k is the minimum number of movesneeded to reach a state where the largest disk can be moved to the goal peg, the optimalsolution to the problem requires 2k + 1 moves.To nd the optimal solution to the standard initial state, we simply perform a breadth- rst search, searching for the rst state where the largest disk can be moved to the goalpeg. This only requires us to search to just less than half the optimal solution depth.Unfortunately, since we can't completely determine the goal state of this search, it is di cultto design an e ective heuristic function for it. Thus, we used breadth- rst frontier search.In general, this technique is faster than our best heuristic search for the goal state, butrequires storing more nodes. For example, for the 15-disk problem, the breadth- rst searchto the half-way point requires only 18 seconds, compared to 265 seconds for the heuristicsearch on a 450 megahertz Sun workstation, counting the time to generate the patterndatabase. On the other hand, it requires storing about 1.47 million nodes, compared toabout 26,000 nodes for the heuristic search. For the 16-disk problem, the breadth- rstsearch technique requires 102 seconds, but stores about 5.7 million nodes, compared to onemillion for the heuristic search. The 17-disk problem was the largest we could solve in onegigabyte of memory. It required 524 seconds and stored about 20 million nodes. By usingdisk storage for memory in a more complex algorithm (Korf, 2004), we have been able tosolve up to 24 disks, in each case verifying the conjectured optimal solution.Note that unlike the heuristic search, this technique only applies to the standard initialand goal states, where all disks are on one peg, and not to arbitrary problem instances.Recently (Felner et al., 2004), we were able to signi cantly improve these results bycompressing large pattern databases to a smaller size. We solved the 17-disk problem in7 seconds and the 18-disk problem in 7 minutes, not counting the time to compute thepattern database.6. Vertex CoverWe now turn our attention to our nal domain, that of computing an optimal vertex coverof a graph. We begin with a description of the problem, then consider the problem spaceto be searched, and nally the computation of additive pattern databases heuristics for thisdomain. Note that we have already seen an application of a more general version of thisproblem, the weighted vertex cover, to the problem of computing an admissible heuristicfor the sliding-tile puzzle.6.1 The ProblemGiven a graph, a vertex cover is a subset of the vertices such that every edge of the graphis incident to at least one of the vertices in the subset. A minimum or optimal vertex coveris one with the fewest number of vertices possible. Vertex cover was one of the earliestproblems shown to be NP-complete (Karp, 1972). There are many approaches to solve thisproblem. A comprehensive survey is beyond the scope of this paper. We refer the readerto a survey that was presented by Balasubramanian et al. (Balasubramanian, Fellows, &Raman, 1998). 302\nAdditive Pattern Database Heuristics6.2 The Search SpaceTo avoid confusing the search space tree with the original input graph, we will use the terms\\nodes\" and \\operators\" to refer to parts of the search tree, and the terms \\vertices\" and\\edges\" to refer to parts of the input graph.One of the simplest and most e ective search spaces for this problem is a binary tree.Each node of the tree corresponds to a di erent vertex of the graph. The left branch includesthe corresponding vertex in the vertex cover, and the right branch excludes the vertex fromthe vertex cover. The complete tree will have depth n, where n is the number of vertices,and 2n leaf nodes, each corresponding to a di erent subset of the vertices. (Downey &Fellows, 1995) classify this method as the bounded search tree method.Of course, much of this tree can be pruned in the course of the search. For example: If a vertex is excluded from the vertex cover, every adjacent vertex must be included,in order to cover the edge between them. In such cases, branches that do not includethese adjacent vertices are pruned. If a vertex is included, all of its incident edges are covered, so we modify the graphby removing this vertex and those edges for the remainder of the search below thatnode. If a vertex has no edges incident to it, we can exclude that vertex from the vertexcover. If a vertex has only one neighbor, we can exclude that vertex and include its neighbor.If there is an edge between two vertices that have no other neighbors, we can arbitrarilychoose one of the vertices to include, and exclude the other.Many attempts have been made recently to reduce the size of the search tree by pruningirrelevant nodes or by combining several nodes together (Chen, Kanj, & Jia, 2001; Bal-asubramanian et al., 1998; Downey, Fellows, & Stege, 1999; Niedermeier & Rossmanith,1999). To date, the best of these methods is the work done by (Chen et al., 2001). How-ever, despite the comprehensive and deep work that they have done, they all use a simpleg(n) cost function for each internal node n of the search tree which is the size of the partialvertex cover associated with node n. We extend this work by adding an admissible heuristicfunction h(n), based on additive pattern databases.During the course of the search, we keep the vertices of the remaining graph sortedin decreasing order of degree, and always branch next on the inclusion or exclusion of aremaining vertex of maximum degree.The best way to search this tree is using depth- rst branch-and-bound (DFBnB). Theinclusion-exclusion scheme guarantees that there is a unique path to each vertex cover, andhence no duplicate nodes, eliminating the need for storing previously generated nodes andallowing a linear-space depth- rst search. Once we reach the rst complete vertex cover, westore the number of vertices it includes. At any node n of the tree, let g(n) represent thenumber of vertices that have been included so far at node n. This is the cost of the partialvertex cover computed so far, and can never decrease as we go down the search tree. Thus,if we encounter a node n for which g(n) is greater than or equal to the smallest complete303\nFelner, Korf, & Hananvertex cover found so far, we can prune that node from further search, since it cannot leadto a better solution. If we nd a complete vertex cover with fewer nodes than the best sofar, we update the best vertex cover found so far.6.3 Heuristic Evaluation FunctionsOur main contribution to this problem is adding a heuristic evaluation function to estimatethe number of additional vertices that must be included to cover the edges of the remaininggraph. Denote such a function h(n), where n is a node of the search tree. In our DFBnBalgorithm, at each interior node n of the search, we apply the cost function f(n) = g(n) +h(n). If this cost equals or exceeds the size of the smallest vertex cover found so far, wecan prune this node. We are still guaranteed an optimal solution as long as h(n) neveroverestimates the number of additional vertices we have to add to the current vertex cover.For example, consider two disjoint edges in the remaining graph, meaning they don'tshare either of their end points. At least two vertices must be added to the vertex cover, sinceone vertex incident to each edge must be included, and no vertex is incident to both edges.More generally, a matching of a graph is a set of vertex-disjoint edges, and a maximummatching is a matching with the largest number of such edges. A maximum matching canbe computed in polynomial time (Papadimitriou & Steiglitz, 1982). Given a remaininggraph, any vertex cover of it must be at least as large as a maximum matching of thegraph. Thus, the size of a maximum matching is an admissible heuristic function for thevertex-cover problem.The size of a maximum matching is not the best admissible heuristic, however, as wecan consider costs of larger groups of vertices. For example, consider a triangle graph ofthree nodes with an edge between each pair, as shown in Figure 2. A maximum matchingof this graph contains one edge, since any two edges have a vertex in common. A minimumvertex cover of the triangle contains two vertices, however, since no single vertex is incidentto all three edges.More generally, consider a clique of k vertices, which is a set of vertices with an edgebetween every pair. A minimum vertex cover of such a clique includes k 1 vertices. This issu cient, since the edges incident to the single vertex left out are covered by the remainingincluded vertices. k 1 vertices are necessary, because if any two vertices are excluded, theedge between them will not be covered. A single edge is the special case of a clique of sizetwo. The idea of using cliques as a lower bound on the size of the minimum vertex cover isdue to (Marzetta, 1998).6.3.1 Computing an Admissible Heuristic with Additive Pattern DatabasesIn order to compute an admissible heuristic for the size of the minimum vertex cover ofa given graph, we partition the vertices into a set of vertex-disjoint cliques, meaning twocliques can't share a vertex. Each clique of size k contributes k 1 vertices to the size ofthe vertex cover, and we can sum these values for a set of vertex-disjoint cliques. In generalthe largest cliques make the largest contribution to the heuristic value. We would like thepartition that yields the maximum value.The clique problem is known to be NP-complete. However, we have found that for therandom graphs that we experimented with (with xed average degree), cliques of four or304\nAdditive Pattern Database Heuristicsmore vertices are very rare. Thus, in the experiments described below, we only looked forcliques of four nodes or less in time O(n4).Building an additive pattern database for this problem is done as follows. We rst scanthe original input graph and identify all cliques of four or fewer nodes. These cliques arethen stored in a pattern database. Then, during the search we need to retrieve disjointcliques from this database as will be shown below.6.3.2 Computing the Heuristic from the Pattern DatabaseAfter building the cliques database for the original graph, we calculate the heuristic for agiven remaining subgraph. A special case of such a graph is the complete input graph atthe root of the search tree where none of the vertices are included yet.We compute the heuristic as follows. We scan the cliques database and nd those cliquesall of whose vertices remain in the given graph. Note that each of these cliques is now ahyperedge in the mutual-cost graph. We then would like to partition these cliques into aset of vertex-disjoint cliques to maximize the total number of vertices included. For eachsuch clique of size k, we add k 1 to the heuristic value.Unfortunately, this partitioning problem is NP-complete. To avoid this NP-completeproblem, we use a greedy approach that looks for a maximal set of cliques. A maximal set ofcliques is a set of vertex-disjoint cliques that cannot be increased by adding more cliques toit, whereas a maximum set of cliques is one for the which the total number of vertices is aslarge as possible. In practice, we use a greedy approach that rst identi es vertex-disjointcliques of size four, then cliques of size three, and nally additional edges.6.3.3 Static Vs. Dynamic PartitioningThe above algorithm gives us a relatively e cient way to compute a lower-bound on thesize of the minimum vertex cover for a given graph. During our branch-and-bound search,however, as vertices are included and excluded, the remaining graph is di erent at eachnode. We need to compute a heuristic value for each of these remaining graphs based ondata from the cliques database.There are at least two ways to do this, corresponding to a static partition of the verticesand a dynamic partition.One way to compute a heuristic for each node is to partition the original graph intovertex-disjoint cliques in order to obtain a vertex cover of maximal size, and then use thissame partition throughout the search. With this approach, at each node of the search, weexamine the remaining graph. For each clique of size k in our original partition, if k 1 ofthe vertices of the partition have already been included, we don't add any to the heuristic,since all the edges of this clique have been covered. If fewer then k 1 vertices of the cliquehave been included so far, we add the di erence between k 1 and the number of verticesincluded to the heuristic value, since at least this many additional vertices of this cliquemust eventually be included. We do this for all the cliques in the partition, and sum theresulting contributions to determine the heuristic value of the node. We refer to this as thestatically-partitioned heuristic. 305\nFelner, Korf, & HananHeuristic Cliques Nodes seconds nodes/secNo heuristic { 720,582,454 4,134 174,306Static 2 187,467,358 1,463 128,139Static 3 105,669,961 849 124,464Static 4 103,655,233 844 123,252Dynamic 2 19,159,780 231 82,982Dynamic 3 4,261,831 70 60,451Dynamic 4 4,170,981 73 57,136Table 6: Performance of the di erent algorithms on graphs with 150 vertices and a densityof 16.Dynamic partitioning is done by repartitioning the vertices of the remaining graph intovertex-disjoint cliques at each node of the search tree, and computing the resulting heuristic.This is done according to the methods described in section6.3.2.Clearly, the dynamically-partitioned heuristic will be more accurate than the statically-partitioned heuristic, since the dynamic partition is based on the current remaining graph ateach node. On the other hand, the dynamically-partitioned heuristic will be more expensiveto compute, since we have to nd a maximal partitioning of the remaining graph at eachnode. To compute the statically-partitioned heuristic, we only have to do the partitioningonce, and then during the search we simply examine each node in each clique of the originalpartition to determine if it has been included or not.6.4 ExperimentsWe compared depth- rst branch-and-bound with the statically-partitioned heuristic todepth- rst branch-and-bound with the dynamically-partition heuristic. We experimentedon random graphs which were built as follows. Given two parameters, N to denote thenumber of vertices and D to denote the density (average degree of the graph) , we createda graph with N vertices such that each edge was added to the graph with probability DN .Table 6 presents the results of solving the vertex-cover problem on random graphs with 150vertices and density of 16. The Cliques column presents the size of the largest clique that westore in our database. The remaining columns present the number of generated nodes andthe number of seconds needed to solve an average problem. All the experiments reportedhere were also tried on other densities and the same tendencies were observed. We focus ondensity of 16 because it seems to be the most di cult case.For the vertex cover problem each problem instance has its own database. However, inall our experiments it took only a fraction of a second to build the database and hence thistime is omitted in all our data.As expected, the dynamically-partitioned heuristic generated fewer nodes than thestatically-partitioned heuristic, but ran slower per node. Overall, however, the dynamically-partitioned heuristic outperformed the statically-partitioned heuristic by up to an order of306\nAdditive Pattern Database Heuristicsmagnitude in running time. Both heuristics clearly outperform a system where no heuristicswere used. We have tried our system on many other graphs and obtained similar results.We have also added our heuristics to the best proven tree search algorithm for vertexcover, which was proposed by (Chen et al., 2001). They divide the nodes of the search treeinto a number of classes. Each class is determined by looking at special structures of vertices(like the degree of the vertex) and edges of the input graph in the area of the relevant vertex.Due to the special structure of the graph, some decisions on these vertices are forced andthus the size of the search tree is reduced by pruning other possibilities for branching. Forexample, consider a degree-2 vertex v whose two neighbors u and w are connected to eachother. In that case, since the three vertices form a triangle, u and w can be added to aminimal vertex-cover. (Chen et al., 2001) provided a large number of di erent classes andcases for pruning the search tree by focusing on special classes of vertices or by combiningseveral vertices together. They prove that their algorithm is asymptotically the fastestalgorithm that optimally solves the vertex-cover problem. Chen's algorithm calculates thecost of a node as the size of the partial vertex cover and does not try to gather knowledgefrom the vertices in the remaining graph in order to get a lower bound estimation on thenumber of vertices that must be added to the partial vertex cover. In other words they onlyuse a cost function of f = g. We have added our heuristics to their system and use the costfunction of f = g + h. Adding the dynamic database described above to Chen's pruningmethods further improved the running time as will be shown below. 0.1 1 10 100 1000 10000 100000 1e+06\n100 110 120 130 140 150 160 170 180 190\nT im\ne in\ns ec\non ds\n.\nNumber of nodes in graph\nVC: time versus number of nodes.density 16.\nnot using any method using only pruning and branching rules\nusing only 4-cliques heuristics using both methods\nFigure 7: Combining both methodsFigure 7 shows four curves that include all the four ways to combine the 4-cliquesheuristics with the pruning methods proposed by (Chen et al., 2001). The four ways are:use them both, use only one of them or not using any of them. The gure presents time inseconds on random graphs with sizes of 100 up to 190 and density of 16. The results clearlyshow that when using each method separately, the heuristic function approach outperformsthe pruning methods. Figure 7 also shows that when the input graph is larger than 140,the best combination between the two approaches is to use both of them. In other words,307\nFelner, Korf, & Hanansize Random graph, density 8 Random graph, density 16 Delaunay graphsSize Vc Nodes Sec Vc Nodes Sec Vc Nodes Sec150 92 2400 1 113 316,746 26 103 27,944 1200 130 296,097 33 153 21,897,066 2,045 137 224,349 3250 164 9,703,639 1,116 187 544,888,130 58,776 171 4,035,989 67300 181 56,854,403 7,815 204 32,757,219 493350 219 137,492,886 19,555 241 1,146,402,687 20,285400 269 27,443,208,087 485,581Table 7: Experimental results of our best combination on di erent graphs.adding our heuristics to the algorithm of (Chen et al., 2001) further improves the runningtime. On graphs with less than 140 nodes, however, it is even preferable to use the mostaccurate heuristic function without any pruning method. In all cases, adding our 4-cliquespattern database heuristic to any combination of the pruning methods is always bene cial.For example, for a graph with 190 vertices, the simple system with no pruning methods andno heuristics needed 453,488 seconds. Adding our 4-cliques heuristics improved the runningtime to only 2,764 seconds. Adding the heuristics to a system with all the pruning methodsfurther improved the overall time from 7,131 seconds for the pure Chen algorithm to 767seconds for the combined algorithm. . . . . .Figure 8: Random and Delaunay graphs of size 15.Since combining both methods seems to be best, we used this combination to solverandom graphs of larger sizes with densities of 8 and 16. We have also experimented withDelaunay graphs. These graphs comprise Delaunay triangulations of planar point patternsthat are constructed by creating a line segment between each pair of points (u,v) suchthat there exists a circle passing through u and v that encloses no other point. Figure 8illustrate a random graph and a Delaunay graph of size 15. In a random graph a vertex canbe connected to any of the other vertices while in a Delaunay graph vertices are connectedonly to nearby vertices. Note that the average degree of Delaunay graphs is around 5.The results are presented in Table 7. A graph of 250 vertices and a density of 16 wassolved in 16 hours while with density of 8 we could solve a graph of 350 vertices in 5 hours.We could solve Delaunay graphs of size up to 400 vertices.308\nAdditive Pattern Database Heuristics7. General Characterization of the MethodHere we abstract from our three example problems a general characterization of additiveheuristic functions and pattern databases. While the sliding-tile puzzles and Towers ofHanoi are similar permutation problems, the vertex-cover problem is signi cantly di erent.We begin by noting that additive heuristics and pattern databases are separate andorthogonal ideas. Additive heuristics are added together to produce an admissible heuristic.Pattern databases are a way of implementing heuristic functions by precomputing theirvalues and storing them in a lookup table. These two techniques can be used separatelyor in combination. For example, Manhattan distance is an additive heuristic that is oftencomputed as a function, rather than stored in a pattern database. Existing pattern databaseheuristics for Rubik's Cube are not additive. The heuristics described earlier for the sliding-tile puzzles and Towers of Hanoi problems are both additive and implemented by patterndatabases. A heuristic such as Euclidean distance for road navigation is neither additivenor implemented by a pattern database. We begin by giving a precise characterization of aproblem and its subproblems, then consider additive heuristics, and nally consider patterndatabases.7.1 Problem CharacterizationA problem space is usually described abstractly as a set of atomic states, and a set ofoperators that map states to states. This corresponds to a labeled graph, called the problem-space graph. In addition, a speci c problem instance is a problem space together witha particular initial state and a (set of) goal state(s). In order to talk precisely aboutsubproblems, however, we need to add more structure to this abstract characterization. Forthis, we adopt a formalism used by Korf (Korf, 1985b), and by others as well.7.1.1 Problem StateWe de ne a problem state as a vector of state variables, each of which is assigned a particularvalue. For example, in the Towers of Hanoi problem, there is a variable for each disk, witha value that indicates which peg the disk is on. For the sliding-tile puzzles, there is avariable for each physical tile, plus the blank position, with a value that indicates theposition occupied by that tile or the blank. We represent the blank position explicitly toeasily determine if a particular move is applicable. For the vertex-cover problem, there isa variable for each edge of the graph, with a value of one or zero, indicating whether theedge has been covered by an included vertex or not. For ease of exposition, below we willoften refer to the variable that corresponds to a particular disk, tile, or edge as simply thatdisk, tile, or edge.The initial state is a particular set of values assigned to each variable in the state vector.For the permutation problems, this speci es the initial positions of the tiles and blank, ordisks. For the vertex-cover problem, the initial state assigns zero to each edge, indicatingthat none of them are covered yet. A goal state is also indicated by a set of assignments tothe variables. For the permutation problems this re ects the desired positions of the tilesor disks. For the vertex-cover problem, the goal state assigns one to each edge, indicatingthat each edge is covered. 309\nFelner, Korf, & Hanan7.1.2 OperatorsAn operator in this formulation is a partial function from a state vector to a state vector,which may or may not apply to a particular state. For the vertex-cover problem, there is anoperator for each vertex. To include a vertex in the solution, we apply the correspondingfunction, which sets the value of each edge incident to the vertex to one, regardless of itsprevious value, and leaves the values of the other edges unchanged. For the sliding-tilepuzzles, we de ne a separate operator for each physical tile. The corresponding functionapplies to any state where the blank is adjacent to the given tile, and swaps the positions orvalues of the blank variable and the tile variable. For the Towers of Hanoi problem, there isa separate operator for each combination of disk and destination peg. If none of the smallerdisks are on the same peg as the disk to move, nor the destination peg, then the e ect of theoperator is to change the value of the moving disk to the destination peg. If this conditiondoesn't hold, then the operator is not applicable. Note that there are other natural waysto de ne the set of operators for the sliding-tile puzzles or the Towers of Hanoi.We say that an operator function application a ects a variable if it changes its value.For the Towers of Hanoi, each operator only a ects a single variable, that of the disk beingmoved, even though its applicability depends on all smaller disks. For the vertex-coverproblem, each operator (vertex) a ects all the variables (edges) that it is incident to. Forthe sliding-tile puzzle, each operator a ects two variables, the physical tile being moved andthe blank variable.7.1.3 SolutionsA feasible solution to a problem is a sequence of operator function applications that mapsthe initial state to a goal state. The cost of a solution is the sum of the costs of theoperators applied, and an optimal solution is one of lowest cost. There may be multipleoptimal solutions. For example, in the vertex-cover problem, the order in which verticesare included doesn't matter, so an optimal set of operators can be applied in any order.In each of these problems, each operator costs one unit, but in general di erent operatorscould have di erent costs.7.1.4 Patterns and their SolutionsWe de ne a pattern as an assignment of values to a subset of the state variables in a problem.For example, a pattern in a permutation problem might be a particular con guration of asubset of the tiles or disks. A pattern is thus an abstract state that corresponds to a set ofcomplete states of the problem which we call the pattern set. All the states in the patternset have the same values assigned to the subset of variables included in the pattern buthave di erent values for other variables.Given a solution to a problem instance, we de ne the cost of solving a pattern in thatproblem instance as the sum of the costs of those operators in the solution that a ect atleast one variable included in the pattern. This is a key di erence between our approach andthat of (Culberson & Schae er, 1998), since they count the cost of all operators applied tosolve a pattern, including those operators that only a ect variables that are not included inthe pattern. Even though other operators must often be applied to establish preconditionsfor operators that a ect the variables in the pattern, we don't count them in the cost of the310\nAdditive Pattern Database Heuristicssolution of the pattern. The optimal cost of solving a pattern is the lowest cost of solvingthe pattern from any complete state in the original problem in which the pattern variableshave their initial values, or in other words, from any state that is a member of the patternset. A solution of a pattern is therefore a lower bound on the solution to the complete statein the original problem. This is true for both our de nition of solution to a pattern, andfor the de nition of (Culberson & Schae er, 1998).7.2 Additive Heuristic FunctionsThe central question of this section is when is the sum of the optimal costs of solving acollection of patterns a lower bound on the optimal cost of solving the original problem? Asu cient condition is given below.Theorem 1 If we partition a subset of the state variables in a problem instance into acollection of subsets, so that no operator function a ects variables in more than one subset,then the sum of the optimal costs of solving the patterns corresponding to the initial valuesof the variables in each subset is a lower bound on the optimal cost of solving the originalproblem instance.Before giving the proof of this result, we illustrate it with each of our example domains.In the Towers of Hanoi problem, if we partition the disks into groups that are disjoint,the sum of the costs of solving the corresponding patterns will be an admissible heuristic,because each operator only a ects one disk. In the vertex-cover problem, we have to par-tition the edges into groups so that no vertex is incident to edges in more than one group,because each vertex represents an operator that a ects all the edges that are incident toit. This is most easily done by partitioning the vertices into disjoint subsets, and includingin each subset only the edges between vertices in the same subset. In a connected graph,this will require leaving some edges out of all patterns. The cost of covering these excludededges may not be counted, but that doesn't a ect the admissibility of the heuristic. In thesliding-tile puzzles, we need to partition the physical tiles into disjoint subsets. We can'tinclude the blank in any of these subsets, because every operator a ects the blank position.The proof of this result is very simple, now that we've made the right de nitions.Proof: Let S be an optimal or lowest-cost solution of a complete problem instance.S consists of a sequence of operators that map all the variables in the problem instancefrom their initial values to their goal values. The cost of S is the sum of the costs of theindividual operators in S.S is also a solution to any pattern in the original problem instance, since it maps anysubset of the variables from their initial values to their goal values. Thus, it is a solutionto each of the patterns in our hypothesized partition. Given a particular pattern, the costof S attributed to the pattern is the sum of the costs of those operators in S that a ectany of the variables in the pattern. Since by assumption no operator a ects variables inmore than one pattern in our partition, each operator of S can be attributed to at mostone pattern, based on the variables it a ects. The cost of S is thus greater than or equal tothe sum of the costs of S attributed to each of the patterns. Note that some variables maynot be included in any patterns, and thus the costs of any operators that only a ect suchvariables will not be attributed to the solution of any included patterns.311\nFelner, Korf, & HananSince the optimal cost of solving a pattern is less than or equal to the cost of any solutionto the pattern, we can conclude that the sum of the optimal costs of solving these patternsis a lower bound on the cost of the optimal solution to the original problem. Q.E.D.Rubik's cube is another example of a permutation problem. In Rubik's cube, however,each operator moves multiple physical subcubes. Furthermore, there is no non-trivial wayto partition the subcubes into disjoint subsets so that the operators only a ect subcubesfrom the same subset. Thus, there are no known additive heuristics for Rubik's cube, andthe best way to combine costs of di erent subproblems is to take their maximum. Similarly,the original pattern databases of (Culberson & Schae er, 1998), which counted all movesrequired to position the pattern tiles, including moves of non-pattern tiles, do not result inadditive costs, even if the pattern tiles are disjoint.An important question is how to divide the variables in a problem into disjoint patterns.There are two considerations here. One is that we would like a partition that will yield alarge heuristic value. This favors partitioning the problem into a small number of patterns,each of which include a large number of variables. The other consideration is that weneed to be able to e ciently compute, or precompute and store, the costs of solving thecorresponding patterns. In general, the fewer the variables in a pattern, the easier it is tocompute and store its solution cost, but the sum of such solutions tends to be a weakerlower bound on the overall solution cost, since the interactions between the patterns arenot included in the heuristic.For vertex-cover, any partition of the graph into edge-disjoint subgraphs will yield anadmissible heuristic. We chose to partition the graphs into subgraphs which are cliques,however, because the cost of a vertex cover of a clique of n nodes is both large relative tothe number of nodes, and easy to compute, since it is always equal to (n 1).7.3 Pattern DatabasesA pattern database is a particular implementation of a heuristic function as a lookup table.It trades space for time by precomputing and storing values of the heuristic function. Thisonly makes sense if the same values will be needed more than once, and there is su cientmemory available to store them. The values stored are the costs of solving subproblems ofthe original problem.A pattern database is built by running a breadth- rst search backwards from the com-plete goal state, until at least one state representing each pattern is generated. The cost ofthe solution to the pattern is the sum of the costs of the operators that changed any of thevariables in the pattern. For the Towers of Hanoi problem, we can ignore any variables thatare not part of the pattern. For the sliding-tile puzzle, we can ignore the tile variables thatare not part of the pattern, but must include the blank tile, to determine the applicabilityof the operators.7.3.1 Complete Pattern DatabasesIn general, a pattern database corresponds to a subset of the variables of the problem. Acomplete pattern database stores the cost of solving every pattern, de ned by the set of allpossible combinations of values of the variables in the pattern subset. For example, in apermutation problem, a pattern database corresponds to a particular set of tiles or disks,312\nAdditive Pattern Database Heuristicsand a complete such database would contain the cost of solving any permutation of thosetiles or disks. In the vertex-cover problem, a pattern database would correspond to a subsetof the edges of the original graph. A complete pattern database would contain the numberof additional vertices that must be included to cover every possible subset of that subset ofedges.One advantage of a complete pattern database is that every value is stored. Anotheradvantage is that the pattern itself can often be used to compute a unique index into thedatabase, so that only the cost of solving it need be stored. The primary disadvantage of acomplete database is its memory requirements.7.3.2 Partial Pattern DatabasesA partial pattern database only stores the cost of solving some of the patterns. This savesmemory by storing fewer values than a complete database. For example, given a subsetof sliding tiles, a partial pattern database might only store those permutations of the tileswhose solution cost is more than the sum of their Manhattan distances. For the remainingpatterns, we would just compute the sum of the Manhattan distances. This turned outto be very e cient for the dynamically-partitioned database as many of the values for thecomplete database were equal to the sum of the Manhattan distances. Another example isthat in the tile puzzles we only stored triples whose values are not captured by any of theirinternal pairs. One disadvantage of a partial database is that since a value isn't stored forevery pattern, we have to explicitly store the patterns that are included, in addition to theirsolution costs.For our vertex-cover pattern databases, we store cliques of the original graph, up toa certain maximum size. Technically, each of these cliques would represent a di erentdatabase, because each represents a di erent subset of the edges. However, we refer to theentire collection as a single database. Furthermore, we don't have to store a separate costfor each pattern of a clique, since in a clique of n vertices of which k vertices are alreadyincluded, the number of additional vertices needed to cover all the edges of the clique issimply n 1 k. Thus, all we really need to store is the vertices of the clique itself, sincethe rest of the information needed is readily computed.7.4 Algorithm SchemasBoth the statically-partitioned and the dynamically-partitioned pattern database heuristicsinclude two phases, the pre-computation phase and the search phase. The steps for each ofthese phases are presented below. For both versions we rst have to verify that additivityapplies according to Theorem 1.7.4.1 The Steps for the Statically-Partitioned Database Heuristics In the pre-computation phase do the following:{ Partition the variables of the problem into disjoint subsets.{ For each subset of variables, solve all the patterns of these variables and storethe costs in a database. 313\nFelner, Korf, & Hanan In the search phase do:{ for each state of the search space, retrieve the values of solving the relevantpatterns for each subset of the variables from the relevant databases, and addthem up for an admissible heuristic.7.4.2 The Steps for the Dynamically-Partitioned Database Heuristics. In the pre-computation phase do the following:{ For each subset of variables to be used (i.e., pair of variables or triple of variablesetc.){ Solve all the patterns of these subsets and store the costs in a database. In the search phase do:{ For each state in the search, retrieve the costs of the current patterns for eachsubset of variables.{ Find a set of disjoint variables from the database such that the sum of the corre-sponding costs is the largest admissible heuristic. In general, the correspondinghypergraph matching problem is NP-complete, and we may have to settle for anapproximation to the largest admissible heuristic value.There are many domain-dependent enhancements that can be applied to the aboveschemas. For example, the question of whether to store a complete pattern database ora partial pattern database is domain dependent. Also, in some cases we may be able toimprove the heuristic for a particular problem. In the tile puzzles for example, we used thesolution to a weighted vertex-cover problem as an admissible heuristic.7.5 Di erences Between the ApplicationsWhile there are many commonalities among the three applications that we study here, thereare also some important di erences.For the sliding-tile puzzles and Towers of Hanoi problem, we have a goal state thatis often shared by many di erent problem instances. In that case, we need to computeonly one set of pattern databases for all problem instances that share the same goal state.For these problems the time to create these tables can be amortized over many probleminstances and can be omitted.For the vertex-cover problem, every problem instance needs its own set of patterndatabases. Thus, the time to compute these tables must be counted in the solution cost.However, our experiments show that the relative amount of time to compute these tablesis insigni cant, compared to the total time to solve large problems.The vertex-cover problem is somewhat di erent than the permutation problems. In thepermutation problems, the costs of solving the di erent instances of the di erent subprob-lems were explicitly stored in the database. For the vertex-cover problem we only stored314\nAdditive Pattern Database Heuristicsthe fact that a given subset of vertices form a clique. This is because calculating the exactcost of covering a given clique for each search node can be done very easily on the y. Fora clique of size k, the cost is k 1 m if m vertices of the clique are already included. Onemight claim that this is a degenerate form of pattern databases since we do not store thecosts with the databases.To summarize, while we have presented a general schema, trying this on new problemsmay involve signi cant creativity in identifying the subproblems, determining the de nitionof disjoint subproblems, computing the pattern databases, doing the partitioning, etc. Also,taking advantage of domain-dependent attributes is often very important.8. Discussion, Conclusions and Future WorkA natural question to ask is given a problem, which type of database to use. While wecan't give a complete answer to this question, we can o er some guidelines based on ourexperiments.8.1 When to use each of the methodsThe bene t of static partitioning is that it is done only once. Thus, the time per node forcomputing the heuristic is very low, usually no more than retrieving and summing a fewvalues from a table. Dynamic partitioning considers multiple groups and partitions themon the y. This produces a more accurate heuristic at a cost of more time per node. Thisoverhead comes from updating the necessary data structures to re ect the current costsof the di erent groups, and from computing a maximum-matching to return the largestadmissible heuristic.One can add more patterns to a dynamic database to improve the heuristic. It appearshowever, that this is bene cial only up to a certain point. Beyond that point, the overheadper node dominates the reduction in the number of generated nodes, and thus the overalltime increases. For example, for the tile puzzles we observed that the best balance is toonly use pairs and triples. When we added quadruples to this system, we obtained a betterheuristic, but the overall time to solve a problem increased. We can observe exactly thesame behavior from the last two lines of Table 6 for the vertex-cover problem. For thegraphs of size 150, adding cliques of size four reduced the number of generated nodes, butincreased the overall time from 70 seconds to 73 seconds. Note however, that this was notthe case for other sizes of graphs that we tested. See (Felner, 2001) for more details. Forthe 4-peg towers of Hanoi problem we observed the same phenomenon. For the 15- diskproblem, the 14-1 dynamic partition solves the problem at the same speed as the 14-1 staticpartitioning, since there are only 15 di erent candidates to choose from. For the 16-diskproblem however, there are 120 di erent partitions with a 14-2 split. This causes a largeroverhead, and the dynamic partitioning runs slower than the static partitioning.The question is where is the point at which adding more patterns is no longer e ective.In this paper we compared two relatively extreme cases of the method where in one casewe only had one possible partition and in the other we had as many partitions as possible.The optimal way might be somewhere in the middle. At this point we believe that given aspeci c problem, only trial and error experimentation will reveal the best option.315\nFelner, Korf, & HananAnother issue is memory requirements. Depending on the problem domain, di erenttechniques may require more or less memory than others. For example, in the sliding-tilepuzzles, the most e ective dynamically-partitioned databases occupied much less memorythan the best statically-partitioned heuristics. For the Towers of Hanoi problem, the mem-ory requirements were the same for either method.Note that if we use A* for frontier-A* as the search algorithm, then there is a competitionfor memory between the pattern databases and the need to store nodes of the search treein Open and/or Closed lists. In that case, there might be a bene t for using the dynamicapproach if it provides a more accurate heuristic. With a more accurate heuristic, a smallernumber of nodes will be generated and the memory requirements of the Open and Closedlists will be decreased. This will allow us to solve larger problems with a given amount ofavailable memory. An example of this is the 4-peg 17-disk Tower of Hanoi problem, whichwe could only solve with the more accurated dynamically-partitioned pattern databases.8.2 Conclusions and Further WorkWe have considered both static and dynamic additive pattern database heuristics in threedi erent domains: sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and vertexcover. In each case, the resulting heuristics are the best known admissible heuristics forthese problems, and heuristic search using these heuristics is the best known method of nding optimal solutions to arbitrary instances these problem. For the special case of thestandard initial and goal states of the Towers of Hanoi problem, however, we can use thesymmetry between the initial and goal states to solve larger problems with breadth- rstsearch (Korf, 2004).In this paper we have studied the similarities and the di erences between the two versionsof additive pattern databases. We conclude that the question of which version to use isdomain dependent. For example, for the Fifteen and Twenty-Four puzzles, heuristics basedon a static partition of the problem are most e ective. For the Thirty-Five puzzle, dynamicpartitioning may be more e ective. For the Towers of Hanoi problem and vertex cover,dynamically partitioning the problem for each state of the search is most e ective for largeproblems.There are a number of avenues for future work on admissible heuristics for these problemdomains. Most recently, we have focussed on a technique for compressing larger patterndatabases (Felner et al., 2004). Another direction is automatically nding the best staticpartitioning. Another avenue is to nd more e cient algorithms for nding the best dynamicpartitionings, rather than simply computing them all and taking their maximum. Anotherissue is to automatically nd the optimal size of dynamically-partitioned databases.Finally, we believe that these techniques can be e ectively extended to new problemdomains. Hopefully the diversity of domains represented in this paper and the generaldiscussion will help in nding new applications of these ideas.9. AcknowledgmentsThe work was carried out while the rst author was at Bar-Ilan University. We wouldlike to thank Eitan Yarden and Moshe Malin for their help with the code for the dynamicdatabases for the tile puzzles. We would like to thank Ido Feldman and Ari Schmorak for316\nAdditive Pattern Database Heuristicstheir help with the code for the 4-peg Towers of Hanoi problem. Thanks to Robert Holtefor some fruitful discussions and for suggesting the 4-peg Towers of Hanoi problem. RichardKorf's work was supported by NSF under grant No. EIA-0113313, by NASA and JPL undercontract No. 1229784, and by the State of California MICRO grant No. 01-044.ReferencesBalasubramanian, R., Fellows, M. R., & Raman, V. (1998). An improved xed-parameteralgorithm for vertex cover. Information Processing Letters, 65(3), 163{168.Bode, J.-P., & Hinz, A. (1999). Results and open problems on the Tower of Hanoi. InProceedings of the Thirtieth Southeastern International Conference on Combinatorics,Graph Theory, and Computing, Boca Raton, FL.Chen, J., Kanj, I., & Jia, W. (2001). Vertex cover: further observations and further im-provements. Journal of Algorithms, 41, 280{301.Culberson, J., & Schae er, J. (1998). Pattern databases. Computational Intelligence, 14 (3),318{334.Downey, R. G., & Fellows, M. R. (1995). Parameterized computational feasibility. P. Clote,J. Remmel (Eds.), Feasible Mathematics, 2, 219{244.Downey, R. G., Fellows, M. R., & Stege, U. (1999). Parameterized complexity: A frameworkfor systematically conforting computational intractability. DIMACS Series in DiscreteMathematics and Theoretical Computer Science, 49.Dunkel, O. (1941). Editorial note concerning advanced problem 3918. American Mathe-matical Monthly, 48, 219.Felner, A. (2001). Improving search techniques and using them on di erent environments.Ph.D. thesis, Department of Computer Science, Bar-Ilan University, Ramat-Gan, Is-rael, available at http://www.ise.bgu.ac.il/faculty/felner.Felner, A., Meshulam, R., Holte, R., & Korf, R. (2004). Compressing pattern databases. InProceedings of the Nineteenth National Conference on Arti cial Intelligence (AAAI-04), pp. 638{643.Frame, J. (1941). Solution to advanced problem 3918. American Mathematical Monthly,48, 216{217.Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory ofNP-Completeness. W.H. Freeman, New York.Gasser, R. (1995). Harnessing computational resources for e cient exhastive search. Ph.D.thesis, Swiss Federal Institute of Technology, Zurich, Switzerland.Hansson, O., Mayer, A., & Yung, M. (1992). Criticizing solutions to relaxed models yieldspowerful admissible heuristics. Information Sciences, 63 (3), 207{227.Hart, P., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determinationof minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4,100{107. 317\nFelner, Korf, & HananHinz, A. M. (1997). The tower of Hanoi. In Algebras and Combinatorics: Proceedings ofICAC'97, pp. 277{289, Hong Kong. Springer-Verlag.Karp, R. (1972). Reducibility among combinatorial problems. In R. E. Miller and J.W.Thatcher (eds.), Complexity of Computer Computations, pp. 85{103, Plenum Press,New York.Korf, R. E. (1985a). Depth- rst iterative-deepening: An optimal admissible tree search.Arti cial Intelligence, 27 (1), 97{109.Korf, R. E. (1985b). Macro-operators: A weak method for learning. Arti cial Intelligence,26 (1), 35{77.Korf, R. E. (1997). Finding optimal solutions to Rubik's cube using pattern databases. InProceedings of the Fourteenth National Conference on Arti cial Intelligence (AAAI-97), pp. 700{705, Providence, RI.Korf, R. E. (1999). Divide-and-conquer bidirectional search: First results. In Proceedingsof the Sixteenth International Joint Conference on Arti cial Intelligence (IJCAI-99),pp. 1184{1189, Stockholm, Sweden.Korf, R. E. (2004). Best- rst frontier search with delayed duplicate detection. In Proceedingsof the Nineteenth National Conference on Arti cial Intelligence (AAAI-04), pp. 650{657, San Jose, CA.Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Arti cial Intelligence,134 (1-2), 9{22.Korf, R. E., Reid, M., & Edelkamp, S. (2001). Time complexity of iterative-deepening-A*.Arti cial Intelligence, 129 (1-2), 199{218.Korf, R. E., & Taylor, L. (1996). Finding optimal solutions to the twenty-four puzzle. InProceedings of the Thirteenth National Conference on Arti cial Intelligence (AAAI-96), pp. 1202{1207, Portland, OR.Korf, R. E., & Zhang, W. (2000). Divide-and-conquer frontier search applied to optimalsequence alignment. In Proceedings of the Sixteenth National Conference on Arti cialIntelligence (AAAI-2000), pp. 910{916, Austin, TX.Marzetta, A. (1998). ZRAM: a library of parallel search algorithms and its use in enu-meration and combinatorial optimization. Ph.D. thesis, Swiss Federal Institute ofTechnology, Zurich, Switzerland.Niedermeier, R., & Rossmanith, P. (1999). Upper bounds for vertex cover further improved.Lecture Notes in Computer Science, 1563, 561{570.Papadimitriou, C., & Steiglitz, K. (1982). Combinatorial Optimization: Algorithms andComplexity. Prentice-Hall, Englewood Cli s, N.J.Pearl, J. (1984). Heuristics. Addison Wesley, Reading Mass.Stewart, B. (1941). Solution to advanced problem 3918. American Mathematical Monthly,48, 217{219.van de Liefvoort, A. (1992). An iterative algorithm for the Reve's puzzle. The ComputerJournal, 35(1), 91{92. 318"
    } ],
    "references" : [ {
      "title" : "An improved xed-parameter",
      "author" : [ "R. Balasubramanian", "M.R. Fellows", "V. Raman" ],
      "venue" : null,
      "citeRegEx" : "Balasubramanian et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Balasubramanian et al\\.",
      "year" : 1998
    }, {
      "title" : "Results and open problems on the Tower of Hanoi",
      "author" : [ "Bode", "J.-P", "A. Hinz" ],
      "venue" : null,
      "citeRegEx" : "Bode et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Bode et al\\.",
      "year" : 1999
    }, {
      "title" : "Vertex cover: further observations and further",
      "author" : [ "J. Chen", "I. Kanj", "W. Jia" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2001
    }, {
      "title" : "Parameterized computational feasibility",
      "author" : [ "R.G. Downey", "M.R. Fellows" ],
      "venue" : null,
      "citeRegEx" : "Downey and Fellows,? \\Q1995\\E",
      "shortCiteRegEx" : "Downey and Fellows",
      "year" : 1995
    }, {
      "title" : "Parameterized complexity: A framework",
      "author" : [ "R.G. Downey", "M.R. Fellows", "U. Stege" ],
      "venue" : null,
      "citeRegEx" : "Downey et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Downey et al\\.",
      "year" : 1999
    }, {
      "title" : "Editorial note concerning advanced problem 3918",
      "author" : [ "O. Dunkel" ],
      "venue" : "American Mathe-",
      "citeRegEx" : "Dunkel,? 1941",
      "shortCiteRegEx" : "Dunkel",
      "year" : 1941
    }, {
      "title" : "Improving search techniques and using them on di erent environments",
      "author" : [ "A. Felner" ],
      "venue" : null,
      "citeRegEx" : "Felner,? \\Q2001\\E",
      "shortCiteRegEx" : "Felner",
      "year" : 2001
    }, {
      "title" : "Compressing pattern databases",
      "author" : [ "A. Felner", "R. Meshulam", "R. Holte", "R. Korf" ],
      "venue" : null,
      "citeRegEx" : "Felner et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Felner et al\\.",
      "year" : 2004
    }, {
      "title" : "Solution to advanced problem 3918",
      "author" : [ "J. Frame" ],
      "venue" : "American Mathematical Monthly,",
      "citeRegEx" : "Frame,? 1941",
      "shortCiteRegEx" : "Frame",
      "year" : 1941
    }, {
      "title" : "Computers and Intractability: A Guide to the Theory",
      "author" : [ "M. Garey", "D. Johnson" ],
      "venue" : null,
      "citeRegEx" : "Garey and Johnson,? \\Q1979\\E",
      "shortCiteRegEx" : "Garey and Johnson",
      "year" : 1979
    }, {
      "title" : "Harnessing computational resources for e cient exhastive search",
      "author" : [ "R. Gasser" ],
      "venue" : "Ph.D.",
      "citeRegEx" : "Gasser,? 1995",
      "shortCiteRegEx" : "Gasser",
      "year" : 1995
    }, {
      "title" : "Criticizing solutions to relaxed models yields",
      "author" : [ "O. Hansson", "A. Mayer", "M. Yung" ],
      "venue" : null,
      "citeRegEx" : "Hansson et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Hansson et al\\.",
      "year" : 1992
    }, {
      "title" : "A formal basis for the heuristic determination",
      "author" : [ "P. Hart", "N.J. Nilsson", "B. Raphael" ],
      "venue" : null,
      "citeRegEx" : "Hart et al\\.,? \\Q1968\\E",
      "shortCiteRegEx" : "Hart et al\\.",
      "year" : 1968
    }, {
      "title" : "The tower of Hanoi",
      "author" : [ "A.M. Hinz" ],
      "venue" : "Algebras and Combinatorics: Proceedings of",
      "citeRegEx" : "Hinz,? 1997",
      "shortCiteRegEx" : "Hinz",
      "year" : 1997
    }, {
      "title" : "Reducibility among combinatorial problems",
      "author" : [ "R. Karp" ],
      "venue" : "R. E. Miller and J.W.",
      "citeRegEx" : "Karp,? 1972",
      "shortCiteRegEx" : "Karp",
      "year" : 1972
    }, {
      "title" : "Depth- rst iterative-deepening: An optimal admissible tree search",
      "author" : [ "R.E. Korf" ],
      "venue" : null,
      "citeRegEx" : "Korf,? \\Q1985\\E",
      "shortCiteRegEx" : "Korf",
      "year" : 1985
    }, {
      "title" : "Macro-operators: A weak method for learning",
      "author" : [ "R.E. Korf" ],
      "venue" : "Arti cial Intelligence,",
      "citeRegEx" : "Korf,? 1985b",
      "shortCiteRegEx" : "Korf",
      "year" : 1985
    }, {
      "title" : "Finding optimal solutions to Rubik's cube using pattern databases",
      "author" : [ "R.E. Korf" ],
      "venue" : "In",
      "citeRegEx" : "Korf,? 1997",
      "shortCiteRegEx" : "Korf",
      "year" : 1997
    }, {
      "title" : "Divide-and-conquer bidirectional search: First results",
      "author" : [ "R.E. Korf" ],
      "venue" : "Proceedings",
      "citeRegEx" : "Korf,? 1999",
      "shortCiteRegEx" : "Korf",
      "year" : 1999
    }, {
      "title" : "Best- rst frontier search with delayed duplicate detection",
      "author" : [ "R.E. Korf" ],
      "venue" : "Proceedings",
      "citeRegEx" : "Korf,? 2004",
      "shortCiteRegEx" : "Korf",
      "year" : 2004
    }, {
      "title" : "Disjoint pattern database heuristics",
      "author" : [ "R.E. Korf", "A. Felner" ],
      "venue" : "Arti cial Intelligence,",
      "citeRegEx" : "Korf and Felner,? \\Q2002\\E",
      "shortCiteRegEx" : "Korf and Felner",
      "year" : 2002
    }, {
      "title" : "Time complexity of iterative-deepening-A",
      "author" : [ "R.E. Korf", "M. Reid", "S. Edelkamp" ],
      "venue" : null,
      "citeRegEx" : "Korf et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Korf et al\\.",
      "year" : 2001
    }, {
      "title" : "Finding optimal solutions to the twenty-four puzzle",
      "author" : [ "R.E. Korf", "L. Taylor" ],
      "venue" : null,
      "citeRegEx" : "Korf and Taylor,? \\Q1996\\E",
      "shortCiteRegEx" : "Korf and Taylor",
      "year" : 1996
    }, {
      "title" : "Divide-and-conquer frontier search applied to optimal",
      "author" : [ "R.E. Korf", "W. Zhang" ],
      "venue" : null,
      "citeRegEx" : "Korf and Zhang,? \\Q2000\\E",
      "shortCiteRegEx" : "Korf and Zhang",
      "year" : 2000
    }, {
      "title" : "ZRAM: a library of parallel search algorithms and its use in enu",
      "author" : [ "A. Marzetta" ],
      "venue" : null,
      "citeRegEx" : "Marzetta,? \\Q1998\\E",
      "shortCiteRegEx" : "Marzetta",
      "year" : 1998
    }, {
      "title" : "Upper bounds for vertex cover further improved",
      "author" : [ "R. Niedermeier", "P. Rossmanith" ],
      "venue" : null,
      "citeRegEx" : "Niedermeier and Rossmanith,? \\Q1999\\E",
      "shortCiteRegEx" : "Niedermeier and Rossmanith",
      "year" : 1999
    }, {
      "title" : "Combinatorial Optimization: Algorithms",
      "author" : [ "C. Papadimitriou", "K. Steiglitz" ],
      "venue" : null,
      "citeRegEx" : "Papadimitriou and Steiglitz,? \\Q1982\\E",
      "shortCiteRegEx" : "Papadimitriou and Steiglitz",
      "year" : 1982
    }, {
      "title" : "Heuristics",
      "author" : [ "J. Pearl" ],
      "venue" : "Addison Wesley, Reading Mass.",
      "citeRegEx" : "Pearl,? 1984",
      "shortCiteRegEx" : "Pearl",
      "year" : 1984
    }, {
      "title" : "Solution to advanced problem 3918",
      "author" : [ "B. Stewart" ],
      "venue" : "American Mathematical Monthly,",
      "citeRegEx" : "Stewart,? 1941",
      "shortCiteRegEx" : "Stewart",
      "year" : 1941
    }, {
      "title" : "An iterative algorithm for the Reve's puzzle",
      "author" : [ "A. van de Liefvoort" ],
      "venue" : null,
      "citeRegEx" : "Liefvoort,? \\Q1992\\E",
      "shortCiteRegEx" : "Liefvoort",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "In section 2 we present previous work on designing more accurate heuristic functions, including pattern databases in general (Culberson & Schae er, 1998; Korf, 1997) and statically-partitioned additive pattern databases in particular (Korf & Felner, 2002).",
      "startOffset" : 125,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "The basic idea of dynamically-partitioned pattern databases was developed independently by Gasser (Gasser, 1995) and by Korf and Taylor (Korf & Taylor, 1996) in the context of the sliding-tile puzzles.",
      "startOffset" : 98,
      "endOffset" : 112
    }, {
      "referenceID" : 27,
      "context" : "1 Heuristics as Optimal Solutions to Relaxed Problems In general, admissible heuristic functions represent the cost of exact solutions to simpli ed or relaxed versions of the original problem (Pearl, 1984).",
      "startOffset" : 192,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : "2 Rubik's Cube Non-additive pattern databases were also used to nd the rst optimal solutions to the 3 3 3 Rubik's Cube (Korf, 1997).",
      "startOffset" : 119,
      "endOffset" : 131
    }, {
      "referenceID" : 17,
      "context" : "Using this method, IDA* was able to nd optimal solutions to random instances of Rubik's Cube (Korf, 1997).",
      "startOffset" : 93,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Dynamically-Partitioned Database Heuristics The main idea behind this section was developed independently by (Gasser, 1995) and (Korf & Taylor, 1996), in the context of the sliding-tile puzzles.",
      "startOffset" : 109,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "See (Felner, 2001) for more details.",
      "startOffset" : 4,
      "endOffset" : 18
    }, {
      "referenceID" : 21,
      "context" : "See (Korf et al., 2001) for more details.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "36761 (Korf et al., 2001), allows us to predict the relative average number of nodes that would be generated by IDA* using each heuristic, when searching to the same depth.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "Figure 6: Five-disk four-peg Towers of Hanoi problem The 4-peg Towers of Hanoi problem, known as Reve's puzzle (van de Liefvoort, 1992; Hinz, 1997) and shown in Figure 6, is more interesting.",
      "startOffset" : 111,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "There exists a deterministic algorithm for nding a solution, and a conjecture that it generates an optimal solution, but the conjecture remains unproven (Frame, 1941; Stewart, 1941; Dunkel, 1941).",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 28,
      "context" : "There exists a deterministic algorithm for nding a solution, and a conjecture that it generates an optimal solution, but the conjecture remains unproven (Frame, 1941; Stewart, 1941; Dunkel, 1941).",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 5,
      "context" : "There exists a deterministic algorithm for nding a solution, and a conjecture that it generates an optimal solution, but the conjecture remains unproven (Frame, 1941; Stewart, 1941; Dunkel, 1941).",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 12,
      "context" : "The heuristic analog of breadth- rst search is the well-known A* algorithm (Hart et al., 1968).",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "3 Frontier A* Algorithm Frontier-A* (FA*) is a modi cation of A* designed to save memory (Korf, 1999; Korf & Zhang, 2000).",
      "startOffset" : 89,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "For more details on frontier-A* and frontier search in general, the reader is referred to (Korf, 1999; Korf & Zhang, 2000).",
      "startOffset" : 90,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "By using disk storage for memory in a more complex algorithm (Korf, 2004), we have been able to solve up to 24 disks, in each case verifying the conjectured optimal solution.",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Recently (Felner et al., 2004), we were able to signi cantly improve these results by compressing large pattern databases to a smaller size.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : "Vertex cover was one of the earliest problems shown to be NP-complete (Karp, 1972).",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Many attempts have been made recently to reduce the size of the search tree by pruning irrelevant nodes or by combining several nodes together (Chen, Kanj, & Jia, 2001; Balasubramanian et al., 1998; Downey, Fellows, & Stege, 1999; Niedermeier & Rossmanith, 1999).",
      "startOffset" : 143,
      "endOffset" : 262
    }, {
      "referenceID" : 2,
      "context" : "To date, the best of these methods is the work done by (Chen et al., 2001).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "The idea of using cliques as a lower bound on the size of the minimum vertex cover is due to (Marzetta, 1998).",
      "startOffset" : 93,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "We have also added our heuristics to the best proven tree search algorithm for vertex cover, which was proposed by (Chen et al., 2001).",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "(Chen et al., 2001) provided a large number of di erent classes and cases for pruning the search tree by focusing on special classes of vertices or by combining several vertices together.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Figure 7: Combining both methods Figure 7 shows four curves that include all the four ways to combine the 4-cliques heuristics with the pruning methods proposed by (Chen et al., 2001).",
      "startOffset" : 164,
      "endOffset" : 183
    }, {
      "referenceID" : 2,
      "context" : "adding our heuristics to the algorithm of (Chen et al., 2001) further improves the running time.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "For this, we adopt a formalism used by Korf (Korf, 1985b), and by others as well.",
      "startOffset" : 44,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "See (Felner, 2001) for more details.",
      "startOffset" : 4,
      "endOffset" : 18
    } ],
    "year" : 2011,
    "abstractText" : null,
    "creator" : "dvips 5.76 Copyright 1997 Radical Eye Software (www.radicaleye.com)"
  }
}