{
  "name" : "1603.01232.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems",
    "authors" : [ "Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "Lina M. Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Steve Young" ],
    "emails" : [ "thw28@cam.ac.uk", "mg436@cam.ac.uk", "nm480@cam.ac.uk", "lmr46@cam.ac.uk", "phs26@cam.ac.uk", "djv27@cam.ac.uk", "sjy@cam.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain."
    }, {
      "heading" : "1 Introduction",
      "text" : "Modern Spoken Dialogue Systems (SDS) are typically developed according to a well-defined ontology, which provides a structured representation of the domain data that the dialogue system can talk about, such as searching for a restaurant or shopping for a laptop. Unlike conventional approaches employing a substantial amount of handcrafting for\neach individual processing component (Ward and Issar, 1994; Bohus and Rudnicky, 2009), statistical approaches to SDS promise a domain-scalable framework which requires a minimal amount of human intervention (Young et al., 2013). Mrkšić et al. (2015) showed improved performance in belief tracking by training a general model and adapting it to specific domains. Similar benefit can be observed in Gašić et al. (2015), in which a Bayesian committee machine (Tresp, 2000) was used to model policy learning in a multi-domain SDS regime.\nIn past decades, adaptive NLG has been studied from linguistic perspectives, such as systems that learn to tailor user preferences (Walker et al., 2007), convey a specific personality trait (Mairesse and Walker, 2008; Mairesse and Walker, 2011), or align with their conversational partner (Isard et al., 2006). Domain adaptation was first addressed by Hogan et al. (2008) using a generator based on the Lexical Functional Grammar (LFG) fstructures (Kaplan and Bresnan, 1982). Although these approaches can model rich linguistic phenomenon, they are not readily adaptable to data since they still require many handcrafted rules to define the search space. Recently, RNN-based language generation has been introduced (Wen et al., 2015a; Wen et al., 2015b). This class of statistical generators can learn generation decisions directly from dialogue act (DA)-utterance pairs without any semantic annotations (Mairesse and Young, 2014) or hand-coded grammars (Langkilde and Knight, 1998; Walker et al., 2002). Many existing adaptation approaches (Wen et al., 2013; Shi et al., 2015; Chen et al., 2015) can be directly applied due to the ar X iv :1 60 3. 01 23\n2v 1\n[ cs\n.C L\n] 3\nM ar\nflexibility of the underlying RNN language model (RNNLM) architecture (Mikolov et al., 2010).\nDiscriminative training (DT) has been successfully used to train RNNs for various tasks. By optimising directly against the desired objective function such as BLEU score (Auli and Gao, 2014) or Word Error Rate (Kuo et al., 2002), the model can explore its output space and learn to discriminate between good and bad hypotheses. In this paper we show that DT can enable a generator to learn more efficiently when in-domain data is scarce.\nThe paper presents an incremental recipe for training multi-domain language generators based on a purely data-driven, RNN-based generation model. Following a review of related work in section 2, section 3 describes the detailed RNN generator architecture. The data counterfeiting approach for synthesising an in-domain dataset is introduced in section 4, where it is compared to the simple model fine-tuning approach. In section 5, we describe our proposed DT procedure for training natural language generators. Following a brief review of the data sets used in section 6, corpus-based evaluation results are presented in section 7. In order to assess the subjective performance of our system, a quality test and a pairwise preference test are presented in section 8. The results show that the proposed adaptation recipe improves not only the objective scores but also the user’s perceived quality of the system. We conclude with a brief summary in section 9."
    }, {
      "heading" : "2 Related Work",
      "text" : "Domain adaptation problems arise when we have a sufficient amount of labeled data in one domain (the source domain), but have little or no labeled data in another related domain (the target domain). Domain adaptability for real world speech and language applications is especially important because both language usage and the topics of interest are constantly evolving. Historically, domain adaptation has been less well studied in the NLG community. The most relevant work was done by Hogan et al. (2008). They showed that an LFG f-structure based generator could yield better performance when trained on in-domain sentences paired with pseudo parse tree inputs generated from a state-of-the-art, but out-ofdomain parser. The SPoT-based generator proposed\nby Walker et al. (2002) has the potential to address domain adaptation problems. However, their published work has focused on tailoring user preferences (Walker et al., 2007) and mimicking personality traits (Mairesse and Walker, 2011). Lemon (2008) proposed a Reinforcement Learning (RL) framework in which policy and NLG components can be jointly optimised and adapted based on online user feedback. In contrast, Mairesse et al. (2010) has proposed using active learning to mitigate the data sparsity problem when training datadriven NLG systems. Furthermore, Cuayhuitl et al. (2014) trained statistical surface realisers from unlabelled data by an automatic slot labelling technique.\nIn general, feature-based adaptation is perhaps the most widely used technique (Blitzer et al., 2007; Pan and Yang, 2010; Duan et al., 2012). By exploiting correlations and similarities between data points, it has been successfully applied to problems like speaker adaptation (Gauvain and Lee, 1994; Leggetter and Woodland, 1995) and various tasks in natural language processing (Daumé III, 2009). In contrast, model-based adaptation is particularly useful for language modeling (LM) (Bellegarda, 2004). Mixture-based topic LMs (Gildea and Hofmann, 1999) are widely used in N-gram LMs for domain adaptation. Similar ideas have been applied to applications that require adapting LMs, such as machine translation (MT) (Koehn and Schroeder, 2007) and personalised speech recognition (Wen et al., 2012).\nDomain adaptation for Neural Network (NN)based LMs has also been studied in the past. A feature augmented RNNLM was first proposed by Mikolov and Zweig (2012), but later applied to multi-genre broadcast speech recognition (Chen et al., 2015) and personalised language modeling (Wen et al., 2013). These methods are based on finetuning existing network parameters on adaptation data. However, careful regularisation is often necessary (Yu et al., 2013). In a slightly different area, Shi et al. (2015) applied curriculum learning to RNNLM adaptation.\nDiscriminative training (DT) (Collins, 2002) is an alternative to the maximum likelihood (ML) criterion. For classification, DT can be split into two phases: (1) decoding training examples using the current model and scoring them, and (2) adjusting the model parameters to maximise the separation\nbetween the correct target annotation and the competing incorrect annotations. It has been successfully applied to many research problems, such as speech recognition (Kuo et al., 2002; Voigtlaender et al., 2015) and MT (He and Deng, 2012; Auli et al., 2014). Recently, Auli and Gao (2014) trained an RNNLM with a DT objective and showed improved performance on an MT task. However, their RNN probabilities only served as input features to a phrase-based MT system."
    }, {
      "heading" : "3 The Neural Language Generator",
      "text" : "The neural language generation model (Wen et al., 2015a; Wen et al., 2015b) is a RNNLM (Mikolov et al., 2010) augmented with semantic input features such as a dialogue act1 (DA) denoting the required semantics of the generated output. At every time step t, the model consumes the 1-hot representation of both the DA dt and a token wt2 to update its internal state ht. Based on this new state, the output distribution over the next output token is calculated. The model can thus generate a sequence of tokens by repeatedly sampling the current output distribution to obtain the next input token until an end-ofsentence sign is generated. Finally, the generated sequence is lexicalised3 to form the target utterance.\nThe Semantically Conditioned Long Short-term Memory Network (SC-LSTM) (Wen et al., 2015b) is a specialised extension of the LSTM network (Hochreiter and Schmidhuber, 1997) for language generation which has previously been shown capable of learning generation decisions from paired DA-utterances end-to-end without a modular pipeline (Walker et al., 2002; Stent et al., 2004). Like LSTM, SC-LSTM relies on a vector of memory cells ct ∈ Rn and a set of elementwise multiplication gates to control how information is stored, forgotten, and exploited inside the network. The SCLSTM architecture used in this paper is defined by\n1A combination of an action type and a set of slot-value pairs. e.g. inform(name=”Seven days”,food=”chinese”)\n2 We use token instead of word because our model operates on text for which slot values are replaced by their corresponding slot tokens. We call this procedure delexicalisation.\n3The process of replacing slot token by its value.\nthe following equations, it ft ot rt ĉt  =  sigmoid sigmoid sigmoid sigmoid tanh W5n,2n ( wt ht−1 )\ndt = rt dt−1\nct = ft ct−1 + it ĉt + tanh(Wdcdt)\nht = ot tanh(ct)\nwhere n is the hidden layer size, it, ft,ot, rt ∈ [0, 1]n are input, forget, output, and reading gates respectively, ĉt and ct are proposed cell value and true cell value at time t, W5n,2n and Wdc are the model parameters to be learned. The major difference of the SC-LSTM compared to the vanilla LSTM is the introduction of the reading gates for controlling the semantic input features presented to the network. It was shown in Wen et al. (2015b) that these reading gates act like keyword and key phrase detectors that learn the alignments between individual semantic input features and their corresponding realisations without additional supervision.\nAfter the hidden layer state is obtained, the computation of the next word distribution and sampling from it is straightforward,\np(wt+1|wt, wt−1, ...w0,dt) = softmax(Whoht)\nwt+1 ∼ p(wt+1|wt, wt−1, ...w0,dt).\nwhere Who is another weight matrix to learn. The entire network is trained end-to-end using a cross entropy cost function, between the predicted word distribution pt and the actual word distribution yt, with regularisations on DA transition dynamics,\nF (θ) = ∑\nt p ᵀ t log(yt) + ‖dT‖+ ∑T−1 t=0 ηξ ‖dt+1−dt‖ (1)\nwhere θ = {W5n,2n,Wdc,Who}, dT is the DA vector at the last index T, and η and ξ are constants set to 10−4 and 100, respectively."
    }, {
      "heading" : "4 Training Multi-domain Models",
      "text" : "Given training instances (represented by DA and sentence tuples {di,Ωi}) from the source domain S (rich) and the target domain T (limited), the goal is to find a set of SC-LSTM parameters θT that can perform acceptably well in the target domain."
    }, {
      "heading" : "4.1 Model Fine-Tuning",
      "text" : "A straightforward way to adapt NN-based models to a target domain is to continue training or fine-tuning a well-trained generator on whatever new target domain data is available. This training procedure is as follows:\n1. Train a source domain generator θS on source domain data {di,Ωi} ∈ S with all values delexicalised4.\n2. Divide the adaptation data into training and validation sets. Refine parameters by training on adaptation data {di,Ωi} ∈ T with early stopping and a smaller starting learning rate. This yields the target domain generator θT.\nAlthough this method can benefit from parameter sharing of the LM part of the network, the parameters of similar input slot-value pairs are not shared4. In other words, realisation of any unseen slot-value pair in the target domain can only be learned from scratch. Adaptation offers no benefit in this case."
    }, {
      "heading" : "4.2 Data Counterfeiting",
      "text" : "In order to maximise the effect of domain adaptation, the model should be able to (1) generate acceptable realisations for unseen slot-value pairs based on similar slot-value pairs seen in the training data,\n4We have tried training with both slots and values delexicalised and then using the weights to initialise unseen slot-value pairs in the target domain. However, this yielded even worse results since the learned semantic alignment stuck at local minima. Pre-training only the LM parameters did not produce better results either.\nand (2) continue to distinguish slot-value pairs that are similar but nevertheless distinct. Instead of exploring weight tying strategies in different training stages (which is complex to implement and typically relies on ad hoc tying rules), we propose instead a data counterfeiting approach to synthesise target domain data from source domain data. The procedure is shown in Figure 1 and described as following:\n1. Categorise slots in both source and target domain into classes, according to some similarity measure. In our case, we categorise them based on their functional type to yield three classes: informable, requestable, and binary5.\n2. Delexicalise all slots and values.\n3. For each slot s in a source instance (di,Ωi) ∈ S, randomly select a new slot s′ that belongs to both the target ontology and the class of s to replace s. Repeat this process for every slot in the instance and yield a new pseudo instance (d̂i, Ω̂i) ∈ T in the target domain.\n4. Train a generator θ̂T on the counterfeited dataset {d̂i, Ω̂i} ∈ T.\n5. Refine parameters on real in-domain data. This yields final model parameters θT.\nThis approach allows the generator to share realisations among slot-value pairs that have similar functionalities, therefore facilitates the transfer learning\n5Informable class include all non-binary informable slots while binary class includes all binary informable slots.\nof rare slot-value pairs in the target domain. Furthermore, the approach also preserves the co-occurrence statistics of slot-value pairs and their realisations. This allows the model to learn the gating mechanism even before adaptation data is introduced."
    }, {
      "heading" : "5 Discriminative Training",
      "text" : "In contrast to the traditional ML criteria (Equation 1) whose goal is to maximise the log-likelihood of correct examples, DT aims at separating correct examples from competing incorrect examples. Given a training instance (di,Ωi), the training process starts by generating a set of candidate sentences Gen(di) using the current model parameter θ and DA di. The discriminative cost function can therefore be written as\nF (θ) = −E[L(θ)] = − ∑\nΩ∈Gen(di)\npθ(Ω|di)L(Ω,Ωi) (2)\nwhere L(Ω,Ωi) is the scoring function evaluating candidate Ω by taking ground truth Ωi as reference. pθ(Ω|di) is the normalised probability of the candidate and is calculated by\npθ(Ω|di) = exp[γ log p(Ω|di,θ)]∑ Ω′∈Gen(di) exp[γ log p(Ω′|di,θ)] (3)\nγ ∈ [0,∞] is a tuned scaling factor that flattens the distribution for γ < 1 and sharpens it for γ > 1. The unnormalised candidate likelihood log p(Ω|di, θ) is produced by summing token likelihoods from the RNN generator output,\nlog p(Ω|di, θ) = ∑ wt∈Ω log p(wt|di, θ) (4)\nThe scoring function L(Ω,Ωi) can be further generalised to take several scoring functions into account\nL(Ω,Ωi) = ∑ j Lj(Ω,Ωi)βj (5)\nwhere βj is the weight for j-th scoring function. Since the cost function presented here (Equation 2) is differentiable everywhere, back propagation can be applied to calculate the gradients and update parameters directly."
    }, {
      "heading" : "6 Datasets",
      "text" : "In order to test our proposed recipe for training multi-domain language generators, we conducted experiments using four different domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. Datasets for the restaurant and hotel domains have been previously released by Wen et al. (2015b). These were created by workers recruited by Amazon Mechanical Turk (AMT) by asking them to propose an appropriate natural language realisation corresponding to each system dialogue act actually generated by a dialogue system. However, the number of actually occurring DA combinations in the restaurant and hotel domains were rather limited (∼200) and since multiple references were collected for each DA, the resulting datasets are not sufficiently diverse to enable the assessment of the generalisation capability of the different training methods over unseen semantic inputs.\nIn order to create more diverse datasets for the laptop and TV domains, we enumerated all possible combinations of dialogue act types and slots based on the ontology shown in Table 1. This yielded\nabout 13K distinct DAs in the laptop domain and 7K distinct DAs in the TV domain. We then used AMT workers to collect just one realisation for each DA. Since the resulting datasets have a much larger input space but only one training example for each DA, the system must learn partial realisations of concepts and be able to recombine and apply them to unseen DAs. Also note that the number of act types and slots of the new ontology is larger, which makes NLG in both laptop and TV domains much harder."
    }, {
      "heading" : "7 Corpus-based Evaluation",
      "text" : "We first assess generator performance using two objective evaluation metrics, the BLEU-4 score (Papineni et al., 2002) and slot error rate ERR (Wen et al., 2015b). Slot error rates were calculated by averaging slot errors over each of the top 5 realisations in the entire corpus. We used multiple references to compute the BLEU scores when available (i.e. for\nthe restaurant and hotel domains). In order to better compare results across different methods, we plotted the BLEU and slot error rate curves against different amounts of adaptation data. Note that in the graphs the x-axis is presented on a log-scale."
    }, {
      "heading" : "7.1 Experimental Setup",
      "text" : "The generators were implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012), and trained by partitioning each of the collected corpora into a training, validation, and testing set in the ratio 3:1:1. All the generators were trained by treating each sentence as a mini-batch. An l2 regularisation term was added to the objective function for every 10 training examples. The hidden layer size was set to be 100 for all cases. Stochastic gradient descent and back propagation through time (Werbos, 1990) were used to optimise the parameters. In order to prevent overfitting, early stopping was implemented using the validation set.\nDuring decoding, we over-generated 20 utterances and selected the top 5 realisations for each DA according to the following reranking criteria,\nR = −(F (θ) + λERR) (6)\nwhere λ is a tradeoff constant, F (θ) is the cost generated by network parameters θ, and the slot error rate ERR is computed by exact matching of the slot tokens in the candidate utterances. λ is set to a large value (10) in order to severely penalise nonsensical outputs. Since our generator works stochastically and the trained networks can differ depending on the initialisation, all the results shown below were averaged over 5 randomly initialised networks."
    }, {
      "heading" : "7.2 Data Counterfeiting",
      "text" : "We first compared the data counterfeiting (counterfeit) approach with the model fine-tuning (tune) method and models trained from scratch (scratch). Figure 2 shows the result of adapting models between similar domains, from laptop to TV. Because of the parameter sharing in the LM part of the network, model fine-tuning (tune) achieves a better BLEU score than training from scratch (scratch) when target domain data is limited. However, if we apply the data counterfeiting (counterfeit) method, we obtain an even greater BLEU score gain. This is mainly due to the better realisation of unseen slotvalue pairs. On the other hand, data counterfeiting (counterfeit) also brings a substantial reduction in slot error rate. This is because it preserves the co-occurrence statistics between slot-value pairs and realisations, which allows the model to learn good semantic alignments even before adaptation data is introduced. Similar results can be seen in Figure 3, in which adaptation was performed on more disjoint domains: restaurant and hotel joint domain to laptop and TV joint domain. The data counterfeiting (counterfeit) method is still superior to the other methods."
    }, {
      "heading" : "7.3 Discriminative Training",
      "text" : "The generator parameters obtained from data counterfeiting and ML adaptation were further tuned by applying DT. In each case, the models were optimised using two objective functions: BLEU-4 score and slot error rate. However, we used a soft version of BLEU called sentence BLEU as described in Auli\nand Gao (2014), to mitigate the sparse n-gram match problem of BLEU at the sentence level. In our experiments, we set γ to 5.0 and βj to 1.0 and -1.0 for BLEU and ERR, respectively. For each DA, we applied our generator 50 times to generate candidate sentences. Repeated candidates were removed. We treated the remaining candidates as a single batch and updated the model parameters by the procedure described in section 5. We evaluated performance of the algorithm on the laptop to TV adaptation scenario, and compared models with and without discriminative training (ML+DT & ML). The results are shown in Figure 4 where it can be seen that DT consistently improves generator performance on both metrics. Another interesting point to note is that slot error rate is easier to optimise compared to BLEU (ERR→ 0 after DT). This is probably because the sentence BLEU optimisation criterion is only an approximation of the corpus BLEU score used for evaluation."
    }, {
      "heading" : "8 Human Evaluation",
      "text" : "Since automatic metrics may not consistently agree with human perception (Stent et al., 2005), human testing is needed to assess subjective quality. To do this, a set of judges were recruited using AMT. We tested our models on two adaptation scenarios: laptop to TV and TV to laptop. For each task, two systems among the four were compared: training from scratch using full dataset (scrALL), adapting with DT training but only 10% of target domain data (DT-10%), adapting with ML training but only 10% of target domain data (ML-10%), and training from scratch using only 10% of target domain data (scr10%). In order to evaluate system performance in the presence of language variation, each system generated 5 different surface realisations for each input DA and the human judges were asked to score each of them in terms of informativeness and naturalness (rating out of 3), and also asked to state a preference between the two. Here informativeness is defined as whether the utterance contains all the information specified in the DA, and naturalness is defined as whether the utterance could plausibly have been produced by a human. In order to decrease the amount of information presented to the judges, utterances that appeared identically in both systems were filtered out. We tested about 2000 DAs for each scenario distributed uniformly between contrasts except that allowed 50% more comparisons between ML10% and DT-10% because they were close.\nTable 2 shows the subjective quality assessments which exhibit the same general trend as the objective results. If a large amount of target domain data is\navailable, training everything from scratch (scrALL) achieves a very good performance and adaptation is not necessary. However, if only a limited amount of in-domain data is available, efficient adaptation is critical (DT-10% & ML-10% > scr-10%). Moreover, judges also preferred the DT trained generator (DT-10%) compared to the ML trained generator (ML-10%), especially for informativeness. In the laptop to TV scenario, the informativeness score of DT method (DT-10%) was considered indistinguishable when comparing to the method trained with full training set (scrALL). The preference test results are shown in Table 3. Again, adaptation methods (DT10% & ML-10%) are crucial to bridge the gap between domains when the target domain data is scarce (DT-10% & ML-10% > scr-10%). The results also suggest that the DT training approach (DT-10%) was preferred compared to ML training (ML-10%), even though the preference in this case was not statistically significant."
    }, {
      "heading" : "9 Conclusion and Future Work",
      "text" : "In this paper we have proposed a procedure for training multi-domain, RNN-based language generators, by data counterfeiting and discriminative training. The procedure is general and applicable to any datadriven language generator. Both corpus-based evaluation and human assessment were performed. Objective measures on corpus data have demonstrated\nthat by applying this procedure to adapt models between four different dialogue domains, good performance can be achieved with much less training data. Subjective assessment by human judges confirm the effectiveness of the approach.\nThe proposed domain adaptation method requires a small amount of annotated data to be collected offline. In our future work, we intend to focus on training the generator on the fly with real user feedback during conversation."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Tsung-Hsien Wen and David Vandyke are supported by Toshiba Research Europe Ltd, Cambridge Research Laboratory."
    } ],
    "references" : [ {
      "title" : "Decoder integration and expected bleu training for recurrent neural network language models",
      "author" : [ "Auli", "Gao2014] Michael Auli", "Jianfeng Gao" ],
      "venue" : "In Proceedings of ACL. Association for Computational Linguistics",
      "citeRegEx" : "Auli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Auli et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale expected bleu training of phrase-based reordering models",
      "author" : [ "Auli et al.2014] Michael Auli", "Michel Galley", "Jianfeng Gao" ],
      "venue" : "In Proceedings of EMNLP. Association for Computational Linguistics",
      "citeRegEx" : "Auli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Auli et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Statistical language model adaptation: review and perspectives",
      "author" : [ "Jerome R. Bellegarda" ],
      "venue" : "Speech Communication",
      "citeRegEx" : "Bellegarda.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bellegarda.",
      "year" : 2004
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "Olivier Breuleux", "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "Blitzer et al.2007] John Blitzer", "Mark Dredze", "Fernando Pereira" ],
      "venue" : "In Proceedings of ACL. Association for Computational Linguistics",
      "citeRegEx" : "Blitzer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "The ravenclaw dialog management framework: Architecture and systems",
      "author" : [ "Bohus", "Rudnicky2009] Dan Bohus", "Alexander I. Rudnicky" ],
      "venue" : "Computer Speech and Language",
      "citeRegEx" : "Bohus et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bohus et al\\.",
      "year" : 2009
    }, {
      "title" : "Recurrent neural network language model adaptation for multi-genre broadcast speech recognition",
      "author" : [ "Xie Chen", "Tan Tian", "Liu Xunying", "Lanchantin Pierre", "Wan Moquan", "Mark Gales", "Woodland Phil" ],
      "venue" : "Proceedings of InterSpeech",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
      "author" : [ "Michael Collins" ],
      "venue" : "In Proceedings of EMNLP. Association for Computational Linguistics",
      "citeRegEx" : "Collins.,? \\Q2002\\E",
      "shortCiteRegEx" : "Collins.",
      "year" : 2002
    }, {
      "title" : "Training a statistical surface realiser from automatic slot labelling",
      "author" : [ "N. Dethlefs", "H. Hastie", "X. Liu" ],
      "venue" : "In Spoken Language Technology Workshop (SLT), 2014 IEEE,",
      "citeRegEx" : "Cuayhuitl et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cuayhuitl et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning with augmented features for heterogeneous domain adaptation. CoRR, abs/1206.4660",
      "author" : [ "Duan et al.2012] Lixin Duan", "Dong Xu", "Ivor W. Tsang" ],
      "venue" : null,
      "citeRegEx" : "Duan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2012
    }, {
      "title" : "Policy committee for adaptation in multi-domain spoken dialogue systems",
      "author" : [ "Gašić et al.2015] Milica Gašić", "Nikola Mrkšić", "Pei-hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve J. Young" ],
      "venue" : "Proceedings of ASRU",
      "citeRegEx" : "Gašić et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gašić et al\\.",
      "year" : 2015
    }, {
      "title" : "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains",
      "author" : [ "Gauvain", "Lee1994] Jean-Luc Gauvain", "Chin-Hui Lee" ],
      "venue" : "Speech and Audio Processing,",
      "citeRegEx" : "Gauvain et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Gauvain et al\\.",
      "year" : 1994
    }, {
      "title" : "Topic-based language models using em",
      "author" : [ "Gildea", "Hofmann1999] Daniel Gildea", "Thomas Hofmann" ],
      "venue" : "In Proceedings of EuroSpeech",
      "citeRegEx" : "Gildea et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Gildea et al\\.",
      "year" : 1999
    }, {
      "title" : "Maximum expected bleu training of phrase and lexicon translation models",
      "author" : [ "He", "Deng2012] Xiaodong He", "Li Deng" ],
      "venue" : "In Proceedings of ACL. Association for Computational Linguistics",
      "citeRegEx" : "He et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Parser-based retraining for domain adaptation of probabilistic generators",
      "author" : [ "Hogan et al.2008] Deirdre Hogan", "Jennifer Foster", "Joachim Wagner", "Josef van Genabith" ],
      "venue" : "In Proceedings of INLG. Association for Computational Linguistics",
      "citeRegEx" : "Hogan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hogan et al\\.",
      "year" : 2008
    }, {
      "title" : "Individuality and alignment in generated dialogues",
      "author" : [ "Isard et al.2006] Amy Isard", "Carsten Brockmann", "Jon Oberlander" ],
      "venue" : "In Proceedings of INLG. Association for Computational Linguistics",
      "citeRegEx" : "Isard et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Isard et al\\.",
      "year" : 2006
    }, {
      "title" : "Experiments in domain adaptation for statistical machine translation",
      "author" : [ "Koehn", "Schroeder2007] Philipp Koehn", "Josh Schroeder" ],
      "venue" : "In Proceedings of StatMT. Association for Computational Linguistics",
      "citeRegEx" : "Koehn et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2007
    }, {
      "title" : "Discriminative training of language models for speech recognition",
      "author" : [ "Kuo et al.2002] Hong-kwang Kuo", "Eric Fosler-lussier", "Hui Jiang", "Chin-hui Lee" ],
      "venue" : "In Proceedings of ICASSP",
      "citeRegEx" : "Kuo et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kuo et al\\.",
      "year" : 2002
    }, {
      "title" : "Generation that exploits corpus-based statistical knowledge",
      "author" : [ "Langkilde", "Knight1998] Irene Langkilde", "Kevin Knight" ],
      "venue" : "In Proceedings of ACL. Association for Computational Linguistics",
      "citeRegEx" : "Langkilde et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Langkilde et al\\.",
      "year" : 1998
    }, {
      "title" : "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models",
      "author" : [ "Leggetter", "Woodland1995] Chris Leggetter", "Philip Woodland" ],
      "venue" : "Computer Speech and Language",
      "citeRegEx" : "Leggetter et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Leggetter et al\\.",
      "year" : 1995
    }, {
      "title" : "Adaptive natural language generation in dialogue using reinforcement learning",
      "author" : [ "Oliver Lemon" ],
      "venue" : "In Proceedings of SemDial",
      "citeRegEx" : "Lemon.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lemon.",
      "year" : 2008
    }, {
      "title" : "Trainable generation of big-five personality styles through data-driven parameter estimation",
      "author" : [ "Mairesse", "Walker2008] Franois Mairesse", "Marilyn Walker" ],
      "venue" : "In Proceedings of ACL. Association for Computational Linguistics",
      "citeRegEx" : "Mairesse et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mairesse et al\\.",
      "year" : 2008
    }, {
      "title" : "Controlling user perceptions of linguistic style: Trainable generation of personality traits",
      "author" : [ "Mairesse", "Walker2011] François Mairesse", "Marilyn A. Walker" ],
      "venue" : "Computer Linguistics",
      "citeRegEx" : "Mairesse et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mairesse et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic language generation in dialogue using factored language models",
      "author" : [ "Mairesse", "Young2014] François Mairesse", "Steve Young" ],
      "venue" : "Computer Linguistics",
      "citeRegEx" : "Mairesse et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mairesse et al\\.",
      "year" : 2014
    }, {
      "title" : "Phrase-based statistical language generation using graphical models and active learning",
      "author" : [ "Milica Gašić", "Filip Jurčı́ček", "Simon Keizer", "Blaise Thomson", "Kai Yu", "Steve Young" ],
      "venue" : "In Proceedings of ACL. Association",
      "citeRegEx" : "Mairesse et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mairesse et al\\.",
      "year" : 2010
    }, {
      "title" : "Context dependent recurrent neural network language model",
      "author" : [ "Mikolov", "Zweig2012] Tomáš Mikolov", "Geoffrey Zweig" ],
      "venue" : "In Proceedings of SLT",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2012
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Martin Karafit", "Lukáš Burget", "Jan Černocký", "Sanjeev Khudanpur" ],
      "venue" : "In Proceedings of InterSpeech",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Multi-domain dialog state tracking",
      "author" : [ "Mrkšić et al.2015] Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Blaise Thomson", "Milica Gašić", "Peihao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve J. Young" ],
      "venue" : null,
      "citeRegEx" : "Mrkšić et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2015
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Pan", "Yang2010] Sinno Jialin Pan", "Qiang Yang" ],
      "venue" : "IEEE Trans. on Knowledge and Data Engineering",
      "citeRegEx" : "Pan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2010
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "In Proceedings of ACL. Association for Computational Linguistics",
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Recurrent neural network language model adaptation with curriculum learning",
      "author" : [ "Shi et al.2015] Yangyang Shi", "Martha Larson", "Catholijn M. Jonker" ],
      "venue" : null,
      "citeRegEx" : "Shi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2015
    }, {
      "title" : "Trainable sentence planning for complex information presentation in spoken dialog systems",
      "author" : [ "Stent et al.2004] Amanda Stent", "Rashmi Prasad", "Marilyn Walker" ],
      "venue" : "In Proceedings of ACL. Association for Computational Linguistics",
      "citeRegEx" : "Stent et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Stent et al\\.",
      "year" : 2004
    }, {
      "title" : "Evaluating evaluation methods for generation in the presence of variation",
      "author" : [ "Stent et al.2005] Amanda Stent", "Matthew Marge", "Mohit Singhai" ],
      "venue" : "In Proceedings of CICLing",
      "citeRegEx" : "Stent et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Stent et al\\.",
      "year" : 2005
    }, {
      "title" : "A bayesian committee machine",
      "author" : [ "Volker Tresp" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "Tresp.,? \\Q2000\\E",
      "shortCiteRegEx" : "Tresp.",
      "year" : 2000
    }, {
      "title" : "Sequencediscriminative training of recurrent neural networks",
      "author" : [ "P. Doetsch", "S. Wiesler", "R. Schluter", "H. Ney" ],
      "venue" : "In Proceedings of ICASSP",
      "citeRegEx" : "Voigtlaender et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Voigtlaender et al\\.",
      "year" : 2015
    }, {
      "title" : "Training a sentence planner for spoken dialogue using boosting",
      "author" : [ "Owen C Rambow", "Monica Rogati" ],
      "venue" : "Computer Speech and Language",
      "citeRegEx" : "Walker et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2002
    }, {
      "title" : "Individual and domain adaptation in sentence planning for dialogue",
      "author" : [ "Amanda Stent", "Franois Mairesse", "Rashmi Prasad" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR)",
      "citeRegEx" : "Walker et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2007
    }, {
      "title" : "Recent improvements in the cmu spoken language understanding system",
      "author" : [ "Ward", "Issar1994] Wayne Ward", "Sunil Issar" ],
      "venue" : "In Proceedings of Workshop on HLT. Association for Computational Linguistics",
      "citeRegEx" : "Ward et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Ward et al\\.",
      "year" : 1994
    }, {
      "title" : "Personalized language modeling by crowd sourcing with social network data for voice access of cloud applications",
      "author" : [ "Wen et al.2012] Tsung-Hsien Wen", "Hung-Yi Lee", "TaiYuan Chen", "Lin-Shan Lee" ],
      "venue" : "Proceedings of SLT",
      "citeRegEx" : "Wen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2012
    }, {
      "title" : "Recurrent neural network based language model personalization by social network crowdsourcing",
      "author" : [ "Wen et al.2013] Tsung-Hsien Wen", "Aaron Heidel", "Hung yi Lee", "Yu Tsao", "Lin-Shan Lee" ],
      "venue" : "Proceedings of InterSpeech",
      "citeRegEx" : "Wen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2013
    }, {
      "title" : "Semantically conditioned lstmbased natural language generation for spoken dialogue systems",
      "author" : [ "Wen et al.2015b] Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "Pei-Hao Su", "David Vandyke", "Steve Young" ],
      "venue" : "In Proceedings of EMNLP. Association",
      "citeRegEx" : "Wen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "Paul J Werbos" ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Werbos.,? \\Q1990\\E",
      "shortCiteRegEx" : "Werbos.",
      "year" : 1990
    }, {
      "title" : "Pomdpbased statistical spoken dialog systems: A review",
      "author" : [ "Young et al.2013] Steve Young", "Milica Gašić", "Blaise Thomson", "Jason D. Williams" ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Young et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2013
    }, {
      "title" : "Kl-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition",
      "author" : [ "Yu et al.2013] Dong Yu", "Kaisheng Yao", "Hang Su", "Gang Li", "Frank Seide" ],
      "venue" : "Proceedings of ICASSP",
      "citeRegEx" : "Yu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 44,
      "context" : "work which requires a minimal amount of human intervention (Young et al., 2013).",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "Mrkšić et al. (2015) showed improved performance in belief tracking by training a general model and adapting it to specific domains.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 35,
      "context" : "(2015), in which a Bayesian committee machine (Tresp, 2000) was used to model policy learning in a multi-domain SDS regime.",
      "startOffset" : 46,
      "endOffset" : 59
    }, {
      "referenceID" : 38,
      "context" : "In past decades, adaptive NLG has been studied from linguistic perspectives, such as systems that learn to tailor user preferences (Walker et al., 2007), convey a specific personality trait (Mairesse",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "and Walker, 2008; Mairesse and Walker, 2011), or align with their conversational partner (Isard et al., 2006).",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 37,
      "context" : "This class of statistical generators can learn generation decisions directly from dialogue act (DA)-utterance pairs without any semantic annotations (Mairesse and Young, 2014) or hand-coded grammars (Langkilde and Knight, 1998; Walker et al., 2002).",
      "startOffset" : 199,
      "endOffset" : 248
    }, {
      "referenceID" : 41,
      "context" : "Many existing adaptation approaches (Wen et al., 2013; Shi et al., 2015; Chen et al., 2015) can be directly applied due to the ar X iv :1 60 3.",
      "startOffset" : 36,
      "endOffset" : 91
    }, {
      "referenceID" : 32,
      "context" : "Many existing adaptation approaches (Wen et al., 2013; Shi et al., 2015; Chen et al., 2015) can be directly applied due to the ar X iv :1 60 3.",
      "startOffset" : 36,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "Many existing adaptation approaches (Wen et al., 2013; Shi et al., 2015; Chen et al., 2015) can be directly applied due to the ar X iv :1 60 3.",
      "startOffset" : 36,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Domain adaptation was first addressed by Hogan et al. (2008) using a generator based on the Lexical Functional Grammar (LFG) fstructures (Kaplan and Bresnan, 1982).",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 28,
      "context" : "flexibility of the underlying RNN language model (RNNLM) architecture (Mikolov et al., 2010).",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "By optimising directly against the desired objective function such as BLEU score (Auli and Gao, 2014) or Word Error Rate (Kuo et al., 2002), the model can explore its output space and learn to discriminate between good and bad hypotheses.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 38,
      "context" : "However, their published work has focused on tailoring user preferences (Walker et al., 2007) and mimicking personality traits (Mairesse and Walker, 2011).",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "The most relevant work was done by Hogan et al. (2008). They showed that an LFG f-structure based generator could yield better performance when trained on in-domain sentences paired with pseudo parse tree inputs generated from a state-of-the-art, but out-ofdomain parser.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "The most relevant work was done by Hogan et al. (2008). They showed that an LFG f-structure based generator could yield better performance when trained on in-domain sentences paired with pseudo parse tree inputs generated from a state-of-the-art, but out-ofdomain parser. The SPoT-based generator proposed by Walker et al. (2002) has the potential to address domain adaptation problems.",
      "startOffset" : 35,
      "endOffset" : 330
    }, {
      "referenceID" : 15,
      "context" : "The most relevant work was done by Hogan et al. (2008). They showed that an LFG f-structure based generator could yield better performance when trained on in-domain sentences paired with pseudo parse tree inputs generated from a state-of-the-art, but out-ofdomain parser. The SPoT-based generator proposed by Walker et al. (2002) has the potential to address domain adaptation problems. However, their published work has focused on tailoring user preferences (Walker et al., 2007) and mimicking personality traits (Mairesse and Walker, 2011). Lemon (2008) proposed a Reinforcement Learning (RL) framework in which policy and NLG components can be jointly optimised and adapted based on online user feedback.",
      "startOffset" : 35,
      "endOffset" : 556
    }, {
      "referenceID" : 15,
      "context" : "The most relevant work was done by Hogan et al. (2008). They showed that an LFG f-structure based generator could yield better performance when trained on in-domain sentences paired with pseudo parse tree inputs generated from a state-of-the-art, but out-ofdomain parser. The SPoT-based generator proposed by Walker et al. (2002) has the potential to address domain adaptation problems. However, their published work has focused on tailoring user preferences (Walker et al., 2007) and mimicking personality traits (Mairesse and Walker, 2011). Lemon (2008) proposed a Reinforcement Learning (RL) framework in which policy and NLG components can be jointly optimised and adapted based on online user feedback. In contrast, Mairesse et al. (2010) has proposed using active learning to mitigate the data sparsity problem when training datadriven NLG systems.",
      "startOffset" : 35,
      "endOffset" : 744
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, Cuayhuitl et al. (2014) trained statistical surface realisers from unlabelled data by an automatic slot labelling technique.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "In general, feature-based adaptation is perhaps the most widely used technique (Blitzer et al., 2007; Pan and Yang, 2010; Duan et al., 2012).",
      "startOffset" : 79,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "In general, feature-based adaptation is perhaps the most widely used technique (Blitzer et al., 2007; Pan and Yang, 2010; Duan et al., 2012).",
      "startOffset" : 79,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "In contrast, model-based adaptation is particularly useful for language modeling (LM) (Bellegarda, 2004).",
      "startOffset" : 86,
      "endOffset" : 104
    }, {
      "referenceID" : 40,
      "context" : "personalised speech recognition (Wen et al., 2012).",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "A feature augmented RNNLM was first proposed by Mikolov and Zweig (2012), but later applied to multi-genre broadcast speech recognition (Chen et al., 2015) and personalised language modeling (Wen et al.",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 41,
      "context" : ", 2015) and personalised language modeling (Wen et al., 2013).",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 45,
      "context" : "However, careful regularisation is often necessary (Yu et al., 2013).",
      "startOffset" : 51,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "A feature augmented RNNLM was first proposed by Mikolov and Zweig (2012), but later applied to multi-genre broadcast speech recognition (Chen et al., 2015) and personalised language modeling (Wen et al., 2013). These methods are based on finetuning existing network parameters on adaptation data. However, careful regularisation is often necessary (Yu et al., 2013). In a slightly different area, Shi et al. (2015) applied curriculum learning to RNNLM adaptation.",
      "startOffset" : 137,
      "endOffset" : 415
    }, {
      "referenceID" : 8,
      "context" : "Discriminative training (DT) (Collins, 2002) is an alternative to the maximum likelihood (ML) criterion.",
      "startOffset" : 29,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "It has been successfully applied to many research problems, such as speech recognition (Kuo et al., 2002; Voigtlaender et al., 2015) and MT (He and Deng, 2012; Auli et al.",
      "startOffset" : 87,
      "endOffset" : 132
    }, {
      "referenceID" : 36,
      "context" : "It has been successfully applied to many research problems, such as speech recognition (Kuo et al., 2002; Voigtlaender et al., 2015) and MT (He and Deng, 2012; Auli et al.",
      "startOffset" : 87,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and MT (He and Deng, 2012; Auli et al., 2014).",
      "startOffset" : 15,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and MT (He and Deng, 2012; Auli et al., 2014). Recently, Auli and Gao (2014) trained an RNNLM with a DT objective and showed improved performance on an MT task.",
      "startOffset" : 35,
      "endOffset" : 85
    }, {
      "referenceID" : 37,
      "context" : "work (Hochreiter and Schmidhuber, 1997) for language generation which has previously been shown capable of learning generation decisions from paired DA-utterances end-to-end without a modular pipeline (Walker et al., 2002; Stent et al., 2004).",
      "startOffset" : 201,
      "endOffset" : 242
    }, {
      "referenceID" : 33,
      "context" : "work (Hochreiter and Schmidhuber, 1997) for language generation which has previously been shown capable of learning generation decisions from paired DA-utterances end-to-end without a modular pipeline (Walker et al., 2002; Stent et al., 2004).",
      "startOffset" : 201,
      "endOffset" : 242
    }, {
      "referenceID" : 40,
      "context" : "It was shown in Wen et al. (2015b) that these reading",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 40,
      "context" : "Datasets for the restaurant and hotel domains have been previously released by Wen et al. (2015b). These were created by workers recruited by Amazon Mechanical Turk (AMT) by asking them to propose an appropriate natural language realisation corresponding to each system dialogue act actually generated by a dialogue system.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 31,
      "context" : "We first assess generator performance using two objective evaluation metrics, the BLEU-4 score (Papineni et al., 2002) and slot error rate ERR (Wen et al.",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "The generators were implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012), and trained by partitioning each of the collected corpora into a training, validation, and testing set in the ratio 3:1:1.",
      "startOffset" : 57,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "The generators were implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012), and trained by partitioning each of the collected corpora into a training, validation, and testing set in the ratio 3:1:1.",
      "startOffset" : 57,
      "endOffset" : 102
    }, {
      "referenceID" : 43,
      "context" : "Stochastic gradient descent and back propagation through time (Werbos, 1990) were used to optimise the parameters.",
      "startOffset" : 62,
      "endOffset" : 76
    }, {
      "referenceID" : 34,
      "context" : "with human perception (Stent et al., 2005), human testing is needed to assess subjective quality.",
      "startOffset" : 22,
      "endOffset" : 42
    } ],
    "year" : 2016,
    "abstractText" : "Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain.",
    "creator" : "LaTeX with hyperref package"
  }
}