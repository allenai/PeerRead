{
  "name" : "1612.00882.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Success Probability of Exploration: a Concrete Analysis of Learning Efficiency",
    "authors" : [ "Liangpeng Zhang", "Ke Tang", "Xin Yao" ],
    "emails" : [ "udars@mail.ustc.edu.cn", "ketang@ustc.edu.cn", "x.yao@cs.bham.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Reinforcement learning, exploration efficiency, learning theory, analytical framework."
    }, {
      "heading" : "1. Introduction",
      "text" : "Exploration is an essential process for Reinforcement Learning (RL) agents to resolve uncertainty (Sutton and Barto, 1998). In an initially unknown environment, a learning agent has to explore through different states and actions to gather information, so that better policies can be discovered accordingly via its planning process. Most RL algorithms include a specific part, often called an exploration strategy, that explicitly deals with exploration. Numerous exploration strategies have been designed and proposed in the literature, and some of the most popular ones among them are ε-greedy, Boltzmann action selection, Explicit Explore or Exploit (Kearns and Singh, 2002), R-MAX (Brafman and Tennenholtz, 2002), Upper Confidence RL (Jaksch et al., 2010), and Bayesian approaches (Vlassis et al., 2012).\nar X\niv :1\n61 2.\n00 88\n2v 1\n[ cs\n.L G\nThere have been several long-standing crucial questions regarding exploration faced by every RL practitioners. These questions can be categorized into three groups as follows.\n(Q1) Exploration parameter setting For every common exploration strategy, there is at least one parameter that balances exploration and exploitation by controlling the activeness of exploration. Very often the best setting of the exploration parameter varies in different learning tasks, and whether knowing it or not before running the algorithm has a great impact on the overall efficiency. Then, given a learning task, what is the corresponding best parameter setting for the exploration strategy? Is it possible to be found out without time-consuming trial-and-error, or even without running the learning algorithm at all?\n(Q2) Situation analysis RL practitioners often come into the situation where a learning algorithm has been executed for a while on a RL task, but the result is not satisfactory. In such cases, RL practitioners have to decide what to do next in order to improve the situation. Should the algorithm simply be kept running for some more time steps? Or should the practitioners tune its exploration parameter, or even reconsider its state and action representations? Is there any metric or descriptive statistics available that can be relied on to make the decision?\n(Q3) Hardness of exploration There have been many real-world applications where a vanilla algorithm could discover a surprisingly good policy within limited steps in an apparently complicated environment. There are also many seemingly simple environments that eventually turned out to be very hard to explore. RL practitioners have to decide how much resource should be allocated to the agent exploring the environment based on how difficult it is. Is there any practical methodology able to describe and quantify the hardness of exploration in different learning tasks? How to allocate resource for exploration accordingly?\nTraditionally, these questions have been dealt with under different frameworks and approaches, but unfortunately none of them could give a solution practically useful and widely applicable. Many solutions appear in the form of empirical rules extracted from experiences. It often happens that these rules do not work well, and the time-consuming trail-and-error process has to be performed to find the best answer instead. For example, in the scenario of exploration parameter setting where practitioner hopes to find the best value of ε for the ε-greedy strategy, the default value ε = 0.1 from the textbook (Sutton and Barto, 1998) will usually be tried first. If it leads to poor performance, then the practitioner has to manually try out values with different magnitude, say 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 etc., to decide the best one.\nIn addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al., 2010). In both of the frameworks, asymptotic bounds of some performance metric (sample complexity and regret, respectively) are derived for a given exploration strategy, and its parameter settings that lead to the corresponding bounds are specified. Many well-known strategies, for example R-MAX and its variants\n(e.g. MoR-MAX (Szita and Szepesvári, 2010), V-MAX (Rao and Whiteson, 2012), ICR and ICV (Zhang et al., 2015)), Model-Based Interval Estimation (Strehl and Littman, 2005), and UCRLγ (Lattimore and Hutter, 2014), have been proved to have sample complexity bounds polynomial to the scale parameters of the learning task. The UCRL families are also proved to have regret bounds sublinear to the horizon of the cumulative rewards (Jaksch et al., 2010; Ortner and Ryabko, 2012).\nThe main drawbacks of these analyses is that their theoretical results are not sufficiently relevant to the practical needs. For example, the PAC theory for R-MAX (Strehl et al., 2009) requires its parameter m to be set polynomial to the scale parameters of the learning task so that its sample complexity can be polynomial as well. However, in practice m is usually fixed to some value around 10-20 (Strehl and Littman, 2004) regardless of the scale of the task, which violates the basic condition of the PAC theory. Meanwhile, the PAC theory does not provide any prediction of the performance of R-MAX with its m fixed to small values like 10-20. This results in a strange dilemma where practitioners have to choose one between theoretical performance guarantee and actual efficiency, and in most cases the latter is chosen, leaving the former invalid in practice. In Zhang et al. (2015), some workarounds are proposed so that the practitioners are not forced to discard theoretical guarantees in exchange for efficiency. Nevertheless this cannot turn these existing guarantees to be more practically relevant and useful.\nIn general machine learning setting, situation analysis is often conducted via observing the learning curve, i.e. the plot of generalization error of the learned model against the size of the training dataset or the number of executed iterations (Perlich, 2011). This approach can also be applied to Reinforcement Learning by plotting the current total reward or the expected cumulative reward over time (Sutton and Barto, 1998). However, the former does not directly represent the goodness of the learned policy, while evaluating the latter is actually a value prediction problem requiring an additional independent learning process, which can be as costly as the original learning process. Even if such curves are accessible to practitioners, they are likely to be in a zigzag style in practice, and the decisions for improving the situation still have to be made according to experiences and domain knowledge.\nThere has been some works related to the hardness questions, such as action gap (Farahmand, 2011) and distribution-norm (Maillard et al., 2014). The action gap captures the hardness of planning rather than exploration, and thus does not provide a direct answer to the hardness questions of exploration. The distribution-norm is a hardness metric covering both planning and exploration parts of learning. This metric explains why some common RL benchmarks are relatively easy in spite of having moderate hardness in terms of usual metrics such as problem size. However, due to its abstractness, practitioners may find it difficult to turn this metric into any prediction of actual exploration behaviour of a learning agent in a given environment, or further into any practical advice on allocating resource for exploration.\nIn fact, the three groups of questions aforementioned can be better answered together rather than separately, because they are fundamentally interrelated. The questions of situation analysis can be answered if we are able to predict what result can be expected for a given algorithm, executing under every reasonable parameter settings, on a given task for a certain time steps. Being able to predict this directly provides us the answer to the ques-\ntions of parameter setting, since what we need to do then is just to choose the parameter that yields best expected results within the time step budget. The hardness questions can also be answered by comparing the expected goodness of results under the same setting of algorithm, its parameters, and the time step budget.\nThis paper proposes the success probability of exploration, or success probability in short, as a unified answer to the three groups of questions. The success probability of exploration is the probability P of an exploration strategy A under parameter setting θ yields a desired result E on learning task M at time step τ . We provide rigorous mathematical formulation of success probabilities, and present that knowing these probabilities is sufficient to answer all three groups of questions.\nThe success probability of exploration can be practically useful only if we are able to estimate it in prior to executing the learning algorithm, or otherwise it will not free practitioners from tedious trial-and-error processes. Therefore, we establish a concrete approach of assessing the success probability, by which its closed-form expression can be derived for certain prototype learning tasks. In addition, we provide a practical approximation to the value of success probability, so that practitioners can use it in actual situations. We then present our empirical results, which not only verify the correctness of our approach, but also display the high accuracy of our approximation. Although our analyses are made mainly on the prototype tasks, we show that the results can be applied to a wider range of general domains and algorithms, which is also supported by the empirical results.\nThe rest of this paper is organized in three parts. The first part consists of three sections. In Section 2, we introduce the preliminary concepts of reinforcement learning that is relevant to this paper. In Section 3, we formulate the success probability of exploration, compare it to the traditional PAC formulation, and provide its several elementary yet crucial properties. In Section 4, we discuss how the success probability can be used to answer the three groups of questions related to exploration.\nThen it comes to the second part, Sections 5, 6 and 7, where we elaborate our concrete approach to computing the success probability of exploration. Specifically, in Section 5, we introduce the chain perspective, which helps transform a general RL task to a more tractable one in the form of chain MDP. In Section 6, we derive the closed-form expression of success probability for a prototype exploration strategy running in chain MDPs. Then in Section 7, we provide a practical approximation to the value of success probability.\nThe last part contains three sections as well. In Section 8, we present our empirical results to justify our approach. Readers may be more interested in Section 9, where we demonstrate how our method can be applied to a general RL task, and then provide a short summary of our whole approach in the form of practice guide. Finally in Section 10, we discuss our approach at the macro level, and point out possible future researches relevant to this work."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "In this paper we follow the standard reinforcement learning framework in Sutton and Barto (1998), where an agent continuously interacts with a stochastic environment, learns its dynamic properties, and searches for the optimal policy that could lead to maximum expected cumulative rewards. The environment here is formulated as a finite discounted Markov\nDecision Process (MDP) M = (S,A, P,R, γ), where S and A are finite sets of states and actions respectively, P is the transition function such that for all s, s′ ∈ S and a ∈ A, P (s′|s, a) := P(st+1 = s′|st = s, at = a) gives the fixed probability of state transition from state s to s′ under action a at arbitrary time t, R : S × A × S 7→ [0,+∞) is the reward function representing the numeric rewards of the transitions, and γ ∈ (0, 1) is a constant called the discount factor.\nA (deterministic) policy π : S 7→ A maps each state to an action that should be taken by the agent when in that state. The state value function of policy π, denoted by V π, maps each state to the expected discounted cumulative reward the agent could get starting from that state and following policy π. Given two arbitrary policies π and π′, we write V π = V π ′ to indicate that their state value functions satisfy V π(s) = V π ′ (s) for all s ∈ S.\nLet Π denotes the set of all possible policies. By Bellman equation, the following holds for any state s ∈ S and any policy π ∈ Π:\nV π(s) = ∑ s′∈S P (s′|s, π(s))(R(s, π(s), s′) + γV π(s′)). (1)\nA policy π∗ ∈ Π is called an optimal policy if V π∗(s) = maxπ∈Π V π(s) holds for all s ∈ S, and the set of all optimal policies is denoted by Π∗. There can be more than one optimal policies for some MDP. However, for any given MDP, its optimal state value function V π ∗ is always unique by definition, and hence is often simply written as V ∗. The optimal state values satisfy Bellman optimality equation: for any state s ∈ S,\nV ∗(s) = max a∈A ∑ s′∈S P (s′|s, a)(R(s, a, s′) + γV ∗(s′)). (2)\nIf the environment is fully known by the agent, that is, the true transition function P and reward function R are given, then the optimal state values V ∗ can be computed by planning algorithms designed based on Equations 1 or 2. Some popular planning algorithms, for example Value Iteration (Puterman, 1994), have been proved that their calculated state values converge to the true optimal values in the limit, or to the near-optimal ones in polynomial time under some assumptions (Littman et al., 1995).\nIn more realistic reinforcement learning settings, the environment is initially unknown to the agent, and thus its dynamic properties must be estimated from the observations. At each time step t, the agent receives an observation (st, at, st+1, rt) which represents the environment transiting from state st to st+1 under the action at while providing the agent an immediate reward of rt. Obviously, the agent has τ observations (s1, a1, s2, r1), (s2, a2, s3, r2), ..., (sτ , aτ , sτ+1, rτ ) after τ steps. This sequence of observations is called a trajectory (of length τ) and is denoted by ψτ .\nWe define the visit numbers of state-action pairs and transitions on the trajectory ψτ respectively as\nNψτs,a := τ∑ t=1 1(st = s)1(at = a), and N ψτ s,a,s′ := τ∑ t=1 1(st = s)1(at = a)1(st+1 = s ′),\nwhere 1(X) is the indicator function which equals 1 if expression X is true and 0 otherwise. Then the transition probabilities can be estimated straightforwardly by P̂ψτ (s′|s, a) =\nNψτs,a,s′/N ψτ s,a . The immediate rewards, on the other hand, are deterministic with respect to the transitions, and thus estimating them is trivial (i.e. R̂ψτ (s, a, s′) = rt such that (st, at, st+1) = (s, a, s\n′)). The resulting tuple M̂ψτ = (S,A, P̂ψτ , R̂ψτ , γ) is called an estimated model of the environment.\nTo simplify the notations, we remove ψτ from the visit numbers and estimated models and write them as Ns,a, Ns,a,s′ , P̂ , R̂, and M̂ when there is no ambiguity from the context. Further, we use 〈Ns,a〉 and 〈Ns,a,s′〉 to collectively represent visit numbers of all stateaction pairs and transitions. In such cases, the subscripts do not refer to any specific state or actions.\nA model-based reinforcement learning agent explicitly maintains an estimated model M̂ of the true MDP M , and uses M̂ instead of the unavailable M as the input of its planning algorithm. Intuitively, if more observations are used to estimate the transition probabilities, then the resulting M̂ will probably be more accurate, and the output of the planning algorithm (optimal or near-optimal policy with respect to M̂) will possibly get closer to the true ones with respect to M . The model-free learning algorithms such as Temporal Difference and Q-Learning (Sutton and Barto, 1998), on the other hand, do not build models explicitly, but use Equations 1, 2 or their modified versions to update the estimated values directly. Still, they can be seen as model-based learning agents that utilize fast but degraded planning algorithms, and it is clear that they benefit from abundant observations just in the same way as model-based agents.\nThe key problem here is, the observations often come at a price, and in many real-world applications that involve interactions between concrete objects (e.g. Robotics), they can be even more expensive than the computational power. Therefore, it is crucial that the agent choose the action wisely, avoiding unnecessary observations of the easily-estimated transitions (e.g. the deterministic ones) or the less relevant ones (for example, playing with a cat in a cooking task), and biasing to the more uncertain and relevant transitions. By doing so, more useful information can be gathered within fewer steps, and consequently high-quality policies are likely to be discovered earlier.\nThis bias or tactic is often referred as an exploration strategy, denoted A, which can be formulated as a function A(ψt−1, st) mapping from the current trajectory ψt−1 and state st to an action at. It can also be viewed as a non-stationary policy followed by the agent during learning that changes over time. The exploration strategy provides the agent a useful heuristic regarding how many observations should be collected for each state-action pair, and in what order these state-action pairs should be visited."
    }, {
      "heading" : "3. The Success Probability of Exploration",
      "text" : "As discussed in the last section, the observations usually don’t come for free, and hence it is critical to know the relation between the cost, in terms of observations, and the outcome, namely the goodness of the policy derived from these observations. In this section, we first formulate this cost-outcome relation through the success probability of exploration, then compare it to the PAC analysis, and finally highlight some elementary yet useful properties of the success probability of exploration."
    }, {
      "heading" : "3.1 Formulating the Cost-outcome Relation",
      "text" : "The most straightforward representation of the cost in this context is the total number of observations. It is the total number of time steps τ as well, since the agent receives exactly one observation at each time step. Further, it is also the sum of visit numbers due to the fact that τ = ∑ s,aNs,a = ∑ s,a,s′ Ns,a,s′ .\nIn some more complicated settings, acquiring observations for certain state-action pairs might be more expensive than the others. In this case, the cost can be represented as a weighted sum of visit numbers ∑ s,aws,aNs,a. However, this case can be transformed to the non-weighted one by augmenting the original MDP with sequences of trivial transitions corresponding to the weights. For example, if the weight is 1 for all state-action pairs except for (s0, a0), which has a weight of 4, then it is mathematically equivalent to the non-weighted case where (s0, a0) always leads to some additional (s ′ 0, a ′ 0), (s ′′ 0, a ′′ 0), and (s ′′′ 0 , a ′′′ 0 ) before transiting to its original destination. Therefore, it is sufficient to focus on the non-weighted case.\nThe formulation of the outcome should reflect the purpose of reinforcement learning, that is, looking for (near-)optimal policies. Naturally, the outcome can be said to be desirable if and only if a learning algorithm outputs a (near-)optimal policy as its result of learning. Therefore, we define the concept of success used throughout this paper as follows.\nDefinition 1 A run of learning algorithm is said to be an ε-success if and only if its output policy π̂∗ satisfies π̂∗ ∈ Πε where ε is a non-negative number and Πε := {π|∀s ∈ S, V π(s) ≥ V ∗(s)− ε}.\nRemark 2 As a special case, when ε = 0, the algorithm has to output an optimal policy π̂∗ ∈ Π∗ where Π∗ := {π|∀s ∈ S, V π(s) = V ∗(s)} to achieve an ε-success. We say this kind of ε-success as a strict success.\nRemark 3 Another kind of special case is that the algorithm achieves a success by exactly choosing one specific desired policy π as its output. We say this kind of success as a πsuccess.\nTo simplify mathematical notation, we write Eε to represent an ε-success event, and write E∗ to represent a strict success event. We also write Eπ to indicate a π-success event which ends up with the output being π.\nAlthough many reinforcement learning algorithms are guaranteed to converge to optimal policies if all state-action pairs have been visited infinitely many times (Sutton and Barto, 1998), in reality the resources for acquiring observations are far less than infinite. With a limited budget of observations, the estimates of transitions made by the agent can be inaccurate. In that case, the algorithm can sometimes fails to discover a (near-)optimal policy, and hence the probability it achieves a success can be less than 1. We call this probability the success probability of exploration; the formal definition is as follows.\nDefinition 4 Let M be an MDP, A be an exploration strategy, θ be its parameters, τ be a number of time steps, and Eε be an ε-success event. Then the ε-success probability of exploration for A(θ) running τ steps in M is defined as:\nPεM,A(θ),τ := P(E ε|M,A(θ), τ).\nIn the special case where ε = 0, which corresponds to the case of strict success, we write P∗M,A(θ),τ . The difference between the near-optimal success and strict success is trivial from the mathematical perspective, since the choice of the value of ε has impact only on the set of desired policies (Πε or Π∗) in both cases. Therefore, if the exact value of ε is not important for the discussion, we can drop them from PεM,A(θ),τ and write PM,A(θ),τ for convenience. Additionally, if the subscripts are clear from the context, then we can drop them as well and simply write Pεθ,τ , Pε, or P.\nRemark 5 For the special case of π-success, we write PπM,A(θ),τ to represent the corresponding success probability, which equals to P(Eπ|M,A(θ), τ). In this case, the exact value of ε is irrelevant because the desired policy has already been explicitly specified.\nThe success probability of exploration defined above provide us a precise description of the cost-outcome relation we are interested. The number of time steps τ represents the cost, the success event corresponds to the outcome, and the conditional probability P connects them together. We put emphasis on exploration rather than planning here because it is the exploration strategy A(θ) that decides the distribution of the observations 〈Ns,a〉 and 〈Ns,a,s′〉, which is the unique source of information that the planning algorithms relies on to make plans."
    }, {
      "heading" : "3.2 Comparison to the Traditional PAC Analysis",
      "text" : "Our formulation is inspired by the notion of Probably Approximately Correct (PAC, Valiant (1984); Fiechter (1994); Kakade (2003)) which tries to figure out how many observations are needed to be ε-optimal with probability at least 1 − δ. However, there are several key differences between our new formulation and the existing PAC notions when applied to reinforcement learning.\nFirstly, the traditional PAC analysis provides results in the form of asymptotic bounds of sample complexity. There lies a big gap between the best upper and lower bounds (Szita and Szepesvári, 2010; Lattimore and Hutter, 2014) been discovered. This means that either the upper bound is too loose, or the specific hard problem used to derive the lower bound is still not difficult enough. Whichever the case, the current gap makes it difficult to definitely compare the efficiency between algorithms. Therefore, a non-asymptotic approach of analysis can be more helpful in practice. Our formulation and approach to the success probability of exploration, as can be seen in the later sections, are not based on asymptotic analysis. By applying our approach, concrete relations between the cost and outcome can be obtained, which can be more convenient and functional compared to loose bounds.\nSecondly, the traditional PAC analysis focuses on the theoretical sample complexity bounds for the most difficult MDP under a given setting of scale parameters (|S|, |A|, γ, etc.). However, the actual MDPs most RL practitioners encounter in real-world applications may not be as difficult as the ones used for PAC analysis. This leads to a paradoxical situation where, if one decide to set the exploration parameters according to the PAC theories, then the learning agent is very likely to over-explore as if it is in the most difficult MDP, resulting in poor actual performance despite its PAC guarantee (Kolter and Ng, 2009; Zhang et al., 2015). Our formulation directly addresses the success probability with respect to the given MDP, rather than to the hardest one, thus avoids this problem.\nThirdly, the sample complexity assessed in PAC-MDP analysis does not actually correspond to the total number of observations required to achieve an (ε; δ)-PAC. Instead, it corresponds to the number of locally non-ε-optimal steps, that is, the steps where the current policy has a non-ε-optimal value at the current state. There can be arbitrarily many locally ε-optimal steps between any two successive locally non-ε-optimal steps. In extreme cases, an agent with a polynomial sample complexity may still requires infinitely many steps to discover a globally ε-optimal policy (i.e. has ε-optimal values at all states). This is not desirable if a locally ε-optimal step is as costly as a locally non-ε-optimal step, which is more likely to be the case in reality. Therefore, it is crucial to have some theoretical result revealing the relation between the total number of steps, including both locally ε-optimal and non-ε-optimal ones, and the outcome, which is exactly what we are trying to achieve in this paper.\nFinally, the optimality discussed here is mathematically stronger than the ones in the literature concerning PAC analyses in RL. The PAC optimality in Fiechter (1994) refers to a local ε-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local ε-optimality along the states the agent actually visits during learning. In our formulation, the output policy has to be ε-optimal in all states of the MDP in order to be ε-successful. Therefore, an ε-success must be ε-optimal in the PAC framework of Fiechter (1994) and Kakade (2003), but the converse is not necessarily correct."
    }, {
      "heading" : "3.3 Basic Properties of the Success Probability of Exploration",
      "text" : "To provide some more clear ideas to the readers about the success probability of exploration, we introduce some of its elementary yet useful properties in this subsection. The first lemma below states the relationship between Pε and Pπ.\nLemma 6 (First-level Decomposition) Pεθ,τ = ∑ π∈Πε Pπθ,τ .\nProof Because the planning algorithms output only one policy at a time, success events Eπ and Eπ\n′ for any π 6= π′ cannot happen at the same time. Therefore, we have Eε =⋃\nπ∈Πε E π, and hence Pεθ,τ = P( ⋃ π∈Πε E π|θ, τ) = ∑ π∈Πε P(Eπ|θ, τ) = ∑ π∈Πε Pπθ,τ .\nLemma 6 actually states that the ε-success probability equals to the sum of π-success probabilities ending up with each policy in Πε. The most intriguing part of this lemma is, it shows that the ε-success probability Pεθ,τ in only determined by the optimality parameter ε in the set of relevant policies Πε. Therefore, changing Πε to any arbitrary set of interested policies does not make this lemma invalid, even if the policies in the new set share no ε-optimality, as stated in Remark 7.\nRemark 7 The set of ε-optimal policies Πε can be changed to any subset of policies Π̃ ⊆ Π and the first-level decomposition still holds, that is,\nPΠ̃θ,τ = ∑ π∈Π̃ Pπθ,τ .\nThis can be particularly convenient in real-world applications, because it is often impossible to figure out what the ε-optimality is for the desired policies. Actually, it is often the case that RL practitioners have a set of desired policies in their mind, but cannot specify what ε should be to describe this set. By Lemma 6, the practitioners only need to work out all π-success probabilities for the desired policies, and add them together to obtain the overall success probability, avoiding the necessity of specifying ε. Nevertheless, we will continue using Πε to refer to the set of desired policies in the rest of this paper.\nTo determine the π-success probabilities, we need to know how exactly the planning algorithm chooses its output policy. Therefore, the following assumptions about the algorithm discussed in this paper has to be introduced in order to specify its behaviour.\nAssumption 8 The planning algorithm outputs exactly one policy π that satisfies V̂ π = V̂ ∗, where V̂ ∗(s) = maxπ∈Π V̂\nπ(s) for any s. If there is more than one policy satisfying V̂ π = V̂ ∗, the planning algorithm randomly and uniformly choose one of them as its output.\nAssumption 9 If a state-action pair is never visited, then the output policy of the planning algorithm will not contain that state-action pair.\nThe first assumption is rather mild and reasonable because it actually holds for most popular planning algorithms. The second one is also reasonable because without any information about a state-action pair, the agent should have no idea how good it is, and hence should not choose it if it has other better options. This assumption is violated when generalization or prior knowledge is involved in planning. By utilizing such techniques and knowledge, the algorithm may have non-trivial estimates to the values of the unvisited state-action pairs. This issue will be dealt with in Section 5, where we propose the chain perspective to include the effect of generalization into the representation of environment.\nNow it is clear that in order to obtain a π-success, the agent must visit all of the stateaction pairs that are related to π at least once. This leads to the lemma which we call the second-level decomposition of success probability as below.\nDefinition 10 A traverse event of π, denoted Eπtrav, is said to have occurred if and only if Ns,π(s) ≥ 1 holds for all s ∈ S.\nLemma 11 (Second-level decomposition)\nPπθ,τ = P(Eπ|Eπtrav, θ, τ)P(Eπtrav|θ, τ).\nProof By the law of total probability, it holds that Pπθ,τ = P(Eπ|Eπtrav, θ, τ)P(Eπtrav|θ, τ) + P(Eπ|¬Eπtrav, θ, τ)P(¬Eπtrav|θ, τ), where ¬Eπtrav represents that the traverse event of π is not happened. However, if the traverse event of π is not happened, then there exists some state s such that Ns,π(s) = 0, and hence the action of the output policy at state s will never be π(s). This leads to P(Eπ|¬Eπtrav, θ, τ) = 0, and hence the theorem.\nLemma 11 allows us to compute the probability of successful traverse events and the conditional success probability separately, then combine them together to get the unconditional success probability. As can be seen in later sections, this can be very helpful because the conditional success probability is easier to work out than the unconditional one.\nAdditionally, this lemma provides an intuition about why the optimism principle proposed by Kaelbling et al. (1996) is so broadly accepted in designing the exploration strategies. The optimism principle requires every state-action pair be tried for some times before any conclusion on its utility is made. This often leads to the occurrence of traverse events, which enables the possibility of success, and thus plays a key role in theoretically guaranteeing the performance of learning algorithms, even if such a success is not the exact objective for these algorithms.\nIn contrast, some naive strategies without optimism, ε-greedy for example, are more likely to fail in traversing, which result in further failures in planning high-quality policies due to lack of key information. Although non-optimistic strategies are widely utilized in real-world applications (e.g. Abbeel and Ng (2005); Riedmiller et al. (2009); Mnih et al. (2015)), their successes are often more dependent on the state/action feature engineering, prior knowledge, the (near-)deterministic environment, and generalization techniques. In less deterministic environments with less prior knowledge available to both machines and RL practitioners, the impact of insufficient exploration becomes more significant, worsening the performance of vanilla strategies.\nIt is worth noting that, although literally traversing an MDP can be impractical in large-scale real-world environments due to state explosion, the discussion for traverse events remains essential and reasonable for analyses. To deal with large-scale (or even continuous) MDPs, we will introduce the chain perspective in Section 5. In short, the chain perspective reduce an arbitrary MDP to a chain MDP with much smaller scale, such that the success probability of exploration in the original MDP can be approximated by that in the reduced chain MDP. It is sufficient to traverse the reduced chain MDP, rather than the original one, for a learning agent to achieve a success in our analyses. This properly reflects the fact that, with generalization techniques applied, learning agents are able to discover desirable policies if all key states are sufficiently explored. Traversing the reduced chain MDPs are both realistic and sensible milestones to success for agents, and therefore the discussion of traverse events are necessary even in realistic scenarios. We will come back to this topic in later sections.\nCombining the first-level and the second-level decomposition lemmas, we get the following theorem.\nTheorem 12 (Divide-and-conquer the success probability) Pεθ,τ = ∑ π∈Πε P(Eπ|Eπtrav, θ, τ)P(Eπtrav|θ, τ).\nProof By Lemma 6 and Lemma 11.\nBy Theorem 12, the success probability can be dealt with in a divide-and-conquer manner. In this way, the necessity of analysing a complex event as a whole is avoided, turning\nthe complicated task of deriving the expression of success probability to a more tractable one."
    }, {
      "heading" : "4. The Solution to the Three Groups of Questions of Exploration",
      "text" : "Having established a new perspective of the cost-outcome relation, it is crucial to examine whether this new perspective is able to reflect our practical needs. In the following subsections, we demonstrate that our success probability of exploration can be used to answer the three groups of questions mentioned in Section 1. To focus on this purpose, we leave the details of our approach to concretely estimating the success probability of exploration to later sections. For now, let us just assume that we have already derived the closed-form expression of the Pθ,τ for MDP M and exploration strategy A."
    }, {
      "heading" : "4.1 (Q1) Questions of Exploration Parameter Setting",
      "text" : "The first group of questions concern how to find the most suitable parameter setting θ for the given learning task such that minimum number of observations τ are required to guarantee the failure probability (1−Pθ,τ ) not exceeding some threshold δ. It corresponds to the question that the traditional PAC analysis tries to answer (“With what parameters the algorithm can be PAC with polynomial sample complexity?”) but have not come with a satisfying result (see Section 3.2). Therefore, the success probability of exploration ought to be more productive in answering these questions.\nMore formally, we are interested in the best parameter setting θδ such that\nθδ = arg min θ\nτ : 1− Pθ,τ ≤ δ. (3)\nThe success probability Pθ,τ here can be seen as a function of τ and write as Pθ(τ). Intuitively, if more observations are obtained and are used to build the model M̂ , then M̂ is more likely to be accurate, and because its estimates of transitions are unbiased, the success probability should increase in most cases. In the ideal case where Pθ(τ) strictly increases with τ and thus has an inverse function P−1θ (δ), we have\nθδ = arg min θ\nτ : Pθ(τ) ≥ 1− δ\n= arg min θ\nτ : τ ≥ P−1θ (1− δ)\n= arg min θ P−1θ (1− δ),\nand thus the best parameter setting can be discovered directly by calculating the minimum point of the function above. Even if Pθ(τ) is not strictly monotonic, Equation 3 can still be view as a constrained optimization problem and be solved via function optimisation techniques. By doing so, the best parameter setting of A for achieving (ε, δ)-optimality can be discovered without trial-and-error. In this way, the actual budget of observations required for training the agent can be significantly reduced, at the price of merely some numerical computations. This can be precious for real-world applications where observations are far more expensive than computational power.\nIn conclusion, knowing the closed-form expression of the success probability is sufficient to answer the questions of exploration parameter setting."
    }, {
      "heading" : "4.2 (Q2) Questions of Situation Analysis",
      "text" : "Suppose that we have run the learning algorithm for a while, but have not achieved a success. We then have to conduct situation analysis, i.e. evaluate the learning process so far to figure out whether we have to increase the sample size, change the parameters of the exploration strategy, or even improve the representation of states and actions.\nThe need of situation analysis arises even if a practitioner chooses the best parameter setting with respective to (ε, δ)-optimality using the method introduced above. The reason is that parameter setting only guarantees a probability 1 − δ of success, and there still exist chances of failure that cannot be ignored. When a failure occurs, the practitioner has to conduct situation analysis to decide what should be done with the information already obtained as well as the remaining resource budgets. Although the need of situation analysis can be removed by controlling the failure probability to a tiny value, this often leads to a greater need of time steps τ for success, or a worse ε-optimality. Therefore, it is reasonable to trade off some success probability for a better optimality and a less need of time steps, and rely on situation analysis to cope with undesirable consequences in a iterative manner.\nComparing the success probability of exploration under different settings leads us to a clear solution to the situation analysis. Specifically, given the parameter setting θ and the time step τ , the possible current situations and their corresponding solutions are as follows:\n(a) Situation The success probability Pθ,τ is sufficiently high. Solution The current failure is probably due to bad luck (e.g. some rare events\nhappened in a row). Therefore, re-run the learning algorithm with the same setting of θ and τ should be sufficient.\n(b) Situation The success probability Pθ,τ is not high enough. However, if we increase τ to a higher yet reasonable value τ ′, then Pθ,τ ′ becomes acceptable.\nSolution The observations are insufficient. If there is still enough time step budget, then we should keep the parameter setting θ unchanged, continue learning and wait for the agent to collect more observations.\n(c) Situation The success probability Pθ,τ is not high enough. However, if we change the parameter setting θ to some reasonable θ′, then Pθ′,τ notably improves.\nSolution The current parameter setting of the exploration strategy is highly risky, and we were not lucky enough to achieve a success. We should change the parameters to θ′ for higher success probability and re-run the learning algorithm for τ steps.\n(d) Situation The success probability Pθ,τ is not high enough. Furthermore, no matter how we change the value of θ and τ within reasonable range, the success probability remains unacceptable.\nSolution It is too risky to expect the learning succeeds under the current problem formulation or representation. Unless there is sufficient time step budget to bear the risk, we may have to examine the problem formulation or the state/action representation and try to improve them.\nAs we can see above, the success probability of exploration is helpful for assessing the present situation of the learning process, identifying the critical factors that may influence\nthe performance and providing the possible solutions accordingly. Additionally, this method of analysis is not based on the learning curves, and hence does not require to keep track of the performance of the output policies at every time step, avoiding the potential problems mentioned in Section 1."
    }, {
      "heading" : "4.3 (Q3) Questions of Hardness of Exploration",
      "text" : "The hardness of reinforcement learning tasks is not obvious, especially for the exploration part since it involves uncertainty. Literature have shown that some benchmarks considered non-trivial are actually relatively easy (Maillard et al., 2014), while some seemingly impossible tasks are solved efficiently by rather simple algorithms (e.g. Mnih et al. (2015); Silver et al. (2016)). Therefore, it is crucial to develop some metrics that can be used to compare the hardness of exploration in different MDPs. Further, this metric should be directly related to the cost of learning so that practitioners can make their budget accordingly.\nThe success probability of exploration reflects the internal properties of MDPs as well as the interaction between exploration strategies and MDPs, and thus provides effective solution to these questions. Given two MDPs M1 and M2, their success probabilities PM1,A(θ),τ and PM2,A(θ),τ using some baseline exploration strategy A can be compared to decide their relative hardness. For example, fixing the exploration parameter setting θ, if we find that for any number of steps τ > 0 it holds that PM1,A(θ),τ > PM2,A(θ),τ , then undoubtedly the MDP M1 is easier than M2 for exploration strategy A(θ). Of course, comparing the success probabilities under all τ > 0 is not always necessary for practical purpose, and it may suffices to compare them within some reasonable budget range τmin < τ < τmax.\nAlternatively, given some small failure probability δ, say 0.05, we can compare the minimum numbers of steps τ1 and τ2 required to satisfy P ≥ (1 − δ) = 0.95, respectively in M1 and M2. Here τ1 and τ2 can be computed by function optimization methods as in Section 4.1, namely by solving the minimization problems\nτ1 = min θ τ : PM1,A(θ)(τ) ≥ 1− δ, and τ2 = min θ τ : PM2,A(θ)(τ) ≥ 1− δ,\nand for strictly increasing functions, they can be simplified to\nτ1 = min θ P−1M1,A(θ)(1− δ), and τ2 = minθ P −1 M2,A(θ)(1− δ).\nComparing τ1 and τ2 reflects, under the best parameters settings of A respectively for M1 and M2, which MDP needs more steps to explore.\nFurthermore, we can draw the whole θ-τ -Pθ,τ surface for M1 and M2, and see whether one of them is above the other (i.e. PM1,A(θ),τ ≥ PM2,A(θ),τ for any θ and τ). If the two surfaces share the same analytic expression except for some parameters, it is possible that just comparing these parameters is sufficient to decide the relative hardness of these MDPs. These parameters may even give a clue about some essential properties of these MDPs, leading to further discovery and deeper understanding of the tasks.\nIn conclusion, the success probability of exploration is helpful for describing and discerning the hardness of exploration over different MDPs, and thus provide possible answers the third group of questions related to exploration."
    }, {
      "heading" : "5. The Chain Perspective of MDP",
      "text" : "Last section has shown that the success probability of exploration does provide useful answers to all three groups of questions of exploration, and obtaining the closed-form expression of the success probability is the key. Working out the closed-form expression itself, on the other hand, is not a trivial task since it involves complicated interactions between MDPs and the learning algorithms. To avoid falling into the trap of ad hoc analysis where one has to analyse every MDP from scratch, we introduce the chain perspective of MDP, which helps form a generalizable foundation for the later discussion.\nIn short, the chain perspective of MDP is to abstract a more complicated MDP as a chain MDP, which is composed of several key elements as follows:\nStart state There is one unique start state s1 in a chain, and the learning agent always begins the interaction with the environment from this state.\nGoal state & Goal reward There is one unique goal state sn at the end of the chain, where n is the length of the chain, or, the number of chain states. There is a positive goal reward rG for taking the right action at the goal state. It is set large enough such that the optimal policy should always tries to obtain it as frequently as possible.\nOther chain states The main body of a chain MDP is the states s2, ..., sn−1 connecting the start state s1 and the goal state sn.\nForward actions At each chain state si except for the goal state, there is at least one forward action by which the agent is most likely to travel to the next state si+1 and thus getting closer to the goal state.\nGoal action This is the only action that collects the goal reward at the goal state sn.\nBackward actions & Distracting rewards At each state si, there is one or more backward actions by which the agent is more likely to travel back to state s1, ..., si−1, getting away from the goal state. A learning agent might be confused by these possible actions because they often yield or lead to some distracting rewards rD, which might seem to be more appealing than the goal reward for the agent under uncertainty.\nA typical chain MDP that can be easily found as a benchmark problem in various literature (e.g. Dearden et al. (1998); Strehl and Littman (2004); Kolter and Ng (2009)) is shown in Figure 1. There are five chain states in this example, and the start state s1 and the goal state s5 are highlighted using bold circles. The triple (a, p, r) written next to a\ntransition indicates that this transition occurs with probability p under action a and yields reward r. For clarity and convenience, all forward actions as well as the goal action are written as a+ and drawn in solid arrows, while all backward ones are written as a− and drawn in dashed arrows.\nAs can be seen from the figure, forward actions at state s1 to s4 have a p = 0.8 chance of sending the agent to the next state, and a (1− p) = 0.2 chance of remaining at the current state. In contrast, all backward actions transit the agent back to the start state s1 with probability 1. The only non-zero distracting reward r1,a− = 0.1 in this example is produced by taking backward action a− at the start state. Although this reward is small compared to the goal reward r5,a+ = 1, it is still distracting for those agents with pessimistic estimates to the probability pi’s of moving closer to the goal by taking forward actions. In addition, if an agent has never collected the goal reward before, then staying at the start state and taking action a− may appear to be optimal, in spite that the real optimal policy is to take action a+ at every state s ∈ S, given that the discounting factor γ is sufficiently large.\nChain MDPs have played a key role in analysing the efficiency of reinforcement learning algorithms due to the simplicity of its structure. In Whitehead (1991), it has been proved that in a homogeneous problem solving task, the expected number of observations required by a Q-Learning agent with an ε-greedy exploration strategy to find an optimal policy is exponential to the number of steps required by the optimal policy to arrive at the goal state. Another expression of this theorem, proposed by Li (2012), is as follows: in a chain MDP, a Q-Learning agent with an ε-greedy exploration strategy, starting from the start state, needs observations exponential to the length of the chain in order to reach the goal state. This theorem indicates the inefficiency of ε-greedy exploration strategy, and inspired a deeper investigation to the exploration strategies thereafter.\nWhile the simplicity of chain MDPs serves well for the purpose of theoretical analysis, they also contain most of the fundamental elements necessary to describe complex realworld tasks. Specifically, there is always some kind of objective for the agent to achieve in a task, which can be represented by a goal state. In order to achieve the objective, the agent has to select the right actions throughout a series of states, without wrongly taking hazardous actions or being distracted by irrelevant rewards.\nAn illustration of this is provided in Figure 2. In the game stage shown in (a), Mario has to make some successful jumps from the leftmost platform to the rightmost one without falling into the pit, and then run through the goal sign. One possible chain abstraction for this is (b), where the agent starts from the state numbered 1 corresponding to the leftmost platform, travels through the chain states 2 to 6 by forward actions, eventually reaches the state 7 indicating the goal sign, and takes the goal action to collect the goal reward.\nAlthough much details of the original game stage are lost in the chain abstraction, it nevertheless captures the main dynamic properties of the original stage. In particular, the hardness of exploring the game stage is properly reflected in its chain abstraction. Taking wrong actions in the first half of the stage (the short platforms and the cliff) will cause Mario to fall into the pit and die, which means Mario has to re-challenge the game stage from the start point. This hazardousness is appropriately expressed in state s1 to s4 since the backward actions at these states send the agent back to s1. The second half of the stage, on the other hand, are less difficult because taking wrong actions only bring Mario back to the previous positions without killing him. Exploring s5 to s7 in the chain abstraction\nis correspondingly easier than s1 to s4. Finally, arriving the goal point indicates a “level complete”, resulting in Mario being sent to the next stage. If the player wants to continue exploring the current stage, Mario has to restart from the start state. This is correctly represented in the chain abstraction as well.\nThe most important aspect of the chain abstraction is, if the effect of generalization techniques applied coincide the chain abstraction, i.e. the underlying policy sees the environment exactly as the reduced chain, then the success probability of the learning algorithm executed on the original environment is equivalent to that of an algorithm without generalization executed on the chain MDP. In the Mario example above, this can be done by create seven grids on the horizontal axis accordingly, and represent the state by the number of the grid where Mario is currently in.\nIn this way, the effect of generalization is completely encapsulated into the representation of the environment. Not only this removes the necessity of simultaneously considering generalization and exploration in our analyses, it also provides useful hints regarding which kind of generalization can be more efficient. Specifically, if some generalization technique reduce the original learning task on MDP M to a chain MDP M ′, and learning on M ′ has a sufficiently high success probability within reasonable resource budget, then that generalization technique is very likely to be an efficient one."
    }, {
      "heading" : "6. Solving the Success Probability in Chain MDPs",
      "text" : "In this section, we elaborate our approach to the closed-form expression of success probability in chain MDPs."
    }, {
      "heading" : "6.1 The Dependency Graph of Relevant Variables",
      "text" : "Considering that the success probability involves almost everything in reinforcement learning, it is crucial to figure out the interrelationship of all relevant variables as a preliminary step. The resulting dependency graph is shown in Figure 3. If there is a directed edge from node X to Y , then it means that the variable X is dependent to variable Y . If some variable is written in angle brackets, then it represents a tuple of all variables of the same kind. For example, 〈Eπ〉 denotes the tuple of Eπ of all possible policies π ∈ Π.\nThe thick arrows from ε-success event Eε to the set of ε-optimal policies Πε and the tuple of all π-success events 〈Eπ〉 correspond to Lemma 6, the first-level decomposition of ε-success probability, in which the probability of Eε equals to the sum of probabilities of Eπ over all π ∈ Πε.\nThe double arrows connecting 〈Eπ〉, estimated values 〈V̂ π〉, visit numbers 〈Ns,a〉 and 〈Ns,a,s′〉, and traverse events 〈Eπtrav〉 correspond to Lemma 11, the second-level decomposition. The occurrence of traverse events will be considered first. Only when it occurs, all relevant visit numbers will be non-zero, so that they can be used to estimate the state values. The estimation process (or the planning process) also involves the expressions of state values 〈V π〉, which are usually derived implicitly from Bellman Equations. The estimated state values are thereafter compared with each other, deciding which π-success event occurs.\nThe detailed analysis of PMC ,A0(m),τ will be carried out in a bottom-up manner according to the dependency graph in the following sections. We start from the in-depth discussions of chain MDPs and exploration strategies respectively in Sections 6.2 and 6.3. They are followed by 〈Eπtrav〉 in Section 6.4, then 〈Ns,a〉 in Section 6.5. Then it comes to be the actual and estimated value functions 〈V π〉 and 〈V̂ π〉 together in Section 6.6. Finally, the probability of π-success and ε-success events will be assembled in Section 6.7."
    }, {
      "heading" : "6.2 The Prototypes of Chain MDPs",
      "text" : "As hinted by the dependency graph, there are four critical factors involved in the success probability of exploration: degree of near-optimality ε, MDP M , exploration strategy A and its parameter setting θ, and the total number of steps (or observations) τ . It is unrealistic to express the very nature of the MDP (even for chain MDPs) and the exploration strategy in only a limited number of numerical variables. Therefore, it is not likely that there exists some uniform expression suitable for all possible MDPs and exploration strategies. Rather, we focus the discussion to some prototypes of chain MDPs and exploration strategies, so that the result derived in this paper can be extended with additional effort as less as possible.\nThis subsection focuses on the four prototypes of chain MDPs. These prototypes should capture the main characteristics that has critical impact on the difficulty of exploration. In general, there are two characteristics of interest in chain MDPs: hazardousness of backward actions, and productivity of goal action.\nThe hazardousness of backward actions refers to how bad the situation will be when a backward action is taken. The degree of hazardousness, denoted by H, can be reflected through the number of chain states the agent is thrown back when taking backward actions. In the least hazardous case, the agent is transited one state back, meaning that it will be sent to si−1 from si by taking a backward action. In the most hazardous case, on the other hand, the agent is transited to the start state s1 regardless how far it is from the current state. We write the former case as H = 1, while the latter as H = ∞. Other cases lie between these two extreme point, taking other positive integers as the degree of hazardousness. A typical chain MDP is a mix of these cases, with each chain state si having a hazardousness level Hi. It is of particular interest to investigate the two extreme scenarios for chain MDPs, namely the ones with all states having Hi = 1, and the ones with all states having Hi =∞.\nThe productivity of goal action, denoted by G, is the second key characteristic, which decides the value of goal as a function of goal reward rG. In the most fruitful case, taking the goal action at the goal state not only yields the goal reward rG, but also transits the agent to exactly the same goal state sn. This allows the agent to receive the goal reward every time step thereafter by simply repeating the goal action. The other extreme case is that the goal action send the agent back to the start state s1 while providing the goal reward. In this case, the agent has to travel again through all chain states in order to obtain the goal reward once more, resulting in the least productivity. We write the former case as G = 1, while the latter as G = 1∞ . Just as the hazardousness, the productivity of goal action for typical chain MDPs lie between these two extreme cases, and therefore it is suitable to regard the extreme cases as the prototype.\nCombining the two characteristics, each with two extreme cases, four prototype chains are formed, as illustrated in Figure 4. The corresponding hazardousness H and goal productivity G of prototype chains are labelled in their captions. For clarity, zero-value rewards as well as the action marks a+ and a− are omitted in this figure.\nActions are still distinguishable by their line pattern. As in previous sections, forward and goal actions are drawn in solid lines, while backward actions are drawn in dashed lines. By taking a forward action at state si (i < n), the agent has probability pi to be transited to si+1, and 1 − pi to remain at si. The transition probabilities p1, ..., pn−1 are\nnot required to be equal in the same chain, but they must be non-zero, otherwise the goal state is unreachable.\nThe goal reward rG is assumed to be set large enough than the distracting reward rD such that the optimal policy in these prototype chains is to take forward action a+ at any state. The exact constraint that should be met for rG and rD will be left until Section 6.6.\nFor convenience, these prototype chains will be referred collectively as MC , and respectively as MC(H = 1, G = 1), MC(H = ∞, G = 1), MC(H = 1, G = 1∞), and MC(H =∞, G = 1∞) in the rest of this paper. Also, we write MC(H = 1) to refer to both MC(H = 1, G = 1) and MC(H = 1, G = 1 ∞), and likewise for MC(H = ∞), MC(G = 1), and MC(G = 1 ∞). Although the length of the chain n and forward probabilities 〈pi〉 are equally important factors, we will not explicitly state them here in order to keep notation short and clear."
    }, {
      "heading" : "6.3 The Prototype Exploration Strategy",
      "text" : "The second issue that should be addressed before further investigation is the prototype of exploration strategies. It is difficult to find a representative strategy that is able to reflect the fundamental properties of both vanilla strategies such as ε-greedy and Boltzmann selection rule (Sutton and Barto, 1998), and the more advanced strategies. Fortunately, there does exist a common principle shared by a large number of non-Bayesian advanced exploration strategies. It is often known as “optimism in the face of uncertainty” or “the optimism principle” Kaelbling et al. (1996), which has been already mentioned in the discussion of the second-level decomposition (Lemma 11).\nThe main idea of the optimism principle is that the agent should assume any stateaction pair to be highly rewarding ones as long as there is no sufficient evidence against this optimistic assumption for that state-action pair. This principle forces the agent to try every action at every state for several times, ensuring a base amount of observations for all visited state-action pairs. The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesvári, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al., 2010). It is also applied in some Monte-Carlo tree search algorithms such as UCT (Kocsis and Szepesvári, 2006).\nAs discussed in Section 3.3, it is less likely for a primitive exploration strategy without optimism, for example ε-greedy, to complete any of the traverse events. As a result, without the help of some additional techniques that could efficiently exploit the prior knowledge, the success probability of exploration for these strategies can be rather low, sometimes even close to zero. There also exists a different family of non-optimistic exploration strategies, namely the Bayesian strategies (Vlassis et al., 2012). However, these works are based on a different optimality criteria called Bayesian optimality, which is not equivalent to the traditional criteria used in this paper. Therefore, within the scope of this paper, it is sufficient to base our prototype strategy on the optimism principle.\nPutting all consideration together, we have the following prototype strategy.\nDefinition 13 (Optimistic Prototype Strategy) The Optimistic Prototype Strategy, denoted by A0(m), is an exploration strategy that can be described as follows. • There is a universal parameter, denoted by m, which is a non-negative integer.\n• At any state s ∈ S, if there exists some action a ∈ A that its number of observations Ns,a < m, then state s is said to be under-explored, and the agent takes action a to collect one more observation. • If there is more than one such action, then the agent randomly and uniformly choose\none of them. • If there is no such action at all, then state s is said to be fully explored, and the agent\nmoves to the under-explored state with least expected step required from s. • If all visited states have become fully explored, then the agent stops optimistic explo-\nration and outputs a policy according to Assumption 8. The agent follows the output policy thereafter, if the learning process is not stopped.\nThis prototype strategy is based on the famous baseline PAC-MDP strategy R-MAX (Brafman and Tennenholtz, 2002; Kakade, 2003). Actually, there is no behavioural difference between R-MAX and the Optimistic Prototype Strategy when they are executed on finite chain MDPs, given that the discount factor γ is sufficiently large. Other advanced optimistic strategies, no matter PAC-MDP or not, can be seen as an extension of this prototype, where the universal parameter m is changed into a function mψ(s, a) such that its value varies for different state-action pairs, and may even changes during learning. Therefore, we will focus on analysing the exploration behaviour of this prototype strategy in the rest of this paper, and make our analysis as flexible and compatible as possible for other optimistic strategies."
    }, {
      "heading" : "6.4 The Traverse Events",
      "text" : "By Definition 10, a π-traverse event is said to have occurred if and only if for all s ∈ S, state-action pair (s, π(s)) has been tried at least once. Although this seems to be dependent to the visit numbers, rather than the converse as in Figure 3, the traverse events are placed deliberately at a level lower than the visit numbers. The reason for this is, the traverse events only care about a small fraction of state-action pairs, and thus is easier to decide than the visit number of all state-action pairs.\nIn the scenario where optimistic prototype strategy A0(m) is applied to the agent in prototype chain MC ’s, the following lemma can be figured out rather effortlessly:\nLemma 14 For any policy π ∈ Π, a traverse event Eπtrav occurs if and only if the agent successfully arrives at the goal state sn.\nProof Because the agent starts from s1, this lemma is obvious by Figure 4.\nWhether or not the agent could reach sn depends on MC and A0(m), and of course, the total number of steps τ . Actually, there is some τm that is of particular importance: the exact time step that the agent finishes its active exploration behaviour promoted by the optimism principle. Its formal definition is as follows.\nDefinition 15 The ending of exploration for A0(m), denoted by τm, is the first time step that, after the transition takes place, Ns,a ≥ m holds for all s ∈ S \\ Su and a ∈ A, where Su := {s|∀a ∈ A,Ns,a = 0} is the set of unvisited states.\nAfter this time step, the agent stops optimistic exploration and begins exploitation according to Definition 13 of prototype strategy. It is less likely that a traverse event not occurred so far happens after τm. Consequently, the probability of traverse events (as well as the success probabilities discussed in later sub-sections) can be directly inferred from the ones at τm. In the case of τ < τm, on the other hand, the probabilities can be interpolated through different setting of m. Therefore, it is sufficient to consider the case of τ = τm. By considering the ending of exploration, the number of steps τ are actually integrated into the parameter m, simplifying the following discussions.\nNow we have the closed-form expression for the probability of π-traverse events as follows.\nTheorem 16 (Probability of Traverse Events) For any π ∈ Π, it holds that\nP(Eπtrav|MC ,A0(m), τm) = n−1∏ i=1 (1− (1− pi)m).\nProof If the agent intends to reach sn eventually, it has to succeed moving ahead through forward actions a+ at every chain state s1, s2, ..., sn−1 at least once. According to Definition 13, the agent will try a+ at least m times at any chain state si. However, these are also the only chances for the agent to try a+. The reason is, if all m tries at si fail, then the agent will never be informed of the existence of si+1, nor the existence of a goal reward. In the following learning process, the agent will either explore the states s1, ..., si−1, the backward action a− at si, or exploits the distraction reward rD; no further attempt to reach goal will be carried out. Therefore, the probability of a traverse event happens is the probability that the agent successively succeeds moving forward at least once at every chain state within m attempts, hence the theorem.\nAn interesting fact of this theorem is that it is dependent neither to π, nor to the hazardousness H or productivity G of prototype chain MC . Considering that τm has been integrated into m, it suffices to write the traverse probability simply as P(Etrav|m) in later discussions."
    }, {
      "heading" : "6.5 The Visit Numbers",
      "text" : "In this subsection we investigate the visit numbers 〈Ns,a〉 and 〈Ns,a,s′〉 given that the traverse event Etrav has happened. All following discussions are based on the occurrence of the traverse event unless otherwise stated, and this precondition will not be repeated every time for the sake of briefness.\nThe direct impact of the occurrence of traverse event on the visit numbers is Nsi,a+,si+1 ≥ 1 for all i < n. Consequently, the agent is informed of the existence of the all chain states, including the goal state sn. This guarantees that there actually exists an ending step of exploration τm within finite steps such that Ns,a ≥ m holds for all s ∈ S and a ∈ A. Then it leads to the estimated transition probability P̂ (s′|s, a) = Ns,a,s′Ns,a > 0 for any s, s\n′ ∈ S and a ∈ A in MC .\nHowever, due to the stochastic nature of environment, it is not very likely that two independent runs of learning algorithm result in the same trajectory. Therefore, the visit\nnumbers should be treated as random variables, and their distributions have to be revealed in order to tell the distributions of the estimated transition probabilities P̂ .\nThe major obstacle here comes from the very nature of interacting in an MDP: the state at the next time step must be the state the agent be transited to at the current time step. This leads to the inter-dependency within all visit numbers 〈Ns,a〉 and 〈Ns,a,s′〉. Fortunately, not all part of this dependency is relevant to the success probability. As an example, assume that by time step τ , some transition (si, aj , sk) occurred three times within five tries of aj at si. Then exactly at which time steps these (si, aj , sk) transitions took place does not make any difference for estimating the transition probability P̂ (sk|si, aj); it will always be 3/5 = 0.6. Actually, it is not even relevant which state is the current one, although it is one of the most important factors in interacting with the environment.\nTherefore, in the context of deriving the success probability, it suffices to deal with the visit numbers in the following manner.\nDefinition 17 (The Three Rules of Visit Numbers)\nRule 1 Regard the expected visit numbers 〈N̄s,a〉 and 〈N̄s,a,s′〉 as constant numbers that are only dependent to other state-actions that share a common state. Specifically, the expected total number of actions taken at any state should equal to the expected total number of being transited into that state.\nRule 2 Regard the actual state-action visit numbers 〈Ns,a〉 as constant numbers that equals to 〈N̄s,a〉.\nRule 3 Regard the actual transition visit numbers 〈Ns,a,s′〉 as random variables following some binomial distributions. More precisely, Ns,a,s′ ∼ Binomial(N̄s,a, P (s′|s, a)) for all s, s′ ∈ S and a ∈ A.\nAlthough the last two rules may introduce a loss of dispersion to the actual visit numbers, this loss is rather insignificant and thus negligible. The reason is, the agent exploits the information from visit numbers through estimated transition probabilities, which are computed by P̂ (s′|s, a) = Ns,a,s′/Ns,a. As long as Ns,a,s′ ∼ Binomial(N̄s,a, P (s′|s, a)), the estimates P̂ (s′|s, a) are unbiased, and the loss of dispersion only has impact on the variance of P̂ (s′|s, a), which is decided mainly by the variance introduced by the binomial distributions rather than 〈Ns,a〉. The empirical results in Section 8.2 also shows that the dispersion are actually preserved well by these rules, further supporting their effectiveness. Therefore, the above three rules are sufficient for the purpose of this paper.\nNow let us consider the closed-form expression of expected visit numbers 〈N̄s,a〉 for prototype strategy A0(m) running in chains MC . For convenience, we write N̄si,a+ , N̄si,a− , N̄si,a+,sj , and N̄si,a−,sj respectively as N̄ + i , N̄ − i , N̄ + i,j , and N̄ − i,j in the rest of this section.\nLet λi denotes the expected total number of being transited into chain state si by any transitions except for (si−1, a +, si). This includes failures of forwarding (si, a +, si), goal transitions (sn, a +, si), and backward transitions (sj , a −, si) with j ≥ i. More formally,\nDefinition 18 For any 1 ≤ i ≤ n, we define λi := N̄+i,i + N̄ + n,i + ∑n j=i N̄ − j,i.\nLemma 19 The following hold in corresponding chain MDP MC .\n(a) MC(H = 1) λi = (1− pi)N̄+i + N̄ − i+1, for 1 < i < n.\n(b) MC(H =∞) λi = (1− pi)N̄+i , for 1 < i < n.\n(c) MC(G = 1) λn = N̄ + n .\n(d) MC(G = 1 ∞) λn = 0.\nProof For all 1 ≤ i < n it holds that N̄+i,i = (1− pi)N̄ + i . Then by Definition 18.\nLemma 19 shows an interesting trait of chain MDPs, that is, if two chains share the same hazardousness H at state si, then they share the expression of λi for 1 < i < n as well; the same can be said for the productivity G and λn, Actually it is easy to see that these also hold for non-prototype chains, meaning that even for a chain with mixed hazardousness and productivity, the properties of λ above can be directly applied. This could be very helpful when analysing more complicated chains, as can be seen in Section 9.\nAccording to Rule 1 of Definition 17, the following equations hold for all four prototype chains. {\nN̄+i + N̄ − i = λi + pi−1N̄ + i−1 (i 6= 1) N̄+1 + N̄ − 1 = λ1.\n(4)\nThe above equations are only a paraphrase of Rule 1, in the sense that the left side of equations are the expected total number of tries at si, while the right side are the expected total number of being transited into si. The latter includes successful forwarding transitions, which is expected to be pi−1N̄ + i−1 times, and the other transitions with λi times. In the special case of start state, there is no forwarding transitions, and hence the right side being simply λ1.\nBy applying Lemma 19 to Equations 4, the expected visit numbers 〈N̄s,a〉 in four prototype chains can be derived accordingly. The result is as follows.\nLemma 20 (The Expected Visit Numbers at τ = τm)\n(a) Backward actions In all four prototype chains, it holds that N̄−i = m (1 ≤ i ≤ n).\n(b) Goal actions In all four prototype chains, it holds that N̄+n = m.\n(c) Forward actions The following holds for 1 ≤ i < n.\n(c.1) MC(H = 1,G = 1) N̄ + i = m (+1) pi . (c.2) MC(H =∞,G = 1) N̄+i = (n−i)m (+1) pi . (c.3) MC(H = 1,G = 1 ∞) N̄ + i = 2m pi . (c.4) MC(H =∞,G = 1∞) N̄ + i = (n+1−i)m pi .\nProof\n(a) By Definition 13, it holds that N−i ≥ m at time step τm. Because it is significantly easier to move backward than forward in prototype chains, the bottleneck of exploration is always at the state nearer to the goal state. Thus it is not likely that the agent take backward action a− at any si after it has been tried m times in order to get back to an under-explored state sj with j < i. Therefore, N̄ − i = m holds for any 1 ≤ i ≤ n at τm.\n(b) As in (a), the bottleneck of exploration is to reach the states closer to the goal state. Therefore, it is not likely that the agent further takes a+ at sn after N̄ + n = m in order to move to other under-explored states, and hence the result.\n(c.1)-(c.4) The results can be computed recursively by Equation 4 from n− 1 down to 1, with all λ’s taking values according to Lemma 19.\nNote that the goal transitions in chains with G = 1 are self-loops, and exploring the goal actions in these chains has no impact on the exploration of other states. Therefore, these state-actions are likely be explored less than m times when all other state-actions are fully explored. In this case, all forward actions have to succeed exactly one more time, so that the agent could get to the goal state and finish its exploration by repeating the goal action, resulting in an additional +1 adjustment to the numerator for (c.1) and (c.2). The most appropriate adjustment should be a value between 0 and 1, but the impact of the actual choice here is negligible.\nRemark 21 By definition it holds that τm = ∑n i=1(N + i +N − i ) and E τm = ∑n i=1(N̄ + i +N̄ − i ) at τ = τm. This can be used to evaluate how many steps have passed when the exploration process ends.\nHaving worked out the closed-form expressions for expected visit numbers 〈N̄s,a〉, the distribution of actual visit numbers 〈Ns,a〉 and 〈Ns,a,s′〉 can be obtained without effort by applying Rule 2 and Rule 3 of Definition 17."
    }, {
      "heading" : "6.6 The Actual and Estimated State Value Functions",
      "text" : "In this subsection we will derive the expressions of the state value functions for chain prototypes. As can be seen in the dependency graph (Figure 3), the actual state value functions V π are only dependent to the MDP M , and thus this is a relatively easy part compared to the others. According to Bellman equation (see Equation 1), actual state value V π(s) for a state s ∈ S can be seen as an expression in terms of all relevant transition probabilities P (s′|s, π(s)) and rewards R(s, π(s), s′). Therefore, the main task here is to solve Bellman equations for all chain states s1, ..., sn to obtain the expressions of state values in terms of p1, ..., pn−1, rG, and rD.\nAfter the expressions of actual state values V π are acquired, it is trivial to turn them to estimated values V̂ π. Actually, expressions of V̂ π can be obtained by directly replacing all P (s′|s, a) with P̂ (s′|s, a) = Ns,a,s′/Ns,a, and all R(s, a, s′) with R̂(s, a, s′). Although most planning algorithms do not estimate state values explicitly in this way, utilizing any\niterative algorithm that is able to asymptotically converge to the actual values is nevertheless effectively equivalent in limit to estimating them straightforwardly via these expressions, and in this paper the computation cost is of no concern.\nThe only additional matter here is to decide which policies should be considered, because there are 2n different possible policies in total for a chain MDP with length n. Fortunately, in chain MDPs, most of the possible policies are unimportant because they are unconditionally dominated by a specific family of policies, meaning that no matter what the values are for p1, ..., pn−1, rG, and rD, these dominated policies have some state value lower than the dominants. The dominant family can be specified as follows.\nDefinition 22 Let π−+k be a policy that chooses a − at states s1, s2, ..., sk, and chooses a + at states sk+1, ..., sn, where 0 ≤ k ≤ n. Let 〈π−+k 〉 collectively denotes all such policies.\nClearly, there are (n+ 1) policies in 〈π−+k 〉. In the extreme case where k = 0, the policy always chooses a+ at any state, while in the other extreme case where k = n, the policy always chooses a−. The dominating power of 〈π−+k 〉 will be explained in later part of this subsection. Before that, some useful properties of 〈π−+k 〉 will be introduced first.\nLemma 23 Let si be an arbitrary non-goal chain state in MC . For any policy π such that π(si) = a +, it holds that V π(si) = γpi\n1−γ(1−pi)V π(si+1).\nProof By Bellman equation it holds that V π(si) = γ(piV π(si+1) + (1− pi)V π(si)), hence the lemma.\nCorollary 24 V π −+ k (si) =\nγpi 1−γ(1−pi)V\nπ−+k (si+1) holds for all k < i < n in MC .\nNow we are ready for working out the closed-form expressions of state value functions for 〈π−+k 〉 in prototype chains. Lemma 25 (State Value Functions of 〈π−+k 〉) Let Fj := ∏n−1 i=j γpi 1−γ(1−pi) for j < n and Fn := 1. Then the following equations hold in corresponding prototype chains.\n(a) j ≤ k, MC(H = 1) V π −+ k (sj) = γj−1\n1−γ rD.\n(b) j ≤ k, MC(H =∞) V π −+ k (sj) = γ 1−γ rD if j > 1, and V π−+k (s1) = 1 1−γ rD.\n(c) j > k, MC(G = 1) V π−+k (sj) = Fj 1−γ rG.\n(d) j > k, MC(G = 1 ∞) V π−+k (sj) = Fj 1−γF1 rG if k = 0, and\nV π −+ k (sj) = Fj(rG + γ 1−γ rD) if k > 0.\nProof (a) This rule (as well as (b)) will not be used if k = 0 because j ranges from 1 to n. In the case that k > 0, V π −+ k (s1) = rD + γV π−+k (s1) = 1\n1−γ rD. In chains with H = 1, we\nhave V π −+ k (sj) = γV π−+k (sj−1) = ... = γ j−1V π −+ k (s1) for 1 < j ≤ k, hence the result.\n(b) As in (a), V π −+ k (s1) = 1 1−γ rD when k > 0. In chains with H = ∞, we have\nV π −+ k (sj) = γV π−+k (s1) for 1 < j ≤ k, hence the result. (c) This rule (as well as (d)) will not be used if k = n because j ranges from 1 to n. In the case that k < j < n, Corollary 24 applies, and hence V π −+ k (sj) = FjV π−+k (sn). In chains with G = 1, we have V π −+ k (sn) = rG + γV π−+k (sn) = rG\n1−γ , hence the result.\n(d) As in (c), it holds that V π −+ k (sj) = FjV π−+k (sn). In chains with G = 1 ∞ , we have\nV π −+ k (sn) = rG + γV π−+k (s1). However, V π−+k (s1) here is decided by whether k = 0 or not. If k = 0, then since V π −+ k (s1) = F1V π−+k (sn), we have V π−+k (sn) =\nrG 1−γF1 , hence\nthe first part of (d). If 0 < k < n, then V π −+ k (s1) = rD 1−γ as in (a) and (b), resulting in V π −+ k (sn) = rG + γ 1−γ rD, and hence the second part of (d).\nLemma 25 covers the closed-form expressions of state values for all four prototype chains, all chain states, and all possible 〈π−+k 〉. For example, to get the expressions for policy π −+ 3 in an (H = ∞, G = 1) chain with length n = 8, rule (b) should be used for s1 to s3, and rule (c) for s4 to s8. Rule (c) and (d) are particularly useful, because they are not limited to prototype chains, but can also be applied to any chain MDPs with G = 1 or G = 1∞ .\nThe following lemma reveals the dominating power of 〈π−+k 〉, which significantly reduces the difficulty of further analysis.\nLemma 26 (Dominating Power of 〈π−+k 〉) In any prototype chain with any possible settings of p1, ..., pn−1 ∈ [0, 1], rG > 0, and rD > 0, every policy π that is not within the family of 〈π−+k 〉 is dominated by some policy in 〈π −+ k 〉, i.e. there exist some 0 ≤ k ≤ n and 0 ≤ i ≤ n such that V π(s) ≤ V π −+ k (s) for all s ∈ S and V π(si) < V π −+ k (si).\nProof If a policy π does not belong to the family of 〈π−+k 〉, then by Definition 22, there exists some i < n such that π(si) = a + and π(si+1) = a −.\nFor MC(H = 1), we have V π(si+1) = γV π(si). By Lemma 23 we have V π(si) =\nγpi 1−γ(1−pi)V π(si+1), and therefore V π(si+1) = V π(si) = 0. However, by Lemma 25, both V π −+ k (si) and V\nπ−+k (si+1) are positive and thus greater than 0. This even holds when p1 = ...pn−1 = 0 as long as k = n. Therefore, π is dominated by the policies in 〈π−+k 〉 for MC(H = 1).\nFor MC(H =∞), we have V π(si+1) = γV π(s1). Again by Lemma 23, we have V π(si) = γpi\n1−γ(1−pi)V π(si+1) = γ2pi 1−γ(1−pi)V π(s1). However γpi 1−γ(1−pi) < 1, and thus if we change π(si) to a−, then V π(si) will increased to γV π(s1), dominating the original π. If we continue improving π by eliminating any cases such that π(si) = a + and π(si+1) = a\n−, it will eventually turns into a policy that belongs to 〈π−+k 〉. Therefore, π is dominated by some policy in 〈π−+k 〉 for MC(H =∞).\nOther than these states it holds that V π(s) ≤ V π −+ k (s) by recursively applying Lemma 23. Since MC(H = 1) and MC(H = ∞) covers all four prototype chains, this lemma has\nbeen proved.\nAs mentioned at the beginning of this subsection, the estimated state values V̂ can be obtained by substitute the estimated transition probabilities P̂ for P in the expressions of V . Because V̂ can be seen as actual state values in estimated chain M̂C , which differs from MC only in p1, ..., pn−1, and because Lemma 26 holds regardless to p1, ..., pn−1, this lemma also holds for V̂ . In other words, all policies outside 〈π−+k 〉 family is dominated by 〈π −+ k 〉 with respect to V̂ ."
    }, {
      "heading" : "6.7 Solving the Success Probabilities",
      "text" : "We have worked out the expressions of traverse probability P(Etrav|MC ,A0(m), τm) that holds for any π. Then we have the expressions of expected visit numbers 〈N̄s,a〉 and 〈N̄s,a,s′〉 at τm, given that the traverse event happened. According to the rules in Definition 17, we have the distribution of 〈Ns,a〉 and 〈Ns,a,s′〉, and then the distribution of estimated transition probabilities P̂ . Lastly, we have the expressions of state value function V π −+ k , and by replacing P with P̂ we have the expressions of estimated state values V̂ π −+ k for the 〈π−+k 〉 family. The time has come to confront the π-success probabilities 〈Pπ〉 and the ε-success probability Pε. Actually, according to Lemma 6, the first-level decomposition, the ε-success probability Pε is simply the sum of π-success probabilities Pπ over all relevant policies π ∈ Πε. As discussed in Remark 7, specifying the set of desired policies is a task that can be handled by RL practitioners themselves, and thus is not within the main concern of this paper. Therefore, the ultimate challenge comes from the π-success probabilities 〈Pπ〉.\nAs Lemma 26 implies, all non-〈π−+k 〉 policies are unconditionally dominated by 〈π −+ k 〉, regardless of the estimated transition probabilities P̂ . As a direct consequent, we have the following result for these dominated policies.\nTheorem 27 (π-success Probability for Non-〈π−+k 〉 Policies) For any policy π that does not belong to the 〈π−+k 〉 family, it holds that P π MC ,A0(m),τm = 0.\nProof Because Lemma 26 does not rely on any specific setting of transition probabilities, this means that no matter what the estimated transition probabilities p̂1, p̂2, ..., p̂n−1 are (even if they are utterly inaccurate), those non-〈π−+k 〉 policies are still dominated by 〈π −+ k 〉 in their estimated values, and thus will never be outputted by the planning algorithm. Therefore, their π-success probabilities are always 0.\nThis theorem allows us to simply ignore all policies outside the 〈π−+k 〉 family thereafter. Now let us focus on the π-success events among the 〈π−+k 〉 family. Clearly, the one in 〈π −+ k 〉 that dominate every other family members must be the one that dominate all policies in Π. Therefore, the occurrence of π-success event is equivalent to π being the dominant within 〈π−+k 〉.\nThe dominance relationship themselves are decided by comparing the estimated state values V̂ π between family members. Although there are (n+1) policies in the 〈π−+k 〉 family,\nthe following lemma removes the necessity of comparing ( n+1\n2\n) pairs of policies, or n ( n+1\n2 ) pairs of estimated state values.\nLemma 28 (π−+j -success in MC(G = 1)) In MC(G = 1), policy π −+ j with some 0 ≤ j ≤ n dominates 〈π−+k 〉 on V̂ if and only if the following hold simultaneously. (1) V̂ π −+ j (sj) > V̂ π−+j−1(sj).\n(2) V̂ π −+ j (sj+1) > V̂ π−+j+1(sj+1). For π−+0 , only (2) is required, while for π −+ n only (1) is required.\nProof Consider an arbitrary π−+k such that k > j. By Lemma 25, V̂ π−+j (si) = V̂ π−+k (si) for all i ≤ j and i > k. For j < i ≤ k, on the other hand, it holds that V̂ π −+ j (sj+1) < V̂ π −+ j (sj+2) < ... < V̂ π−+j (sk), and V̂ π−+k (sj+1) ≥ V̂ π −+ k (sj+2) ≥ ... ≥ V̂ π −+ k (sk). Therefore, π−+j dominates π −+ k if and only if V̂ π−+j (sj+1) > V̂ π−+k (sj+1). Since V̂ π−+k (sj+1) = V̂ π −+ j+1(sj+1), that is equivalent to V̂ π−+j (sj+1) > V̂ π−+j+1(sj+1). Likewise, for k < j, π −+ j dominates π−+k if and only if V̂ π−+j (sj) > V̂ π−+j−1(sj). Hence the lemma.\nLemma 29 (π−+j -success in MC(G = 1 ∞)) In MC(G == 1 ∞), policy π −+ 0 dominates 〈π−+k 〉 on V̂ if and only if (0) V̂ π −+ 0 (s1) > V̂ π−+1 (s1).\nIf this does not happen, then policy π−+j with some 0 < j ≤ n dominates 〈π −+ k 〉 on V̂ if\nand only if the following hold simultaneously.\n(1) V̂ π −+ j (sj) > V̂ π−+j−1(sj). (2) V̂ π −+ j (sj+1) > V̂ π−+j+1(sj+1). For π−+n only (1) is required.\nProof The additional condition (0) here is actually the special case of (2) with j = 0. The complication of logic here is due to the fact that the always-forward policy π−+0 has a different state value expression compared to other 〈π−+k 〉 in MC(G = 1 ∞). Let\nF̂k := ∏n−1 i=k γp̂i 1−γ(1−p̂i) for k < n and F̂n := 1. Consider the dominance relationship between π−+0 and any π −+ j with j 6= 0. By Lemma 25, for all i > j we have V̂ π −+ 0 (si)− V̂ π −+ j (si) = γF̂i( F̂1\n1−γF̂1 rG − 11−γ rD) = γF̂i(V̂\nπ−+0 (s1) − V̂ π −+ 1 (s1)). Therefore, π −+ 0 dominates any π −+ j\nwith j 6= 0 if and only if V̂ π −+ 0 (s1) > V̂ π−+1 (s1), just as in Lemma 28 for π −+ 0 . If this condition is not satisfied, then π−+0 dominates no one in 〈π −+ k 〉, and therefore Lemma 28 for π−+j with j 6= 0 can be applied as usual, hence the lemma.\nBy Lemma 28 and 29, the occurrence of π-success event for any policy in 〈π−+k 〉 can now be decided simply by comparing one to three pairs of estimated state values.\nMore importantly, in both V̂ π −+ j (sj) > V̂ π−+j−1(sj) and V̂ π−+j (sj+1) > V̂ π−+j+1(sj+1), one side of the inequalities are always computed using rule (a) or (b) in Lemma 25, that is,\nV π −+ j (sj) = γj−1 1−γ rD for MC(H = 1), or V π−+j (sj) = γ 1−γ rD and V π−+1 (s1) = 1 1−γ rD for MC(H =∞). There is no estimated transition probability P̂ involved in these expressions at all, and thus these estimated state values are constant and always equal to the actual values, regardless of P̂ . The other side of the inequalities, on the other hand, are all in the form of V̂ π −+ k (sk+1) for some 0 ≤ k < n. Therefore, the π-success probability Pπ in MC are essentially the joint probability that some specific V̂ π −+ k (sk+1) as functions of random variables P̂ exceed or be exceeded by some constants.\nTo provide readers a more direct impression, the expressions of success probability Pπ −+ 0\nfor the always-forward policy π−+0 is given in the following theorem. Theorem 30 (π−+0 -success Probability) Let F̂1 := ∏n−1 i=1 γp̂i 1−γ(1−p̂i) , then it holds that\nPπ −+ 0 MC(G=1),A0(m),τm = P( F̂1 1− γ rG > 1 1− γ rD|〈Ns,a〉)P(Etrav|m),\nPπ −+ 0\nMC(G= 1 ∞ ),A0(m),τm\n= P( F̂1\n1− γF1 rG >\n1\n1− γ rD|〈Ns,a〉)P(Etrav|m).\nThe visit numbers 〈Ns,a〉 here are decided by Lemma 20 and Definition 17, and P(Etrav|m) equals to the one in Theorem 16.\nProof By Lemma 28, Lemma 29 and Lemma 25 (a)(b), we have\nP(Eπ −+ 0 |Etrav,MC ,A0(m), τm) = P(V̂ π −+ 0 (s1) > 1\n1− γ rD|〈Ns,a〉).\nThen by Lemma 25 (c)(d) and the second level decomposition Lemma 11.\nFor the sake of space we will not list all expressions of the π-success probability for 〈π−+k 〉 here. The expressions for other policies in 〈π −+ k 〉 can be obtained effortlessly by applying Lemma 25 to Lemma 28 and Lemma 29 in the same manner as Theorem 30.\nRemark 31 By change p̂ back to p in the expressions of Theorem 30, we get the sufficient and necessary conditions of rG and rD that make π −+ 0 to be the actual optimal policy. (a) MC(G = 1) F1 1−γ rG > 1 1−γ rD\n(b) MC(G = 1 ∞) F1 1−γF1 rG > 1 1−γ rD where F1 is defined as in Lemma 25. This answers the question of the value setting of rG and rD in Section 6.2.\nSince P(Etrav|m) can be computed by Theorem 16 without effort, we have eventually transformed the π−+k -success probabilities into computing the joint probability of events V̂ π −+ k (sk+1) > C and V̂ π−+k−1(sk) < C ′ for some constants C and C ′ that can also be computed easily by following Lemma 25. Therefore, the question remain is whether the cumulative distribution of these V̂ π −+ k (sk+1) can be computed or not.\nThe answer is “Yes, arithmetically”. By observing (c) and (d) in Lemma 25, it is clear that all relevant V̂ π −+ k (sk+1) here are some function of F̂k+1, while F̂k+1 is a function of P̂ . By Rule 2 and 3 of Definition 17, the probability mass function of P̂ is\nP ( p̂i = b\nN̄+i\n) = ( N̄+i b ) pbi(1− pi)N̄ + i −b.\nThen the probability mass function of F̂k+1 is\nP ( F̂k+1 = n−1∏ i=k+1\nγ bi N̄+i\n1− γ(1− bi N̄+i )\n) = n−1∏ i=k+1 ( N̄+i bi ) pbii (1− pi) N̄+i −bi .\nBy further substitute F̂k+1 for V̂ π−+k (sk+1), we have the probability mass function of V̂ π −+ k (sk+1) and thus its cumulative distribution function. For example, in MC(G = 1), it is\nP(V̂ π −+ k (sk+1) ≤ x) = ∑ b 1 ( x ≥ rG 1− γ n−1∏ i=k+1\nγ bi N̄+i\n1− γ(1− bi N̄+i )\n) n−1∏ i=k+1 ( N̄+i bi ) pbii (1−pi) N̄+i −bi ,\nwhere 1(X) is the indicator function, b = (b1, b2, ..., bn−1) and bi ranges from 0 to N̂ + i . In conclusion, by combining Lemma 25 (the expressions of V̂ ), Lemma 20 (the expressions of 〈Ns,a〉), Lemma 28 and Lemma 29 (π−+k -success conditions), the success probabilities 〈Pπ −+ k 〉 are solved from the arithmetic point of view."
    }, {
      "heading" : "7. A Practical Approximation",
      "text" : "Despite having an arithmetic solution already, the actual computation process for π−+k - success probabilities is still rather complicated. Therefore, we introduce a useful approximation to π−+k -success probabilities in this section, which is both easier to compute and more intuitive.\nBy examining (c) and (d) in Lemma 25 more closely, it can be found that except for the\ncase of k = 0 in MC(G = 1 ∞), the expressions of V̂ π−+k (sk+1) are linear to F̂k+1. Since F̂k itself takes the form of the product of sequence of γp̂i1−γ(1−p̂i) , this inspires us to approximate the cumulative distributions of V̂ π −+ k (sk+1) into log-normal distributions by applying the central limit theorem.\nLemma 32 Let Xi := ln γp̂i 1−γ(1−p̂i) , µi := EXi, σ 2 i := VarXi, and k be a positive integer.\nThen it holds that ∑n−1 i=k (Xi − µi)√∑n−1\ni=k σ 2 i\nd−→ N (0, 1)\nwhere N (0, 1) is a standard normal distribution.\nProof (sketch) By applying Lindeberg’s Central Limit Theorem.\nFor the sake of space and relevance, the full proof of this lemma will not be presented here. The next lemma states that F̂k can be approximated to log-normal distribution.\nLemma 33 If n is sufficiently large, then we have the approximation\nF̂k ∼̇ lnN ( n−1∑ i=k µi, n−1∑ i=k σ2i ),\nwhere lnN (0, 1) is a standard log-normal distribution such that lnX ∼ N (0, 1) is equivalent to X ∼ lnN (0, 1). Proof Because F̂k = ∏n−1 i=k γp̂i 1−γ(1−p̂i) , we have ln F̂k = ∑n−1 i=k Xi. Then by Lemma 32, we have ln F̂k− ∑n−1 i=k µi√∑n−1\ni=k σ 2 i\nd−→ N (0, 1). Therefore, if n is sufficiently large, we have ln F̂k ∼̇\nN ( ∑n−1 i=k µi, ∑n−1 i=k σ 2 i ), or F̂k ∼̇ lnN ( ∑n−1 i=k µi, ∑n−1 i=k σ 2 i ).\nBy Lemma 33 and Lemma 25, V̂ π −+ k (sk+1) can also be approximated to log-normal, except for the special case of k = 0 in MC(G = 1 ∞). However, from the empirical results in Section 8 it can be observed that, not only V̂ π −+ k (sk+1) follows log-normal distribution very closely in general cases, even in the case of k = 0 in MC(G = 1 ∞) the approximation is rather good as well.\nThe remaining task in this section is to obtain the parameters of the log-normal distributions for V̂ π −+ k (sk+1). Actually, the parameters of a log-normal distribution can be written as a function of its mean and variance. Therefore, working out the mean and variance of V̂ π −+ k (sk+1), rather than the original parameters of log-normal distribution, is sufficient for the purpose.\nThe following lemma, a special case of the delta method (Oehlert, 1992), provides a useful tool for approximating the mean and variance of functions random variables.\nLemma 34 Suppose X is a random variable with finite moments, µX being its mean and VarX being its variance. Suppose f is a sufficiently differentiable function. Then it holds that\nEf(X) ≈ f(µX),\nVar f(X) ≈ f ′(µX)2 VarX.\nProof By Taylor’s theorem, f(X) = f(µX) + f ′(µX)(X − µX) + Remainder. Therefore, Ef(X) = Ef(µX) + Ef ′(µX)(X − µX) + E[Remainder] ≈ Ef(µX) + Ef ′(µX)(X − µX) = f(µX) + f\n′(µX)E(X − µX). Because E(X − µX) = 0, we have Ef(X) ≈ f(µX). Similarly, Var f(X) ≈ Var f(µX) + Var f ′(µX)(X − µX) = f ′(µX)2 Var (X − µX). Be-\ncause Var (X − µX) = VarX, we have Var f(X) ≈ f ′(µX)2 VarX.\nBy applying Lemma 34 to F̂k+1, the mean and the variance of F̂k+1 can be derived. Then by applying the lemma again to V̂ π −+ k (sk+1), we have the mean and the variance of V̂ π −+ k (sk+1) that decide the log-normal distribution. The concrete results are as follows.\nLemma 35 Let Yi := γp̂i 1−γ(1−p̂i) , and C := 1−γ γ . Then it holds that\nEF̂k+1 = n−1∏ i=k+1 EYi\nVar F̂k+1 = n−1∏ i=k+1 ( VarYi + (EYi)2 ) − n−1∏ i=k+1 (EYi)2\nwhere\nEYi ≈ pi\npi + C\nVarYi ≈ C2 (pi + C)4 · pi(1− pi) N̄+i .\nProof By Definition 17 and p̂i = N+i,i N+i , we have Ep̂i = pi and Var p̂i = pi(1−pi)N̄+i . Because Yi := γp̂i\n1−γ(1−p̂i) = p̂i p̂i+C , by applying Lemma 34 we have EYi ≈ pipi+C and VarYi ≈\nC2 (pi+C)4 · pi(1−pi) N̄+i . Because 〈N+i,i〉 are regarded as independent variables, 〈Yi〉 are uncorrelated. Therefore E[ ∏n−1 i=k+1 Yi] = ∏n−1 i=k+1 EYi. Also it holds that Var [ ∏n−1 i=k+1 Yi] = E[ ∏n−1 i=k+1 Y 2 i ]−\n(E[ ∏n−1 i=k+1 Yi]) 2 = ∏n−1 i=k+1 EY 2i − ∏n−1 i=k+1(EYi)2 = ∏n−1 i=k+1 ( VarYi+(EYi)2 ) − ∏n−1 i=k+1(EYi)2.\nSince F̂k+1 = ∏n−1 i=k+1 Yi, the lemma is proved.\nLemma 36 The following holds for V̂ π −+ k (sk+1) with 0 ≤ k < n in corresponding MC .\n(a) MC(G = 1)\nEV̂ π −+ k (sk+1) = rG 1− γ EF̂k+1\nVar V̂ π −+ k (sk+1) = r2G (1− γ)2 Var F̂k+1.\n(b) MC(G = 1 ∞), π −+ k with k 6= 0\nEV̂ π −+ k (sk+1) = (rG + γ\n1− γ rD)EF̂k+1\nVar V̂ π −+ k (sk+1) = (rG + γ\n1− γ rD)\n2 Var F̂k+1.\n(c) MC(G = 1 ∞), π −+ 0\nEV̂ π −+ 0 (s1) ≈\nrG\n1− γ EF̂1 EF̂1\nVar V̂ π −+ 0 (s1) ≈ r2G (1− γ EF̂1)4 Var F̂1.\nProof Proving (a) and (b) are trivial since E[cX] = cEX and Var [cX] = c2 VarX with c being any constant. For case (c), since V π −+ 0 (s1) =\nF1 1−γF1 rG, we have to apply delta\nmethod. By Lemma 34, Var V̂ π −+ 0 (s1) ≈ ((V̂ π −+ 0 )′(EF̂1))2 Var F̂1 =\n( rG (1−γ EF̂1)2 )2 Var F̂1.\nBy combining Lemma 35 and Lemma 36, we are now able to approximate the mean and the variance of V̂ π −+ k (sk+1). Because the probability mass function of V̂\nπ−+k (sk+1) can be approximated by a log-normal, and the probability of π-success event equals to the joint probability of the satisfaction of conditions in Lemma 28 and Lemma 29, the π-success probabilities can now be approximated as well.\nAs an example, the probability of π−+0 -success event, P π−+0 MC ,A0(m),τm , can be approximated\nby the following theorem.\nTheorem 37 Let µ′ := ln µ√ 1+σ2/µ2\nand σ′ := √ ln(1 + σ 2\nµ2 ) where µ = EV̂ π\n−+ 0 (s1) and\nσ2 = Var V̂ π −+ 0 (s1) are computed as in Lemma 36. Then it holds that\nPπ −+ 0 MC ,A0(m),τm ≈ [ 1− Φ ( ln rD1−γ − µ ′\nσ′\n)] n−1∏ i=1 (1− (1− pi)m)\nwhere Φ is the cumulative distribution function of the standard normal distribution.\nProof A log-normal distribution with mean µ and variance σ2 has a cumulative distribution of P(X ≤ x) = Φ( lnx−µ ′\nσ′ ) where µ ′ and σ′ are computed as above. By Theorem 30, we\nhave P(Eπ −+ 0 |Etrav,MC ,A0(m), τm) = P(V̂ π −+ 0 (s1) > 1 1−γ rD|〈Ns,a〉) ≈ 1 − Φ\n( ln\nrD 1−γ−µ ′\nσ′\n) .\nThen by the second-level decomposition Lemma 11 and Theorem 16.\nAgain for the sake of space we will not fully elaborate the approximation results for the entire 〈π−+k 〉 family. The remaining ones can be easily obtained by replacing the π-success conditions according to Lemma 28 and Lemma 29. The computational complexity of this approximation is only O(n), and therefore is more computationally efficient than applying Theorem 30 directly be computing the exact distribution of V̂ π −+ k (sk+1)."
    }, {
      "heading" : "8. Empirical Verification",
      "text" : "We conducted several experiments to verify our main results, in particular Theorem 16 for the probability of traverse event, Lemma 20 for the expected visit numbers, and Theorem 37 for the approximation of π-success probabilities Pπ.\nThe experiments were carried out by executing the Optimistic Prototype Strategy (OPS) in prototype chains. The OPS was implemented based on R-MAX algorithm (Brafman and Tennenholtz, 2002; Kakade, 2003). The only difference between the OPS and the original R-MAX is that the former has to decide a final output policy while the latter has not. Therefore, in OPS, when the number of time steps reaches τm, i.e. when the optimistic exploration ends, an additional Value Iteration process will be executed with all observations collected so far as its input, in order to decide the final output policy. This modify does not actually change the exploration behaviour of R-MAX. The implemented OPS is also behaviourally indistinguishable to the one defined in Definition 13. The only parameter of OPS is m, just as in its own definition, and as in the original R-MAX.\nIn all of the following experiments, the discount factor γ was set to 0.998, and the stopping criteria of Value Iteration algorithm was Bellman residual < 10−6. The goal reward rG was set to 1, while the distracting reward rD was set to 0.001, unless otherwise stated. According to Remark 31, this setting is sufficient to ensure the always-forward policy π−+0 be the optimal one.\nAs for the forward transition probability 〈pi〉, two sets of settings are used. In the first setting, 〈pi〉 was set the same fixed value for all states; the specific values used will be stated later. In the second setting, 〈pi〉 took first n − 1 numbers from a pre-generated table of random real numbers, i.e. pi = tablei. The numbers in this table were uniformly randomly chosen between 0.3 and 0.7. To keep the results comparable, the table was created before the following experiments, and had never been changed thereafter.\nBecause our experiments involved comparing empirical distributions of binary random variable to the theoretical ones, all experiments were repeated for 1, 000 times to ensure that the empirical distributions can properly reflect the actual ones."
    }, {
      "heading" : "8.1 Verification for Traverse Probability",
      "text" : "The first theoretical result to be verified is the expression for the traverse probability in Theorem 16. What the theorem claims is rather straightforward: in all four prototype chains, the traverse probability for A0(m) at τm is ∏n−1 i=1 (1 − (1 − pi)m), regardless of the hazardousness H or the goal productivity G of the chains.\nWe executed the OPS on all four prototype chains MC with length n = 40 and 〈pi〉 = 0.3. The exploration parameter m of OPS was set from 5 to 15. Each pair of (MC ,m) were executed for 1000 times, and each run was terminated at τ = τm or exceeding the maximum step budget of 300, 000.\nThe experimental results are shown in Figure 5. As can be seen from the figure, the difference between the traverse probabilities in four prototype chains is negligible, and all experimental results are very close to their theoretical values. Friedman test was also carried out to test the null hypothesis that the results of the four prototype chains and the theoretical values come from the same distribution. The resulting p-value was 0.8397, and thus the null hypothesis was not rejected, meaning that there is no significant difference between the traverse probability of four prototype chains and their theoretical values.\nIt is also interesting to see if the theoretical values are correct under different chain length. The setting of 〈pi〉 = 0.3 was still used, while the length of chains n was set as 10, 15, 20, ..., 60. The exploration parameter m was set to 10.\nThe results are shown in Figure 6. Again, no significant difference between the actual traverse probabilities in four chains and their theoretical values were observed. The result of Friedman test was 0.9665, failing to reject the null hypothesis that there is no difference between the actual and theoretical values.\nExperiments on the chains with varying 〈pi〉 were conducted as well. In these experiments, then length of chain was set to 40, and 〈pi〉 for four chains were set according to the same pre-generated random number table, as mentioned before. The traverse probabilities under different m (again, 5 to 15) were compared.\nThe results are shown in Figure 7. Once more, there were no significant difference between the theoretical and the actual traverse probabilities. The p-value of Friedman test was 0.4552, thus the null hypothesis claiming no difference was not rejected.\nThe above three sets of experimental results demonstrate that Theorem 16 provides highly accurate predictions to the traverse probabilities. Additionally, there are some intuitions that can be obtained from the results. Figure 5 and Figure 7 show that, in the same chain MDP, encouraging optimistic exploration by increasing m will increase the traverse probability, and the rate of increase appears to be an s-shaped curve. Figure 6, on the other hand, shows that under the same degree of optimism, the traverse probability in larger chain MDPs are less than the smaller ones, and it can drop quite quickly (from 0.77 to 0.18 in our experiments) with the increase of scale.\nAlthough the qualitative part of these intuitions can be drawn from general experiences easily, the quantitative part are not that obvious, especially for the rates of change. By knowing the rates of change, practitioners are now able to reasonably make trade-off between the learning costs and the gain of such costs. More importantly, because the theoretical values are highly accurate, these intuitions can be inferred directly from these theoretical values without any actual runs of experiment, eliminating the need of trial-and-error on the parameters. This can be very helpful in real-world applications where interacting with the environment can be very expensive."
    }, {
      "heading" : "8.2 Verification for Visit Numbers",
      "text" : "The second theoretical result to be examined is Lemma 20, the expressions for expected visit numbers 〈N̂s, a〉 at τm given that the traverse event has occurred. The second and the third rules in Definition 17 should also be verified to see if these abstraction are able to conserve the dispersion of the visit numbers. Because the occurrence of traverse event is a precondition of these results, all runs without occurrence of traverse event in this subsection were re-executed until the traverse event happens.\nAccording to Lemma 20, the expected visit numbers of backward actions and goal actions are trivially m, while the visit numbers of forward actions, N̄+i , have different expressions in different prototype chains. An interesting observation from these expressions is, N̄+i in MC(H = 1) are invariant to the state position i, while in MC(H =∞) they are linear to i.\nIf the forward transition probabilities 〈pi〉 are set identical, then N̄+i in MC(H = 1) should have the same value, while in MC(H = ∞) they should be in a straight line. Therefore, we executed OPS with m = 15 in four prototype chains with n = 15 and 〈pi〉 = 0.3 to see whether these properties actually exist.\nThe experimental results are shown in Figure 8. The distribution of the actual visit numbers in 1,000 runs are displayed by the box plots, and the sample means are marked by the circles. The theoretical values are drawn in the solid line. It is clear from the results that the expressions of expected visit numbers in Lemma 20 are in accordance with the reality. The observation mentioned above are confirmed as well: N̄+i remained the same in Figure 8 (a) and (c), while decreased linearly to the state position i in (b) and (d).\nAnother important observation from Figure 8 is that all actual visit numbers 〈N+i 〉 showed certain degree of dispersion. According to Rule 2 and Rule 3 in Definition 17, the actual visit numbers 〈N+i 〉 should be dealt as if they were fixed to 〈N̄ + i 〉, while the dispersion of estimated transition probabilities 〈p̂i〉 come from the binomial distribution followed by the transition visit numbers 〈N̄+i,i〉. Since the impact of the dispersion on later analysis is propagated through 〈p̂i〉, it is crucial to verify whether the dispersion are preserved through the abstraction of Definition 17.\nTherefore, we calculated the standard deviation of 〈p̂i〉 from the experimental data above and compared them to the theoretical ones. Specifically, according to Definition 17,\nwe have N+i,i ∼ Binomial(N̄ + i , pi) and p̂i = N+i,i N̄+i , hence the theoretical standard deviation of\np̂i is σp̂i = √ pi(1−pi) N̂+i . This should match the empirical standard deviations of 〈p̂i〉 if the abstraction of Definition 17 preserves the dispersion.\nThe results are shown in Figure 9. In each plot, the dashed line in the middle is the theoretical mean Ep̂i = pi, and the pair of lines immediately above and below indicates the theoretical interval of pi ± σp̂i , while the outermost pair indicates pi ± 2σp̂i . The empirical standard deviation from the experimental data are marked by the circles. Clearly seen from the figure, the rules of Definition 17 effectively preserves the dispersion of estimated transitions originated from the visit numbers, which is exactly what we need for further analysis on π-success probabilities."
    }, {
      "heading" : "8.3 Verification for Success Probability",
      "text" : "Now it comes to our primary result, the success probability, to be verified. According to Theorem 12, the ε-success probability Pε is merely a plain sum of π-success probabilities Pπ over the set of relevant policies. Since the π-success probabilities themselves are the product of the traverse probability P(Eπtrav) and the conditional probability P(Eπ|Eπtrav), and the former has already been verified in Section 8.1, the latter is of main concern in this subsection.\nAs indicated in the conditions of π−+k -success (Lemma 28 and Lemma 29), these probabilities are equivalent to the ones of which some certain estimated state values V π −+ k (sk+1) exceeding or being exceeded by some constants. Section 7 further provides a log-normal approximation to the cumulative distribution of V π −+ k (sk+1) to simplify the computation of theoretical π-success probabilities. Therefore, our experiments were focused on verifying the goodness of this approximation to the distribution of V π −+ k (sk+1).\nExperiments were conducted by executing OPS with m ranged from 8 to 20 on four prototype chains. As in Section 8.2, the runs with traverse event not occurred were reexecuted in order to block the effect of the possible absence of traverse event. Nevertheless, we set length n = 20 and the fixed transition probability 〈pi〉 = 0.5 to raise the probability of traverse event. The same experiments were also conducted in four prototype chains with their 〈pi〉 set according to the pre-generated random number table. The estimated state values V̂ π −+ 0 (s1) at τm were collected for all these experiments.\nThe theoretical and empirical distribution of V̂ π −+ 0 (s1) are shown in Figure 10 for identical 〈pi〉 = 0.5, and in Figure 11 for random 〈pi〉. The empirical distributions are displayed in box plots. Additionally, their sample mean and the points one and two standard deviation away are marked by circle. Their corresponding theoretical values are drawn in solid lines.\nNo significant difference between the theoretical and empirical values can be observed from the figures. One-sample Kolmogorov-Smirnov test was conducted to decide whether the null hypotheses that the data came from log-normal distributions with theoretical parameters suggested by Theorem 37 can be rejected or not. The result was, at the 1% significance level, none of the null hypotheses with different m on different chains were rejected for all m and all chains with 〈pi〉 = 0.5. For chains with 〈pi〉 taken from the random number table, four rejects occurred within 4×(20−8+1) = 52 groups of data. One of them was m = 8 in MC(H = 1, G = 1), and the rest were m = 8, 10, 17 in MC(H = 1, G = 1 ∞).\nThis is a reasonable result because as discussed in Section 7, according to Lemma 33 and Lemma 25, V̂ π −+ 0 (s1) approximates log-normal if n is large enough, and if it is not the case that G = 1∞ . Since no rejection occurred in MC(H = ∞, G = 1 ∞), and only 3 rejections occurred within 13 groups of data in MC(H = 1, G = 1 ∞), it can be concluded that the log-normal approximation is highly accurate for G = 1, and sufficiently accurate even for G = 1∞ .\nAlthough the experiments above are already sufficient to support our theoretical results, it is still interesting to see how the actual success probability curve looks like. Therefore, we conducted experiments to compare the theoretical success probability, approximated by cumulative distribution function of log-normal in Theorem 37, with the empirical success probability of OPS running in prototype chains with n = 20, 〈pi〉 = 0.5. In order to increase the chance of the agent being distracted by backward actions so that it could be observed clearly, the distracting reward rD was set to the 0.993(1− γ)V π −+ 0 (s1). Under this setting, the value V̂ π −+ 0 (s1) estimated by the agent must be greater than 99.3% of its real value, or the percent error must be less than 0.7%, to achieve a strict success, or otherwise the agent will be distracted by the backward actions.\nThe experimental results is shown in Figure 12. The sample success probability was computed from dividing the number of successful runs by the total number of runs 1000,\nand the results are marked by circles in the figure. Additionally, each set of 1000 runs was uniformly separated to 10 groups, each with 100 runs, to draw the box plot for the success probability. The approximated theoretical probabilities by Theorem 37 are displayed by the solid lines. The four prototype chains respectively lying from the top to the bottom are MC(H =∞, G = 1), MC(H = 1, G = 1), MC(H =∞, G = 1∞), and MC(H = 1, G = 1 ∞).\nFrom Figure 12 it can be observed that our theoretical success probabilities can properly reflect their actual values. There seems to be a slight underestimate of success probability for the theoretical results, most visible for MC(H = 1, G = 1 ∞). The reason for this underestimate may be the inaccuracy of planning algorithm. Actually, because the planning algorithm (Value Iteration in our experiments) was halted whenever Bellman residual was less than 10−6, the agent could not really tell the difference between the actions when their estimated values were very close. This could increase the chance of fake success where if the stopping criteria was set smaller, the output policy could change. Since this additional chance of success is not significant according the experimental results, it does not affect the validity of our approach.\nMore importantly, Figure 12 demonstrates that the formulation of success probability of exploration itself is helpful to capture and distinguish the difficulty of exploration in different MDPs. Clearly, under the same degree of optimistic exploration, represented by m, the success probability of exploration in MC(H =∞, G = 1) is the highest with almost always 1, while in two MC(G = 1 ∞) chains it is notably lower. This displays the impact of goal productivity G. Compared to MC(G = 1 ∞), a G = 1 indicates a much higher goal productivity, in which the fruitfulness of reaching the goal is less dependent to the hardness of arriving it. Therefore, the values for policies always trying to achieve goal are more likely to be accurately estimated in these chains, resulting in a lower requirement for active exploration than the G = 1∞ cases.\nOn the other hand, comparing the results with same goal productivity G shows that the chains with higher hazardousness, H =∞, require less active exploration than the less hazardous ones, H = 1. This sounds a little bit counter-intuitive since a higher hazardousness should implicate a higher hardness of exploration by definition. The reason behind this ostensible contradiction is actually very clear. From the theoretical and empirical results of visit numbers in Section 8.2, it can be discovered that in MC(H = ∞), the number of collected observations till the ending of exploration, τ = τm, is much greater than in MC(H = 1). In other words, the sample size used for estimating V̂ in the cases of H =∞ is larger than that of H = 1, resulting in a higher accuracy for the former. If we fix the success probability to the same value, then H =∞ requires more observations than H = 1, and thus the former is still harder to explore than the latter, which accords well with the intuition. The moral is, as what we have discussed in Section 4.3, in order to compare the hardness of exploration among different MDPs, the number of observations τε,δ and the activeness of exploration θε,δ required to achieve certain outcome with respect to (ε, δ) must be considered together.\nTo summarize, the experimental results above verified that our analytical approach is able to predict the outcome of exploration with high accuracy. Furthermore, they showed the usefulness of our approach in forecasting and explaining the exploration behaviour of the agent, as well as in comparing the hardness of exploration among different MDPs."
    }, {
      "heading" : "9. Applying the Theory",
      "text" : "Back in Sections 6 and 7, there has been a long journey climbing up the dependency graph (Figure 3) to its very summit to obtain the success probability of exploration P. However, the actual procedure of obtaining P is not at all that complicated, since much of the previous analysis serves for justifying the approach rather than being a part of the necessary steps. In this section, we provide an instance to demonstrate how the theory proposed in this paper can be applied in more general MDPs, then summarize the procedure of analysis in a short practice guide."
    }, {
      "heading" : "9.1 An Instance of Application to General MDPs",
      "text" : "In this subsection, we will demonstrate that our analytical approach is applicable not only to the prototypes, but also to the general MDPs.\nThe MDP that will be investigated in this section is shown in Figure 13 (a). It is an instance of the maze domain, where the general objective for the agent is to reach goal from the start point without falling into the traps.\nSpecifically, the agent starts from the cell marked with “S”. At each time step, the agent can choose one of the four directions and attempt to go to the adjacent cell in that direction. There is a probability p that the attempt of moving succeeds, and if it fails, the agent will still be in the same cell at next time step. This can be seen as an abstraction of the real-world source of uncertainty, such as the imperfection of state representation due to the discretization of continuous space, or the imperfection of action execution. Also, the agent is not allowed to walk into the blocked cells, which are filled with black, or pass through the walls at the edges of the grid. Stepping into the trapped cells marked with “T”, on the other hand, will result in the agent to be transited back to the starting cell, and therefore this should be avoided. Finally, at the goal cell marked with “G”, an additional action of collecting the reward will be available to the agent. By taking this action, the agent will be given some reward rG > 0, and will be send back to the start point.\nThe optimal policy in this MDP is to walk along the path that is annotated by the numbers [1-2-3-...-11] in Figure 13 (b), and collect the reward at the goal point. Other than this optimal one, there also exist many sub-optimal policies that are able to collect the goal reward. For example, following the path of [1-...-4-4′-5′-5-...-11] is a possible policy of this\nkind. Of course, there are also policies that can never reach goal, for example the one that take the path of [1-2-3-(trap)].\nThe research question here comes to be whether our approach for analysing the exploration efficiency can be applied to this maze MDP. Although this maze MDP appears to be rather far from the typical chain MDPs since half of the possible positions are “off-thechain” ones, exploring this MDP is nevertheless very similar to exploring a chain MDP. Actually, from the chain perspective, the impact of the existence of off-the-chain states on the exploration for the optimal policy is very limited. Leaving from and returning to the chain states, for example from state 4 to 4′ and its reverse 4′ to 4, are symmetric in this maze MDP. The agent has no need to pass through chains states more often in order to explore off-the-chain ones, and vice versa. As a result, despite that these off-the-chain states provide the agent some alternative policies to reach the goal, their actual impacts on the success probability of the optimal policy P∗ are negligible.\nTherefore, the original maze MDP can be abstracted to the chain MDP shown in Figure 14. As in the previous figures of chains, the forward actions, in this case the ones following [1-2-3-...-11], are drawn in solid arrows, while the backward actions are drawn in dashed arrows. The self-transitions due to the failure of movement are not drawn in the figure for clarity. All “off-the-chain” states and actions are ignored for aforementioned reasons.\nThe resulting chain MDP differs from the prototypes in two points. First, the hazardousness of the states are mixed with Hi = 1 for odd i and Hi = ∞ for even i. The variety in hazardousness is due to the difference cases of connection to the trapped cells. Second, all actions except for the goal action, including the backward actions, have a chance of (1− p) to fail and result in a self-transition, while in prototype chains only forward actions have failure probabilities. The second point actually does not change the expression of estimated state values for the optimal policy, since the possible effect of taking backward actions is irrelevant to the optimal policy itself. Therefore, the main concern here is how the first points affects the visit numbers.\nBy undertaking the method provided in Section 6.5, specifically by solving the in and out numbers of transitions according to Lemma 19 and Equation 4, the visit numbers for this chain MDP can be obtained easily. Actually, it can be proved that they follow the recurrence formula as below:\nN̄+i =  (1+2p)m\np i = n− 1 N̄+i+1 i is odd and i < n− 1 N̄+i+1 +m i is even and i < n− 1.\nOther theoretical results are directly applicable to this abstracted chain MDP since there are no further critical difference between this one and the prototypes that could affect the validity of our theory.\nWe conducted some experiments to verify our approach to the maze MDP. The Optimistic Prototype Strategy with m ranging from 8 to 20 was executed in the original maze MDP shown in Figure 13 (a) with p = 0.5 and rG = 1. The theoretical and empirical distribution of V̂ π ∗ (s1) is shown in Figure 15.\nAs can be seen from the figure, our theoretical distribution is generally accurate, although there is a slight lose of dispersion in the theoretical distribution of V̂ π ∗ (s1). This is a reasonable result because we simply ignored all off-the-chain states when considering the visit numbers for the convenience of analysis. By dealing with the visit numbers more carefully, this error is likely to be reduced. Nevertheless, the lose of dispersion here does not have a strong impact on the validity of our analysis, since the difference is only noteworthy at the tails of the distribution. Furthermore, Kolmogorov-Smirnov test did not reject the null hypothesis of the data being generated from the theoretical log-normal distribution at 1% significance level under any setting of m, and thus the theoretical values are sufficiently accurate.\nHaving the cumulative distribution of V̂ π ∗ (s1), we are now able to assess the success probability of exploration P∗ according to Theorem 30 without actually performing the learning procedure in the maze MDP. The critical value appeared in Theorem 30 is actually not of importance here. If we are interested in a set of near-optimal policies that could yield k% of the optimal cumulative rewards, then we just need to compute the cumulative distribution at k% of the mean value of V̂ π ∗ (s1), and see what the chance is for π\n∗ to be regarded as a near-optimal solution.\nGiven these points, our approach is applicable to general MDPs with only slight changes to the theoretical results given in previous sections."
    }, {
      "heading" : "9.2 A Short Practice Guide to Our Approach",
      "text" : "The general steps of obtaining and analysing the success probability of exploration is summarized as follows.\n(1) Abstract a chain MDP from the original MDP as in Section 5 and Section 9.1.\n• Try to keep as much as possible of the critical characteristics that affects the hardness of exploration. • It does not need to be one of the prototype chain defined in Section 6.2, since\nmost of the main results apply to non-prototype chains as well.\n(2) Abstract an easy-to-analysis version of exploration strategy from the original one.\n• If the original one is optimistic, it is highly recommended to carry out abstraction based on the Optimistic Prototype Strategy defined in Section 6.3.\n(3) Evaluate the probability of traverse event.\n• In most chain MDPs, Theorem 16 can be applied directly or with trivial modification for OPS and its modifications.\n(4) Evaluate the visit numbers.\n• Although Lemma 20 cannot directly be applied to general chains, Lemma 19 and Equation 4 are still valid, and thus the expected visit numbers can usually be solved without effort as in Section 9.1. The result is likely to be similar in the form with the ones in Lemma 20. • After obtaining the expected visit numbers, apply Rule 2 and Rule 3 of Definition\n17 to evaluate the distribution of actual visit numbers.\n(5) Solve Bellman equations to obtain the expressions of state values.\n• Not only that this step is independent to exploration strategies, but the expressions in Lemma 25 (c) and (d) are also irrelevant to the hazardousness. Since most general chains differ from the prototype ones in that the hazardousness level varies among the chain states, these expressions are directly applicable to general chains. • Expressions in Lemma 25 (a) and (b) merely serve for deciding the critical values\nthat the key state values must exceed or being exceeded, as discussed Lemma 28 and Lemma 29. It is likely that they can be inferred directly from prior knowledge without solving Bellman equations, as in the example of maze domain in Section 9.1.\n(6) Evaluate the final success probability.\n• There are two key steps: first, decide the relevant estimated state values and their corresponding critical values; second, estimate the cumulative distribution of relevant estimated state values. • The first step can be accomplished by following the dominance analysis in Lemma\n26, Lemma 28 and Lemma 29. In many cases Lemma 28 and Lemma 29 can be applied directly or by slight modification.\n• The approximation by delta method introduced in Lemma 34 in Section 7 is recommended to decide the distribution of estimated state values. The results of Lemma 35 and Lemma 36 are widely applicable because the expressions here are not dependent to the hazardousness.\nAs can be seen from this summary, a considerable number of main results are directly applicable to the general MDPs. Although some of them may have to be adjusted if the original MDP or the exploration strategy deviate from the prototypes too much, in practice this might not be a problem due to the reasons explained in Section 6.2 and Section 6.3, as long as the strategy is optimistic."
    }, {
      "heading" : "10. Conclusion and Discussion",
      "text" : "There has been a long-lasting gap between theory and practice of reinforcement learning. Although several analytical frameworks have been established in the literature, in general they lack the ability to satisfy the practical needs. This paper is an attempt of bridging the gap under a new framework, namely the success probability of exploration, so that the practice can actually benefit from the theory, rather than simply ignores it and relies on experiences and domain knowledge.\nLooking back to the previous sections, we have formulated the success probability of exploration, introduced its basic properties, elaborated our concrete approach to evaluating it, and verified our approach via empirical results. We have also showed that our novel framework does not suffer from the problems as the previous ones, and demonstrated that it can be used to comprehensively solve the three groups of questions mentioned in Section 1. Although our framework and approach may not cover every problem encountered by us RL practitioners every day, we believe that our attempt here will be helpful for accelerating the process of closing the gap and making theories more useful.\nOur analytical framework is the first one that is able to explain and predict the behaviour and possible outcome of exploration strategy in such a detailed manner. To our best knowledge, there is no previous work concerning elaborating the visit numbers for state-action pairs and transitions as ours in Section 6.5, nor to connect them with the final outcomes as in Sections 6.6 and 6.7. We consider these parts as necessary steps for gaining deeper vision of exploration behaviour and sample efficiency of reinforcement learning, and therefore we hope our results could benefit a wider range of theorists as well.\nNevertheless, much works still remain to be done in the future. The chain perspective in Section 5 is still under construction, and a more rigorous formulation is in need. A constructive or algorithmic method of abstracting chain MDPs from general ones is also highly desirable. If such method is made available, then we will be able to compare the hardness of exploration between any two general MDPs, which may lead to deeper understanding of the efficiency of exploration and better strategies. Another possible research direction is to extend the results to non-optimistic exploration strategies, for example the Bayesian approaches. Lastly, our current theoretical analysis mainly focus on finite discrete MDPs; it is interesting to see if our theoretical results can be generalized to other cases, in particular the continuous MDPs."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to acknowledge..."
    } ],
    "references" : [ {
      "title" : "Exploration and apprenticeship learning in reinforcement learning",
      "author" : [ "Pieter Abbeel", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "Abbeel and Ng.,? \\Q2005\\E",
      "shortCiteRegEx" : "Abbeel and Ng.",
      "year" : 2005
    }, {
      "title" : "Logarithmic online regret bounds for undiscounted reinforcement learning",
      "author" : [ "Peter Auer", "Ronald Ortner" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Auer and Ortner.,? \\Q2007\\E",
      "shortCiteRegEx" : "Auer and Ortner.",
      "year" : 2007
    }, {
      "title" : "R-max–a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "Ronen I. Brafman", "Moshe Tennenholtz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Brafman and Tennenholtz.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brafman and Tennenholtz.",
      "year" : 2002
    }, {
      "title" : "Bayesian Q-learning",
      "author" : [ "Richard Dearden", "Nir Friedman", "Stuart J. Russell" ],
      "venue" : "In Proceedings of the Fifteenth National Conference on Artificial Intelligence",
      "citeRegEx" : "Dearden et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Dearden et al\\.",
      "year" : 1998
    }, {
      "title" : "Action-gap phenomenon in reinforcement learning",
      "author" : [ "Amir-massoud Farahmand" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Farahmand.,? \\Q2011\\E",
      "shortCiteRegEx" : "Farahmand.",
      "year" : 2011
    }, {
      "title" : "Efficient reinforcement learning",
      "author" : [ "Claude-Nicolas Fiechter" ],
      "venue" : "In Proceedings of the seventh annual conference on Computational learning theory,",
      "citeRegEx" : "Fiechter.,? \\Q1994\\E",
      "shortCiteRegEx" : "Fiechter.",
      "year" : 1994
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Thomas Jaksch", "Ronald Ortner", "Peter Auer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Jaksch et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "Leslie P. Kaelbling", "Michael L. Littman", "Andrew W. Moore" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Kaelbling et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Kaelbling et al\\.",
      "year" : 1996
    }, {
      "title" : "On the sample complexity of reinforcement learning",
      "author" : [ "Sham M. Kakade" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Kakade.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kakade.",
      "year" : 2003
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kearns and Singh.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kearns and Singh.",
      "year" : 2002
    }, {
      "title" : "Bandit based monte-carlo planning",
      "author" : [ "Levente Kocsis", "Csaba Szepesvári" ],
      "venue" : "In European conference on machine learning,",
      "citeRegEx" : "Kocsis and Szepesvári.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kocsis and Szepesvári.",
      "year" : 2006
    }, {
      "title" : "Near-Bayesian exploration in polynomial time",
      "author" : [ "J. Zico Kolter", "Andrew Y. Ng" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning,",
      "citeRegEx" : "Kolter and Ng.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolter and Ng.",
      "year" : 2009
    }, {
      "title" : "Near-optimal PAC bounds for discounted MDPs",
      "author" : [ "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Lattimore and Hutter.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lattimore and Hutter.",
      "year" : 2014
    }, {
      "title" : "Sample complexity bounds of exploration",
      "author" : [ "Lihong Li" ],
      "venue" : "In Reinforcement Learning,",
      "citeRegEx" : "Li.,? \\Q2012\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2012
    }, {
      "title" : "On the complexity of solving markov decision problems",
      "author" : [ "Michael L. Littman", "Thomas L. Dean", "Leslie P. Kaelbling" ],
      "venue" : "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "Littman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Littman et al\\.",
      "year" : 1995
    }, {
      "title" : "How hard is my MDP? The distribution-norm to the rescue",
      "author" : [ "Odalric-Ambrym Maillard", "Timothy A. Mann", "Shie Mannor" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Maillard et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Maillard et al\\.",
      "year" : 2014
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "A note on the delta method",
      "author" : [ "Gary W. Oehlert" ],
      "venue" : "The American Statistician,",
      "citeRegEx" : "Oehlert.,? \\Q1992\\E",
      "shortCiteRegEx" : "Oehlert.",
      "year" : 1992
    }, {
      "title" : "Online regret bounds for undiscounted continuous reinforcement learning",
      "author" : [ "Ronald Ortner", "Daniil Ryabko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ortner and Ryabko.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ortner and Ryabko.",
      "year" : 2012
    }, {
      "title" : "Learning curves in machine learning. In Encyclopedia of Machine Learning, pages 577–580",
      "author" : [ "Claudia Perlich" ],
      "venue" : null,
      "citeRegEx" : "Perlich.,? \\Q2011\\E",
      "shortCiteRegEx" : "Perlich.",
      "year" : 2011
    }, {
      "title" : "V-max: tempered optimism for better PAC reinforcement learning",
      "author" : [ "Karun Rao", "Shimon Whiteson" ],
      "venue" : "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume",
      "citeRegEx" : "Rao and Whiteson.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rao and Whiteson.",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning for robot soccer",
      "author" : [ "Martin Riedmiller", "Thomas Gabel", "Roland Hafner", "Sascha Lange" ],
      "venue" : "Autonomous Robots,",
      "citeRegEx" : "Riedmiller et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Riedmiller et al\\.",
      "year" : 2009
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "An empirical evaluation of interval estimation for markov decision processes",
      "author" : [ "Alexander L. Strehl", "Michael L. Littman" ],
      "venue" : "In Tools with Artificial Intelligence (ICTAI),",
      "citeRegEx" : "Strehl and Littman.,? \\Q2004\\E",
      "shortCiteRegEx" : "Strehl and Littman.",
      "year" : 2004
    }, {
      "title" : "A theoretical analysis of model-based interval estimation",
      "author" : [ "Alexander L. Strehl", "Michael L. Littman" ],
      "venue" : "In Proceedings of the 22nd International Conference on Machine learning,",
      "citeRegEx" : "Strehl and Littman.,? \\Q2005\\E",
      "shortCiteRegEx" : "Strehl and Littman.",
      "year" : 2005
    }, {
      "title" : "Reinforcement learning in finite MDPs: PAC analysis",
      "author" : [ "Alexander L. Strehl", "Lihong Li", "Michael L. Littman" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Strehl et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2009
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Model-based reinforcement learning with nearly tight exploration complexity bounds",
      "author" : [ "István Szita", "Csaba Szepesvári" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "Szita and Szepesvári.,? \\Q2010\\E",
      "shortCiteRegEx" : "Szita and Szepesvári.",
      "year" : 2010
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "Leslie G. Valiant" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Valiant.,? \\Q1984\\E",
      "shortCiteRegEx" : "Valiant.",
      "year" : 1984
    }, {
      "title" : "Bayesian reinforcement learning",
      "author" : [ "Nikos Vlassis", "Mohammad Ghavamzadeh", "Shie Mannor", "Pascal Poupart" ],
      "venue" : "In Reinforcement Learning,",
      "citeRegEx" : "Vlassis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vlassis et al\\.",
      "year" : 2012
    }, {
      "title" : "Complexity and cooperation in Q-learning",
      "author" : [ "Steven D. Whitehead" ],
      "venue" : "In Proceedings of the Eighth International Workshop on Machine Learning,",
      "citeRegEx" : "Whitehead.,? \\Q1991\\E",
      "shortCiteRegEx" : "Whitehead.",
      "year" : 1991
    }, {
      "title" : "Increasingly cautious optimism for practical PAC-MDP exploration",
      "author" : [ "Liangpeng Zhang", "Ke Tang", "Xin Yao" ],
      "venue" : "In Proceedings of the 24th International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Exploration is an essential process for Reinforcement Learning (RL) agents to resolve uncertainty (Sutton and Barto, 1998).",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "Numerous exploration strategies have been designed and proposed in the literature, and some of the most popular ones among them are ε-greedy, Boltzmann action selection, Explicit Explore or Exploit (Kearns and Singh, 2002), R-MAX (Brafman and Tennenholtz, 2002), Upper Confidence RL (Jaksch et al.",
      "startOffset" : 198,
      "endOffset" : 222
    }, {
      "referenceID" : 2,
      "context" : "Numerous exploration strategies have been designed and proposed in the literature, and some of the most popular ones among them are ε-greedy, Boltzmann action selection, Explicit Explore or Exploit (Kearns and Singh, 2002), R-MAX (Brafman and Tennenholtz, 2002), Upper Confidence RL (Jaksch et al.",
      "startOffset" : 230,
      "endOffset" : 261
    }, {
      "referenceID" : 6,
      "context" : "Numerous exploration strategies have been designed and proposed in the literature, and some of the most popular ones among them are ε-greedy, Boltzmann action selection, Explicit Explore or Exploit (Kearns and Singh, 2002), R-MAX (Brafman and Tennenholtz, 2002), Upper Confidence RL (Jaksch et al., 2010), and Bayesian approaches (Vlassis et al.",
      "startOffset" : 283,
      "endOffset" : 304
    }, {
      "referenceID" : 29,
      "context" : ", 2010), and Bayesian approaches (Vlassis et al., 2012).",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "1 from the textbook (Sutton and Barto, 1998) will usually be tried first.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 28,
      "context" : "In addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al.",
      "startOffset" : 177,
      "endOffset" : 243
    }, {
      "referenceID" : 5,
      "context" : "In addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al.",
      "startOffset" : 177,
      "endOffset" : 243
    }, {
      "referenceID" : 8,
      "context" : "In addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al.",
      "startOffset" : 177,
      "endOffset" : 243
    }, {
      "referenceID" : 25,
      "context" : "In addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al.",
      "startOffset" : 177,
      "endOffset" : 243
    }, {
      "referenceID" : 1,
      "context" : ", 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al., 2010).",
      "startOffset" : 38,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : ", 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al., 2010).",
      "startOffset" : 38,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "MoR-MAX (Szita and Szepesvári, 2010), V-MAX (Rao and Whiteson, 2012), ICR and ICV (Zhang et al.",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "MoR-MAX (Szita and Szepesvári, 2010), V-MAX (Rao and Whiteson, 2012), ICR and ICV (Zhang et al.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 31,
      "context" : "MoR-MAX (Szita and Szepesvári, 2010), V-MAX (Rao and Whiteson, 2012), ICR and ICV (Zhang et al., 2015)), Model-Based Interval Estimation (Strehl and Littman, 2005), and UCRLγ (Lattimore and Hutter, 2014), have been proved to have sample complexity bounds polynomial to the scale parameters of the learning task.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : ", 2015)), Model-Based Interval Estimation (Strehl and Littman, 2005), and UCRLγ (Lattimore and Hutter, 2014), have been proved to have sample complexity bounds polynomial to the scale parameters of the learning task.",
      "startOffset" : 42,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : ", 2015)), Model-Based Interval Estimation (Strehl and Littman, 2005), and UCRLγ (Lattimore and Hutter, 2014), have been proved to have sample complexity bounds polynomial to the scale parameters of the learning task.",
      "startOffset" : 80,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "The UCRL families are also proved to have regret bounds sublinear to the horizon of the cumulative rewards (Jaksch et al., 2010; Ortner and Ryabko, 2012).",
      "startOffset" : 107,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "The UCRL families are also proved to have regret bounds sublinear to the horizon of the cumulative rewards (Jaksch et al., 2010; Ortner and Ryabko, 2012).",
      "startOffset" : 107,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "For example, the PAC theory for R-MAX (Strehl et al., 2009) requires its parameter m to be set polynomial to the scale parameters of the learning task so that its sample complexity can be polynomial as well.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "However, in practice m is usually fixed to some value around 10-20 (Strehl and Littman, 2004) regardless of the scale of the task, which violates the basic condition of the PAC theory.",
      "startOffset" : 67,
      "endOffset" : 93
    }, {
      "referenceID" : 19,
      "context" : "the plot of generalization error of the learned model against the size of the training dataset or the number of executed iterations (Perlich, 2011).",
      "startOffset" : 132,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "This approach can also be applied to Reinforcement Learning by plotting the current total reward or the expected cumulative reward over time (Sutton and Barto, 1998).",
      "startOffset" : 141,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "There has been some works related to the hardness questions, such as action gap (Farahmand, 2011) and distribution-norm (Maillard et al.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "There has been some works related to the hardness questions, such as action gap (Farahmand, 2011) and distribution-norm (Maillard et al., 2014).",
      "startOffset" : 120,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "The UCRL families are also proved to have regret bounds sublinear to the horizon of the cumulative rewards (Jaksch et al., 2010; Ortner and Ryabko, 2012). The main drawbacks of these analyses is that their theoretical results are not sufficiently relevant to the practical needs. For example, the PAC theory for R-MAX (Strehl et al., 2009) requires its parameter m to be set polynomial to the scale parameters of the learning task so that its sample complexity can be polynomial as well. However, in practice m is usually fixed to some value around 10-20 (Strehl and Littman, 2004) regardless of the scale of the task, which violates the basic condition of the PAC theory. Meanwhile, the PAC theory does not provide any prediction of the performance of R-MAX with its m fixed to small values like 10-20. This results in a strange dilemma where practitioners have to choose one between theoretical performance guarantee and actual efficiency, and in most cases the latter is chosen, leaving the former invalid in practice. In Zhang et al. (2015), some workarounds are proposed so that the practitioners are not forced to discard theoretical guarantees in exchange for efficiency.",
      "startOffset" : 108,
      "endOffset" : 1045
    }, {
      "referenceID" : 26,
      "context" : "In this paper we follow the standard reinforcement learning framework in Sutton and Barto (1998), where an agent continuously interacts with a stochastic environment, learns its dynamic properties, and searches for the optimal policy that could lead to maximum expected cumulative rewards.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "Some popular planning algorithms, for example Value Iteration (Puterman, 1994), have been proved that their calculated state values converge to the true optimal values in the limit, or to the near-optimal ones in polynomial time under some assumptions (Littman et al., 1995).",
      "startOffset" : 252,
      "endOffset" : 274
    }, {
      "referenceID" : 26,
      "context" : "The model-free learning algorithms such as Temporal Difference and Q-Learning (Sutton and Barto, 1998), on the other hand, do not build models explicitly, but use Equations 1, 2 or their modified versions to update the estimated values directly.",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 26,
      "context" : "Although many reinforcement learning algorithms are guaranteed to converge to optimal policies if all state-action pairs have been visited infinitely many times (Sutton and Barto, 1998), in reality the resources for acquiring observations are far less than infinite.",
      "startOffset" : 161,
      "endOffset" : 185
    }, {
      "referenceID" : 27,
      "context" : "There lies a big gap between the best upper and lower bounds (Szita and Szepesvári, 2010; Lattimore and Hutter, 2014) been discovered.",
      "startOffset" : 61,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "There lies a big gap between the best upper and lower bounds (Szita and Szepesvári, 2010; Lattimore and Hutter, 2014) been discovered.",
      "startOffset" : 61,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "This leads to a paradoxical situation where, if one decide to set the exploration parameters according to the PAC theories, then the learning agent is very likely to over-explore as if it is in the most difficult MDP, resulting in poor actual performance despite its PAC guarantee (Kolter and Ng, 2009; Zhang et al., 2015).",
      "startOffset" : 281,
      "endOffset" : 322
    }, {
      "referenceID" : 31,
      "context" : "This leads to a paradoxical situation where, if one decide to set the exploration parameters according to the PAC theories, then the learning agent is very likely to over-explore as if it is in the most difficult MDP, resulting in poor actual performance despite its PAC guarantee (Kolter and Ng, 2009; Zhang et al., 2015).",
      "startOffset" : 281,
      "endOffset" : 322
    }, {
      "referenceID" : 23,
      "context" : "Our formulation is inspired by the notion of Probably Approximately Correct (PAC, Valiant (1984); Fiechter (1994); Kakade (2003)) which tries to figure out how many observations are needed to be ε-optimal with probability at least 1 − δ.",
      "startOffset" : 82,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "Our formulation is inspired by the notion of Probably Approximately Correct (PAC, Valiant (1984); Fiechter (1994); Kakade (2003)) which tries to figure out how many observations are needed to be ε-optimal with probability at least 1 − δ.",
      "startOffset" : 98,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "Our formulation is inspired by the notion of Probably Approximately Correct (PAC, Valiant (1984); Fiechter (1994); Kakade (2003)) which tries to figure out how many observations are needed to be ε-optimal with probability at least 1 − δ.",
      "startOffset" : 98,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "The PAC optimality in Fiechter (1994) refers to a local ε-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local ε-optimality along the states the agent actually visits during learning.",
      "startOffset" : 125,
      "endOffset" : 160
    }, {
      "referenceID" : 25,
      "context" : "The PAC optimality in Fiechter (1994) refers to a local ε-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local ε-optimality along the states the agent actually visits during learning.",
      "startOffset" : 125,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : "The PAC optimality in Fiechter (1994) refers to a local ε-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al.",
      "startOffset" : 22,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "The PAC optimality in Fiechter (1994) refers to a local ε-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local ε-optimality along the states the agent actually visits during learning. In our formulation, the output policy has to be ε-optimal in all states of the MDP in order to be ε-successful. Therefore, an ε-success must be ε-optimal in the PAC framework of Fiechter (1994) and Kakade (2003), but the converse is not necessarily correct.",
      "startOffset" : 22,
      "endOffset" : 449
    }, {
      "referenceID" : 5,
      "context" : "The PAC optimality in Fiechter (1994) refers to a local ε-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local ε-optimality along the states the agent actually visits during learning. In our formulation, the output policy has to be ε-optimal in all states of the MDP in order to be ε-successful. Therefore, an ε-success must be ε-optimal in the PAC framework of Fiechter (1994) and Kakade (2003), but the converse is not necessarily correct.",
      "startOffset" : 22,
      "endOffset" : 467
    }, {
      "referenceID" : 6,
      "context" : "Additionally, this lemma provides an intuition about why the optimism principle proposed by Kaelbling et al. (1996) is so broadly accepted in designing the exploration strategies.",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "Abbeel and Ng (2005); Riedmiller et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Abbeel and Ng (2005); Riedmiller et al. (2009); Mnih et al.",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Abbeel and Ng (2005); Riedmiller et al. (2009); Mnih et al. (2015)), their successes are often more dependent on the state/action feature engineering, prior knowledge, the (near-)deterministic environment, and generalization techniques.",
      "startOffset" : 0,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Literature have shown that some benchmarks considered non-trivial are actually relatively easy (Maillard et al., 2014), while some seemingly impossible tasks are solved efficiently by rather simple algorithms (e.",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "Literature have shown that some benchmarks considered non-trivial are actually relatively easy (Maillard et al., 2014), while some seemingly impossible tasks are solved efficiently by rather simple algorithms (e.g. Mnih et al. (2015); Silver et al.",
      "startOffset" : 0,
      "endOffset" : 234
    }, {
      "referenceID" : 13,
      "context" : "Literature have shown that some benchmarks considered non-trivial are actually relatively easy (Maillard et al., 2014), while some seemingly impossible tasks are solved efficiently by rather simple algorithms (e.g. Mnih et al. (2015); Silver et al. (2016)).",
      "startOffset" : 0,
      "endOffset" : 256
    }, {
      "referenceID" : 3,
      "context" : "Dearden et al. (1998); Strehl and Littman (2004); Kolter and Ng (2009)) is shown in Figure 1.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Dearden et al. (1998); Strehl and Littman (2004); Kolter and Ng (2009)) is shown in Figure 1.",
      "startOffset" : 0,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "Dearden et al. (1998); Strehl and Littman (2004); Kolter and Ng (2009)) is shown in Figure 1.",
      "startOffset" : 0,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "In Whitehead (1991), it has been proved that in a homogeneous problem solving task, the expected number of observations required by a Q-Learning agent with an ε-greedy exploration strategy to find an optimal policy is exponential to the number of steps required by the optimal policy to arrive at the goal state.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "Another expression of this theorem, proposed by Li (2012), is as follows: in a chain MDP, a Q-Learning agent with an ε-greedy exploration strategy, starting from the start state, needs observations exponential to the length of the chain in order to reach the goal state.",
      "startOffset" : 48,
      "endOffset" : 58
    }, {
      "referenceID" : 26,
      "context" : "It is difficult to find a representative strategy that is able to reflect the fundamental properties of both vanilla strategies such as ε-greedy and Boltzmann selection rule (Sutton and Barto, 1998), and the more advanced strategies.",
      "startOffset" : 174,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesvári, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al.",
      "startOffset" : 66,
      "endOffset" : 156
    }, {
      "referenceID" : 27,
      "context" : "The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesvári, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al.",
      "startOffset" : 66,
      "endOffset" : 156
    }, {
      "referenceID" : 12,
      "context" : "The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesvári, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al.",
      "startOffset" : 66,
      "endOffset" : 156
    }, {
      "referenceID" : 31,
      "context" : "The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesvári, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al.",
      "startOffset" : 66,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : ", 2015) as well as the strategies with regret bound guarantees (Jaksch et al., 2010).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "It is also applied in some Monte-Carlo tree search algorithms such as UCT (Kocsis and Szepesvári, 2006).",
      "startOffset" : 74,
      "endOffset" : 103
    }, {
      "referenceID" : 29,
      "context" : "There also exists a different family of non-optimistic exploration strategies, namely the Bayesian strategies (Vlassis et al., 2012).",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "It is often known as “optimism in the face of uncertainty” or “the optimism principle” Kaelbling et al. (1996), which has been already mentioned in the discussion of the second-level decomposition (Lemma 11).",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "This prototype strategy is based on the famous baseline PAC-MDP strategy R-MAX (Brafman and Tennenholtz, 2002; Kakade, 2003).",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "This prototype strategy is based on the famous baseline PAC-MDP strategy R-MAX (Brafman and Tennenholtz, 2002; Kakade, 2003).",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "The following lemma, a special case of the delta method (Oehlert, 1992), provides a useful tool for approximating the mean and variance of functions random variables.",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "The OPS was implemented based on R-MAX algorithm (Brafman and Tennenholtz, 2002; Kakade, 2003).",
      "startOffset" : 49,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : "The OPS was implemented based on R-MAX algorithm (Brafman and Tennenholtz, 2002; Kakade, 2003).",
      "startOffset" : 49,
      "endOffset" : 94
    } ],
    "year" : 2016,
    "abstractText" : "Exploration has been a crucial part of reinforcement learning, yet several important questions concerning exploration efficiency are still not answered satisfactorily by existing analytical frameworks. These questions include exploration parameter setting, situation analysis, and hardness of MDPs, all of which are unavoidable for practitioners. To bridge the gap between the theory and practice, we propose a new analytical framework called the success probability of exploration. We show that those important questions of exploration above can all be answered under our framework, and the answers provided by our framework meet the needs of practitioners better than the existing ones. More importantly, we introduce a concrete and practical approach to evaluating the success probabilities in certain MDPs without the need of actually running the learning algorithm. We then provide empirical results to verify our approach, and demonstrate how the success probability of exploration can be used to analyse and predict the behaviours and possible outcomes of exploration, which are the keys to the answer of the important questions of exploration.",
    "creator" : "LaTeX with hyperref package"
  }
}