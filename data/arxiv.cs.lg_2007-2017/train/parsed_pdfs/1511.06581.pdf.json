{
  "name" : "1511.06581.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DUELING NETWORK ARCHITECTURES FOR DEEP REINFORCEMENT LEARNING",
    "authors" : [ "Ziyu Wang", "Nando de Freitas", "Marc Lanctot" ],
    "emails" : [ "ziyu@google.com", "nandodefreitas@google.com", "lanctot@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Over the past years, deep learning has contributed to dramatic advances in scalability and performance of machine learning (LeCun et al., 2015). One exciting application is the sequential decision-making setting of reinforcement learning (RL) and control. Notable examples include deep Q-learning (Mnih et al., 2015), deep visuomotor policies (Levine et al., 2015), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al., 2015). Other recent successes include massively parallel frameworks (Nair et al., 2015) and expert move prediction in the game of Go (Maddison et al., 2015), which have produced policies matching those of Monte Carlo tree search programs.\nIn spite of this, most of the approaches for RL use standard neural networks, such as convolutional networks, MLPs, LSTMs and autoencoders. The focus in these recent advances has been on designing improved control and RL algorithms, or simply on incorporating existing neural network architectures into RL methods. Here, we take an alternative but complementary approach of focusing primarily on innovating a neural network architecture that is better suited for model-free RL. This approach has the benefit that the new architecture can be easily combined with existing and future algorithms for RL.\nWe propose a new model, which we name the dueling architecture, that explicitly separates the representation of state values and (statedependent) action advantages. The dueling architecture consists of two streams, sharing a common convolutional feature learning module, which represent the value and advantage functions. The two streams are aggregated to produce an estimate of the state-action value function; see Figure 1.\nIntuitively, the dueling architecture can learn which states are (or are not) valuable, without having to learn the effect of each action for each state. This is particularly useful when the agent encounters itself in states where its actions do not affect the environment in any\nar X\niv :1\n51 1.\n06 58\n1v 1\n[ cs\n.L G\n] 2\n0 N\nov 2\n01 5\nrelevant way. To illustrate this, consider the saliency maps shown in Figure 21. These maps were generated by computing the Jacobians of the trained value and advantage streams with respect to the input video, following the method proposed by Simonyan et al. (2013). (The experimental section describes this methodology in more detail.) The figure shows the value and advantage saliency maps for two different time steps. In one time step (leftmost pair of images), we see that the value network stream pays attention to the road and in particular to the horizon, where new cars appear. It also pays attention to the score. The advantage stream on the other hand does not pay much attention to the visual input because its action choice is practically irrelevant when there are no cars in front. However, in the second time step (rightmost pair of images) the advantage stream pays attention as there is a car immediately in front, making its choice of action very relevant.\nIn the experiments, we demonstrate that the dueling architecture can more quickly identify the correct action during policy evaluation as redundant or similar actions are added to the learning problem. We also evaluate the gains brought in by the dueling architecture on the challenging Atari 2600 testbed. Here, an RL agent with the same structure and hyper-parameters must be able to play 57 different games by observing image pixels and game scores only. We show that our approach outperforms the baseline Deep Q-Networks (DQN) of Mnih et al. (2015) on 50 out of 57 games and the state-of-the-art Double DQN of van Hasselt et al. (2015)) on 46 out of 57 games."
    }, {
      "heading" : "1.1 RELATED WORK",
      "text" : "The notion of maintaining separate value and advantage functions goes back to Baird (1993). In Baird’s original advantage updating algorithm, the shared Bellman residual update equation is decomposed into two updates: one for a state value function, and one for its associated advantage function. Advantage updating was shown to converge faster than Q-learning in simple continuous time domains in (Harmon et al., 1995). Its successor, the advantage learning algorithm, represents only a single advantage function (Harmon & Baird, 1996).\nThe dueling architecture represents both the value V π(s) and advantage Aπ(s, a) functions with a single deep model whose output combines the two to produce a state-action valueQ(s, a). Unlike in advantage updating, the representation and algorithm are decoupled by construction. Consequently, the dueling architecture can be used in combination with a myriad of model free RL algorithms.\nThere is a long history of advantage functions in policy gradients, starting with Sutton et al. (2000). As a recent example of this line of work, Schulman et al. (2015) estimate advantage values online to reduce the variance of policy gradient algorithms.\nThere have been several attempts at playing Atari with deep reinforcement learning, including Mnih et al. (2015); Guo et al. (2014); Stadie et al. (2015); Nair et al. (2015) and van Hasselt et al. (2015). The results of van Hasselt et al. (2015) are the current published state-of-the-art.\n1The saliency videos are available at https://www.youtube.com/watch?v= TpGuQaswaHs&list=PLkmHIkhlFjiS2EGp3QNITDbA08MIYzupP.\nSimultaneously with our work, innovations in prioritized memory replay (see the co-submission by Schaul et al. (2015)) and in increasing the action gap (Bellemare et al., 2016) have also outperformed the state-of-the-art (van Hasselt et al., 2015). The improvements brought in by the prioritized replay and the dueling architecture are particularly dramatic. The two approaches have great merit on their own, while pursuing two very different but complementary research directions. Combining our dueling architecture with prioritized replay should lead to further improvements in the state-ofthe-art."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "We consider a sequential decision making setup, in which an agent interacts with an environment E over discrete time steps, see Sutton & Barto (1998) for an introduction. In the Atari domain, for example, the agent perceives a video st consisting ofM image frames: st = (xt−M+1, . . . , xt) ∈ S at time step t. The agent then chooses an action from a discrete set at ∈ A = {1, . . . , |A|} and observes a reward signal rt produced by the game emulator.\nThe agent seeks maximize the expected discounted return, where we define the discounted return as Rt = ∑ τ=t γ\nτ−trτ . In this formulation, γ ∈ [0, 1] is a discount factor that trades-off the importance of immediate and future rewards.\nFor an agent behaving according to a stochastic policy π(a|s), the values of the state-action pair (s, a) and the state s at time t are defined as follows\nQπ(s, a) = E [Rt| st = s, at = a, π] , and V π(s) = Ea∼π(s) [Qπ(s, a)] . (1)\nThe preceding state-action value function (Q function for short) can be computed recursively with dynamic programming:\nQπ(s, a) = Es′ [ r + γEa′∼π(s′) [Qπ(s′, a′)] | s, a, π ] . (2)\nThe optimal Q function is defined as Q∗(s, a) = maxπ Qπ(s, a). Under the deterministic policy a = argmaxa′∈AQ\n∗(s, a′), it follows that V ∗(s) = maxaQ∗(s, a). From this, it also follows that the optimal Q function satisfies the Bellman equation:\nQ∗(s, a) = Es′ [ r + γmax\na′ Q∗(s′, a′) | s, a\n] . (3)\nWe define another important quantity, the advantage function, relating the value and Q functions:\nAπ(s, a) = Qπ(s, a)− V π(s). (4)\nNote that Ea∼π(s) [Aπ(s, a)] = 0. Intuitively, the value function V measures the importance of being in a particular state s. The Q function, however, measures the importance about the value of choosing each possible action when in this state. The advantage function subtracts the value of the state from the Q function to obtain a relative measure of the importance of each action."
    }, {
      "heading" : "2.1 DEEP Q-NETWORKS",
      "text" : "The value functions as described in the preceding section are infinite dimensional objects. To approximate them, we can use a deep Q-network: Q(s, a; θ) with parameters θ. To estimate this network, we optimize the following sequence of loss functions at iteration i:\nLi(θi) = Es,a,r,s′ [( yDQNi −Q(s, a; θi) )2] , (5)\nwith yDQNi = r + γmax\na′ Q(s′, a′; θ−i ), (6)\nwhere θ−i represents the parameters of a fixed and separate target network. We could attempt to use standard Q-learning to learn the parameters of the network Q(s, a; θ) online. However, this estimator performs poorly in practice. A key innovation in (Mnih et al., 2015) was to freeze the parameters of the target network Q(s′, a′; θ−i ) for a fixed number of iterations while updating the\nonline networkQ(s, a; θi) by gradient descent. (This greatly improves the stability of the algorithm.) The specific gradient update is\n∇θiLi(θi) = Es,a,r,s′ [( r + γmax\na′ Q(s′, a′; θ−i )−Q(s, a; θi)\n) ∇θiQ(s, a; θi) ] (7)\nThis approach is model free in the sense that the states and rewards are produced by the environment. It is also off-policy because these states and rewards are obtained with an behavior policy (epsilon greedy in DQN) different from the online policy that is being learned.\nAnother key ingredient behind the success of DQN is experience replay (Lin, 1993; Mnih et al., 2015). During learning, the agent accumulates a dataset Dt = {e1, e2, . . . , et} of experiences et = (st, at, rt, st+1) from many episodes. When training the Q-network, instead only using the current experience as prescribed by standard temporal-difference learning, the network is trained by sampling mini-batches of experiences from D uniformly at random. The sequence of losses thus takes the form\nLi(θi) = E(s,a,r,s′)∼U(D) [( yDQNi −Q(s, a; θi) )2] . (8)\nExperience replay increases data efficiency through re-use of experience samples in multiple updates and, importantly, it reduces variance as uniform sampling from the replay buffer reduces the correlation among the samples used in the update."
    }, {
      "heading" : "2.2 DOUBLE DEEP Q-NETWORKS",
      "text" : "The previous section described the main components of DQN as presented in (Mnih et al., 2015). In this paper, we use the improved Double DQN (DDQN) learning algorithm of van Hasselt et al. (2015). In Q-learning and DQN, the max operator uses the same values to both select and evaluate an action. This can therefore lead to overoptimistic value estimates (van Hasselt, 2010). To mitigate this problem, DDQN uses the following target:\nyDDQNi = r + γQ(s ′, argmax a′ Q(s′, a′; θi); θ − i ), (9)\nThe pseudo-code for DDQN is the same as for DQN (see Mnih et al. (2015)), but with the target yDQNi replaced by y DDQN i ."
    }, {
      "heading" : "3 THE DUELING NETWORK ARCHITECTURE",
      "text" : "The key insight behind our new architecture, as illustrated in Figure 2, is that for many states, it is unnecessary to estimate the value of each action choice. For example, in the Enduro game setting, knowing whether to move left or right only matters when a collision is eminent. In some states, it is of paramount importance to know which action to take, but in many other states the choice of action has no repercussion on what happens.\nTo bring this insight to fruition, we must design a network architecture capable of learning the value of a state irrespective of the action choice. In addition, this architecture must also include a component that specifies which actions are preferable in a given state.\nWe accomplish this by considering a decomposition of the Q network into two separate streams: a value stream and an advantage stream, as shown in Figure 1. The input is followed by a number of convolutional and/or pooling layers. Then, the activations of the last of these layers is sent to both separate streams. Each stream contains a number of fully-connected layers. The final layer combines the output of the two streams, and the output of the network is a set of Q values, one for each action. This change in network architecture can be used with any existing algorithms, such as DQN and Double DQN, without modification. Moreover, this two-stream architecture can also take advantage of any improvements to these algorithms, including better replay memories, better exploration policies, intrinsic motivation, reward shaping, and so on.\nWithin this two-stream architecture, one question is how to design the aggregator (i.e., the ⊕\nmodule in Figure 1) for combining the value and advantage streams to produce a Q function. Using the definition of advantage function, from equation (4), we have:\nQ(s, a; θ, α, β) = V̂ (s; θ, β) + Â(s, a; θ, α), (10)\nwhere α, β, and θ denote the separate parameters associated with the advantage stream only, value stream only, and remaining paremeters, respectively. Note that the expression applies to all (s, a) instances; that is, to express equation (10) in matrix form we need to replicate V̂ (s; θ, β) |A| times.\nThe aggregator of equation (10) is unidentifiable in the sense that given Q we cannot recover V̂ and Â. To see this, add a constant to V̂ and subtract the same constant from Â . This constant cancels out resulting in the same Q value. This lack of identifiability is mirrored by poor practical performance when this equation is used directly.\nTo remove the extra degree of freedom in equation (10), we propose the following aggregator:\nQ(s, a; θ, α, β) = V̂ (s; θ, β) + ( Â(s, a; θ, α)− 1\n|A| ∑ a′ Â(s, a′; θ, α)\n) , (11)\ngiving rise to what we call the dueling network. Note that while subtracting the mean helps remove this degree of freedom, it does not change the relative rank of the Â (and henceQ) values, preserving any greedy or -greedy policy based on Q values from equation (10). When acting, it suffices to evaluate the advantage stream to make decisions.\nWith the dueling network, we learn the value stream with every update to theQ values. This frequent updating, enables the dueling network to approximate the state values better. State value estimation is especially important to bootstrapping-based RL algorithms (Sutton & Barto, 1998)."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We now show the practical performance of the dueling network. We start with a simple policy evaluation task and then show larger scale results for learning policies for general Atari game-playing."
    }, {
      "heading" : "4.1 POLICY EVALUATION",
      "text" : "We start by measuring the performance of the dueling architecture on a policy evaluation task. This task is ideal for evaluating network architectures as it is devoid of confounding factors, such as the choice of exploration strategy, and the interaction between policy improvement and policy evaluation.\nIn this experiment, we employ temporal difference learning (without eligibility traces, i.e., λ = 0) to learnQ values. More specifically, given a behavior policy π, we seek to estimate the state-action valueQπ(·, ·) by optimizing the sequence of costs of equation (5), with target\nyi = r + γEa′∼π(s′) [Q(s′, a′; θi)] .\nThe above update rule is the same as that of Expected SARSA (van Seijen et al., 2009). We, however, do not modify the behavior policy as in Expected SARSA.\nTo evaluate the learned Q values, we choose a simple environment where the exact Qπ(s, a) values can be computed separately for all (s, a) ∈ S × A. This environment, which we call the corridor is composed of three connected corridors. A schematic drawing of the corridor environment is shown in Figure 3, The agent starts from the bottom left corner of the environment and must move to the top right to get the largest reward. A total of 5 actions are available: go up, down, left, right and no-op. We also have the freedom of adding an arbitrary number of no-op actions. In our setup, the two vertical sections both have 10 states while the horizontal section has 50.\nWe use an -greedy policy as the behavior policy π, which chooses a random action with probability or an action according to the optimal Q function argmaxa∈AQ\n∗(s, a) with probability 1 − . In our experiments, is chosen to be 0.001.\nWe compare a single-streamQ architecture with the dueling architecture on three variants of the corridor environment with 5, 10 and 20 actions respectively. The 10 and 20 action variants are formed by adding no-ops to the original environment. We measure performance by Squared Error (SE) against the true state values: ∑ s∈S,a∈A(Q(s, a; θ)−Qπ(s, a))2. The single-stream architecture is a three layer MLP with 50 units on each hidden layer. The dueling architecture is also composed of three layers. After the first hidden layer of 50 units, however, the network branches off into two streams each of them a two layer MLP with 25 hidden units. The results of the comparison are summarized in Figure 4.\nWith 5 actions, both architectures converge at about the same speed. But as we increase the number of actions, we can see that the dueling architecture performs better relative to the traditional Qnetwork. The stream V̂ (s; θ, β) learns a general value that is shared across many similar actions at s. This is important as many control tasks with large action spaces manifest this property."
    }, {
      "heading" : "4.2 GENERAL ATARI GAME-PLAYING",
      "text" : "We perform a comprehensive evaluation of our proposed method on the Arcade Learning Environment (Bellemare et al., 2013), which is composed of 57 Atari games. The challenge is to deploy a single algorithm and architecture, with a fixed set of hyper-parameters, to learn to play all the games given only raw pixel observations and game rewards. This environment is very demanding because it is both comprised of a large number of highly diverse games and the observations are high-dimensional.\nWe follow closely the setup of van Hasselt et al. (2015) and compare to their results. Our network architecture has the same low-level convolutional structure of DQN (Mnih et al., 2015; van Hasselt et al., 2015), but it incorporates the dueling streams described in Section 3 after the convolutional layers. We duplicate the fully-connected layers of DQN to construct the dueling architecture streams. In more detail, there are three convolutional layers followed by 2 fully-connected layers. The first convolutional layer has 32 8× 8 filters with stride 4, the second 64 4× 4 filters with stride 2, and the third and final convolutional layer consists 64 3 × 3 filters with stride 1. As depicted in Figure 1, the network diverges into two streams after the convolutional layers, with one stream computing V̂ and the other Â. The value and advantage streams both have a fully-connected layer with 512 units. The final hidden layers of the value and advantage streams are both fully-connected with the value stream having one output and the advantage as many outputs as there are valid actions2. Rectifier non-linearities (Fukushima, 1980) are inserted between all adjacent layers.\n2The number of actions ranges between 4-18 actions in the ALE environment.\nWe adopt the optimizers and hyper-parameters of van Hasselt et al. (2015), with the exception of the learning rate which we chose to be slightly lower (we do not do this for double DQN as it can deteriorate its performance). In addition, we clip the gradients to have their norm less than or equal to 10. This clipping is not standard practice in deep RL, but a common in recurrent network training (Bengio et al., 2013). Since both the advantage and the value stream propagate gradients to the last convolutional layer in the backward pass, we rescale the combined gradient entering the last convolutional layer by 1/ √ 2. This simple heuristic mildly increases stability.\nAs in (van Hasselt et al., 2015), we start the game with up to 30 no-op actions to provide random starting positions for the agent. To evaluate our approach, we measure improvement in percentage (positive or negative) in score over the better of human and DDQN scores:\nImprovement = ScoreAgent − ScoreRandom\nmax{ScoreHuman,ScoreDDQN} − ScoreRandom . (12)\nWe took the maximum over human and DDQN scores as it prevents insignificant changes to appear as large improvements when neither the agent in question nor DDQN are doing well. For example, an agent that achieves 2% human performance should not be interpreted as two times better when DDQN achieves 1% human performance. We also chose not to measure performance in terms of percentage of human performance alone because a tiny difference relative to DDQN on some games can translate into hundreds of percent in human performance difference.\nThe resulting comparison is presented in Figure 5. The mean and median performance against the human performance percentage is shown in Table 13.\nOur approach achieves higher scores compared to the baseline on 80.7% (46 out of 57) of the games. Of all the games with 18 actions, the dueling architecture is better 86.6% of the time (26 out of 30). This is consistent with the findings of the previous section. Overall, our agent achieves human level\n3 When calculating the human performance percentage for the game Video Pinball, the random scores are set to zero. This is because human scores on this game are lower than that of a random agent making the calculation of human performance percentage invalid. Resetting the random score does not affect the median scores of the agents considered. The mean scores, however, are affected.\nperformance on 42 out of 57 games. Raw scores for all the games, as well as measurements in human performance percentage, are presented in Appendix A.\nTo isolate the contributions of the dueling architecture, we re-train DDQN using exactly the same procedure as the one used to train the dueling architecture. Notably, we apply gradient clipping to DDQN, and use 1024 hidden units for the first fully-connected layer of the network in DDQN so that both architectures have roughly the same number of parameters.\nThe dueling architecture does equally or better than the gradient-clipped DDQN on 75.4% of the games (43 out of 57). The gradient-clipped DDQN achieves mean and median scores of 341.2% and 132.6% respectively. These scores are markedly lower than the scores of the dueling architecture. Despite the gains brought in by gradient clipping, the dueling architecture still maintains a significant edge over traditional architectures.\nA few training curves, shown in Figure 6, compare the dueling architecture, and DDQN with the traditional Q architecture with and without gradient clipping.\nRobustness to human starts. One shortcoming of the previous metric is that an agent does not necessarily have to generalize well to play the Atari games. Due to the deterministic nature of the Atari environment, from an unique starting point, an agent could learn to achieve good performance by simply remembering sequences of actions.\nTo obtain a more robust measure, we adopt the methodology of Nair et al. (2015). Specifically, for each game, we use 100 starting points sampled from a human expert’s trajectory. From each of these points, an evaluation episode is launched for up to 108,000 frames. The agents are evaluated only on rewards accrued after the starting point.\nWith this measure, the dueling agent does better than the baseline on 70.2% (40 out of 57) games and on games of 18 actions, the dueling agent is 83.3% better (25 out of 30). The comparison to the baseline in human performance percentage is presented in Table 1.\nSaliency maps. To better understand the roles of the value and the advantage streams, we compute saliency maps (Simonyan et al., 2013). More specifically, to visualize the salient part of the image as seen by the value stream, we compute the absolute value of the Jacobian of V̂ with respect to the input frames: |∇sV̂ (s; θ)|. Similarly, to visualize the salient part of the image as seen by the advantage stream, we compute |∇sÂ(s, argmaxa′ Â(s, a′); θ)|. Both quantities are of the same dimensionality as the input frames and therefore can be visualized easily alongside the input frames.\nHere, we place the gray scale input frames in the green and blue channel and the saliency maps in the red channel. All three channels together form an RGB image. Figure 2 depicts the value and advantage saliency maps on the Enduro game for two different time steps. As observed in the\nintroduction, the value stream pays attention to the horizon where the appearance of a car could affect future performance. The value stream also pays attention to the score. The advantage stream, on the other hand, cares more about cars that are on an immediate collision course."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "We introduced a new neural network architecture that decouples value and advantage in deep Qnetworks, while sharing a common feature learning module. The new architecture, in combination with some algorithmic improvements, leads to dramatic improvements over existing approaches for deep RL in the challenging Atari domain. The new architecture was also shown to provide performance increases in policy evaluation. We also found out that gradient clipping is very effective in this domain and recommend its usage to the deep RL community.\nSince most recent advances in deep RL have been the result of algorithmic improvements (and not architecture improvements), we believe our dueling architecture could lead to further improvements when combined with these new algorithms. Of particular promise is the combination of the dueling architecture with prioritized replay."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank Hado van Hasselt, Arthur Guez, Vlad Mnih, Nicolas Hess, Marc Bellemare, Georg Ostrovski, Tom Schaul and all the folks at Google DeepMind for making this possible."
    }, {
      "heading" : "A APPENDIX",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Multiple object recognition with visual attention",
      "author" : [ "J. Ba", "V. Mnih", "K. Kavukcuoglu" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Ba et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2015
    }, {
      "title" : "Advantage updating",
      "author" : [ "L.C. Baird" ],
      "venue" : "Technical Report WL-TR-93-1146, Wright-Patterson Air Force Base,",
      "citeRegEx" : "Baird,? \\Q1993\\E",
      "shortCiteRegEx" : "Baird",
      "year" : 1993
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Increasing the action gap: New operators for reinforcement learning",
      "author" : [ "M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2016
    }, {
      "title" : "Advances in optimizing recurrent networks",
      "author" : [ "Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
      "author" : [ "K. Fukushima" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Fukushima,? \\Q1980\\E",
      "shortCiteRegEx" : "Fukushima",
      "year" : 1980
    }, {
      "title" : "Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning",
      "author" : [ "X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Guo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-player residual advantage learning with general function approximation",
      "author" : [ "M.E. Harmon", "L.C. Baird" ],
      "venue" : "Technical Report WL-TR-1065, Wright-Patterson Air Force Base,",
      "citeRegEx" : "Harmon and Baird,? \\Q1996\\E",
      "shortCiteRegEx" : "Harmon and Baird",
      "year" : 1996
    }, {
      "title" : "Advantage updating applied to a differential game",
      "author" : [ "M.E. Harmon", "L.C. Baird", "A.H. Klopf" ],
      "venue" : null,
      "citeRegEx" : "Harmon et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Harmon et al\\.",
      "year" : 1995
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "S. Levine", "C. Finn", "T. Darrell", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1504.00702,",
      "citeRegEx" : "Levine et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning for robots using neural networks",
      "author" : [ "L.J. Lin" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Lin,? \\Q1993\\E",
      "shortCiteRegEx" : "Lin",
      "year" : 1993
    }, {
      "title" : "Move Evaluation in Go Using Deep Convolutional Neural Networks",
      "author" : [ "C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Maddison et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2015
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Massively parallel methods for deep reinforcement learning",
      "author" : [ "A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "Maria", "A. De", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver" ],
      "venue" : "In Deep Learning Workshop,",
      "citeRegEx" : "Nair et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2015
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Schaul et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2015
    }, {
      "title" : "High-dimensional continuous control using generalized advantage estimation",
      "author" : [ "J. Schulman", "P. Moritz", "S. Levine", "M.I. Jordan", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1506.02438,",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "author" : [ "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1312.6034,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2013
    }, {
      "title" : "Incentivizing exploration in reinforcement learning with deep predictive models",
      "author" : [ "B.C. Stadie", "S. Levine", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1507.00814,",
      "citeRegEx" : "Stadie et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Stadie et al\\.",
      "year" : 2015
    }, {
      "title" : "Introduction to reinforcement learning",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "R.S. Sutton", "D. Mcallester", "S. Singh", "Y. Mansour" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Sutton et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2000
    }, {
      "title" : "Deep reinforcement learning with double Q-learning",
      "author" : [ "H. van Hasselt", "A. Guez", "D. Silver" ],
      "venue" : "arXiv preprint arXiv:1509.06461,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2015
    }, {
      "title" : "A theoretical and empirical analysis of Expected Sarsa",
      "author" : [ "H. van Seijen", "H. van Hasselt", "S. Whiteson", "M. Wiering" ],
      "venue" : "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,",
      "citeRegEx" : "Seijen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Seijen et al\\.",
      "year" : 2009
    }, {
      "title" : "Embed to control: A locally linear latent dynamics model for control from raw images",
      "author" : [ "M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Watter et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Watter et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art Double DQN method of van Hasselt et al. (2015) in 46 out of 57 Atari games.",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "Notable examples include deep Q-learning (Mnih et al., 2015), deep visuomotor policies (Levine et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : ", 2015), deep visuomotor policies (Levine et al., 2015), attention with recurrent networks (Ba et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : ", 2015), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : ", 2015), and model predictive control with embeddings (Watter et al., 2015).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "Other recent successes include massively parallel frameworks (Nair et al., 2015) and expert move prediction in the game of Go (Maddison et al.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : ", 2015) and expert move prediction in the game of Go (Maddison et al., 2015), which have produced policies matching those of Monte Carlo tree search programs.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "These maps were generated by computing the Jacobians of the trained value and advantage streams with respect to the input video, following the method proposed by Simonyan et al. (2013). (The experimental section describes this methodology in more detail.",
      "startOffset" : 162,
      "endOffset" : 185
    }, {
      "referenceID" : 12,
      "context" : "We show that our approach outperforms the baseline Deep Q-Networks (DQN) of Mnih et al. (2015) on 50 out of 57 games and the state-of-the-art Double DQN of van Hasselt et al.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "We show that our approach outperforms the baseline Deep Q-Networks (DQN) of Mnih et al. (2015) on 50 out of 57 games and the state-of-the-art Double DQN of van Hasselt et al. (2015)) on 46 out of 57 games.",
      "startOffset" : 76,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "Advantage updating was shown to converge faster than Q-learning in simple continuous time domains in (Harmon et al., 1995).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "The notion of maintaining separate value and advantage functions goes back to Baird (1993). In Baird’s original advantage updating algorithm, the shared Bellman residual update equation is decomposed into two updates: one for a state value function, and one for its associated advantage function.",
      "startOffset" : 78,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "There is a long history of advantage functions in policy gradients, starting with Sutton et al. (2000). As a recent example of this line of work, Schulman et al.",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "As a recent example of this line of work, Schulman et al. (2015) estimate advantage values online to reduce the variance of policy gradient algorithms.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "There have been several attempts at playing Atari with deep reinforcement learning, including Mnih et al. (2015); Guo et al.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "(2015); Guo et al. (2014); Stadie et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "(2015); Guo et al. (2014); Stadie et al. (2015); Nair et al.",
      "startOffset" : 8,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "(2015); Guo et al. (2014); Stadie et al. (2015); Nair et al. (2015) and van Hasselt et al.",
      "startOffset" : 8,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "(2015); Guo et al. (2014); Stadie et al. (2015); Nair et al. (2015) and van Hasselt et al. (2015). The results of van Hasselt et al.",
      "startOffset" : 8,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "(2015); Guo et al. (2014); Stadie et al. (2015); Nair et al. (2015) and van Hasselt et al. (2015). The results of van Hasselt et al. (2015) are the current published state-of-the-art.",
      "startOffset" : 8,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "(2015)) and in increasing the action gap (Bellemare et al., 2016) have also outperformed the state-of-the-art (van Hasselt et al.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "Simultaneously with our work, innovations in prioritized memory replay (see the co-submission by Schaul et al. (2015)) and in increasing the action gap (Bellemare et al.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "A key innovation in (Mnih et al., 2015) was to freeze the parameters of the target network Q(s′, a′; θ− i ) for a fixed number of iterations while updating the",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "Another key ingredient behind the success of DQN is experience replay (Lin, 1993; Mnih et al., 2015).",
      "startOffset" : 70,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "Another key ingredient behind the success of DQN is experience replay (Lin, 1993; Mnih et al., 2015).",
      "startOffset" : 70,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "The previous section described the main components of DQN as presented in (Mnih et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "The previous section described the main components of DQN as presented in (Mnih et al., 2015). In this paper, we use the improved Double DQN (DDQN) learning algorithm of van Hasselt et al. (2015). In Q-learning and DQN, the max operator uses the same values to both select and evaluate an action.",
      "startOffset" : 75,
      "endOffset" : 196
    }, {
      "referenceID" : 12,
      "context" : "The pseudo-code for DDQN is the same as for DQN (see Mnih et al. (2015)), but with the target y i replaced by y DDQN i .",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "We perform a comprehensive evaluation of our proposed method on the Arcade Learning Environment (Bellemare et al., 2013), which is composed of 57 Atari games.",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "Our network architecture has the same low-level convolutional structure of DQN (Mnih et al., 2015; van Hasselt et al., 2015), but it incorporates the dueling streams described in Section 3 after the convolutional layers.",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "Rectifier non-linearities (Fukushima, 1980) are inserted between all adjacent layers.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 18,
      "context" : "We follow closely the setup of van Hasselt et al. (2015) and compare to their results.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "This clipping is not standard practice in deep RL, but a common in recurrent network training (Bengio et al., 2013).",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "We adopt the optimizers and hyper-parameters of van Hasselt et al. (2015), with the exception of the learning rate which we chose to be slightly lower (we do not do this for double DQN as it can deteriorate its performance).",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "To obtain a more robust measure, we adopt the methodology of Nair et al. (2015). Specifically, for each game, we use 100 starting points sampled from a human expert’s trajectory.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "To better understand the roles of the value and the advantage streams, we compute saliency maps (Simonyan et al., 2013).",
      "startOffset" : 96,
      "endOffset" : 119
    } ],
    "year" : 2015,
    "abstractText" : "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning inspired by advantage learning. Our dueling architecture represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art Double DQN method of van Hasselt et al. (2015) in 46 out of 57 Atari games.",
    "creator" : "LaTeX with hyperref package"
  }
}