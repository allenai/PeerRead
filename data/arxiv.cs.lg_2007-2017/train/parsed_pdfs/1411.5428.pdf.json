{
  "name" : "1411.5428.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Differentially Private Algorithms for Empirical Machine Learning",
    "authors" : [ "Ben Stoddard", "Yan Chen", "Ashwin Machanavajjhala" ],
    "emails" : [ "stodds@cs.duke.edu", "yanchen@cs.duke.edu", "ashwin@cs.duke.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n54 28\nv2 [\ncs .L\nG ]\n2 1\nN ov\n2 01\nIn this paper, we develop differentially private algorithms that mirror real world empirical machine learning workflows. We consider the private classifier training algorithm as a blackbox. We present private algorithms for selecting features that are input to the classifier. Though adding a preprocessing step takes away some of the privacy budget from the actual classification process (thus potentially making it noisier and less accurate), we show that our novel preprocessing techniques signficantly increase classifier accuracy on three real-world datasets. We also present the first private algorithms for empirically constructing receiver operating characteristic (ROC) curves on a private test set."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Organizations, like statistical agencies, hospitals and internet companies, collect ever increasing amounts of information from individuals with the hope of gleaning valuable insights from this data. The promise of effectively utilizing such ‘big-data’ has been realized in part due to the success of off-the-shelf machine learning algorithms, especially supervised classifiers [3]. As the name suggests, a classifier assigns to a new observation (e.g., an individual, an email, etc.) a class chosen from a set of two or more class labels (e.g., spam/ham, healthy/sick, etc.), based on training examples with known class membership. However, when classifiers are trained on datasets containing sensitive information, ensuring that the training algorithm and the output classifier does not leak sensitive information in the data is important. For instance, Fredrikson et al [8] proposed a model inversion attack using which properties (genotype) of individuals in the training dataset can be learnt from linear regression models built on private medical data.\nTo address this concern, recent work has focused on developing private classifier training algorithms that ensure a strong notion of privacy called ǫ-differential privacy [6] – the classifier output by a differentially private training algorithm does not significantly change due to the insertion or deletion of any one training example. Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9]. All of these techniques work by infusing noise into the training process.\nDespite the burgeoning literature in differentially private classification, their adoption by practitioners in the industry or government agencies has been startlingly rare. We believe there are two important contributing factors. First, we observe that (see experiments in Section 5) an off-the-shelf private classifier training algorithm, when running on real datasets, often results in classifiers with misclassification rates that are significantly higher than that output by a non-private training algorithm. In fact, Fredrikson et al [8] also show that differentially private algorithms for the related problem of linear regression result in unacceptable error when applied to real medical datasets.\nSecond, the state-of-the-art private classification algorithms do not support typical classification workflows. In particular, real datasets usually have many features that are of little to no predictive value, and feature selection techniques [5] are used to identify the predictive subset of features. To date there are no differentially private feature selection techniques.\nMoreover, empirical machine learning workflows perform model diagnostics on a test set that is disjoint from the training set. These diagnostics quantify trained classifier’s prediction accuracy on unseen data. Since the unseen data must be drawn from the same distribution as the training dataset, the classifier is usually trained on a part of the dataset, and tested on the rest of the dataset. Since we assume the training dataset is private, the test dataset used for evaluating the classifier’s prediction accuracy is also private.\nA typical diagnostic for measuring the prediction accuracy of a binary classifier (i.e., two classes) is the receiver operating characteristic (ROC) curve. Recent work [19] has shown that releasing an ROC curve computed on a private test set can leak sensitive information to an adversary with access to certain properties of the test dataset. Currently, there is no known method to privately evaluate the prediction accuracy of a classifier on a private test dataset. Contributions: This paper addresses the aforementioned shortcomings of the current state-of-the-art in differentially private classification. We consider the differentially private classification algorithms as a black box in order to ensure that (a) our algorithms are classifier agnostic, and (b) a privacy non-expert can use our algorithms without any knowledge of how a private classifier works.\nFirst, we develop a suite of differentially private feature selection\ntechniques based on (a) perturbing predictive scores of individual features, (b) clustering features and (c) a novel method called private threshold testing (which may be of independent interest with applications to other problems). In non-private workflows, training a classifier on a subset of predictive features helps reduce the variance in the data and thus results in more accurate classifiers. With multi-step differentially private workflows, either each step of the workflow should work with a different subset of the data, or more noise must be infused in each step of the workflow. Thus it is not necessarily obvious that a workflow constituting private feature selection followed by private classification should improve accuracy over a workflow that executes a private classifier on the original data. Nevertheless, we show on real datasets and with two differentially private classifiers (Naive Bayes [25] and logistic regression [2]) that private feature selection indeed leads to significant improvement in the classifiers prediction accuracy. In certain cases, our differentially private algorithms are able to achieve accuracies attained by non-private classifiers.\nSecond, we develop novel algorithms for constructing the ROC curve given a classifier and a private test set. An ROC curve is constructed by computing the true and false positive rates on different subsets of the data. In the non-private case, typically t subsets are chosen, where t is the size of the test dataset. The main issue is that these subsets of the test dataset are overlapping and, hence, the true positive and false positive rates are correlated. Thus, a naive algorithm that directly adds noise to the t sets of counts results in ROC curves that are very different from the true ROC curve. We solve the first challenge by (a) carefully choosing the subsets using a differentially private recursive partitioning scheme, and (b) modeling the computation of the correlated true and false positive rates as one of privately answering a workload of one sided range queries (a problem that is well studied in the literature). Thus we can utilize algorithms from prior work ([27]) to accurately compute the statistics needed for the ROC curve. The utility of all our algorithms are comprehensively evaluated on three high dimensional datasets. Organization: Section 2 introduces the notation. Section 3 describes our novel feature selection algorithms. We discuss private evaluation of classifiers in Section 4. Experimental results on three real world datasets are presented in Section 5. Finally, we discuss related work in Section 6 and conclude in Section 7."
    }, {
      "heading" : "2. NOTATION",
      "text" : "Let D be a dataset having d attributes, and let D denote the set of all such datasets. One of the attributes is designated as the label L. The remainder of the attributes are called features F . We assume that all the attributes are binary (though all of the results in the paper can be extended to non-binary features). For instance, in text classification datasets (used in our experiments) binary features correspond to presence or absence of specific words from a prespecified vocabulary.\nFor any tuple t in dataset D, let t[L] denote the value of the label of the tuple, and t[F ] denote value of feature F for that tuple. We assume that feature vectors are sparse; every tuple has at most s features with t[F ] 6= 0. We denote by n the number of tuples in D, and by nψ the number of tuples in the dataset (Dψ) that satisfy a boolean predicate ψ. For instance, nF=1∧L=1 denotes the number of tuples t that satisfy t[F ] = 1 ∧ t[L] = 1. We define by PD(ψ) = nψ/n the empirical probability of ψ in the dataset D."
    }, {
      "heading" : "2.1 Differential Privacy",
      "text" : "An algorithm satisfies differential privacy if its output on a dataset does not significantly change due to the presence or absence of any single tuple in the dataset.\nDEFINITION 1 (DIFFERENTIAL PRIVACY [6]). Two datasets are called neighbors, denoted by (D1, D2) ∈ N if either D1 = D2 ∪ {t} or D2 = D1 ∪ {t}. A randomized algorithm M satisfies ǫ-differential privacy if ∀s ∈ range(M) and ∀(D1, D2) ∈ N ,\nPr[M(D) = s] ≤ eǫ · P [M(D′) = s] (1)\nHere, ǫ is the privacy parameter that controls how much an adversary can distinguish between neighboring datasets D1 and D2. Larger ǫ corresponds to lesser privacy and permits algorithms that retain more information about the data (i.e., utility). A variant of the above definition where neighboring datasets have the same number of tuples, but differ in one of the tuples is called bounded differential privacy.\nAlgorithms that satisfy differential privacy work by infusing noise based on a notion called sensitivity.\nDEFINITION 2 (GLOBAL SENSITIVITY). The global sensitivity of a function f : D → Rm, denoted by S(f) is defined as the largest L1 difference ||f(D1)−f(D2)||1, where D1, D2 are neighboring. More formally,\nS(f) = max (D1,D2)∈N ||f(D1)− f(D2)||1 (2)\nA popular differentially private algorithm is the Laplace mechanism [7] defined as follows:\nDEFINITION 3. The Laplace mechanism, denoted by MLap, privately computes a function f : D → Rm by computing f(D) + η. η ∈ Rm is a vector of independent random variables, where each ηi is drawn from the Laplace distribution with parameter S(f)/ǫ. That is, P [ηi = z] ∝ e−z·ǫ/S(f).\nDifferentially private algorithms satisfy the following composition properties that allow us to design complex workflows by piecing together differentially private algorithms.\nTHEOREM 1 (COMPOSITION [20]). Let M1(·) and M2(·) be ǫ1- and ǫ2-differentially private algorithms. Then,\n• Sequential Compositon: Releasing the outputs of M1(D) and M2(D) satisfies ǫ1 + ǫ2-differential privacy.\n• Parallel Composition: Releasing M1(D1) and M2(D2), where D1 ∩D2 = ∅ satisfies max(ǫ1, ǫ2)-differential privacy.\n• Postprocessing: Releasing M1(D) and M2(M1(D)) satisfies ǫ1-differential privacy. That is, postprocessing an output of a differentially private algorithm does not incur any additional loss of privacy.\nHence, the privacy parameter ǫ is also called the privacy budget, and the goal is to develop differentially private workflows that maximize utility given a fixed privacy budget."
    }, {
      "heading" : "3. PRIVATE FEATURE SELECTION",
      "text" : "In this section, we present differentially private techniques for feature selection that improve the accuracy of differentially private classifiers. We consider the classifer as a blackbox.\nMore formally, a classifier C takes as input a record of features and outputs a probability distribution over the set of labels. Throughout this paper we consider binary classifiers; i.e., L = {0, 1}. Thus without loss of generality we can define the classifier as outputting a real number p ∈ [0, 1] which corresponds to the probability of L = 1. Two examples of such classifiers include the Naive Bayes classifier and logistic regression [3].\nFeature selection is a dimensionality reduction technique that typically precedes classification, where only a subset of the features F ′ ⊂ F in the dataset are retained based on some criterion of how well F ′ predicts the label L [11]. The classifier is then trained on the dataset restricted to features in F ′. Since features are selected based on their properties in the data, the fact that a feature is selected can allow attackers to distinguish between neighboring datasets. Thus, by sequential composition, one needs to spend a part of the total privacy budget ǫ for feature selection (say ǫfs), and use the remainder (ǫ− ǫfs) for training the blackbox classifier.\nFeature selection methods can be categorized as filter, wrapper and embedded methods [11]. Filter methods assign scores to features based on their correlation with the label, and are independent of the downstream classification algorithm. Features with the best scores are retained. Wrappers are meta-algorithms that score sets of features using a statistical model. Embedded techniques include feature selection in the classification algorithm. In this paper, we focus on filter methods so that an analyst does not need to know the internals of the private classifier. Filters compute a ranking or a score for features based on their correlation with their label. Filter methods may rank/score individual features or sets of features. We focus on methods that score individual features. Features can be selected by choosing the top-k or those above some threshold τ .\nThus, the problem we consider can be stated as follows: Let D be the set of all training datasets with binary features F and a binary label L. Let Q : F × D → R be a scoring function that quantifies how well F predicts L for a specific dataset. Let Fτ denote the subset of features such that ∀F ∈ Fτ , Q(F,D) > τ . Two subsets of features F1,F2 ⊂ F , are similar if their symmetric difference is small. An example measure of similarity between F1 and F2 is the Jaccard distance (defined as |F1 ∩ F2|/|F1 ∪ F2|).\nPROBLEM 1 (SCOREBASEDFS). Given a dataset D ∈ D and a threshold τ , compute F ′ ⊂ F while satisfying ǫ-differential privacy such that the similarity between F ′ and Fτ is maximized.\nWe next describe a few example scoring methods, and present differentially private algorithms for the SCOREBASEDFS problem."
    }, {
      "heading" : "3.1 Example Scoring Functions",
      "text" : "Total Count: The total count score for a feature F , denoted by TC(F,D), is nF=1 the number of tuples with t[F ] = 1. Picking features with a large total count eliminates features that rarely take the value 1. Difference Count: The difference count score for a feature F , denoted by DC(F,D), is defined as:\nDC(F,D) = |nF=1∧L=1 − nF=1∧L=0| (3)\nDC(F,D) is large whenever one label is more frequent than the other label for F = 1. The difference count is smallest when both labels are equally likely for tuples with F = 1. The difference count is the largest when L is either all 1s or all 0s when conditioned on F = 1. Purity Index [11]: The purity index for a feature F , denoted by PI(F,D), is defined as:\nPI(F,D) = max\n{\n|nF=1∧L=1 − nF=1∧L=0|, |nF=0∧L=1 − nF=0∧L=0|\n}\n(4)\nPI(F,D) is the same as DC(F,D), except that it also considers the difference in counts when the feature takes the value 0. Information Gain: Information gain is a popular measure of correlation between two attributes and is defined as follows.\nAlgorithm 1 Cluster Selection (Q(·), ǫfs, rounds, centers, s, τ )\npoints ← {counts needed for Q(F,D) | F ∈ F)} clusters ← pkmeans(points, rounds, centers, ǫfs, s) accepted ← {} for cluster in clusters do\ncenter ← cluster.center() if score(center) ≥ τ then\naccepted.add(cluster.features()) end if\nend for return accepted\nDEFINITION 4 (INFORMATION GAIN). The information entropy H of a data set D is defined as:\nH(D) = − ∑\nℓ∈L\nPD(L = ℓ) lnPD(L = ℓ) (5)\nThe information gain for a specific feature F is defined as:\nIG(F,D) = H(D)− ∑\nf∈F\nPD(F = f) ·H(DF=f) (6)\nInformation gain of a feature F is identical to the mutual information of F and L."
    }, {
      "heading" : "3.2 Score Perturbation",
      "text" : "A simple strategy for feature selection is: (a) perturb feature scores using the Laplace mechanism, and (b) pick the features whose noisy score crosses the threshold τ (or pick the top-k features sorted by noisy scores). The scale of the Laplace noise required for privacy is S(Q) ·∆(Q)/ǫfs, where (i) S(Q) is the global sensitivity of the scoring function on one feature, and (ii) ∆(Q) is the number of feature scores that are affected by adding or removing one tuple.\nThe sensitivity of the total score TC, difference score DC, and purity index PI are all 1. The sensitivity of information gain function has been shown to be O(log n) [9, 29], where n is (an upper bound on) the number of tuples in the dataset. Information gain is considered a better scoring function for feature selection in the non-private case (than TC, DC or PI). However, due to its high sensitivity, feature selection based on noisy information gain results in lower accuracy, as poor features can get high noisy scores.\nRecall that s is the maximum number of non-zeros appearing in any tuple. Thus, ∆(TC) and ∆(DC) are both s – these scores only change for features with a 1 in the tuple that is added or deleted. On the other hand, IG(F,D) and PI(F,D) can change whether t[F ] is 1 or 0 for the tuple that is added or deleted. Thus, ∆(IG) and ∆(PI) are equal to the total number of features |F| >> s. 1 High sensitivity due to a large s or a large ∆(Q) can result in poor utility (poor features selected). Moreover, we observe (see Section 5.1) that a large s also results in lower accuracy of private classification. We can circumvent this by sampling; from every tuple t choose at most r features that have t[F ] = 1. Sampling is able to force a bound on the number of 1s in any tuples, and thus limit the noise. However, this comes at the cost of throwing away valuable data."
    }, {
      "heading" : "3.3 Cluster Selection",
      "text" : "The shortcoming of score perturbation is that we are adding noise individually to the scores of all the features. As the number of features increases, the probability that undesirable features get chosen 1If we used bounded differential privacy where neighboring datasets have the same number of tuples, we can show that ∆(Q) ≤ 2 · s for any scoring function, since the neighboring datasets differ in values of at most 2 · s attributes.\nAlgorithm 2 Private Threshold Testing (D,Q, τ )\nτ̃ ← τ + Lap(1/ǫ) for each query Qi ∈ Q do\nif Qi(D) ≥ τ̃ then v[i] ← 1 else v[i] ← 0\nend if end for return v\nincreases (due to high noisy scores), thus degrading the utility of the selected features. One method to reduce the amount of noise added is to privately cluster the features based on their scores, compute a representative score for each private cluster, and then pick features from high scoring clusters. This is akin to recent work on data dependent mechanisms for releasing histograms and answering range queries that group categories with similar counts and release a single noisy count for each group [16, 18, 28].\nWe represent each feature F as a vector of counts required to compute the scoring function Q. For instance, for TC and DC scoring functions, F could be represented as a two dimensional point using the counts nF=1∧L=0 and nF=1∧L=1. We use private k-means clustering [4] to cluster the points. k-means clustering initializes the cluster centers (µ1, ..., µk) (e.g. randomly) and updates them iteratively as follows: 1) assign each point to the nearest cluster center, 2) recompute the center of each cluster, until reaching some convergence criterion or a fixed number of iterations. This algorithm can be made to satisfy differential privacy by privately computing in each iteration (a) the number of points in each new cluster, qa, and (b) the sum of the points in each cluster, qb. The global sensitivity of qa is 1, and the global sensitivity of qb is ∆(Q) (or r if sampling is used). The number of iterations is fixed, and the privacy budget is split evenly across all the iterations.\nOnce clusters have been privately assigned, the centers themselves can be evaluated based on their coordinates. For instance, TC and DC can be computed using the sum and difference (resp.) of the two-dimensional cluster centers. Depending on the score of the group all or none of the associated features will be accepted. This score does not have to be perturbed as it is computed via the centers that are the result of a private mechanism."
    }, {
      "heading" : "3.4 Private Threshold Testing",
      "text" : "In this section we present a novel mechanism, called private threshold testing (PTT), for the SCOREBASEDFS problem whose utility is independent of both s and the number of features |F|, and does not require sampling. Rather than perturbing the scores of all the functions, PTT perturbs a threshold τ and returns the set of features with scores greater than the perturbed threshold. We believe PTT has applications beyond feature selection and hence we describe it more generally.\nLet Q = {Q1, Q2, . . . , Qm} denote a set of real valued queries over a dataset D, all of which have the same sensitivity σ. (In our case, each Qi = Q(Fi, D), and m = |F|). PTT has as input the set of queries Q and a real number τ , and outputs a vector v ∈ {0, 1}m, where v[i] = 1 if and only if Qi(D) ≥ τ̃ .\nThe private algorithm is outlined in Algorithm 2. PTT creates a noisy threshold τ̃ by adding Laplace noise with scale σ/ǫ to τ . The output vector v is populated by comparing the unperturbed query answer Q(D) to τ̃ . We can show that despite answering m comparison queries (where m can be very large) each with a sensitivity of σ, PTT ensures 2σǫ-differential privacy (rather than\nmσǫ-differential privacy that results from a simple application of sequential composition).\nTHEOREM 2. Private Threshold Testing is 2σǫ-differentially private for any set of queries Q all of which have a sensitivity σ.\nPROOF. (sketch) Consider the set of queries for which PTT output 1 (call it Q1); i.e., for these queries, Q(D) > τ̃ . Note that for any value of the noisy threshold, say τ̃ = z, if Q(D) ≥ z, then for any neighboring database Q(D′) ≥ z − σ (since σ is the sensitivity). However, since τ̃ is drawn from the Laplace distribution, we have that P (τ̃=z)\nP (τ̃=z−σ) ≤ eσǫ. Therefore,\nP (Q(D) = 1,∀Q ∈ Q1) =\n∫\nz\nP (τ̃ = z) ∏\nQ∈Q1\nP (Q(D) > z)dz\n≤ eσǫ ∫\nz\nP (τ̃ = z − σ) ∏\nQ∈Q1\nP (Q(D′) > z − σ)dz\n= eσǫP (Q(D′) = 1,∀Q ∈ Q1)\nAn analogous bound for Q0 yields the requires e2σǫ bound.\nWe can show that τ can be chosen based on the database D. In fact we can show the following stronger result for count-based queries.\nCOROLLARY 1. Let Q be a set of queries with sensitivity σ. Let τ be a function on D that computes the threshold, also having sensitivity σ. If the values of Q and τ on D are non-decreasing (or non-increasing) when a tuple is added (or deleted resp.) from D, then PTT is σǫ-differentially private.\nPROOF. (sketch) Case (i) τ is a constant: When D = D′ ∪ {t}, for all z, Qi(D) < z implies Qi(D′) < z. Thus, r0 = P (Q(D)=0,∀Q∈Q0) P (Q(D′)=0,∀Q∈Q0)\nis already bounded above by 1, while r1 = P (Q(D)=1,∀Q∈Q1) P (Q(D′)=1,∀Q∈Q1) is bounded above by eσǫ from proof of Thoerem 2. When D′ = D ∪ {t}, we have r1 < 1 and r0 ≤ eσǫ. Case (ii) τ is a function of D: When D = D′∪{t}, it holds that P (τ̃(D) = z) ≤ eσǫP (τ̃(D′) = z − σ). This is because τ (D′) lies between [τ (D)− σ, τ (D)]. The rest of the proof remains.\nWhile Theorem 2 applies to all our scoring functions (TC,DC, PI and IG), the stronger result from Corollary 1 only applies to TC.\nAdvantages over prior work: First, PTT permits releasing whether or not a set of query answers are greater than a threshold τ even if the sensitivity of releasing the answers of all the queries may be large. PTT only requires: (i) query answers to be real numbered and (ii) each query has a small sensitivity σ. The privacy guarantee is independent of the number of queries.\nNext, PTT can output whether or not a potentially unbounded number of query answers cross a threshold. This is a significant improvement over the related sparse vector technique (SVT) first described in Hardt [13], which allows releasing upto a constant c query answers that are above a threshold τ . SVT works as follows: (i) pick a noisy threshold τ̃ using ǫ/2 privacy budget, (ii) perturb all the queries using Laplace noise using a budget of ǫ/2c, and (iii) releasing the first c query answers whose noisy answers are greater than τ̃ . Once c query answers are released the algorithm halts. PTT is able to give a positive or negative answer for all queries, since it does not release the actual query answers.\nFinally, PTT does not add noise to the query answers, but only compares them to a noisy threshold. This means that the answer to a query for which PTT output 1 is in fact greater than the answer to a query for which PTT output 0. This is unlike NOISYCUT, a\ntechnique used in Lee et al [17]. Both PTT and NOISYCUT solve the same problem of comparing a set of query answers to a threshold. While PTT only adds noise to the threshold, NOISYCUT adds noise to both the query answers and the threshold. We experimentally show (in Section 5.1) that PTT has better utility than NOISYCUT. That is, suppose Q1 is the set of queries whose true answers are > τ , and QP1 and Q N 1 are the set of queries with a 1 output according to PTT and NOISYCUT, resp. We show that QP1 is almost always more similar to Q1 than QN1 .\nWe quantify the utility of our feature selection algorithms by experimentally showing their effect on differentially private classifiers in Section 5."
    }, {
      "heading" : "4. PRIVATE EVALUATION OF CLASSIFIERS",
      "text" : "In this section, we describe an algorithm to quantify the accuracy of any binary classifier under differential privacy on a test dataset containing sensitive information."
    }, {
      "heading" : "4.1 ROC curves",
      "text" : "Receiver operating characteristic (ROC) curves are typically used to quantify the accuracy of binary classifers. Let Dtest be a test dataset. For every tuple t ∈ Dtest, let t[L] ∈ {0, 1} denote the true label, and p(t) ∈ [0, 1] denote the prediction returned by some classifier (probability that t[L] = 1). Let n1 and n0 denote the number of tuples with true label 1 and 0 respectively.\nGiven a threshold θ, we say that the predicted label pθ(t) is 1 if p(t) > θ. Based on the true label as well as the predicted label (at a given threshold θ), we can quantify the accuracy of the classifier on the dataset as follows. True positives, TP (θ), are the tuples in Dtest whose true label and predicted label equals 1; i.e., t[L] = 1 ∧ p(t) > θ. True negatives, TN(θ), denote the tuples whose true and predicted labels are 0. False positives, FP (θ) are tuples whose true label is 0 but the predicted label is 1. False negatives, FN(θ) are tuples whose true label is 1 but the predicted label is 0. We will use the notation TP (θ), FP (θ), etc. to both denote the set of tuples as well as the cardinality of these sets.\nThe true-positive rate TPR(θ) is defined as the probability that a tuple in the test set having label 1 is correctly classified to have label 1. The false-positive rateFPR(θ), is defined as the probability that a data having label 0 is wrongly classified to have label 1. Thus,\nTPR(θ) = TP (θ)\nn1 and FPR(θ) =\nFP (θ)\nn0 (7)\nThe Receiver operating characteristic (ROC) curve is defined by plotting pairs of FPR(θ) versus TPR(θ) over all possible thresholds θ ∈ Θ. ROC curve starts at (0,0) and ends at (1,1). In order to evaluate the accuracy of a binary classifier, we consider the area under the ROC curve (AUC). If the classifier is good, the ROC curve will be close to the left and upper boundary and AUC will be close to 1. On the other hand, if the classifier is poor, the ROC curve will be close to the 45◦ line from (0,0) to (1,1) with AUC around 0.5.\nRecent work [19] has shown that releasing the actual ROC curves on a private test dataset can allow an attacker with prior knowledge to reconstruct the test dataset. An extreme yet illustrative example is as follows: suppose an attacker knows the entire test dataset except one record. Given the real ROC curve, the attacker can determine the unknown label by simply enumerating over all labels (and checking which choice led to the given ROC curve). Hence, directly releasing the real ROC curve may leak information of the data and we need a differentially private method for generating ROC curves to protect the private test dataset.\n4.2 Private ROC curves\nAlgorithm 3 PriROC (T, P, ǫ) 1. Use ǫ1 budget to choose the set of thresholds for computing TPRs and FPRs 2. Use ǫ2 = ǫ− ǫ1 budget to compute the noisy TPRs and FPRs at all thresholds 3. Postprocess the TPRs and FPRs sequences to maintain consistency.\nThere are three important challenges when generating differentially private ROC curves – (i) how to privately compute TPR and FPR values, (ii) how many and what thresholds to pick, and (iii) how to ensure the monotonicity of the TPR and FPR values.\nOne can use the Laplace mechanism to compute TPR(θ) and FPR(θ). The global sensitivity of releasing n0 and n1 is 1. The global sensitivity of each of the TP (θ) and FP (θ) values equals 1. Thus they can all released by adding Laplace noise with sensitivity 2|Θ| + 1, where |Θ| is the number of thresholds. However, as we will show later, the linear dependence of sensitivity on the number of thresholds can lead to significant errors in the ROC curves and the area under the curve.\nThis brings us to the next concern of the number of thresholds. In the non-private case, one can pick all the prediction probabilities associated with each tuple in the test dataset as a threshold. However, as |Θ| increases, more counts need to be computed leading to more noise. Moreover, the predictions themselves cannot be publicly released, and hence the thresholds must be chosen in a private manner. Finally, the true TPR and FPR values satisfy the following monotonicity property: for all θ1 ≤ θ2, TPR(θ1) ≤ TPR(θ2) and FPR(θ1) ≤ FPR(θ2). The private TPR and FPR values must also satisfy this property to get a valid ROC curve.\nOur algorithm for computing differentially private ROC curves, called PriROC (Algorithm 3), addresses all the aforementioned concerns. PriROC first privately chooses a set of thresholds (using privacy parameter ǫ1). By modeling TP and FP values as one-sided range queries, PriROC can compute noisy TPRs and FPRs values (using the remaining privacy budget ǫ2) with much lower error than using the Laplace mechanism. Finally, a postprocessing step enforces the monotonicity of TPRs and FPRs. We next describe these steps in detail.\n4.2.1 Computing noisy TPRs & FPRs Suppose we are given a set of thresholds Θ = {θ1, . . . , θℓ},\nwhere θi > θi+1 for all i. Assume that θ0 = 1 and θℓ = 0. That is, for all records t ∈ Dtest, the prediction p(t) is greater than θℓ, but not greater than θ0. Since, TP (θ) corresponds to the number of tuples t with t[L] = 1 ∧ p(t) ≥ θ, TP (θℓ) is the total number of tuples with t[L] = 1 (denoted by n1). Similarly, FP (θℓ) is the total number of tuples with t[L] = 0 (denoted by n0). Thus:\nTPR(θi) = TP (θi)\nn1 =\nTP (θi) TP (θℓ) ∀1 ≤ i ≤ ℓ\nFPR(θi) = FP (θi)\nn0 =\nFP (θi) FP (θℓ) ∀1 ≤ i ≤ ℓ\nTherefore, an ROC curve can be constructed by just computing TP (θi) and FP (θi) for all θi ∈ Θ.\nWe next observe that the true positive and false positive counts each correspond to a set of one-sided range queries.\nDEFINITION 5 (ONE-SIDED RANGE QUERY). Let X = {x1, x2, . . . , xn} denotes a set of counts. A query qj is called a one sided range query, and qj(X) is the sum of the first\nj elements in X . That is, qj(X) = ∑j\ni=1 xi. The set Cn = {q1, . . . , qn} denotes the workload of all one sided range queries.\nIn our context, let XTPΘ = {x TP 1 , x TP 2 , . . . , x TP ℓ }, where x TP i is the number of tuples t ∈ Dtest with t[L] = 1 and θi−1 ≥ p(t) > θi. It is easy to check that TP (θi) is the sum of the first i counts in XTPΘ . We can similarly define X FP Θ , and show that each FP (θi) is also the answer to a one-sided range query qi on XTPΘ . It is well known that the Laplace mechanism is not optimal in terms of error for the workload of one-sided range queries Cn. Under Laplace mechanism, each query answer would have a mean square error of O(n2/ǫ2). Instead, using strategies like the hierarchical mechanism [14] or Privelet [27] allow answering each onesided range query with no more than O(log3 n/ǫ2) error. In our experiments, we use the Privelet mechanism to compute the TP and FP counts with a privacy budget of ǫ2/2 for each. The Privelet algorithm first computes the wavelet coefficients of the counts in X , adds noise to the wavelet coefficients and then reconstructs a new X̂ from the noisy wavelet coefficients. One-sided range queries are computed on X̂TPΘ to get the TP counts and on X̂ FP Θ to get the FP counts, which in turn are used to construct the noisy TPR(θ) and FPR(θ) values. Since all steps subsequent to Privelet do not use the original data, the fact that releasing TPR(θ) and FPR(θ) satisfies ǫ2-differential privacy follows from the privacy of Privelet.\n4.2.2 Choosing Thresholds There are two important considerations when choosing the set of\nthresholds Θ. The number of thresholds must not be very large, as the total error is directly related to |Θ|. At the same time, the thresholds must be chosen carefully so that the ROC curve on those thresholds is a good approximation of the ROC curve drawn using all the predictions in the test data. We present two heuristics for choosing Θ that take into account the above considerations.\nA simple data-independent strategy for picking the set of thresholds is to choose them uniformly from [0, 1]. More precisely, if n is the cardinality of Dtest, we choose the number of thresholds to be an α ∈ [0, 1] fraction of n, and choose the set of thresholds to be Θ = {0, 1\n⌊αn⌋ , 2 ⌊αn⌋ , . . . , ⌊αn⌋−1 ⌊αn⌋\n, 1}. We call this strategy α-FIXEDSPACE. This strategy works well when the predictions P = {p(t)|t ∈ Dtest} are uniformly spread out in [0, 1]. Since, α-FIXEDSPACE is data independent, ǫ1 = 0, and all the privacy budget can be used for computing the TPR and FPR values.\nα-FIXEDSPACE is not a good strategy in the general case. For instance, suppose a majority of the predictions are less than the smallest threshold θ1 = 1⌊αn⌋ . Then the ROC curve for all those points will be approximated with a single point (TPR(θ1), FPR(θ1)) possibly resulting in a significant loss in accuracy in the AUC.\nHence, we present k-RECURSIVEMEDIANS, a data dependent strategy that addresses skewed prediction distributions by recursively partitioning the data domain such that each partition has roughly the same number of tuples (Algorithm 4). The algorithm takes as input ǫ, the privacy budget for choosing thresholds, k, the number of recursive steps, and P = {p(t)|t ∈ Dtest}, the multiset of predictions. As the name suggests the algorithm has k recursive steps, and each uses a privacy budget of ǫ/k.\nThe algorithm recursively calls a subroutine FINDMEDIANS computing the noisy median of all predictions within the range (left, right). Initally, left = 0 and right = 1. Since median has a high global sensitivity (equal to right if all values are in the range (left, right)), we use the smooth sensitivity framework [21] for computing the noisy median. We refer the reader to the original paper for details on computing the smooth sensitivity for median. We choose to\nAlgorithm 4 k-RECURSIVEMEDIANS function k-RECURSIVEMEDIANS(P,ǫ, k)\nǫ′ ← ǫ k\nreturn FINDMEDIANS(P,ǫ′, k, 0, 1) end function\nfunction FINDMEDIANS(P,ǫ′, k, left, right) if k = 0 then return end if m ← median(P )\nm̃ ← m+ 8S∗ fmed,ǫ ′ (P )\nǫ1 ∗ z, z is random noise ∝ 1 1+z2\nif m̃ ≤ left or m̃ ≥ right then m̃ = (left+ right)/2 end if P1 ← {P [i] | P [i]<m̃} P2 ← {P [i] | P [i]>m̃} return FINDMEDIANS(P1, ǫ′, k − 1, left, m̃) ∪ m̃∪\nFINDMEDIANS(P2, ǫ′, k − 1, m̃, right) end function\nsample noise from distribution K/(1+ |z|2) (where K is a normalization constant). We can generate samples from the distribution by picking U uniformly from (0, 1) and computing tan(π(U − 0.5)) (since the CDF of the distribution is ∝ arctan(z)).\nThe resulting noisy median m̃ could fall out of the range (left, right). This could either happen due to random chance, or more likely because the smooth sensitivity of the points within the range is high. A high smooth sensitivity occurs either due to a small number of data points, or when about half the data points are very close to left, and the rest of the points are very close to right. Then a point in the middle of the range (e.g., (left + right)/2) is a good partition point, and is used instead of m̃. The algorithm proceeds to recursively find the medians of points in (left, m̃) and (m̃, right). The algorithm returns after it completes k levels of recursion. The number of thresholds output by k−RecursiveMedians is 2k .\nTHEOREM 3. Algorithm 4 (k-RECURSIVEMEDIANS) satisfies ǫ-differential privacy.\nPROOF. (sketch) The proof follows from the following statements: Computing the median of a set of points in each invocation of FINDMEDIANS satisfies ǫ/k-differential privacy. This is true as long as noise is drawn from the distribution ∝ 1/(1+ |z|γ ), scaled appropriately by the smooth sensitivity and γ ≥ 1. In each recursive step, computing the medians in disjoint partitions of the data satisfies ǫ/k-differential privacy by parallel composition. Since the number of recursions is bounded by k, k-RECURSIVEMEDIANS satisfies ǫ-differential privacy by serial composition.\n4.2.3 Ensuring monotonicity TPR(θ) and FPR(θ) values in the original ROC curve are\nmonotonic. That is, the true positive rates satisfy the following constraint: 0 ≤ TPR(θ1) ≤ . . . ≤ TPR(θℓ) = 1. However, this may not be true of the noisy TPR and FPR values (generated using the strategy from the previous section). We leverage the ordering constraint between the TPR and FPR values to boost the accuracy by using the constrained inference method proposed by Hay et al [14]. Since this is a postprocessing step, there is no impact on privacy.\nThe error introduced by our algorithms for generating ROC curves varies with different datasets. Therefore, we empirically evaluate the utility of our algorithms on real data in the next section."
    }, {
      "heading" : "5. EXPERIMENTS",
      "text" : "In this section we experimentally evaluate our differentially private algorithms for feature selection (Section 5.1)) and generating ROC curves (Section 5.2). The main takeaways from the experimental evaluation on differentially private feature selection are:\n• Spending a part of the privacy budget for private feature selection can significantly improve the misclassification rate (10% - 15%) of a differentially private classifier. This is despite a noisier classifier due to the smaller privacy budget.\n• Feature selection using private threshold testing consistently results in classifiers with higher accuracy than feature selection using score perturbation and cluster selection PTT also significantly outperforms a related technique NOISYCUT in solving the SCOREBASEDFS problem.\n• In the differential privacy regime, simple scoring techniques (like total count TC) perform as well or even better than measures like information gain IG that are considered best in the non-private regime.\nThe main takeaways from the experimental evaluation on differentially private ROC curves are:\n• The area under the curve (AUC) measure for the differentially private ROC curves are close to the AUC measures for the true ROC curves. Therefore, with high probability differentially private ROC curves can be used to distinguish between classifiers that are significantly different.\n• The AUC error for ROC curves generated by PriROC is significantly smaller than AUC error for ROC curves based on true and false positive rates computed using the Laplace mechanism.\n• The k-RECURSIVEMEDIANS method to pick thresholds results in better ROC curves than using α-FIXEDSPACE.\n• The number of thresholds chosen to generate the differentially private ROC curve does not significantly affect the AUC error."
    }, {
      "heading" : "5.1 Feature Selection",
      "text" : "5.1.1 Setup We use three text classification datasets - TWITTER, SMS and\nREUTERS. The TWITTER dataset [10] was collected for the task of sentiment classification. Each tweet is associated with a binary sentiment label – positive or negative. The datast contains 1.6 million tweets from which we randomly sampled 7304 tweets for our experiments. We constructed binary features for every word (excluding stop words) resulting a total of 32935 features. Since each tweet contained at most 20 non-stop words, we set s = 20. The SMS dataset [1] contains 5574 SMS messages associated with spam/ham label. The dataset has a total of 8021 features. Since SMS messages are short, we again set s = 20. The REUTERS dataset consists of 21578 news articles tagged with topics. To get a training dataset with a binary class label, we chose a corpus of 6906 articles labeled as earnings-related or not (based on the ”earn” topic keyword). Since an article does not have a word limit, we do not have a small bound on s like in TWITTER or SMS. The total number of features is 33389.\nWe choose to evaluate our feature selection algorithm on two state of the art differentially private classifiers – Naive Bayes [25], and the differentially private ERM implementation of logistic regression [2]. The Naive Bayes (NB) classifier assumes that the features are conditionally independent given the label L. Given a\nfeature vector x ∈ {0, 1}|F|, the predicting label given by\nargmaxℓ∈{0,1}Pr[L = ℓ] · ∏\nF∈F\nPr[F = x[F ]|L = ℓ]\nThus, the Naive Bayes classifier can be made private by releasing differentially private counts of n, nL=ℓ, and nF=i∧L=ℓ from the training data set, for i, ℓ ∈ {0, 1}.\nLogistic regression models the log odds of the prediction as linear function of the features. Empirical risk minimization is used to fit the linear model given a dataset. For non-private logistic regression, we have used the prepackaged Scikit-learn logistic regression classifier [22]. We use an implementation of Chaudhuri et al’s [2] differentially private empirical risk minimization (henceforth called ERM) for logistic regression.\nThe accuracy of a classifier is measured using the fraction of predictions that match the true label on a held out test set. The results are average over 10 runs (using 10-fold cross validation) to account for the noise introduced due to differential privacy.\n5.1.2 Feature Selection Results Figure 1 presents a comparison of all the discussed feature selec-\ntion methods across all three data sets using a non-private and a private naive bayes classifier. In the non-private case (Figures 1(a),1(c), and 1(e)), we see a small improvement in the accuracy using all three scoring techniques TC, DC and IG. PI resulted in a similar accuracy as DC and is not shown. IG has the highest accuracy for all the datasets.\nIn the private case (Figures 1(b),1(d), and 1(f)), ‘All’ corresponds to no feature selection, and ‘All-sampling’ correponds to using all the features but with sampling (to reduce the sensitivity) with r = 10. For the private graphs, the total ǫ-budget is 1.0. We see that even though sampling throws away valuable data, we already see an increase in the accuracy. This is because sampling also helps reduce the sensitivity of the classifier training algorithm. Note that we do not report the ‘All’ bar for REUTERS– since we can’t bound the lenght of an article, the sufficient statistics for the naive bayes classifier have a very high sensitivity. We also show the accuracy of the majority classifier, which always predicts the majority class.\nNext we add feature selection. Both score perturbation and clustering are used in conjunction with sampling (to reduce sensitivity). Private threshold testing (PTT) does not use sampling. For score perturbation the budget split is .5 for selection and .5 for classification (budget split is discussed in Section 5.1.4). For clustering and PTT the budget split is .2 for selection and .8 for classification.\nWe see that most of the feature selection techniques (and scoring functions) result in a higher accuracy than ‘All-sampling’. One exception is IG due to its high sensitivity. Additionally as noted in section 3.2, experiments with score perturbation of Information Gain were run under bounded differential privacy (since the sensitivity of IG is higher under unbounded differential privacy). We see poor accuracy with IG and score perturbation despite this. We do not report IG under clustering and PI under score perturbation and clustering due to their high sensitivity. We are surprised to see that TC is as good as or better than “best” non-private scoring techniques across all three datasets and all differentially private feature selection techniques. This is due to its low sensitivity. We also note a trend that PTT with TC is more accurate than clustering with TC which is in turn more accurate than score perturbation with TC.\nFigure 2 contains the same tests, but with the ERM classifier. We only show results on the SMS dataset due to space constraints. We found that the private ERM code does not scale well to large number of features. For that reason we first selected the top 5000 features according to TC scoring function and used that in place\nof the ‘All’ features. Feature selection was then performed on this restricted dataset. We see the same trends as in the case of the Naive Bayes classifier. The results are comparable to those run on the private Naive Bayes classifier, but with a lower accuracy overall. This lower accuracy could be because Naive Bayes is known to outperform other methods for the text classification task.\n5.1.3 SCOREBASEDFS Comparison We also evaluate the quality of the just the feature selection algo-\nrithms (without considering a classifier). The accuracy of a feature selection technique is quantified as follows. Let Fτ be the true set of features whose scores are greater than the threshold (under some fixed scoring function), and let F ′ be the set of features returned by a differentially private algorithm for SCOREBASEDFS. We define precision (pre), recall (rec) and F1-score (F1) as follows:\npre = |Fτ ∩ F\n′|\n|F ′| , rec =\n|Fτ ∩ F ′|\n|Fτ | , F1 =\n2 · pre · rec\npre+ rec\nFigure 3 shows the F1 scores for 4 private feature selection methods using TC – score perturbation, clustering, PTT and NOISYCUT\n[17]. The x-axis corresponds to different thresholds τ . The x-axis values on the top represent |Fτ |.\nThere are two notable features of these plots. First, PTT does the best of all selection methods at all thresholds. This is due to the fact that only the threshold is perturbed. Since the ordering of feature scores is maintained, F ′ is a superset of Fτ (with rec = 1) or is a subset of Fτ (with pre = 1). In particular it significantly outperforms NOISYCUT under small thresholds (or when many features must be chosen). Second, we are able to see what settings would cause the other methods to struggle. Both score perturbation and NOISYCUT have poorer accuracy as τ decreases (or number of features increases). This is because feature score are perturbed, and as we increase the number of features to be selected there is a larger chance that good features are eliminated and poorer features are returned just by random chance. Clustering shows the reverse trend. This is because low scoring features tend to cluster together resulting in large clusters (resulting in low sensitivity). The same is not true for high scoring features.\n5.1.4 Parameter Tuning In this section we present empirical justification for some of our\ndesign choices – budget split, and sample rate selection. We defer the problem of classifier agnostic automatic parameter tuning to future work. Privacy Budget Split: We empirically tested the accuracy of the classifier with feature selection under different budget splits. Figures 5(a) and 5(b) show (on the TWITTER and SMS datasets, resp.) one example of Naive Bayes classification with score perturbation using the TC score function. We see that the best accuracy is achieved when feature selection and classification equally split the budget. Since clustering and PTT have much lower sensitivities we find than a much smaller part of the budget (0.2) is required for these techniques to get the best accuracy (graphs not shown). Splitting data vs Privacy Budget: Rather than splitting the privacy budget, one could execute feature selection and classification on disjoint subsets of the data. By parallel composition, one can use all the privacy budget for both tasks. However, experiments on the Naive Byes classifier with PTT showed splitting the data resulted in classifiers whose average accuracy was very close to that of the majority classifier. Since feature selection is run on a slightly different dataset, wrong features are being chosen for classifier training. Sampling Rate Selection: Figures 5(c) and 5(d) show the change in system accuracy as the sampling rate r is changed for the SMS and TWITTER data sets. We see that selecting alow sampling rate is detrimental since too much information is lost. Alternatively selecting a sampling rate that is too high loses accuracy from increasing the sensitivity used when drawing noise for privacy. A moderate rate of sampling that preserves enough information while reducing the required global sensitivity for privacy will do best. Total Budget Selection Figure 4 shows the accuracy of the private classifier with the best private score perturbation, clustering and PTT feature selection algorithm under different settings of the total privacy budget (ǫ = 1, 0.5, 0.25 and 0.1). The same settings for budget split and sampling are held throughout. For the TWITTER and REUTERS datasets (which are harder to predict) we see the ac-\ncuracy begin to approach the majority classifier as the total budget is reduced to 0.1."
    }, {
      "heading" : "5.2 Private Evaluation",
      "text" : "We use the held out test sets of the SMS and TWITTER datasets which come from the previous section. SMS test set contains 558 data, and each tuple t has a true label t[L] ∈ {0, 1} as well as a prediction p(t) ∈ [0, 1] for the label 1. In SMS, 481 out of 558 data have true label equals 1. The TWITTER test dataset contains 684 different tuples, 385 of which have label equal to 1.\nFigure 6 shows the real ROC curves as well as the differentially private ROC curves for both SMS and TWITTER datasets under 4 different privacy budgets by using k-RECURSIVEMEDIANS. Recall that, in k-RECURSIVEMEDIANS, ǫ1 privacy budget is used for selecting a set of thresholds, and ǫ2 = ǫ− ǫ1 is used for generating the ROC curve. We set ǫ1 = 0.2ǫ.\nThe solid line refers to the real ROC curve, while the dashed line represents the differentially private ROC curve. When the privacy budget is not small (ǫ = 1), the private ROC curve is very close to the real ROC curve, which means our private ROC curve is a good replacement of the real ROC curve, correctly reflecting the performance of the input classifier.\nFigure 7 reports the comparison of the errors among three different algorithms. The Laplace line refers to the error by directly using Laplace Mechanism using t thresholds (that can be chosen based on the data). The FixedSpace line shows the error of α-FIXEDSPACE, with α · n thresholds chosen uniformly in [0, 1]. And the line RecursiveMedians presents the error of k-RECURSIVEMEDIANS, with 2k thresholds. The x-axis corresponds to ǫ and the y-axis corresponds to the L1 error of the area between the real ROC curve and the private ROC curve under certain privacy budget. The error shows the median value after running our algorithm 10 times. The reason why we pick the median error instead of the average value is to counter the effect of outliers.\nTo understand the effect of choosing differing numbers of thresholds, we choose α = 1, 0.5, 0.25 and 0.125. The ensure that the noise introduced is roughly the same in all algorithms, we vary t and k as 10, 9, 8 and 7. For instance, for α = 1 and t, k = 10, we have O(n) thresholds for α-FIXEDSPACE and k-RECURSIVEMEDIANS (29 ≤ n ≤ 210 for the SMS and TWITTER datasets), and O(log n) thresholds for Laplace.\nIn figure 7, we can see that k-RECURSIVEMEDIANS and αFIXEDSPACE can largely improve the accuracy of the output compared with directly using Laplace Mechanism under all ǫ and α settings. The difference in error is largest for small epsilon. Although the noise scale is the same for the three methods in each experiment, since the number of thresholds is very small the LaplaceROC curve can’t hope to approximate the true ROC curve well enough. Furthermore, for both datasets, the k-RECURSIVEMEDIANS method performs better than α-FIXEDSPACE method under nearly all parameter settings, which means computing noisy quantiles help choose the right set of thresholds.\nTable 1 represents the graphs in Figure 7 for t, k = 10 and α = 1 in tabular form. It is interesting to note that k-RECURSIVEMEDIANS\nhas 10 times lower error than Laplace for ǫ = 1 on SMS.\n5.2.1 Choosing the number of thresholds The value of α in α-FIXEDSPACE and k in k-RECURSIVEMEDIANS\ndetermines the number of thresholds we will use to compute ROC curve. It will affect three different aspects. First, the bigger size of thresholds, the better we ca hope to approximate the true ROC curve. Second, larger threshold sets result in larger noise being used to perturb TPRs and FPRs. Third, the last step of our algorithm is to do postprocessings in order to maintain consistency and its relationship to α, k is not very clear. Our goal is to pick the value of α, k which lead to the best trade off.\nFigure 8 presents the comparions of the errors for k-RECURSIVEMEDIANS among different choies for k under all ǫ values. The x-axis show 4 different settings for k. We can see that for both datasets, there is no specific setting of k that leads to the best performance of kRECURSIVEMEDIANS for all ǫ settings. The graph looks similar for α-FIXEDSPACE (not shown).\nThus, it seems best to set k = ⌈log n⌉. The following is one possible reason for the AUC error not depending on k: One may pick a small number of thresholds to reduce the noise if the true ROC curve can be accurately described using a small number of points. But in this case, the postprocessing step that enforces monotonicity results in error that has a strong dependence on the number of distinct TPR and FPR values, and not the total number of thresholds (see Theorem 2 [14])."
    }, {
      "heading" : "6. RELATED WORK",
      "text" : "Differentially Private Classifiers: Private models for classification has been a popular area of exploration for privacy research. Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others. Apart from classifier training, Chaudhuri el al. [26] present a generic algorithm for differentially private parameter tuning and\nmodel selection. However, this work does not assume a blackbox classifier, and makes strong stability assumptions about the training algorithm. In contrast, our algorithms are classifier agnostic. Additionally Thakurtha et al. [24] present an algorithm for model selection again assuming strong stability assumptions about the model training algorithm. We would like to note that the work in these paper is in some sense orthogonal to the feature selection algorithms we present, and can be used in conjunction with the results in the paper (for instance, to choose the right threshold τ or the right number of features to select). Private Threshold Testing: As mentioned before, private threshold testing (PTT) is inspired by the sparse vector technique (SVT) [13] which was first used in the context of the multiplicative weights mechanism [12]. While PTT aims to only release whether or not a query answer is greater than a threshold, SVT releases the actual answers that are above the threshold and thus can only release a constant number of answers. Lee et al [17] solve the same problem as PTT in the context of frequent itemset mining. The propose an\nalgorithm NOISYCUT which we show is inferior to PTT. While the techniques for proving the privacy of all these techniques are similar, our proof for PTT is the tightest thus allowing us only add noise to the threshold and get the best utility (amongst competitors) for answering comparison queries. Private Evaluation: Receiver operating characteristic (ROC) curves are used quantifying the prediction accuracy of binary classifiers. However, directly releasing the ROC curve may reveal the sensitive information of the input dataset [19]. In this paper, we propose the first differentially private algorithm for generating private ROC curves under differential privacy. Chaudhuri et al [26] proposes a generic technique for evaluating a classifier on a private test set. However, they assume that the global sensitivity of the evaluation algorithm is low. Hence, their work will not apply to generating ROC curves, since the sufficient statistics for generating the ROC curve (the set of true and false positive counts) have a high global sensitivity. Despite this high sensitivity, we present strategies that can privately compute ROC curves with very low noise by modeling the sufficient statistics as one-sided range queries."
    }, {
      "heading" : "7. CONCLUSIONS",
      "text" : "In this paper, we presented algorithms that can aid the adoption of differentially private methods for classifier training on private data. We present novel algorithms for private feature selection and experimentally show using three real high dimensional datasets that spending a part of the privacy budget for feature selection can improve the prediction accuracy of the classifier trained on the selected features. Moreover, we also solve the problem of privately generating ROC curves. This allows a user to quantify the prediction accuracy of a binary classifier on a private test dataset. In conjunction, these algorithms can now allow a data analyst to mimic typical ‘big-data’ workflows that (a) preprocess the data (i.e., select features), (b) build a model (i.e., train a classifier), and (c) evaluate the model on a held out test set (i.e., generate an ROC curve) on private data while ensuring differential privacy without sacrificing too much accuracy."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] T. A. Almeida, J. M. G. Hidalgo, and T. P. Silva. Towards sms spam\nfiltering: Results under a new dataset. International Journal of Information Security Science, pages 1–18, 2012.\n[2] A. and Sarwate and K. Chaudhuri. Differentially Private Empirical Risk Minimization. CoRR, abs/0912.0, 2009. [3] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006. [4] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: The sulq framework. PODS ’05, pages 128–138, New York, NY, USA, 2005. ACM. [5] M. Dash and H. Liu. Feature selection for classification. Intelligent Data Analysis, 1:131–156, 1997. [6] C. Dwork. Differential privacy. In in ICALP, pages 1–12. Springer, 2006. [7] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. TCC’06, pages 265–284, Berlin, Heidelberg, 2006. Springer-Verlag. [8] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of\npersonalized warfarin dosing. SEC’14, pages 17–32, Berkeley, CA, USA, 2014. USENIX Association. [9] A. Friedman and A. Schuster. Data mining with differential privacy. KDD ’10, pages 493–502, New York, NY, USA, 2010. ACM.\n[10] A. Go, R. Bhayani, and L. Huang. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1–12, 2009. [11] I. Guyon, S. Gunn, M. Nikravesh, and L. A. Zadeh. Feature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing). Springer-Verlag New York, Inc., 2006. [12] M. Hardt and G. N. Rothblum. A multiplicative weights mechanism for privacy-preserving data analysis. In In FOCS, pages 61–70, 2010. [13] M. A. W. Hardt. A Study of Privacy and Fairness in Sensitive Data Analysis. PhD thesis, MIT, 2011. [14] M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the accuracy of differentially private histograms through consistency. Proc. VLDB Endow., 3(1-2):1021–1032, Sept. 2010. [15] G. Jagannathan. A Practical Differentially Private Random Decision Tree Classifier. Trans. Data Privacy, 5(1):273–295, 2012. [16] G. Kellaris and S. Papadopoulos. Practical differential privacy via grouping and smoothing. Proc. VLDB Endow., 6(5):301–312, Mar. 2013. [17] J. Lee and C. W. Clifton. Top-k frequent itemsets via differentially private fp-trees. KDD ’14, pages 931–940, New York, NY, USA, 2014. ACM. [18] C. Li, M. Hay, G. Miklau, and Y. Wang. A data- and workload-aware algorithm for range queries under differential privacy. [19] G. J. Matthews and O. Harel. An examination of data confidentiality and disclosure issues related to publication of empirical roc curves. volume 20, pages 889–896. Elsevier, 2013. [20] F. McSherry and K. Talwar. Mechanism design via differential privacy. FOCS ’07, pages 94–103, Washington, DC, USA, 2007. IEEE Computer Society. [21] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. STOC ’07, pages 75–84, New York, NY, USA, 2007. ACM. [22] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. [23] A. D. Sarwate, K. Chaudhuri, and C. Monteleoni. Differentially private support vector machines. CoRR, abs/0912.0071, 2009. [24] A. G. Thakurta and A. Smith. Differentially private feature selection via stability arguments, and the robustness of the lasso. In Conference on Learning Theory, pages 819–850, 2013. [25] J. Vaidya, B. Shafiq, A. Basu, and Y. Hong. Differentially private naive bayes classification. WI-IAT ’13, pages 571–576, Washington, DC, USA, 2013. IEEE Computer Society. [26] Q. Xiao, R. Chen, and K.-L. Tan. Differentially private network data release via structural inference. KDD ’14, pages 911–920, New York, NY, USA, 2014. ACM. [27] X. Xiao, G. Wang, and J. Gehrke. Differential privacy via wavelet transforms. volume 23, pages 1200–1214, Piscataway, NJ, USA, Aug. 2011. IEEE Educational Activities Department. [28] J. Xu, Z. Zhang, X. Xiao, Y. Yang, and G. Yu. Differentially private histogram publication. ICDE ’12, pages 32–43, Washington, DC, USA, 2012. IEEE Computer Society. [29] J. Zhang, G. Cormode, C. M. Procopiuc, D. Srivastava, and X. Xiao. Privbayes: Private data release via bayesian networks. SIGMOD ’14, pages 1423–1434, New York, NY, USA, 2014. ACM. [30] J. Zhang, X. Xiao, Y. Yang, Z. Zhang, and M. Winslett. Privgene: Differentially private model fitting using genetic algorithms. In SIGMOD, pages 665–676, 2013.\nAPPENDIX"
    }, {
      "heading" : "A. PROOF OF THEOREM 2",
      "text" : "PROOF. For any two neighboring datasets D and D′, we would like to show:\nP (vD → v̂) P (vD′ → v̂) ≤ e2σǫ\nwhere vD and vD′ denote the outputs of the non private threshold test on D and D′ resp., and v̂ is the output of PTT. Let N1 = {i ∈ [m] |v̂[i] = 1} and N0 = {i ∈ [m] |v̂[i] = 0} denote the set of 1 and 0 answers resp. of PTT. Let v̂[< i] denote the answers returned by PTT for queries 1 through i− 1. Then\nP (vD → v̂) P (vD′ → v̂) = ∏\ni∈[m]\nP (Qi(D) = v̂[i] | v̂[< i]) P (Qi(D′) = v̂[i] | v̂[< i])\n= ∏\ni∈N1\nP (Qi(D) = 1 | v̂[< i]) P (Qi(D′) = 1 | v̂[< i]) × ∏\ni∈N0\nP (Qi(D) = 0 | v̂[< i]) P (Qi(D′) = 0 | v̂[< i])\n∏\ni∈N1\nP (Qi(D) = 1 | v̂[< i])\n=\n∫\nz\nP (τ̃ = z) ∏\ni∈N1\nP (Qi(D) = 1 | τ̃ = z)dz\n=\n∫\nz\nP (τ̃ = z) ∏\ni∈N1\nP (Qi(D) > z)dz\nThe following two facts complete the proof. First, for any z,\nP (τ̃ = z) ≤ eσǫP (τ̃ = z − σ) (8)\nSecond, for neighboring databases D and D′,\nQi(D) ≥ z ⇒ Qi(D ′) ≥ z − σ\nP (Qi(D) ≥ z) ≤ P (Qi(D ′) ≥ z − σ) (9)\nTherefore, we get: ∏\ni∈N1\nP (Qi(D) = 1 | v̂[< i])\n=\n∫\nz\nP (τ̃ = z) ∏\ni∈N1\nP (Qi(D) > z)dz\n≤ eσǫ ∫\nz\nP (τ̃ = z − σ) ∏\ni∈N1\nP (Qi(D ′) > z − σ)dz\n= eσǫ ∏\ni∈N1\nP (Qi(D ′) = 1 | v̂[< i])\nAn analogous proof for N0 gives the required bound of e2σǫ."
    } ],
    "references" : [ {
      "title" : "Towards sms spam filtering: Results under a new dataset",
      "author" : [ "T.A. Almeida", "J.M.G. Hidalgo", "T.P. Silva" ],
      "venue" : "International Journal of Information Security Science,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Differentially Private Empirical Risk Minimization",
      "author" : [ "Sarwate", "K. Chaudhuri" ],
      "venue" : "CoRR, abs/0912.0,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Practical privacy: The sulq framework",
      "author" : [ "A. Blum", "C. Dwork", "F. McSherry", "K. Nissim" ],
      "venue" : "PODS ’05,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Feature selection for classification",
      "author" : [ "M. Dash", "H. Liu" ],
      "venue" : "Intelligent Data Analysis,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1997
    }, {
      "title" : "Differential privacy",
      "author" : [ "C. Dwork" ],
      "venue" : "In in ICALP, pages 1–12. Springer,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "C. Dwork", "F. McSherry", "K. Nissim", "A. Smith" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Privacy in pharmacogenetics: An end-to-end case study of  personalized warfarin dosing",
      "author" : [ "M. Fredrikson", "E. Lantz", "S. Jha", "S. Lin", "D. Page", "T. Ristenpart" ],
      "venue" : "USENIX Association",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Data mining with differential privacy",
      "author" : [ "A. Friedman", "A. Schuster" ],
      "venue" : "KDD ’10,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Twitter sentiment classification using distant supervision",
      "author" : [ "A. Go", "R. Bhayani", "L. Huang" ],
      "venue" : "CS224N Project Report,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Feature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing)",
      "author" : [ "I. Guyon", "S. Gunn", "M. Nikravesh", "L.A. Zadeh" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "A multiplicative weights mechanism for privacy-preserving data analysis",
      "author" : [ "M. Hardt", "G.N. Rothblum" ],
      "venue" : "In In FOCS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "A Study of Privacy and Fairness in Sensitive Data Analysis",
      "author" : [ "M.A.W. Hardt" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Boosting the accuracy of differentially private histograms through consistency",
      "author" : [ "M. Hay", "V. Rastogi", "G. Miklau", "D. Suciu" ],
      "venue" : "Proc. VLDB Endow.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "A Practical Differentially Private Random Decision Tree Classifier",
      "author" : [ "G. Jagannathan" ],
      "venue" : "Trans. Data Privacy,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Practical differential privacy via grouping and smoothing",
      "author" : [ "G. Kellaris", "S. Papadopoulos" ],
      "venue" : "Proc. VLDB Endow.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Top-k frequent itemsets via differentially private fp-trees",
      "author" : [ "J. Lee", "C.W. Clifton" ],
      "venue" : "KDD ’14,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "An examination of data confidentiality and disclosure issues related to publication of empirical roc curves",
      "author" : [ "G.J. Matthews", "O. Harel" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Mechanism design via differential privacy",
      "author" : [ "F. McSherry", "K. Talwar" ],
      "venue" : "FOCS ’07,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Smooth sensitivity and sampling in private data analysis",
      "author" : [ "K. Nissim", "S. Raskhodnikova", "A. Smith" ],
      "venue" : "STOC ’07,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Differentially private support vector machines",
      "author" : [ "A.D. Sarwate", "K. Chaudhuri", "C. Monteleoni" ],
      "venue" : "CoRR, abs/0912.0071,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Differentially private feature selection via stability arguments, and the robustness of the lasso",
      "author" : [ "A.G. Thakurta", "A. Smith" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Differentially private naive bayes classification",
      "author" : [ "J. Vaidya", "B. Shafiq", "A. Basu", "Y. Hong" ],
      "venue" : "WI-IAT ’13,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Differentially private network data release via structural inference",
      "author" : [ "Q. Xiao", "R. Chen", "K.-L. Tan" ],
      "venue" : "KDD ’14,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Differential privacy via wavelet transforms",
      "author" : [ "X. Xiao", "G. Wang", "J. Gehrke" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Differentially private histogram publication",
      "author" : [ "J. Xu", "Z. Zhang", "X. Xiao", "Y. Yang", "G. Yu" ],
      "venue" : "ICDE ’12,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Privbayes: Private data release via bayesian networks",
      "author" : [ "J. Zhang", "G. Cormode", "C.M. Procopiuc", "D. Srivastava", "X. Xiao" ],
      "venue" : "SIGMOD ’14,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "Privgene: Differentially private model fitting using genetic algorithms",
      "author" : [ "J. Zhang", "X. Xiao", "Y. Yang", "Z. Zhang", "M. Winslett" ],
      "venue" : "In SIGMOD,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "The promise of effectively utilizing such ‘big-data’ has been realized in part due to the success of off-the-shelf machine learning algorithms, especially supervised classifiers [3].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "For instance, Fredrikson et al [8] proposed a model inversion attack using which properties (genotype) of individuals in the training dataset can be learnt from linear regression models built on private medical data.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "To address this concern, recent work has focused on developing private classifier training algorithms that ensure a strong notion of privacy called ǫ-differential privacy [6] – the classifier output by a differentially private training algorithm does not significantly change due to the insertion or deletion of any one training example.",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 23,
      "context" : "Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "In fact, Fredrikson et al [8] also show that differentially private algorithms for the related problem of linear regression result in unacceptable error when applied to real medical datasets.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "In particular, real datasets usually have many features that are of little to no predictive value, and feature selection techniques [5] are used to identify the predictive subset of features.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "Recent work [19] has shown that releasing an ROC curve computed on a private test set can leak sensitive information to an adversary with access to certain properties of the test dataset.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 23,
      "context" : "Nevertheless, we show on real datasets and with two differentially private classifiers (Naive Bayes [25] and logistic regression [2]) that private feature selection indeed leads to significant improvement in the classifiers prediction accuracy.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "Nevertheless, we show on real datasets and with two differentially private classifiers (Naive Bayes [25] and logistic regression [2]) that private feature selection indeed leads to significant improvement in the classifiers prediction accuracy.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 25,
      "context" : "Thus we can utilize algorithms from prior work ([27]) to accurately compute the statistics needed for the ROC curve.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "DEFINITION 1 (DIFFERENTIAL PRIVACY [6]).",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "A popular differentially private algorithm is the Laplace mechanism [7] defined as follows:",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "THEOREM 1 (COMPOSITION [20]).",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Thus without loss of generality we can define the classifier as outputting a real number p ∈ [0, 1] which corresponds to the probability of L = 1.",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Two examples of such classifiers include the Naive Bayes classifier and logistic regression [3].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "Feature selection is a dimensionality reduction technique that typically precedes classification, where only a subset of the features F ′ ⊂ F in the dataset are retained based on some criterion of how well F ′ predicts the label L [11].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 10,
      "context" : "Feature selection methods can be categorized as filter, wrapper and embedded methods [11].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "Purity Index [11]: The purity index for a feature F , denoted by PI(F,D), is defined as:",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "The sensitivity of information gain function has been shown to be O(log n) [9, 29], where n is (an upper bound on) the number of tuples in the dataset.",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "The sensitivity of information gain function has been shown to be O(log n) [9, 29], where n is (an upper bound on) the number of tuples in the dataset.",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "This is akin to recent work on data dependent mechanisms for releasing histograms and answering range queries that group categories with similar counts and release a single noisy count for each group [16, 18, 28].",
      "startOffset" : 200,
      "endOffset" : 212
    }, {
      "referenceID" : 26,
      "context" : "This is akin to recent work on data dependent mechanisms for releasing histograms and answering range queries that group categories with similar counts and release a single noisy count for each group [16, 18, 28].",
      "startOffset" : 200,
      "endOffset" : 212
    }, {
      "referenceID" : 3,
      "context" : "We use private k-means clustering [4] to cluster the points.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "This is a significant improvement over the related sparse vector technique (SVT) first described in Hardt [13], which allows releasing upto a constant c query answers that are above a threshold τ .",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "technique used in Lee et al [17].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "For every tuple t ∈ Dtest, let t[L] ∈ {0, 1} denote the true label, and p(t) ∈ [0, 1] denote the prediction returned by some classifier (probability that t[L] = 1).",
      "startOffset" : 79,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "Recent work [19] has shown that releasing the actual ROC curves on a private test dataset can allow an attacker with prior knowledge to reconstruct the test dataset.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "Instead, using strategies like the hierarchical mechanism [14] or Privelet [27] allow answering each onesided range query with no more than O(log n/ǫ) error.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "Instead, using strategies like the hierarchical mechanism [14] or Privelet [27] allow answering each onesided range query with no more than O(log n/ǫ) error.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "A simple data-independent strategy for picking the set of thresholds is to choose them uniformly from [0, 1].",
      "startOffset" : 102,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "More precisely, if n is the cardinality of Dtest, we choose the number of thresholds to be an α ∈ [0, 1] fraction of n, and choose the set of thresholds to be Θ = {0, 1 ⌊αn⌋ , 2 ⌊αn⌋ , .",
      "startOffset" : 98,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "This strategy works well when the predictions P = {p(t)|t ∈ Dtest} are uniformly spread out in [0, 1].",
      "startOffset" : 95,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "Since median has a high global sensitivity (equal to right if all values are in the range (left, right)), we use the smooth sensitivity framework [21] for computing the noisy median.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 13,
      "context" : "We leverage the ordering constraint between the TPR and FPR values to boost the accuracy by using the constrained inference method proposed by Hay et al [14].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "The TWITTER dataset [10] was collected for the task of sentiment classification.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "The SMS dataset [1] contains 5574 SMS messages associated with spam/ham label.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "We choose to evaluate our feature selection algorithm on two state of the art differentially private classifiers – Naive Bayes [25], and the differentially private ERM implementation of logistic regression [2].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "We choose to evaluate our feature selection algorithm on two state of the art differentially private classifiers – Naive Bayes [25], and the differentially private ERM implementation of logistic regression [2].",
      "startOffset" : 206,
      "endOffset" : 209
    }, {
      "referenceID" : 20,
      "context" : "For non-private logistic regression, we have used the prepackaged Scikit-learn logistic regression classifier [22].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "We use an implementation of Chaudhuri et al’s [2] differentially private empirical risk minimization (henceforth called ERM) for logistic regression.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "Figure 3 shows the F1 scores for 4 private feature selection methods using TC – score perturbation, clustering, PTT and NOISYCUT [17].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "SMS test set contains 558 data, and each tuple t has a true label t[L] ∈ {0, 1} as well as a prediction p(t) ∈ [0, 1] for the label 1.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "The FixedSpace line shows the error of α-FIXEDSPACE, with α · n thresholds chosen uniformly in [0, 1].",
      "startOffset" : 95,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "But in this case, the postprocessing step that enforces monotonicity results in error that has a strong dependence on the number of distinct TPR and FPR values, and not the total number of thresholds (see Theorem 2 [14]).",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 23,
      "context" : "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 28,
      "context" : "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 24,
      "context" : "[26] present a generic algorithm for differentially private parameter tuning and model selection.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] present an algorithm for model selection again assuming strong stability assumptions about the model training algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "Private Threshold Testing: As mentioned before, private threshold testing (PTT) is inspired by the sparse vector technique (SVT) [13] which was first used in the context of the multiplicative weights mechanism [12].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "Private Threshold Testing: As mentioned before, private threshold testing (PTT) is inspired by the sparse vector technique (SVT) [13] which was first used in the context of the multiplicative weights mechanism [12].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 16,
      "context" : "Lee et al [17] solve the same problem as PTT in the context of frequent itemset mining.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 17,
      "context" : "However, directly releasing the ROC curve may reveal the sensitive information of the input dataset [19].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : "Chaudhuri et al [26] proposes a generic technique for evaluating a classifier on a private test set.",
      "startOffset" : 16,
      "endOffset" : 20
    } ],
    "year" : 2014,
    "abstractText" : "An important use of private data is to build machine learning classifiers. While there is a burgeoning literature on differentially private classification algorithms, we find that they are not practical in real applications due to two reasons. First, existing differentially private classifiers provide poor accuracy on real world datasets. Second, there is no known differentially private algorithm for empirically evaluating the private classifier on a private test dataset. In this paper, we develop differentially private algorithms that mirror real world empirical machine learning workflows. We consider the private classifier training algorithm as a blackbox. We present private algorithms for selecting features that are input to the classifier. Though adding a preprocessing step takes away some of the privacy budget from the actual classification process (thus potentially making it noisier and less accurate), we show that our novel preprocessing techniques signficantly increase classifier accuracy on three real-world datasets. We also present the first private algorithms for empirically constructing receiver operating characteristic (ROC) curves on a private test set.",
    "creator" : "gnuplot 4.6 patchlevel 4"
  }
}