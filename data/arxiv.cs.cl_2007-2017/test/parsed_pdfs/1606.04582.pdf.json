{
  "name" : "1606.04582.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Query-Regression Networks for Machine Comprehension",
    "authors" : [ "Minjoon Seo", "Hannaneh Hajishirzi", "Ali Farhadi" ],
    "emails" : [ "hannaneh}@washington.edu,", "ali@cs.washington.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Machine comprehension (MC) is the task of obtaining the answer to a natural language question given a sequence of natural language sentences (story) [15]. We are particularly interested in the task of end-to-end MC, where (1) external language resources, such as lexicon or dependency parser, are not provided, and (2) the only supervision during training is the answer to the question. Note that both of these restrictions only make the task harder.\nWhile Recurrent Neural Network (RNN) and its variants, such as Long Short-Term Memory [8] and Gated Recurrent Unit [4], are popular choices for modeling sequential data, Weston et al. [19] have shown that RNN-based models perform poorly on end-to-end MC, largely because RNN’s internal memory is inherently unstable over a long term. Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20]. The attention mechanism allows the models to focus on a single sentence in each layer, and the models can sequentially read multiple relevant sentences from the memory with multiple layers. However, the biggest drawback of the vanilla attention mechanism is that it is insensitive to the time step (memory address) of the sentences when accessing them. End-to-end Memory Network [16] attempts to resolve this problem by adding time-dependent variable to the sentence representation at each time step (address) of the memory. Dynamic Memory Network [10, 20] combines RNN and attention mechanism together to incorporate time dependency into the model.\nOur proposed model, Query-Regression Network (QRN), is a single recurrent unit that addresses the long-term dependency problem of most RNN-based models while taking the full advantage of RNN’s capability to model sequential data. QRN considers the sentences (story) as a sequence of state-changing triggers, and QRN transforms (regresses) the original question (query) to an easierto-answer query as it observes each trigger through time. For instance, consider the question-story pair in Figure 1. The original question, “Where is the apple?” cannot be directly answered by any single sentence from the story. Hence, after observing the first sentence “Sandra got the apple there.”, QRN transforms the original question to “Where is Sandra?”, which is presumably easier to answer. This mechanism is akin to logic regression in situation calculus [14]. While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information. We will experimentally demonstrate that the\nar X\niv :1\n60 6.\n04 58\n2v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20\nlocal query regression is effective for handling the time dependency problem, and we will show that QRN can be parallelized over time, unlike most RNN-based models."
    }, {
      "heading" : "2 Model",
      "text" : "Notation. We use lowercase, boldface letters to denote column vectors (e.g. x), and uppercase, boldface letters (e.g. W) to denote matrices. Non-boldface italic letters denote scalar values (e.g. t, T, d), and boldface italic letters denote non-numerical objects such as a sentence (e.g. x). > is used to denote vector or matrix transpose. Scalar and vector functions will be denoted by non-boldface and boldface lowercase Greek letters (e.g. α, γ), respectively. [; ] is the concatenation of vectors across row, and [, ] is the concatenation of vectors across column. 〈, 〉 denotes a sequence. A subscript denotes time step (t) in the story, and a superscript denotes the layer index (k) of the networks. A superscript with parenthesis (e.g. W(z), W(h)) indicates that the weight matrix or vector is used to compute the value in the parenthesis (e.g. z, h).\nIn end-to-end MC, the input is a story as a sequence of sentences, 〈x1, . . . ,xT 〉, where T is the number of sentences in the story, and the question, q. The output is the predicted answer to the question in natural language, ŷ (e.g. “garden\" in Figure 1), which is to be compared against the true answer, y. QRN can be considered as a variant of RNN with two inputs, two outputs, and a hidden state, all of which operate in vector space. Hence, we need (1) an input module that maps each sentence xt and the question q into vector space, and (2) an output module that maps the output vector of QRN to the answer in natural language. In this section, we focus on our main contribution, QRN, and we defer the discussion of the input and output modules, for which we adopt standard solutions, to Section 5. Let d represent the size of the hidden state in a QRN unit. Let 〈x1, . . . ,xT 〉 represent the embeddings of T sentences of the story in d-dimensional vector space, and let q ∈ Rd represent the embedding of the question q. These vector representations will be obtained by the input module. Then QRN produces the output vector ŷ ∈ Rd, which is to be passed to the output module to obtain the predicted answer in natural language.\nWe first formally define the base model of a QRN unit, and then we explain how we connect the input and output modules to it. We also present a few variations to the network that can improve QRN’s performance. In Section 3 we show that QRN can be parallelized over time, giving computational advantage over most RNN-based models by one order of magnitude."
    }, {
      "heading" : "2.1 QRN Unit",
      "text" : "As an RNN-based model, QRN is a single recurrent unit that updates its hidden state through time and layers. A QRN unit accepts two inputs (local query vector qt ∈ Rd and sentence vector xt ∈ Rd), and two outputs (reduced query vector ht ∈ Rd and the sentence vector xt from input without modification). The local query vector is not necessarily identical to the original query (question) vector q. In order to compute the outputs, we use update gate function α : Rd × Rd → [0, 1] and\nregress function γ : Rd × Rd → Rd. Intuitively, the update gate function measures the relevance between the sentence and the local query, and the regress function transforms the local query input to a new regressed (easier) query given the sentence. Now the outputs can be obtained with the following equations:\nzt = α(xt,qt) = σ(W (z)(xt ◦ qt) + b(z)) (1)\nh̃t = γ(xt,qt) = tanh(W (h)[xt;qt] + b (h)) (2)\nht = zth̃t + (1− zt)ht−1 (3) where σ(·) is sigmoid activation, tanh(·) is hyperboolic tangent activation (applied element-wise), W(z) ∈ R1×d, W(h) ∈ Rd×2d are weight matrices, b(z) ∈ R, b(h) ∈ Rd are bias terms, ◦ is element-wise vector multiplication, and [; ] is vector concatenation along the row. As a base case, h0 = 0. Here we have explicitly defined α and γ, but they can be any arbitrary differentiable functions in general. In order to stack several layers of QRN, the outputs of the current layer are used as the inputs to the next layer. That is, using superscript k to denote the current layer’s index (assuming 1-based indexing), we let qk+1t = h k t . Note that xt is passed to the next layer without any modification, so we do not put a layer index on it. Figure 1 left depicts the schematic structure of a QRN unit.\nConnecting input and ouptut modules. In the first layer, q1t = q for all t, where q is obtained from the input module by processing the natural language question input q. xt is also obtained from xt by the same input module. The output at the last time step in the last layer is passed to the output module. That is, ŷ = hKt where K represent the number of layers in the networks. Then the output module gives the predicted answer ŷ in natural language."
    }, {
      "heading" : "2.2 Variations",
      "text" : "Here we introduce a few variations of QRN, and later in our experiments, we test QRN’s performance with and without each of these variations.\nReset gate. Inspired by Gated Recurrent Unit (GRU) [4], we found that it is useful to allow the QRN unit to reset (nullify) the candidate regressed query (i.e., h̃t) when necessary. For this we use reset gate function ρ : Rd ×Rd → [0, 1], which can be defined similarly to the update gate function:\nrt = ρ(xt,qt) = σ(W (r)(xt ◦ qt) + b(r)) (4)\nwhere W(r) ∈ R1×d is a weight matrix, and b(r) ∈ R is a bias term. Then Equation 3 can be rewritten as ht = ztrth̃t + (1− zt)ht−1. (5) Note that we do not use the reset gate in the last layer.\nVector gates. As in LSTM [8] or GRU [4], update and reset gates can be vectors instead of scalar values for fine-controlled gating. For vector gates, we modify the row dimension of weights and biases in Equation 1 and 4 from 1 to d. Then we obtain zt, rt ∈ Rd (instead of zt, rt ∈ R), and these can be element-wise multiplied (◦) instead of being broadcasted in Equations 3 and 5.\nBi-direction. So far we have only shown the forward direction definition of ht. That is, at each time step t, we assumed that QRN only needs to look at past sentences, and does not need to know about the future. However, often times, query answers can depend on future sentences. For instance, consider a sentence “John dropped the football.” at time t. Then, even if there is no mention about the “football” in the past (at time i < t), it can be implied that “John” has the “football” at the current time t. In order to incorporate the future dependency, we obtain −→ h t and ←− h t in both forward and backward directions, respectively, using Equation 3, and then we add them together to get qt for the next layer. That is,\nqk+1t = −→ h kt + ←− h kt (6)\nfor layer indices 1 ≤ k ≤ K − 1. Note that the variables W(z), b(z),W(h),b(h) are shared between the two directions (the variables of the reset gate are not shared)."
    }, {
      "heading" : "3 Parallelization",
      "text" : "An important advantage of QRN is that the recurrent updates in Equations 3 and 4 can be computed in parallel across time. This is in contrast with most RNN-based models that cannot be parallelized, where computing the output at time t requires computation at all previous time steps i < t. Here we primarily show that the query update in Equation 3 can be parallelized by rewriting the equation with matrix operations. The extension to Equation 4 is straightforward. The proof will also give an intuition how QRN with vector gates can be parallelized as well. The recursive definition of Equation 3 can be explicitly written as\nht = t∑ i=1  t∏ j=i+1 1− zj  zih̃i = t∑ i=1 exp  t∑ j=i+1 log (1− zj)  zih̃i (7) where exp(·) and log(·) are exponential and logarithm functions, respectively. Let bi = log(1− zi) for brevity. Then we can rewrite Equation 7 as the following equation:\n h>1 h>2 h>3\n... h>T\n = exp   0 −∞ −∞ . . . −∞ b2 0 −∞ . . . −∞ b2 + b3 b3 0 . . . −∞ ... ... ... . . . ...∑T\nj=2 bj ∑T j=3 bj ∑T j=4 bj . . . 0\n    z1h̃ > 1 z2h̃ > 2 z3h̃ > 3\n... zT h̃ > T  (8) Let H = [h>1 ; . . . ;h > T ] be a T -by-d matrix where the transposes (>) of the column vectors ht are concatenated across row. We similarly define H̃ from h̃t. Also, let z = [z1; . . . ; zT ] and b = [0; b2; . . . ; bT ] be column vectors (note that we use 0 instead of b1). Then Equation 8 is equivalent to\nH = [L ◦ exp (L [B ◦ L′])] [ Z ◦ H̃ ] (9)\nwhere L,L′ ∈ RT×T are lower and strictly lower triangular matrices of 1’s, respectively, ◦ is elementwise multiplication, and B is a matrix where T b’s are tiled across the column, i.e. B = [b, . . . ,b] ∈ RT×T , and similarly Z = [z, . . . , z] ∈ RT×d. All implicit operations are matrix multiplications. With reasonable N (batch size), d and T (e.g. N, d, T = 100), matrix operations in Equation 9 can be comfortably computed in most modern GPUs."
    }, {
      "heading" : "4 Related Work",
      "text" : "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6]. Here, we primarily describe the most related approaches, which use deep neural networks for end-to-end machine comprehension.\nQRN is inspired by RNN-based models with gating mechanism, such as LSTM [8] and GRU [4]. As QRN is designed for sequential data with query, its unit accepts two inputs (query and sentences) instead of one input. While GRU and LSTM use the previous hidden state and the current input to obtain the current hidden state, QRN only uses the current two inputs to obtain the local query (equivalent to hidden state). We conjecture that this not only gives computational advantage via parallelization, but also makes training easier, i.e., avoiding vanishing gradient (which is critical for long-term dependency), overfitting, and converging to a local minima. The advantage of QRN over other RNN-based models is experimentally shown in Table 1, where LSTM performs very poorly.\nEnd-to-end Memory Network (MemN2N) [16] (and Neural Reasoner [13]) uses external memory with multi-layer attention mechanism to focus on sentences that are relevant to the question. There are two key differences between the model and QRN. First, MemN2N summarizes the entire memory in each layer to control the attention in the next layer, as indicated by circle nodes in Figure 2b. This is in contrast with QRN which does not have any summarization node, as shown in Figure 2a. Instead, QRN is able to focus on relevant sentences through the update gate that is internally embodied within its unit. Second, MemN2N adds time-dependent trainable weights to the sentence representations to model the time dependency of the sentences (as discussed in Section 1). QRN does not need\nsuch additional weights as its inherent RNN architecture allows QRN to effectively model the time dependency.\nImproved Dynamic Memory Network (DMN+) [20] uses the hybrid of the attention mechanism and the RNN architecture to model the sequence of sentences. It consists of two distinct GRUs, one for the time axis (rectangle nodes in Figure 2c) and one for the layer axis (circle nodes in Figure 2c). Note that the update gate of the GRU for the time axis is replaced with external softmax attention weights. DMN+ uses the time-axis GRU to summarizes the entire memory in each layer, and then the layer-axis GRU controls the attention weights in each layer. In contrast, QRN is simply a single recurrent unit without any memory summarization node (circle node).\nMemory Networks [18] and Dynamic Memory Networks [10] are earlier models that inspired MemN2N and DMN+, respectively. However, they are strongly supervised (i.e. during training they are given what facts are relevant to the question), where QRN is only supervised by the answers to the questions. So we do not make a direct comparison against them.\nWhile Mitra and Baral [12] and Lee et al. [11] have shown near-perfect accuracies in bAbI QA dataset, they are largely rule-based (requiring human efforts), they use different models for different tasks, and/or they use external language resources such as dependency parser. So we do not consider them as end-to-end machine comprehension systems."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Data",
      "text" : "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19]. bAbi dataset is composed of 20 different tasks, each of which has 1,000 (1k) synthetically-generated story-question pair. A story can be as short as two sentences and as long as 200+ sentences. A system is tested on getting the correct answers to the questions, and the authors of the dataset define that a system passes a task if its error rate is no more than 5%. Most answers are single words, and some of them are lists (e.g. “football, apple”). Answering questions in each task requires selecting a set of relevant sentences and applying a different kinds of logical reasoning over them. See Figures 1 for the complete list of the 20 tasks.\nbAbI dataset also provides 10,000 (10k) dataset (for each task), which was originally intended for evaluating how much more training data (than 1k) does a system need to pass each task. However, since DMN+ [20] only reports on the 10k dataset, we also report our result on it for a fair comparison. Following MemN2N [16] and DMN+, QRN is only supervised by question answers. Hence we do not compare QRN against Memory Networks [18] and DMN [10], which require strong supervision of supporting facts during training."
    }, {
      "heading" : "5.2 Model Details",
      "text" : "Input Module. In the input module, we are given sentences xt and a question q, and we want to obtain their vector representations, xt,q ∈ Rd. We use a trainable embedding matrix A ∈ Rd×V to encode the one-hot vector of each word xtj in each sentence xt into a d-dimensional vector xtj ∈ Rd. Then the sentence representation xt is obtained by Position Encoder [18]: xt = ∑ j lj ◦ xtj where k-th element of the vector lj ∈ Rd is (1− j/J)− (k/d)(1− 2j/J) (assuming 1-based indexing), where J is the number of words in xt. Note that the order of words affects xt. The same encoder with the same embedding matrix is also used to obtain the question vector q from q.\nOutput Module. In the output module, we are given the vector representation of the predicted answer ŷ and we want to obtain the natural language form of the answer, ŷ. We use a V -way singlelayer softmax classifier to map ŷ to a V -dimensional sparse vector, v̂ = softmax ( W(y)ŷ ) ∈ RV , where W(y) ∈ RV×d is a weight matrix. Then the final answer ŷ is simply the argmax word in v̂. For 10k dataset, we instead use v̂ = softmax ( W(y)[ŷ;q] ) (hence W(y) ∈ RV×2d). To handle questions with multiple-word answers, we simply consider each of them as a single word that contains punctuations such as space and comma, and put it in the vocabulary. Although not explored, an alternative way will be to use an RNN decoder [4] to output each word of the answer at each time step of the decoder.\nTraining. We withheld 10% of the training for development. d was set to 50 for all QRN models. Batch sizes of 32 and 128 were used for 1k and 10k datasets, respectively. The weights in the input and output modules were initialized with zero mean and the standard deviation of 1/ √ d. Weights in the QRN unit were initialized using techniques by Glorot and Bengio [7], and were tied across the layers. Forget bias of 2.5 was used for update gates (no bias for reset gates). L2 weight decay of 0.001 was used for all weights. The loss function is the cross entropy between v̂ and the one-hot vector of the true answer. The loss is minimized by stochastic gradient descent for 150 epochs, and the learning rate was controlled by AdaGrad [5] with the initial learning rate of 0.5. Since the model was sensitive to the weight initialization, we repeated each training procedure 10 times (50 times for 10k) with the new random initialization of the weights and reported the result on the test data with the lowest loss on the development data."
    }, {
      "heading" : "5.3 Results",
      "text" : "Table 1 reports the results of our model (QRN) and previous work on bAbI QA. Most notably, in 1k data, the average accuracy of QRN’s ‘2rb’ (2 layers + reset gate + bi-direction) model outperforms that of MemN2N (PE+LS+LW+joint model) by 5.3%, and the task-wise accuracies of the QRN model are equal or higher than those of MemNN in 19 tasks, only losing in Task 16. Also, the QRN’s ‘3rb’ model passes 5 more tasks than MemN2N.\nIn 10k dataset, the average accuracy of QRN’s 10k ‘2rvb’ (2 layers + reset gate + vector gates + bi-direction) model outperforms that of MemN2N (PE+LS+LW+RN* model) by 1.0%. It is also comparable to that of DMN+, differing only by 0.3%, even though our model is simpler and faster to train than DMN+. The same QRN model also sets the state of the art in Tasks 5, 7, and 17.\nAblations. We tested four types of ablations (also discussed in Section 2.2): number of layers (1, 2, 3, or 6), reset gate (r), gate vectorization (v), and bi-directional query update (b). We show a subset of combinations of the ablations in Table 1; other combinations performed poorly and/or did not give interesting observations. According to the ablation results, we can infer that:\n• When the number of layers is only one, the model lacks reasoning capability. When there are too many layers (6), it seems correctly training the model becomes increasingly difficult.\n• Having reset gate helps, especially for Task 3. • Having vector gates hurts in 1k dataset, as the model either overfits to the training data or\nconverges to a local minima. On the other hand, vector gates in 10k dataset give a significant improvement on Task 19 (comparing between ‘2rb’ and ‘2rvb’).\n• Uni-directional models performed poorly, so we did not include them in the result table.\nParallelization. We implemented QRN with and without parallelization in TensorFlow [1] on a single Titan X GPU to qunaitify the computational gain of the parallelization. For QRN without parallelization, we used the RNN library provided by TensorFlow. QRN with parallelization gave 6.2 times faster training and testing than QRN without parallelization on average. We expect that the speedup can be even higher for datasets with longer sentences.\nInterpretations. One of the advantages of QRN is that the intermediate query updates are interpretable. Figure 1 shows intermediate local queries (qkt ) interpreted in natural language, such as “Where is Sandra?”. In order to obtain these, we place a decoder on the input question embedding q and add its loss for recovering the question to the classification loss (similarly to Peng et al. [13]). We then use the same decoder to decode the intermediate queries. This helps us understand the flow of information in the networks. In Figure 1, the question “Where is apple?” is transformed into “Where is Sandra?” at t = 1. At t = 2, as “Sandra dropped the apple.”, the apple is no more relevant to Sandra. We obtain “Where is Daniel?” at time t = 3, and it is propagated until t = 5, where we observe a sentence (fact) that can be used to answer the query. In this case, the regress function γ is trained so that it outputs the answer to the query (“garden”) instead of another regressed query.\nWe also visualize the (scalar) magnitudes of update and reset gates on story sentences, as shown in Figure 3. Consider the Task 2 example (top left): in the first layer, we see high update gate values on facts that tell who has the “apple”, and in the second layer, we see high update gate values on those that tell where that person went to. We also see that the forward reset gate at t = 2 in the first layer (−→r 12) is low, which is signifying that “apple” no more belongs to “Sandra”."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Query-Regression Network (QRN) answers machine comprehension questions by storing local queries and regressing them as QRN observes state-changing sentences. While an RNN-based model, QRN is effective for encoding long-term dependencies between sentences. In addition, it is simpler than previous work [16, 20] and is highly parallelizable, making them a suitable choice for large-scale question answering when the size of stories grows. Lastly, QRN is able to answer questions after each time step, and its internal representation, in the form of updated query, is interpretable. We have experimentally shown that QRN is capable of learning to perform different kinds of reasoning over multiple facts in bAbI QA dataset. Future work involves using QRN for language modeling and machine comprehension on real data such as MCTest [15]."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "author" : [ "Martın Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin" ],
      "venue" : "arXiv preprint arXiv:1603.04467,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Large-scale semantic parsing via schema matching and lexicon extension",
      "author" : [ "Qingqing Cai", "Alexander Yates" ],
      "venue" : "In ACL,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Paraphrase-driven learning for open question answering",
      "author" : [ "Anthony Fader", "Luke S. Zettlemoyer", "Oren Etzioni" ],
      "venue" : "In ACL,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In JMLR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Learning to solve arithmetic word problems with verb categorization",
      "author" : [ "Mohammad Javad Hosseini", "Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Reasoning in vector space: An exploratory study of question answering",
      "author" : [ "Moontae Lee", "Xiaodong He", "Wen tau Yih", "Jianfeng Gao", "Li Deng", "Paul Smolensky" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning",
      "author" : [ "Arindam Mitra", "Chitta Baral" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Towards neural network-based reasoning",
      "author" : [ "Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong" ],
      "venue" : "arXiv preprint arXiv:1508.05508,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Knowledge in Action",
      "author" : [ "Raymond Reiter" ],
      "venue" : "MIT Press, 1st edition,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2001
    }, {
      "title" : "Mctest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher JC Burges", "Erin Renshaw" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Semantic parsing via staged query graph generation: Question answering with knowledge base",
      "author" : [ "Wen tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao" ],
      "venue" : "In ACL,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher" ],
      "venue" : "In ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Machine comprehension (MC) is the task of obtaining the answer to a natural language question given a sequence of natural language sentences (story) [15].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "While Recurrent Neural Network (RNN) and its variants, such as Long Short-Term Memory [8] and Gated Recurrent Unit [4], are popular choices for modeling sequential data, Weston et al.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "While Recurrent Neural Network (RNN) and its variants, such as Long Short-Term Memory [8] and Gated Recurrent Unit [4], are popular choices for modeling sequential data, Weston et al.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "[19] have shown that RNN-based models perform poorly on end-to-end MC, largely because RNN’s internal memory is inherently unstable over a long term.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20].",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20].",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20].",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20].",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "End-to-end Memory Network [16] attempts to resolve this problem by adding time-dependent variable to the sentence representation at each time step (address) of the memory.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "Dynamic Memory Network [10, 20] combines RNN and attention mechanism together to incorporate time dependency into the model.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "Dynamic Memory Network [10, 20] combines RNN and attention mechanism together to incorporate time dependency into the model.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "This mechanism is akin to logic regression in situation calculus [14].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "In order to compute the outputs, we use update gate function α : R × R → [0, 1] and",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "Inspired by Gated Recurrent Unit (GRU) [4], we found that it is useful to allow the QRN unit to reset (nullify) the candidate regressed query (i.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "For this we use reset gate function ρ : R ×R → [0, 1], which can be defined similarly to the update gate function: rt = ρ(xt,qt) = σ(W (xt ◦ qt) + b) (4) where W ∈ R1×d is a weight matrix, and b ∈ R is a bias term.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "As in LSTM [8] or GRU [4], update and reset gates can be vectors instead of scalar values for fine-controlled gating.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "As in LSTM [8] or GRU [4], update and reset gates can be vectors instead of scalar values for fine-controlled gating.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].",
      "startOffset" : 137,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].",
      "startOffset" : 137,
      "endOffset" : 153
    }, {
      "referenceID" : 8,
      "context" : "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].",
      "startOffset" : 137,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].",
      "startOffset" : 137,
      "endOffset" : 153
    }, {
      "referenceID" : 5,
      "context" : "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].",
      "startOffset" : 137,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "QRN is inspired by RNN-based models with gating mechanism, such as LSTM [8] and GRU [4].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "QRN is inspired by RNN-based models with gating mechanism, such as LSTM [8] and GRU [4].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : "End-to-end Memory Network (MemN2N) [16] (and Neural Reasoner [13]) uses external memory with multi-layer attention mechanism to focus on sentences that are relevant to the question.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "End-to-end Memory Network (MemN2N) [16] (and Neural Reasoner [13]) uses external memory with multi-layer attention mechanism to focus on sentences that are relevant to the question.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "(b) MemN2N [16] x\" x# x$ q = h( AGRU AGRU AGRU GRU ⋯ x\" x# x$ AGRU AGRU AGRU GRU ⋯",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 18,
      "context" : "(c) DMN+ [20]",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "Figure 2: The schematics of QRN and the two state-of-the-art models, End-to-End Memory Networks (MemN2N [16]) and Improved Dynamic Memory Networks (DMN+ [20]), simplified to emphasize the differences among the models.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "Figure 2: The schematics of QRN and the two state-of-the-art models, End-to-End Memory Networks (MemN2N [16]) and Improved Dynamic Memory Networks (DMN+ [20]), simplified to emphasize the differences among the models.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Improved Dynamic Memory Network (DMN+) [20] uses the hybrid of the attention mechanism and the RNN architecture to model the sequence of sentences.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "Memory Networks [18] and Dynamic Memory Networks [10] are earlier models that inspired MemN2N and DMN+, respectively.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "While Mitra and Baral [12] and Lee et al.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "[11] have shown near-perfect accuracies in bAbI QA dataset, they are largely rule-based (requiring human efforts), they use different models for different tasks, and/or they use external language resources such as dependency parser.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19].",
      "startOffset" : 82,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19].",
      "startOffset" : 82,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19].",
      "startOffset" : 82,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "However, since DMN+ [20] only reports on the 10k dataset, we also report our result on it for a fair comparison.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "Following MemN2N [16] and DMN+, QRN is only supervised by question answers.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Hence we do not compare QRN against Memory Networks [18] and DMN [10], which require strong supervision of supporting facts during training.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "Although not explored, an alternative way will be to use an RNN decoder [4] to output each word of the answer at each time step of the decoder.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "Weights in the QRN unit were initialized using techniques by Glorot and Bengio [7], and were tied across the layers.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "The loss is minimized by stochastic gradient descent for 150 epochs, and the learning rate was controlled by AdaGrad [5] with the initial learning rate of 0.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "9 Table 1: bAbI QA dataset [19] error rates (%) of QRN and previous work: LSTM [19], End-to-end Memory Networks (N2N [16]), and Dynamic Memory Networks (DMN+ [20]).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "9 Table 1: bAbI QA dataset [19] error rates (%) of QRN and previous work: LSTM [19], End-to-end Memory Networks (N2N [16]), and Dynamic Memory Networks (DMN+ [20]).",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "9 Table 1: bAbI QA dataset [19] error rates (%) of QRN and previous work: LSTM [19], End-to-end Memory Networks (N2N [16]), and Dynamic Memory Networks (DMN+ [20]).",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "9 Table 1: bAbI QA dataset [19] error rates (%) of QRN and previous work: LSTM [19], End-to-end Memory Networks (N2N [16]), and Dynamic Memory Networks (DMN+ [20]).",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "We implemented QRN with and without parallelization in TensorFlow [1] on a single Titan X GPU to qunaitify the computational gain of the parallelization.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "[13]).",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "We present Query-Regression Network (QRN), a variant of Recurrent Neural Network (RNN) that is suitable for end-to-end machine comprehension. While previous work largely relied on external memory and attention mechanism, QRN is a single recurrent unit with internal memory. Unlike most RNN-based models, QRN is able to effectively handle long-term dependencies and is highly parallelizable. In our experiments we show that QRN outperforms the state of the art in end-to-end bAbI QA tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}