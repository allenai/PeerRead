{
  "name" : "1206.6431.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Exact Maximum Margin Structure Learning of Bayesian Networks",
    "authors" : [ "Robert Peharz", "Franz Pernkopf" ],
    "emails" : [ "robert.peharz@tugraz.at", "pernkopf@tugraz.at" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Bayesian networks (BNs) are an important type of probabilistic graphical models (Pearl, 1988; Koller & Friedmann, 2009) and specify a probability distribution over a set of random variables (RVs). They make use of a directed acyclic graph (DAG), with nodes corresponding to the RVs, representing the factorization of the joint distribution. Learning the structure of Bayesian networks from data can be cast as optimization problem, where the goal is to find a DAG maximizing some score function. This is a combinatorial problem and known to be NP-hard in general (Chickering, 1996). Therefore, most approaches to learn the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nBN structure are approximative or greedy heuristics. Recently, there has been much interest in exact structure learning, i.e. in finding globally optimal DAGs. Koivisto et al. (2004) use a dynamic programming approach for efficiently summing over all variable orders, leading to exponential (rather than super-exponential) run-time. Further development of this approach can be found in (Silander & Myllymäki, 2006; Parviainen & Koivisto, 2009). Due to exponential run-time, these methods are currently restricted to approximately 30- 50 variables. Alternatively to dynamic programming, branch-and-bound (B&B) techniques have been exploited for exact structure learning (de Campos et al., 2009; de Campos & Ji, 2011; Jaakkola et al., 2010). In comparison to dynamic programming, these techniques offer the advantage of an any-time solution, i.e. as soon as some feasible solution has been found, the algorithm can be interrupted and returns the currently best solution, together with a worst-case suboptimality bound. However, if the algorithm is kept running, it eventually finds a globally optimal solution.\nThese methods have been developed for generative structure learning, i.e. they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al., 1995). These scores are decomposable, i.e. they can be written as a sum over local scores, one for each random variable. On the other hand, when the learned BN shall be used as classifier, we aim to maximize a discriminative score, such as (parameter penalized) conditional likelihood, classification rate, or a recently proposed probabilistic margin formulation (Guo et al., 2005; Pernkopf et al., 2012). Using a discriminative criterion typically leads to classifiers with higher accuracy, especially, when the selected model class does not capture the underlying data distribution. However, since these discriminative BN scores are not decomposable, the discussed methods for exact generative structure learning cannot be directly applied.\nIn this paper, we propose an exact method for learning a BN structure maximizing the probabilistic margin. For this purpose, we use concepts developed in (de Campos et al., 2009; Jaakkola et al., 2010; Cussens, 2011), leading to a formulation as mixed integer linear program (MILP). For solving a MILP, the problem is relaxed in a linear program (LP), and a B&B method is used to enforce integrality. Therefore, our work falls within the line of research using B&B for exact structure learning, but maximizing a discriminative criterion. Similar as in (Cussens, 2010), we use a set of order constraints to enforce acyclicity in the directed graph, rather than the cluster constraints used in (Jaakkola et al., 2010). The advantage of this formulation is, that for N RVs, we only require N2 −N linear constraints for enforcing acyclicity, rather than super-exponentially many cluster constraints. Consequently, we are able to compactly represent our problem and to use powerful general-purpose solvers for structure learning. Although we learn a discriminative BN structure in order to obtain good classifiers, we still use maximum likelihood parameters. Therefore, the resulting BN consistently approximates the true underlying distribution and is suitable for all kinds of inference scenarios.\nSimilarly as in (Guo et al., 2005), the margin formulation needs one linear constraint per training sample and per competing class. This can render the approach infeasible for problems with many training samples and many class values. Therefore, as a second contribution, we propose a binary margin formulation, which can be interpreted as a local (sample-wise) oneversus-all classification scheme. The problem size using the binary margin does not depend on the number of classes, which is computationally beneficial for problems with many classes. For binary classification problems, the two margin formulations are equivalent. For multi-class problems, we empirically shown that the binary margin classifier competes with the original max-margin structure. We perform classification experiments on 31 datasets, and compare our algorithms with naive Bayes, tree-augmented naive Bayes, generative learned BNs and support vector machines (SVMs). The max-margin structures outperform the other BN classifiers on most datasets, and compete with support vector machines.\nThe paper is organized as follows. In section 2, we review BNs and introduce our notation, and in section 3, we review related work. We present our method for max-margin structure learning in section 4. The binary margin formulation is introduced in 5. In section 6 we present our experiments and section 7 concludes the paper."
    }, {
      "heading" : "2. Background and Notation",
      "text" : "Throughout the paper we assume discrete RVs, where plain capital letters denote single RVs and capital boldface letters represent sets of RVs. The set of states which can be assumed by RV X is denoted as val(X), and we define sp(X) = |val(X)|. For simplicity of notation, we identify the states of an RV X with natural numbers, i.e. val(X) = {1, . . . , sp(X)}. However, we do not assume a particular ordering or interpretation of these states. Furthermore, we use val(X) to denote the set of possible joint states of a set of RVs X, and let sp(X) = |val(X)|. Lower-case plain letters represent values or states of RVs, e.g. x is a value of RV X. Similarly, lower-case boldface letter represent joint states of variable sets, e.g. x is a state of RV set X. When y is a state of Y, and X is a subset of Y, then y(X) denotes the corresponding state of X.\nA BN B over a set of N RVs X = {X1, . . . , XN} is defined as a tuple 〈G,Θ〉, where G is a DAG, with nodes corresponding to the RVs in X. The set of parents of Xi according to G is denoted as Pai. The set Θ = {θ1, . . . , θN} contains parameter sets θi = {θij|h,∀j ∈ val(Xi),∀h ∈ val(Pai)} for each variable Xi, parameterizing a conditional probability distribution: P (Xi = j|Pai = h; θi) = θij|h. A BN defines a probability distribution over the RVs, according to\nPB(X = x) = N∏ i=1 sp(Xi)∏ j=1 ∏ h∈val(Pai) θij|h νij|h , (1)\nwhere νij|h is the indicator function 1(xi = j and x(Pai) = h). For classification, one of the variables in X represents the class variable C, where without loss of generality, we assume that X1 = C and let Z = {X2, . . . , XN}. Let D = {x1, . . .xM} be a collection of M i.i.d. samples drawn from some unknown distribution. It is well known that for a fixed BN structure G, the maximum likelihood (ML) parameters Θ̂ are given as\nθ̂ij|h = nij|h\nnih , (2)\nwhere nij|h = ∑M m=1 ν i,m j|h , n i h = ∑sp(Xi) j=1 n i j|h, and νi,mj|h = 1(x m i = j and x m (Pai) = h). We can regularize these parameters using Laplace-smoothing, by replacing nij|h with n i j|h + 1. The smoothed parameters are also consistent ML estimators, although biased towards a uniform distribution."
    }, {
      "heading" : "3. Related Work",
      "text" : "We adopt the framework developed in (Jaakkola et al., 2010), where the aim was to maximize a generative score such as MDL/BIC or BDe. For each variable Xi, we identify a set of possible parent-sets Si = {Si,1, . . .Si,Qi}, where Qi = |Si| and each Si,j ⊆ X \\ Xi, j ∈ {1, . . . , Qi}. A specific network structure G is represented by selecting a single parent-set from each Si, i ∈ {1, . . . , N}. The sets Si have to be reasonable large to represent a variety of solutions, while being reasonable small, such that the algorithm remains tractable. In (de Campos et al., 2009), a pruning strategy was presented, for a-priori excluding all parent-sets, which can not occur in an optimal DAG. Since not every combination of parent-sets yields an acyclic graph, additional acyclicity constraints have to be imposed. More formally, let ωi = (ωi,1, . . . , ωi,Qi)\nT be a vector of precomputed local scores, where ωi,j is the local score for Si,j . Furthermore, let ηi = (ηi,1, . . . , ηi,Qi)\nT be the parent-set indicator vector, which contains exactly one 1, indicating the selected parent-set in Si, and which is 0 elsewhere. All ηi are stacked into a single vector η = (ηT1 , . . . ,η T N ) T , and similarly ω = (ωT1 , . . . ,ω T N )\nT . Let P be the convex hull of all vectors η which represent valid DAGs. Consequently, all vertices of P represent DAGs, and as easily shown, all vectors η representing cyclic graphs are not elements of P. Generative structure learning is cast as the LP\nmaximize ωTη s.t. η ∈ P. (3)\nSince there is always an optimal solution in some vertex of P, and since each vertex represents a DAG, we are in principle able to recover an optimal structure by solving (3). Note that P has super-exponentially many facets in the number of variables, which, in agreement with theory (Chickering, 1996), makes the problem hard. Unfortunately, no representation of P via linear inequalities is known. Therefore, in (Jaakkola et al., 2010; Cussens, 2011) the constraint in problem (3) was replaced with\nηi,j ≥ 0 i ∈ {1, . . . , N}, j ∈ {1, . . . , Qi} (4) Qi∑ j=1 ηi,j = 1 i ∈ {1, . . . , N} (5)\nηi,j ∈ Z i ∈ {1, . . . , N}, j ∈ {1, . . . , Qi} (6)∑ i:Xi∈C Qi∑ j=1 ηi,j1(Si,j ∩C = ∅) ≥ 1 ∀C ⊆ X (7)\nThe constraints (4), (5) and (7) were used as approximation for P. However, since the solution of the LP\nmight be fractional, the integrality constraint (6) is required. The constraints (4)-(6) can be interpreted as the constraint “η represents a directed graph”, since they enforce that exactly one entry in each ηi is 1, and all others are 0. Constraints (7) enforce acyclicity, since they enforce that for each cluster C ⊆ X, there is at least one variable Xi ∈ C whose parent-set is either outside C, or which is empty. Thus, constraints (4)- (7) force η to represent a DAG. Note that the problem has super-exponentially many constraints. Jaakkola et al. (2010) solve the relaxed problem in the dual, where each in-active cluster constraint corresponds to a zero dual variable, and Cussens (2011) uses a cutting plane approach, iteratively adding violated cluster constraints.\nAs already noted, it is the decomposability of generative scores which yields the linear objective ωTη in (3), leading to the LP formulation. Since discriminative scores are usually not decomposable, the LP approach cannot be directly applied. However, in the next section we derive an exact MILP formulation for the so-called probabilistic soft margin. The margin δm of the mth sample is defined as (Guo et al., 2005):\nδm = PB(c m|zm) max c 6=cm PB(c|zm) = PB(c m, zm) max c6=cm PB(c, zm) . (8)\nWhen δm > 1, then the mth sample is correctly classified, and when δm < 1, it is wrongly classified. Motivated by SVMs, Pernkopf et al. (2012) defined a soft margin (SM) using the hinge loss:\nSM(B) = M∑ m=1 min(log δm, γ). (9)\nThe log-margin of each sample contributes linearly to the overall score. To avoid that the score is mainly determined by a few samples with overly large margin, the sample margins are limited with the hinge function min(·, γ). The parameter γ, which is obtained by cross validation, has the interpretation as “desired log-margin” for each sample. In (Pernkopf et al., 2012) this score was used for parameter learning using a conjugate gradient method, and in (Pernkopf et al., 2011; Pernkopf & Wohlmayr, 2012) the same score was used for inexact BN structure learning, based on greedy hillclimbing and simulated annealing."
    }, {
      "heading" : "4. Max-Margin Structure Learning",
      "text" : "We aim to find a BN structure G globally maximizing the SM score in (9). First, we restrict the maximal number of parents for each variable, to obtain a tractable number of parent-sets. For a variable\nXi 6= C, we only need to consider the empty parent set, and parent-sets containing C, since all other parent-sets do not influence the margin.1 We further assume Laplace-smoothed ML parameters while learning the structure. Firstly because simultaneous learning of max-margin structure and parameters would render our approach intractable. Secondly, by using generative parameters, the resulting BN can still be interpreted as generative model, although its structure is determined discriminatively.\nUsing the notation introduced in section 3, we can expand the BN distribution (1) according to\nPB(x) = N∏ i=1 sp(Xi)∏ j=1 Qi∏ k=1 ∏ h∈val(Si,k) θi,kj|h νi,m,k j|h ηi,k , (10)\nwhere θi,kj|h are the ML parameters when Si,k is the parent-set of variable Xi, and νi,m,kj|h = 1(x m i = j and x m (Si,k) = h). Clearly, (10) represents the same distribution as (1), where the structure G is explicitly encoded with η. Inserting (10) in the margin definition (8) and taking the log, gives\nlog δm = N∑ i=1 sp(Xi)∑ j=1 Qi∑ k=1 ∑ h νi,m,kj|h ηi,k log θ i,k j|h−\nmax c6=cm  N∑ i=1 sp(Xi)∑ j=1 Qi∑ k=1 ∑ h νi,m,k,cj|h ηi,k log θ i,k j|h  = min c6=cm [ N∑ i=1 Qi∑ k=1 ηi,k α(i, k,m, c) ] , (11)\nwhere νi,m,k,cj|h is defined as ν i,m,k j|h , but where the class value in xm is replaced with the value c. The coefficient α(i, k,m, c) is given as\nα(i, k,m, c) = sp(Xi)∑ j=1 ∑ h νi,m,kj|h log θ i,k j|h−ν i,m,k,c j|h log θ i,k j|h. (12) By defining a vector αm,c containing the coefficients α(i, k,m, c) corresponding with the entries of η, the log-margin in (11) can be written as a minimum over inner products:\nlog δm = min c6=cm αTm,cη. (13)\nUsing a standard technique from linear programming, we can express the SM score in (9) as follows. We\n1More precisely, for any Xi 6= C, it is equivalent to select the empty parent-set, or to select a parent-set not containing the class variable.\nintroduce a variable τm for each sample, together with the constraints\nτm ≤ αTm,cη, ∀m,∀c 6= cm, (14) τm ≤ γ, (15)\nand maximize ∑M m=1 τ m. In an optimal LP solu-\ntion we have τm = min(log δm, γ), and ∑M m=1 τ\nm is precisely the SM score. As in generative structure learning, the DAG constraint could in principle be addressed by constraints (4)-(7). However, in this paper we use a more convenient way to express acyclicity, allowing a compact MILP representation of our problem. Therefore, we replace the cluster constraints (7) with alternative order constraints, enforcing a topological ordering among the nodes and thus acyclicity of the resulting graph. We introduce a real-valued order variable oi for each variable Xi, which is constrained to 0 ≤ oi ≤ ∆, where ∆ is some arbitrary positive number. The order constraints are:\n(1− ai,j) 2∆ + oj − oi ≥ ∆\nN (16)\n∀i, j ∈ {1, . . . , N}, i 6= j,\nwhere ai,j = ∑Qi k=1 ηj,k 1(Xi ∈ Sj,k). The following proposition shows that these constraints enforce acyclicity.\nProposition 1. A vector η represents a DAG if and only if there exist some oi, i ∈ {1, . . . , N}, with 0 ≤ oi ≤ ∆, for some arbitrary ∆ > 0, such that constraints (16) and (4)-(6) are fulfilled.\nProof. First we show that when the conditions in the proposition hold, then η is necessarily a DAG. Constraints (4)-(6) enforce that η represents some directed graph G, since in this case each ηi contains exactly one 1 and is 0 elsewhere. It follows that also ai,j is either 1 or 0, and equals an entry of the adjacency matrix of G, indicating an edge from Xi to Xj . We switch cases: When ai,j = 0, i.e. when there is no edge Xi → Xj , then (16) yields\n2∆ + oj − oi ≥ 2∆ + 0−∆ = ∆ ≥ ∆\nN , (17)\nand the constraint is fulfilled regardless of oi, oj . When ai,j = 1, i.e. when there is an edge Xi → Xj , then we have\noj − oi ≥ ∆\nN , (18)\nwhich implies that oj is strictly larger than oi. By sorting all oi, we obtain an ordering among the variables (among several oi with the same value, we pick an arbitrary ordering). Since there can not be an edge\nXi → Xj when Xi comes after Xj , this ordering is a topological ordering, and thus the resulting directed graph is acyclic.\nIt remains to show that when η represents a DAG, then the conditions in proposition 1 hold. When η represents a DAG, then it fulfills constraints (4)-(6), since it is a directed graph. Furthermore, we can obtain some topological ordering from the DAG. Let ōi be the index of Xi in this ordering. Setting oi = (ōi − 1) ∆N fulfills constraints (16) and 0 ≤ oi ≤ ∆.\nIn contrast to the super-exponentially many cluster constraints in (7), we only need N2 − N linear order constraints and N additional real-valued order variables to enforce acyclicity. Thus, the resulting problem has a more compact representation and can be solved by general-purpose MILP solvers. Similar constraints, using the same mechanism as depicted here, have been proposed for maximum-likelihood pedigree learning (Cussens, 2010). To summarize, our MILP formulation for finding a DAG maximizing the SM in (9), is given as:\nmax. ∑M m=1 τ m s.t. τm ≤ αTm,cη, ∀m,∀c 6= cm τm ≤ γ ∀m ηi,j ≥ 0 ∀i, j∑Qi j=1 ηi,j = 1 ∀i\nηi,j ∈ Z ∀i, j (1− ai,j) 2∆ + oj − oi ≥ ∆N ∀i, j, i 6= j\n(19) Note that this formulation can immediately be applied to decomposable (i.e. practically all generative) scores. We simply remove the first two constraints (the margin constraints), and replace the objective with the objective of problem (3). Before we present experimental results in section 6, we introduce an alternative margin formulation, leading to a simpler problem."
    }, {
      "heading" : "5. Binary Margin Formulation",
      "text" : "The main limitation of problem (19) is that we have a constraint τm ≤ αTm,cη for each sample and each competing class, i.e. in total M(sp(C)− 1) linear constraints for specifying the margin.2 One approach would be to reduce the number of samples M , by using a representative sub-set of D. Similar techniques have been proposed for clustering and mixture model training, and we plan to address this point in future work. Here, we propose a formulation which only needs M constraints rather than M(sp(C)−1), which alleviates\n2The constraint τm ≤ γ is a simple variable bound and can be treated rather easily.\nespecially problems with many class values. The basic idea is to employ a one-versus-all classification scheme. When we desire to train a max-margin BN for classifying class c versus all other classes, we replace the parameters used in (10)-(12) with\nθi,kj|h(c) = ni,kj|h(c)\nni,kh (c) , (20)\nwhere\nni,kj|h(c) = M∑ m=1 νi,m,kj|h (c), n i,k h (c) = sp(Xi)∑ j=1 ni,kj|h(c)\nνi,m,kj|h (c) =\n{ νi,m,k,1j|h if c m = c\nνi,m,k,2j|h otherwise.\nIn words, θi,kj|h(c) are those ML parameters when we re-interpret class value c as class 1 and all other class values as class 2. We could now train a one-versusall classifier for each c ∈ {1, . . . , sp(C)} and combine their decisions for the multi-class problem. However, it is generally unclear how to correctly combine the decisions of one-versus-all classifiers. Furthermore, we now would have to train sp(C) different classifiers instead of a single one, although each problem would have only M instead of (sp(C) − 1)M margin constraints. Instead, we use the following local, i.e. sample-wise, oneversus-all scheme. Let Θ(c) be the collection of the parameters according to (20), for all c ∈ {1, . . . , sp(C)}. Let PB,Θ(c)(X) be the BN distribution with parameters Θ(c) and PB,Θ(X) the BN distribution with original parameters Θ. We define the binary margin δ̄m of the mth sample as\nδ̄m = PB,Θ(cm)(1, z\nm)\nPB,Θ(cm)(2, zm) . (21)\nClearly, PB,Θ(c m, zm) = PB,Θ(cm)(1, z m), since we used exactly the same statistics for class c in the original parameterization Θ, and for class 1 in the alternative parameterization Θ(cm). When sp(C) = 2, then Θ(1) and Θ(2) are simply redundant versions of Θ. In this case we have PB,Θ(C 6= cm, zm) = PB,Θ(cm)(2, zm), and δ̄m = δm, i.e. the margin and the binary margin are equivalent for binary classification problems. For sp(C) > 2, PB,Θ(cm)(2, z\nm) is an approximation for PB,Θ(C 6= cm, zm) = ∑ c′ 6=cm PB,Θ(c\n′, zm), assuming that PB,Θ(cm)(z m) ≈ PB,Θ(zm).\nNote that the additional parameters can be stored efficiently, since the parameters for class 1 (for each Θ(c)) are already stored in the original parameter set Θ. Therefore, we need only twice as much memory for\nstoring Θ and Θ(c), c ∈ {1, . . . , sp(C)}, than for storing Θ alone. Following a similar derivation as for (13), we obtain\nlog δ̄m = ᾱTmη, (22)\nwhere ᾱm contains the coefficients (cf. (12))\nā(i, k,m) = sp(Xi)∑ j=1 ∑ h νi,m,k,1j|h log θ i,k j|h(c m)−\nνi,m,k,2j|h log θ i,k j|h(c m). (23)\nSimilar as in (9), we define a soft binary margin SBM(B) = ∑M m=1 min(log δ̄\nm, γ). The MILP for finding a DAG maximizing the SBM is defined as in (19), except that the constraints\nτm ≤ αTm,cη, ∀m, ∀c 6= cm (24)\nare replaced with\nτm ≤ ᾱTmη, ∀m. (25)\nThe alternative parameters Θ(c) are only needed to obtain the coefficients ā(i, k,m). For the final BN classifier, we use the original parameters Θ."
    }, {
      "heading" : "6. Experiments",
      "text" : "We performed classification experiments on 31 dataset obtained from the UCI machine learning repository (Frank & Asuncion, 2010). We used the 25 datasets already used in (Friedman et al., 1997), plus six additional datasets: “abalone”, “adult”, “car”, “mushroom”, “nursery”, and “spambase”. These datasets have between 4 and 57 input features and contain between 80 and 45222 samples. For a more detailed information we refer the reader to (Frank & Asuncion, 2010). To estimate the accuracy of the classifiers, a test set was used for the datasets “chess”, “letter”, “mofn-3-7-10”, “satimage”, “segment”, “shuttle-small”, “waveform-21”, and the six additional datasets. For the remaining datasets, 5- fold cross-validation was used to estimate the accuracy. Samples with missing features were removed beforehand, and continuous features were discretized using the method described in (Fayyad & Irani, 1993). We compared our methods with naive Bayes (NB), the tree-augmented naive Bayes (TAN) (Friedman et al., 1997) and with a BN with generatively trained structure, using MDL as score function (Suzuki, 1993). Furthermore, we compared with SVMs using a Gaussian kernel, using the LIB-SVM implementation (Chang & Lin, 2011). For the SVM parameters σ (width of Gaussian kernel) and C (tradeoff factor), we validated all combinations of σ ∈\n{2−5, 2−4, . . . , 25} and C ∈ {2−3, 2−2, . . . , 25}. For our methods, BN structure learning using the SM and SBM, we validated the parameter γ (desired\nlog margin), where we used γ = log (\np 1−p\n) , with\np ∈ {0.501, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.999}. Additionally, we validated the maximal number of parents, where we used 1 or 2 parents per node. When the training set was sufficiently large (> 1000 samples) we used 20% of the training samples as validation set. Otherwise, we used 5-fold cross validation. In all cases, we used the same validation set/cross-folds for SVM training and for our algorithms.\nFor solving MILPs we used the ILOG CPLEX optimizer.3 For each optimization problem we set a time limit of 2 hours, i.e. if after 2 hours an optimization had not finished, we stopped it and used the best solution found so far. When maximizing the SM, for most datasets an optimal solution was found within these two hours, except for “letter”, “satimage”, “segment”, “soybean-large”, “vehicle”, “adult”, and “spambase”. For the datasets “letter” and “soybean-large” the resulting MILPs were too large to return a reasonable solution at all: only the trivial unconnected DAG, found by an internal CPLEX heuristic was returned. However, when maximizing the SBM, a reasonable solution was found in any case. Table 1 shows the worst-case sub-optimality bounds for the “problematic” datasets, according to 100 z̄−zz̄ %, where z̄ is the B&B upper bound of the margin score, and z is the objective of the best feasible solution. For the generative BNs using the MDL score, we used our formulation (19) without margin constraints and used the linear MDL objective (cf. (3)). We used the same set of parent-sets as for max-margin training, and also cross-validated the number of parents per node (1 or 2 parents). For all datasets an optimal solution was found, where the optimization time was typically under 1 second. The longest optimization time of 716 seconds was needed for “spambase” (58 variables).\nThe classification results for all datasets are shown in table 2, where the estimated accuracy, together with the 95% confidence intervals is shown (Mitchell, 1997). Table 3 summarizes these results, where the plain and boldface numbers are the number of times a classifier outperforms an other classifier with a significance level of 68% and 95%, respectively. For the significance tests, we used a one-sided paired t-test for the datasets with cross-validation, and a one-sided binomial test for the other datasets. We see that the maxmargin structures SM and SBM perform clearly better\n3ILOG CPLEX is freely available for non-commercial research under http://www.ibm.com/\nthan NB, TAN and MDL, since a max-margin structure outperforms the other BNs at least 17 times (11 times significantly), while NB, TAN and MDL outperform a max-margin structure maximal 7 times (2 times significantly). On the other hand, the SVM outperforms the max-margin structures at least 11 times (6 times significantly), while a max-margin structure outperforms the SVM maximal 9 times (3 times significantly). Therefore, there is a trend in favor of the SVM, although the results for the max-margin structures are in the same range. As already mentioned, the BNs with max-margin structure still use generative parameters. Therefore, the resulting models still consistently approximate the empirical data distribution and are amenable for other inference tasks than classification. Additionally, we could train the parameters of the resulting BNs in a discriminative way, to further improve classification results (Guo et al., 2005; Pernkopf et al., 2012). We plan to address this in future work."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We proposed an exact method for the combinatorial problem of finding a BN structure maximizing the probabilistic soft margin, extending previous methods for exact generative BN structure learning. We demonstrated the applicability of our methods on small and medium sized datasets and produced promising results. Having an exact algorithm is valuable – although the problem is NP-hard. Firstly, it is important to address those datasets were the problem\nturns out to be tractable. Secondly, a key feature of the methods presented in this paper is that they provide any-time solutions, i.e. when the problem turns out to be infeasible, they still return an approximation together with a worst-case estimate of sub-optimality. Therefore, these methods can also provide satisfying results on more difficult problems. Furthermore, exact methods provide interesting theoretical insights into the problem nature, and possibly motivate new heuristics and approximations for inexact structure learning."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the Austrian Science Fund (project number P22488-N23)."
    } ],
    "references" : [ {
      "title" : "Theory refinement on Bayesian networks",
      "author" : [ "W. Buntine" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Buntine,? \\Q1991\\E",
      "shortCiteRegEx" : "Buntine",
      "year" : 1991
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chang", "C.-C", "Lin", "C.-J" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning Bayesian networks is NPcomplete",
      "author" : [ "D.M. Chickering" ],
      "venue" : null,
      "citeRegEx" : "Chickering,? \\Q1996\\E",
      "shortCiteRegEx" : "Chickering",
      "year" : 1996
    }, {
      "title" : "A Bayesian method for the induction of probabilistic networks from data",
      "author" : [ "G.F. Cooper", "E. Herskovits" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cooper and Herskovits,? \\Q1992\\E",
      "shortCiteRegEx" : "Cooper and Herskovits",
      "year" : 1992
    }, {
      "title" : "Maximum likelihood pedigree reconstruction using integer programming",
      "author" : [ "J. Cussens" ],
      "venue" : "In Proceedings of WCB-10,",
      "citeRegEx" : "Cussens,? \\Q2010\\E",
      "shortCiteRegEx" : "Cussens",
      "year" : 2010
    }, {
      "title" : "Bayesian network learning with cutting planes",
      "author" : [ "J. Cussens" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Cussens,? \\Q2011\\E",
      "shortCiteRegEx" : "Cussens",
      "year" : 2011
    }, {
      "title" : "Efficient structure learning of Bayesian networks using constraints",
      "author" : [ "C.P. de Campos", "Q. Ji" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Campos and Ji,? \\Q2011\\E",
      "shortCiteRegEx" : "Campos and Ji",
      "year" : 2011
    }, {
      "title" : "Structure learning of Bayesian networks using constraints",
      "author" : [ "C.P. de Campos", "Z. Zeng", "Q. Ji" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Campos et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Campos et al\\.",
      "year" : 2009
    }, {
      "title" : "Multi-interval discretization of continuous-valued attributes for classification learning",
      "author" : [ "U.M. Fayyad", "K.B. Irani" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Fayyad and Irani,? \\Q1993\\E",
      "shortCiteRegEx" : "Fayyad and Irani",
      "year" : 1993
    }, {
      "title" : "Bayesian network classifiers",
      "author" : [ "N. Friedman", "D. Geiger", "M. Goldszmidt" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Friedman et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1997
    }, {
      "title" : "Maximum margin Bayesian networks",
      "author" : [ "Y. Guo", "D. Wilkinson", "D. Schuurmans" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Guo et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning Bayesian networks: the combination of knowledge and statistical data",
      "author" : [ "D. Heckerman", "D. Geiger", "D.M. Chickering" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning Bayesian network structure using LP relaxations",
      "author" : [ "T. Jaakkola", "D. Sontag", "A. Globerson", "M. Meila" ],
      "venue" : "In AI Statistics,",
      "citeRegEx" : "Jaakkola et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaakkola et al\\.",
      "year" : 2010
    }, {
      "title" : "Exact Bayesian structure discovery in Bayesian networks",
      "author" : [ "M. Koivisto", "K. Sood" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Koivisto and Sood,? \\Q2004\\E",
      "shortCiteRegEx" : "Koivisto and Sood",
      "year" : 2004
    }, {
      "title" : "Probabilistic Graphical Models - Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedmann" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedmann,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedmann",
      "year" : 2009
    }, {
      "title" : "Exact structure discovery in Bayesian networks with less space",
      "author" : [ "P. Parviainen", "M. Koivisto" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Parviainen and Koivisto,? \\Q2009\\E",
      "shortCiteRegEx" : "Parviainen and Koivisto",
      "year" : 2009
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Stochastic marginbased structure learning of Bayesian network classifiers",
      "author" : [ "F. Pernkopf", "M. Wohlmayr" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Pernkopf and Wohlmayr,? \\Q2012\\E",
      "shortCiteRegEx" : "Pernkopf and Wohlmayr",
      "year" : 2012
    }, {
      "title" : "Maximum margin structure learning of Bayesian network classifiers",
      "author" : [ "F. Pernkopf", "M. Wohlmayr", "M. Mücke" ],
      "venue" : "In ICASSP, pp. 2076–2079,",
      "citeRegEx" : "Pernkopf et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pernkopf et al\\.",
      "year" : 2011
    }, {
      "title" : "Maximum margin Bayesian network classifiers",
      "author" : [ "F. Pernkopf", "M. Wohlmayr", "S. Tschiatschek" ],
      "venue" : "IEEE TPAMI,",
      "citeRegEx" : "Pernkopf et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Pernkopf et al\\.",
      "year" : 2012
    }, {
      "title" : "Modeling by shortest data",
      "author" : [ "J. Rissanen" ],
      "venue" : "description. Automatica,",
      "citeRegEx" : "Rissanen,? \\Q1978\\E",
      "shortCiteRegEx" : "Rissanen",
      "year" : 1978
    }, {
      "title" : "A simple approach for finding the globally optimal Bayesian network structure",
      "author" : [ "T. Silander", "P. Myllymäki" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Silander and Myllymäki,? \\Q2006\\E",
      "shortCiteRegEx" : "Silander and Myllymäki",
      "year" : 2006
    }, {
      "title" : "A construction of Bayesian networks from databases based on an MDL principle",
      "author" : [ "J. Suzuki" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Suzuki,? \\Q1993\\E",
      "shortCiteRegEx" : "Suzuki",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Bayesian networks (BNs) are an important type of probabilistic graphical models (Pearl, 1988; Koller & Friedmann, 2009) and specify a probability distribution over a set of random variables (RVs).",
      "startOffset" : 80,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "This is a combinatorial problem and known to be NP-hard in general (Chickering, 1996).",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "Alternatively to dynamic programming, branch-and-bound (B&B) techniques have been exploited for exact structure learning (de Campos et al., 2009; de Campos & Ji, 2011; Jaakkola et al., 2010).",
      "startOffset" : 121,
      "endOffset" : 190
    }, {
      "referenceID" : 20,
      "context" : "they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al.",
      "startOffset" : 55,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al.",
      "startOffset" : 55,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al., 1995).",
      "startOffset" : 93,
      "endOffset" : 159
    }, {
      "referenceID" : 11,
      "context" : "they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al., 1995).",
      "startOffset" : 93,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, when the learned BN shall be used as classifier, we aim to maximize a discriminative score, such as (parameter penalized) conditional likelihood, classification rate, or a recently proposed probabilistic margin formulation (Guo et al., 2005; Pernkopf et al., 2012).",
      "startOffset" : 242,
      "endOffset" : 283
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, when the learned BN shall be used as classifier, we aim to maximize a discriminative score, such as (parameter penalized) conditional likelihood, classification rate, or a recently proposed probabilistic margin formulation (Guo et al., 2005; Pernkopf et al., 2012).",
      "startOffset" : 242,
      "endOffset" : 283
    }, {
      "referenceID" : 12,
      "context" : "For this purpose, we use concepts developed in (de Campos et al., 2009; Jaakkola et al., 2010; Cussens, 2011), leading to a formulation as mixed integer linear program (MILP).",
      "startOffset" : 47,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "For this purpose, we use concepts developed in (de Campos et al., 2009; Jaakkola et al., 2010; Cussens, 2011), leading to a formulation as mixed integer linear program (MILP).",
      "startOffset" : 47,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "Similar as in (Cussens, 2010), we use a set of order constraints to enforce acyclicity in the directed graph, rather than the cluster constraints used in (Jaakkola et al.",
      "startOffset" : 14,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "Similar as in (Cussens, 2010), we use a set of order constraints to enforce acyclicity in the directed graph, rather than the cluster constraints used in (Jaakkola et al., 2010).",
      "startOffset" : 154,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "Similarly as in (Guo et al., 2005), the margin formulation needs one linear constraint per training sample and per competing class.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "We adopt the framework developed in (Jaakkola et al., 2010), where the aim was to maximize a generative score such as MDL/BIC or BDe.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "Note that P has super-exponentially many facets in the number of variables, which, in agreement with theory (Chickering, 1996), makes the problem hard.",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "Therefore, in (Jaakkola et al., 2010; Cussens, 2011) the constraint in problem (3) was replaced with",
      "startOffset" : 14,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "Therefore, in (Jaakkola et al., 2010; Cussens, 2011) the constraint in problem (3) was replaced with",
      "startOffset" : 14,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "Jaakkola et al. (2010) solve the relaxed problem in the dual, where each in-active cluster constraint corresponds to a zero dual variable, and Cussens (2011) uses a cutting plane approach, iteratively adding violated cluster constraints.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "(2010) solve the relaxed problem in the dual, where each in-active cluster constraint corresponds to a zero dual variable, and Cussens (2011) uses a cutting plane approach, iteratively adding violated cluster constraints.",
      "startOffset" : 127,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : "The margin δ of the m sample is defined as (Guo et al., 2005):",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "Motivated by SVMs, Pernkopf et al. (2012) defined a soft margin (SM) using the hinge loss:",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : "In (Pernkopf et al., 2012) this score was used for parameter learning using a conjugate gradient method, and in (Pernkopf et al.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : ", 2012) this score was used for parameter learning using a conjugate gradient method, and in (Pernkopf et al., 2011; Pernkopf & Wohlmayr, 2012) the same score was used for inexact BN structure learning, based on greedy hillclimbing and simulated annealing.",
      "startOffset" : 93,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "Similar constraints, using the same mechanism as depicted here, have been proposed for maximum-likelihood pedigree learning (Cussens, 2010).",
      "startOffset" : 124,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : "We used the 25 datasets already used in (Friedman et al., 1997), plus six additional datasets: “abalone”, “adult”, “car”, “mushroom”, “nursery”, and “spambase”.",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "We compared our methods with naive Bayes (NB), the tree-augmented naive Bayes (TAN) (Friedman et al., 1997) and with a BN with generatively trained structure, using MDL as score function (Suzuki, 1993).",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 22,
      "context" : ", 1997) and with a BN with generatively trained structure, using MDL as score function (Suzuki, 1993).",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Additionally, we could train the parameters of the resulting BNs in a discriminative way, to further improve classification results (Guo et al., 2005; Pernkopf et al., 2012).",
      "startOffset" : 132,
      "endOffset" : 173
    }, {
      "referenceID" : 19,
      "context" : "Additionally, we could train the parameters of the resulting BNs in a discriminative way, to further improve classification results (Guo et al., 2005; Pernkopf et al., 2012).",
      "startOffset" : 132,
      "endOffset" : 173
    } ],
    "year" : 2012,
    "abstractText" : "Recently, there has been much interest in finding globally optimal Bayesian network structures. These techniques were developed for generative scores and can not be directly extended to discriminative scores, as desired for classification. In this paper, we propose an exact method for finding network structures maximizing the probabilistic soft margin, a successfully applied discriminative score. Our method is based on branch-and-bound techniques within a linear programming framework and maintains an any-time solution, together with worstcase sub-optimality bounds. We apply a set of order constraints for enforcing the network structure to be acyclic, which allows a compact problem representation and the use of general-purpose optimization techniques. In classification experiments, our methods clearly outperform generatively trained network structures and compete with support vector machines.",
    "creator" : "LaTeX with hyperref package"
  }
}