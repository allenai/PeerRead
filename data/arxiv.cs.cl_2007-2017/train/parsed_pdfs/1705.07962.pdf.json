{
  "name" : "1705.07962.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "pix2code: Generating Code from a Graphical User Interface Screenshot",
    "authors" : [ "Tony Beltramelli" ],
    "emails" : [ "tony@uizard.io" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The process of implementing client-side software based on a Graphical User Interface (GUI) mockup created by a designer is the responsibility of developers. Implementing GUI code is, however, time-consuming and prevent developers from dedicating the majority of their time implementing the actual features and logic of the software they are building. Moreover, the computer languages used to implement such GUIs are specific to each target platform; thus resulting in tedious and repetitive work when the software being built is expected to run on multiple platforms using native technologies. In this paper, we describe a system that can automatically generate platform-specific computer code given a GUI screenshot as input. We extrapolate that a scaled version of our method could potentially end the need for manually-programmed GUIs.\nOur first contribution is pix2code, a novel approach based on Convolutional and Recurrent Neural Networks allowing the generation of computer code from a single GUI screenshot as input. Our model is able to generate computer code from the pixel values of the input image alone. That is, no engineered feature extraction pipeline is designed to pre-process the input data. Our experiments demonstrate the effectiveness of our method for generating computer code for various platforms (i.e. iOS and Android native mobile interfaces, and multi-platform web-based HTML/CSS interfaces) without the need for any change or specific tuning to the model. In fact, pix2code can be used as such to generate code written in different target languages simply by being trained on a different dataset. A video demonstrating our system is available online1.\nOur second contribution is the release of our synthesized datasets consisting of both GUI screenshots and associated source code for three different platforms. They will be made freely available2 upon publication of this paper to foster future research."
    }, {
      "heading" : "2 Related Work",
      "text" : "The automatic generation of programs using machine learning techniques is a relatively new field and program synthesis in a human-readable format have only been addressed very recently. A recent example is DeepCoder [2], a system able to generate computer programs by leveraging statistical\n1https://uizard.io/research#pix2code 2https://github.com/tonybeltramelli/pix2code\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 5.\n07 96\n2v 1\n[ cs\n.L G\n] 2\n2 M\nay 2\npredictions to augment traditional search techniques. In another work by Gaunt et al. [4], the generation of source code is enabled by learning the relationships between input-output examples via differentiable interpreters. Furthermore, Ling et al. [11] recently demonstrated program synthesis from a mixed natural language and structured program specification as input. It is important to note that most of these methods rely on Domain Specific Languages (DSLs); computer languages (e.g. markup languages, programming languages, modeling languages) that are designed for a specialized domain but are typically more restrictive than full-featured computer languages. Using DSLs thus limit the complexity of the programming language that needs to be modeled and thus reduce the size of the search space.\nAlthough the generation of computer programs is an active research field as suggested by these breakthroughs, code generation from visual inputs is still an unexplored research area. This paper is, to the best of our knowledge, the first work attempting to address this very problem. In order to exploit the graphical nature of our input, we can borrow methods from the computer vision literature. In fact, an important number of research [19, 18, 3, 9] have shown that deep neural networks are able to learn latent variables describing objects in an image and generate a variable-length textual description of the objects and their relationships. All these methods rely on two main components. First, a Convolutional Neural Network (CNN) transforming the raw input image into an intermediary learned representation. Second, a Recurrent Neural Network (RNN) performing language modeling on the textual description associated with the input picture. Combining both neural network architectures allows the generation of image captions with impressive results."
    }, {
      "heading" : "3 pix2code",
      "text" : "The task of generating code given a GUI screenshot as input can be compared to the task of generating English textual descriptions given a scene photography as input. We can thus divide our problem into three sub-problems. First, a computer vision problem of understanding the given scene (i.e. in this case, the GUI screenshot image) and inferring the objects present, their identities, positions, and poses (i.e. buttons, labels, element containers). Second, a language modeling problem of understanding text (i.e. in this case, computer code) and generating syntactically and semantically correct samples. Finally, the last challenge is to use the solutions to both previous sub-problems by exploiting the latent variables inferred from scene understanding to generate corresponding textual descriptions (i.e. computer code rather than English text) of the objects represented by these variables."
    }, {
      "heading" : "3.1 Vision Model",
      "text" : "CNNs are currently the method of choice to solve a wide range of vision problems thanks to their topology allowing them to learn rich latent representations from the images they are trained on [14, 10]. We used a CNN to perform unsupervised feature learning by mapping an input image to a learned fixed-length vector; thus acting as an encoder as shown in Figure 1.\nThe input images are initially re-sized to 256× 256 pixels (the aspect ratio is not preserved) and the pixel values are normalized before to be fed in the CNN. No further pre-processing is performed. To encode each input image to a fixed-size output vector, we exclusively used small 3 × 3 receptive fields which are convolved with stride 1 as used by Simonyan and Zisserman for VGGNet [15]. These operations are applied twice before to down-sample with max-pooling. The width of the first convolutional layer is 32, followed by a layer of width 64, and finally width 128. Two fully connected layers of size 1024 applying the rectified linear unit activation complete the vision model."
    }, {
      "heading" : "3.2 Language Model",
      "text" : "We designed a simple DSL to describe GUIs as illustrated in Figure 2. In this work we are only interested in the GUI layout, the different graphical components, and their relationships; thus the actual textual value of the labels is ignored by our DSL. Additionally to reducing the size of the search space, the DSL simplicity also reduces the size of the vocabulary (i.e. the total number of tokens supported by the DSL). As a result, our language model can perform token-level language modeling with a discrete input from using one-hot encoded vectors; eliminating the need for word embedding techniques such as word2vec [12] that can result in costly computations.\nIn most programming languages and markup languages, an element is declared with an opening token; if children elements or instructions are contained within a block, a closing token is usually needed for the interpreter or the compiler. In such a scenario where the number of children elements contained in a parent element is variable, it is important to model long-term dependencies to be able to close a block that has been opened. Traditional RNN architectures suffer from vanishing and exploding gradients preventing them from being able to model such relationships between data points spread out in time series (i.e. in this case tokens spread out in a sequence). Hochreiter and Schmidhuber [8] proposed the Long Short-Term Memory (LSTM) neural architecture in order to address this very problem. The different LSTM gate outputs can be computed as follows:\nit = φ(Wixxt +Wiyht−1 + bi) (1) ft = φ(Wfxxt +Wfyht−1 + bf ) (2) ot = φ(Woxxt +Woyht−1 + bo) (3) ct = ft • ct−1 + it • σ(Wcxxt +Wcyht−1 + bc) (4) ht = ot • σ(ct) (5)\nWith W the matrices of weights, xt the new input vector at time t, ht−1 the previously produced output vector, ct−1 the previously produced cell state’s output, b the biases, and φ and σ the activation functions sigmoid and hyperbolic tangent, respectively. The cell state c learns to memorize information by using a recursive connection as done in traditional RNN cells. The input gate i is used to control the error flow on the inputs of cell state c to avoid input weight conflicts that occur in traditional RNN because the same weight has to be used for both storing certain inputs and ignoring others. The output gate o controls the error flow from the outputs of the cell state c to prevent output weight conflicts that happen in standard RNN because the same weight has to be used for both retrieving information and not retrieving others. The LSTM memory block can thus use i to decide\nwhen to write information in c and use o to decide when to read information from c. We used the LSTM variant proposed by Gers and Schmidhuber [5] with a forget gate f to reset memory and help the network model continuous sequences."
    }, {
      "heading" : "3.3 Combining Models",
      "text" : "Our model is trained in a supervised learning manner by feeding an image I and a sequence X of T tokens xt, t ∈ {0 . . . T − 1} as inputs; and the token xT as the target label. As shown on Figure 1, a CNN-based vision model encodes the input image I into a vectorial representation p. The input token xt is encoded by an LSTM-based language model into an intermediary representation qt allowing the model to focus more on certain tokens and less on others [7]. This first language model is implemented as a stack of two LSTM layers with 128 cells each. The vision-encoded vector p and the language-encoded vector qt are concatenated into a single vector rt which is then fed into a second LSTM-based model decoding the representations learned by both the vision model and the language model. The decoder thus learns to model the relationship between objects present in the input GUI image and the associated tokens present in the DSL code. Our decoder is implemented as a stack of two LSTM layers with 512 cells each. This architecture can be expressed mathematically as follows:\np = CNN(I) (6) qt = LSTM(xt) (7) rt = (q, pt) (8) yt = softmax(LSTM\n′(rt)) (9) xt+1 = yt (10)\nThis architecture allows the whole pix2code model to be optimized end-to-end with gradient descent to predict a token at a time after it has seen both the image as well as the preceding tokens in the sequence. The discrete nature of the output (i.e. fixed-sized vocabulary of tokens in the DSL) allows us to reduce the task to a classification problem. That is, the output layer of our model has the same number of cells as the vocabulary size; thus generating a probability distribution of the candidate tokens at each time step allowing the use of a softmax layer to perform multi-class classification."
    }, {
      "heading" : "3.4 Training",
      "text" : "The length T of the sequences used for training is important to model long-term dependencies; for example to be able to close a block of code that has been opened. After conducting empirical experiments, the DSL code input file used for training was segmented with a sliding window of size 48; in other words, we unroll the recurrent neural network for 48 steps. This was found to be a satisfactory trade-off between long-term dependencies learning and computational cost. For every token in the input DSL file, the model is therefore fed with both an input image and a sequence of T = 48 tokens. While the sequence of tokens used for training is updated at each time step (i.e. each token) by sliding the window, the very same input image I is reused for samples associated with the same GUI. The special tokens <START> and <END> are used to respectively prefix and suffix the DSL code files similarly to the method used by Karpathy et al. [9]. Training is performed by computing the partial derivatives of the loss with respect to the network weights calculated with backpropagation to minimize the multiclass log loss:\nL(I,X) = − T∑\nt=1\nxt+1 log(yt) (11)\nWith xt+1 the expected token, and yt the predicted token. The model is optimized end-to-end hence the loss L is minimized with regard to all the parameters including all layers in the CNN-based vision model and all layers in both LSTM-based models. Training with the RMSProp algorithm [17] gave the best results with a learning rate set to 1e − 4 and by clipping the output gradient to the range [−1.0, 1.0] to cope with numerical instability [7]. To prevent overfitting, a dropout regularization [16] set to 25% is applied to the vision model after each max-pooling operation and at 30% after each fully-connected layer. In the LSTM-based models, dropout is set to 10% and only applied to the non-recurrent connections [21]. Our model was trained with mini-batches of 64 image-sequence pairs."
    }, {
      "heading" : "3.5 Sampling",
      "text" : "To generate DSL code, we feed the GUI image I and a sequence X of T = 48 tokens where tokens xt . . . xT−1 are initially set empty and the last token of the sequence xT is set to the special < START > token. The predicted token yt is then used to update the next sequence of input tokens. That is, xt . . . xT−1 are set to xt+1 . . . xT (xt is thus discarded), with xT set to yt. The process is repeated until the token < END > is generated by the model. The generated DSL code can then be compiled with traditional compilation methods to the desired target language."
    }, {
      "heading" : "4 Experiments",
      "text" : "Access to consequent datasets is a typical bottleneck when training deep neural networks. To the best of our knowledge, no dataset consisting of both GUI screenshots and source code was available at the time this paper was written. As a consequence, we synthesized our own data resulting in the three datasets described in Table 1 that will be open-sourced. Our data synthesis algorithm is designed to synthesize GUIs written in our DSL which is then compiled to the desired target language to be rendered. Using data synthesis also allows us to demonstrate the capability of our model to generate computer code for three different platforms.\nOur model has around 109× 106 parameters to optimize and all experiments are performed with the same model with no specific tuning; only the training datasets differ as shown on Figure 3. Code generation is performed with both greedy search and beam search to find the tokens that maximize the classification probability. To evaluate the quality of the generated output, the classification error is computed for each sampled DSL token and averaged over the whole test dataset. The length difference between the generated and the expected token sequences is also counted as error. The results can be seen on Table 2.\nFigures 4, 5, and 6 show samples consisting of input GUIs (i.e. ground truth), and output GUIs generated by a trained pix2code model. It is important to remember that the actual textual value of the labels is ignored and that both our data synthesis algorithm and our DSL compiler assigns randomly generated text to the labels. Despite occasional problems to select the right color or the right style for specific GUI elements and some difficulties modelling GUIs consisting of long lists of graphical components, our model is generally able to learn the GUI layout in a satisfying manner and can preserve the hierarchical structure of the graphical elements."
    }, {
      "heading" : "5 Conclusion and Discussions",
      "text" : "In this paper, we presented pix2code, a novel method to generate computer code given a single GUI screenshot as input. While our work demonstrates the potential of such a system to automate GUI programming, we only scratched the surface of what is possible. Our model consists of relatively few parameters and was trained on a relatively small dataset. The quality of the generated code could be drastically improved by training a bigger model on significantly more data for an extended number of epochs. Experimenting with various regularization methods or implementing an attention mechanism [1] could further improve the quality of the generated code.\nUsing one-hot encoding as we did does not provide any useful information about the relationships between the tokens since the method simply assigns an arbitrary vector representation to each token. Therefore, pre-training the language model to learn vectorial representations of the tokens would allow the relationships between tokens in the DSL to be inferred (i.e. learning word embeddings such as word2vec [12]) and as a result alleviate semantical error in the generated code. Furthermore, one-hot encoding does not scale to very big vocabulary and thus restrict the number of symbols that the DSL can support.\nGenerative Adversarial Networks GANs [6] have shown to be extremely powerful at generating images and sequences [20, 13, 22]. Applying such techniques to the problem of generating computer code from an input image is so far an unexplored research area. GANs could potentially be used as a standalone method to generate code or could be used in combination with our pix2code model to fine-tune results. A major drawback of deep neural networks is the need for a lot of training data for the resulting model to generalize well on new unseen examples. One of the significant advantages of the method we described in this paper is that there is no need for human-labelled data. In fact, the network can model the relationships between graphical components and associated code by simply being\ntrained on image-sequence pairs. Although we used data synthesis in our paper partly to demonstrate the capability of our method to generate GUI code for various platforms; data synthesis might not be needed at all if one wants to focus only on web-based GUIs. In fact, one could imagine crawling the World Wide Web to collect a dataset of HTML/CSS code associated with screenshots of renderer GUIs. Considering a large number of websites already available online and the fact that new websites are created every day, the web could theoretically supply an unlimited amount of training data. We extrapolate that Deep Learning used in this manner could eventually end the need for manually-programmed GUIs."
    } ],
    "references" : [ {
      "title" : "COURSERA: Neural networks for machine learning",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Training with the RMSProp algorithm [17] gave the best results with a learning rate set to 1e − 4 and by clipping the output gradient to the range [−1.",
      "startOffset" : 36,
      "endOffset" : 40
    } ],
    "year" : 2017,
    "abstractText" : "Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites and mobile applications. In this paper, we show that Deep Learning techniques can be leveraged to automatically generate code given a graphical user interface screenshot as input. Our model is able to generate code targeting three different platforms (i.e. iOS, Android and web-based technologies) from a single input image with over 77% of accuracy.",
    "creator" : "LaTeX with hyperref package"
  }
}