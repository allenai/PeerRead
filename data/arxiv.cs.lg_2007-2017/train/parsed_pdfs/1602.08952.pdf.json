{
  "name" : "1602.08952.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Representation of linguistic form and function in recurrent neural networks",
    "authors" : [ "Ákos Kádár", "Grzegorz Chrupała", "Afra Alishahi" ],
    "emails" : [ "a.kadar@uvt.nl", "g.chrupala@uvt.nl", "a.alishahi@uvt.nl" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recurrent neural networks (RNNs) were introduced by Elman (1990) as a connectionist architecture with the ability to model the temporal dimension. They have proved popular for modeling language data as they learn representations of words and larger linguistic units directly from the input data, without feature engineering. Variations of the RNN architectures have been applied in several NLP domains such as parsing (Vinyals et al., 2015) and machine translation (Bahdanau et al., 2015), as well as in\ncomputer vision applications such as image generation (Gregor et al., 2015) and object segmentation (Visin et al., 2015). RNNs are also important components of systems integrating Vision and Language, e.g. image (Karpathy and Fei-Fei, 2015) and video captioning (Yu et al., 2015).\nThese networks can represent variable-length linguistic expressions by encoding them into a fixedsize low-dimensional vector. The nature and the role of the components of these representations are not directly interpretable as they are a complex, nonlinear function of the input. There have recently been numerous efforts to visualize deep models such as convolutional neural networks in the domain of computer vision, but much less so for variants of RNNs and for language processing.\nThe present paper develops novel methods for uncovering abstract linguistic knowledge, with a specific focus on analysing gated recurrent networks trained on tasks that require a reasonable level of natural language understanding. We do so through analyzing the hidden activation patterns rather than word embeddings, and explore the syntactic generalizations that models learns to capture.\nAs our case study we picked the IMAGINET model introduced by Chrupała et al. (2015). It is a multi-task, multi-modal architecture consisting of two Gated-Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014) pathways and a shared word embedding matrix. One of the GRUs (VISUAL) is trained to predict image vectors given image descriptions, while the other pathway (TEXTUAL) is a language model. This particular architecture allows a comparative analysis of the hidden activations patterns between networks trained on two very diffeent tasks, while keeping the training data and the word\nar X\niv :1\n60 2.\n08 95\n2v 1\n[ cs\n.C L\n] 2\n9 Fe\nb 20\nembeddings fixed. Recurrent neural language models akin to TEXTUAL which are trained to predict the next symbol in a sequence are relatively well understood, and there have been some attempts to analyze their internal states (e.g. Elman, 1991; Karpathy et al., 2015). In constrast, VISUAL maps a complete sequence of words to a representation of a corresponding visual scene and is a less commonly encountered, but a more interesting model from the point of view of representing meaning conveyed via linguistic structure.\nWe report a thorough macro level quantitative analysis to provide a linguistic interpretation of the networks’ activation patterns. Our experiments show that the image prediction pathway learns important aspects of the information structure of language, pays special attention to syntactic categories that carry semantic content, and treats the same input tokens differently depending on their grammatical functions in the sentence. In contrast, the language model is more sensitive to the local syntactic characteristics of the input sentences.\nWe also perform a micro level analysis to explore the function of individual hidden units by identifying the sentential contexts that yield the highest activation values, and provide a qualitative examination of these contexts. Our results show that some of the contexts represent a particular syntactic category or dependency function, while others correspond to a semantic theme. We identify cases where the hidden units encode characteristics of the context that go beyond their lexical properties, and represent abstract patterns that are useful for the network’s task. Furthermore, we explore and visualize units that carry their activation over to later time steps to extract longer dependencies and more complex linguistic features. We also show that features encoded by the language model are more associated with syntactic constructions than in case of the image prediction pathway."
    }, {
      "heading" : "2 Related work",
      "text" : "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to\nhidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems. In Elman (1991) he trains an RNN on a small synthetic sentence dataset and analyzes the activation patterns of the hidden layer. His analysis shows that these distributed representations encode lexical categories, grammatical relationships and hierarchical constituent structures. Giles et al. (1991) trains RNNs similar to Elman networks on strings generated by small deterministic regular grammars with the objective to recognize positive and reject negative strings, and develops the dynamic state partitioning technique to extract the learned grammar from the networks in the form of deterministic finite state automatons.\nMore closely related is the recent work of Li et al. (2015), who develops techniques for a deeper understanding of the activation patterns of RNNs, but focuses on models with modern architectures trained on large scale data sets. More specifically, they train Long Short-Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) for phrase level sentiment analysis and present novel methods to explore the inner workings of RNNs. They measure the salience of tokens in sentences by taking the first-order derivatives of the loss with respect to the word embeddings and provide evidence that LSTMs can learn to attend to important tokens in sentences. Furthermore, they plot the activation values of hidden units through time using heat maps and visualize local semantic compositionality in RNNs. In comparison, the present work focuses more on exploring structure learning in RNNs and on developing methods for a comparative analysis between RNNs.\nKarpathy et al. (2015) also takes up the challenge of rendering RNN activation patterns understandable, but uses character level language models and rather than taking a linguistic point of view, focuses on error analysis and training dynamics of LSTMs and GRUs. They show that certain dimensions in the RNN hidden activation vectors have specific and interpretable functions. One of the goals of the present paper is also to explores the specific functions encoded by individual hidden units, but in a linguistically-informed way.\nLi et al. (2016) train a Convolutional Neural Networks (CNN) with different random initializations on the ImageNet dataset (Krizhevsky et al., 2012).\nFor each layer in all networks they store the activation values produced on the validation set of ILSVRC and align similar neurons of different networks. They conclude that while some features are learned across networks, some seem to depend on the initialization. Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).\nIn general, there has been a growing interest within computer vision in understanding deep models, with a number of papers dedicated to visualizing learned CNN filters and pixel saliencies (Simonyan et al., 2014; Yosinski et al., 2015; Mahendran and Vedaldi, 2015a). These techniques have also led to improvements in model performance (Eigen et al., 2014) and transferability of features (Zhou et al., 2015). To date there has been much less work on such issues within computational linguistics. We aim to fill this gap by adapting existing methods as well as developing novel techniques to explore the linguistic structure learned by recurrent networks."
    }, {
      "heading" : "3 Models",
      "text" : "One of the main difficulties for training traditional Elman networks arises from the fact that they overwrite their hidden states at every time step with a new value computed from the current input xt and the previous hidden state ht−1. Similarly to LSTMs, Gated Recurrent Unit networks introduce a mechanism which facilitates the retention of information over multiple time steps. Specifically, the GRU computes the hidden state at current time step, ht, as the linear combination of previous activation ht−1, and a new candidate activation h̃t:\nGRU(ht−1,xt) = (1− zt) ht−1 + zt h̃t (1) where is elementwise multiplication, and the update gate activation zt determines the amount of new information mixed in the current state:\nzt = σs(Wzxt +Uzht−1) (2)\nThe candidate activation is computed as:\nh̃t = σ(Wxt +U(rt ht−1)) (3)\nThe reset gate rt determines how much of the current input xt is mixed in the previous state ht−1 to form the candidate activation:\nrt = σs(Wrxt +Urht−1) (4)"
    }, {
      "heading" : "3.1 Imaginet",
      "text" : "IMAGINET introduced in Chrupała et al. (2015) is a multi-modal GRU network architecture that learns visually grounded meaning representations from textual and visual input. It consists of two GRU pathways, TEXTUAL and VISUAL, with a shared word-embedding matrix. The inputs to the model are pairs of image descriptions and their corresponding images. Each sentence is mapped to two sequences of hidden states, one by VISUAL and the other by TEXTUAL:\nhVt = GRU V (hVt−1,xt) (5) hTt = GRU T (hTt−1,xt) (6)\nAt each time step TEXTUAL predicts the next word in the sentence S from its current hidden state hTt , while VISUAL predicts the image vector î from its last hidden representation hVt .\nî = VhVτ (7)\np(St+1|S1:t) = softmax(LhTt ) (8)\nThe loss function is a multi-task objective which penalizes error on the visual and the textual targets simultaneously. The objective combines crossentropy loss LT for the word predictions and cosine distance LV for the image predictions.1\nL = αLT + (1− α)LV (9)\nFor more details about the model and its performance see Chrupała et al. (2015)."
    }, {
      "heading" : "4 Experiments",
      "text" : "In the following sections, we report a series of experiments in which we develop methods to explore the kinds of linguistic regularities the TEXTUAL and\n1Our version of the model has two minor modifications compared the the original IMAGINET: we use cosine distance as the visual loss, and we use image vectors where each dimension is transformed by subtracting the mean and dividing by standard deviation.\nVISUAL pathways of IMAGINET learn from wordlevel input. Section 5 presents the macro-level analyses on the final hidden activation vectors of the recurrent pathways. Section 6 reports exploratory experiments on the linguistic features encoded by individual hidden units. For all the experiments, we trained IMAGINET on the training portion of the MSCOCO image-caption dataset (Lin et al., 2014), and analyzed the representations of the sentences in the validation set. The target image representations were extracted from the pre-softmax layer of the 16- layer CNN (Simonyan and Zisserman, 2015)."
    }, {
      "heading" : "5 Analysis of hidden activation vectors",
      "text" : "In this section we propose a novel technique for interpreting the activation patterns of RNNs from a linguistic point of view and focus on the high level understanding of what types of words the networks pay most attention to. The full sentences are represented by the activation vector at the end-ofsentence symbol (hend). We measure the salience of each word Si in an input sentence S1:n based on how much the representation of the partial sentence S\\i = S1:i−1Si+1:n, with the omitted word Si, deviates from that of the original sentence representation. For example, the distance between hend(the black dog is running) and hend(the dog is running) determines the importance of black in the first sentence. We introduce the measure omission(i, S) for estimating the salience of a word Si:\nomission(i, S) = cosine(hend(S),hend(S\\i))\nFigure 1 demonstrates the omission measure for the VISUAL and TEXTUAL pathways for two example captions. For both captions the omission scores are plotted along with the first image retrieved by VISUAL for the full sentence and for the sentence with the word with the highest omission(i, S) removed. In the first example, the omission scores for VISUAL suggest that the model interpreted pizza as the most important word in the sequence and returned an image that depicts a pizza on a plate. Removing the word pizza promotes salad and bowl as the main theme of the sentence and the model retrieves an image with a dining table with saladlike dishes. For the second example, the omission scores for VISUAL show that the model paid attention mostly to baby and bed and slightly to laptop\nand retrieved an image depicting a baby sitting on a bed with a laptop. Removing the word baby leads to an image that depicts an adult male laying on a bed. Figure 1 also shows that in contrast to VISUAL, in both examples TEXTUAL distributes its attention more evenly across time steps instead of focusing on the types of words related to the corresponding visual scene.\nThe next step is to use the omission score to not only estimate the importance of individual words, but also of syntactic categories. We estimate the salience of each syntactic category by accumulating the omission scores for all words in that category. We tag every word in a sentence with the part-ofspeech (POS) category and the dependency relation (deprel) label of its incoming arc. For example, for the sentence the black dog, we get (the, DT, det), (black, JJ, amod), (dog, NN, root). Both POS tagging and dependency parsing are performed jointly using the TurboParser dependency parser (Martins et al., 2013).2 The POS tags used are the Penn Treebank tags and the dependencies are the Stanford basic dependencies."
    }, {
      "heading" : "5.1 Omission score distributions",
      "text" : "Figure 2 shows the distribution of omission scores per POS and deprel label for the two pathways. The general trend is that for the VISUAL pathway, the omission scores are very high for a small subset of labels (corresponding to content words) and low for the rest. For TEXTUAL the differences are smaller, and the pathway seems to be sensitive to the omission of most types of words. Figure 3 compares the two pathways directly using the log of the ratio of the VISUAL to TEXTUAL omission scores, and plots the distribution of this ratio for different POS and deprel labels. Log ratios above zero indicate stronger association with the VISUAL pathway and below zero with the TEXTUAL pathway. We see that in relative terms, VISUAL is more sensitive to nouns (NNP, NNS, NN), numerals (CD) and adjectives (JJ), and TEXTUAL to the prepositions (TO, IN), some types of verbs (VBZ, VBP), determiners (DET, WDT) and particles (RP). This picture is complemented by the analysis of the relative importance of dependency relations: VISUAL pays most\n2Available at github.com/andre-martins/TurboParser.\nattention to the relations NSUBJPASS, NSUBJ, POBJ, ROOT, DOBJ, NN, CONJ, DEP, AMOD and NUM, whereas TEXTUAL is more sensitive to DET, MARK, AUX, AUXPASS, PRT, PREP, POSS, CC and COP . As expected, VISUAL is more focused on grammatical functions typically filled by semantically contentful words, while TEXTUAL attends relatively more to purely grammatical functions."
    }, {
      "heading" : "5.2 Sensitivity to grammatical function",
      "text" : "The omission score distributions in Section 5.1 suggest that the VISUAL pathway of IMAGINET is differentially sensitive to content vs. function words. In\nthis section we investigate to what extent VISUAL discriminates between occurrences of a given content word in different grammatical functions. In order to quantify this, we fit two linear models which predict the omission scores per token: MODEL 1 uses only the word type as a predictor, whereas MODEL 2 uses the word type, dependency label and the interaction term between the two. If MODEL 2 fits the data better than MODEL 1, then this indicates that for a given word type, the network is sensitive to its grammatical function, and thus that it learns to pay attention to aspects of sentence structure over and above purely lexical clues.\nWe split the whole MSCOCO validation set into two parts, fit the two models (regularized via L2 penalty) on the first part, and compute the proportion of variance explained on the second part. Figure 4 shows that MODEL 2 does indeed account for the omission scores better than the model with only word type as a predictor.\nIn order to find out some of the specific configurations leading to variability in omission scores, we next considered all word types with occurrence counts of at least 100 and ranked them according to how much better, on average, MODEL 2 predicted their omission scores compared to MODEL 1. Figure 5 shows the per-dependency score distributions for the five top ranked words plus the word water.\nThere are clear and large differences in how these words impact the network’s representation depending on what grammatical function they fulfill. They all have large omission scores when they occur as NSUBJ (nominal subject) or ROOT, likely due to the fact that these grammatical functions typically have a large contribution to the complete meaning of a sentence. Conversely, all have small omission scores when appearing as CONJ (conjunct): this is probably because in this position they share their contribution with the first, often more important, member of the conjunction (e.g. A cow and its baby eating grass). The pattern for NN (nominal modifier) is a bit more complicated: for four of the words shown (as well as for most other words not shown in the figure), the score is very low in this grammatical function–presumably because most words contribute less to the sentence meaning when used as modifiers than as heads (e.g. a clock tower). However, for the words zebra and water, omission scores are high when they act as a nominal modifier NN. This appears due to two reasons:\n1. For zebra: there are frequent erroneous parses such as zebra/NN browsing/ROOT instead of zebra/NSUBJ browsing/ROOT. The network does not make this mistake, and treats these occur-\nrences of zebra according to its importance as NSUBJ. 2. Water as a modifier often changes the meaning of its head in a visually salient way: e.g. water fall, water balloon, water scene, water skiing, and thus the network learns that this particular word is important in the modifier position."
    }, {
      "heading" : "5.3 Sensitivity to information structure",
      "text" : "In English the information structure (or pragmatic structure) of a sentence is expressed via linear ordering: the TOPIC of a sentence appears sentenceinitially, and the COMMENT follows. In this section we explore to what extent the recurrent pathways of IMAGINET learn this information ordering. Figure 6 shows the distribution of omission scores as a function of linear position in the sentence and POS label for both VISUAL and TEXTUAL.\nSince the omission scores are measured at the end-of-sentence token, the expectation is that for TEXTUAL, as a language model, the words appearing closer to the end of the sentence would have a stronger effect on the omission scores. This appears to be the case in Figure 6 (top), and is confirmed by the Pearson correlation coefficient of 0.24.\nFor the VISUAL model it is less clear what to expect: on the one hand due to the chain structure of RNNs, they are better at keeping track of shortdistance rather than long-distance dependencies. On the other hand, for the task of predicting features of the visual scene, it would likely be advantageous to detect the topic of the sentence and up-weight its importance in the final meaning representation.\nThe tendencies in Figure 6 (bottom) appear to support the latter hypothesis for content words. While the overall correlation coefficient is close to zero (- 0.05), for nouns (NN, NNS and NNP) it is negative (-0.17) suggesting that the network does learn that content appearing sentence-initially is likely the TOPIC and thus visually more salient."
    }, {
      "heading" : "6 Analysis of hidden units",
      "text" : ""
    }, {
      "heading" : "6.1 Top K contexts",
      "text" : "The aim of this section is to gain an understanding of what the individual hidden dimensions are encoding in general. We develop a simple method we dub top K contexts after the top K images of Zhou et al. (2015). We forwarded each sentence from a corpus token-by-token through the RNN, and registered the hidden state ht for each unit at time step t. This results in an activation matrix M ∈ Rd×n, where d is the number of hidden dimensions and n is the total number of time steps (or tokens) in the whole corpus. Each cell Mit in the resulting matrix represents\nthe activation value of the ith unit for some token at time step t. Making the assumption that high activation values indicate importance, we sort the rows of the activation matrix by the magnitude of the activations, leading to the top K contexts for each unit."
    }, {
      "heading" : "6.2 Specialized hidden units",
      "text" : "Table 1 shows the top 5 trigram contexts with the highest activations for five example hidden units for VISUAL and TEXTUAL. It reveals the general pattern that the individual dimensions become highly sensitive towards contexts with syntactically and/or topically related patterns. For example the top 20 trigram contexts for the first hidden unit of VISUAL in Table 1 all contain tokens topically related to home electronics, such as phones, remotes and camera parts. Top-20 5-gram context for this unit include: cell phone calculator and gum, all hanging on wires like, such as beads and cords. More interestingly, the first hidden unit for TEXTUAL in Table 1 seems to be highly active for a combined syntactic and semantic template: contexts including a token corresponding to a vehicle followed by a transportation verb. Exploring a larger context of 5-grams reveals other interesting units with high activations for such semantic/syntactic constructions in TEXTUAL, e.g. a dog pokes his head, white cat sticking his head, a dog sticking its head, a dog sticking his head, with a long tail perched."
    }, {
      "heading" : "6.3 Units predictive of a grammatical function",
      "text" : "To explore the syntactic functions encoded by specialized dimensions, we train two logistic regression models (one for VISUAL and one for TEXTUAL) to predict the dependency label of a token at time step t. The models use two sets of predictors: • hidden activation vectors hVt or hTt • n-gram features up to a window size of 4; for\nexample, to predict the label for dog in the sentence the nice dog we extract the2, nice1, dog0, the2 nice1, nice1 dog0, the2 nice1 dog0, etc.\nFor both VISUAL and TEXTUAL, we pinpoint the hidden units that are predictive of grammatical function by taking the top 5 logistic regression coefficients βV and βT per dependency label corresponding to the dimensions of hVt and h T t with the highest absolute value. Since the models include ngram predictors, the logistic regression model will only have coefficients with high absolute values for units that are predictive of dependency relation over and above the n-gram features, and thus which most likely represent some type of functional information.\nTable 2 shows the top four context representations for the hidden units corresponding to one of the top 5 highest coefficients for a number of the deprels for both VISUAL and TEXTUAL. Some of the units in Table 2 seem to offer solely lexical cues to the deprel prediction model, while others encode more general syntactic information. A number of units have high activations for target words typically fulfilling the\nsame grammatical function: • The example unit for the category POSS in case\nof TEXTUAL has top contexts with target words his, her and their. This is also true for VISUAL. • The example unit for NUM for TEXTUAL has\nhigh activations for both two and three. • The example units for CC given for VISUAL has\nhigh activations in the presence of target tokens and and or. • The contexts for the dimension of VISUAL with\nthe highest coefficient for AUX have both is and are as target tokens."
    }, {
      "heading" : "6.4 Units carrying over information",
      "text" : "Further examination of some of the highly activated units reveals some that are predictive of deprels that require information about the identities or grammatical functions of previous tokens. For example, predictive units for POBJ in the TEXTUAL pathway in Table 2 generalize over the prepositions at and in, and contexts in the top 20 include additional prepositions such as around a dirt and on a grass. But interestingly, rather than being active for the prepositions themselves, all top 20 contexts belong to the construction PREP DET POBJ where the object has to do with outdoors. For VISUAL, one of the top units for the category CONJ has top contexts with greenery and, the table and, colored circles and, wooden furniture and, furniture and a. Given that the value of this dimension predicts the presence of a conjunct at the current time step, this particular dimension seems to carry over its high activation value to the next time step, since all the 5 example trigrams require a conjunct in the next step.\nThis is also the case for the most predictive units of VISUAL for POBJ: the top contexts for this unit are several bunches of, with lots of, the end of, has trays of and in front of, suggesting that the information content of the token of must be carried over the next time step.\nTo visually explore the phenomenon of units carrying over information through time steps, we searched for interesting hidden units in VISUAL using their top-20 5-gram representations, and plotted their activation values through time for some example captions. We only used example sentences where the activation of the hidden unit was in the highest decile. Figure 7 shows the results. The first\ntwo rows are examples of lexicalized units recognizing topically related words and keeping them in memory until the end of the sequence. The next two rows demonstrate a hidden unit active for the multiword expressions next to a and next to an. The following three rows show a unit active for noun phrase constructions which contain a numeral followed by a reference to a person. The last three examples show a dimension that has a modest activation for tokens of category FOOD, but has a high activation for a following food item accompanying it in the visual scene. In the very last example, the unit has a modest activation for the token broccoli, then its activation decreases for on a. With the arrival of the token plate the activation increases again, has even higher activation for with and finally the highest for potatoes. The top 20 5-grams for this hidden unit all contain multiple food items, such as vegetables and meat with chopsticks."
    }, {
      "heading" : "6.5 Comparison of models based on top contexts",
      "text" : "The results in Section 5 highlight some of the differences between TEXTUAL and VISUAL. We saw\nthat VISUAL learns to pay relatively more attention to contentful words and TEXTUAL to words with purely grammatical function. Moreover, while for TEXTUAL tokens appearing near the end of the sentence are more salient in general, VISUAL learns to pay attention to sentence-initial nouns as they are likely the TOPIC of the sentence. In this section we explore a further comparison between the models based on the hypothesis that due to their different objectives, the activity of the hidden dimensions of VISUAL are more characterized by semantic relationships between contexts, whereas the dimensions of TEXTUAL are more focused on extracting syntactic patterns. In order to quantify this intuition, we measure the strength of association between activations of hidden units and either lexical (token n-grams) or structural (deprel n-grams) types of contexts.\nWe define Amj as a discrete random variable corresponding to a binned activation over time steps of model m at hidden dimension j, and C as a discrete random variable indicating the context (where type of context can be for example word trigram or deprel bigram). The strength of association between Amj and C can be measured by their mutual information:\nI(Amj ;C) = ∑\na∈Amj\n∑ c∈C p(a, c) log\n( p(a, c)\np(a)p(c)\n)\nSimilarly to Li et al. (2016), the activation value distributions are discretized into percentile bins per dimension, such that each bin contains 5% of the marginal density. For context types, we used dependency label and word uni-, bi- and trigrams. For simplicity we use the notation MImC to denote the median mutual information score over all units of the pathway m when considering context C.\nWe then compute log ratios log(MITC/MI V C ) for all six context types C. In order to quantify variability we bootstrap this statistic with 5000 replicates. Figure 8 shows the resulting bootstrap distributions for uni-, bi-, and trigram contexts, in the word and deprel conditions. The clear pattern is that the log ratios are much higher in case of deprels, with no overlaps between the bootstrap distitributions. Thus, in general, the size of the relative difference between TEXTUAL and VISUAL median mutual information score is much more pronounced for deprel context\ntypes. This suggests that features that are encoded by the hidden units of the models are indeed different, and that the features encoded by TEXTUAL are more associated with syntactic constructions than in case of VISUAL."
    }, {
      "heading" : "7 Conclusion",
      "text" : "The goal of our paper is to propose novel methods for the encoding of linguistic knowledge in RNNs trained on language tasks. Our analyses of the hidden activation patterns show that the VISUAL model learns an abstract representation of the information structure of the language, and pays selective attention to lexical categories and grammatical functions that carry semantic information. In contrast, the language model TEXTUAL is sensitive to categories of a more syntactic nature. We have also shown that each network contains specialized units which are tuned to both lexical and structural patterns that are useful for the task at hand, some of which can carry activations to later time steps to encode long-term dependencies. In future we would like to apply our techniques to analyze the encoding of linguistic form and function to recurrent neural models trained on different objectives, such as neural machine translation systems (e.g. Sutskever et al., 2014) or the purely distributional sentence embedding system of Kiros et al. (2015). A number of recurrent neural models rely on a so called attention mechanism, first introduced by Bahdanau et al. (2015) under the name of soft alignment. In these networks attention is explicitly represented and it would be interesting to see how our method of discovering implicit attention, the omission score, compares. Finally, one\nof the benefits of understanding how linguistic form and function is represented in RNNs is that it can provide insight into how to improve systems. We plan to draw on lessons learned from our analyses in order to develop models with better general-purpose sentence representations."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representation (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "Eighth Workshop on Syntax, Semantics and Structure in Sta-",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning language through pictures",
      "author" : [ "Grzegorz Chrupała", "Ákos Kádár", "Afra Alishahi." ],
      "venue" : "ACL.",
      "citeRegEx" : "Chrupała et al\\.,? 2015",
      "shortCiteRegEx" : "Chrupała et al\\.",
      "year" : 2015
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "NIPS 2014 Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Understanding deep image representations by inverting them",
      "author" : [ "Alexey Dosovitskiy", "Thomas Brox." ],
      "venue" : "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Dosovitskiy and Brox.,? 2015",
      "shortCiteRegEx" : "Dosovitskiy and Brox.",
      "year" : 2015
    }, {
      "title" : "Understanding deep architectures using a recursive convolutional network",
      "author" : [ "David Eigen", "Jason Rolfe", "Rob Fergus", "Yann LeCun." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Eigen et al\\.,? 2014",
      "shortCiteRegEx" : "Eigen et al\\.",
      "year" : 2014
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman." ],
      "venue" : "Cognitive science, 14(2):179–211.",
      "citeRegEx" : "Elman.,? 1990",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "Distributed representations, simple recurrent networks, and grammatical structure",
      "author" : [ "Jeffrey L Elman." ],
      "venue" : "Machine learning, 7(2-3):195–225.",
      "citeRegEx" : "Elman.,? 1991",
      "shortCiteRegEx" : "Elman.",
      "year" : 1991
    }, {
      "title" : "Visualizing higherlayer features of a deep network",
      "author" : [ "Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent." ],
      "venue" : "International",
      "citeRegEx" : "Erhan et al\\.,? 2009",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2009
    }, {
      "title" : "Extracting and learning an unknown grammar with recurrent neural networks",
      "author" : [ "C. Lee Giles", "Clifford B. Miller", "Dong Chen", "GuoZheng Sun", "Hsing-Hen Chen", "Yee-Chun Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, pages",
      "citeRegEx" : "Giles et al\\.,? 1991",
      "shortCiteRegEx" : "Giles et al\\.",
      "year" : 1991
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra." ],
      "venue" : "International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Gregor et al\\.,? 2015",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Attractor dynamics and parallelism in a connectionist sequential network",
      "author" : [ "Michael I Jordan." ],
      "venue" : "Proceedings of the Eighth Annual Conference of the Cognitive Science Society.",
      "citeRegEx" : "Jordan.,? 1986",
      "shortCiteRegEx" : "Jordan.",
      "year" : 1986
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128–3137.",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Visualizing and understanding recurrent networks",
      "author" : [ "Andrej Karpathy", "Justin Johnson", "Fei-Fei Li." ],
      "venue" : "arXiv preprint arXiv:1506.02078.",
      "citeRegEx" : "Karpathy et al\\.,? 2015",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2015
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3276–3284.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton." ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Visualizing and understanding neural models in NLP",
      "author" : [ "Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1506.01066.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "Computer Vision–ECCV 2014, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Understanding deep image representations by inverting them",
      "author" : [ "Aravindh Mahendran", "Andrea Vedaldi." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 5188–5196.",
      "citeRegEx" : "Mahendran and Vedaldi.,? 2015a",
      "shortCiteRegEx" : "Mahendran and Vedaldi.",
      "year" : 2015
    }, {
      "title" : "Visualizing deep convolutional neural networks using natural pre-images",
      "author" : [ "Aravindh Mahendran", "Andrea Vedaldi." ],
      "venue" : "arXiv preprint arXiv:1512.02017.",
      "citeRegEx" : "Mahendran and Vedaldi.,? 2015b",
      "shortCiteRegEx" : "Mahendran and Vedaldi.",
      "year" : 2015
    }, {
      "title" : "Turning on the turbo: Fast thirdorder non-projective turbo parsers",
      "author" : [ "André FT Martins", "Miguel Almeida", "Noah A Smith." ],
      "venue" : "ACL (2), pages 617–622.",
      "citeRegEx" : "Martins et al\\.,? 2013",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2013
    }, {
      "title" : "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Jeff Clune." ],
      "venue" : "arXiv preprint arXiv:1602.03616.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Simonyan and Zisserman.,? 2015",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "author" : [ "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman." ],
      "venue" : "International Conference on Learning Representation (ICLR) Workshop.",
      "citeRegEx" : "Simonyan et al\\.,? 2014",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2755–2763.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Reseg: A recurrent neural network for object segmentation",
      "author" : [ "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1511.07053.",
      "citeRegEx" : "Cho.,? 2015",
      "shortCiteRegEx" : "Cho.",
      "year" : 2015
    }, {
      "title" : "Understanding neural networks through deep visualization",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson." ],
      "venue" : "International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Yosinski et al\\.,? 2015",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2015
    }, {
      "title" : "Video paragraph captioning using hierarchical recurrent neural networks",
      "author" : [ "Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu." ],
      "venue" : "Describing and Understanding Video & The Large Scale Movie Description Challenge",
      "citeRegEx" : "Yu et al\\.,? 2015",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2015
    }, {
      "title" : "Object detectors emerge in deep scene cnns",
      "author" : [ "Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Variations of the RNN architectures have been applied in several NLP domains such as parsing (Vinyals et al., 2015) and machine translation (Bahdanau et al.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and machine translation (Bahdanau et al., 2015), as well as in computer vision applications such as image generation (Gregor et al.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : ", 2015), as well as in computer vision applications such as image generation (Gregor et al., 2015) and object segmentation (Visin et al.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "image (Karpathy and Fei-Fei, 2015) and video captioning (Yu et al.",
      "startOffset" : 6,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : "image (Karpathy and Fei-Fei, 2015) and video captioning (Yu et al., 2015).",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Recurrent neural networks (RNNs) were introduced by Elman (1990) as a connectionist architecture with the ability to model the temporal dimension.",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "It is a multi-task, multi-modal architecture consisting of two Gated-Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014) pathways and a shared word embedding matrix.",
      "startOffset" : 90,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "It is a multi-task, multi-modal architecture consisting of two Gated-Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014) pathways and a shared word embedding matrix.",
      "startOffset" : 90,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "As our case study we picked the IMAGINET model introduced by Chrupała et al. (2015). It is a multi-task, multi-modal architecture consisting of two Gated-Recurrent Unit (GRU) (Cho et al.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 14,
      "context" : "Recurrent neural language models akin to TEXTUAL which are trained to predict the next symbol in a sequence are relatively well understood, and there have been some attempts to analyze their internal states (e.g. Elman, 1991; Karpathy et al., 2015).",
      "startOffset" : 207,
      "endOffset" : 248
    }, {
      "referenceID" : 6,
      "context" : "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to hidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems.",
      "startOffset" : 92,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to hidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems.",
      "startOffset" : 92,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to hidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems. In Elman (1991) he trains an RNN on a small synthetic sentence dataset and analyzes the activation patterns of the hidden layer.",
      "startOffset" : 92,
      "endOffset" : 343
    }, {
      "referenceID" : 6,
      "context" : "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to hidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems. In Elman (1991) he trains an RNN on a small synthetic sentence dataset and analyzes the activation patterns of the hidden layer. His analysis shows that these distributed representations encode lexical categories, grammatical relationships and hierarchical constituent structures. Giles et al. (1991) trains RNNs similar to Elman networks on strings generated by small deterministic regular grammars with the objective to recognize positive and reject negative strings, and develops the dynamic state partitioning technique to extract the learned grammar from the networks in the form of deterministic finite state automatons.",
      "startOffset" : 92,
      "endOffset" : 628
    }, {
      "referenceID" : 11,
      "context" : "More specifically, they train Long Short-Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) for phrase level sentiment analysis and present novel methods to explore the inner workings of RNNs.",
      "startOffset" : 69,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "More closely related is the recent work of Li et al. (2015), who develops techniques for a deeper understanding of the activation patterns of RNNs, but focuses on models with modern architectures trained on large scale data sets.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "(2016) train a Convolutional Neural Networks (CNN) with different random initializations on the ImageNet dataset (Krizhevsky et al., 2012).",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).",
      "startOffset" : 193,
      "endOffset" : 280
    }, {
      "referenceID" : 24,
      "context" : "Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).",
      "startOffset" : 193,
      "endOffset" : 280
    }, {
      "referenceID" : 28,
      "context" : "Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).",
      "startOffset" : 193,
      "endOffset" : 280
    }, {
      "referenceID" : 22,
      "context" : "Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).",
      "startOffset" : 193,
      "endOffset" : 280
    }, {
      "referenceID" : 20,
      "context" : ", 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).",
      "startOffset" : 70,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : ", 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).",
      "startOffset" : 70,
      "endOffset" : 128
    }, {
      "referenceID" : 24,
      "context" : "In general, there has been a growing interest within computer vision in understanding deep models, with a number of papers dedicated to visualizing learned CNN filters and pixel saliencies (Simonyan et al., 2014; Yosinski et al., 2015; Mahendran and Vedaldi, 2015a).",
      "startOffset" : 189,
      "endOffset" : 265
    }, {
      "referenceID" : 28,
      "context" : "In general, there has been a growing interest within computer vision in understanding deep models, with a number of papers dedicated to visualizing learned CNN filters and pixel saliencies (Simonyan et al., 2014; Yosinski et al., 2015; Mahendran and Vedaldi, 2015a).",
      "startOffset" : 189,
      "endOffset" : 265
    }, {
      "referenceID" : 19,
      "context" : "In general, there has been a growing interest within computer vision in understanding deep models, with a number of papers dedicated to visualizing learned CNN filters and pixel saliencies (Simonyan et al., 2014; Yosinski et al., 2015; Mahendran and Vedaldi, 2015a).",
      "startOffset" : 189,
      "endOffset" : 265
    }, {
      "referenceID" : 5,
      "context" : "These techniques have also led to improvements in model performance (Eigen et al., 2014) and transferability of features (Zhou et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 30,
      "context" : ", 2014) and transferability of features (Zhou et al., 2015).",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "IMAGINET introduced in Chrupała et al. (2015) is a multi-modal GRU network architecture that learns visually grounded meaning representations from textual and visual input.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "For more details about the model and its performance see Chrupała et al. (2015).",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "For all the experiments, we trained IMAGINET on the training portion of the MSCOCO image-caption dataset (Lin et al., 2014), and analyzed the representations of the sentences in the validation set.",
      "startOffset" : 105,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "The target image representations were extracted from the pre-softmax layer of the 16layer CNN (Simonyan and Zisserman, 2015).",
      "startOffset" : 94,
      "endOffset" : 124
    }, {
      "referenceID" : 21,
      "context" : "Both POS tagging and dependency parsing are performed jointly using the TurboParser dependency parser (Martins et al., 2013).",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 30,
      "context" : "We develop a simple method we dub top K contexts after the top K images of Zhou et al. (2015). We forwarded each sentence from a corpus token-by-token through the RNN, and registered the hidden state ht for each unit at time step t.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "Similarly to Li et al. (2016), the activation value distributions are discretized into percentile bins per dimension, such that each bin contains 5% of the marginal density.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : ", 2014) or the purely distributional sentence embedding system of Kiros et al. (2015). A number of recurrent neural models rely on a so called attention mechanism, first introduced by Bahdanau et al.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "A number of recurrent neural models rely on a so called attention mechanism, first introduced by Bahdanau et al. (2015) under the name of soft alignment.",
      "startOffset" : 97,
      "endOffset" : 120
    } ],
    "year" : 2017,
    "abstractText" : "We present novel methods for analysing the activation patterns of RNNs and identifying the types of linguistic structure they learn. As a case study, we use a multi-task gated recurrent network model consisting of two parallel pathways with shared word embeddings trained on predicting the representations of the visual scene corresponding to an input sentence, and predicting the next word in the same sentence. We show that the image prediction pathway is sensitive to the information structure of the sentence, and pays selective attention to lexical categories and grammatical functions that carry semantic information. It also learns to treat the same input token differently depending on its grammatical functions in the sentence. The language model is comparatively more sensitive to words with a syntactic function. Our analysis of the function of individual hidden units shows that each pathway contains specialized units tuned to patterns informative for the task, some of which can carry activations to later time steps to encode long-term dependencies.",
    "creator" : "LaTeX with hyperref package"
  }
}