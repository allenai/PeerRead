{
  "name" : "1302.4957.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Bayesian Networks: A Unification for Discrete and Gaussian Domains",
    "authors" : [ "David Beckerman", "Dan Geiger" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We examine Bayesian methods for learn ing Bayesian networks from a combination of prior knowledge and statistical data. In particular, we unify the approaches we pre sented at last year's conference for discrete and Gaussian domains. We derive a gen eral Bayesian scoring metric, appropriate for both domains. We then use this metric in combination with well-known statistical facts about the Dirichlet and normal-Wishart dis tributions to derive our metrics for discrete and Gaussian domains.\n1 Introduction\nAt last year's conference, we presented approaches for learning Bayesian networks from a combination of prior knowledge and statistical data. These ap proaches were presented in two papers: one address ing domains containing only discrete variables (Beck erman et al., 1994), and the other addressing domains containing continuous variables related by an unknown multivariate-Gaussian distribution (Geiger and Beck erman, 1994). Unfortunately, these presentations were substantially different, making the parallels between the two methods difficult to appreciate. In this pa per, we unify the two approaches. In particular, we abstract our previous assumptions of likelihood equiv alence, parameter modularity, and parameter indepen dence such that they are appropriate for discrete and Gaussian domains (as well as other domains). Using these assumptions, we derive a domain-independent Bayesian scoring metric. We then use this general metric in combination with well-known statistical facts about the Dirichlet and normal-W ishart distributions\n• Author's primary affiliation: Computer Science De partment, Technion, Haifa 32000, Israel.\nto derive our metrics for discrete and Gaussian do mains. In addition, we provide simple proofs that these assumptions are consistent for both domains.\nThroughout this discussion, we consider a domain U of n variables x1, . . . , Xn. Each variable may be discrete-having a finite or countable number of states-or continuous. We use lower-case letters to re fer to variables and upper-case letters to refer to sets of variables. We write x; = k to denote that variable x; is in state k. When we observe the state for every vari able in set X, we call this set of observations a state of X; and we write X kx as a shorthand for the obser vations x; = k;, x; E X. The joint space of U is the set of all states of U. We use p(X = kxiY = ky,�) to de note the generalized probability density that X = kx given Y = ky for a person with current state of in formation e [DeGroot, 1970, p. 19]. We use p(XIY,�) to denote the generalized probability density function (gpdf) for X, given all possible observations of Y. The joint gpdf over U is the gpdf for U.\nWe use B, to denote the structure of a Bayesian net work, and fli to denote the parents of Xi in a given net Work. We assume the reader is familiar with Bayesian networks for the case where all variables in U are dis crete. Here, we describe a Bayesian-network represen tation for continuous variables. In particular, consider the special case where all the variables in U are con tinuous and the joint probability density function for U is a multivariate (nonsingular) normal distribution. In this case, to be in line with more standard notation, we use x to denote the set of variables U. We have\n(1)\nwhere j1 is an n-dimensional mean vector, and L: ( C!ij) is an n x n covariance matrix, which must be both symmetric and positive definite. Both j1 and E are implicitly functions of e. We shall find it convenient to refer to the precision matrix W = r;-t, whose elements are denoted by Wij.\nLearning Bayesian Networks: A Unification for Discrete and Gaussian Domains 275\nThis joint density function can be written as a product of conditional density functions each being a normal distribution. Namely,\nn p(xle) = rrp(x; lx1, · · · , Xi-11e) (2)\ni=1 i-1\np(x;lx1, . . . , x;-1, e) = n(Jl; + :L: bj;(xj- Jlj), 1/v;) j=1\n(3) where Jli is the unconditional mean of x; (i.e. , the ith component of jl), v; is the conditional variance of x; given values for x1, . . . , Xi-1, and bji is a linear coef ficient reflecting the strength of the relationship be tween Xj and x; (e.g., DeGroot, p. 55). Thus, we may interpret a multivariate-normal dis tribution as a Bayesian network, where there is no arc from Xj to x; whenever bji = 0, j < i. Con versely, from a Bayesian network with conditional dis tributions satisfying Equation 3, we may construct a multivariate-normal distribution. We call this special form of a Bayesian network a Gaussian network. The name is adopted from Shachter and Kenley (1989) who first described Gaussian influence diagrams. We note that, in practice, it is typically easier to assess a Gaus sian network than it is to assess directly a symmetric positive-definite precision matrix.\nThe transformations between v = { v1 , ... , Vn} and B = {bji I j < i} of a given Gaussian network G and the precision matrix W of the normal distribution rep resented by G are well known. In this paper, we need only the transformation from W to { v, B}. We use the following recursive form given by Shachter and Kenley (1989). Let W( i) denote the i xi upper left submatrix of W, b; denote the column vector (bli, . . . , b;-1,;) , and b� denote the transposition of b;. Then, for i > 1, we have\nW(; + 1) = ( and W(1) = ..l.. V1 W(i) + b;+1b:+1 Vi+l - b:±l Vi+ I\n_kL ) Vi+l _1_ Vi+l\n(4)\nAlthough Equation 3 is useful for the assessment of a Gaussian network, we shall sometimes find it conve nient to write\ni-1 p(x;lx1, . . . , x;-1, �) = n(m; + :L: bjiXj, 1/v;) (5)\nj=1 where m;, i = 1, .. . , n is defined by\ni-1\nm; = Jl; -2::: bji/Jj j=1 (6)\nNote that m; is the mean of x; when all of x;'s parents are equal to zero.\nAs an example, given the three-node network struc ture x1 -+ X3 f- x2, we have b12 = 0, x1 = n(m1, 1/v! ), x2 = n(m2, 1/v2), and x3 = n (m3 + b13(x1 - m! ) + b23(x2- m2), 1/v3). Also, the preci sion matrix corresponding to this network structure is given by ( l+� Vt V3\nW = � v,\n_fu. v,\n� v,\n..l.+� V2 V3 _ha. V3 -� ) v, ha. (7) V3 ..l. v, Finally, it is important to note that two or more Bayesian-network structures for a given domain can be equivalent in the sense that the structures repre sent the same set of gpdfs for the domain (Verma and Pearl, 1990). For example, for the three vari able domain { x, y, z} , each of the network structures x -+ y-+ z, x f- y -+ z, and x f- y f- z represents the gpdfs where x and z are conditionally independent of y, and are therefore equivalent. As another exam ple, a complete network structure is one that has no missing edges. In a domain with n variables, there are n! complete network structures. All complete network structures for a given domain represent the same set of gpdfs-namely, all possible gpdfs-and are there fore equivalent. In our proofs to follow, we require the following characterization of equivalent networks, proved by Chickering (in this proceedings).\nTheorem 1 (Chickering, 1995) Let B,1 and Bs2 be two Bayesian-network structures, and RB.1,B82 be the set of edges by which Bs1 and Bs2 differ in di rectionality. Then, Bs1 and B,2 are equivalent if and only if there exists a sequence of IRB.1,B,21 distinct arc reversals applied to Bs1 with the following properties:\n1. After each reversal, the resulting network struc ture contains no directed cycles and is equivalent to Bs2\n2. After all reversals, the resulting network structure is identical to Bs2\n3. If x -+ y is the next arc to be reversed in the current network structure, then x and y have the same parents in both network structures, with the exception that x is also a parent of y in B,1\n2 A Bayesian Approach for Learning Bayesian Networks\nOur Bayesian approach for learning Bayesian networks can be understood as follows. Suppose we have a do main of variables { x1, . . . , X n} = U, and a set of cases\n276 Heckerman and Geiger\n{ C1, .. . , Cm} = D where each case is a state of some or of all the variables in U. We sometimes refer to D as a database. We begin with the following random sample assumption: the database is a random sample from some sample distribution with unknown param eters 8u, and this sample distribution satisfies the conditional-independence assertions of some network structure B, for U. We define B: to be the hypoth esis that the sample distribution can be encoded in B,. Now, suppose that we wish to determine the gpdf p( CJ D, �)-the generalized probability density func tion for a new case C, given the database and our current state of information �. Rather than reason about this distribution directly, we assume that the collection of hypotheses B: corresponding to all net work structures for U form a mutually exclusive and collectively exhaustive set1 and compute\np(CJD,�)= 2: p(CID,B;,�)·p(B;ID,�) all B�\nIn practice, it is impossible to sum over all possible net work structures. Consequently, we attempt to identify a small subset H of network-structure hypotheses that account for a large fraction of the posterior probability of the hypotheses. Rewriting the previous equation us ing the fact that p(B:JD, �) = p(D, B: 1�)/p(DI�) , we obtain\np(CJD,�) � c 2: p(CJD, B;, �) · p(D, B:i�) B�EH\nwhere c is the normalization constant 1/[LBhEH p(D, B: 1�)]. From this relation, we see that only the relative posterior probabilities p( D, B: I�) matter. Thus, we compute this relative posterior probability, or alternatively, a Bayes' factor-p(B:ID,�)fp(B:oiD, �)-where B,o is some reference structure such as the empty graph. We call methods for computing these relative posterior probabilities Bayesian scoring metrics.\nExtending the Bayesian analysis, we use 8Bs to denote the parameters of the sample distribution encoded in the network structure B, given hypothesis B�. That is, the parameters 8B• determine the local gpdfs in Bp. From the rules of probability, we have\np(D, B; l�) = p(B:i�) (8)\n· J p(8B,JB:,�) p(DJ8B.,B;,�) d8B• The assessment of the network-structure priors p(B:J�) is treated elsewhere (e.g., Buntine, 1991,\n1 We comment on this assumption in the following section.\nand Beckerman et a!., 1995) . In the following sec tion, we introduce a set of assumptions that simpli fies the assessment of the network-parameter priors p(GB.IB:,�). In the remainder of this section, we show how to compute p(DI8B•, B:, �). A method for computing this term follows from our random-sample assumption. Namely, given hypothe sis B:, it follows that D can be separated into a set of random samples, where these random samples are determined by the structure of B,. First, let us exam ine this decomposition when all the variables in U are discrete. Let Bx=kxiY=ky denote the parameter cor responding to the probability p(X = kx iY = ky , �) , where X and Y are disjoint subsets of U. In addition, let Xii and Il;z denote the variable x; and the parent set II; in the lth case, respectively; and let Dz denote the first l - 1 cases in the database. Then, given B�, we know that the observations of x; in those cases where Ilu = kn, is a random sample with parameters 8x;dll;1=krr,. That is,\n{9) where kn, is the state of IIi/ consistent with {xu = k1, . . . , X(i-1)1 = k;-1}· Using Equation 9, we can compute p(DI8B., B:, �) for any database D and net work structure B, for discrete domain U. Now consider a domain of continuous variables i = { x1, ... , Xn}, and suppose the database D is a ran dom sample from a multivariate-normal distribution with parameters 8u = {j1, W}. From our discussion in Section 1, it follows that, given hypothesis B�, each variable x; is a random sample from a normal distri bution with mean m; + Lx;Ell; bjiXj and variance v; . Thus, with 8B• = {m, B, v}, we have\np(x;zlxu, ... , X(i-1)1, Dl, eBs, B;, �) = n(m; + 2: bjiXjl, 1/v;) (10)\nx;Ell;\nUsing Equation 10, we can compute p(DI8B,, B:, �) for any D and B, in a Gaussian domain. The generalization of Equations 9 and 10 is straight forward, and we state it as our first formal assumption.\nAssumption 1 (Random Sample) Let D = {C1, . . . , Cm} be a database, and B. be a net work structure for U determined by variable ordering (x1, . . . , Xn ) . Let 8(x;, II;) denote the parameters of the network associated. with variable x;. Then, for all variables x; E U,\np(xillxu, .. . , X(i-1)1> Dl, eBs, B;, �) = f(8{x;,II;),xi!,II;z) {11)\nLearning Bayesian Networks: A Unification for Discrete and Gaussian Domains 277\nwhere f is some function of the parameters E>(x;, ll;) and the database entries Xi! and fl;1.\nIn the discrete case, we have 8(x;, ll;) = 8.,;�rr,, and /(8(x;,lli),xi!,II;!) = 8.,,drr.,. In the Gaussian case, we have 8(x;, II;) = {m;, v;, b;}, and f(8(x; , ll;), x;1, IIi!) = n(m; + ExEII bjiXjl, 1/v;). J • 3 Informative Priors\nIn this section, we derive a general approach for assess ing the network-parameter priors p(E>BsiB:, e) . Our derivation is based on four assumptions that are ab stracted from our previous work.\nAssumption 2 (Likelihood Equivalence) Given two network structures B,1 and B.2 such that p(B:de) > 0 and p(B�2Ie) > 0, if B.r and B.2 are equivalent, then p(8u IB�l, e) = p(8u IB�2> e).\nInformally, the assumption states that the observation of a database does not help to discriminate equivalent network structures. We note that an equivalent way to state likelihood equivalence is that p(D IB�1, e) = p(DIB�2, e) for all databases D, whenever B,1 and B.2 are equivalent. 2\nThe motivation for this assumption is differ ent for acausal Bayesian networks-Bayesian net works that represent only assertions of conditional independence-and causal Bayesian networks. For acausal networks, likelihood equivalence is not an as sumption, but rather a consequence of our definition of B�. In particular, recall that the hypothesis B: is true iff the parameters 8u satisfy the conditional independence assertions of B,. Therefore, by defini tion of network-structure equivalence, if B,1 and B,2 are equivalent, then B�1 = B�2. 3 For example, in the domain { x 1, x2, x3}, the equivalent network struc-\n2We assume this equivalence is well known, although we have not found a proof in the literature.\n3We note that there is a flaw with our definition of B� for acausal Bayesian networks. In particular, the definition implies that hypotheses associated with different network structure equivalence classes will not be mutually exclu sive. For example, in the two-binary-variable domain, the hypotheses B!;y and B!;-+y (corresponding to the empty network structure, and the network structure x -+ y, re spectively) both include the possibility Bxy = BxBy. This flaw is potentially troublesome, because mutual exclusiv ity is important for our Bayesian interpretation of network learning (Equation 2). Nonetheless, because the densities p(E>BsiB�,e) must be integrable and hence bounded, the overlap of hypotheses will be of measure zero, and we may use Equation 2 without modification. For example, in our two-binary-variable domain, given the hypothesis B!;-+Y• the probability that B!;y is true (i.e., fJy = By/x) has mea sure zero.\ntures x1 -+ X2 -+ X3 and x1 t-- x2 t-- x3 both corre spond to the assertion 0.,1,x3Jx2 = 0.,1Jx20x3Jx2• Con sequently, B;1-+x2-+.,3 = B;1 +-x2+-.,3• This property, which we call hypothesis equivalence, implies likelihood equivalence. We note that, given hypothesis equiva lence, we should score equivalence classes of network structures-not individual network structures-when learning acausal Bayesian networks.\nFor causal Bayesian networks, we must modify the def inition of B: to include the assertion that each non root node in B, is a direct causal effect of its parents. Consequently, the property of hypothesis equivalence is contradicted by the new definition. Nonetheless, we have found that the assumption of likelihood equiv alence is reasonable for learning causal networks in many domains. (For a detailed discussion of this point, see Heckerman in this proceedings.)\nThe next assumption was adopted implicitly in our previous work.\nAssumption 3 (Structure Possibility) Given a domain U, p(B�cle) > 0 for all complete network structures B sc. As we shall see, the assumption allows us to make good use of the property of likelihood equivalence. Although it is an assumption of convenience, we have found it to be reasonable for many real-world network-learning problems.\nThe remaining two assumptions are abstractions of assumptions made either explicitly or implicitly by all researchers who have considered Bayesian-network learning (e.g., Cooper and Herskovits, 1991, 1992; Buntine, 1991; Spiegelhalter et al., 1993). These as sumptions are made mostly for computational con venience, although they are reasonable for many do mams.\nAssumption 4 (Global Parameter Independence) For all network structures B,,\nn\np(eB.IB� , e) = II p(8(x; , ll;) jB� , e) i=l\nAssumption 4 says that the parameters associated with each variable in a network structure are inde pendent. This assumption was first introduced under the name of global independence by Spiegelhalter and Lauritzen (1990).\nAssumption 5 (Parameter Modularity) Given two network structures B,1 and B,2 such that p(B�1Ie) > 0 and p(B�2Ie) > 0, if x; has the same parents in B,1 and B,2, then\n278 Heckerman and Geiger\nFor example, in our two-binary-variable domain, x has the same parents (none) in the network structure x --+ y and the structure contains no arc. Conse quently, the probability density for e(x, 0) would be the same for both of these structures. We call this property parameter modularity, because it says that the densities for parameters e(x;, TI;) depend only on the structure of the network that is local to variable x;-namely, on the parents of x; .\nGiven Assumptions 2 through 5, we can construct the priors p( e Bs I B:' e) for every network structure B. in u from the single prior p(eu IB:c, e), where B.c is any complete network structure for U. As an illustration of this construction, consider again our two-binary-variable domain. Given the prior den sity p(Oxy, Bxg, Bxy IB�-+y> e), we construct the priors p(eBsiB:,�) for each of the three network structures in the domain. First, consider the network structure x -t y. The joint-space parameters and parameters for this structure are related as follows:\nThus, we may obtain p( Ox' OyJx' OyJx IB�-+y' e) from the given density by changing variables:\np(Ox, OyJx> OyJx IB�-+y> e) = Jx-+y·p(Oxy, Bxy, BxgiB�-+Y' �) (12) where Jx-+y is the Jacobian of the transformation\n88xy/88x 88xy/88yJx 88xyf88yJx\nBx(1-Ox)\n88xg/88x 88xg/88yJx 88xg/88yJx\n(13)\nThe Jacobian JB.c for the transformation from eu to eBsc in an arbitrary discrete domain is given in Section 5.1.\nNext, consider the network structure x t- y. By Assumption 3, the hypothesis B�+-y is also possible, and, by likelihood equivalence, we have p(Oxy, Bxy, Bxg IB�+-y> e) p(Oxy, Bxy, BxgiB�-+y> �). Therefore, we can compute the density for the network structure x t- y using the Jacobian Jx+-y = Oy(1-0y)· Finally, consider the empty network structure. Given the assumption of global parameter independence, we may obtain the densities p(Ox IB�Y' �) and p(Oy IB�Y' e) separately. To obtain the density for Ox, we first extract p( Ox IB�-+y' e) from the density for the net work structure x -t y. This extraction is straight forward, because, by global parameter independence, the parameters for x -t y must be independent. Then, we use parameter modularity, which says that p(OxiB�y,e) = p(OxiB�-+y,e). To obtain the density for By, we extract p( By I B�+-y, �) from the density for\nthe network structure x t- y, and again apply pa rameter modularity. The approach is summarized in Figure 1. In general, we have the following construction.\nTheorem 2 Given domain U and a probability den sity p(eu IB:c, e) where Esc is some complete network structure for U, Assumptions 2 through 5 determine p(eB.IB:,e) for any network structure B. in U.\nWe note that our construction assumes that Assump tions 2 through 5 are consistent. We demonstrate con sistency in Section 7.\n4 A General Metric for Complete Data\nIn this section, we derive a general metric from As sumptions 1 through 5 and the following additional assumption:\nAssumption 6 (Complete Data) The database zs complete. That is, it contains no missing data.\nWe make this assumption only as a computa tional convenience. The reader should recognize that random-sample assumption and the informa tive priors developed in Section 3 can be used in conjunction with well-known statistical techniques to score incomplete databases as well. Such techniques include filling in missing data based on the data that is present [Titterington, 1976, Spiegelhalter and Lauritzen, 1990), the EM algo rithm [Dempster et a!., 1977), and Gibbs sampling [Madigan and Raftery, 1994].\nLearning Bayesian Networks: A Unification for Discrete and Gaussian Domains 279\nGiven our assumptions, we obtain the following lemmas.4\nLemma 3 (Posterior Parameter Independence) Given the random-sample assumption (Assumption 1}, global parameter independence (Assumption 4), and the assumption of no missing data (Assumption 6}, we have\nn\np(8BsJD,B�,e) = rrp(8(x;,II;)JD,B�,e) i=1\nfor all network structures B, (p(B� Je) > 0} and databases D.\nLemma 4 (Posterior Parameter Modularity) Given the random-sample assumption (Assumption 1}, global parameter independence (Assumption 4), pa rameter modularity (Assumption 5 ), and the assump tion of no missing data (Assumption 6 ), if x; has the same parents in any two network structures B,1 and B,2 (p(B�1le) > O,p(B�2Ie) > 0}, then\np(8(x;, II;)JD, B�1, e) = p(8(x;, II;) JD, B�2, e)\nfor all databases D.\nIn the following lemma and in subsequent discus sions, we need the notion of a database D restricted to X � U -that is the projection of database D onto the subset X -denoted DX. For example, given do main U = {x1,x2,x3} and database D = {C1 = {x1 = 1,x2 = 2,xa = 1},C2 = {x1 = 2,x2 = 2,x3 = 1}}, we have D{x,,x2} = {C1 = {x1 = 1, X2 = 2}, c2 = {x1 = 2, X2 = 2}}.\nLemma 5 Let X be a subset of U, and Esc (p(B�cle) > 0} be a complete network structure for any ordering where the variables in X come first. Given the random-sample assumption (Assumption 1}, global parameter independence (Assumption 4), and the as sumption of no missing data (Assumption 6 ),\nfor all databases D.\nReaders familiar with the concept of d-separation will recognize that Lemmas 3 and 5 can be readily obtained from graphical manipulations applied to the Bayesian network representation of the random-sample assump tion and the assumption of global parameter indepen dence.\nWe can now derive the general metric.\n4The proofs are simple and are omitted.\nTheorem 6 Given a domain U, let B, be any net work structure for U and Esc be a some complete net work structure for U. Then, given Assumptions 2 through 6,\nfor any database D.\nProof: From the rules of probability, we obtain\np(DJB�,e) = fiJ p(8B,JD1,B�,e) 1=1\nn . ITp(x;dxll, ... ' X(i-1)1' Dl' eBs' B�' e) d8Bs i=1\nFor every x; with parents II; in B 8, let B sc rr \"' be a complete network structure with variable ord;;i�g II;, x; followed by the remaining variables. By Assump tion 3, p(Bsc,rr;,.,le) > 0. Using Assumption 1 and Lemmas 3 and 4, we get\np(DJB�, e)= ft 1 irp(8(x;, II;)JDI, Bsc,rr,,x;, e) 1=1 i=1\n·p(x;diill, D1, 8(x;, II;), Bsc,rr;,x;,e) d8Bs\nDecomposing the integral over eBs into integrals over the individual parameter sets 8( x;, II;), and perform ing the integrations, we have\nm n\np(DIB�,e) =IT ITp(xiiiii11, D1, Bsc,rr;,x;,e) 1=1 i=1\nAlso, using Lemma 5, we obtain\nm n ( II JDII;,x, B ') ITITPXii, 11 �. , SC,IT,,x;,<, 1=1 i=1 p(II11JD1 ', Bsc,IT;,x;, e)\n(15)\nBy likeli hood �quivalence, we have that p(DJBsc,IT;,xi> e) = p(DJB,c,e). Consequently, for any subset X of U, we obtain p(Dx IBsc,IT;,x;' e) = p(Dx IB�C' e) by sum ming over the variables in DU\\X. Applying this result to Equation 15, we get Equation 14. 0 We call Equation 14 the Be (Bayesian likelihood equivlent) metric.\n280 Heckerman and Geiger\n5 Special-Case Metrics\nOur general metric is powerful, because it tells us that if we know how to compute p( DX I B�c, �) for any sub set X of U under the assumption that the domain con tains no structure (i.e., there are no independencies), then we can compute the probability of any database when there is structure. Therefore, the Be metric al lows us to leverage much of the work in the statistics literature, as statisticians have long dealt with the for mer problem. In this section, we illustrate this claim by deriving likelihood-equivalent metrics for the dis crete and Gaussian cases.\n5.1 The BDe Metric\nSuppose all variables in U are discrete. Recall that we use Bx=kxiY=ky denote the multinomial parameter corresponding to probability p(X = kx IY = ky, �). In addition, we use e XIY denote the collection of param eters B x =k x IY =k¥ for all states of sets X and Y. If y is empty, we simply write eX. Thus, for example, Bu = eXt , ... ,Xn represents the multinomial parameters of the joint space of U. Let us assume that the parameter set Bu has a Dirich let distribution when conditioned on a hypothesis cor responding to some complete network structure Bsc:\n(16) where N� is the equivalent sample size of the Dirich let distrib�tion associated with a complete network structure B.,c· DeGroot (1970, p. 50) shows that, for any subset X of u, eX also has a Dirichlet distri bution:\np(BxiB�c,�) = II B��•cp( X\\BZc,�)-1 (17)\nX\nNow, it is a well-known statistical result that, if a dis crete variable x with r states has a Dirichlet distribu tion with exponents N{ - 1, ... , N;- 1, then\nwhere D is a database for variable x and Nk is the number of times x takes on state kin D. Also, because U is discrete, any subset X of U can also be thought of as a single discrete variable with Tix;EX r; states. Therefore, Equations 17 and 18 allow us to compute each term in the Be metric (Equation 14) . To express the resulting metric for a given network structure B., we use q; = nx,en. r; to denote the number of states of IT; in Bs, and IT; = j to denote that II; has assumed the jth state, j = 1, . . . , q;.\nTheorem 7 (BDe Metric) Given domain U, and network structure B, and database D for U, let Nijk denote the number of times that x; = k and IT; = j in the database D; and let N;j = I::��1 denote the number of times that IT; = j in a database D. Then, if p(Bu IB�c, �) is Dirichlet with equivalent sample size N' for some complete network structure B,c, and if Assumptions 2 through 6 hold, then\nwhere\nr,\nN[j = L N[jk = N' . p(IT; = iiB�C) �) (20) k=l\nEquations 19 and 20 are the BDe (Bayesian Dirichlet likelihood equivalent) metric, originally derived in Heckerman et al. (1994).\nThe assumption that p(BuiB�c' �) is Dirichlet is not as arbitrary as it may seem at first glance. In discrete domains, we can assume not only that the parameters corresponding to each variable are independent, but that the parameters corresponding to each state of ev ery variable's parents are independent. Spiegelhalter and Lauritzen (1990) call this added assumption lo cal independence. Geiger and Heckerman (in this pro ceedings) show that likelihood equivalence, structure possibility, global and local parameter independence, and the assumption that p( Bu I B�c, �) is positive imply that p(BuiB�c,�) must be Dirichlet.\n5.2 The BGe Metric\nSuppose that all variables in U = i are continu ous, and that the database is a random sample from a multivariate-normal distribution. Let us assume that the parameter set {j1, W} has a normal-Wishart distribution when conditioned on B�c for some com plete network structure Esc· Namely, assume that p(j1IW, B�c, �) is a multivariate-normal distribution with mean j10 and precision matrix N� W (N[l > 0); and that p(WIB�c, €) is a Wishart distribution with NT degrees of freedom (NT > n - 1) and positive definite precision matrix To. That is,\nwhere c is a normalization constant [DeGroot, 1970, p. 57] .\nLearning Bayesian Networks: A Unification for Discrete and Gaussian Domains 281\nIt is well known that the normal-Wishart distribu tion is a conjugate family for multivariate-normal sam pling (e.g. , DeGroot, 1970, p. 178). Given a database D = {xl., ... , x;,}, let im and Sm denote its sam ple mean and variance, respectively. Then, given the normal-Wishart prior we have described, the posterior density p(fl, WID, n:c, e) is also a normal-Wishart dis tribution. In particular, p(fliW, D, n:c, e) is multivari ate normal with mean vector flm given by\nNl � � � ;tllO + mxm Jlm = N� + m p.\n(22)\nand precision matrix (N� + m)W; and p(WID, B:c, �) is a Wishart distribution with Nf. + m degrees of free dom and precision matrix Tm given by\nN�m _ _ Tm =To+ Sm + N/ (flo - Xm)(flo - Xm)1 {23) ;t+m\nFrom these equations, we see that Nh and Nf. can be thought of as equivalent sample sizes for the mean Jlo and the precision matrix T0, respectively. Given domain U = {x1, ... , Xn}, subset X of U, and vector y = ( y1 , ... , Yn), let .yx denote the vector formed by the components y; of y such that x; E X. Similarly, given matrix M, let M x denote the subma trix of M containing elements m;j such that x;, Xj E X. It is well known that if D is a random sample from an n-dimensional multivariate-normal distribu tion whose parameters {fl, W} have a normal-Wishart distribution with constants flo, N�, To and Nf., then Dx is a random sample from an IXI-dimensional mul tivariate distribution with parameters {flx, wx}, and these parameters have normal-Wishart distribution with constants fl-}, N�, T{ and Nf.. Furthermore, the formula for p( D I n:c, �) given the normal-Wishart prior is known (e.g., the probability may be obtained by integrating the left-hand-side of Equation 8, De Groot, 1970, p. 179, over the parameters). Conse quently, the evaluation of p(Dx IB:c, �) in Equation 14 is straightforward.\nTheorem 8 (BGe Metric) Given do main x = {x1, ... ,xn}, assume p(fl,WIB:c,�) is an n-dimensional normal-Wishart distribution with con stants flo, N�, To, and Nf.. Given a database D :::: { C1, . . . , Cm} and a subset X of x with l elements, Assumptions 2 through 6 imply the Be metric, where each term is given by\n7r-lm/2 i1 ( N' )112\nN'+m i1 (24)\n(l 1\\TI + ) N1 N1 +rn _c , lVT m IT.xiTITXI-� c(l, Nf.) 0 m\nwhere c(l, Nf.) = D,r (Nf.+21-i) (25)\nand Tm is the precision matrix of the posterior normal- Wishart distribution given by Equation 23.\nThe Be metric in combination with Equation 24 defines the BGe (Bayesian Gaussian likelihood equivalent) metric, originally derived in Geiger and Heckerman (1994). We note that assumptions similar to those used to show the inevitability of the Dirich let distribution for discrete domains imply that the normal-Wishart assumption is inevitable for Gaussian domains (see Geiger and Heckerman in this proceed ings).\nThe BDe and BGe metrics may be combined to score domains containing both discrete variables and con tinuous variables. Namely, let U = Ud U Uc where all variables in Ud and Uc are discrete and contin uous, respectively. Suppose that the observations of ud in the database are a random sample from a multivariate-discrete distribution, and the observa tions of the Uc given each state of Ud are a random sample from a multivariate-normal distribution. Fi nally, suppose that 8ud has a Dirichlet distribution, and that 8uciUd\"'k has a normal-Wishart distribution for every state k of U d. Then, we can apply the Be met ric to any network structure B, where the variables in U d precede the variables in Uc, using Equation 18 to evaluate terms for discrete variables, and Equations 24 and 25 to evaluate terms for continuous variables.\n6 Informative Priors from a Prior Network\nGiven our assumptions, p(8u IB;c, �) determines a Bayesian scoring metric. In this section, we discuss the assessment of this distribution.\nFor discrete domains, we can assess p(8u IB:c, e) by assessing (1) the joint probability distribution for the first cases to be seen in the database p(UIB;, e) and (2) the equivalent sample size N' for the domain. Meth ods for assessing N' are discussed in (e.g.) Heckerman et al. (1995). To assess p(UIB;,�), we can construct a Bayesian network for the first case to be seen. We call this Bayesian network a prior network. The unusual aspect of this assessment is the conditioning hypothe sis n;c (see Heckerman et al. [1995] for a discussion). We can assess p(8u iB:c, �) in the Gaussian case us ing a prior network as well. In this case, however, we require two equivalent samples sizes (N� > 0 and Nf. > n - 1). The details are discussed in last year's proceedings [Geiger and Heckerman, 1994]. Ex amples of the assessment of p( 8u I B;c, e) for discrete\n282 Heckerman and Geiger\nand Gaussian domains, and examples of the metrics that result from these assessments are also given in last year's proceedings.\n7 Consistency of the Assumptions\nThe assumptions of likelihood equivalence, structure possibility, global parameter independence, and pa rameter modularity may not be consistent. In particu lar, the assumptions of global parameter independence and modularity are constraints on parameter densities among individual network structures, whereas likeli hood equivalence is a constraint on parameter densi ties among network-structure equivalence classes. Fur thermore, our choices p(8u IB:c, e) is Dirichlet and p(ji, WI B�c' e) is normal-Wishart may not be consis tent with the assumptions of likelihood equivalence and global parameter independence. In this section, we demonstrate consistency in each case.\n7.1 Consistency of the Dirichlet Assumption\nFirst, we show that the assumption p( eu I B:c' e) is Dirichlet is consistent with the assumptions of likeli hood equivalence and global parameter independence for complete network structures.\nTo see the potential for inconsistency, consider again our approach for constructing priors in the two-binary variable domain. Suppose we choose the density\nc\nwhere cis a normalization constant. By Equations 12 and 13 we obtain\nfor the network structure x -+ y. This density satisfies the assumption of global (and local) parameter inde pendence. Using likelihood equivalence, however, we have for the network structure y -+ x\n( 11 )-c · By(1-By) p By, Bxly, BxiyiB.,+-Y' e - B.,(1-B.,) c · By(1-By)\n(ByBxly + (1-By)Bxlg)(1-(ByBxly + (1-By)Bxlg)) This density satisfies neither global (nor local) param eter independence.\nWhen p(8u IB�C' e) is Dirichlet, however, likelihood equivalence implies global (and local) parameter in dependence for all complete network structures. This result is proved for the two-variable case in Dawid and\nLauritzen (1993, Lemma 7. 2) and for the general case in Heckerman et al. (1995, Theorem 3), which we sum marize here.\nTheorem 9 Let Bsc be any complete network struc ture for domain U = {xt, ... , xn}. The Jacobian for the transformation from 8u to 8Bsc is\n(26)\nTheorem 10 Given a domain U = {x1, . . . , Xn}, if the parameters 8u have a Dirichlet distribution with parameters N�, , ... ,x,. -that is,\np(8ule) = c · II N' 1 [B J .,, , ... ,x,.-Xt,···,Xn (27)\nthen, for any complete network structure Bsc in U, the density p( 8 Esc I e) satisfies global and local parameter independence. In particular,\nn\nP(eB IC) =c . II II [B ·I . ]N�.Ix,, ... ,x;_, -1 SC � Xs Xt, ... ,Xt-1\nwhere N�;ix,, ... ,x;_, = L N'(Xt, . . . , Xn)\n(28)\n(29)\nProof: The result follows by multiplying the right hand-side of Equation 27 by the Jacobian in Theo rem 9, using the relation B.,,, ... ,x,. = TI�=1 Bx;lx1, ... ,x;_1, and collecting powers of Bx;ix1, ... ,x;_1• 0 It is interesting to note that each set of conditional parameters 8x;lx,, ... ,x;_, also has a Dirichlet distribu tion.\n7.2 Consistency of the Normal-Wishart Assumption\nNext, we show that the assumption p(jl, W!B:c, e) is normal-Wishart is consistent with the assumptions of likelihood equivalence and global parameter indepen dence for complete network structures.\nTheorem 11 The Jacobian for the change of vari ables from W to { v, B} is given by\nn\nJv,B = !8Wj8vBl = II v;Ci+1l (30) i=l\nProof: Let J(i) denote the Jacobian for the first variables in W. Then J(i) has the following form:\nJ(i- 1) 0 0 J(i) = 0 -!-;Ji-1,i-1 0 1 0 0 - v�\n(31)\nLearning Bayesian Networks: A Unification for Discrete and Gaussian Domains 283\nwhere h ,k is the identity matrix of size k x k. Thus, we have J(i) = v��1 · J(i -1) ' (32) which gives Equation 30. 0\nTheorem 12 The Jacobian for the change of vari ables from j1 to m is given by Jm, = 1.\nProof: From Equation 6, Jm, is the determinant of a triangular matrix whose diagonal elements are 1. 0\nTheorem 13 If {jl, W} has a normal- Wishart distri bution given background information�� then\nn p(m, v, Bl�) = ITp(m;, v;, b:l�)\ni=1\nProof: To prove the theorem, we factor p(mlv, B, �) and p(v, Bl�) separately. By assumption, we know that p(fliW) is a multivariate-normal distribution with mean J.lo and precision matrix N� W. Transforming this result to conditional distributions for J.li, we ob tain\nc . 1 L . \"\"; -1 b c tor z = , . . . , n. ettmg mo; = J.lo; - uj=1 jiJ.loj tor each i, we get ( N� ) 1/2 p(J.I;IJ.l1, . . . ,J.l;-1,v,B,�) = 21r�; { (m; - mo;) 2 } . exp 2v;/N� ,.. Thus, collecting terms for each i and using the Jaco bian Jm, = 1, we have\nn p(mJv, B,�) = IT n(m0;, N�jv;) (33)\ni=1\nIn addition, by assumption, we have\nFrom Equation 4, we have\nJW(i)J = I_ J W(i - 1}1 = IT n v;1 v· ' i=1\nso that the determinant in Equation 34 factors as a function of i. Also, Equation 4 implies (by induction)\nthat each element Wij in W is a sum of terms each be ing a function of b: and v;. Consequently, the exponent in Equation 34 factors as a function of i. Thus, given the Jacobian lv,B, which also factors as a function of i, we obtain\nn\np(v, Bl�) = ITp(v;, b;J�) (35) i=1\nEquations 33 and 35 imply the theorem. 0\n7.3 Consistency of Likelihood Equivalence, Structure Possibility, Parameter Independence, and Parameter Modularity\nAs mentioned, the assumptions of likelihood equiva lence, structure possibility, global parameter indepen dence, and parameter modularity may not be consis tent. To understand the potential for inconsistency, note that we obtained the Be metric (Equation 14) for all network structures using likelihood equivalence applied only to complete network structures in com bination with the assumptions of structure possibil ity, global parameter independence, parameter mod ularity. Thus, it could be that the Be metric for in complete network structures is not likelihood equiva lent. Nonetheless, the following theorem shows that the Be metric is likelihood equivalent for all network structures-that is, given structure possibility, global parameter independence, and parameter modularity, likelihood equivalence for incomplete structures is im plied by likelihood equivalence for complete network structures. Consequently, the assumptions are consis tent.\nTheorem 14 (Likelihood Equivalence) If Bs1 and Bs2 are equivalent network structures for domain U, then, for all databases D, p(DJB:1,�) = p( D I B:2, �), where each likelihood is computed by the\nBe metric (Equation 14).\nProof: By Theorem 1, we know that a network struc ture can be transformed into an equivalent structure by a series of arc reversals. Thus, we can demonstrate likelihood equivalence in general if we can do so for the case where two equivalent structures differ by a single arc reversal. So, let B,1 and Bs2 be two equivalent network structures that differ only in the direction of the arc between x; and Xj (say x; -+ Xj in B,I). Let R be the set of parents of x; in B •1· By Theorem 1, we know that R U { x;} is the set of parents of Xj in Bs1, R is the set of parents of Xj in B,2, and RU { Xj} is the set of parents of x; in Bs2· Because the two structures differ only in the reversal of a single arc, the only terms in the product of Equation 14 that can differ are those\n284 Heckerman and Geiger\ninvolving x; and Xj. For B,1, these terms are\np(Dx'RjB�c' e) p(Dx;xiRjB�c' e) _ p(Dx;xiRjB�c' e) p(DRjB�c' e) p(flx;RjB�c' e) p(DRjB�c,e)\nwhereas for B,2, they are\np(DxiRjB�c,e) p(Dx;xiRjB�c,e) - p(Dx'xiRjB�c,e) p(DRjB�c,e) p(DxiRjB�c,e) - p(DRjB�c,e)\nThese terms are equal, and consequently, so are the likelihoods. 0\nAcknowledgments\nWe thank Peter Spirtes for identifying an error with Equation 24.\nReferences\n[Buntine, 1991] Buntine, W. (1991). Theory refine ment on Bayesian networks. In Proceedings of Sev enth Conference on Uncertainty in Artificial Intelli gence, Los Angeles, CA, pages 52-60. Morgan Kauf mann.\n[Chickering, 1995] Chickering, D. (1995). A trans formational characterization of equivalent Bayesian network structures. In this proceedings.\n[Cooper and Herskovits, 1992] Cooper, G. and Her skovits, E. (1992). A Bayesian method for the induc tion of probabilistic networks from data. Machine Learning, 9:309-347.\n[Cooper and Herskovits, 1991] Cooper, G. and Her skovits, E. (January, 1991). A Bayesian method for the induction of probabilistic networks from data. Technical Report SMI-91-1, Section on Medical In formatics, Stanford University.\n[Dawid and Lauritzen, 1993] Dawid, A. and Lau ritzen, S. (1993). Hyper Markov laws in the sta tistical analysis of decomposable graphical models. Annals of Statistics, 21:1272-1317.\n[DeGroot, 1970] DeGroot, M. (1970). Optimal Statis tical Decisions. McGraw-Hill, New York.\n[Dempster et al., 1977] Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incom plete data via the EM algorithm. Journal of the Royal Statistical Society, B 39:1-38.\n[Geiger and Heckerman, 1994] Geiger, D. and Hecker man, D. (1994). Learning Gaussian networks. In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence, Seattle, WA, pages 235-243. Morgan Kaufmann.\n[Geiger and Heckerman, 1995] Geiger, D. and Hecker man, D. (1995). A characterization of the Dirichlet distribution with application to learning Bayesian networks. In this proceedings.\n[Heckerman, 1995] Heckerman, D. (1995). A Bayesian approach for learning causal networks. In this pro ceedings.\n[Heckerman et al., 1994] Heckerman, D., Geiger, D., and Chickering, D. (1994). Learning Bayesian net works: The combination of knowledge and statis tical data. In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence, Seattle, WA, pages 293-301. Morgan Kaufmann.\n[Heckerman et al., 1995] Heckerman, D. , Geiger, D., and Chickering, D. (1995). Learning Bayesian net works: The combination of knowledge and statisti cal data. Machine Learning, to appear.\n[Madigan and Raftery, 1994] Madigan, D. and Raftery, A. (1994). Model selection and account ing for model uncertainty in graphical models using Occam's window. Journal of the American Statisti cal Association, 89:1535-1546.\n[Shachter and Kenley, 1989] Shachter, R. and Kenley, C. (1989). Gaussian influence diagrams. Manage ment Science, 35:527-550.\n[Spiegelhalter et al., 1993] Spiegelhalter, D., Dawid, A., Lauritzen, S., and Cowell, R. (1993). Bayesian analysis in expert systems. Statistical Science, 8:219-282.\n[Spiegelhalter and Lauritzen, 1990] Spiegelhalter, D. and Lauritzen, S. (1990). Sequential updating of conditional probabilities on directed graphical struc tures. Networks, 20:579-605.\n[Titterington, 1976] Titterington, D. (1976). Updat ing a diagnostic system using unconfirmed cases. Applied Statistics, 25:238-247.\n[Verma and Pearl, 1990] Verma, T. and Pearl, J. (1990). Equivalence and synthesis of causal mod els. In Proceedings of Sixth Conference on Uncer tainty in Artificial Intelligence, Boston, MA, pages 220-227. Morgan Kaufmann."
    } ],
    "references" : [ {
      "title" : "A Bayesian method for the induc­ tion of probabilistic networks from data",
      "author" : [ "Cooper", "Herskovits", "G. 1992] Cooper", "E. Her­ skovits" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cooper et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Cooper et al\\.",
      "year" : 1992
    }, {
      "title" : "Hyper Markov laws in the sta­ tistical analysis of decomposable graphical models",
      "author" : [ "Dawid", "Lauritzen", "A. 1993] Dawid", "S. Lau­ ritzen" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Dawid et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Dawid et al\\.",
      "year" : 1993
    }, {
      "title" : "Maximum likelihood from incom­ plete data via the EM algorithm",
      "author" : [ "Dempster et al", "A. 1977] Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "Journal of the Royal Statistical",
      "citeRegEx" : "al. et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1977
    }, {
      "title" : "Learning Gaussian networks",
      "author" : [ "Geiger", "Heckerman", "D. 1994] Geiger", "D. Hecker­ man" ],
      "venue" : "In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Geiger et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 1994
    }, {
      "title" : "A characterization of the Dirichlet distribution with application to learning",
      "author" : [ "Geiger", "Heckerman", "D. 1995] Geiger", "D. Hecker­ man" ],
      "venue" : null,
      "citeRegEx" : "Geiger et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning Bayesian net­ works: The combination of knowledge and statis­ tical data",
      "author" : [ "Heckerman et al", "D. 1994] Heckerman", "D. Geiger", "D. Chickering" ],
      "venue" : "In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1994
    }, {
      "title" : "Learning Bayesian net­ works: The combination of knowledge and statisti­ cal data",
      "author" : [ "Heckerman et al", "D. 1995] Heckerman", "D. Geiger", "D. Chickering" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "al. et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1995
    }, {
      "title" : "Model selection and account­ ing for model uncertainty in graphical models using Occam's window",
      "author" : [ "Madigan", "Raftery", "D. 1994] Madigan", "A. Raftery" ],
      "venue" : "Journal of the American Statisti­ cal Association,",
      "citeRegEx" : "Madigan et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Madigan et al\\.",
      "year" : 1994
    }, {
      "title" : "Bayesian analysis in expert systems",
      "author" : [ "Spiegelhalter et al", "D. 1993] Spiegelhalter", "A. Dawid", "S. Lauritzen", "R. Cowell" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1993
    }, {
      "title" : "Sequential updating of conditional probabilities on directed graphical struc­ tures",
      "author" : [ "Spiegelhalter", "Lauritzen", "D. 1990] Spiegelhalter", "S. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "Spiegelhalter et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Spiegelhalter et al\\.",
      "year" : 1990
    }, {
      "title" : "Equivalence and synthesis of causal mod­ els",
      "author" : [ "Verma", "Pearl", "T. 1990] Verma", "J. Pearl" ],
      "venue" : "In Proceedings of Sixth Conference on Uncer­ tainty in Artificial Intelligence,",
      "citeRegEx" : "Verma et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Verma et al\\.",
      "year" : 1990
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "We examine Bayesian methods for learn­ ing Bayesian networks from a combination of prior knowledge and statistical data. In particular, we unify the approaches we pre­ sented at last year's conference for discrete and Gaussian domains. We derive a gen­ eral Bayesian scoring metric, appropriate for both domains. We then use this metric in combination with well-known statistical facts about the Dirichlet and normal-Wishart dis­ tributions to derive our metrics for discrete and Gaussian domains.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}