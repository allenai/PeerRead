{
  "name" : "1605.09088.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The Bayesian Linear Information Filtering Problem",
    "authors" : [ "Bangrui Chen", "Peter Frazier" ],
    "emails" : [ "BC496@CORNELL.EDU", "PF98@CORNELL.EDU" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Information filtering systems automatically distinguish relevant from irrelevant items (emails, news articles, intelligence information) in large information streams (Foltz & Dumais, 1992). They typically use a classifier trained on relevance feedback from past items. However, when filtering for new users, or when item contents or user interests change, sufficient training data may not be available. In such “cold-start” situations, it may be beneficial to actively explore user interests by forwarding those items whose relevance we wish to learn, but too much exploration degrades short-term performance. This is an example of the so-called exploration vs. exploitation tradeoff (Sutton\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\n& Barto, 1998).\nIn this paper, we present a Bayesian sequential decisionmaking formulation of this problem, where user interests are described by a Bayesian linear model, similar in spirit to a Bayesian linear bandit (Agrawal & Goyal, 2013). The first contribution of our paper is to construct an instance-specific computational upper bound on the value of a Bayes-optimal strategy, which may be used to bound the optimality gap for implementable heuristic policies. Our upper bound is most naturally applied to items whose features are weights from a topic model (Blei & Lafferty, 2009) or other mixture model, but can also be applied to other linear models. Our second contribution is to use the idea of decomposing the problem into a collection of forwarding problems with one-dimensional feature “vectors”, developed in the construction of the upper bound, to create a pair of heuristic policies, jointly given the name Decompose-Then-Decide (DTD). The first heuristic, called DTD-Dynamic-Programming (DTD-DP), solves each one-dimensional forwarding problem using stochastic dynamic programming, while the second, called DTD-Upper-Confidence-Bound (DTD-UCB), uses the upper confidence bound policy with a learning parameter that is adjusted based on the distribution of feature vectors in the given direction. Finally, we evaluate our upper bound and proposed policies on real and simulated data, and find that our upper bound is typically tight, and that DTD-UCB outperforms a number of benchmarks, including UCB and Linear Thompson Sampling, in all problem instances.\nThe traditional approach to adaptive information filtering trains on historical feedback and does not actively explore to get the most useful feedback. However, there has been some work on active exploration in information filtering. (Zhang et al., 2003) studies a Bayesian decision-theoretic version of this problem in which a univariate score is observed for each item, and relevance is related to this score via logistic regression. The system does active exploration by valuing the information that results from forwarding, via a one-step lookahead calculation. The multi-step Bayes-\nar X\niv :1\n60 5.\n09 08\n8v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\n01 6\noptimal policy is not calculated or characterized. Zhao & Frazier (2014) studies another Bayesian decision-theoretic version of this problem in which items are described by a hard clustering scheme, and users have independent heterogeneous preferences for item clusters. A computational procedure for calculating the (multi-step) Bayes-optimal policy is provided. However, the learning scheme used does not allow learning user interest in one category from interactions with other related categories, making it difficult to scale to fine-grained item representations.\nA much larger literature on active exploration may be found in work on the multi-armed bandit problem (Herbert, 1952). Indeed, the information filtering problem we study can be seen as a special case of the (Bayesian) contextual linear multi-armed bandit problem (Agrawal & Goyal, 2013; Chu et al., 2011; May et al., 2012; Cesa-Bianchi & Kakade, 2011). The context is the feature vector for the arriving paper, and two arms are available: pulling the first arm corresponds to forwarding the paper, and provides a reward corresponding to the paper’s relevance, minus some cost for the user’s time; pulling the second arm corresponds to discarding the paper, and has known value 0.\nWhile much of the work on multi-armed bandits, including work specifically on linear and contextual bandits, has focused on asymptotic regret guarantees when latent parameters (in our case, the vector of user preferences for features) are chosen by an adversary, we focus on the Bayesian setting, where we assume that latent parameters are drawn from a prior probability distribution.\nOur assumption of a Bayesian framework has advantages and disadvantages. The main advantage is that it supports good performance when the amount of feedback received is small (of great importance in the cold-start setting). In contrast, algorithms designed to have regret with an optimal rate in the linear bandit setting, such as the PEGE algorithm in (Rusmevichientong & Tsitsiklis, 2010), typically need a number of interactions at least as large as the dimension of the feature vector, which may be hundreds of dimensions or more. A Bayesian algorithm can do well much sooner than this, by using information embedded in the prior that, for example, most users have little preference for a particular feature, or that users who prefer one feature tend to not prefer another feature.\nThe main disadvantage of the Bayesian framework is that choosing a reasonable prior typically requires work and assumptions. However, in the specific application context that we study, personalized information filtering, there is a natural way to build a prior from historical interaction data with other users. We explain and illustrate this method in Section 4.1 using the Yelp academic dataset (Yelp) and Section 4.2 using the arXiv (arXiv) condensed matter dataset.\nOur upper bound is an instance-specific computational upper bound on the performance of the optimal policy. It can be used to compute how far DTD-DP, DTD-UCB, or any other policy is from optimal for any given problem instance by computing the value of the heuristic with simulation, computing the upper bound, and subtracting the value from the bound. In industry, where one must allocate engineering and data science effort across projects, and one typically has a collection of concrete problems with business impact, this supports deciding whether the improvements that will be seen from continued algorithmic development are worthwhile, or whether the best existing heuristic is good enough. While our upper bound does not determine whether a proposed algorithm attains the optimal asymptotic rate, nor does it allow computing the worst-case bounds over all problem instances, we argue that knowing distance from the optimal finite-time performance for specific problem instances with business impact is often more useful.\nThis paper is structured as follows. In Section 2, we formulate the Bayesian information filtering problem. In Section 3, we develop a computationally tractable upper bound on the value of an optimal policy (Section 3.1), use this analysis to motivate development of DTD-DP (Section 3.2) and DTD-UCB (Section 3.3). In Section 4 we compare DTD-DP and DTD-UCB’s performance against benchmarks on both real and simulated data, show a significant improvement over the best of these benchmarks, tuned UCB, and show that its performance is close to the computational upper bound across a range of problems."
    }, {
      "heading" : "2. Problem formulation",
      "text" : "We consider information filtering for a single user. Items arrive to the information filtering system following a Poisson distribution with rate Γ. The nth arriving item is described by a k-dimensional feature vector Xn = (x1,n, · · · , xk,n). We assume that xi,n ≥ 0 for all i and n (If xi,n are bounded below, then this is without loss of generality). The vector Xn is observable to the system when the item becomes available for forwarding, and we assume that the system also knows the distribution of Xn. This distribution can typically be estimated from historical data. In this paper, we denote the density function of the feature vectors’ distribution as f(Xn).\nLet θ = (θ1, · · · , θk) denote the single user’s latent preference vector for the k different features. Here we model θ as having been drawn from a multivariate normal distribution with mean µ0 = (µ1,0, · · · , µk,0) and covariance matrix Σ0, which represents our Bayesian prior distribution about the latent preference vector. Usually this initial belief can be obtained using the historical data from other users and we give examples of how this may be accomplished in\nSection 4.1 and Section 4.2. Further, we use µn and Σn to denote our Bayesian posterior distribution about the user’s reward vector after the arrival of the first n items.\nUpon each item’s arrival, the system decides whether to forward this item to the user or not. We let Un ∈ {0, 1} represent this decision for the nth item, where 1 means to forward and 0 means not to forward. If the system decides not to forward, then the item is discarded. Each time the system forwards, it pays a constant cost c and receives the item’s relevance Yn as a reward. This relevance is modeled as the inner product between the user’s unobservable vector of preferences for features θ and the item’s feature vector Xn, perturbed by independent normal noise n with variance I(Xn)λ2, where I(Xn) denotes the number of nonzero elements in Xn. The system only observes Yn if it forwards the item. Except for the fact that some Yn are unobserved, this statistical model is Bayesian linear regression (see (Gelman et al., 2003), Chapter 14).\nIn many applications, I(Xn) = k with probability 1, making our assumed observational variance of I(Xn)λ2 equivalent to assuming homogeneous variance kλ2. Even when I(Xn) varies, we may modify our problem by perturbing each component of Xn by some arbitrarily small > 0 to make I(Xn) = k without substantially affecting the value of any particular policy.\nThe decision of whether or not to forward the nth item can only depend on the previous information Hn−1 = (Um, Xm, UmYm : m ≤ n− 1) as well as our current Xn. A policy π is a sequence of functions π = (π1, π2, · · · ) such that πn = (Rk+ × {0, 1})n−1 × Rk+ 7→ {0, 1} and we use Π to denote the set of all such policies.\nSuppose that the (random) lifetime of the user in the system is T , and let N be the total number of items that arrive to the system before T . Then our goal is to maximize:\nsup π∈Π\nEπ [ N∑ n=1 Un(Yn − c) ] (1)\nwhere Eπ denotes the expected reward using policy π.\nFor analytic tractability, we assume that T is exponentially distributed, and let its rate parameter be r > 0. Then, one can show that N follows a geometric distribution with parameter γ = ΓΓ+r , and the random finite horizon problem (1) can be transformed to a discounted infinite horizon problem:\nEπ [ N∑ n=1 Un(Yn − c) ] = γEπ [ ∞∑ n=1 γn−1Un(Yn − c) ] ,\n(2)\nwhere γ = ΓΓ+r . Details of this computation may be found in the supplement."
    }, {
      "heading" : "3. Main Results",
      "text" : "The problem described in section 2 is a partially observable Markov decision process, and can, in theory, be solved using stochastic dynamic programming, see (Lovejoy, 1991) and (Monahan, 1982). However, the state space of this dynamic program on the belief state is in high dimension (k dimensions are required to represent the posterior mean, and O(k2) dimensions are required for the posterior covariance matrix), which makes solving it computationally intractable.\nInstead, we provide in this section a computational upper bound of this problem (in Section 3.1) and develop two implementable policies DTD-DP and DTD-UCB based on this upper bound in Section 3.2 and Section 3.3. When DTD-DP and DTD-UCB, or any other implementable policy, gives us a result close to the upper bound, then we are reassured that this policy is nearly optimal."
    }, {
      "heading" : "3.1. Upper bound",
      "text" : "In this section, we provide a computational upper bound on the value of the solution to (1). This upper bound is based on the idea of dividing (1) into k different “singlefeature” subproblems, then performing an information relaxation (similar in spirit to (Brown et al., 2010)) in which we give the policy assigned to each single-feature subproblem additional information, which allows us to compute their value efficiently.\nDefine Yi,n = θi + in. Here i n ∼ N(0, λ\n2\nx2i,n ) if xi,n > 0\nand in = 0 if xi,n = 0 for i = 1, 2, · · · , k, independently distributed across i and n. We may think of Yi,n as the reward that we would have seen if Xn were equal to ei, where ei is a unit vector with the ith element 1 and other elements 0. Later, we will use that Yn = ∑k i=1 xi,nθi +\nn = ∑k i=1 xi,n(θi + i n) = ∑k i=1 xi,nYi,n.\nWe will generalize the original problem (1) by introducing notation that allows for separate forwarding decisions to be made for each feature. Define Uj,n to be decision made for the jth feature of the nth item. The original problem (1) can be recovered if we require that Uj,n is identical across j for each n.\nFor each feature j, we now introduce a new set of policies Πj , which will govern the forwarding decisions Uj,n for feature j, and under which these decisions can depend upon information not available in the original problem: they may depend on θ · ei for ∀i 6= j. Formally, the decision of whether or not to forward the jth feature of the nth item depends on the history Hjn−1 = (Uj,m, Xj,m, Uj,mYj,m : m ≤ n − 1), our current Xj,n, and θ−j = (θ1, · · · , θj−1, θj+1, · · · , θk).\nUsing these definitions, we may now state the computational upper bound, which constitutes the main theoretical result of this work. It bounds the value of the optimal policy for our original problem of interest (1), on the left-hand side, by the sum of a collection of values of single-feature problems, each of which have been given additional information. Efficient computation of this right-hand side is discussed below, and summarized in Algorithm 1.\nTheorem 1. For any Xn that are bounded over all n, we have\nsup π∈Π\nEπ [ N∑ n=1 Un(Yn − c) ]\n≤ k∑ j=1 sup π′′∈Πj Eπ ′′ [ N∑ n=1 Uj,n(xj,nYj,n − xj,nc ‖Xn‖ ) ] ,\nwhere ‖Xn‖ is the L1 norm. When ∑k i=1 xi,n = 1, then this theorem becomes:\nsup π∈Π\nEπ [ N∑ n=1 Un(Yn − c) ]\n≤ k∑ j=1 sup π′′∈Πj Eπ ′′ [ N∑ n=1 Uj,n(xj,nYj,n − xj,nc) ] .\nProof. Since ‖Xn‖ = x1,n + · · ·+ xk,n, we know\nsup π∈Π\nEπ [ N∑ n=1 Un(Yn − c) ]\n= sup π∈Π\nEπ [ N∑ n=1 Un(x1,nY1,n + · · ·+ xk,nYk,n − c) ]\n= sup π∈Π\nEπ  N∑ n=1 k∑ j=1 Un(xj,nYj,n − xj,n c ‖Xn‖ )  . (3) Now we introduce two new policy sets Π ′ 0 and Π ′ , which allow different features can make their own decisions Uj,n for the nth item. Further, Π ′\n0 has an additional restriction that U1,n = · · · = Uj,n. Based on the definition, we have\n(3) = sup π′∈Π′0\nEπ ′  N∑ n=1 k∑ j=1 Uj,n(xj,nYj,n − xj,n c ‖Xn‖ )  ≤ sup π′∈Π′ Eπ ′  N∑ n=1 k∑ j=1 Uj,n(xj,nYj,n − xj,n c ‖Xn‖ )\n . (4)\nSince the supremum of a summation is less or equal to the summation of a supremum, we have (4) ≤ k∑ j=1 sup π′∈Π′ Eπ ′ [ N∑ n=1 Uj,n(xj,nYj,n − xj,n c ‖Xn‖ ) ] .\n(5)\nThen based on the definition of our policy set Πj , for j = 1, 2, · · · , k, we know (5) ≤ k∑ j=1 sup π′′∈Πj Eπ ′′ [ N∑ n=1 Uj,n(xj,nYj,n − xj,n c ‖Xn‖ ) ] ,\nwhich concludes the proof of the theorem.\nWe emphasize that this computational upper bound holds true in general, even when the different components of Xn are correlated. Numerical experiments in Section 4.3 and Section 4.1 suggest that the optimality gap between this upper bound and the best heuristic policy is typically small.\nFor simplicity, in this paper we focus on the special case where ∑k i=1 xi,n = 1. We now discuss computation of the upper bound in Theorem 1. To compute this quantity, we must solve these k subproblems:\nsup π∈Πj\nEπ [ N∑ n=1 Uj,n(xj,nYj,n − xj,nc) ] , j = 1, 2, · · · , k,\n(6)\nwhere Yj,n|θj ∼ N(θj , λ 2\nx2j,n ) and θj ∼ N(µj,n, σ2j,n).\nHere θj ∼ N(µj,n, σ2j,n) represents our belief of θj after the first n items.\nTherefore for each subproblem, after the arrival of the nth item, we can update our parameters as the following:\nµj,n =\n{ λ2βj,n−1µj,n−1+Yj,n−1x 2 j,n−1\nλ2βj,n−1+x2j,n−1 if Uj,n−1 = 1;\nµj,n−1 if Uj,n−1 = 0.\nThe precision of our beliefs is updated as follows:\nβj,n =\n{ βj,n−1 + x2j,n−1 λ2 if Uj,n−1 = 1;\nβj,n−1 if Uj,n−1 = 0.\nThe jth single-feature subproblem can be solved using dynamic programming with a three-dimensional state space (µj,n, σj,n, xj,n), where µj,n and σj,n are the mean and variance of our current belief about θj and xj,n is the current item’s jth feature. Initially, µj,0 and σj,0 are given by the conditional distribution of θj given θ−j and the prior\ndistribution θ ∼ N(µ,Σ). Upon each item’s arrival, we move to another state based on the updating formula described above. Define Qj(µ, σ, x, 0) and Qj(µ, σ, x, 1) be the total reward to go if you decided to discard the item and forward the item respectively,\nQj(µ, σ, x, U) = sup π′′∈Πj\nEπ ′′ [ ∞∑ n=1 γn−1Uj,x(xj,nYj,n − xj,nc)\n|θj ∼ N(µ, σ2), xj,1 = x, Uj,1 = U ].\nThen the Bellman equation for this problem is:\nVj(µ, σ, x) = max U=0,1 Qj(µ, σ, x, U). (7)\nThis calculation is summarized as Algorithm 1.\nAlgorithm 1 Calculation of the jth subproblem Solve the dynamic program using backward induction (discretizing and truncating), with state space (µj,n, σj,n, xj,n) ∈ R×R+× [0, 1], infinite horizon and value function Vj(µ, σ, x). for i = 1; i < M ; i+ + do\nGenerate θ ∼ N(µ,Σ); Calculate the conditional distribution of θj ∼ N(µj,0, σj,0), given θ ∼ N(µ,Σ) and θ−j . Generate xj,0 from the distribution of Xn. Find the optimal value of state (µj,0, σj,0, xj,0) and denote it as Vi.\nend for Calculate V̄ = 1M ∑M i=1 Vi and use (2) to get the optimal value for the jth subproblem, where M is the number of simulation.\nWe may improve our upper bound by taking its minimum with a hindsight upper bound, derived in the following way. We first consider a larger class of policies that may additionally base their decisions on full knowledge of θ. An optimal policy among this larger class of policies forwards the nth item to the user only if θ ·Xn > c, and the expected total reward of this optimal policy is\nE [ N∑ n=1 (θ ·Xn − c)+ ] = γ 1− γ E [ (θ ·X1 − c)+ ] . (8)\nSince (8) is the supremum of the same objective as (2), but over a larger set of policies, it forms an upper bound. This style of analysis was also applied in (Chick & Frazier, 2012). In Section 4, we use the minimum of the computational upper bound in Theorem 1 and the hindsight upper bound (8) as our theoretical upper bound."
    }, {
      "heading" : "3.2. The DTD-DP policy",
      "text" : "The analysis in Section 3.1 provides a way to bound the performance of any policy, and is derived by decomposing\nthe original multi-feature problem into many single-feature subproblems. In this section, we build on this same idea to develop an implementable policy, called DTD-DP, and in Section 3.3 we build on this idea further to create a second implementable policy, called DTD-UCB.\nIn DTD-DP, as each item arrives, we consider the decomposition from Section 3.1 taking the incoming feature vector Xn and choosing a basis for which Xn is a unit vector in the basis. (This basis may change with each n.)\nWe then consider the decomposed problem studied in Section 3.1, in which we may make separate forwarding decisions for each direction in the basis, and compute the value of exploration corresponding to Xn in this decomposed problem.\nTo compute this value of exploration, we first compute the distribution of the magnitude x of the projection of future feature vector X along direction Xn, x = Xn·XXn·Xn , by using the distribution of future feature vectors f(X). Denote this distribution by G(x|Xn). We then solve the corresponding single-feature subproblem using (7) as described in Section 3.1.\nFrom this solution, we derive Q factors, Q(µ1,0, σ1,0, x0, 0) and Q(µ1,0, σ1,0, x0, 1) corresponding to the value of discarding and forwarding the current item in the single feature subproblem, given that the current feature vector has magnitude x0 = 1 and given that our current prior mean and variance for the subproblem are\nµ1,0 = Xn · µn, σ2j,0 = XnΣnX T n .\nWe then define the “exploration benefit”E(µ1,0, σ1,0) from forwarding the current item as the overall benefit of forwarding, minus the myopic benefit of forwarding µ1,0 − c and the benefit of discarding:\nE(µ1,0, σ1,0) =Q(µ1,0, σ1,0, 1, 1)− Q(µ1,0, σ1,0, 1, 0)− µ1,0 + c.\nIn DTD-DP, we add a scalar tuning parameter α, mirroring the tuning parameter used in UCB, to scale up or down the exploration benefit. The default value for α is α = 1. Then, returning to the original multi-dimensional problem, we consider the net benefit of forwarding to be the myopic benefitXn ·µn−c plus the exploration benefit αE(µ1,0, σ1,0), and forward when this is strictly positive. This is summarized in Algorithm 2."
    }, {
      "heading" : "3.3. The DTD-UCB algorithm",
      "text" : "In this section, we develop a second heuristic, DTD-UpperConfidence-Bound (DTD-UCB), which builds on the ideas underlying DTD-DP.\nAlgorithm 2 The DTD-DP algorithm for n = 1, 2, · · · do\nDenote µ1,0 = Xn · µn; Denote σ21,0 = XnΣnX T n ; Calculate Q(µ1,0, σ1,0, 1, U) for U = 0, 1 given that x ∼ G(x|Xn); Denote E(µ1,0, σ1,0) = Q(µ1,0, σ1,0, 1, 1) − Q(µ1,0, σ1,0, 1, 0)− µ1,0 + c; if µ1,0 + α · E(µ1,0, σ1,0) > c then\nForward the item else\nDiscard the item end if\nend for\nIn DTD-DP, we considered a single-feature subproblem in which the magnitude x of the projection of future feature vectors is given by G(x|Xn) and in which the prior mean and prior variance were given byXn ·µn andXnΣnXTn respectively. We then quantified the value of exploration by solving the single-feature subproblem using stochastic dynamic programming. In this single-feature subproblem, we observe that when future feature vectors are more closely aligned with Xn, so that samples from G(x|Xn) are large, we are more willing to explore.\nIn our second heuristic DTD-UCB, we take a similar approach, but quantify the value of exploration using an approach adopted from the literature on upper confidence bound policies, which quantifies the value of exploration in terms of some scalar multiple α of the standard deviation of the value of an action, obtained from calculating an upper confidence bound and subtracting the center of the confidence region. In DTD-UCB, we quantify the value of information similarly, but add an additional scaling factor to include the fact that those Xn whose G(x|Xn) have larger moments should induce more exploration.\nTo accomplish this, we let M(Xn) be the mean of the distribution G(x|Xn). This “mean of the projection” is\nM(Xn) = ∫ X Xn ·X Xn ·Xn f(X)dX.\nWe summarize the DTD-UCB algorithm in Algorithm 3."
    }, {
      "heading" : "4. Numerical Experiments",
      "text" : "In this section, we compare DTD-DP and DTD-UCB with three different benchmark algorithms and the computational upper bound from Section 3.1 using both real and simulated data. The benchmark algorithms are:\n• Pure Exploitation: Forward the item if Xn · µn ≥ c.\nAlgorithm 3 The DTD-UCB algorithm for n = 1, 2, · · · do\nif Xn · µn + α ·M(Xn) · √ XnΣnXn > c then\nForward the item else\nDiscard the item end if\nend for\n• Upper Confidence Bound (UCB): Forward the item if Xn · µn + α √ XnΣnXTn ≥ c.\n• Linear Thompson Sampling(LTS): For the nth item Xn, generate θ ∼ N(µn,Σn). Forward the item if θ ·Xn > c.\nFor DTD-DP, DTD-UCB and UCB, there is a tuning parameter α. In our simulation experiments we run these policies with 10 different values of α ranging from 0.1 to 10 on a log scale, and display the one with the best performance (which requires simulating performance for different values of α in a Monte Carlo simulation as a pre-processing step) in each instance.\nWe evaluate our upper bound and proposed policy on real and simulated data, and find that our upper bound is tight enough to be useful (the best policy evaluated is often within 60% of the upper bound and never below 30% of the upper bound), and as tight or tighter than many finite time bounds shown in the literature for Bayesian exploration problems, see, e.g., the 1/(1 − e) ≈ 63% and 1/4 = 25% approximation guarantees on page 3 of Golovin & Krause (2011)."
    }, {
      "heading" : "4.1. Yelp academic data",
      "text" : "In this section, we compare DTD-DP and DTD-UCB against benchmarks (UCB, pure exploitation and LTS) using the Yelp academic dataset (Yelp).\nOur items are businesses, and are described as belonging to one or more of the following six categories: Restaurants, Shopping, Food, Beauty and Spas, Health and Medical and Nightlife. The jth business object is then described by a 6-dimensional feature vector Xj = (x1,j , x2,j , · · · , x6,j) with the ith element xi,j = 1∑6\ni=1 xi,j if the business be-\nlongs to category i, and xi,j = 0 otherwise.\nWe calculate the prior distribution over new customers’ preferences using historical users’ reviews. For each historical user, we use linear regression to regress his reviews’ ratings on the feature vectors of the business objects that he reviewed. We use the estimated linear regression coefficients as his/her true user preference vector. Then we calculate the empirical distribution for all historical users,\nand set the prior on new users’ preference vectors to be multivariate normal with mean vector and covariance matrix equal to the sample mean and sample covariance of the historical users.\nIn Figure 1a, evaluation is done by taking a collection of real historical users, and for each estimating his true preference vector θ using linear regression on historical data. Evaluation is then performed for each algorithm and user by simulating feedback from the user’s held out θ on items forwarded by the algorithm, and an algorithm’s average performance is calculated by averaging across users. We must simulate user feedback given θ because we do not have historical relevance feedback from all users for all items, and so algorithms may present items that have not been rated. We plot the 95% confidence interval of cu-\nmulative reward over 100 items forwarded to the user with discount factor λ = 0.9.\nIn Figure 1b, we calculate the optimality gap between each heuristic algorithm and our computational upper bound. A smaller gap suggests the corresponding policy performs better in this problem instance.\nThe plot in Figure 1 summarizes the results. In this problem instance, DTD-UCB outperforms DTD-DP, UCB, pure exploitation and LTS, with DTD-DP and UCB performing almost identically. Moreover, the optimality gap is relatively small, which shows that DTD-UCB performs close to optimal."
    }, {
      "heading" : "4.2. arXiv.org Condensed Matter Dataset",
      "text" : "In this section, we compare DTD-DP and DTD-UCB with benchmarks using readership data from artices submitted in 2014 to the arXiv condensed matter category. We represent each paper submitted in 2014 by a 10 dimensional vector using Latent Dirichlet allocation (LDA) (Blei et al., 2003). For each user, the rating for a paper is 1 if he/she clicks and otherwise the rating is 0. We then calculate the user’s preference vector by linear regression. Similar to Section 4.1, we use the sample mean and sample variance of users’ preference vectors as our prior distribution parameters.\nIn our simulation, we use true users’ preference vectors calculated using linear regression, as we did in Section 4.1. For each user, we randomly pick 100 papers and make the forwarding decisions using different policies. We evaluate the cumulative reward for these 100 papers with discount factor λ = 0.9.\nThe result is summarized in Figure 2. The best of our heuristic policies in this example, DTD-UCB, outperforms all other heuristic policies. In this specific example, DTDDP does not perform as well as UCB but it outperforms pure exploitation and LTS."
    }, {
      "heading" : "4.3. Simulated Data",
      "text" : "In this section, we compare the performance of DTD-DP and DTD-UCB with three benchmark algorithms, as well as our computational upper bound on simulated data. This simulated data is chosen to give insight into situations where UCB can underperform, and where the structure of a policy like DTD-DP and DTD-UCB are needed to provide near-optimal performance. We emphasize that it is chosen to provide insight, and not to show performance on a typ-\nical real problem instance — we refer this comparison to Section 4.1 and Section 4.2.\nEach item is described by a 100-dimensional feature vector Xn with the following distribution: P (Xn = e1) = 100199 , P (Xn = ei) = 1 199 for i = 2, · · · , 100. Here, ex is the unit vector in the xth dimension. The initial belief on the user’s preference for each feature is N(0.3, 1.0) with independence across features. We set γ = 0.9 and λ = 0.1. In estimating the infinite-horizon discounted sum (2), we truncate after n = 100.\nThe results, summarized in Figure 3, show that DTD-DP and DTD-UCB outperform UCB, pure exploitation and LTS. In most cases, UCB performs very well with a properly chosen α. Moreover, DTD-DP and DTD-UCB outperform UCB for several values of the forwarding cost, and nearly coincides with the theoretical upper bound for all values of the forwarding cost, which shows that it is indistinguishable from optimal in this problem instance.\nLTS does not perform well in this example because it performs poorly at the initial stages and the (discounted) reward in the later stages cannot make up for the loss at the early stages. As (Russo & Roy, 2014a) and (Russo & Roy, 2014b) pointed out, LTS generally underperforms tuned UCB.\nUCB underperforms DTD-DP and DTD-UCB in this example because it cannot account for the frequency with which a feature appears, and thus cannot adjust its level of exploration (encoded as the choice of α) to explore more those features that tend to reoccur frequently, and explore less those features that are unlikely to appear again. In contrast, both DTD-DP and DTD-UCB can adjust its level of exploration, and will explore more those features that will\nreoccur."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We studied the Bayesian linear information filtering problem, providing an instance-specific computational upper bound and a pair of new Decompose-Then-Decide heuristic policies, DTD-DP and DTD-UCB. Numerical experiments show that the best of these two policies is typically close to the computational upper bound and outperforms several benchmarks on real and simulated data."
    }, {
      "heading" : "6. Proof of Equation (2)",
      "text" : "Lemma 1. For each policy π ∈ Π, we have\nEπ [ N∑ n=1 Un(Yn − c) ] = γEπ [ ∞∑ n=1 γn−1Un(Yn − c) ] (9)\nwhere γ = ΓΓ+r .\nProof. The proof is the same as in (Zhao & Frazier, 2014) and we cite the proof as follows. Since\nEπ [ N∑ n=1 Un(Yn − c) ]\n=Eπ [ ∞∑ n=1 1{n≤N}Un(Yn − c) ]\n= ∞∑ n=1 Eπ [ 1{n≤N}Un(Yn − c) ] , (10)\nwhere the last equality is due to Fubini’s theorem and Eπ[ ∑∞ n=1 |1{n≤N}Un(Yn − c)|] <∞.\nFor any fixed n, we have\nEπ[Eπ[1{n≤N}Un(Yn − c)|Un, Yn]] =Eπ[Pπ(n ≤ N |Un, Yn)Un(Yn − c)] =Eπ[γnUn(Yn − c)].\nThus, plugging this expression in (10) and applying Fubini’s theorem again shows\nEπ [ N∑ n=1 Un(Yn − c) ]\n=Eπ [ ∞∑ n=1 γnUn(Yn − c) ]\n=γEπ [ ∞∑ n=1 γn−1Un(Yn − c) ] ,\nwhich is what we need."
    } ],
    "references" : [ {
      "title" : "Thompson sampling for contextual bandits with linear payoffs",
      "author" : [ "Agrawal", "Shipra", "Goyal", "Navin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Agrawal et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2013
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "Blei", "David", "Ng", "Andrew", "Jordan", "Michael" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Information relaxations and duality in stochastic dynamic programs",
      "author" : [ "Brown", "David", "Smith", "James", "Sun", "Peng" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Brown et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2010
    }, {
      "title" : "An optimal algorithm for linear bandits",
      "author" : [ "Cesa-Bianchi", "Nicolo", "Kakade", "Sham" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Sequential sampling with economics of selection procedures",
      "author" : [ "Chick", "Stephen", "Frazier", "Peter" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Chick et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chick et al\\.",
      "year" : 2012
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "Chu", "Wei", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Chu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2011
    }, {
      "title" : "Personalized information delivery: An analysis of information filtering methods",
      "author" : [ "Foltz", "Peter W", "Dumais", "Susan T" ],
      "venue" : "In Communications of the ACM,",
      "citeRegEx" : "Foltz et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Foltz et al\\.",
      "year" : 1992
    }, {
      "title" : "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
      "author" : [ "Golovin", "Daniel", "Krause", "Andreas" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Golovin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Golovin et al\\.",
      "year" : 2011
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "Herbert", "Robbins" ],
      "venue" : "Bulletin of the American Mathematical Society,",
      "citeRegEx" : "Herbert and Robbins.,? \\Q1952\\E",
      "shortCiteRegEx" : "Herbert and Robbins.",
      "year" : 1952
    }, {
      "title" : "A survey of algorithmic methods for partially observed markov decision processes",
      "author" : [ "Lovejoy", "William S" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "Lovejoy and S.,? \\Q1991\\E",
      "shortCiteRegEx" : "Lovejoy and S.",
      "year" : 1991
    }, {
      "title" : "Optimistic bayesian sampling in contextual-bandit problems",
      "author" : [ "May", "Benedict C", "Korda", "Nathan", "Lee", "Anthony", "Leslie", "David S" ],
      "venue" : "In The Journal of Machine Learning Research,",
      "citeRegEx" : "May et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "May et al\\.",
      "year" : 2012
    }, {
      "title" : "A survey of partially observable markov decision processes: Theory, models, and algorithms",
      "author" : [ "Monahan", "George" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Monahan and George.,? \\Q1982\\E",
      "shortCiteRegEx" : "Monahan and George.",
      "year" : 1982
    }, {
      "title" : "Linearly parameterized bandits",
      "author" : [ "Rusmevichientong", "Paat", "Tsitsiklis", "John N" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Rusmevichientong et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rusmevichientong et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning to optimize via information-directed sampling",
      "author" : [ "Russo", "Daniel", "Roy", "Benjamin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Russo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russo et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to optimize via posterior sampling",
      "author" : [ "Russo", "Daniel", "Roy", "Benjamin" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Russo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russo et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Exploration and exploitation in adaptive filtering based on bayesian active learning",
      "author" : [ "Zhang", "Yi", "Xu", "Wei", "Callan", "Jamie" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2003
    }, {
      "title" : "Exploration vs. exploitation in the information filtering problem",
      "author" : [ "Zhao", "Xiaoting", "Frazier", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2014
    }, {
      "title" : "The proof is the same as in (Zhao",
      "author" : [ "Γ+r . Proof" ],
      "venue" : null,
      "citeRegEx" : "Proof.,? \\Q2014\\E",
      "shortCiteRegEx" : "Proof.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "(Zhang et al., 2003) studies a Bayesian decision-theoretic version of this problem in which a univariate score is observed for each item, and relevance is related to this score via logistic regression.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "Indeed, the information filtering problem we study can be seen as a special case of the (Bayesian) contextual linear multi-armed bandit problem (Agrawal & Goyal, 2013; Chu et al., 2011; May et al., 2012; Cesa-Bianchi & Kakade, 2011).",
      "startOffset" : 144,
      "endOffset" : 232
    }, {
      "referenceID" : 10,
      "context" : "Indeed, the information filtering problem we study can be seen as a special case of the (Bayesian) contextual linear multi-armed bandit problem (Agrawal & Goyal, 2013; Chu et al., 2011; May et al., 2012; Cesa-Bianchi & Kakade, 2011).",
      "startOffset" : 144,
      "endOffset" : 232
    }, {
      "referenceID" : 2,
      "context" : "This upper bound is based on the idea of dividing (1) into k different “singlefeature” subproblems, then performing an information relaxation (similar in spirit to (Brown et al., 2010)) in which we give the policy assigned to each single-feature subproblem additional information, which allows us to compute their value efficiently.",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 1,
      "context" : "We represent each paper submitted in 2014 by a 10 dimensional vector using Latent Dirichlet allocation (LDA) (Blei et al., 2003).",
      "startOffset" : 109,
      "endOffset" : 128
    } ],
    "year" : 2017,
    "abstractText" : "We present a Bayesian sequential decisionmaking formulation of the information filtering problem, in which an algorithm presents items (news articles, scientific papers, tweets) arriving in a stream, and learns relevance from user feedback on presented items. We model user preferences using a Bayesian linear model, similar in spirit to a Bayesian linear bandit. We compute a computational upper bound on the value of the optimal policy, which allows computing an optimality gap for implementable policies. We then use this analysis as motivation in introducing a pair of new Decompose-Then-Decide (DTD) heuristic policies, DTD-Dynamic-Programming (DTD-DP) and DTD-Upper-Confidence-Bound (DTD-UCB). We compare DTD-DP and DTDUCB against several benchmarks on real and simulated data, demonstrating significant improvement, and show that the achieved performance is close to the upper bound.",
    "creator" : "LaTeX with hyperref package"
  }
}