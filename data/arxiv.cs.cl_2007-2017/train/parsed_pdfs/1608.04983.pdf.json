{
  "name" : "1608.04983.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Ensemble of Jointly Trained Deep Neural Network-Based Acoustic Models for Reverberant Speech Recognition",
    "authors" : [ "Jeehye Lee", "Myungin Lee" ],
    "emails" : [ "jchang@hanyang.ac.kr)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Reverberant speech recognition, deep neural network, joint training, ensemble acoustic model\nI. INTRODUCTION\nS IGNALS originating from the same speech source usuallyappear differently due to a variety of acoustic reverberation effects. In speech recognition, these acoustic effects result in mismatches between trained speech recognition models and input speech. Two general approaches for reducing acoustic mismatch include feature mapping and model adaptation. In feature mapping techniques, input signal waveforms or feature vectors are converted to their enhanced versions before being fed into a recognizer. This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context. The first approach, termed linear filtering, dereverberates in time or transform domains, while spectral enhancement dereverberates corrupted power spectra. Additionally, feature enhancement aims to directly remove the effect of reverberation from corrupted feature vectors. Recently, deep neural networks (DNNs) have been used for matching reverberant speech to its anechoic version [9]-[14]. The mapper is trained with an architecture of multiple-layers, where the input is the spectral\nJ. Lee, M. Lee, and J.-H. Chang are with Hanyang University, Seoul, 04763, Korea (e-mail : jchang@hanyang.ac.kr).\nrepresentation of reverberant speech and the desired output is that of anechoic speech. Hence, the feature mapping design can be treated as a problem during system identification, with a set of input and corresponding output feature vector sequences. Notice that this approach is still confined to the front-end of the speech recognition system, and thus the recognition results do not affect the design of the DNN-based feature enhancement. Another possible approach to reduce this mismatch is to train the acoustic model by using far more data under various reverberant conditions, which adjusts the speech recognition model parameters to more closely fit the input speech signal. This approach is termed the back-endbased speech recognition system, which includes a hidden Markov model (HMM) adaptation, such as the maximum likelihood linear regression (MLLR) [15] and the maximum a posteriori (MAP) adaptation [16]. These techniques can also be used to mitigate the mismatch between clean HMMs and reverberant data, but their recognition performance in reverberant environments is often insufficient.\nIn recent years, acoustic modeling for the emission distribution of HMMs in speech recognition has been successfully replaced by neural networks with multiple-layers, which are generatively pre-trained without the use of discriminative information. Once generative pre-training is performed, discriminative fine-tuning (using back-propagation) adjusts the weights to improve their ability to predict a probability distribution over the states of monophone HMMs. More recently, neural network has been used to jointly train a single DNN for both feature mapping and acoustic modeling in noisy environments [14], [17], [18]. Instead of extracting acoustic features from enhanced speech, DNNs are employed as highly nonlinear mapping functions for the estimated clean speech features obtained from noisy speech. Then, a hybrid DNN architecture is designed in order to jointly train DNNs for both feature mapping and acoustic modeling. Consequently, the output layer of feature mapping becomes the input layer for acoustic modeling. Thus, joint training enables error backpropagation up to the feature mapping layer. To summarize the previous studies: 1. Can we handle a wide range of sophisticated reverberation in real-world situations, using a single DNN-based acoustic model? 2. No joint training techniques incorporating both feature mapping for dereverberation and acoustic modeling have been developed.\nTo answer the first question, it is worth mentioning the\nar X\niv :1\n60 8.\n04 98\n3v 1\n[ cs\n.C L\n] 1\n7 A\nug 2\n01 6\nneural network ensemble, which builds a set of separately trained neural networks and combines the sets to form the unified prediction model [19]-[27]. Each trained neural network in the ensemble can serve a different role in modeling and can be applied to various deep learning systems to achieve greater recognition accuracy. Indeed, there have been efforts to build ensemble acoustic models (EAMs) by using neural networks for speech recognition. These techniques have generally exploited the sensitivity of acoustic models in order to yield inter-model diversity.\nMotivated by insights mentioned above, we propose an ensemble of DNN acoustic models under a variety of reverberant conditions. Toward this end, we first split the reverberant data into multiple cases according to reverberation time 60 (RT60) in a training step. Next, one acoustic model is trained in a conventional manner for each RT60 range, where RT60 is one of the main parameters describing the reverberant degree. Once the EAMs using DNNs are trained, two types of DNNs are chosen in a probabilistic manner. This is done in a test step for which the probability is found in an independent module based on MAP criteria. As for the MAP criteria, maximum likelihood (ML) estimation is used for the blind estimation of RT60 [28], [29], which is based on a statistical model for representing sound decay under reverberant conditions. Indeed, the posterior probability outputs from two acoustic models are combined using a probabilistic average, and this combination yields superior performance when compared to a single acoustic model. In addition, inspired by the joint training technique [14], we propose jointly training each model in the ensemble for both feature mapping and acoustic modeling, which enables us to use the back-propagation algorithm up to the feature mapping layer. As a result, the feature mapping network is closely refined to the specified acoustic model in each network of ensemble models by connecting the output layer of the feature mapping DNN to the input layer of the DNN acoustic models. Each jointly trained DNN acoustic model is used again to develop the EAM, which is then selected in the same manner as the RT60-based MAP criteria.\nThe remainder of this paper is organized as follows. In the next section, we review the conventional methods for robust speech recognition. In Section III, we describe the design of the proposed ensemble joint acoustic model (EJAM) using ML and a model-selecting method. Extensive evaluation of the proposed method is discussed in Section IV, and conclusions are presented in Section V."
    }, {
      "heading" : "II. REVIEW OF RELATED WORK",
      "text" : "We begin with a brief description of related work to provide sufficient background for understanding our approach. In the ensemble model, we first elaborate on the design of the ensemble classifier. In the second subsection, DNN joint training is introduced."
    }, {
      "heading" : "A. Neural network ensemble",
      "text" : "The ensemble classifier is known to be an effective approach for enhancing the recognition accuracy [21]-[23] for which individual models are independently designed, and the\ndecoding word hypotheses of multiple models are combined to score the speech frames. In a recent study, multiple datasets were generated through normalized noisy features by which beamforming and speech enhancement techniques are used, and additional speaker related features as well as other auxiliary features are also included [23]. One acoustic model is trained from each dataset, which builds up the acoustic model ensemble, as illustrated in Fig. 1. Then, the EAM outputs are combined via a simple posterior strategy, in which the output probability for the triphone HMM state k is obtained as the average of the state posterior probabilities p(kn | x) of the nth acoustic model (n = 1, 2, . . . , N ) as given by:\np(kEAM | x) = 1\nN N∑ n=1 p(kn | x) (1)\nwhere x is an input feature vector. The state posterior probabilities p(kEAM | x) are utilized in decoding searches. In averaging posterior probabilities, simple averaging methods [21], [23] and weighted averaging method [22] have been presented. It is worth noting that the recognition performance of the combined model is enhanced when models are used from a variety of different sources. For a more detailed discussion of the EAM, please refer to [23].\nB. Joint training of DNNs\nInstead of extracting acoustic features from the speech waveform, the DNN is used as a nonlinear feature mapping function. Then, the joint training method is developed by integrating the DNN for feature mapping and the DNN for\nacoustic modeling, and subsequently jointly training the integrated DNN, as shown in Fig. 2. In feature mapping, the DNN is designed to estimate the clean speech feature from a noisy feature, for which minimizing the mean squared error (MSE) between the DNN output and reference target clean speech feature can be described as follows:\nE = 1\nN N∑ n=1 ‖x̂n+τn−τ (yn+τn−τ ,W,b)− xn+τn−τ‖22 + κ‖W‖22 (2)\nwhere x̂n+τn−τ and x n+τ n−τ are the D(2τ +1)-dimensional vectors of the estimated and reference clean features for the nth frame, respectively. yn+τn−τ is a D(2τ+1)-dimensional vectors of input noisy features with the neighboring left and right τ frames as the acoustic context. Additionally, W and b denote the weight and bias parameters, respectively, with κ representing the regularization weighting coefficient and N denoting the mini-batch frame size. The acoustic model is then trained by using the enhanced features obtained from feature mapping and acoustic modeling DNN layers are stacked on top of the feature mapping layer. Naturally, the output layer of feature mapping becomes the input layer for acoustic modeling. Then, the error back-propagation algorithm is utilized in order to jointly train the single DNN, which includes both feature mapping and acoustic modeling. Consequently, the jointly trained DNN is advantageous, in that feature mapping is refined to acoustic modeling (and vice versa). This seamless connection between two DNNs permits high levels of recognition accuracy."
    }, {
      "heading" : "III. PROPOSED ENSEMBLE JOINT ACOUSTIC MODEL FOR REVERBERANT SPEECH RECOGNITION",
      "text" : "In this section, we introduce the proposed algorithm for the DNN ensemble model and ensemble of jointly trained DNN models. We then present the manner in which to combine the\nacoustic model ensemble, for which the blind estimation of RT60 is described."
    }, {
      "heading" : "A. Ensemble of DNN models",
      "text" : "In short, we design seven different classes of neural networks for acoustic modeling, each of which quantifies a unique reverberant condition. After filterbank feature extraction from reverberant speech, features are divided into multiple training sets. For this, the reverberant condition is divided into seven points, from RT60 = 0.3 s to RT60 = 0.9 s, in increments of 0.1 s. From the dataset for each point, the DNN-based EAM is separately established using the pre-training and fine-tuning techniques as described in the previous section. As a result, the EAM consists of seven different DNNs, as shown in Fig. 3. In a test step, the posterior weighted averaging technique is then applied to combine the results of the DNN ensemble. If we assume that N acoustic models are generated from N multiple datasets, the output probability at the nth acoustic model is defined by pn = p(kn | x), where k denotes the HMM states. For the EAM, the final posterior probability is computed with m1,m2 ∈ {n | 1, 2, . . . , N} as P (kEAM | x) = wm1p(km1 | x) + wm2p(km2 | x), (3)\nby fusing the two weighted outputs from the two most likely DNNs among seven different DNNs ensemble. The weights are determined by the MAP probability computation, which is given by the blind ML estimation of RT60 [29].\nSpecifically, to determine weights, we first need to blindly estimate RT60, a core parameter for characterizing reverberant environments. Blind estimation, which implies estimation of RT60, is performed without prior knowledge of speech sources or room geometry. Indeed, the diffuse tail of reverberation is instead mathematically modeled as a simplified noise decay curve [29]:\nd(k) = Arv(k)e −ρkTs (k) (4)\nwhere Ar, ρ, and (k) are the real amplitude, decay rate, and unit step sequence, respectively. Additionally, Ts denotes the sampling period, and v(k) is a sequence of random variables. Since the sequence d(k) for k ∈ {0, 1, . . . , N −1} is modeled by N independent random variables, we can find an ML estimator of RT60. Thus, the log-likelihood function can be expressed as:\nL(ρ) =\n− N 2\n( (N − 1) ln(a) + ln (2π N N−1∑ i=1 a−2id2(i) ) + 1 ) (5)\nwhere a = e−Tsρ. Then, the decay rate ρ is estimated based on the ML, as given by:\nρ̂(ML) = max ρ {L(ρ)} (6)\nThen, ρ̂(ML) is transformed into RT60 as follows:\nRT60 = 3 ρ log10 e ≈ 6.908 ρ . (7)\nNote that we use a downsampling operation and simple preselection of possible sound decays in order to reduce the computational complexity and increase the online estimation speed. Later, if the estimated RT60 falls to one of the seven points between 0.3 s and 0.9 s, the two most likely DNNs (m1 and m2) are chosen as the candidates to be combined as in (3). Other DNN models are ignored to avoid the unwanted effect of outliers. For this, two weights are determined by the ratio of likelihoods, both computed as in (5) at the two most likely RT60 points, given by:\nwm1 = L(ρm1)\nL(ρm1) + L(ρm2) (8)\nand\nwm2 = L(ρm2)\nL(ρm1) + L(ρm2) . (9)"
    }, {
      "heading" : "B. Ensemble of jointly trained DNN models",
      "text" : "In this subsection, we present the jointly trained DNN for the ensemble model by applying both feature mapping and acoustic modeling. Thus, the EJAM in this approach deals with reverberant speech at seven RT60 points, as in the previous EAM, which can be described as a convolution of clean speech with the room impulse response (RIR) at each RT60 point. The reverberant speech is employed as an input for feature mapping, for which the reverberant filterbank features are converted to the target clean filterbank features. Thus, design of the feature mapping rule is regarded as a problem of system identification, with a set of input and corresponding output feature vector sequences. Estimation of DNN model parameters relevant to feature mapping is performed based on three hidden layers with 2048 units in each layer by training to minimize the MSE function between the input and output. A total of seven DNN models are established from RT60 = 0.3 s to RT60 = 0.9 s in increments of 0.1 s, as in the previous section. Since feature mapping can solve the problem of system identification, an example of feature mapping is shown in Fig. 4, which includes the filterbank features of clean speech and reverberant speech with RT60 = 0.3 s, 0.5 s, 0.7 s, and 0.9 s. Note that the prominent distinction between dereverberant speech and reverberant speech can be seen in the smearing of word boundaries.\nOnce the training for feature mapping is completed, the acoustic modeling layer at each RT60 point is independently trained using the output of the corresponding feature mapping layer. As shown in Fig. 2, the acoustic modeling DNN, which includes seven hidden layers with 2048 hidden units in each\nlayer, is directly stacked on top of the feature mapping DNN. The combined ensemble DNN is jointly trained to minimize the total cross entropy error function by using the error backpropagation algorithm, as shown in Fig. 5. What remains is how to select the DNN from the EJAM, established on each RT60 point. Again, after the integration step for joint training is completed, the two joint DNNs that maximize the likelihood at each time are identified. Similar to the previous ensemble model, the ML-based weights are employed to achieve the final acoustic score P (kEJAM | x) for each feature x, as in (3)."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "This section describes the performance evaluation of the proposed EAM and EJAM for distant speech recognition. In order to assess the proposed method, we present a number of experiments performed in various reverberant environments. Moreover, the proposed method was compared with a conventional single DNN acoustic model and a single DNN acoustic model, which set the topological complexity equivalent to that of EAM [30]."
    }, {
      "heading" : "A. Experimental Setup",
      "text" : "The proposed method was evaluated on TIMIT DB [31], which was first applied to the RIR generator [32] in order to\nsimulate reverberant environments. As shown in Fig. 6, we generated a simulated room with size 5 × 3 × 2.5 m3, and varied reflection coefficients to yield specific RT60 conditions. The distance between the microphone and source was set to 50 cm in order to avoid close-talking scenarios. In order to simulate the system, reverberation times were increased from 0.3 s to 0.9 s, which correspond to specific RT60 conditions. In time, 3696 utterances from TIMIT were employed to form the training set in each RT60 condition. As a result, the number of utterances used for training was 3320× 7 = 23240, a number\nalso used in building background models. The development set included 376 utterances mixed with seven RT60s for cross validation. It is worth noting that the test set was prepared to include 192 reverberant sentences, corresponding to seventy RT60s (0.3-0.9 s, increment of 0.01 s). Note that no overlap was permitted in utterances in training, development, or test sets. In the case of speech features, 72 components were generated, including 24-dimensional filterbank features and their first-order and second-order derivatives. Then, feature analysis was performed from each frame of 25 ms, with a 10 ms frame shift for the 16 kHz speech waveforms. Triphone modeling was used for HMMs, each of which was modeled by three emitting states. The number of tied states was 2021. We used a phone-based bigram language model estimated from training utterances. Training of HMM parameters and decoding for speech recognition was carried out using the Kaldi software [33]."
    }, {
      "heading" : "B. Speech recognition results of DNN-based ensemble models",
      "text" : "We evaluated the performance of the proposed EAM algorithm in terms of the phone error rate (PER) compared to the single DNN background model (SBM) [30]. An SBM whose computational complexity was the same as that of the single network of the EAM and which consists of seven hidden layers, was designed. Further, for fair comparison, we developed an extended SBM (eSBM) with the same computational complexity as the EAM, and the eSBM consisted of 10 hidden layers with 3990 units. Additionally, two background models, EAM and EJAM, were trained using data recorded from 0.3 s to 0.9 s, for which a 792-dimensional feature vector consisting of 11 frames of 24-dimensional filterbank features with delta and delta-delta was used in the input layer. In the target layer, 2021 tied states were used, and the learning rate was 0.0001 with a value of 0.9 for momentum. In jointly training\nthe EJAM, a 792-dimensional feature vector was used with three hidden layers, without the softmax output layer. Each of the hidden layers had 2048 units. The target of the feature mapping DNN was a 792-dimensional feature vector of clean speech, and the learning rate and momentum were initially set to 0.0000001 and 0.9, respectively. All DNNs, including the SBM, eSBM, EAM, and EJAM, were initialized using restricted Boltzmann machine (RBM) pre-training followed by fine-tuning using the error back-propagation algorithm (cross entropy for acoustic modeling and MSE for feature mapping). The mini-batch size was set to be 512 for the stochastic gradient algorithm in the error back-propagation algorithm.\nIn the test step, final acoustic scores were measured for the EAM and EJAM, and were compared to scores of the SBM and eSBM (Figs. 3, 5). The proposed ensemble models yielded better performance than the single DNN-based background models in matched test conditions (Table I). From this, the single matched model in the proposed ensemble network, including the EAM and EJAM, were superior to the single DNN-based background models for a given RT60 condition (Fig. 7). It is also noteworthy that the eSBM yielded a slightly worse performance than the SBM, particularly for high values of RT60. This indicates that deeper layers (e.g. eSBM) over the SBM do not exhibit an advantage if the different topological network structures are not presented in high reverberant conditions, unlike our presented ensemble models.\nIn addition, we examined the performance of the EAM and EJAM in an online test condition. For this, the RT60 was blindly estimated at runtime in an ML fashion in order to find the weights for the ensemble structure. This evaluation was performed for RT60 values between 0.3 s and 0.9 s in increments of 0.01 s. ML-based estimates are displayed in Fig. 8, and we adopted the parameters used in [29]. The RT60 estimates of test speech at each RT60 condition are plotted in Fig. 8. The estimated ML of each RT60 condition did not yield drastic changes; therefore, we used the average of the ML estimates over frames for each utterance. For example, as shown in Fig. 8(c), when the utterance was generated in the RT60 = 0.67 s condition, the two most likely RT60 conditions (RT60 = 0.6 s and RT60 = 0.7 s) were selected by two types of largest ML. Further, the ML estimate for RT60 = 0.7 s was larger than the ML estimate for RT60 = 0.6 s, so the weight of RT60 = 0.7 s became larger than the weight of RT60 = 0.6 s. Additionally, it appears that two weights computed by ML estimates, as in (8) and (9), are changed over the test utterance. This makes it possible to use this estimator in an online scenario. It is evident from Fig. 7 that the EAM achieved relative PER reductions of 8.44% over the SBM and 12.37% over the eSBM. It is notable that the eSBM does not yield improved performance when compared to the EAM, though it utilized the same amount of computational complexity. It is clear that the proposed ensemble methods have stronger generalization abilities at runtime conditions, made possible by choosing the best matched models with enhanced sensitivity.\nAs for the comparison between the EAM and EJAM, it is\nclear that the EJAM is superior to the EAM when the reverberant time is normal (RT60 ≤ 0.5 s). However, when acoustic reverberation is severe (RT60 0.5 s), the EJAM performed slightly worse than the EAM. One possible explanation is that the presented DNN-based feature mapping is not good at representing very long reverberation, as it requires a long-term feature vector as an input of the DNN in order to characterize the long-term evolution of reverberant speech. Future work may benefit from additional features, which might be good at reflecting the long-term pattern of reverberant speech. To summarize, the prepared ensemble models are effective for distant speech recognition in various reverberant conditions. Moreover, performance is improved when using the EJAM in normal reverberant environments."
    }, {
      "heading" : "V. CONCLUSIONS",
      "text" : "To the best of our knowledge, this study is the first to use an ensemble model for speech recognition under reverberant conditions. The ensemble structure was initially built where each RT60 in a specific range served as a single DNNbased acoustic modeling. Then, the two most likely DNNs from the ensemble model are chosen based on the ML blind estimation for RT60. The EAM is further enhanced by jointly training the acoustic and feature models, which serve as the dereverberation. From the experimental results, the DNNbased ensemble structure for acoustic modeling was superior to the single DNN-based background model. In particular, the blind ML estimation for RT60 was successfully responsible for choosing the most likely DNNs in the ensemble model. The design of the EJAM permitting improved performance in normal reverberant conditions was verified in terms of speech recognition accuracy."
    } ],
    "references" : [ {
      "title" : "Packet loss concealment based on deep neural networks for digital speech transmission",
      "author" : [ "B.-K. Lee", "J.-H. Chang" ],
      "venue" : "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 2, pp. 378-387, Feb. 2016.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "EVAM: An eigenvector-based algorithm for multichannel blind deconvolution of input colored signals",
      "author" : [ "M.I. Gurelli", "C.L. Nikias" ],
      "venue" : "IEEE Trans. Signal Process., vol. 43, no. 1, pp. 134-149, Jan. 1995.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A two-stage algorithm for one-microphoe reverberant speech enhancement",
      "author" : [ "M. Wu", "D. Wang" ],
      "venue" : "IEEE Trans. Audio, Speech, Language Process., vol. 14, no. 3, pp. 774-784, May. 2006.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Supression of late reverberation effect on speech signal using long-term multiple-step linear prediction",
      "author" : [ "K. Kinoshita", "M. Delcroix", "T. Nakatani", "M. Mioshi" ],
      "venue" : "IEEE Trans. Audio, Speech, Language Process., vol. 17, no. 4, pp. 534-545, May. 2009.  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING  8 (a) ML (RT60 = 0.61 s)  (b) ML (RT60 = 0.63 s) (c) ML (RT60 = 0.67 s) Fig. 8. An example of the ML of the estimated RT",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Single- and multi-microphone speech dereverberation using spectral enhancement",
      "author" : [ "E.A.P. Habets" ],
      "venue" : "Ph.D. dissertation, Technische Univ. Eindhoven, Eindhoven, The Netherlands, 2007.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On the use of lime dereverberation algorithm in an acoustic environment with a noise source",
      "author" : [ "M. Delcroix", "T. Hikichi", "M. Miyoshi" ],
      "venue" : "Proc. ICASSP, 2006, pp. 825-828.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Cepstral channel normalization techniques for HMM-based speaker verification",
      "author" : [ "A. Rosenberg", "C.-H. Lee", "F. Soong" ],
      "venue" : "Proc. ICSLP, 1994, pp. 1835-1838.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Environmentally robust ASR front-end for deep neural network acoustic models",
      "author" : [ "T. yoshioka", "M.J.F. Gales" ],
      "venue" : "Comput. Speech Language, vol. 31, no. 1, pp. 65-86, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep neural network-based bottleneck feature and denoising autoencoder-based dereverberation for distant-talking speaker identification",
      "author" : [ "Z. Zhang", "L. Wang", "A. Kai", "T. Yamada", "W. Li", "M. Iwahashi" ],
      "venue" : "EURASIP J. Audio, Speech, Music. Process., pp. 1-13, May. 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation",
      "author" : [ "X. Xiao", "S. Zhao", "D. Nguyen", "X. Zhong", "D.L. Jones", "E. Chng", "H. Li" ],
      "venue" : "EURASIP J. Advances in Signal Process., pp. 1-18, Dec. 2016.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Exploring deep neural networks and deep autoencoders in reverberant speech recognition",
      "author" : [ "M Mimura", "S Sakai", "T Kawahara" ],
      "venue" : "Proc. HSCMA, 2014, pp. 197-201.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning spectral mapping for speech dereverberation and denoising",
      "author" : [ "K. Han", "Y. Wang", "D. Wang", "W.S. Woods", "I. Merks", "T. Zhang" ],
      "venue" : "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 23, no. 6, pp. 982-992, Jun. 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Joint training of front-end and back-end deep neural networks for robust speech recognition",
      "author" : [ "T. Gao", "J. Du", "L.-R. Dai", "C.-H. Lee" ],
      "venue" : "Proc. ICASSP, 2015, pp. 4375-4379.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models",
      "author" : [ "C.J. Legetter", "P.C. Woodland" ],
      "venue" : "Comput. Speech Language, vol. 9, no. 2, pp. 171-185, 1995.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Maximum a posteriori estimation for mul-  tivariate Gaussian mixture observation of Markov chains",
      "author" : [ "J. Gauvain", "C.-H. Lee" ],
      "venue" : "IEEE Trans. Speech Audio Process., vol. 2, no. 2, pp. 291-298, 1994.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "A universal VAD based on jointly trained deep neural networks",
      "author" : [ "Q. Wang", "J. Du", "X. Bao", "Z.-R. Wang", "L.-R. Dai", "C.-H. Lee" ],
      "venue" : "Proc. Interspeech, 2015, pp. 2282-2286.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Improving robustness of deep neural network acoustic models via speech separation and joint adaptive training",
      "author" : [ "A. Narayann", "D. Wang" ],
      "venue" : "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 23, no. 1, pp. 92-101, Jan. 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Random forests of phonetic decision trees for acoustic modeling in conversational speech recognition",
      "author" : [ "J. Xue", "Y. Zhao" ],
      "venue" : "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 3, pp. 519-528, Mar. 2008.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Ensemble methods in machine learning",
      "author" : [ "T.G. Dietterich" ],
      "venue" : "Proc. MCS, 2000, pp. 1-15.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Building an Ensemble of CD-DNN- HMM acoustic model using random forests of phonetic decision trees",
      "author" : [ "T. Zhao", "Y. Zhao", "X. Chen" ],
      "venue" : "Proc. ISCLP, 2014, pp. 98-102.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Building acoustic model ensembles by data sampling with enhanced trainings and features",
      "author" : [ "X. Chen", "Y. Zhao" ],
      "venue" : "IEEE Trans. Audio, Speech, Language Process., vol. 21, no. 3, pp. 498-507, Mar. 2013.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An information fusion approach to recognizing microphone array speech in the CHIME-3 challenge based on a deep learning framework",
      "author" : [ "J. Du", "Q. Wang", "Y.-H. Tu", "X. Biao", "L.-R. Dai", "C.-H. Lee" ],
      "venue" : "Proc. ASRU, 2015, pp. 430-435.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Ensemble deep learning for speech recognition.",
      "author" : [ "L. Deng", "J.C. Platt" ],
      "venue" : "Proc. Interspeech,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Theoretical analysis of diversity in an ensemble of automatic speech recognition systems",
      "author" : [ "K. Audhkhasi", "A.M. Zavou", "P.G. Georgiou", "S.S. Narayanan" ],
      "venue" : "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 22, no. 3, pp. 711-726, Mar. 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Framewise speech-nonspeech classification by neural networks for voice activity detection with statistical noise supression",
      "author" : [ "Y. Obuchi" ],
      "venue" : "Proc. ICASSP, 2016, pp. 5715-5719.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Ensemble of deep neural networks using acoustic environment classification for statistical modelbased voice activity detection",
      "author" : [ "I. Hwang", "H.-M. Park", "J.-H. Chang" ],
      "venue" : "Comput. Speech Language, vol. 38, pp. 1-12, Jul. 2016.  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING  9",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Blind estimation of reverberation time",
      "author" : [ "R. Ratnam", "D.L. Jones", "B.C. Wheeler", "W.D. OBrien", "C.R. Lansing", "A.S. Feng" ],
      "venue" : "J. Acoust. Soc. Amer., vol. 114, no. 5, pp. 2877-2892, Nov. 2003.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "An improved algorithm for blind reverberation time estimation",
      "author" : [ "H.W. Lollmann", "E. Yilmaz", "M. Jeub", "P. Vary" ],
      "venue" : "Proc. IWAENC, 2010, pp. 1-4.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An investigation of deep neural networks for noise robust speech recognition",
      "author" : [ "M.L. Seltzer", "D. Yu", "Y. Wang" ],
      "venue" : "Proc. ICASSP, 2013, pp. 7398-7402.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "TIMIT acoustic phonetic continuous speech corpus",
      "author" : [ "J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren" ],
      "venue" : "Proc. Linguistic Data Consortium, 1993.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Room impulse response generator",
      "author" : [ "E.A.P. Habets" ],
      "venue" : "Tech. Rep., Technische Univ. Eindhoven, Eindhoven, The Netherlands, 2010.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The Kaldi speech recognition toolkit",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely" ],
      "venue" : "Proc. ASRU, 2011.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "Recently, deep neural networks (DNNs) have been used for matching reverberant speech to its anechoic version [9]-[14].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "Recently, deep neural networks (DNNs) have been used for matching reverberant speech to its anechoic version [9]-[14].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "This approach is termed the back-endbased speech recognition system, which includes a hidden Markov model (HMM) adaptation, such as the maximum likelihood linear regression (MLLR) [15] and the maximum a posteriori (MAP) adaptation [16].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 14,
      "context" : "This approach is termed the back-endbased speech recognition system, which includes a hidden Markov model (HMM) adaptation, such as the maximum likelihood linear regression (MLLR) [15] and the maximum a posteriori (MAP) adaptation [16].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 12,
      "context" : "More recently, neural network has been used to jointly train a single DNN for both feature mapping and acoustic modeling in noisy environments [14], [17], [18].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 15,
      "context" : "More recently, neural network has been used to jointly train a single DNN for both feature mapping and acoustic modeling in noisy environments [14], [17], [18].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "More recently, neural network has been used to jointly train a single DNN for both feature mapping and acoustic modeling in noisy environments [14], [17], [18].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : "neural network ensemble, which builds a set of separately trained neural networks and combines the sets to form the unified prediction model [19]-[27].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "neural network ensemble, which builds a set of separately trained neural networks and combines the sets to form the unified prediction model [19]-[27].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 26,
      "context" : "As for the MAP criteria, maximum likelihood (ML) estimation is used for the blind estimation of RT60 [28], [29], which is based on a statistical model for representing sound decay under reverberant conditions.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : "As for the MAP criteria, maximum likelihood (ML) estimation is used for the blind estimation of RT60 [28], [29], which is based on a statistical model for representing sound decay under reverberant conditions.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "In addition, inspired by the joint training technique [14], we propose jointly training each model in the ensemble for both feature mapping and acoustic modeling, which enables us to use the back-propagation algorithm up to the feature mapping layer.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "The ensemble classifier is known to be an effective approach for enhancing the recognition accuracy [21]-[23] for which individual models are independently designed, and the decoding word hypotheses of multiple models are combined to score the speech frames.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "The ensemble classifier is known to be an effective approach for enhancing the recognition accuracy [21]-[23] for which individual models are independently designed, and the decoding word hypotheses of multiple models are combined to score the speech frames.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : "In a recent study, multiple datasets were generated through normalized noisy features by which beamforming and speech enhancement techniques are used, and additional speaker related features as well as other auxiliary features are also included [23].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 19,
      "context" : "In averaging posterior probabilities, simple averaging methods [21], [23] and weighted averaging method [22] have been presented.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "In averaging posterior probabilities, simple averaging methods [21], [23] and weighted averaging method [22] have been presented.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 20,
      "context" : "In averaging posterior probabilities, simple averaging methods [21], [23] and weighted averaging method [22] have been presented.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 21,
      "context" : "For a more detailed discussion of the EAM, please refer to [23].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "The weights are determined by the MAP probability computation, which is given by the blind ML estimation of RT60 [29].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 27,
      "context" : "Indeed, the diffuse tail of reverberation is instead mathematically modeled as a simplified noise decay curve [29]:",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 28,
      "context" : "Moreover, the proposed method was compared with a conventional single DNN acoustic model and a single DNN acoustic model, which set the topological complexity equivalent to that of EAM [30].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 29,
      "context" : "The proposed method was evaluated on TIMIT DB [31], which was first applied to the RIR generator [32] in order to 0.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 30,
      "context" : "The proposed method was evaluated on TIMIT DB [31], which was first applied to the RIR generator [32] in order to 0.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 31,
      "context" : "Training of HMM parameters and decoding for speech recognition was carried out using the Kaldi software [33].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 28,
      "context" : "We evaluated the performance of the proposed EAM algorithm in terms of the phone error rate (PER) compared to the single DNN background model (SBM) [30].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 27,
      "context" : "8, and we adopted the parameters used in [29].",
      "startOffset" : 41,
      "endOffset" : 45
    } ],
    "year" : 2016,
    "abstractText" : "Distant speech recognition is a challenge, particularly due to the corruption of speech signals by reverberation caused by large distances between the speaker and microphone. In order to cope with a wide range of reverberations in real-world situations, we present novel approaches for acoustic modeling including an ensemble of deep neural networks (DNNs) and an ensemble of jointly trained DNNs. First, multiple DNNs are established, each of which corresponds to a different reverberation time 60 (RT60) in a setup step. Also, each model in the ensemble of DNN acoustic models is further jointly trained, including both feature mapping and acoustic modeling, where the feature mapping is designed for the dereverberation as a front-end. In a testing phase, the two most likely DNNs are chosen from the DNN ensemble using maximum a posteriori (MAP) probabilities, computed in an online fashion by using maximum likelihood (ML)-based blind RT60 estimation and then the posterior probability outputs from two DNNs are combined using the ML-based weights as a simple average. Extensive experiments demonstrate that the proposed approach leads to substantial improvements in speech recognition accuracy over the conventional DNN baseline systems under diverse reverberant conditions.",
    "creator" : "LaTeX with hyperref package"
  }
}