{
  "name" : "1402.5634.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "To go deep or wide in learning?",
    "authors" : [ "Gaurav Pandey" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a twolayered supervised learning model, or learn the features directly using a deep (multilayered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "In spite of the vast research on machine learning and AI in the past many decades, we still do not have systems that can perform as well as humans for many real world tasks. For instance, for the task of scene labelling, human performance far exceeds the performance of all known learning algorithms [Xiao et al., 2010]. One reason often cited for this difference in performance is the insufficient depth of the architecture used by the learning algorithms [Bengio, 2009]. Typically, supervised learning algorithms can be thought to have 2 layers (i.e., depth = 2), whereby, in the first layer, the data is mapped to a feature space (either using the kernel trick or by feature extraction [Lowe, 2004, Dalal and Triggs, 2005]), followed by learning a linear classifier in the feature space. Though the aim is\nAppearing in Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS) 2014, Reykjavik, Iceland. JMLR: W&CP volume 33. Copyright 2014 by the authors.\nto map the data into a feature space, where the classes become linearly separable, these two stages of learning are often kept independent of each other, with notable exceptions being metric learning [Blitzer et al., 2005] and multiple kernel learning [Bach et al., 2004]. Such architectures are called shallow architectures [Bengio, 2009].\nIn recent years, architectures with multiple layers have been shown to achieve state-of-the-art results for many AI tasks such as image and speech recognition, often outperforming other shallow architectures by a huge margin [Ciresan et al., 2012, Wan et al., 2013]. Most of these architectures are minor modifications of multilayer neural networks (MLNN). Though MLNN have been around for more than 2 decades, architectures with more than 2 hidden layers were seldom used for learning [Utgoff and Stracuzzi, 2002]. It was argued that multi-layered neural networks get stuck in bad local optima, thereby giving bad generalization performance. This trend was broken in 2006, when Hinton et al. [Hinton et al., 2006] showed that a multi-layered neural network can be effectively trained to achieve good generalization performance, if the weights of connections between various hidden layers were initialized by using the data only and not their labels in a purely unsupervised fashion using restricted Boltzmann machines (RBMs). He called such models as deep belief nets (DBN).\nThe success of deep learning methods over shallow architectures have caused the revival of multi-layer neural networks in machine learning literature. For most AI tasks, multi-layer neural networks (that may be initialized with unsupervised pre-training) tend to outperform most other learning algorithms. Unfortunately, the algorithms used for multi-layered neural networks are often non-convex and rely on a number of heuristics, such as number of epochs, learning rate, momentum, batch size etc. Incorrect tuning of these hyper-parameters often results in decrease in performance [Bergstra et al., 2011]. Generally, computationally costly grid searches are used to get a suitable value for the hyper-parameters. Furthermore, the al-\nar X\niv :1\n40 2.\n56 34\nv1 [\ncs .L\nG ]\n2 3\nFe b\n20 14\ngorithms themselves are computationally complex and often rely on the processing power of GPUs [Ciresan et al., 2012,Krizhevsky et al., 2012].\nDue to the above mentioned deficiencies in neural networks, one may wonder whether one can achieve the same performance as in deep learning without ever resorting to neural networks? Ideally, a good feature learning algorithm should be able to learn features from the data that disentangle the factors of variation [Bengio, 2009]. In such a case, no fine-tuning of the model using multi-layer neural networks would be necessary. In other words, we can directly take the learnt features as input and feed it to a classifier to get the labels as output. However, results tend to suggest that models that have not been fine-tuned tend to perform poorly. This has led many researchers to believe that restricted Boltzmann machines (RBMs) are not doing a good job at capturing the invariances and disentangling the underlying factors of variation present in data [Lamblin and Bengio, 2010].\nIt is in this light that we present our paper. We introduce wide-learning using covariance arc-cosine kernels (based on arc-cosine kernels introduced in [Cho and Saul, 2010]), that builds upon the existing RBM model. We give exact as well as non-exact methods for learning in such models. In particular, we show that output of the RBM can be directly used to learn the covariance matrix in a covariance arc-cosine kernel. We show that the model is actually capable of learning features, that makes the fine-tuning stage redundant. Secondly, we also show that for the datasets considered, a single wide layer (that is, a single layer of covariance arc-cosine kernel) is sufficient to generate a representation that helps to capture invariances in the data, since preliminary results suggest that stacking multiple wide layers leads to decrease in performance. Using a single RBM to learn a wide layer, we are able to obtain better results for many classification tasks than obtained by multi-layer neural network initialized using a deep belief network and fine-tuned using backpropagation."
    }, {
      "heading" : "2 PRELIMINARIES AND BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 Restricted Boltzmann Machine (RBM)",
      "text" : "An RBM [Hinton, 2002] is a complete bipartite Markov random field with a layer of visible units (x) and another layer of finitely many latent units (h). The visible units correspond to the features of the observed sample, for instance, pixels in an image. Every visible unit is connected to every hidden unit by an edge. Since the graph is bipartite, the cliques of\nthe model correspond to the edges and have size 2. The potential function of an edge (vi, hj) is given by −(wijvihj + aivi + bjhj), where wij , ai, bj , 1 ≤ i ≤ d, 1 ≤ j ≤ K from the parameters of the model. The energy function which is the sum of potential function across all edges, is given by\nE(x,h) = − ∑ i,j wijvihj + aivi + bjhj (1)\n= −(xTWh + aTx + bTh) (2)\nThe corresponding marginal probability of an instance x is given by\np(x) = ∑ h exp(−E(x,h))∑\nx′,h exp(−E(x′,h)) . (3)\nIn order to maximize the log-likelihood of an RBM for a sequence of observations, one can use stochastic gradient descent techniques. The gradient of the loglikelihood for a fixed observation x with respect to the weight matrix W is given by\n∇WL = 1\nN N∑ n=1 Ep(h|x(n))x(n)h T − Ep(x,h)xhT . (4)\nThe gradient with respect to other parameters can be computed similarly. The first quantity in the RHS is straightforward to compute from the following equations.\nEp(h|x)hj = ∑ h exp(x TWh + aTx + bTh)hj∑\nh exp(x TWh + aTx + bTh))\n(5)\n= 1\n1 + exp(−(xTwj + bj)) , (6)\nwhere wj is the k th column in W . In order to compute the second quantity, we need the expected value of xhT for the current choice of W . This can be obtained by using Gibbs sampling. In practise, a small number (p) of iterations of Gibbs sampling is run to get xp and hp and plugged in equation (4). This method, also known as Contrastive Divergence (CD) [Hinton, 2002], has been shown to give a good approximation to the actual gradient. The corresponding update equation is given by\nW t+1 = W t + η(x0h T 0 − xphTp ) , (7)\nwhere η is the learning rate. For more details about training an RBM, we encourage the reader to refer to Fischer et al. [Fischer and Igel, 2012].\nA commonly used variant of RBM has rectified linear units rathar than stochastic binary units. Training of RBMs with rectified linear units is very similar to that with stochastic binary units [Nair and Hinton, 2010].\nThe update equation for an RBM with rectified linear units is exactly similar to that of an RBM with stochastic binary units except that the hidden units are sampled uniformly from the normal distribution with mean max(0,WTx) and identity covariance matrix. In the rest of the paper, we will refer both these Markov random fields as RBM."
    }, {
      "heading" : "2.2 Single Layer Threshold Networks And Corresponding Kernels",
      "text" : "In deep learning, the weights of a multi-layered neural network (excluding those that connect the output units) are initialized using RBMs, and fine-tuned using backpropagation algorithm. Rather than pre-training the weights using RBM, it is possible to sample the weights randomly from a fixed distribution and feed the output of the hidden units directly to a linear classifier such as SVM. Contrary to intuition, it has been observed that when the weights have been sampled from standard normal distribution and the number of hidden units is much greater than the number of visible units, the resultant classifier gives good performance on many classification tasks [Huang et al., 2004]. Furthermore, the performance improves as the number of hidden units increase.\nIt is possible to perform learning tractably, when the number of hidden units in a randomly weighted neural networks tend to ∞ by using the kernel trick. Cho et al. [Cho and Saul, 2010] showed that for threshold neural networks, inner products of the randomly sampled hidden units for two instances becomes deterministic as the number of hidden units tend to ∞. The corresponding kernels are termed as arc-cosine kernels. Hence, learning a linear classifier in the original infinite dimensional space is same as learning a kernel machine using the arc-cosine kernel.\nIn particular, when the hidden units are given by\nh(x) = H(wTx)(wTx)n, w ∼ N (0, 1) (8)\nwhere H is the Heavyside step function, the corresponding kernel [Cho and Saul, 2010] is given by\nKn(x,y) = 1\n2π ‖x‖n‖y‖nJn(θ) , (9)\nwhere θ is the angle between x and y and Jnθ is given by Jn(θ) = (−1)n(sin θ)2n+1 ( 1\nsin θ\n∂\n∂θ )n( π − θ sin θ ) , n ∈ N\n(10)\nAs a spacial case, when n = 1, the hidden units are termed as rectified linear units and the corresponding\nkernel function is given by\nK1(x,y) = 1\n2π ‖x‖‖y‖(sin θ + (π − θ) cos θ) . (11)"
    }, {
      "heading" : "3 COVARIANCE ARC-COSINE KERNELS",
      "text" : "Instead of sampling the entries of the matrix W from standard normal distribution, if we sample the columns from a multivariate Gaussian distribution with zero mean and covariance Σ, we get a modified arc-cosine kernel of the form\nKΣ,n(x,y) = 1\n(2π) d 2 |Σ| 12 ∫ w∈Rd H(wTx)H(wTy)(wTx)n..\n..(wTy)n exp ( −w T Σ−1w\n2\n) dw ,\nwhich we term as covariance arc-cosine kernel. Applying a change of variables u = Σ− 1 2w in the above equation, we get\nKΣ,n(x,y) = |Σ| 12\n(2π) d 2 |Σ| 12 ∫ u∈Rd H(uT Σ 1 2x)H(uT Σ 1 2y)..\n..(uT Σ 1 2x)n(uT Σ 1 2y)n exp ( −‖u‖ 2\n2\n) dw ,\n= 1\n(2π) d 2 ∫ u∈Rd H(uTa)H(uTb)..\n..(uTa)n(uTb)n exp ( −‖u‖ 2\n2\n) du ,\n= Kn(a,b) ,\nwhere a = Σ 1 2x and b = Σ 1 2y respectively. We state the above result below:\nProposition 3.1 Let the columns of the matrix W be sampled from a multivariate normal distribution N (0,Σ), and let h(x) = H(WTx)(WTx)n be the representation of x in the feature space. Here both the Heavyside step function and the polynomial function is applied pointwise on the vector. As the number of columns in the weight matrix W tend to infinity, the inner product between the feature representation is given by\nKΣ,n(x,y) = Kn(a,b) (12)\nwhere a = Σ 1 2x and b = Σ 1 2y respectively and Kn is defined as in (9)."
    }, {
      "heading" : "4 WIDE LEARNING",
      "text" : "It is known [Bengio, 2009] that in case of natural image patches, the features learnt by an RBM are Gabor-like,\nthat is, they correspond to the output of a Gabor filter with some fixed frequency and orientation. Since the set of all possible frequencies and orientations has uncountably many elements, an RBM tries to extract a subset of these frequencies/orientations that best capture the invariances present in the data. A covariance arc-cosine kernel, on the other hand, tries to find a distribution over the weight vectors that best capture the invariances in the data, thereby allowing one to use infinitely many features for any given data. This is also the reason why we call distribution learning for arc-cosine kernels as wide learning.\nIn the rest of the paper, whenever we refer to an arccosine kernel, we imply the kernel with rectified linear units, that is, the kernel corresponding to n = 1 given by (11)"
    }, {
      "heading" : "4.1 Exact Wide Learning",
      "text" : "In order to derive an algorithm for training the kernel to learn the covariance matrix Σ, we rewrite the update equation for rectified linear units mentioned in equation (7) as\nW t+1 = W t + η(x0h T 0 − xphTp ) , (13)\nwhere x0 and h0 are the initial values for the visible and hidden units and xp and hp are the values for visible and hidden units after p iteration of Gibbs sampling. In our case, we assume that W t has infinitely many columns sampled from some distribution with covariance matrix Σt, and we are interested in computing the covariance matrix Σt+1 of the columns of W t+1, which is given by\nΣt+1 = lim M→∞\n1\nM M∑ k=1 wt+1k w t+1 k T (14)\n= lim M→∞\n1\nM W t+1W t+1\nT , (15)\nHence, if we multiply equation (13) by its transpose and use (15) in the resultant equation, we get\nΣt+1 = Σt+ lim M→∞\n1\nM\n[ η(W th0x T 0 −W thpxTp )) ] + lim\nM→∞\n1\nM\n[ η(x0h T 0 W tT − xphTpW t T )) ]\n+ lim M→∞\n1\nM\n[ η2(x0h T 0 h0x T 0 + xph T p hpx T p−\n−x0hT0 hTp xTp − xphTp hT0 xT0 ) ] ,\n(16)\nIf we assume that the kth unit in the hidden layer is sampled from a normal distribution with unit variance and mean (xTwk)+ (as is commonly done for training RBMs with rectified linear units [Nair and Hinton,\n2010]), then\nlim M→∞\n1\nM Wh\n= lim M→∞\n1\nM M∑ k=1 wk((w T k x)+ + k), k ∼ N (0, 1)\n= ∫ w∈Rd w(wTx)+p(w) dw + lim M→∞ 1 M M∑ k=1 kwk = Σx\n2\nHere, we have omitted the superscript t denoting the iteration number from all variables for ease of presentation. For the first equation, we use the fact that h has been sampled from normal distribution with unit variance and mean (xTwk)+. The second equation follows from the law of large numbers and for the third equation we use the fact that k and wk are independent random variables each with zero mean and the random vector w has been sampled from a distribution with covariance matrix Σ.\nIn order to further simplify equation (16), we need the values for hT0 h0, h T 0 hp and h T p hp. However, this is exactly the inner product between the feature representation for the input data, and hence is equivalent to the covariance arc-cosine kernel between the corresponding visible input. Combining all the above results, we get\nΣt+1 =Σt + η\n2 (Σtx0x T 0 − ΣtxpxTp + x0xT0 Σt − xpxTp Σt)\n+ η2(KΣt(x0,x0)x0x T 0 +KΣt(xp,xp)xpx T p −KΣt(x0,xp)x0xTp −KΣt(xp,x0)xpxT0 ) (17)\nHere, KΣt denotes the covariance arc-cosine kernel function with covariance matrix set to Σt. In order to sample xp from x0, we run a Markov chain with the following transition matrix\np(xq+1,i = 1|xq;W t) = 1\n1 + exp(−hTq W ti )\n= 1\n1 + exp(−x T q Σ t i 2 ) ,\nwhere Σti is the i th column of the covariance matrix and W ti is the i th row of matrix W t. It is easy to see that the above Markov chain sampling technique is the same as Gibbs sampling in RBM if the weight matrix is assumed to have infinite number of columns sampled independently from a multivariate Gaussian distribution. The above update gives a stochastic gradient descent method for learning the covariance matrix of the covariance arc-cosine kernel. It is easy to see that\nafter every update the covariance matrix remains positive definite. This is necessary for the resultant kernel to be a valid kernel."
    }, {
      "heading" : "4.2 Inexact Wide Learning",
      "text" : "Instead of learning the covariance matrix from the data, one can use an RBM to learn the weight matrix W . Then assuming the columns of the weight matrix have been sampled from a multivariate Gaussian distribution with zero mean, one can use maximum likelihood to estimate the covariance matrix W. The corresponding covariance matrix is given by\nΣ = 1\nM WWT , (18)\nwhere W is the weight matrix learnt by the RBM and M is the number of columns in W .\nIn our experiments, we found that training the covariance matrix using the first approach took much more time than training an RBM. Secondly, for exact training we had to perform kernel computations and matrix products as mentioned in equation (17) that made each iteration of exact training much slower than the iterations of RBM. It is for this reason, that we use inexact training in the rest of the paper."
    }, {
      "heading" : "5 DEEP-WIDE LEARNING",
      "text" : "In deep learning, one stacks multiple RBMs one on top of the other such that the output of the previous layer RBM is fed as input to the next layer RBM. Thus, the weight matrix in the second layer is learnt based on the output of the first layer. Similarly, one can stack multiple covariance arc-cosine kernels one on top of the other. However, as mentioned earlier, the feature representation learnt by an arc-cosine kernel has infinite width. Hence, the covariance matrix to be learnt will have infinite number of rows as well as columns. Hence, for learning the covariance matrix in the second layer, one cannot directly use equation (17). At first glance, it appears that exact learning of the covariance matrix in the second layer is not possible.\nIt is here, that kernels come to our rescue. However, when learning multiple layers of kernels notations can become complicated. Hence, it is important to fix the notation. We will use uppercase alphabets to denote both the kernel function and kernel matrices and vectors.\n1. For a given layer, we use K̃ to denote the inner product between the feature representation of the previous layer. That is, for the first layer, K̃(x,y) = xTy. For the second layer K̃(x,y) = h(x)Th(y).\n2. For a given layer, we use K̃Σ to denote the inner product between the feature representation of the previous layer using the covariance matrix Σ. That is, for the first layer, K̃Σ(x,y) = x\nT Σy. For the second layer K̃(x,y) = h(x)T Σh(y).\n3. For a given layer, we use KΣ to denote the covariance arc-cosine kernel over the feature representation of the previous layer using the covariance matrix Σ. ."
    }, {
      "heading" : "5.1 Exact Deep-Wide Learning",
      "text" : "Given the kernel matrix between the feature representation of the previous layer using the covariance matrix Σ, that is, K̃Σ, we compute the covariance arc-cosine kernel matrix KΣ as follows.\nKΣ(x,y) = 1\n2π [m(x)m(y)(sin θ+(π−θ) cos θ)] , (19)\nwhere\nm(x) = K̃Σ(x,x)\nm(y) = K̃Σ(y,y)\nθ = cos−1  K̃Σ(x,y)√ K̃Σ(x,x)K̃Σ(y,y)  . In order to compute the covariance arc-cosine kernel matrix over the features learnt by the previous layer, we make use of equation (17). Let h(a) and h(b) be the infinite dimensional feature representation learnt by the previous layer. We pre-multiply equation (17) by h(a)T and post-multiply it by h(b) to get\nK̃Σt+1(a, b) = K̃Σt(a, b)\n+ η\n2 (K̃Σt(a,x0)K̃(x0, b)− K̃Σt(a,xp)K̃(xp, b))\n+ η\n2 (K̃(a,x0)K̃Σt(x0, b)− K̃(a,xp)K̃Σt(xp, b))\n+ η2KΣt(x0,x0)K̃(a,x0)K̃(x0, b)\n+ η2KΣt(xp,xp)K̃(a,xp)K̃(xp, b)\n− η2KΣt(x0,xp)K̃(a,x0)K̃(xp, b) − η2KΣt(xp,x0)K̃(a,xp)K̃(x0, b) .\n(20)\nThe above equation updates each entry of the kernel matrix in a sequential fashion. However, one can choose to update the entire kernel matrix in one go by\nusing the following equation.\nK̃Σt+1 =K̃Σt + η\n2 (K̃Σt,x0K̃\nT x0 − K̃Σt,xpK̃ T xp)\n+ η\n2 (K̃x0K̃\nT Σt,x0 − K̃xpK̃TΣt,xp)\n+ η2KΣt(x0,x0)K̃x0K̃ T x0 + η2KΣt(xp,xp)K̃xpK̃ T xp − η2KΣt(x0,xp)K̃x0K̃Txp − η2KΣt(xp,x0)K̃xpK̃Tx0 ,\n(21)\nwhere K̃ and K̃Σt denote the kernel matrices corresponding to the kernel functions K̃ and K̃Σt respectively as defined above and K̃x0 and K̃Σt,x0 denote the column in kernel matrices K̃ and K̃Σt corresponding to x0 respectively. It is interesting to note the similarity between the above equation and equation (17).\nThis suggests the following steps for computing the kernel matrix in the second layer based on the kernel matrix of the previous layer.\n1. Let K̃ be the kernel matrix corresponding to the first layer. Initialize K̃Σ0 to be a random positive definite matrix of size N ×N .\n2. Update the kernel matrix K̃Σt using equation (21) until convergence.\n3. The kernel matrix K of the second layer can then be computed from K̃Σt by composing the arccosine kernel with the kernel matrix KΣt as given in equation (19)."
    }, {
      "heading" : "5.2 Inexact Deep-Wide Learning",
      "text" : "Exact learning of the kernel matrix as given above requires the entire matrix to be present in memory. Since unsupervised feature learning only works when the number of instances is huge, this means that the kernel matrix will also be very huge. Hence, learning using such a huge kernel matrix will be infeasible both in terms of memory and processing time requirements.\nHence, we tried inexact approaches to extend the architecture to multiple layers. In the first approach, we learn a finite dimensional first layer using RBM. Next, a covariance arc-cosine kernel is learnt on top of the activities of the first level RBM as mentioned in the previous section. However, we found that for all datasets that we tried, this approach resulted in reduction in accuracy. For instance, for MNIST digit recognition task, the accuracy reduced from 99.05% to 97.85%.\nWe also tried to sample a subset of features by applying kernel PCA for the covariance arc-cosine kernel.\nHowever, this method further resulted in reduction in performance. Hence, for the rest of the experiments, only the first layer is learnt using RBM."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : "In order to understand why a covariance arc-cosine kernel should work, we divert ourselves from feature learning to feature extraction. It has been known for a long time now that gradient (orientation as well as magnitude) at every pixel location is a more natural representation for an image then the actual pixel values. For instance, in SIFT [Dalal and Triggs, 2005], the gradient orientation at each pixel is binned into one of the finitely many bins available to get a finite dimensional vector representation. In soft binning, multiple bins are allowed to be non-zero. The feature representation of a patch is then computed by doing a weighted sum of the gradient orientation vector at all pixels in the patch. Finally a linear classifier is applied on the resultant feature representation.\nThe above model is equivalent to defining a linear kernel on the weighted sum of binned gradient orientation vectors. However, instead of computing a linear kernel between the binned orientation vectors, one can choose to compute an RBF kernel over the gradient orientation vectors themselves (without binning). This is equivalent to computing an inner product between the infinite dimensional representation of the orientation vectors. As shown in [Bo et al., 2010], this very small trick results in improved performance.\nHere, our idea is very similar. When we learn the weight matrix W using an RBM and apply the resultant matrix on visible data, we get a finite dimensional representation of the data. If we assume now that each bin has been labelled by a column wj of the weight matrix W , then this approach is equivalent to soft binning, where jth bin is non-zero, if and only if wTj x is positive (assuming rectified linear units). This creates a finite dimensional vector representation of the data.\nHowever, if we learn the distribution of the columns in the weight matrix W , we can, in principle, project the data to infinite dimensions by sampling infinitely many vectors wj from the distribution, using the kernel trick. A covariance arc-cosine kernel makes the additional assumption that the distribution of the columns in W , is multivariate Gaussian. Hence, while an RBM can only bin the data into finite many bins, use of covariance arc-cosine kernel allows one to bin the data in infinite number of bins by using the kernel trick. This is also a reason why we term our proposed learning method as wide learning. An important point to note at this juncture is that though the final model has infinite width, the model learnt by RBM still has a finite\nsmall width. Hence, the number of parameters in the model are much lesser than in a deep learning model. This approach is very fast since only a single RBM is trained and no fine-tuning needs to be done."
    }, {
      "heading" : "7 EXPERIMENTS",
      "text" : "We tested the covariance kernel so obtained for many datasets commonly used for comparing deep learning architectures."
    }, {
      "heading" : "7.1 MNIST",
      "text" : "The MNIST dataset [LeCun et al., 1998] consists of grayscale images of handwritten digits from 0 to 9 with 50, 000 training and 10, 000 test examples. We normalize the pixel values to be between 0 and 1. Except that, we do not use any preprocessing for the dataset. A standard Bernoulli RBM with 1000 hidden units is trained on the raw pixel values using stochastic gradient descent with a fixed momentum of 0.5. In our experiments for MNIST dataset, we found that fixing the bias to zero doesn’t affect the performance of the final model.\nFinally, we use the weight matrix W learnt by RBM to compute the kernel matrix. The kernel matrix is fed into a regularized least square kernel classifier. We use one-vs-one classification to classify the MNIST images. Using a single layer, we were able to achieve an error rate of 0.95% after 20 epochs, which is quite better compared to the 1.25% error rate obtained by a deep belief network [Hinton et al., 2006]. This is surprising since a deep belief network uses multiple stacked RBMs, while we used a single RBM in our model. Furthermore, both the RBMs were trained similarly. This suggests that a single RBM might be enough to learn the invariances in the data.\nIn order to show the advantage of having a representation of infinite width (that is, by using the kernel), we compare the performance of the covariance arc-cosine kernel against a neural network with 1 hidden layer with number of epochs of training in Figure 1. The network has 1000 hidden units. The parameters of both the neural network and the covariance arc-cosine kernel are obtained using a restricted Boltzmann machines with 1000 hidden units. No fine tuning is done for either of the models. There are two important observations that can be made from the figure. Firstly, the infinite width model reaches an acceptable performance in a very few number of epochs. Secondly, even after running the unsupervised learning algorithm for 20 epochs, the accuracy of the finite width model never comes any close to the accuracy of the infinite width model after 2 epochs. In fact, when we ran the train-\ning algorithm for 300 epochs, the accuracy of the finite width model converged to 97.93%. This is in stark contrast with the accuracy achieved by the infinite width model after 1 epoch (98.9%). This is a very surprising result, which suggests that even if the training time for deep learning is small, a covariance arc-cosine kernel can still give acceptable performance.\nFinally, we will briefly mention about the time required for training and testing for finite and infinite width models compared in the previous paragraph. The unsupervised learning phase in both the models comprises of learning a weight matrix from the data by defining a restricted Boltzmann machine. In infinite width model, the weight matrix is used to learn a covariance matrix Σ. The covariance arc-cosine kernel is then computed for the data using the covariance matrix. Finally, an algorithm based on kernel methods (such as SVM), is then used to compute the parameters for the last layer. On the other hand, for the finite width model, a linear classifier is learnt on top of the activities of the hidden layer. Thus, the only difference in computation cost for finite and infinite width model lies in time taken to learn the last layer. Kernel methods can be more computationally costly, when the number of instances is huge, because of their quadratic (at least) dependence on the number of instances.\nTwo variants of MNIST are also considered for comparison as listed below:\n1. Rotated MNIST data set: This is a modified version of MNIST dataset, where the digits are rotated by an angle generated uniformly between 0 and 2π. This dataset is one of the several MNIST variations generated for comparing deep vs shallow architectures in [Larochelle et al., 2007]. The dataset has 12, 000 training samples and 50, 000 test samples.\n2. MNIST with random background: This is also a modified version of the MNIST dataset where the background pixels have been sampled randomly between 0 and 255. This dataset also has 12, 000 training and 50, 000 test samples."
    }, {
      "heading" : "7.2 Tall and wide rectangles",
      "text" : "This dataset [Larochelle et al., 2007] consists of rectangles of varying widths and heights. The aim is to classify the rectangles into classes, where the rectangles in one class have more height than width, while the rectangles in the other class have more width than height. We trained an RBM over the pixel values and computed the covariance matrix from the learnt weight matrix W . This covariance matrix was used in the covariance arc-cosine kernel in the first layer.\nFor this dataset, we found that using a first layer of covariance arc-cosine kernel, followed by multiple layers of arc-cosine kernels where the covariance matrix is set to identity, resulted in improvement in performance. Note that, this is still not a case of deep learning since only a single layer is learnt. In fact, this approach can be shown as equivalent to learning the covariance matrix of a different kernel which is obtained by composing multiple arc-cosine kernels with identity covariance matrix. For more details, we encourage the reader to refer to [Cho and Saul, 2010], where composition of arc-cosine kernels is discussed in further detail. The result for covariance arc-cosine\nkernel against other standard classifiers is given in Table 1. Clearly, the best results are obtained when the first layer is learnt using a covariance arc-cosine kernel.\nThis dataset [Larochelle et al., 2007] consists of black and white images of convex and concave sets. The task is to separate the concave sets from convex sets. The results are given in Table 1."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we proposed the notion of wide learning, that makes the fine-tuning stage commonly used in deep architectures redundant. We have given exact as well as inexact methods for learning in such models. We found that for the datasets considered, whenever we replace a finite width layer by a layer of infinite width, this results in drastic improvement in performance. Furthermore, use of a single layer severely reduces the number of hyper-parameters to be estimated, thereby saving time in computationally costly grid searches. Further experimentation on more complicated datasets, such as natural image patches, is needed to test its suitability for general AI tasks."
    } ],
    "references" : [ {
      "title" : "Multiple kernel learning, conic duality, and the SMO algorithm",
      "author" : [ "Bach et al", "F.R. 2004] Bach", "G.R. Lanckriet", "M.I. Jordan" ],
      "venue" : "In Proceedings of the 21st International Conference on Machine learning,",
      "citeRegEx" : "al. et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2004
    }, {
      "title" : "Algorithms for hyper-parameter optimization",
      "author" : [ "Bergstra et al", "J. 2011] Bergstra", "R. Bardenet", "Y. Bengio", "B Kégl" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS",
      "citeRegEx" : "al. et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2011
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "Blitzer et al", "J. 2005] Blitzer", "K.Q. Weinberger", "L.K. Saul" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    }, {
      "title" : "Kernel descriptors for visual recognition",
      "author" : [ "Bo et al", "L. 2010] Bo", "X. Ren", "D. Fox" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "Large-margin classification in infinite neural networks",
      "author" : [ "Cho", "Saul", "Y. 2010] Cho", "L.K. Saul" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Cho et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2010
    }, {
      "title" : "Multi-column deep neural networks for image classification",
      "author" : [ "Ciresan et al", "D. 2012] Ciresan", "U. Meier", "J. Schmidhuber" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "al. et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2012
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "Dalal", "Triggs", "N. 2005] Dalal", "B. Triggs" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Dalal et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dalal et al\\.",
      "year" : 2005
    }, {
      "title" : "Training invariant support vector machines",
      "author" : [ "Decoste", "Schölkopf", "D. 2002] Decoste", "B. Schölkopf" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Decoste et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Decoste et al\\.",
      "year" : 2002
    }, {
      "title" : "An introduction to restricted Boltzmann machines",
      "author" : [ "Fischer", "Igel", "A. 2012] Fischer", "C. Igel" ],
      "venue" : "In Progress in Pattern Recognition,",
      "citeRegEx" : "Fischer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fischer et al\\.",
      "year" : 2012
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Hinton et al", "G.E. 2006] Hinton", "S. Osindero", "Teh", "Y.-W" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "al. et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2006
    }, {
      "title" : "Extreme learning machine: a new learning scheme of feedforward neural networks",
      "author" : [ "Huang et al", "2004] Huang", "G.-B", "Zhu", "Q.-Y", "Siew", "C.-K" ],
      "venue" : "In International Joint Conference on Neural Networks,",
      "citeRegEx" : "al. et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2004
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky et al", "A. 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "al. et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2012
    }, {
      "title" : "Important gains from supervised finetuning of deep architectures on large labeled sets",
      "author" : [ "Lamblin", "Bengio", "P. 2010] Lamblin", "Y. Bengio" ],
      "venue" : "NIPS*",
      "citeRegEx" : "Lamblin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lamblin et al\\.",
      "year" : 2010
    }, {
      "title" : "An empirical evaluation of deep architectures on problems with many factors of variation",
      "author" : [ "Larochelle et al", "H. 2007] Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio" ],
      "venue" : "In Proceedings of the 24th International Conference on Ma-",
      "citeRegEx" : "al. et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2007
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun et al", "Y. 1998] LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "al. et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1998
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Nair", "Hinton", "V. 2010] Nair", "G.E. Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Nair et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2010
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "Wan et al", "L. 2013] Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a twolayered supervised learning model, or learn the features directly using a deep (multilayered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}