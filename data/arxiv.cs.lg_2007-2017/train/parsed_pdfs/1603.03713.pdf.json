{
  "name" : "1603.03713.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cost-sensitive Learning for Bidding in Online Advertising Auctions",
    "authors" : [ "Flavian Vasile", "Damien Lefortier" ],
    "emails" : [ "d.lefortier}@criteo.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Online advertising is becoming a big part of the global marketing reaching $170 billion revenue in 2015 [1]. Depending on the goal of the advertising campaign, different pricing schemes exist, but, out of them, brand and performance advertising are the most prevalent. Brand advertising is used by advertisers that want to maximize the exposure of their advertising message to online users and is priced in terms of number of ad impressions, with the cost usually referred as CPM (cost-per-mille). By contrast, performance advertising is appealing to advertisers that are interested in reaching certain measurable goals such as increased number of visits to their websites, increased number of leads, sales or downloads. In this case, the cost is referred as CPC (cost-per-(ad)click) or CPA (cost-per-action/conversion).\nThe marketplace that makes online advertising possible is roughly formed out of three types of players, namely the advertiser (the demand of ad display opportunities), the publisher (the offer of ad display opportunities) and the auction house, represented by an RTB (real-time bidding) platform. Most of the RTB platforms function using a 2nd price model [2], where advertisers or agents representing the advertisers bid for display opportunities, and the winner pays the maximum between the bid of the second participant in the auction and the reserve price. In order to determine the winner for CPC and CPA clients, where the pay-off to the publisher is conditioned on a user action, the bids get converted in expected gains (also known as eCPMs) using click and conversion rate (CR) prediction models. One differentiating factor between the click and conversion models, is that for conversion (sale) prediction models, the ratio of positives to negatives is much smaller, thus making the learning problem more challenging. For this reason, we will concentrate our attention on improving the performance of conversion models.\nar X\niv :1\n60 3.\n03 71\n3v 1\n[ cs\n.L G\n] 1\n1 M\nOne important aspect of the marketplace is that the numeric range of the possible CPAs is very large and it depends on the economic value of the action that the advertiser is trying to incentivize for. The resulting eCPMs vary from ones based on expectations over actions that are frequent and low-value (e.g. conversions on classified ads1) and ones that are extremely rare and high-value (e.g. successful mortgage applications). Importantly, an improvement in prediction performance on high CPA sales has a bigger impact on the revenue than a similar improvement that affects low CPA traffic. To take this into account during evaluation, recently proposed metrics on bidders performance are making use of the advertisers’ CPAs [3, 4].\nRecent business-aware offline metrics Indeed, taking the example of a conversion-rate predictor, the classical mean squared error (MSE) can be interpreted as the offline metric that penalizes the raw volume of poorly explained observed sales. This metric can be extended to weight the display-level squared error with the CPA of the corresponding advertiser and to therefore penalize the model proportionally with the unexplained revenue (mentioned as a special case in [4]) — thus yielding a Weighted MSE (MSEW). Another example is the Utility metric, recently proposed in [4], which takes into account both the potential upside of the display represented by the CPA, but also the associated display cost (modeled as a Gamma distribution over potential display costs given the observed cost). This metric can be interpreted as the offline metric counterpart of the change in profit. We detail all these metrics in Section 3.\nShortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9]. We see a discrepancy between the CPA-aware offline metrics and the standard loss functions of the current prediction models, such as the log loss function optimized in the logistic regression. As a result, we focus on the following questions: Can our predictive models benefit from taking into account the sales’ CPAs? How to better add this information during learning in order to improve the performance of our models? To the best of our knowledge, the only solution to this question was proposed recently in [3], where the authors design specific loss functions that take into account the bidder economic performance and which inspired the work on the Utility metric in [4]. By comparison, we investigate the use of the well-established cost-sensitive learning framework to tackle this task.\nProposal The learning framework where the misclassification costs vary across examples is called example-dependent cost-sensitive learning. In [10], the authors classify the cost-sensitive learning approaches in three main classes as follows. The first approach makes learning cost-sensitive by adding costs to the learner — this works for any model that falls under the statistical query model [11]. The second approach changes the final decision function and assigns each example to its lowest cost-sensitive risk [12]. Finally, the third approach changes directly the training dataset by duplicating each example by a factor proportional to its relative cost, which has the advantage that it works with any error minimizing classifier [10]. Our proposal falls under the first approach, where we alter the loss function to take into account the relative misprediction costs. One of the main reasons for our choice was the relative ease of implementation in our current system.\nIn Section 2, we present our method for taking into account the advertisers’ CPAs using cost-sensitive learning and discuss its impact on learning and regularization. In Section 3, we present the experimental results when applying our method to improve a state-of-the-art conversion-rate prediction model used as a sub-model for predicting either the expected number of sales or the expected sales amount generated by a user following a display for bidding in online advertising auctions. We present both offline experiments using a large real-world dataset collected at Criteo and online experiments on live traffic through an A/B\n1 Personal ads that usually contain personal services advertising, consumer-to-consumer home renting and home selling offers.\ntest. We show that our method brings statistically significant improvements offline with a large effect size on our metrics. We also show that our method brings significant gains in online performance."
    }, {
      "heading" : "2 Cost-sensitive Logistic Regression",
      "text" : ""
    }, {
      "heading" : "2.1 Cost-sensitive learning and Risk Minimization",
      "text" : "As discussed in [3], current state-of-the-art models for online bidding suffer from ”misspecification”2, both due to omitted variables bias and due to functional form misspecification. To improve the performance of models under misspecification, we show how to align better the loss function with the actual offline evaluation metrics using a cost-sensitive approach. In order to extend the logistic regression to cost sensitive tasks, we change the original loss function (NLL) to weighted negative log likelihood (denoted as WNLL).\nWNLL = 1\nN N∑ i (−yi log(pi)− (1− yi) log(1− pi)) ∗ ci (1)\nwhere N is the size of the dataset, yi is the true outcome, pi is the prediction and ci is the cost associated with instance i.\nAdvertiser-constant revenue weighted logistic loss We propose a simple cost weighting scheme that weighs each display by the CPA of the corresponding advertiser: ci = CPAi for all displays i = 1..N , where CPAi is the CPA of the advertiser associated with the ith display. This is equivalent with generating a dataset where the examples from each advertiser are re-sampled proportionally with its CPA and where all positive examples (sales) have equal economic value:\nWNLLCPA = 1\nN N∑ i (−yi log(pi)− (1− yi) log(1− pi)) ∗ CPAi (2)\nThe WNLLCPA loss is calibrated, i.e. is minimized by predicting the true conversion rate of the ad.\nRelationship with Utility andMSEW MinimizingMSEW is equivalent with maximizing expected profit (a.ka. Utility) when display costs are uniform (see [4, 3]). By analogy, WNLLCPA is the logistic counterpart of MSEW and in Section 3 we show that, empirically, it has good performance when evaluated using both MSEW and Utility.\nThe alignment of NLL and WNLLCPA with ad revenue We expect that the weighted loss should align more with the revenue than the original log loss. In order to check that, we simulate the impact of a constant over-prediction of the true conversion rate: q = p × 1.2 on the NLL and WNLLCPA as a function of an increasing p and decreasing associated CPA: pi ∈ [0.1%, 1%] and CPAi = 1/pi. We assume equal display traffic for all i, so each i has equal revenue: Revenuei ∝ CPAi × pi. We plot in Figure 1 the per display expectations of: NLL,WNLLCPA and revenue, without (a) and with (b) rescaling by substracting H(p), the entropy of the true conversion rate (thus obtaining the weighted and un-weighted KL divergences of p and q: DKL(p|q) = H(p, q)−H(p)). We rescale by H(p) in order to make the expectations of the loss due to over-prediction comparable across different CR levels. As expected, we observe that the un-weighted loss puts more mass on higher CR areas, having no relationship with the expected revenue, but that the weighted loss is extremely well-aligned with the expected revenue.\n2A regression model is considered ”misspecified” when one of the variables is correlated with the error term."
    }, {
      "heading" : "2.2 Impact of weighting on learning with SGD and L-BFGS",
      "text" : "Let us now discuss how to do learning in the case of cost-sensitive logistic regression, i.e. using WNLLCPA as loss function, using two well-known learning algorithms: limited memory BFGS [13] (L-BFGS) and stochastic gradient descent [14] (SGD). These two algorithms are widely used for learning click and conversion prediction models in the context of display advertising (see, e.g., [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD). Using an advertiser-constant revenue weighting scheme is equivalent to defining an importance weight for each display (see above). In our case, the weight is the average CPA of the advertiser associated with each display."
    }, {
      "heading" : "2.2.1 Learning with importance weights",
      "text" : "Batch case: L-BFGS\nL-BFGS is a batch algorithm (as, e.g., gradient descent), which means that one pass over the data is required before each update of the weights. In this case, using an importance weight for each example during the learning is straightforward [16]. Indeed, in batch algorithms, we typically first compute the sum of the gradients of all examples before updating the weights once, and adding x time the gradient of an example or adding x times the gradient once is the same when computing this sum. So we can simply do that, i.e. multiplying the gradient by x – which is both easy and inexpensive too add in the code of L-BFGS. Note that this is not an approximation.\nOnline case: SGD\nOn the other hand, SGD is an online, or streaming algorithm, which means that an update of the weights is performed after seeing each example. In this case, adding an importance weight of x is not equivalent to multiplying the gradient by x when doing the update as in the batch case covered above [17]. Indeed, if we would have an example duplicated x times in the training set, we would do x independent updates, which is not equivalent with doing a single update that is x times larger [17]. We could therefore do x successive updates (and multiply the gradient by the fractional part if greater than 0 as CPAi is not necessarily an integer), which is less approximate but not efficient (although, ideally, these updates would be done at different times during the learning and not successively as the training set is often randomly\nshuffled at the beginning). [17] also proposes an approximated way to perform importance weight aware updates without doing x updates, which is time-consuming, and better than multiplying by x.\nHere, we simply multiply the gradient with the importance weight CPAi, which is an approximate solution, but straightforward to implement. As discussed above we use SGD only for warm-starting L-BFGS and it seems that, in this case, even an approximate method of including importance weights is sufficient (see Section 3). Our experiments, excluded for brevity, show that this method works as well as the method of doing CPAi successive updates in our setting. Since this method showed no additional value, there was no benefit in exploring the method from [17]."
    }, {
      "heading" : "2.2.2 Impact on the regularization parameter",
      "text" : "In most applications, the logistic regression objective function is accompanied by a regularization term Ω on the weights w (typically, L1 or L2 regularization terms) to prevent overfitting the training data. We obtain the following loss function for regularized WNLLCPA:\nL = 1\nN N∑ i (−yi log(pi)− (1− yi) log(1− pi)) ∗ CPAi + λ ∗ Ω(w)\nwhere λ is a hyper-parameter to tune called the regularization parameter. In this paper, we use L2 regularization where Ω(w) = ‖w‖22. Importantly, when doing cost-sensitive logistic regression, this hyperparameter λ needs to be adapted (as, e.g., when adding new features). To do that, we use the following simple heuristic to adapt λ depending on the value of the importance weights used, i.e. of the average CPA of each advertiser:\nλWNLLCPA = λNLL ∗ ∑ i CPAi N\nWe investigated how well this heuristic performs in comparison with the optimal value of λ (for results, see Section 3)."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we present our experimental results when applying our method to improve a state-of-the-art conversion-rate prediction model used as a sub-model for predicting either the expected number of sales or the expected sales amount generated by a user following a display for bidding in online advertising auctions. First, we present offline experiments using a data set of more than 3 billion examples with binary labels (sale vs. no-sale) and hundreds of millions of unique attribute values from the production click logs of Criteo from March 2015. Then, we present online experiments on live traffic."
    }, {
      "heading" : "3.1 Offline metrics",
      "text" : "For the offline evaluation of our WNLLCPA model, we use offline metrics that approximate the business impact of the model change, namely Normalized Weighted MSE (NMSEW) and Utility (discussed in Section 1). The Normalized Weighted MSE shows the relative improvement in MSEW of the model to be evaluated versus a baseline predictor, in our case the average empirical CR rate of the dataset, similar to the normalization in [8, 15]. We denote si the binary outcome variable indicating if there was a sale or not, xi the input display features vector, and ci the display cost. In order to model offline the potential change in profit due to a prediction model change, the Utility measure has been proposed recently in [4]. Since the observed profit in historical data is fixed, the metric makes the assumption that the display costs\nare coming from a second price auction and that they are generated according to a Gamma distribution conditioned on the observed display cost: P(c̃|c) ∝ c̃βc exp(−βc̃) (free parameter β).\nWeighted Mean Squared Error (MSEW) 1\nN ∑ i ((si − p(xi) · CPAi)2\nNormalized Weighted Mean Squared Error (NMSEW) 1− MSEWModel MSEWAverageEmpiricalCR\n(Expected) Utility ∑ i ∫ p(xi)∗CPAi 0 (si · CPAi − c̃)P(c̃|ci)dc̃"
    }, {
      "heading" : "3.2 Offline results",
      "text" : "We compare the difference in performance between the standard logistic regression NLL and the weighted version WNLLCPA using L-BFGS initialized with SGD [5, 15]. Given the fact that our cost-weigthing scheme re-weights each advertiser’s negative and positive examples by a constant, the resulting conversion probabilities remain calibrated and we can make the loss function change only in the conversion-rate model, while keeping the rest of the systems unchanged. For the regularization parameter λ, our experiments show that using our heuristic (Section 2.2.2) allows us to reach results that are comparable with the best λ (tuned manually), as shown by Figure 2. We see that the optimal lambda value increases the performance on high CPA traffic, but decreases the performance on other areas of traffic (overall, the heuristic lambda is within noise of the optimal one). In our experiments, we therefore use the updated regularization parameter λ heuristic, which has the great benefit of avoiding to re-tuning λ every time the costs, i.e. CPAi, change:\nThe results in Table 1 are divided over two optimization tasks used at Criteo that aim to respectively optimize the number of sales and the total value of sales for each advertiser. We observe that WNLLCPA performs significantly better than NLL on all metrics and with a higher magnitude than typically observed model improvements."
    }, {
      "heading" : "3.3 Online experiments",
      "text" : "We ran an A/B test of the change of the loss function to WNLLCPA in the conversion-rate model for both number of sales prediction and total value of sales prediction and we observed significant savings in display cost, coupled with an increase in sales performance (number of sales, total sales amount) for the advertisers. Our change therefore resulted in a significant positive impact on the projected long-term revenue. In terms of development and operational costs, the change in the loss function took only a couple of weeks to put in production, since the code change is minimal, as shown in Section 2. Furthermore, the observed training time of the model did not change."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we investigated the impact of applying the cost-sensitive learning framework in the context of bidding in online advertising auctions. We introduced a problem-specific weighting scheme for the logistic loss that we denoted WNLLCPA, which weights differently displays (and their associated sales) from different advertisers, based on their expected value, namely their CPA. We applied this change to a state-of-the-art conversion-rate prediction model used as a sub-model for predicting either the expected number of sales or the expected sales amount generated by a user following a display. Compared with the vanilla version of the logistic loss, we showed large improvements in offline metrics (Normalized Weighted MSE, Utility), followed by meaningful impact on the business metrics in the live bidding system.\nIn the next steps, we will continue our research in two different directions: first, we plan to compare our current approach with other cost-sensitive learning methods, such as the “costing” approach [10]. As a second direction, we will look into different problem-specific weighting schemes, such as weighting-byprofit and weighting with product-level cost estimators.\nAcknowledgments We would like to thank our colleagues Olivier Chapelle, Nicolas Le Roux, Olivier Koch, Etienne Sanson, Cyrille Dubarry, Alexandre Gilotte and Dmitry Pavlov for their useful comments on early versions of this paper."
    } ],
    "references" : [ {
      "title" : "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords,",
      "author" : [ "B. Edelman", "M. Ostrovsky", "M. Schwarz" ],
      "venue" : "tech. rep., National Bureau of Economic Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Loss functions for predicted click through rates in auctions for online advertising,",
      "author" : [ "P. Hummel", "R.P. McAfee" ],
      "venue" : "Preprint, Google Inc,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Offline evaluation of response prediction in online advertising auctions,",
      "author" : [ "O. Chapelle" ],
      "venue" : "Proceedings of the 24th International Conference on World Wide Web Companion,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Simple and scalable response prediction for display advertising,",
      "author" : [ "O. Chapelle", "E. Manavoglu", "R. Rosales" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Ad click prediction: a view from the trenches,",
      "author" : [ "H.B. McMahan", "G. Holt", "D. Sculley", "M. Young", "D. Ebner", "J. Grady", "L. Nie", "T. Phillips", "E. Davydov", "D. Golovin" ],
      "venue" : "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Estimating rates of rare events with multiple hierarchies through scalable log-linear models,",
      "author" : [ "D. Agarwal", "R. Agrawal", "R. Khanna", "N. Kota" ],
      "venue" : "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Practical lessons from predicting clicks on ads at facebook,",
      "author" : [ "X. He", "J. Pan", "O. Jin", "T. Xu", "B. Liu", "Y. Shi", "A. Atallah", "R. Herbrich", "S. Bowers" ],
      "venue" : "Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Click-through prediction for advertising in twitter timeline,",
      "author" : [ "C. Li", "Y. Lu", "Q. Mei", "D. Wang", "S. Pandey" ],
      "venue" : "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Cost-sensitive learning by cost-proportionate example weighting,",
      "author" : [ "B. Zadrozny", "J. Langford", "N. Abe" ],
      "venue" : "Data Mining,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Efficient noise-tolerant learning from statistical queries,",
      "author" : [ "M. Kearns" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "The foundations of cost-sensitive learning,",
      "author" : [ "C. Elkan" ],
      "venue" : "in International joint conference on artificial intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2001
    }, {
      "title" : "On the limited memory bfgs method for large scale optimization,",
      "author" : [ "D.C. Liu", "J. Nocedal" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1989
    }, {
      "title" : "Sources of variability in large-scale machine learning systems,",
      "author" : [ "D. Lefortier", "A. Truchet", "M. de Rijke" ],
      "venue" : "Machine Learning Systems (NIPS 2015 Workshop),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Online importance weight aware updates,",
      "author" : [ "N. Karampatziakis", "J. Langford" ],
      "venue" : "arXiv preprint arXiv:1011.1576,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Most of the RTB platforms function using a 2nd price model [2], where advertisers or agents representing the advertisers bid for display opportunities, and the winner pays the maximum between the bid of the second participant in the auction and the reserve price.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "To take this into account during evaluation, recently proposed metrics on bidders performance are making use of the advertisers’ CPAs [3, 4].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : "To take this into account during evaluation, recently proposed metrics on bidders performance are making use of the advertisers’ CPAs [3, 4].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : "This metric can be extended to weight the display-level squared error with the CPA of the corresponding advertiser and to therefore penalize the model proportionally with the unexplained revenue (mentioned as a special case in [4]) — thus yielding a Weighted MSE (MSEW).",
      "startOffset" : 227,
      "endOffset" : 230
    }, {
      "referenceID" : 2,
      "context" : "Another example is the Utility metric, recently proposed in [4], which takes into account both the potential upside of the display represented by the CPA, but also the associated display cost (modeled as a Gamma distribution over potential display costs given the observed cost).",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 7,
      "context" : "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 1,
      "context" : "As a result, we focus on the following questions: Can our predictive models benefit from taking into account the sales’ CPAs? How to better add this information during learning in order to improve the performance of our models? To the best of our knowledge, the only solution to this question was proposed recently in [3], where the authors design specific loss functions that take into account the bidder economic performance and which inspired the work on the Utility metric in [4].",
      "startOffset" : 318,
      "endOffset" : 321
    }, {
      "referenceID" : 2,
      "context" : "As a result, we focus on the following questions: Can our predictive models benefit from taking into account the sales’ CPAs? How to better add this information during learning in order to improve the performance of our models? To the best of our knowledge, the only solution to this question was proposed recently in [3], where the authors design specific loss functions that take into account the bidder economic performance and which inspired the work on the Utility metric in [4].",
      "startOffset" : 480,
      "endOffset" : 483
    }, {
      "referenceID" : 8,
      "context" : "In [10], the authors classify the cost-sensitive learning approaches in three main classes as follows.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "The first approach makes learning cost-sensitive by adding costs to the learner — this works for any model that falls under the statistical query model [11].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "The second approach changes the final decision function and assigns each example to its lowest cost-sensitive risk [12].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "Finally, the third approach changes directly the training dataset by duplicating each example by a factor proportional to its relative cost, which has the advantage that it works with any error minimizing classifier [10].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : "1 Cost-sensitive learning and Risk Minimization As discussed in [3], current state-of-the-art models for online bidding suffer from ”misspecification”2, both due to omitted variables bias and due to functional form misspecification.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "Utility) when display costs are uniform (see [4, 3]).",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Utility) when display costs are uniform (see [4, 3]).",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "using WNLLCPA as loss function, using two well-known learning algorithms: limited memory BFGS [13] (L-BFGS) and stochastic gradient descent [14] (SGD).",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : ", [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD).",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : ", [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD).",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : ", [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD).",
      "startOffset" : 22,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : ", [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD).",
      "startOffset" : 22,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "In this case, adding an importance weight of x is not equivalent to multiplying the gradient by x when doing the update as in the batch case covered above [17].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 13,
      "context" : "Indeed, if we would have an example duplicated x times in the training set, we would do x independent updates, which is not equivalent with doing a single update that is x times larger [17].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 13,
      "context" : "[17] also proposes an approximated way to perform importance weight aware updates without doing x updates, which is time-consuming, and better than multiplying by x.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Since this method showed no additional value, there was no benefit in exploring the method from [17].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "The Normalized Weighted MSE shows the relative improvement in MSEW of the model to be evaluated versus a baseline predictor, in our case the average empirical CR rate of the dataset, similar to the normalization in [8, 15].",
      "startOffset" : 215,
      "endOffset" : 222
    }, {
      "referenceID" : 12,
      "context" : "The Normalized Weighted MSE shows the relative improvement in MSEW of the model to be evaluated versus a baseline predictor, in our case the average empirical CR rate of the dataset, similar to the normalization in [8, 15].",
      "startOffset" : 215,
      "endOffset" : 222
    }, {
      "referenceID" : 2,
      "context" : "In order to model offline the potential change in profit due to a prediction model change, the Utility measure has been proposed recently in [4].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "We compare the difference in performance between the standard logistic regression NLL and the weighted version WNLLCPA using L-BFGS initialized with SGD [5, 15].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 12,
      "context" : "We compare the difference in performance between the standard logistic regression NLL and the weighted version WNLLCPA using L-BFGS initialized with SGD [5, 15].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "In the next steps, we will continue our research in two different directions: first, we plan to compare our current approach with other cost-sensitive learning methods, such as the “costing” approach [10].",
      "startOffset" : 200,
      "endOffset" : 204
    } ],
    "year" : 2017,
    "abstractText" : "One of the most challenging problems in computational advertising is the prediction of ad click and conversion rates for bidding in online advertising auctions. State-ofthe-art prediction methods include using the maximum entropy framework (also known as logistic regression) and log linear models. However, one unaddressed problem in the previous approaches is the existence of highly non-uniform misprediction costs. In this paper, we present our approach for making cost-sensitive predictions for bidding in online advertising auctions. We show that one can get significant lifts in offline and online performance by using a simple modification of the logistic loss function.",
    "creator" : "LaTeX with hyperref package"
  }
}