{
  "name" : "1301.7414.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bayesian Networks from the Point of View of Chain Graphs*",
    "authors" : [ "Milan Studenyt" ],
    "emails" : [ "studeny@utia.cas.cz" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nTwo traditional approaches to description of proba bilistic conditional independence structures use undi rected graphs (Markov networks) and directed acyclic graphs (Bayesian networks) - see (Pearl 1988). Markov networks have lines (undirected edges) while Bayesian\n*This work was supported by the grants of Grant Agency of Czech Republic n. 201/98/0478, of Grant Agency of Academy of Sciences of Czech Republic n. K1075601 and n. A1075801 and of Ministry of Education of Czech Republic n. VS98008.\ntThe second affiliation is Laboratory of Intelligent Sys tems, University of Economics, Ekonomicka 957, 14800 Prague, Czech Republic. E-mail: studeny@utia.cas.cz\nnetworks have arrows (directed edges). In middle eighties Lauritzen and Wermuth (1984) introduced the class of chain graphs, that is acyclic hybrid graphs hav ing both lines and arrows. Since then many theoretical results analogous to the results concerning Bayesian networks were achieved - for an overview see (Studeny 1996).\nChain graphs provide an elegant unifying point of view on Markov and Bayesian networks. However, researchers interested in graphical modelling of prob abilistic structure may still have sensible objections against (wider acceptance of) chain graphs. Let us mention three of them.\n1. The original way of introducing of the class of Markovian distributions with respect to a chain graph by means of the moralization criterion (Lauritzen 1989) seems to be too complex to be re membered immediately. It does not lead directly to an evident interpretation of chain graph mod els.\n2. The first version of the separation criterion for chain graphs (Bouckaert and Studeny 1995) seems even more complicated, especially in comparison with d-separation criterion for Bayesian networks. Moreover, it is not evident whether it can be im plemented locally.\n3. Many researchers believe that Bayesian networks provide a sufficiently general class of probabilis tic models. Why one should use a more complex models for description of certain situations if one can describe it by a Bayesian network?\nThe aim of this paper is to respond to these three pos sible objections by adequate arguments in favour of chain graphs. In Section 2 some basic concepts are recalled. Then, in Section 3 we show that the class of Markovian distributions with respect to a chain graphs can be sometimes introduced very simply: by means of a factorization formula with respect to the chain graph. This holds in the case of a discrete strictly pos itive probability distribution and also in the case of a chain graph which is Markov equivalent to a Bayesian network (by equivalence is meant that the graphs de-\nBayesian Networks from the Point of View of Chain Graphs 497\nscribe the same probabilistic model, that is they have the same class of Markovian distributions). The for mula leads to a more natural explanation of meaning of a chain graph model: such a model is a result of com bination of pieces of structural information obtained from different experts.\nIn Section 4 a graphical characterization of chain graphs equivalent to Bayesian networks is given. More over, we propose to represent every equivalence class of Bayesian networks by the largest chain graph of the corresponding class of equivalent chain graphs. Note for explanation that the concept of largest chain graph is not identical with the concept of essential graph from (Andersson et. al. 1997a). Such a way of representa tion has certain advantage. First, one has a relatively simple graphical characterization of the largest chain graphs (equivalent to Bayesian networks) in graphical terms. Then every Bayesian network within the equiv alence class can be obtained from the largest chain graph by directing its edges. Second, every chain graph within the corresponding equivalence class (in cluding every mentioned Bayesian network) leads to a certain factorization formula which can be used to store Markovian distributions in memory of a com puter. The formula with respect to the largest chain graph seems to lead to the most effective way from the point of view of memory demands (among formulas).\nIn Section 5 a simplified version of the separation criterion for chain graphs is presented. It resembles the well-known d-separation criterion for Bayesian net works very much. Moreover, we show that it can be implemented locally . Section 6 (Conclusions) summa rizes content.\n2 BASIC CONCEPTS\nThroughout the paper N will denote a non-empty fi nite set of variables which will be identified with nodes of graphs. For sake of brevity, juxtaposition UV will denote the union U U V of sets of variables U, V C N.\n2.1 GRAPHS\nA hybrid graph G over N (= the set of nodes) is speci fied by a set of two-element subsets of N, called edges, where for every edge { u, v} just one of the following three cases occurs. Either it is a line between u and v (= undirected edge), denoted by u - v, or an ar row from u to v ( = directed edge), denoted by u -+ v, or an arrow from v to u, denoted by u � v. An undirected graph is a hybrid graph without arrows, a directed graph is a hybrid graph without lines. The underlying graph of G is obtained from G by replacing arrows with lines. Every set 0 =f. A C N induces a subgraph over A, denoted by GA, which has exactly those edges in G which are subsets of A. Components of G are obtained by removing all arrows of G and taking the connectivity components of the remaining undirected graph.\na b c\ndtd? ' 0-0 f g\nFigure 1: Example of a chain graph.\nA chain graph is a hybrid graph whose components can be ordered into a sequence C1, ... , Cn, n � 1 (called a chain) such that if { u, v} is an edge with u, v E C; then u- v, and if {u, v} is an edge with u E C;, v E Cj, i < j then u -+ v. For several other equivalent defini tions see (Studeny 1997). The reader can easily verify that chain graphs involve both undirected graphs and directed acyclic graphs. Figure 1 gives an example of a chain graph. The corresponding chain of components is {a, b, c}, {d,e}, {f, g}.\nThe set of parents of a set A C N denoted by paa(A) is { v E N \\ A ; v -+ u for some u E A } . The symbol of the graph G can be omitted if it is clear from context. A sequence of distinct nodes v1 , . . . , Vk, k � 1 such that { v;, v;+ l } is an edge in G for every i = 1 , . . . , k -1 is called a path in G .\n2.2 PROBABILITY\nGiven a collection of non-empty finite sets {X;; i E N}, and 0 =f. A C N the symbol XA will denote TiiEA X;. By convention X0 is a fixed singleton. Whenever x = [x;];eN E XN the symbol XA will denote its projection [x;];EA to XA. A probability distribution over N is specified by a col lection of non-empty finite sets {X;; i E N} and by a function\nP: XN -+ [0, 1) with L {P(x); x E XN} = 1 .\nIf P(x) > 0 for all x E XN , then P is called strictly positive. The marginal distribution of P for 0 f. A C N is a probability distribution pA (over A) defined by:\nP A(y) = L{P(x); x E XN , XA = y} for y E XA.\nWe accept the convention P0 (-) ::= 1 . Having dis joint A, B C N, the conditional probability PAlE is a function on XA x XB defined by pAB([xA,XB]) PAIB(xAi XB) = pB(xB) for XA E XA, XB E XB in case P8(xB) > 0. We accept the convention that P AlB ( XA ixB) = 0 whenever P8(xB) = 0. Note that PAI0 = pA. Let us denote by T (N) the class of triplets (A, BIG) of disjoint subsets of N whose first two components A\nand B are nonempty. Having (A, BIG} E T(N) and a probability distribution P over N we say that A is conditionally independent of B given C with respect toP and write AlL B I C [P] if"
    }, {
      "heading" : "PAjBc(xAI [xB,xc]) = PAjc(xAixc)",
      "text" : "for every XA E XA, XB E XB, xc E Xc.\n3 FACTORIZATION FORMULA\nThe aim of this section is to show that probability distributions and chain graphs can be related quite simply by means of a factorization formula for chain graphs which has a natural intuitive interpretation.\nA basic concept is factorization with respect to an undirected graph. Given an undirected graph G over N, a set A C N is complete if u - v for every dis tinct u, v E A. A clique of G is a maximal complete set with respect to inclusion. A non-negative func tion f defined on XN is factorizable according to G if there exists a collection of non-negative functions { 'lj; K ; /{ E K } where K the class of cliques of G and 'lj;K is defined on XK such that\nf(x) = II 'lj;K(XK) for x E XN . KE/C\n3.1 FACTORIZABLE DISTRIBUTIONS\nLet G be a chain graph over N and C a component of G. Then by the closure graph for C we understood the undirected graph H (C) over C U paG (C) obtained in this way:\n• take lines and arrows in G (directed) to nodes of C and drop direction (change arrows into lines),\n• connect by a line every pair of distinct nodes of paG(C).\nEquivalently, make paG( C) complete in the underlying graph of Gcupa(C) · Figure 2 shows the closure graphs for components of the graph from Figure 1. Nodes of pa( C) are marked by an asterisk.\nA probability distribution P over N is factorizable with respect to a chain graph G over N if the following two conditions hold:\n(a) component-wise factorization, that is\nP(x) = II Pcjpa(C)(xclxpa(C)) for x E XN cec\nwhere Cis the class of components of G. (b) clique-wise factorization, that is for every compo\nnent C of G the conditional distribution Pqpa(C) is factorizable according to the closure graph H(C).\nThe condition (b) can be equivalently expressed as the requirement that the marginal distribution pCupa(C) is factorizable according to H(C), or that Pcjpa(C) is computed from a distribution Q over CUpaG(C) which is factorizable according to H (C).\nEXAMPLE 3.1 Let G be the graph from Figure 1. A probability distribution P over {a, b, c, d, e, f, g} is fac torizable with respect to G iff P = pabc · Pdeja · Pjgjbde , and moreover pabc = 'lj;ab''lj;bc, Pdeja = 'lj;ad''lj;de, Pjgjbde = 'lj;bdej·'lj;jg where 'lj;'s are arbitrary respective non-negative func tions.\nIn the preceding example, the elementary factors are not specified. Indeed, one has freedom in their choice in general. In the next section we will mention more specific factorizations in which the factors are condi tional (marginal) probabilities computed from P. Such a two-degree factorization formula does not look natural at first sight. However, it has very good intu itive interpretation. Let us consider a situation when our probabilistic model is composed from pieces of (structural) information obtained from different ex perts. Thus, each expert has his/her exclusive non empty area of competence; the areas are disjoint and cover together the whole set of factors N. The ex pert is supposed not only to give information about structural relationships within his/her area of com petence but also indicate outside factors which influ ence the factors within the area of competence. To prevent discrepancy between experts we order the ar eas of competence into a sequence C1, . .. , Cn, n ;::: 1 and ask every expert to indicate the influential fac tors from preceding areas only. That is, the i-th ex pert can 'provide' information about the conditional probability Pc,jc,u ... uc,_, (i = 1, . . . , n) in the form of Pc,jpa(C;) where pa(C;) C C1 U . . . U Ci-1 is the influence area determined by the expert. However, we suppose that the expert can provide more detailed information about the structure of Pc;jpa(C;)· Thus, we want him/her to indicate how it factorizes, that is to provide the corresponding undirected graph H ( C;) over C; U pa( C;). Since the expert has C; as the lim ited area of competence, he/she is not entitled to evi dence relationships within pa( C;). Thus, it is natural\nBayesian Networks from the Point of View of Chain Graphs 499\nto suppose that pa(G;) is a complete in H(C;) (this is mathematical representation of 'missing' structural information). For simplicity we suppose that C; is a connected subset in each H(G;). Altogether, Pis fac torizable with respect to a certain chain graph over N having G1, ... , Gn as components. Indeed, for every C, create 'local' hybrid graph G(G;) from H(Ci): re move edges within pa( G;) and direct edges from pa( Ci) to C;. Then compose G of these local hybrid graphs. Let us mention a special case of Bayesian networks. In this case components are singletons and P is factoriz able with respect to an acyclic directed graph G iff P = IleN P;lpa(i)· This well-known formula than can be interpreted as above: this time the i-th expert has G; = { i} as his/her area of competence and his/her only role is to indicate pa(G;) (H(C;) is complete!).\n3.2 MARKOVIAN DISTRIBUTIONS\nLet us recall the original way how probabilistic struc ture was ascribed to a chain graph (Lauritzen 1989), (Frydenberg 1990). Supposing G is a chain graph over N, its moral graph is an undirected graph over N in which { u, v} is an edge iff { u, v} is an edge in G or u, v E paG(C), u f. v for a component C of G. For example, the closure graph H(C) mentioned in 3.1 is nothing but the moral graph of Gcupa(C)· Having a set of nodes A C N the symbol anG(A) denotes the set of ancestors of A, that is the set of those nodes v E N that there exists a path v = w1, . . . , Wk = u, k 2: 1 in G from v to a node u E A such that w; --> Wi+l or w; - w;+l for 1 :::; i :::; k- 1. Note that A C anG(A). A triplet (A, BIG) E T(N) is represented in a chain graph G (according to the moralization criterion) if every path in the moral graph of G an(ABC) from a node of A to a node of B contains a node of C.\nA probability distribution P over N i s Markovian with respect to a chain graph G over N if A ll B I C [ P] for every triplet (A, BIG) E T(N) represented in G.\nLEMMA 3.1 Every probability distribution factorizable with respect to a chain graph G is Markovian with re spect to G.\nProof: Suppose that P is factorizable with respect to G, and (A, BIG) E T(N) is represented in G ac cording to the moralization criterion. One can find a chain of components C1, ... , Cn, n 2: 1 such that an(ABC) = G1 U . . . U Cm for some 1 :::; m :::; n. We leave it to the reader to verify using this fact that pan(ABC) is factorizable with respect to Gan(ABC)· Hence, pan( ABC) is factorizable according to the moral graph of Gan(ABC) · Therefore pan(ABC) is Markovian with respect to the moral graph of Gan(ABC) - see (Lauritzen et. al. 1990). Hence All B I G [P]. D Thus, the factorization property implies Markovness. The converse holds often, too. Frydenberg (1990) showed that Markovness implies factorization in case of strictly positive distributions. In sequel we show\nthat both conditions are also equivalent for certain special chain graphs. Thus, the class of Markovian distributions can be often introduced very simply by means of the factorization formula.\n4 MARKOV EQUIVALENCE\nWe say that two chain graphs over N are Markov equivalent if their classes of Markovian distributions coincide. Let us recall Frydenberg's (1990) graphical characterization of Markov equivalence which general izes an analogous result for Bayesian networks (Verma Pearl 1991).\nA complex in a chain graph G is a special induced subgraph of G, namely a path v1, ... , Vk, k 2: 3, such that v1 --> v2, Vi - Vi+1 fori = 2, ... , k-2, Vk-1 f- vk in G, and no additional edges between nodes of { v1, ... , Vk} exist in G. For example, the only complexes in the graph from Figure 1 are b-> f f- d and b-> f +-e.\nTHEOREM 4.1 Two chain graphs over N are Markov equivalent iff they have the same underlying graph and the same complexes.\n4.1 CHAIN GRAPHS MARKOV EQUIVALENT TO BAYESIAN NETWORKS\nAn undirected graph G is called decomposable if the collection of its cliques can be ordered into a sequence K1, . . . , Kk, k 2: 1 satisfying the running intersection property, that is\nVi> 2 3j < i K; n CU Kz) C I<i. l<i\nThen, for every clique K of G, one can find such an or dering which starts by K. For the proof of this fact and further equivalent definitions of a decomposable graph see (Lauritzen 1996). In (Andersson et. al. 1997b) the following result is shown.\nTHEOREM 4.2 A chain graph G is Markov equivalent to an acyclic directed graph (Bayesian network) iff for every component G of G the closure graph H( C) is decomposable.\nAn important fact is that a probability distribution Q factorizable according to a decomposable undirected graph can be factorized in such a way that the factors are conditional probabilities (computed from Q). In fact, there exists a distinguished formula for Q in terms of its marginals.\nLEMMA 4.1 Let Q be a probability distribution factor izable according to a decomposable undirected graph G, and K 1 , . . . , Kk, k 2: 1 is an ordering of its cliques sat isfying the running intersection property. Then\nk QK;\nQ = II (1) . QK;n(U . K1) •=1 '<•\n500 Studeny\nk Q }] QK;\\ (U1<; K ,) 1 K;n (U,<; K 1) ( 2)\nProof: One can use induction according to the num ber of cliques k. The asumption implies that Q = 1/JL ·1/JM where L\n= Ui<k Ki, M = Kk. We leave it to the reader to show that Q = ( QL · QM) I QL nM. Since QL is factorizable according to G L (which is decom posable) one can apply the induction assumption to QL. 0 The sets Si = Ki n ( Uz< i Kz) determined by the chosen sequence K 1, ... , Kk are called separators of the se quence (Lauritzen 1996). It may happen that Si = Si for i =F j. However, the set of separators and the num ber of their occurences in s1' . . . 'sk does not depend on' the choice of the sequence K 1, ... , K k satisfying the running intersection property - see for example Lemma 2.18 in (Studeny 199 2). In particular, the expression ( 1) for Q in Lemma 4.1 does not depend on the choice of K1, ... ,Kk. Thus, in case of a chain graph G which equivalent to a Bayesian network the factorization formula from Sec tion 3.1 can be made more specific. Indeed, for each component C of G one can choose a clique of the clo sure graph H(C) containing paa(C). Then, one can find an ordering K1, ... , Kk(C), k(C);::;: 1 of the cliques of H (C) satisfying the running intersection property startin� by the chosen clique and apply Lemma 4.1 to Q = p upa(C). After that, one can write by ( 2)\npKl k(C) Pclpa(C) = ppa(C) · II PK;\\ S;IS; i=2\nwhere Si are the corresponding separators. This leads to the following global factorization:\nk(C) P = II ( PK1 \\pa(C) lpa(C) · IT PK;\\ S; IS;)·\nCEC i:2 Note that in case G is directly an acyclic directed graph it collapses to the classical formula p = f1iEN pilpa(i)· EXAMPLE 4.1 To illustrate it let us continue with Ex ample 3.1. Each closure graph from Figure 2 is decom posable. Both cliques of the component {a, b, c} con tain pa( {a, b, c}) = 0. Hence, there are two possible ways of writing pabc , that is\neither pabc = pab . Pclb or pabc = pbc . Palb . However, the required ordering of cliques is unique for the other components: Pdela = Pdla · Peld' Pfglbde = pflbde · Pglf · This leads to two factorization formulas for P hav ing conditional probabilities as basic factors. Never theless, one can also write it using (1) as a 'ratio of marginals' which leads to the following formula:\npab . pbc pad . pde pbdef . pig p = pb pa . pd pbde . p f ·\nAs explained in the following remark one can obtain such an 'unique' formula for every chain graph which is Markov equivalent to a Bayesian network.\nRemark In the formula before Example 4.1 P is ex pressed as a product of conditional probabilities. The overall number of factors is the number of cliques in all closure graphs for components. However, as men tioned in Example 4.1, the formula may depend on the choice of orderings of cliques. Let us mention two ways how to avoid seeming ambiguity. First, one can use the first expression ( 1) from Lemma 4.1 for p Cupa(C) which leads to the formula\nII TIKEX:(C) pK p = CEC ppa(C). TisES(C)(P S )m(S)'\nwhere JC(C) is the class of cliques of H(C), S(C) is the class of separators of H (C), and m( S) denotes the number of occurences of a separator S. Here, the el ementary factors are only marginals of P . Seemingly, the number of factors both in numerator and denomi nator is the overall number of cliques. However, some factors can cancel out. I have some reasons to believe that this form of expression for P even does not depend on the choice of the graph from the class of Markov equivalent chain graphs. The second way is the formula which has elemen tary factors in the form of conditional probabilities PA\\pa(C) 1 Anpa(C) where A is either a clique or a sep arator of H(C) for a component C. Indeed, since pa(C) c K1 one has Ki n pa(C) = Si n pa(C) for i ;::;: 2 and therefore\nII TIKEX:(C) PK\\pa(C) I Knpa(C) p = CEC f1sES(C)(Ps\\pa(C) I S npa(C))m(S)\nPROPOSITION 4.1 Let G be a chain graph which is Markov equivalent to a Bayesian network. Then every Markovian distribution with respect to G is factorizable with respect to G.\nProof: Let us fix a chain of components C1, . .. , Cn, n > 1 of G. Then use Theorem 4.2 and fix a re spe�tive ordering K1, ... , Kk(C), k(C) ;::;: 1 of cliques of H(C) for every component C (see above). Con struct a sequence of all nodes of N in the follow ing way: take components in their order and within each component consider the blocks B1 = K 1 \\ pa( C), Bi = Ki \\ Ui<i B i for i ;::;: 2, in their order (the order within those blocks is immaterial). This ordering is consonant with orientation of arrows in G. One can direct all edges of G according to this ordering and obtain an acyclic directed graph D which is Markov equivalent toG by Theorem 4.1. Thus, every distribu tion P which is Markovian with respect toG is Marko vian with respect to D. By Theorem 1 in (Lauritzen et. al. 1990) P is factorizable with respect to D, that is P = Ti iEN p iupav(i) I ppav(i). Hence, the reader can verify that P is factorizable with respect to G. 0\nBayesian Networks from the Point of View of Chain Graphs 501\nFigure 3: Example of a Bayesian network and the cor responding largest chain graph.\n4.2 LARGEST CHAIN GRAPH\nSupposing G and H are Markov equivalent chain graphs over N we say that G is larger than H and write H-< G if every arrow in G is an arrow in H with the same orientation. Informally, G has 'more' lines than H. Frydenberg (1990) showed that within each Markov equivalence class Q of chain graph there exists a chain graph Goo E Q which is larger than every other G E Q. The graph Goo is then called the largest chain graph of g. Let us recall a graphical characterization of the largest chain graphs from (Volf and Studeny 1998).\nAn arrow u -r v covers an arrow x -r y in a chain graph G if u is an ancestor of x and y is an ancestor of v in G. An arrow u -r vis protected in G if it covers an arrow which belongs to a complex in G. In particular, every complex arrow is protected.\nTHEOREM 4.3 A chain graph is the largest chain graph (of a Markov equivalence class of chain graphs) iff every its arrow is protected.\nIn fact, Goo has an arrow u -r v iff u -r v is shared by all graphs from g. By the preceding result, these shared arrows can be identified as protected arrows in every G E Q.\nEXAMPLE 4.2 Let us consider the acyclic directed graph D in the left picture in Figure 3. The only protected arrows in D are the arrows of the complex a -r c ._ b. Thus, the corresponding largest chain graph G00 is in the right picture of Figure 3. The factorization formula with respect to D is P = pa · pb · Pcjab · Pdjc · Pejcd · Pfjcde · Note that each Markov equivalent Bayesian network has the arrows c -r d, c -r e and c -r f. Thus, Markov equivalent Bayesian networks differ only in permutation of nodes d, e, f. In particular, all equiv alent Bayesian networks induce the same type of the factorization formula. On the other hand, the factor ization formula with respect to Goo is different:\nP = pa · pb · Pcjab · Pdeflc ·\nHow it is related to Bayesian networks? Let 1) be a class of Markov equivalent Bayesian networks. Well, every D E 1) is only an auxiliary tool to describe the (common) underlying probabilistic structure (induced by each D E 1J) which is the crucial concept. Is it possible to represent the structure by a distinguished graph which reflects (only) the substantial features of the structure? This problem has not an appropri ate solution within the framework of acyclic directed graphs. All equivalent Bayesian networks from Exam ple 4.2 are equally entitled to represent the correspond ing probabilistic structure but none of them expresses exchangeability of d, e, f. However, perhaps a solution can be found in a wider class of graphs. Let us con sider the class Q of all chain graphs which are Markov equivalent to the graphs in 1J. Of course, Q is wider than 1) but the point is that the graphs from Q de scribe the same probabilistic structure as the graphs from 1)!\nWe propose to represent the structure by the largest chain graph Goo ofQ. Note that Theorem 4.3 together with Theorem 4.2 gives a graphical characterization of the largest chain graphs equivalent Bayesian net works. This approach has several advantages. First, the choice of the representative is made on basis of a 'fair' mathematical criterion - the maximal number of undirected edges. Second, every Bayesian network D E 1J can be obtained from Goo by directing all its lines. The point of view of chain graphs gives a bet ter insight into the class of equivalent graphs. While Goo is the maximal graph with respect to -< within Q Bayesian networks are the minimal graphs. Perhaps the task of checking equivalence of two Bayesian net works D1 and D2 looks more transparent now. They are equivalent iff their corresponding largest chain graphs coincide. Instead of 'converting' arrows in D1 to obtain D2 one can apply an algorithm transform ing both into the corresponding largest chain graph (Studeny 1997). Third, the factorization formula with respect to the largest chain graph mentioned in 4.1 of fers a promising method how to represent Markovian distributions in memory of a computer. Let us returm to Example 4.2. The difference between respective for mulas is that the term Pdeflc from the formula induced by Goo is in the formula induced by D formally disin tegrated:\nPdeflc = Pdjc · Pejcd · Pficde · However, the memory demands for Pdeflc and for Pfjcde are the same: in both cases one needs IXcdef I numbers.1 Thus, the terms Pdjc and Pejcd only raise memory demands. I think that this holds in general.\n1The reader can object that only IXBI· (IXAI-1) values suffices to represent PAlE since the values of a probability distribution sum to l. In fact, this 'actual algebraic di mension' of Pdefic and one of Pdlc · Peicd · Pficde are the same, namely IXcl · (IXdefl - 1). It does not depend on the choice of the graph at all, I guess. However, this ex tremely frugal way of 'parametrization' almost surely leads to other computational difficulties. I doubt whether this is\n502 Studeny\nThe number of factors in the factorization formula from 4.1 (the formula induced by a Bayesian network is a special case) is the number of cliques of all closure graphs. One can expect less number of these cliques in a graph with less number of components. And the min imal number of components within g has the largest chain graph of Q! An intuitive hope that the above mentioned approach could be computationally feasi ble is based on Theorem 4.2. Distributions complying with decomposable models can be represented in form of a junction tree. This is a basis of a local computa tion algorithm based on 'probability propagation' - see (Lauritzen 1996). Perhaps one can somehow to utilize the fact that in the formula from 4.1 every Pc!pa(C) (or equivalently pCupa(C)) complies with 'local' de composable modeL\nRemark Andersson et. a/. (1997a) proposed to rep resent every class V of Markov equivalent Bayesian networks by the essential graph of V which has the same underlying graph as every D E V and only those arrows which are shared by all graphs in V. This is always a chain graph. Example 4.2 shows that the cor responding Markov equivalence class of chain graphs g is wider than V and therefore the essential graph does not coincide with the corresponding largest chain graph, in general.\n5 SEPARATION CRITERION\nThe aim of this section is to formulate a separation criterion for chain graphs in such a way that its affinity with d-separation is evident. Moreover, we indicate how to implement it locally.\n5.1 SUPERACTIVE ROUTES\nA route in a chain graph G is a sequence of nodes p: VI, ... , Vk, k 2:: 1 such that {v;, v;+I} is an edge of\nG for every i = 1, . . . , k - 1. The difference from the concept of path is that nodes in a path are distinct, but the nodes in a route can be repeated! A section of p is its maximal undirected sub route v; - ... - Vj, 1 � i � j � k. If 1 < i � j < k, Vi-I -> v; and Vj +- Vj+l, then it is called a head-to-head section. In case of a Bayesian network sections are just single nodes. Suppose that C is a set of nodes of a chain graph G. We say that p is superactive with respect to Ciff\n• every head-to-head section of p has a node of C,\n• every other section of p is outside C.\nA triplet (A, BIG) E T(N) is represented in G (accord ing to the separation criterion) if there is no route in G from a node in A to a node in B which is superactive with respect to C.\nan effective way of internal representation of distributions in a computer.\nWhat does it mean in case of Bayesian networks? A route p in an acyclic directed graph D is superactive iff\nu E C {::} u is a head-to-head node\nfor every node u of p. This condition is stronger than the concept of active route (Pearl 1988). In an ac tive route u non-head-to-head nodes are outside C and head-to-head nodes have descendants in C. But when ever u is a head-to-head node in u outside C, then there exists a path u = WI -> .. . -> Wk E C, k 2:: 2 with {wi, . . . , Wk- d n C = 0 and u can be patched by w1 -> . . . -> wk +- . . . +- w1. Thus, any active route can be modified into a superactive route. Therefore, one can formulate d-separation criterion in the follow ing way. A triplet (A, BJC) E T (N) is represented in D if there is no route between A and B which is super active with respect to C. I think that this formulation of d-separation is even simpler than the original one. The surpising simplicity is due to the fact we consider the class of all routes which may be infinite. Every ac tive route can be shorthened to an active path but this is not true for superactive routes. In either case, it is evident that the above mentioned separation criterion for chain graphs generalizes (a simplified version of) d-separation.\nNote that the original c-separation criterion ( = separa tion criterion for chain graph) was more complicated. It was formulated for certain finite class of routes, called trails, and an auxiliary concept of slide was nec essary. The proof of the following lemma is analogous to the proof of Proposition 3 from (Lauritzen et. a/. 1990). We omit it for page limitation.\nLEMMA 5.1 Let G be a chain graph over N. Then (A, BJC) E T(N) is represented in G according to the separation criterion iff it is represented in G according to the moralization criterion.\n5.2 ALGORITHM\nPotentially infinite number of routes in a chain graph may cause doubts whether c-separation can be imple mented on a computer. To show that is possible we propose an algorithm which by 'local propagation' in dicated the nodes connected by a superactive route.\nInput A chain graph G over N, A, C C N disjoint.\nSince the class 'I of triplets represented in G satis fies the following 'composition property' (Studeny and Bouckaert 1998):\n(A, B1JC), (A, B2JC) E 'I � (A, BI U B2JC) E 'I,\nthere exists unique maximal B C N \\ AC (possibly empty) such that (A, BIG) is represented in G.\nOutput Maximal B C N \\ AC such that (A, BJC) is represented in G according to the separation criterion.\nInitiation Four sets U, V, W, Z C N will be modified dynamically by the algorithm. Put U = A, V = W = z = 0.\nBayesian Networks from the Point of View of Chain Graphs 503\nInference rules This will be done by the use of the following 'propagation' rules applicable to edges { u, v} in G (one of them is applied to nodes u E N only).\n1. u E U, u - v, v fl. C :::} v E U,\n2. u E U, u <--- v, v fl. C :::} v E U,\n3. u E UV, u-+ v, v fl. C :::} v E V ,\n4. u E V, u- v, v fl. C :::} v E V ,\n5. u E UV, u-+ v :::} v E W ,\n6. u E W, u- v :::} v E W ,\n7. u E W, u E C :::} v E Z,\n8. u E Z, u- v :::} v E Z,\n9. u E Z, u <--- v, v fl. C :::} v E U.\nStopping rule The algorithm will end when preced ing rules cannot enlarge the sets U, V, W, Z. Then put B = N\\UVC.\nLEMMA 5.2 The algorithm above indicates as UV the nodes v E N such that there exists a route in G from u E A to v which is superactive with respect to C.\nProof: Let us ascribe meaning to sets U, V, W, Z. The set V contains those v E N such that there exists a superactive route from u E A to v whose last section has the form w; -+ Wi+l - ... - wi+k = v, k 2:: 1. The set U involves those v E N such that there exists a superactive route from u E A to v whose last section has not such a form. The set W contains those v E N that there exists u E UV and a route u = to -+ t1 - ... - tr = v, r 2:: 1 in G. And Z denotes v E N such that there exists a route u = to -+ t1 - ... - tr = v, r 2:1 in G with u E UV and {t1, ... ,tr}n C ::j: 0. We leave it to the reader to check that the rules above have to indicate gradually all nodes of U, V, W, Z. 0\n6 CONCLUSIONS\nLet us summarize the paper. Section 3 responds to an objection that the way of introducing the class of Markovian distributions for chain graphs is too com plex. The factorization formula is quite simple, has reasonable interpretation, and fits in case of chain graphs equivalent to Bayesian networks (Lemma 3.1, Proposition 4.1) . Section 4 tries to show that chain graphs can be useful even in situations which can be described by Bayesian networks. We propose to repre sent the class of equivalent Bayesian networks by the corresponding largest chain graph and argue that it leads to an effective way of computer representation of Markovian distributions. Section 5 responds to an objection that the separation criterion for reading in dependences from a chain graph is too complicated in comparison with d-separation. Much simpler version of c-separation is presented. Moreover, we propose a method how to implement it in such a way that in each step only neighbor nodes are consulted.\nAcknowledgments\nI would like to express my thanks to anomymous re viewers for their comments.\nReferences\nS.A. Andersson, D. Madigan, and M.D. Perlman (1997a). A characterization of Markov equivalence classes for acyclic digraphs. Ann. Statist. 25: 505- 541.\nS.A. Andersson, D. Madigan, and M. D. Perlman (1997b). On the Markov equivalence of chain graphs, undirected graphs, and acyclic digraphs. Scand. J. Statist. 24: 81-102.\nR.R. Bouckaert, and M. Studeny ( 1995). Chain graphs: semantics and expressiveness. In Ch. Froide vaux, and J. Kohlas (eds.) , Symbolic and Quantita tive Approaches to Reasoning and Uncertainty (Lec ture Notes in AI 946) , 67-76, Springer-Verlag.\nM. Frydenberg (1990). The chain graph Markov prop erty. Scand. J. Statist. 17: 333-353.\nS.L. Lauritzen, and N. Wermuth (1984). Mixed in teraction models. Res. rep. R-84-8, Inst. Elec. Sys., University of Aalborg, Denmark.\nS.L. Lauritzen (1989). Mixed graphical association models. Scand. J. Statist. 16: 273-306.\nS.L. Lauritzen, A.P. Dawid, B.N. Larsen, and H.-G. Leimer (1990). Independence properties of directed Markov fields. Networks 20: 491-505.\nS.L. Lauritzen (1996). Graphical Models. Clanderon Press.\nJ. Pearl (1988). Probabilistic Reasoning in Intelli gent Systems: Networks of Plausible Inference, Mor gan Kaufmann.\nM. Studeny (1992). Multiinformation and conditional independence II. Res. rep. 1751, Inst. Inf. Theory and Autom., Prague, Czech Republic.\nM. Studeny (1996) . On separation criterion and re covery algorithm for chain graphs. In E. Horvitz, and F. Jensen ( eds. ), Uncertainty in Artificial Intelligence 12, 509-516, Morgan Kaufmann.\nM. Studeny (1997). On recovery algorithm for chain graphs. Int. J. Approx. Reasoning 17: 265-293.\nM. Studeny, and R. R. Bouckaert (1998). On chain graph models for description of conditional indepen dence structure. Accepted in Ann. Statist.\nT. Verma, and J. Pearl (1991). Equivalence and syn thesis of causal models. In Uncertainty in Artificial Intelligence 6, 220-227, Elsevier.\nM. Volf, and M. Studeny (1998). A graphical charac terization of the largest chain graphs. In preparation."
    } ],
    "references" : [ {
      "title" : "A characterization of Markov equivalence classes for acyclic digraphs",
      "author" : [ "S.A. Andersson", "D. Madigan", "M.D. Perlman" ],
      "venue" : "Ann. Statist. 25: 505541.",
      "citeRegEx" : "Andersson et al\\.,? 1997a",
      "shortCiteRegEx" : "Andersson et al\\.",
      "year" : 1997
    }, {
      "title" : "On the Markov equivalence of chain graphs, undirected graphs, and acyclic digraphs",
      "author" : [ "S.A. Andersson", "D. Madigan", "M.D. Perlman" ],
      "venue" : "Scand. J. Statist. 24: 81-102.",
      "citeRegEx" : "Andersson et al\\.,? 1997b",
      "shortCiteRegEx" : "Andersson et al\\.",
      "year" : 1997
    }, {
      "title" : "Chain graphs: semantics and expressiveness",
      "author" : [ "R.R. Bouckaert", "M. Studeny" ],
      "venue" : null,
      "citeRegEx" : "Bouckaert and Studeny,? \\Q1995\\E",
      "shortCiteRegEx" : "Bouckaert and Studeny",
      "year" : 1995
    }, {
      "title" : "The chain graph Markov prop­ erty",
      "author" : [ "M. Frydenberg" ],
      "venue" : "Scand. J. Statist. 17: 333-353.",
      "citeRegEx" : "Frydenberg,? 1990",
      "shortCiteRegEx" : "Frydenberg",
      "year" : 1990
    }, {
      "title" : "Mixed in­ teraction models",
      "author" : [ "S.L. Lauritzen", "N. Wermuth" ],
      "venue" : "Res. rep. R-84-8, Inst. Elec. Sys., University of Aalborg, Denmark.",
      "citeRegEx" : "Lauritzen and Wermuth,? 1984",
      "shortCiteRegEx" : "Lauritzen and Wermuth",
      "year" : 1984
    }, {
      "title" : "Mixed graphical association models",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : "Scand. J. Statist. 16: 273-306.",
      "citeRegEx" : "Lauritzen,? 1989",
      "shortCiteRegEx" : "Lauritzen",
      "year" : 1989
    }, {
      "title" : "Independence properties of directed Markov fields",
      "author" : [ "S.L. Lauritzen", "A.P. Dawid", "B.N. Larsen", "H.-G. Leimer" ],
      "venue" : "Networks 20: 491-505.",
      "citeRegEx" : "Lauritzen et al\\.,? 1990",
      "shortCiteRegEx" : "Lauritzen et al\\.",
      "year" : 1990
    }, {
      "title" : "Graphical Models",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : "Clanderon Press.",
      "citeRegEx" : "Lauritzen,? 1996",
      "shortCiteRegEx" : "Lauritzen",
      "year" : 1996
    }, {
      "title" : "Probabilistic Reasoning in Intelli­ gent Systems: Networks of Plausible Inference, Mor­",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Multiinformation and conditional independence II",
      "author" : [ "M. Studeny" ],
      "venue" : "Res. rep. 1751, Inst. Inf. Theory and Autom., Prague, Czech Republic.",
      "citeRegEx" : "Studeny,? 1992",
      "shortCiteRegEx" : "Studeny",
      "year" : 1992
    }, {
      "title" : "On recovery algorithm for chain graphs",
      "author" : [ "M. Studeny" ],
      "venue" : "Int. J. Approx. Reasoning 17: 265-293.",
      "citeRegEx" : "Studeny,? 1997",
      "shortCiteRegEx" : "Studeny",
      "year" : 1997
    }, {
      "title" : "On chain graph models for description of conditional indepen­ dence structure",
      "author" : [ "M. Studeny", "R.R. Bouckaert" ],
      "venue" : "Accepted in Ann. Statist.",
      "citeRegEx" : "Studeny and Bouckaert,? 1998",
      "shortCiteRegEx" : "Studeny and Bouckaert",
      "year" : 1998
    }, {
      "title" : "Equivalence and syn­ thesis of causal models",
      "author" : [ "T. Verma", "J. Pearl" ],
      "venue" : "Uncertainty in Artificial Intelligence 6, 220-227, Elsevier.",
      "citeRegEx" : "Verma and Pearl,? 1991",
      "shortCiteRegEx" : "Verma and Pearl",
      "year" : 1991
    }, {
      "title" : "A graphical charac­ terization of the largest chain graphs",
      "author" : [ "M. Volf", "M. Studeny" ],
      "venue" : "preparation.",
      "citeRegEx" : "Volf and Studeny,? 1998",
      "shortCiteRegEx" : "Volf and Studeny",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Two traditional approaches to description of proba­ bilistic conditional independence structures use undi­ rected graphs (Markov networks) and directed acyclic graphs (Bayesian networks) - see (Pearl 1988).",
      "startOffset" : 193,
      "endOffset" : 205
    }, {
      "referenceID" : 4,
      "context" : "In middle eighties Lauritzen and Wermuth (1984) introduced the class of chain graphs, that is acyclic hybrid graphs hav­ ing both lines and arrows.",
      "startOffset" : 19,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "The original way of introducing of the class of Markovian distributions with respect to a chain graph by means of the moralization criterion (Lauritzen 1989) seems to be too complex to be re­ membered immediately.",
      "startOffset" : 141,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "The first version of the separation criterion for chain graphs (Bouckaert and Studeny 1995) seems even more complicated, especially in comparison with d-separation criterion for Bayesian networks.",
      "startOffset" : 63,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "For several other equivalent defini­ tions see (Studeny 1997).",
      "startOffset" : 47,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "Let us recall the original way how probabilistic struc­ ture was ascribed to a chain graph (Lauritzen 1989), (Frydenberg 1990).",
      "startOffset" : 91,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "Let us recall the original way how probabilistic struc­ ture was ascribed to a chain graph (Lauritzen 1989), (Frydenberg 1990).",
      "startOffset" : 109,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Frydenberg (1990) showed that Markovness implies factorization in case of strictly positive distributions.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "Let us recall Frydenberg's (1990) graphical characterization of Markov equivalence which general­ izes an analogous result for Bayesian networks (Verma Pearl 1991).",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "For the proof of this fact and further equivalent definitions of a decomposable graph see (Lauritzen 1996).",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : ", Kk are called separators of the se­ quence (Lauritzen 1996).",
      "startOffset" : 45,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "Let us recall a graphical characterization of the largest chain graphs from (Volf and Studeny 1998).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "Frydenberg (1990) showed that within each Markov equivalence class Q of chain graph there exists a chain graph Goo E Q which is larger than every other G E Q.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "Instead of 'converting' arrows in D1 to obtain D2 one can apply an algorithm transform­ ing both into the corresponding largest chain graph (Studeny 1997).",
      "startOffset" : 140,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "This is a basis of a local computa­ tion algorithm based on 'probability propagation' - see (Lauritzen 1996).",
      "startOffset" : 92,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "This condition is stronger than the concept of active route (Pearl 1988).",
      "startOffset" : 60,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "Since the class 'I of triplets represented in G satis­ fies the following 'composition property' (Studeny and Bouckaert 1998):",
      "startOffset" : 97,
      "endOffset" : 125
    } ],
    "year" : 2011,
    "abstractText" : "The paper gives a few arguments in favour of use of chain graphs for description of proba­ bilistic conditional independence structures. Every Bayesian network model can be equiva­ lently introduced by means of a factorization formula with respect to chain graph which is Markov equivalent to the Bayesian network. A graphical characterization of such graphs is given. The class of equivalent graphs can be represented by a distinguished graph which is called the largest chain graph. The factoriza­ tion formula with respect to the largest chain graph is a basis of a proposal how to represent the corresponding (discrete) probability dis­ tribution in a computer (i.e. 'parametrize' it). This way does not depend on the choice of a particular Bayesian network from the class of equivalent networks and seems to be the most efficient way from the point of view of memory demands. A separation criterion for reading indepen­ dences from a chain graph is formulated in a simpler way. It resembles the well-known d-separation criterion for Bayesian networks and can be implemented 'locally'.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}