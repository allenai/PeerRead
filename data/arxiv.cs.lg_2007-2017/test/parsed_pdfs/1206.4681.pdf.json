{
  "name" : "1206.4681.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "LPQP for MAP: Putting LP Solvers to Better Use",
    "authors" : [ "Patrick Pletscher", "Sharon Wulff" ],
    "emails" : [ "pletscher@inf.ethz.ch", "sharon.wulff@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "We study the problem of maximum a posteriori (MAP) inference in graphical models. The MAP task is to compute a minimal energy assignment of a set of dependent variables. This is a crucial problem in many applications such as computational biology, natural language processing and computer vision. In the general case, MAP inference is intractable, and therefore most of the current research efforts are concentrated on finding efficient and accurate approximation algorithms. In recent years, linear programming (LP) relaxations gained popularity due to their proven success in relevant applications. Several efficient algorithms have been developed to solve the linear program emerging from the relaxation. Despite their success, in many practical problems the solution attained by the LP relaxations is still far from the global minima.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nOur work improves over the LP relaxation by leveraging on a second class of relaxations, namely the quadratic programming (QP) relaxation. The QP formulation offers a concise and compact description of the MAP problem. We formulate a joint LP and QP MAP objective, that encourages auxiliary variables present in the LP relaxation, to agree with their counterpart in the QP relaxation, through a penalty function. Despite of the non-convexity of this objective, we show that by slowly increasing the weight of the penalty, the solutions found are either competitive with, or in most cases better than the LP relaxation solutions. This is in general not the case for the few existing QP relaxation solvers.\nWe propose two variants of the penalty function, each leading to a different LPQP objective. We show that the resulting non-convex objectives can be decomposed into a difference of convex functions, which we solve using the convex-concave procedure (CCCP). Having tackled the non-convexity with the CCCP, we solve one of the remaining convex problems with the dual decomposition method, and show that the other can be addressed with the norm-product belief propagation. Interestingly, the main computational task of both of the resulting LPQP algorithms, turns out to be solving known entropy-augmented LPs.\nOur contributions are as follows: First we introduce a combined LPQP objective, incorporating the QP constraints through a soft penalty function in the objective. We propose two alternatives for the penalty function, which differ in the way the edges in the graph are weighted. Secondly, we derive CCCP based algorithms for the LPQP objectives, and show that their core computational effort reduces to current entropyaugmented LP solvers. This demonstrates that these modern LP solvers can in some cases be utilized in a better way, leading to possibly faster convergence, as well as lower energy MAP solutions. Through experiments on various datasets, we demonstrate the performance of the suggested LPQP MAP inference in comparison to other commonly used solvers."
    }, {
      "heading" : "2. Background and Notation",
      "text" : "For an undirected graph G = (V, E), the MAP problem is to assign each node in the graph to a class or category, such that the overall assignment minimizes an associated energy. Let xi denote a discrete variable with a finite domain Xi1, representing the assignment of the i-th node. The MAP problem is defined as\nmin x ∑ i∈V θi(xi) + ∑ (i,j)∈E θij(xi, xj). (1)\nWhere θi(xi) and θij(xi, xj) are unary and pairwise potential functions associated with the node and edge assignments. Problem (1) can be expressed as an integer quadratic program using a K-ary coding:\nmin µ ∑ i∈V θTi µi + ∑ (i,j)∈E µTi Θijµj (2)\ns.t. µi;k ∈ {0, 1} ∀i, k and ∑ k µi;k = 1 ∀i.\nThe pairwise and unary potentials in (2), are represented as a matrix Θij and a vector θi, respectively.\nVariational approaches to MAP inference reformulate the combinatorial optimization problem in (1) as a continuous optimization problem. The next sections formally define two such approaches, namely the LP and QP relaxations. In general, the LP minimization results in a lower bound on the energy of the global minimizer, while the QP results in an upper bound."
    }, {
      "heading" : "2.1. Linear Programming Relaxation",
      "text" : "The LP approach (Schlesinger, 1976; Wainwright & Jordan, 2008) is based on a convex relaxation of (2), where an additional variable µij is included for each edge. Proper local marginalization is enforced through summation constraints. The LP reads as\nmin µ∈LG ∑ i∈V θTi µi + ∑ (i,j)∈E θTijµij , (3)\nwith LG , the local marginal polytope:\nLG = µ ∣∣∣∣∣∣∣∣ ∑ k µi;k = 1 ∀i ∈ V∑ l µij;kl = µi;k ∀k, (i, j) ∈ E∑ k µij;kl = µj;l ∀l, (i, j) ∈ E\nµij;kl ≥ 0 ∀k, l, (i, j) ∈ E  . In the general case, LG is an inexact description of the so called marginal polytope MG , which requires an exponentially large number of constraints (Wainwright\n1For notational convenience we assume Xi ={1, . . . ,K}, in the experiments we will however also consider settings where the domain of the variables has different size.\n& Jordan, 2008). If LG in (3) is replaced by MG , then the solution recovers the true MAP assignment. A solution to an LP-based approach, admits an easy to verify certificate of optimality. If the solution is integer, it is the global optimum.\nThe work in (Sontag et al., 2008) proposes to tighten the polytope by including summation constraints over larger subsets of variables. This approach has been successful in identifying the global minima for some problems. However, it suffers from an increased complexity as ultimately an exponentially large set of possible constraints might need to be searched over."
    }, {
      "heading" : "2.2. Quadratic Programming Relaxation",
      "text" : "An alternative relaxation of the integer quadratic program in (2) is obtained by simply dropping the integer constraints. The resulting QP is given by:\nmin µ ∑ i∈V θTi µi + ∑ (i,j)∈E µTi Θijµj (4)\ns.t. 0 ≤ µi;k ≤ 1 ∀i, k and ∑ k µi;k = 1 ∀i.\nA major advantage of the QP relaxation, is the fact that it is tight. In this context the tightness means that the minimizer of (4) is also the minimizer of (1), as was shown in (Ravikumar & Lafferty, 2006). The QP also benefits from a more compact description compared to the LP relaxation, as it requires fewer constraints and variables to formulate the exact MAP problem. The variable vector µ, is of size K· |V| + K2· |E| in the LP (3) and only K· |V| in the QP. The biggest drawback of the QP relaxation, is that in the general case the optimization problem is non-convex due to the edges product term. This fact renders an exact minimization difficult.\nIn terms of motivation, our work is similar to the QP relaxation approach. The QP formulation of the MAP problem (4) was introduced in (Ravikumar & Lafferty, 2006), but stems from classical mean-field approaches. Ravikumar & Lafferty (2006) solved the non-convex problem using a convex relaxation. The solution was later improved in (Kappes & Schnoerr, 2008) through a difference of convex functions formulation. Both solvers are generic in the sense that they do not exploit the graph structure. Recently Kumar & Zilberstein (2011) introduced a message-passing algorithm for solving the QP relaxation. While improving the run time over the other two algorithms, it still generally suffers from poor solutions due to local minima. The QP solvers often deal with this drawback by restarting with different initializations. We observed that our LPQP algorithms, are much more resilient\nwith respect to the initialization. In all of the experiments we conducted, a restart was never required. We attribute this behavior to the gradual progression between the LP and QP. Finally, in concurrent work Kumar et al. (2012) propose a hybrid LP and QP approach to MAP, similar to our formulation discussed in the next section. The resulting optimization problem is solved by a custom message-passing scheme. Our work on the other hand, in its essence reduces to well-known entropy-augmented LP objectives, for which efficient message-passing algorithms exist."
    }, {
      "heading" : "3. Combined LP and QP Relaxation",
      "text" : "We propose to optimize an objective which is a combination of the LP and QP relaxations. We retain the auxiliary variables µij of the pairwise terms, but force these variables to agree with the product of the unary marginals µi and µj . The constraints, given by vec(µiµ T j ) = µij ∀(i, j) ∈ E2, are enforced through a penalty function g(· ) incorporated in the objective. The extent to which the constraint is enforced, is regulated by the parameter ρ.\nWe focus on the Kullback-Leibler (KL) divergence as the penalty function, due to the probabilistic nature of the compared marginal terms. For probability distributions p and q of a discrete random variable, their KL divergence is defined to be\nDKL(p, q) := ∑ k pk log ( pk qk ) .\nThe general form of the combined objective reads as\nmin µ∈LG\nθTµ+ ρg(µ). (5)\nThe first term is simply the LP objective (3), written as a scalar product between the potential function, and the concatenated unary and pairwise variables. We investigate two constructions of the penalty term. The constructions differ in the weighting of the edges.\nUniform Weighting The KL divergence is penalized in the same way for all the edges in the graph:\nguni(µ) := ∑\n(i,j)∈E\nDKL(µij , vec(µiµ T j )). (6)\nTree-based Weighting The KL divergence is penalized uniformly within a forest-shaped sub-graph:\ngtree(µ) := ∑ a∈A ηa  ∑ (i,j)∈Ea DKL(µij , vec(µiµ T j ))  . (7) 2Here vec(µiµ T j ) denotes the vectorized version of the\nouter product of µi and µj .\nWe assume that a decomposition of the original graph into acyclic subgraphs exists, and is given by\nGa = (Va, Ea), V = ⋃ a∈A Va, E = ⋃ a∈A Ea.\nThe positive weights ηa are tree specific, and assumed to sum to one. In this work we simply used ηa = 1/|A|.\nFor ρ = 0, (5) amounts to the standard LP relaxation. On the other extreme when ρ → ∞, the constraints vec(µiµ T j ) = µij ∀(i, j) ∈ E are fulfilled and the QP relaxation is recovered. By successively increasing ρ during the run of our algorithms, we achieve a gradual enforcement of the constraints."
    }, {
      "heading" : "4. LPQP Algorithms",
      "text" : "In this section we derive two algorithms for the nonconvex LPQP objective in (5), with the different penalty terms in (6) and (7)."
    }, {
      "heading" : "4.1. Difference of Convex Functions (DC)",
      "text" : "The convex-concave procedure (CCCP) (Yuille & Rangarajan, 2003), can be applied to a constrained optimization problem, where the objective is non-convex, provided that the objective has a decomposition into a convex and a concave part. In our setting, we wish to find a decomposition of the form\nmin µ∈LG\nuρ(µ)− vρ(µ),\nwhere both, uρ(µ) and vρ(µ) are convex. The CCCP algorithm proceeds by iteratively solving a convexified objective, obtained by a linearization of vρ(µ):\nµt+1 = argmin µ∈LG uρ(µ)− µT∇vρ(µt). (8)\nThe decompositions of the two objectives, as well as the gradients of the concave part, are shown in Figure 1. In the derivations we used the following identity\nDKL(µij , vec(µiµ T j )) = H(µi) +H(µj)−H(µij),\nwhich holds due to the marginalization constraints of the pairwise marginals (Wainwright & Jordan, 2008).\nFor both objectives, the convex part uρ(µ) consists of the original LP formulation, with an additional term that encourages configurations with a large entropy. This term in the uniform weights penalty, is the entropy of the pairwise marginals, whereas in the treebased penalty, it is the sum of tree entropies.\nThe concave part of the decompositions, vρ, corresponds to an entropy of the unary marginals. In the\nCCCP step (8), log(µi) is replaced by log(µ t i), the marginal from the previous iteration, resulting in an entropy approximation."
    }, {
      "heading" : "4.2. Algorithm Overview",
      "text" : "The general scheme of the suggested LPQP algorithms is shown in Algorithm 1. The algorithm consists of two loops. The inner loop solves the DC problem for a fixed penalty parameter ρ, whereas the outer loop gradually increases the value of ρ.\nAlgorithm 1 LPQP algorithm scheme for MAP. Require: G = (V, E),θ. 1: initialize µ ∈ LG uniform, ρ = ρ0. 2: repeat 3: t = 0,µ0 = µ. 4: repeat 5: µt+1 = argminτ∈LG uρ(τ )− τ\nT∇vρ(µt). 6: t = t+ 1. 7: until ‖µt − µt−1‖2 ≤ dc. 8: µ = µt. 9: increase ρ.\n10: until ‖µ− µ0‖2 ≤ ρ. 11: return µ.\nThe main computational task is in line 5, where a particular instance of a convex optimization problem is solved. Warm-starting the problem in line 5 with the previous solution between successive calls, leads to a substantial speed-up. We choose the initial ρ = ρ0 depending on the scaling of the energies, and use a multiplicative increase with a fixed value. In the experiments we use a multiplicative factor of 1.5, but the results were not very sensitive to this choice.\nSolution Rounding Similarly to the LP and QP relaxations, the solutions returned by the LPQP algorithms can be fractional. Since the LPQP scheme ultimately solves a variant of the QP relaxation, to attain the final integer solutions, we use the QP solution rounding scheme suggested in (Ravikumar & Lafferty, 2006). Given a unary marginals vector µ∗, we assign the i-th node the label x∗i given by\nx∗i = argmin k θi;k + ∑ j∈N (i) ∑ l θi,j;k,lµ ∗ j;l  . Here N (i) denotes the neighbors of node i. After determining the label of the i-th variable, we set µ∗i;x∗i = 1 and µ∗i;k = 0 ∀ k 6= x∗i , and continue until labels are assigned to all nodes. It can be verified that the rounded solution has an energy that is smaller or equal to the energy of the initial solution µ∗."
    }, {
      "heading" : "4.3. Uniform Weighting",
      "text" : "The convex sub-problem we get in the CCCP step with the uniform weighting penalty function (6), is given by\nmin µ∈LG ∑ i∈V θ̃Ti µi + ∑ (i,j)∈E θTijµij − ρ ∑ (i,j)∈E H(µij). (9)\nwhere θ̃i, is a modification of the unary potentials by an additional gradient term, originating in the linearized part of the DC decomposition (8) 3.\nθ̃i = θi − ρdi log(µti), (10) 3The ρdi term in ∇vρ is constant and can therefore be\ndropped.\nAs a result of this unary potentials modification, configurations with small probability in the previous iteration t, are vigorously dis-encouraged.\nBelief Propagation The convex problem in (9) is solved by the norm-product belief-propagation (BP) algorithm (Hazan & Shashua, 2010). It is a primaldual ascent algorithm and is guaranteed to converge to the global optimum for any choice of ρ > 0.\nThe norm-product algorithm applied to (9) computes messages passed from node j to node i as follows\nmj→i(xi) ∝∑ xj ψ 1/ρ ij (xi, xj) ψ 1/(djρ) j (xj) ∏ s∈N (j)m 1/(djρ) s→j (xj) m 1/ρ i→j(xj) ρ\nhere we define ψij(xi, xj) = exp(−θij(xi, xj)) and ψi(xi) = exp(−θ̃i(xi)). Upon convergence the marginals µi are obtained by multiplying the incoming messages at variable i:\nµi(xi) ∝ ψi(xi) ∏ j∈N (i) mj→i(xi) 1/(diρ) . Due to warm starting with the previous DC iteration solution, typically only few passes through the graph are needed for the messages to converge in the later stages of the run."
    }, {
      "heading" : "4.4. Tree-based Weighting",
      "text" : "The convex sub-problem corresponding to the CCCP step with the tree-based weighting penalty (7) is,\nmin µ∈LG ∑ i∈V θ̃Ti µi+ ∑ (i,j)∈E θTijµij−ρ ∑ a∈A ηaH a tree(µ). (11)\nWhere we define the entropy of a tree by\nHatree(µ) :=  ∑ (i,j)∈Ea H(µij)− ∑ i∈Va (dai − 1)H(µi)  . As before, the linearization of the concave part in the CCCP step, results in a modification of the unaries\nθ̃i = θi − ρ ∑\na∈A(i)\nηa log(µ t i). (12)\nDual Decomposition The dual decomposition framework (Bertsekas, 1999; Komodakis et al., 2007), can be applied to an optimization problem provided\nthat the objective can be decomposed into several subproblems, also known in the literature as the slave problems. The global variables, µ in our case, are replaced with local copies in each slave problem, denoted here by νa, such that the minimization of the slave problems can be carried out independently. To enforce the local variables corresponding to the same original variables to assume the same value, a designated constraint is introduced. The optimization of the sum of slave problems, subject to these constraints, is called the master problem. A dual decomposition of problem (11), was carried out in (Domke, 2011). We use the same decomposition, but take a different route optimizing the resulting master problem.\nmin µ∈LG ∑ a∈A min νa∈LGa sa(ν a) (13)\ns.t. νai = µi ∀i, a ∈ A. νaij = µij ∀(i, j), a ∈ A.\nWhere the slave problems are defined as sa(ν) := ∑ i∈Va θ̄Ti νi + ∑ (i,j)∈Ea θ̄Tijνij − ρηaHatree(ν).\nNote that since the summation over the trees now extends to include the unary and pairwise terms, the corresponding potentials should be adjusted accordingly\nθ̄i = θ̃i |A(i)| , θ̄ij = θij |A(i, j)| .\nEach slave problem is defined over a tree structured graph and can therefore be solved exactly using the sum-product algorithm, in two passes over the tree. The temperature in this case is ρηa. We solve the dual of the master problem using the FISTA (Beck & Teboulle, 2009) algorithm. A similar solution was demonstrated in (Savchynskyy et al., 2011), on a more restricted decomposition. We refer to the appendix for more technical details."
    }, {
      "heading" : "4.5. Entropy-augmented LP Solvers",
      "text" : "Recently, several works (Jojic et al., 2010; Savchynskyy et al., 2011) proposed to smooth the LP objective by adding a term that favors entropic marginals. The merit of this additional term is in overcoming the nonsmoothness of the objective. In order to ultimately solve the original LP, these entropy-augmented solvers progressively lower the entropy term. Naturally, the convergence of these algorithms is fairly fast in the beginning. This line of research originates in Nesterov’s work on fast gradient methods (Nesterov, 1983).\nThe proposed LPQP solvers have the opposite behavior with respect to the smoothness of the objective.\nThe influence of the entropy term is rather increased through the progression of the algorithm, leading to favorable convergence properties."
    }, {
      "heading" : "5. Experiments",
      "text" : "We use LPQP-U to refer to the implementation of the uniform weighting penalty, and LPQP-T for the treebased weighting. In the experiments where the graph did not have a natural decomposition, we used a depthfirst search algorithm to construct a tree decomposition in a greedy fashion for LPQP-T.\nBenchmarked Methods We compare the performance of LPQP-U and LPQP-T with the widely used MAP algorithms, tree-reweighted belief propagation (TRWS) (Kolmogorov, 2006) and max-product LP (MPLP) (Sontag et al., 2008), both of which are LP relaxations. For both algorithms we used the implementation made available by the authors. These algorithms represent different trade-offs in performance. TRWS is a highly efficient message-passing algorithm for the standard LP relaxation. It is much faster than the MPLP, especially on large instances where the MPLP convergence is pretty slow. MPLP on the other hand, initially solves the LP relaxation over the local polytope, and in later iterations includes additional summation constraints over sets of three or four variables. This strategy naturally leads to lower (better) energy solutions, on instances where the LP relaxation is not tight. The MPLP was shown to identify the global optimum for some problems.\nPerformance Measures In this work we mainly compared the quality of the solutions, which in the MAP setting is most naturally measured by the energy associated with an assignment (1). Strictly comparing energy values is problematic for two reasons. The values lack proper scaling required for quantitative comparison of different results on the same problem instance, and are not comparable across instances. We therefore exercise the following scoring procedure. Let e1, . . . , eJ denote the energies of the compared solutions, we set\nsi = max1≤j≤J(ej)− ei\nmax1≤j≤J(ej)−min1≤j≤J(ej) (14)\nas the score of the i-th method. This scheme assigns the worst and the best methods, scores of zero and one respectively. The remaining methods get a fraction relative to their value between the best and the worst result. This procedure is not flawless since the scores are still computed relative to the worst energies. It was most often the case though, that TRWS was the lowest\nscoring method. Being an often used algorithm with provable merits, using it as a normalizing measure is in our opinion a sensible choice. In experiments where the optimal value is known, we use this value instead of min1≤j≤J ej . In addition to comparing the quality of the solution, we comment about the trends in the efficiency (run-time) of the various methods."
    }, {
      "heading" : "5.1. Synthetic Potts Model Data",
      "text" : "We follow a similar experimental setup as in (Ravikumar et al., 2010). The graph is a 4-nearest neighbor grid of varying size. We used M = 60, 90, 120 where M is the grid side-length, and M2 is the overall number of variables. We used K = 2 and K = 5 for the number of states. The unary potentials were randomly set to θi;k(xi) ∼ Uniform(−σ, σ), and for σ we used values in [0.05, 0.5]. Note that the problem instance gets harder for small values of σ, this parameter can be understood as the signal-to-noise ratio. The pairwise potentials θij(xi, xj), were set to penalize agreements or disagreements of the labels, by an amount αij ∼ Uniform(−1, 1), chosen at random. We set θij(xi, xj) = 0 if xi 6= xj and αij otherwise. In this experiment we choose the graph decomposition for the LPQP-T solution as the vertical and horizontal split of the grid edges. The two trees have all the original nodes in common, but no overlapping edges.\nThe results of the comparison using the performance measure given in (14), are presented in Table 1. For each choice of parameters, we averaged the scores of 5 runs. Furthermore, Figure 2 shows the progress of the objective during a run of the LPQP-U algorithm.\nIn terms of running time, TRWS was always first to output a solution, followed by the LPQP algorithms. MPLP was always slower and on the larger instances\ndid not converge within a predefined maximal time. We therefore restricted the number of tightening iterations of MPLP to a maximum of 1000. A tightening iteration includes additional constraints into the local marginal polytope. Even after this change, MPLP was still considerably slower than the other algorithms. Between the LPQP algorithms, the LPQP-U was most often faster than LPQP-T.\nAs we expect, TRWS returned the worst assignment on almost all configurations. The energies obtained by LPQP-U, LPQP-T and MPLP were in general very close. We observe that both of the LPQP algorithms, returned slightly better solutions in comparison to the MPLP, when the potentials were sampled with lower signal-to-noise ratio σ.\nThe run time of LPQP-T seems to be mostly influenced by the structure of the decomposition. In later experiments where the decomposition consisted of a larger number of trees with more variables in common, the LPQP-T was significantly slower compared to the LPQP-U. In terms of the energy of the solutions, the two algorithms were very similar. For this reason we report from now on the LPQP-U only. The LPQP-T can still be beneficial in settings where the computations are done on a distributed system."
    }, {
      "heading" : "5.2. Protein Design & Prediction",
      "text" : "The protein inference problem discussed in (Yanover et al., 2006), consists of two tasks: protein side-chain prediction and protein design. For the protein prediction task, it was shown in (Yanover et al., 2006) that only for 30 out of the 370 protein prediction instances, the LP relaxation is not tight. For 28 of them, the true MAP was computed using general integer pro-\ngramming techniques, Figure 3 visualizes the results on these instances. The LPQP found the global minimum of roughly 2/3 of these more difficult instances. On the remaining 340 instances, the LP is tight. The LPQP found the global optimum in all but three cases (results are not shown). MPLP was applied to this task in (Sontag et al., 2008), and achieved the global optimum on all instances.\nThe protein design task consists of 97 instances. We used MPLP to compute the global optimum, but for one of the instances, MPLP did not finish within a time-budget of 7 days. The average scores for the remaining 96 instances are as follows. LPQP-U: 0.93, MPLP: 1 and TRWS: 0.03. The average energies are: LPQP-U: −184.06, MPLP: −184.60, TRWS: −173.55. The QP message-passing algorithm in (Kumar & Zilberstein, 2011), was tested on this task as well. The evaluation criteria used in this work was the average (across the 97 instances) percentage of the optimal value. While the reported average value in (Kumar & Zilberstein, 2011) is 97.7%, our solution achieves 99.7% percentage of the optimal value on average."
    }, {
      "heading" : "5.3. Decision Tree Fields",
      "text" : "As a last experiment we apply our LPQP algorithm to the recently published “hard discrete energy minimization instances” dataset (Nowozin et al., 2011), available on the authors webpage. The task is to fill in, or inpaint, a blanked out area in a binary image of Chinese handwritten characters, see Figure 4. The dataset consists of 100 energy minimization instances, and comes with approximate MAP solutions obtained using simulated annealing (SA) inference, which was found to work better than TRWS. For 43 instances the LPQP algorithm obtained better solutions than the previously best known solutions. Figure 4 visualizes some of the instances where the LPQP algorithm leads to a better solution. We observed that the SA solutions seem to hallucinate too much regularity which"
    }, {
      "heading" : "51.6 46.4 45.9 43.5 42.5",
      "text" : "is not supported by the underlying energy. The scoring of the three algorithms is as follows. LPQP-U: 0.84, SA: 0.74 and TRWS: 0.21. We failed to apply MPLP as the tightening operation did not succeed."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We introduce a novel formulation for MAP inference in graphical models, combining the LP and QP relaxation terms through a KL divergence measure. The resulting problem, albeit being non-convex, gives rise to efficient algorithms built upon known LP solvers."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Joachim Buhmann, Andreas Krause, Cheng Soon Ong and Christian Sigg for insightful discussions. The work was partially supported by the NCCR-MICS, a the Swiss National Science Foundation center, under grant #51NF40-111400."
    } ],
    "references" : [ {
      "title" : "A fast iterative shrinkagethresholding algorithm for linear inverse problems",
      "author" : [ "A Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "Beck and Teboulle,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck and Teboulle",
      "year" : 2009
    }, {
      "title" : "Dual decomposition for marginal inference",
      "author" : [ "J. Domke" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Domke,? \\Q2011\\E",
      "shortCiteRegEx" : "Domke",
      "year" : 2011
    }, {
      "title" : "Norm-product belief propagation: Primal-dual message-passing for approximate inference",
      "author" : [ "T Hazan", "A. Shashua" ],
      "venue" : "IEEE TIT,",
      "citeRegEx" : "Hazan and Shashua,? \\Q2010\\E",
      "shortCiteRegEx" : "Hazan and Shashua",
      "year" : 2010
    }, {
      "title" : "Accelerated dual decomposition for MAP inference",
      "author" : [ "V Jojic", "S Gould", "D. Koller" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Jojic et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jojic et al\\.",
      "year" : 2010
    }, {
      "title" : "MAP-Inference for Highly-Connected Graphs with DC-Programming",
      "author" : [ "J Kappes", "C. Schnoerr" ],
      "venue" : "In DAGM,",
      "citeRegEx" : "Kappes and Schnoerr,? \\Q2008\\E",
      "shortCiteRegEx" : "Kappes and Schnoerr",
      "year" : 2008
    }, {
      "title" : "Convergent tree-reweighted message passing for energy minimization",
      "author" : [ "V. Kolmogorov" ],
      "venue" : null,
      "citeRegEx" : "Kolmogorov,? \\Q2006\\E",
      "shortCiteRegEx" : "Kolmogorov",
      "year" : 2006
    }, {
      "title" : "MRF optimization via dual decomposition: Message-passing revisited",
      "author" : [ "N Komodakis", "N Paragios", "G. Tziritas" ],
      "venue" : "In In ICCV,",
      "citeRegEx" : "Komodakis et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Komodakis et al\\.",
      "year" : 2007
    }, {
      "title" : "Message-Passing Algorithms for Quadratic Programming Formulations of MAP Estimation",
      "author" : [ "A Kumar", "S. Zilberstein" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Kumar and Zilberstein,? \\Q2011\\E",
      "shortCiteRegEx" : "Kumar and Zilberstein",
      "year" : 2011
    }, {
      "title" : "MessagePassing Algorithms for MAP Estimation Using DC Programming",
      "author" : [ "A Kumar", "S Zilberstein", "M. Toussaint" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Kumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2012
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate o(1/k)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet. Math. Dokl.,",
      "citeRegEx" : "Nesterov,? \\Q1983\\E",
      "shortCiteRegEx" : "Nesterov",
      "year" : 1983
    }, {
      "title" : "Decision tree fields",
      "author" : [ "S Nowozin", "C Rother", "S Bagon", "T Sharp", "B Yao", "P. Kohli" ],
      "venue" : "In ICCV, pp",
      "citeRegEx" : "Nowozin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nowozin et al\\.",
      "year" : 2011
    }, {
      "title" : "Quadratic Programming Relaxations for Metric Labeling and Markov Random Field MAP Estimation",
      "author" : [ "P Ravikumar", "J. Lafferty" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ravikumar and Lafferty,? \\Q2006\\E",
      "shortCiteRegEx" : "Ravikumar and Lafferty",
      "year" : 2006
    }, {
      "title" : "Message-passing for graph-structured linear programs: Proximal methods and rounding schemes",
      "author" : [ "P Ravikumar", "A Agarwal", "Wainwright", "M J" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2010
    }, {
      "title" : "A study of Nesterov’s scheme for Lagrangian decomposition and MAP labeling",
      "author" : [ "B Savchynskyy", "J H Kappes", "S Schmidt", "C. Schnörr" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Savchynskyy et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Savchynskyy et al\\.",
      "year" : 2011
    }, {
      "title" : "Syntactic analysis of two-dimensional visual signals in noisy conditions",
      "author" : [ "Schlesinger", "M I" ],
      "venue" : "Kibernetika,",
      "citeRegEx" : "Schlesinger and I.,? \\Q1976\\E",
      "shortCiteRegEx" : "Schlesinger and I.",
      "year" : 1976
    }, {
      "title" : "Tightening LP relaxations for MAP using message-passing",
      "author" : [ "D Sontag", "T Meltzer", "A Globerson", "Y Weiss", "T. Jaakkola" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Sontag et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sontag et al\\.",
      "year" : 2008
    }, {
      "title" : "Graphical Models, Exponential Families, and Variational Inference",
      "author" : [ "M J Wainwright", "Jordan", "M I" ],
      "venue" : null,
      "citeRegEx" : "Wainwright et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright et al\\.",
      "year" : 2008
    }, {
      "title" : "Linear programming relaxations and belief propagation – an empirical study",
      "author" : [ "C Yanover", "T Meltzer", "Y. Weiss" ],
      "venue" : null,
      "citeRegEx" : "Yanover et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Yanover et al\\.",
      "year" : 2006
    }, {
      "title" : "The concave-convex procedure",
      "author" : [ "A L Yuille", "A. Rangarajan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Yuille and Rangarajan,? \\Q2003\\E",
      "shortCiteRegEx" : "Yuille and Rangarajan",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "The work in (Sontag et al., 2008) proposes to tighten the polytope by including summation constraints over larger subsets of variables.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "Finally, in concurrent work Kumar et al. (2012) propose a hybrid LP and QP approach to MAP, similar to our formulation discussed in the next section.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "Dual Decomposition The dual decomposition framework (Bertsekas, 1999; Komodakis et al., 2007), can be applied to an optimization problem provided that the objective can be decomposed into several subproblems, also known in the literature as the slave problems.",
      "startOffset" : 52,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "A dual decomposition of problem (11), was carried out in (Domke, 2011).",
      "startOffset" : 57,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "A similar solution was demonstrated in (Savchynskyy et al., 2011), on a more restricted decomposition.",
      "startOffset" : 39,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "Entropy-augmented LP Solvers Recently, several works (Jojic et al., 2010; Savchynskyy et al., 2011) proposed to smooth the LP objective by adding a term that favors entropic marginals.",
      "startOffset" : 53,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "Entropy-augmented LP Solvers Recently, several works (Jojic et al., 2010; Savchynskyy et al., 2011) proposed to smooth the LP objective by adding a term that favors entropic marginals.",
      "startOffset" : 53,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "This line of research originates in Nesterov’s work on fast gradient methods (Nesterov, 1983).",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "Benchmarked Methods We compare the performance of LPQP-U and LPQP-T with the widely used MAP algorithms, tree-reweighted belief propagation (TRWS) (Kolmogorov, 2006) and max-product LP (MPLP) (Sontag et al.",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "Benchmarked Methods We compare the performance of LPQP-U and LPQP-T with the widely used MAP algorithms, tree-reweighted belief propagation (TRWS) (Kolmogorov, 2006) and max-product LP (MPLP) (Sontag et al., 2008), both of which are LP relaxations.",
      "startOffset" : 192,
      "endOffset" : 213
    }, {
      "referenceID" : 12,
      "context" : "Synthetic Potts Model Data We follow a similar experimental setup as in (Ravikumar et al., 2010).",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "The protein inference problem discussed in (Yanover et al., 2006), consists of two tasks: protein side-chain prediction and protein design.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "For the protein prediction task, it was shown in (Yanover et al., 2006) that only for 30 out of the 370 protein prediction instances, the LP relaxation is not tight.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "MPLP was applied to this task in (Sontag et al., 2008), and achieved the global optimum on all instances.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "As a last experiment we apply our LPQP algorithm to the recently published “hard discrete energy minimization instances” dataset (Nowozin et al., 2011), available on the authors webpage.",
      "startOffset" : 129,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "Middle: solutions from (Nowozin et al., 2011) obtained by simulated annealing.",
      "startOffset" : 23,
      "endOffset" : 45
    } ],
    "year" : 2012,
    "abstractText" : "MAP inference for general energy functions remains a challenging problem. While most efforts are channeled towards improving the linear programming (LP) based relaxation, this work is motivated by the quadratic programming (QP) relaxation. We propose a novel MAP relaxation that penalizes the Kullback-Leibler divergence between the LP pairwise auxiliary variables, and QP equivalent terms given by the product of the unaries. We develop two efficient algorithms based on variants of this relaxation. The algorithms minimize the non-convex objective using belief propagation and dual decomposition as building blocks. Experiments on synthetic and real-world data show that the solutions returned by our algorithms substantially improve over the LP relaxation.",
    "creator" : "LaTeX with hyperref package"
  }
}