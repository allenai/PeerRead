{
  "name" : "1206.6819.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Robustness of Most Probable Explanations",
    "authors" : [ "Hei Chan", "Adnan Darwiche" ],
    "emails" : [ "chanhe@eecs.oregonstate.edu", "darwiche@cs.ucla.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A Most Probable Explanation (MPE) in a Bayesian network is a complete variable instantiation which has the highest probability given current evidence [1]. Given an MPE solution for some piece of evidence, we concern ourselves in this paper with the following question: What is the amount of change one can apply to some network parameter without changing this current MPE solution? Our goal is then to deduce robustness conditions for MPE under single parameter changes. This problem falls into the realm of sensitivity analysis. Here, we treat the Bayesian network as a system which accepts network parameters as inputs, and produces the MPE as an output. Our goal is then to characterize conditions under which the output is guaranteed to be the same (or different) given a change in some input value.\nThis question is very useful in a number of application areas, including what-if analysis, in addition to the\n∗This work was completed while Hei Chan was at UCLA.\ndesign and debugging of Bayesian networks. For an example, consider Figure 1 which depicts a Bayesian network for diagnosing potential problems in a car. Suppose now that we have the following evidence: the dashboard test and the lights test came out positive, while the engine test came out negative. When we compute the MPE in this case, we get a scenario in which all car components are working normally. This seems to be counterintuitive as we expect the most likely scenario to indicate at least that the engine is not working. The methods developed in this paper can be used to debug this scenario. In particular, we will be able to identify the amount of change in each network parameter which is necessary to produce a different MPE solution. We will revisit this example later in the paper and discuss the specific recommendations computed by our proposed algorithm.\nPrevious results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9]. Because probability values are continuous, while MPE solutions are discrete instantiations, abrupt changes in MPE so-\nlutions may occur when we change a parameter value. This makes the sensitivity analysis of MPE quite different from previous work on the subject.\nThis paper is structured as follows. We first provide the formal definition of Bayesian networks and MPE in Section 2. Then in Section 3, we explore the relationship between the MPE and a single network parameter, and also look into the case where we change co-varying parameters in Section 4. We deduce that the relationship can be captured by two constants that are independent of the given parameter. Next in Section 5, we show how we can compute these constants for all network parameters, allowing us to automatically identify robustness conditions for MPE, and provide a complexity analysis of our proposed approach. Finally, we show some concrete examples in Section 6, and then extend our analysis to evidence change in Section 7."
    }, {
      "heading" : "2 Most Probable Explanations",
      "text" : "We will formally define most probable explanations in this section, but we specify some of our notational conventions first. We will denote variables by uppercase letters (X) and their values by lowercase letters (x). Sets of variables will be denoted by bold-face uppercase letters (X) and their instantiations by bold-face lowercase letters (x). For variable X and value x, we will often write x instead of X = x, and hence, Pr(x) instead of Pr(X = x). For a binary variable X with values true and false, we will use x to denote X = true and x̄ to denote X = false. Therefore, Pr(X = true) and Pr(x) represent the same probability in this case. Similarly, Pr(X = false) and Pr(x̄) represent the same probability. Finally, for instantiation x of variables X, we will write ¬x to mean the set of all instantiations x? 6= x of variables X. For example, we will write Pr(x) + Pr(¬x) = 1.\nA Bayesian network is specified by its structure, a directed acyclic graph (DAG), and a set of conditional probability tables (CPTs), with one CPT for each network variable [1]. In the CPT for variable X with parents U, we define a network parameter θx|u for every family instantiation xu such that θx|u = Pr(x | u).\nGiven the network parameters, we can compute the probability of a complete variable instantiation x as follows:\nPr(x) = ∏\nxu∼x θx|u, (1)\nwhere ∼ is the compatibility relation between instantiations, i.e., xu ∼ x means that xu is compatible with x). Now assume that we are given evidence e. A most probable explanation (MPE) given e is a complete variable instantiation that is consistent with e and has the\nhighest probability [1]:\nMPE (e) def = argmax\nx∼e Pr(x) (2)\n= argmax x∼e ∏ xu∼x θx|u.\nWe note that the MPE may not be a unique instantiation as there can be multiple instantiations with the same highest probability. Therefore, we will define MPE (e) as a set of instantiations instead of just one instantiation. Moreover, we will sometimes use MPE (e,¬x) to denote the MPE instantiations that are consistent with e but inconsistent with x.\nIn the following discussion, we will find it necessary to distinguish between the MPE identity and the MPE probability. By the MPE identity, we mean the set of instantiations having the highest probability. By the MPE probability, we mean the probability assumed by a most likely instantiation, which is denoted by:\nMPEp(e) def = max\nx∼e Pr(x). (3)\nThis distinction is important when discussing robustness conditions for MPE since a change in some network parameter may change the MPE probability, but not the MPE identity."
    }, {
      "heading" : "3 Relation Between MPE and Network Parameters",
      "text" : "Assume that we are given evidence e and are able to find its MPE, MPE (e). We now address the following question: How much change can we apply to a network parameter θx|u without changing the MPE identity of evidence e? To simplify the discussion, we will first assume that we can change this parameter without changing any co-varying parameters, such as θx̄|u, but we will relax this assumption later.\nOur solution to this problem is based on some basic observations which we discuss next. In particular, we observe that complete variable instantiations x which are consistent with e can be divided into two categories:\n• Those that are consistent with xu. From Equation 1, the probability of each such instantiation x is a linear function of the parameter θx|u.\n• Those that are inconsistent with xu. From Equation 1, the probability of each such instantiation x is a constant which is independent of the parameter θx|u.\nLet us denote the first set of instantiations by Σe,xu and the second set by Σe,¬(xu). We can then conclude that:\n• The set of most likely instantiations in Σe,xu remains unchanged regardless of the value of parameter θx|u, even though the probability of such instantiations may change according to the value of θx|u. This is because the probability of each instantiation x ∈ Σe,xu is a linear function of the value of θx|u: Pr(x) = r · θx|u, where r is a coefficient independent of the value of θx|u. Therefore, the relative probabilities among instantiations in Σe,xu remain unchanged as we change the value of θx|u. Note also that the most likely instantiations in this set Σe,xu are just MPE (e, xu) and their probability isMPEp(e, xu). Therefore, if we define:\nr(e, xu) def = ∂MPEp(e, xu) ∂θx|u , (4)\nwe will then have:\nPr(x) = r(e, xu) · θx|u,\nfor any x ∈ MPE (e, xu).\n• Both the identity and probability of the most likely instantiations in Σe,¬(xu) are independent of the value of parameter θx|u. This is because the probability of each instantiation x ∈ Σe,¬(xu) is independent of the value of θx|u. Note that the most likely instantiation in this set Σe,¬(xu) is just MPE (e,¬(xu)). We will define the probability of such an instantiation as:\nk(e, xu) def = MPEp(e,¬(xu)). (5)\nGiven the above observations, MPE (e) will either be MPE (e, xu), MPE (e,¬(xu)), or their union, depending on the value of parameter θx|u:\nMPE (e)\n=  MPE (e, xu), if r(e, xu) · θx|u > k(e, xu);MPE (e,¬(xu)), if r(e, xu) · θx|u < k(e, xu);MPE (e, xu) ∪MPE (e,¬(xu)), otherwise. Moreover, the MPE probability can always be expressed as:\nMPEp(e) = max(r(e, xu) · θx|u, k(e, xu)).\nFigure 2 plots the relation between the MPE probability MPEp(e) and the value of parameter θx|u. According to the figure, if θx|u > k(e, xu)/r(e, xu), i.e., region A of the plot, then we have MPE (e) = MPE (e, xu), and thus the MPE solutions are consistent with xu. Moreover, the MPE identity will remain unchanged as long as the value of θx|u remains greater than k(e, xu)/r(e, xu).\nMPEPr(e)\nθx|u\nk(e,xu)\nk(e,xu) / r(e,xu) 0\nRegion A Region B\nFigure 2: A plot of the relation between the MPE probability MPEp(e) and the value of parameter θx|u.\nOn the other hand, if θx|u < k(e, xu)/r(e, xu), i.e., region B of the plot, then we have MPE (e) = MPE (e,¬(xu)), and thus the MPE solutions are inconsistent with xu. Moreover, the MPE identity and probability will remain unchanged as long as the value of θx|u remains less than k(e, xu)/r(e, xu).\nTherefore, θx|u = k(e, xu)/r(e, xu) is the point where there is a change in the MPE identity if we were to change the value of parameter θx|u. At this point, MPE (e) = MPE (e, xu)∪MPE (e,¬(xu)) and we have both MPE solutions consistent with xu and MPE solutions inconsistent with xu. There are no other points where there is a change in the MPE identity. If we are able to find the constants r(e, xu) and k(e, xu) for the network parameter θx|u, we can then compute robustness conditions for MPE with respect to changes in this parameter."
    }, {
      "heading" : "4 Dealing with Co-Varying Parameters",
      "text" : "The above analysis assumed that we can change a parameter θx|u without needing to change any other parameters in the network. This is not realistic though in the context of Bayesian networks, where co-varying parameters need to add up to 1 for the network to induce a valid probability distribution. For example, if variable X has two values, x and x̄, we must always have:\nθx|u + θx̄|u = 1.\nWe will therefore extend the analysis conducted in the previous section to account for the simultaneously changes in the co-varying parameters. We will restrict our attention to binary variables to simplify the discussion, but our results can be easily extended to multivalued variables as we will show later.\nIn particular, assuming that we are changing parame-\nters θx|u and θx̄|u simultaneously for a binary variable X, we can now categorize all network instantiations which are consistent with evidence e into three groups, depending on whether they are consistent with xu, x̄u, or ¬u. Moreover, the most likely instantiations in each group are just MPE (e, xu), MPE (e, x̄u), and MPE (e,¬u) respectively. Therefore, if x ∈ MPE (e), then:\nPr(x) =  r(e, xu) · θx|u, if x ∈ MPE (e, xu);r(e, x̄u) · θx̄|u, if x ∈ MPE (e, x̄u); k(e,u), if x ∈ MPE (e,¬u);\nwhere:\nr(e, xu) = ∂MPEp(e, xu)\n∂θx|u ;\nr(e, x̄u) = ∂MPEp(e, x̄u)\n∂θx̄|u ;\nk(e,u) = MPEp(e,¬u);\nand the MPE probability is:\nMPEp(e) = max(r(e, xu) ·θx|u, r(e, x̄u) ·θx̄|u, k(e,u)).\nTherefore, changing the co-varying parameters θx|u and θx̄|u will not affect the identity of either MPE (e, xu) orMPE (e, x̄u), nor will it affect the identity or probability of MPE (e,¬u).\nThe robustness condition of an MPE solution can now be summarized as follows:\n• If an MPE solution is consistent with xu, it remains a solution as long as the following inequalities are true:\nr(e, xu) · θx|u ≥ r(e, x̄u) · θx̄|u; r(e, xu) · θx|u ≥ k(e,u).\n• If an MPE solution is consistent with x̄u, it remains a solution as long as the following inequalities are true:\nr(e, x̄u) · θx̄|u ≥ r(e, xu) · θx|u; r(e, x̄u) · θx̄|u ≥ k(e,u).\n• If an MPE solution is consistent with ¬u, it remains a solution as long as the following inequalities are true:\nk(e,u) ≥ r(e, xu) · θx|u; k(e,u) ≥ r(e, x̄u) · θx̄|u.\nWe note here that one can easily deduce whether an MPE solution is consistent with xu, x̄u, or ¬u since it is a complete variable instantiation.\nTherefore, all we need are the constants r(e, xu) and k(e,u) for each network parameter θx|u in order to define robustness conditions for MPE. The constants k(e,u) can be easily computed from the constants r(e, xu) by observing the following:\nk(e,u) = MPEp(e,¬u) = max\nu?:u? 6=u MPEp(e,u?)\n= max xu?:u? 6=u MPEp(e, xu?)\n= max xu?:u? 6=u r(e, xu?) · θx|u? . (6)\nAs the algorithm we will describe later computes the r(e, xu) constants for all family instantiations xu, the algorithm will then allow us to compute all the k(e,u) constants as well.\nAs a simple example, for the Bayesian network whose CPTs are shown in Figure 3, the current MPE solution without any evidence is A = a,B = b̄, and has probability .4. For the parameters in the CPT of B, we can compute the corresponding r(e, xu) constants. In particular, we have r(e, ba) = r(e, b̄a) = r(e, bā) = r(e, b̄ā) = .5 in this case. The k(e,u) constants can also be computed as k(e, a) = .3 and k(e, ā) = .4. Given these constants, we can easily compute the amount of change we can apply to covarying parameters, say θb|a and θb̄|a, such that the MPE solution remains the same. The conditions we must satisfy are:\nr(e, b̄a) · θb̄|a ≥ r(e, ba) · θb|a; r(e, b̄a) · θb̄|a ≥ k(e, a).\nThis leads to θb̄|a ≥ θb|a and θb̄|a ≥ .6. Therefore, the current MPE solution will remain so as long as θb̄|a ≥ .6, which has a current value of .8.\nWe close this section by pointing out that our robustness equations can be extended to multi-valued variables as follows. If variable X has values x1, . . . , xj , with j > 2, then each of the conditions we showed earlier will consist of j inequalities instead of just two. For example, if an MPE solution is consistent with x1u, it remains a solution as long as the following inequalities are true:\nr(e, x1u) · θx1|u ≥ r(e, x ?u) · θx?|u for all x? 6= x1; r(e, x1u) · θx1|u ≥ k(e,u)."
    }, {
      "heading" : "5 Computing Robustness Conditions",
      "text" : "In this section, we will develop an algorithm for computing the constants r(e, xu) for all network parameters θx|u. In particular, we will show that they can be computed in time and space which is O(n exp(w)), where n is the number of network variables and w is its treewidth."
    }, {
      "heading" : "5.1 Arithmetic Circuits",
      "text" : "Our algorithm for computing the r(e, xu) constants is based on an arithmetic circuit representation of the Bayesian network [10]. Figure 4 depicts an arithmetic circuit for a small network consisting of two binary nodes, A and B, shown in Figure 3. An arithmetic circuit is a rooted DAG, where each internal node corresponds to multiplication (∗) or addition (+), and each leaf node corresponds either to a network parameter θx|u or an evidence indicator λx; see Figure 4. Operationally, the circuit can be used to compute the probability of any evidence e by evaluating the circuit while setting the evidence indicator λx to 0 if x contradicts e and setting it to 1 otherwise. Semantically though, the arithmetic circuit is simply a factored representation of an exponential-size function that captures the network distribution. For example, the circuit in Figure 4 is simply a factored representation of the following function:\nλaλbθaθb|a + λaλb̄θaθb̄|a + λāλbθāθb|ā + λāλb̄θāθb̄|ā.\nThis function, called the network polynomial, includes a term for each instantiation x of network variables, where the term is simply a product of the network parameters and evidence indicators which are consistent\nwith x. Moreover, the term for x evaluates to the probability value Pr(e,x) when the evidence indicators are set according to e. Note that this function is multilinear. Therefore, a corresponding arithmetic circuit will have the property that two sub-circuits that feed into the same multiplication node will never contain a common variable. This property is important for some of the following developments."
    }, {
      "heading" : "5.2 Complete Sub-Circuits and Their",
      "text" : "Coefficients\nEach term in the network polynomial corresponds to a complete sub-circuit in the arithmetic circuit. A complete sub-circuit can be constructed recursively from the root, by including all children of each multiplication node, and exactly one child of each addition node. The bold lines in Figure 4 depict a complete sub-circuit, corresponding to the term λaλb̄θaθb̄|a. In fact, it is easy to check that the circuit in Figure 4 has four complete sub-circuits, corresponding to the four terms in the network polynomial.\nA key observation about complete sub-circuits is that if a network parameter is included in a complete subcircuit, there is a unique path from the root to this parameter in this sub-circuit, even though there may be multiple paths from the root to this parameter in the original arithmetic circuit. This path is important as one can relate the value of the term corresponding to the sub-circuit and the parameter value by simply traversing the path as we show next.\nConsider now a complete sub-circuit which includes a network parameter θx|u and let α be the unique path in this sub-circuit connecting the root to parameter θx|u. We will now define the sub-circuit coefficient w.r.t. θx|u, denoted as r, in terms of the path α such that r ·θx|u is just the value of the term corresponding to the sub-circuit.\nLet Σ be the set of all multiplication nodes on this path α. The sub-circuit coefficient w.r.t. θx|u is defined as the product of all children of nodes in Σ which are themselves not on the path α. Consider for example the complete sub-circuit highlighted in Figure 4 and the path from the root to the network parameter θa. The coefficient w.r.t. θa is r = λaλb̄θb̄|a. Moreover, r ·θa = λaλb̄θaθb̄|a, which is the term corresponding to the sub-circuit."
    }, {
      "heading" : "5.3 Maximizer Circuits",
      "text" : "An arithmetic circuit can be easily modified into a maximizer circuit to compute the MPE solutions, by simply replacing each addition node with a maximization node; see Figure 5. This corresponds to a circuit\nthat computes the value of the maximum term in a network polynomial, instead of adding up the values of these terms. The value of the root will thus be the MPE probability MPEp(e). The maximizer circuit in Figure 5 is evaluated under evidence A = a, leading to an MPE probability of .4.\nTo recover an MPE solution from a maximizer circuit, all we need to do is construct the MPE sub-circuit recursively from the root, by including all children of each multiplication node, and one child c for each maximization node v, such that v and c have the same value; see Figure 5. The MPE sub-circuit will then correspond to an MPE solution. Moreover, if a parameter θx|u is in the MPE sub-circuit, and the sub-circuit coefficient w.r.t θx|u is r, then we have r · θx|u as the probability of MPE, MPEp(e).\nConsider Figure 5 and the highlighted MPE subcircuit, evaluated under evidence A = a. The term corresponding to this sub-circuit is A = a,B = b̄, which is therefore an MPE solution. Moreover, we have two parameters in this sub-circuit, θa and θb̄|a, with coefficients .8 = (1)(.8) and .5 = (.5)(1)(1) respectively. Therefore, the MPE probability can be obtained by multiplying any of these coefficients with the corresponding parameter value, as (.8)θa = (.8)(.5) = .4 and (.5)θb̄|a = (.5)(.8) = .4.\n5.4 Computing r(e, xu)\nSuppose now that our goal is to compute MPE (e, xu) for some network parameter θx|u. Suppose further that α1, . . . , αm are all the complete sub-circuits that\nAlgorithm 1 D-MAXC(M: a maximizer circuit, e: evidence) 1: evaluate the circuit M under evidence e; afterwards\nthe value of each node v is p[v] 2: r[v]← 1 for root v of circuit M 3: r[v]← 0 for all non-root nodes v in circuitM 4: for non-leaf nodes v (parents before children) do 5: if node v is a maximization node then 6: r[c]← max(r[c], r[v]) for each child c of node v 7: if node v is a multiplication node then 8: r[c]← max (r[c], r[v] ∏ c?\np[c?]) for each child c of node v, where c? are the other children of node v\ninclude θx|u. Moreover, let x1, . . . ,xm be the instantiations corresponding to these sub-circuits and let r1, . . . , rm be their corresponding coefficients w.r.t. θx|u. It then follows that the probabilities of these instantiations are r1 · θx|u, . . . , rm · θx|u respectively. Moreover, it follows that:\nMPEp(e, xu) = max i r1 · θx|u, . . . , rm · θx|u,\nand hence, from Equation 4:\n∂MPEp(e, xu) ∂θx|u = r(e, xu) = max i r1, . . . , rm.\nTherefore, if we can compute the maximum of these coefficients, then we have computed the constant r(e, xu).\nAlgorithm 1 provides a procedure which evaluates the maximizer circuit and then traverses it top-down, parents before children, computing simultaneously the constants r(e, xu) for all network parameters. The procedure maintains an additional register value r[.] for each node in the circuit, and updates these registers as it visits nodes. When the procedure terminates, it is guaranteed that the register value r[θx|u] will be the constant r(e, xu). We will also see later that the register value r[λx] is also a constant which provides valuable information for the MPE solutions. Figure 6 depicts an example of this procedure.\nAlgorithm 1 can be modelled as the all-pairs shortest path procedure, with edge v −→ c having weight 0 = − ln 1 if v is a maximization node, and weight − lnπ if v is a multiplication node, where π is the product of the values of the other children c? 6= c of node v. The length of the shortest path from the root to the network parameter θx|u is then − ln r(e, xu). It should be clear that the time and space complexity of the above algorithm is linear in the number of circuit nodes.1 It is well known that we can compile a\n1More precisely, this algorithm is linear in the number of circuit nodes only if the number of children per multiplication node is bounded. If not, one can use a technique which gives a linear complexity by simply storing two additional bits with each multiplication node [10].\ncircuit for any Bayesian network in O(n exp(w)) time and space, where n is the number of network variables and w is its treewidth [10]. Therefore, all constants r(e, xu) can be computed with the same complexity.\nWe close this section by pointing out that one can in principle use the jointree algorithm to compute MPEp(e, xu) = r(e, xu) · θx|u for all family instantiations xu with the above complexity. In particular, by replacing summation with maximization in the jointree algorithm, one obtainsMPEp(e, c) for each cluster instantiation c. Projecting on the families XU in cluster C, one can then obtain MPEp(e, xu) for all family instantiations xu, which is all we need to compute robustness conditions for MPE.2 Our method above, however, is more general for two reasons:\n• The arithmetic circuit for a Bayesian network can be much smaller than the corresponding jointree by exploiting the local structures of the Bayesian network [12, 13].\n• The constants computed by the algorithm for the evidence indicators can be used to answer additional MPE queries, which results after variations on the current evidence. This will be discussed in Section 7."
    }, {
      "heading" : "6 Example",
      "text" : "We now go back to the example network in Figure 1, and compute robustness conditions for the current\n2However, in case some of the parameters are equal to 0, one needs to use a special jointree [11].\nMPE solution using the inequalities we obtain in Section 4, and an implementation of Algorithm 1. After going through the CPT of each variable, our procedure found nine possible parameter changes that would produce a different MPE solution, as shown in Figure 7. From these nine suggested changes, only three changes make sense from a qualitative point of view:\n• Decreasing the probability that the ignition is working from .9925 to at most .9133. (6th row)\n• Decreasing the probability that the engine is working given both the battery and the ignition are working from .97 to at most .9108. (1st row)\n• Decreasing the false-negative rate of the engine test from .09 to at most .0285. (9th row)\nIf we apply the first parameter change, we get a new MPE solution in which both the ignition and the engine are not working. If we apply either the second or third parameter change, we get a new MPE solution in which the engine is not working."
    }, {
      "heading" : "7 MPE under Evidence Change",
      "text" : "We have discussed in Section 5.2 the notion of a complete sub-circuit and its coefficient with respect to a network parameter θx|u which is included in the subcircuit. In particular, we have shown how each subcircuit corresponds to a term in the network polynomial, and that if a complete sub-circuit has coefficient r with respect to parameter θx|u, then r · θx|u will be the value of the term corresponding to this sub-circuit.\nThe notion of a sub-circuit coefficient can be extended to evidence indicators as well. In particular, if a complete sub-circuit has coefficient r with respect to an evidence indicator λx which is included in the sub-circuit,\nthen r ·λx will be the value of the term corresponding to this sub-circuit.\nSuppose now that α1, . . . , αm are all the complete subcircuits that include λx. Moreover, let x1, . . . ,xm be the terms corresponding to these sub-circuits and let r1, . . . , rm be their corresponding coefficients with respect to λx. It then follows that the values of these terms are r1 ·λx, . . . , rm ·λx respectively. Moreover, it follows that:\nMPEp(e−X,x) = max i r1, . . . , rm,\nwhere e − X denotes the new evidence after having retracted the value of variable X from e (if X ∈ E, otherwise e−X = e). Therefore, if we can compute the maximum of these coefficients, then we have computed MPEp(e − X,x). Note, however, that Algorithm 1 already computes the maximum of these coefficients for each λx as the evidence indicators are nodes in the maximizer circuit as well, and therefore the register value r[λx] gives us MPEp(e−X,x) for every variable X and value x.\nConsider for example the circuit in Figure 6, and the coefficients computed by Algorithm 1 for the four evidence indicators. According to the above analysis, these coefficients have the following meanings:\nλx e−X,x r[λx] = MPEp(e−X,x) λa a .4 λā ā .3 λb a, b .1 λb̄ a, b̄ .4\nFor example, the second row above tells us that the MPE probability would be .3 if the evidence was A = ā instead of A = a. In general, if we have n variables, we then have O(n) variations on the current evidence of the form e − X,x. The MPE probability of all of these variations are immediately available from the coefficients with respect to the evidence indicators.\nThe computation of these coefficients allows us to deduce the MPE identity after evidence retraction. In particular, suppose that variable X is set as x in evidence e, andMPEp(e) ≥ MPEp(e−X,x?) for all other values x? 6= x. We can then conclude that MPEp(e) = MPEp(e − X). Moreover, MPE (e) = MPE (e − X) if MPEp(e) > MPEp(e − X,x?) for all other values x? 6= x, or MPE (e) ⊂ MPE (e − X) if there exists some x? 6= x such that MPEp(e) = MPEp(e−X,x?). Therefore, the current MPE solutions will remain so even after we retract X = x from the evidence. This means that X = x is not integral in the determination of the current MPE solutions given the other evidence, i.e., e−X.\nThe result above also has implications on the identification of multiple MPE solutions given evidence e. In particular, suppose that variable X is not set in evidence e, then:\n• If the coefficients for the evidence indicators λx and λx̄ are equal, we must have both MPE solutions with X = x and MPE solutions with X = x̄. In fact, the coefficients must both equal the MPE probability MPEp(e) in this case.\n• If the coefficient for the evidence indicator λx is larger than the coefficient for the evidence indicator λx̄, then every MPE solution must have X = x.\nIn the example above, we have r[λb̄] > r[λb], suggesting that every MPE solution must have b̄ in this case."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We considered in this paper the problem of finding robustness conditions for MPE solutions of a Bayesian network under single parameter changes. We were able to solve this problem by identifying some interesting relationships between an MPE solution and the network parameters. In particular, we found that the robustness condition of an MPE solution under a single parameter change depends on two constants that are independent of the parameter value. We also proposed a method for computing such constants and, therefore, the robustness conditions of MPE in O(n exp(w)) time and space, where n is the number of network variables and w is the network treewidth. Our algorithm is the first of its kind for ensuring the robustness of MPE solutions under parameter changes in a Bayesian network."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work has been partially supported by Air Force grant #FA9550-05-1-0075-P00002 and JPL/NASA grant #1272258. We would also like to thank James Park for reviewing this paper and making the observation on how to compute k(e,u) in Equation 6."
    } ],
    "references" : [ {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1988
    }, {
      "title" : "When do numbers really matter",
      "author" : [ "Hei Chan", "Adnan Darwiche" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Sensitivity analysis in Bayesian networks: From single to multiple parameters",
      "author" : [ "Hei Chan", "Adnan Darwiche" ],
      "venue" : "In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Sensitivity analysis in discrete Bayesian networks",
      "author" : [ "Enrique Castillo", "José Manuel Gutiérrez", "Ali S. Hadi" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part A (Systems and Humans),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Using sensitivity analysis for efficient quantification of a belief network",
      "author" : [ "Veerle M.H. Coupé", "Niels Peek", "Jaap Ottenkamp", "J. Dik F. Habbema" ],
      "venue" : "Artificial Intelligence in Medicine,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "Making sensitivity analysis computationally efficient",
      "author" : [ "Uffe Kjærulff", "Linda C. van der Gaag" ],
      "venue" : "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Sensitivity analysis for probability assessments in Bayesian networks",
      "author" : [ "Kathryn B. Laskey" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1995
    }, {
      "title" : "The sensitivity of belief networks to imprecise probabilities: An experimental investigation",
      "author" : [ "Malcolm Pradhan", "Max Henrion", "Gregory Provan", "Brendan Del Favero", "Kurt Huang" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1996
    }, {
      "title" : "Analysing sensitivity data from probabilistic networks",
      "author" : [ "Linda C. van der Gaag", "Silja Renooij" ],
      "venue" : "In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "A differential approach to inference in Bayesian networks",
      "author" : [ "Adnan Darwiche" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "A differential semantics for jointree algorithms",
      "author" : [ "James D. Park", "Adnan Darwiche" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Compiling Bayesian networks with local structure",
      "author" : [ "Mark Chavira", "Adnan Darwiche" ],
      "venue" : "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "Compiling relational Bayesian networks for exact inference",
      "author" : [ "Mark Chavira", "Adnan Darwiche", "Manfred Jaeger" ],
      "venue" : "International Journal of Approximate Reasoning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A Most Probable Explanation (MPE) in a Bayesian network is a complete variable instantiation which has the highest probability given current evidence [1].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 206
    }, {
      "referenceID" : 3,
      "context" : "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 206
    }, {
      "referenceID" : 6,
      "context" : "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 206
    }, {
      "referenceID" : 8,
      "context" : "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 206
    }, {
      "referenceID" : 0,
      "context" : "A Bayesian network is specified by its structure, a directed acyclic graph (DAG), and a set of conditional probability tables (CPTs), with one CPT for each network variable [1].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "A most probable explanation (MPE) given e is a complete variable instantiation that is consistent with e and has the highest probability [1]:",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 9,
      "context" : "Our algorithm for computing the r(e, xu) constants is based on an arithmetic circuit representation of the Bayesian network [10].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "If not, one can use a technique which gives a linear complexity by simply storing two additional bits with each multiplication node [10].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "circuit for any Bayesian network in O(n exp(w)) time and space, where n is the number of network variables and w is its treewidth [10].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 11,
      "context" : "• The arithmetic circuit for a Bayesian network can be much smaller than the corresponding jointree by exploiting the local structures of the Bayesian network [12, 13].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "• The arithmetic circuit for a Bayesian network can be much smaller than the corresponding jointree by exploiting the local structures of the Bayesian network [12, 13].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 10,
      "context" : "However, in case some of the parameters are equal to 0, one needs to use a special jointree [11].",
      "startOffset" : 92,
      "endOffset" : 96
    } ],
    "year" : 2006,
    "abstractText" : "In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instantiation with the highest probability given the current evidence. In this paper, we discuss the problem of finding robustness conditions of the MPE under single parameter changes. Specifically, we ask the question: How much change in a single network parameter can we afford to apply while keeping the MPE unchanged? We will describe a procedure, which is the first of its kind, that computes this answer for all parameters in the Bayesian network in time O(n exp(w)), where n is the number of network variables and w is its treewidth.",
    "creator" : "TeX"
  }
}