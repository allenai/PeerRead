{
  "name" : "1709.00023.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "R: Reinforced Reader-Ranker for Open-Domain Question Answering",
    "authors" : [ "Shuohang Wang", "Mo Yu", "Xiaoxiao Guo", "Zhiguo Wang", "Tim Klinger", "Wei Zhang", "Shiyu Chang", "Gerald Tesauro", "Bowen Zhou", "Jing Jiang" ],
    "emails" : [ "shwang.2014@smu.edu.sg,", "yum@us.ibm.com,", "xiaoxiao.guo@ibm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we present a novel opendomain QA system called Reinforced Ranker-Reader (R3), based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker ∗This work has been done during the 1st author’s internship with IBM. 1In the QA community, “openness” can be interpreted as referring either to the scope of question topics or to the breadth and generality of the knowledge source used to answer each question. Following Chen et al. 2017a we adopt the latter definition.\nalong with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets."
    }, {
      "heading" : "1 Introduction",
      "text" : "Open-domain question answering (QA) is a key challenge in natural language processing. A successful open-domain QA system must be able to effectively retrieve and comprehend one or more knowledge sources to infer a correct answer. Knowledge sources can be knowledge bases (Berant et al., 2013) or structured or unstructured text passages (Ferrucci et al., 2010; Baudiš and Šedivỳ, 2015).\nRecent deep learning-based research has focused on open-domain QA based on large text corpora such as wikipedia, applying a two-stepprocess of information retrieval (IR) to extract passages and reading comprehension (RC) to select an answer phrase from them (Chen et al., 2017a; Dhingra et al., 2017b). These methods, which we call QA with Search-and-Reading (SR-QA), are a simple yet powerful approach for open-domain QA. Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).\nThe main difference between training SR-QA and standard RC models is in the passages used for training. In standard RC model training, passages are manually selected to guarantee that groundtruth answers are contained and annotated within the passage (Rajpurkar et al., 2016). 2 By con-\n2This forms a closed-domain QA by our adopted definition where the domain consists of the given passage only.\nar X\niv :1\n70 9.\n00 02\n3v 1\n[ cs\n.C L\n] 3\n1 A\nug 2\n01 7\nQ: What is the largest island in the Philippines? A: Luzon P1 Mindanao is the second largest and easternmost island in the Philippines. P2 As an island, Luzon is the Philippine’s largest at\ntrast, in SR-QA approaches such as (Chen et al., 2017a; Dhingra et al., 2017b), the model is given only QA-pairs and uses an IR component to retrieve passages similar to the question from a large corpus. Depending on the quality of the IR component, retrieved passages may not contain or entail the correct answer, making RC training more difficult. Table 1 shows an example which illustrates the difficulty. This ordering was produced by an off-the-shelf IR engine using the BM25 algorithm. The correct answer is contained in passage P2. The top passage (P1), despite being ranked highest by the IR engine, is ineffective for answering the question, since it fails to capture the semantic distinction between “largest” and “second largest”. Passage P3 contains the answer text (“Luzon”) but does not semantically entail the correct answer (“Luzon is the largest island in the Philippines”). Including passages such as P1 and P3 in the text used for the RC component introduces noise which can degrade training.3\nIn this paper we propose a different approach to the problem which explicitly separates the tasks of predicting the likelihood that a passage provides the answer (Ranking) and reading those passages to extract the correct answer (Reading). Specifically we propose an end-to-end framework consisting of two components: A Ranker and a Reader (i.e. RC model). The Ranker selects the passage most likely to entail the answer and passes it to the Reader, which reads and extracts from that passage. The Reader is trained using SGD/backprop to maximize the likelihood of the span containing the correct answer (if one exists). The Ranker is trained using REINFORCE (Williams, 1992) with a reward de-\n3Passage ranking models for non-factoid QA (Wang et al., 2007; Yang et al., 2015) are able to learn to rank these passages; but these models are trained using human annotated answer labels, which are not available here.\ntermined by how well the Reader extracts answers from the top ranked passages. In this way the Ranker is optimized with an objective determined by the end-performance on answer prediction, which provides a strong signal to distinguish passages lexically similar to but semantically different from the question.\nWe discuss the Ranker-Reader model in detail in the next section but briefly, the Ranker and Reader are implemented as variants of MatchLSTM models (Wang and Jiang, 2016). These models were originally designed for solving the text entailment problem. To adapt them to this task, different non-linear layers have been added for either selecting the passages or predicting the start and end positions of the answer in the passage.\nWe evaluate our model on five different datasets and achieve the state-of-the-art results on four of the them. Our results also show the merits of employing a separate REINFORCE-trained ranking component over several challenging fully supervised baselines."
    }, {
      "heading" : "2 Framework",
      "text" : "Problem Definition We assume that we have available a factoid question q to be answered and a set of passages which may contain the groundtruth answer ag. Those passages4 are the top N retrieved from a corpus by an IR model supplied with the question, for N a hyper-parameter. During training we are given only the (q,ag) pairs, together with an IR model with index built on an open-domain corpus.\nFramework Overview An overview of the Ranker-Reader model is shown in Figure 1. It shows two key components: a Ranker, which selects passages from which an answer can be extracted, and a Reader which extracts answers from supplied passages. Both the Ranker and Reader compare the question to each of the passages to generate passage representations based on how well they match the question. The Ranker uses these “matched” representations to select a single passage which is most likely to contain the answer. The selected passage is then processed by the Reader to extract an answer sequence. We train the reader using SGD/backprop and produce a reward to train the Ranker via REINFORCE.\n4In this paper we use sentence-level index thus each passage is an individual sentence. See the experimental setting.\n3 R3: Reinforced Ranker-Reader\nIn this section, we first review the MatchLSTM (Wang and Jiang, 2016) which provides input for both the Reader and Ranker. We then detail the Reader and Ranker components, and the procedure for joint training, including the objective function used for RL training.\nPassage Representation Using Match-LSTM To effectively rank and read passages they must be matched to the question. This comparison is performed with a Match-LSTM, a state-of-theart model for text entailment, shown on the right in Figure 1. Match-LSTMs use an attention mechanism to compute word similarities between the passage and question sequences. These are first encoded as matrices Q and P, respectively, by a Bidirectional LSTM (BiLSTM) with hidden dimension l. With Q words in question Q and P words in passage P we can write:\nHp = BiLSTM(P), Hq = BiLSTM(Q), (1)\nwhere Hp ∈ Rl×P and Hq ∈ Rl×Q are the hidden states for the passage and the question. In order to improve computational efficiency without degrading performance, we simplify the attention mechanism of the original Match-LSTM by computing the attention weights G as follows:\nG = SoftMax ( (WgHq + bg ⊗ eQ)THp ) where Wg ∈ Rl×l and bg ∈ Rl are the learnable parameters. The outer product (· ⊗ eQ) repeats the column vector bg Q times to form an l× l matrix. The i-th column of G ∈ RQ×P represents the normalized attention weights over all the question words for the i-th word in passage.\nWe can use this attention matrix G to form representations of the question for each word in passage:\nH q = HqG,\n(2)\nNext, we produce the word matching representations M ∈ R2l×P using Hp and Hq as follows:\nM = ReLU Wm \nHp H q\nHp ⊙ H q Hp −Hq\n  , (3)\nwhere Wm ∈ R2l×4l are learnable parameters; [ · · ] is the column concatenation of matrices; Elementwise operations (· ⊙ ·) and (· − ·) are also used to represent word-level matching (Wang and Jiang, 2017a; Chen et al., 2017b).\nFinally, we aggregate the word matching representations through another bi-directional LSTM:\nHm = BiLSTM(M), (4)\nwhere Hm ∈ Rl×P is the sequence matching representation between a passage and a question.\nTo produce the input for the Ranker and Reader described next, we apply Match-LSTMs to the question and each of the passages. To reduce model complexity, the Ranker and Reader share the same M but have separate parameters for the aggregation stage shown in Eqn.(4), resulting different Hm, denoted as HRank and HRead respectively.\nRanker The role of the Ranker in our model is to select a passage for reading by the Reader. To do this, we train the Ranker using reinforcement learning, to output a policy or probability distribution over passages. At training time that policy will be sampled while at test time we take the argmax passage. First, we create a fixed-size vector representation for each passage from the matching representations HRanki , i ∈ [1, N ], using a standard max pooling operation. The result ui is a representation of the i-th passage. We then concatenate the individual passage representations and apply a non-linear transformation followed by a normalization to compute the passage probabilities γ. Specifically:\nui = MaxPooling(HRanki ), C = Tanh (Wc[u1; u2; ...uN ] + bc ⊗ eN ) , γ = Softmax(wcC), (5)\nwhere Wc ∈ Rl×l and bc,wc ∈ Rl are the parameters to optimize; ui ∈ Rl represents how the ith passage matches the question; C ∈ Rl×N is a non-linear transformation of passage representations; and γ ∈ RN is a vector of the predicted probabilities that each passage entails the answer.\nThe action policy is then defined as follows:\nπ(τ |q; θr) = γτ (6)\nwhere γτ is the probability of passage τ being selected, computed through Eqn.(5); θr represents\nparameters to learn. In the rest of the paper we denote the policy π(τ |q) = π(τ |q; θr) for simplicity. In this way, the action is to sample a passage according to its policy π(τ |q) as the input of Reader.\nReader Our Reader extracts an answer span from the passage τ selected by the Ranker. As in previous work (Wang and Jiang, 2017b; Xiong et al., 2017; Seo et al., 2017; Wang et al., 2017), the Reader is used to predict the start and end positions of the answer phrase in the passage.\nFirst we process the output of Match-LSTMs on all the passages to produce the probability of the start position of the answer span βs:\nFs = Tanh ( Ws[HReadτ ;H Read neg1 ; ...;H Read negn ] + b s ⊗ eV ) , βs = SoftMax (wsFs) , (7)\nwhere negn is the id of a sampled passage not containing ground-truth answer during training; V is the total number of words in these passages; eV is thus a V -dimension vector with ones; [·; ·] is the column concatenation operation; Ws ∈ Rl×l and bs,ws ∈ Rl are the parameters to optimize; βs ∈ RV is the probability of the start point of the span.\nWe similarly compute the probability of the ending position, βe ∈ RV , using separate parameters We,be and we. The loss function can then be expressed as follows:\nL(ag|τ,q) = −log(βsasτ )− log(β e aeτ ), (8)\nwhere ag is the ground-truth answer; τ is sampled according to Eqn.(6), and during training, we keep sampling until passage τ contains ag; βsasτ and β e aeτ represents the probability of the start and end positions of ag in passage τ .\nTraining To train the Ranker-Reader model (R3) we must train both the Ranker, which produces a passage selection, and the Reader which consumes it. We adopt a joint training strategy shown in Algorithm 1. Because the Ranker makes a hard selection of the passage, it is trained using the REINFORCE algorithm. The Reader is trained using standard stochastic gradient descent and backpropagation.\nOur training objective is to minimize the following loss function\nJ(Θ) = −Eτ∼π(τ |q) [L(ag|τ,q)] , (9)\nwhere L is the loss of the Reader defined in Eqn. (8); π(τ |q) is the action policy defined in Eqn.(6); and Θ are parameters to be learned. During training, action sampling is limited solely to passages containing the ground-truth answer, to guarantee Reader updating (line 10 in Algorithm 1) based on\n5Baseline method SR2, described in Experimental Settings.\n6For computational efficency, we sample 10 passages during training, and make sure there are at least 2 negative passages and as many positive passages as possible.\nAlgorithm 1 Reinforced Ranker-Reader (R3) 1: Input: ag, q, passages from IR 2: Output: Θ 3: Initialize:\nΘ← pre-trained Θ with a baseline method5\n4: for each q in dataset do 5: For question q, sample K passages from\nthe top N passages retrieved by IR model for training. 6 6: Randomly sample a positive passage τ ∼ π(τ |q) 7: Extract the answer arc through RC model 8: Get reward r according to R(ag,arc|τ). 9: Updating Ranker (ranking model) through\npolicy gradient r ∂∂Θ log(π(τ |q)) 10: Updating Reader (RC model) through su-\npervised gradient ∂∂ΘL(a g|τ,q)\n11: end for\nthe sampled passages with supervised gradients. The gradient of J(Θ) with respect to Θ is:\n∇ΘJ(Θ) = −∇Θ ∑ τ π(τ |q)L(ag|τ,q)\n= − ∑ τ (L(ag|τ,q)∇Θπ(τ |q) + π(τ |q)∇ΘL(ag|τ,q)) = −Eτ∼π(τ |q) [L(ag|τ,q)∇Θ log(π(τ |q)) + ∇ΘL(ag|τ,q)] ≈ −Eτ∼π(τ |q) [R(ag,arc|τ)∇Θ log(π(τ |q))\n+ ∇ΘL(ag|τ,q)] (10)\nSo in training, we first sample a passage τ according to the policy π(τ |q). Then the Reader updates its parameters given the passage τ using standard Backprop and the ranker updates its parameters via policy gradient using L(a|τ,q) as rewards. However, L(a|τ,q) is not bounded and introduces a large variance in gradients (similar to what was reported in Mnih et al. 2014). To address this, we replace L(a|τ,q) with a bounded reward R(ag,arc|τ), which captures how well the answer extracted by the Reader matches the ground-truth answer. Specifically:\nR(ag,arc|τ) =  2, if a g == arc\nf1(ag,arc), else if ag ∩ arc! = ø −1, else\n(11)\nwhere ag is the ground-truth answer; arc is the answer extracted by Reader; f1(·, ·) ∈ [0, 1] is\nthe function to compute word-level F1 score between two sequences. The F1 score is used as reward when the two answers ag and arc share some words but do not exactly match; an exact match is given a larger reward of 2. We give negative reward -1 when there is no overlap.\nPrediction During testing, we combine the Ranker and Reader for answer extraction as follows:\nPr(a, τ) = Pr(a|τ) Pr(τ) = e−L(a|τ,q)π(τ |q), (12) where Pr(a, τ) is the probability of extracting the answer a from passage τ . We select the answer with the largest Pr(a, τ) as the final prediction."
    }, {
      "heading" : "4 Experimental Settings",
      "text" : "To evaluate our model we have chosen five challenging datasets under the distant supervision setting following (Chen et al., 2017a)’s work, including Quasar-T (Dhingra et al., 2017b), SQuAD (Rajpurkar et al., 2016), WikiMovie (Miller et al., 2016), CuratedTREC (Baudiš and Šedivỳ, 2015), and WebQuestion (Berant et al., 2013). Based on these datasets, we compare to three public baseline models: GA (Dhingra et al., 2017a,b), BiDAF (Seo et al., 2017), and DrQA (Chen et al., 2017a). We also compare to two internal baselines as described below."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We experiment with five different data sets whose statistics are shown in Table 2.\nQuasar-T (Dhingra et al., 2017b) is a dataset for SR-QA, with question-answer pairs from various internet sources. Each question is compared to 100 sentence-level candidate passages, retrieved by their IR model from the ClueWeb09 data source, to extract the answer.\nThe other four datasets we consider are: SQuAD (Rajpurkar et al., 2016), the Stanford QA dataset, where each question-answer pair is generated by annotators using a given Wikipedia paragraph; WikiMovie (Miller et al., 2016) which contains movie-related questions from the OMDb and MovieLens databases and where the questions can be answered using Wikipedia pages; CuratedTREC (Baudiš and Šedivỳ, 2015), based on TREC (Voorhees and Tice, 2000) and designed for open-domain QA; and WebQuestion (Berant\net al., 2013) which is designed for knowledge-base QA with answers restricted to Freebase entities.\nFor these four datasets under the distant supervision setting, no candidate passages are provided so we build a similar sentence-level Search Index based on English Wikipedia, following (Chen et al., 2017a). To provide a small yet sufficient search space for our model, we employ a traditional IR method to retrieve relevant passages from the whole of Wikipedia. We use the 2016-12- 21 dump of English Wikipedia as our sole knowledge source, and build an inverted index with Lucene7. We then take each input question as a query to search for top-200 articles, rank them with BM25, and split them into sentences. The sentences are then ranked by TF-IDF and the top200 sentences for each question retained."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "We consider three public baseline models8: GA (Dhingra et al., 2017a,b), a gated-attention reader for text comprehension; BiDAF (Seo et al., 2017), a reader with bidirectional attention flow for machine comprehension; and DrQA (Chen et al., 2017a), a document reader for question answering. We also compare our model R3 with two internal baselines:\nSingle Reader (SR) This model is trained in the same way as Chen et al. 2017a and Dhingra\n7https://lucene.apache.org/ 8We only compare to the results from the public papers.\net al. 2017b. We find all the answer spans that exactly match the ground-truth answers from the retrieved passages and train the Reader using the objective of Eqn.(8). Here τ is randomly sampled from [1, N ] instead of using Eqn.(6).\nSimple Ranker-Reader (SR2) This RankerReader model is trained by combining the two different objective functions for the Single Reader and the Ranker models together. In order to train the Ranker, we treat all the passages that contain the ground-truth answer as positive cases and use the following for the Ranker loss:\nN∑ n=1 yn (log(yn)− log(γn)) , (13)\nwhich is the KL divergence between γ computed through Eqn.(5) and a probability vector y, where yi = 1/Np when the passage i contains the ground-truth answer, and yi = 0/Np otherwise. Np is the total number of passages which contain the ground-truth answer in the top-N passage list."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "In order to increase the likelihood that questionrelated context will be contained in the retrieved passages for the training dataset, if the answer is unique, we combine the question with the answer to form the query for information retrieval. For the testing dataset, we use only the question as a query and collect the top 50 passages for answer extraction.\nDuring training, our R3 model is first initialized by pre-training the model using the Simple Ranker-Reader (R2), to encourage convergence.\nAs discussed earlier, the pre-processing and matching layers, Eqn.(1-3), are shared by both Ranker and Reader. The LSTM layer in Eqn.(4) is set to 3 for the Reader and 1 for the Ranker.\nOur model is optimized using Adamax (Kingma and Ba, 2015). We use fixed GloVe (Pennington et al., 2014) word embeddings. We set l to 300, batch size to 30, learning rate to 0.002 and we tune the dropout probability only9."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "In this section, we will show the performance of different models on five QA datasets and offer further analysis.\n9Our code will be released under https://github. com/shuohangwang/mprc"
    }, {
      "heading" : "5.1 Overall Results",
      "text" : "Our results are shown in Table 3. We use F1 score and Exact Match (EM) evaluation metrics10. We first observe that on Quasar-T, the Single Reader can exceed state-of-the-art performance. Moreover, unlike DrQA, our models are all trained using distant supervision and, without pre-training on the original SQuAD dataset11, our Single Reader model still achieves better performance on the WikiMovie and CuratedTREC datasets.\nNext we observe that the Reinforced RankerReader (R3) achieves the best performance on the Quasar-T, WikiMovies, and CuratedTREC datests and achieves significantly better performance than our internal baseline model Simple Ranker-Reader (SR2) on all datasets except CuratedTREC. These results demonstrate the effectiveness of using RL to jointly train the Ranker and Reader both as compared to competing approaches and the non-RL Ranker-Reader baseline."
    }, {
      "heading" : "5.2 Further Analysis",
      "text" : "In this subsection, we first present an analysis of the improvement of both Ranker and Reader trained with our method, and then discuss ideas for further improvement.\n10Evaluation tooling is from SQuAD (Rajpurkar et al., 2016).\n11The performance of our Single Reader model on the original SQuAD dev set is F1 77.0, EM 67.6 which is close to the BiDAF model, F1 77.3, EM 67.7 and DrQA model, F1 78.8, EM 69.5.\nQuantitative Analysis First, we examine whether our RL approach could help the Ranker overcome the absence of any ground-truth ranking score. To control everything but the change in Ranker, we conduct two experiments combining the same Single Reader with two different Rankers trained from SR2 and R3, respectively. Table 4 shows the results on the Quasar-T test dataset. Note that the Single Reader combined with the Ranker trained from R3 model achieves an EM 1.3 higher performance than combined with the Ranker from SR2 which treats all passages containing ground-truth answer as positive cases. That means our proposed Ranker is better than the Ranker normally trained in the distant supervision setting.\nWe also find that the performance of R3 can still achieve an EM 1.0 higher than the Single Reader combined with the Ranker from R3 through Ta-\nble 4. In this setting, the Ranker is the same, while the Reader is trained differently. We infer from this that our proposed methods R3 can not only improve the Ranker but also the Reader.\nPotential Further Improvement We offer a statistical analysis to approximate the upper bound achievable by only improving the ranking models. This is evaluated by computing the QA performance with the best passage among the top-k ranked passages. Specifically, for each question, we extract one answer from each of the top-50 passages retrieved from the IR system, and take the top-k answers with the highest scores according to Eqn.(12) from these. Based on the k answer candidates, we compute the TOP-k F1/EM by evaluating on the answer with highest F1/EM score for each question. This is equivalent to having an oracle ranker that assigns a +∞ score to the passage (from the passages providing top-k candidates) yielding the best answer candidate.\nTable 5 shows a clear gap between TOP-3/5 and TOP-1 QA performances (over 12-20%). According to our evaluation approach of TOP-k F1/EM and since the same SR model is used, this gap is only from the contribution of the oracle ranker. Although our model is far from the oracle performance, it still provides a useful upper bound for improvement.\nRanker Performance Analysis Next we show the intermediate performance of our method on the ranking step. Since we do not have the groundtruth for the ranking task, we evaluate on pseudo labels: a passage is considered positive if it contains the ground-truth answer. Then a ranker’s topk output is considered accurate if any of the k passages contain the answer (i.e. top-k recall). Note\nthat this way of evaluation on top-1 is consistent with the training objective of the ranker in SR2.\nFrom the results in Table 7, the Ranker from R3 performs significantly better than the one from SR2 on top-1 and top-3 performance, despite the fact that it is not directly trained to optimize this pseudo accuracy. Given the evaluation bias that favors the SR2, this indicates that our R3 model could make Ranker training easier, compared to training on the objective in Eqn.13 with pseudo labels.\nStarting from top-5, the Ranker from R3 gives slightly lower recall. This is because the two Rankers have a similar ability to rank the potentially useful passages in the top-5, but the evaluation bias benefits the SR2 Ranker. Overall, our R3 could successfully rank the potentially more useful passages to the highest positions (top 1-3), improving the overall QA performance.\nIn Table 6 we show an example which illustrates the importance of good ranking. The passages on the left are from the R3 Ranker and the ones on the right from the SR2 Ranker. If SR2 ranked P2 or P3 higher, it could also have extracted the right answer. In general, if passages that can entail the answer are ranked more accurately, both models could be improved."
    }, {
      "heading" : "6 Related Work",
      "text" : "The task of Open domain question answering dates back to as early as (Green Jr et al., 1961) and was popularized with TREC-8 (Voorhees, 1999). The task is to produce the answer to a question by exploiting resources such as documents (Voorhees, 1999), webpages (Kwok et al., 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017). An early consensus since TREC-8 has produced an approach with three major components: question analysis, document retrieval and ranking, and answer extraction. Although question analysis is relatively mature, answer extraction and document ranking still represent significant challenges.\nVery recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b). These datasets deal with the end-to-end open-domain QA setting, where\nonly question-answer pairs provide supervision. Similarly to previous work on open-domain QA, existing deep learning based solutions to the above datasets also rely on a document retrieval module to retrieve a list of passages for RC models to extract answers. Therefore, these approaches suffer from the limitation that the passage ranking scores are determined by n-gram matching (with tf-idf weighting), which is not ideal for the goal of question answering.\nOur ranker module in R3 could help to alleviate the above problem, and RL is a natural fit to jointly train the ranker and reader since the passages do not have ground-truth labels. Our work is related to the idea of soft or hard attentions (usually with reinforcement learning) for hierarchical or coarseto-fine decision sequences making in NLP, where the attentions themselves are latent variables. For example, (Lei et al., 2016) propose to first extract informative text fragments then feed them to text classification and question retrieval models. (Cheng and Lapata, 2016) and (Choi et al., 2017) proposed coarse-to-fine frameworks with an additional sentence-level prediction followed by the original word-level prediction for text summariza-\ntion and reading comprehension, respectively. To the best of our knowledge, we are the first apply this kind of framework to the open-domain question answering."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have proposed and evaluated R3, a new opendomain QA framework which combines IR with a deep learning based Ranker and Reader. First the IR model retrieves the top-N passages (N a hyperparameter) conditioned on the question. Then the Ranker and Reader are trained jointly using reinforcement learning to directly optimize the expectation of extracting the ground-truth answer from the retrieved passages. To predict, the scores computed from Ranker and Reader are summed for the final answer extraction. Our model outperforms a baseline model which jointly trains Ranker and Reader using supervised learning, and R3 achieves the best performance on several datasets."
    } ],
    "references" : [ {
      "title" : "Modeling of the question answering task in the yodaqa system",
      "author" : [ "Petr Baudiš", "Jan Šedivỳ." ],
      "venue" : "International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222– 228. Springer.",
      "citeRegEx" : "Baudiš and Šedivỳ.,? 2015",
      "shortCiteRegEx" : "Baudiš and Šedivỳ.",
      "year" : 2015
    }, {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Bordes et al\\.,? 2015",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2015
    }, {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the Conference on Association for Computational Linguistics.",
      "citeRegEx" : "Chen et al\\.,? 2017a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Enhanced lstm for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Chen et al\\.,? 2017b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural summarization by extracting sentences and words",
      "author" : [ "Jianpeng Cheng", "Mirella Lapata." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Cheng and Lapata.,? 2016",
      "shortCiteRegEx" : "Cheng and Lapata.",
      "year" : 2016
    }, {
      "title" : "Coarse-to-fine question answering for long documents",
      "author" : [ "Eunsol Choi", "Daniel Hewlett", "Jakob Uszkoreit", "Illia Polosukhin", "Alexandre Lacoste", "Jonathan Berant." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Choi et al\\.,? 2017",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2017
    }, {
      "title" : "Gated-attention readers for text comprehension",
      "author" : [ "Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William W Cohen", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the Conference on Association for Computational Linguistics.",
      "citeRegEx" : "Dhingra et al\\.,? 2017a",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "QUASAR: Datasets for question answering by search and reading",
      "author" : [ "Bhuwan Dhingra", "Kathryn Mazaitis", "William W Cohen." ],
      "venue" : "arXiv preprint arXiv:1707.03904.",
      "citeRegEx" : "Dhingra et al\\.,? 2017b",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "SearchQA: A new q&a dataset augmented with context from a search engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "Ugur Guney", "Volkan Cirik", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1704.05179.",
      "citeRegEx" : "Dunn et al\\.,? 2017",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "Building watson: An overview of the deepqa project",
      "author" : [ "David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager" ],
      "venue" : "AI magazine,",
      "citeRegEx" : "Ferrucci et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ferrucci et al\\.",
      "year" : 2010
    }, {
      "title" : "Baseball: an automatic question-answerer",
      "author" : [ "Bert F Green Jr", "Alice K Wolf", "Carol Chomsky", "Kenneth Laughery." ],
      "venue" : "Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference, pages 219–224. ACM.",
      "citeRegEx" : "Jr et al\\.,? 1961",
      "shortCiteRegEx" : "Jr et al\\.",
      "year" : 1961
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Scaling question answering to the web",
      "author" : [ "Cody Kwok", "Oren Etzioni", "Daniel S Weld." ],
      "venue" : "ACM Transactions on Information Systems (TOIS), 19(3):242– 262.",
      "citeRegEx" : "Kwok et al\\.,? 2001",
      "shortCiteRegEx" : "Kwok et al\\.",
      "year" : 2001
    }, {
      "title" : "Rationalizing neural predictions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Lei et al\\.,? 2016",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2016
    }, {
      "title" : "Key-value memory networks for directly reading documents",
      "author" : [ "Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Miller et al\\.,? 2016",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "MS MARCO: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "arXiv preprint arXiv:1611.09268.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Seo et al\\.,? 2017",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2017
    }, {
      "title" : "The trec-8 question answering track report",
      "author" : [ "Ellen M. Voorhees." ],
      "venue" : "Trec, volume 99, pages 77–82.",
      "citeRegEx" : "Voorhees.,? 1999",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 1999
    }, {
      "title" : "Building a question answering test collection",
      "author" : [ "Ellen M Voorhees", "Dawn M Tice." ],
      "venue" : "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200–207. ACM.",
      "citeRegEx" : "Voorhees and Tice.,? 2000",
      "shortCiteRegEx" : "Voorhees and Tice.",
      "year" : 2000
    }, {
      "title" : "What is the jeopardy model? a quasisynchronous grammar for qa",
      "author" : [ "Mengqiu Wang", "Noah A Smith", "Teruko Mitamura." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning natural language inference with LSTM",
      "author" : [ "Shuohang Wang", "Jing Jiang." ],
      "venue" : "Proceedings of the Conference on the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Wang and Jiang.,? 2016",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2016
    }, {
      "title" : "A compareaggregate model for matching text sequences",
      "author" : [ "Shuohang Wang", "Jing Jiang." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Wang and Jiang.,? 2017a",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2017
    }, {
      "title" : "Machine comprehension using match-LSTM and answer pointer",
      "author" : [ "Shuohang Wang", "Jing Jiang." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Wang and Jiang.,? 2017b",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2017
    }, {
      "title" : "Gated self-matching networks for reading comprehension and question answering",
      "author" : [ "Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou." ],
      "venue" : "Proceedings of the Conference on Association for Computational Linguistics.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-perspective context matching for machine comprehension",
      "author" : [ "Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian." ],
      "venue" : "arXiv preprint arXiv:1612.04211.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams." ],
      "venue" : "Machine Learning.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Dynamic coattention networks for question answering",
      "author" : [ "Caiming Xiong", "Victor Zhong", "Richard Socher." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Xiong et al\\.,? 2017",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2017
    }, {
      "title" : "Wikiqa: A challenge dataset for open-domain question answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved neural relation detection for knowledge base question answering",
      "author" : [ "Mo Yu", "Wenpeng Yin", "Kazi Saidul Hasan", "Cicero dos Santos", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "Proceedings of the Conference on Association for Computational Linguistics.",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "End-to-end answer chunk extraction and ranking for reading comprehension",
      "author" : [ "Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "arXiv preprint arXiv:1610.09996.",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "These approaches have achieved state of the art results in simplified closeddomain settings1 such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted.",
      "startOffset" : 111,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : ", wikipedia) instead of a pre-selected passage (Chen et al., 2017a).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "Knowledge sources can be knowledge bases (Berant et al., 2013) or structured or unstructured text passages (Ferrucci et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : ", 2013) or structured or unstructured text passages (Ferrucci et al., 2010; Baudiš and Šedivỳ, 2015).",
      "startOffset" : 52,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : ", 2013) or structured or unstructured text passages (Ferrucci et al., 2010; Baudiš and Šedivỳ, 2015).",
      "startOffset" : 52,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Recent deep learning-based research has focused on open-domain QA based on large text corpora such as wikipedia, applying a two-stepprocess of information retrieval (IR) to extract passages and reading comprehension (RC) to select an answer phrase from them (Chen et al., 2017a; Dhingra et al., 2017b).",
      "startOffset" : 258,
      "endOffset" : 301
    }, {
      "referenceID" : 8,
      "context" : "Recent deep learning-based research has focused on open-domain QA based on large text corpora such as wikipedia, applying a two-stepprocess of information retrieval (IR) to extract passages and reading comprehension (RC) to select an answer phrase from them (Chen et al., 2017a; Dhingra et al., 2017b).",
      "startOffset" : 258,
      "endOffset" : 301
    }, {
      "referenceID" : 27,
      "context" : "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 34,
      "context" : "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 29,
      "context" : "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 31,
      "context" : "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 28,
      "context" : "Dividing the pipeline into IR and RC stages leverages an enormous body of research in both IR and RC, including recent successes in RC via neural network techniques (Wang and Jiang, 2017b; Yu et al., 2016; Wang et al., 2016; Xiong et al., 2017; Wang et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 263
    }, {
      "referenceID" : 20,
      "context" : "In standard RC model training, passages are manually selected to guarantee that groundtruth answers are contained and annotated within the passage (Rajpurkar et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "trast, in SR-QA approaches such as (Chen et al., 2017a; Dhingra et al., 2017b), the model is given only QA-pairs and uses an IR component to retrieve passages similar to the question from a large corpus.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "trast, in SR-QA approaches such as (Chen et al., 2017a; Dhingra et al., 2017b), the model is given only QA-pairs and uses an IR component to retrieve passages similar to the question from a large corpus.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 30,
      "context" : "The Ranker is trained using REINFORCE (Williams, 1992) with a reward de-",
      "startOffset" : 38,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : "Passage ranking models for non-factoid QA (Wang et al., 2007; Yang et al., 2015) are able to learn to rank these passages; but these models are trained using human annotated answer labels, which are not available here.",
      "startOffset" : 42,
      "endOffset" : 80
    }, {
      "referenceID" : 32,
      "context" : "Passage ranking models for non-factoid QA (Wang et al., 2007; Yang et al., 2015) are able to learn to rank these passages; but these models are trained using human annotated answer labels, which are not available here.",
      "startOffset" : 42,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "We discuss the Ranker-Reader model in detail in the next section but briefly, the Ranker and Reader are implemented as variants of MatchLSTM models (Wang and Jiang, 2016).",
      "startOffset" : 148,
      "endOffset" : 170
    }, {
      "referenceID" : 25,
      "context" : "In this section, we first review the MatchLSTM (Wang and Jiang, 2016) which provides input for both the Reader and Ranker.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "is the column concatenation of matrices; Elementwise operations (· ⊙ ·) and (· − ·) are also used to represent word-level matching (Wang and Jiang, 2017a; Chen et al., 2017b).",
      "startOffset" : 131,
      "endOffset" : 174
    }, {
      "referenceID" : 4,
      "context" : "is the column concatenation of matrices; Elementwise operations (· ⊙ ·) and (· − ·) are also used to represent word-level matching (Wang and Jiang, 2017a; Chen et al., 2017b).",
      "startOffset" : 131,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "vision setting following (Chen et al., 2017a)’s work, including Quasar-T (Dhingra et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : ", 2017a)’s work, including Quasar-T (Dhingra et al., 2017b), SQuAD (Rajpurkar et al.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : ", 2017b), SQuAD (Rajpurkar et al., 2016), WikiMovie (Miller et al.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : ", 2016), WikiMovie (Miller et al., 2016), CuratedTREC (Baudiš and Šedivỳ, 2015), and WebQuestion (Berant et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : ", 2016), CuratedTREC (Baudiš and Šedivỳ, 2015), and WebQuestion (Berant et al.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : ", 2016), CuratedTREC (Baudiš and Šedivỳ, 2015), and WebQuestion (Berant et al., 2013).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : ", 2017a,b), BiDAF (Seo et al., 2017), and DrQA (Chen et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : ", 2017), and DrQA (Chen et al., 2017a).",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "Quasar-T (Dhingra et al., 2017b) is a dataset for SR-QA, with question-answer pairs from various internet sources.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "The other four datasets we consider are: SQuAD (Rajpurkar et al., 2016), the Stanford QA dataset, where each question-answer pair is generated by annotators using a given Wikipedia paragraph; WikiMovie (Miller et al.",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : ", 2016), the Stanford QA dataset, where each question-answer pair is generated by annotators using a given Wikipedia paragraph; WikiMovie (Miller et al., 2016) which contains movie-related questions from the OMDb and MovieLens databases and where the questions can be answered using Wikipedia pages; CuratedTREC (Baudiš and Šedivỳ, 2015), based on TREC (Voorhees and Tice, 2000) and designed for open-domain QA; and WebQuestion (Berant",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : ", 2016) which contains movie-related questions from the OMDb and MovieLens databases and where the questions can be answered using Wikipedia pages; CuratedTREC (Baudiš and Šedivỳ, 2015), based on TREC (Voorhees and Tice, 2000) and designed for open-domain QA; and WebQuestion (Berant",
      "startOffset" : 160,
      "endOffset" : 185
    }, {
      "referenceID" : 23,
      "context" : ", 2016) which contains movie-related questions from the OMDb and MovieLens databases and where the questions can be answered using Wikipedia pages; CuratedTREC (Baudiš and Šedivỳ, 2015), based on TREC (Voorhees and Tice, 2000) and designed for open-domain QA; and WebQuestion (Berant",
      "startOffset" : 201,
      "endOffset" : 226
    }, {
      "referenceID" : 3,
      "context" : "For these four datasets under the distant supervision setting, no candidate passages are provided so we build a similar sentence-level Search Index based on English Wikipedia, following (Chen et al., 2017a).",
      "startOffset" : 186,
      "endOffset" : 206
    }, {
      "referenceID" : 21,
      "context" : ", 2017a,b), a gated-attention reader for text comprehension; BiDAF (Seo et al., 2017), a reader with bidirectional attention flow for machine comprehension; and DrQA (Chen et al.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : ", 2017), a reader with bidirectional attention flow for machine comprehension; and DrQA (Chen et al., 2017a), a document reader for question answering.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "Our model is optimized using Adamax (Kingma and Ba, 2015).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "We use fixed GloVe (Pennington et al., 2014) word embeddings.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "GA (Dhingra et al., 2017a) 26.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "4 - - - - - - - BiDAF (Seo et al., 2017) 28.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "9 - - - - - - - DrQA (Chen et al., 2017a) - - - 28.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "DrQA-MTL (Chen et al., 2017a) - - - 29.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "7 YodaQA (Baudiš and Šedivỳ, 2015) - - - - - - - 31.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "Evaluation tooling is from SQuAD (Rajpurkar et al., 2016).",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "was popularized with TREC-8 (Voorhees, 1999).",
      "startOffset" : 28,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "The task is to produce the answer to a question by exploiting resources such as documents (Voorhees, 1999), webpages (Kwok et al.",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "The task is to produce the answer to a question by exploiting resources such as documents (Voorhees, 1999), webpages (Kwok et al., 2001) or structured knowledge bases (Berant et al.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : ", 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017).",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : ", 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017).",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 33,
      "context" : ", 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017).",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).",
      "startOffset" : 209,
      "endOffset" : 312
    }, {
      "referenceID" : 3,
      "context" : "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).",
      "startOffset" : 209,
      "endOffset" : 312
    }, {
      "referenceID" : 12,
      "context" : "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).",
      "startOffset" : 209,
      "endOffset" : 312
    }, {
      "referenceID" : 9,
      "context" : "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).",
      "startOffset" : 209,
      "endOffset" : 312
    }, {
      "referenceID" : 8,
      "context" : "Very recently, information retrieval plus machine reading comprehension (SR-QA) become a promising solution to open-domain QA, especially after datasets created specifically for the multiplepassage RC setting (Nguyen et al., 2016; Chen et al., 2017a; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b).",
      "startOffset" : 209,
      "endOffset" : 312
    }, {
      "referenceID" : 15,
      "context" : "For example, (Lei et al., 2016) propose to first extract informative text fragments then feed them to text classification and question retrieval models.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "(Cheng and Lapata, 2016) and (Choi et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "(Cheng and Lapata, 2016) and (Choi et al., 2017) proposed coarse-to-fine frameworks with an additional sentence-level prediction followed by the original word-level prediction for text summarization and reading comprehension, respectively.",
      "startOffset" : 29,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closeddomain settings1 such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that “reads” the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance. In this paper, we present a novel opendomain QA system called Reinforced Ranker-Reader (R3), based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker ∗This work has been done during the 1st author’s internship with IBM. In the QA community, “openness” can be interpreted as referring either to the scope of question topics or to the breadth and generality of the knowledge source used to answer each question. Following Chen et al. 2017a we adopt the latter definition. along with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}