{
  "name" : "1302.6837.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Anytime Decision Making with Imprecise Probabilities",
    "authors" : [ "Michael Pittarelli" ],
    "emails" : [ "mike@sunyit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 ANYTIME ALGORITHMS FOR\nDECISION MAKING\nIn general, _ an anytime algorithm [Boddy and Dean, 1989] provtdes output at each step of its execution; the output improves, in some sense, the longer the algorithm runs. For a decision problem under risk, where the probability of each of the conditions is known and is represented as a single real number, an anytime decision algorithm might consider the actions in some sequence and output at each step the action with highest expected utility among those so far examined.\nA more realistic representation of uncertainty is by means of intervals of real numbers [Kyburg, 1992]. A reasonable criterion might interpret the intervals as linear inequality constraints determining a set of real-valued probability functions over the conditions and identify an action as inadmissible if there does not exist an element of the set relative to which the action maximizes expected utility [Levi, 1980]. This may be determined for each action by checking for\nthe existence of a feasible solution to a linear pro gram [Pittarelli, 1991]. Any action not yet ruled out as inadmissible by some time may be classified (pos-\nsibly incorrectly) as admissible relative to the current collection of probability intervals. At a higher level, the set of intervals may be iteratively refined (e.g., for confidence intervals, by decreasing the confidence level [Loui, 1986]) and the process of testing for inadmissibility repeated for each refinement, for the elements of the most recently computed admissible set.\nAnytime methods for probabilistic inference have recently been developed [Frisch and Haddawy, 1992]. These determine an interval of probability for a conclusion from a collection of premises each of which has an associated probability interval. The\ninterval of probability for the conclusion is initially [0,1] and is narrowed with each application of a rule of infere�ce. This approach differs from [Nilsson, 1986] which requires the construction of a poten tially very large linear program for determining the endpoints of the probability interval for the entailed sentence. However, it is straightforward to base an\nanytime inference technique directly on Nilsson's methods, by considering only subsets of the given set of premises. We will discuss methods of anytime decision making that utilize both forms of probabilis tic inference.\nProbabilistic databases [Pittarelli, 1994], that is, col lections of contingency tables of joint frequencies or probabilities for finite variables, may also provide linear constraints relevant to a decision problem. There is again a tradeoff between the tightness of the constraint that can be inferred and the cost of\ndoing the inference. We will discuss methods of any time decision making utilizing probabilistic data bases also.\nAnytime Decision Making with Imprecise Probabilities 471\n2 ANYTIME DEDUCTION AND DECISIONS\nFrisch and Haddawy [1992] have developed a system of deduction for probabilistic logic based on infer ence rules. The rules may be employed to compute increasingly na.JTow probability intervals for the con clusion of an argument. At any time, the currently computed interval is correct in the sense that it con tains the narrowest interval computable from the probabilities associated with all of the premises. Thus, there is a tradeoff between the precision of an entailed probability interval and the time required to compute it. This feature makes anytime deduc tion especially suitable for use by ·resource bounded\" systems [Horvitz et al, 1989]; as Frisch and Haddawy point out, however, how to control the time/precision tradeoff depends on the particular decision situation in which the system finds itself.\nFrisch and Haddawy's anytime deduction, Nilsson's probabilistic logic (unless maximum entropy or related techniques are used), and related systems, produce probability intervals for entailed sentences. A criterion applicable to decision problems in which probabilities are given as intervals and that reduces to standard maximization of expected utility when the intervals reduce to point values is Levi's E· admissibility criterion [Levi, 1980]: all and only those actions maximizing expected utility relative to some member of the set of (point valued) probability functions compatible with (or actually representing [Kyburg, 1992]) current beliefs are admissible. We will consider decision problems consisting of a set of actions A == { a1, • 0 0 , am} and a set of mutu ally exclusive and exhaustive conditions 0 = { cl> . . . 1 en} such that p (c;l a;) = p (c;), for all i,j. (Jeffrey [1976] shows how a problem in which condi tions are not probabilistically independent of actions can be converted to an equivalent problem in which they are.) A family D of subsets of the ( n -I)-dimensional sim plex Pc of all probability distributions over 0 may be defined as where D = {D(ad, ... ,D(am)},\nn\nD(a;) = {pEPcl � p (c;) x U(a;,c;)?: i=J\nn\n� p (c;) x U(aJ:,c;),k=l, . . ,m}. i=l\nD (a;) is the set of probability functions relative to each of which action a; maximizes expected utility, and is referred to as the domain of a; [Starr, 1966]0\nEach domain is convex; domains may intersect at faces; and U D(a) = Pc. GEA Suppose the current belief state regarding probabili ties of conditions is represented as Kc;;,Pc. An action a is E-admissible if and only if K nD (a) '* 0. The fewer admissible actions, the better. The number of admissible actions decreases monotonically with the size of K: K1c;;,K2 __. K1nD(a;)c;;, K2nD(a;). Thus, there will be a tradeoff between the quality of a decision - i.e., the number of admissible actions among which choice must be made using non probabilistic criteria - and the computational expense of shrinking K. Reduction in the size of K may be achieved by applying additional inference rules; by adding premises, thereby enlarging the linear constraint system; by \"extending\" and \"pro jecting\" larger and larger portions of a probabilistic database; etc. (Of course, the perceived quality of a decision may be enhanced without this much work: invoke the maximum entropy principle. This approach will be criticized below.) Computing aimed at reducing the number of admissible actions may be interleaved with analysis of the type pro posed by Horvitz et al. [1989] to determine whether the cost of such computation exceeds its expected value.\nConsider a decision the outcome of which is con tingent on the truth or falsity of a single probabilist ically entailed sentence: \"It will rain this after noon\". Suppose the actions under consideration are wGo to the beach • and \"Do not go to the beach •. Utilities of the four possible outcomes are:\nU(Go, Rain) = 0, U(Go, No rain) = 1,\nU(Do not, Rain) = 0. 8, U(Do not, No rain) = 0. 2.\nThe agent's knowledge is represented in part by the propositions and associated probability intervals:\n(1) p(Temperature > 85) E [.95, 1] (%) p(Temperature > 85 __.Rain) E [. 4,. 6J (S) p((B. pressure < 30 & Humidity > 80) __.Rain) E [.65,. 95 ] (l) p(B. pressure < 30) E [.95, 1] (5) p(Humidity > 80) E [. 95, 1] (6) p(August __. Rain) E [.2, 1] (7) p(August) E [1, 1J.\nBoth \"Go to the beach • and wDo not go to the beach· have non-empty domains: \"Go\" maximizes expected utility when p(Rain) � 0.5; \"Do not\" does for p(Rain) � 0. 5. Neither can be ruled out a priori. However, Frisch and Haddawy's probabilistic infer-\n472 Pittarelli\nence rules may be applied one-at-a-time to narrow the interval for p(Rain) until a single admissible action emerges, or it is no longer economical to con tinue refining (e.g., the last train to the beach is about to leave) and a choice among the admissible actions must be made using some other criterion (e.g., choose at random [Elster, 1989], use maximin, maximize expected utility relative to the midpoint of the probability interval, etc.) . Initially, we can deduce (8) p(Rain) E [0,1], from the •Trivial derivation • rule: 1- p (a IS) E [0,1]. We may next apply \"Forward implication propaga tion\",\np (.fllo) E lz,y], p (.8-+alc5) E [u,v]lp (ajc5) E [min (O,z+u -1) ,v],\nto statements (1) and {2), yielding (9) p(Rain) E [.35,.6]. Although it does not have any effect at this stage, the ·Multiple derivation • rule should be applied to maintain the tightest interval for the •target • sen tence:\np (alc5) E [z,y], p (alc5) E [u,v]l p (a IS) E [max (z,u) , min (y,v)].\nSince .5E I -J5,.6], both actions remain admissible. Next, ·conjunction introduction·,\np (alo) E [z,y], p (.816) E [u,v] 1- p (a 81 .816) E [max (O,x+u-1) , min (y,v)]\nis applied to statements (4) and (5) , yielding (10) p(B . pressure < 30 & Humidity > 80) E [.9,1]. Applying forward implication propagation to state ments (3) and (10) gives (11) p(Rain) E [.55,.95] Although combining statement (11) with statement (9) via the multiple derivation rule will further nar row the target interval, there is no need to do so; nor is there any need to consider statements (6) and (7). ·no not go\" has emerged as uniquely admissi ble:\nD(Do not go) = {pEP{Rt�in, No rt�in} I p(Rain) � .5}, D(Go) = {pEP{R4in, No rain} I p(Rain) 5 .5},\nD(Go)n{p EP{RIIin, No rain} I p(Rain) E[.55,.95]} = 0. D(Do not go) n\n{p EP{Rain, No rain} I p(Rain) E[.55,.95]} * 0.\n3 NILSSON'S PROBABILISTIC LOGIC AND DECISION MAKING\nNilsson's methods may be modified to yield an any time procedure for decision making. Rather than construct the linear system corresponding to the full\nset of sentences, increasingly larger systems may be constructed by adding sentences to the subset currently in use until a uniquely admissible action emerges or it is necessary to choose among the currently admissible actions.\nThis may be illustrated with the sentences and deci sion problem above. Suppose sentences (3) and (5) are chosen for the first iteration. Using Nilsson's \"semantic tree· method, five sets of possible worlds are identified. Both actions are E-admissible. ·ao· is E-admissible because there exist feasible solutions to the system of linear inequalities below, where p; is the probability of set wi of possible worlds; •Rain • is true in sets Ws and Ws, \"Humidity > so· is true in sets w11 w4 and w5, etc.: Pi + P2 + Ps + p, + Ps = 1 P2 + Ps + p.,. + Ps � 0.65 P2 + Ps + P-�a + Ps 5 0.95 Pi + P-�a + P& � 0.95 (Ps + P6)x0 +(Pi+ P2 + P-�a)X1 � (Ps + P&) x0.8 + (p1 + P2 + P-�a) x0.2 \"Do not go• is also E-admissible, since the system resulting from reversing the direction of the final inequality also has feasible solutions.\nNow add sentence (4). The resulting 8 sets of possi ble worlds may be determined by expanding only the •live • terminal nodes of the semantic tree con structed at the first iteration. (To eliminate the need for a row interchange, the root of the initial tree should represent the target sentence. One may proceed in this way until it is no longer necessary to continue, possible to continue, or worth continuing. H the number of sets of worlds generated becomes excessive, Snow's compression method [1991] may be attempted.) \"Do not go• is now identified as uniquely E-admissible; there exist feasible solutions to the system below, but not to the corresponding system for ·ao·: �+P2+�+�+�+h+�+�=1 Pi + P2 + P3 + P-�a + Ps + PG + � � 0.65 Pi + P2 + Ps + P-�a + Ps + Ps + � 5 0.95 Pi + P2 + Ps + Ps � 0.95 Pi + Ps + Ps + Ps � 0.95 (Pt + P2 + Ps + P-�a) X0.8 + (Ps + P6 + P1 + Ps) X0.2 � (Pi+ P2 + P3 + p.,.)xO + (Ps + Ps + P7 + Ps)X1\n4 DECISIONS WITH MULTIPLE CONDITIONS\nFrisch and Haddawy's system is applicable to deci sion problems with an arbitrary number n of mutu1 ally exclusive conditions. The (2n (n -1)+1) statements\nAnytime Decision Making with Imprecise Probabilities 473\np (c1 v • · · v c,.) E [1,1] p ( c:18c:z) E [O,OJ\np(c,.-1�c,.) E IO,OJ must be included. Intervals must be maintained for each of the conditions c;. The soundness of Frisch and Haddawy's inference rules guarantees that, at any time, the interval [l;,ui] associated with any c:; is a superset of the tightest interval entailed (algebrai cally) by the full collection of sentences. Thus, the sharpest intervals available at any time yield a linear system from which it can be determined whether an action would not be E-admissible relative to the sharper probability bounds computable at any later time; action a; is {ultimately} admissible only if there exist feasible solutions to p ( c1) + · · · + p ( c,.} = 1 p (cd � 11 p (cd � u1\np (c:,.) � l,. p (c:,.) � u,. p(ct)xU(a;,cd + · · · + p(c,.)xU(a;,c,.) �\np (cd X U(a1,c.) + · · · + p (c,.) X U(a\"c,.)\np(c:t)xU(a;,c:t) + · · · + p(c,.)xU(a;,c,.) � p(cl)xU(am,c.J+ ··· +p(c,.)xU(a\".,c,.), where l; and u; are the current bounds on p ( c;)·\nInformation may be lost if probability intervals are computed separately for each of the conditions in a decision problem with more than two conditions. There are convex polytopes of probability distribu tions over n > 2 conditions such that the solution set of the linear system obtained by combining the unicity constraint with the inequalities correspond ing to the tightest probability bounds inferable from the polytope for the conditions is a proper superset of it. The intersection of the domain of an action with the original polytope may be empty, although its intersection with the solution set is not. Thus, actions that are not E-admissible may not be identified as such, resulting in unnecessary indeter minateness.\nNilsson's semantic tree method can be adapted to take into account the mutual exclusivity and exhaustiveness of multiple (i.e., more than two) con ditions in a decision problem. The first n levels of\nthe tree will correspond to the n conditions. (This facilitates the anytime adaptation of Nilsson's methods discuSBed above.) At level n there will be n live nodes, one for each of the assignments in which exactly one of the conditions is true. The remaining levels of the tree are constructed as usual.\nFor example, with conditions c:1, c:z and ':\" an arbi trary number m � 2 of actions a;, and data p (B-et} E 10.9,1] and p (B) E !0.8,1], there are 6 sets of possible worlds, corresponding to the matrix\n11 0 0 0 0 (ci) 001100 (�) 0 0 0 0 1 1 ( c3) 1 1 1 0 1 0 (B-ed 1 0 0 1 0 1 (B)"
    }, {
      "heading" : "Action a; is E-admissible iff there exist feasible solu",
      "text" : "tions to the system of linear inequalities: Pt + . . . + P6 = 1 Pt + P2 + Ps + Pb � 0.9 Pt + P-& + Pe � 0.8 (Pt + P2)xU(a;,cl) + (P3 + P-&)xU(a;,�)\n+ (Ps + P&)xU(a;,c3) � (Pt + P2) X U(at,ct) + (Ps + P-&)X U{at1C:Z)\n+ (Ps + P6)x U(a.,c3)\n(Pt + P2)xU(a;,c1) + (p3 + p4)xU(a;,�) + (Ps + P6)xU(a;,c3) �\n(PI + P2) X U( a,.., c.) + (Ps + p4) X U( a,.., c:z) + (Pt. + P6) X U( am, c3)\n5 MAXIMUM ENTROPY AND PROBABILISTIC LOGIC\nNilsson 11986] shows how to maximize entropy within the set of probability distributions over the possible worlds in order to compute a point-valued probability for an entailed sentence. The maximum entropy estimate of the probability of the entailed sentence is the sum of the components of the max imum entropy distribution corresponding to the worlds in which the sentence is true. Point-valued probabilities for each of the conditions in a decision problem are computable also from the distribution over the possible worlds maximizing entropy.\nIf an action maximizes expected utility relative to the maximum entropy estimate, it is guaranteed to be E-admissible relative to any set of distributions to which the estimate belongs. But, of course, the converse does not hold. E-admissible actions that ' depending on one's philosophy of decision making, perhaps should be retained for further consideration ' are eliminated. It may be that one of these actions\n474 Pittarelli\nuniquely maximizes expected utility relative to the (pace, inter alio6, DeFinetti) true but unknown dis tribution.\nIf the maximum entropy distribution tends to be close, on some metric, to the actual distribution over the worlds, then its projection will tend to be close to the actual probability distribution over the condi tions. (But note that the result of marginalising the maximum entropy element of a set K is not always the maximum entropy element of the set of margi nals of elements of K.) The closer the estimate of the probabilities of the conditions is to the true dis tribution, the likelier it is that it will belong to one of the domains containing the true distribution. Thus, the likelier it is that an action maximizing utility relative to the true distribution will be selected.\nHow close can one expect the maximum entropy estimate to be to the true distribution over the pos sible worlds? If you accept Jaynes' concentration theorem [Jaynes, 1982], i.e., if probabilities are observed relative frequencies and sequences of obser vations are equiprobable a priori and the number of observations is infinite and if you accept difference in entropy as a distance measure, then the answer is ·very•. If you want to stick with metric distances and probabilities are allowed to be subjective, then it might be reasonable to ask how close the max imum entropy element is to the centroid of the set, which minimizes expected sum-of-squares error [MacQueen and Marschak, 1975], but is more expen sive to calculate [Piepel, 1983]. When the set of distributions is either a singleton or the full probability simplex, the maximum entropy element is guaranteed to coincide with the centroid. It always coincides with the centroid for the modu6 · ponen.s inference pattern: Let z = p ( P) and y = p ( P- Q) . There are four sets of possible worlds, the probabilities of which a.re the solutions to the system of equations:\nPt + P2 + P3 + P4 = 1 Pt + P2 = z Pt + P3 + P4 = Y· The solution set is either a single point (when p (P)=1) or a line segment in [0,1]4 with vertices (Pt•P2•P3tP4) = (y-(1-z), (1-y),1-x,O) (PttP2tP3tP4) = (y- (1-x),(1-y),O,l-z). The centroid of the line segment is the average of the vertices, which coincides with the maximum entropy distribution calculated by Nilsson.\nThis will not always be the case. It does not even appear that one can expect the maximum entropy\nestimate to be especially close to the centroid (which one cares about if one wishes to minimize expected squared error) . Consider the conjunction pattern of inference: from A and B, infer A fJ B. There are four sets of possible worl�s: those in which A&B is true, those in which Ae!B is true, etc. The set of solutions to the system is again either a single point or a line segment in [0,1)4• Let l (A&B) and u (A&B) denote, respectively, the greatest lower and least upper bounds on p (A&B). Ordering com ponents as\n(p (A&B),p (A&B),p (A&B),p (AFJB)), the solution set has vertices\nv1 = (l (A&B),p (A)-l (A&B),p (B)-l (A&B), (1-(p (A)+p (B) -l (A&B))) v2 = (u (A&B),p (A)-u (A&B),p (B)-u(A&B), (1-(p (A)+p (B)-u(A&B))). The centroid is the average of the two vertices: ce = (v1+v2)/2. The maximum entropy element coincides with the distribution computed under the assumption of pro babilistic independence of A and B:\nm = (p (A)xp (B),p (A)xp (B),p (A)xp (B), p (A)xp (B)).\nThe eccentricity of an element of any non-unit solu tion set K is the ratio between its (Euclidean) dis tance from the centroid and the maximum distance of any element of the set from the centroid:\necc (p,K) = d (p,ce)fmax d (p,ce). pEK\nThe eccentricity will have a minimum value of 0 (when p = ce) and a maximum value of 1 (when p is a vertex) .\nFor conjunction entailment, it is possible for the value of ecc ( m, K) to be quite high. For example, when p (A)=0.9 and p(B)=0.1, ecc (m,K)=0.8. The expected value of ecc (p, K) for a randomly selected element p of Kis 1/2. Letting <p (A),p (B)> range with uniform probability over (0,1)2, the expected value of ecc (m,K) is 1/3. So, for conjunction entail ment anyway, one cannot expect the maximum entropy approximation to be especially low-risk.\nKane [1990, 1991] has developed a method of com puting the maximum entropy solution that is faster than that proposed by Nilsson. Deutsch-McLeish [1990] has determined conditions under which Nilsson's proiection approxirna.tion (which is not, in general, the centroid of the solution set) coincides with the maximum entropy solution. These can be tested to determine whether the (much cheaper) projection approximation method can be substituted for direct maximization of entropy. But, as argued above, computing any type of point-valued estimate\nAnytime Decision Making with Imprecise Probabilities 475\nof condition probabilities for a decision problem is neither necessary nor wise.\n6 ANYTIME DECISION MAKING WITH PROBABUISTIC DATABASES\nA probabilistic database [Cavallo and Pittarelli, 1987; Pittarelli, 1994; Barbara et al, 1993] generalizes a relational database by replacing the characteristic function of a relation with a probability distribution (or probability intervals). For example, the tables below represent estimates of probabilities for various (joint) events on a typical August day in a fictitious developing country:\nRain No Phones pl\nyes true 0.4 yes false 0.1 no true 0.2 no false 0.3\nNo Phones Trains p2\ntrue yes 0.25 true no 0.35 false yes 0.25 false no 0.15\nNo Phones Temperature p 3\ntrue high 0.45 true med 0.1 true low 0.05 false high 0.25 false med 0.1 false low 0.05\nTemperature Humidity p\" high high 0.6 high low 0.1 med high 0.15 med low 0.05 low high 0 low low 0.1\nSuppose, again, that it must be decided whether or not to go to the beach. It is believed that the only relevant conditions are whether or not it will rain and whether or not evening trains will run. Utilities this time are:\nDon't go Go\n(rain, train) 3/4 1/2 (rain, no train) 7/8 0 (no rain, train) 1/8 1 (no rain, no train) 1/2 5/8\nConditional independence relations that would per· mit calculation of a unique (maximum entropy) joint distribution over all of the attributes men· tioned in the tables, and from which (by marginaJi.. zation} a unique probability distribution over the four joint conditions for the decision problem could he calculated, are not assumed. Nonetheless, it can he determined from the database that exactly one of the actions is &admissible. There are infinitely many distributions over the Cartesian product of the domains of the attributes in the database whose marginals coincide with the distributions in the database. Each of these is a solution to a system of 20 linear equations in 48 unknowns. (The solution set is referred to as the ezten.sion of the database.) The probabilities of any of the 4 rain/train condi tions is the sum of 12 of the 48 unknowns. Thus, E admissibility can he determined as in the previous examples.\nWe have seen that, for various systems of probabilis tic logic, it is not necessary to take into account all of the available sentences (even those that are relevant in the sense of having an effect on the entailed probabilities of the conditions) in order to solve decision problems. Similarly, working with an entire database ma.y introduce unnecessary expense. Anytime algorithms can be devised as well for deci sion ma.king with probabilistic databases.\nThe structure of a database, i.e., the set of seta of attributes on which it is defined, is referred to aa its !cheme. The scheme for the database above is\n{{Rain,No Phones},{No Phones,Trains}, {No Phones,Temp.},{Temp.,Humidity} }.\nScheme S is a refinement of scheme S' iff for each VES there exists a YES' such that V� Y. A database may he proiected onto any scheme that is a refinement of its own. The result is a database whoee elements a.re marginals of its own elements. For example, the projection of the database above onto the scheme {{Trains},{Temperature}} is:\nTrains p f>\nTemperature p6\nyes 0.5 high 0.7 no 0.5 med 0.2\nlow 0.1\nIf Sis a refinement of S', then the extension (to any number of attributes) of the projection of a database onto s· is a subset of the extension of the projection onto S [Pittarelli, 1994). Thus, if an action is E admissible relative to the set of probabilities over the conditions that can be calculated from a database, then it is E-admissible relative to the probabil ities calculated from any projection of the database.\n476 Pittarell i\nEquivalently, if an action can be determined not to be &admissible relative to a projection, it can be inferred that it is not &admissible relative to the original database.\nSince the set of &admissible actions decreases monc> tonically as schemes become less refined, anytime decision methods are possible for problems in which the set of conditions is the Cartesian product of attribute domains from the database (or can be con structed from the tuples in such a product). Let V0 denote this set of attributes and let S denote the scheme for the database. For purposes of illustra tion only, a particularly simple-minded approach would be the following: Project first onto\n{{t�}[vEVc}. Next, if necessary, project onto {VnV0[ VES, VnVc*0}. Next, try {VIVES, VnVc*0}. Extend the entire database (or extend its projection onto some scheme that can be identified, at some cost, as producing the same result less expensively [Pittarelli, 1993]) only as a last resort.\nFor the beach Vc = {Rain, Trains}. {{ t�}[t�E V c} is\nTrains p6 yes 0.5 no 0.5\ndecision problem, The projection onto\nRain p1 yes 0.5 no 0.5\nBoth actions are E-admissible relative to the set of joint probabilities compatible with this database.\nFor this problem,\n{VnVcl VES, VnVc*0} = {{t�}lt�EVc}.\nThe projection onto {VIVES, VnVc*0} is the set of distributions {p1, p2}, above. MDon't go\" is identified from this set of distributions as uniquely &admissible. There exist feasible solutions to the system of inequalities below, but not to the corresponding system for MGoM. p (Rain =yu,No Phonu =true, Trains =yes) + p (yes, true, no) = 0.4 p (yes, false, yes) + p (yes, false, no) = 0.1 p (no, true, yes) + p (no, true, no) = 0.2 p (no,/alse, yes) + p (no, false, no) = 0.3 p (yes, true, yes) + p (no, true, yes) = 0.25 p (yu, true, no) + p (no, true, no) = 0.35 p (yes, false, yes) + p (no, false, yes) = 0.25 p (yes, false, no) + p (no, false, no) = 0.15 3 1 ( 4-2) X (p (yes, true, yes) +p (yes, false, yes)) 7 + (B-O)x(p(yes,true,no)+p(yes,false,no) ) +\n1 ( --1) X (p (no, true, yes) +p (no, false, yes)} 8\n1 5 + (2-s)x(p(no,true,no)+p(no,false,no)) � 0. Note that even relative to the (projection of) the extension of the entire database there may be more than one &admissible action. If this is so, and the database contains probability intervals, then Loui's methods [1986J may be applied to narrow them. Alternatives applicable to point-valued probabilistic databases are the variable and structural refinements discussed by Poh and Horvit1 [1993J and \"coarsenings\" of the database scheme. The latter, which includes structural refinement as a special case (i.e. may, but needn't, introduce new variables) requires the assessment of joint probabilities over supersets of the sets of variables contained in the original database scheme. If the old database is a projection of the new database, then the new set of E-admissible actions is a subset of the old.\n1 CONCLUSION\nAnytime decision methods may be devised for use with probabilistic databases, Frisch and Haddawy's anytime deduction system, and Nilsson's probabilis tic logic. Common to each of these methods is the generation of a system of linear inequalities the unknowns of which are probabilities of the condi tions for a decision problem. Levi's E-admissibility criterion may be applied to the solution set of the system of inequalities. The size of the system of ine qualities increases, and the set of admissible actions shrinks, as more of the knowledge base or database is taken into account.\nSpecific measures of the quality of a decision are not explored. It seems that, for a fixed set of actions under consideration, reasonable measures will be such that the quality of the decision based on a set E of admissible actions will be higher (ignoring the cost of computation) than that of any decision based on a superset of E. For each of the methods dis cussed, actions are eliminated from consideration as computation proceeds. Thus, the quality of a deci sion (made by choosing an action from the currently admissible set using some criterion other than & admissibility) increases with time.\nDetermining which sentences, or projections of a database, will eliminate the greatest number of actions at the least cost, and whether it is worth the effort to consider additional sentences or projections at all, is a difficult problem which remains for future research.\nAnytime Decision Mak ing with Imprecise Probabilities 477\nB.d'erences\n[Barbara et al., 1993J D. Barbara, H. Garcia-Molina, and D. Porter. The management of proba bilistic data. IEEE Tran4. on Knowledge and Data Engineering, v. 4, pp. 387-402. [Boddy and Dean, 1989] M. Boddy :md T. Dean. Solving time-dependent plannmg problellUI. Proc. IJCAI-89, Morgan Kaufmann, pp. 979-984. [Cavallo and Pittarelli, 1987J R. Cav�o .\nand M. Pit tarelli. The theory of probabilistic databases, Proc. 19th Conf. on Very Large Databases, Morgan Kaufmann, pp. 71-81.\n[Deutsch-McLeish, 1990] M. Deutsch-�cLeish. �n investigation of the general solut10n to ent� ment in probabilistic logic. Int. J. of Intelli gent Systems, 11. 5, pp. 477-486. [Elster, 1989] J. Elster. Solomonie Judgements. Cam bridge University Press. [Frisch and Haddawy, 1992] A. Frisch and P . . ��\ndawy. Anytime deduction for probabilistic logic. Technical Report UIUC-BI-AI-92-01, Beckman Institute, Univ. of Illinois, Urbana. To appear in Artificial Intelligence.\n[Grosof, 1986] B. Grosof. An inequality paradigm for probabilistic knowledge. In 1. Kanal and J. Lemmer, Eds., Uncertainty in Artificial Intel ligence, North-Holland. [Horvitz et al., 1989] E. Horvitz, G. Cooper and D. Beckerman. Reflection and action under scarce resources: theoretical principles and empirical study. Proc. IJCAI-89, Morgan Kaufmann, pp. 1121-1127. [Jaynes, .1982] E. T. Jaynes. The rationale of maximum-entropy methods. Proc. of the IEEE, II. 70, pp. 939-952. [Jeffrey, 1976] R. Jeffrey. Savage's omelet. In F. Suppe and P. Asquith, Eds., PSA 1976, v. £, Philosophy of Science Association. [Kane, 1990] T. Kane. Enhancing th · e · �eren�e\nmechanism of Nilsson's probabilistic logic. Int. J. of Intelligent Systems, v. 5, pp. 487- 504.\n[Kane, 1991] T. Kane. Reasoning with maximum entropy in expert systems. In W. T. Grandy, Jr. and L. Schick, Eds., Maximum Entropy and Bayuian Methods, Kluwer. [Kyburg, 1992] H. E. Kyburg, Jr. Getting fancy with probability. Synthese, 11. 90, pp. 189-203. [Levi, 1980] I. Levi. The Enterprise of Knowledge. MIT Press. [Loui, 1986] R. Loui. Decisions with indeterminate probabilities. Theory and Decision, 11. £1, pp. 283-309.\n[MacQueen and Marscha.k, 1975] J. MacQueen and J. Marschak. Partial knowledge, entropy, and estimation. Proc. Nat. Acad. Sci., v. 1£, pp. 3819-3824. [Nilsson, 1986] N. Nilsson. Probabilistic logic. Artificial Intelligence, v. £8, pp. 71-87. [Piepel, 1983] F. Piepel. Calculating �entroids in constrained mixture expenments. Technometrics, v. £5, pp. 279-283. [Pittarelli, 1991] M. Pittarelli. Decisions with proba bilities over finite product spaces. IEEE Trans. SMO, 11. B1, pp. 1238-1242. [Pittarelli, 1993] M. Pittarelli. Probabilistic data bases and decision problellUI: results and a conjecture. Kybernetika, v. £9, pp. 149-165. [Pittarelli, 1994] M. Pittarelli. An algebra for proba bilistic databases. IEEE Trans. on Knowledge and Data Engineering, v. 6, pp. 293-303. [Poh and Horvitz, 1993] K. 1. Poh and �: Horvitz. Reasoning about the value of deciSion-model refinement: methods and application. Proc. of the 9th Conf. on Uncertainty in Artificial Intelligence, pp. 174-182. [Quinlan, 1983] R. Quinlan. Inferno: a cautious approach to uncertain inference. The Com puter Journal, v. 26, pp. 255-269. [Snow, 1991J P. Snow. Compressed constraints in probabilistic logic and their revision. Proc. 7th Conf. on Uncertainty in Artificial InteUi gence, Morgan Kaufmann, pp. 38�391. [Starr, 1966] M. Starr. A discussion of some norma tive criteria for decision-making under uncer tainty. Industrial Management Review, v. 8, pp. 71-78."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "This paper examines methods of decision making that are able to accommodate limitations on both the form in which uncertainty pertaining to a deci­ sion problem can be realistically represented and the amount of computing time available before a deci­ sion must be made. The methods are anytime algo­ \"_th� in the sense of Boddy and Dean [1989] . Tech­ Diques are presented for use with Frisch and Haddawy's [ 1992] anytime deduction system, with an anytime adaptation of Nilsson's [1986j probabilis­ tic logic, and with a probabilistic database model. 1 ANYTIME ALGORITHMS FOR",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}