{
  "name" : "1206.3284.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bounding Search Space Size via (Hyper)tree Decompositions",
    "authors" : [ "Lars Otten" ],
    "emails" : [ "lotten@ics.uci.edu", "dechter@ics.uci.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper develops a measure for bounding the performance of AND/OR search algorithms for solving a variety of queries over graphical models. We show how drawing a connection to the recent notion of hypertree decompositions allows to exploit determinism in the problem specification and produce tighter bounds. We demonstrate on a variety of practical problem instances that we are often able to improve upon existing bounds by several orders of magnitude."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "This paper develops a measure for bounding the performance of search algorithms for solving a variety of queries over graphical models. It has been known for a while that the complexity of inference algorithms (e.g., join-tree clustering, variable elimination) is exponentially bounded by the tree width of the graphical model’s underlying graph. The base of the exponent is often taken to be the maximum domain size.\nMore accurate bounds were derived by looking at the respective domain sizes and their product in each cluster in of tree decomposition of the underlying graph [Kjærulff, 1990]. These tighter bounds were used in selecting good variable orderings, for example. It was recently shown that these bounds are also applicable to search algorithms that explore the context-minimal AND/OR search graph [Dechter and Mateescu, 2007].\nThe shortcoming of these bounds is that they are completely blind to context-sensitivity hidden in the functions of the graphical model and especially determinism. When a problem possesses high levels of determinism, its tree width bound can be large while its search space can be extremely pruned, due to propagation of inconsistencies across functions.\nPart of this shortcoming in worst-case complexity bounds\nis addressed by the more recent concept of hypertree decompositions [Gottlob et al., 2000]. It was shown that the maximum number of functions in the clusters of a hypertree decomposition (the hypertree width) exponentially bounds the problem complexity for constraint inference, a result that was extended to general graphical model inference in [Kask et al., 2005]. The base of the exponent in this case is the relation tightness, thus allowing the notion of determinism to play a role. However, in practice this bound often turns out to be far worse than the tree width bound, unless the problem exhibits substantial determinism [Dechter et al., 2008].\nThe contribution of this paper is in combining both ideas to tighten the existing bounds, using the relationship between AND/OR graph search and tree decompositions. Starting with the tree width bound, we show that one can also incorporate the concept of hypertree decompositions by greedily covering variables with tight functions. This yields better bounds on the number of nodes in the search graph, which translates directly to search complexity.\nTighter bounds are desirable for a number of reasons:\n1. We can better predict parameters of the algorithm ahead of time (primarily the variable ordering for search), fitting the algorithm to the problem.\n2. It enables us to dynamically update parameters during search, e.g., for dynamic variable orderings.\n3. In a distributed setup, search can often be implemented as centralized conditioning followed by independent solving of the conditioned subproblems on different machines. Better bounds can help in balancing these two phases by varying the size of the central conditioning set [Silberstein et al., 2006].\nWe provide extensive empirical results on 112 probabilistic problem instances and 30 weighted constraint satisfaction problems. We show that exploiting determinism has a significant effect for a number of problem classes. We furthermore compare our bound to the exact size of the search space on a subset of feasible instances, and show that it can be very tight in some cases.\nAn approach that is related but orthogonal to the work here is described in [Zabiyaka and Darwiche, 2006], where the standard complexity measure of tree width is refined by taking into account functional dependencies – i.e., knowing one set of variables determines the values of another set. [Fishelson et al., 2005] develops a bound specifically for an interleaved variable elimination and conditioning algorithm on linkage analysis problems.\nSection 2 provides the background and definitions. Section 3 discusses hypertree decompositions and related complexity bounds. In Section 4 we introduce our new bounding scheme, for which Section 5 provides empirical evaluation. Section 6 concludes."
    }, {
      "heading" : "2 PRELIMINARIES AND DEFINITIONS",
      "text" : "In the following we will assume a graphical model given as a set of variables X = {x1, . . . , xn}, their finite domains D = {D1, . . . ,Dn}, a set of functions F = {f1, . . . , fm}, each of which is defined over a subset of X , and a combination operator (typically sum, product or join) over all functions. Together with an marginalization operator such as minX and maxX we obtain a reasoning problem.\nThe special cases of reasoning tasks which we have in mind are belief networks, (weighted) constraint networks or mixed networks that combine both. The primary tasks over belief networks are belief updating and finding the most probable explanation. They are often specified using conditional probability functions defined on each variable and its parents in a given directed acyclic graph (see Figure 1(a)), and use multiplication and summation or maximization as the combination and marginalization operators [Kask et al., 2005]. For constraint networks we are mainly concerned with problems of finding or enumerating solutions; they are defined using relations as functions, and relational join and projection as the combination and marginalization operators, respectively. For weighted constraint networks one typically has real-valued functions and summation and minimization as combination and marginalization operators, respectively."
    }, {
      "heading" : "2.1 EXPRESSING STRUCTURE",
      "text" : "If one wants to analyze the complexity of a given problem instance, it has proven useful to look at the underlying structure of interactions between variables:\nDEFINITION 2.1 The hypergraph of a graphical model is a pair H = (V, S), where the vertices are the problem variables (V = X) and where S = {S1, ..., Sr} is a set of subsets of V , called hyperedges, which represent the scopes of the functions in the problem (Si = scope(fi)). The primal graph of a hypergraph H = (V, S) is an undirected graph G = (V,E) such that there is an edge (u, v) ∈ E\nfor any two vertices u, v ∈ V that appear in the same hyperedge (namely, there exists Si, s.t., u, v ∈ Si). The dual graph of a hypergraph H = (V, S) is an undirected graph G = (S,E) that has a vertex for each hyperedge, and there is an edge (Si, Sj) ∈ E when the corresponding hyperedges share a vertex (Si ∩ Sj 6= ∅).\nDEFINITION 2.2 A hypergraph is a hypertree, also called acyclic hypergraph, if and only if its dual graph has an edge subgraph that is a tree, such that all the nodes in the dual graph that contain a common variable form a connected subgraph.\nIt is well-known that problems whose underlying graph have tree structure can be solved efficiently [Pearl, 1988]. If this is not the case, we aim to transform the problem into an equivalent one that exhibits tree structure [Lauritzen and Spiegelhalter, 1988, Dechter and Pearl, 1989, Kask et al., 2005]. Intuitively, we do this by grouping variables and the functions over them into clusters that can be arranged as a tree:\nDEFINITION 2.3 Let X,D,F be the variables, domains and functions of a reasoning problem P . A tree decomposition of P is a triple 〈T, χ, ψ〉, where T = (V,E) is a tree and χ and ψ are labeling functions that associate with each vertex v ∈ V two sets, χ(v) ⊆ X and ψ(v) ⊆ F , that satisfy the following conditions:\n1. For each fi ∈ F , there is at least one vertex v ∈ V such that fi ∈ ψ(v) .\n2. If fi ∈ ψ(v), then scope(fi) ⊆ χ(v) .\n3. For each variable Xi ∈X , the set {v∈V |Xi ∈χ(v)} induces a connected subtree of T . This is also called the running intersection property.\nThe tree width of a tree decomposition 〈T, χ, ψ〉 is w = maxv|χ(v)| − 1 . The tree width w∗ of P is the minimum tree width over all its tree decompositions.\nThe problem of finding the tree decomposition of minimal tree width is known to be NP-complete. To obtain tree decompositions in practice, one can apply a triangulation algorithm to the problem’s primal graph along an ordering\nand then construct the bucket tree by extracting and connecting the cliques for each variable, as described for instance in [Pearl, 1988]. The ordering to use as the basis for the triangulation algorithm is often computed heuristically.\nExample 2.1 Assume the belief network in Figure 1(a) over variables X = {A,B,C,D,E, F} is given. We pick the ordering d = A,B,C,D,E, F and triangulate the primal graph as shown in Figure 1(b). If we extract each variable’s bucket – the variable and its earlier neighbors – we obtain the tree decomposition shown in Figure 1(c), the bucket tree decomposition."
    }, {
      "heading" : "2.2 SOLVING REASONING PROBLEMS",
      "text" : "Two principal methods exist to solve reasoning problems, search (e.g., depth-first branch-and-bound, best-first search) and inference (e.g., variable elimination, join-tree clustering). Both can be shown to be time and space exponential in the problem instance’s tree width [Lauritzen and Spiegelhalter, 1988, Dechter and Pearl, 1989, Kask et al., 2005, Dechter and Mateescu, 2007], with a dominant factor of kw, where k denotes the maximum domain of the problem variables."
    }, {
      "heading" : "2.2.1 Search",
      "text" : "Search-based algorithms traverse the problem search space. Given a variable ordering d, the simplest way to perform search is to instantiate variables one at a time. This will define a search tree, where each node represents a state in the space of partial assignments. Leaf nodes signify either full solutions or dead ends. Standard depth-first algorithms typically have time complexity exponential in the number of variables and require linear space. If memory is available, one can apply caching to traversed nodes and retrieve their values when “similar” nodes are encountered.\nThese elementary search spaces, however, don’t fully capture the structure of the underlying graphical model. Introducing AND nodes into the search space can exploit independence of subproblems by effectively conditioning on values, thus avoiding redundant computation [Dechter and Mateescu, 2007]. Since the size of the AND/OR search tree may be exponentially smaller than the traditional OR search one, any algorithm exploring the AND/OR space enjoys a better computational bound.\nExample 2.2 Figure 2 depicts the AND/OR search tree for the problem introduced in Example 2.1 if we assume binary variable domains and the ordering d = A,B,C,D,E, F . The AND nodes for variable B each have two OR children, expressing that at this point the problem decomposes into independent subproblems, rooted at C and E, respectively.\nWe can equally apply caching techniques to an algorithm exploring the AND/OR search tree. As a result this algo-\nrithm will effectively explore the AND/OR search graph. With caching, identical subproblems are recognized based on their context, which is a graphical model parameter that denotes the part of the search tree above that is relevant to the subproblem below.\nCaching will avoid redundant computations, thus reducing time complexity, at the cost of increased memory requirements. By varying the maximum size of contexts to cache on, this tradeoff can be fine-tuned. Assuming full caching, search has been shown to exhibit both time and space complexity exponential in the problem’s tree width.\nExample 2.3 If we extend AND/OR search with caching, given the problem in Example 2.1 it will explore the AND/OR search graph shown in Figure 3. Note how the child nodes of variables C and E are merged. This expresses the fact that in the example the subproblems rooted at D and F , as children of C and E, respectively, are independent of the value of A further up."
    }, {
      "heading" : "2.3 EXPLOITING DETERMINISM",
      "text" : "In practice, however, problem instances across many domains will exhibit a significant degree of determinism (e.g., disallowed tuples in constraint problems, zero probability entries in belief networks). Search algorithms detect the resulting inconsistencies early in the search process and prune the respective portion of the search space. This can lead to significant savings in running time, but is not reflected in the standard worst-case bounds described above.\nTo exploit determinism in the context of variable elimination, the concept of (generalized) hypertree decompositions has been introduced for constraint networks in\n[Gottlob et al., 2000]. As a subclass of tree decompositions, it was shown that it provides a stronger indicator of tractability than the tree width.\nDEFINITION 2.4 Let T = 〈T, χ, ψ〉, where T = (V,E) be a tree decomposition of a reasoning problem P over a graphical model with variables X , their domains D and functions F . T is a hypertree decomposition of P if the following additional condition is satisfied:\n4. For each v ∈ V , χ(v) ⊆ ⋃\nfj∈ψ(v) scope(fj) .\nThe hypertree width of a hypertree decomposition is hw = maxv |ψ(v)|. The hypertree width hw∗ of P is the minimum hypertree width over all its hypertree decompositions.\nTo analyze the complexity of algorithms operating on hypertree decompositions, we introduce the notion of tightness of a function or relation:\nDEFINITION 2.5 The tightness t of a function f is the number of relevant tuples (e.g., allowed tuples in constraints, nonzero entries in conditional probability tables).\nThe motivation behind this is to store and process the function in a “compressed” form, with only the t relevant tuples. Given a hypertree decomposition, one can then modify an inference algorithm to make use of these compact representations when computing the messages to be passed.\nIn [Gottlob et al., 2000] the complexity of processing a hypertree decomposition for solving a constraint satisfaction problem is shown to be exponential in hw∗, with a dominant factor of thw ∗\n. This result was extended in [Kask et al., 2005] to any graphical model that is absorbing relative to 0. (A graphical model is absorbing relative to a 0 element if its combination operator has the property that x ⊗\n0 = 0 ∀x; for example, multiplication has this property while summation does not.)"
    }, {
      "heading" : "3 HYPERTREE WIDTH BOUNDS FOR INFERENCE",
      "text" : "In this section we briefly explore whether the bounds based on hypertree width can provide a practical improvement over the established tree width bounds described above. To that end we recently looked at a selection of over 140 problem instances from various domains [Dechter et al., 2008].\nWe used the code developed for [Dermaku et al., 2005], which is available online. It generates a tree decomposition along a minfill ordering and extends it to a (generalized) hypertree decomposition by applying a greedy heuristic.\nWe used the lowest tree width w and hypertree width hw out of 20 runs as a basis for our investigation: For every problem instance, we compute the dominant factors of the\ntwo worst-case complexity bounds, i.e., kw and thw, where k denotes the maximum domain size and t the maximum function tightness in the problem instance. In order for the hypertree decomposition to provide a better bound than the tree decomposition, thw should be significantly smaller than kw.\nIntuitively, it is clear that t ∈ O(kr), where r is the maximum function arity of the problem. Hence we should expect that only when the function table contains many irrelevant values (e.g., zeros in probability tables), the hypertree width bound can be superior.\nAnd indeed, out of all the instances we evaluated, only for five of them was thw less than kw, whereas it was orders of magnitude worse on almost all of the remaining problems. Looking at the instances in more detail, it becomes evident that in most of them the functions are not sufficiently tight and often have highly intersecting scopes, which renders the hypertree width bound ineffective."
    }, {
      "heading" : "4 SEARCH SPACE ESTIMATION",
      "text" : "Even though the results in [Dechter et al., 2008] suggest that complexity bounds based on hypertree width are often not competitive in practice, the idea of exploiting determinism remains promising. Furthermore, while all considerations in section 3 were targeted at inference algorithms, we are in particular interested in estimating how determinism in a problem will impact the size of the search space discussed in Section 2.2.1, since search is a widespread method in practice. To that end, we will aim to upper bound the size of the AND/OR context minimal search graph, which is explored by AND/OR search augmented with caching, as described in Section 2."
    }, {
      "heading" : "4.1 TREE DECOMPOSITION CORRESPONDENCE",
      "text" : "Assume that a variable ordering d = x1, . . . , xn is fixed and that search instantiates variables first to last (while inference would proceed last to first). We note that the way the search space is decomposed by AND/OR search with caching can be represented by the bucket tree decomposition along the same ordering. For details we refer to [Mateescu and Dechter, 2005], to illustrate we revisit our previous example:\nExample 4.1 Consider the bucket tree decomposition in Figure 1(c) and the AND/OR search graph in Figure 3. It is easy to see that the decomposition clusters can be related to the “layers” of the search graph, i.e., the nodes associated with a variable and its values, as shown in Figure 4. For instance, the cluster {B,C,D} represents the search layer for variable D and the fact that the subproblem only depends on B and C – and not A.\nBased on this observation, we can also partition the search space into “clusters”, according to the corresponding bucket tree decomposition. Our approach of upper bounding the size of the entire search space will then be to bound the portion of the search space in each cluster, subsequently summing over the clusters (for simplicity we consider only the AND nodes of the search space, since OR nodes are actually not implemented as such in practice). We note that a cluster in a bucket tree decomposition will always correspond to exactly one variable’s layer in the AND/OR search graph (due to the way it is constructed)."
    }, {
      "heading" : "4.2 BOUNDING CLUSTER SIZE",
      "text" : "A straightforward upper bound is obtained by multiplying the domain sizes of all variables in the cluster [Kjærulff, 1990]. If the set of clusters is given by C = {C1, . . . , Cn}, Ck⊆X , summing over clusters gives\ntwb := n ∑\nk=1\n∏\nxi∈Ck\n|Di| .\nNote that this is very closely related to the worst case complexity, since the tree width is just the maximum number of variables in any cluster of the decomposition. This, however, does not take determinism into account.\nIf the bucket tree decomposition we are working with is also a hypertree decomposition (meaning all variables in a cluster are covered by the functions in that cluster), we can take the product ∏\ni ti as an upper bound to the number of nodes in that cluster, where ti is the tightness of the i-th function in it. (This is closely related to the complexity bound on hypertree decompositions as outlined above.)\nTaking this one step further, if the bucket tree decomposition at hand does not satisfy the additional hypertree decomposition condition, we can compute the product over the tightness of each function in the cluster and multiply this by the domain sizes of the uncovered variables, to account for the lack of information about these variables.\nHowever, since the scopes of the different functions in a cluster can overlap, this tightness based bound will typically be worse than the simple “product of domain sizes” (as we already observed). Consequentially, we use the latter as a starting point and exploit tight functions to improve\nAlgorithm GreedyCovering Input: Cluster Ck containing variables Xk and functions Fk, with xi ∈Xk having domain size di and fj ∈Fk having tightness tj Output: A subset of Fk (forming a partial covering of Xk) Init: Uncov = Xk, Covering = ∅ (1) Find j∗ that minimizes rj = tj/ ∏\nxk∈Ij dk, where\nIj = Uncov ∩ scope(fj). (2) If rj∗≥ 1 , terminate with current covering. (3) Add fj∗ to Covering and set Uncov := Uncov\\scope(fj∗). (4) If Uncov = ∅ , terminate with current covering. (5) Goto (1).\nFigure 5: Greedy covering algorithm for a single cluster.\nupon it, therefore combining the concept of tree and hypertree decompositions.\nIn essence this can be seen as a weighted variant of the well-known SET COVER problem, where one aims to cover a set or vertices by as few as possible subsets from a set of given subsets of the variables. The problem is generally NP-complete, but simple greedy approximations exist [Johnson, 1973], which give rise to our method:\nStart with an empty covering (if we assume dummy unary functions over uncovered variables, this is equivalent to the bound twb). Then, for each function fj in the cluster, compute the coverage ratio rj as follows: Divide the function’s tightness tj by the product over the domain sizes of the variables that have not yet been covered and are in the scope of fj . Pick the function for which the coverage ratio is the lowest and add it to the covering. Repeat this for as long as we can still find a function with a coverage ratio less than 1. The algorithm is given in Figure 5.\nIt will produce a set of functions as the covering, but might leave some variables uncovered. As before, we can multiply the tightness of the functions in the covering and the domain sizes of the uncovered variables to obtain an upper bound on the number of nodes in the cluster.\nIt is worth noting that we are not limited to functions from the decomposition cluster in question, but we can include any function from the clusters higher up in the rooted tree decomposition (since their scope will have been fully instantiated at this point of the search).\nProposition 1 Executing algorithm GreedyCovering for each cluster of the bucket tree decomposition and summing up, we obtain an upper bound on the number of nodes in the AND/OR search graph, which we denote:\nhwb :=\nn ∑\nk=1\n\n\n∏\nfj∈Cov(Ck)\ntj · ∏\nxi∈Ck\\Cov(Ck)\n|Di|\n\n ,\nwhere Cov(Ck) is the set of functions returned by the algorithm GreedyCovering for cluster Ck.\nTHEOREM 4.2 Given a reasoning problem with n variables and m relations with maximal tightness t. If the bucket tree decomposition along a given variable ordering d has tree width w, the complexity of computing the bound hwb is O(n · w ·m) time-wise and O(m) space-wise.\nProof. The time complexity of algorithm GreedyCovering is linear both on the number of variables in the cluster as well as in the number of functions considered, i.e., worstcase O(w ·m). Keeping track of the coverage ratio of each function requires O(m) space. Iterating and summing over all n clusters results in the stated asymptotic bounds. 2\nExample 4.3 Assume we have a cluster containing 3 variables X , Y , Z with domain sizes dX = 4, dY = 4, and dZ = 3, as well as 2 functions f1(X,Y ) and f2(Y,Z) with tightness t1 = 9 and t2 = 11. The twb bound on the number of nodes in this cluster is dX ·dY ·dZ = 48. In the first iteration of the greedy covering algorithm for hwb the gain ratios of f1 and f2 will be computed as 916 and 11 12 , respectively. Therefore f1 will be added to the covering, leaving only Z uncovered. The next gain ratio of f2 will be computed as 113 , which is greater than 1. Therefore the algorithm terminates and f2 will not be part of the covering. The hwb bound for this cluster will then be t1 · dZ = 9 · 3 = 27."
    }, {
      "heading" : "5 EXPERIMENTAL RESULTS",
      "text" : "For empirical evaluation we return to the problem instances that were described in [Dechter et al., 2008] (see also Section 3). These comprise 112 belief networks from areas such as coding networks, dynamic Bayesian networks, genetic linkage instances and CPCS medical diagnosis networks. We also evaluated 30 weighted constraint network instances. All problem instances are available online1.\nFor every instance, we report the number of variables n, the maximum variable domain size k, the maximum function arity r, and the average tightness ratio tr (defined as the average percentage of relevant tuples in a full function table).\nOn each problem instance we run our bounding method along 100 different minfill orderings (with random tie breaking) and record the lowest bound, with w as the tree width of the bucket tree decomposition. For every instance we then compute the asymptotic worst-case bound for the search space size, which is n · kw+1 (cf. [Dechter and Mateescu, 2007], adapted for AND nodes).\nConsulting the bucket tree decomposition for a more finegrained analysis (still without considering determinism) gives the bound twb. We then apply our covering heuristic to exploit determinism and obtain the bound hwb. (Note\n1http://graphmod.ics.uci.edu/\nthat these three bounds can produce very large integers, therefore we report them as their log10 in Table 1.) Even for the larger problem instances this bound computation takes only a few hundred milliseconds on a 2.66 GHz CPU."
    }, {
      "heading" : "5.1 BOUND IMPROVEMENTS",
      "text" : "If we compare the values for n · kw+1 and twb, we can see that the bound improves for every problem instance by at least one order of magnitude, but often more than that (recall that the table shows log10 of the bounds). For most pedigree genetic linkage instances, for example, the reduction is ten orders of magnitude or more. Similar results hold for the dynamic Bayesian networks we tested on, with many orders of magnitude improvement.\nIt seems that problems with a higher number of variables benefit the most from the fine-grained analysis. This makes sense if one considers the fact that the worst-case bound will greatly overestimate the size of almost all clusters, since in practice the tree decomposition contains only very few clusters of full tree width.\nIf we try to exploit determinism by going from the twb to hwb bound, there is, just looking at the problem parameters, no obvious indicator for when the bound will improve: On pedigree instances, for example, the decrease is not very significant, although these problems exihibit some determinism. On digital circuits, on the other hand, with an average of 50% determinism, the bound improves another 3 to 4 orders of magnitude over twb.\nOn almost all weighted CSP instances we were able to lower the bound by exploiting determinism, often by orders of magnitude. For example, on the satellite scheduling problem 408b the twb bound of 83, 206, 198, 094 decreases to a hwb value of 248, 197. A significantly tighter bound is also achieved on randomly generated k-trees where the probability tables were forced to exihibit determinism, with for example twb = 29, 983, 742 decreasing to hwb = 1, 528 on problem BN 107.\nThe crucial point here seems to be at which point during the search functions with high determinism will have their scope fully instantiated, i.e., at which point they become available to our covering heuristic. This is not predictable by only looking at the instance parameters but will require a more detailled look at the guiding bucket tree decomposition instead."
    }, {
      "heading" : "5.2 BOUND EVALUATION",
      "text" : "Most problem instances are too big to compute the exact size of the context minimal AND/OR search space, which would be equivalent to solving the problem (for solution counting or computing P (e)). But for some of the smaller instances this is actually feasible, which gives us the option of testing how tight our bound is.\nTable 2 shows the upper bounds nkw+1, twb, and hwb (this time not in their log10), as well as the exact number of AND nodes in the actual context-minimal search graph. The values in each row were obtained on the same minfill ordering (not neccessarily the one used for Table 1).\nFor smaller instances the bound we compute turns out to be rather tight (note that CPCS instances exhibit no determinism at all and thus twb and hwb match the size of the search space exactly). As the problems become bigger and their structure more complicated, however, the bound quality deteriorates. It should be interesting to perform this comparison on bigger problem instances, but as of now this is limited by the resources available in current computers."
    }, {
      "heading" : "6 CONCLUSIONS & FUTURE WORK",
      "text" : "While asymptotic bounds for search algorithms can give a rough idea about problem hardness, it is often desirable to obtain a tighter, more fine-grained bound. As has previously been shown, this can be accomplished by looking at a suitable tree decomposition of the problem’s underlying graph structure and the domains of variables in the decomposition clusters. This, however, is blind to determinism, which can greatly prune the search space in practice.\nThe contribution of this paper is to introduce ideas from the framework of hypertree decompositions into the bounding of the search space. This allows us to exploit determinism in the function specification, but only if it is beneficial to the overall complexity bound.\nWe demonstrated on a set of 112 belief networks and 30 weighted constraint networks that the proposed scheme is indeed able to further improve the bound on search complexity, in some cases by several orders of magnitude. On a subset of the instances we also showed that the bound can indeed be very tight, although it seems to deteriorate for bigger instances. In this respect we hope to be able to conduct more in-depth comparisons on even bigger problem instances in the future. Note that the ability to bound,\nsometimes accurately, the size of the search space in linear time is very important, especially for problem instances which are completely unsolvable exactly.\nWe believe that the current version of our bounding scheme can be further improved by incorporating some form of propagation of information down the bucket tree. Another path we plan to pursue is using approximate counting methods, such as sampling, to compute approximations to the number of solutions in each cluster, which will also approximate the number of nodes. Finally, for optimization tasks and for approximating branch-and-bound and bestfirst search algorithms we hope to accomplish further tightening of the search space using the cost function itself.\nOn a higher level, we plan to use the bounds we obtain to guide the selection of static and dynamic variable orderings. We also intend to deploy our scheme for parallelizing search algorithms over a networks of many machines (e.g., grids and clusters)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by NSF grant IIS0713118 and NIH grant R01-HG004175-02."
    } ],
    "references" : [ {
      "title" : "Pearl: Tree Clustering for Constraint Networks",
      "author" : [ "Dechter", "Pearl", "J. 1989] R. Dechter" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Dechter et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Dechter et al\\.",
      "year" : 1989
    }, {
      "title" : "AND/OR search spaces for graphical models",
      "author" : [ "Dechter", "Mateescu", "2007] R. Dechter", "R. Mateescu" ],
      "venue" : "In Artificial Intelligence",
      "citeRegEx" : "Dechter et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dechter et al\\.",
      "year" : 2007
    }, {
      "title" : "On the Practical Significance of Hypertree vs. Tree Width",
      "author" : [ "Dechter et al", "2008] R. Dechter", "L. Otten", "R. Marinescu" ],
      "venue" : "In Proceedings of ECAI’08",
      "citeRegEx" : "al. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2008
    }, {
      "title" : "Heuristic Methods for Hypertree Decompositions",
      "author" : [ "Dermaku et al", "2005] A. Dermaku", "T. Ganzow", "G. Gottlob", "B. McMahan", "N. Musliu", "M. Samer" ],
      "venue" : "Technical Report DBAI-TR-2005-53, Vienna University of Technology,",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    }, {
      "title" : "Maximum Likelihood Haplotyping for General Pedigrees",
      "author" : [ "Fishelson et al", "2005] M. Fishelson", "N. Dovgolevsky N", "D. Geiger" ],
      "venue" : "Human Heredity",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    }, {
      "title" : "A comparison of structural CSP decomposition methods",
      "author" : [ "Gottlob et al", "2000] G. Gottlob", "N. Leone", "F. Scarcello" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "al. et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2000
    }, {
      "title" : "Unifying tree decompositions for reasoning in graphical models",
      "author" : [ "Kask et al", "2005] K. Kask", "R. Dechter", "J. Larrosa", "A. Dechter" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    }, {
      "title" : "Local Computations with Probabilities on Graphical Structures and Their Application to Expert Systems",
      "author" : [ "Lauritzen", "Spiegelhalter", "1988] S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Journal of the Royal Statistical Society. Series B",
      "citeRegEx" : "Lauritzen et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Lauritzen et al\\.",
      "year" : 1988
    }, {
      "title" : "The Relationship Between AND/OR Search Spaces and Variable Elimination",
      "author" : [ "Mateescu", "Dechter", "2005] R. Mateescu", "R. Dechter" ],
      "venue" : "In Proceedings of",
      "citeRegEx" : "Mateescu et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Mateescu et al\\.",
      "year" : 2005
    }, {
      "title" : "Online system for faster linkage analysis via parallel execution on thousands of personal computers",
      "author" : [ "Silberstein et al", "2006] M. Silberstein", "A. Tzemach", "N. Dovgolevskiy", "M. Fishelson", "A. Schuster", "D. Geiger" ],
      "venue" : "American Journal of Human Genetics,",
      "citeRegEx" : "al. et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2006
    }, {
      "title" : "Bounding Complexity in the Presence of Functional Dependencies",
      "author" : [ "Zabiyaka", "Darwiche", "2006] Y. Zabiyaka", "A. Darwiche" ],
      "venue" : "In Proceedings of",
      "citeRegEx" : "Zabiyaka et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zabiyaka et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ ],
    "year" : 2008,
    "abstractText" : "This paper develops a measure for bounding the performance of AND/OR search algorithms for solving a variety of queries over graphical models. We show how drawing a connection to the recent notion of hypertree decompositions allows to exploit determinism in the problem specification and produce tighter bounds. We demonstrate on a variety of practical problem instances that we are often able to improve upon existing bounds by several orders of magnitude.",
    "creator" : null
  }
}