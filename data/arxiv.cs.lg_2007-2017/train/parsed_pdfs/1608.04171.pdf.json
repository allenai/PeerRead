{
  "name" : "1608.04171.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Power Series Classification: A Hybrid of LSTM and a Novel Advancing Dynamic Time Warping",
    "authors" : [ "Yuanlong Li", "Yonggang Wen" ],
    "emails" : [ "ygwen}@ntu.edu.sg.", "zhang@ieee.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nNOWADAYS, a growing number of data centers have beenbuilt to support complicated computation and massive storage required by various blooming applications [1]. Each data center is typically equipped with hundreds of thousands servers and requires many mega-watts electricity to power its hosted servers and the auxiliary facilities [2]. An essential problem is to monitor such a large amount of servers for energy saving and maintaining the business continuity.\nMonitoring technologies [3] can be divided into two categories: intrusive and non-intrusive. Intrusive technologies require the install of certain monitoring software which requires the administration role of the system. Compared to the intrusive methods, non-intrusive methods are more flexible, which only require limited data for the monitoring and analysis.\nIn this paper, for the purpose of energy consumption monitoring, we propose to label the running program in a server by analyzing the observed power consumption series. The power data can be measured without the administration right\nYuanlong Li, Han Hu and Yonggang Wen are with School of Computer Engineering, Nanyang Technological University, Nanyang Avenue, Singapore 639798. Email: {liyuanl, hhu, ygwen}@ntu.edu.sg.\nJun Zhang is with School of Computer Science and Engineering, South China University of Technology, Guangzhou, China. Email: junzhang@ieee.org.\nof the server, which can be useful in collecting the power related information of the servers for the purpose of energy consumption analysis. The proposed classification analysis can only gain the type of the running program, avoiding any possibility in accessing the privacy-related contents in the server.\nThe propose power series labeling problem falls into the field of time series classification. As a time series classification problem, the power series classification problem can be challenging as the power series collected in detection may be only a small piece of the whole power series of a program, with incomplete and limited information. For this problem, the key is to design an accurate classification algorithm.\nCurrently there are a few works studies on classify signals like in [4] [5] [6]. However, the technologies applied in these literature are based on common spectral or statistical features with classifiers such as nearest neighbor or neural network. For the general time series classification problem, in [7], dynamic time warping (DTW) distance metric based method 1NNDTW is proven to be the most suitable algorithm. Another line of research has also become notable recently, i.e. the long short term neural network (LSTM), which shows great modeling ability for sequential data. In this work, we propose and utilize a novel classifier with much higher accuracy based on the great efforts in the current literature.\nIn this research, for the first time as far as we know, we propose a hybrid algorithm of the 1NN-ADTW and LSTM. Before designing the hybird algorithm, we first define an advancing dynamic time warping (ADTW) distance measurement, which can perform better than the DTW distance when used to classify the power series. Second we apply the stateof-art sequential data modeling neural network long time short time memory (LSTM) [8] [9] to classify the power series. Our study show that 1NN-ADTW and LSTM both can outperform the 1NN-DTW with similar accuracy; however, these two algorithms have their unique different natures and the accurately classified samples of these two algorithms have significant differences. In this sense, we propose a hybrid algorithm of the two classifiers termed as LSTM/ADTW, which further improves the accuracy.\nThe main contributions of this paper are summarized as follows: • We propose a new distance measurement ADTW. The\nADTW is non-commutative and is flexible for the nearest neighbor classifier for the time series classification problem. • For the first time, we develop a hybrid algorithm of\nar X\niv :1\n60 8.\n04 17\n1v 1\n[ cs\n.N E\n] 1\n5 A\nug 2\n01 6\n2\n1NN-ADTW and LSTM termed as LSTM/ADTW. The hybrid algorithm is based on a well trained LSTM neural network. For each test sample, we analyze the classification results of the well trained LSTM on the original test sample and its nearest neighbor with the ADTW distance to identify the weak point of LSTM to get a better accuracy. • Numerical experiments show that with the ADTW distance measurement, the accuracy of the 1NN-ADTW classifier can be improved from about 84% to about 89% compared to the 1NN-DTW. With the hybrid algorithm LSTM/ADTW, the accuracy can be upto about 92%.\nThe remainder of this paper is organized as follows. In Section II, we briefly introduce the state-of-art time series classification algorithms. In Section III, we introduce the experimental data collection design and some preliminary analysis on the data. In Section IV, we introduce the new proposed algorithm and in Section V we show the numerical evaluation results and the analysis. In Section VI we conclude the whole paper and introduce the future works."
    }, {
      "heading" : "II. RELATED WORKS",
      "text" : "The power series labeling problem studied in this paper can be taken as a time series classification problem, which has been studied extensively for the past decades. For this problem, on one hand, common classifiers like support vector machine (SVM), k-nearest neighbor (KNN) have been proved to be noncompetitive to the DTW distance metric based method like 1NN-DTW [7]. On the other hand, recently with the fast development of deep learning [10], LSTM neural network has also been proved to hold high modeling ability for sequential data. In the following we will mainly introduce these two related classification models.\nA. 1NN-DTW\nThe 1NN-DTW is a special k-nearest neighbor classifier with k = 1 and a special DTW distance metric. For the 1- NN classifier, the common standard procedure to label of a test sample given a set of training samples is as follows. First the distances of the test sample to all the training samples are computed; then the training sample that has the smallest distance to the test sample is chosen and its label is assigned to the test sample as the classification result. In the above procedure, the key is to utilize a proper distance metric. For 1NN-DTW, the DTW distance is used, which has superior performance for time series data.\nThe DTW calculates the distance of two sequences x and y in a manner of finding the best match between them, as shown in Fig. 1. The idea is that sequential data often contain similar fluctuation patterns, however, a same pattern, when existed in different sequences as sub-sequences, may be stretched, shrank or delayed in the time axis. In this case, the DTW distance metric aims to warp the time axis non-linearly and finds the best match between the two samples such that when a same pattern exists in both sequences, the distance is smaller.\nMathematically, the DTW distance is computed by the following dynamic programming process. Denote D(i, j) as the DTW distance between sub-sequences x[1 : j] and y[1 : j], then the DTW distance between x and y can be computed by the dynamic programming process with the following iterative equation:\nD(i, j) = min{D(i−1, j−1), D(i−1, j), D(i, j−1)}+|xi−yj |. (1) The time complexity to compute the DTW distance is O(nm), where n and m are the length of x and y respectively. The DTW distance metric actually re-align the time step index pairs in the computing of the distance. In practice, usually a threshold r is used to restrict the index offset in the alignment, which can be critical to the classification results [11]. Also there are many study [12] working on accelerating the computing speed of DTW, which results in the fast DTW that can be computed in linear time of the length of the sequences. In this paper, we follows the idea of DTW but propose a new distance metric, which can be computed in an advancing manner without a dynamic programming process and has a special non-communicative nature that can be helpful."
    }, {
      "heading" : "B. LSTM",
      "text" : "LSTM is first proposed by Hochreiter and Gers et al. as an upgrade of the recurrent neural network (RNN) [13]. RNN is used to handle sequential data with a special calculation process following the time step increment, while traditional neural network simply treats the sequence as a plain vector. With such nature, RNN is suitable for modeling sequential data. However, it suffers from a problem called diminishing gradient, which is caused by the iterative process on the time axis and makes the gradient used in the training process extremely small and causes training failure. To solve the problem, the LSTM is proposed and it utilizes a memory core to avoid the diminishing gradient. The details of the LSTM neural network will be introduced in Section IV.\nLSTM has shown great modeling power for sequential data and has been successfully applied in various machine learning fields like natural language process (NLP) [14], video analysis [15] and etc. It is also noted that LSTM can be both discriminative and generative. By discriminative, LSTM can be used for classification tasks while by generative, LSTM can be used to generate similar sequences like the training samples [16]. In this paper, we utilize the discriminative ability of LSTM for our power series labeling task.\n3"
    }, {
      "heading" : "III. POWER SERIES DATA COLLECTION AND PRELIMINARY ANALYSIS",
      "text" : "In this section we present the power series data we collected followed by some preliminary analysis on the data. We will detail the simulation design rules for the data collection and the data samples collected with some pretreatment. The proposed preliminary analysis includes data visualization with different dimension reduction methods, classification results with some canonical classifiers, and feature study."
    }, {
      "heading" : "A. Power Series Data Collection",
      "text" : "We first introduce the designing rules of the simulation for data set collection. As a data-driven study on the power series classification methodology, we need to collect a set of sample power data. The data collection should be designed carefully to make sure that the classification problem is neither trivial nor impossible to accomplish. In this sense, our guiding line for power data collection is to collect “different” and “similar” power series: By “different”, the power series must be generated by different programs. By “similar”, the different programs can have some similar features so that the classification algorithms need to be really discriminative.\nFollow the above guideline, we collected in total 13 classes of power data as shown in Table. I (for convenience they are labeled as 0 12 respectively). These data fall into two major categories: • Web server power data usually fluctuate in a continuous\npattern; • Spark/Hadoop MapReduce programs show stage-pattern,\ne.g. the Map stage and the Reduce stage. For the Hadoop/Spark category, we test different programs on these platforms, some are the same for both platforms, such as the “Word Count” program; some only exist on one platform, for example, the “MLlib” programs on the Spark platform. With such design, we can achieve the proposed “different” and “similar” design goal.\nNote that the collected data series are of different lengths as the running duration can vary among different programs. Although classification methods like 1NN-DTW can deal with power series of different lengths, to apply other canonical methods, in the following we cut the collected series into fixed length sequences. It is also reasonable to label sub-sequences instead of the complete power sequences of the programs as in\na blind test, we have no information about the start/end point of a program. The detailed cutting method we utilize here is as shown below.\nThe goal is to cut the power sequences into length 200 samples. To do so, first we discard sequences with length smaller than 200 time slots (time unit: 3 seconds). The left number of sequences for each class is: [77, 31, 30, 35, 28, 7, 40, 14, 5, 100, 58, 36, 40]. Although some power data are discarded, the total duration of the left sequences is about 199 hours and with the time unit being 3 seconds, the amount of the data are still adequate for the study. Then these sequences are further cut into length 200 sub-sequences in the following way: For each sequence q of length n, we cut it into multiple sequences q[0 : 200], q[50 : 250], .... , q[(n − 200) : n]. We obtain 3200 test sequences in this cutting procedure, which are used as the power data in our classification study. Note that these sequences are overlapped, as indicated by the cutting method.\nFurthermore, for the purpose of multi-fold tests, we divide these samples into five folds F0-F4. Note that to avoid the overlapping of the training data and the test data, the fold partition is done before the sequence cutting. For each fold of test, we use Fi,F(i+1)%5 as the test data, and the left folds as the training data."
    }, {
      "heading" : "B. Preliminary Analysis",
      "text" : "We do some preliminary analysis on the pretreated data. The following analysis are meant to provide a basic understanding of the power data in view of classification.\n1) Basic Characteristic Analysis Based on Visualization: We use various dimension reduction methods to visualize the data, which can help to identify if the power series can be successfully classified to a certain degree. We utilize eight different dimension reduction methods with scikit-learn [17] and project the original fixed length power sequences into a 2-dimensional space. These dimension reduction methods are PCA, LDA, LLE, modified LLE, Isomap, MDS, Spectral Embedding and t-SNE, which are widely adopted dimension reduction methods. The 2-dimensional codes of the power data generated by these methods are shown in Fig. 2. We use different colors to show samples from different classes.\nFrom the Fig. 2 we can observe that the power series data are not easy to distinguish after the dimension reduction. This may be due to the short length (2 here) of the embedding code; however, it still shows that the power series classification task cannot be easily done.\n2) Tackling the Classification Problem with the Canonical Classifiers: We test some canonical classifiers to tackle the power series classification problem. The canonical classifiers tested here are listed as follows: Nearest Neighbors, Linear SVM, RBF SVM, Decision Tree, Random Forest, AdaBoost, Naive Bayes, LDA and QDA [17] . Parameter settings for these classifiers are tuned manually. The classification results of these methods are shown in Table II.\nFrom the results we can observe that for a 13-classes classification problem, the highest accuracy achieved by these methods are about 60% (by Random forest). The classification\naccuracy is not promising (when compared to the 1NN-DTW shown below), which actually proves that our power series labeling problem is a typical time series classification problem, as stated in [7], for such problem, canonical Euclidean distance metric based classifiers cannot achieve good results usually.\n3) Feature Based Classification Study: In general, as a signal classification problem, the power series labeling problem can be solved by first extracting certain features from the raw power series and then carry out the classification with these features. In this subsection, we study such possibility and test power series classification with the DFT [18] feature of the original power sequences. With DFT, each power sequence can be transformed into the spectrum space resulting a new representation. The spectrum representation can be aligned as a vector as the input to the classifiers. We test the classification result of 1NN-DTW with the raw data compared to with the DFT feature. The classification results are shown in Table III. Note that for the 1NN-DTW, the maximum offset r is set to 15% of the sample length, which is manually tuned in the experiment.\nFrom Table III we can observe that the DFT features are not helping. The reason is that classification with the original data can maximize the information used in classification, while the DFT feature is less informative.\nTo summary, we find that the power series classification problem is not easy to tackle especially with the canonical classifiers and with some common used features. In the following, we will propose a new distance measurement inspired from DTW and combine it with the state-of-art sequence\nmodeling neural network LSTM."
    }, {
      "heading" : "IV. THE PROPOSED POWER SERIES CLASSIFICATION ALGORITHM",
      "text" : "In this section we present the proposed new power series classification algorithm which hybridizes a nearest neighbor classifier with a novel distance measurement and a LSTM classifier. In the following we first introduce the two components respectively and then present the hybrid algorithm."
    }, {
      "heading" : "A. Nearest Neighbour with the Advancing Dynamic Time Warping (ADTW)",
      "text" : "We propose a new classifier which utilize a novel distance measurement to compute the distance between two sequences which we termed as Advancing Dynamic Time Warping (ADTW) as its computation can be done with the time step advancing from the beginning to the end (for several rounds) which is different with the dynamic programming process in\n5\nDTW. The ADTW is developed to replace the DTW distance measurement in the 1NN-DTW classifier.\nThe ADTW distance measurement is computed in the following way. Suppose we have two sequences x and y, both of length n. We define the order k ADTW distance between x and y as:\nADTWk(x,y) = k∑ i=1 n−i−1∑ j=0 min(|xj − yj |, |xj − yj+1|,\n..., |xj − yj+k|)\n(2)\nThe idea of ADTW is as following. In computing the distance between x and y when we want to find a nearest neighbor of x, we set x as the base sequence and test the similarity of y to x: with a window size i, for time step j in x, we compute the nearest absolute distance between xj and yj ,yj+1,...,yj+i; then we add these distance measures for j = 0, ..., n − i − 1 and for i = 1, ..., k up to be the order k ADTW distance from y to x.\nNote that the ADTWk(x,y) distance is non-commutative, which means that ADTWk(x,y) 6= ADTWk(y,x) can be true. We use ADTWk(x,y) to compute the nearest neighbor of sequence x, in a sense that to find the best match of x among the other samples such as y. For comparison, the DTW distance is obviously commutative. The non-commutative feature of ADTW can be beneficial, as our target is to find the nearest neighbor for each x. A non-commutative distance measurement is enough to serve the purpose and can provide more flexibility by enforcing less constraints to the distance measurement."
    }, {
      "heading" : "B. Long Short Term Memory Neural Network",
      "text" : "We utilize the LSTM classifier following [19] for our power series classification problem. The LSTM neural network consists of an input layer, a LSTM layer and a logistic regression layer as depicted in Fig. 3. The three layers function in the following way respectively: • Input layer: The input data sample, which is a length n\nvector x, is firstly discretized into range [0, S]. Such an operation is a smoothing operation to the original power series, which can affect the performance of the LSTM. Then each time step xt, t = 0, ..., n− 1 is enriched into\na m-dimensional vector xt which can ease the following computation, i.e. xt = xt ·e, where e is a m-dimensional vector with all entries equal to 1. After the above process, the new sequence x0,x1, ...,xn−1 is used as the input to the LSTM layer. • LSTM layer: The LSTM layer contains n LSTM node, where each LSTM node t can output an m dimensional code ht. The operation inside the LSTM node is shown below. First, for each time step xt, the LSTM node needs to compute a new state denoted by Ct. To compute Ct, a candidate state C′t is firstly computed as:\nC′t = tanh(Wcxt +Ucht−1 + bc). (3)\nThen two gates, an input gate it and a forget gate ft are computed to update the new state:\nit = σ(Wixt +Uiht−1 + bi). (4)\nft = σ(Wfxt +Ufht−1 + bf ). (5)\nThen the new state of the LSTM node is computed as:\nCt = it ·Ct + ft ·C′t. (6)\nWith the node state, to further compute the output, an output gate is firstly computed as:\not = σ(Woxt +Uoht−1 + bo). (7)\nFinally the output of a LSTM node t is computed as:\nht = ot · tanh(Ct). (8)\nThe output of all LSTM nodes are then added together as the output of the LSTM layer:\nh = n−1∑ t=0 ht. (9)\n• Logistic regression layer: In this layer the output of the LSTM layer is used to compute the label of the test sample in the following way. First we use the softmax [20] function to compute the probability vector P with its each entry representing the probability of the test sample belonging to a class:\nP = softmax(Wh+ b). (10)\nThen the prediction ypred is the class which achieves the largest probability:\nypred = argmaxi(P). (11)\nTo train the LSTM classifier, the loss function is defined as the negative log-likelihood function with the label of the training data y:\n− L(θ,D) = − |D|−1∑ i=0 log(Py(i) |x(i), θ)), (12)\nwhere θ is the set of all the weight and bias parameters in the LSTM neural network (which are adjusted in the training process); D is a batch of training samples. Size of D can be important for the performance of of the classifier.\n6"
    }, {
      "heading" : "C. Hybridization of LSTM and 1NN-ADTW",
      "text" : "In this subsection we propose to combine the 1NN-ADTW classifier and the LSTM classifier. The underlying rational is that both classifier can achieve high classification accuracy for our problem but in very different manners: the 1NN-ADTW is a nearest neighbor classifier, which is a data-based classifier without a training process; while LSTM is a model based classifier in which the training data are firstly used to build a model and then the model is used to classify the test data. In our experiments, both classifiers can perform promisingly individually; however, our numerical simulation shows that the accurately classified samples by the two classifiers have significant differences. In such sense, we propose to combine the two algorithms to construct a even stronger classifier.\nThe hybrid algorithm is designed in the following way. Considering that in practice, the training of LSTM can be time-consuming, we propose a method that will not need to increase the training time of LSTM. In our method, we run 1NN-ADTW first, in which for each test sample x, we find its nearest neighbor yADTW with the ADTW. Then we train the LSTM and get a well trained LSTM classifier CLSTM . After that, we use different weight parameters to generate the linear combination of the the original test sample and its nearest neighbor to generate multiple auxiliary test samples and predict their labels with the LSTM classifier. By analyzing the prediction results of the LSTM, we can identify when the LSTM is weak and a voting procedure is designed to determine the final prediction result. Details are as shown in Algorithm 1.\nThe rational behind the above hybrid algorithm of 1NNADTW and LSTM is explained below. Given any test sample x, we have its nearest neighbor yADTW (x) with the ADTW. We use the LSTM classifier CLSTM (·) to predict the label of both x and yADTW (x). Also we have the true label of yADTW (x) termed as Label(yADTW (x)). First, if we have that CLSTM (x) is the same as Label(yADTW (x)), which means that the classification result of the LSTM and 1NNADTW is the same, then we can largely determine that we have the accurate label (line 3-4) as both the LSTM and 1NN-ADTW are strong classifiers. Second, if we have that CLSTM (x) 6= Label(yADTW (x)), which means that LSTM predicted a different label to the 1NN-ADTW. In this case, we examine that if we have that CLSTM (x) == CLSTM (yADTW (x)). If this is true, which means that LSTM classifies the test sample x and its ADTW-nearest neighbor to a same class but different to the true label of that neighbor sample, we can judge that largely the LSTM classifier is not good at detecting the samples in this class and the label should be decided by the 1NN-ADTW classifier (line 5-6). Third, if LSTM has a different classification result with 1NN-ADTW while it can also accurately classify yADTW (x), we develop a voting procedure as follows. We first initialize two voting counters vknn and vlstm to 0 for 1NN-ADTW and LSTM respectively (line 8). Then we create a new sample x′ by the linear combination of x and yADTW (x) with weight parameter wj ∈ w (line 10). If x′ is classified the same label assigned to x by 1NN-ADTW, then in this case that the true label has a\nAlgorithm 1 Hybridization of 1NN-ADTW and LSTM 1: Individual training process: For each test sample x, we\nfind its nearest neighbour yADTW (x) with the ADTW. Then train the LSTM neural network and get a welltrained LSTM classifier CLSTM , which can predict the label of a sample x as CLSTM (x).\n2: Define weight parameters set w = {0.1, 0.2, ..., 0.9}. 3: if Label(yADTW (x)) == CLSTM (x) then 4: lhybrid ← Label(yADTW (x)). 5: else if CLSTM (x) == CLSTM (yADTW (x)) then 6: lhybrid ← Label(yADTW (x)). 7: else 8: vknn ← 0; vlstm ← 0. 9: for j ∈ w do\n10: x′ ← (1− wj) · x+ wj · yADTW (x). 11: if CLSTM (x′) == Label(yADTW (x)) then 12: vknn+ = 1− wj . 13: else if CLSTM (x′) == CLSTM (x) then 14: vlstm+ = wj . 15: else 16: vknn− = 0.2 · wj ; 17: vlstm− = 0.2 · (1− wj). 18: end if 19: end for 20: if vlstm ≥ vknn then 21: lhybrid ← CLSTM (x). 22: else 23: lhybrid ← Label(yADTW (x)). 24: end if 25: end if 26: Output: lhybrid, as the predicted label for sample x.\nlarger probability to be the same by 1NN-ADTW and LSTM favors this label, in this case the vknn is increased by 1−wj . Note that the rational behind the amount increased 1− wj is that if wj is small, then a small fraction of yADTW (x) added to the x can make LSTM favors yADTW (x); in this case we have more confidence that 1NN-ADTW is right, so its voting counter should be increased by a large value, in this case we set to 1−wj which is negative-correlated with wj . Otherwise, if the x′ is classified to be the same class of x with LSTM, then the results shows that the LSTM has certain degree of confidence in its classification on sample x. In this case, we increase the voting counter for LSTM vLSTM by wj . The amount wj increased follows the same design rational as for vknn. If both cases does not happen, which means the x′ is classified as a third class; in this case, the result does not favor anyone of 1NN-ADTW or LSTM, so their voting counters are decreased by certain amount as shown in line 16-17. At last we count the voting result and assign the final label (line 20-24)."
    }, {
      "heading" : "V. NUMERICAL EVALUATION AND ANALYSIS",
      "text" : "In this section we present the experimental results of the above proposed algorithms and the analysis. We compare the classification accuracy of the proposed 1NN-ADTW, LSTM and their hybrid algorithm ASTM/ADTW with the\n7\nbaseline algorithm 1NN-DTW. For the base line algorithm 1NN-DTW, the maximum warping offset r is manually tuned and set to be 0.15*n, where n is the length of the power series sample. For the 1NN-ADTW, the order of the ADTW distance is set to 8, and we will show analysis on the affects of the order. For the LSTM neural network, we set the maximum number of epoch to be 100. For some key parameters which can affect the performance of LSTM ,we give a detailed discussion in the following parameter settings study. Test data and codes for the ADTW tests are available at https://www.dropbox.com/s/pob07mwfxa4bop6/DataAndCode.zip?dl=0."
    }, {
      "heading" : "A. The Classification Accuracy Rate Comparison",
      "text" : "The five-fold classification accuracy results for different algorithms are shown in Table IV. For convenience, we will simply use test Fold i to denote the test with test samples in Fi and F(i+1)%5.\nFrom Table IV we can observe that: • The proposed 1NN-ADTW method outperforms 1NN-\nDTW in all five fold tests, which proves that the proposed ADTW is more suitable for our power series classification problem. • The proposed LSTM classifier shows similar accuracy compared to 1NN-ADTW and it also outperforms 1NNDTW. • The hybrid algorithm LSTM/ADTW can achieve higher accuracy compared to 1NN-ADTW and LSTM by an increment of 1%-2%, which proves that the hybrid algorithm can indeed improve the classification accuracy.\nFor the first observation, the reason that 1NN-ADTW outperforms 1NN-DTW may come from its non-commutative property, which is more suitable to a nearest neighbor classifier. For the second observation, we can see that LSTM, as a neural network, can significantly outperform the other canonical classifiers like SVM, which proves its strong modeling ability for sequential data. For the third observation, we can see that the improvement is small, which is reasonable as the baseline algorithms already achieve a high accuracy individually, which makes it difficult to achieve large improvement for the hybrid algorithm. The improvement caused by the hybrid algorithm will be shown clearly in the following detailed analysis."
    }, {
      "heading" : "B. Analysis on the Accurately Classified Samples",
      "text" : "In this subsection we analyze the accurately classified samples of the power series and study the difference between\ndifferent classifiers. In doing so we will be able to identify why and how the hybrid algorithm works.\nFig. 4 shows the predicted labels for the test samples in Fold 0 of the 1NN-DTW, 1NN-ADTW, LSTM and the hybrid algorithm LSTM/ADTW. Fig. 5 shows the accurately classified samples for each class and for each algorithm. From Fig. 4 and 5 we can observe that: • The proposed 1NN-ADTW method performs similarly to\n1NN-DTW, although 1NN-ADTW can accurately predict more test samples. This is reasonable as they two classifiers are both nearest-neighbor classifiers and they have similar measurement definition. • The proposed LSTM classifier shows certain degree of difference compared to the other two classifiers. One example, the LSTM classifiers cannot predict any test samples from the Spark-MLlib-LR class, while both 1NNDTW and 1NN-ADTW can; also, LSTM performs better than the other algorithms on class Hadoop-WordCount. • The proposed LSTM/ADTW classifier can successfully combine the advantages of LSTM and 1NN-ADTW, Such as for the Spark-MLlib-LR class and Hadoop-WordCount classes, the hybrid algorithm achieve similar performance to the better one of the LSTM and 1NN-ADTW. • All the classifiers can successfully classify the test samples of the web server class, which is normal as samples in the web server class are of completely different from the other MapReduce programs.\nThe above results show the difference of the 1NN-ADTW and the LSTM classifier which makes the hybrid algorithm work. Although 1NN-ADTW and the LSTM can achieve similar accuracy, their accurately classified samples have significant differences. To make this more clearly, we compute the union-accuracy accunion of the two classifiers as follows:\naccunion = |ALSTM ∪A1NN−ADTW |\nN , (13)\nwhere ALSTM and A1NN−ADTW are the sets of the accurately classified samples by LSTM and 1NN-ADTW respectively; N is the total number of test samples in this test fold. The union-accuracy of the five fold tests are shown in Table V. It can be seen that the union-accuracy is between 94%-96%. It shows the potentiality of a hybrid algorithm of the two classifiers. Note that the hybrid algorithm can only achieve accuracy smaller than the union-accuracy, as the union-accuracy is computed in an ideal way."
    }, {
      "heading" : "C. Discussion on the Parameter Settings",
      "text" : "In this subsection we discuss the parameters settings in the above algorithms. First we study the parameter used in\n9\nthe ADTW measurement, the order k. The test results with different k settings are shown in Table VI. It can be seen that a proper k setting is needed as a value too small or too large can both deteriorate the performance. In our experiments we find that a value in range [8,12] is suitable. Note that increasing the order can cause higher computing cost.\nSecond, we discuss the parameter settings for the LSTM classifier. Tuning of the hyper-parameters of the LSTM network is critical. In our experiments, we find that a improper can result a bad performance with accuracy lower than 50% for the LSTM. We find the following key settings in the LSTM classifier, which we have tested and find the proper setting, although detailed experimental results are omitted here. 1)Dropout layer, which is tested and not helpful in our case, as the number of time steps in our problem is small (200). 2) More than one LSTM layers, which is also not helpful. We test the results with more than one LSTM layers and the accuracy is not improved. 3) The hyper-parameter settings: three parameters are specially tuned in our experiments, which are the batch size (we set to 60), the dimension of the LSTM node (we set to 90), and the discretized range parameter S (we set to 100). These parameters can significantly affect the performance of the LSTM classifier, for example, an improper setting can lead to an accuracy as low as 40%."
    }, {
      "heading" : "VI. CONCLUSION AND FUTURE WORKS",
      "text" : "In this research, we propose an hybrid algorithm of the 1NN-ADTW and LSTM neural network. We first define an advancing dynamic time warping (ADTW) distance measurement, which can perform better than the DTW when used to classify the power series. Second we apply the stateof-art sequential data modeling neural network LSTM to classify the power series. Our study show that 1NN-ADTW and LSTM both can outperform the 1NN-DTW with similar accuracy; however, these two algorithms have their unique different natures and the accurately classified samples of these two algorithms have significant difference. In this sense, we propose a hybrid algorithm of the two classifiers termed as LSTM/ADTW, which further improves the accuracy. The proposed hybrid algorithm can achieve classification accuracy as high as 92% in our experiments.\nFor the future work, one interesting problem is to study the case that the power series generated by multiple programs thus with multiple labels. The problem is especially interesting when we have the test data being the combination of different programs (such as a pair of programs (A, B)) where this special pair may not be seen in the training data, for example, the training data may only contain samples generated by\nprogram pairs like (B,C) and (A,C). In this case, the classifier should be able to recognize the new pair (A, B)."
    } ],
    "references" : [ {
      "title" : "The case for energy-proportional computing",
      "author" : [ "L.A. Barroso", "U. Holzle" ],
      "venue" : "Computer, vol. 40, no. 12, pp. 33–37, 2007.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Data center workload monitoring, analysis, and emulation",
      "author" : [ "J. Moore", "J. Chase", "K. Farkas", "P. Ranganathan" ],
      "venue" : "Eighth Workshop on Computer Architecture Evaluation using Commercial Workloads, 2005.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A new approach to signal classification using spectral correlation and neural networks",
      "author" : [ "A. Fehske", "J. Gaeddert", "J.H. Reed" ],
      "venue" : "New Frontiers in Dynamic Spectrum Access Networks, 2005. DySPAN 2005. 2005 First IEEE International Symposium on. IEEE, 2005, pp. 144– 150.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Signal classification using statistical moments",
      "author" : [ "S.S. Soliman", "S.-Z. Hsue" ],
      "venue" : "Communications, IEEE Transactions on, vol. 40, no. 5, pp. 908–916, 1992.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Techniques of emg signal analysis: detection, processing, classification and applications",
      "author" : [ "M. Reaz", "M. Hussain", "F. Mohd-Yasin" ],
      "venue" : "Biological procedures online, vol. 8, no. 1, pp. 11–35, 2006.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fast time series classification using numerosity reduction",
      "author" : [ "X. Xi", "E. Keogh", "C. Shelton", "L. Wei", "C.A. Ratanamahatana" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 1033–1040.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning to forget: Continual prediction with lstm",
      "author" : [ "F.A. Gers", "J. Schmidhuber", "F. Cummins" ],
      "venue" : "Neural computation, vol. 12, no. 10, pp. 2451–2471, 2000.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science, vol. 313, no. 5786, pp. 504–507, 2006.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Everything you know about dynamic time warping is wrong.",
      "author" : [ "C.A. Ratanamahatana", "E. Keogh" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Toward accurate dynamic time warping in linear time and space",
      "author" : [ "S. Salvador", "P. Chan" ],
      "venue" : "Intelligent Data Analysis, vol. 11, no. 5, pp. 561–580, 2007.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Recurrent neural network based language model.",
      "author" : [ "T. Mikolov", "M. Karafiát", "L. Burget", "J. Cernockỳ", "S. Khudanpur" ],
      "venue" : "in INTERSPEECH,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2625–2634.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Beyond short snippets: Deep networks for video classification",
      "author" : [ "J. Yue-Hei Ng", "M. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 4694–4702.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
      "author" : [ "T.-H. Wen", "M. Gasic", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young" ],
      "venue" : "arXiv preprint arXiv:1508.01745, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg" ],
      "venue" : "The Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Each data center is typically equipped with hundreds of thousands servers and requires many mega-watts electricity to power its hosted servers and the auxiliary facilities [2].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "Monitoring technologies [3] can be divided into two categories: intrusive and non-intrusive.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "Currently there are a few works studies on classify signals like in [4] [5] [6].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "Currently there are a few works studies on classify signals like in [4] [5] [6].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "Currently there are a few works studies on classify signals like in [4] [5] [6].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "For the general time series classification problem, in [7], dynamic time warping (DTW) distance metric based method 1NNDTW is proven to be the most suitable algorithm.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "Second we apply the stateof-art sequential data modeling neural network long time short time memory (LSTM) [8] [9] to classify the power series.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "Second we apply the stateof-art sequential data modeling neural network long time short time memory (LSTM) [8] [9] to classify the power series.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "For this problem, on one hand, common classifiers like support vector machine (SVM), k-nearest neighbor (KNN) have been proved to be noncompetitive to the DTW distance metric based method like 1NN-DTW [7].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 8,
      "context" : "On the other hand, recently with the fast development of deep learning [10], LSTM neural network has also been proved to hold high modeling ability for sequential data.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "In practice, usually a threshold r is used to restrict the index offset in the alignment, which can be critical to the classification results [11].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 10,
      "context" : "Also there are many study [12] working on accelerating the computing speed of DTW, which results in the fast DTW that can be computed in linear time of the length of the sequences.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "as an upgrade of the recurrent neural network (RNN) [13].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "LSTM has shown great modeling power for sequential data and has been successfully applied in various machine learning fields like natural language process (NLP) [14], video analysis [15] and etc.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "LSTM has shown great modeling power for sequential data and has been successfully applied in various machine learning fields like natural language process (NLP) [14], video analysis [15] and etc.",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 14,
      "context" : "By discriminative, LSTM can be used for classification tasks while by generative, LSTM can be used to generate similar sequences like the training samples [16].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "The left number of sequences for each class is: [77, 31, 30, 35, 28, 7, 40, 14, 5, 100, 58, 36, 40].",
      "startOffset" : 48,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "The left number of sequences for each class is: [77, 31, 30, 35, 28, 7, 40, 14, 5, 100, 58, 36, 40].",
      "startOffset" : 48,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "The left number of sequences for each class is: [77, 31, 30, 35, 28, 7, 40, 14, 5, 100, 58, 36, 40].",
      "startOffset" : 48,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "We utilize eight different dimension reduction methods with scikit-learn [17] and project the original fixed length power sequences into a 2-dimensional space.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "The canonical classifiers tested here are listed as follows: Nearest Neighbors, Linear SVM, RBF SVM, Decision Tree, Random Forest, AdaBoost, Naive Bayes, LDA and QDA [17] .",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 5,
      "context" : "accuracy is not promising (when compared to the 1NN-DTW shown below), which actually proves that our power series labeling problem is a typical time series classification problem, as stated in [7], for such problem, canonical Euclidean distance metric based classifiers cannot achieve good results usually.",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 6,
      "context" : "In our experiments we find that a value in range [8,12] is suitable.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "In our experiments we find that a value in range [8,12] is suitable.",
      "startOffset" : 49,
      "endOffset" : 55
    } ],
    "year" : 2017,
    "abstractText" : "As many applications organize data into temporal sequences, the problem of time series data classification has been widely studied. Recent studies show that the 1-nearest neighbor with dynamic time warping (1NN-DTW) and the long short term memory (LSTM) neural network can achieve a better performance than other machine learning algorithms. In this paper, we build a novel time series classification algorithm hybridizing 1NN-DTW and LSTM, and apply it to a practical data center power monitoring problem. Firstly, we define a new distance measurement for the 1NN-DTW classifier, termed as Advancing Dynamic Time Warping (ADTW), which is non-commutative and non-dynamic programming. Secondly, we hybridize the 1NNADTW and LSTM together. In particular, a series of auxiliary test samples generated by the linear combination of the original test sample and its nearest neighbor with ADTW are utilized to detect which classifier to trust in the hybrid algorithm. Finally, using the power consumption data from a real data center, we show that the proposed ADTW can improve the classification accuracy from about 84% to 89%. Furthermore, with the hybrid algorithm, the accuracy can be further improved and we achieve an accuracy up to about 92%. Our research can inspire more studies on non-commutative distance measurement and the hybrid of the deep learning models with other traditional models.",
    "creator" : "LaTeX with hyperref package"
  }
}