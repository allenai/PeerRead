{
  "name" : "1511.00215.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding",
    "authors" : [ "Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao" ],
    "emails" : [ "helei}@microsoft.com,", "zhaohai@cs.sjtu.edu.cn,", "yqian@ets.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n00 21\n5v 1\n[ cs\n.C L\n] 1\nN ov\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of promising recurrent architecture, able to bridge long time lags between relevant input and target output, and thereby incorporate long range context. This type of structure is theoretically well suited and has been proven a powerful model for tagging tasks. For applications in natural language processing (NLP), LSTM has proved advantageous in language modeling (Sundermeyer et al., 2012; Sundermeyer et al., 2015), language understanding (Yao et al., 2013; Mesnil et al., 2013), and machine translation (Sundermeyer et al., 2014). A bidirectional LSTM (BLSTM) (Schuster and Paliwal, 1997), furthermore, introduces two independent layers to\n∗*Work performed as an intern in the Speech Group, Microsoft Research Asia\naccumulate contextual information from the past and future histories. It seems natural to expect BLSTM to be an effective model for tagging tasks in NLP while to our best knowledge no successful case of this application has been reported.\nIn this work, we apply BLSTM-RNN to three typical tagging tasks: part-of-speech (POS) tagging, chunking and named entity recognition (NER). As a neural network model, BLSTM-RNN is awkward for using discrete features. Since these features have to be represented as one-hot vector usually with very large size, using this type of features would lead to too large input layer to operate. Therefore, we only use word form and simple capital features, disregarding of all the other discrete conventional NLP features, such as morphological features. Using such simple task independent features assures our model quite unified so that it can be directly applied to various tagging tasks.\nTo further improve the performance of our approach without disrupting the universality, we introduce word embedding, which is a real-valued vector associated with each word. It is an internal representation that is considered containing syntactic and semantic information and has shown a very attractive feature for various NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Collobert et al., 2011). Word embedding can be obtained by training a neural network language model (Bengio et al., 2006), shallow neural network (Mikolov et al., 2013a; Pennington et al., 2014a; Collobert et al., 2011), or a recurrent neural network (Mikolov et al., 2010). In this work, we also propose a novel method to train word embedding on unlabeled data with BLSTM-RNN.\nThe main contributions of this work include: First, it shows an effective way to use BLSTMRNN for dealing with various NLP tagging tasks. Second, it proposes a unified tagging system that can get competitive tagging accuracy without using any task specific features, which makes this\napproach more practical for tagging tasks that lack of prior knowledge.\nThe remainder of this paper is organized as follows. Section 2 gives a brief introduction of BLSTM architecture. Section 3 describes the BLSTM-RNN based tagging approach and Section 4 introduces the training and usage of word embeddings. Section 5 presents experimental results. Section 6 discusses related works and concluding remarks are given in Section 7."
    }, {
      "heading" : "2 Bidirectional LSTM Architecture",
      "text" : "Recurrent neural network (RNN) is a kind of artificial neural network that contains cyclic connections, which can model contextual information dynamically. Given an input sequence x1, x2, ..., xn, a standard RNN computes the output vector yt of each word xt by iterating the following equations from t = 1 to n:\nht = H(Wxhxt +Whhht−1 + bh)\nyt = Whyht + by\nwhere ht is the vector of hidden states, W denotes weight matrix connecting two layers (e.g. Wxh is the weights between input and hidden layer), b denotes bias vector (e.g. bh is the bias vector of hidden layer) and H is the activation function of hidden layer. Note that ht persists information from previous step’s hidden state ht−1, and thus theoretically RNN can make use of all input history.\nHowever, in practice, the range of input history that can be accessed is limited, since the influence of a given input would decay or blow up exponentially as it circulates around the hidden states, which is known as vanishing gradient problem (Hochreiter et al., 2001). The most effective solution of this problem so far is the long short-term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997). An LSTM network is formed like the standard RNN except that the self-connected hidden units are replaced by special designed units called memory blocks as illustrated in Figure 1. The output of LSTM hidden layer ht given input xt is computed as following composite function (Graves et al., 2013):\nit = σ(Wxixt +Whiht−1 +Wcict−1 + bi)\nft = σ(Wxfxt +Whfht−1 +Wcfct−1 + bf )\nct = ftct−1 + it tanh(Wxcxt +Whcht−1 + bc)\not = σ(Wxoxt +Whoht−1 +Wcoct + bo)\nht = ot tanh(ct)\nwhere σ is the logistic sigmoid function, and i, f , o and c are respectively the input gate, forget gate, output gate and cell activation vectors. Weights matrices are represented as arrows in Figure 1. These multiple gates allow the cell in LSTM memory block to store information over long periods of time, thereby avoiding the vanishing gradient problem. More interpretation about this architecture can be found in (Graves, 2012).\nAnother shortcoming of conventional RNN is that only historic context can be exploited. In a typical tagging task where the whole sentence is given, it is helpful to exploit future context as well. Bidirectional RNN (BRNN) (Schuster and\nPaliwal, 1997) offers an effective solution that can access both the preceding and succeeding contexts by involving two separate hidden layers. As illustrated in Figure 2, BRNN first computes the forward hidden sequence −→ h and the backward hidden sequence ←− h respectively, and then combines\n−→ ht and ←− ht to generate output yt. The process can be expressed as:\n−→ ht = H(Wx −→ h xt +W−→h −→ h −→ h t−1 + b−→h ) ←− ht = H(Wx ←− h xt +W←−h ←− h ←− h t+1 + b←−h ) yt = W−→h y −→ h t +W←−h y ←− h t + by\nReplacing the hidden states in BRNN with LSTM memory blocks gives bidirectional LSTM (BLSTM) (Graves and Schmidhuber, 2005), i.e., the main architecture used in this paper, which can incorporate long periods of contextual information from both directions. Besides, as feedforward layers stacked in deep neural networks, the BLSTM layer can also be stacked on the top of the others to form a deep BLSTM architecture. As a type of RNN, deep BLSTM can be trained via various gradient-based algorithms designed for general RNN, for example, real-time recurrent learning (RTRL) and back-propagation through time (BPTT). In this work, we use the BPTT algorithm as described in (Graves, 2012) since it is conceptually simple and efficient for computation."
    }, {
      "heading" : "3 Tagging System",
      "text" : "The schematic diagram of BLSTM-RNN based tagging system is illustrated in Figure 3. Given\na sentence w1, w2, ..., wn with tags y1, y2, ..., yn, BLSTM-RNN is first used to predict the tag probability distribution o(wi) of each word, then a decoding algorithm is proposed to generate the final predicted tags y′1, y ′ 2, ..., y ′ n."
    }, {
      "heading" : "3.1 BLSTM-RNN for tagging",
      "text" : "The usage of BLSTM RNN is illustrated in Figure 4. Here wi is the one hot representation of the current word which is a binary vector with dimension |V |where V is the vocabulary. To reduce |V |,\neach letter of input word is transferred to its lowercase. The upper case information is kept by introducing a three-dimensional binary vector f(wi) to indicate if wi is full lowercase, full uppercase or leading with a capital letter. The input vector Ii of the network is computed as:\nIi = W1wi +W2f(wi)\nwhere W1 and W2 are weight matrixes connecting two layers. W1wi is also known as the word embedding of wi which is a real-valued vector with a much smaller dimension than wi. In practice, to reduce the computational cost, W1 is implemented as a lookup table, W1wi is returned by referring to wi’s word embedding stored in this table. The output layer is a softmax layer whose dimension is the number of tag types. It outputs the tag probability distribution of word wi."
    }, {
      "heading" : "3.2 Decoding",
      "text" : "According to BLSTM-RNN, the obtained probability distribution of each step is supposed independent with each other. However, in some tasks such as NER and chunking, tags are highly related with each other and a few of types of tags can only follow specific types of tags. To make use of this kind of labeling constraints, we introduce a transition matrix A between each step’s output as illustrated in Figure 5. Each circle represents\na tag probability predicted by BLSTM-RNN, Aij stores the score of transition from tag ti to tj . The score is determined in a very simple way that if tj appears immediately behind ti in training corpus, Aij is 1, otherwise 0. It implies that tag bigrams that do not appear in training corpus are supposed invalid and would not appear in test case, no matter whether they are actually valid. The score of a sentence w1, w2, ..., wn ([w]n1 for short) along a path of tags y1, y2, ..., yn ([y]n1 for short) is then given by the product of transition score and BLSTMRNN output probability:\ns([w]n1 , [y] n 1 ) =\nn∏\ni=1\n(Ayi−1yi × o(wi)yi)\nThe goal of decoding is to find the path which gives the highest sentence score:\n[y′]n1 = argmax [y]n\n1\ns([w]n1 , [y] n 1 )\nThis is a typical dynamic programming problem and can be solved with Viterbi algorithm (Viterbi, 1967)."
    }, {
      "heading" : "4 Word Embedding",
      "text" : "As a neural network, BLSTM-RNN can easily adopt already trained word embedding by initializing W1 (illustrated in Figure 4) with those external embeddings. Currently, many word embeddings trained on very large corpora are available on line. However, these embeddings are trained by neural networks that are very different from BLSTM-RNN. This inconsistency is supposed as an shortcoming to make the most of these trained word embeddings. To conquer this shortcoming, we also propose a novel method to train word embedding on unlabeled data with BLSTM-RNN.\nIn this method, BLSTM-RNN is applied to perform a tagging task with only two types of tags to predict: incorrect/correct. The input is a sequence of words which is a normal sentence with some words replaced by words randomly chosen from vocabulary. The words to be replaced are chosen randomly from the sentence. For those replaced words, their tags are 0 (incorrect) and for those that are not replaced, their tags are 1 (correct). A simple sample is shown in Figure 6. Although it is possible that some replaced words are also reasonable in the sentence, they are still considered “incorrect”. Then BLSTM-RNN is trained to minimize the binary classification error on the training\ncorpus. The neural network structure is the same as that in Figure 4. When the neural network is trained, W1 contains all trained word embeddings."
    }, {
      "heading" : "5 Experiments",
      "text" : "All of our approaches are implemented based on CURRENT (Weninger et al., 2014), an open source GPU-based toolkit of BLSTM-RNN. For constructing and training the neural network, we follow the default setup of CURRENT: The activation functions of input layer and hidden layers are logistic function, while the output layer uses softmax function for multiclassification. Neural network is trained using statistical gradient descent algorithm with constant learning rate.\nIn all experiments, consecutive digits occurring within a word are relpaced with the symbol “#” . For example, both words “Tel192” and “Tel6” are converted into “Tel#”. The vocabulary we used is the most common 100,000 words in North American news corpus (Graff, 2008), plus one single “UNK” symbol for replacing all out of vocabulary words."
    }, {
      "heading" : "5.1 Tasks",
      "text" : "In this section, we briefly introduce three typical tagging tasks and their experimental setup on which we evaluate the performance of the proposed approach: part-of-speech tagging (POS), chunking (CHUNK) and named entity recognition (NER).\nPOS is the task of labeling each word with its part of speech, e.g. noun, verb, adjective, etc. Our POS tagging experiment is conducted on the Wall Street Journal data from Penn Treebank III (Marcus et al., 1993). Training, development and test sets are split following setup in (Collins, 2002). Table 1 lists statistical information of the three data sets. Performance is evaluated by the accuracy of predicted tags on test set.\nCHUNK, also known as shallow parsing, divides a sentence into phrases that each phrase con-\ntains syntactically related words, such as noun phrase (NP), verb phrase (VP), etc. To identify the phrase boundaries, we use a commonly used IOBES tagging scheme that further fractionizes each tag type into four subtypes to indicate whether the word is inside (I), outside (O), begin (B), end(E) a multiple words chunk or a single word chunk (S). We conduct our experiment on a standard experimental setup of CHUNK according to the CoNLL-2000 shared task (Sang and Buchholz, 2000). Basic information about this setup is listed in Table 2. Performance is assessed by\nthe F1 score computed by the evaluation script released by the CoNLL-2000 shared task1.\nNER recognizes phrases of named entities such as names of persons, organizations and locations. IOBES tagging scheme is also applied in this task. Our experimental setup follows the CoNLL2003 shared task (Tjong Kim Sang and De Meulder, 2003). Table 3 shows its basic information. Performance is measured by the F1 score calcu-\nlated by the evaluation script of the CoNLL-2003 shared task 2.\n1http://www.cnts.ua.ac.be/conll2000/ chunking\n2http://www.cnts.ua.ac.be/conll2003/ ner/"
    }, {
      "heading" : "5.2 Network Structure",
      "text" : "In all experiments, without specific description, the input layer size is fixed to 100 and output layer size is set as the number of tag types according to the specific tagging task.\nIn this experiment, we evaluate different sizes of hidden layer in BLSTM-RNN to pick up the best size for later experiments. Performances on three tasks are shown in Figure 7. It shows that\nhidden layer size has a limited impact on performance when it becomes large enough. To keep a good trade-off of accuracy, model size and training time, we choose 100 which is the smallest layer size to get a “reasonable” performance as the hidden layer size in all the following experiments.\nBesides, we also evaluate deep structure which uses multiple BLSTM layers. This deep BLSTM has been reported achieving significantly better performance than single layer BLSTM in various applications such as speech synthesis (Fan et al., 2014; Fernandez et al., 2014), speech recognition (Graves et al., 2013) and handwriting recognition (Graves and Schmidhuber, 2009). Table 4 compares the performance of BLSTM-RNNs with one (B) and two (BB) hidden layers. Size of all hidden layers is set 100. Using more layers brings a\nslightly improvement for NER task, while does not show much help for POS and slightly decreases the performance of CHUNK. A possible explanation is that one BLSTM layer is adequate to learn an effective model for tasks like POS and CHUNK\nwhich are relatively simple compared with NER or speech tasks, thus in these cases involving more layers would not provide a further help. Meanwhile additional more parameters makes the network harder to converge to a locally optimal model which leads to a worse performance. Based on this observation, in following experiments, BLSTMRNN is set only one hidden layer."
    }, {
      "heading" : "5.3 Decoder",
      "text" : "In this experiment, we test the effect of decoder. Performance of BLSTM tagging systems without and with decoder are listed in Table 6. Without using decoder, predicted tag y′ is determined by directly selecting the tag with the highest probability among network output o(wi). The results show that decoder significantly improves the performance of CHUNK and NER tasks, though shows no help for POS.\nFor this difference of improvment, we provide a possible explanation. In tasks like CHUNK and NER which uses IOBES tagging scheme, tags are highly dependent with their previous tags. For example, I-X can only appear behind B-X. CHUNK task has 42 tag types that can combine 42 × 42 = 1764 tag bigrams, but only 252 (14.3%) of them actually appear in training corpus. In NER task, 78 (27.0%) of total 289 tag bigrams have occurred more than once. Decoding in this case filters considerable invalid paths including more than half of candidates and thus improves the performance. As a contrast, in POS task, tags are directly predicted on each word without using any tagging schemes and thus do not have such strong dependence among tags. In POS training corpus, 1439 (71.0%) of total 2025 tag bigrams have occurred more than once. In this case, the dependences of tags are mainly learnt by BLSTM layer and the help from the decoder is very limited.\nThe improvement provided by decoder shows that although BLSTM is considered can adopt contextual information automatically, the model is still far from ideal and a decoder with prior knowledge of tagging scheme is essential for achieving a good performance."
    }, {
      "heading" : "5.4 Word Embedding",
      "text" : "In this experiment, we evaluate BLSTM-RNN tagging approach using various of word embeddings including those trained by the proposed approach in Section 4 as well as four types of published word embeddings.\nTo train word embeddings with our approach, we use North American news (Graff, 2008) as the unlabeled data. To construct corpus for training embeddings, the North American news data is first tokenized with the Penn Treebank tokenizer script 3. Then about 20% words in normal sentences of the corpus are replaced with randomly selected word. BLSTM-RNN is trained to judge which word has been replaced as described in Section 4. To use word embedding, we just initialize the word embedding lookup table (W1) with these already trained embeddings. For words without corresponding external embeddings, their word embeddings are initialized with uniformly distributed random values, ranging from -0.1 to 0.1.\nTable 5 lists the basic information of involved word embeddings and performances of BLSTMRNN tagging approaches using these embeddings, where RCV1 represents the Reuters Corpus Volume 1 news set. RANDOM is the word embedding set composed of random values which is the baseline. BSLTMWE(10m), BSLTMWE(100m) and BSLTMWE(all) are word embeddings respectively trained by BLSTM-RNN on the first 10 million words, first 100 million words and all 536 million words of North American news corpus. While BSLTMWE(10m) does not bring about obvious improvement, BSLTMWE(100m) and BSLTMWE(all) significantly improve the performance. It shows that BLSTM-RNN can benefit from word embeddings trained by our approach and larger training corpus indeed leads to a better performance. This suggests that the result may be further improved by using even bigger unlabeled dataset. In our experiment, BSLTMWE(all) can be trained in about one day (23 hrs) on a NVIDIA Tesla M2090 GPU. The training time increases linearly with the training corpus size.\nWhen uses published word embeddings, the input layer size of BLSTM-RNN is set to the dimension of utiembeddings. All of the four published word embeddings significantly enhance BLSTMRNN. It proves that word embedding is a use-\n3https://www.cis.upenn.edu/˜treebank/ tokenization.html\nful feature and is an effective way to make use of big unlabeled data. Among all mentioned embeddings, BLSTMWE(all) achieves the best performance on POS and CHUNK tasks while slightly falls behind (Collobert, 2011) in NER task. One possible explanation is that (Collobert, 2011) is trained on Wikipedia data which contains more named entities, thereby their word embedding contains more useful information for NER task. BLSTMWE(all) is trained on a news corpus which is written with more formal grammar, thus it learns better representations for tagging syntactic role. Based on this conjecture, it is natural to expect the combination of (Collobert, 2011) and BLSTMWE(all) to bring a further improvement. This idea yields the BLSTMWE(all) + (Collobert, 2011). This word embeddings are first initialized with (Collobert, 2011) and then trained by BLSTM-RNN on North American corpus as BLSTMWE(all). This embeddings help BLSTM-RNN obtain the best performance in all three tasks."
    }, {
      "heading" : "5.5 Comparison with Previous Systems",
      "text" : "In this section, we compare our approach with previous state-of-the-art systems on POS, CHUNK and NER tasks. Table 7 lists the related works of these three tasks. BLSTM represents the BLSTMRNN tagging approach using BLSTMWE(all) + (Collobert, 2011) word embeddings.\nPOS: (Huang et al., 2012) reports the high-\nest accuracy on WSJ test set (97.35%). Besides, (Moore, 2014) (97.34%) and (Shen et al., 2007) (97.33%) also reach accuracy above 97.3%. These three systems are considered as state-of-the-art systems in POS tagging. (Toutanova et al., 2003) is one of the most commonly used approaches which is also known as Stanford tagger. All of these methods utilize rich morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4). Moreover, (Shen et al., 2007) also involves prefix and suffix of length from 5 to 9. (Moore, 2014) adds extra elaborately designed features, including flags indicating if word ends with −ed or −ing, etc.\nCHUNK: (Kudoh and Matsumoto, 2000) (93.48%) is the system ranked first in CoNLL2000 shared task challenge. Later, (Sun et al., 2008) reports the state-of-the-art F1 score (94.34%) on CoNLL-2000 task. Besides, (Sha and Pereira, 2003) (94.29%) and (McDonald et al., 2005) (94.29%) both report F1 score around 94.3%. These systems use features composing of words and POS tags.\nNER: (Florian et al., 2003) (88.76%) is the top system in CoNLL-2003 shared task challenge. (Ando and Zhang, 2005) (89.31%) reports a better F1 score with a semi-supervised approach. The unlabeled corpus they used is 27M words from Reuters. Features used in these works include words, POS tags, CHUNK tags, prefixes, suffixes, and a large gazetteer (geographical dictionary).\nAll of these top systems use rich features and feature sets in different tasks are quite different. In contrast, our system only uses one set of taskindependent features in all tasks and do not require any feature engineering to achieve the state-of-theart performance in CHUNK and NER tasks. In POS, our approach also get a competitive performance that is comparable with Stanford POS tagger. Our results show that feature engineering in conventional methods can be effectively replaced by built-in modeling of BLSTM-RNN."
    }, {
      "heading" : "6 Related Works",
      "text" : "(Collobert et al., 2011) is the most similar work as ours. It is a unified tagging solution based on neural network which also uses simple taskindependent features and word embeddings learnt from unlabeled text. The main difference is that (Collobert et al., 2011) uses feedforward neural network instead of BLSTM-RNN. A comparison of (Collobert et al., 2011) and our approach is listed in Table 8, where NN is the uni-\nfied tagging system of (Collobert et al., 2011) and NN+WE is that system using word embedding trained by their approach. Without using word embedding, BLSTM outperforms NN in all three tasks. It is consistent with the observations in previous works that BLSTM-RNN is a more powerful model for sequential labeling than feedforward network. With word embeddings, our approach also significantly surpasses NN+WE in all three tasks.\n(Suzuki and Isozaki, 2008) proposes a semisupervised approach which incorporates one billion words of unlabeled data during training. They claim that their model can be applied to various tasks and report the state-of-the-art performance on all these three tasks (97.40 for POS, 95.15 for CHUNK, 89.92 for NER). However, they also utilize rich feature templates for each task (47 feature templates for POS, 39 templates for CHUNK, 79 templates for NER) which makes their system not so unified. Their method still requires prior\nknowledge to design these feature templates which limits its application to tasks that lack of knowledge or tools to extract necessary features. Besides, they have to train model with quite big unlabeled data for each task which would lead to a time consuming training process. In contrast, our approach separates the lengthy training for word embedding from the relatively fast training for the supervised tagging task. Once our word embeddings are trained, they can be regarded as a kind of linguistic resources like semantic dictionary and be directly used."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we propose a unified tagging solution based on BLSTM-RNN. This system avoids involving task-specific features, instead it utilizes word embeddings learnt automatically from unlabeled text. Without reliance on feature engineering or prior knowledge, this approach can be easily applied to various tagging tasks. Experiments are conducted on three typical tagging tasks: POS tagging, chunking and named entity recognition. Using simple task-independent input features, our approach gets nearly state-of-the-art results on all these three tasks. Our results suggest that BLSTM-RNN with word embedding is an effective unified tagging solution and worth further exploration."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTMRNN) has been shown to be very effective for modeling and predicting sequential data, e.g. speech utterances or handwritten documents. In this study, we propose to use BLSTM-RNN for a unified tagging solution that can be applied to various tagging tasks including partof-speech tagging, chunking and named entity recognition. Instead of exploiting specific features carefully optimized for each task, our solution only uses one set of task-independent features and internal representations learnt from unlabeled text for all tasks. Requiring no task specific knowledge or sophisticated feature engineering, our approach gets nearly state-ofthe-art performance in all these three tagging tasks.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}