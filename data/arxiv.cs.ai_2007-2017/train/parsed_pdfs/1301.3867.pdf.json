{
  "name" : "1301.3867.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Planning in Stochastic Games",
    "authors" : [ "Michael Kearns", "Yishay Mansour", "Satinder Singh" ],
    "emails" : [ "mkearns@research.att.com", "mansour@math.", "baveja@research.att.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Stochastic games generalize Markov decision processes (MDPs) to a multiagent setting by allowing the state transitions to depend jointly on all player actions, and having rewards determined by multiplayer matrix games at each state. We consider the prob lem of computing Nash equilibria in stochas tic games, the analogue of planning in MDPs. We begin by providing a simple general ization of finite-horizon value iteration that computes a Nash strategy for each player in general-sum stochastic games. The algo rithm takes an arbitrary Nash selection func tion as input, which allows the translation of local choices between multiple Nash equilib ria into the selection of a single global Nash equilibrium.\nOur main technical result is an algorithm for computing near-Nash equilibria in large or in finite state spaces. This algorithm builds on our finite-horizon value iteration algorithm, and adapts the sparse sampling methods of Kearns, Mansour and Ng (1999) to stochas tic games. We conclude by describing a coun terexample showing that infinite-horizon dis counted value iteration, which was shown by Shapley to converge in the zero-sum case (a result we give extend slightly here), does not converge in the general-sum case.\n1 INTRODUCTION\nThere has been increasing interest in artificial intel ligence in multi-agent systems and problems. Fueled by the expanding potential for large-scale populations of autonomous programs in areas as diverse as online trading and auctions, personalized web crawling, and\nmany other areas, such work seeks to provide both al gorithms and mathematical foundations for agents in complex, distributed environments.\nGiven the detailed theoretical and practical under standing of single-agent planning and learning in Markov decision processes (MDPs) that has been built over the last decade, one natural line of research is the extension of these algorithms and analyses to a multi agent setting (Boutilier, Goldszmidt, and Sabata 1999; Brafman and Tennenholtz 1999; Hu and Wellman 1998). The work presented here is a contribution to this line. We consider the problem of computing Nash equilibria in stochastic games. Stochastic games gen eralize MDPs to a multi-agent setting by allowing the state transitions to depend jointly on all agent actions, and having the immediate rewards at each state de termined by a multi-agent general-sum matrix game associated with the state. If we view the computation of an optimal policy in a given MDP as the problem of planning for a single agent's interaction with its fixed stochastic environment, the natural analogue in a stochastic game would be the computation of a Nash equilibrium, which allows all agents to simultaneously enjoy a best response policy to the others.\nWe begin by providing a generalization of finite horizon value iteration in MDPs that computes, in time polynomial in the number of states, a Nash strat egy for each player in any given two-player 1, general sum stochastic game. We introduce the notion of a Nash selection function, which is simply a way of ex tracting local Nash equilibria from the matrix games stored at each state. The key observation behind our value iteration generalization is that local appli cation of any Nash selection function to the appro priate backup matrices will yield a global Nash equi librium for the stochastic game. Different global so-\n1 For simplicity, we present all of our results for the two player case. They can be generalized to the k-player case at the cost of a factor exponential in k, which cannot be avoided without special assumptions on the game.\n310 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nlutions can be found by varying the underlying Nash selection function used by the algorithm.\nWe then proceed to present our central technical con tribution, which is an online sparse sampling algo rithm for computing near-Nash equilibria in general sum stochastic games. In the same way that recent work of Kearns, Mansour and Ng (1999) provided an online planning algorithm for large or infinite MDPs, we build on our finite-horizon value iteration and give a randomized algorithm for computing each half of a Nash policy in a stochastic game for which we have only a generative model (the ability to sample the state transition distributions and rewards). Like the sparse sampling algorithm of Kearns et al. for the MDP case, the per-state running time of our algorithm has no de pendence on the state space size, but is exponential in the horizon time. This provides a rather differ ent trade-off than our basic value iteration algorithm, whose dependence on the state space size is quadratic, but whose dependence on the horizon time is linear. Like the earlier work, one can view the new algorithm as greatly reducing the width or degree (from linear in state space size to constant) of a full look-ahead tree, while leaving the depth unchanged.\nThe strong guarantee we prove for our algorithm is that it computes a near-Nash equilibrium in any stochastic game: as long as one player chooses to play their half of the output of the algorithm at each state, the other player provably has little incentive to de viate from playing their half. Since our algorithm is randomized (due to the sparse sampling), and because of the potential sensitivity of Nash equilibria to small fluctuations in the game, it is important that both players play according to a common copy of the al gorithm. (Technically, this is known as a correlated equilibrium.) Our algorithm can thus be viewed as providing a centralized solution for the players that is near-Nash.\nWe close with a discussion of value iteration in the in finite horizon discounted case. We give a small gener alization of Shapley's proof of convergence in the zero sum case, but then provide a rather strong counterex ample to convergence in the general sum case, high lighting an interesting difference between planning in MDPs and planning in stochastic games.\n2 DEFINITIONS A ND NOTATION\nWe adopt standard terminology and notation from classical game theory. A two-player game is defined by a matrix pair (M1, M2), specifying the payoffs for the row player (player 1) and the column player (player 2), respectively. We shall assume without loss of gen erality throughout the paper that all game matrices\nM1 and M2 are n by n, and that their indices thus range from 1 to n. If the row player chooses the in dex (or pure strategy) i and the column player chooses the index j, the former receives payoff M1 ( i, j) and the latter M2(i,j). More generally, if a and f3 are dis tributions (or mixed strategies) over the row and col umn indices, the expected payoff to player k E {1, 2} is Mk(a, /3) = EiEa:,jE,B[Mk(i, j)], where the notation i E a indicates that i is distributed according to a. We say that the game (M1, M2) is zero-sum if M2 = -M1.\nThe mixed strategy pair (a, /3) is said to be a Nash equilibrium (or Nash pair) for the game (M1, M2) if (i) for any mixed strategy a', M1(a',/3) � M1(a,/3), and (ii) for any mixed strategy /31, M2(a,f3') � M2(a,f3). In other words, as long as one player plays their half of the Nash pair, the other player has no in centive to switch from their half of the Nash pair. We will also need the standard notion of approxi mate Nash equilibria. Thus, we say that (a, /3) is f. Nash for (M1, M2) if (i) for any mixed strategy a', M1(a',/3) � M1(a,f3)+€, and (ii) for any mixed strat egy /3', M2(a, /3') � M2(a, /3) +f.\nIt is well-known (Owen 1995) that every game has at least one Nash pair in the space of mixed (but not necessarily pure) strategies, and many games have multiple Nash pairs. Furthermore, in the zero-sum case, if (a, /3) and (a', /3') are both Nash pairs, then M1(a,f3) = M1(a',/3'), and (a,/3') and (a',/3) are also Nash pairs. In other words, in the zero-sum case, the payoff to the players is always the same under any Nash pair, and Nash pairs can be freely \"mixed\" to gether to form new Nash pairs. Since in the zero-sum case the payoff to the column player is always the neg ative of the row player, we can unambiguously refer to the value of the game v(MI) = M1(a,{3), where (a,/3) is any Nash pair. In the general-sum case, different Nash pairs may yield different payoffs to the players, and in general Nash pairs cannot be mixed in the man ner described.\nThe security level for the row player is defined as s1(M1,M2) = si (MI) = maxa:min,eMI(a,/3) and for the column player as s2(M1, M2) = s2(M2) = max,e mina: M2(a, {3). A mixed strategy achieving the maximum in each case is referred to as a security strat egy for the player. The security level for a player is the payoff that player can ensure for themselves regardless of their opponent's behavior. In the zero-sum case, the notion of Nash pair coincides with that of security pair, and the value of a game is simply the security level for the row player; in the general case, however, the security level of a player may be lower than their payoff in a Nash pair.\nA stochastic game G over a state space S consists\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 311\nof a designated start state so E S, a matrix game (MI(s], M2[s]) for every state s E S, and transition probabilities P(s'Js, i, j) for every s, s' E S, every pure row strategy i, and every pure column strategy j. Each step of play in a stochastic game proceeds as follows. If play is currently in state s and the two players play mixed strategies a and /3, then pure strategies i and j are chosen according to a and /3 respectively, the players receive immediate payoffs MI(s](i, j) and M2[s](i, j) (and thus have expected immediate pay offs M1[s](a, /3) and M2[s](a, /3)), and the next state s' is drawn according to the transition probabilities P( ·Js, i, j). Thus both the immediate payoffs to the players and the state transition depend on the actions of both players. If the game matrices at every state of G are zero-sum we say that G is zero-sum.\nWe shall consider two different standard measures of the overall total return received by the players. In the infinite-horizon discounted case, play begins at s0 and proceeds forever; if a player receives pay offs ro, r1 , r2, . . . , they are credited with total return ro + '\")'r1 + ,.,Pr2 + · · · , where 0 :::; \"( < 1 is the discount factor. In the finite-horizon undiscounted (also called the T-step average) case, which is our main interest, play begins at s0 and proceeds for exactly T steps; if a player receives payoffs r0, r1, r2, . . . , rr-1 they are credited with total return (1/T)(ro+rl +· · ·rr-d- We shall use Rmax to denote the largest absolute value of any element in the matrices Mk[s]. For ease of exposi tion we will often make the assumption that Rmax :::; 1 without loss of generality. The goal of each player in a stochastic game is to maximize their expected total return from the designated start state.\nA policy for a player in a stochastic game G is a map ping 7r( s) from states s E S to mixed strategies to be played at the matrix game at s. A time-dependent policy 7r(s, t) allows the mixed strategy chosen to de pend on the number t of steps remaining in the T step game. It is known (Owen 1995) that we can restrict attention to policies (in the infinite-horizon discounted case) or time-dependent policies (in the finite-horizon case) without loss of generality- that is, no advantage can be gained by a player by con sidering the history of play. If 1!\"1 and 1!\"2 are policies in a matrix game G with designated start state so, we use Gk(so, 1!\"1, 7r2), k E {1, 2}, to denote the ex pected infinite-horizon discounted return to player k, and G k (T, so, 1!\"1, 1!\"2) to denote the expected T -step average return. The notion of Nash pairs extends naturally to stochastic games: we say that ( 1!\"1, 1!\"2) is a Nash pair if for any start state s0 and any 1!\"�, Gl(so, 7l\"�, 7r2):::; G1(so, 7l\"J, 1l\"2), and for any start state so and any 1!\"�, G2(so, 7l\"J, 7l\"�) :::; G2(so, 7l\"J, 1l\"2) (with the obvious generalization for the T-step case, and\nthe obvious generalization to the notion of t-Nash). Again, it is known (Owen 1995) that for the infinite discounted case, a Nash pair always exists in the space of policies, and for the average return case, a Nash pair always exists in the space of time-dependent policies. The notions of security level and security policies in a stochastic game are defined analogously. The subject of this paper is the computation of Nash and t-Nash policy pairs in stochastic games, including stochastic games with large or infinite state spaces.\nSince the matrix game at any given state in a stochas tic game may have many Nash equilibria, it is easy to see that there may be exponentially many Nash equilibria in policy space for a stochastic game. One question we face in this paper is how to turn local de cisions at each state into a global Nash or near-Nash policy pair. For this we introduce the notion of a Nash selection function. For any matrix game (M1, M2), a Nash selection function f returns a pair of mixed strategies f ( M1, M2) = (a, /3) that are a Nash pair for (M1, M2). We denote the payoff to the row player un der this Nash pair by v}(M1, M2) = M1(f(M1, M2)) and the payoff to the column player by v}(M1, M2) = M2(f(M1, M2)). Thus, a Nash selection function  which we allow to be arbitrary in most of our results is simply a specific way of making choices of how to be have in isolated matrix games. Looking ahead slightly, we will show how the application of any Nash selection function to the appropriate backup matrices yields a Nash policy pair for the global stochastic game.\nSimilarly, a security selection function f returns a pair of mixed strategies f ( M1, M2) = (a, /3) that are a secu rity pair for (M1, M2). In this case we use v'(M1, M2) to denote the security level of player k in the game (M1, M2). For both Nash and security selection func tions, we denote the row and column strategies re turned by f by !J(M1, M2) and h(M1, M2), respec tively.\n3 FINITE VALUE ITERATION IN GENERAL S TOCHAS TIC GAMES\nWe begin by describing a finite-horizon, undiscounted value iteration algorithm for general-sum stochastic games, given in Figure 1. This algorithm and its anal ysis are a straightforward generalization of classical finite-horizon value iteration in Markov decision pro cesses, but form a necessary basis for our large state space algorithm and analysis.\nThe algorithm outputs a pair of time-dependent poli cies 1I\"k(s, t), k E {1, 2}, mapping any state s and the time remaining t :::; T to a mixed strategy for player k. The algorithm assumes access to an arbi-\n312 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\ntrary, fixed stochastic game G (specifying the payoff matrices (M1[s], M8[s]) and the transition probabili ties P(s'ls, i, j) for every state), and an arbitrary, fixed Nash selection function f . We will show that the algo rithm outputs a pair of policies that are a Nash equi librium for the T-step stochastic game from any start state. The running time is quadratic in the number of states.\nThe algorithm must maintain backup matrices at each step rather than just backup values. The main obser vation behind the algorithm is the fact that choices be tween the many global Nash equilibria for the T-step stochastic game can be implicitly made by applica tion of an arbitrary Nash selection function. The algo rithm also bears similarity to the infinite-horizon dis counted value iteration algorithm studied by Shapley (1953), which we revisit in Section 5. While Shapley proved that the infinite-horizon discounted algorithm converges to a Nash equilibrium in the zero-sum case (and indeed, as we shall show, does not converge in the general-sum case), the main result of this section shows that our finite-horizon algorithm converges to Nash in the general-sum setting.\nTheorem 1 Let G be any stochastic game, and let f be any Nash selection function. Then the policy pair 7rk(s, t) output by Finite VI(!) is a Nash pair for the T -step stochastic game G from any start state.\nProof: The proof is by induction on the number of steps T. The base case T = 0 is straightforward. If the start state is s0, and this is the only step of play, then clearly the players should play a Nash pair for the game (Ml[so], M2[s0]) defined at so. Since the initialization step given in algorithm Finite VI explicitly specifies that the 7rk(s0, 0) are simply the Nash pair identified by the Nash selection function f , this will be satisfied.\nNow suppose the theorem holds for all T < T0. Let\nus fix the policy played by player 2 to be n2 ( s, t) for all states s and time remaining t � T0, and con sider whether player 1 could benefit by deviating from n2(s, t). By the inductive hypothesis, player 1 cannot benefit by deviating at any point after the first step of play at s0. At the first step of play, since player 2 is playing according to n2 ( s0, T0) , he will play his half of the Nash pair for the game (Ql[so, To], Q2[so, To]) chosen by the Nash selection function f. Furthermore, since play after the first step must be the Nash pair computed by the algorithm for the T0 - 1 step game, the matrices Qk[s0, To] contain the true total average return received by the players under any choice of ini tial actions at so. Therefore player 1 cannot bene fit by deviating from the choice of action dictated by n(so, To), and the theorem is proved. O(Theorem 1)\n4 A S PARS E SAMPLING ALGORITHM IN LARGE GAMES\nAlgorithm Finite VI computes full expectations over next states in order to compute the backup matri ces Qk[s, t], and thus has a running time that scales quadratically with the number of states in the stochas tic game. In contrast, in this section we present a sparse sampling algorithm for on-line planning in stochastic games. The per-state running time of this algorithm has no dependence on the state space size, and thus can be applied even to infinite state spaces, but it depends exponentially on the horizon time T. This is exactly the trade-off examined in the work of Kearns, Mansour and Ng (1999) (see also McAllester and Singh (1999)), who gave an on-line algorithm for the simpler problem of computing near-optimal poli cies in large Markov decision processes. The algorithm presented here can be viewed as a generalization of al gorithm Finite VI that extends the sparse sampling methods of these earlier works to the problem of com-\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 313\nputing Nash equilibria in large or infinite state spaces.\nAlgorithm SparseGame, which is presented in Fig ure 2, takes as input any state s and time T. It assumes access to an arbitrary, fixed Nash selection function f . Rather than directly accessing full transi tion distributions for the underlying stochastic game G, the algorithm only assumes access to the immedi ate payoff matrices Mk [s], and the ability to sample P(·ls, i,j) for any (i,j). This is sometimes referred to as a generative model or simulator for the stochastic game. The algorithm returns a mixed strategy pair (a, (3) to be played at s, along with values Q1 and Q2, whose properties we will discuss shortly.\nThe only aspect of the algorithm that has been left unspecified is the sample size m of next states drawn. Note that due to the recursion, a call to SparseGame will build a recursive tree of size m T, so the overall running time of the algorithm will be exponential in T. The question of interest is how large we must make\nm.\nAlgorithm SparseGame is an on-line or local algo rithm in the sense that it takes a single state s and the amount of time remaining T, and produces a mixed strategy pair (a,/3). This algorithm, however, defines an obvious (global) policy for player k in the stochas tic game: namely, upon arriving in any state s with T steps left in the finite-horizon game, play the mixed strategy for player k output by SparseGame(s, T). Our goal is to prove that for a choice of m with no de pendence on the state space size, these global policies are near-Nash - that is, if one player chooses to al ways play their half of the mixed strategy pairs output by the algorithm at each state visited, then the other player cannot benefit much by deviating from the same policy. This result will be derived via a series of techni cal lemmas. We begin with some necessary definitions.\nFor any s and T, we define Vk[s, T] to be the expected return to player k in the T -step game starting at s when both players play according to SparseGame at every step. For any pure strategy pair (i,j), we define Vk[s,T](i,j) to be the expected return to player kin the T -step game starting at s when the first step play at s is restricted to be ( i, j), and both players play according to SparseGame at every step afterwards. It will be fruitful to view the values Qk returned by the call SparseGame(s, T) as approximations to the Vk [ s, T], and to view the related matrices Q k [ s, T] ( i, j) computed by this call as approximations to the matri ces Vk[s, T](i,j). This view will be formally supported by a lemma shortly.\nWe define Bk [s, T] to be the expected return to player k when we fix their opponent to play according to SparseGame at every step, and player k plays the\nbest response policy - that is, the policy that maxi mizes their return given that their opponent plays ac cording to SparseGame. (It is not important that we be able to compute this best response, since it is merely an artifact of the analysis.) Similarly, we de fine Bk[s,T](i,j) to be the expected return to player k when the first step of play at s is restricted to be ( i, j), we fix the opponent of player k to play accord ing to SparseGame afterwards, player k plays the best response policy afterwards.\nIn this terminology, the main theorem we would like to prove asserts that for any s and T, the Vk [s, T] (the values the players will receive by following the algorithm) are near the Bk[s, T] (the values the players could obtain by deviating from the algorithm if their opponent follows the algorithm), for a sufficiently large sample size that is independent of the state space size.\nTheorem 2 For any s and T, and for any t > 0, provided the sample size m in algorithm SparseGame obeys\nfor some constant c (where n is the number of pure strategies available to both players at any state), then IBk [s, T] - vk [s, T]l :::; 2Tt. In other words, the policy of always playing according to SparseGame is 2Tt Nash for the T -step game starting at s .\nProof: All expectations in the proof are over all the random samples generated by the call SparseGame(s, T) (including the random samples generated by the recursive calls). We begin by not ing:\nIBk[s,T]- Vk[s,T]I E[IBk[s,T]- Vk[s,TJIJ\n< E [ IBk[s, T]- Qk[s, TJI] + E [ IQk[s, T]- Vk[s, TJI)\nby the triangle inequality and linearity of expectation. We now state a lemma bounding the second expecta tion in this sum, and also bounding a related quantity that we will need later.\nLemma 3 For any s and T, and for any t > 0, pro vided the sample size m in algorithm SparseGame obeys\nfor some constant c (where n is the number of pure strategies available to both players at any state}, then\nE [ IQk[s, T]- Vk[s, TJI] :::; t\n314 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000"
    }, {
      "heading" : "Algorithm SparseGame(s, T):",
      "text" : "and\nE [max{IVk[8, T)(i, j)- Qk[s, T](i, j)l}] ::; f. ( •,J)\nProof: By induction on T, we show that the prob ability that IQk[s, T]- Vk[8, T)l :2: >.T is bounded by some quantity 6r, and that for any fixed (i, j), the probability that 1Qk[8, T](i, j)- Vk[s, T](i, j)l :2: >.T is bounded by some quantity s;.,. These bounds will then be used to bound the expectations of interest.\nFor the base case T = 0, we have 6r = J!r = 0 . Assume the inductive hypothesis holds for horizon times 1, ... , T - 1. We define the random variable Uk [8, T]( i, j) = (1Im) 2::;':1 Vk [s£, T-1], where the s£ are the states sampled in the call SparseGame(8, T). By the triangle inequality we may write\n1Qk[8, T](i, j) - Vk[s, T)(i, j)l < IQk[s, T](i, j)- Uk[s, T](i, j)l +\nIUk[s, T](i, j) - Vk[s, T](i, j)l.\nFor the second term in this sum, the probability of the event 1Uk[8, T](i, j)-Vk[s, T](i, j)l :2: >.can be bounded by e-.>.2m via a standard Chernoff bound analysis. In order to bound the first term in the sum, we note that by definition,\nQk[s, T](i, j)- Uk[s, T](i, j) = m\nBy the inductive hypothesis, for each state s£, we have\nwith probability 1-6r_1. This implies that with prob ability 1-mJr-1, the bound applies to all the sampled states s£. In such a case we have that for any fixed (i,j)\nIQk[s, T](i, j)- Uk[8, T](i, j)l::; >.(T- 1).\nTherefore, with probability\n1- J!r ='= 1- (mJT-1 + e-.>.2m)\nwe have that\nIQ k [8, T] ( i, j) - vk [s, T]( i, j) I ::; >.T.\nThis completes the proof for the second half of the inductive hypothesis. For the first half of the inductive hypothesis we have\nIQk[s, T]- Vk[s, T]I::; maxiQk[s, T](i, j)- Vk[s, T](i, j)l. (i,j)\nThis implies that with probability 1 - 6r = 1 - n26!r we have\nwhich completes the proof of the first half of the in ductive hypothesis.\nWe would like to use the inductive hypothesis to prove the lemma. By the definition of 6r we have\nE [IQk[s, T]- Vk[8, TJI] ::; 6r + >.T.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 315\nHere we have used the assumption that Rmax :S 1. Now we need to solve for 8r in the recurrence set up above:\n8r = n 28� = n2(m8r-I + e->.2m) :S (n2mf n2e->.2m .\nTo complete the proof we set .X= Ej2T and\nm > (2T/.X2) log(T/.X2) + Tlog(2n2 /E)+ 2 log(n).\nThus\nm = 0( (T3 / E 2) log(T /E) + T log(n / E)).\nThe second bound of the lemma follows since with probability at most n 28� we have\nmax{/Vk[s, T](i,j)- Qk[s, T](i,j)/} 2 .XT ( >,J)\nwhich completes the proof of the lemma. D(Lemma 3)\nReturning to the main development, we have in curred an approximation cost E by applying Lemma 3, and in exchange have reduced the task of bound ing /Bk[s, T] - Vk[s, T]/ to the problem of bounding E [ /Bk[s,T]- Qk[s,TJ/] . For this we need the follow ing general lemma.\nLemma 4 Let (M1,M2) and (lVh,Mz) be general sum matrix games such that for all pure strategy pairs (i,j), /Mk(i,j)- Mk(i,j)/ :S �fork E {1, 2}. Then if the mixed strategy pair (a, {3) is a Nash in ( M1, Mz) , for any mixed strategy pair (a', {3) we have M1(a',{3)- M1(a,{3) :S �.\nProof: It is easy to see that for any (a, {3) and k E {1,2}, /Mk(a,{3)- Mk(a,f3)/ :S �. Now fix (a,{3) to be some Nash pair in (M1, M2). This implies that for any a' we have M1(a',{3) :S M1(a,{3). Combining the two observations we have that\nM1(a',{3) :S M1(a',{3) + � :S M1(a,{3) + �.\nD(Lemma 4)\nRecall that by definition of SparseGame, Qk[s, T] is obtained by computing a Nash equilibrium (a, {3) (determined by the fixed Nash selection function) of the matrices Qk[s, T](i,j), and setting (for example) Q1[s,T] = QI[s,T](a,{3). Application of Lemma 4 now lets us write\nE [ /Bk[s, T]- Qk[s, TJ/]\n< E [�ax{/Bk[s,T](i,j)- Qk[s,T](i,j)/}] ( >,J)\n< E [m�{/Bk[s,T](i,j)- Vk[s,T](i,j)/}] (>,J)\n+E [max{/Vk[s, T](i,j)- Qk[s, T](i,j)/}] . {i,j)\nThe second expectation in this sum, by Lemma 3, is bounded by E, incurring our second E approximation cost. The first expectation no longer involves any ran dom variables, so we may remove the expectation and concentrate on bounding\n�ax{/Bk[s, T](i,j)- Vk[s, T](i,j)/}. ( >,J)\nFor each fixed ( i, j), the difference\n/Bk [s, T](i, j)- Vk[s, T](i, j)/\nis just like the quantity we are attempting to bound in the theorem, but now we have fixed the first step of play to be ( i, j) and there are only T - 1 steps af terwards. Thus we may apply a simple inductive ar gument:\n/Bk[s, T](i, j) - Vk [s, T](i, j)/\n� P(s'/s, i,j)(Bk[s', T- 1]- Vk[s', T - 1])1 < max{/Bk[s',T- 1]- Vk[s',T- 1]/} .\ns'\nThis implies that we have\nmax{/Bk[s, T]- Vk[s, T]/} s < 2 E + max{/Bk[s, T- 1]- Vk[s, T- 1]}/ s < 2TE.\nD(Theorem 2)\nTo apply Theorem 2, note that if we set E = E1 /T in the statement of the theorem and compute the re sulting m (which is polynomial in T and 1/E'), then the policies computed by the algorithm will be E1-Nash for the T-step stochastic game, and as already empha sized, the total per-state running time will be m T. As mentioned in the introduction, it is important that the players play their respective halves of a common copy of the algorithm (a correlated equilibrium); playing an independent copy may not be a Nash strategy in some games. Briefly, the reason for this is that small vari ations in sampling may result in instabilities in the backup matrices computed. This instability is not a problem as long as both players adhere to a common run of the algorithm.\n5 INFINITE VALUE ITERATION IN S TOCHAS TIC GAMES\nSo far we have presented an exact algorithm for com puting Nash policy pairs in stochastic games with small state spaces, and a sparse sampling algorithm for computing approximately Nash policy pairs in large\n316 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nstochastic games. Both of these algorithms applied to the T-step average return setting. In this section, we examine the situation for the infinite horizon dis counted return setting, and find some curious differ ence with the finite horizon case. In particular, we provide a small generalization of the classical result of Shapley on value iteration in the zero-sum case, but also provide a counterexample proving that the algorithm cannot (in a fairly strong sense) converge to Nash in the general-sum setting.\nIn Figure 3, we present a value iteration algorithm for the infinite-horizon discounted setting. This algorithm is quite similar to Finite VI, except that the output policies 1fk ( s) are now independent of time, and the discount factor is incorporated into the backup matri ces computed at each step.\nWe now present a slight generalization of a classical re sult of Shapley (1953) on the convergence of infinite horizon value iteration. Shapley proved the conver gence in the case of zero-sum stochastic games, where the notions of Nash and security coincide. Here we give a more general analysis proving convergence to a security strategy in the general-sum setting.\nTheorem 5 Let G be any stochastic game, and let f be any security selection function. Then as T -t oo, the policy pair 1fk ( s) output by Infinite VI(T) con verges to a security pair for the infinite-horizon dis counted stochastic game G from any start state.\nA very natural question to ask about algorithm In finite VI is: if we allow the arbitrary security selection function f to be replaced by an arbitrary Nash selec tion function, can we prove the generalization of The orem 5 in which convergence to a security pair is re placed by convergence to a Nash pair? This would pro vide the infinite-horizon discounted analogue to The orem 1 for the finite-horizon case.\nIn the full paper, we provide a counterexample prov ing that such a generalization is not possible. The\ncounterexample is rather strong in several dimensions. First, it applies even to any fixed choice of Nash se lection function. In other words, the difficulty does not lie in the generality of the Nash selection function, and particular choices or conditions on this function will not help. Second, the counterexample is such that there will be infinitely many time steps at which the policies currently computed by the algorithm are not even an approximate Nash pair. Thus, unlike the MDP setting, in stochastic games there is a significant dif ference in the convergence status of value iteration in the finite-horizon and infinite-horizon cases.\n6 REFERENCES\nBoutilier C., Goldszmidt M., and Sabata B. (1999) Contin uous Value Function Approximation for Sequential Bidding Policies. In Proceedings of the 15th Conference on Uncer tainty in Artificial Intelligence.\nBrafman R.I. and Tennenholtz M. (1999). A Near Optimal Polynomial Time Algorithm for Learning in Cer tain Classes of Stochastic Games. In Proceedings of the 16th International Joint Conference on Artificial Intelli gence.\nHu J. and Wellman M.P. (1998). Multiagent reinforce ment learning: Theoretical framework and an algorithm. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 242-250. Kearns M., Mansour Y., and Ng A. (1999). A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. In Proceedings of the 16th In ternational Joint Conference on Artificial Intelligence.\nMcAJJester D. and Singh S. (1999). Approximate Planning for Factored POMDPs using Belief State Simplification. In Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence.\nOwen G. (1995). Game Theory. Academic Press, UK. Shapley L.S. (1953). Stochastic Games. Proceedings of the National Academy of Sciences of the United States of America, 39:1095-1100."
    } ],
    "references" : [ {
      "title" : "Contin­ uous Value Function Approximation for Sequential Bidding Policies",
      "author" : [ "C. Boutilier", "M. Goldszmidt", "B. Sabata" ],
      "venue" : "In Proceedings of the 15th Conference on Uncer­ tainty in Artificial Intelligence",
      "citeRegEx" : "Boutilier et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Boutilier et al\\.",
      "year" : 1999
    }, {
      "title" : "A Near­ Optimal Polynomial Time Algorithm for Learning in Cer­ tain Classes of Stochastic Games",
      "author" : [ "R.I. Brafman", "M. Tennenholtz" ],
      "venue" : "Proceedings of the 16th International Joint Conference on Artificial Intelli­ gence.",
      "citeRegEx" : "Brafman and Tennenholtz,? 1999",
      "shortCiteRegEx" : "Brafman and Tennenholtz",
      "year" : 1999
    }, {
      "title" : "Multiagent reinforce­ ment learning: Theoretical framework and an algorithm",
      "author" : [ "J. Hu", "M.P. Wellman" ],
      "venue" : "Proceedings of the Fifteenth International Conference on Machine Learning, pages 242-250. Kearns M., Mansour Y., and Ng A. (1999). A Sparse",
      "citeRegEx" : "Hu and Wellman,? 1998",
      "shortCiteRegEx" : "Hu and Wellman",
      "year" : 1998
    }, {
      "title" : "Approximate Planning for Factored POMDPs using Belief State Simplification",
      "author" : [ "D. McAJJester", "S. Singh" ],
      "venue" : "Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence.",
      "citeRegEx" : "McAJJester and Singh,? 1999",
      "shortCiteRegEx" : "McAJJester and Singh",
      "year" : 1999
    }, {
      "title" : "Game Theory",
      "author" : [ "G. Owen" ],
      "venue" : "Academic Press, UK. Shapley L.S. (1953). Stochastic Games. Proceedings of the National Academy of Sciences of the United States of America, 39:1095-1100.",
      "citeRegEx" : "Owen,? 1995",
      "shortCiteRegEx" : "Owen",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Given the detailed theoretical and practical under­ standing of single-agent planning and learning in Markov decision processes (MDPs) that has been built over the last decade, one natural line of research is the extension of these algorithms and analyses to a multi­ agent setting (Boutilier, Goldszmidt, and Sabata 1999; Brafman and Tennenholtz 1999; Hu and Wellman 1998).",
      "startOffset" : 282,
      "endOffset" : 373
    }, {
      "referenceID" : 2,
      "context" : "Given the detailed theoretical and practical under­ standing of single-agent planning and learning in Markov decision processes (MDPs) that has been built over the last decade, one natural line of research is the extension of these algorithms and analyses to a multi­ agent setting (Boutilier, Goldszmidt, and Sabata 1999; Brafman and Tennenholtz 1999; Hu and Wellman 1998).",
      "startOffset" : 282,
      "endOffset" : 373
    }, {
      "referenceID" : 4,
      "context" : "It is well-known (Owen 1995) that every game has at least one Nash pair in the space of mixed (but not necessarily pure) strategies, and many games have multiple Nash pairs.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "It is known (Owen 1995) that we can restrict attention to policies (in the infinite-horizon discounted case) or time-dependent policies (in the finite-horizon case) without loss of generality- that is, no advantage can be gained by a player by con­ sidering the history of play.",
      "startOffset" : 12,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "Again, it is known (Owen 1995) that for the infinite discounted case, a Nash pair always exists in the space of policies, and for the average return case, a Nash pair always exists in the space of time-dependent policies.",
      "startOffset" : 19,
      "endOffset" : 30
    } ],
    "year" : 2011,
    "abstractText" : "Stochastic games generalize Markov decision processes (MDPs) to a multiagent setting by allowing the state transitions to depend jointly on all player actions, and having rewards determined by multiplayer matrix games at each state. We consider the prob­ lem of computing Nash equilibria in stochas­ tic games, the analogue of planning in MDPs. We begin by providing a simple general­ ization of finite-horizon value iteration that computes a Nash strategy for each player in general-sum stochastic games. The algo­ rithm takes an arbitrary Nash selection func­ tion as input, which allows the translation of local choices between multiple Nash equilib­ ria into the selection of a single global Nash equilibrium. Our main technical result is an algorithm for computing near-Nash equilibria in large or in­ finite state spaces. This algorithm builds on our finite-horizon value iteration algorithm, and adapts the sparse sampling methods of Kearns, Mansour and Ng (1999) to stochas­ tic games. We conclude by describing a coun­ terexample showing that infinite-horizon dis­ counted value iteration, which was shown by Shapley to converge in the zero-sum case (a result we give extend slightly here), does not converge in the general-sum case.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}