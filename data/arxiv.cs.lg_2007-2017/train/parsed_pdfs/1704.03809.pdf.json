{
  "name" : "1704.03809.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A NEURAL PARAMETRIC SINGING SYNTHESIZER",
    "authors" : [ "Merlijn Blaauw", "Jordi Bonada" ],
    "emails" : [ "merlijn.blaauw@upf.edu", "jordi.bonada@upf.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that’s competitive in both speed and quality."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Currently, many of the more successful singing synthesizers are based on concatenative methods. That is, they transform and concatenate short waveform units selected from an inventory of recordings of a singer. While such systems are the state-of-the-art in terms of sound quality and naturalness (Bonada et al., 2016), they are limited in terms of flexibility and can be difficult to extend or significantly improve upon. On the other hand, machine learning-based approaches, such as statistical parametric methods (Saino et al., 2006; Oura et al., 2010), are much less rigid and do allow for things such as combining data from multiple speakers, model adaptation using small amounts of training data, joint modeling of timbre and expression, etc. Unfortunately, so far these systems have been unable to match the sound quality of concatenative methods, in particular suffering from oversmoothing in frequency and time.\nRecent advances in generative models for Text-to-Speech Synthesis (TTS) using Deep Neural Networks (DNNs), in particular the WaveNet model (van den Oord et al., 2016a), showed that modelbased approaches can achieve sound quality on-par or even beyond that of concatenative systems. This model’s ability to accurately generate raw speech waveform sample-by-sample, clearly shows that oversmoothing is not an issue. While directly modeling the waveform signal is very attractive, we feel that for singing voice the more traditional approach of using a parametric vocoder is better suited. This is in part because singing voice has a relatively wide range of possible pitch and timbre combinations which in turn result in a wide range of possible waveforms, potentially requiring very large amounts of (costly) training data. Using a parametric vocoder effectively separates pitch and timbre, thus significantly simplifying the problem of synthesizing any melody. While a vocoder unavoidably introduces some degradation in sound quality, we consider the degradation introduced by current models is still a more important factor. Thus, if we can improve the quality of the generative model, we should be able to achieve a quality closer to the upper bound the vocoder can provide, i.e. round-trip vocoder analysis-synthesis.\nar X\niv :1\n70 4.\n03 80\n9v 1\n[ cs\n.S D\n] 1\n2 A\npr 2\n01 7"
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Our method is heavily based on a class of fully-visible probabilistic autoregressive generative models that use neural networks with similar architectures. This type of model was first proposed to model natural images (PixelCNN) (van den Oord et al., 2016b; Reed et al., 2017; Salimans et al., 2017), but was later also applied to modeling raw audio waveform (WaveNet) (van den Oord et al., 2016a), video (Video Pixel Networks) (Kalchbrenner et al., 2016b) and text (ByteNet) (Kalchbrenner et al., 2016a).\nThe SampleRNN (Mehri et al., 2017) model proposes an alternative architecture for unconditional raw waveform generation based on multi-scale hierarchical Recurrent Neural Networks (RNNs) rather than dilated Convolutional Neural Networks (CNNs). Other works (Sotelo et al., 2017; Arik et al., 2017) have extended these two architectures to include attention mechanisms to allow performing end-to-end TTS, i.e. generation conditioned on unaligned orthographic text or phonetic sequences rather than aligned linguistic features.\nMore traditional neural parametric speech synthesizers tend to be based on feed-forward architectures such as DNNs and Mixture Density Networks (MDNs) (Zen & Senior, 2014), or on recurrent architectures such as Long Short-Term Memory RNNs (LSTM-RNNs) (Zen & Sak, 2015). Feedforward networks learn a frame-wise mapping between linguistic and acoustic features, thus potentially producing discontinuous output. This is often partly mitigated by predicting static, delta and delta-delta feature distributions combined with a parameter generation algorithm that maximizes output probability (Tokuda et al., 2000). Recurrent architectures avoid this issue by propagating hidden states (and sometimes the output state) over time. In contrast, autoregressive architectures like the one we propose make predictions based on past predictions of acoustic features, that is, outputs are fed back as inputs during generation.\nThere have been several works proposing different types of singing synthesizers. The more prominent of which are based on concatenative methods (Bonada & Serra, 2007; Bonada et al., 2016) and statistical parametric methods centered around Hidden Markov Models (HMMs) (Saino et al., 2006; Oura et al., 2010). While this work focuses on the generation of timbre and does not consider modeling of expression, an important difference between these methods is that statistical models allow joint modeling of timbre and expression from natural singing (Mase et al., 2010; Oura et al., 2012), unlike concatenative methods which typically use disjoint modeling and specialized recordings. Many of the techniques developed for HMM-based TTS are also applicable to singing synthesis, e.g. speaker-adaptive training (Shirota et al., 2014). The main drawback of HMM-based approaches is that phonemes are modeled using a small number of discrete states and within each state statistics are constant. This causes excessive averaging, an overly static “buzzy” sound and noticeable state transitions in long sustained vowels in the case of singing. More recently some work has also been done on using feed-forward DNNs for singing synthesis (Nishimura et al., 2016), albeit with a somewhat limited architecture."
    }, {
      "heading" : "3 PROPOSED SYSTEM",
      "text" : "The architecture of our proposed model, its inputs and its outputs are summarized in fig. 1. The model consists of a neural network that takes as input a window of past acoustic features and predicts a probability distribution corresponding to the current acoustic features. Additionally, the network has control inputs in the form of linguistic features, which allow controlling when the model generates which phoneme.\nLike its predecessors, our model is based on the idea of factorizing a joint probability as a product of conditional probabilities with some causal ordering. The conditional probability distributions are predicted by a neural network trained to maximize likelihood of a observation given past observations. To synthesize, predictions are made by sampling the distribution conditioned on past predictions, that is, in a sequential, autoregressive manner.\nHowever, while for other models this factorization is done for individual variables (i.e. waveform samples or pixels), we do so for vectors of variables corresponding to a single frame,\np (x1, . . . ,xT ) = T∏ t=1 p (xt | x<t) , (1)\nwhere xt is an N -dimensional vector of acoustic features [xt,1, . . . , xt,N ], and T is the length of the signal. In our case we consider the variables within a frame to be conditionally independent,\np (xt | x<t) = N∏ i=1 p (xt,i | x<t) . (2)\nIn other words, a single neural network predicts the parameters of a multivariate conditional distribution with diagonal covariance, corresponding to the acoustic features of a single frame.\nThe main reason for choosing this model is that, unlike raw audio waveform, features produced by a parametric vocoder have two dimensions, similar to (single channel) images. However, unlike images, these two dimensions are not both spatial dimensions, but rather time-frequency dimensions. The translation invariance that 2D convolutions offer is an undesirable property for the frequency (or cepstral quefrency) dimension. Therefore we model the features as 1D data with multiple channels. Note that these channels are only independent within the current frame; the prediction of each of the features in the current frame still depends on all of the features of all past frames (within the receptive field). This can be explained easily as all input channels of the initial causal convolution contribute to all resulting feature maps, and so on for the other convolutions.\nPredicting all channels at once rather than one-by-one simplifies the models as it avoids the need for masking channels and separating them in groups. This approach is similar to (Salimans et al., 2017) where all three RGB channels of a pixel in an image are predicted at once, although in our work we do not incorporate additional linear dependencies between channel means.\nThe network we propose, depicted in fig. 1, shares most of its architecture with WaveNet. To speed up training, we use gated convolutional units instead of gated recurrent units such as LSTM. The\ninput is fed through an initial causal convolution which is then followed by stacks of 2x1 dilated convolutions (Yu & Koltun, 2016) where the dilation factor is doubled for each layer. This allows exponentially growing the model’s receptive field, while linearly increasing the number of required parameters. To increase the total non-linearity of the model without excessively growing its receptive field, the dilation factor is increased up to a limit and then the sequence is repeated. While our architecture uses a number of layers closer to PixelCNN than WaveNet, like both these models we use residual and skip connections to facilitate training deeper networks (He et al., 2016). As we wish to control the synthesizer by inputting lyrics, we use a conditional version of the model. At every layer, before the gated non-linearity, feature maps derived from linguistic features are summed to the feature maps from the layer’s main convolution. In our case we do the same thing at the output stack, similar to (Reed et al., 2017)."
    }, {
      "heading" : "3.1 MULTI-STREAM ARCHITECTURE",
      "text" : "Most parametric vocoders separate the speech signal into several components. In our case we obtain three feature streams; a harmonic spectral envelope, an aperiodicity envelope and a voiced/unvoiced decision (pitch is is provided as a control input). These components are largely independent, but their coherence is important (e.g. synthesizing a harmonic component corresponding to a voiced frame as unvoiced will generally cause artifacts). Rather than jointly modeling all data streams with a single model, we decided to model these components using independent networks. This allows us to use different architectures for each stream and, more importantly, avoids one stream possibly interfering with the other streams. For instance, the harmonic component is by far the most important, therefore we wouldn’t want any other jointly modeled stream potentially reducing model capacity dedicated to this component.\nTo encourage predictions being coherent, we use predictions of one network as the input of another. We currently condition the voiced/unvoiced decision on the harmonic component and the aperiodic component on both harmonic component and voiced/unvoiced decision. All the networks are similar, but have slightly different hyper-parameters. The voiced/unvoiced decision network has a Bernoulli output distribution rather than a mixture density (see 3.2). There are many other possible variations on this scheme, but we did not investigate these as we found the current setup to be satisfactory."
    }, {
      "heading" : "3.2 CONSTRAINED MIXTURE DENSITY OUTPUT",
      "text" : "A categorical output distribution is used in many of the architectures on which we base our model. The advantage of this approach is that it does not make any prior assumptions about the distribution of the data, allowing things such as multiple modes, skewed distributions, truncated distributions, and so on. Drawbacks of this approach include an increase in model parameters, a loss of relative ordering of values, and the need to discretize data which is not naturally discrete or has high bitdepth.\nBecause our model predicts an entire frame at once, the issue of increased parameter count is aggravated. Instead, we opted to use a mixture density output similar to (Salimans et al., 2017). This decision was partially motivated because in earlier, bin-wise versions of our model with softmax output (Blaauw & Bonada, 2016), we noted the predicted distributions were generally quite close to Gaussian or skewed Gaussian. In our proposed model we use a mixture of four Gaussian components, constrained in a way so that there are only four free parameters (mean, variance, skewness and a shape parameter). We found such constraints to be useful to avoid certain pathological distributions, and in our case explicitly not allowing multi-modal distributions was helpful to improve results. We also found this approach to speed up convergence compared to using categorical output."
    }, {
      "heading" : "3.3 ROBUST GENERATION BY REGULARIZATION",
      "text" : "One of the principal issues we found when training our model was that training (or validation) log likelihood is often not very indicative of the final synthesis quality. The most noticeable symptom of this was that the synthesizer not always followed its control input, and changed phonemes in unpredictable ways.\nOur explanation for this is that it is because the training objective does not exactly match the generation setting. During training many samples are predicted in parallel, conditioned on actual past observations. However, in generation, samples are generated one-by-one in sequential order, each one\nconditioned on past predictions rather than past observations. Thus the model may overfit to observations from the dataset, causing generation to fail whenever predictions diverge even slightly. We expect that this issue is much more noticeable in our case because we use relatively small datasets. Additionally, the data from a parametric vocoder is inherently less structured than other types of data such as raw waveforms or natural images, that is, it tends to be smoothly varying in time and have low noise, etc.\nIn order to make the generation process more robust to prediction errors, we propose to use a denoising objective, L = −Ext Ex̃<t∼p(x̃<t|x<t)[log p(xi|x̃<t)], (3) where p(x̃|x) is a Gaussian corruption distribution,\np(x̃|x) = N (x̃;x, λI), (4) with noise level λ ≥ 0. That is, Gaussian noise is added to the input of the network, but the network is trained to predict the uncorrupted target.\nWhen sufficiently large values of λ are used, this technique is very effective for solving the issue of overfitting. However, the generated output can also become noticeably noisier. One way to reduce this undesirable side effect is to apply some post processing to the predicted output distribution, much in the same vein as the temperature softmax used in similar models, e.g. (Reed et al., 2017). Another way to view this is that as temperature goes towards zero, parameter generation goes towards maximum likelihood, which is a commonly used criteria for parameter generation.\nWe have also tried other regularization techniques, such as drop-out, but found them to be ultimately inferior to simply injecting input noise. One possible explanation for this is that adding noise to observed context is similar to the noise introduced by prediction errors during autoregressive generation."
    }, {
      "heading" : "3.4 FAST GENERATION ON CPU",
      "text" : "One drawback of autoregressive models such as the one we propose is that generation is inherently sequential and thus cannot exploit massively parallel hardware such as modern GPUs. Naive implementations of the generation algorithm thus tend to be much slower than for most other types of models. We have independently developed a fast generation algorithm using caching techniques similar to those used in (Ramachandran et al., 2017; Arik et al., 2017). Combining this with our frame-wise model, we can achieve speeds of 20-35x real-time on CPU. These runtimes, and low memory and disk footprint make our system competitive with most existing systems in terms of deployability."
    }, {
      "heading" : "4 EXPERIMENTAL CONDITIONS",
      "text" : ""
    }, {
      "heading" : "4.1 ACOUSTIC AND LINGUISTIC FRONT-END",
      "text" : "We use a fairly standard acoustic front-end based on the WORLD vocoder (Morise et al., 2016) with a 32 kHz sample rate and 5 ms hop time. We further reduce dimensionality of the harmonic and aperiodic components to 60 and 5 coefficients respectively by transforming them to mel-generalized coefficients (MGCs) (Tokuda et al., 1994) with frequency warping coefficient α = 0.45 and pole/zero\nweighting γ = 0. To facilitate interpretation, the cepstral features are finally converted back to warped frequency features.\nThe linguistic features we use are relatively simple compared to most TTS systems as we do not model prosody. We use previous, current and next phoneme identity as one-hot encoded vectors. Additionally, we include the normalized position of the current frame within the current phoneme as a 3-state coarse coded vector, roughly corresponding to the probability of being in the beginning, middle or end of the phoneme. We do not use any features related to duration as our datasets have relatively uniform phoneme durations which may vary wildly from synthesis durations (esp. for vowels). The linguistic features are aligned to the acoustic features using a speaker-dependent Hidden Semi-Markov Model (HSMM) trained using deterministic annealing (Ueda & Nakano, 1998)"
    }, {
      "heading" : "4.2 DATASETS",
      "text" : "In the initial evaluation of our system, we use three voices; one English male and female (M1, F1), and one Spanish female (F2). The recordings consist of short sentences sung at a single pitch and an approximately constant cadence. The sentences were selected to favor high diphone coverage. For the Spanish dataset there are 123 sentences, for the English datasets 524 sentences (approx. 16 and 35 minutes respectively, including silences). Note that these datasets are very small compared to the datasets typically used to train TTS systems, but this is a realistic constraint given the difficulty and cost of recording a professional singer."
    }, {
      "heading" : "4.3 CONFIGURATION AND HYPER-PARAMETERS",
      "text" : "We use an initial causal convolution operating on 10 past values. Then we use a stack of convolutions with dilation factors [1, 2, 4, 1, 2]. This results in a total receptive field of 105 ms. For the harmonic feature stream, we use 100 channels in the convolutions, 240 channels for the skip connections. For the aperiodicity feature stream, we use 20 channels in the convolutions, 20 channels for the skip connections. The voiced/unvoiced decision stream uses 20 channels in the convolutions and 4 channels in the skip connections. All networks use a single output stage with tanh non-linearity. We use mini-batches of 16 sequences, each with a length of 210 frames. We use the Adam optimizer (Kingma & Ba, 2015) with an initial learning rate of 5 × 10−4. The model was trained for a total of 1000 epochs, taking around 8 hours on a single Titan X Pascal GPU. The entire multi-stream network contains approx. 747k trainable parameters."
    }, {
      "heading" : "5 EVALUATION",
      "text" : "We compare our system (“NPSS”) against two alternative systems. The first is a HMM-based system (“HTS”) build using the HTS toolkit (version 2.3) (Zen et al., 2007). Mostly standard settings were used, except for a somewhat simplified context dependency (just the two previous and two following phonemes). The second system (“IS16”) (Bonada et al., 2016) is based on concatenative synthesis and was the highest rated system in the Interspeech 2016 Singing Synthesis Challenge.\nIn table 1 we show some quantitative results comparing there systems to our proposed method. These metrics were compute over a 10% validation split, silence frames and frames with mismatched voiced/unvoiced decision between target and prediction were excluded. The IS16 system is omitted from this table because the low redundancy of diphones in the dataset would force this system to find replacements for some diphones in the held-out validation utterances, giving the system an unfair disadvantage.\nAfter the quantitative experiments we re-trained our models using the full datasets and synthesized one song for each of the three voices. To be able to better compare the different systems, we used pitch and phonetic timings from target recordings. From each song we extracted two short (under 10s) excerpts and made versions with and without background music. These stimuli were presented in 24 pairs to 18 participants who rated their preference between the different systems. The results of this listening tests are summarized in figure 3. Full versions of the songs used in the listening test are available at: http://www.dtic.upf.edu/∼mblaauw/IS2017 NPSS/"
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "We presented a singing synthesizer based on neural networks that can be successfully trained on relatively small amounts of data. Listening tests showed a notable preference for our system compared to a statistical parametric system, and a moderate preference compared to a concatenative system. Interestingly, the quantitative metrics do not reflect this, showing nearly identical results. One explanation for this is that the frame-wise metrics used do not take into account variations in time which can be important perceptually. Compared to the HMM-based baseline system, we consider our method to produce less oversmoothing, sounding noticeably less static and “buzzy”, reproducing consonants more faithfully, and producing smoother transitions in long sustained vowels typical of singing. Compared to the concatenative system, we feel our method produces a similar overall sound quality. However, in certain segments, such as fast singing, subtle errors in segmentation become evident in the concatenative system. Generating phonetic contexts not in the training set is also generally handled better by our system compared to having to find a phonetic replacement. Thanks to its fast CPU-based generation algorithm, our proposed system becomes a feasible option for many practical applications. We hope that in the near future approaches based on neural networks will open pathways to many interesting improvements in singing synthesis. In particular the area of multi-speaker training is promising, as it might help to overcome the issue of limited dataset sizes typical of singing voice."
    }, {
      "heading" : "7 ACKNOWLEDGEMENTS",
      "text" : "We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. We also thank Zya for providing the English datasets used. Voctro Labs provided the Spanish dataset and the implementation of the fast generation algorithm. This work is partially supported by the Spanish Ministry of Economy and Competitiveness under the CASAS project (TIN2015-70816-R)."
    } ],
    "references" : [ {
      "title" : "A singing synthesizer based on PixelCNN",
      "author" : [ "Merlijn Blaauw", "Jordi Bonada" ],
      "venue" : "http://www.dtic.upf. edu/∼mblaauw/MdM NIPS seminar/,",
      "citeRegEx" : "Blaauw and Bonada.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blaauw and Bonada.",
      "year" : 2016
    }, {
      "title" : "Synthesis of the singing voice by performance sampling and spectral models",
      "author" : [ "Jordi Bonada", "Xavier Serra" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Bonada and Serra.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bonada and Serra.",
      "year" : 2007
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation in linear time",
      "author" : [ "Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aäron van den Oord", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "CoRR, abs/1610.10099,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Lei Ba" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR-15),",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "SampleRNN: An unconditional end-to-end neural audio generation model",
      "author" : [ "Soroush Mehri", "Kundan Kumar", "Ishaan Gulrajani", "Rithesh Kumar", "Shubham Jain", "Jose Sotelo", "Aaron C. Courville", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Mehri et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Mehri et al\\.",
      "year" : 2017
    }, {
      "title" : "WORLD: A vocoder-based high-quality speech synthesis system for real-time applications",
      "author" : [ "Masanori Morise", "Fumiya Yokomori", "Kenji Ozawa" ],
      "venue" : "IEICE Transactions on Information and Systems,",
      "citeRegEx" : "Morise et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Morise et al\\.",
      "year" : 2016
    }, {
      "title" : "Pitch adaptive training for HMM-based singing voice synthesis",
      "author" : [ "Keiichiro Oura", "Ayami Mase", "Yoshihiko Nankaku", "Keiichi Tokuda" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Oura et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Oura et al\\.",
      "year" : 2012
    }, {
      "title" : "Fast generation for convolutional autoregressive models",
      "author" : [ "Prajit Ramachandran", "Tom Le Paine", "Pooya Khorrami", "Mohammad Babaeizadeh", "Shiyu Chang", "Yang Zhang", "Mark Hasegawa-Johnson", "Roy Campbell", "Thomas Huang" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Ramachandran et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Ramachandran et al\\.",
      "year" : 2017
    }, {
      "title" : "Generating interpretable images with controllable structure",
      "author" : [ "Scott Reed", "Aäron van den Oord", "Nal Kalchbrenner", "Victor Bapst", "Matt Botvinick", "Nando de Freitas" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Reed et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2017
    }, {
      "title" : "An HMM-based singing voice synthesis system",
      "author" : [ "Keijiro Saino", "Heiga Zen", "Yoshihiko Nankaku", "Akinobu Lee", "Keiichi Tokuda" ],
      "venue" : "In Interspeech 2006 - ICSLP, Ninth International Conference on Spoken Language Processing,",
      "citeRegEx" : "Saino et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Saino et al\\.",
      "year" : 2006
    }, {
      "title" : "PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications",
      "author" : [ "Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Salimans et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2017
    }, {
      "title" : "Integration of speaker and pitch adaptive training for HMM-based singing voice synthesis",
      "author" : [ "Kanako Shirota", "Kazuhiro Nakamura", "Kei Hashimoto", "Keiichiro Oura", "Yoshihiko Nankaku", "Keiichi Tokuda" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Shirota et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shirota et al\\.",
      "year" : 2014
    }, {
      "title" : "Char2Wav: End-to-end speech synthesis",
      "author" : [ "José Sotelo", "Soroush Mehri", "Kundan Kumar", "João Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Sotelo et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sotelo et al\\.",
      "year" : 2017
    }, {
      "title" : "Mel-generalized cepstral analysis - a unified approach to speech spectral estimation",
      "author" : [ "Keiichi Tokuda", "Takao Kobayashi", "Takashi Masuko", "Satoshi Imai" ],
      "venue" : "In Proceedings of the International Conference on Spoken Language Processing (ICSLP-94),",
      "citeRegEx" : "Tokuda et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Tokuda et al\\.",
      "year" : 1994
    }, {
      "title" : "Speech parameter generation algorithms for HMM-based speech synthesis",
      "author" : [ "Keiichi Tokuda", "Takayoshi Yoshimura", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura" ],
      "venue" : "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-00),",
      "citeRegEx" : "Tokuda et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Tokuda et al\\.",
      "year" : 2000
    }, {
      "title" : "Deterministic annealing EM algorithm",
      "author" : [ "Naonori Ueda", "Ryohei Nakano" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "Ueda and Nakano.,? \\Q1998\\E",
      "shortCiteRegEx" : "Ueda and Nakano.",
      "year" : 1998
    }, {
      "title" : "WaveNet: A generative model for raw audio",
      "author" : [ "Aäron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew W. Senior", "Koray Kavukcuoglu" ],
      "venue" : "CoRR, abs/1609.03499,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional image generation with PixelCNN decoders",
      "author" : [ "Aäron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions",
      "author" : [ "Fisher Yu", "Vladlen Koltun" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Yu and Koltun.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yu and Koltun.",
      "year" : 2016
    }, {
      "title" : "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis",
      "author" : [ "Heiga Zen", "Hasim Sak" ],
      "venue" : "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),",
      "citeRegEx" : "Zen and Sak.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zen and Sak.",
      "year" : 2015
    }, {
      "title" : "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis",
      "author" : [ "Heiga Zen", "Andrew Senior" ],
      "venue" : "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),",
      "citeRegEx" : "Zen and Senior.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zen and Senior.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "On the other hand, machine learning-based approaches, such as statistical parametric methods (Saino et al., 2006; Oura et al., 2010), are much less rigid and do allow for things such as combining data from multiple speakers, model adaptation using small amounts of training data, joint modeling of timbre and expression, etc.",
      "startOffset" : 93,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "This type of model was first proposed to model natural images (PixelCNN) (van den Oord et al., 2016b; Reed et al., 2017; Salimans et al., 2017), but was later also applied to modeling raw audio waveform (WaveNet) (van den Oord et al.",
      "startOffset" : 73,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "This type of model was first proposed to model natural images (PixelCNN) (van den Oord et al., 2016b; Reed et al., 2017; Salimans et al., 2017), but was later also applied to modeling raw audio waveform (WaveNet) (van den Oord et al.",
      "startOffset" : 73,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "The SampleRNN (Mehri et al., 2017) model proposes an alternative architecture for unconditional raw waveform generation based on multi-scale hierarchical Recurrent Neural Networks (RNNs) rather than dilated Convolutional Neural Networks (CNNs).",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "Other works (Sotelo et al., 2017; Arik et al., 2017) have extended these two architectures to include attention mechanisms to allow performing end-to-end TTS, i.",
      "startOffset" : 12,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : "This is often partly mitigated by predicting static, delta and delta-delta feature distributions combined with a parameter generation algorithm that maximizes output probability (Tokuda et al., 2000).",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 10,
      "context" : ", 2016) and statistical parametric methods centered around Hidden Markov Models (HMMs) (Saino et al., 2006; Oura et al., 2010).",
      "startOffset" : 87,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "While this work focuses on the generation of timbre and does not consider modeling of expression, an important difference between these methods is that statistical models allow joint modeling of timbre and expression from natural singing (Mase et al., 2010; Oura et al., 2012), unlike concatenative methods which typically use disjoint modeling and specialized recordings.",
      "startOffset" : 238,
      "endOffset" : 276
    }, {
      "referenceID" : 12,
      "context" : "speaker-adaptive training (Shirota et al., 2014).",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "This approach is similar to (Salimans et al., 2017) where all three RGB channels of a pixel in an image are predicted at once, although in our work we do not incorporate additional linear dependencies between channel means.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "While our architecture uses a number of layers closer to PixelCNN than WaveNet, like both these models we use residual and skip connections to facilitate training deeper networks (He et al., 2016).",
      "startOffset" : 179,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "In our case we do the same thing at the output stack, similar to (Reed et al., 2017).",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "Instead, we opted to use a mixture density output similar to (Salimans et al., 2017).",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "(Reed et al., 2017).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "We have independently developed a fast generation algorithm using caching techniques similar to those used in (Ramachandran et al., 2017; Arik et al., 2017).",
      "startOffset" : 110,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "We use a fairly standard acoustic front-end based on the WORLD vocoder (Morise et al., 2016) with a 32 kHz sample rate and 5 ms hop time.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "We further reduce dimensionality of the harmonic and aperiodic components to 60 and 5 coefficients respectively by transforming them to mel-generalized coefficients (MGCs) (Tokuda et al., 1994) with frequency warping coefficient α = 0.",
      "startOffset" : 172,
      "endOffset" : 193
    } ],
    "year" : 2017,
    "abstractText" : "We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that’s competitive in both speed and quality.",
    "creator" : "LaTeX with hyperref package"
  }
}