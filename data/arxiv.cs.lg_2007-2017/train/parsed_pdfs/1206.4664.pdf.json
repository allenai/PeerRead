{
  "name" : "1206.4664.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tighter Variational Representations of f-Divergences via Restriction to Probability Measures",
    "authors" : [ "Avraham Ruderman", "Mark D. Reid", "Darío García-García" ],
    "emails" : [ "AVRAHAM.RUDERMAN@NICTA.COM.AU", "MARK.REID@ANU.EDU.AU", "DARIO.GARCIA@ANU.EDU.AU", "JAMES.PETTERSON@NICTA.COM.AU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "An important class of discrepancy measures between probability distributions is the family of f -divergences (also known as Ali-Silvey (Ali & Silvey, 1966) or Csisz·r divergences (Csiszár, 1967)). These include the variational divergence, the Hellinger distance, and the well-known Kullback-Leibler (KL) divergence. Estimates of these measures from two i.i.d. samples can be used to test whether or not those samples come from similar distributions. Due to the convexity of the eponymous f function defining them, f -divergences can be expressed variationally as the maximum of an optimisation problem. This variational representation has been recently used for f -divergence estimation (Nguyen et al., 2010; 2007; Kanamori et al., 2011) homogeneity testing (Kanamori et al., 2011) and parameter estimation (Broniatowski & Keziou, 2009).\nThis paper stems from a simple observation: the vari-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\national representation currently being used in the literature fails to take into account the fact that divergences are defined between probability distributions. In an analysis similar to that of Altun and Smola’s (Altun & Smola, 2006), we present a derivation of a quantifiably tighter variational representation of f -divergences. Our derivation restricts the convex dual used in the variational representation of an f - divergence to the space of probability distributions. This generalises similar observations for the specific case of the KL divergence, such as Banerjee’s compression lemma (Banerjee, 2006). These results are given in the remainder of this section.\nClearly, use of this tighter variational representation in any algorithm relying on the previous bounds would be advantageous. Thus, we suspect this tighter bound will find broad applicability as the weaker variational form is already widely used. As an example of this, in section 2. we derive a general dual program for an RKHS estimator which has a simple closed form for any f -divergence. We show this dual program has a natural interpretation as a trade off between the minimisation of the f -divergence in question and the minimum mean discrepancy (MMD) (Gretton et al., 2008) between the empirical distributions. Experiments in section 3 confirm that the tighter variational form underlying our approach leads to better KL divergence estimates than the well known estimator of (Nguyen et al., 2010; 2007) which is based on the looser representation. We also provide an empirical comparison to other state of the art methods for f -divergence estimation."
    }, {
      "heading" : "1.1. Convex Duality",
      "text" : "We briefly introduce some key ideas from convex duality. The reader is referred to (Barbu & Precupanu,\n1986) for details. For functions f : X → R defined over a Banach space X the (Fenchel or convex) dual f? : X? → R of a function is defined over the dual space X? by f?(x?) := supx∈X〈x?, x〉 − f(x) where 〈·, ·〉 is the dual pairing of X and X?. In finite dimensional spaces such as Rd the dual space is also Rd and 〈·, ·〉 is the usual inner product. The bidual f?? : X → R of f is just the dual of f? (restricted to X), that is, f??(x) = supx?∈X?〈x, x?〉 − f?(x?). For convex, lower semi-continuous (l.s.c.) functions f the bidual is the identity transformation, that is, f?? = f . As seen below, this fact forms the basis of many variational representations of operators defined by convex functions.1\nWe make use of a few specific, one-dimensional instances of duals. Specifically, when f(t) = |t− 1| we have f?(t?) = t? for t? ∈ [−1, 1]; and when f(t) = − ln t for t > 0 we have f?(t?) = −1 − ln t? for t? < 0. Further details and properties of convex duals can be found in texts on convex analysis, e.g., (Hiriart-Urruty & Lemaréchal, 2001).\nOne obvious but important property we make use of is that the restriction of a supremum leads to smaller optima. Specifically, if X ′ ⊆ X then supx∈X′ φ(x) ≤ supx∈X φ(x). When applied to convex duals, this means if R ⊆ X? is a restriction of the dual space then\nf(x) ≥ fR(x) := sup x?∈R 〈x?, x〉 − f?(x?) for all x∈X.\n(1)\n1.2. Variational Approximations of f-divergences\nAn f -divergence is defined via a convex function f : [0,∞)→ R satisfying f(1) = 0. Given such a function, the f -divergence from a finite measure P to a distribution Q defined on a common space X is defined2 as\nIf (P,Q) := EQ [ f ( dP\ndQ\n)] =\nˆ X f\n( dP\ndQ (x)\n) dQ(x)\nif P Q and +∞ otherwise. We will refer to the definition above as the general (or unrestricted) f - divergence in contrast to the restricted f -divergence that is only defined when P and Q are both probability distributions. When necessary, the restricted\n1For finite dimensional spaces, this is, in some sense, the only dual that can be used for the kind of variational representations we are interested in (see (Artstein-Avidan & Milman, 2009) for details).\n2The choice of order of the arguments P and Q is arbitrary and other authors, notably (Nguyen et al., 2010), define f -divergence in terms of dQ/dP .\nf -divergence will be distinguished by a superscript R: IRf (P,Q). We emphasise this distinction to later show how a tighter variational representation can be obtained from explicitly taking into account the restriction. Several common divergences are members of this class: the variational divergence is obtained by choosing f(t) = |t − 1|, Hellinger divergence via f(t) = √ t2 − 1, and the KL divergence via f(t) = t ln t (see, e.g., (Reid & Williamson, 2011)). For technical reasons we also require f to be lower semi-continuous. All f -divergences discussed above and used in practice satisfy this condition.\nAs in (Altun & Smola, 2006; Barbu & Precupanu, 1986; Broniatowski & Keziou, 2009), we now wish to consider f -divergences as acting over spaces of functions. Given a measure µ over X (with some σ-algebra), the norms ‖g‖1 := ´ X |g| dµ and ‖g‖∞ := inf {K ≥ 0 : |g(x)| ≤ K for µ-almost all x} can be used to define the space of absolutely integrable functions L1(µ) := {g : X → R : ‖g‖1 <∞} and its dual space L∞(µ) := {g : X → R : ‖g‖∞ <∞} of functions with bounded essential supremum. The space of probability densities w.r.t. µ will be denoted ∆(µ) := { g ∈ L1(µ) : g ≥ 0, ‖g‖1 = 1 } . On finite domains X = {x1, . . . , xn}, the space of densities will be denoted ∆n. A general f -divergence can be seen as acting on L1(Q) by defining If,Q (r) := EQ [f (r)] for all r ∈ L1(Q). The restricted f -divergence is then just IRf,Q(r) := If,Q(r) when r ∈ ∆(Q) and +∞ otherwise.\nAs shown below, these functions are convex and lower semi-continuous and therefore admit dual representations. As explored in (Nguyen et al., 2010; Altun & Smola, 2006) and in section 2 below, variational representations such as these readily admit approximation\ntechniques via a restriction of the optimisation for If,Q to functions from F ⊆ L∞(Q) (e.g., an RKHS). Our main result is that the restricted variational form guarantees tighter lower bounds on f -divergences than the unrestricted form for any choice of function class.\nTheorem 1. For any distributions P and Q such that P Q we have\nIf (P,Q) = sup φ∈L∞(Q)\nEP [φ]− ( IRf,Q )? (φ) (2)\nand for any choice of function class F ⊆ L∞(Q)\nIf (P,Q) ≥ sup φ∈F\nEP [φ]− ( IRf,Q )? (φ)\n≥ sup φ∈F EP [φ]− EQ [f?(φ)] . (3)\nWe illustrate the result of Theorem 1 in Figure 1. While our new expression and that of (Nguyen et al., 2010) have the same supremum, our expression is closer to the supremum at every point φ ∈ L∞(Q). In particular, if one restricts φ to vary over a subset F ⊂ L∞ then the supremum of our expression over F will yield a better estimate of If (P,Q).\nProof. We first establish that If,Q is a convex function over L1(Q) via Proposition 2.7 in (Barbu & Precupanu, 1986) which states that for any finite measure Q over X and any proper convex l.s.c. function f : [0,∞) → R ∪ {+∞} the function F : Lq(Q) → (−∞,+∞] is convex and l.s.c when defined by F (u) :=´ X f(u) dQ for f(u) ∈ L1(Q) and +∞ otherwise. Since\nIf,Q(r) = ´ X f(r) dQ and f satisfies the conditions of the proposition, we have that If,Q is convex and l.s.c.\nThe variational representation for If,Q is then obtained by using Lemma 4.5.8 of (Dembo & Zeitouni, 2009). This states that for any Banach spaceX with dualX?, if F : X → R∪{+∞} is convex and l.s.c. then F (x) = supx?∈X? {〈x?, x〉 − F ?(x?)}. Since If,Q is convex and l.s.c. on L1(Q) we see that\nIf,Q (r) = sup φ∈L∞(Q)\n{ EQ [φr]− (If,Q)? (φ) } = (If,Q)?? (r)\nfor all r ∈ L1(Q) since L∞(Q) is the dual space to L1(Q). In particular, we can conclude that for all r ∈ L1(Q) and all φ ∈ L∞(Q)\nIf,Q(r) ≥ EQ [φr]− (If,Q)? (φ). (4)\nNow, since IRf,Q is defined to be If,Q on ∆(Q) and +∞ on the rest of L1(Q) we see that for all φ ∈ L∞(Q)\n( IRf,Q )? (φ) = sup\nr∈L1(Q)\n{ EQ [φr]− IRf,Q (r) } = sup p∈∆(Q) {EQ [φp]− If,Q (p)} . (5)\nThis implies that for all φ ∈ L∞(Q) and for all p ∈ ∆(Q) we have ( IRf,Q )? (φ) + If,Q(p) ≥ EQ [φp] which rearranged and optimised over φ ∈ L∞(Q) yields (2). We now observe that (5) is just a constrained version of the optimisation defining (If,Q)?, and so by (1) we must have ( IRf,Q )? ≤ (If,Q)?. Substituting this into (4) we see that for any p ∈ ∆ (Q) , φ ∈ L∞ (Q)\nIf,Q (p) ≥ EQ [φp]− ( IRf,Q )? (φ) ≥ EQ [φp]−(If,Q)? (φ) .\nTaking supremums over these inequalities for φ ∈ F ⊆ L∞(Q) yields (3), as required.\nFor the particular case of KL divergence estimation the above theorem specialises to show that for any choice of φ ∈ L∞(Q)\nKL(P,Q) ≥ EP [φ]−lnEQ [ eφ ] ≥ EP [φ]−EQ [ eφ + 1 ] .\nThe first inequality is the well-known representation of the KL divergence in the large deviations literature (Donsker & Varadhan, 1983) which has been rediscovered in the PAC-Bayes community as the compression lemma (Banerjee, 2006). Although the second inequality can be obtained immediately from the fact that − log (y) ≥ −y + 1 for all y > 0, Theorem (1) shows that a similar result holds for general f -divergences."
    }, {
      "heading" : "2. Estimation using RKHS methods",
      "text" : "Given two samples3 Xn := {x1, . . . , xn} and Yn := {y1, . . . , yn} from P and Q respectively, we wish to use the empirical measures Pn := 1n ∑n i=1 δxi and Qn := 1 n ∑n i=1 δyi as proxies for P and Q. However, since we no longer have Pn Qn some form of smoothing is required. We make use of the restricted dual variational representation of the f -divergence and choose a sufficiently constrained function class over which the supremum is taken when computing the dual. Specifically, we let H ⊂ L∞(Q) be an RKHS with reproducing kernel K and corresponding feature map Φ and choose some convex regularizer Ω : H → R ∪ {+∞}, for example, some function of the RKHS norm ‖ · ‖H. As in (Nguyen et al., 2010), our estimator is then defined via the dual representation of IRf,Qn that takes\n3We assume that the samples are of equal size for simplicity. The analysis also goes through in the general case.\ninto account the regulariser Ω, i.e.,\nE(Pn, Qn) := sup h∈H\n{ EPn [h]− ( IRf,Qn )? (h)− Ω(h) } .\n(6) The following result gives an explicit dual optimisation program for computing E in the RKHS H.\nTheorem 2. Let H be an RKHS of functions over X with associated feature map Φ. Then the estimator E(Pn, Qn) satisfies, for all Pn and Qn\nE(Pn, Qn) = min α∈∆n\n{ 1\nn n∑ i=1 f (nαi)\n+ Ω?\n( 1\nn n∑ i=1 Φ (xi)− 1 n n∑ i=1 nαiΦ (yi) )} (7)\nwhere the minimisation is over the n-simplex ∆n.\nProof. The proof techniques here are based on those in (Nguyen et al., 2007). Since H is a RKHS, we can represent each function h ∈ H by h (x) = 〈w,Φ (x)〉 for x ∈ X where Φ is the feature map corresponding to K. In this case, the estimator in (6) is given by\nsup w\n{ 1\nn n∑ i=1 〈w,Φ (xi)〉 − ( IRf,Qn )? (〈w,Φ (·)〉)− Ω (w)\n} .\nLetting ψ (w) := − 1n ∑n i=1 〈w,Φ (xi)〉 and ϕ (w) :=(\nIRf,Qn )?\n(〈w,Φ (·)〉) and substituting into the above expression gives\nsup w {〈w, 0〉 − (ψ (w) + ϕ (w) + Ω (w))} = (ψ + ϕ+ Ω)? (0)\nby the definition of a dual. By the infimal convolution theorem (Rockafellar, 1997) we therefore have\nE(Pn, Qn) = min s,r {ψ? (s) + ϕ? (r) + Ω? (−s− r)} .\n(8) Now, since ψ is linear in w its dual is simply\nψ? (s) =\n{ 0 , if s = − 1n ∑n i=1 Φ (xi)\n+∞ , otherwise.\nTo compute the dual of ϕ we observe that ( IRf,Qn )? (h) only depends on the values of h at y1, . . . , yn so we can\nwrite\nϕ? (r) = sup w\n{ 〈w, r〉 − ( IRf,Qn )? (〈w,Φ (·)〉) } = sup w,α,h { 〈w, r〉 − ( IRf,Qn )? (h)\n− n∑ i=1 α(yi) (〈w,Φ (yi)〉 − h(yi)) } = sup w,α,h { 〈w, r〉 − ( IRf,Qn )? (h)\n− 〈 w,\nn∑ i=1 α(yi)Φ (yi)\n〉 + 1\nn n∑ i=1 nα(yi)h(yi)\n}\nby the introduction of Lagrange multipliers α(yi) for the constraints h(yi) = 〈w,Φ(yi)〉. Noting that 1 n ∑n i=1 nα(yi)h(yi) = EQn [nαh] we get\nϕ?(r) = sup w,α,h\n{ EQn [nαh]− ( IRf,Qn )? (h)\n− 〈 w, r −\nn∑ i=1 αiΦ (yi)\n〉}\n= sup α { sup h { EQn [nαh]− ( IRf,Qn )? (h) }\n+ sup w\n{〈 w,\nn∑ i=1 αiΦ (yi)− r\n〉}}\n= sup α\n{ IRf,Qn (nα) : n∑ i=1 αiΦ(yi) = r }\nsince the first inner supremum is the bidual of IRf,Qn and the second supplies the constraint. Furthermore, since Φ(yi) are linearly independent, each r uniquely determines α at y1, . . . , yn so ϕ?(r) = IRf,Qn(nα). Substituting ψ?, ϕ?, and the corresponding constraints on s and r back into the minimisation (8) and noting that IRf,Qn is +∞ for nα /∈ ∆(Qn) w ∆\nn gives the required result:\nE(Pn, Qn) = min α∈∆n\n{ 1\nn n∑ i=1 f (nαi)\n+ Ω?\n( 1\nn n∑ i=1 Φ (xi)− 1 n n∑ i=1 (nαi) Φ (yi)\n)} .\nWe note that E(Pn, Qn) is not a direct estimate of If (P,Q) due to the inclusion of the regularisation term Ω(h). However, ϕ?(r) = IRf,Qn(nα) = 1 n ∑n i=1 f (nαi) can be used as an empirical estimate of If (P,Q) once the values of αi are obtained and r̂ = nα can be seen\nas an estimate of dP/dQ. Thus, the theorem above gives an easily implementable algorithm for estimating f -divergences.\nComputing Ω? for particular choices of Ω in equation (7) immediately gives the following corollary which defines two concrete estimators. Corollary 3. For Ω(g) = λn2 ‖g‖ 2 H\nE(Pn, Qn) = min α∈∆n\n{ 1\nn n∑ i=1 f (nαi)\n+ 1\n2λn ∥∥∥∥∥ 1n n∑ i=1 Φ (xi)− 1 n n∑ i=1 nαiΦ (yi) ∥∥∥∥∥ 2\nH  (9)\nand for Ω(g) = √ λn‖g‖H\nE(Pn, Qn) = min α∈∆n\n{ 1\nn n∑ i=1\nf (nαi) :∥∥∥∥∥ 1n n∑ i=1 Φ (xi)− 1 n n∑ i=1 nαiΦ (yi) ∥∥∥∥∥ H ≤ √ λn } .\n(10)\nThe minimisation in (10) is similar to the one discussed in (Altun & Smola, 2006) which is concerned with density estimation from a single sample. Our estimator can be seen as an extension of that procedure to the two sample setting. The estimator M2 proposed in (Nguyen et al., 2010) uses square norm regularisation in a two sample setting and is therefore directly comparable to (9). The key difference is that we restrict the minimisation to ∆n whereas the M2 minimisation is over α ≥ 0."
    }, {
      "heading" : "2.1. Connections with Maximum Mean Discrepancy",
      "text" : "The relation of the optimisation program in (9) to the original f -divergence is compelling. The first term 1 n ∑n i=1 f (nαi) is simply the empirical estimate of the f divergence of the likelihood ratio dPdQ since each nαi = αi 1/n is an estimate of dP dQ (yi) when Qn is taken to be uniform over y1, . . . , yn. Since f(1) = 0, minimising this term alone would force the αi = 1n for all i. In this sense, the first term can be seen as a kind of generalised MaxEnt regularisation.\nThe second term can be seen as a term that forces the αi terms to “match” empirical means of the feature vectors Φ(xi) and Φ(yi). Following (Gretton et al., 2008), we can formalise the observation regarding the second term by considering the mean map µ from\ndistributions R over X to functions in an RKHS H defined by R 7→ µ [R] := Ex∼R [Φ(x)]. The maximum mean discrepancy (MMD) between distributions P and Q is then defined to be the distance between their respective images under µ, that is, MMD(P,Q) = ‖µ [P ]− µ [Q]‖H. The second term in our estimator is then just MMD2(Pn, α) which is a measure of the discrepancy between the distributions corresponding to the densities dPn and nαdQn. Thus, minimising that term alone corresponds to an unregularised estimation of the density ratio dP/dQ. Similarly, for other choices of regularisation Ω which are a function of ‖ · ‖H, this “data matching” term will be a dual function of MMD(Pn, α).\nThis analysis also leads to an intuitive explanation why we should use the regularisation schedule λn = Θ ( n−1 ) as per (Nguyen et al., 2009). It was shown in (Gretton et al., 2008) that the MMD estimator\n√ n ∥∥∥∥∥ 1n n∑ i=1 Φ (yi)− 1 n n∑ i=1 Φ (xi) ∥∥∥∥∥ H\nconverges to a normal distribution with constant variance. If α is suitably bounded away from infinity, the same holds for the second term in (9) as long as λn = Θ ( n−1 ) . If λn is of smaller order, then the MMD term will eventually dominate the general MaxEnt term which converges to a positive constant if P 6= Q. On the other hand if λn diminishes more slowly, then the MMD term will go to zero even for an incorrect density ratio."
    }, {
      "heading" : "3. Experiments",
      "text" : "Theorem 1 shows the restricted variational bound derived here is strictly tighter than the one proposed by (Nguyen et al., 2010) (henceforth NWJ) for every function r ∈ L1(Q) except when r = dP/dQ in which case they coincide and attain the optimum. This suggests that the optimisation problem derived using our tighter bound should result in an estimator with a smaller bias. This section presents some empirical results demonstrating this improvement. We also conducted experiments to compare our method and Nguyen et al’s method to others methods in the literature which are not based on variational representations of f -divergence. While these non variational methods are not the focus of the experiments, we include them here as they may be of interest to others. In the context of the current work however, we emphasise the superior performance of our estimator compared to that of NWJ illustrating the utility of the tighter variational representation. We include the following recent estimators for comparison:\n• Wang et al (Wang et al., 2009): This estimator is based on nearest neighbour estimates of the two densities and does not make use of a variational representation.\n• Kanamori et al (Kanamori et al., 2009): This is a least-squares estimator for the density ratio, bypassing individual density estimations. Once the density ratio is estimated, it can be directly plugged in the f -divergence formulae. We also experimented with another density ratio estimation method (Sugiyama et al., 2008), with very similar results.\n• García et al (García-García et al., 2011): This estimator uses nearest neighbour misclassification rates and a reformulation of f -divergences in terms of risks."
    }, {
      "heading" : "3.1. Method",
      "text" : "Both our estimator based on (9) and the M2 estimator of NWJ were implemented using the nonlinear convex optimisation routine from the python package Cvxopt to perform the optimization. The implementation of the Wang et al. (Wang et al., 2009) estimator (henceforth WKV) was based on the cKDTree nearest neighbour routine from the SciPy library. Kanamori et al 4. (uLSIF) and García et al 5 ((f, l)) algorithms were implemented using code provided by the respective authors 6. The method for choosing the parameters λn and σ for the NWJ estimator are not specified in (Nguyen et al., 2010). For both NWJ and our estimator, we therefore set λn = 1n (as discussed above) and set σ to the sample variance overXn∪Yn to ensure invariance with respect to rescaling of the data.\nIn every experiment, the distributions P and Q were set to beta distributions B(α, β) for some choice of parameters α, β > 0. Beta distributions were chosen as they cover a wide variety of shapes and have a KL divergence with the following analytic form\nKL(B(α1, β1), B(α2, β2)) = ln B(α2, β2)\nB(α1, β1)\n−dαψ(α1)− dβψ(β1) + (dα + dβ)ψ(α1 + β1)\nwhere dα = α2 − α1 and ψ(v) = Γ ′(v)\nΓ(v) is the digamma function. For each choice of P and Q, a 1-dimensional and a 10-dimensional experiment was performed. In the 1-d experiment, samples of n = 100 values, X100\n4http://sugiyama-www.cs.titech.ac.jp/~sugi/ software/uLSIF\n5http://www.tsc.uc3m.es/~dggarcia/code.html 6All code has been submitted as supplementary mate-\nrial.\nand Y100, were each drawn i.i.d. from P and Q respectively. In the 10-d experiment, each x ∈ X100 ⊂ R10 and y ∈ Y100 ⊂ R10 was drawn i.i.d. from the respective product distributions P× ∏9 i=1N(0, 0.01) and\nQ× ∏9 i=1N(0, 0.01). This gives samples from two distributions embedded in a 10-dimensional space where all but one of the dimensions is zero mean Gaussian noise. The KL divergences for the 10-d product distributions for each choice of P and Q are the same as for the 1-d case, that is, KL(P,Q). The specific P and Q in the experiments were chosen to give a range of different KL divergence values and explore a few different pairings of distributional shapes."
    }, {
      "heading" : "3.2. Results",
      "text" : "Table 1 summarises the application of all five estimators over 250 runs of the 1-d (odd rows) and 10- d experiments (even rows) for various choices of P and Q shown in the first column. The pairs of rows are ordered in increasing value of true KL divergence (shown in the second column) and is the same for both rows. The table lists the divergence estimates averaged over the different runs as well as the empirical Mean Squared Error (MSE). The bold values for the MSE correspond to the lowest amongst the different estimators. Where the MSE of our estimator or that of NWJ is strictly lower than the other, we have italicised the MSE. The last three columns are in grey as they are not the main point of the experiments."
    }, {
      "heading" : "3.3. Discussion",
      "text" : "For the most part our estimator performs better in terms of MSE than that of the NWJ. When the true divergence is large, the difference is especially pronounced. This is unsurprising as our estimator is based on a tighter bound for the divergence as Theorem 1 shows. For small divergences the difference is smaller since roughly speaking, 0 ≤ NWJ ≤ Ours ≤ KL(P,Q). Thus, for small divergences the estimators must necessarily return similar values.\nIn contrast, the nearest neighbour-based methods (WKV and (f, l)) behave very differently to variational estimators. In general, their bias is significantly lower than both variational methods when the real divergence is large. This is a natural conclusion, since the variational methods presented here are intrinsically lower bounds of the real divergence. Finally, we note that uLSIF does not perform as well as the other methods. This is to be expected as uLSIF is primarily designed for density ratio estimation while the rest of the methods are derived specifically for divergence estimation.\nOn the 10-d experiments, the MSE performance of the WKV estimator is typically much worse than the rest of methods. It consistently over-estimates the true KL divergence and, for B(1, 4) vs B(3, 1), drastically overshoots it, resulting in an order of magnitude larger MSE than the other estimators. One likely explanation of this poor performance of WKV on the higher dimensional problems is that its estimated values scale with the dimension of the data. This scaling occurs even if the two distributions differ only on a low dimensional manifold, as they do in the 10-d experiments. The success of this estimator in the B(1, 1) vs B(10, 10) experiment is likely a coincidence. All the estimators underestimate this divergence in the 1- d case and we expect that the scaling of WKV with the dimension has pushed its estimate up to the true KL. The (f, l) estimator, although also based on nearest neighbour techniques, does not suffer from this problem since it does not present a explicit dependance of the estimated divergence with respect to the ambient dimension.\nIn light of these observations, we offer some guidelines as to which estimator to use if one has some prior knowledge or suspicion about the data. Use our method when suspecting a low divergence; use (f, l) for high divergence. We also recommend using variational methods over nearest neighbour estimators for hypothesis testing if false positives are a concern since the variational methods are much more likely to consistently underestimate the true divergence. It is important however to note that if running time is an issue then WKV becomes a very attractive option. There are many fast approximate nearest neighbour\nalgorithms resulting in fast estimation of the WKV statistic."
    }, {
      "heading" : "4. Summary and Discussion",
      "text" : "We have shown how tighter variational representations for f -divergences can be derived by restricting the effective domain of the divergence functional to the set of probability measures. Since many works in the literature are based on variational representations, this tighter version presents many potential applications. As an example of this, a dual program for f -divergence estimators based on this tighter representation was derived for density ratios within an RKHS H and arbitrary convex regularizers. This tightened and extended the M2 estimator proposed in (Nguyen et al., 2010) and we demonstrated empirically the benefits of our analysis. We also gave a novel interpretation of the dual program in terms of MMD which showed that our estimator can be seen to find an approximation r̂ ∈ H of the density ratio that attempts to simultaneously minimises MMD between Pn and r̂Qn and the empirical f -divergence EQn [f (r̂)]. This second minimisation can be seen as a generalised maximum entropy regularisation. We have also provided a comparison to other state of the art estimators. We concluded that variational methods are good for settings in which a low divergence is suspected or in scenarios where overestimation is detrimental.\nAs future work we intend to investigate the impact of this tightened representation on other divergence estimators based on the looser representation such as (Kanamori et al., 2011), as well as to areas other than\nf -divergence estimation (hypothesis testing and statistical inference). We also plan to find general conditions under which consistency of our family of estimators holds. The work of (Nguyen et al., 2010) has already paved the way for this investigation. Failing that, the very general consistency results of (Altun & Smola, 2006) for single sample divergence estimation may also be amenable to the analysis of our estimator. The performance of our estimator on distributions on low dimensional manifolds suggests that it would be worth testing on domains involving audio or images. It would also be interesting to apply our method for estimating divergences other than KL. For instance we could study α-divergence estimation as in (Poczos & Schneider, 2011)."
    } ],
    "references" : [ {
      "title" : "A general class of coefficients of divergence of one distribution from another",
      "author" : [ "S.M. Ali", "S.D. Silvey" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Ali and Silvey,? \\Q1966\\E",
      "shortCiteRegEx" : "Ali and Silvey",
      "year" : 1966
    }, {
      "title" : "Unifying divergence minimization and statistical inference via convex duality",
      "author" : [ "Y. Altun", "A. Smola" ],
      "venue" : "Learning theory, pp",
      "citeRegEx" : "Altun and Smola,? \\Q2006\\E",
      "shortCiteRegEx" : "Altun and Smola",
      "year" : 2006
    }, {
      "title" : "The concept of duality in convex analysis, and the characterization of the legendre transform",
      "author" : [ "S. Artstein-Avidan", "V. Milman" ],
      "venue" : "Annals of mathematics,",
      "citeRegEx" : "Artstein.Avidan and Milman,? \\Q2009\\E",
      "shortCiteRegEx" : "Artstein.Avidan and Milman",
      "year" : 2009
    }, {
      "title" : "On Bayesian bounds",
      "author" : [ "A. Banerjee" ],
      "venue" : "Proceedings of the 23rd International Conference on Machine learning,",
      "citeRegEx" : "Banerjee,? \\Q2006\\E",
      "shortCiteRegEx" : "Banerjee",
      "year" : 2006
    }, {
      "title" : "Convexity and optimization in Banach spaces, volume",
      "author" : [ "V. Barbu", "T. Precupanu" ],
      "venue" : null,
      "citeRegEx" : "Barbu and Precupanu,? \\Q1986\\E",
      "shortCiteRegEx" : "Barbu and Precupanu",
      "year" : 1986
    }, {
      "title" : "Parametric estimation and tests through divergences and the duality technique",
      "author" : [ "Broniatowski", "Michel", "Keziou", "Amor" ],
      "venue" : "Journal of Multivariate Analysis,",
      "citeRegEx" : "Broniatowski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Broniatowski et al\\.",
      "year" : 2009
    }, {
      "title" : "Information-type measures of difference of probability distributions and indirect observations",
      "author" : [ "I. Csiszár" ],
      "venue" : "Studia Scientiarum Mathematicarum Hungarica,",
      "citeRegEx" : "Csiszár,? \\Q1967\\E",
      "shortCiteRegEx" : "Csiszár",
      "year" : 1967
    }, {
      "title" : "Large deviations techniques and applications, volume 38",
      "author" : [ "A. Dembo", "O. Zeitouni" ],
      "venue" : null,
      "citeRegEx" : "Dembo and Zeitouni,? \\Q2009\\E",
      "shortCiteRegEx" : "Dembo and Zeitouni",
      "year" : 2009
    }, {
      "title" : "Asymptotic evaluation of certain markov process expectations for large time",
      "author" : [ "MD Donsker", "Varadhan", "SRS" ],
      "venue" : "iv. Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Donsker et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Donsker et al\\.",
      "year" : 1983
    }, {
      "title" : "Risk-based generalizations of fdivergences",
      "author" : [ "D. García-García", "U. von Luxburg", "R. SantosRodríguez" ],
      "venue" : "In Proceedings of the International Conference on Machine learning,",
      "citeRegEx" : "García.García et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "García.García et al\\.",
      "year" : 2011
    }, {
      "title" : "A Kernel Method for the Two-SampleProblem",
      "author" : [ "A. Gretton", "K.M. Borgwardt", "M. Rasch", "B. Schölkopf", "A.J. Smola" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2008
    }, {
      "title" : "Fundamentals of Convex Analysis",
      "author" : [ "Hiriart-Urruty", "Jean-Baptiste", "Lemaréchal", "Claude" ],
      "venue" : null,
      "citeRegEx" : "Hiriart.Urruty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hiriart.Urruty et al\\.",
      "year" : 2001
    }, {
      "title" : "A least-squares approach to direct importance estimation",
      "author" : [ "T. Kanamori", "S. Hido", "M. Sugiyama" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kanamori et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kanamori et al\\.",
      "year" : 2009
    }, {
      "title" : "f-divergence estimation and two-sample homogeneity test under semiparametric density-ratio models",
      "author" : [ "T. Kanamori", "T. Suzuki", "M. Sugiyama" ],
      "venue" : "Information Theory, IEEE Transactions on (To appear),",
      "citeRegEx" : "Kanamori et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kanamori et al\\.",
      "year" : 2011
    }, {
      "title" : "Estimating divergence functionals and the likelihood ratio by convex risk minimization",
      "author" : [ "X. Nguyen", "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Technical report, Department of Statistics, UC Berkeley,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2007
    }, {
      "title" : "On surrogate loss functions and f -divergences",
      "author" : [ "X. Nguyen", "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2009
    }, {
      "title" : "Estimating divergence functionals and the likelihood ratio by convex risk minimization",
      "author" : [ "X.L. Nguyen", "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2010
    }, {
      "title" : "On the estimation of alpha-divergences",
      "author" : [ "Poczos", "Barnabas", "Schneider", "Jeff" ],
      "venue" : "AISTATS",
      "citeRegEx" : "Poczos et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Poczos et al\\.",
      "year" : 2011
    }, {
      "title" : "Information, divergence and risk for binary experiments",
      "author" : [ "Reid", "Mark D", "Williamson", "Robert C" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Reid et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Reid et al\\.",
      "year" : 2011
    }, {
      "title" : "Convex analysis, volume 28",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : "Princeton Univ Pr,",
      "citeRegEx" : "Rockafellar,? \\Q1997\\E",
      "shortCiteRegEx" : "Rockafellar",
      "year" : 1997
    }, {
      "title" : "Direct importance estimation for covariate shift adaptation",
      "author" : [ "M. Sugiyama", "T. Suzuki", "S. Nakajima", "H. Kashima", "P. von Bunau", "M. Kawanabe" ],
      "venue" : "Annals of the Institute of Statistical Mathematics,",
      "citeRegEx" : "Sugiyama et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sugiyama et al\\.",
      "year" : 2008
    }, {
      "title" : "Divergence estimation for multidimensional densities via k-nearestneighbor distances",
      "author" : [ "Q. Wang", "S.R. Kulkarni", "S. Verdú" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Wang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "An important class of discrepancy measures between probability distributions is the family of f -divergences (also known as Ali-Silvey (Ali & Silvey, 1966) or Csisz·r divergences (Csiszár, 1967)).",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 16,
      "context" : "This variational representation has been recently used for f -divergence estimation (Nguyen et al., 2010; 2007; Kanamori et al., 2011) homogeneity testing (Kanamori et al.",
      "startOffset" : 84,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "This variational representation has been recently used for f -divergence estimation (Nguyen et al., 2010; 2007; Kanamori et al., 2011) homogeneity testing (Kanamori et al.",
      "startOffset" : 84,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : ", 2011) homogeneity testing (Kanamori et al., 2011) and parameter estimation (Broniatowski & Keziou, 2009).",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "This generalises similar observations for the specific case of the KL divergence, such as Banerjee’s compression lemma (Banerjee, 2006).",
      "startOffset" : 119,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "We show this dual program has a natural interpretation as a trade off between the minimisation of the f -divergence in question and the minimum mean discrepancy (MMD) (Gretton et al., 2008) between the empirical distributions.",
      "startOffset" : 167,
      "endOffset" : 189
    }, {
      "referenceID" : 16,
      "context" : "Experiments in section 3 confirm that the tighter variational form underlying our approach leads to better KL divergence estimates than the well known estimator of (Nguyen et al., 2010; 2007) which is based on the looser representation.",
      "startOffset" : 164,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "The choice of order of the arguments P and Q is arbitrary and other authors, notably (Nguyen et al., 2010), define f -divergence in terms of dQ/dP .",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "The dashed line and solid line represent our new expression and the expression used by (Nguyen et al., 2010) respectively as they vary over L∞(Q).",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "As explored in (Nguyen et al., 2010; Altun & Smola, 2006) and in section 2 below, variational representations such as these readily admit approximation",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "While our new expression and that of (Nguyen et al., 2010) have the same supremum, our expression is closer to the supremum at every point φ ∈ L∞(Q).",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "The first inequality is the well-known representation of the KL divergence in the large deviations literature (Donsker & Varadhan, 1983) which has been rediscovered in the PAC-Bayes community as the compression lemma (Banerjee, 2006).",
      "startOffset" : 217,
      "endOffset" : 233
    }, {
      "referenceID" : 16,
      "context" : "As in (Nguyen et al., 2010), our estimator is then defined via the dual representation of IRf,Qn that takes We assume that the samples are of equal size for simplicity.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "The proof techniques here are based on those in (Nguyen et al., 2007).",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "By the infimal convolution theorem (Rockafellar, 1997) we therefore have",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "The estimator M2 proposed in (Nguyen et al., 2010) uses square norm regularisation in a two sample setting and is therefore directly comparable to (9).",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "Following (Gretton et al., 2008), we can formalise the observation regarding the second term by considering the mean map μ from distributions R over X to functions in an RKHS H defined by R 7→ μ [R] := Ex∼R [Φ(x)].",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "This analysis also leads to an intuitive explanation why we should use the regularisation schedule λn = Θ ( n−1 ) as per (Nguyen et al., 2009).",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : "It was shown in (Gretton et al., 2008) that the MMD estimator",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "Theorem 1 shows the restricted variational bound derived here is strictly tighter than the one proposed by (Nguyen et al., 2010) (henceforth NWJ) for every function r ∈ L(Q) except when r = dP/dQ in which case they coincide and attain the optimum.",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 21,
      "context" : "• Wang et al (Wang et al., 2009): This estimator is based on nearest neighbour estimates of the two densities and does not make use of a variational representation.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "• Kanamori et al (Kanamori et al., 2009): This is a least-squares estimator for the density ratio, bypassing individual density estimations.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "We also experimented with another density ratio estimation method (Sugiyama et al., 2008), with very similar results.",
      "startOffset" : 66,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "• García et al (García-García et al., 2011): This estimator uses nearest neighbour misclassification rates and a reformulation of f -divergences in terms of risks.",
      "startOffset" : 15,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "(Wang et al., 2009) estimator (henceforth WKV) was based on the cKDTree nearest neighbour routine from the SciPy library.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "The method for choosing the parameters λn and σ for the NWJ estimator are not specified in (Nguyen et al., 2010).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "This tightened and extended the M2 estimator proposed in (Nguyen et al., 2010) and we demonstrated empirically the benefits of our analysis.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "As future work we intend to investigate the impact of this tightened representation on other divergence estimators based on the looser representation such as (Kanamori et al., 2011), as well as to areas other than",
      "startOffset" : 158,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "The work of (Nguyen et al., 2010) has already paved the way for this investigation.",
      "startOffset" : 12,
      "endOffset" : 33
    } ],
    "year" : 2012,
    "abstractText" : "We show that the variational representations for f -divergences currently used in the literature can be tightened. This has implications to a number of methods recently proposed based on this representation. As an example application we use our tighter representation to derive a general f -divergence estimator based on two i.i.d. samples and derive the dual program for this estimator that performs well empirically. We also point out a connection between our estimator and MMD.",
    "creator" : "TeX"
  }
}