{
  "name" : "1305.1343.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards an Author-Topic-Term-Model Visualization of 100 Years of German Sociological Society Proceedings",
    "authors" : [ "Arnim Bleier", "Andreas Strotmann" ],
    "emails" : [ "andreas.strotmann}@gesis.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Towards an Author-Topic-Term-Model Visualization of 100 Years of German Sociological Society Proceedings\nArnim Bleier and Andreas Strotmann\n{arnim.bleier, andreas.strotmann}@gesis.org GESIS - Leibniz Institute for the Social Sciences, Unter Sachsenhausen 6-8,\nCologne (Germany)"
    }, {
      "heading" : "Introduction",
      "text" : "Author co-citation studies (Zhao & Strotmann, 2008) employ factor analysis to reduce highdimensional co-citation matrices to low dimensional and possibly interpretable factors, but these studies do not use any information from the text bodies of publications. We hypothesise that term frequencies may yield useful information for scientometric analysis. In our work we ask if word features in combination with Bayesian analysis allows for well-founded science mapping studies. This work goes back to the roots of Mosteller and Wallace’s (1964) statistical text analysis using word frequency features and a Bayesian inference approach, tough with different goals. To answer our research question we (i) introduce the data set on which the experiments are carried out, (ii) describe the Bayesian model employed for inference and (iii) present first results of the analysis."
    }, {
      "heading" : "The DGS Dataset",
      "text" : "The collection of documents D we use in the experiment covers ~100 years of proceedings (from 1910 to 2006) of meetings of the Deutsche Gesellschaft für Soziologie (DGS), a total of 5,010 documents. Early proceedings had been scanned and OCRed, others were used in original digital form. Metadata for the documents included 3,661 distinct full names of authors J. From each document, the 21st-320th words were extracted. After unifying word case, we removed stop-words, short and/or rare words (< 4 letters; > 10 occurrences; mostly OCR fragments) and words found in more than half of the documents, resulting in 1,067,128 occurrences from a vocabulary V with 12,665 distinct words."
    }, {
      "heading" : "Statistical Model",
      "text" : "We now review the statistical model we employ to relate authors and documents via a flexible number of topics. Following a common notation (RosenZvi, 2004), a document d is modelled as a vector of \uD835\uDC41! words, \uD835\uDC98!, where the i\nth word \uD835\uDC64!\" is chosen from the unique terms in vocabulary V. Each document d is associated with a set of authors \uD835\uDC8B! from the set of all authors, J. Our model assumes that documents are generated in the following steps:\n1. Draw a shared discrete probability distribution from a Dirichlet Process (DP) (Teh et al. 2006) with base measure H and prior concentration parameter \uD835\uDEFE as a global mixture over topics \uD835\uDF0F~\uD835\uDC37\uD835\uDC43(\uD835\uDC3B, \uD835\uDEFE) 2. For author j, draw an author specific distribution over topics from the global topic mixture \uD835\uDF0F, with prior concentration parameter \uD835\uDEFC \uD835\uDF03!~\uD835\uDC37\uD835\uDC43(\uD835\uDF0F,\uD835\uDEFC) 3. For each topic k, draw a topic specific distribution over vocabulary V from the symmetric Dirichlet prior \uD835\uDEFD \uD835\uDF19!~\uD835\uDC37\uD835\uDC56\uD835\uDC5F \uD835\uDEFD 4. For all \uD835\uDC41! words in document d, (i) draw an author indicator x from the set of authors \uD835\uDC8B! of document d; (ii) a topic indicator z from the author specific topic distribution \uD835\uDF03!; (iii) the observed word w itself from the respective topic\n\uD835\uDC65!\"~\uD835\uDC37\uD835\uDC56\uD835\uDC60\uD835\uDC50\uD835\uDC5F\uD835\uDC52\uD835\uDC61\uD835\uDC52(\uD835\uDC8B!) \uD835\uDC67!\"~\uD835\uDC37\uD835\uDC56\uD835\uDC60\uD835\uDC50\uD835\uDC5F\uD835\uDC52\uD835\uDC61\uD835\uDC52(\uD835\uDF03!!\") \uD835\uDC64!\"~\uD835\uDC37\uD835\uDC56\uD835\uDC60\uD835\uDC50\uD835\uDC5F\uD835\uDC52\uD835\uDC61\uD835\uDC52 \uD835\uDF19!!\"\nFigure 1 illustrates the independence assumptions made by the generative storyline via plate notation. Circles represent statistical variables, with observed ones shaded. Arrows represent conditional dependence − i.e., the order in which variables are drawn. Plates indicate repetition, as indicated by universal quantifiers. For the posterior analysis, the topic distributions \uD835\uDF19! over terms as well as the author distributions \uD835\uDF03!\nar X\niv :1\n30 5.\n13 43\nv1 [\ncs .D\nL ]\n6 M\nay 2\n01 3\nFigure 1: Excerpt of author topic analysis result: visualization of four topics and their main authors\nover topics are of particular interest. The former span a latent semantic space via meaningful word probabilities for each topic; the latter allow us to position each author j in this topic space."
    }, {
      "heading" : "Posterior Analysis and Visualization",
      "text" : "The generative model is structured as a directed acyclic graph beginning from causes and ending with observed words in documents. Bayes’ Rule reverses causality, and parameters of interest can be estimated from observed data and priors. We use an MCMC sampler for this posterior analysis. After running the sampler for 2,000 steps with priors \uD835\uDEFE=.5, \uD835\uDEFC=.5 and \uD835\uDEFD=.2, the model converged to 89 components. Samples of \uD835\uDF19! and \uD835\uDF03! are shown in Figure 1 and Table 1. The reader is referred to Rosen-Zvi et al. (2004), Teh et al. (2006) and Bleier (2012) for an in-depth discussion of this method.\nDue to space constraints we restrict the visualization in Figure 1 (using Pajek) to four of 89 components. Authors and topics are represented as square and circular nodes, resp. The size of topic nodes is proportional to their usage and the strength of the arcs proportional to \uD835\uDF03!\" ,  the probability for topic k specific to author j. We are not constrained to interpreting topics as only having a distinct probability for each author, but equally have for each topic k a distribution \uD835\uDF19! over distinct words in the vocabulary. Table 1 displays the six most probable terms for the sample topics of Figure 1."
    }, {
      "heading" : "Discussion",
      "text" : "Our approach to science mapping uses a flexible version latent Dirichlet allocation to (i) identify an optimal number and set of topics for a given set of documents based on the words that occur in them, (ii) to identify the most relevant words to describe each topic, and (iii) to identify weighted links between authors and the topics of their writings. The statistical model takes into account that documents are written by multiple authors, that authors write on different topics to different degrees, and that words pertain to different topics to varying degrees. Figure 1 shows a small fragment of a map of German sociological science based on ~100 years of DGS proceedings, inspired by the visualization of results of co-citation-based factor analysis in Zhao & Strotmann (2008), but generated fully automatically from the results of applying this statistical analysis technique to full texts. While a full evaluation remains to be done, these results show some promise for the application of these methods in scientometric studies."
    } ],
    "references" : [ {
      "title" : "A simple non-parametric Topic Mixture for Authors and Documents",
      "author" : [ "A. Bleier" ],
      "venue" : "Pre-print arXiv:1211.6248 [cs.LG].",
      "citeRegEx" : "Bleier,? 2012",
      "shortCiteRegEx" : "Bleier",
      "year" : 2012
    }, {
      "title" : "Inference and Disputed Authorship: The Federalists. AddisonWesley",
      "author" : [ "F. Mosteller", "D. Wallace" ],
      "venue" : null,
      "citeRegEx" : "Mosteller and Wallace,? \\Q1964\\E",
      "shortCiteRegEx" : "Mosteller and Wallace",
      "year" : 1964
    }, {
      "title" : "The author-topic model for authors and documents",
      "author" : [ "M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth" ],
      "venue" : "Proceedings of the 20 Conference on Uncertainty in artificial intelligence (UAI",
      "citeRegEx" : "Rosen.Zvi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rosen.Zvi et al\\.",
      "year" : 2004
    }, {
      "title" : "Hierarchical dirichlet processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "Information Science during the first decade of the Web: An enriched author co-citation analysis",
      "author" : [ "D. Zhao", "A. Strotmann" ],
      "venue" : "Journal of the American Society for Information Science and Technology,",
      "citeRegEx" : "Zhao and Strotmann,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhao and Strotmann",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "This work goes back to the roots of Mosteller and Wallace’s (1964) statistical text analysis using word frequency features and a Bayesian inference approach, tough with different goals.",
      "startOffset" : 36,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "Draw a shared discrete probability distribution from a Dirichlet Process (DP) (Teh et al. 2006) with base measure H and prior concentration parameter",
      "startOffset" : 78,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "The reader is referred to Rosen-Zvi et al. (2004), Teh et al.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "The reader is referred to Rosen-Zvi et al. (2004), Teh et al. (2006) and Bleier (2012) for an in-depth discussion of this method.",
      "startOffset" : 26,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "(2006) and Bleier (2012) for an in-depth discussion of this method.",
      "startOffset" : 11,
      "endOffset" : 25
    } ],
    "year" : 2013,
    "abstractText" : "Introduction Author co-citation studies (Zhao & Strotmann, 2008) employ factor analysis to reduce highdimensional co-citation matrices to low dimensional and possibly interpretable factors, but these studies do not use any information from the text bodies of publications. We hypothesise that term frequencies may yield useful information for scientometric analysis. In our work we ask if word features in combination with Bayesian analysis allows for well-founded science mapping studies. This work goes back to the roots of Mosteller and Wallace’s (1964) statistical text analysis using word frequency features and a Bayesian inference approach, tough with different goals. To answer our research question we (i) introduce the data set on which the experiments are carried out, (ii) describe the Bayesian model employed for inference and (iii) present first results of the analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}