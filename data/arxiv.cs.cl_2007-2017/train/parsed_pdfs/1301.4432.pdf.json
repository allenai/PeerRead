{
  "name" : "1301.4432.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Running Head: Simplicity-based approach to language learning Language learning from positive evidence, reconsidered: A simplicity-based approach",
    "authors" : [ "Anne S. Hsu", "Nick Chater", "Paul Vitányi" ],
    "emails" : [ "Nick.Chater@wbs.ac.uk", "paul.vitanyi@cwi.nl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Children learn their native language by exposure to their linguistic and communicative environment, but apparently without requiring that their mistakes are corrected. Such learning from “positive evidence” has been viewed as raising “logical” problems for language acquisition. In particular, without correction, how is the child to recover from conjecturing an over-general grammar, which will be consistent with any sentence that the child hears? There have been many proposals concerning how this “logical problem” can be dissolved. Here, we review recent formal results showing that the learner has sufficient data to learn successfully from positive evidence, if it favours the simplest encoding of the linguistic input. Results include the learnability of linguistic prediction, grammaticality judgments, language production, and form-meaning mappings. The simplicity approach can also be “scaled-down” to analyse the learnability of specific linguistic constructions, and is amenable to empirical test as a framework for describing human language acquisition.\nChildren appear to learn language primarily by exposure to the language of others. But how is this possible? The computational challenges of inferring the structure of language from mere exposure are formidable. In light of this, many theorists have conjectured that language acquisition is only possible because the child possesses cognitive machinery that fits especially closely with the structure of natural language. This could be because the brain has adapted to language (Pinker & Bloom, 1990), or because language has been shaped by the brain Christiansen & Chater, 2007).\nA number of informal arguments concerning the challenge of language learning from\nexperience have been influential. Chomsky (1980) argued that the “poverty of the stimulus” available to the child was sufficiently great that the acquisition of language should be viewed as analogous to the growth of an organ, such as the lung or the heart, unfolding along channels pre-specified in the genome. Here, we focus on a specific facet of poverty of the stimulus: that children do not appear to receive or attend to “negative evidence:\" explicit feedback that certain utterances are ungrammatical (Bowerman, 1988; Brown & Hanlon, 1970; Marcus, 1993). 1\nThe ability to learn language in the absence of negative evidence is especially puzzling,\ngiven that linguistic rules are riddled with apparently capricious restrictions. For example, a child might naturally conclude from experience that there is a general rule that is can be contracted, as in He’s taller than she is. But contractions are not always allowed, for example: *He is taller than she’s. The puzzle is that, once the learner has entertained the possibility that the overgeneral rule is correct, it appears to have no way to “recover” from overgeneralization and recognise that restrictions should be added. This is because each contraction that it hears\nconforms to the overgeneral rule. Now, if the learner uses the overgeneral rule to generate language, then it will from time to time produce utterances such as *John isn't coming but Mary’s. A listener’s startled reaction or look of incomprehension might provide a crucial clue that the rule is overgeneral. However, such feedback is the very negative evidence that appears to be inessential to child language acquisition. Thus, if children do not require such negative evidence, how can they recover from such overgeneralisations? Various scholars argue that they cannot: Restrictions on overgeneral grammatical rules must, instead, be innately specified (e.g., Crain & Lillo-Martin, 1999). Other theorists argue that avoiding overgeneral rules poses a fundamental “logical problem” for language acquisition (Baker & McCarthy, 1981; Dresher & Hornstein, 1976).\nOne way to defuse the puzzle is to challenge its premise. One possibility is that, despite\nappearances, children can access and use negative evidence in a subtle form. In this paper, we set aside these contentious issues (e.g., Demetras, Post & Snow, 1986; Marcus, 1993) and argue that, whether or not negative evidence is available to, or used by, the child, language can successfully be learned without it (following, for example, MacWhinney, 1993; 2004; Rohde & Plaut, 1999; Tomasello, 2004).\nThe arguments for learnability from positive evidence presented here are part of a\nbroader tradition of research on learnability (e.g., Angluin, 1980, 1988; Clark, & Eyraud, 2007; Feldman, 1972; Gold, 1967; Horning, 1969; Jain, Osherson, Royer & Kumar Sharma, 1999; Niyogi, 2006; Wharton, 1974). And formal learnability arguments are complementary to recent developments of language engineering systems, which have shown that it is possible to learn automatically non-trivial aspects of phonology, morphology, syntax and semantics from positive language input (Goldsmith, 2001; Klein & Manning, 2005; Steyvers, Griffiths &\nTenenbaum, 2006). While such systems are still very far from being able to acquire language from mere exposure, the pace of progress suggests that a priori barriers to learning may not necessarily be insurmountable.\nRather than surveying these developments, and indicating how they may be extended,\nhere we will take a more direct approach: we focus on one major line of positive learnability results based on the ‘simplicity principle’. We begin by introducing the simplicity principle (Section 1) and considering how it can be embodied in an “ideal learner” (Section 2). We then outline some recent formal results on how the simplicity principle can be used to learn aspects of language such as utterance prediction, grammaticality judgments, language production, and mapping between form and meaning (Sections 3-6). We then briefly describe a practical method for assessing learnability of linguistic patterns using the simplicity approach, and how this assessment can be linked with experimental data (Section 7). Overall, the contribution of the work reviewed here is to show that, under fairly mild conditions, language acquisition from sufficient amounts of positive evidence is possible; and to indicate how the simplicity-based approach can potentially provide a framework for understanding child language acquisition.\n1. Ideal learning using a simplicity principle\nThe simplicity principle has a long history in the philosophy of science and the study of perception (e.g., Mach, 1959/1886), and has been proposed as a general cognitive principle (Chater & Vitányi, 2002). A formal analysis of simplicity learning starts with supposing a learner (human or artificial) that is faced with a set of positive data. For language, this data is a set of observed grammatical sentences. 2 Any set of observed sentences will be consistent with an infinite number of grammars. That is, any set of sentences could have been generated by any\nof an infinite number of grammars. How can the learner choose among these infinite possibilities?\nThe simplicity principle recommends that the learner prefers hypotheses which allows\nfor the simplest encoding of the data. For language, the data will be the observed sentences, and hypotheses are grammars (or other linguistic representations), which can be viewed as a set of probabilistic rules which captures the patterns in the linguistic input to the learner. Simplicity can be measured by viewing hypotheses (here, grammars) as computer programs which encode the data (the data is generated as the output of the program). The simplicity principle thus favors the grammar that provides the shortest encoding of the data. 3\nHow can a grammar be viewed as a computer program for encoding linguistic input?\nOne concrete approach involve two steps. The first step is to specifying the grammatical rules (and, crucially, probabilities of their use). This defines a probabilistic process for generating sentences; and thus defines a probability distribution over possible strings. The second step is to encode the specific sentences in the input. It is intuitively clear that the most efficient way to do this is to reserve shorter codes for probable strings; and longer codes for less probable strings. A basic result from information theory (e.g., Cover & Thomas, 2006) is that the optimal way to do this is to assign a binary code of length log21/p to a string with probability p. 4 So, intuitively, a (probabilistic) grammar provides a short encoding of the linguistic input if it can itself be specified briefly; and if it makes the sentences that are actually observed as probable as possible. There is a tension between these objectives. An “over-precise” grammar, which encodes exactly those sentences that have been encountered and no others will make those particular sentences highly probably; but the code for such a grammar will be long (roughly, it will consist just of a list of the “allowed” sentences). Conversely, a very simple but\novergeneral grammar (e.g., stating, roughly, that words can occur in any order with equal probability) will have a short code, but, because the space of possible allowed sentences is vast, the code for the specific sentences observed by the learner will be very long. The simplicity principle recommends finding an balance between these extremes: postulating restrictions in the grammar just when these “pay off” by sufficiently reducing the code length of the sentences, encoded by the grammar.\nAs we have indicated, in general, the better the grammar captures the structure of the\nlanguage, the shorter the encoded representation of the linguistic input will be. For a concrete example, let us first consider hypotheses (i.e. grammars) describing artificially simplistic language data. Suppose the observed language was the following repeating infinite string of sentences: Hi! Bye! Hi! Bye!… One hypothesis could be “The language is a sequence of ‘Hi!’ and ‘Bye!’ occurring independently, and each with .5 probability.” Under this hypothesized grammar, the encoded specification of the language input will be “0101…”, where 0 and 1 correspond to ‘Hi!’ and ‘Bye!’ respectively. Now if the hypothesis was a more powerfully descriptive grammar such as “The language contains a single sentence ‘Hi! Bye!’, then no further code at all is required to specify the linguistic input. Now, an infinite language input is fully specified in a simple finite description—and, more generally, the more precisely the grammar captures the structure of the linguistic input, the shorter the encoding of that linguistic input will be.\nInitially, the learner may not have sufficient data to favour the latter hypothesis; but\neventually the latter “grammar” will provide the simpler encoding, because it correctly captures regularities in the input. Hence, as linguistic input accumulates, the grammar which provides the simplest encoding will be updated. An ideal simplicity learner (as in the mathematical\nresults below) will have access to all (infinite) possibly hypothetical grammars that describe its current language data input, and choose the “simplest;’ any real, and hence computationally limited, learner can of course only approximate this calculation to some degree.\nCrucially, note that the simplicity-based learner has a mechanism for avoiding\novergeneral grammars, when learning from positive evidence. Although our artificial data is compatible with a random sequence of ‘Hi!’ and ‘Bye!,’ the corresponding grammar is eliminated without the need for negative evidence, but because another grammar provides a shorter encoding of the input. 5\nThis point applies equally to learning natural languages. Consider the case of is\ncontraction mentioned above. Consider two possible grammars, one that allows is contraction everywhere, and one that is more restricted (allowing He’s taller than she is but not *He is taller than she’s). The latter “grammar” will be more complex (because it involves specifying more precisely when contraction can occur); but it will encode the linguistic input more briefly, because it more accurately captures the structure of the language. Given sufficient linguistic input, the benefit of the more accurate encoding of the linguistic input will overwhelm any additional costs in encoding the grammatical rule, and the more precise rule will be favoured. Thus, it appears that an overgeneral grammar can be eliminated by applying the simplicity principle to positive data only.\nThis intuition is encouraging but hardly definitive. Knowing that a learner can\npotentially eliminate a single over-general grammar does not, of course, indicate that it can successfully choose between an infinity of possible grammars, and home in on the “true” grammar, or some approximation to it. We shall see, however, that positive mathematical results along these lines are possible. Moreover, in Section 7, we shall apply the style of\nargument sketched above to the learnability of some specific, and much-discussed, linguistic regularities (see Hsu, Chater & Vitányi, 2011).\n2 An“ideal” learner\nBelow, we consider some formal theoretical results describing what an “ideal” learner can learn purely from exposure to an (indefinitely long) sequence of linguistic input (i.e., positive evidence) by using the simplicity principle.\nWhat is the structure of the linguistic material to be learned? Fortunately, it turns out\nthat we need assume only that this input is generated probabilistically by some computable process. 6 This restriction is mild because cognitive science takes computability constraints on mental processes, including the generation of language, as founding assumptions (Pylyshyn, 1984) and, indeed, specific models of language structure and generation all adhere to this assumption. Finally, for mathematical convenience, and without loss of generality, we assume that the linguistic input is coded in binary form.\nImportantly, note that these assumptions allow that there can be any (computable)\nrelationship between different parts of the input---we do not, for example, assume that sentences are independently sampled from a specific probability distribution. Our very mild assumption allows sentences to be highly interdependent (this is one generalization with respect to earlier results, e.g., Angluin, 1980; Jerome Feldman, 1972; Wharton, 1974), and includes the possibility that the language may be modified or switched during the input or indeed that sentences from many different languages might be interleaved.\nSpecifically, suppose that the linguistic input, coded as a binary sequence, x, is\ngenerated by a computable probability distribution, C(x). 7 Intuitively, we can view this as\nmeaning that there is a computer program, C, (which might, for example, encode a grammar, as above) which receives random input y, from a stream of coin flips. When fed to C, this random input generates x as output, i.e., C(y) = x. The probability of this y is 2 -l(y) (the probability of generating any specific binary sequence of length l(y) from unbiased coin flips). Many y may\ngenerate the same x, so the probability of an output with initial segment x, C(x), is the sum of the probabilities of such y:\n   ...)(: )(2)( xyCy yl C x (1)\nThe distribution C(x) is built on a simplicity principle: outputs which correspond to short programs for the computer program, C, are overwhelmingly more probable than outputs for which there are no short programs.\nThe learner’s task, then, can be viewed as approximating C(x), given a sample x,\ngenerated from the computer program, C. So, for example, if C generated independent samples from a specific stochastic phrase structure grammar, then the learner’s aim is to find a probability distribution which matches the probabilities generated by that stochastic grammar as accurately as possible. To the extent that this is possible, we might conjecture that the learner should (i) be able to predict how the corpus will continue; (ii) decide which strings are\nallowed by C(x); and (iii) generate output similar to that generated by C(x). Framing these points in terms of language acquisition, this means that, by approximating C(x), the learner can, to some approximation, (i) predict what phoneme, word, or sentence will come next (insofar as this is predictable at all); (ii) learn to judge grammaticality; and (iii) learn to produce language, indistinguishable from that to which it has been exposed. We explore these issues in turn in Sections 3-5.\nHow, then, can the learner approximate C(x), given that it has exposure to just one\n(admittedly arbitrarily long) corpus x, and no prior knowledge of the specific computational process, C, which has generated this corpus? It turns out that we can make progress by assuming only that the learner can, in principle, entertain all and only computable hypotheses—i.e., that the learner’s representational resources are universal: i.e., sufficient to encode any possible computation. Elementary results in computability theory (e.g., Odifreddi, 1988) have shown that this assumption of universality is surprisingly mild, and is satisfied by very simple abstract languages (such as the lambda calculus, Barendregt, 1984) and familiar practical languages from Fortran, to C++, to Java and Scheme. We assume, then, that the brain (and our ideal learner) has at least these representational resources.\nWe have stated that a simplicity-based learner favor simple “explanations,” measured in\nterms of code length in some programming language. But surely the length of a program depends on the programming language used? What may be easy to write in Matlab may be difficult to write in Prolog; and vice versa. It turns out, though, that the choice of programming language affects program lengths only to a limited degree. An important result, known as the invariance theorem (Li & Vitányi, 1997), states that, for any two universal programming languages, the length of the shortest program for any computable object in each language is bounded by a fixed constant. A caveat is appropriate, however: “invariance” up to an additive constant is sufficient for establishing mathematical results, such as those below; but choice of representation language is crucial for making learning practically feasible, as we shall note in Section 7. 8 Nonetheless, so long as we assume that the learner’s coding language is universal, we can avoid having to provide a specific account of the program that the learner uses. 9\nNow suppose the learner assumes only that the corpus, x, is generated by a computable\nprocess (and hence makes no assumptions that it is generated by a specific type of grammar, or indeed, any grammar at all; makes no assumption that “sentences” are sampled independently, etc.). Then the probability of each possible x is given by the probability that this sequence will be generating from the output of a random input, y, of length l(y) (as before, by random coin flips) fed to a universal computer, U. 10 Analogous to (1), we can define this “universal\nmonotone distribution” (Solomonoff, 1978) (x):\n   xyUy ylx ...)(: )(2)( (2)\nwhere U(y) are programs y written in the universal programming language. Thus, an ideal learner draws on its universal programming language and the simplicity principle to formulate\n(x). Remarkably, it turns out that (x) serves as a good enough approximation to C(x) to allow the ideal learner to predict future linguistic input; and we show below that this allows the ideal learner to make grammaticality judgments, produce grammatical utterances, and map sound to meaning.\nWhat is the mysterious (x) in more concrete terms? Roughly, it is what would result\nfrom randomly typing into a computer; feeding the resulting “programs” (most of which will, of course, not even be syntactically valid, or will loop indefinitely) to the interpreter for some universal programming language (say, C++); and considering the outputs of the (small number of) valid and terminating programs. Thus, we can alter the familiar image of monkeys randomly hitting the keys on a typewriter and, supposedly, eventually generating the works of Shakespeare, to the image of monkeys typing computer programs, and generating outputs x\naccording to (x). The probability (x) will depend, of course, on the length of the shortest\nprogram generating x, as short programs are overwhelmingly more likely to be chanced upon by the monkey.\nWe shall explore the remarkable properties of (x) shortly. But it is worth noting at the\noutset that (x) is known to be uncomputable (Li & Vitányi, 1997), and hence must be\napproximated. It remains an open question how closely (x) can be approximated and how this\naffects learnability results. Promisingly, computable approximations to the universal distribution can be developed into practical tools in statistics and machine learning (e.g., Rissanen, 1987; Wallace & Freeman, 1987). Related approximations will be considered briefly in Section 7 in relation to developing a methodology for assessing the learnability of specific linguistic patterns.\n3. Prediction\nOne indication of the degree to which a learner understands the patterns in the data in any domain, is its ability to predict. Thus, if the linguistic input is governed by grammatical or other principles of whatever complexity, any learner that can predict how the linguistic material will continue, arbitrarily well, must, in some sense, have learned such regularities. Prediction has been used as a measure of how far the structure of a language has been learned since Shannon (1951); and is widely used as a measure of learning in connectionist models of language processing (Christiansen & Chater, 1994, 1999; Elman, 1990) . And, as we have noted, this result for prediction will be a foundation for results concerning grammaticality judgments, language production, and form-meaning mapping, as we discuss in subsequent sections.\nWe formulate the task of prediction as follows. At each point in a binary sequence x (encoding our linguistic input), generated by computer C, the probabilities that, given input x, that the next symbol is 0 or 1 can be written:\n)(\n)0( )|0(\nx\nx x\nC\nC\nC \n   ;\n)(\n)1( )|1(\nx\nx x\nC\nC\nC \n   (3)\nwhere )|0( xC and )|1( xC represent the probabilities that the subsequence x is followed by a 0 and 1 respectively; and )0(xC and )1(xC are the probabilities the specific sequence of x followed by 0 or 1, respectively. But the ideal learner does not have access to C(x), but instead uses (x) for prediction. Thus, the learner’s predictions for the next item of a binary\nsequence that has started with x is:\n)(\n)0( )|0(\nx\nx x\n\n   ;\n)(\n)1( )|1(\nx\nx x\n\n   (4)\nA key result by Solomonoff (1978), which we call the Prediction Theorem, shows that, in a\nspecific rigorous sense, the universal monotone distribution , described above, is reliable for\npredicting any computable monotone distribution, , with very little expected error. More\nspecifically, the difference in these predictions is measured by the square of difference in the\nprobabilities that  and  assign to 0 being the next symbol:\n 2)|0()|0()(Error xxx  \n(5)\nAnd the expected sum-squared error for the nth item in the sequence is:\nsn  (x)Error x:l (x )n 1\n (x) (6)\nThe better  predicts , the smaller sn will be. Given this, the overall expected predictive success of the method across the entire sequence is obtained by summing the sn across all n:\n  1=n ns (7)\nSolomonoff’s Prediction Theorem shows that predictions using  approximate any computable distribution, , so that  \n1=n\nns is bounded by a constant. Thus, as the amount of data increases,\nthe expected prediction error goes to 0. Specifically, the following result holds:\nPrediction Theorem (Solomonoff, 1978): Let  be a computable monotone distribution,\npredicted by a universal distribution . Then,\n)( 2\n2log\n1\nKs e\nn\nn  \n\n(8)\nwhere K() is the length of the shortest program on the universal machine that implements ,\nknown as its Kolmogorov complexity (see Li & Vitányi, 1997, for further details, and an accessible proof).\nThe Prediction Theorem shows that learning by simplicity can, in principle, be expected\nto converge to the correct conditional probabilities for predicting subsequent linguistic material. This implies that the learner is able to learn the structure of the language---because if\nnot, the learner will not know which sentences are likely to be said, and hence will make prediction errors. This results suggest that, given sufficient positive evidence, linguistic restrictions, such as those on the allowed contraction of is mentioned above, are learnable from positive evidence. Here “sufficient” means enough language input has been observed such that the (more complex) grammar which contains the restriction provides the simplest overall coding of the data, because it provides an efficient specification of that input. The learner which does not learn these restrictions will continue to predict the ungrammatical form when it is not allowed, and thus accrue an infinite number of prediction errors. Note that while the Prediction Theorem demonstrates that an ideal learner, with sufficient positive evidence, will learn to respect these linguistic restrictions, there is no claim that the learner can recover grammar that generated the language---but the learner’s predictions will capture the structure of the language arbitrarily closely.\n4. Learning grammatical judgments\nOne of the distinctive shifts from Bloomfield’s (1933) version of structural linguistics to Chomsky’s (1957) generative grammar concerns methodology: while Bloomfield considered the goal of linguistics to be inducing patterns in language from corpora of utterances, Chomsky rejected this approach, and stressed instead capturing native speaker intuitions about, for example, the grammaticality of sentences. Our discussion of prediction, based on the linguistic input to the learner, seems closely allied to Bloomfield’s perspective. But Chomsky’s approach presents a fresh challenge: human language learners appear not just to learn to predict, based on the structure of what they hear---instead, people appear to be able learn to be able to distinguish grammatical from ungrammatical sentences from positive evidence alone. This\nraises the question: under what conditions are grammaticality judgements learnable from positive data?\nIt turns out that the task of prediction naturally extends surprisingly naturally to that of\ngrammaticality judgments. The crucial move is to consider predictions for larger units linguistic material (e.g., words, rather than binary codes) and ask how often the predicted utterance will correspond to a continuation that is a grammatical sentence. The crucial question is how far the learner’s predictions fit with the set of options that are grammatically possible in the language. Specifically, we can ask: How often does the learner overgeneralize such that its guesses violate the rules of the language (e.g., predicting a contraction of is where it is not allowed)? And, conversely, how often does the learner undergeneralize what is possible, such that it fails to guess continuations that are acceptable (e.g., not predicting a contraction when it is allowed)? Results for overgeneralization and undergeneralization errors are examined in turn."
    }, {
      "heading" : "4.1 Grammaticality errors: overgeneralization",
      "text" : "When considering grammaticality, it is, as we have noted, convenient to consider language input as a sequence of words, rather than coded as a binary form. Thus, instead of dealing with\ndistributions, , , over binary sequences, one may consider distributions P and P.over sequences of a finite vocabulary of words. Suppose that the learner has seen a corpus, x, of j-1\nwords and has a probability j(x) of incorrectly guessing a jth word which happens to be ungrammatical, i.e., the string cannot be completed to produce a grammatical sentence. One can write:\n j (x)  P(k | x) k : xk is un grammatical, l (x ) j1  (9)\nAs before, we focus on the expected value  j :\n j  P(x) x:l (x) j1   j(x) (10)\nThis expected value captures the probability that the learner’s prediction concerning the jth word will not actually be allowable in the language—that the learner overgeneralizes what the language contains. But such overgeneralizations are, of course, a failure of prediction---and we know, from the Prediction Theorem above, that errors in the learner’s predictions are gradually eliminated. So the Prediction Theorem can be used to provide a ‘bound’ on the number of overgeneralization errors that the learner will generated. Specifically, it is possible to derive the following ‘overgeneralization theorem’ (Chater & Vitányi, 2007):\n j j1\n\n  K()\nloge2 (11)\nThat is, the total expected amount of probability devoted by the learner to overgeneralizations, in the course of encountering an infinite corpus, sums to a finite quantity; and this quantity is close to the length of the shortest program that generates the linguistic data. Thus, the expected amount of overgeneralization must tend to zero, as more of the corpus has been encountered; and the number of errors will depend on the complexity of the language to be learned (where complexity is measured in terms of program length).\nThe ability to deal with overgeneralization of the grammar from linguistic experience is\nparticularly relevant to previous discussions of the “logical problem” of language learnability, discussed above (Baker & McCarthy, 1981; Hornstein & Lightfoot, 1981; Pinker, 1979; Pinker, 1984). The learner only hears a finite corpus of sentences. Assuming the language is\ninfinite, a successful learner must therefore infer the acceptability of an infinite number of sentences that it has never heard. Thus, not hearing a sentence cannot be evidence against its existence. As noted above, this has raised the puzzle of whether it is possible for overly general grammars ever to be eliminated by the learner. The overgeneralization theorem shows that an ideal learner using the simplicity principle will eliminate overly general grammars, given a sufficiently large corpus."
    }, {
      "heading" : "4.2 Grammaticality errors: undergeneralization",
      "text" : "The universal distribution used by the ideal learner was defined as being a combination of all possible (computable) distributions over corpora, and thus all grammatical sentence in the language will always be deemed possible (assigned non-zero probability). This immediately implies that an ideal learner will never strictly undergeneralize, i.e., incorrectly deem a grammatical utterance to have probability 0. But perhaps an ideal learner could drastically underestimate a sentence’s probability of occurrence. One can investigate the extent to which an ideal learner might commit such errors of ‘soft’ undergeneralization, putting an upper bound on the number of soft undergeneralizations an ideal learner will make. Suppose that the learner underestimates, by a factor of at least f, the probability that word k will occur after linguistic\nmaterial x. That is, P(k|x) f P(k|x). Let j,f(x) denote the probability that the word that is the true continuation will be one of the k for which this underestimation occurs:\n   )|()|(.: , )|()( xkPxkPfk fj xkPx   (12)\nThe corresponding expected probability is:\n)()( 1)(: , xxP j jxlx fj     (13)\nThen, the following undergeneralization theorem holds, which bounds the expected number of undergeneralization errors throughout the corpus, i.e.,  \n  1 , j fj :\nef K\nj\nfj\n21\n, log\n1 )(\n \n(14)\nso long as f > e, where e is the mathematical constant 2.71...\nThus the expected number of ‘soft’ undergeneralizations is bounded, even for an\ninfinitely long sequence of linguistic input and the expected rate at which such errors occur\nconverges to zero. As with overgeneralizations, the upper bound is proportional to K(), the\ncomplexity of the underlying computational mechanism generating the language (including, presumably, the grammar). The higher the underestimation factor f to be, the fewer such undergeneralizations occur.\nIn summary, formal results have shown that an ideal learner, using the universal\nprobability distribution, P, and derived from the simplicity principle, can learn to make accurate grammaticality judgments that avoid both overgeneralizations and undergeneralizations---an issue that, as noted above, has been viewed as of fundamental importance in recent linguistic theorizing. In the description above, grammaticality judgments have been framed as the process of guessing which word comes next. However, it is important to note that these results extend to all other units of linguistic analysis, e.g., prediction of utterances on the level phonemes, syllables, or sentences.\n5. Learning to Produce Language\nOne method of describing language production is to assume that it is simply a matter of predicting future utterances of arbitrarily long lengths. Thus, a learner, given an entire history\nof linguistic input, can eventually “join in” and starts saying its predictions. Production success can be assessed by how well these productions blend in with the linguistic input --i.e., how well the learner’s productions match those that other speakers of the language (i.e., those producing the learner’s corpus) might equally well have said. This is, of course, a highly limited linguistic goal, given that a key purpose of language is to express one’s own thoughts, which may be diverge from what others have said before. (We will consider how this limitation can partially be dealt with in the next section.) However, as a first step, one can begin to assess a learner’s ability to speak a language by assessing whether the learner can blend into the ongoing “conversation.”\nBlending in can be described as the ability to match the actual probability that a new\nsequence of utterances, y, will follow the previous utterances, x, which have been heard so far\nin the conversation. This is the probability (y|x), which reflects the distribution of continued\nsequences that would be uttered by speakers of the language. As before, the learner’s stream of utterances can be defined on any linguistic level, e.g., phonemes, words or sentences. Because\nthe ideal learner generates utterances using the distribution it learned in prediction, , the\nlearner will predict continuations according to (y|x). The learner will blend in, to the extent\nthat (y|x) is a good approximation to (y|x)--i.e., the extent to which the learner has a\npropensity to produce language that other speakers have a propensity to produce. Note, though, that the objective is now not merely predicting the next binary code, piecemeal; the material to be predicted, y, can be an arbitrarily large chunk of linguistic material (e.g., an entire clause or sentence).\nIt turns out that (y|x) is a good approximation to any relevant (y|x) (Li & Vitányi,\n1997; this result does not follow directly from the Prediction Theorem): If  is, as above, a\nprobability distribution associated with a monotone computable process, and  denotes the\nuniversal distribution, then for any finite sequence y, as the length of sequence x tends to infinity:\n1 )|(\n)|( \nxy\nxy\n\n (15)\nwith a probability tending to 1, for fixed utterance y and growing prior linguistic experience x. Thus, viewing (15) in the context of language production, this means that, in the asymptote, the learner will blend in arbitrarily well, so that its language productions are indistinguishable from those of the language community to which it has been exposed.\n6. Learning to map linguistic forms to semantic representations\nIn addition to being able to predict, make grammatical judgments, and produce linguistic regularities, language acquisition also involves associating linguistic forms with meanings. Indeed, to the ability to judge grammaticality, or produce language indistinguishable from that of one’s speech community, would be pointless unless it were associated with the ability to communicate: to map from utterances to some representation of their interpretations, and back (we remain neutral here about nature of these representations).\nA common assumption among researchers (Pinker, 1989) is that the child can infer\nsemantic interpretations from linguistic context. Therefore the problem of learning interpretations from linguistic input can be framed as a problem of induction from pairs of linguistic and semantic representations. One can then show that, given sufficient pairs, the ideal learner is able to learn this mapping, in either direction, in a probabilistic sense. This result holds even though the mapping between linguistic and semantic representations can be many-\nto-many. That is, linguistic representations are often ambiguous; and the same meaning can often be expressed linguistically in a number of different ways.\nConcretely, we view the learner’s problem as learning a relation between linguistic\nrepresentations (e.g., as the i th\nstring of words), Si, and a semantic interpretations, Ij,\n(representing the j th meaning of the string). Suppose that the language consists of a set of ordered pairs {<Si,Ij>}, which we sample randomly and independently according to computable probability distribution Pr(Si,Ij).\nNow we can apply the Prediction Theorem, as described above, but where the data now\nconsist of pairs of sentences and interpretation, rather than strings of phonemes or words. So, when provided with a stream of sentence-interpretation pairs sampled from Pr(Si,Ij), the learner can, to some approximation, infer the joint distribution Pr(Si,Ij). But, of course, approximating this joint distribution is only possible if the learner can approximate the relationship between sentences Si and interpretations Ij.\nWriting the length of the shortest program that will generate the computable joint\ndistribution, Pr(Si,Ij), as K(Pr(Si,Ij)), the Prediction Theorem above ensures that this joint distribution is learnable from positive data by an ideal learner---if that positive data includes both form and meaning. Specifically, by (8), this has an expected sum-squared error bound of\n)),(Pr( 2\n2log ji e ISK . Hence the expected value of error per data sample, will tend to zero because\nthis bound is finite, but the data continues indefinitely.\nIf ordered pairs of <Si,Ij> items can be predicted, then the relation between sentences\nand interpretations can be captured; and this implies that the mapping from sentences to probabilities of interpretations of those sentences, Pr(Ij| Si), and the mapping from interpretations to probabilities of sentences with those interpretations, Pr(Si|Ij), are learnable. 11\nThus, we can conclude that the ideal learner is able to learn to map back and forth between sentences and their interpretations, given a sufficiently large supply of sentence-interpretation pairs as data. That is, in this specific setting at least, the relation between form and meaning can be derived from positive data alone.\n7. Scaling down simplicity: A practical method for assessing learnability\nWe have described a range of rather abstract theoretical results concerning the viability of language learning by simplicity. But how far can the simplicity-based approach be “scaleddown” to inspire concrete models of learning? The practical instantiation of the simplicity principle has been embodied using the minimum description length (MDL, Rissanen, 1987) and minimum message length (MML, Wallace & Freeman, 1987) frameworks. Simplicity has also widely been explored as general principle underpinning concrete models in a range of areas of perception and cognition (e.g., Attneave & Frost, 1969; Jacob Feldman, 2000; Hochberg & McAlister, 1953; Leeuwenberg, 1969), including language (e.g., Brent & Cartwright, 1996; Dowman, 2000; Ellison, 1992; Goldsmith, 2001; Onnis, Roberts & Chater, 2002; Vousden, Ellefson, Solity & Chater, 2011; Wolff, 1988). Closely related Bayesian methods have also been widely employed (e.g., Kemp, Perfors & Tenenbaum, 2007; Langley & Stromsten, 2000; Perfors, Regier & Tenenbaum, 2006; Stolcke, 1994).\nThe type of theoretical analysis that we have outlined above applies, by the invariance\ntheorem, irrespective of specific choices of representations (as long as these are sufficiently powerful). But to make the approach computationally concrete requires choosing a specific representation—typically this will be a representational formalism developed in linguistics (e.g., some type of grammar). A code length can then be assigned both to the rules of the\ngrammar as well as to the corpus when encoded in terms of those rules (the corpus might consist of all the utterances that a learner has experienced so far, or a subset of these).\nSuppose that we wish to evaluate how much data is required to learn a particular\nlinguistic regularity. This can be heuristically assessed by comparing two grammars which are identical aside from the fact that only one captures the regularity of interest. For example, consider how we might assess whether the corpus contains sufficient information to learn the restrictions on cases where is can be contracted that we described earlier. A grammar containing this additional regularity requires, of course, greater code-length than one that does not; but, on the other hand, because the resulting model of the language is more accurate, the code length of the corpus, given this more accurate model, will be shorter. Whether the ‘balance’ favors the more complex but accurate grammar (thus allowing the restrictions on contraction to be learned) depends on the corpus. For a null, or a short, corpus, the advantage of a more accurate language model will not be sufficient; however, once the corpus becomes sufficiently long, the more accurate model will produce a shorter overall code-length, and the regularity will be learned. The question is: how long does the corpus need to be, for the regularity to learnable?\nAs discussed in Section 1, the simplicity principle automatically trades-off competing\nsimpler and complex grammars. Simple, but over-general, grammars can be described more briefly, but because they are less accurate descriptions of actual language structure, they give an inefficient descriptions of language input. More complex grammars, which include linguistic restrictions, have more complex descriptions, but better capture the language and so give more efficient descriptions of the language input. By “investing” in a more complicated grammar, which contains a restriction on a construction, the language speaker obtains encoding\nsavings every time the construction occurs. Intuitively, a linguistic restriction is learned when the relevant linguistic context occurs often enough that the accumulated savings makes the more complicated grammar worthwhile, just as the extra cost of an energy saving appliance is justified if it is used sufficiently often.\nRecently, a simple and practical framework for assessing learnability of a wide variety\nof linguistic constructions under simplicity has been proposed (Hsu and Chater, 2010). Using natural-language corpora to simulate the language input available to the learner, this framework quantifies learnability (e.g., in estimated number of years of linguistic exposure) for any given linguistic constraint, such as the contraction of is mentioned earlier.\nTo get started, we need some description of the grammatical rule to be learned, i.e., a\ndescription of an original, incorrect (over-general) grammar and the new, correct grammar, which contains the restriction rule. Moreover, we need a corpus which will serve as a proxy for the learner’s input. Given these, the framework provides a method for quantifying an upper bound on learnability from language input. This framework assumes an ideal statistical learner and thus provides an upper bound on learnability based on language statistics. However, measures of learnability should give an indication of the ease with which various linguistic constraints can be learned.\nWhile the details of implementing this framework are described elsewhere (Hsu &\nChater, 2010; Hsu, Chater & Vitányi, 2011), an intuitive description of how this framework works is as follows: Under this framework, the learnability is affected by three factors. (1) The first is the complexity of the rule to be learned (greater complexity will decreases learnability). (2) The second concerns with what probability the “disallowed forms” would otherwise be expected to appear in place of other similar constructions which do occur (e.g., how often do\nnon-contracted forms “he is,” “she is” etc., and in which syntactic contexts). (3) How frequently does the putative regularity apply in real language input. (1) and (2) determine how many occurrences of contexts where the regularity applies are needed for learning and (3) then will determine how many millions of words (or years of language input) are required to accrue the number of occurrences needed. These assumptions are all, of course, provisional; and hence results from this approach are suggestive rather than definitive.\nHsu and Chater (2010) applied this general framework to consider the learnability of\nvarious linguistic restrictions, many of which have been viewed as presenting fundamental learnability challenges. They assuming that a learner’s input can be approximated using corpora of adult speech and writing, such as the Corpus of Contemporary American English (COCA). They found that the number of years of linguistic input required to learn putatively “unlearnable” constructions varied surprisingly widely, from a matter of months to more than a lifetime.\nMight these learnability differences across different linguistic restrictions correlate with\nhow well people actually do learn them? This was tested in an experiment on adult native English speakers in Hsu, Chater and Vitányi (2011). Figure 1a shows the predictions for 15 constructions from Hsu and Chater (2010), sorted in descending learnability. Figure 1b shows how often the constructions occur per year of linguistic input, estimated from COCA. Note that the occurrence rates do not monotonically decrease with the years required to learn the construction, because other factors that affect learnability, e.g., (1) and 2) listed above). Interestingly, the more learnable the constraints according to the simplicity analysis, the better they are learned in practice by native speakers: as log(1/predicted years needed) increased, the difference in the grammatical acceptability of the grammatical vs. ungrammatical form of the\nconstruction also increased. Thus, a simplicity-based approach to language acquisition can provide not only general learnability results, but concrete predictions concerning how people learn language.\n8. Conclusion\nIn this paper, we have reviewed some recent results concerning learning language from\nexperience by employing the simplicity principle: that is, favouring models of the language to the degree that they provide a short encoding of the linguistic input. We have shown theoretical results that indicate that an “ideal learner” implementing the simplicity principle can learn to predict language from experience; to determine which sentences of a language are grammatical to an arbitrarily good approximation (assuming, somewhat unrealistically, that the corpus of linguistic experience is noise-free, i.e., containing only grammatical sentences); to produce language; and to map between sentences and their interpretations. This “ideal” learning approach is valuable for determining what information is contained in a corpus. Yet it cannot be implemented computationally, as the relevant calculations are known to be uncomputable (Li & Vitányi, 1997). Nonetheless, we have also shown how a local approximation to such calculations can be used to choose between different grammars which do or do not contain specific regularities (especially those concerned with exceptions) that have been viewed as posing particular problems for theories of language acquisition. Overall, these results form part of a wider tradition of analytic and computational results on language learning which suggest that general a priori arguments about whether language acquisition requires language-specific innate constraints can be replaced by a more precise formal and empirical analysis.\nFigure 1:\nReference List\nAngluin, D. (1980). Inductive inference of formal languages from positive data.\nInformation and Control, 45, 117–135.\nAngluin, D. (1988). Identifying languages from stochastic examples. Technical Report.\nDepartment of Computer Science, Yale University.\nAttneave, E, & Frost, R. (1969). The determination of perceived tridimensional\norientation by minimum criteria. Perception & Psychophysics, 6, 391-396.\nBaker, C. L. & McCarthy, J. J. (1981). The logical problem of language acquisition.\nCambridge, Mass: MIT Press.\nBarendregt, H. P. (1984). The lambda calculus. Amsterdam: Elsevier.\nBloomfield, L. (1933). Language. New York: Henry Holt.\nBowerman, M. (1988). The 'No Negative Evidence' Problem: How do Children avoid\nconstructing an overly general grammar? In J.Hawkins (Ed.), Explaining Language Universals (pp. 73-101). Oxford: Blackwell.\nBrent, M. R. & Cartwright, T. A. (1996). Distributional regularity and phonotactic\nconstraints are useful for segmentation. Cognition, 61, 93-126.\nBrown, R. & Hanlon, C. (1970). Derivational complexity and order of acquisition in\nchild speech. (J.R. Hayes ed.) New York: Wiley.\nCarlucci & Case, J. (2013). On the necessity of U-shaped learning. Topics in Cognitive\nScience (this issue).\nChater, N. & Vitányi, P. (2002). Simplicity: A unifying principle in cognitive science?\nTrends in Cognitive Sciences, 7, 19–22.\nChater, N. & Vitányi, P. (2007). Ideal learning' of natural language: positive results\nabout learning from positive evidence. Journal of Mathematical Psychology, 51, 135-163.\nChomsky, N. (1957). Syntactic structures, The Hague/Paris: Mouton.\nChomsky, N. (1980). Rules and representations. Cambridge, MA: MIT Press.\nChristiansen, M. H. & Chater, N. (1994). Generalization and connectionist language\nlearning. Mind & Language, 9, 273-287.\nChristiansen, M. H. & Chater, N. (1999). Connectionist natural language processing:\nThe state of the art. Cognitive Science, 23, 417-437.\nChristiansen, M. H. & Chater, N. (2007). Generalization and connectionist language\nlearning. Mind & Language, 9, 273-287.\nChristiansen, M. & Chater, N. (2010). Language acquisition meets language evolution.\nCognitive Science, 34, 1131-1157.\nClark, A. & Eyraud. R. (2007). Polynomial identification in the limit of substitutable\ncontext-free languages. Journal of Machine Learning Research, 8, 1725-1745.\nClark, A., & Lappin, S. (2013). Complexity in language acquisition. Topics in\nCognitive Science (this issue).\nCover, T. M. & Thomas, J. A. (2006). Elements of Information Theory (2 nd Edition),\nWiley.\nCrain, S. & Lillo-Martin, D. (1999). Linguistic theory and language acquisition.\nOxford: Blackwell.\nDemetras, M., Post, K., & Snow, C. (1986). Feedback to first language learners: The\nrole of repetitions and clarification questions. Journal of Child Language, 13, 275-292.\nDowman, M. (2000). Addressing the learnability of verb subcategorizations with\nBayesian inference. In L. R. Gleitman & A. K. Joshi (Eds.). Proceedings of the Twenty Second Annual Conference of the Cognitive Science Society. Mahwah, NJ: Erlbaum.\nDresher, B. & Hornstein, N. (1976). On Some Supposed Contributions of Artificial\nIntelligence to the Scientific Study of Language. Cognition, 4, 321-398.\nEllison, M. (1992). The machine learning of phonological structure. PhD Thesis,\nUniversity of Western Australia.\nElman, J. (1990). Finding Structure in Time. Cognitive Science, 14, 179-211.\nFeldman, Jacob (2000). Minimization of boolean complexity in human concept\nlearning. Nature, 403, 630-633.\nFeldman, Jerome (1972). Some decidability results on grammatical inference and\ncomplexity. Information and Control, 20, 244–262.\nGold, E. M. (1967). Language identification in the limit. Information and Control, 16,\n447–474.\nGoldsmith, J. (2001). Unsupervised learning of the morphology of a natural language.\nComputational Linguistics, 27, 153-198.\nHochberg, J. & McAlister, E. (1953). A quantitative approach to figure “goodness.”\nJournal of Experimental Psychology, 46, 361-364.\nHorning, J. J. (1969). A study of grammatical inference. Technical Report CS 139,\nComputer Science Department, Stanford University.\nHornstein, N. & Lightfoot, D. (1981). Explanation in linguistics: the logical problem of\nlanguage acquisition. London: Longman.\nHsu, A. & Chater, N. (2010). The logical problem of language acquisition: A\nprobabilistic perspective. Cognitive Science, 34, 972-1016.\nHsu, A., Chater, N., & Vitányi, P. (2011). The probabilistic analysis of language\nacquisition: Theoretical, computational, and experimental analysis. Cognition, 120, 380-390.\nJain, S., Osherson, D. N., Royer, J. S., & Kumar Sharma, A. (1999). Systems that learn\n(2nd edition). Cambridge, MA: MIT Press.\nKemp, C., Perfors, A., & Tenenbaum, J. B. (2007). Learning overhypothesis with\nhierarchical Bayesian models. Developmental Science, 10, 307-321.\nKlein, D. & Manning, C. (2005). Natural language grammar induction with a generative\nconstituent-context model. Pattern Recognition, 38, 1407-1409.\nLangley, P. & Stromsten, S. (2000). Learning context-free grammars with a simplicity\nbias. Proceedings of the Eleventh European Conference on Machine Learning, 220-228.\nLeeuwenberg, E. L. J. (1969). Quantitative specification of information in sequential\npatterns. Psychological Review, 76, 216-220.\nLi, M. & Vitányi, P. (1997). An introduction to Kolmogorov complexity and its\napplications (2 nd Edition). Springer.\nMach, E. (1959). The analysis of sensations and the relation of the physical to the\npsychical. New York: Dover Publications (Original work published 1886).\nMacWhinney, B. (1993). The (il)logical problem of language acquisition. In\nProceedings of the 15th annual conference of the Cognitive Science Society (pp. 61–70). Mahwah, NJ: Erlbaum.\nMacWhinney, B. (2004). A multiple process solution to the logical problem of language\nacquisition. Journal of Child Language, 31, 883–914.\nMarcus, G. F. (1993). Negative evidence in language acquisition. Cognition, 46, 53-85.\nNiyogi, P. (2006). The computational nature of language learning and evolution.\nCambridge, MA: MIT Press.\nOdifreddi, P. (1988). Classical recursion theory. North Holland: Elsevier.\nOnnis, L., Roberts, M., & Chater, N. (2002). Simplicity: A cure for overgeneralizations\nin language acquisition? Proceedings of the 24th Annual Conference of the Cognitive Science Society, 720-725.\nPereira, F. C. N. & Warren, D. H. D. (1980). Definite clause grammars for language\nanalysis. Artificial Intelligence, 13, 231-278.\nPerfors, A., Regier, T., & Tenenbaum, J. B. (2006). Poverty of the Stimulus? A rational\napproach. Proceedings of the Twenty-Eighth Annual Conference of the Cognitive Science Society, 663-668.\nPinker, S. (1979). Formal models of language learning. Cognition, 7, 217-283.\nPinker, S. (1984). Language learnability and language development. (7 ed.)\nCambridge, MA: Harvard University Press.\nPinker, S. (1989). Learnability and Cognition: The acquisition of argument structure.\nCambridge, MA: MIT Press.\nPinker, S. & Bloom, P. (1990). Natural language and natural selection. Behavioral and\nBrain Sciences, 13, 707-784.\nPylyshyn, Z. W. (1984). Computation and Cognition: Toward a Foundation for\nCognitive Science, Cambridge, MA: Bradford Books/MIT Press.\nRissanen, J. (1987). Stochastic complexity. Journal of the Royal Statistical Society,\nSeries B, 49, 223–239.\nRohde, D. L. T., & Plaut, D. C. (1999). Language acquisition in the absence of explicit\nnegative evidence: How important is starting small? Cognition, 72, 68–109.\nShannon, C. E. (1951). Prediction and entropy of printed English. Bell Systems\nTechnical Journal, 31, 64.\nSolomonoff, R. J. (1978). Complexity-based induction systems: comparisons and\nconvergence theorems. IEEE Transactions on Information Theory, IT, 24, 422-432.\nSteyvers, M., Griffiths, T., & Dennis, S. (2006). Probabilistic inference in human\nsemantic memory. Trends in Cognitive Sciences, 10, 309-318.\nStolcke, A. (1994). Bayesian Learning of Probabilistic Language Models. Department\nof Electrical Engineering and Computer Science, University of California Berkeley.\nTomasello, M. (2004). Syntax or semantics? Response to Lidz et al. Cognition, 93,\n139–140.\nWallace, C. S., & Freeman, P. R. (1987). Estimation and inference by compact coding.\nJournal of the Royal Statistical Society, Series B, 49, 240–251.\nWharton, R. M., (1974). Approximate language identification, Information and\nControl, 26, 236-255\nVousden, J.I., Ellefson, M.R., Solity, J.E., & Chater, N. (2011). Simplifying reading:\nApplying the simplicity principle to reading. Cognitive Science, 35, 34-78.\nWolff, J. G. (1988). Learning syntax and meanings through optimisation and\ndistributional analysis. In Y. Levy, I. M. Schlesinger & M. D. S. Braine (Eds.), Categories and processes in language acquisition, (pp. 179-215). Hillsdale, NJ: LEA.\nFootnotes 1. Language acquisition involves dealing with other challenges, including the computational complexity of searching the space of grammars (Clark & Lappin, 2013), but these are outside our scope here. 2. Typical speech is, of course, full of grammatical errors, repetitions, and incomplete utterances. Along with most other learnability analyses, we will ignore the “noisy” character of linguistic input. 3. All code lengths are assumed, by convention, to be written in a binary alphabet. 4. This approach implicitly assumes, among other things, no sequential dependencies between sentences, but generalizations are relatively straightforward. 5. Recovery from overgeneralization can be explored in a number of frameworks, for example, Carlucci and Case (2013). 6. Informally, we can view this process as embodying a Turing machine (or any other computer) combined with a source of randomness (i.e., a sequence of coin flips). The source of randomness captures the possibility that the process of generating the linguistic input may be non-deterministic (although it need not be); the restriction to computable probability distributions requires that the structure in the linguistic input is computable.\n7. Strictly,  is a measure, rather than a probability distribution, as the sequence is infinite;\nindeed, it is actually a semi-measure. Measures and semi-measures are generalizations of the standard notion of probability distributions. We ignore these technicalities here (see Chater & Vitányi, 2007; and Li & Vitányi, 1997). 8. Note, though, that people presumably will share a mental representation language. Hence, the representational language used to formulate hypotheses in learning language will\npresumably automatically be ideally suited to the natural languages that have been learned and generated by past generations of speakers (see, e.g., Christiansen & Chater, 2010). 9. We have discussed how (probabilistic) generative grammars and computer programs generate linguistic data. The relation between these can be very close: in some formalisms (e.g., Definite Clause Grammars, Pereira & Warren, 1980), the program generating from the grammar is just a specification of the grammar itself. In general, the picture is slightly more complex, but we do not consider this further here. 10. Technically it is important that this computer is monotone (Chater & Vitányi, 2007), but we shall ignore this complication. 11. Of course, if interpretation, Ij, is such that Pr(Ij)=0, then the fact that Pr(Ij, Si) can be approximated arbitrarily well says nothing about Pr(Si|Ij); similarly for sentences, Si, such that Pr(Si)=0. But the learner presumably needs only learn sentences that express meanings that might actually arise; and interpret sentences that might actually be said, so this restriction is fairly mild."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "Children learn their native language by exposure to their linguistic and communicative environment, but apparently without requiring that their mistakes are corrected. Such learning from “positive evidence” has been viewed as raising “logical” problems for language acquisition. In particular, without correction, how is the child to recover from conjecturing an over-general grammar, which will be consistent with any sentence that the child hears? There have been many proposals concerning how this “logical problem” can be dissolved. Here, we review recent formal results showing that the learner has sufficient data to learn successfully from positive evidence, if it favours the simplest encoding of the linguistic input. Results include the learnability of linguistic prediction, grammaticality judgments, language production, and form-meaning mappings. The simplicity approach can also be “scaled-down” to analyse the learnability of specific linguistic constructions, and is amenable to empirical test as a framework for describing human language acquisition. Simplicity-based approach to language learning 3 Children appear to learn language primarily by exposure to the language of others. But how is this possible? The computational challenges of inferring the structure of language from mere exposure are formidable. In light of this, many theorists have conjectured that language acquisition is only possible because the child possesses cognitive machinery that fits especially closely with the structure of natural language. This could be because the brain has adapted to language (Pinker & Bloom, 1990), or because language has been shaped by the brain Christiansen & Chater, 2007). A number of informal arguments concerning the challenge of language learning from experience have been influential. Chomsky (1980) argued that the “poverty of the stimulus” available to the child was sufficiently great that the acquisition of language should be viewed as analogous to the growth of an organ, such as the lung or the heart, unfolding along channels pre-specified in the genome. Here, we focus on a specific facet of poverty of the stimulus: that children do not appear to receive or attend to “negative evidence:\" explicit feedback that certain utterances are ungrammatical (Bowerman, 1988; Brown & Hanlon, 1970; Marcus, 1993). 1 The ability to learn language in the absence of negative evidence is especially puzzling, given that linguistic rules are riddled with apparently capricious restrictions. For example, a child might naturally conclude from experience that there is a general rule that is can be contracted, as in He’s taller than she is. But contractions are not always allowed, for example: *He is taller than she’s. The puzzle is that, once the learner has entertained the possibility that the overgeneral rule is correct, it appears to have no way to “recover” from overgeneralization and recognise that restrictions should be added. This is because each contraction that it hears Simplicity-based approach to language learning 4 conforms to the overgeneral rule. Now, if the learner uses the overgeneral rule to generate language, then it will from time to time produce utterances such as *John isn't coming but Mary’s. A listener’s startled reaction or look of incomprehension might provide a crucial clue that the rule is overgeneral. However, such feedback is the very negative evidence that appears to be inessential to child language acquisition. Thus, if children do not require such negative evidence, how can they recover from such overgeneralisations? Various scholars argue that they cannot: Restrictions on overgeneral grammatical rules must, instead, be innately specified (e.g., Crain & Lillo-Martin, 1999). Other theorists argue that avoiding overgeneral rules poses a fundamental “logical problem” for language acquisition (Baker & McCarthy, 1981; Dresher & Hornstein, 1976). One way to defuse the puzzle is to challenge its premise. One possibility is that, despite appearances, children can access and use negative evidence in a subtle form. In this paper, we set aside these contentious issues (e.g., Demetras, Post & Snow, 1986; Marcus, 1993) and argue that, whether or not negative evidence is available to, or used by, the child, language can successfully be learned without it (following, for example, MacWhinney, 1993; 2004; Rohde & Plaut, 1999; Tomasello, 2004). The arguments for learnability from positive evidence presented here are part of a broader tradition of research on learnability (e.g., Angluin, 1980, 1988; Clark, & Eyraud, 2007; Feldman, 1972; Gold, 1967; Horning, 1969; Jain, Osherson, Royer & Kumar Sharma, 1999; Niyogi, 2006; Wharton, 1974). And formal learnability arguments are complementary to recent developments of language engineering systems, which have shown that it is possible to learn automatically non-trivial aspects of phonology, morphology, syntax and semantics from positive language input (Goldsmith, 2001; Klein & Manning, 2005; Steyvers, Griffiths & Simplicity-based approach to language learning 5 Tenenbaum, 2006). While such systems are still very far from being able to acquire language from mere exposure, the pace of progress suggests that a priori barriers to learning may not necessarily be insurmountable. Rather than surveying these developments, and indicating how they may be extended, here we will take a more direct approach: we focus on one major line of positive learnability results based on the ‘simplicity principle’. We begin by introducing the simplicity principle (Section 1) and considering how it can be embodied in an “ideal learner” (Section 2). We then outline some recent formal results on how the simplicity principle can be used to learn aspects of language such as utterance prediction, grammaticality judgments, language production, and mapping between form and meaning (Sections 3-6). We then briefly describe a practical method for assessing learnability of linguistic patterns using the simplicity approach, and how this assessment can be linked with experimental data (Section 7). Overall, the contribution of the work reviewed here is to show that, under fairly mild conditions, language acquisition from sufficient amounts of positive evidence is possible; and to indicate how the simplicity-based approach can potentially provide a framework for understanding child language acquisition. 1. Ideal learning using a simplicity principle The simplicity principle has a long history in the philosophy of science and the study of perception (e.g., Mach, 1959/1886), and has been proposed as a general cognitive principle (Chater & Vitányi, 2002). A formal analysis of simplicity learning starts with supposing a learner (human or artificial) that is faced with a set of positive data. For language, this data is a set of observed grammatical sentences. 2 Any set of observed sentences will be consistent with an infinite number of grammars. That is, any set of sentences could have been generated by any Simplicity-based approach to language learning 6 of an infinite number of grammars. How can the learner choose among these infinite possibilities? The simplicity principle recommends that the learner prefers hypotheses which allows for the simplest encoding of the data. For language, the data will be the observed sentences, and hypotheses are grammars (or other linguistic representations), which can be viewed as a set of probabilistic rules which captures the patterns in the linguistic input to the learner. Simplicity can be measured by viewing hypotheses (here, grammars) as computer programs which encode the data (the data is generated as the output of the program). The simplicity principle thus favors the grammar that provides the shortest encoding of the data. 3 How can a grammar be viewed as a computer program for encoding linguistic input? One concrete approach involve two steps. The first step is to specifying the grammatical rules (and, crucially, probabilities of their use). This defines a probabilistic process for generating sentences; and thus defines a probability distribution over possible strings. The second step is to encode the specific sentences in the input. It is intuitively clear that the most efficient way to do this is to reserve shorter codes for probable strings; and longer codes for less probable strings. A basic result from information theory (e.g., Cover & Thomas, 2006) is that the optimal way to do this is to assign a binary code of length log21/p to a string with probability p. 4 So, intuitively, a (probabilistic) grammar provides a short encoding of the linguistic input if it can itself be specified briefly; and if it makes the sentences that are actually observed as probable as possible. There is a tension between these objectives. An “over-precise” grammar, which encodes exactly those sentences that have been encountered and no others will make those particular sentences highly probably; but the code for such a grammar will be long (roughly, it will consist just of a list of the “allowed” sentences). Conversely, a very simple but Simplicity-based approach to language learning 7 overgeneral grammar (e.g., stating, roughly, that words can occur in any order with equal probability) will have a short code, but, because the space of possible allowed sentences is vast, the code for the specific sentences observed by the learner will be very long. The simplicity principle recommends finding an balance between these extremes: postulating restrictions in the grammar just when these “pay off” by sufficiently reducing the code length of the sentences, encoded by the grammar. As we have indicated, in general, the better the grammar captures the structure of the language, the shorter the encoded representation of the linguistic input will be. For a concrete example, let us first consider hypotheses (i.e. grammars) describing artificially simplistic language data. Suppose the observed language was the following repeating infinite string of sentences: Hi! Bye! Hi! Bye!... One hypothesis could be “The language is a sequence of ‘Hi!’ and ‘Bye!’ occurring independently, and each with .5 probability.” Under this hypothesized grammar, the encoded specification of the language input will be “0101...”, where 0 and 1 correspond to ‘Hi!’ and ‘Bye!’ respectively. Now if the hypothesis was a more powerfully descriptive grammar such as “The language contains a single sentence ‘Hi! Bye!’, then no further code at all is required to specify the linguistic input. Now, an infinite language input is fully specified in a simple finite description—and, more generally, the more precisely the grammar captures the structure of the linguistic input, the shorter the encoding of that linguistic input will be. Initially, the learner may not have sufficient data to favour the latter hypothesis; but eventually the latter “grammar” will provide the simpler encoding, because it correctly captures regularities in the input. Hence, as linguistic input accumulates, the grammar which provides the simplest encoding will be updated. An ideal simplicity learner (as in the mathematical Simplicity-based approach to language learning 8 results below) will have access to all (infinite) possibly hypothetical grammars that describe its current language data input, and choose the “simplest;’ any real, and hence computationally limited, learner can of course only approximate this calculation to some degree. Crucially, note that the simplicity-based learner has a mechanism for avoiding overgeneral grammars, when learning from positive evidence. Although our artificial data is compatible with a random sequence of ‘Hi!’ and ‘Bye!,’ the corresponding grammar is eliminated without the need for negative evidence, but because another grammar provides a shorter encoding of the input. 5 This point applies equally to learning natural languages. Consider the case of is contraction mentioned above. Consider two possible grammars, one that allows is contraction everywhere, and one that is more restricted (allowing He’s taller than she is but not *He is taller than she’s). The latter “grammar” will be more complex (because it involves specifying more precisely when contraction can occur); but it will encode the linguistic input more briefly, because it more accurately captures the structure of the language. Given sufficient linguistic input, the benefit of the more accurate encoding of the linguistic input will overwhelm any additional costs in encoding the grammatical rule, and the more precise rule will be favoured. Thus, it appears that an overgeneral grammar can be eliminated by applying the simplicity principle to positive data only. This intuition is encouraging but hardly definitive. Knowing that a learner can potentially eliminate a single over-general grammar does not, of course, indicate that it can successfully choose between an infinity of possible grammars, and home in on the “true” grammar, or some approximation to it. We shall see, however, that positive mathematical results along these lines are possible. Moreover, in Section 7, we shall apply the style of Simplicity-based approach to language learning 9 argument sketched above to the learnability of some specific, and much-discussed, linguistic regularities (see Hsu, Chater & Vitányi, 2011). 2 An“ideal” learner Below, we consider some formal theoretical results describing what an “ideal” learner can learn purely from exposure to an (indefinitely long) sequence of linguistic input (i.e., positive evidence) by using the simplicity principle. What is the structure of the linguistic material to be learned? Fortunately, it turns out that we need assume only that this input is generated probabilistically by some computable process. 6 This restriction is mild because cognitive science takes computability constraints on mental processes, including the generation of language, as founding assumptions (Pylyshyn, 1984) and, indeed, specific models of language structure and generation all adhere to this assumption. Finally, for mathematical convenience, and without loss of generality, we assume that the linguistic input is coded in binary form. Importantly, note that these assumptions allow that there can be any (computable) relationship between different parts of the input---we do not, for example, assume that sentences are independently sampled from a specific probability distribution. Our very mild assumption allows sentences to be highly interdependent (this is one generalization with respect to earlier results, e.g., Angluin, 1980; Jerome Feldman, 1972; Wharton, 1974), and includes the possibility that the language may be modified or switched during the input or indeed that sentences from many different languages might be interleaved. Specifically, suppose that the linguistic input, coded as a binary sequence, x, is generated by a computable probability distribution, C(x). 7 Intuitively, we can view this as Simplicity-based approach to language learning 10 meaning that there is a computer program, C, (which might, for example, encode a grammar, as above) which receives random input y, from a stream of coin flips. When fed to C, this random input generates x as output, i.e., C(y) = x. The probability of this y is 2 -l(y) (the probability of generating any specific binary sequence of length l(y) from unbiased coin flips). Many y may generate the same x, so the probability of an output with initial segment x, C(x), is the sum of the probabilities of such y:     ... ) ( : ) ( 2 ) ( x y C y y l C x  (1) The distribution C(x) is built on a simplicity principle: outputs which correspond to short programs for the computer program, C, are overwhelmingly more probable than outputs for which there are no short programs. The learner’s task, then, can be viewed as approximating C(x), given a sample x, generated from the computer program, C. So, for example, if C generated independent samples from a specific stochastic phrase structure grammar, then the learner’s aim is to find a probability distribution which matches the probabilities generated by that stochastic grammar as accurately as possible. To the extent that this is possible, we might conjecture that the learner should (i) be able to predict how the corpus will continue; (ii) decide which strings are allowed by C(x); and (iii) generate output similar to that generated by C(x). Framing these points in terms of language acquisition, this means that, by approximating C(x), the learner can, to some approximation, (i) predict what phoneme, word, or sentence will come next (insofar as this is predictable at all); (ii) learn to judge grammaticality; and (iii) learn to produce language, indistinguishable from that to which it has been exposed. We explore these issues in turn in Sections 3-5. Simplicity-based approach to language learning 11 How, then, can the learner approximate C(x), given that it has exposure to just one (admittedly arbitrarily long) corpus x, and no prior knowledge of the specific computational process, C, which has generated this corpus? It turns out that we can make progress by assuming only that the learner can, in principle, entertain all and only computable hypotheses—i.e., that the learner’s representational resources are universal: i.e., sufficient to encode any possible computation. Elementary results in computability theory (e.g., Odifreddi, 1988) have shown that this assumption of universality is surprisingly mild, and is satisfied by very simple abstract languages (such as the lambda calculus, Barendregt, 1984) and familiar practical languages from Fortran, to C++, to Java and Scheme. We assume, then, that the brain (and our ideal learner) has at least these representational resources. We have stated that a simplicity-based learner favor simple “explanations,” measured in terms of code length in some programming language. But surely the length of a program depends on the programming language used? What may be easy to write in Matlab may be difficult to write in Prolog; and vice versa. It turns out, though, that the choice of programming language affects program lengths only to a limited degree. An important result, known as the invariance theorem (Li & Vitányi, 1997), states that, for any two universal programming languages, the length of the shortest program for any computable object in each language is bounded by a fixed constant. A caveat is appropriate, however: “invariance” up to an additive constant is sufficient for establishing mathematical results, such as those below; but choice of representation language is crucial for making learning practically feasible, as we shall note in Section 7. 8 Nonetheless, so long as we assume that the learner’s coding language is universal, we can avoid having to provide a specific account of the program that the learner uses. 9 Simplicity-based approach to language learning 12 Now suppose the learner assumes only that the corpus, x, is generated by a computable process (and hence makes no assumptions that it is generated by a specific type of grammar, or indeed, any grammar at all; makes no assumption that “sentences” are sampled independently, etc.). Then the probability of each possible x is given by the probability that this sequence will be generating from the output of a random input, y, of length l(y) (as before, by random coin flips) fed to a universal computer, U. 10 Analogous to (1), we can define this “universal monotone distribution” (Solomonoff, 1978) (x):     x y U y y l x ...) ( : ) ( 2 ) (  (2) where U(y) are programs y written in the universal programming language. Thus, an ideal learner draws on its universal programming language and the simplicity principle to formulate (x). Remarkably, it turns out that (x) serves as a good enough approximation to C(x) to allow the ideal learner to predict future linguistic input; and we show below that this allows the ideal learner to make grammaticality judgments, produce grammatical utterances, and map sound to meaning. What is the mysterious (x) in more concrete terms? Roughly, it is what would result from randomly typing into a computer; feeding the resulting “programs” (most of which will, of course, not even be syntactically valid, or will loop indefinitely) to the interpreter for some universal programming language (say, C++); and considering the outputs of the (small number of) valid and terminating programs. Thus, we can alter the familiar image of monkeys randomly hitting the keys on a typewriter and, supposedly, eventually generating the works of Shakespeare, to the image of monkeys typing computer programs, and generating outputs x according to (x). The probability (x) will depend, of course, on the length of the shortest Simplicity-based approach to language learning 13 program generating x, as short programs are overwhelmingly more likely to be chanced upon by the monkey. We shall explore the remarkable properties of (x) shortly. But it is worth noting at the outset that (x) is known to be uncomputable (Li & Vitányi, 1997), and hence must be approximated. It remains an open question how closely (x) can be approximated and how this affects learnability results. Promisingly, computable approximations to the universal distribution can be developed into practical tools in statistics and machine learning (e.g., Rissanen, 1987; Wallace & Freeman, 1987). Related approximations will be considered briefly in Section 7 in relation to developing a methodology for assessing the learnability of specific linguistic patterns. 3. Prediction One indication of the degree to which a learner understands the patterns in the data in any domain, is its ability to predict. Thus, if the linguistic input is governed by grammatical or other principles of whatever complexity, any learner that can predict how the linguistic material will continue, arbitrarily well, must, in some sense, have learned such regularities. Prediction has been used as a measure of how far the structure of a language has been learned since Shannon (1951); and is widely used as a measure of learning in connectionist models of language processing (Christiansen & Chater, 1994, 1999; Elman, 1990) . And, as we have noted, this result for prediction will be a foundation for results concerning grammaticality judgments, language production, and form-meaning mapping, as we discuss in subsequent sections. Simplicity-based approach to language learning 14 We formulate the task of prediction as follows. At each point in a binary sequence x (encoding our linguistic input), generated by computer C, the probabilities that, given input x, that the next symbol is 0 or 1 can be written: ) ( ) 0 ( ) | 0 ( x x x C C C     ; ) ( ) 1 ( ) | 1 ( x x x C C C     (3) where ) | 0 ( x C  and ) | 1 ( x C  represent the probabilities that the subsequence x is followed by a 0 and 1 respectively; and ) 0 (x C  and ) 1 (x C  are the probabilities the specific sequence of x followed by 0 or 1, respectively. But the ideal learner does not have access to C(x), but instead uses (x) for prediction. Thus, the learner’s predictions for the next item of a binary sequence that has started with x is: ) ( ) 0 ( ) | 0 ( x x x     ; ) ( ) 1 ( ) | 1 ( x x x     (4) A key result by Solomonoff (1978), which we call the Prediction Theorem, shows that, in a specific rigorous sense, the universal monotone distribution , described above, is reliable for predicting any computable monotone distribution, , with very little expected error. More specifically, the difference in these predictions is measured by the square of difference in the probabilities that  and  assign to 0 being the next symbol:   ) | 0 ( ) | 0 ( ) ( Error x x x     (5) Simplicity-based approach to language learning 15 And the expected sum-squared error for the nth item in the sequence is: sn  (x)Error x:l (x )n 1  (x) (6) The better  predicts , the smaller sn will be. Given this, the overall expected predictive success of the method across the entire sequence is obtained by summing the sn across all n:",
    "creator" : "Microsoft® Word 2010"
  }
}