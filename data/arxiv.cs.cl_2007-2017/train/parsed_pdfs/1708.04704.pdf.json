{
  "name" : "1708.04704.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evaluating Word Embeddings for Sentence Boundary Detection in Speech Transcripts",
    "authors" : [ "Marcos V. Treviso", "Christopher D. Shulby", "Sandra M. Aluı́sio", "Paulo (USP" ],
    "emails" : [ "marcostreviso@usp.br", "cshulby@icmc.usp.br", "sandra@icmc.usp.br" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The concept of a sentence in written or spoken texts is important in several Natural Language Processing (NLP) tasks, such as morpho-syntactic analysis [Kepler and Finger 2010, Fonseca and Aluı́sio 2016], sentiment analysis [Anchiêta et al. 2015, Brum et al. 2016], summarization [Nóbrega and Pardo 2016], and speech processing [Mendonça et al. 2014], among others. However, punctuation marks that constitute a sentence boundary are ambiguous The Disambiguation of Punctuation Marks (DPM) task analyzes punctuation marks in texts and indicates whether they correspond to a sentence boundary. The purpose of the DPM task is to answer the question: Among the tokens of punctuation marks in a text, which of them correspond to sentence boundaries?\nThe Sentence Boundary Detection (SBD) task is very similar to DPM, both of which attempt to break a text into sequential units that correspond to sentences, where DPM is text-based and SBD can be applied to either written text or audio transcriptions and often for clauses, which do not necessarily end in final punctuation marks but are complete thoughts nonetheless. However, performing SBD in speech texts is more complicated due to the lack of information such as punctuation and capitalization; moreover text output is susceptible to recognition errors, in case of Automatic Speech Recognition (ASR) systems are used for automatic transcriptions [Gotoh and Renals 2000]. SBD from speech transcriptions is a task which has gained more attention in the last decades due to the increasing popularity of ASR software which automatically generate text from audio input. This task can also be applied to written texts, like online product reviews [Silla Jr and Kaestner 2004, Read et al. 2012, López and Pardo 2015], in order to better their intelligibility and facilitate the posterior use of NLP tools.\nIt is important to point out that the differences between spoken and written texts are notable, mainly when we take into consideration the size of the utterances and the\nar X\niv :1\n70 8.\n04 70\n4v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 7\nnumber of disfluencies provided in speech. Disfluencies include filled pauses, repetitions, modifications, repairs, partial utterances, nonword vocalizations and false starts. These phenomena are very common in spontaneous speech. [Liu 2004].\nFigure 1 shows the result of a transcript from a neuropsychological retelling task that does not include either capitalization or sentence segmentation, preventing the direct application of NLP methods that rely on these marks for their correct use, such as taggers and parsers. One can easily note that this type of text differs greatly in style and form from written/edited text (on which most NLP tools are trained), such as text found in novels or a newspaper.\nThese tests are applied by clinicians who tell a story to patients who are instructed to try and remember as many details as possible so that they may retell it. The evaluation of language in discourse production, mainly in narratives, is an attractive alternative because it allows the analysis of linguistic microstructures and phonetic-phonological, morpho-syntactic, semantic-lexical components, as well as semantic-pragmatic macrostructures. Neuropsychological tests are used in clinical settings for detection, progress monitoring and treatment observation in patients with dementias. In an ideal scenario we would like to automate the application of neuropsychological tests and the discourse analysis of the retellings.\nNLP applications generally receive text as input; therefore, words can be considered the basic processing unit. In this case, it is important that they are represented in a way which carries the load of all relevant information. In the current approach used here, words are induced representations in a dense vector space. These representations are known as word embeddings; able to capture semantic, syntactic and morphological information from large unannotated corpora [Mikolov et al. 2013, Ling et al. 2015, Lai et al. 2015]. Various studies show that textual information is important for SBD [Gotoh and Renals 2000,Batista et al. 2012,Che et al. 2016]. Even though textual information is a strong indicator for sentence delimitation, boundaries are often associated with prosodic information [Shriberg et al. 2000,Batista et al. 2012], like pause duration, change in pitch and change in energy. However, the extraction of this type of information requires the use of high quality resources, and consequently, few resources with prosodic information are available. On the other hand, textual information can easily be extracted in large scale from the web. Textual information can also be represented in various ways for SBD, for example, n-gram based techniques have presented good results for SBD [Gotoh and Renals 2000, Kim and Woodland 2003, Favre et al. 2008]; however, in contrast to word embeddings, they are induced representations in a sparse vector space.\nOur aim in this paper is to verify which embedding induction method works best\n1http://www.letras.ufrj.br/nurc-rj/\nfor the SBD task, specifically whether it be those which were proposed to capture semantic, syntactic or morphological similarities. For example, we imagine that methods that capture morphological similarities may benefit the SBD performance for impaired speech, since a large number of words produced in this type of speech are out-of-vocabulary words.The paper is organized as follows. Section 2 presents related work on SBD using word embeddings; Section 3 describes the word embedding models evaluated in this paper; Section 4 presents our experimental setup, describing the datasets, method, and preprocessing steps used; Section 5 presents our findings and discussions. Finally, Section 6 concludes the paper and outlines some future work."
    }, {
      "heading" : "2. Related Work",
      "text" : "The work of [Che et al. 2016] and [Tilk and Alumäe 2015] use word embeddings to detect boundaries in prepared speech sentences, more specifically in the corpus from 2012 TED talks2. [Che et al. 2016] propose a CNN (Convolution Neural Network)-based method with 50 dimensions using GloVe [Pennington et al. 2014]. In [Klejch et al. 2016, Klejch et al. 2017] the authors show that that textual information influences the retrieval of punctuation marks more than prosodic information, even without the use of word embeddings.\nThe work in [Tilk and Alumäe 2015] is expanded in [Tilk and Alumäe 2016], using bidirectional neural networks with attention mechanisms to evaluate a spontaneous telephone conversation corpus. The authors point out that the bidirectional vision of the RNN (Recurrent Neural Network) is a more impacting feature than the attention mechanism for SBD; with only the use of word embeddings, the achieved results yielded only 10% less than when prosodic information was used together. In [Hough and Schlangen 2017] a system that uses RNNs with word embeddings is proposed for the SBD task in conjunction disfluencies, where results are competitive with the state of the art are achieved on the Switchboard corpus [Godfrey et al. 1992], showing that the simultaneous execution of these tasks is superior to when done individually.\nRecently, the work of [Treviso et al. 2017] proposed an automatic SBD method for impaired speech in Brazilian Portuguese, to allow a neuropsychological evaluation based on discourse analysis. The method uses RCNNs (Recurrent Convolutional Neural Networks) which independently treat prosodic and textual information, reaching state-ofthe-art results for impaired speech. Also, this study showed that it is possible to achieve good results when comparing them with prepared speech, even when practically the same quantity of text is used. Another interesting evidence was that the use of word embeddings, without morpho-syntactic labels was able to present the same results as when they were used; this indicates that word embeddings contain sufficient morpho-syntactic information for SBD. It was also shown that the method gains the better results than the state-of-the-art method used by [Fraser et al. 2015] by a great margin for both impaired and prepared speech (an absolute difference of ∼0.20 and ∼0.30, respectively). Beyond these findings, the method showed that the performance remains the same when a different story is used.\n2https://www.ted.com/talks"
    }, {
      "heading" : "3. Word Embeddings Models",
      "text" : "The generation of vector representations of words (or word embeddings) is linked to the induction method utilized. The work of [Turian et al. 2010] divides these representations into three categories: cluster-based, distributional and distributed methods. In this paper, we focus only on distributed representations, because generally they are computationally faster to be induced. These representations are based on real vectors distributed in a multidimensional space induced by unsupervised learning. In the following paragraphs, we describe the three induction methods for word embeddings utilized in our evaluations.\nA well-used NLP technique, Word2vec [Mikolov et al. 2013] follows the same principle as the natural language model presented in [Collobert and Weston 2008], with the exception that it does not use a hidden layer, generating a computationally faster log-linear model. This technique is divided into two modeling types: (i) Continuous Bagof-Words (CBOW), which given a window of words as input, the network tries to predict the word in the middle as output and (ii) the Skip-gram model, which tries to predict the window given the center word as input.\nAs Word2vec does not consider the word order in the window, this make the process less syntactic in nature, since word order is an essential phenomenon for syntax. In order to deal with this, a modification of Word2vec was proposed which is able to deal with word order by concatenating inputs in the CBOW model (instead of using the sum) and incremental weighting for Skip-gram. This technique is known as Wang2vec [Ling et al. 2015].\nA recent induction technique called FastText [Bojanowski et al. 2016,Joulin et al. 2016] uses n-grams of characters of a given word in the hope of capturing morphological information. In order to do this, the Skip-gram Word2vec model was modified so that that the scoring function of the network’s output is calculated basing itself on the character n-gram vectors, which are summed with the context vectors in order to represent a word."
    }, {
      "heading" : "4. Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1. Corpora/Datasets",
      "text" : "The datasets were divided into two categories: impaired speech and prepared speech. Impaired speech is not only spontaneous, but also noisy. The noise is produced internally due to the impaired neuropsychological condition of the participants studied. When people participate in neuropsychological tests, they produce the following phenomena: syntactically malformed sentences; mispronounced words (modifying the original morphology); low quality prosody (due to the shallow voices of the participants and/or abnormal fluctuations in vocal quality); and in general a great quantity and variety of types of disfluencies.\nThe first dataset of discourse tests is a set of impaired speech narratives, based on a book of sequenced pictures from the well-known Cinderella story. This dataset consists of 60 narrative texts told by Brazilian Portuguese speakers; 20 healthy subjects, called controls (CTL), 20 patients with Alzheimer’s disease (AD), and 20 patients with Mild Cognitive Impairment (MCI), diagnosed at Medical School of the University of São Paulo (FMUSP) and also used in [Aluı́sio et al. 2016]. The second dataset was made available by the FalaBrasil project, and its contents are structured in the same way as the Brazilian\nConstitution from 1988 [Batista et al. 2013]. The speech in this corpus can be categorized as prepared and also as read speech. To use these files in our scenario a preprocessing step was necessary, which removed lexical tips which indicate the beginning of articles, sections and paragraphs. This removal was carried out on both the transcripts and the audio. In addition, we separated the new dataset organized by articles, yielding 357 texts in total. Both datasets’ properties are presented in Table 1.\nThe corpus used to induce the vectors is made up of text from Wikipedia in 2016, a news crawler which collected articles from the G1 portal3 and the PLN-BR [Bruckschen et al. 2008] corpus. We also executed some basic preprocessing steps on this corpus, being that we forced all of the text to lowercase forms and separated each token from punctuation marks and tokenized the text using whitespace. We do not remove stopwords. After these steps, the embedding induction on the corpus returned ∼356M tokens, of which ∼1.7M were distinct."
    }, {
      "heading" : "4.2. Method",
      "text" : "In order to automatically extract new features from the input and at the same time deal with the long dependency problems between words, we propose a method based on RCNNs which was inspired by the sentence segmentation work done by [Tilk and Alumäe 2015] and [Che et al. 2016], and also by the work on text classification utilizing RCNNs by [Lai et al. 2015], where we made some adaptations so that the basic unit of classification was a data sequence. The architecture of our RCNN is the same as the one used in [Treviso et al. 2017] and can be seen in Figure 2.\nThe final model in [Treviso et al. 2017] consists of a linear combination between a model which deals only with lexical information and another which treats only prosodic\n3http://g1.globo.com/\ninformation. In this paper, we ignore the prosodic model and focus only on the textual information provided by the word embeddings. The strategy to utilize only this information is based on the idea that one can train a text-based model with a large amount of data, since text is readily found on the web.\nThe model’s input is a tridimensional word embedding matrix E ∈ Rm×ϕ×d, where m is equal to the vocabulary size used for training the embeddings. Once we have an input matrix composed by word embeddings, the convolutional layer extracts nf new features from a sliding window with the size hc, which corresponds to the size of the filter applied to the concatenated vectors [e1, ..., ehc ] corresponding to a region of hc neighboring embeddings [Kim 2014].\nThe convolutional layer produces features for each t-th word as it applies the shared filter for a window of hc embeddings et−hc+1:t in a sentence with the size ϕ. Our convolutional layer moves in a single vertical dimension (CNN 1D), one step at a time, which results in a quantity of filters qf equal to ϕ − hc + 2 ∗ p + 1. And since we want to classify exactly ϕ elements, we added p = bhc/2c elements of padding to both sides of the sentence. In addition, we applied a max-pooling operation on the temporal axis focusing on a region of hm words, with the idea of feeding only the most important features to the next layer.\nThe features selected by the max-pooling layer are fed to a recurrent layer. The values of the hidden units are computed utilizing nr LSTM cells [Hochreiter and Schmidhuber 1997] defined as activation units. As in [Tilk and Alumäe 2016], our recurrent layer is based on anterior and posterior temporal states using the bidirectional recurrent mechanism (BRNN). With the use of a bidirectional layer which treats convolutionized features, the network is adept at exploring the principal that nearby words usually have a great influence, while considering that distant words, either to the left or right, can also have an impact on the classification. This frequently happens in the SBD task, for example, in the case of interrogatives, question words like “quem” (“who”), “qual” (“what”) and “quando” (“when”) can define a sentence.\nAfter the BRNN layer, we use dropout as a regularization strategy, which attempts to prevent co-adaptation between hidden units during forward and back-propagation, where some neurons are ignored with the purpose of reducing the chance of overfitting [Srivastava et al. 2014]. Finally, the last layer, receives the output of the BRNN for each instance t, and feeds each into a simple fully connected layer which produces predictions using the softmax activation function, which gives us the final probability that a word precedes a sentence boundary (B) or not (NB).\nThe word embeddings matrix E was adjusted during training. Our RCNN uses the same hyperparameters described in [Treviso et al. 2017] and the same training strategy, which consists of cost-function minimization utilizing the RMSProp procedure [Tieleman and Hinton 2012] with back-propagation, considering the unbalanced task of sentence segmentation by penalizing errors from the minority class harsher (B)."
    }, {
      "heading" : "5. Results and Discussion",
      "text" : "We ran a 5-fold cross-validation for each group analyzed (CLT, MCI or AD), which left about 10% of the data for testing, the rest for training.\nThe performance results of the RCNN in terms of F1 on each type of patient and on the Constitution dataset are shown in Figure 3, for which we vary the embedding methods and its training strategies along with the induced vector dimensions between the values of: d ∈ {50, 100, 300, 600}.\nIn most cases Word2vec achieved better performance than other methods. Specifically, for CTL with Skip-gram and FastText with CBOW, yielding an F1 of 0.76, On the other hand, we see that for MCI patients, Wang2vec with Skip-gram was the best technique, yielding an F1 of 0.74. For the AD subjects the best technique was Word2vec with CBOW, returning an F1 of 0.66. As expected, results for CTL were higher than for MCI and AD, since the CTL narratives contain less noise. For Constitution data our method performs better using Wang2vec with Skip-gram strategy: F1 of 0.62.\nIt is possible to see in Figure 3 that our method tends to better its performance with increasing dimension size. Furthermore, the Skip-gram strategy generally returned better results than CBOW for the FastText and Wang2vec methods, whereas for Word2Vec there were some variations when strategies were switched. Still, the Word2vec Skip-gram with 600 dimensions and CBOW with 300 dimensions were those which returned the best\nresults for spontaneous and/or impaired speech (CTL, MCI and AD). In the case of the Constitution dataset, which is characterized as prepared and read speech, the best results were achieved by Wang2vec Skip-gram with 300 dimensions.\nContrary to the results reported in [Treviso et al. 2017] using textual and prosodic information, our method obtained better performance for impaired speech transcriptions than for prepared speech. This is probably due to the fact that the Constitution includes more impacting prosodic clues, whereas for spontaneous/impaired speech, the lexical clues are of greater influence for classification. This difference between lexical and prosodic features for prepared and spontaneous speech is consistent with the finding reported in other studies [Kolár et al. 2009, Fraser et al. 2015, Treviso et al. 2017]."
    }, {
      "heading" : "6. Conclusion and Future Work",
      "text" : "Our objective in this work was to identify the embedding with the best performance for SBD, specifically whether it would be one which captures semantic information, like Word2vec; syntactic, like Wang2vec; or morphological, like FastText. Still, we were not able to discern which type was most influential in general, since the differences from one to another are very small. Also even when one technique was superior to another for a particular set, we still need to investigate whether this was actually the fault of the technique or due to secondary factors, like hyperparameters, random initialization, or even the conditions of the data used.\nIn general, our results show that using only embeddings the RCNN method achieved similar results (difference of 1%) to the state of the art in terms of F1, using the same method published in [Treviso et al. 2017] for both classes: CTL and MCI using embeddings with 600 dimensions and prosodic information4. However, the results for the Constitution dataset were considerably lower (a difference of 17%) than the results of the model which uses both lexical and prosodic information in conjunction, but the difference is less (4%) for the models which used only prosodic information. Summing up, this indicates that by using a good word embedding model to represent textual information it is possible to achieve similar results with the state-of-the-art for impaired speech.\nFuture work will include some investigation of the lexical and prosodic clues which impact the classification. Also, we would like to investigate whether disfluency detection in conjunction with SBD can yield better results. Since the method presented in this paper can easily be applied to any language, we plan to evaluate it using English language corpora in order to directly compare the results with the related work."
    } ],
    "references" : [ {
      "title" : "Evaluating progression of alzheimer’s disease by regression and classification methods in a narrative language test in portuguese",
      "author" : [ "Aluı́sio" ],
      "venue" : "In PROPOR,",
      "citeRegEx" : "Aluı́sio,? \\Q2016\\E",
      "shortCiteRegEx" : "Aluı́sio",
      "year" : 2016
    }, {
      "title" : "Using stylometric features for sentiment classification. In CICLing, pages 189–200. 4Results using embeddings with 600 dimensions were obtained from the authors of the original article",
      "author" : [ "Anchiêta" ],
      "venue" : null,
      "citeRegEx" : "Anchiêta,? \\Q2015\\E",
      "shortCiteRegEx" : "Anchiêta",
      "year" : 2015
    }, {
      "title" : "Bilingual experiments on automatic recovery of capitalization and punctuation of automatic speech transcripts",
      "author" : [ "Batista" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing,",
      "citeRegEx" : "Batista,? \\Q2012\\E",
      "shortCiteRegEx" : "Batista",
      "year" : 2012
    }, {
      "title" : "Avanços em reconhecimento de fala para português brasileiro e aplicações: ditado no libreoffice e unidade de resposta audı́vel com asterisk",
      "author" : [ "Batista" ],
      "venue" : null,
      "citeRegEx" : "Batista,? \\Q2013\\E",
      "shortCiteRegEx" : "Batista",
      "year" : 2013
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Bojanowski" ],
      "venue" : "arXiv preprint arXiv:1607.04606",
      "citeRegEx" : "Bojanowski,? \\Q2016\\E",
      "shortCiteRegEx" : "Bojanowski",
      "year" : 2016
    }, {
      "title" : "Anotaçao lingüıstica em xml do corpus pln-br. Série de relatórios do NILC, ICMC-USP",
      "author" : [ "Bruckschen" ],
      "venue" : null,
      "citeRegEx" : "Bruckschen,? \\Q2008\\E",
      "shortCiteRegEx" : "Bruckschen",
      "year" : 2008
    }, {
      "title" : "Sentiment analysis for brazilian portuguese over a skewed class corpora",
      "author" : [ "Brum" ],
      "venue" : "In PROPOR,",
      "citeRegEx" : "Brum,? \\Q2016\\E",
      "shortCiteRegEx" : "Brum",
      "year" : 2016
    }, {
      "title" : "Punctuation prediction for unsegmented transcript based on word vector",
      "author" : [ "Che" ],
      "venue" : null,
      "citeRegEx" : "Che,? \\Q2016\\E",
      "shortCiteRegEx" : "Che",
      "year" : 2016
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Collobert", "R. Weston 2008] Collobert", "J. Weston" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient sentence segmentation using syntactic features",
      "author" : [ "Favre" ],
      "venue" : "Spoken Language Technology Workshop",
      "citeRegEx" : "Favre,? \\Q2008\\E",
      "shortCiteRegEx" : "Favre",
      "year" : 2008
    }, {
      "title" : "Improving pos tagging across portuguese variants with word embeddings",
      "author" : [ "Fonseca", "E.R. Aluı́sio 2016] Fonseca", "S.M. Aluı́sio" ],
      "venue" : "In PROPOR,",
      "citeRegEx" : "Fonseca et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fonseca et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentence segmentation of aphasic speech",
      "author" : [ "Fraser" ],
      "venue" : null,
      "citeRegEx" : "Fraser,? \\Q2015\\E",
      "shortCiteRegEx" : "Fraser",
      "year" : 2015
    }, {
      "title" : "Switchboard: Telephone speech corpus for research and development",
      "author" : [ "Godfrey" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Godfrey,? \\Q1992\\E",
      "shortCiteRegEx" : "Godfrey",
      "year" : 1992
    }, {
      "title" : "Sentence boundary detection in broadcast speech transcripts",
      "author" : [ "Gotoh", "Y. Renals 2000] Gotoh", "S. Renals" ],
      "venue" : "ISCA Workshop,",
      "citeRegEx" : "Gotoh et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Gotoh et al\\.",
      "year" : 2000
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "Hochreiter", "S. Schmidhuber 1997] Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Joint, incremental disfluency detection and utterance segmentation from speech",
      "author" : [ "Hough", "J. Schlangen 2017] Hough", "D. Schlangen" ],
      "venue" : "In EACL,",
      "citeRegEx" : "Hough et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hough et al\\.",
      "year" : 2017
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Joulin" ],
      "venue" : "arXiv preprint arXiv:1607.01759",
      "citeRegEx" : "Joulin,? \\Q2016\\E",
      "shortCiteRegEx" : "Joulin",
      "year" : 2016
    }, {
      "title" : "Variable-length markov models and ambiguous words in portuguese",
      "author" : [ "Kepler", "F.N. Finger 2010] Kepler", "M. Finger" ],
      "venue" : "In NAACL,",
      "citeRegEx" : "Kepler et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kepler et al\\.",
      "year" : 2010
    }, {
      "title" : "A combined punctuation generation and speech recognition system and its performance enhancement using prosody",
      "author" : [ "Kim", "Woodland 2003] Kim", "J.-H", "P.C. Woodland" ],
      "venue" : "Speech Communication,",
      "citeRegEx" : "Kim et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2003
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Y. Kim" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Kim,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim",
      "year" : 2014
    }, {
      "title" : "Punctuated Transcription of Multi-genre Broadcasts Using Acoustic and Lexical Approaches",
      "author" : [ "Klejch" ],
      "venue" : null,
      "citeRegEx" : "Klejch,? \\Q2016\\E",
      "shortCiteRegEx" : "Klejch",
      "year" : 2016
    }, {
      "title" : "Sequence-to-Sequence Models for Punctuated Transcription Combing Lexical and Acoustic Features",
      "author" : [ "Klejch" ],
      "venue" : null,
      "citeRegEx" : "Klejch,? \\Q2017\\E",
      "shortCiteRegEx" : "Klejch",
      "year" : 2017
    }, {
      "title" : "Genre effects on automatic sentence segmentation of speech: A comparison of broadcast news and broadcast conversations",
      "author" : [ "Kolár" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Kolár,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolár",
      "year" : 2009
    }, {
      "title" : "Recurrent convolutional neural networks for text classification",
      "author" : [ "Lai" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Lai,? \\Q2015\\E",
      "shortCiteRegEx" : "Lai",
      "year" : 2015
    }, {
      "title" : "Two/too simple adaptations of word2vec for syntax problems",
      "author" : [ "Ling" ],
      "venue" : null,
      "citeRegEx" : "Ling,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling",
      "year" : 2015
    }, {
      "title" : "STRUCTURAL EVENT DETECTION FOR RICH TRANSCRIPTION OF SPEECH",
      "author" : [ "Y. Liu" ],
      "venue" : null,
      "citeRegEx" : "Liu,? \\Q2004\\E",
      "shortCiteRegEx" : "Liu",
      "year" : 2004
    }, {
      "title" : "Experiments on sentence boundary detection in user-generated web content",
      "author" : [ "López", "R. Pardo 2015] López", "T.A.S. Pardo" ],
      "venue" : "In CICLing,",
      "citeRegEx" : "López et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "López et al\\.",
      "year" : 2015
    }, {
      "title" : "A method for the extraction of phoneticallyrich triphone sentences",
      "author" : [ "Mendonça" ],
      "venue" : "In Telecommunications Symposium (ITS),",
      "citeRegEx" : "Mendonça,? \\Q2014\\E",
      "shortCiteRegEx" : "Mendonça",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Mikolov,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov",
      "year" : 2013
    }, {
      "title" : "Investigating machine learning approaches for sentence compression in different application contexts for portuguese",
      "author" : [ "Nóbrega", "F.A.A. Pardo 2016] Nóbrega", "T.A.S. Pardo" ],
      "venue" : "In PROPOR,",
      "citeRegEx" : "Nóbrega et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nóbrega et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Pennington" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Pennington,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington",
      "year" : 2014
    }, {
      "title" : "Sentence boundary detection: A long solved problem",
      "author" : [ "Read" ],
      "venue" : null,
      "citeRegEx" : "Read,? \\Q2012\\E",
      "shortCiteRegEx" : "Read",
      "year" : 2012
    }, {
      "title" : "Prosody-based automatic segmentation of speech into sentences and topics",
      "author" : [ "Shriberg" ],
      "venue" : "Speech Communication,",
      "citeRegEx" : "Shriberg,? \\Q2000\\E",
      "shortCiteRegEx" : "Shriberg",
      "year" : 2000
    }, {
      "title" : "An analysis of sentence boundary detection systems for english and portuguese",
      "author" : [ "Silla Jr.", "C.N. Kaestner 2004] Silla Jr.", "C.A. Kaestner" ],
      "venue" : null,
      "citeRegEx" : "Jr et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Jr et al\\.",
      "year" : 2004
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava",
      "year" : 2014
    }, {
      "title" : "Rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning",
      "author" : [ "Tieleman", "T. Hinton 2012] Tieleman", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Tieleman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman et al\\.",
      "year" : 2012
    }, {
      "title" : "LSTM for punctuation restoration in speech transcripts",
      "author" : [ "Tilk", "O. Alumäe 2015] Tilk", "T. Alumäe" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Tilk et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tilk et al\\.",
      "year" : 2015
    }, {
      "title" : "Bidirectional recurrent neural network with attention mechanism for punctuation restoration",
      "author" : [ "Tilk", "O. Alumäe 2016] Tilk", "T. Alumäe" ],
      "venue" : "In INTERSPEECH",
      "citeRegEx" : "Tilk et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tilk et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentence segmentation in narrative transcripts from neuropsychological tests using recurrent convolutional neural networks",
      "author" : [ "Treviso" ],
      "venue" : "In EACL,",
      "citeRegEx" : "Treviso,? \\Q2017\\E",
      "shortCiteRegEx" : "Treviso",
      "year" : 2017
    }, {
      "title" : "Word representations: A simple and general method for semi-supervised learning",
      "author" : [ "Turian" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Turian,? \\Q2010\\E",
      "shortCiteRegEx" : "Turian",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "[Liu 2004].",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 19,
      "context" : ", ehc ] corresponding to a region of hc neighboring embeddings [Kim 2014].",
      "startOffset" : 63,
      "endOffset" : 73
    } ],
    "year" : 2017,
    "abstractText" : "This paper is motivated by the automation of neuropsychological tests involving discourse analysis in the retellings of narratives by patients with potential cognitive impairment. In this scenario the task of sentence boundary detection in speech transcripts is important as discourse analysis involves the application of Natural Language Processing tools, such as taggers and parsers, which depend on the sentence as a processing unit. Our aim in this paper is to verify which embedding induction method works best for the sentence boundary detection task, specifically whether it be those which were proposed to capture semantic, syntactic or morphological similarities.",
    "creator" : "LaTeX with hyperref package"
  }
}