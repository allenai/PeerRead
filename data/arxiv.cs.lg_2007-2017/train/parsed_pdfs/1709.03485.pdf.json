{
  "name" : "1709.03485.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "NiftyNet: a deep-learning platform for medical imaging",
    "authors" : [ "Eli Gibson", "Wenqi Lia", "Carole Sudre", "Lucas Fidon", "Dzoshkun Shakir", "Guotai Wang", "Zach Eaton-Rosen", "Robert Gray", "Tom Doel", "Yipeng Hu", "Tom Whyntie", "Parashkev Nachev", "Dean C. Barratt", "Sébastien Ourselin", "M. Jorge Cardoso", "Tom Vercauteren" ],
    "emails" : [ "wenqi.li@ucl.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. This has resulted is substantial duplication of effort and incompatible infrastructure across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. This TensorFlow-based infrastructure provides a complete modular deep learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications with data loading, data augmentation, network architectures, loss functions and evaluation metrics that are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted interventions.\nKeywords: medical image analysis, deep learning, convolutional neural network, framework, segmentation, variational auto encoder, generative adversarial network\n∗Corresponding author Email: wenqi.li@ucl.ac.uk Mailing Address: Wellcome / EPSRC Centre for Interventional and Surgical Sciences University College London Gower Street London, WC1E 6BT\n1Wenqi Li and Eli Gibson contributed equally to this work.\nPreprint submitted to Elsevier September 12, 2017\nar X\niv :1\n70 9.\n03 48\n5v 1\n[ cs\n.C V\n] 1"
    }, {
      "heading" : "1. Introduction",
      "text" : "Computer-aided analysis of medical images plays a critical role at many stages of the clinical workflow from population screening and diagnosis to treatment delivery and monitoring. This role is poised to grow as analysis methods become more accurate and cost effective. In recent years, a key driver of such improvements has been the adoption deep learning and convolutional neural networks in many medical image analysis and computer-assisted intervention tasks.\nDeep learning refers to a deeply nested composition of many simple functions (principally linear combinations such as convolutions, scalar non-linearities and moment normalizations) parameterized by variables. The particular composition of functions, called the architecture, defines a parametric function (typically with millions of parameters) that can be optimized to minimize an objective, or ’loss’, function, usually using some form of gradient descent.\nAlthough the first use of neural networks for medical image analysis dates back more than twenty years (Lo et al., 1995), their usage has increased by orders of magnitude in the last five years. Recent reviews (Shen et al., 2017; Litjens et al., 2017) have highlighted that deep learning has been applied to a wide range of medical image analysis tasks (segmentation, classification, detection, registration, image reconstruction, enhancement, etc.) across a wide range of anatomical sites (brain, heart, lung, abdomen, breast, prostate, musculature, etc.). Although each of these applications have their own specificities, there is substantial overlap in software pipelines implemented by many research groups.\nDeep-learning pipelines for medical image analysis comprise many interconnected components. Many of these are common to all deep-learning pipelines:\n• separation of data into training, testing and validation sets; • randomized sampling during training; • image data loading and sampling; • data augmentation; • a network architecture defined as the composition of many simple function; • a fast computational framework for optimization and inference; • metrics for evaluating performance during training and inference. In medical image analysis, many of these components have domain specific idiosyncrasies, detailed in Section 4. For example, medical images are typically stored in specialized formats that handle large 3D images with anisotropic voxels and encode additional spatial information and/or patient information, requiring different data loading pipelines. Processing large volumetric images has high memory requirements and motivates domain-specific memory-efficient networks or custom data sampling strategies. Images are often acquired in standard anatomical views and can represent physical properties quantitatively, motivating domain-specific data augmentation and model priors. Additionally, the clinical implications of certain errors may warrant custom evaluation metrics. Independent reimplementation of all of this custom infrastructure results in substantial duplication of effort, poses a barrier to dissemination of research tools and inhibits fair comparisons between competing methods.\nThis work presents the open-source NiftyNet2 platform to 1) facilitate efficient deep learning research in medical image analysis and computer-assisted intervention; and 2) reduce duplication of effort. The NiftyNet platform comprises an implementation of the common infrastructure and common networks used in medical imaging, a database of pre-trained networks for specific applications and tools to facilitate the adaptation of deep learning research to new clinical applications with a shallow learning curve."
    }, {
      "heading" : "2. Background",
      "text" : "The development of common software infrastructure for medical image analysis and computer-assisted interventions has a long history. Early efforts included the development of medical imaging file formats (e.g. ACR-NEMA (1985), Analyze 7.5 (1986), DICOM (1992) MINC (1992), and NIfTI (2001)). Toolsets to solve common challenges such as registration (e.g. NiftyReg (Modat et al., 2010), ANTs (Avants et al., 2011) and elastix (Klein et al., 2010)), segmentation (e.g. NiftySeg (Cardoso et al., 2012)), and biomechanical modeling (e.g. (Johnsen et al., 2015)) are available for use as part of image analysis pipelines. Pipelines for specific research applications such as FSL (Smith et al., 2004) for functional MRI analysis and Freesurfer (Fischl et al., 1999; Dale et al., 1999) for structural neuroimaging have reached widespread use. More general toolkits offering standardized implementations of algorithms (VTK and ITK (Pieper et al., 2006)) and application frameworks (NifTK (Clarkson et al., 2015), MITK (Nolden et al., 2013) and 3D Slicer (Pieper et al., 2006)) enable others to build their own pipelines. Common software infrastructure has supported and accelerated medical image analysis and computer-assisted intervention research across hundreds of research groups. However, despite the wide availability of general purpose deep learning software tools, deep learning technology has limited support in current software infrastructure for medical image analysis and computer-assisted interventions.\nSoftware infrastructure for general purpose deep learning is a recent development. Due to the high computational demands of training deep learning models and the complexity of efficiently using modern hardware resources (general purpose graphics processing units and distributed computing, in particular), numerous deep learning libraries and platforms have been developed and widely adopted, including cuDNN (Chetlur et al.), TensorFlow (Abadi et al., 2016), Theano (Bastien et al., 2012), Caffe (Jia et al., 2014), Torch (Collobert et al., 2011), CNTK (Seide and Agarwal, 2016), and MatConvNet (Vedaldi and Lenc, 2015).\nThese platforms facilitate the definition of complex deep learning networks as compositions of simple functions, hide the complexities of differentiating the objective function with respect to trainable parameters during training, and execute efficient implementations of performance-critical functions during training\n2Available at http://niftynet.io\nand inference. These frameworks have been optimized for performance and flexibility, and using them directly can be challenging, inspiring the development of platforms that simplify the development process for common usage scenarios, such as Keras (Chollet et al., 2015), and TensorLayer (Dong et al., 2017) for TensorFlow and Lasagne (Dieleman et al., 2015) for Theano. However, by avoiding assumptions about the application to remain general, the platforms are unable to provide specific functionality for medical image analysis and adapting them for this domain of application requires substantial implementation effort.\nDeveloped concurrently with the NiftyNet platform, the Deep Learning Toolkit (https://dltk.github.io) aims to support fast prototyping and reproducibility by implementing deep learning methods and modules for medical image analysis. While still in preliminary development, it appears to focus on deep learning building blocks rather than analysis pipelines. NifTK (Clarkson et al., 2015; Gibson et al., 2017c) and Slicer3D (via the DeepInfer (Mehrtash et al., 2017) plugin) provide infrastructure for distribution of trained deep learning pipelines. Although this does not address the substantial infrastructure needed for training deep learning pipelines, integration with existing medical image analysis infrastructure and modular design makes these platforms promising routes for distributing deep-learning pipelines."
    }, {
      "heading" : "3. Typical deep learning pipeline",
      "text" : "Deep learning adopts the typical machine learning pipeline consisting of three phases: model selection (picking and fitting a model on training data), model evaluation (measuring the model performance on testing data), and model distribution (sharing the model for use on a wider population). Within these simple phases lies substantial complexity, illustrated in Figure 1. The most obvious complexity is in implementing the network being studied. Deep neural networks generally use simple functions, but compose them in complex hierarchies; researchers must implement the network being tested, as well as previous networks (often incompletely specified) for comparison. To train, evaluate and distribute these networks, however, requires further infrastructure. Data sets must be correctly isolated to avoid biassed evaluations, considering sometimes complex data correlations. The data must be sampled, loaded and passed to the network, in different ways depending on the phase of the pipeline. Algorithms for tuning hyper-parameters within a family of models and optimizing model parameters on the training data are needed. Logging and visualization is needed to debug and dissect models during and after training. In applications with limited data, data sets must be augmented by perturbing the training data in realistic ways to prevent over-fitting. In deep learning, it is common practice to adapt previous network architectures, trained or untrained, in part or in full for similar or different tasks; this requires a community repository or ’model zoo’ storing models and parameters in an adaptable format. Much of this infrastructure is recreated by each researcher or research group undertaking a deep learning project, and much of it depends on the application domain being addressed."
    }, {
      "heading" : "4. Design considerations for deep learning in medical imaging",
      "text" : "Medical image analysis differs from other domains where deep learning is applied due to characteristics of the data itself, and the applications in which they are used. In this section, we present the domain-specific requirements driving the design of NiftyNet."
    }, {
      "heading" : "4.1. Data availability",
      "text" : "Acquiring, annotating and distributing medical image data sets have higher costs than in many computer vision tasks. For many medical imaging modalities, generating an image is costly. Annotating images for many applications requires high levels of expertise from clinicians with limited time. Additionally, due to privacy concerns, sharing data sets between institutions, let alone internationally, is logistically and legally challenging. Although recent tools such as DeepIGeoS (Wang et al., 2017b) for semi-automated annotation and GIFT-Cloud (Doel et al., 2017) for data sharing are beginning to reduce these barriers, typical data sets remain small. Using smaller data sets increases the importance of data augmentation, regularization, and cross-validation to prevent over-fitting. The additional cost of data set annotation also places a greater emphasis on semi- and unsupervised learning."
    }, {
      "heading" : "4.2. Data dimensionality and size",
      "text" : "Data dimensionality encountered in medical image analysis and computerassisted intervention typically ranges from 2D to 5D. Many medical images, including MRI, CT, PET and SPECT, capture volumetric images. Longitudinal imaging (multiple images taken over time) is typical in interventional settings as well as clinically useful for measuring organ function (e.g. blood ejection fraction in cardiac imaging) and disease progression (e.g. cortical thinning in neurodegenerative diseases).\nAt the same time, capturing high-resolution data in multiple dimensions is often necessary to detect small but clinically important anatomy and pathology. The combination of these factors results in large data sizes for each sample, which impact computational and memory costs. Deep learning in medical imaging uses various strategies to account for this challenge. Many networks are designed to use partial images: 2D slices sampled along one axis from 3D images (Zhou et al., 2016), 3D subvolumes (Li et al., 2017), anisotropic convolution Wang et al. (2017a), or combinations of subvolumes along multiple axes (Roth et al., 2014). Other networks use multi-scale representations allowing deeper and wider networks on lower resolution representations (Milletari et al., 2016; Kamnitsas et al., 2017). A third approach uses dense networks to reuse feature representations multiple times in the network (Gibson et al., 2017b). Smaller batch sizes can reduce the memory cost, but rely on different weight normalization functions such as batch renormalization (Ioffe, 2017), weight normalization (Salimans and Kingma, 2016) or layer normalization (Ba et al., 2016)."
    }, {
      "heading" : "4.3. Data formatting",
      "text" : "Data sets in medical imaging are typically stored in different formats than in many computer vision tasks. To support the higher dimension medical image data, specialized formats have been adopted (e.g., DICOM, NIfTI, Analyze). These formats frequently also store metadata that is critical to image interpretation, including spatial information (anatomical orientation and voxel anisotropy), patient information (demographics and identifiers), and acquisition information (modality types and scanner parameters). These medical imaging specific data formats are typically not supported by existing deep learning frameworks, requiring custom infrastructure for loading images."
    }, {
      "heading" : "4.4. Data properties",
      "text" : "The characteristic properties of medical image content poses opportunities and challenges. Medical images are obtained under controlled conditions, allowing more predictable data distributions. In many modalities, images are calibrated such that spatial relationships and image intensities map directly to physical quantities and inherently normalized across subjects. For a given clinical workflow, image content is typically consistent, potentially enabling the characterization of plausible intensity and spatial variation for data augmentation. However, some clinical applications introduce additional challenges. Because small image features can have large clinical importance, and because some pathology is very rare but life-threatening, medical image analysis must deal with large class imbalances, motivating special loss functions (Milletari et al., 2016; Fidon et al., 2017; Sudre et al., 2017). Furthermore, different types of error may have very different clinical impacts, motivating specialized loss functions and evaluation metrics (e.g. spatially weighted segmentation metrics). Applications in computer assisted interventions (e.g., (Gibson et al., 2017c; Garcia-Peraza-Herrera et al., 2017)) where analysis results are used in real time have additional constraints on analysis latency."
    }, {
      "heading" : "5. NiftyNet: a platform for deep learning in medical imaging",
      "text" : "The NiftyNet platform aims to augment current deep learning infrastructure to address the ideosyncracies of medical imaging described in Section 4, and lower the barrier to adopting this technology in medical imaging applications. NiftyNet is built using the TensorFlow library, which provides the tools for defining computational pipelines and executing them efficiently on hardware resources, but does not provide any specific functionality for processing medical images, or high level interfaces for common medical image analysis tasks. NiftyNet provides a high level deep learning pipeline with components optimized for medical imaging applications (data loading, sampling and augmentation, networks, loss functions, evaluations, and a model zoo) and specific interfaces for medical image segmentation, classification, regression, image generation and representation learning applications."
    }, {
      "heading" : "5.1. Design goals",
      "text" : "The design of NiftyNet follows several core principles which support a set of key requirements:\n• enable research in one aspect of the deep learning pipeline without the need for recreating the other parts; • be simple to use for common use cases, but flexible enough for complex use cases; • support built-in TensorFlow features (parallel processing, visualization) by default; • support best practices (data augmentation, data set separation) by default; • support model distribution and adaptation."
    }, {
      "heading" : "5.2. System overview",
      "text" : "The NiftyNet platform comprises several modular components. The NiftyNet ApplicationDriver defines the common structure across all applications, and is responsible for instantiating the data analysis pipeline and distributing computation across the available computational resources. The NiftyNet Application classes encapsulate standard analysis pipelines for different medical image analysis applications, by connecting four components: a Reader to load data from files, a Sampler to generate appropriate samples for processing, a Network to process the inputs, and an output handler (comprising the Loss and Optimizer during training and an Aggregator during inference and evaluation). The Sampler includes sub-components for data augmentation. The Network includes sub-components representing individual network blocks or larger conceptual units. These components are detailed in the following sections.\nAs a concrete illustration, one instantiation of the SegmentationApplication could use the following modules. During training, it could use a UniformSampler to generate small image patches and corresponding labels; a vnet Network would process batches of images to generate segmentations; a Dice LossFunction would compute the loss used for backpropagation using the Adam Optimizer. During inference, it could use a GridSampler to generate a set of non-overlapping patches to cover the image to segment, the same network to generate corresponding segmentations, and a GridSamplesAggregator to aggregate the patches into a final segmentation."
    }, {
      "heading" : "5.2.1. Component details: ApplicationDriver class",
      "text" : "The NiftyNet ApplicationDriver defines the common structure for all NiftyNet pipelines. It is responsible for instantiating the data and Application objects and distributing the workload across and recombining results from the computational resources (potentially including multiple CPUs and GPUs). It is also responsible for handling variable initialization, variable saving and restoring and logging. Implemented as a template design pattern (Gamma et al., 1994), the ApplicationDriver delegates application-specific functionality to separate Application classes.\nThe ApplicationDriver can be configured from the command line or programmatically using a human-readable configuration file. This file contains the data set definitions and all the settings that deviate from the defaults. When the ApplicationDriver saves its progress, the full configuration (including default parameters) is also saved so that the analysis pipeline can be recreated to continue training or carry out inference internally or with a distributed model."
    }, {
      "heading" : "5.2.2. Component details: Application class",
      "text" : "Medical image analysis encompasses a wide range of tasks for different parts of the pre-clinical and clinical workflow: segmentation, classification, detection, registration, reconstruction, enhancement, model representation and generation. Different applications use different types of inputs and outputs, different networks, and different evaluation metrics; however, there is common structure and functionality among these applications supported by NiftyNet. NiftyNet currently supports\n• image segmentation, • image regression, • image model representation (via auto-encoder applications), and • image generation (via auto-encoder and generative adversarial networks), and it is designed in a modular way to support the addition of new application types, by encapsulating typical application workflows in Application classes.\nThe Application class defines the required data interface for the Network and Loss, facilitates the instantiation of appropriate Sampler and output handler objects, connects them as needed for the application, and specifies the training regimen. For example, the SegmentationApplication specifies that networks accept images (or patches thereof) and generate corresponding labels, that the losses accepts generated and reference segmentations and an optional weight map, and that the optimizer trains all trainable variables in each iteration. In contrast, the GANApplication specifies that networks accept a noise source, samples of real data and an optional conditioning image, losses accept logits denoting if a sample is real or generated, and the optimizer alternates between training the discriminator sub-network and the generator sub-network."
    }, {
      "heading" : "5.2.3. Component details: Networks and Layers",
      "text" : "The complex composition of simple functions that comprise a deep learning architecture is simplified in typical networks by the repeated reuse of conceptual blocks. In NiftyNet, these conceptual blocks are represented by encapsulated Layer classes, or inline using TensorFlow’s scoping system. Composite layers, and even entire networks, can be constructed as simple compositions of NiftyNet layers and TensorFlow operations. This supports the re-use of existing networks by clearly demarcating conceptual blocks of code that can be re-used and assigning names to corresponding sets of variables that can be re-used in other networks (detailed in Section 5.2.8). This also enables automatic support for visualisation of the network graph as a hierarchy at different levels of detail using the TensorBoard visualiser (Mané et al., 2015) as shown in Figure 2. Following the model used in Sonnet (Reynolds et al., 2017), Layer objects define\na scope upon instantiation, which can be reused repeatedly to allow complex weight-sharing without breaking encapsulation."
    }, {
      "heading" : "5.2.4. Component details: data loading",
      "text" : "The Reader class is responsible for loading corresponding image files from medical file formats for a specified data set, and applying image-wide preprocessing. For simple use cases, NiftyNet can automatically identify corresponding images in a data set by searching a specified file path and matching user-specified patterns in file names, but it also allows explicitly tabulated comma-separated value files for more complex data set structures (e.g. cross-validation studies). Input and output of medical file formats are already supported in multiple existing python libraries, although each libraries support different sets of formats. To facilitate a wide range of formats, NiftyNet uses nibabel (Brett et al., 2016) as a core dependency but can fall back on other libraries (e.g. SimpleITK (Lowekamp et al., 2013) if they are installed and a file format is not supported by nibabel. A pipeline of image-wide pre-processing functions, described in Section 5.2.6, is applied to each image before samples are taken."
    }, {
      "heading" : "5.2.5. Component details: Samplers and output handlers",
      "text" : "To handle the breadth of applications in medical image analysis and computerassisted interventions, NiftyNet provides flexibility in mapping from an input dataset into packets of data to be processed and from the processed data into useful outputs. The former is encapsulated in Sampler classes, and the latter is encapsulated in output handlers. Because the sampling and output handling are tightly coupled and depend on the action being performed (i.e. training, inference or evaluation), the instantiation of matching Samplers and output handlers is delegated to the Application class.\nSamplers generate a sequence of packets of corresponding data for processing. Each packet contains all the data for one independent computation (e.g. one step of gradient descent during training), including images, labels, classifications, noise samples or other data needed for processing. During training, samples are taken randomly from the training data, while during inference and evaluation the samples are taken systematically to process the whole data set. To feed these samples to TensorFlow, NiftyNet automatically takes advantage of TensorFlow’s data queue support: data can be loaded and sampled in multiple CPU threads, combined into mini-batches and consumed by one or more GPUs. NiftyNet includes Sampler classes for sampling image patches (uniformly or based on specified criteria), sampling whole images rescaled to a fixed size and sampling noise; and it supports composing multiple samplers for more complex inputs.\nOutput handlers take different forms during training and inference. During training, the output handlers takes the network output, computes a loss and the gradient of the loss with respect to the trainable variables, and uses an Optimizer to iteratively train the model. During inference, the output handler generates useful outputs by aggregating one or more network outputs and performing any necessary post-processing (e.g. resizing the outputs to the original image size).\nNiftyNet currently supports Aggregators for combining image patches, resizing images, and computing evaluation metrics."
    }, {
      "heading" : "5.2.6. Component details: data normalization and augmentation",
      "text" : "Data normalization and augmentation are two approaches to compensating for small training data sets in medical image analysis, wherein the training data set is too sparse to represent the variability in the distribution of images. Data normalization reduces the variability in the data set by transforming inputs to have specified invariant properties, such as fixed intensity histograms or moments (mean and variance). Data augmentation artificially increases the variability of the training data set by introducing random perturbations during training, for example applying random spatial transformations or adding random image noise. In NiftyNet, data augmentation and normalization are implemented as Layer classes applied in the Sampler, as plausible data transformations will vary between applications. Some of these layers, such as histogram normalization, are data dependent; these layers compute parameters over the data set before training begins. NiftyNet currently supports mean, variance and histogram intensity data normalization, and flip, rotation and scaling spatial data augmentation."
    }, {
      "heading" : "5.2.7. Component details: data evaluation",
      "text" : "Summarizing and comparing the performance of image analysis pipelines typically rely on standardized descriptive metrics and error metrics as surrogates for performance. Because individual metrics are sensitive to different aspects of performance, multiple metrics are reported together. Reference implementations of these metrics reduce the burden of implementation and avoid implementation inconsistencies. NiftyNet currently supports the calculation of descriptive and error metrics for segmentation. Descriptive statistics include spatial metrics (e.g. volume, surface/volume ratio, compactness) and intensity metrics (e.g. mean, quartiles, skewness of intensity). Error metrics, computed with respect to a reference segmentation include overlap metrics (Dice and Jaccard scores; voxel-wise sensitivity, specificity and accuracy; etc.), boundary distances (mean absolute distance and Hausdorff distances) and region-wise errors (e.g. detection rate; region-wise sensitivity, specificity and accuracy; etc.)."
    }, {
      "heading" : "5.2.8. Component details: model zoo for network reusability",
      "text" : "To support the reuse of network architectures and trained models, many deep learning platforms host a database of existing trained and untrained networks in a standardized format, called a model zoo. Trained networks can be used directly (as part of a workflow or for performance comparisons), fine-tuned for different data distributions (e.g. a different hospital’s images), or used to initialize networks for other applications (i.e. transfer learning). Untrained networks or conceptual blocks can be used within new networks. NiftyNet provides several mechanisms to support distribution and reuse of networks and conceptual blocks.\nTrained NiftyNet networks can be restored directly using configurations options. Trained networks developed outside of NiftyNet can be adapted to NiftyNet by encapsulating the network within a Network class derived from TrainableLayer. Externally trained weights can be loaded within NiftyNet using a restore initializer, adapted from Sonnet (Reynolds et al., 2017), for the complete network or individual conceptual blocks. restore initializer initializes the network weights with those stored in a specified checkpoint, and supports variable scope renaming for checkpoints with incompatible scope names. Smaller conceptual blocks, encapsulated in Layer classes, can be reused in the same way. Trained networks incorporating previous networks are saved in a self-contained form to minimize dependencies.\nThe NiftyNet model zoo contains both untrained networks (e.g., unet (Çiçek et al., 2016) and vnet (Milletari et al., 2016) for segmentation), as well as trained networks for some tasks (e.g. dense vnet (Gibson et al., 2017a) for multiorgan abdominal CT segmentation, wnet (Wang et al., 2017a) for brain tumor segmentation and simulator gan (Hu et al., 2017) for generating ultrasound images). Model zoo entries should follow a standard format comprising\n• python code defining any components not included in NiftyNet (e.g. external Network classes, Loss functions); • an example configuration file defining default setting and the data ordering; • documentation describing the network and assumptions on the input data (e.g., dimensionality, shape constraints, intensity statistic assumptions). For trained networks, it should also include • a Tensorflow checkpoint containing the trained weights; • documentation describing the data used to train the network and on which\nthe trained network is expected to perform adequately."
    }, {
      "heading" : "5.3. Platform processes",
      "text" : "In addition to the implementation of common functionality, NiftyNet development has adopted good software development processes to support the ease-of-use, robustness and longevity of the platform as well as the creation of a vibrant community. The platform supports easy installation via the pip install niftynet command and provides several analysis pipelines immediately using command-line interfaces. Examples demonstrating the platform in multiple use cases are included within the repository to lower the learning curve. The NiftyNet repository uses continuous integration with a database of system and unit tests to mitigate the risks of bugs and code-breaking changes. NiftyNet releases will follow the semantic versioning 2.0 standard (Preston-Werner, 2015) to ensure clear communication about backwards compatibility."
    }, {
      "heading" : "6. Illustrative applications and results",
      "text" : ""
    }, {
      "heading" : "6.1. Abdominal organ segmentation",
      "text" : "Segmentations of anatomy and pathology on medical images can support image-guided interventional workflows by enabling the visualization of hidden\nanatomy and pathology during surgical navigation. This example, based on a simplified version of (Gibson et al., 2017a), illustrates the use of NiftyNet to train a Dense V-network segmentation network to segment organs on abdominal CT that are important to pancreatobiliary interventions: the gastrointestinal tract (esophagus, stomach and duodenum), the pancreas, and anatomical landmark organs (liver, left kidney, spleen and stomach).\nThe data used to train the network comprised 90 abdominal CT with manual segmentations from two publicly available data sets (Landman et al., 2015; Roth et al., 2016), with additional manual segmentations performed at our centre.\nThe network was trained and evaluated in a 9-fold cross-validation, using code available in contrib/DenseVNet in NiftyNet. Briefly, the network, available as dense vnet in NiftyNet, uses a V-shaped structure (with downsampling, upsampling and skip connections) where each downsampling stage is a dense feature stack (i.e. a sequence of convolution blocks where the inputs are concatenated features from all preceding convolution blocks), upsampling is bilinear upsampling and skip connections are convolutions. The loss is a modified Dice loss (with additional hinge losses to mitigate class imbalance) implemented external to NiftyNet and included via a reference in the configuration file. The network was trained for 3000 iterations on whole images (using the ResizeSampler) with random affine spatial augmentations.\nSegmentation metrics, computed using NiftyNet’s ‘evaluation‘ action, and aggregated over all folds, are given in Table 1. The segmentation with Dice scores closest to the median is shown in Figure 3."
    }, {
      "heading" : "6.2. Ultrasound simulation using generative adversarial networks",
      "text" : "Generating plausible images with specified image content can support training for radiological or image-guided interventional tasks. Conditional GANs have shown promise for generating plausible photographic images. (Mirza and Osindero, 2014) Recent work on spatially-conditioned GANs (Hu et al., 2017) suggests that conditional GANs could enable software-based simulation in place of costly physical ultrasound phantoms used for training. This example illustrates porting a pre-trained ultrasound simulation network to NiftyNet for\ninclusion in the NiftyNet model zoo. The network was originally trained outside of the NiftyNet platform as described in (Hu et al., 2017). Briefly, a conditional GAN network was trained to generate ultrasound images of specified views of a fetal phantom using 26,000 frames of optically tracked ultrasound. An image can be sampled from the generative model based on a conditioning image (denoting the pixel coordinates in 3D space) and a model parameter (sampled from a 100-D Gaussian distribution).\nThe network was ported to NiftyNet for inclusion in the model zoo. The network weights were transferred to the NiftyNet network using NiftyNet’s restore initializer, adapted from Sonnet (Reynolds et al., 2017), which enables trained variables to be loaded from networks with different architectures or naming schemes.\nThe network was evaluated multiple times using the linear interpolation inference in NiftyNet, wherein samples are taken from the generative model based on one conditioning image and a sequence of model parameters evenly interpolated between two random samples. Two illustrative results are shown in Figure 4. The first shows the same anatomy, but a smooth transition between different levels of ultrasound shadowing artifacts. The second shows a sharp transition in the interpolation, suggesting the presence of mode collapse, a common issue in GANs (Goodfellow, 2016)."
    }, {
      "heading" : "7. Platform availability",
      "text" : "The NiftyNet platform is available from http://niftynet.io/. Source code can be accessed from the git repository or installed as a python library using pip install niftynet. NiftyNet is licensed under an open-source Apache 2.0 license (https://www.apache.org/licenses/LICENSE-2.0), and the NiftyNet Consortium welcomes contributions to the platform and seeks inclusion of new community members to the consortium."
    }, {
      "heading" : "8. Future direction",
      "text" : "The active NiftyNet development roadmap is focused on three key areas: new application types, a larger model zoo and more advanced experimental design. NiftyNet currently supports image segmentation, regression, generation and representation learning applications. Future applications under development include image classification, registration, and enhancement (e.g. superresolution) as well as pathology detection. The current NiftyNet model zoo contains a small number of models as proof of principle; expanding the model zoo to include state-of-the-art models for common tasks and public challenges (e.g. brain tumor segmentation (BRaTS) (Menze et al., 2015; Wang et al., 2017a)); and models trained on large data sets for transfer learning will be critical to accelerating research with NiftyNet. Finally, NiftyNet currently supports a simplified machine learning pipeline that trains a single network, but relies on users for data partitioning and model selection (e.g. hyper-parameter tuning). Infrastructure to facilitate more complex experiments, such as built-in support for cross-validation and standardized hyper-parameter tuning will, in the future, reduce the implementation burden on users."
    }, {
      "heading" : "9. Summary of contributions/conclusions",
      "text" : "This work presents the open-source NiftyNet platform for deep learning in medical imaging. Our modular implementation of the typical medical imaging machine learning pipeline allows researchers to focus implementation effort on their specific innovations, while leveraging the work of others for the remaining pipeline. The NiftyNet platform provides implementations for data loading, data augmentation, network architectures, loss functions and evaluation metrics that are tailored for the idiosyncracies of medical image analysis and computerassisted interventions. This infrastructure enables researchers to rapidly develop deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to acknowledge all of the contributors to the NiftyNet platform. This work was supported by the Wellcome/EPSRC [203145Z/16/Z, WT101957, NS/A000027/1]; the Wellcome [106882/Z/15/Z, WT103709]; the Department of Health and Wellcome Trust [HICF-T4-275, WT 97914]; EPSRC [EP/M020533/1, EP/K503745/1, EP/L016478/1]; the National Institute for Health Research University College London Hospitals Biomedical Research Centre (NIHR BRC UCLH/UCL High Impact Initiative); Cancer Research UK (CRUK) [C28070/A19985]; the Royal Society [RG160569]; a UCL Overseas Research Scholarship, and a UCL Graduate Research Scholarship. The authors would like to acknowledge that the work presented here made use of Emerald, a GPU-accelerated High Performance Computer, made available by the Science & Engineering South Consortium operated in partnership with the STFC Rutherford-Appleton Laboratory; and hardware donated by NVIDIA."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous distributed systems. White paper arXiv:1603.04467v2",
      "author" : [ "V. Vasudevan", "F. Viegas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vasudevan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vasudevan et al\\.",
      "year" : 2016
    }, {
      "title" : "A reproducible evaluation of ANTs similarity metric performance in brain image registration",
      "author" : [ "B.B. Avants", "N.J. Tustison", "G. Song", "P.A. Cook", "A. Klein", "J.C. Gee" ],
      "venue" : null,
      "citeRegEx" : "Avants et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Avants et al\\.",
      "year" : 2011
    }, {
      "title" : "2016. Layer normalization arXiv:1607.06450v1",
      "author" : [ "J.L. Ba", "J.R. Kiros", "G.E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Ba et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Theano: new features and speed improvements; 2012, in: Proceedings of Deep Learning and Unsupervised Feature Learning NIPS’12 Workshop",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "NiftySeg: opensource software for medical image segmentation, label fusion and cortical thickness estimation, in: ISBI Workshop on Open Source Medical Image Analysis Software",
      "author" : [ "M. Cardoso", "M. Clarkson", "M. Modat", "S. Ourselin" ],
      "venue" : null,
      "citeRegEx" : "Cardoso et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cardoso et al\\.",
      "year" : 2012
    }, {
      "title" : "cuDNN: Efficient primitives for deep learning arXiv:1410.0759v3",
      "author" : [ "S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer" ],
      "venue" : null,
      "citeRegEx" : "Chetlur et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chetlur et al\\.",
      "year" : 2014
    }, {
      "title" : "3D U-net: learning dense volumetric segmentation from sparse annotation, in: MICCAI",
      "author" : [ "Ö. Çiçek", "A. Abdulkadir", "S.S. Lienkamp", "T. Brox", "O. Ronneberger" ],
      "venue" : null,
      "citeRegEx" : "Çiçek et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Çiçek et al\\.",
      "year" : 2016
    }, {
      "title" : "The NifTK software platform for image-guided interventions: platform overview and NiftyLink messaging. nternational",
      "author" : [ "M.J. Clarkson", "G. Zombori", "S. Thompson", "J. Totz", "Y. Song", "M. Espak", "S. Johnsen", "D. Hawkes", "S. Ourselin" ],
      "venue" : "Journal for Computer Assisted Radiology and Surgery",
      "citeRegEx" : "Clarkson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clarkson et al\\.",
      "year" : 2015
    }, {
      "title" : "Torch7: A MATLAB-like environment for machine learning, in: Proceedings of Big Learning 2011: NIPS’11",
      "author" : [ "R. Collobert", "K. Kavukcuoglu", "C. Farabet" ],
      "venue" : "Workshop on Algorithms,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Cortical surface-based analysis: I. segmentation and surface reconstruction",
      "author" : [ "A.M. Dale", "B. Fischl", "M.I. Sereno" ],
      "venue" : null,
      "citeRegEx" : "Dale et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Dale et al\\.",
      "year" : 1999
    }, {
      "title" : "Gift-cloud: A data sharing and collaboration platform for medical imaging research",
      "author" : [ "T. Doel", "D.I. Shakir", "R. Pratt", "M. Aertsen", "J. Moggridge", "E. Bellon", "A.L. David", "J. Deprest", "T. Vercauteren", "S. Ourselin" ],
      "venue" : "Computer Methods and Programs in Biomedicine",
      "citeRegEx" : "Doel et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Doel et al\\.",
      "year" : 2017
    }, {
      "title" : "TensorLayer: A versatile library for efficient deep learning development",
      "author" : [ "H. Dong", "A. Supratak", "L. Mai", "F. Liu", "A. Oehmichen", "S. Yu", "Y. Guo" ],
      "venue" : "ACM Multimedia URL: http://tensorlayer.org",
      "citeRegEx" : "Dong et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2017
    }, {
      "title" : "Generalised Wasserstein Dice score for imbalanced multi-class segmentation using holistic convolutional networks. Preprint arXiv:1707.00478",
      "author" : [ "L. Fidon", "W. Li", "L.C. Garcia-Peraza-Herrera", "J. Ekanayake", "N. Kitchen", "S. Ourselin", "T. Vercauteren" ],
      "venue" : null,
      "citeRegEx" : "Fidon et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Fidon et al\\.",
      "year" : 2017
    }, {
      "title" : "Cortical surface-based analysis: II: inflation, flattening, and a surface-based coordinate system",
      "author" : [ "B. Fischl", "M.I. Sereno", "A.M. Dale" ],
      "venue" : null,
      "citeRegEx" : "Fischl et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Fischl et al\\.",
      "year" : 1999
    }, {
      "title" : "Design patterns: elements of reusable object-oriented software",
      "author" : [ "E. Gamma", "J. Vlissides", "R. Johnson", "R. Helm" ],
      "venue" : null,
      "citeRegEx" : "Gamma et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Gamma et al\\.",
      "year" : 1994
    }, {
      "title" : "Toolnet: Holistically-nested real-time segmentation of robotic surgical tools. arXiv:1706.08126v2",
      "author" : [ "L.C. Garcia-Peraza-Herrera", "W. Li", "L. Fidon", "C. Gruijthuijsen", "A. Devreker", "G. Attilakos", "J. Deprest", "E.V. Poorten", "D. Stoyanov", "T. Vercauteren", "S. Ourselin" ],
      "venue" : null,
      "citeRegEx" : "Garcia.Peraza.Herrera et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Garcia.Peraza.Herrera et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic multi-organ segmentation on abdominal CT with dense v-networks",
      "author" : [ "E. Gibson", "F. Giganti", "Y. Hu", "E. Bonmati", "S. Bandula", "K. Gurusamy", "B. Davidson", "S.P. Pereira", "M.J. Clarkson", "D.C. Barratt" ],
      "venue" : "IEEE Transactions on Medical Imaging Submitted",
      "citeRegEx" : "Gibson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gibson et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards image-guided pancreas and biliary endoscopy: automatic multi-organ segmentation on abdominal CT with dense dilated networks",
      "author" : [ "E. Gibson", "F. Giganti", "Y. Hu", "E. Bonmati", "S. Bandula", "K. Gurusamy", "B.R. Davidson", "S.P. Pereira", "M.J. Clarkson", "D.C. Barratt" ],
      "venue" : "in: Proceedings of the 20th International Conference on Medical Image Computing and Computer",
      "citeRegEx" : "Gibson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gibson et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep residual networks for automatic segmentation of laparoscopic videos of the liver",
      "author" : [ "E. Gibson", "M.R. Robu", "S. Thompson", "P.E. Edwards", "C. Schneider", "K. Gurusamy", "B. Davidson", "D.J. Hawkes", "D.C. Barratt", "M.J. Clarkson" ],
      "venue" : "in: Proceedings of the SPIE, Medical Imaging",
      "citeRegEx" : "Gibson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gibson et al\\.",
      "year" : 2017
    }, {
      "title" : "Nips 2016 tutorial: Generative adversarial networks arXiv:1701.00160v4",
      "author" : [ "I. Goodfellow" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow,? \\Q2016\\E",
      "shortCiteRegEx" : "Goodfellow",
      "year" : 2016
    }, {
      "title" : "Freehand ultrasound image simulation with spatially-conditioned generative adversarial networks, in: Proceedings of MICCAI’17 Workshop on Reconstruction and Analysis of Moving Body Organs (RAMBO’17)",
      "author" : [ "Y. Hu", "E. Gibson", "L.L. Lee", "W. Xie", "D.C. Barratt", "T. Vercauteren", "J.A. Noble" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Batch renormalization: Towards reducing minibatch dependence in batch-normalized models arXiv:1702.03275v2",
      "author" : [ "S. Ioffe" ],
      "venue" : null,
      "citeRegEx" : "Ioffe,? \\Q2017\\E",
      "shortCiteRegEx" : "Ioffe",
      "year" : 2017
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "in: Proceedings of the 22nd ACM International Conference on Multimedia (ACMMM’14),",
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "NiftySim: A GPU-based nonlinear finite element package for simulation of soft tissue",
      "author" : [ "S.F. Johnsen", "Z.A. Taylor", "M.J. Clarkson", "J. Hipwell", "M. Modat", "B. Eiben", "L. Han", "Y. Hu", "T. Mertzanidou", "D.J. Hawkes", "S. Ourselin" ],
      "venue" : "biomechanics. nternational Journal for Computer Assisted Radiology and Surgery",
      "citeRegEx" : "Johnsen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Johnsen et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation",
      "author" : [ "K. Kamnitsas", "C. Ledig", "V.F. Newcombe", "J.P. Simpson", "A.D. Kane", "D.K. Menon", "D. Rueckert", "B. Glocker" ],
      "venue" : "Medical Image Analysis",
      "citeRegEx" : "Kamnitsas et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kamnitsas et al\\.",
      "year" : 2017
    }, {
      "title" : "Elastix: a toolbox for intensity-based medical image registration",
      "author" : [ "S. Klein", "M. Staring", "K. Murphy", "M.A. Viergever", "J.P. Pluim" ],
      "venue" : "IEEE Transactions on Medical Imaging",
      "citeRegEx" : "Klein et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2010
    }, {
      "title" : "Multi-atlas labeling beyond the cranial vault. URL: https://www.synapse. org/#!Synapse:syn3193805, doi:10.7303/syn3193805",
      "author" : [ "B. Landman", "Z. Xu", "J.E. Igelsias", "M. Styner", "T.R. Langerak", "A. Klein" ],
      "venue" : null,
      "citeRegEx" : "Landman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Landman et al\\.",
      "year" : 2015
    }, {
      "title" : "On the compactness, efficiency, and representation of 3D convolutional networks: Brain parcellation as a pretext",
      "author" : [ "W. Li", "G. Wang", "L. Fidon", "S. Ourselin", "M.J. Cardoso", "T. Vercauteren" ],
      "venue" : "Proceedings of Information Processing in Medical Imaging",
      "citeRegEx" : "Li et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "A survey on deep learning in medical image analysis",
      "author" : [ "G. Litjens", "T. Kooi", "B.E. Bejnordi", "A.A.A. Setio", "F. Ciompi", "M. Ghafoorian", "van der Laak", "J.A.W.M", "B. van Ginneken", "C.I. Snchez" ],
      "venue" : "Preprint arXiv:1702.05747v1",
      "citeRegEx" : "Litjens et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Litjens et al\\.",
      "year" : 2017
    }, {
      "title" : "Artificial convolution neural network techniques and applications for lung nodule detection",
      "author" : [ "S.C. Lo", "S.L. Lou", "J.S. Lin", "M.T. Freedman", "M.V. Chien", "S.K. Mun" ],
      "venue" : "IEEE Transactions on Medical Imaging",
      "citeRegEx" : "Lo et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Lo et al\\.",
      "year" : 1995
    }, {
      "title" : "The design of simpleitk. Frontiers in neuroinformatics",
      "author" : [ "B.C. Lowekamp", "D.T. Chen", "L. Ibáñez", "D. Blezek" ],
      "venue" : null,
      "citeRegEx" : "Lowekamp et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lowekamp et al\\.",
      "year" : 2013
    }, {
      "title" : "TensorBoard: TensorFlow’s visualization toolkit. https: //github.com/tensorflow/tensorboard",
      "author" : [ "D Mané" ],
      "venue" : null,
      "citeRegEx" : "Mané,? \\Q2015\\E",
      "shortCiteRegEx" : "Mané",
      "year" : 2015
    }, {
      "title" : "DeepInfer: Open-source deep learning deployment toolkit for image-guided therapy, in: Proceedings of the SPIE, Medical Imaging 2017",
      "author" : [ "A. Mehrtash", "M. Pesteie", "J. Hetherington", "P.A. Behringer", "T. Kapur", "W.M. Wells III", "R. Rohling", "A. Fedorov", "P. Abolmaesumi" ],
      "venue" : "NIH Public Access",
      "citeRegEx" : "Mehrtash et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Mehrtash et al\\.",
      "year" : 2017
    }, {
      "title" : "The multimodal brain tumor image segmentation benchmark (BraTS)",
      "author" : [ "B.H. Menze", "A. Jakab", "S. Bauer", "J. Kalpathy-Cramer", "K. Farahani", "J. Kirby", "Y. Burren", "N. Porz", "J. Slotboom", "R Wiest" ],
      "venue" : "IEEE Transactions on Medical Imaging",
      "citeRegEx" : "Menze et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Menze et al\\.",
      "year" : 2015
    }, {
      "title" : "V-Net: Fully convolutional neural networks for volumetric medical image segmentation",
      "author" : [ "F. Milletari", "N. Navab", "S.A. Ahmadi" ],
      "venue" : "in: Proceedings of the Fourth International Conference on 3D Vision (3DV’16),",
      "citeRegEx" : "Milletari et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Milletari et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional generative adversarial nets arXiv:1411.1784",
      "author" : [ "M. Mirza", "S. Osindero" ],
      "venue" : null,
      "citeRegEx" : "Mirza and Osindero,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza and Osindero",
      "year" : 2014
    }, {
      "title" : "Fast free-form deformation using graphics processing units",
      "author" : [ "M. Modat", "G.R. Ridgway", "Z.A. Taylor", "M. Lehmann", "J. Barnes", "D.J. Hawkes", "N.C. Fox", "S. Ourselin" ],
      "venue" : "Computer Methods and Programs in Biomedicine",
      "citeRegEx" : "Modat et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Modat et al\\.",
      "year" : 2010
    }, {
      "title" : "The Medical Imaging Interaction Toolkit: challenges and advances",
      "author" : [ "M. Nolden", "S. Zelzer", "A. Seitel", "D. Wald", "M. Müller", "A.M. Franz", "D. Maleike", "M. Fangerau", "M. Baumhauer", "L. Maier-Hein", "K.H. Maier-Hein", "H.P. Meinzer", "I. Wolf" ],
      "venue" : "nternational Journal for Computer Assisted Radiology and Surgery",
      "citeRegEx" : "Nolden et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nolden et al\\.",
      "year" : 2013
    }, {
      "title" : "The NA-MIC kit: ITK, VTK, pipelines, grids and 3D Slicer as an open platform for the medical image computing community",
      "author" : [ "S. Pieper", "B. Lorensen", "W. Schroeder", "R. Kikinis" ],
      "venue" : "in: Proceedings of the IEEE International Symposium on Biomedical Imaging: From Nano to Macro (ISBI’06),",
      "citeRegEx" : "Pieper et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Pieper et al\\.",
      "year" : 2006
    }, {
      "title" : "Semantic versioning",
      "author" : [ "T. Preston-Werner" ],
      "venue" : "Technical Report. URL: http: //semver.org/",
      "citeRegEx" : "Preston.Werner,? \\Q2015\\E",
      "shortCiteRegEx" : "Preston.Werner",
      "year" : 2015
    }, {
      "title" : "Data from TCIA Pancreas-CT",
      "author" : [ "H.R. Roth", "A. Farag", "E.B. Turkbey", "L. Lu", "J. Liu", "R.M. Summers" ],
      "venue" : null,
      "citeRegEx" : "Roth et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Roth et al\\.",
      "year" : 2016
    }, {
      "title" : "A new 2.5d representation for lymph node detection using random sets of deep convolutional neural network observations, in: MICCAI. doi:10.1007/978-3-319-10404-1_65",
      "author" : [ "H.R. Roth", "L. Lu", "A. Seff", "K.M. Cherry", "J. Hoffman", "S. Wang", "J. Liu", "E. Turkbey", "R.M. Summers" ],
      "venue" : null,
      "citeRegEx" : "Roth et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Roth et al\\.",
      "year" : 2014
    }, {
      "title" : "Weight normalization: A simple reparameterization to accelerate training of deep neural networks arXiv:1602.07868v3",
      "author" : [ "T. Salimans", "D.P. Kingma" ],
      "venue" : null,
      "citeRegEx" : "Salimans and Kingma,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans and Kingma",
      "year" : 2016
    }, {
      "title" : "CNTK: Microsoft’s open-source deep-learning toolkit",
      "author" : [ "F. Seide", "A. Agarwal" ],
      "venue" : "in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Seide and Agarwal,? \\Q2016\\E",
      "shortCiteRegEx" : "Seide and Agarwal",
      "year" : 2016
    }, {
      "title" : "Deep learning in medical image analysis",
      "author" : [ "D. Shen", "G. Wu", "H.I. Suk" ],
      "venue" : "Annual Review of Biomedical Engineering",
      "citeRegEx" : "Shen et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Advances in functional and structural MR image analysis and implementation as FSL",
      "author" : [ "S.M. Smith", "M. Jenkinson", "M.W. Woolrich", "C.F. Beckmann", "T.E. Behrens", "H. Johansen-Berg", "P.R. Bannister", "M. De Luca", "I. Drobnjak", "Flitney", "D.E" ],
      "venue" : "Neuroimage 23,",
      "citeRegEx" : "Smith et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2004
    }, {
      "title" : "Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentationsn, in: Proceedings of MICCAI’17 Workshop on Deep Learning in Medical Image Analysis (DLMIA’17)",
      "author" : [ "C.H. Sudre", "W. Li", "T. Vercauteren", "S. Ourselin", "M.J. Cardoso" ],
      "venue" : null,
      "citeRegEx" : "Sudre et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sudre et al\\.",
      "year" : 2017
    }, {
      "title" : "MatConvNet – convolutional neural networks for MATLAB, in: ACMM",
      "author" : [ "A. Vedaldi", "K. Lenc" ],
      "venue" : null,
      "citeRegEx" : "Vedaldi and Lenc,? \\Q2015\\E",
      "shortCiteRegEx" : "Vedaldi and Lenc",
      "year" : 2015
    }, {
      "title" : "Automatic brain tumor segmentation using cascaded anisotropic convolutional neural networks",
      "author" : [ "G. Wang", "W. Li", "S. Ourselin", "T. Vercauteren" ],
      "venue" : "Preprint arXiv:1709.00382",
      "citeRegEx" : "Wang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "2017b. Deepigeos: A deep interactive geodesic framework for medical image segmentation. Preprint arXiv:1707.00652v1",
      "author" : [ "G. Wang", "M.A. Zuluaga", "W. Li", "R. Pratt", "P.A. Patel", "M. Aertsen", "T. Doel", "A.L. David", "J. Deprest", "S. Ourselin", "T. Vercauteren" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Threedimensional CT image segmentation by combining 2D fully convolutional network with 3D majority voting",
      "author" : [ "X. Zhou", "T. Ito", "R. Takayama", "S. Wang", "T. Hara", "H. Fujita" ],
      "venue" : "in: LABELS2016,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Although the first use of neural networks for medical image analysis dates back more than twenty years (Lo et al., 1995), their usage has increased by orders of magnitude in the last five years.",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 44,
      "context" : "Recent reviews (Shen et al., 2017; Litjens et al., 2017) have highlighted that deep learning has been applied to a wide range of medical image analysis tasks (segmentation, classification, detection, registration, image reconstruction, enhancement, etc.",
      "startOffset" : 15,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : "Recent reviews (Shen et al., 2017; Litjens et al., 2017) have highlighted that deep learning has been applied to a wide range of medical image analysis tasks (segmentation, classification, detection, registration, image reconstruction, enhancement, etc.",
      "startOffset" : 15,
      "endOffset" : 56
    }, {
      "referenceID" : 36,
      "context" : "NiftyReg (Modat et al., 2010), ANTs (Avants et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : ", 2010), ANTs (Avants et al., 2011) and elastix (Klein et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : ", 2011) and elastix (Klein et al., 2010)), segmentation (e.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "NiftySeg (Cardoso et al., 2012)), and biomechanical modeling (e.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "(Johnsen et al., 2015)) are available for use as part of image analysis pipelines.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 45,
      "context" : "Pipelines for specific research applications such as FSL (Smith et al., 2004) for functional MRI analysis and Freesurfer (Fischl et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : ", 2004) for functional MRI analysis and Freesurfer (Fischl et al., 1999; Dale et al., 1999) for structural neuroimaging have reached widespread use.",
      "startOffset" : 51,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : ", 2004) for functional MRI analysis and Freesurfer (Fischl et al., 1999; Dale et al., 1999) for structural neuroimaging have reached widespread use.",
      "startOffset" : 51,
      "endOffset" : 91
    }, {
      "referenceID" : 38,
      "context" : "More general toolkits offering standardized implementations of algorithms (VTK and ITK (Pieper et al., 2006)) and application frameworks (NifTK (Clarkson et al.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : ", 2006)) and application frameworks (NifTK (Clarkson et al., 2015), MITK (Nolden et al.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 37,
      "context" : ", 2015), MITK (Nolden et al., 2013) and 3D Slicer (Pieper et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 38,
      "context" : ", 2013) and 3D Slicer (Pieper et al., 2006)) enable others to build their own pipelines.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : ", 2016), Theano (Bastien et al., 2012), Caffe (Jia et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : ", 2012), Caffe (Jia et al., 2014), Torch (Collobert et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : ", 2014), Torch (Collobert et al., 2011), CNTK (Seide and Agarwal, 2016), and MatConvNet (Vedaldi and Lenc, 2015).",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 43,
      "context" : ", 2011), CNTK (Seide and Agarwal, 2016), and MatConvNet (Vedaldi and Lenc, 2015).",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 47,
      "context" : ", 2011), CNTK (Seide and Agarwal, 2016), and MatConvNet (Vedaldi and Lenc, 2015).",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : ", 2015), and TensorLayer (Dong et al., 2017) for TensorFlow and Lasagne (Dieleman et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "NifTK (Clarkson et al., 2015; Gibson et al., 2017c) and Slicer3D (via the DeepInfer (Mehrtash et al.",
      "startOffset" : 6,
      "endOffset" : 51
    }, {
      "referenceID" : 32,
      "context" : ", 2017c) and Slicer3D (via the DeepInfer (Mehrtash et al., 2017) plugin) provide infrastructure for distribution of trained deep learning pipelines.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : ", 2017b) for semi-automated annotation and GIFT-Cloud (Doel et al., 2017) for data sharing are beginning to reduce these barriers, typical data sets remain small.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 50,
      "context" : "Many networks are designed to use partial images: 2D slices sampled along one axis from 3D images (Zhou et al., 2016), 3D subvolumes (Li et al.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 27,
      "context" : ", 2016), 3D subvolumes (Li et al., 2017), anisotropic convolution Wang et al.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 41,
      "context" : "(2017a), or combinations of subvolumes along multiple axes (Roth et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 34,
      "context" : "Other networks use multi-scale representations allowing deeper and wider networks on lower resolution representations (Milletari et al., 2016; Kamnitsas et al., 2017).",
      "startOffset" : 118,
      "endOffset" : 166
    }, {
      "referenceID" : 24,
      "context" : "Other networks use multi-scale representations allowing deeper and wider networks on lower resolution representations (Milletari et al., 2016; Kamnitsas et al., 2017).",
      "startOffset" : 118,
      "endOffset" : 166
    }, {
      "referenceID" : 21,
      "context" : "Smaller batch sizes can reduce the memory cost, but rely on different weight normalization functions such as batch renormalization (Ioffe, 2017), weight normalization (Salimans and Kingma, 2016) or layer normalization (Ba et al.",
      "startOffset" : 131,
      "endOffset" : 144
    }, {
      "referenceID" : 42,
      "context" : "Smaller batch sizes can reduce the memory cost, but rely on different weight normalization functions such as batch renormalization (Ioffe, 2017), weight normalization (Salimans and Kingma, 2016) or layer normalization (Ba et al.",
      "startOffset" : 167,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "Smaller batch sizes can reduce the memory cost, but rely on different weight normalization functions such as batch renormalization (Ioffe, 2017), weight normalization (Salimans and Kingma, 2016) or layer normalization (Ba et al., 2016).",
      "startOffset" : 218,
      "endOffset" : 235
    }, {
      "referenceID" : 21,
      "context" : ", 2016), 3D subvolumes (Li et al., 2017), anisotropic convolution Wang et al. (2017a), or combinations of subvolumes along multiple axes (Roth et al.",
      "startOffset" : 24,
      "endOffset" : 86
    }, {
      "referenceID" : 34,
      "context" : "Because small image features can have large clinical importance, and because some pathology is very rare but life-threatening, medical image analysis must deal with large class imbalances, motivating special loss functions (Milletari et al., 2016; Fidon et al., 2017; Sudre et al., 2017).",
      "startOffset" : 223,
      "endOffset" : 287
    }, {
      "referenceID" : 12,
      "context" : "Because small image features can have large clinical importance, and because some pathology is very rare but life-threatening, medical image analysis must deal with large class imbalances, motivating special loss functions (Milletari et al., 2016; Fidon et al., 2017; Sudre et al., 2017).",
      "startOffset" : 223,
      "endOffset" : 287
    }, {
      "referenceID" : 46,
      "context" : "Because small image features can have large clinical importance, and because some pathology is very rare but life-threatening, medical image analysis must deal with large class imbalances, motivating special loss functions (Milletari et al., 2016; Fidon et al., 2017; Sudre et al., 2017).",
      "startOffset" : 223,
      "endOffset" : 287
    }, {
      "referenceID" : 15,
      "context" : ", (Gibson et al., 2017c; Garcia-Peraza-Herrera et al., 2017)) where analysis results are used in real time have additional constraints on analysis latency.",
      "startOffset" : 2,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "Implemented as a template design pattern (Gamma et al., 1994), the ApplicationDriver delegates application-specific functionality to separate Application classes.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 30,
      "context" : "SimpleITK (Lowekamp et al., 2013) if they are installed and a file format is not supported by nibabel.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : ", unet (Çiçek et al., 2016) and vnet (Milletari et al.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 34,
      "context" : ", 2016) and vnet (Milletari et al., 2016) for segmentation), as well as trained networks for some tasks (e.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : ", 2017a) for brain tumor segmentation and simulator gan (Hu et al., 2017) for generating ultrasound images).",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 39,
      "context" : "0 standard (Preston-Werner, 2015) to ensure clear communication about backwards compatibility.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 26,
      "context" : "The data used to train the network comprised 90 abdominal CT with manual segmentations from two publicly available data sets (Landman et al., 2015; Roth et al., 2016), with additional manual segmentations performed at our centre.",
      "startOffset" : 125,
      "endOffset" : 166
    }, {
      "referenceID" : 40,
      "context" : "The data used to train the network comprised 90 abdominal CT with manual segmentations from two publicly available data sets (Landman et al., 2015; Roth et al., 2016), with additional manual segmentations performed at our centre.",
      "startOffset" : 125,
      "endOffset" : 166
    }, {
      "referenceID" : 35,
      "context" : "(Mirza and Osindero, 2014) Recent work on spatially-conditioned GANs (Hu et al.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 20,
      "context" : "(Mirza and Osindero, 2014) Recent work on spatially-conditioned GANs (Hu et al., 2017) suggests that conditional GANs could enable software-based simulation in place of costly physical ultrasound phantoms used for training.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "The network was originally trained outside of the NiftyNet platform as described in (Hu et al., 2017).",
      "startOffset" : 84,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "The second shows a sharp transition in the interpolation, suggesting the presence of mode collapse, a common issue in GANs (Goodfellow, 2016).",
      "startOffset" : 123,
      "endOffset" : 141
    }, {
      "referenceID" : 33,
      "context" : "brain tumor segmentation (BRaTS) (Menze et al., 2015; Wang et al., 2017a)); and models trained on large data sets for transfer learning will be critical to accelerating research with NiftyNet.",
      "startOffset" : 33,
      "endOffset" : 73
    } ],
    "year" : 2017,
    "abstractText" : "Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. This has resulted is substantial duplication of effort and incompatible infrastructure across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. This TensorFlow-based infrastructure provides a complete modular deep learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications with data loading, data augmentation, network architectures, loss functions and evaluation metrics that are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted interventions.",
    "creator" : "LaTeX with hyperref package"
  }
}