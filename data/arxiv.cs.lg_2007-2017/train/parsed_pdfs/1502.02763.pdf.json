{
  "name" : "1502.02763.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cascading Bandits",
    "authors" : [ "Branislav Kveton", "Csaba Szepesvári", "Zheng Wen", "Azin Ashkan" ],
    "emails" : [ "KVETON@ADOBE.COM", "SZEPESVA@CS.UALBERTA.CA", "ZHENGWEN@YAHOO-INC.COM", "AZIN.ASHKAN@TECHNICOLOR.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The cascade model (Craswell et al., 2008) was originally proposed in the context of web search and is one of the most popular models of user interaction with content. The user interacts with the model as follows. The user is recommended a list of K items, such as web pages or movies. Each item attracts the user with some probability, independently of the other items. The user examines the list from\nthe first item to the last, and stops at the first attractive item. In the context of web search, this is interpreted as a click. The items before the first attractive item are not attractive, because the user examines these items but does not stop at them. It is unknown which items after the first attractive item are attractive, because the user never examines these items. The optimal solution to the problem, the list of K items that maximizes the probability that the user finds an attractive item, are K items with highest attraction probabilities. Although simple, the model was found effective in explaining the so-called position bias in the statistics of search engine click logs (Craswell et al., 2008). Therefore, it is a reasonable model of user behavior.\nIn this paper, we propose an online learning variant of the cascade model, which we call cascading bandits. In this model, the attraction probabilities of items are unknown. At each time, the learning agent recommends to the user a list of K items out of L and receives feedback, the index of the item where the user stops. If the user does stop, the agent receives a reward of one. The goal of the agent is to maximize its total reward, or equivalently to minimize its regret with respect to the list of K most attractive items. Our problem can be viewed as a bandit problem where the agent receives the reward as a part of the feedback. However, the feedback is richer than the reward. In particular, the agent knows that the items before the first attractive item are not attractive.\nRanked bandits (Radlinski et al., 2008; Slivkins et al., 2013) are a popular approach in learning to rank and they are closely related to our paper. The key idea in ranked bandits is to model each position in the recommended list as an independent bandit problem, which is then solved by some “base” bandit algorithm. The algorithms for ranked bandits learn (1 − 1/e)-approximate solutions (Radlinski\nar X\niv :1\n50 2.\n02 76\n3v 1\n[ cs\n.L G\n] 1\n0 Fe\net al., 2008; Streeter & Golovin, 2009) and their regret is Ω(K), whereK is the number of recommended items. Our work can be viewed as a special case of ranked bandits, where each recommended item attracts the user independently. Under this assumption, we propose novel bandit algorithms that can learn the optimal solution and whose regret actually decreases with K. We compare one of our algorithms to ranked bandits in Section 5.3.\nOur problem is of combinatorial nature. In particular, it can be viewed as a combinatorial optimization problem, where the goal is to find K items out of L. In this sense, the problem is similar to stochastic combinatorial bandits, which are often studied with linear rewards and semi-bandit feedback (Gai et al., 2012; Kveton et al., 2014a;b), where the learning agent receives feedback for every selected item. This class of problems can be solved both computationally and sample efficiently (Kveton et al., 2015).\nOn the other hand, cascading bandits pose several novel challenges. The first challenge is that the reward function is non-linear in unknown parameters. Several other authors studied this setting. Filippi et al. (2010) study a generalized linear bandit with bandit feedback. Chen et al. (2013) study a stochastic combinatorial semi-bandit where the reward function is a known monotone function of a linear function in unknown parameters. Le et al. (2014) consider a network-optimization problem, where the payoff is a nonlinear function of the observations.\nThe second challenge is that the feedback in cascading bandits is between semi-bandit and bandit feedbacks. Bartók et al. (2012) considers finite partial monitoring problems. This is a general class of problems, with finitely many “outcomes” and actions. For each outcome and action pair there is a known payoff and a known feedback. Cascading bandits can be modeled as finite partial monitoring problems. In our case, the outcomes are the elements of a binary hypercube of dimension L and the actions are all lists of length K of L items. The algorithm reasons over all pairs of actions and stores vectors whose dimension is the number of outcomes. Therefore, it is unclear if it could be efficiently implemented for our problem. Moreover, no logarithmic distribution-dependent regret bounds are proved in this work. Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bartók et al. (2012), this algorithm would suffer from computational in-\nefficiencies in our setting. Lin et al. (2014) recently studied partial monitoring in combinatorial bandits. The setting of this paper is different from ours. In particular, Lin et al. (2014) assume that the observation is a linear transformation of the weights of the items. This transformation is fixed, known, and indexed only by the action. In our work, the transformation depends on the weights of the items.\nSeveral other papers (Mannor & Shamir, 2011; Chen et al., 2014) considered an opposite problem to ours, where the learning agent observes the weights of items that are similar to the chosen items. Chen et al. (2014) studied this problem in the context of stochastic combinatorial semi-bandits.\nWe make the following contributions. First, we propose and formalize a learning variant of the cascade model as a stochastic combinatorial partial monitoring problem. Second, we propose two computationally-efficient UCB-like algorithms for solving the problem, CascadeUCB1 and CascadeKL-UCB. The former is motivated by CombUCB1 (Gai et al., 2012; Kveton et al., 2015), a computationally and sample efficient algorithm for stochastic combinatorial semi-bandits. The latter is motivated by KL-UCB (Garivier & Cappe, 2011) and we expect it to perform better when the attraction probabilities of items are low, O(1/K). This setting is common in our domains of interest. Third, we bound the regret of both algorithms. Fourth, we derive a lower bound for cascading bandits and show that the upper bound of CascadeKL-UCB matches it up to a logarithmic factor. Finally, we evaluate both proposed algorithms on synthetic problems. Our experiments show that our algorithms perform well and can learn good policies even when our modeling assumptions are violated.\nOur paper is organized as follows. In Section 2, we review the cascade model. In Section 3, we introduce cascading bandits and propose two algorithms for solving them, CascadeUCB1 and CascadeKL-UCB. In Section 4, we derive gap-dependent upper bounds on the n-step regret of CascadeUCB1 and CascadeKL-UCB. Moreover, we derive a lower bound and discuss its relation to our upper bounds. In Section 5, we evaluate our algorithms and show that they can learn good policies even when our modeling assumptions are violated. In Section 6, we conclude and discuss extensions of our work."
    }, {
      "heading" : "2. Background",
      "text" : "Ranking functions in web search are typically learned by training a model of user interaction with content from click data (Agichtein et al., 2006; Radlinski & Joachims, 2005). In all models, the user is recommended an ordered list of K web pages A = (a1, . . . , aK), which we call items. The items belong to some ground set E = {1, . . . , L}, such as all possible web pages. The models differ in how\nthey explain the clicks and many models have been proposed (Becker et al., 2007; Craswell et al., 2008; Richardson et al., 2007). Due to space constraints, we only review the cascade model.\nThe cascade model (Craswell et al., 2008) is one of the most popular models of user interaction with content (Chapelle & Zhang, 2009). In this model, the user is recommended a list of K items A = (a1, . . . , aK) ∈ ΠK(E), where ΠK(E) is the set of all K-permutations of E, and examines them from the first item a1 to the last aK . Each item e ∈ E is associated with attraction probability w̄(e), the probability that the item attracts the user after the user examines it, independently of the other items. If the item attracts the user, the user clicks on it and does not examine the remaining items. Therefore, the probability that item ak is examined is ∏k−1 i=1 (1 − w̄(ai)). The probability that\nat least one item in A is attractive is 1− ∏K i=1(1− w̄(ai)). This quantity is maximized by K most attractive items.\nThe key assumption in the cascade model is that the user clicks on at most one item. In practice, the user may and typically does click on multiple items. The cascade model cannot explain this behavior. Therefore, the model has been extended to more realistic settings (Chapelle & Zhang, 2009; Guo et al., 2009a;b), such as multiple clicks and the persistence of the user. While these more complex models provide a better fit to actual web data, the cascade model is still attractive as it is simpler and still provides a reasonable fit. Therefore, in this paper, we only consider the online learning variant of the cascade model as a first step towards understanding more complex models."
    }, {
      "heading" : "3. Cascading Bandits",
      "text" : "In this section, we propose a learning variant of the cascade model (Section 2) and two learning algorithms for solving it (Section 3.2). For simplicity of exposition, all random variables are shown in bold."
    }, {
      "heading" : "3.1. Setting",
      "text" : "A generalized cascading bandit is a tuple B = (E,P ), where E = {1, . . . , L} is a ground set of L items and P is a probability distribution over a hypercube {0, 1}E . We refer to this bandit as generalized because the form of P is not specified.\nWe illustrate our bandit on the problem of web search. Let (wt) n t=1 be an i.i.d. sequence of n weights drawn from P , where wt ∈ {0, 1}E is a vector of item preferences of the user at time t. In particular, wt(e) = 1 if and only if item e attracts the user at time t. We also call wt(e) the weight of item e. At time t, the learning agents selects a list of K items At = (at1, . . . ,a t K) ∈ ΠK(E) and recommends\nthem to the user. The list At is chosen based on the observations of the agent up to time t. The user examines the items in the list, in the order in which the items are presented, and clicks on the first item that attracts the user. If no item attracts the user, the user does not click on any item. Then the bandit proceeds to time t+ 1.\nThe reward of the agent at time t can be written in many forms, such as maxk wt(atk), any recommended item at time t is attractive; or as f(At,wt), where:\nf(A,w) = 1− K∏ k=1 (1− w(ak)) ,\nA = (a1, . . . , aK) ∈ ΠK(E), and w ∈ [0, 1]E . This later algebraic form is particularly useful in our proofs.\nThe agent at time t receives feedback: Ct = arg min { 1 ≤ k ≤ K : wt(atk) = 1 } ,\nwhere arg min ∅ = ∞. The feedback Ct represents the click of the user. In particular, if Ct ≤ K, the user clicks on item Ct. If Ct = ∞, the user does not click on any item. Since the user clicks on the first attractive item, Ct fully determines the observed weights of the recommended items at time t. In particular, note that:\nwt(a t k) = 1{Ct = k} k = 1, . . . ,min {Ct,K} . (1)\nFormally, we say that the weight of item e is observed at time t if e = atk for some 1 ≤ k ≤ min {Ct,K}.\nThe assumption in the cascade model is that the weights of the items in the ground set E are distributed independently (Section 2). We make the same assumption. Assumption 1. The weights w are distributed as:\nP (w) = ∏ e∈E Pe(w(e)) ,\nwhere Pe is a Bernoulli distribution with mean w̄(e).\nUnder Assumption 1, we refer to B = (E,P ) as a cascading bandit. In this model, the weight of any item at time t is chosen independently of the weights of the other items at that, and any other, time. This assumption has profound consequences and leads to a very efficient learning algorithm in Section 3.2. Under this assumption, the expected reward for recommending items A ∈ ΠK(E), the probability that at least one item in A attracts the user, can be expressed as E [f(A,w)] = f(A, w̄), and depends only on the attraction probabilities of individual items in A.\nThe quality of the agent’s policy is measured by its expected cumulative regret:\nR(n) = E [ n∑ t=1 R(At,wt) ] ,\nwhere R(At,wt) = f(A∗,wt)− f(At,wt) is the instantaneous stochastic regret of the agent at time t and:\nA∗ = arg max A∈ΠK(E) f(A, w̄)\nis the optimal list of items, the list that maximized the reward at any time t. Since f is invariant to permutations of A, there are at least K! optimal lists. For simplicity of exposition, we assume that the optimal solution, as a set, is unique."
    }, {
      "heading" : "3.2. Algorithms",
      "text" : "We propose two algorithms for cascading bandits. The first algorithm is motivated by UCB1 (Auer et al., 2002) and we call it CascadeUCB1. The second algorithm is motivated by KL-UCB (Garivier & Cappe, 2011) and therefore we call it CascadeKL-UCB.\nBoth algorithms have the same form (Algorithm 1) and differ only in how they compute the upper confidence bounds (UCBs) on the expected weights of items at time t. After the UCBs are computed, the algorithms choose K items with largest UCBs:\nAt = arg maxA∈ΠK(E) f(A,Ut) (2)\nand recommend them to the user. Note that the solution At is determined only up to a permutation of the items in it. While the payoff is not affected by this ordering, the observations are. In our algorithms, we leave the order of items unspecified and return to it later in our discussions. After the algorithms observe a click Ct, they update the estimates of w̄(e) for all e = atk, k ≤ Ct, based on (1).\nThe UCBs are computed as follows. In CascadeUCB1, the UCB on the expected weight of item e at time t is:\nUt(e) = ŵTt−1(e)(e) + ct−1,Tt−1(e) ,\nwhere ŵs(e) is the average of s observed weights of item e, Tt(e) is the number of times that item e is observed in t steps, and:\nct,s = √ (1.5 log t)/s\nis the radius of a confidence interval around ŵs(e) at time t such that w̄(e) ∈ [ŵs(e)− ct,s, ŵs(e) + ct,s] holds with high probability. In CascadeKL-UCB, the UCB on the expected weight of item e at time t is:\nUt(e) = max{q ∈ [ŵTt−1(e)(e), 1] : Tt−1(e)DKL(ŵTt−1(e)(e) ‖ q) ≤ log(t) + 3 log(log(t))} ,\nwhere DKL(p ‖ q) is the Kullback-Leibler (KL) divergence between two Bernoulli distributions with means p and q. Note that the above UCB can be computed efficiently because DKL(p ‖ q) is an increasing function of q for q ≥ p.\nAlgorithm 1 UCB-like algorithm for cascading bandits. // Initialization Observe w0 ∼ P ∀e ∈ E : T0(e)← 1 ∀e ∈ E : ŵ1(e)← w0(e)\nfor all t = 1, . . . , n do Compute UCBs Ut(e) (Section 3.2)\n// Recommend K items and get feedback Let at1, . . . ,a t K be K items with largest UCBs At ← (at1, . . . ,atK) Observe click Ct ∈ {1, . . . ,K,∞}\n// Update statistics ∀e ∈ E : Tt(e)← Tt−1(e) for all k = 1, . . . ,min {Ct,K} do e← atk Tt(e)← Tt(e) + 1\nŵTt(e)(e)← Tt−1(e)ŵTt−1(e)(e) + 1{Ct = k}\nTt(e)"
    }, {
      "heading" : "3.3. Initialization",
      "text" : "We assume that both CascadeUCB1 and CascadeKL-UCB are initialized by a sample of weights w0 ∼ P . This assumption is relatively mild when L is small. In particular, such a sample can generated in at most L steps, by repeatedly recommending a list of items that have not been observed yet. The corresponding regret is O(L)."
    }, {
      "heading" : "4. Analysis",
      "text" : "Our analysis is organized as follows. In Section 4.1, we decompose the regret at time t such that we get the gaps between the attraction probabilities of individual items and indicators of the events that the recommended items are observed. This is the key step in our analysis. In Section 4.2, we bound the regret of CascadeUCB1 and CascadeKL-UCB. In Section 4.3, we derive a lower bound for cascading bandits. Finally, we discuss our results in Section 4.4.\nThe combinatorial structure of our problem is a matroid, find K most attractive items out of L. However, we cannot analyze our problem straightforwardly as a matroid bandit (Kveton et al., 2014a), because our reward function is nonlinear and the weights of recommended items are partially observed. Our analysis is also very different from that of Radlinski et al. (2008). To obtain tight regret bounds, we need modify the analysis of our base algorithms, UCB1 and KL-UCB, instead of treating them just like black boxes."
    }, {
      "heading" : "4.1. Regret Decomposition",
      "text" : "Before we start, we introduce some notation and conventions. Without loss of generality, we assume that the items in E are ordered such that w̄(1) ≥ . . . ≥ w̄(L). Therefore, the optimal solution are the first K items in E, A∗ = (1, . . . ,K). We say that item e is optimal if 1 ≤ e ≤ K. We say that item e is suboptimal if K < e ≤ L. The hardness of discriminating a suboptimal item e from an optimal item e∗ is measured by a gap between the attraction probabilities of the items:\n∆e,e∗ = w̄(e ∗)− w̄(e) . (3)\nWhenever convenient, we view an ordered list of items as the set of items on the list.\nOur main technical lemma is below. The lemma says that the expected value of the difference of products of random variables can be written in a particularly useful form. Lemma 1. Let u ∈ [0, 1]K and v ∈ [0, 1]K be random vectors whose entries are drawn independently from some distributions. Then:\nE [ K∏ k=1 u(k)− K∏ k=1 v(k) ] =\nK∑ k=1 E [ k−1∏ i=1 u(i) ] E [u(k)− v(k)]  K∏ j=k+1 E [v(j)]  . Proof. The claim is proven in Appendix B.\nLet:\nFt = (C1,A1, . . . ,At−1,Ct−1,At) (4)\nbe the history of the learning agent up to choosing solution At, the first t − 1 observations and t actions; and Et [ · ] = E [ · |Ft] be the conditional expectation given this history. In the next theorem, we decompose the expected regret R̄t = Et [R(At,wt)] conditioned on history Ft. Theorem 1. For any item e and optimal item e∗, let:\nGe,e∗,t = {∃1 ≤ k ≤ K s.t. atk = e, πt(k) = e∗, (5) wt(a t 1) = . . . = wt(a t k−1) = 0)}\nbe the event that item e is chosen instead of item e∗ at time t, and the weight of item e is observed; where πt is a permutation of optimal items {1, . . . ,K}, which depends on history Ft in a deterministic way. Then: R̄t ≤ L∑\ne=K+1 K∑ e∗=1 ∆e,e∗Et [1{Ge,e∗,t}]\nR̄t ≥ (1− w̄(K))K−1 L∑\ne=K+1 K∑ e∗=1 ∆e,e∗ Et [1{Ge,e∗,t}] ,\nwhere w̄(K) is the attraction probability of the least attractive optimal item. Furthermore, πt can be chosen such that for any item e and optimal item e∗, Ut(e) ≥ Ut(e∗).\nProof. We define the permutation πt as follows. For any k, if the k-th item in At is optimal, the permutation assigns this item to position k, πt(k) = atk. In all remaining positions, the optimal items are in increasing order. Since A∗ is optimal with respect to w̄, w̄(atk) ≤ w̄(πt(k)) for every k. Furthermore, since At is optimal with respect to Ut, Ut(a t k) ≥ Ut(πt(k)) for every k. Therefore, our permutation satisfies the last requirement of our theorem.\nThe permutation πt reorders the optimal items in a convenient way. Since time t is fixed, let a∗k = πt(k). Then:\nR̄t = Et [f(A∗,wt)− f(At,wt)]\n= Et [ K∏ k=1 (1−wt(atk))− K∏ k=1 (1−wt(a∗k)) ] .\nNow we exploit the fact that the entries of wt are independent of each other given Ft. By Lemma 1, we can rewrite the right-hand-side in the above equation as:\nK∑ k=1 Et [ k−1∏ i=1 (1−wt(ati)) ] × Et [ wt(a ∗ k)−wt(atk) ] × K∏\nj=k+1\nEt [ 1−wt(a∗j ) ] . Note that Et [wt(a∗k)−wt(atk)] = ∆atk,a∗k . Furthermore,∏k−1 i=1 (1−wt(ati)) = 1 { Gatk,a∗k,t } by conditioning on Ft. Therefore, we get that the regret R̄t is equal to:\nK∑ k=1 ∆atk,a∗kEt [ 1 { Gatk,a∗k,t }] K∏ j=k+1 Et [ 1−wt(a∗j ) ] .\nBy the definition of πt, ∆atk,a∗k = 0 when item a t k is optimal. Moreover, 1− w̄(K) ≤ Et [ 1−wt(a∗j ) ] ≤ 1 for any optimal item a∗j . Our upper and lower bounds on the regret R̄t follow directly from these observations."
    }, {
      "heading" : "4.2. Upper bounds",
      "text" : "In this section, we derive two upper bounds on the n-step regret of CascadeUCB1 and CascadeKL-UCB.\nTheorem 2. The expected n-step regret of CascadeUCB1 is bounded as:\nR(n) ≤ L∑\ne=K+1\n12\n∆e,K log n+\nπ2\n3 L .\nProof. The complete proof is in Appendix A.1. The proof has four main steps. First, we bound the regret of the event that w̄(e) is outside of the high-probability confidence interval around ŵTt−1(e)(e) for some item e. Second, we decompose the regret at time t and bound it from above using Theorem 1. Third, we bound the number of times that each suboptimal item is chosen in n steps. Fourth, we apply the peeling argument of Kveton et al. (2014a) and eliminate an extra factor of K in our upper bound. Finally, we sum up the regret of all suboptimal items.\nTheorem 3. For any ε > 0, the expected n-step regret of CascadeKL-UCB is bounded as:\nR(n) ≤ L∑\ne=K+1\n(1 + ε)∆e,K log(1/∆e,K)\nDKL(w̄(e) ‖ w̄(K)) ×\n(log n+ 3 log(log n)) + C ,\nwhere C = KLC2(ε) nβ(ε) + 7K log(log n), and the constants C2(ε) and β(ε) are defined in Garivier & Cappe (2011).\nProof. The complete proof is in Appendix A.2. The proof has four main steps. First, we bound the regret of the event that w̄(e) ≤ Ut(e) for some optimal item e. Second, we decompose the regret at time t and bound it from above using Theorem 1. Third, we bound the number of times that each suboptimal item is chosen in n steps, along the lines of Garivier & Cappe (2011). Fourth, we derive a new peeling argument for KL-UCB and eliminate an extra factor of K in our upper bound (Lemma 3). Finally, we sum up the regret of all suboptimal items."
    }, {
      "heading" : "4.3. Lower Bound",
      "text" : "Our lower bound is derived on the following problem. The ground set are L itemsE = {1, . . . , L}. The distribution P is a product of L Bernoulli distributions Pe, each of which is parameterized by:\nw̄(e) = { p , e ≤ K ; p−∆ , otherwise ,\n(6)\nwhere ∆ ∈ (0, p) is the gap between any optimal and suboptimal item. We refer to the resulting bandit as Btop-K = (E,P (K, p,∆)).\nOur lower bound is derived for the class of consistent algorithms, which is defined as follows. We say that the algorithm is consistent if for any cascading bandit, any suboptimal A, and any α > 0, E [Tn(A)] = o(nα), where Tn(A) is the number of times that solution A is chosen in n steps. The restriction to the consistent algorithms is without loss of generality. In particular, an inconsistent algorithm is guaranteed to perform poorly on some problems in our class, and therefore cannot achieve logarithmic regret on all problems in the class as our algorithms.\nTheorem 4. For any cascading bandit Btop-K, the regret of any consistent algorithm is bounded from below as:\nlim inf n→∞\nR(n) log n ≥ (L−K)(1− p) K−1∆ DKL(p−∆ ‖ p) .\nProof. Based on Theorem 1, the expected regret at time t conditioned on history Ft is bounded from below as:\nR̄t ≥ ∆(1− p)K−1 L∑\ne=K+1 K∑ e∗=1 E [1{Ge,e∗,t}] .\nBased on this bound, the n-step regret is bounded as:\nR(n) ≥ ∆(1− p)K−1 L∑\ne=K+1\nE [ n∑ t=1 K∑ e∗=1 1{Ge,e∗,t} ]\n= ∆(1− p)K−1 L∑\ne=K+1\nE [Tn(e)] ,\nwhere the last step is based on the fact that the observation counter of item e increases if and only if event Ge,e∗,t happens. Following the same argument as in the proof of the lower bound of Lai & Robbins (1985), we have:\nlim inf n→∞ E [Tn(e)] log n ≥ 1 DKL(p−∆ ‖ p)\nfor any suboptimal item e. Otherwise, the learning algorithm would not have a sufficient number of observations Tn(e) to distinguish item e in the instances of our problem where this item is optimal, and therefore would not be consistent. Now we put all inequalities together and get:\nlim inf n→∞\nR(n) log n ≥ (L−K)(1− p) K−1∆ DKL(p−∆ ‖ p) .\nThis concludes our proof."
    }, {
      "heading" : "4.4. Discussion",
      "text" : "We prove two gap-dependent upper bounds on the n-step regret of CascadeUCB1 (Theorem 2) and CascadeKL-UCB (Theorem 3). The bounds are O(log n), scale linearly with the number of items L, and improve as the number of recommended items K increases. The bounds do not depend on the order of recommended items. This is due to the nature of our proofs, where we count item-specific events that ignore the positions of the items. Extending our analysis in this direction is an interesting idea for future work.\nWe discuss the tightness of our upper bounds below. For this, we consider Btop-K = (E,P (K, p,∆)) bandit of Section 4.3 where p = 1/K. In this problem, Theorem 4 implies an asymptotic lower bound of:\nΩ((L−K)(∆/DKL(p−∆ ‖ p)) log n) (7)\nbecause (1 − 1/K)K−1 ≥ 1/e. The upper bound on the n-step regret of CascadeUCB1 (Theorem 2) is:\nO((L−K)(1/∆) log n) = O((L−K)(∆/∆2) log n) = O(p−1(1− p)−1(L−K)(∆/DKL(p−∆ ‖ p)) log n) = O(K(L−K)(∆/DKL(p−∆ ‖ p)) log n) , (8)\nwhere the second equality is because DKL(p − ∆ ‖ p) ≤ ∆2/(p(1 − p)). The upper bound on the n-step regret of CascadeKL-UCB (Theorem 3) is:\nO((L−K)(∆ log(1/∆)/DKL(p−∆ ‖ p)) log n) (9)\nand matches the lower bound in (7) up to log(1/∆). Note that the upper bound of CascadeKL-UCB (9) is below that of CascadeUCB1 (8) for as long as log(1/∆) = O(K), or equivalently ∆ = Ω(e−K). It is an open problem whether the factor of log(1/∆) in (9) can be eliminated."
    }, {
      "heading" : "5. Experiments",
      "text" : "We conduct three experiments. In the first experiment, we show that the regret of our algorithms grows as suggested by our upper bounds (Section 4.2). In the second experiment, we modify our algorithms such that they recommend K items in the opposite order, from the smallest UCB to the largest. In the third experiment, we show that CascadeKL-UCB performs well on various synthetic problems. We also compare it to RankedKL-UCB."
    }, {
      "heading" : "5.1. Regret Bounds",
      "text" : "In this section, we validate the qualitative behavior of our bounds (Section 4.2). We experiment with the class of problems Btop-K = (E,P (K, p,∆)) of Section 4.3 with p = 0.2. We vary L, K, and ∆; and run CascadeUCB1 and CascadeKL-UCB for n = 105 steps. The maximum attraction probability p = 0.2 is chosen such that is it close to 1/K for the maximum number of recommended items K in our experiment. In this setting, our upper bounds are\nreasonably tight (Section 4.4), and the regret of our algorithms should scale similarly to our upper bounds. The recommended items in Algorithm 1 are ordered in decreasing order of their UCBs. This ordering is motivated by practical applications, higher ranked items in web search are expected to be more attractive.\nOur results are reported in Table 1. We observe four trends. First, the regret doubles when the number of items L doubles. Second, the regret decreases as the number of chosen items K increases. Both observations are consistent with the shape of our upper bounds, O(L − K). Third, the regret increases when ∆ increases. Finally, CascadeKL-UCB consistently outperforms CascadeUCB1. This last observation is not surprising. In particular, KL-UCB is known to outperform UCB1 when the expected payoffs of arms are low (Garivier & Cappe, 2011). The reason is that the confidence bounds in KL-UCB get tighter as the Bernoulli parameters get closer to zero, or one, as compared with the confidence bounds in UCB1."
    }, {
      "heading" : "5.2. “Worst-of-Best First” Item Ordering",
      "text" : "In the second experiment, we evaluate a variant of our algorithms where the recommended items are ordered in increasing order of their UCBs. This choice seems strange, perhaps even dangerous. In practice, the user could get annoyed if highly ranked items were not very attractive. On the other hand, the user would provide a lot of feedback about low quality items and speed up learning. Note that the expected return in our model is invariant to the order of recommended items (Section 3.2). Therefore, the payoff does not change when the items are reordered. In any case, we find it important to study the effect of this counterintuitive ordering in the context of the present model, at least to demonstrate an unwanted effect of our modeling assumptions.\nThe experimental setup is the same as in Section 5.1. Our results are reported in Table 2. In comparison to Table 1, the regret of both CascadeUCB1 and CascadeKL-UCB decreases for all settings of K, L, and ∆; most prominently\nfor large values of K. Our current analysis cannot explain this phenomenon and it is an interesting open question how to incorporate it."
    }, {
      "heading" : "5.3. Imperfect Model",
      "text" : "In the third experiment, we study CascadeKL-UCB when our modeling assumptions are violated, to test its potential beyond the cascade model. In particular, we generate data from a dynamic Bayesian network (DBN) model (Chapelle & Zhang, 2009), a popular extension of the cascade model. The DBN model is parameterized by attraction probabilities ρ ∈ [0, 1]E , satisfaction probabilities ν ∈ [0, 1]E , and the persistence γ of the user. As in the cascade model, the user is recommended a list of K items A = (a1, . . . , aK) and examines them from the first item a1 to the last aK . Item ak attract the user with probability ρ(ak). If the user is attracted, the user clicks on the item and the item satisfies the user with probability ν(ak). If the user is satisfied, the user does not examine the remaining items. If the user is not attracted nor satisfied, the user examines the next item with probability γ. The reward is one if the user is satisfied. Note that this is unobserved. The regret is modified accordingly. The feedback are clicks on the recommended items. Note that the user can click on multiple items.\nIt is easy to see that the probability that the user is satisfied with at least one item in A = (a1, . . . , aK) is:∑K\nk=1 γ k−1w̄(ak) ∏k−1 i=1 (1− w̄(ai)) ,\nwhere w̄(e) = ρ(e)ν(e) is the probability that item e satisfies the user after the item is examined. This objective is maximized by a list of K items with highest w̄(e), which are ordered from the largest weight to the smallest. Note that the order matters. When γ = 1, the reward is the same as f(A, w̄). Therefore, our algorithms can solve a learning variant of this problem. The feedback is the position of the last click. Our theoretical guarantees also hold.\nWhen γ < 1, the situation is less pleasant. We still know that if the user clicks on multiple items, only the last click may be satisfactory. However, it may not be. For instance, the user could have lost patience and stopped searching after the last click. In this case, our current approach inflates the estimates of w̄ and introduces bias.\nWe experiment with the problem in Section 5.1 and alter it as follows. The ground set are L = 16 items and K = 4. The attraction probability of item e is ρ(e) = w̄(e), where w̄(e) is defined in (6). We set ∆ to 0.15. The satisfaction probabilities ν(e) of all items are the same. We experiment with two settings of ν(e), 1 and 0.7. We experiment with two settings of the persistence γ, 1 and 0.7. We run CascadeKL-UCB for n = 105 steps and use the last click as an indicator that the user is satisfied with that item.\nOur results are reported in Figure 1. In all experiments, the regret of CascadeKL-UCB flattens out and we verified that CascadeKL-UCB learns a near-optimal policy. An intuitive explanation for this result is that the exact values of w̄ are not needed to perform well. Our theory does not explain this behavior. It remains for future work to find conditions under which this is true.\nWe also compare CascadeKL-UCB to a ranked bandit (Section 1), where the “base” bandit algorithm is KL-UCB. We refer to this method as RankedKL-UCB. The feedback is the same as in CascadeKL-UCB. We implement this variant of a ranked bandit for two reasons. First, KL-UCB is the best performing oracle in our experiments. Second, if both compared methods use the same oracle, the difference in their regret must be due to the efficiency of using the oracle.\nWe observe in Figure 1 that the regret of RankedKL-UCB is about three times larger than that of CascadeKL-UCB. This is consistent with the fact that the regret of ranked bandits is Ω(K) (Section 1) and that K = 4 in our experiments. Note that CascadeKL-UCB outperforms RankedKL-UCB in a problem where CascadeKL-UCB is not guaranteed to be optimal. So CascadeKL-UCB may be a viable alternative to ranked bandits, a well-established approach."
    }, {
      "heading" : "6. Conclusions",
      "text" : "In this work, we propose a learning variant of the cascade model (Craswell et al., 2008), a popular model of user interaction with content. We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB, and bound their regret. Our analysis addresses two challenging problems in stochastic combinatorial bandits. First, the reward function is non-linear. Second, the weights of the chosen items are only partially observed. We also prove a lower bound for cascading bandits and show that it matches the upper bound of CascadeKL-UCB up to a logarithmic factor. Finally, we evaluate the proposed algorithms on several synthetic prob-\nlems and show that they perform robustly even when our modeling assumptions are violated.\nWe leave open many questions of interest. First, our algorithms perform very well even our modeling assumptions are violated (Section 5.3). This indicates that the DBN model is learnable in the online setting and that our work can be extended to multiple-click models. Second, the regret of our algorithms is Ω(L). So they are not very practical when L is large. Therefore, similarly to ranked bandits (Slivkins et al., 2013), we would like to extend our work to the contextual setting to speed up learning. Third, we want to extend our approach to more complex combinatorial constraints, such as learning paths in routing networks where the links fail with unknown probabilities. From the theoretical point of view, we want to close the gap between our upper and lower bounds, and derive distribution-free bounds. Finally, we want to refine our analysis so that it explains that the reverse ordering of recommended items yields smaller regret (Section 5.2)."
    }, {
      "heading" : "A. Proofs of Main Theorems",
      "text" : "A.1. Proof of Theorem 2\nLet Rt = R(At,wt) be the regret of CascadeUCB1 at time t, where At is the solution chosen by CascadeUCB1 at time t and wt are the weights at time t. Let Et = { ∃e ∈ E s.t. ∣∣w̄(e)− ŵTt−1(e)(e)∣∣ ≥ ct−1,Tt−1(e)} be the event that w̄(e) is not in the high-probability confidence interval around ŵTt−1(e)(e) for some e at time t; and let Et be the complement of Et, w̄(e) is in the high-probability confidence interval around ŵTt−1(e)(e) for all e at time t. Then the expected regret of CascadeUCB1 can be written as:\nR(n) = E [ n∑ t=1 1{Et}Rt ] + E [ n∑ t=1 1 { Et } Rt ] . (10)\nNow we bound both terms in the above regret decomposition.\nThe first term in (10) is small because all of our confidence intervals hold with high probability. In particular, Hoeffding’s inequality (Boucheron et al., 2012, Theorem 2.8) yields that for any e, s, and t:\nP (|w̄(e)− ŵs(e)| ≥ ct,s) ≤ 2 exp[−3 log t] ,\nand therefore:\nE [ n∑ t=1 1{Et} ] ≤ ∑ e∈E n∑ t=1 t∑ s=1 P (|w̄(e)− ŵs(e)| ≥ ct,s) ≤ 2 ∑ e∈E n∑ t=1 t∑ s=1 exp[−3 log t] ≤ 2 ∑ e∈E n∑ t=1 t−2 ≤ π 2 3 L .\nSince Rt ≤ 1, E [ ∑n t=1 1{Et}Rt] ≤ π2 3 L.\nRecall that Et [ · ] = E [ · |Ft], where Ft are the first t − 1 observations and t actions, and is defined in (4). Based on this, we rewrite the second term in (10) as:\nE [ n∑ t=1 1 { Et } Rt ] (a) = n∑ t=1 E [ 1 { Et } Et [Rt] ] (b) ≤ L∑ e=K+1 E [ K∑ e∗=1 n∑ t=1 ∆e,e∗1 { Et, Ge,e∗,t }] ,\nwhere equality (a) is due to the tower rule and that 1 { Et }\nis only a function of Ft, and inequality (b) is due to the upper bound in Theorem 1.\nNow we bound ∑K e∗=1 ∑n t=1 ∆e,e∗1 { Et, Ge,e∗,t } for any suboptimal item e. Choose any optimal item e∗. When event Et\nhappens, ∣∣w̄(e)− ŵTt−1(e)(e)∣∣ < ct−1,Tt−1(e). Moreover, by Theorem 1, when event Ge,e∗,t happens, Ut(e) ≥ Ut(e∗). Therefore, when both Ge,e∗,t and Et happen:\nw̄(e) + 2ct−1,Tt−1(e) > Ut(e) ≥ Ut(e ∗) ≥ w̄(e∗) ,\nwhich implies:\n2ct−1,Tt−1(e) ≥ ∆e,e∗ .\nThis, together with cn,Tt−1(e) ≥ ct−1,Tt−1(e), implies Tt−1(e) ≤ τe,e∗ , where τe,e∗ = 6 logn ∆2 e,e∗ . Therefore:\nK∑ e∗=1 n∑ t=1 ∆e,e∗1 { Et, Ge,e∗,t } ≤ K∑ e∗=1 ∆e,e∗ n∑ t=1 1{Tt−1(e) ≤ τe,e∗ , Ge,e∗,t} . (11)\nLet:\nMe,e∗ = n∑ t=1 1{Tt−1(e) ≤ τe,e∗ , Ge,e∗,t}\nbe the inner sum in (11). Now note that (i) the counter Tt−1(e) of item e increases by one when the event Ge,e∗,t happens for any optimal item e∗, (ii) the event Ge,e∗,t happens for at most one optimal e∗ at any time t; and (iii) τe,1 ≤ . . . ≤ τe,K .\nBased on these, it follows that Me,e∗ ≤ τe,e∗ , and moreover ∑K e∗=1 Me,e∗ ≤ τe,K . Therefore, the right-hand side of (11) can be bounded from above by:\nmax\n{ K∑\ne∗=1\n∆e,e∗me,e∗ : 0 ≤ me,e∗ ≤ τe,e∗ , K∑\ne∗=1\nme,e∗ ≤ τe,K } .\nSince the gaps are decreasing, ∆e,1 ≥ . . . ≥ ∆e,K , the solution to the above problem is m∗e,1 = τe,1, m∗e,2 = τe,2 − τe,1, . . . , m∗e,K = τe,K − τe,K−1. Therefore, the value of (11) is bounded from above by:[\n∆e,1 1\n∆2e,1 + K∑ e∗=2 ∆e,e∗\n( 1\n∆2e,e∗ − 1 ∆2e,e∗−1\n)] 6 log n .\nBy Lemma 3 of Kveton et al. (2014a), the above term is bounded by 12∆e,K log n. Finally, we chain all inequalities and sum over all suboptimal items e.\nA.2. Proof of Theorem 3\nLet Rt = R(At,wt) be the regret of CascadeKL-UCB at time t, where At and wt are the solution and the weights of the items at time t, respectively. Moreover, let Et = {∃1 ≤ e ≤ K s.t. w̄(e) > Ut(e)} be the event that the attraction probability of at least one optimal item is above its upper confidence bound at time t. Let Et be the complement of Et. Then we can decompose the regret of CascadeKL-UCB as:\nR(n) = E [ n∑ t=1 1{Et}Rt ] + E [ n∑ t=1 1 { Et } Rt ] . (12)\nBy Theorems 2 and 10 of Garivier & Cappe (2011), thanks to the choice of the upper confidence bound Ut, the first term in (12) is bounded as E [ ∑n t=1 1{Et}Rt] ≤ 7K log(log n). As in the proof of Theorem 2, we rewrite the second term as:\nE [ n∑ t=1 1 { Et } Rt ] = n∑ t=1 E [ 1 { Et } Et [Rt] ] (a) ≤ L∑ e=K+1 E [ K∑ e∗=1 n∑ t=1 ∆e,e∗1 { Et, Ge,e∗,t }] .\nInequality (a) is due to the upper bound in Theorem 1. Now note that for any suboptimal e and τe,e∗ > 0:\nE\n[ K∑\ne∗=1 n∑ t=1 ∆e,e∗1 { Et, Ge,e∗,t\n}] ≤ E [ K∑\ne∗=1 n∑ t=1 ∆e,e∗1{Tt−1(e) ≤ τe,e∗ , Ge,e∗,t}\n] + (13)\nK∑ e∗=1 ∆e,e∗E [ n∑ t=1 1 { Tt−1(e) > τe,e∗ , Et, Ge,e∗,t }] .\nLet:\nτe,e∗ = 1 + ε\nDKL(w̄(e) ‖ w̄(e∗)) (log n+ 3 log(log n)) .\nThen by Lemma 8 of Garivier & Cappe (2011):\nE [ n∑ t=1 1 { Tt−1(e) > τe,e∗ , Et, Ge,e∗,t }] ≤ C2(ε) nβ(ε)\nholds for any suboptimal e and optimal e∗. So the second term in (13) is bounded from above by K C2(ε) nβ(ε) . Now we bound the term inside of the first expectation in (13). In particular, by the same argument as in the proof of Theorem 2:\nK∑ e∗=1 n∑ t=1\n∆e,e∗1{Tt−1(e) ≤ τe,e∗ , Ge,e∗,t} ≤[ ∆e,1\nDKL(w̄(e) ‖ w̄(1)) + K∑ e∗=2 ∆e,e∗ ( 1 DKL(w̄(e) ‖ w̄(e∗)) − 1 DKL(w̄(e) ‖ w̄(e∗ − 1)) )] (1 + ε)(log n+ 3 log(log n))\nholds for any suboptimal e. By Lemma 3, the leading constant is bounded as:\n∆e,1 DKL(w̄(e) ‖ w̄(1)) + K∑ e∗=2 ∆e,e∗ ( 1 DKL(w̄(e) ‖ w̄(e∗)) − 1 DKL(w̄(e) ‖ w̄(e∗ − 1)) ) ≤ ∆e,K(1 + log(1/∆e,K)) DKL(w̄(e) ‖ w̄(K)) .\nFinally, we chain all inequalities and sum over all suboptimal items e."
    }, {
      "heading" : "B. Technical Lemmas",
      "text" : "Lemma 2. Let u ∈ {0, 1}K and v ∈ {0, 1}K be random vectors whose entries are drawn independently from some distributions. Then:\nE [ K∏ k=1 u(k)− K∏ k=1 v(k) ] = K∑ k=1 E [ k−1∏ i=1 u(i) ] E [u(k)− v(k)]  K∏ j=k+1 E [v(j)]  . Proof. First, we prove that:\nK∏ k=1 u(k)− K∏ k=1 v(k) = K∑ k=1 ( k−1∏ i=1 u(i) ) (u(i)− v(i)) ( K∏ i=k+1 E [v(i)] )\nholds for any u ∈ {0, 1}K and v ∈ {0, 1}K . The proof is by induction on K. In particular, note that the claim holds when K = 1. Now suppose that the claim holds for any u ∈ {0, 1}K−1 and v ∈ {0, 1}K−1. Let u ∈ RK and v ∈ RK . Then:\nK∏ k=1 u(k)− K∏ k=1 v(k) = K∏ k=1 u(k)− v(K) K−1∏ k=1 u(k) + v(K) K−1∏ k=1 u(k)− K∏ k=1 v(k)\n= (u(K)− v(K)) K−1∏ k=1 u(k) + v(K) [ K−1∏ k=1 u(k)− K−1∏ k=1 v(k) ]\n= K−1∑ k=1 [( k−1∏ i=1 u(i) ) (u(i)− v(i)) ( K∏ i=k+1 v(i) )] + (u(K)− v(K)) K−1∏ k=1 u(k)\n= K∑ k=1 [( k−1∏ i=1 u(i) ) (u(i)− v(i)) ( K∏ i=k+1 v(i) )] .\nThe third equality is by our induction hypothesis. Finally, note that the entries of u and v are drawn independently. As a result, the expectation of the product decomposes into the product of expectations, and our claim follows.\nLemma 3. Let p1 ≥ . . . ≥ pK > p be K + 1 probabilities and ∆k = pk − p for 1 ≤ k ≤ K. Then:\n∆1 DKL(p ‖ p1) + K∑ k=2 ∆k ( 1 DKL(p ‖ pk) − 1 DKL(p ‖ pk−1) ) ≤ ∆K(1 + log(1/∆K)) DKL(p ‖ pK) .\nProof. First, we note that:\n∆1 DKL(p ‖ p1) + K∑ k=2 ∆k ( 1 DKL(p ‖ pk) − 1 DKL(p ‖ pk−1) ) = K−1∑ k=1 ∆k −∆k+1 DKL(p ‖ pk) + ∆K DKL(p ‖ pK) .\nThe summation over k can be bounded from above by a definite integral:\nK−1∑ k=1 ∆k −∆k+1 DKL(p ‖ pk) = K−1∑ k=1 ∆k −∆k+1 DKL(p ‖ p+ ∆k) ≤ ∫ ∆1 ∆K\n1\nDKL(p ‖ p+ x) dx ≤ ∫ 1 ∆K\n1\nDKL(p ‖ p+ x) dx ,\nwhere the first inequality follows from the fact that 1/DKL(p ‖ p + x) decreases on x ≥ 0 .To the best of our knowledge, the integral of 1/DKL(p ‖ p + x) over x does not have a simple analytic form solution. Therefore, we integrate an upper bound on 1/DKL(p ‖ p+ x) which does. In particular, note that for any x ≥ ∆K :\nDKL(p ‖ p+ x) ≥ DKL(p ‖ p+ ∆K)\n∆K x = DKL(p ‖ pK) ∆K x\nbecause DKL(p ‖ p+ x) is convex, increasing in x ≥ 0, and its minimum is attained at x = 0. Therefore:∫ 1 ∆K 1 DKL(p ‖ p+ x) dx ≤ ∆K DKL(p ‖ pK) ∫ 1 ∆K 1 x dx = ∆K DKL(p ‖ pK) log(1/∆K) .\nFinally, we chain all inequalities and get the final result."
    } ],
    "references" : [ {
      "title" : "Improving Web search ranking by incorporating user behavior information",
      "author" : [ "Agichtein", "Eugene", "Brill", "Eric", "Dumais", "Susan" ],
      "venue" : "In Proceedings of the 29 annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "Agichtein et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Agichtein et al\\.",
      "year" : 2006
    }, {
      "title" : "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space",
      "author" : [ "R. Agrawal", "D. Teneketzis", "V. Anantharam" ],
      "venue" : "IEEE Transaction on Automatic Control,",
      "citeRegEx" : "Agrawal et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 1989
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "An adaptive algorithm for finite stochastic partial monitoring",
      "author" : [ "G. Bartók", "N. Zolghadr", "Szepesvári", "Cs" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bartók et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bartók et al\\.",
      "year" : 2012
    }, {
      "title" : "Modeling contextual factors of click rates",
      "author" : [ "H. Becker", "C. Meek", "D.M. Chickering" ],
      "venue" : "In Proceedings of the 22 AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Becker et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Becker et al\\.",
      "year" : 2007
    }, {
      "title" : "Concentration inequalities: A nonasymptotic theory of independence",
      "author" : [ "S. Boucheron", "G. Lugosi", "P. Massart" ],
      "venue" : null,
      "citeRegEx" : "Boucheron et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Boucheron et al\\.",
      "year" : 2012
    }, {
      "title" : "A dynamic Bayesian network click model for Web search ranking",
      "author" : [ "O. Chapelle", "Y. Zhang" ],
      "venue" : "In Proceedings of the 18 World Wide Web Conference,",
      "citeRegEx" : "Chapelle and Zhang,? \\Q2009\\E",
      "shortCiteRegEx" : "Chapelle and Zhang",
      "year" : 2009
    }, {
      "title" : "Combinatorial multi-armed bandit and its extension to probabilistically triggered arms",
      "author" : [ "Chen", "Wei", "Wang", "Yajun", "Yuan", "Yang" ],
      "venue" : "CoRR, abs/1407.8339,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "An experimental comparison of click position-bias models",
      "author" : [ "N. Craswell", "O. Zoeter", "M. Taylor", "B. Ramsey" ],
      "venue" : "In Proceedings of the First International Conference on Web search and Web Data Mining,",
      "citeRegEx" : "Craswell et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Craswell et al\\.",
      "year" : 2008
    }, {
      "title" : "Parametric bandits: The generalized linear case",
      "author" : [ "S. Filippi", "O. Cappé", "A. Garivier", "Szepesvári", "Cs" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Filippi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Filippi et al\\.",
      "year" : 2010
    }, {
      "title" : "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations",
      "author" : [ "Gai", "Yi", "Krishnamachari", "Bhaskar", "Jain", "Rahul" ],
      "venue" : "IEEE/ACM Transactions on Networking,",
      "citeRegEx" : "Gai et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gai et al\\.",
      "year" : 2012
    }, {
      "title" : "The KL-UCB algorithm for bounded stochastic bandits and beyond",
      "author" : [ "Garivier", "Aurelien", "Cappe", "Olivier" ],
      "venue" : "In Proceeding of the 24th Annual Conference on Learning Theory, pp",
      "citeRegEx" : "Garivier et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Garivier et al\\.",
      "year" : 2011
    }, {
      "title" : "Click chain model in Web search",
      "author" : [ "F. Guo", "C. Liu", "A. Kannan", "T. Minka", "M. Taylor", "Y.M. Wang", "C. Faloutsos" ],
      "venue" : "In Proceedings of 18 World Wide Web Conference,",
      "citeRegEx" : "Guo et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient multiple-click models in Web search",
      "author" : [ "F. Guo", "C. Liu", "Y.M. Wang" ],
      "venue" : "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,",
      "citeRegEx" : "Guo et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2009
    }, {
      "title" : "Matroid bandits: Fast combinatorial optimization with learning",
      "author" : [ "Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Eydgahi", "Hoda", "Eriksson", "Brian" ],
      "venue" : "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Kveton et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kveton et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to act greedily: Polymatroid semibandits",
      "author" : [ "Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Valko", "Michal" ],
      "venue" : "CoRR, abs/1405.7752,",
      "citeRegEx" : "Kveton et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kveton et al\\.",
      "year" : 2014
    }, {
      "title" : "Tight regret bounds for stochastic combinatorial semi-bandits",
      "author" : [ "Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Szepesvari", "Csaba" ],
      "venue" : "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Kveton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kveton et al\\.",
      "year" : 2015
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "Robbins", "Herbert" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 1985
    }, {
      "title" : "Sequential learning for multi-channel wireless network monitoring with channel switching costs",
      "author" : [ "T. Le", "R. Zheng", "Szepesvári", "Cs" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Le et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2014
    }, {
      "title" : "Combinatorial partial monitoring game with linear feedback and its applications",
      "author" : [ "Lin", "Tian", "Abrahao", "Bruno", "Kleinberg", "Robert", "Lui", "John", "Chen", "Wei" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "From bandits to experts: On the value of side-observations",
      "author" : [ "Mannor", "Shie", "Shamir", "Ohad" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mannor et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mannor et al\\.",
      "year" : 2011
    }, {
      "title" : "Query chains: learning to rank from implicit feedback",
      "author" : [ "Radlinski", "Filip", "Joachims", "Thorsten" ],
      "venue" : "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,",
      "citeRegEx" : "Radlinski et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Radlinski et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning diverse rankings with multi-armed bandits",
      "author" : [ "Radlinski", "Filip", "Kleinberg", "Robert", "Joachims", "Thorsten" ],
      "venue" : "In Proceedings of the Twenty-Fifth International Conference on Machine Learning,",
      "citeRegEx" : "Radlinski et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Radlinski et al\\.",
      "year" : 2008
    }, {
      "title" : "Predicting clicks: estimating the clickthrough rate for new ads",
      "author" : [ "M. Richardson", "E. Dominowska", "R. Ragno" ],
      "venue" : "In Proceedings of the 16 International World Wide Web Conference,",
      "citeRegEx" : "Richardson et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2007
    }, {
      "title" : "Ranked bandits in metric spaces: learning diverse rankings over large document collections",
      "author" : [ "Slivkins", "Aleksandrs", "Radlinski", "Filip", "Gollapudi", "Sreenivas" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Slivkins et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Slivkins et al\\.",
      "year" : 2013
    }, {
      "title" : "An online algorithm for maximizing submodular functions",
      "author" : [ "Streeter", "Matthew", "Golovin", "Daniel" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Streeter et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Streeter et al\\.",
      "year" : 2009
    }, {
      "title" : "2014a), the above term is bounded by 12",
      "author" : [ "Kveton" ],
      "venue" : null,
      "citeRegEx" : "Kveton,? \\Q2014\\E",
      "shortCiteRegEx" : "Kveton",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Introduction The cascade model (Craswell et al., 2008) was originally proposed in the context of web search and is one of the most popular models of user interaction with content.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "Although simple, the model was found effective in explaining the so-called position bias in the statistics of search engine click logs (Craswell et al., 2008).",
      "startOffset" : 135,
      "endOffset" : 158
    }, {
      "referenceID" : 22,
      "context" : "Ranked bandits (Radlinski et al., 2008; Slivkins et al., 2013) are a popular approach in learning to rank and they are closely related to our paper.",
      "startOffset" : 15,
      "endOffset" : 62
    }, {
      "referenceID" : 24,
      "context" : "Ranked bandits (Radlinski et al., 2008; Slivkins et al., 2013) are a popular approach in learning to rank and they are closely related to our paper.",
      "startOffset" : 15,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "This class of problems can be solved both computationally and sample efficiently (Kveton et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "Several other papers (Mannor & Shamir, 2011; Chen et al., 2014) considered an opposite problem to ours, where the learning agent observes the weights of items that are similar to the chosen items.",
      "startOffset" : 21,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "The former is motivated by CombUCB1 (Gai et al., 2012; Kveton et al., 2015), a computationally and sample efficient algorithm for stochastic combinatorial semi-bandits.",
      "startOffset" : 36,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "The former is motivated by CombUCB1 (Gai et al., 2012; Kveton et al., 2015), a computationally and sample efficient algorithm for stochastic combinatorial semi-bandits.",
      "startOffset" : 36,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "Filippi et al. (2010) study a generalized linear bandit with bandit feedback.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "Chen et al. (2013) study a stochastic combinatorial semi-bandit where the reward function is a known monotone function of a linear function in unknown parameters.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "Chen et al. (2013) study a stochastic combinatorial semi-bandit where the reward function is a known monotone function of a linear function in unknown parameters. Le et al. (2014) consider a network-optimization problem, where the payoff is a nonlinear function of the observations.",
      "startOffset" : 0,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : "Bartók et al. (2012) considers finite partial monitoring problems.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite.",
      "startOffset" : 0,
      "endOffset" : 446
    }, {
      "referenceID" : 1,
      "context" : "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bartók et al. (2012), this algorithm would suffer from computational inefficiencies in our setting.",
      "startOffset" : 0,
      "endOffset" : 582
    }, {
      "referenceID" : 1,
      "context" : "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bartók et al. (2012), this algorithm would suffer from computational inefficiencies in our setting. Lin et al. (2014) recently studied partial monitoring in combinatorial bandits.",
      "startOffset" : 0,
      "endOffset" : 679
    }, {
      "referenceID" : 1,
      "context" : "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bartók et al. (2012), this algorithm would suffer from computational inefficiencies in our setting. Lin et al. (2014) recently studied partial monitoring in combinatorial bandits. The setting of this paper is different from ours. In particular, Lin et al. (2014) assume that the observation is a linear transformation of the weights of the items.",
      "startOffset" : 0,
      "endOffset" : 824
    }, {
      "referenceID" : 1,
      "context" : "Agrawal et al. (1989) consider a partial monitoring problem with nonlinear rewards. In their model, in each step a state is drawn from a distribution that depends on the action and an unknown parameter. The form of these dependencies is known. The state information is received as feedback and also determines the reward. The reward is a function of the feedback and action. Again, the form of this function is known. While Agrawal et al. (1989) prove distribution dependent logarithmic regret bounds, they assume the parameter set is finite. As in the case of Bartók et al. (2012), this algorithm would suffer from computational inefficiencies in our setting. Lin et al. (2014) recently studied partial monitoring in combinatorial bandits. The setting of this paper is different from ours. In particular, Lin et al. (2014) assume that the observation is a linear transformation of the weights of the items. This transformation is fixed, known, and indexed only by the action. In our work, the transformation depends on the weights of the items. Several other papers (Mannor & Shamir, 2011; Chen et al., 2014) considered an opposite problem to ours, where the learning agent observes the weights of items that are similar to the chosen items. Chen et al. (2014) studied this problem in the context of stochastic combinatorial semi-bandits.",
      "startOffset" : 0,
      "endOffset" : 1262
    }, {
      "referenceID" : 0,
      "context" : "Background Ranking functions in web search are typically learned by training a model of user interaction with content from click data (Agichtein et al., 2006; Radlinski & Joachims, 2005).",
      "startOffset" : 134,
      "endOffset" : 186
    }, {
      "referenceID" : 4,
      "context" : "they explain the clicks and many models have been proposed (Becker et al., 2007; Craswell et al., 2008; Richardson et al., 2007).",
      "startOffset" : 59,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "they explain the clicks and many models have been proposed (Becker et al., 2007; Craswell et al., 2008; Richardson et al., 2007).",
      "startOffset" : 59,
      "endOffset" : 128
    }, {
      "referenceID" : 23,
      "context" : "they explain the clicks and many models have been proposed (Becker et al., 2007; Craswell et al., 2008; Richardson et al., 2007).",
      "startOffset" : 59,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "The cascade model (Craswell et al., 2008) is one of the most popular models of user interaction with content (Chapelle & Zhang, 2009).",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "The first algorithm is motivated by UCB1 (Auer et al., 2002) and we call it CascadeUCB1.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "However, we cannot analyze our problem straightforwardly as a matroid bandit (Kveton et al., 2014a), because our reward function is nonlinear and the weights of recommended items are partially observed. Our analysis is also very different from that of Radlinski et al. (2008). To obtain tight regret bounds, we need modify the analysis of our base algorithms, UCB1 and KL-UCB, instead of treating them just like black boxes.",
      "startOffset" : 78,
      "endOffset" : 276
    }, {
      "referenceID" : 14,
      "context" : "Fourth, we apply the peeling argument of Kveton et al. (2014a) and eliminate an extra factor of K in our upper bound.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "Conclusions In this work, we propose a learning variant of the cascade model (Craswell et al., 2008), a popular model of user interaction with content.",
      "startOffset" : 77,
      "endOffset" : 100
    } ],
    "year" : 2017,
    "abstractText" : "The cascade model is a well-established model of user interaction with content. In this work, we propose cascading bandits, a learning variant of the model where the objective is to learn K most attractive items out of L ground items. We cast the problem as a stochastic combinatorial bandit with a non-linear reward function and partially observed weights of items. Both of these are challenging in the context of combinatorial bandits. We propose two computationally-efficient algorithms for our problem, CascadeUCB1 and CascadeKL-UCB, and prove gap-dependent upper bounds on their regret. We also derive a lower bound for cascading bandits and show that it matches the upper bound of CascadeKL-UCB up to a logarithmic factor. Finally, we evaluate our algorithms on synthetic problems. Our experiments demonstrate that the algorithms perform well and robustly even when our modeling assumptions are violated.",
    "creator" : "TeX"
  }
}