{
  "name" : "1701.03185.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "NEURAL CONVERSATION MODELS",
    "authors" : [ "Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil" ],
    "emails" : [ "overmind@google.com", "sgouws@google.com", "dennybritz@google.com", "agoldie@google.com", "bps@google.com", "raykurzweil@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models – purely data-driven systems trained end-to-end on dialogue corpora – have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds selfattention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A backoff strategy produces better responses overall, in the full spectrum of lengths."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Building computer systems capable of general-purpose conversation is a challenging problem. However, it is a necessary step toward building intelligent agents that can interact with humans via natural language, and for eventually passing the Turing test. The sequence-to-sequence (seq2seq) model has proven very popular as a purely data-driven approach in domains that can be cast as learning to map to and from variable-length sequences, with state-of-the art results in many domains, including machine translation (Cho et al., 2014; Sutskever et al., 2014; Wu et al., 2016). Neural conversation models are the latest development in the domain of conversation modeling, with the promise of training computers to converse in an end-to-end fashion (Vinyals & Le, 2015; Shang et al., 2015; Sordoni et al., 2015). Despite promising results, there are still many challenges with this approach. In particular, these models produce short, generic responses that lack diversity (Sordoni et al., 2015; Li et al., 2015). Even when longer responses are explicitly encouraged (e.g. via length normalization), they tend to be incoherent (“The sun is in the center of the sun.”), redundant (“i like cake and cake”), or contradictory (“I don’t own a gun, but I do own a gun.”).\nIn this paper, we provide two methods to address these issues with minimal modifications to the standard seq2seq model. First, we present a glimpse model that only trains on fixed-length segments of the target-side at a time, allowing us to scale up training to larger data sets. Second, we introduce a segment-based stochastic decoding technique which injects diversity earlier in the generated responses. Together, we find that these two methods lead to both longer responses and higher ratings, compared to a baseline seq2seq model with explicit length and diversity-promoting heuristics integrated into the generation procedure (see Table 1 for examples generated using our model).\n∗Both authors contributed equally to this work. †Work done as a member of the Google Brain Residency program (g.co/brainresidency).\nar X\niv :1\n70 1.\n03 18\n5v 1\n[ cs\n.C L\n] 1\n1 Ja\nn 20\n17\nIn Section 2, we present a high-level overview of these two techniques. We then discuss each technique in more detail in Sections 3 and 4. Finally, we report small and large-scale experimental evaluations of the proposed techniques in Section 5."
    }, {
      "heading" : "2 OVERVIEW AND MOTIVATION",
      "text" : "A major difference between translation and responding to conversations is that, in the former, the high-level semantic content to generate in the target sequence is completely given by the source sequence. I.e., given the source x, there is low conditional entropy in the target distribution p(y|x). In the seq2seq approach, the decoder network therefore only has to keep track of where it is in the output, and the content to generate can be transformed from the relevant parts in the source via the attention mechanism (Bahdanau et al., 2014). In contrast, in conversation response generation, the prompt turn may be short and general (e.g., “what do you have planned tonight”), while an appropriate response may be long and informative.\nThe standard seq2seq model struggles with generating long responses, since the decoder has to keep track of everything output so far in its fixed-length hidden state vector, which leads to incoherent or even contradictory outputs. To combat this, we propose to integrate target-side attention into the decoder network, so it can keep track of what has been output so far. This frees up capacity in the hidden state for modeling the higher-level semantics required during the generation of coherent longer responses. We were able to achieve small perplexity gains using this idea on the small OpenSubtitles 2009 data set (Tiedemann, 2009). However, we found it to be too memory-intensive when scaling up to larger data sets.\nAs a trade-off, we propose a technique (called the ‘glimpse model’) which interpolates between source-side-only attention on the encoder, and source and target-side attention on the encoder and decoder, respectively. Our solution simply trains the decoder on fixed-length glimpses from the target side, while having both the source sequence and the part of the target sequence before the glimpse on the encoder, thereby sharing the attention mechanism on the encoder. This can be implemented as a simple data-preprocessing technique with an unmodified standard seq2seq implementation, and allows us to scale training to very large data sets without running into any memory issues. See Figure 1 for a graphical overview, where we illustrate this idea with a glimpse-model of length 3.\nGiven such a trained model, the next challenge is how to generate long, coherent, and diverse responses with the model. As observed in the previous section and in other work, standard maximum a posteriori (MAP) decoding using beam search often yields short, uninformative, and high-frequency responses. One approach to produce longer outputs is to employ length-promoting heuristics (such as length-normalization (Wu et al., 2016)) during decoding. We find this increases the length of the outputs, however often at the expense of coherence. Another approach to explicitly create variation in the generated responses is to rerank the N -best MAP-decoded list of responses from the model using diversity-promoting heuristics (Li et al., 2015). We find this works for shorter responses, but not for long responses, primarily for two reasons: First, the method relies on the MAP-decoding to produce the N -best list, and as mentioned above, MAP-decoding prefers short, generic responses. Second, it is too late to delay reranking in the beam search until the whole sequence has been generated, since beam-search decoding tends to yield beams with low diversity per given prompt, even when the number of beams is high. Instead, our solution is to break up the reranking over shorter segments, and to rerank segment-by-segment, thereby injecting diversity earlier during the decoding process, where it has the most impact on the resulting diversity of the generated beams.\nTo further improve variation in the generated responses, we replace the deterministic MAP-decoding of the beam search procedure with sampling. If a model successfully captures the distribution of responses given targets, one can expect simple greedy sampling to produce reasonable responses. However, due to model underfitting, the learned distributions are often not sharp enough, causing step-by-step sampling to accumulate errors along the way, manifesting as incoherent outputs. We find that integrating sampling into the beam-search procedure yields responses that are more coherent and with more variation overall.\nIn summary, the contributions of this work are the following:\n1. We propose to integrate target-side attention in neural conversation models, and provide a practical approach, referred to as the glimpse model, which scales well and is easy to implement on top of the standard sequence-to-sequence model.\n2. We introduce a stochastic beam-search procedure with segment-by-segment reranking which improves the diversity of the generated responses.\n3. We present large-scale experiments with human evaluations showing the proposed techniques improve over strong baselines.\n4. We release our collection of context-free conversation prompts used in our evaluations as a benchmark for future open-domain conversation response research."
    }, {
      "heading" : "3 SEQ2SEQ MODEL WITH ATTENTION ON TARGET",
      "text" : "We discuss conversation response generation in the sequence-to-sequence problem setting. In this setting, there is a source sequence X = (x1, x2, ..., xM ), and a target sequence Y = (y0, y1, y2, ..., yN ). We assume y0 is always the start-of-sequence token and yN is the end-ofsequence token. In a typical sequence-to-sequence model, the encoder gets its input from the source sequence X and the decoder models the conditional language model P (Y|X) of the target sequence Y, given X.\nSeq2seq models with attention (Bahdanau et al., 2014) parameterize the per-symbol conditional probability as:\nP (yi|Y[0:i−1];X) = P (yi|y0, y1, ..., yi−1;x1, x2, x3, ..., xM ) = DecoderRNN (hi−1,EncoderRNN(X),Attention(hi−1,X)) ,\n(1)\nfor 1 ≤ i ≤ N , where EncoderRNN() and DecoderRNN() are recurrent neural networks that map the sequence of input and output symbols into fixed-length vectors, and Attention() is a function that yields a fixed-size vector summary of the input symbols X (the ‘focus’) most relevant to predicting yi, given the previous recurrent state of the network hi−1 (the ‘context’). The full conditional probability follows from the product rule, as:\nP (Y|X) = P (Y|x1, x2, x3, ..., xM )\n= N∏ i=1 P (yi|y0, y1, y2, ..., yi−1;x1, x2, x3, ..., xM ) (2)\nWe propose to implement target-side attention by augmenting the attention mechanism to include the part of the target sequence already generated, i.e. we include Y[0:i−2] in the arguments to the attention function: Attention(yi−1,Y[0:i−2],X). We implemented this in TensorFlow (Abadi et al., 2015) using 3 LSTM layers on both the encoder and the decoder, with 1024 units per layer. We experimented on the OpenSubtitles 2009 data set, and obtained a small perplexity gain from the target-side attention: 24.6 without versus 24.2 with. However, OpenSubtitles is a small data set, and the majority of its response sequences are shorter than 10 tokens. This may prevent us from seeing bigger gains, since our method is designed to help with longer outputs. In order to train on the much larger Reddit data set, we implemented this method on top of the GNMT model (Wu et al., 2016). Unfortunately, we met with frequent out-of-memory issues, as the 8-layer GNMT model is already very memory-intensive, and adding target-side attention made it even more so. Ideally, we would like to retain the model’s capacity in order to train a rich response model, and therefore a more efficient approach is necessary.\nTo this end, we propose the target-glimpse model which has a fixed-length decoder. The targetglimpse model is implemented as a standard sequence-to-sequence model with attention, where the decoder has a fixed length K. During training, we split the target sequence into non-overlapping, contiguous segments (glimpses) with fixed length K, starting from the beginning. We then train on each of these glimpses, one at a time on the decoder, while putting all target-side symbols before the glimpse on the encoder. For example, if a sequence Y is split into two glimpses Y1 and Y2, each with length K (Y2 may be shorter than K), then we will train the model with two examples, (X→ Y1), and (X,Y1 → Y2). Each time the concatenated sequence on the left of the arrow is put on the encoder and the sequence on the right is put on the decoder. Figure 1(b) illustrates the training of (X,Y1 → Y2) whenK = 3. In our implementation, we always put the source-side endof-sequence token at the end of the whole encoder sequence, and we split the glimpses according to the decoder time steps. For example, if the sequence Y is y0, y1, y2, ..., y10, and K = 3, the first example will have y0, y1, y2 on the input layer of the decoder, and y1, y2, y3 on the output layer of the decoder. The second example has y3, y4, y5 as input of the decoder and y4, y5, y6 as the output of the decoder, and so on. In our experiments, we use K = 10.\nWhile decoding each glimpse, the decoder therefore attends to both the source sequence and the part of the target sequence that precedes the glimpse, thereby benefiting from the GNMT encoder’s bidirectional RNN. Through generalization, the decoder should learn to decode a glimpse of length K in any arbitrary position of the target sequence (which we will exploit in our decoding technique discussed in Section 4). One drawback of this model, however, is that the context inputs to the attention mechanism only include the words that have been generated so far in this glimpse, rather than the words from the full target side. The workaround that we use is to simply connect the last hidden state of the GNMT-encoder to the initial hidden state of the decoder1, thereby giving the decoder access to all previous symbols regardless of the starting position of the glimpse.\n1This is the default in standard seq2seq models, but not in the GNMT model."
    }, {
      "heading" : "4 STOCHASTIC DECODING WITH SEGMENT-BY-SEGMENT RERANKING",
      "text" : "We now turn our attention from training to inference (decoding). Our strategy is to perform reranking with a normalized score at the segment level, where we generate the candidate segments using a trained glimpse-model and using a stochastic beam search procedure, which we discuss next. The full decoding algorithm proceeds segment by segment.\nThe standard beam search algorithm generates symbols step-by-step by keeping a set of the B highest-scoring beams generated so far at each step2. The algorithm adds all possible single-token extensions to every existing beam, and then selects the top B beams. In our stochastic beam search algorithm, we replace this deterministic top-B selection by a stochastic sampling operation in order to encourage variation. Further, to discourage a single beam from dominating the search and decreasing the final response diversity, we perform a two-step sampling procedure: 1) For each singletoken extension of an individual beam we don’t enumerate all possibilities, but instead sample a fixed number of D candidate tokens to be added to the beam. This yields a total of B ×D beams, each with one additional symbol. 2) We then compute the accumulated conditional log-probabilities for each beam (normalized across all B ×D beams), and treat these as the logits for sub-sampling B beams for the next step. We repeat this procedure until we reach the desired segment-length H , or until a segment ends with the end-of-sequence token.\nFor a given source sequence, we can use this stochastic beam search algorithm to generate B candidate H-length segments as the beginning of the target sequence. We then perform a reranking step (described below), and keep one of these. The concatenation of the source and the first target segment is then used as the input for generating the next B candidate segments. The algorithm continues until the segment selected ends with an end-of-sequence token.\nThis algorithm behaves similarly to standard beam search when the categorical distribution used during the process is sharp (‘peaked’), since the samples are likely to be the top categories (words) . However, when the distribution is smooth, many of the choices are likely. In conversation response generation we are dealing with a conditional probability model with high entropy, so this is what often happens in practice.\nFor the reranking, we normalize the scores using random prompts. In particular, suppose Yk = y1, ..., yk−1 is a candidate segment, and (X,Y1:k−1) is the input to the stochastic beam search. The normalized score is then computed as follows:\nS (Yk|X,Y1:k−1) = P (Yk|X,Y1:k−1)∑\nX′∈Φ P (Yk|X′,Y1:k−1) (3)\nIn this equation, the set Φ is a collection of randomly sampled source sequences (prompts). In our experiments, we randomly select Q prompts from the context-free evaluation set (introduced in the Experiments section).\nIt is worth noting that when Φ is an unbiased sample from P (X), the summation in the denominator is a Monte-Carlo approximation of P (Yk|Y1:k−1). In the case of reranking whole target sequences Y, this becomes the marginal P (Y), which corresponds to the same diversity-promoting objective used in Li et al. (2015). However, we found that our approximation works better in terms of N-choose-1 accuracy (see Section 5.2), which suggests that its value may be closer to the true conditional probability.\nIn our experiments, we set number of random prompts Q to 15, segment length H to 10, number of beams B to 2, and samples per beam D to 10. We select a small value for B, since we find that larger values makes the algorithm behave more like standard beam search."
    }, {
      "heading" : "5 EXPERIMENTAL RESULTS",
      "text" : "In this section we present experimental results for evaluating the target-glimpse model and the stochastic decoding method that we presented. We train the model using the Google neural machine translation model (GNMT, Wu et al. (2016)), on a data set that combines multiple sources mined from the Web:\n2Beams are also called ‘hypotheses’, and B is referred to as the ‘beam width’.\n1. The full Reddit data3 that contains 1.7 billion messages (221 million conversations).\n2. The 2009 Open Subtitles data (0.5 million conversations, Tiedemann (2009)).\n3. The Stack Exchange data (0.8 million conversations).\n4. Dialogue-like texts that we recognized and extracted from the web (17 million conversations).\nFor all these data sets, we extract pairs of messages where one can be considered as a response to the other. For example, in the Reddit data set, the messages belonging to the same post are organized as a tree. A child node is a message that replies to its parent. This may not necessarily be true as people may be replying to other messages that are also visually close. However, for our current single-turn experiments, we treat these as a single exchange.\nIn this setting, the GNMT model trained on prompt-to-response pairs works surprisingly well without modification when generating short responses with beam search. Similar to previous work on neural conversation models, we find that the generated responses are almost always grammatical, and sometimes even interesting. They are also usually on topic. In addition, we found that even greedy sampling from the 8-layer GNMT model produces grammatical responses most of the time, although these responses are more likely to be semantically-broken than responses generated using standard beam search. We would like to leverage the benefits of greedy sampling, because the induced variation generates more surprises and may potentially help improve user-engagement, and we found that our proposed segment-based beam sampling procedure accomplishes this to some extent."
    }, {
      "heading" : "5.1 EVALUATION METRIC",
      "text" : "It is difficult to come up with an objective evaluation metric for conversation response generation that can be computed automatically. The conditional distribution P (Y|X) is supposed to have high entropy in order to be interesting (many possible valid responses to a given prompt). Therefore BLEU scores used in translation are not a good fit. Other than looking at the evaluation set perplexity, we use two metrics, the N-choose-1 accuracy and 5-scale side-by-side human evaluation. In the N-choose-K metric, given a prompt, we ask the model to ranke N candidate responses, where one is the ground truth and the other N − 1 are random responses from the same data set. We then calculate the N-choose-K accuracy as the proportion of trials where the true response is in the top K. The prompts used for evaluation are selected randomly from the same data set. This metric isn’t necessarily correlated well with the true response quality, but provides a useful first diagnostic for faster experimental iteration. It takes about a day to train a small model on a single GPU that reaches 2-choose-1 accuracies of around 70% or 80%, but it is much harder to make progress on the 50-choose-1 accuracy. As a reference, human performance on the 10-choose-1 task is around 45% accuracy.\nIn the 5-scale human evaluation, we use a collection of 200 context-free prompts4. These prompts are collected from the following sources, and filtered to prompts that are context-free (i.e. do not depend on previous turns in the conversation), general enough, and by eliminating near duplicates:\n1. The questions and statements that users asked an internal testing bot.\n2. The Fisher corpus (David et al., 2004).\n3. User inputs to the Jabberwacky chatbot5.\nThese can be either generic or specific. Some example prompts from this collection are shown in Table 1. These prompts are open-domain (not about any specific topic), and include a wide range of topics. Many require some creativity for answering, such as “Tell me a story about a bear.” Our evaluation set is therefore not from the same distribution as our training set. However, since our goal is to produce good general conversation responses, we found it to be a good general purpose evaluation set.\n3Download links are at https://redd.it/3bxlg7 4This list will be released to the community. 5http://www.jabberwacky.com/\nThe evaluation itself is done with well-trained human evaluaters. A 5-scale rating is produced for each prompt-response pair: Excellent, Good, Acceptable, Mediocre, and Bad. For example, the instructions for rating Excellent is “On topic, interesting, shows understanding, moves the conversation forward. It answers the question.” The instruction for Acceptable is “On topic but with flaws that make it seem like it didnt come from a human. It implies an answer.” The instruction for Bad is “A completely off-topic statement or question, nonsensical, or grammatically broken. It does not provide an answer.”\nIn our experiments, we perform the evaluations side-by-side, each time using responses generated from two methods. Every prompt-response pair is rated by three raters. We rate 200 pairs in total for every method, garnering 600 ratings overall. After the evaluation, we report aggregated results from each method individually."
    }, {
      "heading" : "5.2 MOTIVATING EXPERIMENTS",
      "text" : "To see whether generating long responses is indeed a challenging problem, we trained the plain seq2seq with the GNMT model where the encoder holds the source sequence and the decoder holds the target sequence. We experimented with the standard beam search and the beam search with length normalization α = 0.8 similar to Wu et al. (2016). With this length normalization the generated responses are indeed longer. However, they are more often semantically incoherent. It produces “I have no idea what you are talking about.” more often, similarly observed in Li et al. (2016). The human evaluation results are summarized in Figure 3(a). Methods that generate longer responses have more Bad and less Excellent / Good ratings.\nWe also performed the N-choose-1 evaluation on the baseline model using different normalization schemes. The results are shown in Table 2(a). No Normalization means that we use P (Y|X) for scoring, Normalize by Marginal uses P (Y|X)/P (Y), as suggested in Li et al. (2015), and Normalize by Random Prompts is our scoring objective described in Section 4. The significant boost when using both normalization schemes indicates that the conditional log probability predicted by the model may be biased towards the language model probability of P (Y). After adding the normalization, the score may be closer to the true conditional log probability.\nOverall, this reranking evaluation indicates that our heuristic is preferred to scoring using the marginal. However, it is unfortunately hard to directly make use of this score during beam search decoding (i.e. generation), since the resulting sequences are usually ungrammatical, as also observed by Li et al. (2015). This is the motivation for using a segment-by-segment reranking procedure, as described in Section 4."
    }, {
      "heading" : "5.3 LARGE-SCALE EXPERIMENTS",
      "text" : "For our large-scale experiments, we train our target-glimpse model on the full combined data set. Figure 2(b) shows the training progress curve. In this figure, we also include the curve for K = 1, that is, the glimpse model with decoder-length 1. It is clear enough that this model progresses much slower, so we terminated it early. However, it is surprising that the glimpse model with K = 10 progresses faster than the baseline model with only source-side attention, because the model is trained on examples with decoder-length fixed at 10, while the average response length is 38 in our data set. This means it takes on average 3.8x training steps for the glimpse model to train on the same number of raw training-pairs as the baseline model. Despite this, the faster progress indicates that target-side attention indeed helps the model generalize better.\nThe human evaluation results shown in Figure 3 compare our proposed method with the baseline seq2seq model. For this, we trained a length-10 target-glimpse model and decoded with stochastic beam-search using segment-by-segment reranking. In our experiments, we were unable to generate better long, coherent responses using the whole-sequence level reranking method from Li et al. (2015) compared to using standard beam search with length-normalization6. We therefore choose the latter as our baseline, because it is the only method which generates responses that are long enough that we can compare to.\n6This is because the method reranks the responses in the N -best list resulting from the beam search, which tend to be short with not much variation to begin with.\nFigure 3 shows that our proposed method generates more long responses overall. One third of all responses are longer than 100 characters, while the baseline model produces only a negligible fraction. Although we do not employ any length-promoting objectives in our method, length-normalization is used for the baseline. For responses generated by our method, the proportion of Acceptable and Excellent responses remains constant or even increases as the responses grow longer. Conversely, human ratings decline sharply with length for the baseline model.\nHowever, shorter responses have a much smaller search space, and we find that standard beam search tends to generate better (“safer”) short responses. To maximize cumulative response quality, we therefore implemented a back-off strategy that combines the strengths of the two methods. We choose to fallback to the baseline model without length normalization when the latter produces a response shorter than 40 characters, otherwise we use the response from our method. This corresponds to the white histogram in Figure 3(a). Compared to the other methods in the figure, the combined strategy results in more ratings of Excellent, Good, Acceptable, and Mediocre, and fewer Bad ratings. With this strategy, among the responses generated for the same 200 prompts, 133 were from the standard beam search and 67 were from our model. Out of the 67 long responses, two thirds were longer than 60 characters and half were longer than 75 characters. To compare the combined model’s performance with the baseline, we generated responses from both models using the same 200 prompts. For 20 of the response pairs, human raters had no preference, but for the remaining 180, human raters preferred the combined model’s response in 103 cases and the baseline’s in only 77, indicating a significant win."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "The research of building end-to-end systems that can engage in general-purpose conversation is still in its infancy. More significant progress is expected to be made with more advanced neural architectures. However, our results reported in this paper show that minimal modeling change and a slightly more advanced decoding technique, combined with training over very large data sets, can still lead to noticeable improvements in the quality of responses generated using neural conversation models. Overall, we found using fixed-lengths in the decoder to make it easier to train on large data sets, as well as to allow us to improve the diversity and coherence of the generated responses earlier during generation, when it has most impact. While the focus of this work has been on conversation modeling, we expect some of these results to carry over to other sequence-to-sequence settings, such as machine translation or image-captioning."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank Quoc Le, Oriol Vinyals and Jakob Uszkoreit for many helpful discussions, and Scott Benson, Fuchun Peng for collecting the context-free prompt set, and Amin Ahmad for setting up the human evaluation, and Rami Eid, Daniel Cer for collecting training data sets, and Yonghui Wu, Zhifeng Chen, Mike Schuster for help on training the GNMT model."
    }, {
      "heading" : "A THE CONTEXT-FREE PROMPT LIST AND EVALUATION RESULTS",
      "text" : "The following are samples from each rating scale, for both our method (without the fallback strategy) and the baseline method. For each rating we show 5 randomly selected examples. These were selected purely at random, and not cherry-picked, in order to show an unbiased sample of the types of responses the methods generate.\nA.1 OUR METHOD 1. Excellent\n(a) Prompt: what does your handwriting look like? Response: I have a pen on the left hand side.\n(b) Prompt: have you seen Suicide Squad? Response: I saw it in a theater once and it was great. It was very... I don’t know, a little dark. I like the psychological effects and the way it portrays the characters in the movie.\n(c) Prompt: I do not own an Iphone Response: I own an iphone\n(d) Prompt: what are you watching right now? Response: watch the new season of rick and morty\n(e) Prompt: Actually I like the snow too, except for today. It’s all slushy and it’s all over the sidewalks and it’s gross. Response: I’m in the same boat as you. I hate it when it’s snowy and I don’t like to shovel it all day. :( So this is a perfect excuse to not get out of bed and go somewhere else.\n2. Good\n(a) Prompt: I went on a bike ride today. Response: That is awesome.\n(b) Prompt: how was your weekend? what’d you do? Response: I had a good weekend and I had a good time. I just wanted to get some rest.\n(c) Prompt: get anything good for xmas? Response: I got a bunch of stuff to give away, and I don’t know how long it will take to send. I know I have a lot of stuff that I want to give to people and I don’t know if I can give it away to someone else.\n(d) Prompt: What do you want to be? Response: I want to be a psychologist.\n(e) Prompt: Nothing makes me happy. :(. Response: I can’t even imagine how awful this feels to be.\n3. Acceptable\n(a) Prompt: Are you good at math? Response: I have to admit, I am a terrible coder...\n(b) Prompt: I didn’t used to like Indian food, but I think at this point I like just about anything. Response: Indian indian food is so good.\n(c) Prompt: i was in bed all day yesterday Response: how do you feel?\n(d) Prompt: It sounds like you’re a sports fan. Response: I am, I watch a lot of baseball and I don’t want to be a sports fan.\n(e) Prompt: I’ve never learned how to fly Response: You have no idea how to fly a plane. It’s not that hard.\n4. Mediocre\n(a) Prompt: I live alone, and if I eat at home I eat whatever I can put in the microwave that night. A lot of times it’s just that, or I’ll trade with friends; I’ll cook and they’ll cook and then we’ll swap leftovers. Response: I live alone, and if I eat at home, I eat whatever I can put in the microwave that night. I do this all the time. I eat the same thing over and over again.\n(b) Prompt: I don’t eat seafood. I don’t eat anything that used to swim. I know that’s kind of rigid, but I had a bad experience when I was a kid. Just doesn’t do it for me. Response: I don’t eat anything that used to swim. That’s not the point. The point is that you can eat fish that will eat whatever you want. That’s just not the same thing as fish.\n(c) Prompt: I find I like Japanese food a lot. I can make a number of the types of rolls and various types of sushi. Of course I end up doing a lot of things with noodle soup just because it’s relatively cheap and fast. Response: I don’t understand, how would you go about getting a sushi roll with Japanese food?\n(d) Prompt: Let’s talk about cooking. Do you own a steamer? Response: I own a steamer but my user agent is a hunter, so he owns a steamer, but I don’t own one, he uses it a lot but I used to own a steamer.\n(e) Prompt: Have you heard of Turing? Response: I think I have. I like it.\n5. Bad\n(a) Prompt: I’ve had a cold myself this week. Right now I’m trying to figure out how to get rid of it. Response: I have a cold and I don’t have any colds because I have a cold.\n(b) Prompt: How much time would you say you spend attached to a computer? 2 hours? 10 hours? Response: I have a gaming computer that I’d rather not have to spend hours on, so I spend all day playing games on it on a regular basis.\n(c) Prompt: How significant is the potential for nuclear energy? Response: I think there is a very good chance it will be a very strong nuclear weapon, but it wouldn’t make a huge impact. The only thing that has a chance of making it into nuclear power is a nuclear disaster, which will be very very very unlikely if it ever does.\n(d) Prompt: I really like the trees of san francisco Response: I like the new one too\n(e) Prompt: I don’t eat seafood. I don’t eat anything that used to swim. I know that’s kind of rigid, but I had a bad experience when I was a kid. Just doesn’t do it for me. Response: I don’t eat anything that used to swim. That’s not the point. The point is that you can eat fish that will eat whatever you want. That’s just not the same thing as fish.\nA.2 BASELINE METHOD 1. Excellent\n(a) Prompt: What do you think of Asimov’s novels? Response: I read them when I was in high school and I loved them.\n(b) Prompt: Finally we’re getting a winter with some snow. Response: Where is this? I want to go to there.\n(c) Prompt: Are you a bot? Response: I am a bot, and this action was performed automatically.\n(d) Prompt: Are you at work now? Response: Yes, I am.\n(e) Prompt: Actually, we just got a snow storm yesterday. I live in Maryland. Where do you live, on the east coast? Response: I live on the east coast of the US.\n2. Good\n(a) Prompt: Do you read the bible? Response: I don’t read the bible. I read the bible. I read the bible. I read the bible\n(b) Prompt: have you seen Suicide Squad? Response: I have not, but I will check it out.\n(c) Prompt: Are you at work now? Response: Yes, I am.\n(d) Prompt: I run alot. That’s my jam. I run a marathon a year. Response: Do you run on a treadmill or a treadmill?\n(e) Prompt: how was your dentist appt? Response: It was a lot better than I thought it would be.\n3. Acceptable\n(a) Prompt: What do you think of Asimov’s novels? Response: I read them when I was in high school and I loved them.\n(b) Prompt: is chicken healthier than beef? Response: Yes, it is.\n(c) Prompt: It’s a beautiful day. Response: I wish I could upvote this more than once.\n(d) Prompt: Let’s talk about cooking. Do you own a steamer? Response: I have a steamer and a steamer.\n(e) Prompt: what r u doing this summer? Response: I have no idea what I want to do with my life.\n4. Mediocre\n(a) Prompt: What do you think about universal healthcare? Response: I don’t like the idea of universal healthcare. I don’t like the idea of universal healthcare. I think\n(b) Prompt: It will be very cool when we succeed in sending astronauts to Mars. Response: I think it would be a good idea to send astronauts to Mars.\n(c) Prompt: I’ve a software engineer at Google, it’s a good life Response: I have no idea what you are talking about.\n(d) Prompt: I taught in Kansas. I taught French and Spanish at high school and college level. Response: Did you learn it in high school?\n(e) Prompt: What makes you happy? Response: The fact that I don’t have to worry about my\n5. Bad\n(a) Prompt: i need to start eating healthier Response: Your submission has been automatically removed pending manual approval because your account is either too new or does not meet\n(b) Prompt: where did you grow up? Response: where are you from?\n(c) Prompt: i hate being the center of attention Response: I hate it when people do this.\n(d) Prompt: You can’t eat pasta every day. You can’t live on only Italian food. Response: I don’t eat Italian food, but I do eat a lot of Italian food.\n(e) Prompt: You are a program, and I am human. Response: I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "cent Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "The fisher corpus: a resource for the next generations of speech-to-text",
      "author" : [ "Christopher Cieri David", "David Miller", "Kevin Walker" ],
      "venue" : "In in Proceedings 4th International Conference on Language Resources and Evaluation,",
      "citeRegEx" : "David et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "David et al\\.",
      "year" : 2004
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "arXiv preprint arXiv:1510.03055,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep reinforcement learning for dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky" ],
      "venue" : "CoRR, abs/1606.01541,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li" ],
      "venue" : "arXiv preprint arXiv:1503.02364,",
      "citeRegEx" : "Shang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "arXiv preprint arXiv:1506.06714,",
      "citeRegEx" : "Sordoni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces",
      "author" : [ "Jörg Tiedemann" ],
      "venue" : "Recent Advances in Natural Language Processing (vol V),",
      "citeRegEx" : "Tiedemann.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2009
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le" ],
      "venue" : "arXiv preprint arXiv:1506.05869,",
      "citeRegEx" : "Vinyals and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The sequence-to-sequence (seq2seq) model has proven very popular as a purely data-driven approach in domains that can be cast as learning to map to and from variable-length sequences, with state-of-the art results in many domains, including machine translation (Cho et al., 2014; Sutskever et al., 2014; Wu et al., 2016).",
      "startOffset" : 261,
      "endOffset" : 320
    }, {
      "referenceID" : 5,
      "context" : "Neural conversation models are the latest development in the domain of conversation modeling, with the promise of training computers to converse in an end-to-end fashion (Vinyals & Le, 2015; Shang et al., 2015; Sordoni et al., 2015).",
      "startOffset" : 170,
      "endOffset" : 232
    }, {
      "referenceID" : 6,
      "context" : "Neural conversation models are the latest development in the domain of conversation modeling, with the promise of training computers to converse in an end-to-end fashion (Vinyals & Le, 2015; Shang et al., 2015; Sordoni et al., 2015).",
      "startOffset" : 170,
      "endOffset" : 232
    }, {
      "referenceID" : 6,
      "context" : "In particular, these models produce short, generic responses that lack diversity (Sordoni et al., 2015; Li et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "In particular, these models produce short, generic responses that lack diversity (Sordoni et al., 2015; Li et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "In the seq2seq approach, the decoder network therefore only has to keep track of where it is in the output, and the content to generate can be transformed from the relevant parts in the source via the attention mechanism (Bahdanau et al., 2014).",
      "startOffset" : 221,
      "endOffset" : 244
    }, {
      "referenceID" : 8,
      "context" : "We were able to achieve small perplexity gains using this idea on the small OpenSubtitles 2009 data set (Tiedemann, 2009).",
      "startOffset" : 104,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Another approach to explicitly create variation in the generated responses is to rerank the N -best MAP-decoded list of responses from the model using diversity-promoting heuristics (Li et al., 2015).",
      "startOffset" : 182,
      "endOffset" : 199
    }, {
      "referenceID" : 1,
      "context" : "Seq2seq models with attention (Bahdanau et al., 2014) parameterize the per-symbol conditional probability as:",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "In the case of reranking whole target sequences Y, this becomes the marginal P (Y), which corresponds to the same diversity-promoting objective used in Li et al. (2015). However, we found that our approximation works better in terms of N-choose-1 accuracy (see Section 5.",
      "startOffset" : 152,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : "5 million conversations, Tiedemann (2009)).",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "The Fisher corpus (David et al., 2004).",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "” more often, similarly observed in Li et al. (2016). The human evaluation results are summarized in Figure 3(a).",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "” more often, similarly observed in Li et al. (2016). The human evaluation results are summarized in Figure 3(a). Methods that generate longer responses have more Bad and less Excellent / Good ratings. We also performed the N-choose-1 evaluation on the baseline model using different normalization schemes. The results are shown in Table 2(a). No Normalization means that we use P (Y|X) for scoring, Normalize by Marginal uses P (Y|X)/P (Y), as suggested in Li et al. (2015), and Normalize by Random Prompts is our scoring objective described in Section 4.",
      "startOffset" : 36,
      "endOffset" : 475
    }, {
      "referenceID" : 3,
      "context" : "” more often, similarly observed in Li et al. (2016). The human evaluation results are summarized in Figure 3(a). Methods that generate longer responses have more Bad and less Excellent / Good ratings. We also performed the N-choose-1 evaluation on the baseline model using different normalization schemes. The results are shown in Table 2(a). No Normalization means that we use P (Y|X) for scoring, Normalize by Marginal uses P (Y|X)/P (Y), as suggested in Li et al. (2015), and Normalize by Random Prompts is our scoring objective described in Section 4. The significant boost when using both normalization schemes indicates that the conditional log probability predicted by the model may be biased towards the language model probability of P (Y). After adding the normalization, the score may be closer to the true conditional log probability. Overall, this reranking evaluation indicates that our heuristic is preferred to scoring using the marginal. However, it is unfortunately hard to directly make use of this score during beam search decoding (i.e. generation), since the resulting sequences are usually ungrammatical, as also observed by Li et al. (2015). This is the motivation for using a segment-by-segment reranking procedure, as described in Section 4.",
      "startOffset" : 36,
      "endOffset" : 1165
    }, {
      "referenceID" : 3,
      "context" : "In our experiments, we were unable to generate better long, coherent responses using the whole-sequence level reranking method from Li et al. (2015) compared to using standard beam search with length-normalization6.",
      "startOffset" : 132,
      "endOffset" : 149
    } ],
    "year" : 2017,
    "abstractText" : "Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models – purely data-driven systems trained end-to-end on dialogue corpora – have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds selfattention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A backoff strategy produces better responses overall, in the full spectrum of lengths.",
    "creator" : "LaTeX with hyperref package"
  }
}