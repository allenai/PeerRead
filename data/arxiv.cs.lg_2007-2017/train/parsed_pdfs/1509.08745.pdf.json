{
  "name" : "1509.08745.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compression of Deep Neural Networks on the Fly",
    "authors" : [ "Guillaume Soulié", "Vincent Gripon" ],
    "emails" : [ "name.surname@telecom-bretagne.eu" ],
    "sections" : [ {
      "heading" : "I. MOTIVATION",
      "text" : "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval. Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture. Importing CNNs onto embedded platforms [10], the recent trend toward mobile computing, has a wide range of application impacts. It is especially useful when the bandwidth is limited and when files are sensitive and should not be sent to distant servers. However, the size of a CNN is typically large (e.g. more than 200M bytes), which limits the applicability of such models on embedded platforms. For example, it is hard to force users to download an iPhone application that big. The purpose of this paper is to propose new techniques for compressing networks without sacrificing performance.\nIn this article we focus on compressing CNNs for computer vision tasks, although our methodology is not taking any advantage of this particular application field and we expect it to perform similarly on other types of learning tasks. A typical CNN [5, 7, 8] with good performance in visual object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters in order to achieve state-of-the-art results. These parameters are overparameterized [11] and we aim at compressing them. Note that our motivation is mainly reducing storage rather than speeding up the testing time [12].\nA few early works on compressing CNNs have been published. In [12] and [13] the authors use compressing methods\nThis work was founded in part by the European Research Council under the European Union’s Seventh Framework Program ( FP7 / 2007 - 2013 ) / ERC grant agreement number 290901.\nfor speeding up CNN testing time. More recently, some works focus on compressing neural network to reduce storage of the network.\nThese works can generally be put in two different categories: some of them focus on compressing the fully connected layers and others on compressing the convolutionnal layers. In [14] the authors focus on compressing dense connected layers. In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers. They obtain good results by applying Product Quantization. In [17] the authors focus on compressing the fully connected layers of a MultiLayer Perceptron (MLP) using Hashing Trick, a low cost hash function to randomly group connection weights into hash buckets, and set the same value to all the parameters in the same bucket. In [18] the authors propose compressing convolutionnal layers using a Discrete Cosinus Transform applied on the convolutionnal filters, followed by Hashing Trick, as for the fully connected layers.\nFor a typical CNN as the one introduced in [8], about 90% of the storage is taken up by the dense connected layers and more than 90% of the running time is taken by the convolutional layers. This is why our work focus upon how to compress the dense connected layers to reduce storage of neural networks. Instead of using a post-learning method to compress the network, our approach consists in modifying the regularization function used during the learning phase in order to favor quantized weights in some layers – especially the output ones. To achieve this, we use an idea that was originally proposed in [19]. In order to compress furthermore our obtained networks, we also use PQ as described in [14] afterwards. We perform some experiments both on Multi-Layer Perceptrons (MLP) and Convolutionnal Neural Networks (CNN).\nIn this paper, we introduce a novel way to quantize weights in deep learning systems. More precisely : • We introduce a regularization term that forces weights\nto converge to either 0 or 1, before using the product quantization on the learned weights. • We show how this extra term impacts performance depending on the depth of the layer it is used onto. • We experiment our proposed method on celebrated benchmarks and compare with state-of-the-art techniques.\nThe outline of the paper is as follows. In Section II we discuss related work. Section III introduced our methodology\nar X\niv :1\n50 9.\n08 74\n5v 1\n[ cs\n.L G\n] 2\n9 Se\np 20\n15\nfor compressing layers in deep neural networks. In Section IV we run experiments on celebrated databases. In section V we discuss the biological motivation for our work. Section VI is a conclusion."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "As already mentioned in the introduction, a state-of-theart CNN usually involves hundreds of millions of parameters, thus requiring an important storage capability that may be hard to obtain in practice. The bottleneck comes from model storage and testing speed. Several works have been published on speeding up CNN prediction speed. In [20] the authors use tricks of CPUs to speed up the execution of CNN. In particular they focus on aligning memory and SIMD operations to boost matrix operations. In [21] the authors show the convolutional operations can be efficiently carried out in the Fourier domain, leading to a speed-up of 200%. Two very recent works [12, 13] use linear matrix factorization methods for speeding up convolutional layers and obtain a 200% speed-up gain with almost no lost in classification performance. Almost all of the above mentioned works focus on making the prediction speed of CNN faster; little work has been specifically devoted to making CNN models smaller.\nIn [11], the authors demonstrate the redundancies in neural network parameters. Indeed, they show that the weights within one layer can be accurately predicted by a small (5%) subset of the parameters. These results motivate [9] to apply vector quantization methods to compress the redundancy in parameter space. Their work can be viewed as a compression of the parameter prediction results reported in [11]. They obtain similar results to those of [11], in that they are able to compress the parameters about 20 times with little decrease of performance. This result further confirms the interesting empirical findings in [11]. In their paper, they tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, they show that in terms of compressing the most storage demanding dense connected layers, vector quantization method performs much better than existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, they are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN. In [17], for the first time a learn-based method is proposed to compress neural networks. This method, based on Hashing Trick, allows efficient compression rates. In particular, they show that compressing a big neural network may be more efficient than using a smaller one: in their example they are able to divide the loss by two using a eight times larger neural network compressed eight times. The same authors also propose in [18] to compress filters in convolutional layers, arguing that the size of the convolutional layers in state-ofthe-art’s CNN is increasing year after year. Using the fact\nthat learned CNN filters are often smooth, their Discrete Cosinus Transform followed by Hashing Tricks allows them to compress a whole neural network without loosing too much accuracy."
    }, {
      "heading" : "III. METHODOLOGY",
      "text" : "In this section, we consider two methods for compressing the parameters in CNN layers. We first introduce the product quantization method proposed in [14], and then introduce our proposed learn-based method."
    }, {
      "heading" : "A. Product Quantization (PQ)",
      "text" : "For more details on this quantization method, refer to [14]. The key idea is to consider structured vector quantization methods for compressing parameters. More precisely, the idea consists in applying Product Quantization (PQ) [15] to exploit the redundancy of structures in vector space. PQ partitions the vector space into disjoint sub-spaces, and perform quantization in each of them. It performs increasingly better as the redundancy in each subspace grows. Specifically, given the matrix W , we partition it column-wise into several sub-matrices:\nW = [W 1,W 2, ...,W s], (1)\nwhere W i ∈ Rm(̇n/s) assuming n is divisible by s ((̇n/s) is named the segment size). We can then perform a k-means clustering for each sub-matrix W i:\nmin m∑ z=1 k∑ j=1 ‖wiz − cij‖22, (2)\nwhere wzi denotes the z-th row of sub-matrix W i, and cij denotes the j-th row of sub-codebook Ci ∈ Rk(̇n/s). For each sub-vector wzi , we need only store its corresponding cluster index and the associated codebooks. Thus, the reconstructed matrix is:\nŴ = [Ŵ 1, Ŵ 2, ..., Ŵ s], (3)\nwhere\nŵij = c i j , j being a minimizer of min j ‖wiz − cij‖22.\nNote that PQ can be applied to either the x-axis or the yaxis of the matrix. In our experiments, we evaluated both cases and obtained similar performance. We need to store the cluster indexes and codebooks for each sub-vector. In particular, the codebook is not negligible. The compression rate for this method is (32mn)/(32kn+log2(k)ms). With a fixed segment size, increasing k will lead to decreasing the compression rate."
    }, {
      "heading" : "B. Proposed method",
      "text" : "Our proposed method is twofold: first, we use a specific added regularization term in order to attract network weights to binary values, then we coarsely quantize the output layers.\nLet us recall that training a neural network is generally performed thanks to the minimization of a cost function using a derivative of a gradient descent algorithm. In order to attract network weights to binary values, we add a binarization\ncost (regularizer) during the learning phase. This added cost pushes weights to binary values. As a result, solutions of the minimization problem are expected to be binary or almost binary, depending on the scaling parameter of the added cost with respect to the initial one. This idea is not new, although we did not find any work applying it to deep learning in the literature. Our choice for the regularization term has been greatly inspired by [19].\nMore precisely, let us denote by W the weights of the neural network, f(W ) the cost associated with W , hW (X) and y(X) respectively the output and the label for a given input X , we obtain:\nf(W ) = ∑ X ‖hW (X)− y(X)‖+ α ∑ w∈W ‖w‖‖w − 1‖ , (4)\nwhere α is a scaling parameter representing the importance of the binarization cost with respect to the initial cost.\nFinding a good value for α may be tricky, as a too small value results in a failure of the binarization process and a too large value results in the creation of local minima that will prevent the network from successfully training. To facilitate this selection of α, we use a barrier method [19] that consists in starting with small values of α and incrementing it regularly to help the quantization process.\nWe observed that typically some layers are very well quantified at the end of this learning phase, whereas others are still far from binary. For that reason we then binarize some of the layers but not all. In order to improve further our compression rate, we then use the PQ method presented in the previous subsection.\nThe compression rate for this method is (32mn)/(kn + log2(k)ms) (instead of (32mn)/(32kn + log2(k)ms) for single Product Quantization). With a fixed segment size, increasing k will lead to decreasing the compression rate."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "We evaluate these different methods on two image classifi-\ncation datasets : MNIST and CIFAR10."
    }, {
      "heading" : "A. Experimental settings",
      "text" : "1) MNIST: The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixedsize image.\nThe neural network we use with MNIST is LeNet5. LeNet5 is a convolutional neural network introduced by Yann Lecun [22]. LeNet5 is one of the LeNet family networks, described in Figure 1. We use exactly the same parameters for the network (number of layer, sizes of layers, etc...).\n2) CIFAR10: The CIFAR10 database has a training set of 50,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from the 80 million tiny images dataset. It consists in 32x32 colour images partitioned into ten classes.\nWith the CIFAR10 database, we use a convolutional neural network made of four convolutional layers followed by two\nfully connected layers. This network has been introduced in [24]."
    }, {
      "heading" : "B. Layers to quantify",
      "text" : "Our first experiments (with the MNIST database) depicted in Figure 2 show the influence of quantified layers on performance. We observe that performance strongly depends on which layers are quantized. More precisely, this experiment shows that one should quantize layers from the output to the input rather than the contrary. This result is not surprising to us as input layers have often been described as similar to wavelet transforms, which are intrinsically analog operators, whereas output layers are often compared to high level patterns which detection in an image is often enough for good classification results."
    }, {
      "heading" : "C. Performance comparison",
      "text" : "Our second experiment shows a comparison with previous work. The results are depicted in Figures 3 and 4. Note that in both cases compared networks have the exact same architecture.\nAs far as our proposed method is concerned, we chose to compress only the two outputs layers, which are fully connected. Since their sizes are distinct, we were not able to use the same PQ coefficients k and m twice. Note that layer 2 contains almost all weights and is therefore the one we chose to investigate the role of each parameters.\nWe found that our added regularization cost allows to significantly improve performance. For example for the MNIST database, if we want to respect a loss of 2%, we have a compression rate of 33 with single PQ, whereas our learnbased method leads to a compression rate of 107."
    }, {
      "heading" : "V. BIOLOGICAL DISCUSSION",
      "text" : "Deep neural networks have often been compared to sensor parts of the human cortex, both in terms of architecture and performance [25]. As a matter of fact, some authors [26] propose models very similar to CNNs and biologically inspired.\nFor decades the question of whether the encoding of information in the brain is analog or digital has been debated. A recent paper [27] performed some experiments emphasizing the brain might have different encodings depending on the region it is applied to. In particular they show that as far as the visual cortex is concerned, the further the region is from the sensors the most likely its encodings are digital. It is tempting to draw a parallel between these findings and the ability of the brain to associate analog-like data, such as sounds and images, with digital-data, such as language and labels.\nThe results depicted in Figure 2 suggest a similar behavior in deep artificial neural systems."
    }, {
      "heading" : "VI. CONCLUSION AND PERSPECTIVES",
      "text" : "In this paper we introduced a new method to compress convolutionnal neural networks. This method consists in adding an extra term to the cost function that forces weights to become almost binary. In order to compress even more the network, we then apply Product Quantization and the combination of both allows us to reach performance above state-of-the-art methods.\nWe also demonstrate the influence of the depth of the binarized layer on performance. These findings are of particular interest to us, and a motivation to further explore the connections between actual biological data and deep neural systems.\nIn future work, we consider exploring more complex regularization functions. Also, we plan on performing PQ on the fly as well during the learning phase. Finally, other strategies could try to take benefit from sparseness of the trained weights. Such sparsity could be enforced thanks to similar mechanisms as the ones we introduce in this paper."
    } ],
    "references" : [ {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Handwritten digit recognition with a back-propagation network",
      "author" : [ "B.B. Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "Advances in neural information processing systems. Citeseer, 1990.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "arXiv preprint arXiv:1409.4842, 2014.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Caffe: An open source convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia" ],
      "venue" : "2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1310.1531, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1312.6229, 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Stochastic pooling for regularization of deep convolutional neural networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1301.3557, 2013.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multiscale orderless pooling of deep convolutional activation features",
      "author" : [ "Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik" ],
      "venue" : "Computer Vision–ECCV 2014. Springer, 2014, pp. 392–407.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A 240 g-ops/s mobile coprocessor for deep neural networks",
      "author" : [ "V. Gokhale", "J. Jin", "A. Dundar", "B. Martini", "E. Culurciello" ],
      "venue" : "Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on. IEEE, 2014, pp. 696–701.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Predicting parameters in deep learning",
      "author" : [ "M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 2148– 2156.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Exploiting linear structure within convolutional networks for efficient evaluation",
      "author" : [ "E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus" ],
      "venue" : "Advances in Neural Information Processing Systems, 2014, pp. 1269– 1277.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Speeding up convolutional neural networks with low rank expansions",
      "author" : [ "M. Jaderberg", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1405.3866, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Compressing deep convolutional networks using vector quantization",
      "author" : [ "Y. Gong", "L. Liu", "M. Yang", "L. Bourdev" ],
      "venue" : "arXiv preprint arXiv:1412.6115, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Product quantization for nearest neighbor search",
      "author" : [ "H. Jegou", "M. Douze", "C. Schmid" ],
      "venue" : "Pattern Analysis  and Machine Intelligence, IEEE Transactions on, vol. 33, no. 1, pp. 117–128, 2011.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Approximate nearest neighbor search by residual vector quantization",
      "author" : [ "Y. Chen", "T. Guan", "C. Wang" ],
      "venue" : "Sensors, vol. 10, no. 12, pp. 11 259–11 273, 2010.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Compressing neural networks with the hashing trick",
      "author" : [ "W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen" ],
      "venue" : "arXiv preprint arXiv:1504.04788, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Compressing convolutional neural networks",
      "author" : [ "——" ],
      "venue" : "arXiv preprint arXiv:1506.04449, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An algorithm for nonlinear optimization problems with binary variables",
      "author" : [ "W. Murray", "K.-M. Ng" ],
      "venue" : "Computational Optimization and Applications, vol. 47, no. 2, pp. 257–288, 2010.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Improving the speed of neural networks on cpus",
      "author" : [ "V. Vanhoucke", "A. Senior", "M.Z. Mao" ],
      "venue" : "Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, vol. 1, 2011.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Fast training of convolutional networks through ffts",
      "author" : [ "M. Mathieu", "M. Henaff", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1312.5851, 2013.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278– 2324, 1998.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep neural networks rival the representation of primate it cortex for core visual object recognition",
      "author" : [ "C.F. Cadieu", "H. Hong", "D.L. Yamins", "N. Pinto", "D. Ardila", "E.A. Solomon", "N.J. Majaj", "J.J. DiCarlo" ],
      "venue" : "PLoS computational biology, vol. 10, no. 12, p. e1003963, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Robust object recognition with cortex-like mechanisms",
      "author" : [ "T. Serre", "L. Wolf", "S. Bileschi", "M. Riesenhuber", "T. Poggio" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 3, pp. 411–426, 2007.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Analog and digital codes in the brain",
      "author" : [ "Y. Mochizuki", "S. Shinomoto" ],
      "venue" : "Physical Review E, vol. 89, no. 2, p. 022705, 2014.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Importing CNNs onto embedded platforms [10], the recent trend toward mobile computing, has a wide range of application impacts.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "CNN [5, 7, 8] with good performance in visual object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters in order to achieve state-of-the-art results.",
      "startOffset" : 4,
      "endOffset" : 13
    }, {
      "referenceID" : 6,
      "context" : "CNN [5, 7, 8] with good performance in visual object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters in order to achieve state-of-the-art results.",
      "startOffset" : 4,
      "endOffset" : 13
    }, {
      "referenceID" : 7,
      "context" : "CNN [5, 7, 8] with good performance in visual object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters in order to achieve state-of-the-art results.",
      "startOffset" : 4,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : "These parameters are overparameterized [11] and we aim at compressing them.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "speeding up the testing time [12].",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "In [12] and [13] the authors use compressing methods",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : "In [12] and [13] the authors use compressing methods",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "In [14] the authors focus on compressing dense connected layers.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers.",
      "startOffset" : 171,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers.",
      "startOffset" : 171,
      "endOffset" : 179
    }, {
      "referenceID" : 16,
      "context" : "In [17] the authors focus on compressing the fully connected layers of a MultiLayer Perceptron (MLP) using Hashing Trick, a low cost hash function to randomly group connection weights into hash buckets, and set the same value to all the parameters in the same bucket.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "In [18] the authors propose compressing convolutionnal layers using a Discrete Cosinus Transform applied on the convolutionnal filters, followed by Hashing Trick, as for the fully connected layers.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "For a typical CNN as the one introduced in [8], about 90% of the storage is taken up by the dense connected layers and more than 90% of the running time is taken by the convolutional layers.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "originally proposed in [19].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "In order to compress furthermore our obtained networks, we also use PQ as described in [14] afterwards.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "In [20] the authors use tricks of CPUs to speed up the execution of CNN.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "In [21] the authors show the convolutional operations can be efficiently carried out in the Fourier domain, leading to a speed-up of 200%.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "Two very recent works [12, 13] use linear matrix factorization methods for speeding up convolutional layers and obtain a 200% speed-up gain with almost no lost in classification performance.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "Two very recent works [12, 13] use linear matrix factorization methods for speeding up convolutional layers and obtain a 200% speed-up gain with almost no lost in classification performance.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "In [11], the authors demonstrate the redundancies in neural network parameters.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "These results motivate [9] to apply vector quantization methods to compress the redundancy in parameter space.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "Their work can be viewed as a compression of the parameter prediction results reported in [11].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "They obtain similar results to those of [11], in that they are able to compress the parameters about 20 times with little decrease of performance.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "This result further confirms the interesting empirical findings in [11].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "In [17], for the first time a learn-based method is proposed to compress neural networks.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "propose in [18] to compress filters in convolutional layers, arguing that the size of the convolutional layers in state-ofthe-art’s CNN is increasing year after year.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "We first introduce the product quantization method proposed in [14], and then introduce our proposed learn-based method.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "For more details on this quantization method, refer to [14].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "More precisely, the idea consists in applying Product Quantization (PQ) [15] to exploit the redundancy of structures in vector space.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "greatly inspired by [19].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : "To facilitate this selection of α, we use a barrier method [19] that consists in starting with small values of α and incrementing it regularly to help the quantization process.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "LeNet5 is a convolutional neural network introduced by Yann Lecun [22].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : "A graphical depiction of a LeNet model (figure inspired by [23])",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 23,
      "context" : "Deep neural networks have often been compared to sensor parts of the human cortex, both in terms of architecture and performance [25].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "As a matter of fact, some authors [26] pro-",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "A recent paper [27] performed some experiments emphasizing the brain might have different encodings depending on the region it is applied to.",
      "startOffset" : 15,
      "endOffset" : 19
    } ],
    "year" : 2017,
    "abstractText" : "Because of their performance, deep neural networks are increasingly used for object recognition. They are particularly attractive because of their ability to “absorb” great quantities of labeled data through millions of parameters. However, as the accuracy and the model sizes increase, so does the memory requirements of the classifiers. This prohibits their usage on resource limited hardware, including cell phones or other embedded devices. We introduce a novel compression method for deep neural networks that performs during the learning phase. It consists in adding an extra regularization term to the cost function of fully-connected layers. We combine this method with Product Quantization (PQ) of the trained weights for higher savings in memory and storage consumption. We evaluate our method on two data sets (MNIST and CIFAR10), on which we achieve significantly larger compression than state-of-the-art methods.",
    "creator" : "LaTeX with hyperref package"
  }
}