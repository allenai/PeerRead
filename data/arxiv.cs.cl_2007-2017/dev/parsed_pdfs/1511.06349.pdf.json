{
  "name" : "1511.06349.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Samuel R. Bowman", "Luke Vilnis" ],
    "emails" : [ "sbowman@stanford.edu", "luke@cs.umass.edu", "bengio}@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recurrent neural network language models (RNNLMs, Mikolov et al., 2011) represent the state of the art in unsupervised generative modeling for natural language sentences. In supervised settings, RNNLM decoders conditioned on task-specific features have yielded the state of the art in tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) and image captioning (Vinyals et al., 2015; Mao et al., 2015; Donahue et al., 2015). The RNNLM generates sentences word-by-word based on an evolving distributed state representation, which makes it a probabilistic model with no significant independence assumptions, and makes it capable of modeling complex distributions over sequences, including those with long-term dependencies. However, by breaking the model structure down into a series of next-step predictions, the RNNLM does not expose an interpretable representation of global features like style, topic, and high-level syntactic properties.\nWe propose an extension of the RNNLM that is designed to explicitly capture such global features in a continuous latent variable. Naively, maximum likelihood learning in such a model presents an intractable inference problem over the latent variable. Drawing inspiration from recent successes in modeling images (Gregor et al., 2015), handwriting, and natural speech (Chung et al., 2015), our model circumvents these difficulties using the architecture of a variational autoencoder and takes advantage of recent advances in variational inference (Kingma & Welling, 2015; Rezende et al.,\n∗First two authors contributed equally. Work was done when all authors were at Google, Inc.\nar X\niv :1\n51 1.\n06 34\n9v 1\n[ cs\n.L G\n] 1\n2014) that introduce a practical training technique for powerful neural network generative models with latent variables.\nOur contributions are as follows: We propose a variational autoencoder language model and discuss some of the obstacles to training it as well as our proposed solutions. We find that on standard next-step language modeling tasks where a global variable is not explicitly needed, inclusion of the global variable yields similar performance to existing RNNLMs. We also evaluate our model using a larger corpus on the task of imputing missing words. For this task, we introduce a novel evaluation strategy using an adversarial classifier, sidestepping the issue of intractable likelihood computations by drawing inspiration from work on non-parametric two-sample tests and adversarial training. In this setting, our model’s global latent variable allows it to substantially outperform RNNLMs. We finally introduce several qualitative techniques for analyzing the ability of our model to learn high level features of sentences. We find that they can produce diverse and coherent sentences through purely deterministic decoding and that they can interpolate smoothly between sentences."
    }, {
      "heading" : "2 PRIOR WORK",
      "text" : ""
    }, {
      "heading" : "2.1 UNSUPERVISED MODELS FOR WHOLE-SENTENCE ENCODING",
      "text" : "A standard RNN language model predicts each word of a sentence conditioned on the previous word and an evolving hidden state. While effective, it does not learn a vector representation of the entire sentence. In order to incorporate a continuous global latent sentence representation, we first need a method to map between sentences and distributed representations that can be trained in an unsupervised setting. While no strong generative model is available for this problem, three nongenerative techniques have shown promise: sequence autoencoders, skip-thought, and paragraph vector.\nSequence autoencoders have seen some success in pre-training sequence models for supervised downstream tasks (Dai & Le, 2015) and in generating complete documents (Li et al., 2015a). An autoencoder consists of an encoder function ϕenc and a probabilistic decoder model p(x|~z = ϕenc(x)), and maximizes the likelihood of a data case x conditioned on ~z, the learned code for x. In the case of a sequence autoencoder, both encoder and decoder are RNNs and data cases are sequences of tokens.\nThere are serious problems with using standard autoencoders to learn feature extractors for global sentence features. In Table 1, we present the results of computing a path or homotopy between the encodings for two sentences and decoding each intermediate code. The intermediate sentences are generally ungrammatical and do not transition smoothly from one to the other. This suggests that these models do not generally learn a smooth, interpretable feature system for sentence encoding. In addition, since these models do not incorporate a prior over ~z, there is no practical way to use them in a generative setting to assign probabilities to sentences or to sample novel sentences.\nTwo other models have shown promise in learning sentence encodings, but cannot be used in a generative setting: Skip-thought models (Kiros et al., 2015) are unsupervised learning models that take the same model structure as a sequence autoencoder, but are trained to predict the words in a sentence given an encoded neighboring sentence from the same text, instead of given the target sentence itself. Finally, paragraph vector models (Le & Mikolov, 2014) are non-recurrent sentence representation models. In a paragraph vector model, the encoding of a sentence is obtained by\nperforming gradient-based inference on a prospective encoding vector with the goal of using it to predict the words in the sentence."
    }, {
      "heading" : "2.2 THE VARIATIONAL AUTOENCODER",
      "text" : "The variational autoencoder (VAE, Kingma & Welling, 2015; Rezende et al., 2014) is a generative model that is based on a regularized version of the standard autoencoder. This model imposes a prior distribution on the hidden codes ~z which enforces a regular geometry over codes and makes it possible to draw proper samples from the model using ancestral sampling.\nThe VAE modifies the autoencoder architecture by replacing the deterministic function ϕenc with a learned posterior recognition model, q(~z|x). This model parametrizes an approximate posterior distribution over ~z (usually a diagonal Gaussian) with a neural network conditioned on x. Intuitively, the VAE learns codes not as single points, but as soft ellipsoidal regions in latent space, forcing the codes to fill the space rather than memorizing the training data as isolated codes.\nIf the VAE were trained with a standard autoencoder’s reconstruction objective, it would learn to encode its inputs deterministically by making the posterior variances in q(~z|x) vanishingly small (Raiko et al., 2015). Instead, the VAE uses a specialized objective which encourages the model to keep its posterior distributions close to a prior p(~z), generally a standard Gaussian (µ = ~0, σ = ~1). Additionally, this objective is a valid lower bound on the true log likelihood of the data, making the VAE a generative model. This objective takes the following form:\nL(θ;x) = −KL(qθ(~z|x)||p(~z)) + Eqθ(~z|x)[log pθ(x|~z)] ≤ log p(x) . (1)\nThis forces the model to be able to decode plausible sentences from every point in the latent space that has a reasonable probability under the prior.\nIn the experiments presented below using VAE models, we use diagonal Gaussians for the prior and posterior distributions p(~z) and q(~z|x), using the Gaussian reparameterization trick of Kingma & Welling (2015). We train our models with stochastic gradient descent, and at each gradient step we estimate the reconstruction cost using a single sample from q(~z|x), but compute the KL divergence term of the cost function in closed form, again following Kingma & Welling (2015)."
    }, {
      "heading" : "3 THE VARIATIONAL AUTOENCODER LANGUAGE MODEL",
      "text" : "We adapt the variational autoencoder to text by using single-layer LSTM RNNs (Hochreiter & Schmidhuber, 1997) for both the encoder and the decoder, essentially forming a sequence autoencoder with the Gaussian prior acting as a regularizer on the hidden code. The decoder serves as a special RNN language model that is conditioned on this hidden code, and in the degenerate setting where the hidden code incorporates no useful information, this model is effectively equivalent to an RNNLM. The model is depicted in Figure 1, and is used in all of the experiments discussed below.\nWe explored several variations on this architecture, including concatenating the sampled ~z to the decoder input at every time step, using a softplus parametrization for the variance, and using deep feedforward networks between the encoder and latent variable and the decoder and latent variable. We noticed little difference in the model’s performance when using any of these variations. However, when including feedforward networks between the encoder and decoder we found that it is\nnecessary to use highway network layers (Srivastava et al., 2015) for the model to learn. We discuss hyperparameter tuning in the appendix.\nWe also experimented with more sophisticated recognition models q(~z|x), including a multistep sampling model styled after DRAW (Gregor et al., 2015), and a posterior approximation using normalizing flows (Rezende & Mohamed, 2015). However, we were unable to reap significant gains over our plain VAE.\nWhile the strongest results with VAEs to date have been on continuous domains like images, there has been some limited work on discrete sequences: a technique for doing this using RNN encoders and decoders, which shares the same high-level architecture as our model, was proposed under the name Variational Recurrent Autoencoder (VRAE) for the modeling of music in Fabius & van Amersfoort (2014). While there has been other work on including continuous latent variables in RNN-style models for modeling speech, handwriting, and music (Bayer & Osendorfer, 2015; Chung et al., 2015), these models include separate latent variables per timestep and are unsuitable for our goal of modeling global features."
    }, {
      "heading" : "3.1 OPTIMIZATION CHALLENGES",
      "text" : "Our model aims to learn global latent representations of sentence content. We can quantify the degree to which our model learns global features by looking at the variational lower bound objective (1). The bound breaks into two terms: the data likelihood under the posterior (expressed as cross entropy), and the KL divergence of the posterior from the prior. A model that encodes useful information in the latent variable ~z will have a non-zero KL divergence term and a relatively small cross entropy term. Straightforward implementations of our VAE fail to learn this behavior: except in vanishingly rare cases, most training runs with most hyperparameters yield models that consistently set q(~z|x) equal to the prior p(~z), bringing the KL divergence term of the cost function to zero. This is perhaps understandable. In this regime, the model is essentially behaving as an RNNLM. Because of this, it is able to express arbitrary distributions over the output sentences (albeit with a potentially awkward left-to-right factorization) and can thereby achieve likelihood values that may come close to capturing the true degree of variation in the data. Previous work on VAEs for image modeling (Kingma & Welling, 2015) used a much weaker independent pixel decoder model p(x|~z), forcing the model to use the global latent variable to achieve good likelihoods. In a related result, recent approaches to image generation that use LSTM decoders are able to do well without VAE-style global latent variables (Theis & Bethge, 2015).\nThis problematic tendency in learning is compounded by the LSTM decoder’s sensitivity to subtle variation in the hidden states, such as that introduced by the posterior sampling process. This causes the model to initially learn to ignore ~z and go after the ‘low hanging fruit’, explaining the data with the powerful and more easily optimized LSTM language model. Once this has happened, the decoder ignores the encoder, little to no gradient signal passes between the two, and the encoder can thus easily fail to capture any useful features, yielding a stable equilibrium with the KL cost term at zero.\nKL cost annealing In this simple approach to this problem, we add a variable weight to the KL term in the cost function at training time. At the start of training, we set that weight to zero, so that the model learns to encode as much information in ~z as it can. Then, as training progresses, we gradually increase this weight, forcing the model to smooth out its encodings and pack them into the prior. We increase this weight until it reaches 1, at which point the weighted cost function is equivalent to the true variational lower bound. In this setting, we do not optimize the proper lower bound on the training data likelihood during the early stages of training, but we nonetheless see improvements on the value of that bound at convergence. This can be thought of as annealing from a vanilla autoencoder to a VAE. The rate of this increase is tuned as a hyperparameter.\nFigure 2 shows the behavior of the KL cost term during the first 50k steps of training on Penn Treebank (Marcus et al., 1993) language modeling with KL cost annealing in place. This example reflects a pattern that we observed often: KL spikes early in training while the model can encode information in ~z cheaply, then drops substantially once it begins paying the full KL divergence penalty, and finally slowly rises again before converging as the model learns to condense more information into ~z efficiently.\nUnder review as a conference paper at ICLR 2016\nStep KL term weightStep KL term value 1 0.000435 1 0.015607\n101 0.000555 751 6.977695 201 0.000708 1501 7.502852 301 0.000903 2251 2.323177 401 0.001152 3001 1.373341 501 0.001469 3751 0.886143 601 0.001874 4501 0.850737 701 0.002389 5251 0.891682 801 0.003047 6001 0.820286 901 0.003884 6751 0.880547\n1001 0.004951 7501 0.887476 1101 0.006309 8251 0.922485 1201 0.008036 9001 0.874522 1301 0.010231 9751 0.969236 1401 0.013018 10501 0.986424 1501 0.016551 11251 0.942297 1601 0.021022 12001 0.989414\n1701 0.026669 12751 0.972859\n1801 0.03378 13501 1.024596\n1901 0.042705 14251 1.04533\n2001 0.053856 15001 1.025573 2101 0.067712 15751 1.072477 2201 0.084814 16501 1.092722 2301 0.105745 17251 1.041643 2401 0.131103 18001 1.128839\n0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 0% 20% 40% 60% 80% 1 0%\n0 100 0 20000 30000 40000 50000\nK L\nte rm\nv al\nue\nK L\nte rm\nw ei\ngh t\nStep\nKL term weight\nKL term value\nFigure 2: The weight of the KL divergence term of variational lower bound according to a typical sigmoid annealing schedule plotted alongside the (unweighted) value of the KL divergence term for our VAE on the Penn Treebank.\nModel Standard Inputless Decoder Train NLL Train PPL Test NLL Test PPL Train NLL Train PPL Test NLL Test PPL\nRNNLM 100 – 95 100 – 116 135 – 600 135 – > 600 VAE 98 (2) 100 101 (2) 119 120 (15) 300 125 (15) 380\nTable 2: Penn Treebank language modeling results, reported as negative log likelihoods and as perplexities. Lower is better for both metrics. For the VAE, the KL term of the likelihood is shown in parentheses alongside the total likelihood.\nWord dropout and decoding without history In addition to weakening the penalty term on the encodings, we also experiment with weakening the decoder. As in RNNLMs and sequence autoencoders, during learning our decoder predicts each word conditioned on the ground-truth previous word. A natural way to weaken the decoder is to remove some or all of this conditioning information during learning. We do this by randomly replacing some fraction of the conditioned-on word tokens with the generic unknown word token UNK. This forces the model to rely on the latent variable ~z to make good predictions. This technique is a variant of word dropout (Iyyer et al., 2015; Kumar et al., 2015), applied not to a feature extractor but to a decoder. We also experimented with standard dropout (Srivastava et al., 2014) applied to the input word embeddings in the decoder, but this did not help the model learn to use the latent variable.\nThis technique is parameterized by a keep rate k ∈ [0, 1]. We tune this parameter both for our VAE and for our baseline RNNLM. Taken to the extreme of k = 0, the decoder sees no input, and is thus able to condition only on the number of words produced so far, yielding a model that is extremely limited in the kinds of distributions it can model without using ~z."
    }, {
      "heading" : "4 RESULTS: LANGUAGE MODELING",
      "text" : "In this section, we report on language modeling experiments on the Penn Treebank in an effort to discover whether the inclusion of a global latent variable is helpful for this standard task. For this reason, we restrict our VAE hyperparameter search to those models which encode a non-trivial amount in the latent variable, as measured by the KL divergence term of the variational lower bound.\nResults We used the standard train–test split for the corpus, and report test set results in Table 2. The results shown reflect the training and test set performance of each model at the training step at which the model performs best on the development set. Our reported figures for the VAE reflect the variational lower bound on the test likelihood, while for the RNNLMs, which can be evaluated exactly, we report the true test likelihood. This discrepancy puts the VAE at a potential disadvantage.\nIn the standard setting, the VAE performs slightly worse than the RNNLM baseline, though it does succeed in using the latent space to a limited extent: it has a reconstruction cost (99) better than that of the baseline RNNLM, but makes up for this with a KL divergence cost of 2. Training a VAE in the standard setting without both word dropout and cost annealing reliably results in models with equivalent performance to the baseline RNNLM, and zero KL divergence.\nTo demonstrate the ability of the latent variable to encode the full content of sentences in addition to more abstract global features, we also provide numbers for an inputless decoder that does not condition on previous tokens, corresponding to a word dropout keep rate of 0. In this regime we can see that the variational lower bound contains a significantly larger KL term and shows a substantial improvement over the RNNLM, which is essentially limited to using unigram statistics in this setting. While it is weaker than a standard decoder, the inputless decoder has the interesting property that its sentence generating process is fully differentiable. Advances in generative models of this kind could be promising as a means of generating text while using adversarial training methods, which require differentiable generators. In this setting then, the VAE has a large advantage.\nEven with the techniques described in the previous section, including the inputless decoder, we were unable to train models for which the KL divergence term of the cost function dominates the reconstruction term. This suggests that it is still substantially easier to learn to factor the data distribution using simple local statistics, as in the RNNLM, such that an encoder will only learn to encode information in ~z when that information cannot be described by these local statistics."
    }, {
      "heading" : "5 RESULTS: IMPUTING MISSING WORDS",
      "text" : "We claim that the global features that our VAE uses make it especially well suited to the task of imputing missing words in otherwise known sentences. In this section, we present a technique for imputation and a novel evaluation strategy inspired by adversarial training. Qualitatively, we find that the VAE yields more diverse and plausible imputations for the same amount of computation (see the examples given in Table 3), but quantitative comparison requires a novel evaluation strategy, which we propose below.\nWhile the standard RNNLM is a powerful generative model, the sequential nature of likelihood computation and decoding makes it unsuitable for performing inference over unknown words given some known words (the task of imputation). Except in the special case where the unknown words all appear at the end of the decoding sequence, sampling from the posterior over the missing variables is intractable for all but the smallest vocabularies. For a vocabulary of size V , it requires O(V ) runs of full RNN inference per step of Gibbs sampling or iterated conditional modes. Worse, because of the unidirectional nature of the graphical model given by an RNNLM, many steps of sampling could be required to propagate information between unknown variables and the known downstream variables. The VAE, while it suffers from the same intractability problems when sampling or computing MAP imputations, can more easily propagate information between all variables, by virtue of having a global latent variable and a tractable recognition model.\nFor this experiment and subsequent analysis, we train our models on the Book Corpus introduced in Kiros et al. (2015). This is a collection of text from 12k e-books, mostly fiction. The dataset, after pruning, contains approximately 80m sentences. We find that this much larger amount of data produces more subjectively interesting generative models than smaller standard language modeling datasets. We use a fixed word dropout rate of 75% when training this model and all subsequent mod-\nels unless otherwise specified. Our models (the VAE and RNNLM) are trained as language models, decoding right-to-left to shorten the dependencies during learning for the VAE. We use a model with 512 hidden units for these and subsequent experiments.\nInference method To generate imputations from the two language models, we use beam search with beam size 15 for the RNNLM and approximate iterated conditional modes (Besag, 1986) with 3 steps of a beam size 5 search for the VAE. This allows us to compare the same amount of computation for both models. We find that breaking decoding for the VAE into several sequential steps is necessary to propagate information among the variables. Iterated conditional modes is a technique for finding the maximum joint assignment of a set of variables by alternately maximizing conditional distributions, and is a generalization of “hard-EM” algorithms like k-means (Kearns et al., 1998). For approximate iterated conditional modes, we first initialize the unknown words to the UNK token. We then alternate assigning the latent variable to its mode from the recognition model, and performing constrained beam search to assign the unknown words. Both of our generative models are trained to decode sentences from right-to-left to shorten the dependencies during learning, and we impute the final 20% of each sentence. This lets us demonstrate the advantages of the global latent variable in the regime where the RNNLM suffers the most from its inductive bias.\nAdversarial evaluation Drawing inspiration from adversarial training methods for generative models as well as non-parametric two-sample tests (Goodfellow et al., 2014; Li et al., 2015b; Denton et al., 2015; Gretton et al., 2012), we evaluate the imputed sentence completions by examining their distinguishability from the true sentence endings. While the non-differentiability of the discrete RNN decoding phase prevents us from easily applying the adversarial criterion at train time, we can define a very flexible test time evaluation by training a discriminant function to separate the generated and true sentences, which defines an adversarial error.\nWe train two classifiers – the first a bag-of-unigrams logistic regression classifier, and the second an LSTM logistic regression classifier that reads the input sentence and produces a binary prediction after seeing the final EOS token. We train these classifiers using early stopping on a 80/10/10 train/dev/test split of 320k sentences, constructing a dataset of 50% complete sentences from the corpus (positive examples) and 50% sentences with imputed completions (negative examples). We define the adversarial error as the gap between the ideal accuracy of the discriminator (50%, i.e. indistinguishable samples), and the actual accuracy attained.\nResults As an obvious consequence of this experimental setup, the RNNLM cannot choose anything outside of the top 15 tokens given by the RNN’s initial unigram language model P (x1|Null) when producing the final token of the sentence, since it has not yet generated anything to condition on, and has a beam size of 15. As demonstrated in Table 4, this weakness makes the RNNLM produce far less diverse samples than the VAE and suffer accordingly versus the adversarial classifier. Additionally, we include the score given to the entire sentence with the imputed completion given a separate independently trained language model. The likelihood results are comparable, though the RNNLMs favoring of cliched but high-probability endings such as “he said,” gives it a slightly lower negative log-likelihood. Measuring the RNNLM likelihood of sentences themselves produced by an RNNLM is not a good measure of the power of the model, but demonstrates that the RNNLM can produce what it sees as high-quality imputations by favoring typical local statistics, even though their repetitive nature produces easy failure modes for the adversarial classifier. Accordingly, under the\nUnder review as a conference paper at ICLR 2016 keep rate Cross entropy KL divergence 100% 45.01017 0.010358 90% 40.897953 4.665799 75% 37.710022 8.751512 50% 33.433636 15.13052 0% 34.825763 20.906685\nkeep prob xent kl 100% 3.059872 0.000953 3.060825 90% 2.706509 0.388772 3.095281 75% 2.462569 0.695894 3.158463 50% 2.174506 1.109832 3.284338 0% 2.235086 1.478137 3.713223\nadversarial evaluation our model substantially outperforms the baseline since it is able to efficiently propagate information bidirectionally through the latent variable."
    }, {
      "heading" : "6 ANALYZING VARIATIONAL MODELS",
      "text" : "We now turn to more qualitative analysis of the model. Since our decoder model p(x|~z) is a sophisticated RNNLM, simply sampling from the directed graphical model (first p(~z) then p(x|~z)) would not tell us much about how much of the data is being explained by the learned vector vs. the language model. Instead, for this part of the evaluation, we sample from the Gaussian prior, but use a greedy deterministic decoder for p(x|~z), the RNNLM conditioned on ~z. This allows us to get a sense of how much of the variance in the data distribution is being captured by the distributed vector ~z as opposed to the language model. Interestingly, these results qualitatively demonstrate that large amounts of variation in generated language can be achieved by following this procedure."
    }, {
      "heading" : "6.1 ANALYZING THE IMPACT OF WORD DROPOUT",
      "text" : "For this experiment, we train on the Books Corpus and test on a held out 10k sentence test set from that corpus. We find that train and test set performance are very similar. In Figure 3, we examine the impact of word dropout on the variational lower bound, broken down into KL divergence and cross entropy components. We drop out words with the specified keep rate at training time, but supply all words as inputs at test time except in the 0% setting.\nWe do not re-tune the hyperparameters for each run, which results in the model with no dropout encoding very little information in ~z (i.e., the KL component is small). We can see that as we lower the keep rate for word dropout, the amount of information stored in the latent variable increases, and the overall likelihood of the model degrades somewhat. Note that, as demonstrated in the previous section, a model with no latent variable would degrade in performance significantly more in the presence of heavy word dropout.\nWe also qualitatively evaluate samples, to demonstrate that the increased KL allows meaningful sentences to be generated purely from continuous sampling. Since our decoder model p(x|~z) is a sophisticated RNNLM, simply sampling from the directed graphical model (first p(~z) then p(x|~z)) would not tell us about how much of the data is being explained by the learned vector vs. the language model. Instead, for this part of the qualitative evaluation, we sample from the Gaussian prior, but use a greedy deterministic decoder for x, taking each token xt = argmaxxtp(xt|x0,...,t−1, ~z). This allows us to get a sense of how much of the variance in the data distribution is being captured by the distributed vector ~z as opposed to by local language model dependencies.\nThese results, shown in Table 5, qualitatively demonstrate that large amounts of variation in generated language can be achieved by following this procedure. At the low end, where very little of the variance is explained by ~z, we see that greedy decoding applied to a Gaussian sample does not produce diverse sentences. As we increase the amount of word dropout and force ~z to encode more information, we see the sentences become more varied, but past a certain point they begin to repeat words or show other signs of ungrammaticality. Even in the case of a fully dropped-out decoder, the model is able to capture higher-order statistics not present in the unigram distribution.\nAdditionally, in Table 6 we examine the effect of using lower-probability samples from the latent Gaussian space for a model with a 75% word keep rate. We find lower-probability samples by\napplying an approximately volume-preserving transformation to the Gaussian samples that stretches some eigenspaces by up to a factor of 4. This has the effect of creating samples that are not too improbable under the prior, but still reach into the tails of the distribution. We use a random linear transformation, with matrix elements drawn from a uniform distribution from [−c, c], with c chosen to give the desired properties (0.1 in our experiments). Here we see that the sentences are far less typical, but for the most part are grammatical and contain interesting topic information, indicating that the latent variable is capturing a rich variety of global features even for rare sentences."
    }, {
      "heading" : "6.2 SAMPLING FROM THE POSTERIOR",
      "text" : "In addition to generating unconditional samples, we can also examine the sentences decoded from the posterior vectors p(z|x) for various sentences x. Because the model is regularized to produce distributions rather than deterministic codes, it does not exactly memorize and round-trip the input. Instead, we can see what the model considers to be similar sentences by examining the posterior samples. In Table 7, observe that the model stores information about the number of tokens and parts of speech for each token, as well as apparent topic information. As the sentences get longer, the fidelity of the round-tripped sentences decreases."
    }, {
      "heading" : "6.3 HOMOTOPIES",
      "text" : "The use of a variational autoencoder allows us to generate sentences using greedy decoding on continuous samples from the space of codes. Additionally, the volume-filling and smooth nature of the code space allows us to examine for the first time a concept of homotopy (linear interpolation) between sentences. In this context, a homotopy between two codes ~z1 and ~z2 is the set of points on the line between them, inclusive, ~z(t) = ~z1 ∗ (1 − t) + ~z2 ∗ t for t ∈ [0, 1]. Similarly, the homotopy between two sentences decoded (greedily) from codes ~z1 and ~z2 is the set of sentences decoded from the codes on the line. Examining these homotopies allows us to get a sense of what neighborhoods in code space look like – how the autoencoder organizes information and what it regards as a continuous deformation between two sentences.\nWhile a standard non-variational RNNLM does not have a way to perform these homotopies, a vanilla sequence autoencoder can do so. As mentioned earlier in the paper, if we examine the homotopies created by the sequence autoencoder in Table 1, though, we can see that the transition between sentences is sharp, and results in ungrammatical intermediate sentences. This gives evidence for our intuition that the VAE learns representations that are smooth and “fill up” the space.\nIn Tables 8 and 10 (in the appendix) we can see that the codes mostly contain syntactic information, such as the number of words and the parts of speech of tokens, and that all intermediate sentences are grammatical. Some topic information also remains consistent in neighborhoods along the path. Additionally, sentences with similar syntax and topic but flipped sentiment valence, e.g. “the pain was unbearable” vs. “the thought made me smile”, can have similar embeddings, a phenomenon which has been observed with single-word embeddings (for example the vectors for “bad” and “good” are often very similar due to their similar distributional characteristics)."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "This paper introduces the use of a variational autoencoder for unsupervised generative language modeling. We present novel techniques that allow us to train our model successfully, and find that it can substantially outperform an RNNLM baseline in the imputation of missing words. We analyze the latent space learned by our model, and find that it is able to generate coherent and diverse sentences through purely continuous sampling and provides interpretable homotopies that smoothly interpolate between sentences.\nWe hope in future work to investigate factorization of the latent variable into separate style and content components, to generate sentences conditioned on extrinsic features, to learn sentence embeddings in a semi-supervised fashion for classification tasks like textual entailment, and to go beyond adversarial evaluation to a fully adversarial training objective."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank the Google Brain team, Alireza Makhzani, Laurent Dinh, Jon Gauthier, and the Stanford NLP Group for their helpful contributions and feedback."
    }, {
      "heading" : "APPENDIX: HYPERPARAMETER TUNING",
      "text" : "We extensively tune the hyperparameters of each model using an automatic Bayesian hyperparameter tuning algorithm (based on Snoek et al., 2012) over development set data. We run the model with each set of hyperpameters for 10 hours, operating 12 experiments in parallel, and choose the best set of hyperparameters after 200 runs. Results for our language modeling experiments are reported in Table 9."
    }, {
      "heading" : "APPENDIX: ADDITIONAL HOMOTOPIES",
      "text" : "Table 10 shows additional homotopies of the kind discussed in Section 6.3. We observe that intermediate sentences are almost always grammatical, and often contain consistent topic, vocabulary and syntactic information in local neighborhoods as they interpolate between the endpoint sentences. Because the model is trained on fiction, including romance novels, the topics are often rather dramatic."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The standard unsupervised recurrent neural network language model (RNNLM)<lb>generates sentences one word at a time and does not work from an explicit global<lb>distributed sentence representation. In this work, we present an RNN-based vari-<lb>ational autoencoder language model that incorporates distributed latent represen-<lb>tations of entire sentences. This factorization allows it to explicitly model holis-<lb>tic properties of sentences such as style, topic, and high-level syntactic features.<lb>Samples from the prior over these sentence representations remarkably produce<lb>diverse and well-formed sentences through simple deterministic decoding. By ex-<lb>amining paths through this latent space, we are able to generate coherent novel<lb>sentences that interpolate between known sentences. We present techniques for<lb>solving the difficult learning problem presented by this model, demonstrate strong<lb>performance in the imputation of missing tokens, and explore many interesting<lb>properties of the latent sentence space.",
    "creator" : "LaTeX with hyperref package"
  }
}