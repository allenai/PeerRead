{
  "name" : "1703.02437.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PathTrack: Fast Trajectory Annotation with Path Supervision",
    "authors" : [ "Santiago Manen", "Michael Gygli", "Dengxin Dai", "Luc Van Gool" ],
    "emails" : [ "vangool}@vision.ee.ethz.ch" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Progress in vision has been fueled by the emergence of datasets of ever-increasing scale. An example is the surge of Deep Learning thanks to ImageNet [26, 42]. The scaling up of datasets for Multiple Object Tracking (MOT) however has been limited due to the difficulty and cost to annotate complex video scenes with many objects. As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51]. This has hindered the development of fully learned MOT systems that can generalize to any scenario. In this paper, we tackle these issues by introducing a fast\n1We will provide our dataset and deep models at http://www. project-website.com.\nand intuitive way to annotate trajectories in videos and use it to create a large-scale MOT dataset.\nObjects can be annotated at different levels of detail. The cheapest way is to provide video-level object labels [38] or action labels [4]. On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects. Per-frame bounding box annotations lie in between these extremes. We call this the trajectory annotation task. The common approach to it is to annotate a sparse set of boxes and interpolate between them linearly [53] or with shortest-paths [45]. This is expensive, e.g. it cost tens of thousands of dollars to annotate the VIRAT dataset [47].\nThe typical annotation pipeline involves the user idly watching the video in-between manual annotations. This is arguably a waste of time. In this paper, we present path supervision as a more productive alternative. In it, the annotator follows the object with the cursor while playing the video, collecting a path annotation, c.f . Fig. 1. Hence, watching time is efficiently turned into annotation time. Our experiments show that these paths are fast to annotate, almost in real time.\nar X\niv :1\n70 3.\n02 43\n7v 2\n[ cs\n.C V\n] 2\n2 M\nar 2\nPath annotations are approximate and do not provide the scale of the object. So recovering full box trajectories from them is far from trivial. We alleviate these problems by using object detections, since our goal is to generate large MOT datasets, for which we know the class of interest. Our optimization produces an accurate box trajectory for each path annotation, by linking detections in a global optimization. Our approach is presently the fastest way to annotate MOT trajectories for any annotation quality.\nSince our annotation approach is intuitive, we could crowd source a large-scale dataset with Amazon Mechanical Turk (AMT) [1]. This PathTrack dataset is our second major contribution: a large MOT dataset of more than 15,000 person trajectories in 720 sequences, 30 times more than currently available ones [29]. Its focus lies on a largescale and diverse training set, aimed to initiate a new generation of fully data-driven MOT systems. We show its potential by learning better detection-association models for MOT, which substantially improves the top-performing tracker in MOT15, i.e. NOMT [9]. In summary, our contributions are:\n– A novel approach to produce full box trajectories from path annotations. It is currently the fastest way to annotate trajectories for any annotation quality and it specially shines for quick quantity-over-quality data collection strategies, ideal for training data.\n– The novel PathTrack MOT dataset, which includes the collection and annotation of 720 challenging sequences. It focuses on providing abundant training data to learn data-driven trackers. We show its potential by improving the top tracker on MOT15 [29].\n– Insights into collection of training data for MOT. Our experiments show that the MOT community can still benefit from more training data and a saturation point has not yet been reached. Furthermore, quantity seems to be more important than quality when learning to link detections into trajectories."
    }, {
      "heading" : "2. Related work",
      "text" : "There is quite some work on multimedia annotation [11]. The most related works annotate objects in videos and can generate datasets for MOT training and evaluation.\nTrajectory annotation in videos We focus on frameworks aimed at annotating persons with the purpose of generating tracking datasets. Of less relevance to us are those that work on videos with only a few people, such as [48, 35]. The naive way to annotate trajectories is to indicate the object location in every frame. This is inefficient as objects tend to move little between frames. Hence, VIPER-GT [33] and LabelMe video [53] propose to linearly interpolate boxes between annotated keyframes. There is also a family of methods that learn an appearance model from a sparse set of box annotations. VATIC [47] uses this appearance model to define a graph on which it performs a shortestpath interpolation between manual annotations with Dynamic Programming [6]. The shortest-path interpolation allows for larger time gaps without manual annotations, assuming that the object is clearly visible, and it can be efficient [49]. A VATIC improvement [46] incorporated active learning to decide which frames to annotate, to maximize the gain coming with such frames [39]. [10] built on top of shortest-path interpolation by updating the optimization weights with each extra annotation. Recently, [19] reconstructed annotated boxes and interpolated the final trajectories in 3D space. Based on the aforementioned approaches, multiple annotation tools have been developed [23, 8, 36]. Some gamify the annotation process [12]. As an alternative to trajectory supervision, some works aim to automatically discover and track objects in video collections, e.g. [27].\nCompared to previous approaches, we annotate large quantities of videos with the minimum effort possible and prefer quantity over quality in our training data, which have shown success in other tasks.\nPath supervision Pointing at objects comes very natural and has often been used in human-computer interaction [21, 22], yet it only recently gained popularity in Computer Vision. In parallel with our work, [32] found path annotations promising for action localization in videos. Compared to [32, 50], we annotate dozens of people in highly-crowded sequences, ideal for MOT purposes. Also recently, [5] and [22] used point supervision to segment objects in images and videos, resp. [22] uses multiple points to segment, by iteratively re-ranking a collection of thousands of object proposals, called Click Carving.\nWe are the first to propose a trajectory annotation framework based on linking detections with path supervision and use it to generate a large MOT dataset.\nTracking datasets There is a corpus of video datasets that provide frame-level [15, 20] or pixel-level annotations [37]. [25] and [40] are the largest datasets for single object tracking. Most large-scale MOT datasets are restricted to surveillance videos [13, 43, 51], since they depict smooth and quasi-linear trajectories that are easy to annotate. More related to ours, KITTI [18] is collected from a car-mounted camera and focuses on pedestrians and vehicles. Parts of this dataset have been reproduced and rendered virtually, to show the potential of virtual datasets [17]. [29, 34] have become the standard benchmarks for MOT, containing complex pedestrian scenes with static or moving cameras. Compared to these datasets, ours exhibits more diverse scenes and camera movement and is 33 times larger. Tab. 1 shows a quantitative comparison."
    }, {
      "heading" : "3. Trajectory annotation with path supervision",
      "text" : "In this section, we describe our annotation framework: we formalize path supervision in Sec. 3.1 and then detail how we leverage it to infer accurate trajectories in Sec. 3.2. In Sec. 3.3 we show how to incorporate box supervision."
    }, {
      "heading" : "3.1. Path supervision",
      "text" : "A path annotation of an object i consists of an (x, y)coordinate pi(t) that lies inside its bounding box at frame id t. Path annotations are intuitive and efficient to obtain by watching each object independently while following its location with the mouse cursor, c.f . Fig. 1. Our results show that annotating paths is only 33% slower than watching the video in real time. We say that a video has path supervision if a human annotator has provided a path annotation for the objects of interest. The following section explains how we use these annotations to obtain accurate box trajectories."
    }, {
      "heading" : "3.2. From path supervision to full box trajectories",
      "text" : "While path supervision is intuitive and efficient, it comes with its own set of challenges: a) It offers no information about the spatial extent of the object. b) The relative position of the path annotation inside the object is unknown. We partially solve these two problems by drawing on the success of object detection, since our final goal is to generate large MOT datasets and we know what kind of objects we want to annotate. Object detection is gaining maturity for objects of primary interest, so it is natural to use it as an established technique. Each detection is represented with a box and a confidence score at a given frame.\nOur goal is to infer the trajectories T of the objects in the sequence, given the set of input path annotations P and\nobject detectionsD. This problem is similar to the trackingby-detection data association problem, but with additional information from path supervision: the number of objects, their time span and their rough location are given. Our optimization considers the following intuitive forces:\n1. Path potential: Detections should be assigned to trajectories with compatible path annotations.\n2. Video-content potential: Confident detections should be used and affine detections should be encouraged to have the same label. We say that two detections are affine if they are likely to belong to the same object in different frames, according to the content of the video.\n3. Trajectory constraint: Trajectories have a single location per frame. Therefore, at most one detection can be assigned to one trajectory at any given frame.\nWe include these conditions in a two-step optimization. We first relax the trajectory constraint and label each detection with a provisional trajectory. This clusters the detections according to their corresponding trajectory, c.f . Fig. 2b. We can assume that a final trajectory can be constructed with detections from its cluster, and will not contain detections from another cluster. This detection pre-labeling step is detailed in Sec. 3.2.1. At this point each cluster is might include false positives, which violate the trajectory constraint. So, in a second step, we find the most probable trajectory in each cluster in a detection linkage step, see Fig. 2c. We describe this step in Sec. 3.2.2."
    }, {
      "heading" : "3.2.1 Detection pre-labeling",
      "text" : "The goal of this step is to assign a path annotation label yi to each detection di. Dropping the trajectory constraint allows us encode the path and video-content potentials in a global discrete energy minimization framework. Intuitively, we will assign path annotations to compatible object detections, assigning affine detections to the same cluster. The optimal label assignment Y∗ is that which minimizes:\nminimize Y ∑ i∈D U(yi) + ∑ (i,j)∈E W (yi, yj) (1)\nwhere the unary potential U(yi) is the cost of assigning label yi to detection i and the pairwise potential W (yi, yj) the cost of assigning different labels to detections i and j according to their affinity. For computational reasons, we limited to a temporal window of 4 seconds, which did not worsen the empirical results. Fig. 2b illustrates a typical pre-labeling result. We now describe the potentials we use.\nAs aforementioned, we do not assume the path annotations to be pixel-accurate center annotations. Instead we assume that they frequently lie in the bounding box of the object, a much weaker restriction. Therefore, our unary potential encourages assigning a label y to a detection di if the\ncorresponding path annotation py(ti) falls inside the detection for the corresponding frame ti:\nU(yi) = { 0, if py(ti) ∈ di, ∞, otherwise.\n(2)\nIndeed our unary only requires a rough location of the path annotation somewhere inside the bounding box of the object. Note that this requirement does not need to be satisfied in every frame: the path supervision occasionally falling inside the object is usually enough to annotate it accurately. We prune detections which do not contain path annotations.\nWhile the unary potential is based on the path supervision, the pairwise encodes video content. It discourages affine detections being assigned to different clusters:\nW (yi, yj) = { − log aij , if yi 6= yj , 0, otherwise.\n(3)\nwhere aij represents the affinity between detections i and j and must be decimal number between 0 and 1. This pairwise potential is submodular, so the energy function Eq. (1) can be solved with Graph Cuts [24] efficiently. We now describe the affinity measure we used.\nOF-trajectory affinity measure In our work, we use an affinity measure based on optical-flow trajectories (OF trajectory). These are obtained by linking pixels through time using frame-to-frame optical flow and forward-backward consistency checks [16]. These trajectories are represented with an (x, y)-position for each frame in their time span. Intuitively, two detections that share many OF trajectories are very likely to belong to the same object. Thus, we define the affinity between two detections as the intersection-overunion of their OF-trajectories, in the spirit of [9]. More details follow in the supplementary material.\nSo far we have discussed how we pre-assign object detections to path annotations c.f . Fig. 2b. In the following section, we describe how to obtain the most likely trajectory for each detection cluster via shortest-paths, c.f . Fig. 2c."
    }, {
      "heading" : "3.2.2 Detection linkage",
      "text" : "In this second step, the goal is to infer the final object trajectories. Finding the most probable detection-paths in a set of\ndetections has been well studied in the MOT literature [31]. We assume that the detection pre-labeling step has labeled the set of detections appropriately. So each detection can either be part of its assigned trajectory or a false positive, but it can not belong to another trajectory. Thus, we process each detection-cluster independently Fig. 2c and find the most probable detection-path in the cluster Fig. 2d.\nLet Ti be the final trajectory corresponding to detectioncluster i. It will be composed of a set of time-sorted detections x1 to xK . We find the most likely trajectory by minimizing the sum of detection-confidence costs and between-detections transition costs [54]. Fig. 3 shows how this can be intuitively interpreted as finding the shortestpath in a directed ST-graph where detections are represented by detection-confidence edges. The optimal detection-path will have the lowest cost:\nminimize T K∑ i=1 C(xi) + K−1∑ i=1 W (xi, xi+1) (4)\nwhere C is the detection-confidence cost. Ci follows the expression log((1 − si)/si), where si is the 0-to-1 scoreconfidence of the detection. Importantly, we use the same transition costs W when linking detections as we used in step one Eq. (1) for pre-labeling detections. Reusing pairwise costs makes the method more efficient. The detectionconfidence costs become negative for confident detections, encouraging the optimization to include them in the final position, while the transition costs penalize the association of detections which are unlikely to be connected. We refer the reader to [54] for details. The entry and exit nodes, S and T, are connected only to the earliest and latest detections in the cluster, respectively, ensuring that the trajectory has the same time span as its corresponding path annotation.\nAs result of the optimization we have a sparse detectionpath. Empirically we find the gap between detections to be small, 0.2 on average in our data. Thus we opt to linearly interpolate between detections to obtain the final trajectory, as per standard practice [53].\nUntil now we have presented our annotation approach using path supervision. It is useful for quickly annotating many sequences, particularly interesting for training data collection. We propose next an extension to incorpo-\nrate additional box annotations, improving trajectories up to ground-truth quality."
    }, {
      "heading" : "3.3. Incorporating box supervision",
      "text" : "We propose a simple yet effective way to extend our method with box annotations, to achieve ground-truth quality. Consider the detection-path we used to interpolate a trajectory. To include a box annotation, we simply add it to the path and then remove temporally close detections, those less than half a second away. Interpolating the updated detection-path produces the final trajectory. These fast updates progressively improve trajectory annotations as more box annotations are included. Our method is more accurate than the state of the art for any number of updates, as we show in the experiments."
    }, {
      "heading" : "4. The PathTrack dataset",
      "text" : "We use our annotation approach to collect a MOT dataset of unprecedented size. This PathTrack dataset is an important part of our contribution. We first provide an overview of the dataset in Sec. 4.1. Then, we describe how we crowdsourced the annotations in Sec. 4.2. We generate the final trajectory annotations with our approach, which associates R-CNN detections [41] with the help of path supervision. Importantly, we focus on training data in order to encourage research in fully data-driven trackers."
    }, {
      "heading" : "4.1. Dataset overview",
      "text" : "The PathTrack dataset consists of 720 sequences with a total of 16,287 trajectories of humans. Focusing on tracking humans allows us to collect more data for this specific class, which is of great interest both in the MOT community and in practical applications. The sequences are partitioned in a training set of 640 sequences with 15,380 trajectories and a test set of 80 sequences with 907 trajectories. Importantly, we allow a certain amount of noise in the training set annotations. This noise stems from inaccuracies in the path supervision and full-trajectory inference and has allowed us to annotate more sequences for a given time budget. Our experiments show that we can learn strong appearance models from large quantities of data even if the annotations are not perfectly clean (Sec. 5). Indeed, favoring quantity over\nquality when collecting training data has also been found to be beneficial for other tasks [52, 20]. Additional effort has been made for test annotations to be clean for evaluation purposes. Tab. 1 compares PathTrack with other popular MOT datasets. Compared to MOT15 [29], our dataset contains 33 times more sequences and 26 times more trajectory annotations available. We hope that the large scale of PathTrack encourages research in more data-driven tracking algorithms.\nDataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17]. With PathTrack, we aim to explore tracking in new types of sequences. We have thus collected a diverse set of sequences and we have labeled each one according to two criteria: a camera-movement label and one out of 7 scene labels, c.f . Fig. 4. There is a clear emphasis in street scenes and moving cameras, due to their challenge, ubiquity and general interest. But our dataset also allows focusing on static cameras or sequences with a lot of motion, such as sports and dancing. These fine-grained categories can also help to evaluate tracking under different conditions. Additional statistics that show the diversity of our data are presented in Fig. 4c. In the following sections, we describe how we crowdsourced the path annotations and detail in the supplementary material how we collected the videos."
    }, {
      "heading" : "4.2. Crowdsourcing path annotations",
      "text" : "A critical aspect of any annotation framework is whether it is easy to use. This is an often-overlooked factor that is vital if we want to crowdsource annotations. Path annotation is intuitive and straightforward. This has allowed us to crowdsource 16,287 path annotations of PathTrack using AMT. We now describe our interface and the measures we took to ensure the quality of our annotations.\nInterface Our interface features a video player with browsing capabilities and a list of the current annotations. The key difference with other interfaces is that ours records the path of the object by following the cursor. Additionally, the user easily can speed up and slow down the video, according to the speed of the object. In our measurements, path annotation was only 30% slower than watching the\nstreet (57%) dancing (13%) sports (9%) interview (9%)\nfashion (4%) event (2%)music (6%)\n16411 91 67 61 43 27\na) Camera-movement b) Scene-type label\nMoving\n512\nAlmost static\n116\nStatic\n89\nc) PathTrack statistics\nNormalized object heightAverage density [person/frame]\nVideo length [s] Track length [s]\n0\n2000\n4000\n6000\n8000\n10000\n0 20 40 60 80 100 1200 20 40 60 80 100 120 0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n50\n100\n150\n200\n250\n0 0 10 20 30 40 50 60 70 0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\n400000\n0 0.2 0.4 0.6 0.8 1\nFigure 4: Scene-label distribution in PathTrack. We show in a) the distribution of camera-movement labels. Almost three quarters of the sequences have been recorded with a moving camera. We show in b) the distribution of scene-type labels and corresponding examples. More than half of the sequences are street scenes. c) Statistics of PathTrack. Videos are up to 2 minutes long.\nvideo in real time. To further improve our final trajectories, we also asked the workers to provide a bounding box for the first and last appearance of each object and a third one in between. Since some sequences are very long and can contain dozens of people, the workers were allowed to partially annotate sequences. This also means that some workers received partially annotated videos and had to continue annotating them. This was not much of a challenge with our annotation framework. We have received very encouraging feedback from our workers, validating the ease of use of our interface and suggesting a potential for gamification. Here are some examples:\n“System was very easy to use and the normal speed was perfect for tracking each subject.”\n“I really enjoyed your hit. I like to do a lot of annotation work on mechanical turk and thought your interface was, once I got used to it, one of the best I have worked with.”\nQualification process After a short training video, c.f . supplementary material, each worker was asked to qualify by annotating the TUD-Stadtmitte sequence. The qualification certificate was only provided if the path and box annotations were similar to the ground truth up to a certain threshold. This was checked automatically.\nReviewing process If the users are not trained properly or the interface is cumbersome to use, crowdsourced annotations can be erroneous [47]. So we have made an extensive effort to review every single video and remove bad annotations. Videos with missing annotations were sent back to the annotation pool. We revoked the qualification of workers who continuously provided faulty annotations. Interestingly, only 3 out of our 81 workers were revoked, while previous work had difficulties collecting annotations of sufficient quality [45]. This further confirms that path annotation is an engaging and natural way to annotate trajectories."
    }, {
      "heading" : "5. Experiments",
      "text" : "We present our experiments in three parts. First we evaluate our annotation framework in Sec. 5.1. We then demonstrate in Sec. 5.2 its impact on training data collection for matching detections, which is a key problem of MOT [9] that is shared by most trackers. We finalize by evaluating the impact of our data on the Multi Object Tracking task."
    }, {
      "heading" : "5.1. Trajectory annotation efficiency",
      "text" : "In this section we evaluate the effectiveness and efficiency of path supervision and compare it to other trajectory annotation approaches.\nDataset description We evaluate our method on the MOT15 dataset [29] since it is most similar to our final goal, the generation of a massive MOT dataset. This dataset consists of 22 sequences, 11 of which belong to the training set. The sequences are challenging. Pedestrians are frequently occluded and some sequences have been recorded with a moving camera. We evaluate on the 521 trajectories of the training set, for which the ground truth is provided.\nState of the art We compare to other existing trajectory annotation approaches. LabelMe [53] is an effective framework based on linear interpolation between box annotations. The more sophisticated VATIC [47] learns an appearance model from the box annotation, which it uses for a shortest-paths interpolation. An additional extension of VATIC uses active learning to propose to the user which frame to annotate [46].\nEffectiveness of path supervision We first follow the standard evaluation of trajectory annotation frameworks [45]. In Fig. 5, we compare the annotation accuracy for different amounts of box annotations. Except for the active learning version of VATIC [46], box annotations are distributed uniformly in time, e.g., every 10, 5, 1 seconds. The performance of each framework is measured in terms of how many ground truth boxes are recalled,\nfor different Intersection-over-Union (IOU) [14] thresholds. Fig. 5 demonstrates the effectiveness of our path supervision: our cheap path supervision improves performance for any amount of box annotations. Interestingly, the annotation frameworks seem to converge in performance for large annotation budgets. A problem of this classical comparison is that it does not take into account the effort required to annotate path trajectories, i.e., it assumes that path annotations can be produced in real time, which is not always the case. We address evaluate time performance in the next section.\nAnnotation efficiency We compare the efficiency of path supervision with previous approaches using a common unit to measure effort: the annotation time. Our time measurements are based on a user study of 78 AMT workers and 13 vision-expert annotators. We consider three timeconsuming components: 1) watching the video at least once to identify the objects, 2) following each trajectory individually while annotating its boxes or path (for ours) and 3) the time required to annotate the bounding boxes. Our measurements revealed that box annotations take 5.2 seconds on average and that path annotations require slowing down the video by 33% on average. We provide a detailed explanation in the supplementary material. We use these time measurements to produce Fig. 6, where we compare the efficiency of our framework with the state of the art. Our\nmethod is efficient, as VATIC [47] and LabelMe [53] respectively require almost twice and three times more time to obtain our accuracy with only path supervision. We observe again how all methods converge to the same performance for a larger annotation budget, but ours is much more accurate for very small annotation-budgets.\nOverall, our framework is ideal for fast video annotation, which is desirable for generating large training sets, as we demonstrate in the next section."
    }, {
      "heading" : "5.2. Person matching",
      "text" : "We demonstrated in the previous section that path supervision is an efficient way to obtain accurate annotations in a short amount of time. We now explore the implications for a key task in MOT applications: person matching. This key problem consists of determining the likelihood that two detections belong to the same object in different frames Fig. 7a. There is a long tradition of handcrafted matching functions in the literature, with Convolutional Neural Networks (CNN) becoming more popular in the last few years. These models require extensive training data [26, 52], which we can provide with PathTrack. Learning tracker-specific components (e.g. entry/exit costs, mixing coefficients) is outside of the scope of this paper, but should be possible with our data.\nWe aim to answer the questions: i) does the tracking community benefit from more training data?, ii) for a limited budget, should we prioritize data quantity or quality?\nExperimental protocol We base our conclusions on a person matching network similar to SiameseCNN [28]. The network takes as input the crops of the two detections, resized to 121x53, and outputs a confidence score that they belong to the same object. These input crops are stacked, so the input volume is of 121x53x6. The network has a simple AlexNet style architecture of 3 convolutional and 2 fully connected layers [28]. In our evaluations, we sample 2 million training and test samples. Positives are randomly sampled pairs of detections that belong to the same object up to 6 seconds away. For each positive we sample a negative pair belonging to another trajectory in the same video. We use a learning rate of 0.001. In our experiments, we train this network with different data sources and compare their test accuracies. Accuracy refers to the percentage of properly classified pairs. We evaluate on the test set of PathTrack, for which the ground truth annotation is clean.\nImpact of training data In Fig. 7b we evaluate how the accuracy evolves as more training data becomes available. The left extreme corresponds to training on the 521 trajectories of the MOT15, which yields an accuracy of 78%. Training on the full 15,380 trajectories of PathTrack we improve the accuracy by 10%, almost halving the misclassification rate. This clearly shows the potential of PathTrack. More-\nover, we observe a certain effect of diminishing returns, but have not reached a plateau. If we use context features (e.g. relative distance, size) [28] in the network, we also see an improvement when using our data, from 84% to 90%. This shows that our data is useful to learn data-driven MOT.\nQuantity-over-quality annotation When collecting and annotating data for training purposes, a vital question is whether we should coarsely annotate a large amount of data or precisely annotate a small amount of data. That is, whether we should follow a quantity or a quality strategy. We estimate that it would take 22h to perfectly annotate the 11 videos in the training set of the MOT Challenge with LabelMe [53]. We reach this number by counting only the number of windows necessary to obtain an accuracy larger than 0.95 IoU. This represents the high-quality strategy. We compare this with a high-quantity strategy, in which, for the same annotation time, we annotate 140 videos of PathTrack with our framework, with path supervision and 3 boxes per trajectory. We show the results in Fig. 7b. A high quantity approach boosts the final accuracy from 78% to 85%. Interestingly, we can also use our method to quickly annotate the MOT 15 training set and train a model with exactly the same accuracy c.f . Fig. 7b. These results further showcase the benefit of our framework, which is ideal for fast annotation of large datasets. Other works [52, 20] have also found a quantity strategy to be advantageous to train deep models."
    }, {
      "heading" : "5.3. Multi Object Tracking",
      "text" : "In the previous section we demonstrated how we can train strong person-matching models with PathTrack. We now evaluate what impact this improvement has on MOT performance. We first use a standard tracker based on Linear Programming (LP) [54] and evaluate it on the test set of PathTrack with the standard CLEAR MOT metrics [7]. In Tab. 2a We compare the performance of this tracker with two different person-matching models: one trained\non MOT15 and the other on our data. Training on PathTrack substantially improves all the metrics. These also represent the first tracking results on our dataset. We further show the potential of PathTrack by improving the topperforming tracker in MOT15 [9] with our person-matching model c.f . Tab. 2b. More specifically, we use our discriminative person matcher to further link their trajectory results through occlusions, improving the number of ID Switches by 18% with 5% less fragments. Low-level details about the trackers are presented in the supplementary material."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this work, we propose a new framework to annotate trajectories in videos using path supervision, with the goal of generating massive MOT datasets. In the path supervision paradigm, the user annotates the position of the objects of interest with the cursor while watching the video. Our user study shows that this operation is efficient. Our optimization takes path annotations and object detections and outputs accurate box-trajectories. We show in our experiments that we can quickly generate large datasets with our path supervision. We use our approach to annotate PathTrack, a crowdsourced MOT dataset 33 times larger than currently available ones. Our experiments show that we can improve current person-matching deep models using our data and that this has an impact on MOT accuracy. We\nrelease PathTrack to promote research in richer and more complete tracking models."
    } ],
    "references" : [ {
      "title" : "Maria Florina Balcan and J",
      "author" : [ "X.R. Alireza Fathi" ],
      "venue" : "M. Rehg. Combining Self Training and Active Learning for Video Segmentation. In BMVC",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Label Propagation in Video Sequences",
      "author" : [ "V. Badrinarayanan", "F. Galasso", "R. Cipolla" ],
      "venue" : "CVPR",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Active Learning of an Action Detector from Untrimmed Videos",
      "author" : [ "S. Bandla", "K. Grauman" ],
      "venue" : "ICCV",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "What’s the Point: Semantic Segmentation with Point Supervision",
      "author" : [ "A. Bearman", "O. Russakovsky", "V. Ferrari", "L. Fei-Fei" ],
      "venue" : "ECCV",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The theory of dynamic programming",
      "author" : [ "R. Bellman" ],
      "venue" : "BAMS",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1954
    }, {
      "title" : "Evaluating multiple object tracking performance: The clear mot metrics",
      "author" : [ "K. Bernardin", "R. Stiefelhagen" ],
      "venue" : "EURASIP Journal on Image and Video Processing",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An Interactive Tool for Manual",
      "author" : [ "S. Bianco", "G. Ciocca", "P. Napoletano", "R. Schettini" ],
      "venue" : "Semi-automatic and Automatic Video Annotation. CVIU",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Near-Online Multi-Target Tracking With Aggregated Local Flow Descriptor",
      "author" : [ "W. Choi" ],
      "venue" : "In ICCV, December",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Minimizing human effort in interactive tracking by incremental learning of model parameters",
      "author" : [ "A. Ciptadi", "J.M. Rehg" ],
      "venue" : "ICCV",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Knowledge-driven Multimedia Information Extraction and Ontology Evolution",
      "author" : [ "S. Dasiopoulou", "E. Giannakidou", "G. Litos", "P. Malasioti", "Y. Kompatsiaris" ],
      "venue" : "chapter A Survey of Semantic Image and Video Annotation Tools, pages 196–239. Springer-Verlag, Berlin, Heidelberg",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A Crowdsourcing Approach to Support Video Annotation",
      "author" : [ "R. Di Salvo", "D. Giordano", "I. Kavasidis" ],
      "venue" : "VIGTA, VIGTA ’13, pages 8:1–8:6, New York, NY, USA",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "PETS2010: Dataset and Challenge",
      "author" : [ "A. Ellis", "J. Ferryman" ],
      "venue" : "AVSS, 00(undefined):143–150",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The pascal visual object classes (voc) challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding",
      "author" : [ "B.G. Fabian Caba Heilbron", "Victor Escorcia", "J.C. Niebles" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Two- Granularity Tracking: Mediating Trajectory and Detection Graphs for Tracking under Occlusions",
      "author" : [ "K. Fragkiadaki", "W. Zhang", "G. Zhang", "J. Shi" ],
      "venue" : "pages 552–565. Springer Berlin Heidelberg, Berlin, Heidelberg",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Virtual worlds as proxy for multi-object tracking analysis",
      "author" : [ "A. Gaidon", "Q. Wang", "Y. Cabon", "E. Vig" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite",
      "author" : [ "A. Geiger", "P. Lenz", "R. Urtasun" ],
      "venue" : "CVPR",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Geometric bounding box interpolation: an alternative for efficient video annotation",
      "author" : [ "P. Gil-Jiménez", "H. Gómez-Moreno", "R.J. López-Sastre", "S. Maldonado-Bascón" ],
      "venue" : "EURASIP J. Image and Video Processing, 2016:8",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Video2GIF: Automatic Generation of Animated GIFs from Video",
      "author" : [ "M. Gygli", "Y. Song", "L. Cao" ],
      "venue" : "CVPR",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Arm-Pointer: 3D Pointing Interface for Real- World Interaction",
      "author" : [ "E. Hosoya", "H. Sato", "M. Kitabata", "I. Harada", "H. Nojima", "A. Onozawa" ],
      "venue" : "pages 72–82. Springer Berlin Heidelberg, Berlin, Heidelberg",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Click Carving: Segmenting Objects in Video with Point Clicks",
      "author" : [ "S.D. Jain", "K. Grauman" ],
      "venue" : "CoRR, abs/1607.01115",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A Semi-automatic Tool for Detection and Tracking Ground Truth Generation in Videos",
      "author" : [ "I. Kavasidis", "S. Palazzo", "R. Di Salvo", "D. Giordano", "C. Spampinato" ],
      "venue" : "VIGTA, VIGTA ’12, pages 6:1–6:5, New York, NY, USA",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "What Energy Functions Can Be Minimized via Graph Cuts? In ECCV",
      "author" : [ "V. Kolmogorov", "R. Zabih" ],
      "venue" : "London, UK, UK",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The Visual Object Tracking VOT2015 Challenge Results",
      "author" : [ "M. Kristan", "J. Matas", "A. Leonardis", "M. Felsberg", "L. Cehovin", "G. Fernandez", "T. Vojir", "G. Hager", "G. Nebehay", "R. Pflugfelder" ],
      "venue" : "In ICCV Workshops,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Unsupervised Object Discovery and Tracking in Video Collections",
      "author" : [ "S. Kwak", "M. Cho", "I. Laptev", "J. Ponce", "C. Schmid" ],
      "venue" : "ICCV",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning by tracking: Siamese cnn for robust target association",
      "author" : [ "L. Leal-Taixé", "C. Canton-Ferrer", "K. Schindler" ],
      "venue" : "CVPR Workshop",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards a Benchmark for Multi- Target Tracking",
      "author" : [ "L. Leal-Taixé", "A. Milan", "I. Reid", "S. Roth", "K. Schindler" ],
      "venue" : "MOTChallenge",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Key-segments for video object segmentation",
      "author" : [ "Y.J. Lee", "J. Kim", "K. Grauman" ],
      "venue" : "ICCV, pages 1995–2002",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multiple Object Tracking: A Literature Review",
      "author" : [ "W. Luo", "J. Xing", "X. Zhang", "X. Zhao", "T.-K. Kim" ],
      "venue" : "arXiv preprint arXiv:1409.7618",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Spot On: Action Localization from Pointly-Supervised Proposals",
      "author" : [ "P. Mettes", "J.C. van Gemert", "C.G.M. Snoek" ],
      "venue" : "In ECCV",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "The Design and Implementation of ViPER",
      "author" : [ "D. Mihalcik", "D. Doermann" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2003
    }, {
      "title" : "MOT16: A Benchmark for Multi-Object Tracking",
      "author" : [ "A. Milan", "L. Leal-Taixé", "I. Reid", "S. Roth", "K. Schindler" ],
      "venue" : "[cs],",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2016
    }, {
      "title" : "A",
      "author" : [ "J. Niño-Castañeda" ],
      "venue" : "Frı́as-Velázquez, N. B. Bo, M. Slembrouck, J. Guan, G. Debard, B. Vanrumste, T. Tuytelaars, and W. Philips. Scalable Semi-Automatic Annotation for Multi- Camera Person Tracking. IEEE Transactions on Image Processing",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Efficient semantic video annotation by object and shot re-detection",
      "author" : [ "O.S.P. Schallauer", "H. Neuschmied" ],
      "venue" : "SAMT",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation",
      "author" : [ "F. Perazzi", "J. Pont-Tuset", "B. McWilliams", "L.V. Gool", "M. Gross", "A. Sorkine-Hornung" ],
      "venue" : "CVPR",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning object class detectors from weakly annotated video",
      "author" : [ "A. Prest", "C. Leistner", "J. Civera", "C. Schmid", "V. Ferrari" ],
      "venue" : "CVPR",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Does active learning work? A review of the research",
      "author" : [ "M. Prince" ],
      "venue" : "JEE, pages 223–231",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "YouTube-BoundingBoxes: A Large High-Precision Human- Annotated Data Set for Object Detection in Video",
      "author" : [ "E. Real", "J. Shlens", "S. Mazzocchi", "X. Pan", "V. Vanhoucke" ],
      "venue" : null,
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2017
    }, {
      "title" : "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "author" : [ "S. Ren", "K. He", "R. Girshick", "J. Sun" ],
      "venue" : "NIPS",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : "IJCV",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "AVSS2011 demo session: Real-time human detection using fast contour template matching for visual surveillance",
      "author" : [ "D. Shao", "M. Rauter", "C. Beleznai" ],
      "venue" : "AVSS, 00:514",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Active Frame Selection for Label Propagation in Videos",
      "author" : [ "S. Vijayanarasimhan", "K. Grauman" ],
      "venue" : "pages 496–509. Springer Berlin Heidelberg, Berlin, Heidelberg",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficiently Scaling Up Crowdsourced Video Annotation",
      "author" : [ "C. Vondrick", "D. Patterson", "D. Ramanan" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2013
    }, {
      "title" : "Video Annotation and Tracking with Active Learning",
      "author" : [ "C. Vondrick", "D. Ramanan" ],
      "venue" : "NIPS",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficiently Scaling Up Video Annotation with Crowdsourced Marketplaces",
      "author" : [ "C. Vondrick", "D. Ramanan", "D. Patterson" ],
      "venue" : "ECCV, pages 610–623, Berlin, Heidelberg",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Ensemble-Based Tracking: Aggregating Crowdsourced Structured Time Series Data",
      "author" : [ "N. Wang", "D. yan Yeung" ],
      "venue" : "JMLR Workshop and Conference Proceedings,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2014
    }, {
      "title" : "Interactive Offline Tracking for Color Objects",
      "author" : [ "Y. Wei", "J. Sun", "X. Tang", "H.-Y. Shum" ],
      "venue" : "ICCV",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning to track for spatio-temporal action localization",
      "author" : [ "P. Weinzaepfel", "Z. Harchaoui", "C. Schmid" ],
      "venue" : "In ICCV 2015 - IEEE International Conference on Computer Vision,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2015
    }, {
      "title" : "DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking",
      "author" : [ "L. Wen", "D. Du", "Z. Cai", "Z. Lei", "M. Chang", "H. Qi", "J. Lim", "M. Yang", "S. Lyu" ],
      "venue" : "arXiv CoRR, abs/1511.04136",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning From Massive Noisy Labeled Data for Image Classification",
      "author" : [ "T. Xiao", "T. Xia", "Y. Yang", "C. Huang", "X. Wang" ],
      "venue" : null,
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2015
    }, {
      "title" : "LabelMe video: Building a video database with human annotations",
      "author" : [ "J. Yuen", "B.C. Russell", "C. Liu", "A. Torralba" ],
      "venue" : "ICCV, pages 1451–1458. IEEE Computer Society",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Global data association for multi-object tracking using network flows",
      "author" : [ "L. Zhang", "Y. Li", "R. Nevatia" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "An example is the surge of Deep Learning thanks to ImageNet [26, 42].",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 40,
      "context" : "An example is the surge of Deep Learning thanks to ImageNet [26, 42].",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : "As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51].",
      "startOffset" : 76,
      "endOffset" : 88
    }, {
      "referenceID" : 27,
      "context" : "As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51].",
      "startOffset" : 76,
      "endOffset" : 88
    }, {
      "referenceID" : 32,
      "context" : "As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51].",
      "startOffset" : 76,
      "endOffset" : 88
    }, {
      "referenceID" : 49,
      "context" : "As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 36,
      "context" : "The cheapest way is to provide video-level object labels [38] or action labels [4].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "The cheapest way is to provide video-level object labels [38] or action labels [4].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 35,
      "context" : "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 42,
      "context" : "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 51,
      "context" : "The common approach to it is to annotate a sparse set of boxes and interpolate between them linearly [53] or with shortest-paths [45].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 43,
      "context" : "The common approach to it is to annotate a sparse set of boxes and interpolate between them linearly [53] or with shortest-paths [45].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 45,
      "context" : "it cost tens of thousands of dollars to annotate the VIRAT dataset [47].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "Virtual KITTI [17] - - - - - - 5* 4* 261* C* car-mounted KITTI [18] 21 13 29 18 - 50 30 - C + P car-mounted MOT15 [29] 11 6 500 11 10 721 22 16 1221 P S+M MOT16 [34] 7 4 512 7 4 830 14 8 1342 C+P** S+M PathTrack (ours) 640 161 15,380 80 11 907 720 172 16,287 P S+M 3 3",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 16,
      "context" : "Virtual KITTI [17] - - - - - - 5* 4* 261* C* car-mounted KITTI [18] 21 13 29 18 - 50 30 - C + P car-mounted MOT15 [29] 11 6 500 11 10 721 22 16 1221 P S+M MOT16 [34] 7 4 512 7 4 830 14 8 1342 C+P** S+M PathTrack (ours) 640 161 15,380 80 11 907 720 172 16,287 P S+M 3 3",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "Virtual KITTI [17] - - - - - - 5* 4* 261* C* car-mounted KITTI [18] 21 13 29 18 - 50 30 - C + P car-mounted MOT15 [29] 11 6 500 11 10 721 22 16 1221 P S+M MOT16 [34] 7 4 512 7 4 830 14 8 1342 C+P** S+M PathTrack (ours) 640 161 15,380 80 11 907 720 172 16,287 P S+M 3 3",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 32,
      "context" : "Virtual KITTI [17] - - - - - - 5* 4* 261* C* car-mounted KITTI [18] 21 13 29 18 - 50 30 - C + P car-mounted MOT15 [29] 11 6 500 11 10 721 22 16 1221 P S+M MOT16 [34] 7 4 512 7 4 830 14 8 1342 C+P** S+M PathTrack (ours) 640 161 15,380 80 11 907 720 172 16,287 P S+M 3 3",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "* [17] provides 10 different conditions (e.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 32,
      "context" : "** [34] provides a rich set of labels, such as whether an object is an occluder or a target is riding a vehicle.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 27,
      "context" : "This PathTrack dataset is our second major contribution: a large MOT dataset of more than 15,000 person trajectories in 720 sequences, 30 times more than currently available ones [29].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "NOMT [9].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "We show its potential by improving the top tracker on MOT15 [29].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "There is quite some work on multimedia annotation [11].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 46,
      "context" : "Of less relevance to us are those that work on videos with only a few people, such as [48, 35].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 33,
      "context" : "Of less relevance to us are those that work on videos with only a few people, such as [48, 35].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 31,
      "context" : "Hence, VIPER-GT [33] and LabelMe video [53] propose to linearly interpolate boxes between annotated keyframes.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 51,
      "context" : "Hence, VIPER-GT [33] and LabelMe video [53] propose to linearly interpolate boxes between annotated keyframes.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 45,
      "context" : "VATIC [47] uses this appearance model to define a graph on which it performs a shortestpath interpolation between manual annotations with Dynamic Programming [6].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : "VATIC [47] uses this appearance model to define a graph on which it performs a shortestpath interpolation between manual annotations with Dynamic Programming [6].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 47,
      "context" : "The shortest-path interpolation allows for larger time gaps without manual annotations, assuming that the object is clearly visible, and it can be efficient [49].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 44,
      "context" : "A VATIC improvement [46] incorporated active learning to decide which frames to annotate, to maximize the gain coming with such frames [39].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 37,
      "context" : "A VATIC improvement [46] incorporated active learning to decide which frames to annotate, to maximize the gain coming with such frames [39].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : "[10] built on top of shortest-path interpolation by updating the optimization weights with each extra annotation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Recently, [19] reconstructed annotated boxes and interpolated the final trajectories in 3D space.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 21,
      "context" : "Based on the aforementioned approaches, multiple annotation tools have been developed [23, 8, 36].",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "Based on the aforementioned approaches, multiple annotation tools have been developed [23, 8, 36].",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 34,
      "context" : "Based on the aforementioned approaches, multiple annotation tools have been developed [23, 8, 36].",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "Some gamify the annotation process [12].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 25,
      "context" : "[27].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "Path supervision Pointing at objects comes very natural and has often been used in human-computer interaction [21, 22], yet it only recently gained popularity in Computer Vision.",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 20,
      "context" : "Path supervision Pointing at objects comes very natural and has often been used in human-computer interaction [21, 22], yet it only recently gained popularity in Computer Vision.",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 30,
      "context" : "In parallel with our work, [32] found path annotations promising for action localization in videos.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 30,
      "context" : "Compared to [32, 50], we annotate dozens of people in highly-crowded sequences, ideal for MOT purposes.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 48,
      "context" : "Compared to [32, 50], we annotate dozens of people in highly-crowded sequences, ideal for MOT purposes.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Also recently, [5] and [22] used point supervision to segment objects in images and videos, resp.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 20,
      "context" : "Also recently, [5] and [22] used point supervision to segment objects in images and videos, resp.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 20,
      "context" : "[22] uses multiple points to segment, by iteratively re-ranking a collection of thousands of object proposals, called Click Carving.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Tracking datasets There is a corpus of video datasets that provide frame-level [15, 20] or pixel-level annotations [37].",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "Tracking datasets There is a corpus of video datasets that provide frame-level [15, 20] or pixel-level annotations [37].",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 35,
      "context" : "Tracking datasets There is a corpus of video datasets that provide frame-level [15, 20] or pixel-level annotations [37].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "[25] and [40] are the largest datasets for single object tracking.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[25] and [40] are the largest datasets for single object tracking.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 11,
      "context" : "Most large-scale MOT datasets are restricted to surveillance videos [13, 43, 51], since they depict smooth and quasi-linear trajectories that are easy to annotate.",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 41,
      "context" : "Most large-scale MOT datasets are restricted to surveillance videos [13, 43, 51], since they depict smooth and quasi-linear trajectories that are easy to annotate.",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 49,
      "context" : "Most large-scale MOT datasets are restricted to surveillance videos [13, 43, 51], since they depict smooth and quasi-linear trajectories that are easy to annotate.",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "More related to ours, KITTI [18] is collected from a car-mounted camera and focuses on pedestrians and vehicles.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "Parts of this dataset have been reproduced and rendered virtually, to show the potential of virtual datasets [17].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "[29, 34] have become the standard benchmarks for MOT, containing complex pedestrian scenes with static or moving cameras.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 32,
      "context" : "[29, 34] have become the standard benchmarks for MOT, containing complex pedestrian scenes with static or moving cameras.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "(1) can be solved with Graph Cuts [24] efficiently.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "These are obtained by linking pixels through time using frame-to-frame optical flow and forward-backward consistency checks [16].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "Thus, we define the affinity between two detections as the intersection-overunion of their OF-trajectories, in the spirit of [9].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 29,
      "context" : "Finding the most probable detection-paths in a set of detections has been well studied in the MOT literature [31].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 52,
      "context" : "We find the most likely trajectory by minimizing the sum of detection-confidence costs and between-detections transition costs [54].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 52,
      "context" : "We refer the reader to [54] for details.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 51,
      "context" : "Thus we opt to linearly interpolate between detections to obtain the final trajectory, as per standard practice [53].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 39,
      "context" : "We generate the final trajectory annotations with our approach, which associates R-CNN detections [41] with the help of path supervision.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 50,
      "context" : "Indeed, favoring quantity over quality when collecting training data has also been found to be beneficial for other tasks [52, 20].",
      "startOffset" : 122,
      "endOffset" : 130
    }, {
      "referenceID" : 18,
      "context" : "Indeed, favoring quantity over quality when collecting training data has also been found to be beneficial for other tasks [52, 20].",
      "startOffset" : 122,
      "endOffset" : 130
    }, {
      "referenceID" : 27,
      "context" : "Compared to MOT15 [29], our dataset contains 33 times more sequences and 26 times more trajectory annotations available.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 49,
      "context" : "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 32,
      "context" : "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 45,
      "context" : "Reviewing process If the users are not trained properly or the interface is cumbersome to use, crowdsourced annotations can be erroneous [47].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 43,
      "context" : "Interestingly, only 3 out of our 81 workers were revoked, while previous work had difficulties collecting annotations of sufficient quality [45].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "2 its impact on training data collection for matching detections, which is a key problem of MOT [9] that is shared by most trackers.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 27,
      "context" : "Dataset description We evaluate our method on the MOT15 dataset [29] since it is most similar to our final goal, the generation of a massive MOT dataset.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 51,
      "context" : "LabelMe [53] is an effective framework based on linear interpolation between box annotations.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 45,
      "context" : "The more sophisticated VATIC [47] learns an appearance model from the box annotation, which it uses for a shortest-paths interpolation.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 44,
      "context" : "An additional extension of VATIC uses active learning to propose to the user which frame to annotate [46].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 43,
      "context" : "Effectiveness of path supervision We first follow the standard evaluation of trajectory annotation frameworks [45].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 44,
      "context" : "Except for the active learning version of VATIC [46], box annotations are distributed uniformly in time, e.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "for different Intersection-over-Union (IOU) [14] thresholds.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 45,
      "context" : "Our method is efficient, as VATIC [47] and LabelMe [53] respectively require almost twice and three times more time to obtain our accuracy with only path supervision.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 51,
      "context" : "Our method is efficient, as VATIC [47] and LabelMe [53] respectively require almost twice and three times more time to obtain our accuracy with only path supervision.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "These models require extensive training data [26, 52], which we can provide with PathTrack.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 50,
      "context" : "These models require extensive training data [26, 52], which we can provide with PathTrack.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 26,
      "context" : "Experimental protocol We base our conclusions on a person matching network similar to SiameseCNN [28].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 26,
      "context" : "The network has a simple AlexNet style architecture of 3 convolutional and 2 fully connected layers [28].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : "relative distance, size) [28] in the network, we also see an improvement when using our data, from 84% to 90%.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 51,
      "context" : "We estimate that it would take 22h to perfectly annotate the 11 videos in the training set of the MOT Challenge with LabelMe [53].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 50,
      "context" : "Other works [52, 20] have also found a quantity strategy to be advantageous to train deep models.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 18,
      "context" : "Other works [52, 20] have also found a quantity strategy to be advantageous to train deep models.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 52,
      "context" : "We first use a standard tracker based on Linear Programming (LP) [54] and evaluate it on the test set of PathTrack with the standard CLEAR MOT metrics [7].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "We first use a standard tracker based on Linear Programming (LP) [54] and evaluate it on the test set of PathTrack with the standard CLEAR MOT metrics [7].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 27,
      "context" : "MOT15 [29] 24.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 7,
      "context" : "NOMTwSDP [9] 55.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "We further show the potential of PathTrack by improving the topperforming tracker in MOT15 [9] with our person-matching model c.",
      "startOffset" : 91,
      "endOffset" : 94
    } ],
    "year" : 2017,
    "abstractText" : "Progress in Multiple Object Tracking (MOT) has been historically limited by the size of the available datasets. We present an efficient framework to annotate trajectories and use it to produce a MOT dataset of unprecedented size. In our novel path supervision the annotator loosely follows the object with the cursor while watching the video, providing a path annotation for each object in the sequence. Our approach is able to turn such weak annotations into dense box trajectories. Our experiments on existing datasets prove that our framework produces more accurate annotations than the state of the art, in a fraction of the time. We further validate our approach by crowdsourcing the PathTrack dataset, with more than 15,000 person trajectories in 720 sequences1. Tracking approaches can benefit training on such large-scale datasets, as did object recognition. We prove this by re-training an off-the-shelf person matching network, originally trained on the MOT15 dataset, almost halving the misclassification rate. Additionally, training on our data consistently improves tracking results, both on our dataset and on MOT15. On the latter, we improve the top-performing tracker (NOMT) dropping the number of ID Switches by 18% and fragments by 5%.",
    "creator" : "LaTeX with hyperref package"
  }
}