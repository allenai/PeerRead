{
  "name" : "1611.05896.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Answering Image Riddles using Vision and Reasoning through Probabilistic Soft Logic",
    "authors" : [ "Somak Aditya", "Yezhou Yang", "Chitta Baral", "Yiannis Aloimonos" ],
    "emails" : [ "saditya1@asu.edu", "yz.yang@asu.edu", "chitta@asu.edu", "yiannis@cs.umd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We develop a Probabilistic Reasoning-based approach that utilizes probabilistic commonsense knowledge to answer these riddles with a reasonable accuracy. We demonstrate the results of our approach using both automatic and human evaluations. Our approach achieves some promising results for these riddles and provides a strong baseline for future attempts. We make the entire dataset and related materials publicly available to the community in ImageRiddle Website (http://bit.ly/22f9Ala)."
    }, {
      "heading" : "1. Introduction",
      "text" : "Figure 1. An Image Riddle Example. Question: “What word connects these images?” .\nA key component of computer vision is understanding of images and it comes up in various tasks such as image captioning and visual question answering (VQA). In this paper, we propose a new task of “image riddles” which requires deeper and conceptual understanding of images. In this task a set of images are provided and one needs to find a concept\n(described in words) that is invoked by all the images in that set. Often the common concept is not something that even a human can observe in her first glance but can come up with after some thought about the images. Hence the word “riddle” in the phrase “image riddles”. Figure 1 shows an example of an image riddle. The images individually connect to multiple concepts such as: outdoors, nature, trees, road, forest, rainfall, waterfall, statue, rope, mosque etc. On further thought, the common concept that emerges for this example is “fall”. Here, the first image represents the fall season (concept). There is a “waterfall” (region) in the second image. In the third image, it shows “rainfall” (concept) and the fourth image depicts that a statue is “fall”ing (action/event). The word “Fall” is invoked by all the images as it shows logical connections to objects, regions, actions or concepts specific to each image.\nIn addition, the answer also connects the most significant1 aspects of the images. Other possible answers like “nature” or “outdoors” do not demonstrate such properties. They are too general. In essence, image riddles is a challenging task that not only tests our ability to detect visual items in a set of images, but also tests our knowledge and our ability to think and reason.\nBased on the above analysis, we argue that a system should have the following capabilities to answer Image Riddles appropriately: i) the ability to detect and locate the objects, regions, and their properties; ii) the ability to recognize actions; iii) the ability to infer concepts from the detected words; and iv) the ability to rank a concept (described in words) based on its relative appropriateness; in other words, the ability to reason with and process background or commonsense knowledge about the semantic similarity and relations between words and phrases. These capabilities, in fact, are also desired of any automated system that aims to understand a scene and answer questions about it. For example, in VQA dataset [1], “Does this man have children”, “Is this a vegetarian Pizza?” are some such examples, where one needs explicit commonsense knowledge.\n1Formally, an aspect is as significant as the specificity of the information it contains.\nar X\niv :1\n61 1.\n05 89\n6v 1\n[ cs\n.C V\n] 1\n7 N\nov 2\n01 6\nThese riddles can be thought of as a visual counterpart to IQ test question types such as sequence filling (x1, x2, x3, ?) and analogy solving (x1 : y1 :: x2 : ?)2 where one needs to find commonalities between items. This task is different from traditional VQA, as in VQA the queries provide some clues regarding what to look for in the image in question. Most riddles in this task require both superior detection and reasoning capabilities, whereas a large percentage (of questions) of the traditional VQA dataset tests system’s detection capabilities. This task differs from both VQA and Captioning in that this task requires analysis of multiple images. While video analysis may require analysis of multiple images, this task of “image riddles” focuses on analysis of seemingly different images.\nHence, this task of Image Riddles is simple to explain; shares similarities with well-known and pre-defined types of IQ questions and it requires a combination of vision and reasoning capabilities. In this paper, we introduce a promising approach in tackling the problem.\nIn our approach, we first use state-of-the-art Image Classification techniques [21] to get the top identified classlabels from each image. Given these probabilistic detections, we use the knowledge of connections and relations of these words to infer a set of most probable words (or phrases). We use ConceptNet 5 [15] as the source of commonsense and background knowledge that encodes the relations between words and short phrases using a structured graph. Note, the possible range of candidates are the entire vocabulary of ConceptNet 5 (roughly 0.2 million). For representation and reasoning with this huge probabilistic knowledge we use the Probabilistic Soft Logic (PSL) [10, 2] framework3. Given the inferred words for each image, we then infer the final set of answers for each riddle.\nOur contributions are threefold: i) we introduce the 3K Image Riddles Dataset; ii) we present a probabilistic reasoning approach to solve the riddles with reasonable accuracy; iii) our reasoning module inputs detected words (a closed set of class-labels) and logically infers all relevant concepts (belonging to a much larger vocabulary)."
    }, {
      "heading" : "2. Related Work",
      "text" : "The problem of Image Riddles has some similarities to the genre of topic modeling [3] and Zero-shot Learning [13]. However, this dataset imposes a few unique challenges: i) the possible set of target labels is the entire Natural Language vocabulary; ii) each image, when grouped with different set of images can map to a different label; iii) almost all the target labels in the dataset are unique (3k examples with 3k class-labels). These challenges make it hard\n2Examples are: word analogy tasks (male : female :: king : ?); numeric sequence filling tasks: (1, 2, 3, 5, ?).\n3PSL is shown to be a powerful framework for high-level Computer Vision tasks like Activity Detection [16].\nto directly adopt topic model-based or Zero-shot learningbased approaches.\nOur work is also related to the field of Visual Question Answering. Very recently, researchers spent a significant amount of efforts on both creating datasets and proposing new models [1, 18, 6, 17]. Interestingly both [1] and [6] adapted MS-COCO [14] images and created an open domain dataset with human generated questions and answers. Both [18] and [6] use recurrent networks to encode the sentence and output the answer.\nEven though some questions from [1] and [6] are very challenging which actually require logical reasoning in order to answer correctly, popular approaches are still hoping to learn the direct signal-to-signal mapping from image and question to its answer, given a large enough annotated data. The necessity of common-sense reasoning could be easily neglected. Here we introduce the new Image Riddle problem which is 1) a well-defined cognitively challenging task that requires both vision and reasoning capability, 2) it is impossible to model the problem as direct signal-to-signal mapping, due to the data sparsity and 3) system’s performance could still be bench-marked automatically for comparison. All these qualities make our Image Riddle dataset a good testbed for vision and reasoning research."
    }, {
      "heading" : "3. Background",
      "text" : "In this Section, we briefly introduce the different techniques and Knowledge Sources used in our system."
    }, {
      "heading" : "3.1. Probabilistic Soft Logic (PSL)",
      "text" : "PSL is a recently proposed framework for Probabilistic Logic [10, 2]. A PSL model is defined using a set of weighted if-then rules in first-order logic.\nLet C = (C1, ..., Cm) be such a collection where each Cj is a disjunction of literals, where each literal is a variable yi or its negation ¬yi, where yi ∈ y. Let I+j (resp. I − j ) be the set of indices of the variables that are not negated (resp. negated) in Cj . Each Cj can be written as: wj : ∧i∈I−j yi → ∨i∈I+j yi (1) or equivalently, wj : ∨i∈I−j (¬yi) ∨ ∨i∈I+j yi. Each rule Cj is associated with a non-negative weight wj . PSL relaxes the boolean truth values of each ground atom a (constant term or predicate with all variables replaced by constants) to the the interval [0, 1], denoted by I(a). To compute soft truth values for logical formulas, Lukasiewiczs relaxation [11] of conjunctions (∧), disjunctions (∨) and negations (¬) is used :\nI(l1 ∧ l2) = max{0, I(l1) + I(l2)− 1} I(l1 ∨ l2) = min{1, I(l1) + I(l2)}\nI(¬l1) = 1− I(l1) (2)\nIn PSL, the ground atoms are considered as random variables and the distribution is modeled using Hinge-Loss\nMarkov Random Field, which is defined as follows:\nDefinition 3.1. Let y and x be two vectors of n and n′ random variables respectively, over the domain D = [0, 1]n+n ′ . The feasible set D̃ is a subset of D, defined as:\nD̃ = { (y,x) ∈ D ∣∣ck(y,x)=0,∀k∈E ck(y,x)≤0,∀k∈I } where c = (c1, ..., cr) are linear constraint functions associated with the index sets E and I denoting equality and inequality constraints. A Hinge-Loss Markov Random Field P is a probability density, defined as: if (y,x) /∈ D̃, then P(y|x) = 0; if (y,x) ∈ D̃, then:\nP(y|x) = 1 Z(w,x) exp(−fw(y,x)) (3)\nwhere Z(w,x) = ∫ y|(y,x)∈D̃ exp(−fw(y,x))dy.\nThe hinge-loss energy function fw is defined as: fw(y,x) = m∑ j=1 wj(max{lj(y,x), 0})pj , where wj’s are non-negative free parameters and lj(y,x) are linear constraints over y,x and pj = {1, 2}.\nThe final inference objective of HL-MRF is:\nP(y|x) ≡ argmin y∈[0,1]n m∑ j=1 wj(max{lj(y,x), 0})pj (4)\nIn PSL, each logical rule Cj in the databaseC is used to define lj(y,x) i.e. the linear constraints over (y,x). Given a set of weighted logical formulas, PSL builds a graphical model defining a probability distribution over the continuous space of values of the random variables in the model.\nThe final optimization problem is defined in terms of “distance to satisfaction”. For each rule Cj ∈ C this distance to satisfaction is measured using the term wj × max { 1 − ∑ i∈I+j yi − ∑ i∈I−j (1 − yi), 0 } . This encodes the penalty to the system if a rule is not satisfied. The final optimization problem becomes:\nargmin y∈[0,1]n ∑ Cj∈C wj max { 1− ∑ i∈I+j yi − ∑ i∈I−j (1− yi), 0 } (5)"
    }, {
      "heading" : "3.2. ConceptNet",
      "text" : "ConceptNet [22], is a multilingual Knowledge Graph, that encodes commonsense knowledge about the world and is built primarily to assist systems that attempts to understand natural language text. The knowledge in ConceptNet is semi-curated. The nodes (called concepts) in the graph are words or short phrases written in natural language. The nodes are connected by edges (called assertions) which are labeled with meaningful relations (selected from a welldefined closed set of relation-labels). For example: (reptile, IsA, animal), (reptile, HasProperty, cold blood) are some edges. Each edge has an associated confidence score. Also, compared to other knowledge-bases like WordNet, YAGO,\nNELL [23, 20]; ConceptNet has a more extensive coverage of English language words and phrases. These properties make this Knowledge Graph a perfect source for the required probabilistic commonsense knowledge."
    }, {
      "heading" : "3.3. Word2vec",
      "text" : "Word2vec uses the theory of distributional semantics4 to capture word meanings and produce word embeddings (vectors). The pre-trained word-embeddings have been successfully used in numerous Natural Language Processing applications and the induced vector-space is known to capture the graded similarities between words with reasonable accuracy [19]. Throughout the paper, for word2vec-based similarities, we use the 3 Million word-vectors trained on Google-News corpus [19]."
    }, {
      "heading" : "4. Approach",
      "text" : "Given a set of images (in our case four: {I1, I2, I3, I4}), the objective is to determine a set of ranked words (T ) based on how well the word semantically connects these image. In this work, we present an approach that uses Probabilistic Reasoning on top of a probabilistic Knowledge Base (ConceptNet). It also uses additional semantic knowledge of words from Word2vec. Using these knowledge sources, we predict the answers to the riddles."
    }, {
      "heading" : "4.1. Outline of our Framework",
      "text" : "Algorithm 1. Solving Riddles 1: procedure UNRIDDLER(I = {I1, I2, I3, I4},Kcnet) 2: for Ik ∈ I do 3: P̃ (Sk|Ik) = getClassLabelsNeuralNetwork(Ik). 4: for s ∈ Sk do 5: Ts,Wm(s,Ts) = retrieveTargets(s,Kcnet); .\nWm(s, tj) = sim(s, tj)∀tj ∈ Ts 6: end for 7: Tk = rankTopTargets(P̃ (Sk|Ik),TSk ,Wm); 8: I(T̂k) = inferConfidenceStageI(Tk, P̃ (Sk|Ik)). 9: end for\n10: I(T ) = inferConfidenceStageII([T̂k]4k=1, [P̃ (Sk|Ik)] 4 k=1). 11: end procedure\nAs outlined in algorithm 1, for each image Ik (here, k ∈ {1, ..., 4}), we follow three stages to infer related words and phrases: i) Image Classification: we get top class labels and the confidence from Image Classifier (Sk, P̃ (Sk|Ik)), ii) Rank and Retrieve: using these labels and confidence scores, we rank and retrieve top related words from ConceptNet (Kcnet), iii) Probabilistic Reasoning and Inference (Stage I): using the labels (Sk) and the top related words (Tk), we design an inference model to logically infer final set of words (T̂k) for each image. Lastly, we use another probabilistic reasoning model (Stage II) on the combined set of inferred words (targets) from all images in a riddle.\n4The central idea is: “a word is known by the company it keeps”.\nThis model assigns the final confidence scores on the combined set of targets (T ). The pipeline followed for each image is depicted with an example in Figure 2."
    }, {
      "heading" : "4.2. Image Classification",
      "text" : "Neural Networks trained on ample source of images and numerous image classes has been very effective. Studies have found that convolutional neural networks (CNN) can produce near human level image classification accuracy [12], and related work has been used in various visual recognition tasks such as scene labeling [5] and object recognition [7]. To exploit these advances, we use the stateof-the-art class detections provided by the Clarifai API [21] and the Deep Residual Network Architecture by [8] (using the trained ResNet-200 model). For each image (Ik) we use top 20 detections (Sk). Let us call these detections as seeds. An example is provided in the Figure 2. Each detection is accompanied with the classifier’s confidence score (P̃ (Sk|Ik))."
    }, {
      "heading" : "4.3. Rank and Retrieve Related Words",
      "text" : "Our goal is to logically infer words or phrases that represent (higher or lower-level) concepts that can best explain the co-existence of the seeds in a scene. Say, for “hand” and “care”, implied words could be “massage”, “ill”, “ache” etc. For “transportation” and “sit”, implied words/phrases could be “sit in bus”, “sit in plane” etc. The reader might be inclined to infer other concepts. However, to “infer” is to derive “logical” conclusions. Hence, we prefer the concepts which shares strong explainable connections with the seedwords.\nA logical choice would be traversing a knowledge-graph like ConceptNet and find the common reachable nodes from these seeds. As this is computationally quite infeasible, we use the association-space matrix representation of ConceptNet, where the words are represented as vectors. The similarity between two words approximately embodies the strength of the connection over all paths connecting the two words in the graph. We get the top similar words for each seed, approximating the reachable nodes."
    }, {
      "heading" : "4.3.1 Retrieve Related Words For a Seed",
      "text" : "Visual Similarity: We observe that, for objects, the ConceptNet-similarity gives a poor result (See Table 1). So, we define a metric called visual similarity. Let us call the similar words as targets. In this metric, we represent the seed and the target as vectors. To define the dimensions, for each seed, we use a set of relations (HasA, HasProperty, PartOf and MemberOf). We query ConceptNet to get the related words (say, W1,W2,W3...) under such relations for the seed-word and its superclasses. Each of these relation-word pairs (i.e. HasA-W1,HasA-W2,PartOf-W3,...) becomes a separate dimension. The values for the seed-vector are the weights assigned to the assertions. For each target, we query ConceptNet and populate the target-vector using the edge-weights for the dimensions defined by the seed-vector.\nTo get the top words using visual similarity, we use the cosine similarity of the seed-vector and the target-vector to re-rank the top 10000 retrieved similar target-words using ConceptNet-similarity. For abstract seed-words, we do not get any such relations and we use the ConceptNet similarity directly. Table 1 shows the top similar words using\nConceptNet, word2vec and visual-similarity for the word “men”. Moreover, the ranked list based on visual-similarity ranks boy, chap, husband, godfather, male person, male in the ranks 16 to 22.\nFormulation: For each seed (s), we get the top words (Ts) from ConceptNet using the visual similarity metric and the similarity vector Wm(s,Ts). Together for an image, these constitute TSk and the matrix Wm, where Wm(si, tj) = simvis(si, tj)∀si ∈ Sk, tj ∈ TSk . Next we describe the defined similarity metric.\nA large percentage of the error in Image Classifiers are due to visually similar (or semantically similar) objects or objects from the same category [9]. In such cases, we use this visual similarity metric to predict the possible visually similar objects and then use an inference model to infer the actual object."
    }, {
      "heading" : "4.3.2 Rank Targets",
      "text" : "We use P̃ (Sk|Ik) as an approximate vector representation for the image, in which the seed-words are the dimensions. The columns of Wm provides vector representations for the target words (t ∈ TSk ) in the space. We calculate cosine similarities for each target with such a image-vector and then re-rank the targets. We consider the top θ#t targets and we call it Tk."
    }, {
      "heading" : "4.4. Probabilistic Reasoning and Inference",
      "text" : ""
    }, {
      "heading" : "4.4.1 PSL Inference Stage I",
      "text" : "Given a set of candidate targets Tk and a set of weighted seeds (Sk, P̃ (Sk|Ik)), we build an inference model to infer a set of most probable targets (T̂k). We model the joint distribution using PSL as this formalism adopts Markov Random Field which obeys the properties of Gibbs Distribution. In addition, a PSL model is declared using rules. Given the final answer from the system, the set of satisfied (grounded) rules show the logical connections between the detected words and the final answer, which demonstrates the system’s explainability.\nThe PSL model can be best depicted as an Undirected Graphical Model involving seeds and targets, as given in Figure 3.\nFormulation: Using PSL, we add two sets of rules: i) to define seed-target potentials, we add rules of the form wtij : sik → tjk for each word sik ∈ Sk and target tjk ∈ Tk; ii) to define target-target potentials, for each target tjk, we take the most similar θt-t targets (Tmaxj ). For each target tjk and each tmk ∈ Tmaxj , we add two rules wtjm : tjk → tmk and wtjm : tmk → tjk. Next, we describe the choices in detail.\ni) From the perspective of optimization, the rule wtij : sik → tjk adds the term wtij ∗ max{I(sik) − I(tjk), 0} to the objective. This means that if confidence score of the target tjk is not greater than I(sik) (i.e. P̃ (Sk|Ik)), then the rule is not satisfied and we penalize the model by wtij times the difference between the confidence scores. We add the above rule for seeds and targets for which the combined weighted similarity exceeds certain threshold θsim,psl1.\nWe encode the commonsense knowledge of words and phrases obtained from different knowledge sources into the weights of these rules wtij . Both the knowledge sources are considered because ConceptNet embodies commonsense knowledge and word2vec encodes word-meanings. It is also important that the inference model is not biased towards more popular targets (i.e. abstract words or words too commonly used/detected in corpus). We compute eigenvector centrality score (C(.)) for each word in the context of ConceptNet (a network of words and phrases). Higher C(.)\nindicates higher connectivity of a word in the graph. This yields a higher similarity score to many words and might give an unfair bias to this target in the inference model. Hence, the higher the C(.), the word provides less specific information for an image. Hence, the weight becomes\nwtij = θα1 ∗ simcn(sik, tjk)+ θα2 ∗ simw2v(sik, tjk) + 1/C(tjk),\n(6)\nwhere simcn(., .) is the normalized ConceptNet-based similarity. simw2v(., .) is the normalized word2vec similarity of two words and C(.) is the eigenvector-centrality score of the argument in the ConceptNet matrix.\nii) To model dependencies among the targets, we observe that if two concepts t1 and t2 are very similar in meaning, then a system that infer t1 should infer t2 too, given the same set of observed words. Therefore, the two rules wtjm : tjk → tmk and wtjm : tmk → tjk are designed to force the confidence values of tjk and tmk to be as close to each other as possible. wtjm is the same as Equation 6 without the penalty for popularity.\nThe combined PSL model inference objective becomes:\nargmin I(Tk)∈[0,1]|Tk| ∑ sik∈Sk ∑ tjk∈Tk wtij max { I(sik)− I(tjk), 0 } +\n∑ tjk∈Tk ∑ tmk∈Tmaxj wtjm { max { I(tmk)− I(tjk), 0 } +\nmax { I(tjk)− I(tmk), 0 }} .\nTo let the targets compete against each other, we add a constraint on the sum of the confidence scores of the targets i.e. ∑ j:tjk∈Tk I(tjk) ≤ θsum1. Here θsum1 ∈ {1, 2} and I(tjk) ∈ [0, 1]. As a result of this model, we get an inferred reduced set of targets [T̂k]4k=1."
    }, {
      "heading" : "4.4.2 PSL Inference Stage II",
      "text" : "To learn the most probable set of common targets jointly, we consider the targets and the seeds from all images together. Assume that the seeds and the targets are nodes in a knowledge-graph. Then, the most appropriate targetnodes should observe similar properties as an appropriate answer to the riddle: i) a target-node should be connected to the high-weight seeds in an image i.e. should relate to the important aspects of the image; ii) a target-node should be connected to seeds from all images.\nFormulation: Here, we use the rules wtij : sik → tjk for each word sik ∈ Sk and target tjk ∈ T̂k for all k ∈ {1, 2.., 4}. To let the set of targets compete against each other, we add the constraint ∑4 k=1 ∑ j:tjk∈T̂k I(tjk) ≤ θsum2. Here θsum2 = 1 and I(tjk) ∈ [0, 1]. To minimize the penalty for each rule, the optimal solution will try to maximize the confidence score of tjk. To\nminimize the overall penalty, it should maximize the confidence scores of those targets which will satisfy most of the rules (or rules with maximum total weight). As the summation of confidence scores is bounded, only a few top inferred targets should have non-zero confidence."
    }, {
      "heading" : "5. Experiments and Results",
      "text" : "In this section, we provide the results of the validation experiments of the newly introduced Image Riddle dataset, followed by empirical evaluation of the proposed approach against vision-only baselines."
    }, {
      "heading" : "5.1. Dataset Validation and Analysis",
      "text" : "We have collected a set of 3333 riddles from the internet (puzzle websites). Each riddle has 4 images (66 × 66, 6KB in size) and a groundtruth label associated with it. To verify the groundtruth answers, we define the metrics: i) “correctness” - how correct and appropriate the answers are, and ii) “difficulty” - how difficult are the riddles. We conduct an Amazon Mechanical Turker-based evaluation. We ask them to rate the correctness from 1-65. The “difficulty” is rated from 1-76. According to the Turkers, the mean correctness rating is 4.4 (with Standard Deviation 1.5). The “difficulty” ratings show the following distribution: toddler (0.27%), younger child (8.96%), older child (30.3%), teenager (36.7%), adult (19%), linguist (3.6%), no-one (0.64%). In short, the average age to answer the riddles seems to be closer to 13-17yrs. Also, few of these (4.2%) riddles seem to be incredibly hard. Interestingly, the average age perceived reported for the recently proposed VQA dataset [1] is 8.92 yrs. Although, this experiment measures “the turkers’ perception of the required age”, one can conclude that the riddles are comparably harder."
    }, {
      "heading" : "5.2. System Evaluation",
      "text" : "The presented approach suggests the following hypotheses that requires empirical tests: I) the proposed approach (and their variants) attain reasonable accuracy in solving the riddles; II) the individual stages of the framework improves the final inference accuracy of the answers. In addition, we also experiment to observe the effect of using commercial classification methods like Clarifai against a published state-of-the-art Image Classification method.\n51: Completely gibberish, incorrect, 2: relates to one image, 3 and 4: connects two and three images respectively, 5: connects all 4 images, but could be a better answer, 6: connects all images and an appropriate answer.\n6These gradings are adopted from VQA AMT instructions [1]. 1: A toddler can solve it (ages:3-4), 2: A younger child can solve it (ages:58), 3: A older child can solve it (ages:9-12), 4: A teenager can solve it (ages:13-17), 5: An adult can solve it (ages:18+), 6: Only a Linguist (one who has above-average knowledge about English words and the language in general) can solve it, 7: No-one can solve it."
    }, {
      "heading" : "5.2.1 Systems",
      "text" : "We propose several variations of the proposed approach and compare them with a simple vision-only baseline (hypothesis I). We introduce an additional Bias-Correction stage after the Image Classification, which aims to re-weight the detected seeds using additional information from other images. The variations then, are created to test the effects of varying the Bias-Correction stage and the effects of the individual stages of the framework on the final accuracy (hypothesis II). We also vary the initial Image Classification Method (Clarifai, Deep Residual Network).\nBias-Correction: We experimented with two variations: i) greedy bias-correction and ii) no bias-correction. We follow the intuition that the re-weighting of the seeds of one image can be influenced by the others7. To this end, we develop the “GreedyUnRiddler” (GUR) approach. In this approach, we consider all of the images together to dictate the new weight of each seed. Take image Ik for example. To reweight seeds in Sk, we calculate the weights using the following equation: W̃ (sk) = ∑ j∈1,..4 simcosine(Vsk,j ,Vj)\n4.0 . Vj is vector of the weights assigned P̃ (Sj |Ij) i.e. confidence scores of each seed in the image. Each element of Vsk,j [i] is the ConceptNet-similarity score between the seed sk and si,j i.e. the ith seed of the jth image. The re-weighted seeds (Sk, W̃ (Sk)) of an image are then passed through the rest of the pipeline to infer the final answers.\nIn the original pipeline (“UnRiddler”,in short UR), we just normalize the weights of the seeds and pass on to the next stage. We experiment with another variation (called BiasedUnRiddler or BUR), the results of which are included in appendix, as GUR achieves the best results.\nEffect of Stages: We observe the accuracy after each stage in the pipeline (VB: Upto Bias Correction, RR: Upto Rank and Retrieve stage, All: The entire Pipeline). For VB, we use the normalized weighted seeds, get the weighted centroid vector over the word2vec embeddings of the seeds for each image. Then we obtain the mean vector over these centroids. The top similar words from the word2vec vocabulary to this mean vector, constitutes the final answers. For RR, we get the mean vector over the top predicted targets for all images. Again, the most similar words from the word2vec vocabulary constitutes the answers.\nBaseline: We create Vision-only Baselines. We directly use the class-labels and the confidence scores predicted using a Neural Network-based Classifier. For each image, we calculate the weighted centroid of the word2vec embeddings of these labels and the mean of these centroids for the 4 images. For the automatic evaluation we use this centroid and for the human evaluation, we use the most similar word to this vector, from the word2vec vocabulary. The Baseline\n7A person would often skim through all the images at one go and will try to come up with the aspects that needs more attention.\nperformances are listed in Table 2 in the VB+UR cells."
    }, {
      "heading" : "5.2.2 Experiment I: Automatic Evaluation",
      "text" : "We evaluate the performance of the proposed approach on the 3333 Image Riddles dataset using both automatic and Amazon Mechanical Turker (AMT)-based evaluations.\nAs an evaluation metric, we use word2vec similarity measure. An answer to a riddle may have several semantically similar answers. Hence it is reasonable to use such a metric. For each riddle, we calculate the maximum similarity between the groundtruth and top 10 detections from an approach. To calculate phrase similarities, we use n similarity method of the gensim.models.word2vec package. The average of such maximum similarities is reported in percentage form.\nTo select the parameters in the parameter vector θ, We employed a random search on the parameter-space over first 500 riddles over 500 combinations. The final set of parameters used and their values are tabulated in Table 3.\nEach of the stage-variants (VB, RR and All) are combined with different variations of the Bias-Correction stage (GUR and UR respectively). The accuracies on all are listed in Table 2. We provide our experimental results on this 3333 riddles and 2833 riddles (barring 500 riddles we used for the parameter search)."
    }, {
      "heading" : "5.2.3 Experiment II: Human Evaluation",
      "text" : "We conduct an AMT-based comparative evaluation of the results of the proposed approach (GUR+All using Clarifai) and two vision-only baselines. We define two metrics: i) “correctness” and ii) “intelligence”. Turkers are presented\nwith a scenario: We have three separate robots that attempted to answer this riddle. You have to rate the answer based on the correctness and the degree of intelligence (explainability) shown through the answer.. The correctness is defined as before. In addition, turkers are asked to rate intelligence in a scale of 1-48. We plot the the percentage of total riddles per each value of correctness and intelligence in Figure 4. In these histograms plots, we expect a increase in the rightmost buckets for the more “correct” and “intelligent” systems.\n.Figure 4. AMT Results of The GUR+All (our), Clarifai (baseline 1) and ResidualNet (baseline 2) approaches. Correctness Means are: 2.6 ± 1.4, 2.4 ± 1.45, 2.3 ± 1.4. For Intelligence: 2.2 ± 0.87, 2± 0.87, 1.8± 0.8"
    }, {
      "heading" : "5.2.4 Analysis",
      "text" : "Experiment I shows that the GUR variant (GUR+All in Table 2) achieves the best results in terms of word2vec-based accuracy. Similar trend is reflected in the AMT-based evaluations (Figure 4). Our system has increased the percentage of puzzles for the rightmost bins i.e. produces more “correct” and “intelligent” answers for more number of puzzles. The word2vec-based accuracy puts the performance of ResNet baseline close to that of the GUR variant. However, as evident from Figure 4, the AMT evaluation of the correctness shows clearly that the ResNet baseline lags in predicting meaningful answers. Experiment II also includes what the turkers think about the intelligence of the systems that tried to solve the puzzles. This also puts the GUR variant at the top. The above two experiments empirically show\n81: Not intelligent, 2: Moderately Intelligent, 3: Intelligent, 4: Very Intelligent.\nthat our approach achieves a reasonable accuracy in solving the riddles (Hypothesis I). In table 2, we observe how the accuracy varies after each stage of the pipeline (hypothesis II). The table shows a jump in the accuracy after the RR stage, which leads us to believe the primary improvement of our approach is attributed to the Probabilistic Reasoning model. We also provide our detailed results for the “GUR” approach using a few riddles in Figure 5."
    }, {
      "heading" : "6. Conclusion and Future Works",
      "text" : "In this work, we presented a Probabilistic Reasoning based approach to solve a new class of image puzzles, called “Image Riddles”. We have collected over 3k such riddles. Crowd-sourced evaluation of the dataset demonstrates the validity of the annotations and the nature of the difficulty of the riddles. We empirically show that our approach\nimproves on vision-only baselines and provides a stronger baseline for future attempts.\nThe task of “Image Riddles” is equivalent to conventional IQ test questions such as analogy solving, sequence filling; which are often used to test human intelligence. This task of “Image Riddles” is also in line with the current trend of VQA datasets which require visual recognition and reasoning capabilities. However, it focuses more on the combination of both vision and reasoning capabilities. In addition to the task, the proposed approach introduces a novel inference model to infer related words (from a large vocabulary) given class labels (from a smaller set), using semantic knowledge of words. This method is general in terms of its applications. Systems such as [24], which use a collection of high-level concepts to boost VQA performance; can benefit from this approach."
    }, {
      "heading" : "A. BiasedUnRiddler (BUR): A Variation of the",
      "text" : "BiasCorrection Stage\nIn Figure 6: dinosaur, animal and reptile all provide evidence that the image has an animal. Only the word dinosaur indicates what kind of animal is in the image. The other words do not add any additional information. Some highconfidence detections also provide erroneous abstract information. Here, the labels monstrous, monster are some such detections. Hence, the objective is to re-weight the seeds so that: i) the more specific seed-words should have higher weight than the ones which provide similar but more general information; ii) the seeds that are too frequently used or detected in corpus, should be given lower weights.\nSpecificity and Popularity: We compute eigenvector centrality score (ECS) for each word in the context of ConceptNet. Higher ECS indicates higher connectivity and yields a higher similarity score to many words and might give an unfair bias to this seed (and words implied by this seed) in the inference model. Hence, the higher the ECS, the word provides less specific information for an image. Additionally, we use the concreteness rating (CR) from [4]. In this paper, the top 39955 frequent English words are rated from the scale of 1 (very abstract) to 5 (very concrete). For example, the mean ratings for monster, animal and dinosaur are 3.72, 4.61 and 4.87 respectively.\nProblem Formulation: We formulate the problem as a resource flow problem on a graph. The directed graph G is constructed in the following way: we order the seeds based on decreasing centrality scores (CS). We compute CS as:\nCS = (ECS + (−CR))/2, (7)\nwhere we normalize ECS and −CR to the scale of 0 to 1. For each seed u, we check the immediate next node v and add an edge (u, v) if the (ConceptNet-based) similarity between u and v is greater than θsim,ss9. If in this iteration, a node v is not added in G, we get the most recent predecessor u for which the similarity exceed θsim,ss and add (u, v). The idea is that if a word u is more abstract than v and if they are quite similar in terms of conceptual similarity, then\n9θ denotes the set of parameters used in the model.\nword v provides similar but more specific information than word u. Each node has a resource P̃ (u|Ik), the confidence assigned by the Neural Network. If there is an edge from the node, some of this resource should be sent along this edge until for all edges (u, v) ∈ G, wv becomes greater than wu. We formulate the problem as a Linear Optimization problem:\nminimize w=(w1,...w|Sk| )\n∑ (u,v)∈G max{wu − wv, 0}\nsubject to ∑ s∈Sk ws = ∑ sk∈Sk P̃ (sk|Ik)\nwu = P̃ (u|Ik), u /∈ G\nwu ≥ 0.5P̃ (u|Ik), ∀u ∈ G\nTo limit the resource a node u can send, we limit the final minimum value by 0.5 P̃ (u|Ik). The solution provides us with the necessary weights for the set of seeds Sk in Ik. We normalize these weights and get W̃ (Sk).\nB. Intermediate Results for the “Aardvark” Riddle\nFrom the four figures in Figure 7, we get the top 20 Clarifai detections as given in the Table 4.\nBased on the GUR approach (GUR+All in paper), our PSL Stage I outputs probable concepts (words or phrases) depending on the initial set of detected class-labels (seeds). They are provided in Table 5. Note that, these are the top targets detected from almost 0.2 million possible candidates. Observe the following:\ni) the highlighted detected animals have a few visual features in common, such as four short legs, a visible tail, short height etc.\nii) the detections from the third image does not at all lead us to an animal and the PSL Stage I still thinks that its a cartoon of sort.\niii) the detections from second gets affected because of its close relation to the detections from third image and it infers that the image just depicts cartoon.\nIn the final PSL Stage II however, the model figures out that there is an animal that is common to all these images. This is mainly because seeds from the three images confidently predict that some animal is present in the images.\nThat is why most of the top detections correspond to animals and animals having certain characteristics in common.\nThe top detections from PSL Stage II (GUR) are: monotreme, gecko, hippopotamus, pyrography, anteater, lizard, mule deer, chimaera, liger, iguana, komodo dragon, echidna, turtle, art deco, sgraffito, gorilla, loch ness monster, prairie dog.\nBUR: For BUR, PSL Stage I outputs probable concepts (words or phrases) depending on the current set of seeds.\nThey are provided in the Table 6. Observe that the individual detections are better compared to GUR10.\nFinal output from PSL Stage II (for BUR) is comparable to that of the GUR approach. The top detections are: hadrosaur, sea otter, diagrammatic, panda, iguana, pyrography, mule deer, placental mammal, liger, panda bear, art deco, squirrel monkey, giraffe, echidna, otter, anteater, pygmy marmoset, hippopotamus.\nHere, the set of output mainly contains the concepts (words or phrases) that either represents “animals with some similar visual characteristics to aardvark” or it pertains to “cartoon or art”."
    }, {
      "heading" : "C. Detailed Accuracy Histograms For Different Variants",
      "text" : "In this section, we plot the accuracy histograms for the entire dataset for all the variants (using Clarifai API) of our approach (listed in Table 2 of the paper). We also add the accuracy histograms for variants using BUR approach. The plots are shown in the Figure 8. From the plots, the shift towards greater accuracy is evident as we go along the stages of our pipeline.\nD. Visual Similarity: Additional Results Additional results for Visual Similarity are provided in Tables 7 and 8.\n10The output from the PSL Stage I for BUR, is completely independent of the other images. In essence, for each image, we are predicting all relevant concepts from a large vocabulary given a few detections from a small set of class-labels."
    }, {
      "heading" : "E. More Positive and Negative Results",
      "text" : "We provide positive and Negative results in Figures 9 and 10 of the ”GUR+All” variant of the pipeline. We obtain better results with Clarifai detections rather than Residual Network detections. Based on our observations, one of the key property of the ResidualNetwork confidence score distribution is that there are few detections (1-3) which are given the strongest confidence scores and the other detections have very negligible confidence scores. These top detections are often quite noisy.\nFor example, for the aardvark image 1, the ResidualNetwork detections are: triceratops, wallaby, armadillo, hog, fox squirrel, wild boar, kit fox, grey fox, Indian elephant, red fox, mongoose, Egyptian cat, wombat, tusker, mink, Arctic fox, toy terrier, dugong, lion. Only the first detection has 0.84 score and the rest of the scores are very negligible. For the second, third and fourth images, the top detections are respectively:\n1. pick (0.236), ocarina (0.114), maraca (0.091), chain saw (0.06), whistle (0.03), can opener (0.03), triceratops (0.02), muzzle, spatula, loupe, hatchet, letter opener, thresher, rock beauty, electric ray, tick, gong, Windsor tie, cleaver, electric guitar\n2. jersey (0.137), fire screen (0.129), sweatshirt (0.037), pick (0.035), comic book (0.030), book jacket\n(0.029), plate rack, throne, wall clock, face powder, binder, hair slide,velvet,puck, redbone.\n3. hog (0.48), wallaby (0.19), wild boar (0.10), Mexican hairless (0.045), gazelle (0.023), wombat (0.017), dhole (0.016), hyena (0.015), armadillo (0.009), ibex, hartebeest, water buffalo, bighorn, kit fox, mongoose, hare, wood rabbit, warthog, mink, polecat.\nThese predictions show that for the first and fourth image, there are some animals detected with some distant visual similarities. The second and third image has almost no animal mentions. This also shows some very confident detections (such as triceratops for the first image) is quite noisy.\nIn many cases, due to these high-confidence noisy detections, the PSL-based inference system gets biased towards them. Compared to that, Clarifiai detections provide quite a few (abstract but) correct detections about different aspects of the image (for example, for 2nd Image, predicts labels related to “cartoon/art” and “animal” both). This seems to be one of the reasons, for which the current framework provide better results for Clarifai Detections. Using Residual Network, the final output from the GUR system for the “aardvark” riddle is: antelope, prairie dog, volcano rabbit, marsupial lion, peccary, raccoon, pouch mammal, rabbit, otter, monotreme, jackrabbit, hippopotamus, moose, tapir, echidna, gorilla."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this work, we explore a genre of puzzles (“image riddles”) which involves a set of images and a question. An-<lb>swering these puzzles require both capabilities involving vi-<lb>sual detection (including object, activity recognition) and,<lb>knowledge-based or commonsense reasoning. We compile<lb>a dataset of over 3k riddles where each riddle consists of 4<lb>images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an<lb>automatic evaluation metric to track future progress. Our<lb>task bears similarity with the commonly known IQ tasks<lb>such as analogy solving, sequence filling that are often used<lb>to test intelligence. We develop a Probabilistic Reasoning-based approach<lb>that utilizes probabilistic commonsense knowledge to an-<lb>swer these riddles with a reasonable accuracy. We demon-<lb>strate the results of our approach using both automatic and<lb>human evaluations. Our approach achieves some promising<lb>results for these riddles and provides a strong baseline for future attempts. We make the entire dataset and related ma-<lb>terials publicly available to the community in ImageRiddle<lb>Website (http://bit.ly/22f9Ala).",
    "creator" : "LaTeX with hyperref package"
  }
}