{
  "name" : "1102.2739.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A General Framework for Development of the Cortex-like Visual Object Recognition System: Waves of Spikes, Predictive Coding and Universal Dictionary of Features",
    "authors" : [ "Sergey S. Tarasenko" ],
    "emails" : [ "infra.core@gmail.com)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nV ISUAL object recognition systems have been broadlydiscussed in numerous studies from various fields. Originated from vision research, visual object recognition systems are now subjects for interdisciplinary studies. At present, this is the field, where artificial intelligence (AI) meets neuroscience.\nOne of the strongest recent trends is to create the systems, which exhibit biologically plausible performance. This purpose is achieved by tuning the parameters of the systems to the neurophysiological data. On the other hand, their architectures replicate the structure of the corresponding cortical systems.\nAccording to neurophysiological findings, the ventral visual stream (VVS) is the brain system responsible for visual object recognition. The VVS includes the V1 area (primary visual cortex), the V2, V3 and V4 areas and the IT area (Inferior Temporal Cortex).\nThe VVS has a hierarchical bottom-up structure of visual areas. Along the VVS in the anterior direction starting from the primary visual cortex, the size of receptive fields (RFs) of neurons increase. Simultaneously, the selectivity of neurons decreases, i.e., if the V1 neurons are strictly tuned to a bar of a single orientation and of a particular size, the V4 neurons are tuned to various geometric forms and exhibit higher\nS. Tarasenko is with Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, under JST ERATO Asada Synergistic Intelligence Project (email: infra.core@gmail.com).\ndegree of translation and scale invariance. Consequently, a complexity and a size of visual features, recognized in the consecutive areas along the ventral stream, increase.\nTherefore, most visual object recognition systems, born of a symbiosis of AI and neuroscience, have the functional architecture that resembles the hierarchical structure of the VVS. Although not all the visual areas of real VVS are employed by the models, usually V1 and IT areas are used. The area in-between is usually either V2 [17] or V4 [6].\nResults of neurophysiological studies indicate that recognition task in the VVS is performed extremely fast. The processing time varies from 100-150ms for humans [22], and it is even shorter for monkeys [23].\nThe extremely short processing time implies that no feedback (top-down) interaction occurs during visual processing. Therefore many researchers attempted to model the feature of high speed processing by implementing only feed-forward connections [1], [6], [11]. Furthermore, rapid processing implies that a single neuron at each level fires only once, producing a single spike[21](p. 716).\nOn the other hand, the bottom-up (feed-forward) and topdown (feedback) processing circuits can be found throughout the cortex. Therefore, application of only feed-forward processing is a great simplification. The reciprocal information processing in the brain has been efficiently used in the concept of predictive coding, which has been introduced in [19], [20].\nMost recently, a hierarchical network with reciprocal (bottom-up and top-down) connections has been presented [17]. This model employes propagation of waves of spikes and uses reciprocal connections to implement predictive coding.\nAlthough, numerous models of the VVS have been presented, the matter of development of visual areas in VVS was not broadly discussed. Several studies have addressed the matter of development of orientation selectivity in primary visual cortex V1 [2], [25], modular structures [24] and intercortical development [3]. The key mechanism of structure formation in developmental models is Hebbian learning [24], while growth is implemented by growing Self-Organizing Maps[3].\nWe consider development on the scale of the VVS as extraction of new visual features in the V4 area and new objects in IT. This is implemented by means of growing selforganizing networks [5]. Nevertheless, growing structure is not the only feature of the development. We also consider\nar X\niv :1\n10 2.\n27 39\nv1 [\ncs .C\nV ]\n1 4\nFe b\n20 11\ndevelopment to reflect the emergence of new connections between V4 and IT due to inclusion of the new instances (features in V4 and objects in IT).\nTherefore, development is regarded as a process of growing and establishing new connections of both intra- and interlevel types. In other words, development is the emergence of new neurons and new connections within and between hierarchical levels. The origin of development is based on extraction of new visual features across all hierarchical levels.\nIn this study, we introduce the framework to model developmental processes in the cortex-like visual object recognition system."
    }, {
      "heading" : "II. FUNCTIONAL ARCHITECTURE OF THE SYSTEM",
      "text" : "This framework consists of three processing levels (modules). These levels correspond to the V1, V4, and IT areas of the VVS. The V4 and IT levels can be also segregated into layers. Both V4 and IT levels have layers of growing SOM and Radial Basis Function (RBF) units. Meanwhile, the V4 level also has integrating layer. Each layer is a single neural network. The schematic representation of the entire system, including the connections between the levels (cortical areas) and layers (networks inside the cortical areas), is presented in Fig. 1.\nThe V1 level contains four distinct groups of neurons (filter maps), which are mutually interconnected with inhibitory connection (round-head solid lines). After inhibition the information from all four maps is integrated by the layer of the V4 level. The output of this layer is Integrated Orientation Map (IOM). The IOM is then sent to the V4 growing SOM layers by means of excitatory connections. The excitatory connections are indicated by arrow-head solid lines throughout the schema. After IOM is translated into the Feature Map, it is sent V4 RBF units layer and to the IT growing SOM layer. The IT RBF units layer receives excitatory inputs from the IT growing SOM layer as well. The IT RBF units layer sends modulatory (amplification of existing activation level) signals (marked with diamondhead dashed lines) back to the V4 RBF units layer. The feedback modulated (amplified) signal from the V4 RBF\nunits layer to IT RBF unit layer is marked with arrow-head double solid line. The V4 and IT growing SOM layers serve as repositories of visual features and objects, respectively. During the development the collection of visual features is changing. To update objects with valid features, interaction between the V4 and IT growing SOM layers emerges. This is indicated by double-end arrow-head dashed line.\nThe details of processing in each module, intra- and inter- layer and level intractions are discussed in subsequent sections.\nFor the purpose of simulation we use ten stimuli (objects) presented in Fig. 2. The stimuli are 100× 100 pixel pictures of hands and cup. We use 11 black and white color gradations from 0 (black) to 1 (white) with step 0.1 on the normalized scale throughout the processing."
    }, {
      "heading" : "A. Processing in the V1 area",
      "text" : "In this study, we employ Gabor filters, as described by eq. (1), to model V1 simple cells:\nF (x, y) = exp ( − (x 2 o+γ\n2y2o) 2σ2\n) × cos ( 2π λ xo ) xo = x cos θ + y sin θ yo = −x sin θ + y cos θ\n(1)\nwhere θ is an orientation parameter, σ is a scale parameter, and λ is a wave length parameter. The equations for the Gabor filter in our study have been taken from [11].\nWe use a single value for the scale parameter: σ = 2.8, which corresponds to 7 × 7 pixels RFs. The relationship between scale and wavelength parameters are presented in [10](p. 69).\nThe input (retinal image) is processed with Gabor filters of four different orientations (0◦, 45◦, 90◦, and 135◦). Therefore in the V1 area there are four distinct groups of neurons with different orientations. The RFs of these cells ‘densely cover’ the entire retinal image. Performance of each group is implemented as convolution of the Gabor filter kernel with the retinal image. The result of convolution is an orientation map. Each orientation map contains the response of the neurons selective for the same orientation, but situated at different locations. To identify a particular orientation at each location, we employ all-to-all lateral intra-layer inhibitory connections to suppress the competing features in the V1 area [17]."
    }, {
      "heading" : "B. Processing in the V4 area",
      "text" : "We use three computational types of V4 neurons. Our assumption of several types of neurons is supported by the recently proposed models [6], [7].\nType I neurons integrate information from different orientation maps (V 4INT ), resulting from mutual inhibition. The IOM (output of the V 4INT neurons) is then processed to extract complex visual features as combinations of different orientations. To do so, the gSOM [5] is used. The neurons in gSOM are the V4 type II neurons (V 4gSOM ). The V4 neurons of type III are RBF-units [8]:\nV 4RBF = exp(−βV 4||X − Pi||2) (2)\nwhere βV 4 is a tuning parameter, Pi is the center of the RBF unit (prototype) and X is the current input of the RBFunit. A prototype is a visual feature (preferred stimulus) of a certain V 4RBF neuron.\nIn terms of Gaussian RBF, parameter βV 4 is the inverted doubled variance: βV 4 = 12V ar2 . On the other hand, variance V ar2 of the Gaussian function is usually chosen as some fraction of the distance from the origin to the object in the feature space [4]. Such distance can be calculated as\ndistV 4 = M∑ i=1 M∑ j=1 Θ2ij , (3)\nwhere Θij is a particular orienation at a given location of a visual feature, where M and N , are vertical and horizontal\ndimensions of the IOM, respectively. Θ can take one of four integer values 1, 2, 3, or 4, which correspond to 0◦, 45◦, 90◦, and 135◦ orientation, respectively. The V ar2 is then taken to be one tenth of the distV 4.\nIn our model, the RF size of V 4gSOM is 3 × 3 matrix (RF-matrix) of the IOM elements. Concatenating the strings of the RF-matrix, we obtain 9-dim vectors, which are used as inputs to the gSOM network.\nThe information about the features is stored in synaptic weights, therefore gSOM serves as feature repository. We do not use any updates for synaptic weight vectors, but only record the inputs that are sufficiently far from each other (in terms of Euclidean distance between the 9-dim synaptic vectors: if the distance between the input and V 4gSOM neuron is greater than 0.1 distV 4, input is considered to be distant enough).\nEach V 4gSOM neuron plays a role of RBF-center (a prototype) for V 4RBF . Therefore, if k is a total number of V 4gSOM neurons, there are k different classes of V 4RBF neurons. The V 4RBF neurons of a certain class cover densely the IOM. The RF size of V 4RBF is identical to the RF size of the V 4gSOM neurons. Thus, the total number of V 4RBF neurons is then kL, where L is the total number of neurons required to densely cover IOM.\nThe process of on-line visual feature extraction is illustrated in Fig. 3. Here sample RFs of gSOM neurons, densely covering the IOM, are presented. The 3 × 3 features are processed with gSOM, and a collection of distant enough visual features is extracted. The sample features extracted from the first stimulus are presented in Fig. 4.\nProcessing the Integrated Orientation Map with V 4RBF neurons: Feature and Response Maps. After the features have been collected, the visual stimulus is represented as a Response Map. To do so, we first assemble the Feature Map of the visual stimulus by finding the feature that best matches at each spatial location on the IOM. Therefore, the Feature Map is a m × n matrix, where m < M and n < N , are vertical and horizontal dimensions, respectively. The Feature\nMap contains numbers of corresponding prototypes that best match the actual visual feature, presented at a particular spatial location. Then the responses of V 4RBF neurons are calculated. This way, we obtain the Response Map.\nThe Response Map is also m × n matrix, containing responses of V 4RBF neurons at each location of the IOM.\nThe sample parts of the Feature and Response Maps are presented in Fig. 5."
    }, {
      "heading" : "C. Waves of Spikes: Information Transfer from the V4 area to the IT area",
      "text" : "The neurons, whose preferred stimulus is closer to the actual visual input, fire faster [18]. Therefore, the neurons with unit activation (response is 1 on the normalized scale) fire first. Then the neurons with activation greater or equal to 1- ( = 0.1), excluding previously fired neurons, will fire and so on. Thus, the neurons with the same activation level form ‘waves of spikes’. Consequently, the original retinal image is unfolded in a time domain. Thus, information about the stimulus is carried by the sequence of waves of spikes.\nTo extract the the First Wave of Spikes, we eliminate from the Feature Map the features that resulted in neural activation levels less than 1 on the normalized scale. The resultant map is called a First Wave Feature Map (Fig. 6). This map consists only of the features that identically matched the actual visual stimulus.\nThe waves of spikes corresponding to the first stimulus are presented in Fig. 7."
    }, {
      "heading" : "D. Processing in the IT area",
      "text" : "Some of inferior temporal cortex neurons can be considered as RBF-units [12]. The IT area is also addressed as a storage place of various objects and is suggested to have a function of object recognition and classification [15].\nThe objects in the IT area are obtained through everyday visual experience. Therefore, the structure of IT changes in time [15]. To model this change, we applied gSOM to simulate the developmental activity of the IT area. The ITgSOM neurons are the IT neurons of type I. The activity of the gSOM network is similar to the V4 gSOM network.\nThe neurons of type II are ITRBF neurons. The ITgSOM neurons store the objects themselves, while ITRBF neurons perform matching between the object stored by the corresponding ITgSOM neuron (preferred stimulus) and the actual stimulus (input):\nITRBF = exp(−βIT ||X −Obji||2) (4)\nwhere βIT is a tuning parameter, Obji is the center of the RBF-unit and X is the current input. The tuning parameter βIT estimated in the same way as it is done for V 4RBF neurons:\ndistIT = m∑ i=1 n∑ j=1 Feat2ij , (5)\nwhere Featij is a feature number at ij-th element of Feature Map, m and n are dimensions of Feature Map, i = 1, ...,m and j = 1, ..., n. We consider that βIT = (distIT )−1. For\ninstance, the absolute value of parameter βIT for ITRBF (1) is ≈ 10−7. Here notation “(1)” indicates the number of the object stored.\nThe recognition procedure depends on the threshold value α, i.e., if activation of the ITRBF neuron is less then α, the actual stimulus is considered to be different from the corresponding object. We set α to 0.67 [9](p. 1690) on the normalized scale. Therefore the threshold value α is used for detection of new objects (novelty detection) and stored objects’ recognition.\nIn general case, a stimulus processing should be based on the predictive coding (see next section). However, to apply predictive coding some objects should be already stored in IT. Consider the Object 1 in Fig. 2. This object is regarded as the very first input to the system. Therefore, the representation of the input image is stored by the ITgSOM (1) neuron as a new object. This causes creation of an entire new class of ITRBF (1) neurons. The RBF-units of a single class densely cover the Feature Map and produce the ITRBF response grid.\nResponse of ITRBF neurons. Now, there is a single object in IT. Here, we illustrate the response grids of ITRBF neurons, densely covering the Feature Map, to the sequence\nof waves of spike. The response grids are presented in Fig. 8."
    }, {
      "heading" : "E. Predictive Coding: Interaction between V4 and IT",
      "text" : "Predictive coding is a widely discussed scientific paradigm [19], [20]. It involves two stages. First, based on some preliminary data about the stimulus, the Initial Hypothesis is generated. Then additional information about the stimulus is used to either confirm or reject the Initial Hypothesis through the process of Iterative Refinement. Thus, formation of the Initial Hypothesis and its Iterative Refinement are two key points of the predictive coding.\nSelection of the Initial Hypothesis. Generation of the Initial Hypothesis is based on the information delivered by the First Wave of Spikes and the objects stored in IT. In this study, the system stores 10 objects in IT (Fig. 2). The steps of the Initial Hypothesis Generation are the following: 1) the response grids for each type i of ITRBF (i), i = 1, ...,K, K is a total number of objects stored, neurons are calculated; 2) maximum activation value maXi for each grid is identified; 3) maximum value maX from maXi: maX = max\ni {maXi};\nand 4) identify the number of the object, which will be considered as Initial Hypothesis: i∗ = arg(maX). Steps 2 and 3 are based on the max-pooling operation performed in IT [6]. The object contained by ITgSOM (i∗), where i∗ = arg(maX), is considered to be the initial hypothesis for the presented stimulus. The Initial Hypothesis generation procedure is presented in Fig. 9. Here K = 8, maX = 0.53 and i∗ = 3.\nIterative Refinement. The Initial Hypothesis is presented in a form of a Feature Map of a certain object stored in IT. When appropriate Feature Map is found, it is possible to send modulatory signal from IT back to V4. The purpose of this modulation is to amplify the activity of the neurons, which are coherent with Initial Hypothesis.\nFor extraction of the coherent/identical elements (neurons), the Feature Maps of the actual stimulus and the Initial Hypothesis are compared element-wise. The identical elements in both maps are localized. Thus, the neurons representing identical visual features in both maps are detected. Among the localized identical elements, the elements with activation level 1 belong to the First Wave of Spikes. Therefore they are excluded by means of inhibition. This is called to be a matching procedure. The activation levels of remaining identical elements in Feature Map are amplified by the topdown modulatory signals from IT:\nALamp = AL+ 1; (6) ALamp > 1→ ALamp = 1. (7)\nwhere AL is the activation level before the modulatory topdown signal, and ALamp is the amplified activation level after modulatory top-down signal.\nIt is possible that actual stimulus is one of the objects already stored in IT or slightly damaged with noise. Then the Initial Hypothesis has many common features with internal representation of the stimulus distributed among the waves of spikes. Therefore by means of modulation (amplification), the major amount of information will be instantly accumulated by the Second Wave of Spikes, forming “Tsunami of Spikes”. In this case, the significant accelerating effect will occur. We refer to such Initial Hypothesis as coherent. On\nthe other hand, if the actual stimulus is not an object from IT, then the Initial Hypothesis will have few identical neurons with the waves of spikes produced by this stimulus. Consequently, only some small amount of information distributed among the waves of spikes will be shifted to the Second Wave of Spikes, and no significant accelerating effect will take place. Such Initial Hypothesis as called an incoherent.\nSuggest, considering only Objects 2 and 3. We suppose that only Object 3 is stored in IT. In such a case, when Object 2 is presented as stimulus, there will be no accelerating effect (Fig. 10), because the stimulus (Object 2) and RBFcenter (Object 3) have only few common features. On the other hand, if Object 3 is used as a stimulus, the significant accelerating effect takes place (Fig. 11), because in such a case both stimulus and RBF-center have many common features. The numerical aspects of acceleration are illustrated in Fig. 12.\nSummarizing, we present algorithm of Iterative Refinement process as follows: 1) after the activation levels of neurons have been modulated (amplified), the excitatory waves of spikes is sent to IT; 2) the hypothesis generation is repeated. The Initial Hypothesis is either confirmed and retained or rejected and changed; 3) the procedure of matching identical elements is repeated; 4) and modulatory top-\ndown signals are sent back to V4. The Iterative Refinement continues until all waves of spikes reach IT."
    }, {
      "heading" : "III. DEVELOPMENT: COLLECTING NEW FEATURES AND STORING NEW OBJECTS",
      "text" : ""
    }, {
      "heading" : "A. Towards universal dictionary of features",
      "text" : "Development is characterized by the dynamical change in topology of in intra- and inter-level connection. This changed involves not only extraction of new features by means of growing, but also disposal of existing rarely used features. In this section we discuss a mechanism of disposal of existing features.\nThe features are extracted on the basis of the distance. On the other hand, during the processing of a particular stimulus, some features can be found more often than the others.\nTo distinguish frequently used features, the counter variable τ is used. Each time feature i is the best matching feature, the counter τi is incremented by 1. Then the relative frequency fri of activation for each feature is calculated:\nfri = τi F∑ i=1 τi , (8)\nwhere i is a feature number, i = 1, ..., F , and F a total number of features [5].\nFinally, we dispose the features, which have relative frequency less or equal to the chance level 1/F . We refer to remaining features as to survived features. The process of\nfeature extraction and disposal is illustrated in Fig. 13. The outburst at x = 1 on abscissa axis is explained by the great number of originally extracted object specific features.\nAmong the features collected from ten objects, there are features that have been extracted from the very first stimulus and survived throughout the objects. We illustrate the evolution of number of such features in Fig. 14. The inlay in Fig. 14 illustrates dynamics of survival rate.\nThe features survived all ten stimuli represent the collection of features universal for all the objects. Therefore, during development, extraction of universal feature intrinsic for all stimuli and disposal of stimulus specific features. This illustrates convergence of the collection of features to the Universal Dictionary of Features1."
    }, {
      "heading" : "B. Dynamic structure of the inter-level connections between the V4 and IT areas",
      "text" : "The developmental process implies two major changes: 1) the growth of number of features (in V4) and objects (in IT) together; and 2) emergence of new connections between the new neurons and the existing ones. From the very beginning , the networks in the V4 and IT levels of the system are empty. During the development, these networks are growing, thus, causing the installation of new intra- and inter-level connections. The new features are extracted and disposed in V4. In the IT module, the new object is added if the maX value after predictive coding is less than threshold α. If accelerating effect takes place, the predictive coding is terminated instantly. Therefore, emergence of both intra- and inter-level connections in our system involves both the V4 and IT areas.\nThe process of feature disposal takes place after the information about the Feature Map is send to the IT area by the waves of spikes. Therefore, after the extraction, collection of features in gSOM is updated, the Feature Maps of the objects stored in IT should be updated as well. This is implemented\n1Universality is understood from the point of view of the visual experience represented by ten different objects\nby means of substitution of previously used features for the best matching survived features (marked by double-end arrow-head dashed line in Fig. 1). This substitution process takes place after the Predictive Coding. This process causes reciprocal process of updating objects’ Feature Maps.\nTherefore in the presented framework the developmental process is illustrated from both aspects of structure change (growing and disposal), which cause the dynamical change in topology of intra- and inter-level connections.\nThe schema of development of the V4 and IT areas, including intra- and inter-level connections, is presented in Fig. 15."
    }, {
      "heading" : "IV. DISCUSSION AND CONCLUSION",
      "text" : "We have presented a framework to model development of cortex-like visual object recognition system. This system is bases on the neurophysiological principles of information processing along the VVS. These are hierarchical organization of the visual object recognition system, propagation of waves of spikes, reciprocal interaction of different hierarchical levels, predictive coding and dynamic structure of\nthe visual features repository. Our model also inherits major features of the RBF-units as a model of a single neuron in V4 [8] and IT [12].\nThe novelty of our model is the application of growing SOMs to model the processes of feature and object extraction and storage in areas V4 and IT, respectively. We have also implemented the processes of feature extraction and feature disposal. The interplay of these processes allows to simulate dynamical changes in the structure of visual feature repository and consequent update of the objects.\nWe have illustrated the ability of our system to develop over time by extracting new features and storing new objects. We have shown that the predictive coding can speed up the processing time by amplifying the neuronal responses. This suggests the short processing time could be achieved in architectures with reciprocal connections. The evolution of the feature repository allows to illustrate a process of formation of Universal Dictionary of Features.\nTherefore, our framework is a successfully working system of visual object recognition, capable of on-line development from visual experience. The development of the system takes place for each presented stimulus in two ways. First, on-line extraction of features and objects allows to model emergence of feature selectivity in V4 and object selectivity in IT. Second, the interaction between the V4 and IT growing SOM layers provides the system with ability of extracting only the features found most frequently in various stimuli, thus, collecting the universal features.\nThere are many aspects for the further extension of the system: 1) the classification ability of the neurons in the Inferior Temporal cortex [12]; 2) emergence of modular units like columnar structures [17]; 3) continuous topological structure of object representation [9] etc. One of the possible further extension beyond the visual domain is to make this framework capable of operating with multimodal signals and perform cross-modal integration. Moreover, the modular structure of the system implies the flexibility of the system’s architecture. Therefore, this framework can be used as a basis to test various biologically plausible configurations. For example, it is possible to extend the system with V2 and/or V3 areas without restrictions.\nFurthermore, the capability of predictive coding makes this framework to be applicable and extendable to model Dorsal Visual Stream for motion recognition, Superior Temporal Sulcus for biological motion recognition etc.\nMoreover, this framework has a potential serve as a platform to model large-scale cortical networks like Mirror Neuron System."
    }, {
      "heading" : "V. ACKNOWLEDGMENT",
      "text" : "The author would like to thank professor Toshio Inui, JST ERATO Asada Synergistic Intelligence Project and Kyoto University, Graduate School of Informatics, for valuable comments and comprehensive discussions."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "This study is focused on the development of the<lb>cortex-like visual object recognition system. We propose a<lb>general framework, which consists of three hierarchical levels<lb>(modules). These modules functionally correspond to the V1,<lb>V4 and IT areas. Both bottom-up and top-down connections<lb>between the hierarchical levels V4 and IT are employed. The<lb>higher the degree of matching between the input and the<lb>preferred stimulus, the shorter the response time of the neuron.<lb>Therefore information about a single stimulus is distributed in<lb>time and is transmitted by the waves of spikes. The reciprocal<lb>connections and waves of spikes implement predictive coding:<lb>an initial hypothesis is generated on the basis of information<lb>delivered by the first wave of spikes and is tested with the<lb>information carried by the consecutive waves. The development<lb>is considered as extraction and accumulation of features in<lb>V4 and objects in IT. Once stored a feature can be disposed,<lb>if rarely activated. This cause update of feature repository.<lb>Consequently, objects in IT are also updated. This illustrates the<lb>growing process and dynamical change of topological structures<lb>of V4, IT and connections between these areas.",
    "creator" : "LaTeX with hyperref package"
  }
}