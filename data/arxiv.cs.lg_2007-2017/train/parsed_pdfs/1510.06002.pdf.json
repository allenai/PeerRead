{
  "name" : "1510.06002.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast and Scalable Structural SVM with Slack Rescaling",
    "authors" : [ "Heejin Choi", "Ofer Meshi", "Nathan Srebro" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present an efficient method for training slackrescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violatinglabel in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many problems in machine learning can be seen as structured output prediction tasks, where one would like to predict a set of labels with rich internal structure [1]. This general framework has proved useful for a wide range of applications from computer vision, natural language processing, computational biology, and others. In order to achieve high prediction accuracy, the parameters of structured predictors are learned from training data. One of the most effective and commonly used approaches for this supervised learning task is Structural SVM, a method that generalizes binary SVM to structured outputs [15]. Since the structured error is non-convex, Tsochantaridis et al. [15] propose to replace it with a convex surrogate loss function. They formulate two such surrogates, known as margin and slack rescaling.\nWhile slack rescaling often produces more accurate pre-\ndictors, margin rescaling has been far more popular due to its better computational requirements. In particular, both formulations require optimizing over the output space, but while margin rescaling preserves the structure of the score and error functions, the slack-rescaling does not. This results in harder inference problems during training. To address this challenge, Sarawagi and Gupta [11] propose a method to reduce the problem of slack rescaling to a series of modified margin rescaling problems. They show that their method outperforms margin rescaling in several domains. However, there are two main caveats in their approach. First, the optimization is only heuristic, that is, it is not guaranteed to solve the slack rescaling objective exactly. Second, their method is specific to the cutting plane algorithm and does not easily extend to stochastic training algorithms. More recently, Bauer et al. [2] proposed an elegant dynamic programming approach to the slack rescaling optimization problem. However, their formulation is restricted to sequence labeling and hamming error, and does not apply to more general structures.\nIn this paper we propose an efficient method for solving the optimization problem arising from slack rescaling formulation. Similar to Sarawagi and Gupta [11] our method reduces finding the most violated label in slack rescaling to a series of margin rescaling problems. However, in contrast to their approach, our approach can be easily used with training algorithms like stochastic gradient descent (SGD) [10] and block Frank-Wolfe (FW) [7], which often scale much better than cutting plane. We first propose a very simple approach that minimizes an upper bound on the slack rescaling objective function, and only requires access to a margin rescaling oracle. This formulation is quite general and can be used with any error function and model structure, and many training algorithms such as cutting plane, SGD and FW. However, this method is not guaranteed to always find the most violating label. Indeed, we show that always finding the most violating label for a slack-rescaled formulation is impossible using only a margin rescaling oracle. To address this, we suggest using a modified oracle, that is typically as easy to implement as margin rescaling, and present a more sophisticated algorithm which solves the slack rescaling formulation exactly, and also enjoys good approximation guarantees after a small number of itar X iv :1 51 0.\n06 00\n2v 2\n[ cs\n.L G\n] 2\n7 O\nct 2\n01 5\nerations. We demonstrate empirically that our algorithm outperforms existing baselines on several real-world applications, including hierarchical and multi-label classification."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "In this section we review the basics of structured output prediction and describe the relevant training objectives. In structured output prediction the task is to map data instances x to a set of output labels y ∈ Y . Structured SVMs use a linear discriminant mapping of the form y(x;w) = argmaxy∈Y w\n>φ(x, y), where φ(x, y) ∈ Rd is a feature function relating input-output pairs, and w ∈ Rd is a corresponding vector of weights. Our interest is in the supervised learning setting, where w is learned from training data {xi, yi}ni=1 by minimizing the empirical risk. The prediction quality is measured by an error function L(y, yi) ≥ 0 which determines how bad it is to predict y when the ground-truth is in fact yi.\nSince optimizing L(y, yi) directly is hard due to its complicated dependence on w, several alternative formulations minimize a convex upper bound instead. Structural SVM is an elegant max-margin approach which uses a structured hinge loss surrogate [15, 14]. Two popular surrogates are margin and slack rescaling. In particular, denoting the model score by f(y) = w>φ(x, y) (we omit the dependence on x and w to simplify notation), the margin rescaling training objective is given by:\nmin w,ξ\nC 2 ‖w‖22 + 1 n ∑ i ξi (1)\ns.t. f(yi)− f(y) ≥ L(y, yi)− ξi ∀i, y 6= yi ξi ≥ 0 ∀i\nwhere C is the regularization constant. Similarly, the slack rescaling formulation scales the slack variables by the error term:\nmin w,ξ\nC 2 ‖w‖22 + 1 n ∑ i ξi (2)\ns.t. f(yi)− f(y) ≥ 1− ξi\nL(y, yi) ∀i, y 6= yi\nξi ≥ 0 ∀i\nIntuitively, both formulations seek to find a w which assigns high scores to the ground-truth compared to the other possible labellings. When y is very different than the true yi (L is large) then the difference in scores should also be larger. There is, however, an important difference between the two forms. In margin rescaling, high loss can occur for labellings with high error even though they are already classified correctly with a margin. This may divert training\nfrom the interesting labellings where the classifier errs, especially when L can take large values, as in the common case of hamming error. In contrast, in slack rescaling labellings that are classified with a margin incur no loss. Another difference between the two formulations is that the slack rescaling loss is invariant to scaling of the error term, while in margin rescaling such scaling changes the meaning of the features φ.\nIn many cases it is easier to optimize an unconstrained problem. In our case it is easy to write (1) and (2) in an unconstrained form:\nmin w\nC 2 ‖w‖22 + 1n ∑ i max y∈Y (L(y, yi) + f(y)− f(yi)) (3)\nmin w\nC 2 ‖w‖22 + 1n ∑ i max y∈Y L(y, yi) (1 + f(y)− f(yi)) (4)\nMost of the existing training algorithms for structural SVM require solving the maximization-over-labellings problems in (4) and (3):\nMargin rescaling : argmaxy∈Y L(y, yi) + f(y)− f(yi) (5)\nSlack rescaling : argmaxy∈Y L(y, yi) (1 + f(y)− f(yi)) (6)\nTo better understand the difference between margin and slack rescaling we focus on a single training instance i and define the functions: h(y) = 1 + f(y) − f(yi) and g(y) = L(yi, y). With these definitions we see that the maximization (5) for margin rescaling is maxy∈Y h(y) + g(y), while the maximization (6) for slack rescaling is maxy∈Y h(y)g(y). It is now obvious why margin rescaling is often easier. When the score and error functions h and g decompose into a sum of simpler functions, we can exploit that structure in order to solve the maximization efficiently [15, 14, 5]. In contrast, the slack rescaling score does not decompose even when both h and g do. What we show, then, is how to solve problems of the form maxy h(y)g(y), and thus the maximization (6), having access only to an oracle for additive problems of the form maxy h(y) +λg(y).\nThat is, we assume that we have access to a procedure, referred to as the λ-oracle, which can efficiently solve the problem:\nyλ = O(λ) = argmax y∈Y Lλ(y) (7)\nwhereLλ(y) = h(y)+λg(y). This problem is just a rescaling of (5). E.g., for linear responses it is obtained by scaling the weight vector by 1/λ. If we can handle margin rescaling efficiently we can most likely implement the λoracle efficiently. This is also the oracle used by Sarawagi and Gupta [11]. In Section 4, we show how to obtain a solution to the slack-rescaling problem (6) using such a λoracle. Our method can be used as a subroutine in a variety of training algorithms, and we demonstrate that it is more\nscalable than previous methods. However, we also show that this approach is limited, since no procedure that only has access to a λ-oracle can guarantee the quality of its solution, no matter how much time it is allowed to run.\nTherefore, we propose an alternative procedure that can access a more powerful oracle, which we call the constrained λ-oracle:\nyλ,α,β = Oc(λ, α, β) = max y∈Y, αh(y)>g(y), βh(y)≤g(y) Lλ(y),\n(8)\nwhere α, β ∈ R. This oracle is similar to the λ-oracle, but can additionally handle linear constraints on the values h(y) and g(y). In the sequel we show that in many interesting cases this oracle is not more computationally expensive than the basic one. For example, when the λ-oracle is implemented as a linear program (LP), the additional constraints are simply added to the LP formulation and do not complicate the problem significantly. Before presenting our algorithms for optimizing (6), we first review the training framework in the next section."
    }, {
      "heading" : "3 Optimization for Slack Rescaling",
      "text" : "In this section we briefly survey cutting plane and stochastic gradient descent optimization for the slack rescaled objective (2) and (4). This will be helpful in understanding the difference between our approach and that of prior work on the slack rescaled objective by Sarawagi and Gupta [11].\nThe cutting plane algorithm was proposed for solving the structural SVM formulation in [15, 6]. This algorithm has also been used in previous work on slack rescaling optimization [11, 2]. The difficulty in optimizing (2) stems from the number of constraints, which is equal to the size of the output space Y (for each training instance). The cutting plane method maintains a small set of constraints and solves the optimization only over that set. At each iteration the active set of constraints is augmented with new violated constraints, and it can be shown that not too many such constraints need to be added for a good solution to be found [6]. The main computational bottleneck here is to find a violating constraint at each iteration, which is challenging since it requires searching over the output space for some violating labeling y.\nRelying on this framework, Sarawagi and Gupta [11] use the formulation in (2) and rewrite the constraints as:\n1 + f(y)− f(yi)− ξi\nL(y, yi) ≤ 0 ∀i, y 6= yi (9)\nHence, to find a violated constraint they attempt to maximize: solve the problem with substitution of:\nargmax y∈Y′\n( h(y)− ξi\ng(y)\n) (10)\nwhere Y ′ = {y|y ∈ Y, h(y) > 0, y 6= yi} and we use our notation h(y) = 1 + f(y) − f(yi) and g(y) = L(y, yi). They suggest minimizing a convex upper bound of (10) which stems from the convex conjugate function of ξi g(y) :\nmax y∈Y′ h(y)− ξi g(y) = max y∈Y′ min λ≥0\n( h(y) + λg(y)− 2 √ ξiλ )\n≤min λ≥0 max y∈Y′ F ′(λ, y) = min λ≥0 max y∈Y′ F ′(λ, y) = min λ≥0 F (λ)\n(11)\nwhere F (λ) = max y∈Y′\nF ′(λ, y) = maxy∈Y′ h(y) + λg(y)−\n2 √ ξiλ. Since F (λ) is a convex function, (11) can be solved by a simple search method such as golden search over λ [11].\nAlthough this approach is suitable for the cutting plane algorithm, unfortunately it cannot be easily extended to other training algorithms. In particular, F ′(λ, y) is defined in terms of ξi, which ties it to the constrained form (2). On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4). These methods are typically preferable in the large scale setting, since they have very low per-iteration cost, handling a single example at a time, with the same overall iteration complexity as cutting plane methods. In contrast, the cutting plane algorithm considers the entire training set at each iteration, so the method does not scale well to large problems. Since our goal in this work is to handle large datasets, we would like to be able to use the stochastic methods mentioned above, working on the unconstrained formulation (4). The update in these algorithms requires solving the maximization problem (6), which is the goal of Section 4. Note that solving (6) also allows using a cutting plane method if desired."
    }, {
      "heading" : "4 Algorithms",
      "text" : "In this section we present our main contribution, a framework for solving the maximization problem (6), which we write as:\nmax y Φ(y) := max y h(y)g(y) (12)\nWe describe two new algorithms to solve this problem using access to the λ-oracle, which have several advantages over previous approaches. However, we also show that any algorithm which uses only the λ-oracle cannot always recover an optimal solution. Therefore, in Section 4.5 we proposed an improved algorithm which requires access to an augmented λ-oracle that can also handle linear constraints."
    }, {
      "heading" : "4.1 Binary search",
      "text" : "We first present a binary search algorithm similar to the one proposed by Sarawagi and Gupta [11], but with one main difference. Our algorithm can be easily used with training methods that optimize the unconstrained objective (4), and can therefore be used for SGD, SDCA and FW. The algorithm minimizes a convex upper bound on Φ without slack variable ξi. The algorithm is based on the following lemma (details and proofs are in Appendix A).\nLemma 1. Let F̄ (λ) = 14 maxy∈Y+ ( 1 λh(y) + λg(y) )2 , then\nmax y∈Y Φ(y) ≤ min λ>0 F̄ (λ)\nand F̄ (λ) is a convex function in λ.\nRather than minimizing this upper bound, we next present an algorithm that aims to optimize Φ(y) in a more direct manner, using a geometrical interpretation of mapping labels into R2."
    }, {
      "heading" : "4.2 Geometrical Interpretation of λ-oracle search",
      "text" : "To understand the problem better and motivate our methods, it is useful to consider the following geometrical interpretation of (12): we map each labels y to a vector ~y = [h(y) g(y)] ∈ R2. Let ~Y = {~y ∈ R2|y ∈ Y} be the set of the all mapped labels. The maximization (12) reduces to the problem: given a set of points ~Y ⊂ R2, maximize the product of their coordinates ~y∗ = argmax~y∈~Y [~y]1 · [~y]2.\nThe contours of our objective function ~Φ(~y) = [~y]1 · [~y]2 are then hyperbolas. We would like to maximize this function by repeatedly finding points that maximize linear objectives of the form ~Lλ(~y) = [~y]1 + λ[~y]2, whose contours form lines in the plane. See Figure 1.\nAn example of mapping of label into R2 is shown in Appendix B.\nThe importance of the R2 mapping is that each yλ revealed\nby the λ-oracle shows that y∗ can only reside in a small slice of the plane. See figure 2.\nLemma 2. Let Sλ be a line through ~yλ and ~z = [λ[~yλ]2, 1 λ [~yλ]1], and let Cλ = {~y ∈ R\n2|[~y]1 · [~y]2 = ~Φ(~yλ)} be the hyperbola through ~yλ. Then, ~y∗ is on or below line Sλ, and ~y∗ is on or above hyperbola Cλ.\nProof. If there exists a ~y ∈ ~Y which is above Sλ, it contradicts the fact that ~yλ is the argmax point for function ~Lλ. And the second argument follows from ~y∗ being the argmax label w.r.t. ~Φ, and the area above Cλ corresponds to points whose ~Φ value is greater than ~yλ.\nIt follows that h(y∗) and g(y∗) must each reside in a segment:\nLemma 3. Let Ḣ = [min([ ~yλ]1, [~z]1),max([ ~yλ]1, [~z]1)] and Ġ = [min([ ~yλ]2, [~z]2),max([ ~yλ]2, [~z]2)]. Then,\nh(y∗) ∈ Ḣ, g(y∗) ∈ Ġ\nProof. This follows from the fact that Sλ and Cλ intersects at two points, ~yλ and ~z, and the boundaries, Sλ and Cλ, are strictly decreasing functions."
    }, {
      "heading" : "4.3 Bisecting search",
      "text" : "In this section, we propose a search algorithm which is based on the previous geometric interpretation. Similar to the binary search, our method also relies on the basic λoracle.\nWe next give an overview of the algorithm. We maintain a set of possible value ranges λ∗ = argmaxλ>0 Φ(yλ),\nh(λ∗), and g(λ∗) as L,H, and G, respectively; all initialized as R. First, for each yλ returned by the oracle, we take an intersection of G and H with a segment of possible values of h(y) and g(y), respectively, using Lemmas 2 and 3. Second, we reduce the space L of potential λ’s based on the following Lemma (proved in the Appendix). Lemma 4. h(yλ) is a non-increasing function of λ, and g(yλ) is a non-decreasing function of λ.\nThus, we can discard {λ′|λ′ > λ} if h(y∗λ) > h(yλ) or {λ′|λ′ < λ} otherwise from L. Next, we pick λ ∈ L in the middle, and query yλ. The algorithm continues until at least one of L,H, and G is empty.\nSimilar to the binary search from the previous section, this algorithm can be used with training methods like SGD and SDCA, as well as the cutting-plane algorithm. However, this approach has several advantages compared to the binary search. First, the binary search needs explicit upper and lower bounds on λ, thus it has to search the entire λ space [11]. However, the bisecting search can directly start from any λ without an initial range, and for instance, this can be used to warm-start from the optimal λ in the previous iteration. Furthermore, we point out that since the search space of h and g is also bisected, the procedure can terminate early if either of them becomes empty.\nFinally, in Appendix D we propose two improvements that can be applied to either the binary search or the bisecting search. Specifically, we first provide a simple stopping criterion that can be used to terminate the search when the current solution yλt will not further improve. Second, we show how to obtain a bound on the suboptimality of the current solution, which can give some guarantee on its quality.\nSo far we have used the λ-oracle as a basic subroutine in our search algorithms. Unfortunately, as we show next, this approach is limited as we cannot guarantee finding the optimal solution y∗, even with unlimited number of calls to the λ-oracle. This is somewhat distressing since with unlimited computation we can find the optimum of (6) by enumerating all y’s."
    }, {
      "heading" : "4.4 Limitation of the λ-oracle",
      "text" : "Until now, we used only the λ-oracle to search for Φ∗ without directly accessing the functions h and g. We now show that this approach, searching Φ∗ with only a λ-oracle, is very limited: even with an unlimited number of queries, the search cannot be exact and might return a trivial solution in the worst case (see Appendix E for proof). Theorem 1. Let Ĥ = maxy h(y) and Ĝ = maxy g(y). For any > 0, there exists a problem with 3 labels such that for any λ ≥ 0, Φ(yλ) = miny∈Y Φ(y) < , while Φ(y∗) = 1\n4 ĤĜ.\nTheorem 1 shows that any search algorithm that can access\nAlgorithm 1 Bisecting search 1: procedure BISECTING(λ0) Input: Initial λ for the search λ0 ∈ R+ Output: ŷ ∈ Y. Initialize: H = G = L = R+, λ = λ0, Φ̂ = 0.\n2: while H 6= ∅ and G 6= ∅ do 3: y′ ← O(λ) 4: u← [h(y′) λg(y′)], v ← [g(y′) 1λh(y\n′)] 5: H ← H ∩ {h′|minu ≤ h′ ≤ maxu} .\nUpdate 6: G← G ∩ {g′|min v ≤ g′ ≤ max v} 7: if v1 ≤ v2 then . Increase λ 8: L← L ∩ {λ′ ∈ R|λ′ ≥ λ} 9: else . Decrease λ\n10: L← L ∩ {λ′ ∈ R|λ′ ≤ λ} 11: λ← 12 (minL+ maxL) 12: if h(y′)g(y′) ≥ Φ̂ then 13: ŷ ← y′, Φ̂← h(y′)g(y′).\nthe function only through λ-oracle, including the method of Sarawagi and Gupta [11] and both methods presented above, cannot be guaranteed to find a label optimizing Φ(y), even approximately, and even with unlimited accesses to the oracle. This problem calls for a more powerful oracle."
    }, {
      "heading" : "4.5 Angular search with the constrained-λ-oracle",
      "text" : "The constrained λ-oracle defined in (8) has two inequality constraints to restrict the search space. Using this modified algorithm, we can present an algorithm that is guranteed to find the most violating constraint, as captured by the following guarantee, proved in Appendix F:\nTheorem 2. Angular search described in Algorithm 2 finds the optimum y∗ = argmaxy∈Y Φ(y) using at most t = 2M + 1 iteration where M is the number of the labels.\nThis is already an improvement over the previous methods, as at least we are guaranteed to return the actual most violating label. However, it is still disappointing since the number of iterations, and thus number of oracle accesses might actually be larger than the number of labels. This defies the whole point, since we might as well just enumerate over all M possible labels. Unfortunately, even with a constrained oracle, this is almost the best we can hope for. In fact, even if we allow additional linear constraints, we might still need M oracle accesses, as indicated by the following Theorem, proved in Appendix E.\nTheorem 3. Any search algorithm accessing labels only through a λ-oracle with any number of linear constraints cannot find y∗ using less than M iterations in the worst case, where M is the number of labellings.\nFortunately, even though we cannot guarantee optimizing\nΦ(y) exactly using a small number of oracle accesses, we can at least do so approximately. This can be achieved by Algorithm 2 (see Appendix F), as the next theorem states.\nTheorem 4. In angular search, described in Algorithm 2, at iteration t,\nΦ(y∗) Φ(ŷt) ≤ (v1) 4 t+1\nwhere ŷt = argmaxt y t is the optimum up to t, v1 =\nmax { λ0 ∂(y1) , ∂(y1) λ0 } , λ0 is the initial λ used, and y1 is the first label returned by constrained λ-oracle.\nWe use ∂(a) = a2a1 to denote the slope of a vector.\nWith proper initialization, we get the following runtime guarantee:\nTheorem 5. Assuming Φ(y∗) > φ, angular search de-\nscribed in algorithm 2 with λ0 = Ĝ\nĤ , α0 =\nĜ2\nφ , β0 =\nφ\nĤ2 ,\nfinds an -optimal solution, Φ(y) ≥ (1 − )Φ(y∗), in T\nqueries and O(T ) operations, where T = 4 log\n( ĜĤ\nφ\n) ·\n1\n, and δ-optimal solution, Φ(y) ≥ Φ(y∗)−δ, in T ′ queries\nand O(T ′) operations, where T ′ = 4 log\n( ĜĤ\nφ\n) · Φ(y ∗)\nδ .\nHere we give an overview of the algorithm with an illustration in Figure 3. The constrained λ-oracle restricts the search space, and this restriction can be illustrated as a linear upper bound U and a lower bound L. The search is initialized with the entire right angle: U = [0 ∞] and L = [∞ 0], and maintains that ~y∗ is always between U and L. The constrained λ-oracle is used with U,L and a certain λ to reduce the potential area where ~y∗ can reside.\nSpecifically, the search space is reduced using an angle defined by U = OP and L = OQ. In the next iteration, the constrained λ-oracle is invoked with U1 = OP and L1 = OM , and also with U2 = OM and L2 = OQ. Intuitively, each such query shrinks the search space, and as the search space shrinks, the suboptimaly bound improves. This process is continued until the remaining search space is empty. The angular search algorithm defines the optimal λ and values to be passed to the constrained λ-oracle.\nIn Algorithm 2 each angle is dequeued, split, and enqueued recursively. Each angle maintains its upper bound from the previous iterations and stops splitting itself and terminate if it is ensured that there exists no label with larger Φ value within the angle. When the oracle reveals a label with Φ(yλ) = c, we can safely discard all area corresponding to {~y|Φ(~y) ≤ c}. This works as a global constraint which shrinks the search space. Therefore, acquiring a label with high Φ value in the early stages facilitate convergence. Thus, it is suggested to use a priority queue, and dequeue the angle with the highest upper bound on Φ. A similar strategy is to have a label cache, the subset of previous most violated labels, denoted as C. With the label cache, we can discard a large part of the search space {~y|Φ(~y) ≤ maxy′∈C Φ(~y′)} immediately. Algorithm 2 also uses the constrained λ-oracle to avoid returning previously found labels. Finally, for λ0, we suggest to use λ0 =\nĤ Ĝ , with Ĥ calculated from the current weights w.\nSee Appendix G for the illustration of the angular search."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we validate our contributions by comparing the different behaviors of the search algorithms on standard benchmark datasets, and its effect on the optimization. Specifically, we show that angular search with SGD is not only much faster than the other alternatives, but also enables much more precise optimization with slack rescaling formulation, outperforming margin rescaling.\nUnlike the simple structure used in [2], we show applicability to complicated structure. We experiment with multilabel dataset modeled by a Markov Random Field with pair-wise potentials as in [5]. Since the inference of margin scaling is NP-hard in this case, we rely on linear programming relaxation. Note that this complicates the problem, and the number of labels becomes even larger (adds fractional solutions). Also notice that all of our results in previous sections apply with mild modification to this harder setting. Two standard benchmark multi-label datasets, yeast[4] (14 labels)and RCV1[8], are tested. For RCV1 we reduce the data to the 50 most frequent labels. For angular search, we stop the search whenever Φ(ŷ)Φ(y∗) > 0.999 holds, to avoid numerical issues."
    }, {
      "heading" : "5.1 Comparison of the search algorithms",
      "text" : "Table 1 compares the performance of the search in terms of the time spend, the number of queries, and the success percentage of finding the most violating label. The cutting-plane algorithm calls the search algorithms to find the most violating label ŷ, and adds it the active set if the violation is larger than some margin , i.e., ∆(ŷ, yi)(1 + f(ŷ) − f(yi)) > ξi + . For cutting-plain optimization, we compare all three algorithms: Angular search, Bisecting search, and Sarawagi and Gupta’s [11] (but just used Angular search for the update). Success percentage is the percentage that the search algorithm finds such a violating label. As expected from Theorem 1, bisecting and Sarawagi’s search miss the violating label in cases where angular search successfully finds one. This is important for obtaining high accuracy solution. For RCV1 dataset, not only is angular search more accurate, but it also uses about 2.6 times less queries than bisecting and 10.1 times less queries than Sarawagi’s search. As for the timing, angular search is 1.18 times faster than bisecting search, and 4.7 times faster than Sarawagi’s algorithm.\nIn figure 4, we compare the convergence rate and the accuracy of the different optimization schemes using different search algorithms. Additional plots showing convergence w.r.t. the number of queries and iterations are in Appendix I. These show that angular search with SGD converges order of magnitude faster.\nTable 2 shows a performance comparison for the multilabel datasets. For RCV1 dataset it shows a slight performance gain, which shows that the benefit of slack rescaling formulation is greater when the label space is large."
    }, {
      "heading" : "5.2 Hierarchical Multi-label Classification",
      "text" : "We further experimented on problem of hierarchical multilabel classification [3]. In hierarchical multi-label classification, each label y is a leaf node in a given graph, and each label y shares ancestor nodes. It can be described as a graphical model where a potential of a multi-label Y = {y1, . . . , yk} is the sum of all potentials of its ancestors, i.e., Φ(Y ) = ∑ n∈ ⋃ n∈Y Anc(n) Φ(n). We extracted\n1500 instances with dimensionality 17944 with a graph structure of 156 nodes with 123 label from SWIKI-2011. SWIKI-2011 is a multi-label dataset of wikipedia pages from LSHTC competition1. We used 750 instances as training set, 250 instances as holdout set, and 750 instances as test set. The Hamming distance is used as label loss. We show that slack rescaling in such large label structure is tractable and outperforms margin rescaling."
    }, {
      "heading" : "6 Summary",
      "text" : "As we saw in our experiments, and has also been previously noted, slack rescaling is often beneficial compared to margin rescaling in terms of predictive performance. However, the margin-rescaled argmax (5) is often much easier computationally due to its additive form. Margin rescaling is thus much more frequently used in practice. Here, we show how an oracle for solving an argmax of the form (5), or perhaps a slightly modified form (the constrained-λ oracle), is sufficient for also obtaining exact solutions to the slackrescale argmax (6). This allows us to train slack-rescaled SVMs using SGD, obtaining better predictive performance than using margin rescaling. Prior work in this direction [11] was only approximate, and more significantly, only enabled using cutting-plane methods, not SGD, and was thus not appropriate for large scale problems. More recently, [2] proposed an efficient dynamic programming approach for solving the slack-rescaled argmax (6), but their approach is only valid for sequence problems2 and only when using hamming errors, not for more general structured prediction problems. Here, we provide a generic method relying on a simple explicitly specified oracle that is guaranteed to be exact and efficient even when the number of labels are in-\n1http://lshtc.iit.demokritos.gr/ 2The approach can also be generalized to tree-structured prob-\nlems.\nfinite and allows using SGD and thus working with large scale problems."
    }, {
      "heading" : "Appendix A Details of binary search",
      "text" : "Lemma 1. Let F̄ (λ) = 14 maxy∈Y+ ( 1 λh(y) + λg(y) )2 , then\nmax y∈Y Φ(y) ≤ min λ>0 F̄ (λ)\nand F̄ (λ) is a convex function in λ.\nProof. First, let Y+ = {y|y ∈ Y, h(y) > 0}, then maxy∈Y Φ(y) = maxy∈Y+ Φ(y), since any solution y such that h(y) < 0 is dominated by yi, which has zero loss. Second, we prove the bound w.r.t. y ∈ Y+. In the following proof we use a quadratic bound (for a similar bound see [9]).\nmax y∈Y+ Φ(y) = max y∈Y+ h(y)g(y) = max y∈Y+\n1\n4\n( 2 √ h(y)g(y) )2 = 1\n4 ( max y∈Y+ min λ>0 { 1 λ h(y) + λg(y) })2 ≤ 1\n4 ( min λ>0 max y∈Y+ { 1 λ h(y) + λg(y) })2 (13)\nTo see the convexity of F̄ (λ), we differentiate twice to obtain:\n∂2F̄ (λ)\n∂λ2 =\n1 4 max y∈Y+ 6 1 λ4 h(y)2 + 2g(y)2 > 0\nSimilar to [11], we obtain a convex upper bound on our objective. Evaluation of the upper bound F̄ (λ) requires using only the λ-oracle. Importantly, this alternative bound F̄ (λ) does not depend on the slack variable ξi, so it can be used with algorithms that optimize the unconstrained formulation (4), such as SGD, SDCA and FW. As in [11], we minimize F̄ (λ) using binary search over λ. The algorithm keeps track of yλt , the label returned by the λ-oracle for intermediate values λt encountered during the binary search, and returns the maximum label maxt Φ(yλt). This algorithm focuses on the upper bound minλ>0 F̄ (λ), and interacts with the target function Φ only through evaluations Φ(yλt) (similar to [11])."
    }, {
      "heading" : "Appendix B An example of label mapping",
      "text" : ""
    }, {
      "heading" : "Appendix C Monotonicity of h and g in λ",
      "text" : "Proof. Let g1 = g(yλ1), h1 = h(yλ1), g2 = g(yλ2), and h2 = h(yλ2).\nh1 + λ1g1 ≥ h2 + λ1g2, h2 + λ2g2 ≥ h1 + λ2g1 ⇔ h1 − h2 + λ1(g1 − g2) ≥ 0,−h1 + h2 + λ2(g2 − g1) ≥ 0 ⇔ (g2 − g1)(λ2 − λ1) ≥ 0\nFor h, change the role of g and h."
    }, {
      "heading" : "Appendix D Improvements for the binary",
      "text" : "search"
    }, {
      "heading" : "Appendix D.1 Early stopping",
      "text" : "If L = [λm, λM ], and both endpoints have the same label, i.e., yλm = yλM , then we can terminate the binary search safely because from lemma 4, it follows that the solution yλ will not change in this segment."
    }, {
      "heading" : "Appendix D.2 Suboptimality bound",
      "text" : "Let K(λ) be the value of the λ-oracle. i.e.,\nK(λ) = max y∈Y h(y) + λg(y). (14)\nLemma 5. Φ∗ is upper bounded by\nΦ(y∗) ≤ K(λ) 2\n4λ (15)\nProof.\nh(y) + λg(y) ≤ K(λ) ⇐⇒ g(y)(h(y) + λg(y)) ≤ g(y)K(λ) ⇐⇒ Φ(y) ≤ g(y)K(λ)− λg(y)2\n= −λ ( g(y)− K(λ)\n2λ\n)2 + K(λ)2\n4λ ≤ K(λ)\n2\n4λ"
    }, {
      "heading" : "Appendix E Proof of the limitation of the",
      "text" : "λ-oracle search\nTheorem 1. Let Ĥ = maxy h(y) and Ĝ = maxy g(y). For any > 0, there exists a problem with 3 labels such that for any λ ≥ 0, yλ = argminy∈Y Φ(y) < , while Φ(y∗) = 1\n4 ĤĜ.Let Ĥ = maxy h(y) and Ĝ = maxy g(y).\nFor any > 0 and λ > 0, there exists a problem of 3 labels that yλ = argminy∈Y Φ(y) < , and Φ(y\n∗) − Φ(yλ) = 1 4 ĤĜ.\nProof. We will first prove following lemma which will be used in the proof.\nLemma 6. Let A = [A1 A2] ∈ R2, B = [B1 B2] ∈ R2, and C = [C1 C2] ∈ R2, and A1 < B1 < C1. If B is under the line AC, i.e.,∃t,0 ≤ t ≤ 1,D = tA + (1 − t)C, D1 = B1, D2 > B2. Then, @λ ≥ 0, v = [1 λ] ∈ R2, such that\nv ·B > v ·A and v ·B > v · C (16)\nProof. Translate vectors A,B, and C into coordinates of [0, A2], [a, b], [C1, 0] by adding a vector [−A1,−C2] to each vectors A,B, and C, since it does not change B − A or B − C. Let X = C1 and Y = A2.\nIf 0 ≤ λ ≤ X Y\n, then v·A = λY ≤ X = v·C. v·(B−C) > 0 ⇐⇒ (a − X) + λb > 0 corresponds to all the points above line AC. Similarly, if λ ≥ X Y , (16) corresponds to a+ λ(b− Y ) > 0 is also all the points above AC.\nFrom lemma 6, if y1,y2 ∈ Y , then all the labels which lies under line y1 and y2 will not be found by λ-oracle. In the adversarial case, this holds when label lies on the line also. Therefore, Theorem 1 holds when there exists three labels, for arbitrary small > 0, A = [ , Ĝ], B = [Ĥ, ], and C = [ 12Ĥ, 1 2 Ĝ], Y = {A,B,C}. In this case Φ̂ ≈ 0.\nTherefore, corollary ?? holds for any problems with any Y that {A,B,C} ⊆ Y and ∀y ∈ Y , y is on or below AC."
    }, {
      "heading" : "Appendix F Angular search",
      "text" : "We first introduce needed notations. ∂⊥(a) be the perpendicular slope of a, i.e., ∂⊥(a) = − 1∂(a) = − a1 a2 . For A ⊆ R2, let label set restricted to A as ~YA = ~Y ∩ A, and yλ,A = O(λ,A) = argmaxy∈Y,~y∈A h(y) + λg(y) = argmax~y∈~YA [~y]1 + λ[~y]2. Note that if A = R 2, yλ,R2 = yλ. For P,Q ∈ R2, define Λ(P,Q) to be the area below the line PQ, i.e., Λ(P,Q) = {~y ∈ R2|[~y]2− [P ]2 ≤ ∂⊥(Q − P )([~y]2 − [P ]2)}. Υλ = {~y ∈ R2|~Φ(~y) = [~y]1 · [~y]2 ≥ ~Φ(~yλ,A)} be the area above Cλ, and Υλ = {~y ∈ R2|~Φ(~y) = [~y]1 · [~y]2 ≤ ~Φ(~yλ,A)} be the area below Cλ.\nRecall the constrained λ-oracle defined in (8):\nyλ,α,β = Oc(λ, α, β) = max y∈Y, αh(y)≥g(y), βh(y)<g(y) Lλ(y)\nwhere α, β ∈ R+ and α ≥ β > 0. Let A(α, β) ⊆ R2 be the restricted search space, i.e., A(α, β) = {a ∈ R2|β < ∂(a) ≤ α}. Constrained λ-oracle reveals maximal Lλ label within restricted area defined by α and β. The area is bounded by two lines whose slope is α and β. Define a pair (α, β), α, β ∈ R+, α ≥ β > 0 as an angle. The angular search recursively divides an angle into two different angles, which we call the procedure as a split. For α ≥ β ≥ 0, let λ =\n1√ αβ , z = ~yλ,α,β and z′ = [λ[z]2, 1λ [z]1].\nLet P be the point among z and z′ which has the greater slope (any if two equal), and Q be the other point, i.e., if ∂(z) ≥ ∂(z′), P = z and Q = z′, otherwise P = z′ and Q = z. Let R = [√ λ[z]1 · [z]2 √ 1 λ [z]1 · [z]2 ] . Define split(α, β) as a procedure divides (α, β) into two angles (α+, γ+) = (∂(P ), ∂(R)) and (γ+, β+) = (∂(R), ∂(Q)).\nFirst, show that ∂(P ) and ∂(Q) are in between α and β, and ∂(R) is between ∂(P ) and ∂(Q).\nLemma 7. For each split(α, β),\nβ ≤∂(Q) ≤ ∂(R) ≤ ∂(P ) ≤ α\nProof. β ≤ ∂(z) ≤ α follows from the definition of constrained λ-oracle in (8).\n∂(z′) = 1\nλ2∂(z) =\nαβ\n∂(z) =⇒ β ≤ ∂(z′) ≤ α =⇒ β ≤\n∂(Q) ≤ ∂(P ) ≤ α. ∂(Q) ≤ ∂(R) ≤ ∂(P ) ⇐⇒ min { ∂(z), 1\nλ2∂(z)\n} ≤\n1 λ ≤ max\n{ ∂(z), 1\nλ2∂(z)\n} from ∀a, b ∈ R+, b ≤ a =⇒\nb ≤ √ ab ≤ a.\nAfter each split, the union of the divided angle (α+, γ) and (γ, β+) can be smaller than angle (α, β). However, follow-\ning lemma shows it is safe to use (α+, γ) and (γ, β+) when our objective is to find y∗.\nLemma 8.\n∀a ∈ ~YA(α,β),Φ(a) > Φ(yλ,α,β) =⇒ β+ < ∂(a) < α+\nProof. From lemma 2, ~YA(α,β) ⊆ Λ(P,Q). Let U = {a ∈ R2|∂(a) ≥ α+ = ∂(P )}, B = {a ∈ R2|∂(a) ≤ β+ = ∂(Q)}, and two contours of function C = {a ∈ R2|~Φ(a) = Φ(yλ,α,β)}, S = {a ∈ R2|Lλ(a) = Lλ(~yλ,α,β)}. S is the upper bound of Λ(P,Q), and C is the upper bound of C = {a ∈ R2|~Φ(a) ≤ Φ(yλ,α,β)}. P and Q are the intersections of C and S. For area of U and B, S is under C, therefore, Λ(P,Q) ∩ U ⊆ C, and Λ(P,Q) ∩ B ⊆ C. It implies that ∀a ∈ (Λ(P,Q) ∩ U) ∪ (Λ(P,Q) ∩ B) =⇒ ~Φ(a) ≤ Φ(yλ,α,β). And the lemma follows from A(α, β) = U ∪ B ∪ {a ∈ R2|β+ < ∂(a) < α+}.\nWe associate a quantity we call a capacity of an angle, which is used to prove the suboptimality of the algorithm. For an angle (α, β), the capacity of an angle v(α, β) is\nv(α, β) :=\n√ α\nβ\nNote that from the definition of an angle, v(α, β) ≥ 1. First show that the capacity of angle decreases exponentially for each split.\nLemma 9. For any angle (α, β) and its split (α+, γ+) and (γ+, β+),\nv(α, β) ≥ v(α+, β+) = v(α+, γ+)2 = v(γ+, β+)2\nProof. Assume ∂(P ) ≥ ∂(Q) (the other case is follows the same proof with changing the role of P and Q), then α+ = ∂(P ) and β+ = ∂(Q). ∂(Q) = 1\nλ2∂(P ) =\nαβ\n∂(P ) , v\n(α+, β+) = v(∂(P ), ∂(Q)) = λ∂(P ) = ∂(P )√ αβ . Since α is\nthe upper bound and β is the lower bound of ∂(P ),\n√ β\nα ≤ v(∂(P ), ∂(Q)) ≤ √ α\nβ . Last two equalities in the lemma are from v(∂(P ), ∂(R)) = v(∂(R), ∂(Q)) = √\n∂(P )√ αβ by plugging in the coordinate of R.\nLemma 10. Let B(a) = 1 4\n( a+ 1\na\n)2 . The suboptimality\nbound of an angle (α, β) with λ = 1√ αβ is\nmax~y∈~YA(α,β) ~Φ(~y)\nΦ(yλ,α,β) ≤ B(v(α, β)).\nProof. From lemma 2, ~YA(α,β) ⊆ Λ(P,Q) = Λ(z, z′). Let ∂(z) = γ. From 7, β ≤ γ ≤ α. Let m = argmaxa∈Λ(z,z′) ~Φ(a). m is on line zz′ otherwise we can move m increasing direction of each axis till it meets the boundary zz′ and Φ only increases, thusm = tz+(1−t)z′. ~Φ(m) = maxt ~Φ(tz + (1 − t)z′). ∂~Φ(tz + (1− t)z′)\n∂t =\n0 =⇒ t = 1 2 . m = 12 [z1 + λz2 z2 + z1 λ ].\nmax~y∈~YA(α,β) ~Φ(~y)\nΦ(yλ,α,β) =\n1\n4 (√ z1 λz2 + √ λz2 z1 )2\n= 1\n4 √√αβ γ + √ γ√ αβ 2\nSince v(a) = v (\n1 a\n) and v(a) increases monotonically for\na ≥ 1,\nB(a) ≤ B(b) ⇐⇒ max { a, 1\na\n} ≤ max { b, 1\nb\n}\nIf √ αβ\nγ ≥ γ√ αβ , then\n√ αβ γ ≤ √ α β since γ ≥ β.\nIf γ√ αβ\n≥ √ αβ\nγ , then γ√ αβ\n≤ √ α\nβ since γ ≤\nα. Therefore, max~y∈~YA(α,β)\n~Φ(~y)\nΦ(yλ,α,β) = B\n(√ αβ\nγ\n) ≤\nB(v(α, β)).\nNow we can prove the theorems.\nTheorem 2. Angular search described in algorithm 2 finds optimum y∗ = argmaxy∈Y Φ(y) at most t = 2M + 1 iteration where M is the number of the labels.\nProof. Denote yt, αt, βt, zt, z′t,K 1 t , and K 2 t for y, α, β, z, z′,K1, and K2 at iteration t respectively. A(αt, βt) is the search space at each iteration t. At the first iteration t = 1, the search space contains all the labels with positive Φ, i.e., {y|Φ(y) ≥ 0} ⊆ A(∞, 0). At iteration t > 1, firstly, when yt = ∅, the search area A(αt, βt) is removed from the search since yt = ∅ implies there is no label inside A(αt, βt). Secondly, when yt 6= ∅, A(αt, βt) is dequeued, and K1t and K 2 t is enqueued. From lemma 8, at every step, we are ensured that do not loose y∗. By using strict inequalities in the constrained oracle with valuable s, we can ensure yt which oracle returns is an unseen label. Note that split only happens if a label is found, i.e., yt 6= ∅. Therefore, there can be only M splits, and each split can be viewed as a branch in the binary tree, and the number of queries are the number of nodes. Maximum number of the nodes with M branches are 2M + 1.\nTheorem 4. In angular search described in algorithm 2, at iteration t,\nΦ(y∗)\nΦ(ŷ) ≤ (v1)\n4 t+1\nwhere v1 = max {\nλ0 ∂(y1) , ∂(y1) λ0\n} , λ0 is the initial λ used,\nand y1 is the first label returned by constrained λ-oracle.\nProof. After t ≥ 2r − 1 iteration as in algorithm 2 where r is an integer, for all the angle (α, β) in the queue Q, v(α, β) ≤ (v1)2 1−r . This follows from the fact that since the algorithm uses the depth first search, after 2r − 1 iterations all the nodes at the search is at least r. At each iteration, for a angle, the capacity is square rooted from the lemma 9, and the depth is increased by one. And the theorem follows from the fact that after t ≥ 2r−1 iterations, all splits are at depth r′ ≥ r, and at least one of the split contains the optimum with suboptimality bound with lemma 10. Thus,\nΦ(y∗)\nΦ(ŷ) ≤ B\n( (v1) 21−r ) < (v1) 22−r ≤ (v1) 4 t+1\nTheorem 5. Assuming Φ(y∗) > φ, angular search de-\nscribed in algorithm 2 with λ0 = Ĝ\nĤ , α0 =\nĜ2\nφ , β0 =\nφ\nĤ2 ,\nfinds -optimal solution, Φ(y) ≥ (1− )Φ(y∗), in T queries\nand O(T ) operations where T = 4 log\n( ĜĤ\nφ\n) · 1 , and δ-\noptimal solution, Φ(y) ≥ Φ(y∗) − δ, in T ′ queries and\nO(T ′) operations where T ′ = 4 log\n( ĜĤ\nφ\n) · Φ(y ∗)\nδ .\nProof. Φ(y∗) > φ ⇔ φ Ĥ2\n< g(y∗)\nh(y∗) = ∂( ~y∗) <\nĜ2\nφ .\nv1 = max { λ0 ∂(y1) , ∂(y1) λ0 } from Theorem 4. Algorithm finds y∗ if β ≤ ∂( ~y∗) ≤ α, thus set α = Ĝ 2\nφ and\nβ = φ\nĤ2 . Also from the definition of constrained λ-\noracle, β = φ\nĤ2 ≤ ∂(y1) ≤ α =\nĜ2\nφ . Therefore, v1 ≤ max {\nλ0 ∂(y1) , ∂(y1) λ0\n} . And the upper bound of\ntwo terms equal when λ0 = Ĝ\nĤ , then v1 ≤\nĜĤ\nφ . δ\nbound follows plugging in the upper bound of v1, and\n= δ\nΦ(y∗) .\nAlgorithm 2 Angular search 1: procedure ANGULARSEARCH(λ0, T ) Input: λ0 ∈ R+, and maximum iteration T ∈ R+ Output: ŷ ∈ Y. Initialize: α0 = ∞, β0 = 0, Empty queue Q, ŷ = ∅.λ ←\nλ0 2: ADD(Q, (α, β, 0)) 3: while Q 6= ∅ do 4: (α, β, s)← Dequeue(Q) 5: if β 6= 0 then 6: λ← 1√\nαβ\n7: if s = 0 then 8: y ← Oc(λ, α, β) 9: else\n10: y ← Oc(λ, α, β) 11: if Φ(y) > Φ(ŷ) then 12: ŷ ← y 13: if y 6= ∅ then 14: z ← [h(y) g(y)], z′ ← [λg(y) 1\nλ h(y)] 15: r ← [√ λh(y)g(y) √\n1 λh(y)g(y) ] 16: if z1 = z′1 then 17: return y 18: else if ∂(z) > ∂(z′) then 19: K1 ← (∂(z), ∂(r), 1) 20: K2 ← (∂(r), ∂(z′), 0) 21: else 22: K1 ← (∂(z′), ∂(r), 1) 23: K2 ← (∂(r), ∂(z), 0) 24: ADD(Q,K1) .ADD(Q,K2)"
    }, {
      "heading" : "25: t← t+ 1",
      "text" : "26: if t = T then . maximum iteration reached 27: return ŷ"
    }, {
      "heading" : "Appendix G Illustration of the angular",
      "text" : "search\nFollowing figure 6 illustrates Angular search. Block dots are the labels from figure 5. Blue X denotes the new label returned by the oracle. Red X is the maximum point. Two straights lines are the upper bound and the lower bound used by the constrained oracle. Constrained oracle returns a blue dot between the upper and lower bounds. We can draw a line that passes blue X that no label can be above the line. Then, split the angle into half. This process continues until the y∗ is found."
    }, {
      "heading" : "Appendix H Limitation of the constraint",
      "text" : "λ-oracle search\nTheorem 3. Any search algorithm accessing labels only through λ-oracle with any number of the linear constraints cannot find y∗ in less than M iterations in the worst case where M is the number of labels.\nProof. We show this in the perspective of a game between a searcher and an oracle. At each iteration, the searcher query the oracle with λ and the search space denoted as A, and the oracle reveals a label according to the query. And the claim is that with any choice of M − 1 queries, for each query the oracle can either give an consistent label or indicate that there is no label in A such that after M − 1 queries the oracle provides an unseen label y∗ which has bigger Φ than all previous revealed labels.\nDenote each query at iteration t with λt > 0 and a query closed and convex set At ⊆ R2, and denote the revealed label at iteration t as yt. We will use yt = ∅ to denote that there is no label inside query space At. Let Yt = {yt′ |t′ < t}.\nAlgorithm 3 describes the pseudo code for generating such yt. The core of the algorithm is maintaining a rectangular areaRt for each iteration t with following properties. Last two properties are for yt.\n1. ∀t′ < t,∀y ∈ Rt,Φ(y) > Φ(yt′).\n2. ∀t′ < t,∀y ∈ Rt ∩ At′ , h(yt′) + λt′g(yt′) > h(y) + λt′g(y).\n3. Rt ⊆ Rt−1.\n4. Rt is a non-empty open set.\n5. yt ∈ Rt ∩ At\n6. yt = argmaxy∈Yt∩At h(y) + λtg(y).\nNote that if these properties holds till iteration M , we can simply set y∗ as any label inRM which proves the claim.\nFirst, we show that property 4 is true. R0 is a non-empty open set. Consider iteration t, and assume Rt−1 is a nonempty open set. Then R̃ is an open set since Rt−1 is an open set. There are two unknown functions, Shrink and FindRect. For open set A ⊆ R2, y ∈ R2, let Shink(A, y, λ) = A − {y′|Φ(y′) ≤ Φ(y) or h(y′) + λg(y′) ≥ h(y) + λg(y)}. Note that Shrink(A, y, λ) ⊆ A, and Shrink(A, y, λ) is an open set. Assume now that there exists a y such that Shrink(Rt−1, y, λt) 6= ∅ and FindPoint(Rt−1, λt) returns such y. Function FindPoint will be given later. FindRect(A) returns an open non-empty rectangle inside A. Note that Rect(A) ⊆ A, and since input to Rect is always non empty open set,\nAlgorithm 3 Construct a consistent label set Y .\nInput: {λt,At}M−1t=1 , λt > 0,At ⊆ R2,Atis closed and convex region. Output: {yt ∈ R2}t=M−1t=1 , y∗ ∈ R2 Initialize: R0 = {(a, b)|0 < a, 0 < b},Y0 = ∅.\n1: for t = 1, 2, . . . ,M − 1 do 2: if Yt−1 ∩ At = ∅ then 3: ỹ = argmaxy∈Yt h(y) + λtg(y). 4: R̃ = Rt−1∩{y|h(y)+λtg(y) < h(ỹ)+λtg(ỹ) or y /∈ At}. 5: else 6: ỹ = ∅, R̃ = Rt−1 −At. 7: if R̃ 6= ∅ then 8: yt = ∅. Rt = FindRect(R̃) 9: else\n10: yt = FindPoint(Rt−1, λt). 11: Rt = FindRect(Shrink(Rt−1, yt, λt)). 12: if yt 6= ∅ then 13: Y = Y ∪ {yt}. 14: Pick any y∗ ∈ RM−1\nsuch rectangle exists. Since R0 is non-empty open set, ∀t,Rt is a non-empty open set.\nProperty 3 and 5 are easy to check. Property 1 and 2 follows from the fact that ∀t ∈ {t|yt 6= ∅},∀t′ > t,Rt′ ⊆ Shrink(Rt−1, yt, λt−1).\nProperty 6 follows from the facts that if Yt−1 ∩ At 6= ∅, R̃ = 0 =⇒ Rt−1 ⊆ {y|h(y) + λtg(y) > h(ỹ) + λtg(ỹ) and y ∈ At}, otherwise Yt−1 ∩ At = ∅, andRt−1 ⊆ At.\nFindPoint(A, λ) returns any y ∈ A − {y ∈ R2|λy2 = y1}. Given input A is always an non-empty open set, such y exists. Shrink(Rt−1, y, λt) 6= ∅ is ensured from the fact that two boundaries, c = {y′|Φ(y′) = Φ(y)} and d = {h(y′) + λg(y′) = h(y) + λg(y)} meets at y. Since c is a convex curve, c is under d on one side. Therefore the intersection of set above c and below d is non-empty and also open.\nAppendix I Additional Plots from the Experiments\nqueries #105 0.5 1 1.5 2 2.5 3 3.5 4 4.5\nob je\nct iv\ne\n8\n9\n10\n11\n12\n13\n14\n15 Yeast\n(7.38) Angular(SGD) (7.4) Angular(CP) (7.28) Bisecting(SGD) (7.34) Bisecting(CP) (9.44) Sarawagi(CP)\n(a) Objective vs queries\niterations 5 10 15 20 25 30\nob je\nct iv\ne\n8\n9\n10\n11\n12\n13\n14\n15 Yeast\n(7.38) Angular(SGD) (7.4) Angular(CP) (7.28) Bisecting(SGD) (7.34) Bisecting(CP) (9.44) Sarawagi(CP)\n(b) Objective vs iterations\niterations 5 10 15 20 25 30\nac cu\nra cy\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nYeast\n(0.54) Angular(SGD) (0.528) Angular(CP) (0.516) Bisecting(SGD) (0.537) Bisecting(CP) (0.483) Sarawagi(CP)\n(c) Accuracy vs iterations"
    } ],
    "references" : [ {
      "title" : "Predicting Structured Data",
      "author" : [ "G.H. Bakir", "T. Hofmann", "B. Schölkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Efficient algorithms for exact inference in sequence labeling svms",
      "author" : [ "A. Bauer", "N. Gornitz", "F. Biegler", "Muller", "K.-R", "M. Kloft" ],
      "venue" : "Neural Networks and Learning Systems, IEEE Transactions on,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Hierarchical document categorization with support vector machines",
      "author" : [ "L. Cai", "T. Hofmann" ],
      "venue" : "In Proceedings of the thirteenth ACM international conference on Information and knowledge management,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "A kernel method for multi-labelled classification",
      "author" : [ "A. Elisseeff", "J. Weston" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Training structural svms when exact inference is intractable",
      "author" : [ "T. Finley", "T. Joachims" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Cutting-plane training of structural svms",
      "author" : [ "T. Joachims", "T. Finley", "Yu", "C.-N. J" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Block-coordinate frank-wolfe optimization for structural svms",
      "author" : [ "S. Lacoste-Julien", "M. Jaggi", "M. Schmidt", "P. Pletscher" ],
      "venue" : "In ICML 2013 International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Minimizing the product of two discrete convex functions",
      "author" : [ "N.D. Nghia", "D.D. Chinh", "P.C. Duong" ],
      "venue" : "ACTA MATHEMATICA VIETNAMICA,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1995
    }, {
      "title" : "Online) subgradient methods for structured prediction",
      "author" : [ "N. Ratliff", "J.A.D. Bagnell", "M. Zinkevich" ],
      "venue" : "In AISTATS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "Accurate maxmargin training for structured output spaces",
      "author" : [ "S. Sarawagi", "R. Gupta" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Many problems in machine learning can be seen as structured output prediction tasks, where one would like to predict a set of labels with rich internal structure [1].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "To address this challenge, Sarawagi and Gupta [11] propose a method to reduce the problem of slack rescaling to a series of modified margin rescaling problems.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "[2] proposed an elegant dynamic programming approach to the slack rescaling optimization problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "Similar to Sarawagi and Gupta [11] our method reduces finding the most violated label in slack rescaling to a series of margin rescaling problems.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "However, in contrast to their approach, our approach can be easily used with training algorithms like stochastic gradient descent (SGD) [10] and block Frank-Wolfe (FW) [7], which often scale much better than cutting plane.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "However, in contrast to their approach, our approach can be easily used with training algorithms like stochastic gradient descent (SGD) [10] and block Frank-Wolfe (FW) [7], which often scale much better than cutting plane.",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : "When the score and error functions h and g decompose into a sum of simpler functions, we can exploit that structure in order to solve the maximization efficiently [15, 14, 5].",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "This is also the oracle used by Sarawagi and Gupta [11].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "This will be helpful in understanding the difference between our approach and that of prior work on the slack rescaled objective by Sarawagi and Gupta [11].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "The cutting plane algorithm was proposed for solving the structural SVM formulation in [15, 6].",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "This algorithm has also been used in previous work on slack rescaling optimization [11, 2].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "This algorithm has also been used in previous work on slack rescaling optimization [11, 2].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "At each iteration the active set of constraints is augmented with new violated constraints, and it can be shown that not too many such constraints need to be added for a good solution to be found [6].",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 10,
      "context" : "Relying on this framework, Sarawagi and Gupta [11] use the formulation in (2) and rewrite the constraints as:",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "Since F (λ) is a convex function, (11) can be solved by a simple search method such as golden search over λ [11].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4).",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4).",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4).",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4).",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 10,
      "context" : "We first present a binary search algorithm similar to the one proposed by Sarawagi and Gupta [11], but with one main difference.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "First, the binary search needs explicit upper and lower bounds on λ, thus it has to search the entire λ space [11].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "the function only through λ-oracle, including the method of Sarawagi and Gupta [11] and both methods presented above, cannot be guaranteed to find a label optimizing Φ(y), even approximately, and even with unlimited accesses to the oracle.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "Unlike the simple structure used in [2], we show applicability to complicated structure.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "We experiment with multilabel dataset modeled by a Markov Random Field with pair-wise potentials as in [5].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "Two standard benchmark multi-label datasets, yeast[4] (14 labels)and RCV1[8], are tested.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "Two standard benchmark multi-label datasets, yeast[4] (14 labels)and RCV1[8], are tested.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "For cutting-plain optimization, we compare all three algorithms: Angular search, Bisecting search, and Sarawagi and Gupta’s [11] (but just used Angular search for the update).",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "We further experimented on problem of hierarchical multilabel classification [3].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Prior work in this direction [11] was only approximate, and more significantly, only enabled using cutting-plane methods, not SGD, and was thus not appropriate for large scale problems.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "More recently, [2] proposed an efficient dynamic programming approach for solving the slack-rescaled argmax (6), but their approach is only valid for sequence problems2 and only when using hamming errors, not for more general structured prediction problems.",
      "startOffset" : 15,
      "endOffset" : 18
    } ],
    "year" : 2015,
    "abstractText" : "We present an efficient method for training slackrescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violatinglabel in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.",
    "creator" : "LaTeX with hyperref package"
  }
}