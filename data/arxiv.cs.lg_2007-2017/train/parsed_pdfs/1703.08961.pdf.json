{
  "name" : "1703.08961.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scaling the Scattering Transform: Deep Hybrid Networks",
    "authors" : [ "Edouard Oyallon" ],
    "emails" : [ "edouard.oyallon@ens.fr", "eugene.belilovsky@inria.fr", "sergey.zagoruyko@enpc.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n08 96\n1v 2\n[ cs\n.C V\n] 4\nA pr\nWe use the scattering network as a generic and fixed initialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1 × 1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their endto-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset."
    }, {
      "heading" : "1. Introduction",
      "text" : "Image classification is a high dimensional problem that requires building lower dimensional representations that reduce the non-informative images variabilities. For example, some of the main source of variability are often due to geometrical operations such as translations and rotations. An efficient classification pipeline necessarily builds invariants to these variabilities. Deep architectures build representations that lead to state-of-the-art results on image classification tasks [13]. These architectures are designed as very deep cascades of non-linear end-to-end learned modules [22]. When trained on large-scale datasets they have been shown to produce representations that are transferable to other datasets [42, 15], which indicate they have captured generic properties of a supervised task that consequently do\nnot need to be learned. Indeed several works indicate geometrical structures in the filters of the earlier layers [19, 39] of Deep CNNs. However, understanding the precise operations performed by those early layers is a complicated [38, 26] and possibly intractable task. In this work we investigate if it is possible to replace these early layers, by simpler cascades of non-learned operators that reduce variability while retaining discriminative information.\nIndeed, there can be several advantages to incorporating pre-defined geometric priors, via a hybrid approach of combining pre-defined and learned representations. First, endto-end pipelines can be data hungry and ineffective when the number of samples is low. Secondly, it could permit to obtain more interpertable classification pipelines which are amenable to analysis. Finally, it can reduce the spatial dimensions and the required depth of the learned modules.\nA potential candidate for an image representation is the SIFT descriptor [23] that was widely used before 2012 as a feature extractor in classification pipelines [30, 31]. This representation was typically encoded via an unsupervised Fisher Vector (FV) and fed to a linear SVM. However, several works indicate that this is not a generic enough representation to build further modules on top of [21, 2]. Indeed end-to-end learned features produce substantially better classification accuracy. A major improvement over SIFT can be found in the scattering transform [24, 6, 33], which is a type of deep convolutional network, which permits to retain discriminative information normally discarded by methods like SIFT while introducing geometric invariances and stability. Scattering transforms have been shown to already produce representations that lead to the top results on complex image datasets when compared to other unsupervised representations (even learned ones) [27]. This makes them an excellent candidate for the initial layers of a deep network. We thus investigate the use of scattering as a generic representation to combine with deep neural networks.\nRelated to our work [28] proposed a hybrid representation for large scale image recognition combining a prede-\n1\nfined representation and Neural Networks (NN), that uses Fisher Vector encoding of SIFT and leverages NNs as scalable classifiers. In contrast we use the scattering transform in combination with convolutional architectures. Our main contributions are as follows: First, we demonstrate that using supervised local descriptors, obtained by shallow 1× 1 convolutions, with very small spatial window sizes permits to obtain AlexNet accuracy on the imagenet classification task (Subsection 2.3). We show empirically these encoders build explicit invariance to local rotations (Subsection 3.2). Second, we propose hybrid networks that combine scattering with modern CNNs (Section 4) and show that using scattering and a ResNet of reduced depth, we obtain similar accuracy to ResNet-18 on Imagenet (Subsection 4.1). Finally, we demonstrate in Subsection 4.3 that scattering permits a substantial improvement in accuracy in the setting of limited data.\nOur highly efficient GPU implementation of the scattering transform is, to our knowledge, orders of magnitude faster than any other implementations, and allows training very deep networks applying scattering on the fly. Our scattering implementation 1 and pre-trained hybrid models 2are available."
    }, {
      "heading" : "2. Scattering Networks and Hybrid Architectures",
      "text" : "We introduce the scattering transform and motivate its use as a generic input for supervised tasks. A scattering network belongs to the class of CNNs whose filters are fixed as wavelets [27]. The construction of this network has strong mathematical foundations [24], meaning it is well understood, relies on few parameters and is stable to a large class of geometric transformations. In general, the parameters of this representation do not need to be adapted to the bias of the dataset [27], making its output a suitable generic representation.\nWe then propose and motivate the use of supervised CNNs built on top of the scattering network. Finally we propose a supervised encodings of scattering coefficients using 1x1 convolutions, that can retain interpertability and locality properties."
    }, {
      "heading" : "2.1. Scattering Networks",
      "text" : "In this section, we recall the definition of the scattering transform. Consider a signal x(u), with u the spatial position index and an integer J ∈ N, which is the spatial scale of our scattering transform. Let φJ be a local averaging filter with a spatial window of scale 2J (here, a Gaussian smoothing function). Applying the local averaging operator, AJx(u) = x ⋆ φJ (2 Ju) we obtain the zeroth order scattering coefficient, S0x(u) = AJx(u). This operation builds an approximate invariant to translations smaller than 2J , but it also results in a loss of high frequencies that are necessary to discriminate signals.\nA solution to avoid the loss of high frequency information is provided by the use of wavelets. A wavelet is an integrable and localized function in the Fourier and space domain, with zero mean. A family of wavelets is obtained by dilating a complex mother wavelet ψ (here, a Morlet wavelet) such that ψj,θ(u) = 1\n22j ψ(r−θ u 2j ), where r−θ is\nthe rotation by −θ, and j ≥ 0 is the scale of the wavelet. A given wavelet ψj,θ has thus its energy concentrated at a scale j, in the angular sector θ. Let L ∈ N be an integer parametrizing a discretization of [0, 2π]. A wavelet transform is the convolution of a signal with the family of wavelets introduced above, with an appropriate downsampling:\nW1x(j1, θ1, u) = {x ⋆ ψj1,θ1(2 j1u)}j1≤J,θ1=2π lL ,1≤l≤L\nObserve that j1 and θ1 have been discretized: the wavelet is chosen to be selective in angle and localized in Fourier. With appropriate discretization [27], {AJx,W1x} is approximatively an isometry on the set of signals with limited bandwidth, and this implies the energy of the signal is preserved. This operator then belongs to the category of multiresolution analysis operators, each filter being excited by a specific scale and angle, but with the output coefficients not being invariant to translation. To achieve invariance we can not apply AJ to W1x since it gives a trivial invariant, namely zero.\nTo tackle this issue, we apply a non-linear point-wise complex modulus to W1x, followed by an averaging AJ , which builds a non trivial invariant. Here, the mother wavelet is analytic, thus |W1x| is regular [1] which implies that the energy in Fourier of |W1x| is more likely to be contained in a lower frequency domain than W1x. Thus, AJ preserves more energy of |W1x|. It is possible to define S1x = AJ |W1|x, which can also be written as: S1x(j1, θ1, u) = |x ⋆ ψj1,θ1 | ⋆ φJ (2\nJu); this is the first order scattering coefficients. Again, the use of the averaging builds an invariant to translation up to 2J .\n1http://github.com/edouardoyallon/pyscatwave 2http://github.com/edouardoyallon/scalingscattering\nOnce more, we apply a second wavelet transform W2, with the same filters as W1, on each channel. This permits the recovery of the high-frequency lost due to the averaging applied to the first order, leading to S2x = AJ |W2||W1|, which can also be written as S2x(j1, j2, θ1, θ2, u) = |x ⋆ ψj1,θ1| ⋆ ψj2,θ2 | ⋆ φJ (2\nJu). We only compute increasing paths, e.g. j1 < j2 because non-increasing paths have been shown to bear no energy [6]. We do not compute higher order scatterings, because their energy is negligible [6]. We call Sx(u) the final scattering coefficient corresponding to the concatenation of the order 0, 1 and 2 scattering coefficients, intentionally omitting the path index of each representation. In the case of colored images, we apply independently a scattering transform to each RGB channel of the image, which means Sx(u) has a size equal to 3 × (\n1 + JL + 1 2 J(J − 1)L2\n)\n, and the original image is\ndown-sampled by a factor 2J [6].\nThis representation is proved to linearize small deformations [24] of images, be non-expansive and almost complete [10, 5], which makes it an ideal input to a deep network algorithm, that can build invariants to this local variability via a first linear operator. We discuss it as an ideal initialization in the next subsection."
    }, {
      "heading" : "2.2. Cascading a supervised Deep architecture",
      "text" : "We now motivate the use of a supervised architecture on top of a scattering network. Scattering transforms have yielded excellent numerical results [6] on datasets where the variabilities are completely known, such as MNIST or FERET. In these task, the problems encountered are linked to sample and geometric variance and handling these variances leads to solving these problems. However, in classification tasks on more complex image datasets, such variabilities are only partially known as there are also non geometrical intra-class variabilities. Although applying the scattering transform on datasets like CIFAR or Caltech leads to nearly state-of-the-art results in comparison to other unsupervised representations there is a large gap in performance when comparing to supervised representations [27]. CNNs fill in this gap, thus we consider the use of deep neural networks utilizing generic scattering representations in order to reduce more complex variabilities than geometric ones.\nRecent works [25, 7, 17] have suggested that deep networks could build an approximation of the group of symmetries of a classification task and apply transformations along the orbits of this group, like convolutions. This group of symmetry corresponds to some of the non-informative intra class variabilities, which must be reduced by a supervised classifier. [25] motivates that to each layer corresponds an approximated Lie group of symmetry, and this approximation is progressive, in the sense that the dimension of these groups is increasing with depth. For instance, the main linear Lie group of symmetry of an image is the translation\ngroup, R2. In the case of a wavelet transform obtained by rotation of a mother wavelet, it is possible to recover a new subgroup of symmetry after a modulus non-linearity, the rotation SO2, and the group of symmetry at this layer is the roto-translation group: R2 ⋉ SO2. If no non-linearity was applied, a convolution along R2 ⋉ SO2 would be equivalent to a spatial convolution. Discovering explicitly the next new and non-geometrical groups of symmetry is however a difficult task [17]; nonetheless, the roto-translation group seems to be a good initialization for the first layers. In this work, we investigate this hypothesis and avoid learning those well-known symmetries.\nThus, we consider two types of cascaded deep network on top of scattering. The first, referred to as the Shared Local Encoder (SLE), learns a supervised local encoding of the scattering coefficients. We motivate and describe the SLE in the next subsection as an intermediate representation between unsupervised local pipelines, widely used in computer vision prior to 2012, and modern supervised deep feature learning approaches. The second, referred to as a hybrid CNN, is a cascade of a scattering network and a standard CNN architecture, such as a ResNet [13]. In the sequel we empirically analyse hybrid CNNs, which permits to greatly reduce the spatial dimensions on which convolutions are learned and can reduce sample complexity."
    }, {
      "heading" : "2.3. SharedLocal Encoder for ScatteringRepresentations",
      "text" : "We now discuss the spatial support of different approaches, in order to motivate our local encoder for scattering. In CNNs constructed for large scale image recognition, the representations at a specific spatial location and depth depend upon large parts of the initial input image and thus mixes global information. For example, at depth 2 of [19], the effective spatial support of the corresponding filter is already 32 pixels (out of 224). The specific representations derived from CNNs trained on large scale image recognition are often used as representations in other computer vision tasks or datasets [40, 42].\nOn the other hand prior to 2012 local encoding methods led to state of the art performance on large scale visual recognition tasks [30]. In these approaches local neighborhoods of an image were encoded using method such as SIFT descriptors [23], HOG [9], and wavelet transforms [32]. They were also often combined with an unsupervised encoding, such as sparse coding [4] or Fisher Vectors(FVs) [30]. Indeed, many works in classical image processing or classification [18, 4, 30, 28] suggests that the local encoding of an image permit to describe efficiently an image. Additionally for some algorithms that rely on local neighbourhoods, the use of local descriptors is essential [23]. Observe that a representation based on local non overlapping spatial neighborhood is simpler to analyze, as there is no ad-hoc\nmixing of spatial information. Nevertheless, on large scale classification, this approach was surpassed by fully supervised learned methods [19].\nWe show that it is possible to apply, a similarly local, yet supervised encoding algorithm to a scattering transform, as suggested in the conclusion of [28]. First observe that at each spatial position u, a scattering coefficient S(u) corresponds to a descriptor of a local neighborhood of spatial size 2J . As explained in the first Subsection 2.1, each of our scattering coefficients are obtained using a stride of 2J , which means the final representation can be interpreted as a non-overlapping concatenation of descriptors. Then, let f be a cascade of fully connected layers that we identically apply on each Sx(u). Then f is a cascade of CNN operators with spatial support size 1 × 1, thus we write fSx , {f(Sx(u))}u. In the sequel, we do not make any distinction between the 1 × 1 CNN operators and the operator acting on Sx(u), ∀u. We refer to f as a Shared Local Encoder. We note that similarly to Sx, fSx corresponds to non-overlapping encoded descriptors. To learn a supervised classifier on a large scale image recognition task, we cascade fully connected layers on top of the SLE.\nCombined with a scattering network, the supervised SLE, has several advantages. Since the input corresponds to scattering coefficients, whose channels are structured, the first layer of f is as well structured. We further explain and investigate this first layer in Subsection 3.2. Unlike standard CNNs, there is no linear combinations of spatial neighborhoods of the different feature maps, thus the analysis of this network need only focus on the channel axis. Observe that if f was fed with raw images, for example in gray scale, it could not build any non-trivial operation except separating different level sets of these images.\nIn the next section, we investigate empirically this super-\nvised SLE trained on the ILSVRC2012 dataset."
    }, {
      "heading" : "3. Local Encoding of Scattering",
      "text" : "We evaluate the supervised SLE on the Imagenet ILSVRC2012 dataset. This is a large and challenging natural color image dataset consisting of 1.2 million training images and 50, 000 validation images, divided into 1000 classes. We then show some unique properties of this network and evaluate its features on a separate task."
    }, {
      "heading" : "3.1. Shared Local Encoder on Imagenet",
      "text" : "We first describe our training pipeline, which is similar to [41]. We trained our network for 90 epochs to minimize the standard cross entropy loss, using SGD with momentum 0.9 and a batch size of 256. We used a weight decay of 1× 10−4. The initial learning rate is 0.1, and is dropped off by 0.1 at epochs 30, 50, 70, 80. During the training process, each image is randomly rescaled, cropped, and flipped as in [13]. The final crop size is 224×224. At testing, we rescale\nthe image to a size of 256, and extract a center crop of size 224× 224.\nWe use an architecture which consists of a cascade of a scattering network, a SLE f , followed by fully connected layers. Figure 2 describes our architecture. We select the parameter J = 4 for our scattering network, which means the output representation has size 224\n24 × 224 24 = 14× 14 spa-\ntially and 1251 in the channel dimension. f is implemented as 3 layers of 1x1 convolutions F1, F2, F3 with layer size 1024. There are 2 fully connected layers of ouput size 1524. For all learned layers we use batch normalization [16] followed by a ReLU [19] non-linearity. We compute the mean and variance of the scattering coefficients on the whole Imagenet, and standardized each spatial scattering coefficients with it.\nTable 3.1 reports our numerical accuracies obtained with a single crop at testing, comparedwith local encodingmethods, and the AlexNet that was the state-of-the-art approach in 2012. We obtain 20.4% at Top 5 and 43.0% Top 1 errors. The performance is analogous to the AlexNet [19]. In term of architecture, our hybrid model is analogous, and comparable to that of [30, 28], for which SIFT features are extracted followed by FV [31] encoding. Observe the FV is an unsupervised encoding compared to our supervised encoding. Two approaches are then used: either the spatial localization is handled either by a Spatial Pyramid Pooling [20], which is then fed to a linear SVM, either the spatial variables are directly encoded in the FVs, and classified with a stack of four fully connected layers. This last method is a major difference with ours, as the obtained de-\nscriptor does not have a spatial indexing anymore which are instead quantified. Furthermore, in both case, the SIFT are densely extracted which correspond to approximatively 2 104 descriptors, whereas in our case, only 142 = 196 scattering coefficients are extracted. Indeed, we tackle the nonlinear aliasing (due to the fact the scattering transform is not oversampled) via random cropping during training, allowing to build an invariant to small translations. In Top 1, [30] and [28] obtain respectively 44.4% and 45.7%. Our method brings a substantial improvement of 1.4% and 2.7% respectively.\nThe BVLCAlexNet 3 obtains a of 43.1% single-crop Top 1 error, which is nearly equivalent to the 43.0% of our SLE network. The AlexNet has 8 learned layers and as explained before, large receptive fields. On the contrary, our training pipeline consists in 6 learned layers with constant receptive field of size 16 × 16, except for the fully connected layers that build a representation mixing spatial information from different locations. This is a surprising result, as it seems to suggest context information is only necessary at the very last layers, to reach AlexNet accuracy.\nWe study briefly the local SLE, which has only a spatial extent of 16 × 16, as a generic local image descriptor. We use the Caltech-101 benchmark which is a dataset of 9144 image and 102 classes. We followed the standard protocol for evaluation [4] with 10 folds and evaluate per class accuracy, with 30 training samples per class, using a linear SVM used with the SLE descriptors. Applying our raw scattering network leads to an accuracy of 62.8 ± 0.7, and the outputs features from F1, F2, F3 brings respectively an absolute improvement of 13.7, 17.3, 20.1. The accuracy of the final SLE descriptor is thus 82.9 ± 0.4, similar to that reported for the final AlexNet final layer in [42] and sparse coding with SIFT [4]. However in both cases spatial variability is removed, either by Spatial Pyramid Pooling [20], or the cascade of large filters. By contrasts the concatenation of SLE descriptors are completely local."
    }, {
      "heading" : "3.2. Interprating SLE’s first layer",
      "text" : "Finding structure in the kernel of the layers of depth less than 2 [39, 42] is a complex task, and few empirical analyses exist that shed light on the structure [17] of deeper layers. A scattering transform with scale J can be interpreted as a CNN with depth J [27], whose channels indexes correspond to different scattering frequency indexes, which is a structuration. This structure is consequently inherited by the first layerF1 of our SLE f . We analyseF1 and show that it builds explicitly invariance to local rotations, yet also that the Fourier bases associated to rotation are a natural bases of our operator. It is a promising direction to understand the nature of the two next layers.\n3https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-\n2012-val\nWe first establish some mathematical notions linked to the rotation group that we use in our analysis. For the sake of clarity, we do not consider the roto-translation group. For a given input image x, let rθ.x(u) , x(r−θ(u)) be the image rotated by angle θ, which corresponds to the linear action of rotation on images. Observe the scattering representation is covariant with the rotation in the following sense:\nS1(rθ.x)(θ1, u) = S1x(θ1 − θ, r−θu) , rθ.(S1x)(θ1, u)\nS2(rθ.x)(θ1, θ2, u) = S2x(θ1 − θ, θ2 − θ, r−θu)\n, rθ.(S2x)(θ1, θ2, u)\nBesides, in the case of the second order coefficients, (θ1, θ2) is covariant with rotations, but θ2 − θ1 is an invariant to rotation that correspond to a relative rotation.\nUnitary representation framework [36] permits the building of a Fourier transform on compact group, like rotations. It is even possible to build a scattering transform on the rototranslation group [33]. Fourier analysis permits the measurement of the smoothness of the operator and, in the case of CNN operator, it is a natural basis.\nWe can now numerically analyse the nature of the operations performed along angle variables by the first layer F1 of f , with output size K = 1024. Let us define as {F 0\n1 S0x, F\n1 1 S1x, F 2 1 S2x} the restrictions of F1 to the order\n0,1,2 scattering coefficients respectively. Let 1 ≤ k ≤ K an index of a feature channel and 1 ≤ c ≤ 3 the color index. In this case, F 0\n1 S0x is simply the weights associated to the\nsmoothing S0x. F 1 1 S1x depends only (k, c, j1, θ1), and F 2 1\ndepends on (k, c, j1, j2, θ1, θ2). We would like to characterize the smoothness of these operators with respect to the variables (θ1, θ2), because Sx is covariant to rotations.\nTo this end, we define by F̂ 1 1 , F̂ 2 1 the Fourier transform of these operators along the variables θ1 and (θ1, θ2) respectively. These operator are expressed in the tensorial Frequency domain, which corresponds to a change of basis. In this experiment, we normalized each filter of F such that they have a ℓ2 norm equal to 1, and each order of the scattering coefficients are normalized as well. Figure 3 shows the distribution of the amplitude of F̂ 11 , F̂ 2 2 . We observe that the distribution is shaped as a Laplace distribution, which is an indicator of sparsity.\nTo illustrate that this is a natural basis we explicitly sparsify this operator in its frequency basis and verify that empirically the network accuracy is minimally changed. We do this by thresholding by ǫ the coefficients of the operators in the Fourier domain. Specifically we replace the operators F̂ 11 , F̂ 2 1 by 1|F̂ 1\n1 |>ǫF̂ 1 1 and 1|F̂ 2 1 |>ǫF̂ 2 1 . We select an ǫ\nthat sets 80% of the coefficients to 0, which is indicated on Figure 3. Without retraining our network performance degrades by only an absolute value of 2% worse on Top 1 and Top 5 ILSVRC2012. We have thus shown that this basis permits a sparse approximation of the first layer, F1. We now show evidence that this operator builds an explicit invariant to local rotations.\nTo aid our analysis we introduce the following quanti-\nties:\nΩ1{F}(ω1) , ∑\nk,j1,c\n|F̂ 11 (k, c, j1, ωθ1)| 2 (1)\nΩ2{F}(ωθ1, ωθ2) , ∑\nk,c,j1,j2\n|F̂ 2 1 (k, c, j1, j2, ωθ1 , ωθ2)| 2\nThey correspond to the energy propagated by F1 for a given frequency, and permit to quantify the smoothness of our first layer operator w.r.t. the angular variables. Figure 4 shows variation of Ω1{F} and Ω2{F} along frequencies. For example, if F 1\n1 and F 2 1 were convolutional along θ1 and\n(θ1, θ2), these quantities would correspond to their respective singular values. One sees that the energy is concentrated in the low frequency domain, which indicates that F1 builds explicitly an invariant to local rotations."
    }, {
      "heading" : "4. Numerical performances of hybrid networks",
      "text" : "We now demonstrate cascading modern CNN architectures on top of the scattering network can produce high performance classification systems. We apply hybrid convolutional networks on the Imagenet ILSVRC 2012 dataset as well as the CIFAR-10 dataset and show that they can achieve performance comparable to modern end-to-end learned approaches. We then evaluate the hybrid networks in the setting of limited data by utilizing a subset of CIFAR10 as well as the STL-10 dataset and show that we can\nobtain substantial improvement in performance over analogous end-to-end learned CNNs."
    }, {
      "heading" : "4.1. Deep Hybrid CNNs on ILSVRC2012",
      "text" : "We showed in the previous section that a SLE followed by FC layers can produce results comparable with the AlexNet [19] on the Imagenet classification task. Here we consider cascading the scattering transform with a modern CNN architecture, such as Resnet [41, 13]. We take the Resnet-18 [41], as a reference and construct a similar architecture with only 10 layers on top of the scattering network. We utilize a scattering transform with J = 3 such that the CNN is learned over a spatial dimension of 28 × 28 and a channel dimension of 651 (3 color channels of 217 each). The ResNet-18 typically has 4 residual stages of 2 blocks each which gradually decrease the spatial resolution [41]. Since we utilize the scattering as a first stage we remove two blocks from our model. The network is described in Table 4.\nWe use the same optimization and data augmentation procedure described in Section 3.1 but with learning rate drops at 30, 60, and 80. We find that, when both methods are trained with the same settings of optimization and data\naugmentation, and when the number of parameters is similar (12.8M versus 11.7 M) the scattering network combined with a resnet can achieve analogous performance (11.4% Top 5 for our model versus 11.1 %), while utilizing fewer layers. The accuracy is reported in Table 2 and compared to other modern CNNs.\nThis demonstrates both that the scattering networks does not lose discriminative power and that it can be used to replace early layers of standard CNNs. We also note that learned convolutions occur over a drastically reduced spatial resolution without resorting to pre-trained early layers which can potentially lose discriminative information or become too task specific."
    }, {
      "heading" : "4.2. Hybrid Supervised and Unsupervised Representations on CIFAR-10",
      "text" : "We now consider the popular CIFAR-10 dataset consisting of colored images composed of 5×104 images for training, and 1× 104 images for testing divided into 10 classes. We perform two experiments, the first with a cascade of fully connected layers, that allows us to evaluate the scattering transform as an unsupervised representation. In a sec-\nond experiment, we again use a hybrid CNN architecture with a ResNet built on top of the scattering transform.\nFor the scattering transformwe used J = 2which means the output of the scattering stage will be 8× 8 spatially and 243 in the channel dimension. We follow the training procedure prescribed in [41] utilizing SGD with momentum of 0.9, batch size of 128, weigh decay of 5×10−4, and modest data augmentation of the dataset by using random cropping and flipping. The initial learning rate is 0.1, and we reduce it by a factor of 5 at epochs 60, 120 and 160. The models are trained for 200 epochs in total. We used the same optimization and data augmentation pipeline for training and evaluation in both case. We utilize batch normalization techniques at all layers which lead to a better conditioning of the optimization [16]. Table 4.1 reports the accuracy in the unsupervised and supervised settings and compares them to other approaches.\nIn the unsupervised comparison we consider the task of classification using only unsupervised features. Combining the scattering transform with a NN classifier consisting of 3 hidden layers, with width 1.1× 104, we show that one can obtain a new state of the art classification for the case of unsupervised features. This approach outperforms all methods utilizing learned and not learned unsupervised features further demonstrating the discriminative power of the scattering network representation.\nIn the case of the supervised task we compare to stateof-the-art approaches on CIFAR-10, all based on end-to-end learned CNNs. We use a similar hybrid architecture to the successful wide residual network (WRN) [41]. Specifically we modify the WRN of 16 layers which consists of 4 convolutional stages. Denoting the widening factor, k, after the scattering output we use a first stage of 32 × k. We add intermediate 1 × 1 to increase the effective depth, without increasing too much the number of parameters. Finally we apply a dropout of 0.2 as specified in [41]. Using a width of 32 we achieve an accuracy of 93.1%. This is superior to several benchmarks but performs worse than the original ResNet [13] and the wide resnet [41]. We note that training procedures for learning directly from images, including data augmentation and optimization settings, have been heavily optimized for networks trained directly on natural images, while we use them largely out of the box we do believe there are regularization techniques, normalization techniques, and data augmentation techniques which can be designed specifically for the scattering networks."
    }, {
      "heading" : "4.3. Limited samples setting",
      "text" : "A major application of a hybrid representation is in the setting of limited data. Here the learning algorithm is limited in the variations it can observe or learn from the data, such that introducing a geometric prior can substantially improve performance. We evaluate our algorithm on the\nlimited sample setting using a subset of CIFAR-10 and the STL-10 dataset."
    }, {
      "heading" : "4.3.1 CIFAR-10",
      "text" : "We take subsets of decreasing size of the CIFAR dataset and train both baseline CNNs and counterparts that utilize the scattering as a first stage. We perform experiments using subsets of 1000, 500, and 100 samples, that are split uniformly amongst the 10 classes.\nWe use as a baseline the Wide ResNet [41] of depth 16 and width 8, which shows near state-of-the-art performance on the full CIFAR-10 task in the supervised setting. This network consists of 4 stages of progressively decreasing spatial resolution detailed in Table 1 of [41]. We construct a comparable hybrid architecture that removes a single stage and all strides, as the scattering already down-sampled the spatial resolution. This architecture is described in Table 5. Unlike the baseline, refereed from here-on as WRN 16-8, our architecture has 12 layers and equivalent width, while keeping the spatial resolution constant through all stages prior to the final average pooling.\nWe use the same training settings for our baseline, WRN 16-8, and our hybrid scattering and WRN-12. The settings are the same as those described for CIFAR-10 in the previous section with the only difference being that we apply a multiplier to the learning rate schedule and to the maximum number of epochs. The multiplier is set to 10,20,100 for the 1000,500, and 100 sample case respectively. For example the default schedule of 60,120,160 becomes 600,1200,1600 for the case of 1000 samples and a multiplier of 10. Finally in the case of 100 samples we use a batch size of 32 in lieu of 128.\nTable 6 corresponds to the averaged accuracy over 5 different subsets, with the corresponding standard error. In this small sample setting, a hybrid network outperforms the purely CNN based baseline, particularly when the sample size is smaller. This is not surprising as we incorporate a geometric prior in the representation."
    }, {
      "heading" : "4.3.2 STL-10",
      "text" : "The SLT-10 dataset consists of colored images of size 96× 96, with only 5000 labeled images in the training set divided\nequally in 10 classes and 8000 images in the test set. The larger size of the images and the small number of available samples make this a challenging image classification task. The dataset also provides 100 thousand unlabeled images for unsupervised learning. We do not utilize these images in our experiments, yet we find we are able to outperform all methods which learn unsupervised representations using these unlabeled images, obtaining very competitive results on the STL-10 dataset.\nWe apply a hybrid convolutional architecture, similar to the one applied in the small sample CIFAR task, adapted to the size of 96 × 96. The architecture is described in Table 5 and is similar to that used in the CIFAR small sample task. We use the same data augmentation as with the CIFAR datasets. We apply SGD with learning rate 0.1 and learning rate decay of 0.2 applied at epochs 1500,2000,3000,4000. Training is run for 5000 epochs. We use at training and evaluation the standard 10 folds procedure which takes 1000 training images. The averaged result for 10 folds is reported in Table 7. Unlike other approaches we do not use the 4000 remaining training image to perform hyper-parameter tuning on each fold, as this is not representative of typical small sample situations, instead we train the same settings on each fold. The best reported result in the purely supervised case is a CNN [37, 11] whose hyper parameters have been automatically tuned using 4000 images for validation achieving 70.1% accuracy. The other competitive methods on this dataset utilize the unlabeled data to learn in an unsupervised manner before applying supervised methods. To compare with [14] we also train on the full training set of 5000 images obtaining an accuracy of 87.6% on the test set, which is substantially higher than 81.3% reported in [14] using unsupervised learning and the full unlabeled and labeled training set. The competing techniques add several hyper parameters and require an additional engineering process. Applying a hybrid network is on the other hand straightforward and is very competitive with all the existing approaches, without using any unsupervised learning.\nIn addition to showing hybrid networks perform well\nin the small sample regime these results, along with our unsupervised CIFAR-10 results, suggest that completely unsupervised feature learning on natural image data, for downstream discriminative tasks, may still not outperform supervised learning methods and pre-defined representations. One possible explanation is that in the case of natural images, learning in an unsupervised way more complex variabilities than geometric ones ( e.g the rototranslation group), might be very challenging or possibly ill-posed."
    }, {
      "heading" : "5. Conclusion",
      "text" : "This work demonstrates a competitive approach for large scale visual recognition, based on scattering networks, in particular for ILSVRC2012. When compared with unsupervised representation on CIFAR-10 or small data regimes on CIFAR-10 and STL-10, we demonstrate state-of-the-art results. We build a supervised Shared Local Encoder that permits the scattering networks to surpass other local encoding methods on ILSVRC2012. This network of just 3 learned layers permits analysis on the operation performed.\nOur work also suggests that pre-defined features are still of interest and can provide enlightenment on deep learning techniques and to allow them to be more interpretable. Combined with appropriate learning methods, they could permit having more theoretical guarantees that are necessary to engineer better deep models and stable representations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thankMathieu Andreux,Matthew Blaschko, Carmine Cella, Bogdan Cirstea, Michael Eickenberg, Stéphane Mallat for helpful discussions and support. The authors would also like to thank Rafael Marini and Nikos Paragios for use of computing resources. We would like to thank Florent Perronnin for providing important details of their work. This work is funded by the ERC grant InvariantClass 320959, via a grant for PhD Students of the Conseil régional d’Ile-de-France (RDM-IdF), Internal Funds KU Leuven, FP7-MC-CIG 334380, an Amazon Academic Research Award, and DIGITEO 2013-0788D - SOPRANO."
    } ],
    "references" : [ {
      "title" : "Generalized analytic signals in image processing: comparison, theory and applications",
      "author" : [ "S. Bernstein", "J.-L. Bouchot", "M. Reinhardt", "B. Heise" ],
      "venue" : "Quaternion and Clifford Fourier Transforms and Wavelets, pages 221–246. Springer,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multipath sparse coding using hierarchical matching pursuit",
      "author" : [ "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 660–667,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Unsupervised feature learning for rgb-d based object recognition",
      "author" : [ "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "Experimental Robotics, pages 387–402. Springer,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Ask the locals: multi-way local pooling for image recognition",
      "author" : [ "Y.-L. Boureau", "N. Le Roux", "F. Bach", "J. Ponce", "Y. LeCun" ],
      "venue" : "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2651–2658. IEEE,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Audio texture synthesis with scattering moments",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "arXiv preprint arXiv:1311.0407,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Invariant scattering convolution networks",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 35(8):1872–1886,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning stable group invariant representations with convolutional networks",
      "author" : [ "J. Bruna", "A. Szlam", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1301.3537,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Selecting receptive fields in deep networks",
      "author" : [ "A. Coates", "A.Y. Ng" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2528–2536,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "N. Dalal", "B. Triggs" ],
      "venue" : "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Inverse problems with invariant multiscale statistics",
      "author" : [ "I. Dokmanić", "J. Bruna", "S. Mallat", "M. de Hoop" ],
      "venue" : "arXiv preprint arXiv:1609.05502,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Discriminative unsupervised feature learning with convolutional neural networks",
      "author" : [ "A. Dosovitskiy", "J.T. Springenberg", "M. Riedmiller", "T. Brox" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 766–774,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning both weights and connections for efficient neural network",
      "author" : [ "S. Han", "J. Pool", "J. Tran", "W. Dally" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1135–1143,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep unsupervised learning through spatial contrasting",
      "author" : [ "E. Hoffer", "I. Hubara", "N. Ailon" ],
      "venue" : "arXiv preprint arXiv:1610.00243,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "What makes imagenet good for transfer learning",
      "author" : [ "M. Huh", "P. Agrawal", "A.A. Efros" ],
      "venue" : "arXiv preprint arXiv:1608.08614,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multiscale hierarchical convolutional networks",
      "author" : [ "J.-H. Jacobsen", "E. Oyallon", "S. Mallat", "A.W. Smeulders" ],
      "venue" : "arXiv preprint arXiv:1703.01775,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "The structure of locally orderless images",
      "author" : [ "J.J. Koenderink", "A.J. Van Doorn" ],
      "venue" : "International Journal of Computer Vision, 31(2-3):159–168,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Beyond bags of features: Spatial pyramid matching for recognizing natural  scene categories",
      "author" : [ "S. Lazebnik", "C. Schmid", "J. Ponce" ],
      "venue" : "Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pages 2169–2178. IEEE,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis",
      "author" : [ "Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 3361–3368. IEEE,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Convolutional networks and applications in vision",
      "author" : [ "Y. LeCun", "K. Kavukcuoglu", "C. Farabet" ],
      "venue" : "In ISCAS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Object recognition from local scale-invariant features",
      "author" : [ "D.G. Lowe" ],
      "venue" : "Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150–1157. Ieee,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Group invariant scattering",
      "author" : [ "S. Mallat" ],
      "venue" : "Communications on Pure and Applied Mathematics, 65(10):1331–1398,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Understanding deep convolutional networks",
      "author" : [ "S. Mallat" ],
      "venue" : "Phil. Trans. R. Soc. A, 374(2065):20150203,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Building a regular decision boundary with deep networks",
      "author" : [ "E. Oyallon" ],
      "venue" : "arXiv preprint arXiv:1703.01775,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Deep roto-translation scattering for object classification",
      "author" : [ "E. Oyallon", "S. Mallat" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2865– 2873,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fisher vectors meet neural networks: A hybrid classification architecture",
      "author" : [ "F. Perronnin", "D. Larlus" ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3743–3752,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "A. Radford", "L. Metz", "S. Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "High-dimensional signature compression for large-scale image classification",
      "author" : [ "J. Sánchez", "F. Perronnin" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Image classification with the fisher vector: Theory and practice",
      "author" : [ "J. Sánchez", "F. Perronnin", "T. Mensink", "J. Verbeek" ],
      "venue" : "International journal of computer vision, 105(3):222–245,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Realistic modeling of simple and complex cell tuning in the hmax model, and implications for invariant object recognition in cortex",
      "author" : [ "T. Serre", "M. Riesenhuber" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Rotation, scaling and deformation invariant scattering for texture discrimination",
      "author" : [ "L. Sifre", "S. Mallat" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1233–1240,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1412.6806,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Highway networks",
      "author" : [ "R.K. Srivastava", "K. Greff", "J. Schmidhuber" ],
      "venue" : "arXiv preprint arXiv:1505.00387,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unitary representations and harmonic analysis: an introduction, volume 44",
      "author" : [ "M. Sugiura" ],
      "venue" : "Elsevier,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Multi-task bayesian optimization",
      "author" : [ "K. Swersky", "J. Snoek", "R.P. Adams" ],
      "venue" : "Advances in neural information processing systems, pages 2004–2012,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "These de doctorat de lEcole normale supérieure",
      "author" : [ "I. Waldspurger" ],
      "venue" : "PhD thesis, École normale supérieure,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2014
    }, {
      "title" : "Wide residual networks",
      "author" : [ "S. Zagoruyko", "N. Komodakis" ],
      "venue" : "arXiv preprint arXiv:1605.07146,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "European conference on computer vision, pages 818–833. Springer,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stacked what-where auto-encoders",
      "author" : [ "J. Zhao", "M. Mathieu", "R. Goroshin", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1506.02351,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Deep architectures build representations that lead to state-of-the-art results on image classification tasks [13].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 21,
      "context" : "These architectures are designed as very deep cascades of non-linear end-to-end learned modules [22].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 41,
      "context" : "When trained on large-scale datasets they have been shown to produce representations that are transferable to other datasets [42, 15], which indicate they have captured generic properties of a supervised task that consequently do not need to be learned.",
      "startOffset" : 125,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "When trained on large-scale datasets they have been shown to produce representations that are transferable to other datasets [42, 15], which indicate they have captured generic properties of a supervised task that consequently do not need to be learned.",
      "startOffset" : 125,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "Indeed several works indicate geometrical structures in the filters of the earlier layers [19, 39] of Deep CNNs.",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 38,
      "context" : "Indeed several works indicate geometrical structures in the filters of the earlier layers [19, 39] of Deep CNNs.",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 37,
      "context" : "However, understanding the precise operations performed by those early layers is a complicated [38, 26] and possibly intractable task.",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : "However, understanding the precise operations performed by those early layers is a complicated [38, 26] and possibly intractable task.",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "A potential candidate for an image representation is the SIFT descriptor [23] that was widely used before 2012 as a feature extractor in classification pipelines [30, 31].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 29,
      "context" : "A potential candidate for an image representation is the SIFT descriptor [23] that was widely used before 2012 as a feature extractor in classification pipelines [30, 31].",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 30,
      "context" : "A potential candidate for an image representation is the SIFT descriptor [23] that was widely used before 2012 as a feature extractor in classification pipelines [30, 31].",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : "However, several works indicate that this is not a generic enough representation to build further modules on top of [21, 2].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "However, several works indicate that this is not a generic enough representation to build further modules on top of [21, 2].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "A major improvement over SIFT can be found in the scattering transform [24, 6, 33], which is a type of deep convolutional network, which permits to retain discriminative information normally discarded by methods like SIFT while introducing geometric invariances and stability.",
      "startOffset" : 71,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "A major improvement over SIFT can be found in the scattering transform [24, 6, 33], which is a type of deep convolutional network, which permits to retain discriminative information normally discarded by methods like SIFT while introducing geometric invariances and stability.",
      "startOffset" : 71,
      "endOffset" : 82
    }, {
      "referenceID" : 32,
      "context" : "A major improvement over SIFT can be found in the scattering transform [24, 6, 33], which is a type of deep convolutional network, which permits to retain discriminative information normally discarded by methods like SIFT while introducing geometric invariances and stability.",
      "startOffset" : 71,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : "Scattering transforms have been shown to already produce representations that lead to the top results on complex image datasets when compared to other unsupervised representations (even learned ones) [27].",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 27,
      "context" : "Related to our work [28] proposed a hybrid representation for large scale image recognition combining a prede-",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 26,
      "context" : "A scattering network belongs to the class of CNNs whose filters are fixed as wavelets [27].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 23,
      "context" : "The construction of this network has strong mathematical foundations [24], meaning it is well understood, relies on few parameters and is stable to a large class of geometric transformations.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : "In general, the parameters of this representation do not need to be adapted to the bias of the dataset [27], making its output a suitable generic representation.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 26,
      "context" : "With appropriate discretization [27], {AJx,W1x} is approximatively an isometry on the set of signals with limited bandwidth, and this implies the energy of the signal is preserved.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Here, the mother wavelet is analytic, thus |W1x| is regular [1] which implies that the energy in Fourier of |W1x| is more likely to be contained in a lower frequency domain than W1x.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "j1 < j2 because non-increasing paths have been shown to bear no energy [6].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "We do not compute higher order scatterings, because their energy is negligible [6].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : ", and the original image is down-sampled by a factor 2 [6].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "This representation is proved to linearize small deformations [24] of images, be non-expansive and almost complete [10, 5], which makes it an ideal input to a deep network algorithm, that can build invariants to this local variability via a first linear operator.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "This representation is proved to linearize small deformations [24] of images, be non-expansive and almost complete [10, 5], which makes it an ideal input to a deep network algorithm, that can build invariants to this local variability via a first linear operator.",
      "startOffset" : 115,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "This representation is proved to linearize small deformations [24] of images, be non-expansive and almost complete [10, 5], which makes it an ideal input to a deep network algorithm, that can build invariants to this local variability via a first linear operator.",
      "startOffset" : 115,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "Scattering transforms have yielded excellent numerical results [6] on datasets where the variabilities are completely known, such as MNIST or FERET.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 26,
      "context" : "Although applying the scattering transform on datasets like CIFAR or Caltech leads to nearly state-of-the-art results in comparison to other unsupervised representations there is a large gap in performance when comparing to supervised representations [27].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 24,
      "context" : "Recent works [25, 7, 17] have suggested that deep networks could build an approximation of the group of symmetries of a classification task and apply transformations along the orbits of this group, like convolutions.",
      "startOffset" : 13,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "Recent works [25, 7, 17] have suggested that deep networks could build an approximation of the group of symmetries of a classification task and apply transformations along the orbits of this group, like convolutions.",
      "startOffset" : 13,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "Recent works [25, 7, 17] have suggested that deep networks could build an approximation of the group of symmetries of a classification task and apply transformations along the orbits of this group, like convolutions.",
      "startOffset" : 13,
      "endOffset" : 24
    }, {
      "referenceID" : 24,
      "context" : "[25] motivates that to each layer corresponds an approximated Lie group of symmetry, and this approximation is progressive, in the sense that the dimension of these groups is increasing with depth.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Discovering explicitly the next new and non-geometrical groups of symmetry is however a difficult task [17]; nonetheless, the roto-translation group seems to be a good initialization for the first layers.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "The second, referred to as a hybrid CNN, is a cascade of a scattering network and a standard CNN architecture, such as a ResNet [13].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : "For example, at depth 2 of [19], the effective spatial support of the corresponding filter is already 32 pixels (out of 224).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 39,
      "context" : "The specific representations derived from CNNs trained on large scale image recognition are often used as representations in other computer vision tasks or datasets [40, 42].",
      "startOffset" : 165,
      "endOffset" : 173
    }, {
      "referenceID" : 41,
      "context" : "The specific representations derived from CNNs trained on large scale image recognition are often used as representations in other computer vision tasks or datasets [40, 42].",
      "startOffset" : 165,
      "endOffset" : 173
    }, {
      "referenceID" : 29,
      "context" : "On the other hand prior to 2012 local encoding methods led to state of the art performance on large scale visual recognition tasks [30].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 22,
      "context" : "In these approaches local neighborhoods of an image were encoded using method such as SIFT descriptors [23], HOG [9], and wavelet transforms [32].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "In these approaches local neighborhoods of an image were encoded using method such as SIFT descriptors [23], HOG [9], and wavelet transforms [32].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 31,
      "context" : "In these approaches local neighborhoods of an image were encoded using method such as SIFT descriptors [23], HOG [9], and wavelet transforms [32].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "They were also often combined with an unsupervised encoding, such as sparse coding [4] or Fisher Vectors(FVs) [30].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 29,
      "context" : "They were also often combined with an unsupervised encoding, such as sparse coding [4] or Fisher Vectors(FVs) [30].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "Indeed, many works in classical image processing or classification [18, 4, 30, 28] suggests that the local encoding of an image permit to describe efficiently an image.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "Indeed, many works in classical image processing or classification [18, 4, 30, 28] suggests that the local encoding of an image permit to describe efficiently an image.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : "Indeed, many works in classical image processing or classification [18, 4, 30, 28] suggests that the local encoding of an image permit to describe efficiently an image.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "Indeed, many works in classical image processing or classification [18, 4, 30, 28] suggests that the local encoding of an image permit to describe efficiently an image.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "Additionally for some algorithms that rely on local neighbourhoods, the use of local descriptors is essential [23].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "Nevertheless, on large scale classification, this approach was surpassed by fully supervised learned methods [19].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "We show that it is possible to apply, a similarly local, yet supervised encoding algorithm to a scattering transform, as suggested in the conclusion of [28].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 40,
      "context" : "We first describe our training pipeline, which is similar to [41].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "During the training process, each image is randomly rescaled, cropped, and flipped as in [13].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 27,
      "context" : "FV + FC [28] 55.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 29,
      "context" : "4 FV + SVM [30] 54.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 27,
      "context" : "[28] single-crop result was",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "For all learned layers we use batch normalization [16] followed by a ReLU [19] non-linearity.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "For all learned layers we use batch normalization [16] followed by a ReLU [19] non-linearity.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "The performance is analogous to the AlexNet [19].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : "In term of architecture, our hybrid model is analogous, and comparable to that of [30, 28], for which SIFT features are extracted followed by FV [31] encoding.",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 27,
      "context" : "In term of architecture, our hybrid model is analogous, and comparable to that of [30, 28], for which SIFT features are extracted followed by FV [31] encoding.",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 30,
      "context" : "In term of architecture, our hybrid model is analogous, and comparable to that of [30, 28], for which SIFT features are extracted followed by FV [31] encoding.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "Two approaches are then used: either the spatial localization is handled either by a Spatial Pyramid Pooling [20], which is then fed to a linear SVM, either the spatial variables are directly encoded in the FVs, and classified with a stack of four fully connected layers.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 29,
      "context" : "In Top 1, [30] and [28] obtain respectively 44.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 27,
      "context" : "In Top 1, [30] and [28] obtain respectively 44.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "We followed the standard protocol for evaluation [4] with 10 folds and evaluate per class accuracy, with 30 training samples per class, using a linear SVM used with the SLE descriptors.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 41,
      "context" : "4, similar to that reported for the final AlexNet final layer in [42] and sparse coding with SIFT [4].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "4, similar to that reported for the final AlexNet final layer in [42] and sparse coding with SIFT [4].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "However in both cases spatial variability is removed, either by Spatial Pyramid Pooling [20], or the cascade of large filters.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 38,
      "context" : "Finding structure in the kernel of the layers of depth less than 2 [39, 42] is a complex task, and few empirical analyses exist that shed light on the structure [17] of deeper layers.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 41,
      "context" : "Finding structure in the kernel of the layers of depth less than 2 [39, 42] is a complex task, and few empirical analyses exist that shed light on the structure [17] of deeper layers.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "Finding structure in the kernel of the layers of depth less than 2 [39, 42] is a complex task, and few empirical analyses exist that shed light on the structure [17] of deeper layers.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 26,
      "context" : "A scattering transform with scale J can be interpreted as a CNN with depth J [27], whose channels indexes correspond to different scattering frequency indexes, which is a structuration.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 35,
      "context" : "Unitary representation framework [36] permits the building of a Fourier transform on compact group, like rotations.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 32,
      "context" : "It is even possible to build a scattering transform on the rototranslation group [33].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "1 61M VGG-16 [12] 68.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 40,
      "context" : "7M Resnet-200 [41] 78.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "Roto-Scat + SVM [27] 82.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "3 ExemplarCNN [11] 84.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 28,
      "context" : "3 DCGAN [29] 82.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 34,
      "context" : "1 Highway network [35] 92.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 33,
      "context" : "4 All-CNN [34] 92.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 40,
      "context" : "8 WRN 16 - 8 [41] 95.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 40,
      "context" : "7 WRN 28 - 10 [41] 96.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "We showed in the previous section that a SLE followed by FC layers can produce results comparable with the AlexNet [19] on the Imagenet classification task.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 40,
      "context" : "Here we consider cascading the scattering transform with a modern CNN architecture, such as Resnet [41, 13].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "Here we consider cascading the scattering transform with a modern CNN architecture, such as Resnet [41, 13].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 40,
      "context" : "We take the Resnet-18 [41], as a reference and construct a similar architecture with only 10 layers on top of the scattering network.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 40,
      "context" : "The ResNet-18 typically has 4 residual stages of 2 blocks each which gradually decrease the spatial resolution [41].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 40,
      "context" : "Taking the convention of [41] we describe the convolution size and channels in the Stage details",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 40,
      "context" : "We follow the training procedure prescribed in [41] utilizing SGD with momentum of 0.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "We utilize batch normalization techniques at all layers which lead to a better conditioning of the optimization [16].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 40,
      "context" : "We use a similar hybrid architecture to the successful wide residual network (WRN) [41].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 40,
      "context" : "2 as specified in [41].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "This is superior to several benchmarks but performs worse than the original ResNet [13] and the wide resnet [41].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 40,
      "context" : "This is superior to several benchmarks but performs worse than the original ResNet [13] and the wide resnet [41].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 40,
      "context" : "We use as a baseline the Wide ResNet [41] of depth 16 and width 8, which shows near state-of-the-art performance on the full CIFAR-10 task in the supervised setting.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 40,
      "context" : "This network consists of 4 stages of progressively decreasing spatial resolution detailed in Table 1 of [41].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 36,
      "context" : "6 CNN[37] 70.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "Exemplar CNN [11] 75.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 42,
      "context" : "3 Stacked what-where AE [43] 74.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "33 Hierarchical Matching Pursuit (HMP) [3] 64.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "5±1 Convolutional K-means Network [8] 60.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 36,
      "context" : "The best reported result in the purely supervised case is a CNN [37, 11] whose hyper parameters have been automatically tuned using 4000 images for validation achieving 70.",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "The best reported result in the purely supervised case is a CNN [37, 11] whose hyper parameters have been automatically tuned using 4000 images for validation achieving 70.",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "To compare with [14] we also train on the full training set of 5000 images obtaining an accuracy of 87.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "3% reported in [14] using unsupervised learning and the full unlabeled and labeled training set.",
      "startOffset" : 15,
      "endOffset" : 19
    } ],
    "year" : 2017,
    "abstractText" : "We use the scattering network as a generic and fixed initialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1 × 1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their endto-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}