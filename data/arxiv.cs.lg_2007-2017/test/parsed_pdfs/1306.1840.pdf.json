{
  "name" : "1306.1840.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Loss-Proportional Subsampling for Subsequent ERM",
    "authors" : [ "Paul Mineiro", "Nikos Karampatziakis" ],
    "emails" : [ "pmineiro@microsoft.com", "nikosk@microsoft.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Data volumes are growing at a faster rate than available computing power, storage space, or network bandwidth. This has fueled interest in distributed approaches to machine learning. However, the substantial jump in communication costs between a multicore and a multicomputer system currently confines many popular techniques to the single computer regime. Consequently, even those machine learning workflows that today originate with distributed data in a clustered environment often terminate with the learning problem being solved on a single machine. Furthermore, because of computation, storage, or network limitations there is often a subsampling step between the data store and the single machine.\nOur concern here is to make the subsampling step as statistically efficient as possible, knowing that the data will subsequently be given to an empirical risk minimization (ERM) algorithm. What properties can we hope to have from a subsample of the original data? If we were to perform ERM on the original data, the\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nexcess risk would be O(1/ √ n), where n is the size of the original data set. If we uniformly subsampled the original data set to size m and performed ERM on this subsample, the excess risk would be O(1/ √ m). From the perspective of ERM this compression is lossy. Ideally, we would like to reduce the data set size while still retaining excess risk O(1/ √ n).\nAt first blush, this appears to be the active learning scenario. However, the subsampling strategy has access to all examples and labels but cannot assume knowledge of the hypothesis set ultimately used for ERM. This is because the types of hypotheses ultimately considered are presumed to be intractable on the full dataset. Instead we will only assume access to a subset of the (otherwise unknown) hypothesis set. As an example, suppose that the training on the subsample will be via neural networks with an unknown architecture, but known to have direct connections from the input layer to the output layer. In this case, the set of linear predictors is a subset of the final hypothesis space, and linear learning is feasible at terafeature scale (Agarwal et al., 2011). Our results show we can compress the data set by encoding relative to the mistakes of a linear predictor, without distorting the subsequent ERM over neural networks; the amount of compression possible is limited by the quality of the linear predictor.\nIn effect many current workflows look like 1) a subsampling step followed by 2) a model selection step. We propose to replace the first step by 1a) a simpler model selection step, followed by 1b) a subsampling step. Although our approach does apply recursively, in practice we believe much of the benefit would be captured by the introduction of a single simple model selection step prior to subsampling."
    }, {
      "heading" : "1.1. Relation to Prior Work",
      "text" : "This work bears strong resemblance to multiple threads of research, and the main contribution is inter-\nar X\niv :1\n30 6.\n18 40\nv2 [\ncs .L\nG ]\n2 3\nJu n\n20 13\npreting previous understanding and practices through the lens of empirical Bernstein bounds (Maurer & Pontil, 2009).\nBoosting algorithms have popularized the idea of sequential model selection via importance weighting. Of particular relevance is FilterBoost (Bradley & Schapire, 2008), which leverages the correspondence between importance-weighting and rejection sampling in the large data setting: the large data set is iteratively subjected to a weak learner on a manageable subsample. The scheme herein is akin to a degenerate two-stage version of FilterBoost, but the differences are important. Theoretically, the second stage of our procedure is on an unknown superset of hypotheses. This mandates enforcing a minimum sampling probability related to the quality of the initial model. Practically, in typical workflows there really is a large fixed dataset from which a single subsample is extracted and subsequently used for (multiple!) model selection experiments. This commonplace scenario motivates the study of this particular two-stage procedure.\nThe architecture of cascading classifiers with increasing computational complexity to achieve an efficient ensemble was popularized by Viola and Jones (Viola & Jones, 2001). More recently, the FCBoost algorithm (Saberian & Vasconcelos, 2010) was introduced, which implements fully automatic cascade design within a boosting framework. While similar architecturally, cascades focus on short-circuiting a sequential chain of classifiers in order to minimize evaluation time complexity, whereas the primary concern here is reduction of training set size. This is necessitated by the superlinear scaling in training time complexity of many popular methods such as decision trees and Gaussian processes. Nonetheless the procedure outlined herein can be considered a simple two-stage cascade, leading to the same important differences as highlighted in the previous paragraph.\nActive learning is concerned with achieving good generalization while limiting the number of labels revealed to the learner, and our results here are clearly related. In particular our subsampling rate is lowerbounded similarly to worst-case label complexity results (Beygelzimer et al., 2008). It is tempting to conclude that more sophisticated active learning approaches would achieve lower subsampling rates, but again the fact that the second stage of our procedure is on an unknown superset of hypotheses is important. In particular, since disagreement regions (Hanneke, 2007) only increase on the hypothesis superset, for our particular scenario an active learning algorithm which attempts to exploit what appears to be an iso-\nlated empirical minimizer is subject to poor worst-case behaviour.\nInteresting connections exist between this work and research directions in sample compression and Minimum Description Length (MDL). Sample compression algorithms (Floyd & Warmuth, 1995) learn classifiers that can be described by only a small fraction of the training data. Algorithms based on the MDL principle (Grünwald, 2007) treat training examples and models as data that need to be transmitted to a receiver and they operate by conceptually finding a code that minimizes communication costs. However, both sample compression and MDL are aware of the subsequent steps in the protocol they are operating. In sample compression the reconstruction function has to match the compression function and in MDL the code used to communicate has to be known to the receiver. In our work, the subsampling step is oblivious to the subsequent ERM step which provides great flexibility in practical applications.\nSubsampling to mitigate computational constraints during training is an old idea and a common practice in the machine learning community. The exact setup considered here was investigated empirically almost 2 decades ago (Lewis & Catlett, 1994), where a simpler computationally inexpensive hypothesis was used to select importance-weighted training data for a more expressive computationally intensive hypothesis. More recent work from neural language modeling (Bengio & Senecal, 2008) indicates the issue of controlling worstcase subsample deviations remains open. In that work, the authors address the issue by adapting the compressing hypothesis to match the empirical minimizer (both of which are learned online). The approach is this paper is much simpler: we enforce a minimum sampling probability to upper-bound worst-case empirical variance."
    }, {
      "heading" : "1.2. Contributions",
      "text" : "We define an optimal subsample selection problem given a compressing hypothesis using empirical Bernstein bounds. The solution is a simple strategy parameterized by the overall subsample budget. We prove a bound on deviations between the subsample empirical risk minimizer and the true risk minimizer which compares favorably to ERM on the original sample.\nWe demonstrate the effectiveness by achieving competitive results on a large public dataset for which naive subsampling techniques are not an effective strategy."
    }, {
      "heading" : "2. Derivation of the Sampling Strategy",
      "text" : "Our starting point is the old practitioner’s chestnut: when faced with a binary classification problem with a highly unbalanced label distribution, discard examples associated with the more frequent label until the relative number of examples with each label is about even. Remaining examples, associated with the formally more frequent class label, must be importance weighted to retain an unbiased sample. Curiously, for logistic regression this can be done analytically by adjusting the bias weight after training, and in language modeling this results in large training time speedups without significant degradation of generalization performance (Xu et al., 2011).\nAlthough this practice is widespread and intuitively reasonable, our goal is a satisfactory theoretical explanation of the approach which in turn suggests how to improve the technique. Note that since we can leverage the labels of the entire dataset, our setup does not obviously correspond to active learning. We believe a thorough understanding requires a non-uniform view of the hypothesis space, and in particular, our results leverage empirical Bernstein bounds.\nA key observation is that subsampling the more frequent class exactly preserves the empirical 0-1 loss of the best constant hypothesis, because the discarded points have a loss of 0. In fact, if the only purpose of the subsample was to transmit the empirical risk of the best constant hypothesis, all instances associated with the more frequent class could be discarded. However, the subsample will be used for empirical risk minimization. By definition, the empirical minimizer on the subsample will have empirical risk on the subsample at least as good as the best constant hypothesis. However, the deviation between the empirical risk on the subsample and the true risk might be large. Fortunately, retaining the instances associated with the more frequent class has the effect of bounding the worst-case empirical variance of the loss of the subsample empirical risk minimizer. This, together with an application of empirical Bernstein bounds, indicates that the deviation between subsample risk and true risk is small."
    }, {
      "heading" : "2.1. A Compressing Hypothesis",
      "text" : "One way to generalize frequent label subsampling is to let the set of initial predictors considered be richer than the constant predictors. For instance, the class labels might be approximately balanced. Yet, when conditioned on a single feature, the class label distribution might be somewhat imbalanced. In general, we might be able to easily search at scale over sim-\nple hypothesis spaces prior to subsampling for model selection: can we take advantage of this?\nConsider any hypothesis h̃ which is guaranteed to be in the set of hypotheses for the final ERM step. For now, let us assume the subsampling procedure does not distort the empirical risk of h̃. Then, we can upper bound the subsample empirical risk of the subsample empirical risk minimizer, which in turn will allow us to bound deviations of the subsample minimizer from the true underlying distribution.\nFormally, consider that we have an i.i.d. empirical sample X = (X1, . . . , Xn) of size n. With a slight abuse of notation in what follows, we will consider the loss function fixed and we will not distinguish between a hypothesis h and the induced loss function `h. Therefore, we do not need to distinguish between features and labels. Empirical risk minimization on the original sample would be driven by the risk\nRX(h) = 1\nn n∑ i=1 h(Xi).\nGiven h, our subsampling strategy makes conditionally independent decisions to sample each Xi, where Qi ∈ {0, 1} is a random variable indicating whether or not Xi is included in the subsample, and Pi = E[Qi|X] is the sampling probability. The final ERM step minimizes importance-weighted empirical risk on the resulting subsample:\nRQ,X(h) = 1\nn n∑ i=1 Qi Pi h(Xi).\nWe want to limit the degradation introduced by subsampling, i.e., bound deviations between RX(h) and RQ,X(h). The empirical Bernstein bound (Maurer & Pontil, 2009) suggests that deviations are driven by the subsample empirical variance\nVn(h|Q,X)\n= 1\nn− 1 n∑ i=1 ( Qi Pi h(Xi)− ( 1 n n∑ i=1 h(Xi) ))2 .\nIt turns out the worst case scenario is when h has high loss on examples where Pi is small. For any distribution D and any random variable Z ∈ [0, w] we have VD[Z] ≤ wED[Z]. For the subsample empirical distribution, in particular, Vn(h|Q,X) ≤ 1/(mini Pi)RQ,X(h). Thus, we can bound the worstcase subsample empirical variance of RQ,X(h) by enforcing a minimum sampling probability Pmin. If we knew that the subsample empirical minimizer ĥ had subsample empirical risk RQ,X(ĥ), we could choose\nPmin = RQ,X(ĥ). This choice guarantees that deviations introduced by subsampling are of the same order as deviations in the original sample. Unfortunately we cannot choose Pmin in this fashion as it involves circular reasoning.\nInstead we can leverage the compressing hypothesis h̃ to choose Pmin. In particular, if the subsampling procedure does not distort the empirical risk of h̃, then RQ,X(ĥ) ≤ RQ,X(h̃) ≈ RX(h̃), so we can set Pmin = RX(h̃). This indicates bounding the deviation of h̃ between sample and subsample is critical. For h̃ we can use Bennett’s inequality to bound the deviation introduced by subsampling via the variance of the subsampling procedure,\nVQ(h̃|X)\n= EQ  1 n n∑ i=1 ( Qi Pi h̃(Xi)− EQ [ 1 n n∑ i=1 Qi Pi h̃(Xi) ])2 ∣∣∣∣X \n= 1\nn n∑ i=1 ( 1 Pi − 1 ) h̃(Xi) 2.\nThe above considerations motivate the following formulation of optimal subsampling,\nmin {Pi}\n1\nn\n∑ Pi\ns.t.\n1\nn n∑ i=1 ( 1 Pi − 1 ) h̃(Xi) 2 ≤ V,\n∀i : Pi ≥ Pmin.\nThe KKT conditions reveal Pi = max{Pmin, λh̃(Xi)}, where λ depends upon both the variance budget V and the minimum probability Pmin. Thus we will be sampling at a rate proportional to the instantaneous loss of compressing hypothesis, subject to a minimum sampling rate."
    }, {
      "heading" : "2.2. The Sampling Strategy",
      "text" : "The above considerations lead to the following.\nDefinition 1 (Sampling Strategy). Fix a sample X = (X1, . . . , Xn) ∈ Xn, let λ > 0, let Pmin > 0, and let h̃ : X → [0, 1] be any hypothesis. The sampling strategy wrt (h̃, λ, Pmin) is a set of random variables Q = (Q1, . . . , Qn) that defines a subsample of X, where the Qi ∈ {0, 1} have conditional independence Qi ⊥ Qj 6=i, Xj 6=i|Xi and conditional expectation\nPi . = E[Qi|Xi] = min { 1,max { Pmin, λh̃(Xi) }} .\nFor this strategy we can prove the following.\nTheorem 1. Let X be a random variable with values in set X with distribution D, let X = (X1, . . . , Xn) ∼ Dn be an i.i.d. empirical sample of size n, let H be a finite set of hypotheses h : X → [0, 1], let h̃ ∈ H be any hypothesis with empirical mean RX(h̃), and let Q = (Q1, . . . , Qn) be a set of random variables according to the sampling strategy wrt (h̃, λ, Pmin). Let h\n∗ ∈ H be any hypothesis with minimum true mean, and let ĥ ∈ H be any hypothesis with minimum subsampled empirical mean RQ,X(h). For δ > 0, n ≥ 2 we have with probability at least 1− 3δ in Q and X,\nED[ĥ(X)] ≤ ED[h∗(X)]\n+ 2 + √ RX(h̃)\nPmin\n√2 ln(|H|/δ) n\n+  4√RX(h̃)Pmin λ + 2 3 (2 ln(|H|/δ) Pminn )3/4 + 4\nln(|H|/δ) Pmin(n− 1) .\nProof. See the appendix.\nAnalogous results are possible for infinite hypothesis classes whose complexity can be suitably controlled.\nFrom Theorem 1 it is clear that our scheme cannot subsample at a rate below the average loss of the compressing hypothesis without incurring increasing excess risk; this is analogous to a lossless compression rate threshold. However if Pmin ≥ RX(h̃) and λ ≥ 1, then excess risk is O(1/ √ n) + O(1/m3/4), where n is the original data set size and m is a lower bound on the subsampled data set size.\nIn practice Pmin and λ are chosen according to the subsample budget, since the expected size of the subsample is upper bounded by (Pmin+λRX(h̃))n. Unfortunately there are two hyperparameters and the analysis presented here does not guide the choice except for suggesting the constraints Pmin ≥ RX(h̃) and λ ≥ 1; this is a subject for future investigation.\nFor binary classification 0-1 loss, using the best constant predictor as the compressing hypothesis, Pmin = RX(h̃), and λ = 1, the strategy reduces to the familiar “subsample instances with the rarer class label in order to make a balanced data set.”"
    }, {
      "heading" : "3. Experiments",
      "text" : "To demonstrate the technique we used the DNA dataset from the 2008 Pascal Large Scale Learning\nchallenge (Sonnenburg, 2008). This dataset consists of 50 million instances of 200 base pair oligonucleotides with associated binary labels corresponding to whether or not the sequence contains a splice site. This is a highly imbalanced data set, with 144,823 positives and 49,855,177 negatives. This dataset is notable because it is a large public data set for which subsampling has not heretofore been an effective learning strategy (Sonnenburg & Franc, 2010; Agarwal et al., 2011).\nThe conventional evaluation metric for this data set is area under the precision-recall curve. AuPRCs of circa 0.2 are typical of “fast” methods for this dataset, although the best known technique for this dataset achieves an AuPRc of 0.586 on the validation set (Sonnenburg & Franc, 2010). The labels for the validation set for this dataset are not published, and are accessible only via a submission oracle. We took the original published training set and split it into training and test sets by reserving the first 1 million instances as test. Unless otherwise indicated, we utilize our train/test split and the reported metrics are not directly comparable with other published results. To assess the sensitivity of our results to the exact test set, we use the bootstrap to estimate the dispersion in the AuPRc. We generate bootstrap samples of the test set and compute the AuPRc statistic on each bootstrap sample using the same predictor. In what follows, a 90% confidence interval refers to the 5th and 95th quantile of the distribution of AuPRc values obtained this way."
    }, {
      "heading" : "3.1. Trigram final model",
      "text" : "For our initial (compressing) model we used logistic regression as implemented in Vowpal Wabbit (Langford, 2011), encoding the nucleotide at each position with a one-hot encoding. This model achieves 0.215\ntest AuPRc.\nTo generate a subsample, we used the initial model to subsample the original data set as per definition 1 with Pmin = RX(h̃) and for a range of λ from 1 to 65536 exponentially spaced; we name this subsampling method linear. For the loss function we used logistic loss, normalized on the training set to be in the range [0, 1]. We compared this to the well-known and ubiquitously applied strategy of taking all the positively labelled instances plus a uniform sample of the negatively labelled instances; we name this subsampling method constant.\nFor our final model we again used logistic regression but included one-hot encodings of bigrams and trigrams at each position. Figure 1 shows the results of training a trigram model on the subsample as a function of subsample fraction and subsampling method. Only the training set is subsampled: the complete test set is used every time for evaluation. For the range of subsample fractions roughly between 1% and 10%, the subsample generated via linear results in better test performance; performance at other subsampling rates is essentially equivalent. At a 7% fraction, linear achieves a test AuPRc of 0.491 with 90% confidence interval [0.474, 0.506]. This is equivalent to training a trigram logistic regression on the entire data set, which achieves test AuPRc of 0.494 with 90% confidence interval [0.475, 0.511], as summarized in table 1.\nThe confusion matrix for the initial linear model on the training set provides some intuition regarding the improved efficiency.\nPrediction Positive Negative\nTruth Positive 4677 137252 Negative 3412 48854660\nBoth linear and constant will have a positively labeled instance enriched subsample, the latter by explicit design, and the former because most true positives have large logistic loss using the initial model. The constant model, however, will have a uniform subsample of negatively labeled instances. By contrast, linear\nwill treat the 3412 false positives similarly to positively labeled instances, and furthermore negatively labelled instances that are near the classification boundary will be more likely to be incorporated into the subsample. This non-uniform view of the negatively labelled data helps prevent overfitting in the subsample."
    }, {
      "heading" : "3.2. GBM final model",
      "text" : "Next we experimented with the gbm decision tree package (Ridgeway, 2005), with which using the complete dataset is not feasible on a current commodity desktop machine. We used the trigram feature encoding using depth 3 trees, i.e., 3-way interactions between trigrams. For the initial model we used constant and linear as above, but additionally employed trigram which is the final model from the previous experiment trained on the entire data set. The results are in table 2.\ngbm utilizing the subsample defined by trigram achieves AuPRc of 0.567 ([0.545, 0.582]), which is better than trigram model trained on the entire dataset. Hence, a more computationally demanding model selection step on a subsample can achieve better results than a simpler model selection step utilizing all the data. Furthermore, this is competitive with the best known solutions, despite gbm only having access to less than 2% of the data.\nThe difference in training time between linear and trigram is quite modest: roughly 60 vs. 75 minutes for the entire data set on a single core of a commodity laptop. On the same hardware gbm takes roughly 3 days to produce a 10,000 tree ensemble using 800,000 examples. We speculate that for some domains there is a knee in the performance of classifiers relative to computational effort, such that reasonable performance can be achieved with modest effort, as with the trigram model above. In such cases, using a “sweet spot” model as the compressing hypothesis for a more computationally demanding technique is a productive strategy."
    }, {
      "heading" : "4. Conclusion",
      "text" : "We have derived a general technique for subsampling prior to model selection which leverages a compressing hypothesis, proven a deviation bound for subsampled\nempirical risk minimization which compares favorably to empirical risk minimization on the original sample, and demonstrated the approach experimentally on a large public dataset.\nThese results enable the beneficial use of effective but non-scalable learning algorithms on larger datasets."
    }, {
      "heading" : "5. Appendix (Proofs)",
      "text" : "The next two Theorems are from (Maurer & Pontil, 2009), slightly modified to range over [0, w].\nTheorem 2 (Bennett’s Inequality). Let Z,Z1, . . . Zn be i.i.d. random variables with values in [0, w] and let δ > 0. With probability at least 1−δ in the i.i.d. vector Z = (Z1, . . . Zn) we have\nE[Z]− 1 n n∑ i=1 Zi ≤ √ 2V(Z) ln 1/δ n + w ln 1/δ 3n ,\nwhere V(Z) = E[(Z − E[Z])2] is the variance.\nTheorem 3 (Empirical Bernstein Inequality). Let Z,Z1, . . . Zn be i.i.d. random variables with values in [0, w] and let δ > 0. With probability at least 1− δ in the i.i.d. vector Z = (Z1, . . . Zn) we have\nE[Z]− 1 n n∑ i=1 Zi ≤ √ 2Vn(Z) ln 2/δ n + 7w ln 2/δ 3(n− 1) ,\nwhere Vn(Z) = (1/(n−1)) ∑n i=1(Zi−(1/n) ∑n j=1 Zj) 2 is the empirical variance.\nLemma 1. Fix a sample X = (X1, . . . , Xn), let H be a finite set of hypotheses h : X → [0, 1], let h̃ ∈ H be any hypothesis with empirical mean RX(h̃), and let Q = (Q1, . . . , Qn) be a set of random variables according to the sampling strategy wrt (h̃, λ, Pmin). For δ > 0 we have with probability at least 1− δ in Q,\n1\nn n∑ i=1 Qi Pi h̃(Xi)\n≤ RX(h̃) +\n√ 2RX(h̃) ln(|H|/δ)\nλn + ln(|H|/δ) 3Pminn .\nProof. First we bound the variance due to sampling,\nVQ(h̃|X)\n= EQ ( 1 n n∑ i=1 Qi Pi h̃(Xi)− 1 n n∑ i=1 h̃(Xi) )2 ∣∣∣∣X \n= 1\nn n∑ i=1 ( 1 Pi − 1 ) h̃(Xi) 2\n≤ 1 n n∑ i=1 1λh(Xi)<1 ( 1 λh̃(Xi) − 1 ) h̃(Xi) 2 ≤ 1 λ RX(h̃).\nApplying Bennett’s inequality using range [0, 1/Pmin] yields the desired result.\nLemma 2. Fix a sample X = (X1, . . . , Xn), let H be a finite set of hypotheses h : X → [0, 1], let h̃ ∈ H be any hypothesis with empirical mean RX(h̃), and let Q = (Q1, . . . , Qn) be a set of random variables according to the sampling strategy wrt (h̃, λ, Pmin). Let ĥ ∈ H be any hypothesis with minimum subsample empirical mean RQ,X(ĥ). For δ > 0, n ≥ 2 we have with probability at least 1− 2δ in Q,\n1\nn n∑ i=1 Qi Pi ĥ(Xi) ≥ 1 n n∑ i=1 ĥ(Xi)−\n√ RX(h̃)\nPmin\n√ 2 ln(|H|/δ)\nn\n− 4\n√ RX(h̃)Pmin\nλ\n( 2 ln(|H|/δ) Pminn )3/4 − 10 ln(|H|/δ) 3Pmin(n− 1) .\nProof. First we bound the empirical subsample variance,\nVn(ĥ|Q,X)\n= 1\nn n∑ i=1 ( Qi Pi ĥ(Xi)− ( 1 n n∑ i=1 Qi Pi ĥ(Xi) ))2\n≤ 1 Pmin 1 n n∑ i=1 Qi Pi ĥ(Xi)\n≤ 1 Pmin 1 n n∑ i=1 Qi Pi h̃(Xi)\n≤ 1 Pmin\nRX(h̃) + √\n2RX(h̃) ln(|H|/δ) λn + ln(|H|/δ) 3Pminn  , where the first inequality is due to Pi ≥ Pmin, the second due to optimality of ĥ on the filtered sample, and the third due to the previous lemma. Applying empirical Bernstein and the concavity of square root\nyields\n1\nn n∑ i=1 Qi Pi ĥ(Xi)\n≥ 1 n n∑ i=1 ĥ(Xi)−\n√ 2Vn(ĥ|Q,X) ln(|H|/δ)\nn\n− 7 ln(|H|/δ) 3Pmin(n− 1)\n≥ 1 n n∑ i=1 ĥ(Xi)−\n√ RX(h̃)\nPmin\n√ 2 ln(|H|/δ)\nn\n− 4\n√ RX(h̃)Pmin\nλ\n( 2 ln(|H|/δ) Pminn )3/4 − √ 2\n3 ln(|H|/δ) Pminn − 7 ln(|H|/δ) 3Pmin(n− 1) .\nThe desired result follows from upper-bounding constants by 10/3.\nLemma 3. Fix a sample X = (X1, . . . , Xn), let H be a finite set of hypotheses h : X → [0, 1], let h̃ ∈ H be any hypothesis with empirical mean RX(h̃), and let Q = (Q1, . . . , Qn) be a set of random variables according to the sampling strategy wrt (h̃, λ, Pmin). Let h\n∗ ∈ H be any hypothesis with minimum true mean. For δ > 0, n ≥ 2 we have with probability at least 1− 2δ in Q,\n1\nn n∑ i=1 Qi Pi h∗(Xi)\n≤ 1 n n∑ i=1 h∗(Xi) +\n√ RX(h̃)\nPmin\n√ 2 ln(|H|/δ)\nn\n+ 2\n3 ( 2 ln(|H|/δ) Pminn )3/4 + ln(|H|/δ) 3Pminn .\nProof. First we bound the variance due to sampling,\nVQ(h∗|X))\n= EQ ( 1 n n∑ i=1 Qi Pi h∗(Xi)− 1 n n∑ i=1 h∗(Xi) )2 ∣∣∣∣X \n= 1\nn n∑ i=1 ( 1 Pi − 1 ) h∗(Xi) 2\n≤ 1− Pmin Pmin 1 n n∑ i=1 h∗(Xi) 2\n≤ 1− Pmin Pmin 1 n n∑ i=1 h∗(Xi),\nwhere the first inequality is due to Pi ≥ Pmin and the second due to h∗(Xi) ∈ [0, 1]. The true optimality of\nh∗ and Hoeffding’s inequality imply\n1\nn n∑ i=1 h∗(Xi) ≤ RX(h̃) + 2 √ ln(|H|/δ) 2n ,\ntherefore\nVQ(h∗|X) ≤ 1− Pmin Pmin\n( RX(h̃) + 2 √ ln(|H|/δ)\n2n\n) .\nNext applying Bennett’s inequality and the concavity of square root yields\n1\nn n∑ i=1 Qi Pi h∗(Xi)\n≤ 1 n n∑ i=1 h∗(Xi) +\n√ 2VQ(h∗|X) ln(|H|/δ)\nn + ln(|H|/δ) 3Pminn\n≤ 1 n n∑ i=1 h∗(Xi) + √ RX(h̃) 1− Pmin Pmin √ 2 ln(|H|/δ) n\n+ 4 √ Pmin(1− Pmin)2 ( 2 ln(|H|/δ) Pminn )3/4 + ln(|H|/δ) 3Pminn .\nThe result follows from maxx∈[0,1] 4 √ x(1− x)2 < 2/3.\nProof of Theorem 1. Combining the two previous lemmas with the empirical filtered optimality of ĥ yields\n1\nn n∑ i=1 ĥ(Xi)− 1 n n∑ i=1 h∗(Xi)\n≤ 2\n√ RX(h̃)\nPmin\n√ 2 ln(|H|/δ)\nn\n+  4√RX(h̃)Pmin λ + 2 3 (2 ln(|H|/δ) Pminn )3/4 + 4\nln(|H|/δ) Pmin(n− 1) .\nApplying Hoeffding’s inequality twice yields the desired result."
    } ],
    "references" : [ {
      "title" : "A reliable effective terascale linear learning system",
      "author" : [ "Agarwal", "Alekh", "Chapelle", "Olivier", "Dud́ık", "Miroslav", "Langford", "John" ],
      "venue" : "CoRR, abs/1110.4198,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2011
    }, {
      "title" : "Adaptive importance sampling to accelerate training of a neural probabilistic language model",
      "author" : [ "Y. Bengio", "J.S. Senecal" ],
      "venue" : "Trans. Neur. Netw.,",
      "citeRegEx" : "Bengio and Senecal,? \\Q2008\\E",
      "shortCiteRegEx" : "Bengio and Senecal",
      "year" : 2008
    }, {
      "title" : "Importance weighted active learning",
      "author" : [ "Beygelzimer", "Alina", "Dasgupta", "Sanjoy", "Langford", "John" ],
      "venue" : "CoRR, abs/0812.4952,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2008
    }, {
      "title" : "Sample compression, learnability, and the Vapnik-Chervonenkis dimension",
      "author" : [ "S. Floyd", "M. Warmuth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Floyd and Warmuth,? \\Q1995\\E",
      "shortCiteRegEx" : "Floyd and Warmuth",
      "year" : 1995
    }, {
      "title" : "The minimum description length principle",
      "author" : [ "P.D. Grünwald" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Grünwald,? \\Q2007\\E",
      "shortCiteRegEx" : "Grünwald",
      "year" : 2007
    }, {
      "title" : "A bound on the label complexity of agnostic active learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Hanneke,? \\Q2007\\E",
      "shortCiteRegEx" : "Hanneke",
      "year" : 2007
    }, {
      "title" : "URL https:// github.com/JohnLangford/vowpal_wabbit/wiki",
      "author" : [ "Langford", "John" ],
      "venue" : "Vowpal wabbit,",
      "citeRegEx" : "Langford and John.,? \\Q2011\\E",
      "shortCiteRegEx" : "Langford and John.",
      "year" : 2011
    }, {
      "title" : "Heterogeneous uncertainty sampling for supervised learning",
      "author" : [ "Lewis", "David D", "Catlett", "Jason" ],
      "venue" : "Proceedings of the Eleventh International Conference on Machine Learning,",
      "citeRegEx" : "Lewis et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 1994
    }, {
      "title" : "Empirical Bernstein bounds and sample-variance penalization",
      "author" : [ "Maurer", "Andreas", "Pontil", "Massimiliano" ],
      "venue" : "In The 22nd Conference on Learning Theory,",
      "citeRegEx" : "Maurer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Maurer et al\\.",
      "year" : 2009
    }, {
      "title" : "Generalized boosted models: A guide to the gbm package",
      "author" : [ "Ridgeway", "Greg" ],
      "venue" : null,
      "citeRegEx" : "Ridgeway and Greg.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ridgeway and Greg.",
      "year" : 2005
    }, {
      "title" : "Boosting classifier cascades",
      "author" : [ "Saberian", "Mohammad", "Vasconcelos", "Nuno" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Saberian et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Saberian et al\\.",
      "year" : 2010
    }, {
      "title" : "Pascal large scale learning",
      "author" : [ "Sonnenburg", "Sören" ],
      "venue" : "URL http://largescale.ml. tu-berlin.de/about/",
      "citeRegEx" : "Sonnenburg and Sören.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sonnenburg and Sören.",
      "year" : 2008
    }, {
      "title" : "Coffin : A computational framework for linear svms",
      "author" : [ "Sonnenburg", "Sören", "Franc", "Vojtech" ],
      "venue" : "In Proc. ICML 2010,",
      "citeRegEx" : "Sonnenburg et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sonnenburg et al\\.",
      "year" : 2010
    }, {
      "title" : "Rapid object detection using a boosted cascade of simple features",
      "author" : [ "Viola", "Paul", "Jones", "Michael" ],
      "venue" : null,
      "citeRegEx" : "Viola et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Viola et al\\.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In this case, the set of linear predictors is a subset of the final hypothesis space, and linear learning is feasible at terafeature scale (Agarwal et al., 2011).",
      "startOffset" : 139,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "In particular our subsampling rate is lowerbounded similarly to worst-case label complexity results (Beygelzimer et al., 2008).",
      "startOffset" : 100,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "In particular, since disagreement regions (Hanneke, 2007) only increase on the hypothesis superset, for our particular scenario an active learning algorithm which attempts to exploit what appears to be an isolated empirical minimizer is subject to poor worst-case behaviour.",
      "startOffset" : 42,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "Algorithms based on the MDL principle (Grünwald, 2007) treat training examples and models as data that need to be transmitted to a receiver and they operate by conceptually finding a code that minimizes communication costs.",
      "startOffset" : 38,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "This dataset is notable because it is a large public data set for which subsampling has not heretofore been an effective learning strategy (Sonnenburg & Franc, 2010; Agarwal et al., 2011).",
      "startOffset" : 139,
      "endOffset" : 187
    } ],
    "year" : 2013,
    "abstractText" : "We propose a sampling scheme suitable for reducing a data set prior to selecting a hypothesis with minimum empirical risk. The sampling only considers a subset of the ultimate (unknown) hypothesis set, but can nonetheless guarantee that the final excess risk will compare favorably with utilizing the entire original data set. We demonstrate the practical benefits of our approach on a large dataset which we subsample and subsequently fit with boosted trees.",
    "creator" : "LaTeX with hyperref package"
  }
}