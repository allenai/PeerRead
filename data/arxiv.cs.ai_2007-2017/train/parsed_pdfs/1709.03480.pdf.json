{
  "name" : "1709.03480.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combining Strategic Learning and Tactical Search in Real-Time Strategy Games",
    "authors" : [ "Nicolas A. Barriga", "Marius Stanescu", "Michael Buro" ],
    "emails" : [ "barriga@ualberta.ca", "astanesc@ualberta.ca", "mburo@ualberta.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, numerous challenging research problems have attracted AI researchers to using real-time strategy (RTS) games as test-bed in several areas, such as casebased reasoning and planning (Ontañón et al. 2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessière 2011), deep learning (Usunier et al. 2017; Foerster et al. 2017; Peng et al. 2017) and heuristic and adversarial search (Churchill and Buro 2011; Barriga, Stanescu, and Buro 2015). Functioning AI solutions to most RTS sub-problems exist, but combining those doesn’t come close to human level performance1.\nTo cope with large state spaces and branching factors in RTS games, recent work focuses on smart sampling of\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1http://www.cs.mun.ca/˜dchurchill/ starcraftaicomp/report2015.shtml#mvm\nthe search space (Churchill and Buro 2013; Ontañón 2017; 2016; Ontañón and Buro 2015) and state and action abstractions (Uriarte and Ontañón 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b). The first approach produces strong agents for small scenarios. The latter techniques work well on larger problems because of their ability to make good strategic choices. However, they have limited tactical ability, due to their necessarily coarsegrained abstractions. One compromise would be to allocate computational time for search-based approaches to improve the tactical decisions, but this allocation would come at the expense of allocating less time to strategic choices.\nWe propose to train a deep convolutional neural network (CNN) to predict the output of Puppet Search, thus leaving most of the time free for use by a tactical search algorithm. Puppet Search is a strategic search algorithm that uses action abstractions and has shown good results, particularly in large scenarios. We will base our network on previous work on CNNs for state evaluation (Stanescu et al. 2016), reformulating the earlier approach to handle larger maps.\nThis paper’s contributions are a network architecture capable of scaling to larger map sizes than previous approaches, a policy network for selecting high-level actions, and a method of combining the policy network with a tactical search algorithm that surpasses the performance of both individually.\nThe remainder of this paper is organized as follows: Section 2 discussed previous related work, Section 3 describes our proposed approach and Section 4 provides experimental results. We then conclude and outline future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "Ever since the revolutionary results in the ImageNet competition (Krizhevsky, Sutskever, and Hinton 2012), CNNs have been applied successfully in a wide range of domains. Their ability to learn hierarchical structures of spatially invariant local features make them ideal in settings that can be represented spatially. These include uni-dimensional streams in natural language processing (Collobert and Weston 2008), two-dimensional board games (Silver et al. 2016), or threedimensional video analysis (Ji et al. 2013).\nThese diverse successes have inspired the application of CNNs to games. They have achieved human-level perfor-\nar X\niv :1\n70 9.\n03 48\n0v 1\n[ cs\n.A I]\n1 1\nSe p\n20 17\nmance in several Atari games, by using Q-learning, a well known reinforcement learning (RL) algorithm (Mnih et al. 2015). But the most remarkable accomplishment may be AlphaGo (Silver et al. 2016), a Go playing program that last year defeated Lee Sedol, one of the top human professionals, a feat that was thought to be at least a decade away. As much an engineering as a scientific accomplishment, it was achieved using a combination of tree search and a series of neural networks trained on millions of human games and self-play, running on thousands of CPUs and hundreds of GPUs.\nThese results have sparked interest in applying deep learning to games with larger state and action spaces. Some limited success has been found in micromanagement tasks for RTS games (Usunier et al. 2017), where a deep network managed to slightly outperform a set of baseline heuristics. Additional encouraging results were achieved for the task of evaluating RTS game states (Stanescu et al. 2016). The network significantly outperforms other state-of-the-art approaches at predicting game outcomes. When it is used in adversarial search algorithms, they perform significantly better than using simpler evaluation functions that are three to four orders of magnitude faster.\nMost of the described research on deep learning in multiagent domains assumes full visibility of the environment and lacks communication between agents. Recent work addresses this problem by learning communication between agents alongside their policy (Sukhbaatar, Szlam, and Fergus 2016). In their model, each agent is controlled by a deep network which has access to a communication channel through which they receive the summed transmissions of other agents. The resulting model outperforms models without communication, fully-connected models, and models using discrete communication on simple imperfect information combat tasks. However, symmetric communication prevents handling heterogeneous agent types, limitation later removed by (Peng et al. 2017) which use a dedicated bidirection communication channel and recurrent neural networks. This would be an alternative to the search algorithm we use for the tactical module on section 4.3, in cases where there is no forward model of the game, or there is imperfect information.\nA new search algorithm that has shown good results particularly in large RTS scenarios, is Puppet Search (Barriga, Stanescu, and Buro 2015; 2017a; 2017b). It is an action abstraction mechanism that uses fast scripts with a few carefully selected choice points. These scripts are usu-\nally hard-coded strategies, and the number of choice points will depend on the time constraints the system has to meet. These choice points are then exposed to an adversarial lookahead procedure, such as Alpha-Beta or Monte Carlo Tree Search (MCTS). The algorithm then uses a forward model of the game to examine the outcome of different choice combinations and decide on the best course of action. Using a restricted set of high-level actions results in low branching factor, enabling deep look-ahead and favouring strong strategic decisions. Its main weakness is its rigid scripted tactical micromanagement, which led to modest results on small sized scenarios where good micromanagement is key to victory."
    }, {
      "heading" : "3 Algorithm Details",
      "text" : "We build on previous work on RTS game state evaluation (Stanescu et al. 2016) applied to µRTS (see figure 1). This study presented a neural network architecture and experiments comparing it to simpler but faster evaluation functions. The CNN-based evaluation showed a higher accuracy at evaluating game states. In addition, when used by stateof-the-art search algorithms, they perform significantly better than the faster evaluations. Table 1 lists the input features their network uses.\nThe network itself is composed of two convolutional layers followed by two fully connected layers. It performed very well on 8×8 maps. However, as the map size increases, so does the number of weights on the fully connected layers, which eventually dominates the weight set. To tackle this problem, we designed a fully convolutional network (FCN) which only consists of intermediate convolutional layers (Springenberg et al. 2014) and has the advantage of being an architecture that can fit a wide range of board sizes.\nTable 2 shows the architectures of the evaluation network and the policy network we use, which only differ in the first and last layers. The first layer of the policy network has an extra plane which indicates which player’s policy it is computing. The last layer of the evaluation network has two outputs, indicating if the state is a player 1 or player 2 win, while the policy network has four outputs, each corresponding to one of four possible actions. The global averaging used after the convolutional layers does not use any extra weights, compared to a fully connected layer. The benefit is that the number of network parameters does not grow when the map size is increased. This allows for a network to be quickly pre-trained on smaller maps, and then fine-tuned on the larger target map.\nPuppet Search requires a forward model to examine the outcome of different actions and then choose the best one. Most RTS games do not have a dedicated forward model or simulator other than the game itself. This is usually too slow to be used in a search algorithm, or even unavailable due to technical constraints such as closed source code or being tied to the graphics engine. Using a policy network for script selection during game play allows us to bypass the need for a forward model of the game. Granted, the forward model is still required during the supervised training phase, but execution speed is less of an issue in this case, because training is performed offline. Training the network via reinforcement learning would remove this constraint completely.\nFinally, with the policy network running significantly faster (3ms versus a time budget of 100ms per frame for search-based agents) than Puppet Search we can use the unused time to refine tactics. While the scripts used by Puppet Search and the policy network represent different strategic choices, they all share very similar tactical behaviour. Their tactical ability is weak in comparison to state-of-theart search-based bots, as previous results (Barriga, Stanescu, and Buro 2017b) suggest.\nFor this reason, the proposed algorithm combines an FCN for strategic decisions and an adversarial search algo-\nrithm for tactics. The strategic component handles macromanagement: unit production, workers, and sending combat units towards the opponent. The tactical component handles micro-management during combat.\nThe complete procedure is described by Algorithm 1. It first builds a limited view of the game state, which only includes units that are close to opposing units (line 2). If this limited state is empty, all available computation time is assigned to the strategic algorithm, otherwise, both algorithms receive a fraction of the total time available. This fraction is decided empirically for each particular algorithm combination. Then, in line 9 the strategic algorithm is used to compute actions for all units in the state, followed by the tactical algorithm that computes actions for units in the limited state. Finally, the actions are merged (line 11) by replacing the strategic action in case both algorithms produced actions for a particular unit.\nAlgorithm 1 Combined Strategy and Tactics 1: procedure GETCOMBINEDACTION(state, stratAI,\ntactAI, stratT ime, tactT ime)\n2: limState← EXTRACTCOMBAT(state) 3: if ISEMPTY(limState) then 4: SETTIME(stratAI, stratT ime+ tactT ime) 5: else 6: SETTIME(stratAI, stratT ime) 7: SETTIME(tactAI, tactT ime) 8: end if 9: stratActions← GETACTION(stratAI, state)\n10: tactActions← GETACTION(tactAI, limState) 11: return MERGE(stratActions, tactActions) 12: end procedure"
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "All experiments were performed in machines running Fedora 25, with an Intel Core i7-7700K CPU, with 32GB of RAM and an NVIDIA GeForce GTX 1070 with 8GB of RAM. The Java version used for µRTS was OpenJDK 1.8.0, Caffe git commit 365ac88 was compiled with g++ 5.3.0, and pycaffe was run using python 2.7.13.\nThe Puppet Search version we used for all the following experiments utilizes alpha-beta search over a single choice point with four options. The four options are WorkerRush, LightRush, RangedRush and HeavyRush, and were also used as baselines in the following experiments. More details about these scripts can be found in (Stanescu et al. 2016).\nTwo other recent algorithms were also used as benchmarks, Na¨veMCTS (Ontañón 2013) and Adversarial Hierarchical Task Networks (AHTNs) (Ontañón and Buro 2015). Na¨veMCTS is an MCTS variant with a sampling strategy that exploits the tree structure of Combinatorial MultiArmed Bandits — bandit problems with multiple variables. Applied to RTS games, each variable represents a unit, and the legal actions for each of those units are the values that each variable can take. Naı̈veMCTS outperforms other game tree search algorithms on small scenarios. AHTNs are an alternative approach, similar to Puppet Search, that instead of sampling from the full action space, uses scripted actions to reduce the search space. It combines minimax tree search with HTN planning.\nAll experiments were performed on 128x128 maps ported from the StarCraft: Brood War maps used for the AIIDE competition. These maps, as well as implementations of Puppet Search, the four scripts, AHTN and Naı̈veMCTS are readily available in the µRTS repository."
    }, {
      "heading" : "4.1 State Evaluation Network",
      "text" : "The data for training the evaluation network was generated by running games between a set of bots using 5 different maps, each with 12 different starting positions. Ties were discarded, and the remaining games were split into 2190 training games, and 262 test games. 12 game states were randomly sampled from each game, for a total of 26,280 training samples and 3,144 test samples. Data is labelled by a Boolean value indicating whether the first player won. All evaluation functions were trained on the same dataset.\nThe network’s weights are initialized using Xavier initialization (Glorot and Bengio 2010). We used adaptive moment estimation (ADAM) (Kingma and Ba 2014) with default values of β1 = 0.9, β2 = 0.999, = 10−8 and a base learning rate of 10−4. The batch size was 256.\nThe evaluation network reaches 95% accuracy in classifying samples as wins or losses. Figure 2 shows the accuracy of different evaluation functions as game time progresses. The functions compared are the evaluation network, Lanchester (Stanescu, Barriga, and Buro 2015), the simple linear evaluation with hard-coded weights that comes with µRTS, and a version of the simple evaluation with weights optimized using logistic regression. The network’s accuracy is even higher than previous results in 8x8 maps (Stanescu et\nal. 2016). The accuracy drop of the simple evaluation in the early game happens because it does not take into account units currently being built. If a player invests resources in units or buildings that take a long time to complete, its score lowers, despite the stronger resulting position after their completion. The other functions learn appropriate weights to mitigate this issue.\nTable 5 shows the performance of PuppetSearch when using the Lanchester evaluation function and the neural network. The performance of the network is significantly better (P-value = 0.0011) than Lanchester’s, even though the network is three orders of magnitude slower. Evaluating a game state using Lanchester takes an average of 2.7µs, while the evaluation network uses 2,574µs.\nTable 6 shows the same comparison, but with Puppet Search searching to a fixed depth of 4, rather than having 100ms per frame. The advantage of the neural network is much more clear, as execution speed does not matter in this case. (P-value = 0.0044)"
    }, {
      "heading" : "4.2 Policy Network",
      "text" : "We used the same procedure as in the previous subsection, but now we labelled the samples with the outcome of a 10 second Puppet Search using the evaluation network. The resulting policy network has an accuracy for predicting the correct puppet move of 73%, and a 95% accuracy for predicting any of the top 2 moves.\nTable 3 shows the policy network coming close to Puppet Search and defeating all the scripts."
    }, {
      "heading" : "4.3 Strategy and Tactics",
      "text" : "Finally, we compare the performance of the policy network and Puppet Search as the strategic part of a combined strategic/tactical agent. We will do so by assigning a fraction of the allotted time to the strategic algorithm and the remainder to the tactical algorithm, which will be Naı̈veMCTS in our experiments. We expect the policy network to perform better in this scenario, as it runs significantly faster than Puppet\nSearch while maintaining similar action performance. The best time split between strategic and tactical algorithm was determined experimentally to be 20% for Puppet Search and 80% for Naı̈veMCTS. The policy network uses a fixed time (around 3ms), and the remaining time is assigned to the tactical search.\nTable 4 shows that both strategic algorithms greatly benefit from blending with a tactical algorithm. The gains are"
    }, {
      "heading" : "PS CNN - 59.2 89.2 72.5 73.6",
      "text" : ""
    }, {
      "heading" : "PS CNN - 80 95 82.5 85.8",
      "text" : "more substantial for the policy network, which now scores 56.7% against its Puppet Search counterpart. It also has a 4.3% higher overall win rate despite markedly poorer results against WorkerRush and AHTN-P. These seems to be due to a strategic mistake on the part of the policy network, which, if its cause can be detected and corrected, would lead to even higher performance."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "We have extended previous research that used CNNs to accurately evaluate RTS game states in small maps to larger map sizes usually used in commercial RTS games. The average win prediction accuracy at all game times is higher compared to smaller scenarios. This is probably the case because strategic decisions are more important than tactical decisions in larger maps, and strategic development is easier to quantify by the network. Although the network is several orders of magnitude slower than competing simpler evaluation functions, its accuracy makes it more effective. When the Puppet Search high-level adversarial search algorithm uses the CNN, its performance is better than when using simpler but faster functions.\nWe also trained a policy network to predict the outcome of Puppet Search. The win rate of the resulting network is similar to that of the original search, with some exceptions against specific opponents. However, while slightly weaker in playing strength, a feed-forward network pass is much faster. This speed increase created the opportunity for using the saved time to fix the shortcomings introduced by highlevel abstractions. A tactical search algorithm can micromanage units in contact with the enemy, while the policy chosen by the network handles routine tasks (mining, march-\ning units toward the opponent) and strategic tasks (training new units). The resulting agent was shown to be stronger than the policy network alone in all tested scenarios, but can only partially compensate for the network’s weaknesses against specific opponents.\nLooking into the future, we recognize that most tactical search algorithms, like the MCTS variant we used, have the drawback of requiring a forward model of the game. Using machine learning techniques to make tactical decisions would eliminate this requirement. However, this has proved to be a difficult goal, as previous attempts by other researchers have had limited success on simple scenarios (Usunier et al. 2017; Synnaeve and Bessière 2016). Recent research avenues based on integrating concepts such as communication (Sukhbaatar, Szlam, and Fergus 2016), unit grouping and bidirectional recurrent neural networks (Peng et al. 2017) suggest that strong tactical networks might soon be available.\nThe network architecture presented in this paper, being fully convolutional, can be used on maps of any (reasonable) size without increasing its number of parameters. Hence, future research could include assessing the speed-up obtained by taking advantage of transfer learning from smaller maps to larger ones. Also of interest would be to determine whether different map sizes can be mixed within a training set. It would also be interesting to investigate the performance of the networks on maps that have not previously been seen during training .\nBecause the policy network exhibits some weaknesses against specific opponents, further experiments should be performed to establish whether this is due to a lack of appropriate game state samples in the training data or other reasons. A related issue is our reliance on labelled training data, which could be resolved by using reinforcement learning techniques, such as DQN (deep Q network) learning. However, full RTS games are difficult for these techniques, mainly because the only available reward is the outcome of the game. In addition, action choices near the endgame (close to the reward), have very little impact on the outcome of the game, while early ones (when there is no reward), matter most. There are several strategies available that could help overcome these issues, such as curriculum learning (Bengio et al. 2009), reward shaping (Devlin, Kudenko, and Grześ 2011), or implementing double DQN learning (Hasselt, Guez, and Silver 2016). These strategies have proved useful on adversarial games, games with sparse rewards, or temporally extended planning problems respectively."
    } ],
    "references" : [ {
      "title" : "Building placement optimization in real-time strategy games",
      "author" : [ "N.A. Barriga", "M. Stanescu", "M. Buro" ],
      "venue" : "Workshop on Artificial Intelligence in Adversarial RealTime Games, AIIDE. Barriga, N. A.; Stanescu, M.; and Buro, M. 2015. Puppet",
      "citeRegEx" : "Barriga et al\\.,? 2014",
      "shortCiteRegEx" : "Barriga et al\\.",
      "year" : 2014
    }, {
      "title" : "Combining scripted behavior with game tree search for stronger, more robust game ai",
      "author" : [ "N.A. Barriga", "M. Stanescu", "M. Buro" ],
      "venue" : "Game AI Pro 3: Collected Wisdom of Game AI Professionals. CRC Press. chapter 14.",
      "citeRegEx" : "Barriga et al\\.,? 2017a",
      "shortCiteRegEx" : "Barriga et al\\.",
      "year" : 2017
    }, {
      "title" : "Game tree search based on non-deterministic action scripts in realtime strategy games",
      "author" : [ "N.A. Barriga", "M. Stanescu", "M. Buro" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games (TCIAIG).",
      "citeRegEx" : "Barriga et al\\.,? 2017b",
      "shortCiteRegEx" : "Barriga et al\\.",
      "year" : 2017
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston" ],
      "venue" : "Proceedings of the 26th annual international conference on machine learning, 41–48. ACM.",
      "citeRegEx" : "Bengio et al\\.,? 2009",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Build order optimization in StarCraft",
      "author" : [ "D. Churchill", "M. Buro" ],
      "venue" : "AI and Interactive Digital Entertainment Conference, AIIDE (AAAI), 14–19.",
      "citeRegEx" : "Churchill and Buro,? 2011",
      "shortCiteRegEx" : "Churchill and Buro",
      "year" : 2011
    }, {
      "title" : "Portfolio greedy search and simulation for large-scale combat in StarCraft",
      "author" : [ "D. Churchill", "M. Buro" ],
      "venue" : "IEEE Conference on Computational Intelligence in Games (CIG), 1–8. IEEE.",
      "citeRegEx" : "Churchill and Buro,? 2013",
      "shortCiteRegEx" : "Churchill and Buro",
      "year" : 2013
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning, 160–167. ACM.",
      "citeRegEx" : "Collobert and Weston,? 2008",
      "shortCiteRegEx" : "Collobert and Weston",
      "year" : 2008
    }, {
      "title" : "An empirical study of potential-based reward shaping and advice in complex, multi-agent systems",
      "author" : [ "S. Devlin", "D. Kudenko", "M. Grześ" ],
      "venue" : "Advances in Complex Systems 14(02):251–278.",
      "citeRegEx" : "Devlin et al\\.,? 2011",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2011
    }, {
      "title" : "Stabilising experience replay for deep Multi-Agent reinforcement learning",
      "author" : [ "J. Foerster", "N. Nardelli", "G. Farquhar", "P.H.S. Torr", "P. Kohli", "S. Whiteson" ],
      "venue" : "Thirty-fourth International Conference on Machine Learning.",
      "citeRegEx" : "Foerster et al\\.,? 2017",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "International conference on artificial intelligence and statistics, 249–256.",
      "citeRegEx" : "Glorot and Bengio,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio",
      "year" : 2010
    }, {
      "title" : "Deep reinforcement learning with double q-learning",
      "author" : [ "H. v. Hasselt", "A. Guez", "D. Silver" ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2094–2100. AAAI Press.",
      "citeRegEx" : "Hasselt et al\\.,? 2016",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "3d convolutional neural networks for human action recognition",
      "author" : [ "S. Ji", "W. Xu", "M. Yang", "K. Yu" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence 35(1):221–231.",
      "citeRegEx" : "Ji et al\\.,? 2013",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2013
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "CoRR abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba,? 2014",
      "shortCiteRegEx" : "Kingma and Ba",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, 1097–1105.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Humanlevel control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski" ],
      "venue" : "Nature",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial hierarchicaltask network planning for complex real-time games",
      "author" : [ "S. Ontañón", "M. Buro" ],
      "venue" : "Proceedings of the 24th International Conference on Artificial Intelligence (IJCAI), 1652–1658.",
      "citeRegEx" : "Ontañón and Buro,? 2015",
      "shortCiteRegEx" : "Ontañón and Buro",
      "year" : 2015
    }, {
      "title" : "Case-based planning and execution for real-time strategy games",
      "author" : [ "S. Ontañón", "K. Mishra", "N. Sugandh", "A. Ram" ],
      "venue" : "ICCBR ’07, 164–178. Berlin, Heidelberg: Springer-Verlag.",
      "citeRegEx" : "Ontañón et al\\.,? 2007",
      "shortCiteRegEx" : "Ontañón et al\\.",
      "year" : 2007
    }, {
      "title" : "The combinatorial multi-armed bandit problem and its application to real-time strategy games",
      "author" : [ "S. Ontañón" ],
      "venue" : "AIIDE.",
      "citeRegEx" : "Ontañón,? 2013",
      "shortCiteRegEx" : "Ontañón",
      "year" : 2013
    }, {
      "title" : "Informed monte carlo tree search for real-time strategy games",
      "author" : [ "S. Ontañón" ],
      "venue" : "Computational Intelligence and Games (CIG), 2016 IEEE Conference on, 1–8. IEEE.",
      "citeRegEx" : "Ontañón,? 2016",
      "shortCiteRegEx" : "Ontañón",
      "year" : 2016
    }, {
      "title" : "Combinatorial multi-armed bandits for real-time strategy games",
      "author" : [ "S. Ontañón" ],
      "venue" : "Journal of Artificial Intelligence Research 58:665–702.",
      "citeRegEx" : "Ontañón,? 2017",
      "shortCiteRegEx" : "Ontañón",
      "year" : 2017
    }, {
      "title" : "Multiagent Bidirectionally-Coordinated nets for learning to play StarCraft combat games",
      "author" : [ "P. Peng", "Q. Yuan", "Y. Wen", "Y. Yang", "Z. Tang", "H. Long", "J. Wang" ],
      "venue" : null,
      "citeRegEx" : "Peng et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2017
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search. Nature 529(7587):484–489",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1412.6806.",
      "citeRegEx" : "Springenberg et al\\.,? 2014",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical adversarial search applied to real-time strategy games",
      "author" : [ "M. Stanescu", "N.A. Barriga", "M. Buro" ],
      "venue" : "Proceedings of the Tenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE), 66–72.",
      "citeRegEx" : "Stanescu et al\\.,? 2014",
      "shortCiteRegEx" : "Stanescu et al\\.",
      "year" : 2014
    }, {
      "title" : "Using Lanchester attrition laws for combat prediction in StarCraft",
      "author" : [ "M. Stanescu", "N.A. Barriga", "M. Buro" ],
      "venue" : "Eleventh Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE), 86–",
      "citeRegEx" : "Stanescu et al\\.,? 2015",
      "shortCiteRegEx" : "Stanescu et al\\.",
      "year" : 2015
    }, {
      "title" : "Evaluating real-time strategy game states using convolutional neural networks",
      "author" : [ "M. Stanescu", "N.A. Barriga", "A. Hess", "M. Buro" ],
      "venue" : "IEEE Conference on Computational Intelligence and Games (CIG).",
      "citeRegEx" : "Stanescu et al\\.,? 2016",
      "shortCiteRegEx" : "Stanescu et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning multiagent communication with backpropagation",
      "author" : [ "S. Sukhbaatar", "A. Szlam", "R. Fergus" ],
      "venue" : "Lee, D. D.; Sugiyama, M.; Luxburg, U. V.; Guyon, I.; and Garnett, R., eds., Advances in Neural Information Processing Systems 29. Curran Associates, Inc. 2244–2252.",
      "citeRegEx" : "Sukhbaatar et al\\.,? 2016",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2016
    }, {
      "title" : "A Bayesian model for plan recognition in RTS games applied to StarCraft",
      "author" : [ "G. Synnaeve", "P. Bessière" ],
      "venue" : "AAAI., ed., Proceedings of the Seventh Artificial Intelligence and Interactive Digital Entertainment Conference (AIIDE 2011), Proceedings of AIIDE, 79–84.",
      "citeRegEx" : "Synnaeve and Bessière,? 2011",
      "shortCiteRegEx" : "Synnaeve and Bessière",
      "year" : 2011
    }, {
      "title" : "Multiscale Bayesian modeling for RTS games: An application to StarCraft AI",
      "author" : [ "G. Synnaeve", "P. Bessière" ],
      "venue" : "IEEE Transactions on Computational intelligence and AI in Games 8(4):338–350.",
      "citeRegEx" : "Synnaeve and Bessière,? 2016",
      "shortCiteRegEx" : "Synnaeve and Bessière",
      "year" : 2016
    }, {
      "title" : "Game-tree search over high-level game states in RTS games",
      "author" : [ "A. Uriarte", "S. Ontañón" ],
      "venue" : "Proceedings of the Tenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE’14, 73–79. Usunier, N.; Synnaeve, G.; Lin, Z.; and Chintala, S. 2017.",
      "citeRegEx" : "Uriarte and Ontañón,? 2014",
      "shortCiteRegEx" : "Uriarte and Ontañón",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "In recent years, numerous challenging research problems have attracted AI researchers to using real-time strategy (RTS) games as test-bed in several areas, such as casebased reasoning and planning (Ontañón et al. 2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessière 2011), deep learning (Usunier et al.",
      "startOffset" : 197,
      "endOffset" : 218
    }, {
      "referenceID" : 27,
      "context" : "2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessière 2011), deep learning (Usunier et al.",
      "startOffset" : 85,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessière 2011), deep learning (Usunier et al. 2017; Foerster et al. 2017; Peng et al. 2017) and heuristic and adversarial search (Churchill and Buro 2011; Barriga, Stanescu, and Buro 2015).",
      "startOffset" : 129,
      "endOffset" : 190
    }, {
      "referenceID" : 20,
      "context" : "2007), evolutionary computation (Barriga, Stanescu, and Buro 2014), machine learning (Synnaeve and Bessière 2011), deep learning (Usunier et al. 2017; Foerster et al. 2017; Peng et al. 2017) and heuristic and adversarial search (Churchill and Buro 2011; Barriga, Stanescu, and Buro 2015).",
      "startOffset" : 129,
      "endOffset" : 190
    }, {
      "referenceID" : 4,
      "context" : "2017) and heuristic and adversarial search (Churchill and Buro 2011; Barriga, Stanescu, and Buro 2015).",
      "startOffset" : 43,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "shtml#mvm the search space (Churchill and Buro 2013; Ontañón 2017; 2016; Ontañón and Buro 2015) and state and action abstractions (Uriarte and Ontañón 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b).",
      "startOffset" : 27,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "shtml#mvm the search space (Churchill and Buro 2013; Ontañón 2017; 2016; Ontañón and Buro 2015) and state and action abstractions (Uriarte and Ontañón 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b).",
      "startOffset" : 27,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "shtml#mvm the search space (Churchill and Buro 2013; Ontañón 2017; 2016; Ontañón and Buro 2015) and state and action abstractions (Uriarte and Ontañón 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b).",
      "startOffset" : 27,
      "endOffset" : 95
    }, {
      "referenceID" : 29,
      "context" : "shtml#mvm the search space (Churchill and Buro 2013; Ontañón 2017; 2016; Ontañón and Buro 2015) and state and action abstractions (Uriarte and Ontañón 2014; Stanescu, Barriga, and Buro 2014; Barriga, Stanescu, and Buro 2017b).",
      "startOffset" : 130,
      "endOffset" : 225
    }, {
      "referenceID" : 25,
      "context" : "We will base our network on previous work on CNNs for state evaluation (Stanescu et al. 2016), reformulating the earlier approach to handle larger maps.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "These include uni-dimensional streams in natural language processing (Collobert and Weston 2008), two-dimensional board games (Silver et al.",
      "startOffset" : 69,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "These include uni-dimensional streams in natural language processing (Collobert and Weston 2008), two-dimensional board games (Silver et al. 2016), or threedimensional video analysis (Ji et al.",
      "startOffset" : 126,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : "2016), or threedimensional video analysis (Ji et al. 2013).",
      "startOffset" : 42,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "mance in several Atari games, by using Q-learning, a well known reinforcement learning (RL) algorithm (Mnih et al. 2015).",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "But the most remarkable accomplishment may be AlphaGo (Silver et al. 2016), a Go playing program that last year defeated Lee Sedol, one of the top human professionals, a feat that was thought to be at least a decade away.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "Additional encouraging results were achieved for the task of evaluating RTS game states (Stanescu et al. 2016).",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "However, symmetric communication prevents handling heterogeneous agent types, limitation later removed by (Peng et al. 2017) which use a dedicated bidirection communication channel and recurrent neural networks.",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 25,
      "context" : "We build on previous work on RTS game state evaluation (Stanescu et al. 2016) applied to μRTS (see figure 1).",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "To tackle this problem, we designed a fully convolutional network (FCN) which only consists of intermediate convolutional layers (Springenberg et al. 2014) and has the advantage of being an architecture that can fit a wide range of board sizes.",
      "startOffset" : 129,
      "endOffset" : 155
    }, {
      "referenceID" : 25,
      "context" : "More details about these scripts can be found in (Stanescu et al. 2016).",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "Two other recent algorithms were also used as benchmarks, Na ̈veMCTS (Ontañón 2013) and Adversarial Hierarchical Task Networks (AHTNs) (Ontañón and Buro 2015).",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "Two other recent algorithms were also used as benchmarks, Na ̈veMCTS (Ontañón 2013) and Adversarial Hierarchical Task Networks (AHTNs) (Ontañón and Buro 2015).",
      "startOffset" : 135,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "The network’s weights are initialized using Xavier initialization (Glorot and Bengio 2010).",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "We used adaptive moment estimation (ADAM) (Kingma and Ba 2014) with default values of β1 = 0.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "The network’s accuracy is even higher than previous results in 8x8 maps (Stanescu et al. 2016).",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "However, this has proved to be a difficult goal, as previous attempts by other researchers have had limited success on simple scenarios (Usunier et al. 2017; Synnaeve and Bessière 2016).",
      "startOffset" : 136,
      "endOffset" : 185
    }, {
      "referenceID" : 20,
      "context" : "Recent research avenues based on integrating concepts such as communication (Sukhbaatar, Szlam, and Fergus 2016), unit grouping and bidirectional recurrent neural networks (Peng et al. 2017) suggest that strong tactical networks might soon be available.",
      "startOffset" : 172,
      "endOffset" : 190
    }, {
      "referenceID" : 3,
      "context" : "There are several strategies available that could help overcome these issues, such as curriculum learning (Bengio et al. 2009), reward shaping (Devlin, Kudenko, and Grześ 2011), or implementing double DQN learning (Hasselt, Guez, and Silver 2016).",
      "startOffset" : 106,
      "endOffset" : 126
    } ],
    "year" : 2017,
    "abstractText" : "A commonly used technique for managing AI complexity in real-time strategy (RTS) games is to use action and/or state abstractions. High-level abstractions can often lead to goodions. High-level abstractions can often lead to good strategic decision making, but tactical decision quality may suffer due to lost details. A competing method is to sample the search space which often leads to good tactical performance in simple scenarios, but poor high-level planning. We propose to use a deep convolutional neural network (CNN) to select among a limited set of abstract action choices, and to utilize the remaining computation time for game tree search to improve low level tactics. The CNN is trained by supervised learning on game states labelled by Puppet Search, a strategic search algorithm that uses action abstractions. The network is then used to select a script — anions. The network is then used to select a script — an abstract action — to produce low level actions for all units.action — to produce low level actions for all units. Subsequently, the game tree search algorithm improves the tactical actions of a subset of units using a limited view of the game state only considering units close to opponent units. Experiments in the μRTS game show that the combined algorithm results in higher win-rates than either of its two independent components and other state-of-the-art μRTS agents. To the best of our knowledge, this is the first successful application of a convolutional network to play a full RTS game on standard game maps, as previous work has focused on subproblems, such as combat, or on very small maps.",
    "creator" : "TeX"
  }
}