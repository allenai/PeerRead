{
  "name" : "1506.02535.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Learning of Ensembles with QuadBoost",
    "authors" : [ "Louis Fortier-Dubois", "François Laviolette", "Mario Marchand" ],
    "emails" : [ "mario.marchand@ift.ulaval.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n02 53\n5v 1\n[ cs\n.L G\n] 8\nJ un"
    }, {
      "heading" : "1 Introduction",
      "text" : "As data is becoming very abundant, machine learning is now confronted with the challenge of having to learn complex models from huge data sets. Among the learning algorithms which seem most likely to be able to scale up to meet this challenge are ensemble methods based on the idea of boosting weak learners [1]. Take AdaBoost [2] for example. If a weak learner is (almost always) able to produce in linear time a classifier achieving an empirical error just slightly better than random guessing, then the exponential rate of decrease of the training error of AdaBoost will give us a good majority vote in linear time.\nAfter AdaBoost was published, it soon became clear that infinitely many surrogate loss functions [3] and regularizers could be used for boosting and, without surprise, many variants have been proposed—to the point where the practitioner is often completely overwhelmed when confronted with the choice of picking a boosting algorithm for his learning task. Are some algorithms better than others? If so, then under what circumstances are they better? If not, then are they, somehow, all equivalent? In an attempt to answer these questions we have decided to search for a risk bound guarantee that applies to all ensemble methods, no matter what are the surrogate loss and regularizer used by the algorithm. What comes out from the risk bound presented in the next section is the distinct difference between a L1 norm regularizer and all the other Lp norm regularizers with p > 1. This difference appears to be fundamental in the sense that the Rademacher complexity of a unit Lp>1 norm combination of functions depends explicitly on the number of functions used in the ensemble while no such dependence occurs for the L1 norm case. Consequently, an explicit control of the number of voters in the ensemble should be exercised while boosting with a Lp>1 regularizer, but no such control is needed while regularizing with the L1 norm.\nConcerning the issue of the surrogate loss to be used for boosting, we propose the simple quadratic loss (hence the name QuadBoost). Although the theory suggests using the hinge loss, this leads to linear programming algorithms which could become computationally prohibitive with large number of voters and huge data sets. The quadratic loss, on the other hand, leads to very simple rules for setting the weights on the voters, does not need to assign weights on the training examples, and exhibits a rate of decrease of the training error which is slightly faster than the one achieved\nby AdaBoost. The experimental results confirm the expectation of the theory that QuadBoost is a very efficient method for learning ensembles and, consequently, is likely to be effective for learning complex models from data."
    }, {
      "heading" : "2 A General Risk Bound for Ensembles",
      "text" : "We consider the difficult task of finding classifiers having small expected zero-one loss. In the supervised learning setting, the learner has access to a training set S , {(x1, y1), . . . , (xm, ym)} of m examples where each example (xi, yi) is drawn independently from a fixed, but unknown, distribution D on X × Y . For the binary classification case, the input space X is arbitrary, whereas the output space Y = {−1,+1}. Given access to S, the task of the learner is to find, in reasonable time, a classifier f : X → Y having a small expected zero-one loss E(x,y)∼DI(f(x) 6= y), where I(a) = 1 if predicate a is true, and 0 otherwise.\nWe are not only concerned here with the problem of finding classifiers with good generalization (i.e., small expected zero-one loss), but also with the running time complexity of finding such classifiers. In that respect, ensemble methods, such as AdaBoost [2], appear to us as mostly promising. Let us then investigate these methods with respect to both objectives.\nAs is often the case with ensemble methods, we assume that we have access to a (possibly continuous) set H of real-valued functions that we call the set of possible voters. Our task is to select from H a finite subset of n voters on which a weighted majority vote classifier is produced. Let h , (h1, . . . , hn) denote the vector formed by concatenating these n voters and let α , (α1, . . . , αn) denote the vector of n non-negative weights used to weight the voters. For any input x ∈ X , the output fα,h(x) on x of the weighted majority vote is given by\nfα,h(x) = sgn (α · h(x)) , sgn ( n ∑\ni=1\nαihi(x)\n)\n,\nwhere sgn(z) = +1 if z > 0, and −1 otherwise. We assume that H is auto-complemented which means that if h ∈ H, then there exists h′ ∈ H such that h′(x) = −h(x) ∀x ∈ X . The advantage of using an auto-complemented set H of voters is that we can assume, without loss of generality (w.l.o.g.), that each weight αi is non negative because a negative weight αi multiplying hi(x) gives the same real value as |αi| multiplying −hi(x). To find out what the majority vote fα,h should optimize on the training data S to have good generalization, we have investigated guarantees known as uniform risk bounds. In particular, those which are based on the Rademacher complexity are particularly appealing and tight. In a nutshell, the Rademacher complexity of a class of functions measures its capacity to fit random noise. More precisely, given a set S of m examples, the empirical Rademacher complexity RS(F) of a class F of real-valued functions and its expectation Rm(F) are defined as\nRS(F) , E σ sup f∈F\n1\nm\nm ∑\ni=1\nσif(xi) ; Rm(F) , E S∼Dm RS(F) ,\nwhere σ , (σ1, . . . , σm) and where each σi is a ±1-valued random variable drawn independently according to the uniform distribution.\nGiven a weighted majority vote fα,h of functions taken from a function class H, let ‖α‖p denote the Lp norm of vector α for any p ≥ 1 and let ‖α‖0 denote the number of non-zero components of α (i.e., the L0 norm of α). An important issue concerning majority votes is the complexity of the set of functions induced by taking weighted combinations of functions at fixed norm. Hence, given a class H of real-valued functions, let us consider\nCnp (H) , {x 7→ α · h(x) | hi ∈ H ∀i, ‖α‖0 = n, ‖α‖p = 1} . Note that RS(Cn1 (H)) = RS(H) for any n since it is well known that the Rademacher complexity of the convex hull of H is equal to the Rademacher complexity of H. But what happens if the weighted combination is at unit Lp norm for p > 1? The next lemma, which is apparently new, tells us that taking a weighted combination of functions strictly increases the Rademacher complexity for p > 1.\nLemma 1. For any class H of real-valued functions, any n ∈ N, and any p ∈ [1,+∞], we have\nRS(Cnp (H)) = n1− 1 pRS(H) .\nProof. Let (1/q) , 1 − (1/p). We will make use of the known fact that Hölder’s inequality is attained at the supremum, namely for all p ≥ 1 and any vector v, we have\nsup α:‖α‖p=1\nn ∑\ni=1\nαivi =\n(\nn ∑\ni=1\nvqi\n)1/q\n.\nConsequently, we have\nRS(Cnp ) = Eσ suph1,...,hn sup α:‖α‖p=1\n1\nm\nm ∑\nk=1\nσk\nn ∑\ni=1\nαihi(xk)\n= E σ sup h1,...,hn sup α:‖α‖p=1\nn ∑\ni=1\nαi\n[\n1\nm\nm ∑\nk=1\nσkhi(xk)\n]\n= E σ sup h1,...,hn\n(\nn ∑\ni=1\n[\n1\nm\nm ∑\nk=1\nσkhi(xk)\n]q)1/q\n= E σ\n(\nsup h1,...,hn\nn ∑\ni=1\n[\n1\nm\nm ∑\nk=1\nσkhi(xk)\n]q)1/q\n= E σ\n(\nn sup h∈H\n[\n1\nm\nm ∑\nk=1\nσkh(xk)\n]q)1/q\n= E σ\n(\nn\n[\nsup h∈H\n1\nm\nm ∑\nk=1\nσkh(xk)\n]q)1/q\n= n1/qRS(H) ,\nwhich proves the lemma.\nThe next theorem, which is built on Lemma (1), constitutes the main theoretical result of the paper. It provides a uniform upper-bound on the expected zero-one loss of weighted majority votes fα,h in terms of their empirical risk (i.e., the expected loss estimated on the training data) measured with respect to any loss function L which upper-bounds the zero-one loss. The upper-bound also depends on the Lipschitz property of the clipped version of L. To define these notions precisely, let L(yα · h(x)) denote the loss incurred by fα,h, as measured by L, on example (x, y). Then the loss incurred by fα,h, as measured by the clipped version of L, is defined to be JL(yα · h(x))K1 where JxK1 , min (x, 1). Finally, a function A : R → R is said to be ℓ-Lipschitz for some ℓ > 0 if and only if |A(x) −A(x′)| ≤ ℓ|x− x′| for any x and x′. Theorem 1. Consider any distribution D on X × Y . Consider any loss function L which upperbounds the zero-one loss and for which its clipped version is ℓ-Lipschitz. Let H be any class of real-valued functions on the input space X . For all p ∈ [1,+∞], for all δ ∈ (0, 1], with probability at least 1− δ over the random draws of S ∼ Dm, we have simultaneously for all α on H,\nE (x,y)∼D I(yα · h(x) ≤ 0) ≤ 1 m\nm ∑\ni=1\nL(yiα · h(xi)) + 2ℓ‖α‖ 1− 1 p 0 ‖α‖pRm(H)\n+\n√\n1\n2m\n(\nlog 1\nδ + 2 log log2 [2‖α‖p]\n)\n. (1)\nAs it is usual with Rademacher complexities, Theorem (1) also applies with Rm(H) replaced by RS(H) if δ is replaced by δ/2 and if the last term is multiplied by 3.\nProof. The fundamental theorem on Rademacher complexities (see, for example, Mohri et al. [4], Shawe-Taylor and Cristianini [5]) states that for any class G mapping some domain Z to [0, 1], for any distribution D on Z , for any δ > 0, with probability at least 1− δ, we have simultaneously for all g ∈ G\nE z∼D g(z) ≤ 1 m\nm ∑\ni=1\ng(zi) + 2Rm(G) + √ 1\n2m log\n1 δ .\nGiven any L, any p ∈ [1,+∞], and any γ > 0, we can apply this theorem to the set Gγ of functions that maps each example (x, y) to JL(y qγ · h(x))K1 for ‖q‖p = 1 when each hi ∈ H. By hypothesis, L is ℓ-Lipschitz. Hence, by Talagran’s lemma (see, for example, Theorem 4.2 of Mohri et al. [4]), we have that RS(Gγ) = ℓRS(G′γ), where G′γ is the set of functions mapping (x, y) to y qγ · h(x).\nAlso, since γ is a constant and y = ±1, we have that RS(G′γ) = (1/γ)RS(Cnp ) where Cnp is the set of functions mapping x to q · h(x) such that ‖q‖p = 1 and ‖q‖0 = n. Thus, RS(Gγ) = (ℓ/γ)RS(Cnp ) = (ℓ/γ)‖q‖ 1−(1/p) 0 RS(H) according to Lemma 1.\nThen, for any γ > 0, with probability at least 1− δ we have\nE (x,y)∼D JL(yq γ ·h(x))K1 ≤ 1 m\nm ∑\ni=1\nJL(yi q γ ·h(xi))K1+2(ℓ/γ)‖q‖1−(1/p)0 Rm(H)+\n√\n1\n2m log\n1 δ .\nBy using the union bound technique of Theorem 4.5 of Mohri et al. [4], we can make the above bound valid uniformly over all values for γ by adding √\n(1/m) log log2(2/γ) to its right hand side. Then, by using α = q/γ, we have that ‖α‖p = 1/γ. The theorem then follows from the fact that I(yα · h(x) ≤ 0) ≤ JL(yα · h(x))K1 ≤ L(yα · h(x)) ∀(x, y) ∈ X × Y .\nIf we ignore the slowly increasing log log2(‖α‖p) term in Equation (1), Theorem (1) tells us that to obtain a majority vote with a small zero-one loss, it is sufficient to minimize the empirical risk, as measured with respect to a surrogate loss L, plus a regularization term equal to 2ℓ‖α‖1− 1 p\n0 ‖α‖pRm(H). Note that when p = 1, this regularization term is equal to 2ℓ‖α‖1Rm(H) and, therefore, does not depend on the number ‖α‖0 of voters used by the majority vote. Hence, when performing L1 regularization with a fixed set H of voters and surrogate loss L, the only thing that matters is to control the L1 norm of α while minimizing the empirical risk. This is in sharp contrast with the p > 1 cases where we need to control both the Lp norm ofα and the number ‖α‖0 of voters used by fα,h. Therefore, iterative learning algorithms that minimize the empirical loss under Lp regularization should also perform early stopping or exercise some other explicit control on the number of voters used by the majority vote when p > 1. This explicit control on ‖α‖0 is mostly important with L∞ regularization because the regularization term then grows linearly with ‖α‖0. But it is also important with L2 regularization since the regularization term then grows with √\n‖α‖0. Finally, and perhaps most importantly, just minimizing iteratively the empirical risk and using early stopping to control overfitting is a simple learning strategy that is supported by Theorem (1). Indeed, as long as the iterative procedure does not choose large weights for the voters, early stopping keeps ‖α‖1 under control and the right hand side (r.h.s.) of Equation (1) should be small when ‖α‖1 ≪ √ m (since Rm(H) ∈ O(1/ √ m) when H has finite VC dimension) and the empirical risk of the ensemble has reached a small value.\nThe other important issue regarding Theorem (1) concerns the choice of the surrogate loss L. Obviously, the closer (or tighter) L is to the zero-one loss, the better. However, to avoid computational problems associated with the existence of several local minima of the empirical risk, let us settle for a surrogate L convex in α. Perhaps the tightest convex surrogate that we can think of is the hinge loss. This is the surrogate used by the LPBoost algorithm of Demiriz et al. [6]. Hence, the hinge loss surrogate, used in conjunction with L1 regularization as is done with LPBoost, is a learning strategy strongly supported by Theorem (1). However, LPBoost iteratively solves a linear program each time a new voter is inserted into the ensemble—a computationally expensive strategy when the number of voters in the ensemble is large. As machine learning is entering into the Big Data era, this should typically occur for challenging complex tasks involving huge data sets. One solution is to use a smoother surrogate, containing no discontinuities in its derivatives, at the price of sacrificing a bit of the tightness with respect to the zero-one loss. The exponential loss minimized by AdaBoost [2] has infinitely many continuous derivatives but is very far from being a tight upper bound of the zero-one\nloss. The logistic loss minimized by LogitBoost [7] is much better in that respect but, somehow, does not nicely mix with Lp regularization and does not produce simple update rules in the sense that each example of the training set needs to be reweighed each time a new voter is added to the ensemble. Are there simpler updates rules that perform as well as AdaBoost and LogitBoost? Let’s try to answer this question by analyzing what happens if we use the simple quadratic loss for the surrogate L. The quadratic loss is commonly used in classical learning methods such as (kernel) ridge regression and back-propagation neural network learning. But it is surprisingly absent among boosting methods for ensembles that attempt to produce a majority vote by iteratively adding voters chosen from a possibly continuous set H. One notable exception is the work of Germain et al. [8], where boosting with the quadratic loss was proposed with a Kullback-Leibler regularizer. Consequently, their boosting algorithm turned out to be different than the algorithms proposed in the next section. Moreover, their PAC-Bayesian theory based on quasi-uniform posteriors was developed only for the case of a finite set of voters and does not extend to the continuous case. Hence, it was not realized that it was necessary to control the number of voters when boosting with a Lp regularizer with p > 1, while no such control is needed for p = 1."
    }, {
      "heading" : "3 QuadBoost",
      "text" : "We now investigate if the quadratic loss can yield simple and efficient iterative algorithms for producing ensemble of voters. For this task, consider any n ∈ N, any vector h , (h1, . . . , hn) of voters where each hi ∈ H, and any vectorα , (α1, . . . , αn) of non-negative weights on h. Let us start by writing the quadratic risk (on m examples) as\n1\nm\nm ∑\nk=1\n(yk −α · h(xk))2 = 1− 2 n ∑\nj=1\nαj 1\nm\nm ∑\nk=1\nykhj(xk) +\nn ∑\nj=1\nα2j 1\nm\nm ∑\nk=1\nh2j(xk)\n+ 2\nn ∑\nj=2\nαj 1\nm\nm ∑\nk=1\nhj(xk)\nj−1 ∑\ni=1\nαihi(xk) . (2)\nIf, for each voter hj of the ensemble, we now define its margin µj as\nµj , 1\nm\nm ∑\nk=1\nykhj(xk) , (3)\nand its correlation Mj with the weighted sum of the previous voters as\nMj ,\n{\n1 m ∑m k=1 hj(xk) ∑j−1 i=1 αihi(xk) if j > 1 0 if j = 1 , (4)\nwe obtain the following decomposition of the quadratic risk\n1\nm\nm ∑\nk=1\n(yk−α ·h(xk))2 = 1−2 n ∑\nj=1\nαj(µj−Mj)+ n ∑\nj=1\nα2jηj , where ηj , 1\nm\nm ∑\nk=1\nh2j(xk) . (5)\nThis decomposition tells us that to minimize the quadratic risk iteratively, we should, at each step j, find a voter hj ∈ H that maximizes µj − Mj . Once a voter hj is chosen, its weight αj that minimizes the quadratic risk is obtained by setting to 0 its partial derivative with respect to αj . This gives\nαj = 1\nηj (µj −Mj) ; (without regularization) . (6)\nThus, adding a voter hj with weight αj in the ensemble decreases the empirical quadratic risk of the ensemble by (µj −Mj)2/ηj . Note that, unless each function h ∈ H has a margin equal to the correlation with the weighted sum of the voters currently in the ensemble, we can always find hj in an auto-complemented set H for which µj is greater than Mj because if µi − Mi < 0 for hi, we have µj − Mj = −(µi − Mi) > 0 for hj = −hi. Note that when it is computationally very expensive to find hj ∈ H maximizing µj −Mj , we can settle to find more rapidly some hj having some positive µj−Mj since, in that case, we still make progress by lowering the empirical quadratic risk by (µj −Mj)2/ηj .\nVanilla QuadBoost : Finding, at each step j, a voter hj ∈ H achieving some positive value for µj−Mj and inserting this voter hj with the weight αj in the majority vote defines, what we call, the vanilla version of QuadBoost. Of course, in that case, Theorem (1) tells that we should eventually early stop this greedy process to avoid over fitting–which is also the case for AdaBoost.\nOne reason often invoked for using AdaBoost is its exponentially fast decrease of the empirical error as a function of the number of iterations (boosting rounds). More precisely, assuming that, at each iteration, the weak learner can always produce a classifier achieving a training error (on the weighted examples) of at most (1/2)−γ, the (zero-one loss) training error produced by the AdaBoost ensemble is at most exp(−2γ2T ) after T iterations [2]. Consequently, the number of iterations needed for AdaBoost to obtain an ensemble achieving less than ǫ empirical error is [1/(2γ2)] log(1/ǫ).\nIn comparison, under the equivalent assumption that the weak learner is always able to find a voter hj ∈ H where µj − Mj > γ, the decrease in the quadratic empirical risk (which upper-bounds the zero-one training error) achieved by the QuadBoost ensemble is at least (µj − Mj)2 (since ηj = 1 for classifiers) at each iteration. Hence, under this hypothesis, the training error produced by the QuadBoost ensemble after T iterations is at most 1 − Tγ2. Hence, under this hypothesis, QuadBoost needs at most (1/γ2) iterations to have an an ensemble achieving at most ǫ training error. Consequently, in comparison with AdaBoost, and under an equivalent hypothesis, the convergence rate of QuadBoost is slightly better.\nLet us now investigate the different Lp regularized versions of QuadBoost for p = 1, 2, and +∞, in accordance with the insights given by Theorem (1). To this end, we first note that the clipped quadratic loss is 2-Lipschitz, so ℓ = 2 in Theorem (1) when L is the quadratic loss.\nQuadBoost-L1 : For p = 1, we should minimize the empirical quadratic risk plus 2λ‖α‖1, where, according to Theorem (1), λ should be equal to 2Rm(H) But, in practice, a smaller value for λ should provide better results as there are always some looseness in risk bounds. If we add this regularization term to the expression of the empirical risk given by Equation (5) and then set to zero the first derivative w.r.t. αj of this objective, we find that, at each step j, the solution for αj is given by\nαj = 1\nηj (µj −Mj − λ) ; (L1 regularization) . (7)\nHere, no explicit early stopping is needed as, for some chosen λ > 0, we will eventually be unable to find a voter hj having µj − Mj > λ. Hence, the amount of voters contained in the ensemble is controlled by parameter λ: the larger λ is, the smaller the ensemble will be. Finally note that this algorithm can be viewed as an iterative version of the LASSO method [9, 10] but where the functions are selected from a possibly continuous set H.\nQuadBoost-L2 : For p = 2, we should minimize the empirical quadratic risk plus λ‖α‖2, where, according to Theorem (1), λ should be equal to 4 √\n‖α‖0Rm(H). If we add this regularization term to the expression of the empirical risk given by Equation (5) and then set to zero the first derivative w.r.t. αj of this objective, we find that, at each step j, the solution for αj is given by\nαj = 1\nηj + λ (µj −Mj) ; (L2 regularization) . (8)\nSince according to theory,λ should increase with the number ‖α‖0 of voters in the ensemble, explicit early-stopping should be performed in addition to the above rule for αj . Finally note that this algorithm can be viewed as an iterative version of ridge regression but where the functions are selected from a possibly continuous set H.\nQuadBoost-L∞ : For p = +∞, we should minimize the empirical quadratic risk plus λ‖α‖∞, where, according to Theorem (1), λ should be equal to 4‖α‖0Rm(H). Since ‖α‖∞ = αmax, which is the allowed upper-bound for the weight values of the voters, each αj should simply minimise the empirical risk provided that it does not exceed αmax. The solution for αj is then given by\nαj =\n{ 1 ηj (µj −Mj) if 1ηj (µj −Mj) ≤ αmax\nαmax otherwise ; (L∞ regularization) . (9)\nSince according to theory, λ should increase rapidly with the number ‖α‖0 of voters in the ensemble, explicit early-stopping should be performed in addition to the above rule for αj ."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "Let us first note that, although all the boosting algorithms tested in this section can select the voters from a continuous set, we have, for the sake of comparison, used only finite sets. We feel that continuous sets of voters raise several important issues, including a non trivial trade off between precision, time complexity, and capacity (or Rademacher complexity), which clearly need extra work to be lucidly addressed and, consequently, should be treated in a separate paper.\nWe now report empirical experiments on binary classification datasets from the UCI Machine Learning Repository [11]. Each dataset was randomly split into a training set S of at least half of the examples and at most 500 examples, and a testing set containing the remaining examples. Each dataset has been normalized using a hyperbolic tangent, whose parameters have been chosen using the training set S only. We considered a finite set of decision stumps (one-level decision trees) which consists of a single input attribute and a threshold. For each dataset, we generated 10 decision stumps per attribute and their complements.\nWe compared QuadBoost (L1, L2, L∞, and its vanilla version that has no regularizer) with LPBoost [6], and AdaBoost [2]. For each algorithm, all hyperparameters have been chosen amont 10 values in a logarithmic scale, by performing 5-fold cross-validation on the training set S, and the reported values are the risks on the testing set.\nFor QuadBoost-L1, λ was chosen in a range between 10−4 and 100. For QuadBoost-L2, the range for λ was between 100 and 103, and the range for the number of iteration T was between 101 and 105. For QuadBoost-L∞, λ was chosen between 10−4 and 10−1, and T between 100 and 105. Vanilla QuadBoost’s T was chosen between 100 and 103, hyperparameterC of LPBoost was chosen between 10−3 and 103, and finally the number of iterations T of AdaBoost was chosen between 102 and 106.\nTable 1 reports the resulting testing risks and training times. The results show that all four variants of QuadBoost that we considered are competitive with state-of-the-art boosting algorithms. When\ncomparing vanilla QuadBoost with AdaBoost, where the only hyperparameter to tune is the number of iterations, QuadBoost wins or ties 17 times over 25 datasets and is 20 times faster. When comparing QuadBoost-L1 with LPBoost, which is also a L1-norm regularized algorithm, QuadBoost outperforms or ties with LPBoost 16 times over 25 datasets and is also 20 times faster.\nTable 2 shows a statistical comparison between all these algorithms, using the pairwise Poisson binomial test of Lacoste et al. [12]. Given a set of datasets, this test gives the probability that a learning algorithm is better than another one. This table also shows the pairs of algorithms having a significant performance difference using the pairwise sign test [13]. The only significant values indicate that QuadBoost with L∞-norm and L2-norm regularization outperform QuadBoost without regularization, and that QuadBoost with L2-norm regularization outperforms QuadBoost with L1norm regularization. Note however that for the two former algorithms, we have performed crossvalidation over two hyperparameters, which gave them an advantage. Another possible explanation for the observed improved performance of the L2 and L∞ regularized versions is the increased Rademacher complexity of L2 and L∞ combinations over L1 combinations.\nIn conclusion, the empirical experiments show that QuadBoost is a fast and accurate ensemble method that competes well against other state of the art boosting algorithms."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have presented a uniform risk bound for ensembles which holds for any surrogate loss and Lp norm of the weighted combination of voters which can be selected from a continuous set. An important feature of this result is the fact that weighted combinations of unit Lp norm for p > 1 have strictly larger Rademacher complexity than weighted combinations of unit L1 norm and, as a consequence, the risk bound exhibits an explicit dependence on the number of voters when p > 1 while no such dependence occurs when p = 1. This result suggests to perform an explicit control of the number of voters when regularizing with the Lp norm for p > 1 while no such control is needed for p = 1. Finally, our theoretical and empirical results suggest that the simple quadratic loss surrogate should be used for boosting instead of the usual exponential loss."
    } ],
    "references" : [ {
      "title" : "The strength of weak learnability",
      "author" : [ "Robert E. Schapire" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1990
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Boosting algorithms as gradient descent",
      "author" : [ "Llew Mason", "Jonathan Baxter", "Peter L. Bartlett", "Marcus R. Frean" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2000
    }, {
      "title" : "Foundations of machine learning",
      "author" : [ "Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "John Shawe-Taylor", "Nello Cristianini" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Linear programming boosting via column generation",
      "author" : [ "Ayhan Demiriz", "Kristin P Bennett", "John Shawe-Taylor" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "Additive logistic regression: a statistical view of boosting",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "From pac-bayes bounds to kl regularization",
      "author" : [ "Pascal Germain", "Alexandre Lacasse", "Francois Laviolette", "Mario Marchand", "Sara Shanian" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society B,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1996
    }, {
      "title" : "Lasso, iterative feature selection and the correlation selector: Oracle inequalities and numerical performances",
      "author" : [ "Pierre Alquier" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "UCI Repository of machine learning databases. Department of Information and Computer",
      "author" : [ "C.L. Blake", "C.J. Merz" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Bayesian comparison of machine learning algorithms on single and multiple datasets",
      "author" : [ "Alexandre Lacoste", "François Laviolette", "Mario Marchand" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Nonparametric statistics",
      "author" : [ "W. Mendenhall" ],
      "venue" : "Introduction to Probability and Statistics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1983
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Among the learning algorithms which seem most likely to be able to scale up to meet this challenge are ensemble methods based on the idea of boosting weak learners [1].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 1,
      "context" : "Take AdaBoost [2] for example.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "After AdaBoost was published, it soon became clear that infinitely many surrogate loss functions [3] and regularizers could be used for boosting and, without surprise, many variants have been proposed—to the point where the practitioner is often completely overwhelmed when confronted with the choice of picking a boosting algorithm for his learning task.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "In that respect, ensemble methods, such as AdaBoost [2], appear to us as mostly promising.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "[4], Shawe-Taylor and Cristianini [5]) states that for any class G mapping some domain Z to [0, 1], for any distribution D on Z , for any δ > 0, with probability at least 1− δ, we have simultaneously for all g ∈ G E z∼D g(z) ≤ 1 m m",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[4], Shawe-Taylor and Cristianini [5]) states that for any class G mapping some domain Z to [0, 1], for any distribution D on Z , for any δ > 0, with probability at least 1− δ, we have simultaneously for all g ∈ G E z∼D g(z) ≤ 1 m m",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "[4], Shawe-Taylor and Cristianini [5]) states that for any class G mapping some domain Z to [0, 1], for any distribution D on Z , for any δ > 0, with probability at least 1− δ, we have simultaneously for all g ∈ G E z∼D g(z) ≤ 1 m m",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "[4]), we have that RS(Gγ) = lRS(G γ), where G′ γ is the set of functions mapping (x, y) to y q γ · h(x).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4], we can make the above bound valid uniformly over all values for γ by adding √",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "The exponential loss minimized by AdaBoost [2] has infinitely many continuous derivatives but is very far from being a tight upper bound of the zero-one",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "The logistic loss minimized by LogitBoost [7] is much better in that respect but, somehow, does not nicely mix with Lp regularization and does not produce simple update rules in the sense that each example of the training set needs to be reweighed each time a new voter is added to the ensemble.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "[8], where boosting with the quadratic loss was proposed with a Kullback-Leibler regularizer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "More precisely, assuming that, at each iteration, the weak learner can always produce a classifier achieving a training error (on the weighted examples) of at most (1/2)−γ, the (zero-one loss) training error produced by the AdaBoost ensemble is at most exp(−2γ2T ) after T iterations [2].",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 8,
      "context" : "Finally note that this algorithm can be viewed as an iterative version of the LASSO method [9, 10] but where the functions are selected from a possibly continuous set H.",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "Finally note that this algorithm can be viewed as an iterative version of the LASSO method [9, 10] but where the functions are selected from a possibly continuous set H.",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "We now report empirical experiments on binary classification datasets from the UCI Machine Learning Repository [11].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "We compared QuadBoost (L1, L2, L∞, and its vanilla version that has no regularizer) with LPBoost [6], and AdaBoost [2].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "We compared QuadBoost (L1, L2, L∞, and its vanilla version that has no regularizer) with LPBoost [6], and AdaBoost [2].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "This table also shows the pairs of algorithms having a significant performance difference using the pairwise sign test [13].",
      "startOffset" : 119,
      "endOffset" : 123
    } ],
    "year" : 2017,
    "abstractText" : "We first present a general risk bound for ensembles that depends on the Lp norm of the weighted combination of voters which can be selected from a continuous set. We then propose a boosting method, called QuadBoost, which is strongly supported by the general risk bound and has very simple rules for assigning the voters’ weights. Moreover, QuadBoost exhibits a rate of decrease of its empirical error which is slightly faster than the one achieved by AdaBoost. The experimental results confirm the expectation of the theory that QuadBoost is a very efficient method for learning ensembles.",
    "creator" : "LaTeX with hyperref package"
  }
}