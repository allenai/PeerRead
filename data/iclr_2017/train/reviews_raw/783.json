{"conference": "ICLR 2017 conference submission", "title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.", "reviews": [{"is_meta_review": true, "comments": "This paper was easy to read, the main idea was presented very clearly.\n\nThe main points of the paper (and my concerns are below) can be summarized as follows:\n1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)?\n2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.\n3.they propose to take gradient from the first \"N\" workers out of \"N+b\" \nworkers available. My concern here is that they focused only on the \nworkers, but what if the \"parameter server\" will became to slow? What \nif the parameter server would be the bottleneck? How would you address \nthis situation? But still if the number of nodes (N) is not large, and \nthe deep DNN is used, I can imagine that the communciation will not \ntake more than 30% of the run-time.\n\n\nMy largest concern is with the experiments. Different batch size \nimplies that different learning rate should be chosen, right? How did \nyou tune the learning rates and other parameters for e.g. Figure 5 you \nprovide some formulas in (A2) but clearly this can bias your Figures, \nright? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could \nbe somehow more representative? also it would be nicer if you run the \nexperiment many times and then report average, best and worst case \nbehaviour. because now it can be just coinsidence, right?", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that \"backup workers\" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Interesting Idea, but can be improved and I have a few concerns about experiments", "is_meta_review": false, "comments": "This paper was easy to read, the main idea was presented very clearly.\n\nThe main points of the paper (and my concerns are below) can be summarized as follows:\n1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)?\n2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.\n3.they propose to take gradient from the first \"N\" workers out of \"N+b\" \nworkers available. My concern here is that they focused only on the \nworkers, but what if the \"parameter server\" will became to slow? What \nif the parameter server would be the bottleneck? How would you address \nthis situation? But still if the number of nodes (N) is not large, and \nthe deep DNN is used, I can imagine that the communciation will not \ntake more than 30% of the run-time.\n\n\nMy largest concern is with the experiments. Different batch size \nimplies that different learning rate should be chosen, right? How did \nyou tune the learning rates and other parameters for e.g. Figure 5 you \nprovide some formulas in (A2) but clearly this can bias your Figures, \nright? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could \nbe somehow more representative? also it would be nicer if you run the \nexperiment many times and then report average, best and worst case \nbehaviour. because now it can be just coinsidence, right? \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "02 Jan 2017", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "no title", "is_meta_review": false, "comments": "This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. \n\nMy main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:\n\n- provide more experiments to show the performance with different efficiency distributions of learners.\n- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Official Review", "is_meta_review": false, "comments": "The paper claim that, when supported by a number of backup workers, synchronized-SGD \nactually works better than async-SGD. The paper first analyze the problem of staled updates\nin async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the \nauthors shows the effectiveness of the proposed method in applications to Inception Net\nand PixelCNN.\n\nThe idea is very simple, but in practice it can be quite useful in industry settings where \nadding some backup workders is not a big problem in cost. Nevertheless, I think the \nproposed solution is quite straightforward to come up with when we assume that \neach worker contains the full dataset and we have budge to add more workers. So, \nunder this setting, it seems quite natural to have a better performance with the additional \nbackup workers that avoid the staggering worker problem. And, with this assumtion I'm not \nsure if the proposed solution is solving difficult enough problem with novel enough idea. \n\nIn the experiments, for fair comparison, I think the Async-SGD should also have a mechanism \nto cut off updates of too much staledness just as the proposed method ignores all the remaining \nupdates after having N updates. For example, one can measure the average time spent to \nobtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD \nso that Async-SGD does not perform so poorly.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "efficiency", "OTHER_KEYS": "Asim Kadav", "comments": "Is this technique efficient? If you are using 5-10% extra resources (backup workers), do you get at least 5-10% speedup over sync SGD? Figure 8(a) does not have any sync results. It is well understood that async may sometimes never converge to the correct results.", "IS_META_REVIEW": false, "DATE": "18 Dec 2016", "is_meta_review": false}, {"TITLE": "question", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "09 Dec 2016", "is_meta_review": false}, {"TITLE": "Some discussion on tuning", "OTHER_KEYS": "Ioannis Mitliagkas", "comments": "The proposed technique of over-provisioning a few workers to improve the hardware efficiency of synchronous training gives a very welcome boost, as evidenced beautifully by Figure 6.\n\nHowever, as I have discussed in private communication with the authors, the comparison with asynchronous methods leaves a few questions regarding tuning.\u00a0\n\nLearning rate:\u00a0\n\nThe authors carefully describe the process of tuning the synchronous implementation. However no tuning is reported for the asynchronous implementation: the value is set to 0.045 for all configurations. Figure 7(a) shows that tuning the learning rate can cause a difference of almost a whole percentage point in test precision.\n1. What other values of LR have the authors tried for asynchronous training?\n2. Is 0.045 the best one for all configurations they report?\n3. Is there any chance that the 0.5% difference in precision between sync and async is due to insufficient LR tuning (as suggested by Figure 7(a))?\u00a0\n\n\nMomentum:\n\nOur results (", "IS_META_REVIEW": false, "DATE": "06 Dec 2016", "is_meta_review": false}, {"TITLE": "assumption", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Jianmin Chen*, Xinghao Pan*, Rajat Monga, Samy Bengio, Rafal Jozefowicz", "KEYWORDS": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "accepted": false, "id": ""}
