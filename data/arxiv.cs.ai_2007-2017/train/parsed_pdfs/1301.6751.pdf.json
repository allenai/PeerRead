{
  "name" : "1301.6751.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Method for Speeding Up Value Iteration in Partially Observable Markov Decision Processes",
    "authors" : [ "Nevin L. Zhang", "Stephen S. Lee", "Weihong Zhang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present a technique for speeding up the convergence of value iteration for par tially observable Markov decisions processes (POMDPs). The underlying idea is similar to that behind modified policy iteration for fully observable Markov decision processes (MDPs). The technique can be easily incor porated into any existing POMDP value it eration algorithms. Experiments have been conducted on several test problems with one POMDP value iteration algorithm called in cremental pruning. We find that the tech nique can make incremental pruning run sev eral orders of magnitude faster.\n1 INTRODUCTION\nPOMDPs are a model for sequential decision making problems where effects of actions are nondeterministic and the state of the world is not known with certainty. They have attracted many researchers in Operations Research and AI because of their potential applica tions in a wide range of areas (Monahan 1982, Cas sandra 1998b). However, there is still a significant gap between this potential and actual applications, primar ily due to the lack of effective solution methods. For this reason, much recent effort has been devoted to finding efficient algorithms for POMDPs.\nThis paper is concerned with only exact algorithms. Most exact algorithms are value iteration algorithms. They begin with an initial value function and improve it iteratively until the Bellman residual falls below a predetermined threshold. See Cassandra (1998a) for excellent descriptions, analyses, and empirical compar isons of those algorithms.\nThere are also policy iteration algorithms for POMDPs. The first one is proposed by Sondik (1978).\nA simpler one is recently developed by Hansen (1998).\nIt is known that, in terms of number of iterations, pol icy iteration for MDP converges quadratically while value iteration converges linearly (e.g. Puterman 1990, page 369). Hansen has empirically shown that his pol icy iteration algorithm for POMDPs also converges much faster than one of the most efficient known value iteration algorithms, namely incremental prun ing (Zhang and Liu 1997, Cassandra et a/1997).\nPolicy iteration for MDPs solves a system of linear equations at each iteration. The numbers of unknowns and equations in the system are the same as the size of the state space. Consequentially, it is computationally prohibitive to solve the system when the state space is large. Modified policy iteration (MPI) (Puterman 1990, page 371) alleviates the problem using a method that computes an approximate solution without actu ally solving the system. Numerical results reported in Puterman and Shin (1978) suggest that modified pol icy iteration is more efficient than either value iteration or policy iteration in practice.\nHansen (1998) points out that the idea of MPI can also be incorporated into his POMDP policy iteration algorithm and finds that such an exercise is not very helpful (Hansen 1999).\nThe paper describes another way to apply the MPI idea to POMDPs. Our method is based on the view that MPI is also a variant of value iteration (van N unen 1976) 1. Under this view, the basic idea is to \"improve\" the current value function for several steps using the current policy before feeding it to the next step of value iteration. Those improvement steps are less expensive than standard value iteration steps. Nonetheless, they do get the current value function closer to the optimal value function.\nMPI for MDPs improves a value function at all states.\n1 As a matter of fact, it was first proposed as a variant of value iteration by van Nunen.\nThis cannot be done for POMDPs since there are infi nite many belief states. Our method improves a value function at a finite number of selected belief states. A nice property of POMDPs is that when a value func tion is improved at one belief state, it is also improved in the neighborhood of that belief state.\nWe call our method point-based improvement for the lack of a better name. It is conceptually much simpler than Hensen's policy iteration algorithm. Nonetheless, it is as effective as, in some cases more effective than, Hansen's algorithm in reducing the number of itera tions it takes to find a policy of desired quality and hence drastically speeds up incremental pruning.\n2 POMDP AND VALUE ITERATION\nWe begin with a brief review of POMDPs and value iteration.\n2.1 POMDPs\nA partially observable Markov decision process (POMDP) is a sequential decision model for an agent who acts in a stochastic environment with only partial knowledge about the state of the environment. The set of possible states of the environment is referred to as the state space and is denoted by S. At each point in time, the environment is in one of the possible states. The agent does not directly observe the state. Rather, it receives an observation about it. We denote the set of all possible observations by Z. After receiving the observation, the agent chooses an action from a set A of possible actions and execute that action. There after, the agent receives an immediate reward and the environment evolves stochastically into a next state.\nMathematically, a POMDP is specified by the three sets S, Z, and A; a reward function r(s, a); a transi tion probability P(s'js, a); and an observation probabil ity P(zjs', a). The reward function characterizes the dependency of the immediate reward on the current state s and the current action a. The transition prob ability characterizes the dependency of the next state s' on the current state s and the current action a. The observation probability characterizes the dependency of the observation z at the next time point on the next state s' and the current action a.\n2.2 Policies and value functions\nSince the current observation does not fully reveal the identity of the current state, the agent needs to con sider all previous observations and actions when choos ing an action. Information about the current state\nSpeeding Up POMDP Value Iteration 697\ncontained in the current observation, previous obser vations, and previous actions can be summarized by a probability distribution over the state space. The probability distribution is sometimes called a belief state and denoted by b. For any possible states, b(s) is the probability that the current state is s. The set of all possible belief states is called the belief space. We denote it by B.\nA policy prescribes an action for each possible belief state. In other words, it is a mapping from B to A. Associated with policy 1r is its value function V\". For each belief state b, V\" (b) is the expected total dis counted reward that the agent receives by following the policy starting from b, i.e.\n00\nV\" (b) = E,,b[\"L A1rt] t=O\nwhere r1 is the reward received at timet and A (0 < A < 1) is the discount factor. It is known that there exists a policy 7r* such that V\"' (b) � V\"(b) for any other policy 1r and any belief state b. Such a policy is called an optimal policy. The value function of an optimal policy is called the optimal value function. We denote it by v•. For any positive number E, a policy 1r is E-optimal if\nV\"(b) + f?: V*(b) Vb E B.\n2.3 Value iteration\nTo explain value iteration, we need to consider how belief state evolves over time. Let b be the current belief state. The belief state at the next point in time depends not only on the current belief state, but also on the current action a and the next observation z. We denote it by b�. For any states', b�(s') is given by\nba( ') = E. P(s ', zjs, a)b(s)\nz 8 P(zjb, a) ' (1)\nwhere P(z, s'js,a)=P(zjs', a)P(s'js,a) and P(zjb, a)= Es,s' P(z, s'js, a)b(s) is the renormalization constant. As the notation suggests, the constant can also be interpreted as the probability of observing z after taking action a in belief state b.\nDefine an operator T that takes a value function V and returns another value function TV as follows:\nTV(b) = maxa[r(b,a) +A L P(zjb, a)V(b�)]Vb E B (2) z\nwhere r(b, a) = E. r(s, a)b(s) is the expected imme diate reward for taking action a in belief state b. For\n698 Zhang, Lee, and Zhang\na given value function V, a policy 71\" is said to be V  improving if\n7r(b) = arg maxa[r(b, a)+ A L P(zlb, a)V(b�)] (3) z\nfor all belief states b.\nValue iteration is an algorithm for finding <-optimal policies. It starts with an initial value function Vo and iterates using the following formula:\nVn = TVn-1·\nIt is known (e.g. Puterman 1990, Theorem 6.9) that Vn converges to V* as n goes to infinity. Value iteration terminates when the Bellman residual ma.xb IVn(b)- Vn-1 (b)l falls below <(1- >.)/2>.. When it does, a Vn-improving policy is <-optimal.\nSince there are infinite many possible belief states, value iteration cannot be carried explicitly. Fortu nately, it can be carried out implicitly. Before explain ing how, we first introduce several technical concepts and notations.\n2.4 Technical and notational considerations\nFor convenience, we call functions over the state space vectors. We use lower case Greek letters a and (3 to refer to vectors and script letters V and U to refer to sets of vectors. In contrast, the upper letters V and U always refer to value functions, i.e. functions over the belief space B. Note that a belief state is a function over the state space and hence can be viewed as a vector.\nA set V of vectors induces a value function as follows:\nf(b) = maxaEva.b Vb E B,\nwhere a.b is the inner product of a and b, i.e. a.b= Ls a(s)b(s). For convenience, we also use V(.) to denote the value function defined above: For any belief state b, V(b) stands for the quantity given at the right hand side of the above formula.\nA vector in a set is extraneous if its removal does not change the function that the set induces. It is useful otherwise. A set of vector is parsimonious if it contains no extraneous vectors.\n2.5 Implicit value iteration\nA value function V is represented by a set of vectors if it equals the value function induced by the set. When a value function is representable by a finite set of vec tors, there is a unique parsimonious set of vectors that represents the function.\nVI: 1. Vo +- {0}, n +- 0. 2. do { 3. n +- n + 1. 4. Vn +- DP-UPDATE(Vn-1). 5. } while maxbiVn(b)- Vn-1 (b)l > <(1- >.)/2>.. 6. return Vn.\nFigure 1: Value iteration for POMDPs.\nSondik (1971) has shown that if a value function Vis representable by a finite set of vectors, then so is the value function TV. The process of obtaining a par simonious representation for TV from a parsimonious representation of V is usually referred to as dynamic programming update. Let V be the parsimonious set that represents V. For convenience, we sometimes use TV to denote the parsimonious set of vectors that rep resents TV.\nIn practice, value iteration for POMDPs is implicitly carried in the way as shown in Figure 1. One be gins with a value function Vo that is representable by a finite set of vectors. In this paper, we assume the initial value function is 0. At each iteration, one per forma dynamic-programming update on the parsimo nious set V n-1 of vectors that represents the previous value function Vn-1 and obtains a parsimonious set of vectors Vn that represents the current value nmc tion Vn. One continues until the Bellman residual maxbJVn(b)- Vn-1 (b)l, which is determined by solving a sequence of linear programs, falls below a threshold.\n3 PROPERTIES OF VALUE ITERATION\nThis paper presents a technique for speeding up the convergence of value iteration. The technique is de signed for POMDPs with nonnegative rewards, i.e. POMDPs such that r(s, a)?:O for all s and a. In this section, we study the properties of value iteration in such POMDPs and show how a POMDP with negative rewards can be transformed into one with nonnegative rewards that is in some sense equivalent. Most proofs are omitted due to space limit.\nWe begin with a few definitions. In a POMDP, a value function U dominates another V if U(b)?:V(b) for all belief states b. It strictly dominates V if it dominates V and U(b)>V(b) for at least one belief state b. A value function is (strictly) suboptimal if it is (strictly) dominated by the optimal value function.\nA set of vectors is (strictly) dominated by a value func tion if its induced value function is. A set of vectors\nis (strictly) suboptimal if it is (strictly) dominated by the optimal value function.\nA set of vectors is (strictly) dominated by another set of vectors if it is (strictly) dominated by the value func tion induced by the that set.\n3.I General properties of value iteration\nLemma I In any POMDP, if a set of vectors V is suboptimal, then so is TV. Moreover, if V dominates another set of vectors V', then TV dominates TV'.\nLemma 2 In any POMDP, if a set of vectors V is strictly suboptimal, then there exist at least one belief state b such that TV(b)>V(b).\n3.2 Properties of value iteration in PO MOPs with nonnegative rewards\nUsing Lemma 2, one can show\nTheorem I Consider running VI on a POMDP with nonnegative rewards. Let Vn-! and Vn be respectively the sets of vectors produced at the n-lth and nth iter ation. Then, V n-! is strictly dominated by V n, which in tum is dominated by the optimal value function.\nNote that the theorem falls short of saying that, when the reward function is nonnegative, TV strictly domi nates V for any suboptimal set of vectors V. As a mat ter of fact, this is not always the case. As a counter ex ample, assume r(s0, a)=O for a certain state so regard less of the action. Let b0 be the belief state that is 1 at so and 0 everywhere else. Further assume V* (bo>O and let n0 be a vector such that no(so)=V*(bo) and n0 ( s) =0 for any other states s. It is easy to see that if V consists of only no, then TV(bo)<V(bo).\nDespite of the fact TV does not always strictly dom inate V, TV(b) is strictly larger than V(b) for beliefs b in most parts of the belief space when the reward function is nonnegative.\n3.3 POMDPs with negative rewards\nA POMDP with negative rewards can always be trans formed into one with nonnegative rewards by adding a large enough constant to the reward function. It is easy to see that an t-optimal policy for the transformed POMDP is also t-optimal for the original POMDP and vice versa. Moreover, the value function in the original POMDP of a policy equals that in the trans formed POMDP minus C/(1->..), where Cis the con stant added. In other words, the original POMDP is solved if the transformed POMDP is solved. There fore, we can restrict to POMDPs with nonnegative rewards without losing generality.\nSpeeding Up POMDP Value Iteration 699\nVI1: 1. Vo +- {0}, n +- 0. 2. do { 3. n +- n + 1; 4. Un +- DP-UPDATE(Vn-di 5. .Sf- maxb\\Un(b)- Vn-I(b)\\; 6. if .s > t(1 - >..) /2>.. 7. Vn f- improve(Un)i 8. } while .S > e(1 - >..) /2>.. 9. Return Vn·\nThe section develops our technique for speeding up value iteration in POMDPs with nonnegative rewards. We begin with the basic idea.\n4.I Point-based improvement\nConsider a suboptimal set of vectors V. By improving V, we mean to find another suboptimal set of vectors that strictly dominates V. By improving V at a belief state b, we mean to find another suboptimal set of vectors U such that U(b)> V(b).\nValue iteration starts with the singleton set {0}, which is of course suboptimal, and improves the set itera tively using dynamic-programming update (Theorem 1). Dynamic-programming is quite expensive, espe cially when performed on large sets of vectors. To speed up value iteration, we devise a very inexpensive technique called point-based improvement for improv ing a set of vectors and use it multiple times in be tween dynamic-programming updates. This technique can be incorporated into value iteration as shown in Figure 2. Applications of the technique are encapsu lated in the subroutine improve. The Bellman residual li is used in improve to determine how many times the technique is to be used.\nTheorem 2 If, for any suboptimal set of vectors U, the output of improve(U, li) - another set of vectors - is suboptimal and dominates U, then VIl termi nates in a finite number of steps.\nProof: Let Vn and V� be respectively the sets of vec tors produced at the nth iteration of VIl and VI. From Lemma 1 and the condition imposed on improve, we conclude that Vn is suboptimal and dominates V�.\n700 Zhang, Lee, and Zhang\nSince V� monotonically increases with n (Theorem 1) and converges to V* as n goes to infinity, Vn must also converge to V*. The theorem follows. D\n4.2 Improving a set of vectors at one belief state\nFor the rest of this section, we let V be a fixed sub optimal set of vectors and let U=TV. We develop a method for improving U.\nTo begin with, consider how U might be improved at a particular belief state b. According to (2), there exists an action a such that\nU(b) = r(b, a) + A L P(zlb, a)V(b�). (4) z\nFor each observation z, let f3z be a vector in U that has maximum inner product with b�. Define a new vector by\nf3(s) = r(s, a) + A L P(z, s'ls, a)f3z(s') Vs E S. (5) z,s'\nWe sometimes denote this vector by backup(b, a, U).\nTheorem 3 For the vector f3 given by (5), we have\nb.f3 = r(b, a) + A L P(zlb, a)U(b�). (6) z\nAs pointed out at the end of Section 3.2, U(b�) is of ten larger than V(b�). A quick comparison of (4) and (6) leads us to conclude that b.f3 is often larger than U(b). When this is the case, we have found a set that improves U at b, namely the singleton set {(3}. The set is obviously suboptimal.\nThere is an obvious variation to the idea presented above. Instead of using the vector backup(b, a, U) for the action that satisfies ( 4), we can consider the vectors backup(b, a', U) for all possible actions a' and choose the one whose inner product with b is the maximum. This should, hopefully, improve U at b even further. We tried this variation and found that the costs are almost always greater than the benefits.\n4.3 Improving a set of vectors at multiple belief states\nIt is straightforward to generalize the idea of the previ ous subsection from one belief state to multiple belief states. The question is what belief states to use. There are many possible answers. Our answer is motivated by the properties of dynamic-programming update.\nFor any vector ainU, define its witness region R(a,U) and closed witness region R(a,U) w.r.t U to be regions\nof the belief space B respectively given by\nR(a,U) = {bEBia.b>a'.b \\la'EU\\{a}},\nR(a,U) = {bEBia.b�a'.b Va'EU\\{a}}.\nDuring dynamic-programming update, each vector a in U is associated with a belief state that is in the closed witness region of a. We say that the belief state is the anchoring point provided for a by dynamic programming update and denote it by point( a). The vector is also associated with an action, which we de note by action(a). It is the action prescribed for the belief state point( a) by a V(.)-improving policy. Be cause of those, equation (4) is true if b is point( a) and a is action(a).\nWe choose to improve U on the anchoring points using\nU1 = {backup(point(a), action(a),U)Ia E U}. (7)\nAccording to the discussions of the previous subsec tion, the value function U1 (.) is often larger than U(.) at the anchoring points. When a value function is larger than another one at one belief state, it is also larger in the neighborhood of the belief state. There fore, the value function U1 (.) is often larger than U(.) in regions around the anchoring points. Our experi ence reveal that it is often larger in most parts of the belief space. The explanation is that the anchoring points scatter \"evenly\" over the belief space w .r. t U in the sense that there is one in the closed witness region of each vector of U.\nThere is one optimization issue. Even that the inner product of the vector backup(point(a), action(a),U) with the belief state point( a) is often larger than that of a with the belief state, it might be smaller some times. When this is the case, we use a instead of backup(point(a), action(a),U) so that the value at the belief state is as large as possible.\n4.4 Relation to modified policy iteration for MDPs\nPoint-based improvement is closely related to MPI for MDPs (Puterman 1990, page 371). It can be shown that, for each anchoring point b,\nul (b) = r(b, 7r(b)) + A L P(zlb, 7r(b))U(b�(b)), (8) z\nwhere 1r is a V(. )-improving policy. This formula is very similar to formula (6.37) of Puterman (1990). MD P modified policy iteration uses the latter formula to \"improve\" the value of each possible state of the state space. We cannot apply the above formula to all possible belief states since there are infinite many of\nimprove(U): 1. Uo +-- U, k t-- 0. 2. do { 3. k +-- k + 1, uk +-- 0, w +-- 0. 4. for each vector a in Uk-1 5. a' +--backup(point(a), action(a),Uk-1 U W). 6. if a. point( a)> a' .point(a) 7. a' +--a. 8. else W +-- WU {a'}. 9. point( a') t-- point( a), 10. action(a') +--action(a); 11. uk +--uk u {a'}. 12.} while stop(Uk,Uk-d =false. 13. return uk u u.\nFigure 3: The improve subroutine.\nthem. So, we choose to use the formula to improve the values of a finite number of belief states, namely the anchoring points.\n4.5 Repeated improvements\nWe now know how we might improve U at the anchor ing points. In hope to get as much improvement as possible, we want, of course, to apply the technique on U1 and try to improve it further. This can easily be done. Observe that there is a one-to-one correspon dence between vectors in U and U1: for each vector a in U, we have backup(point(a),action(a),U) in U1. We associate the latter with the same belief state and action as the former. Then we can improve U1 at the anchoring points the same way as we improve U. The process can of course be repeated for the resulting set of vectors and so on.\nThe above discussions lead to the routine shown in Fig ure 3. The routine improves the input vector set at the anchoring points iteratively. Improvement takes place at line 5. Lines 6 and 7 guarantee that the values of the anchoring point never decrease. The improved vector a' is added to W at line 8 so that better improv.e ments can be achieved for vectors yet to be processed. At lines 9 and 10, the belief state and action associated with a vector of the previous iteration are assigned to the corresponding vector at the current iteration.\nThe stopping criterion we use is\nwhere E1 is a positive number smaller than 1. In our experiments, E1 is set at 0 .. Compared with the stop ping criterion of value iteration, the stopping criterion is stricter. The reason for this is that the improvement step is computationally cheap.\nSpeeding Up PO MOP Value Iteration 701\nFinally, the union UkUU is returned instead of Uk for the following reason. While Uk(b) is no smaller than U(b) at the anchoring points, it might be smaller at some other belief states. In other words, Uk might not dominate v. If improve simply returns uk, the conditions of Theorem 2 are not met. Consequently, the union ukuu is returned.\n4.6 Pruning extraneous vectors\nThe union UkUU usually contains many extraneous vectors. They should be pruned to avoid unnecessary computations in the future. One way to doing so is to simply apply Lark's algorithm (White 1991).\nLark's algorithm solves a linear program for each input vector. It is expensive when there is a large number of vectors. We use a more efficient method. The moti vation lies in two observations: First, most vectors in Uk are not extraneous. Second, many vectors in U are componentwise dominated by vectors in uk and hence are extraneous. The method is to replace line 13 with the following lines:\n13. Prune from U vectors that are componentwise dominated by vectors in uk. 14. Prunes from U vectors a such that R(a,UkUU) is empty. 15. return ukuu.\nAt line 14, a linear program is solved for each vector in U. Since no linear programs are solved for vectors in Uk and the set U usually becomes very small in cardinality after line 13, the method is much more efficient than simply applying Lark's algorithm to the union UkUU.\n4. 7 Recursive calls to improve\nConsider the set U after line 14 of the algorithm seg ment given in the previous subsection. If it is not empty, then every vector a in the set is useful. This is determined by solving a linear program. In addition to determining the usefulness of a, solving the linear program also produces a belief state b that is in the closed witness region R(a,UkUU).\nThe facts that a is from the input set U and that b is in R(a,UkUU) imply that the value at b has not been improved. To achieve as much improvement as possible, we improve the value by a recursive call to improve. To be more specific, we reset point( a) to b at line 14 and replace line 15 with the following:\n15. if U 'f 0, return improve(UkUU, 8). 16. else return uk.\n702 Zhang, Lee, and Zhang\n5 EMPIRICAL RESULTS\nExperiments have been conducted to empirically de termine the effectiveness of point-based improvement in speeding up value iteration and to compare it im provement with Hansen's policy iteration algorithm. Four problems were used in the experiments for both purposes. The problems are commonly referred to as Tiger, Network, Shuttle, and Aircraft ID in the litera ture and were downloaded from Cassandra's POMDP page 2• Information about their sizes is summarized in the following table.\nIISIIIZIIIAII Tiger 2 2 3\nNetwork 7 2 4 Shuttle 8 2 3 Aircraft ID 12 5 6\n5.1 Effectiveness of point-based improvement\nThe effectiveness of point-based improvement is deter mined by comparing VI and VI1. We borrowed an implementation of VI by Cassandra and VIl was im plemented on top of his program. Cassandra's pro gram provides a number of algorithms for dynamic programming update. For our experiments, we used a variation of incremental pruning called restricted re gion (Cassandra et a/1997). The discount factor was set at 0.95 and experiments were conducted on an UJ traSparc II.\nFor the purpose of comparison, we collected informa tion about the quality of the policies that VI and VIl produce as a function of the times they take. The quality of a policy is measured by the distance be tween its value function to the optimal value function, i.e. the minimum € such that the policy is €-optimal. The smaller the distance, the better the policy. Since we do not know the optimal value function, the dis tance cannot be exactly computed. We use an upper bound derived from the Bellman residual.\nOne experiment was conducted for each algorithm problem combination. The experiment was terminated when either an 0.01-optimal policy was produced or the run time exceeded 24 hours, i.e. 86400 seconds, CPU time.\nThe data are summarized in the four charts in Figure 4. Note that both axes are in logarithmic scale. There is one chart for each problem. In each chart, there are two curves: one for VI and one for VI 1. On each curve, there is data point for each iteration taken.\n2\nhttp://vvw.cs.brovn.edu/research/ai/pomdp/index.html\nWe see that VIl was able to produce a 0.01-optimal policy for all four problems in a few iterations. On the other hand, VI took 215 iterations to produce a 0.01- optimal policy for Network. Within the time limit, VI completed only 35, 16, and 10 iterations respectively for Tiger, Shuttle, and Aircraft ID. Those suggest that the technique proposed in this paper is very effective in reducing the number of iterations that is required to produce good policies.\nIt is also clear that VIl is much faster than VI. For Network, VI took about 17,000 seconds to produce a 0.01-optimal policy, while VIl took only about 350 seconds. The speedup is about 50 times. VI was not able to produce \"good\" policies for Tiger, Shuttle, and Aircraft ID within the time limit, while VIl produced 0.01-optimal or better policies for them in 0.53, 130, and 38,424 seconds respectively.\n5.2 Comparisons with Hansen's policy iteration algorithm\nIn his implementation, Hansen used standard in cremental pruning, instead of restricted region, for dynamic-programming update. Moreover, while the round-off threshold is set at w-9 in Cassandra's pro gram, Hansen set it at w-6 probably because the rou tines for solving linear equations cannot handle pre cision beyond w-6• For fairness of comparison, we implemented VI1 on top Hansen's code.\nThe following table shows the numbers of iterations and amounts of time VI1 and Hansen's algorithm took to find 0.01-optimal policies. We see that VI1 took fewer iterations than Hansen's algorithms on all prob lems. It took less on the first two problems and took roughly the same time on the last two problems.\nTime VI1 I Hansen VI1 I Hansen\nTiger 4 14 0.51 3.3 Network 10 18 395 1122 Shuttle 6 9 65 73 Aircraft ID 8 9 72,377 66,964\n6 CONCLUS IONS\nWe have developed a technique, namely point-based improvement, for speeding up the convergence of value iteration for POMDPs. The underlying idea is simi lar to that behind modified policy iteration for MDPs. The technique can easily be incorporated into any ex isting POMDP value iteration algorithms.\nExperiments have been conducted on several test prob lems. We found that the technique is very effective in reducing the number of iterations that is required to obtain policies with desired quality. Because of this, it greatly speeds up value iteration. In our experiments, orders of magnitude speedups were observed.\nAcknowledgement\nResearch is supported by Hong Kong Research Grants Council Grant HKUST6125 /98E. The authors thank Cassandra and Hansen's for sharing with us their pro grams and the anonymous reviewers for useful com ments.\nReferences\nCassandra, A. R. (1998a), Exact and approximate al gorithms for partially observable Markov decision processes, PhD thesis, Department of Computer Science, Brown University, Providence, Rhode Is land, USA.\nSpeeding Up POMDP Value Iteration 703\nCassandra, A. R. (1998b), A survey of POMDP ap plications, in Working Notes of AAAI Jggs Fall Symposium on Planning with Partially Obseroable Markov Decision Processes, 17-24.\nCassandra, A. R., Littman, M. 1., & Zhang, N. L. (1997). Incremental pruning: A simple, fast, ex act method for partially observable Markov deci sion processes. In Proceedings of Thirteenth Con ference on Uncertainty in Artificial Intelligence, 54-61.\nHansen, E. A. (1998), Finite-memory control of par tially obseroable systems, Ph.D. Diss., Depart ment of Computer Science, University of Mas sachusetts at Amherst.\nHansen, E. A. (1999), Personal communication.\nG. E. Monahan (1982), A survey of partially observ able Markov decision processes: theory, models, and algorithms, Management Science, 28 (1), 1- 16.\nPuterman, M. L. (1990), Markov decision processes, in D. P. Heyman and M. J. Sobel (eds.), Hand books in OR & MS., Vol. 2, 331-434, Elsevier Science Publishers.\nPuterman, M. L. and M. C. Shin (1982), Action elim ination procedures for modified policy iteration algorithms, Operations Research, 30, 301-318.\nSondik, E. J. (1971). The optimal control of partially observable Markov processes. PhD thesis, Stan ford University, Stanford, California, USA.\nSondik, E. J. (1978), The optimal control of partially observable Markov processes over a finite horizon, Operations Resear-ch, 21, 1071-1088.\nvan Nunen, J. A. E. E. (1976), A set of successive ap proximation methods for discounted Markov de cision problems, z. Operations Research, 20, 203- 208.\nWhite III, C. C. (1991). Partially observed Markov decision processes: A survey. Annals of Opera tions Research, 32.\nZhang, N. L. and W. Liu (1997), A model approxi mation scheme for planning in stochastic domains, Journal of Artificial Intelligence Research, 7, 199- 230."
    } ],
    "references" : [ {
      "title" : "Exact and approximate al­ gorithms for partially observable Markov decision processes",
      "author" : [ "A.R. Cassandra" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Cassandra,? \\Q1998\\E",
      "shortCiteRegEx" : "Cassandra",
      "year" : 1998
    }, {
      "title" : "A survey of POMDP ap­ plications, in Working Notes of AAAI",
      "author" : [ "A.R. Cassandra" ],
      "venue" : "Jggs Fall Symposium on Planning with Partially Obseroable Markov Decision Processes,",
      "citeRegEx" : "Cassandra,? \\Q1998\\E",
      "shortCiteRegEx" : "Cassandra",
      "year" : 1998
    }, {
      "title" : "Finite-memory control of par­ tially obseroable systems, Ph.D",
      "author" : [ "E.A. Hansen" ],
      "venue" : null,
      "citeRegEx" : "Hansen,? \\Q1998\\E",
      "shortCiteRegEx" : "Hansen",
      "year" : 1998
    }, {
      "title" : "A survey of partially observ­",
      "author" : [ "G.E. Monahan" ],
      "venue" : null,
      "citeRegEx" : "Monahan,? \\Q1982\\E",
      "shortCiteRegEx" : "Monahan",
      "year" : 1982
    }, {
      "title" : "Action elim­",
      "author" : [ "M.L. Puterman", "M.C. Shin" ],
      "venue" : null,
      "citeRegEx" : "Puterman and Shin,? \\Q1982\\E",
      "shortCiteRegEx" : "Puterman and Shin",
      "year" : 1982
    }, {
      "title" : "The optimal control of partially",
      "author" : [ "E.J. Sondik" ],
      "venue" : null,
      "citeRegEx" : "Sondik,? \\Q1971\\E",
      "shortCiteRegEx" : "Sondik",
      "year" : 1971
    }, {
      "title" : "The optimal control of partially",
      "author" : [ "E.J. Sondik" ],
      "venue" : null,
      "citeRegEx" : "Sondik,? \\Q1978\\E",
      "shortCiteRegEx" : "Sondik",
      "year" : 1978
    }, {
      "title" : "A set of successive ap­",
      "author" : [ "van Nunen", "J.A.E. E" ],
      "venue" : null,
      "citeRegEx" : "Nunen and E.,? \\Q1976\\E",
      "shortCiteRegEx" : "Nunen and E.",
      "year" : 1976
    }, {
      "title" : "Partially observed Markov decision processes: A survey",
      "author" : [ "III C.C. White" ],
      "venue" : "Annals of Opera­ tions Research,",
      "citeRegEx" : "White,? \\Q1991\\E",
      "shortCiteRegEx" : "White",
      "year" : 1991
    }, {
      "title" : "A model approxi­",
      "author" : [ "N.L. Zhang", "W. Liu" ],
      "venue" : null,
      "citeRegEx" : "Zhang and Liu,? \\Q1997\\E",
      "shortCiteRegEx" : "Zhang and Liu",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "See Cassandra (1998a) for excellent descriptions, analyses, and empirical compar­ isons of those algorithms.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "The first one is proposed by Sondik (1978). A simpler one is recently developed by Hansen (1998).",
      "startOffset" : 29,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "A simpler one is recently developed by Hansen (1998).",
      "startOffset" : 39,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "Numerical results reported in Puterman and Shin (1978) suggest that modified pol­ icy iteration is more efficient than either value iteration or policy iteration in practice.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "One way to doing so is to simply apply Lark's algorithm (White 1991).",
      "startOffset" : 56,
      "endOffset" : 68
    } ],
    "year" : 2012,
    "abstractText" : "We present a technique for speeding up the convergence of value iteration for par­ tially observable Markov decisions processes (POMDPs). The underlying idea is similar to that behind modified policy iteration for fully observable Markov decision processes (MDPs). The technique can be easily incor­ porated into any existing POMDP value it­ eration algorithms. Experiments have been conducted on several test problems with one POMDP value iteration algorithm called in­ cremental pruning. We find that the tech­ nique can make incremental pruning run sev­ eral orders of magnitude faster.",
    "creator" : "Adobe Acrobat 7.0"
  }
}