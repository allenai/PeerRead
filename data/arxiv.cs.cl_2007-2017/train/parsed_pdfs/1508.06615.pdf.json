{
  "name" : "1508.06615.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Character-Aware Neural Language Models",
    "authors" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush" ],
    "emails" : [ "yoonkim@seas.harvard.edu", "srush@seas.harvard.edu", "jernite@cs.nyu.edu", "dsontag@cs.nyu.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Language modeling is a fundamental task in artificial intelligence and natural language processing (NLP), with applications in speech recognition, summarization, and machine translation. A language model is formalized as a probability distribution over a sequence of strings (words), and traditional methods usually involve making an n-th order Markov assumption and estimating n-gram probabilities via counting and subsequent smoothing (Chen and Goodman 1998). While simplicity of training such models has made it possible to train them on corpora with over a trillion word tokens (Buck et al. 2014), they treat each word as a completely independent unit of information, and hence probabilities of rare n-grams can be poorly estimated despite smoothing techniques.\nNeural Language Models (NLM) address the data sparsity issue through parameterization of words as vectors (word embeddings) and using them as inputs to a neural network (Bengio et al. 2003; Mikolov et al. 2010). The parameters are learned as part of the training process. Word embeddings obtained through NLMs exhibit the property whereby semantically close words are likewise close in the induced vector space (as is the case with non-neural techniques such as Latent Semantic Analysis (Deerwester et al. 1990)).\nWhile NLMs have been shown to outperform count-based n-gram language models (Mikolov et al. 2011), they are blind to subword information (e.g. morphemes). For exam-\nple, they do not know, a priori, that eventful, eventfully, uneventful, and uneventfully should have structurally related embeddings in the vector space.1 Hence embeddings of rare words can often be poorly estimated, leading to high perplexities for rare words (and words surrounding them)— NLMs are less vulnerable, but not immune, to Zipf’s law. This is especially problematic in morphologically rich languages with long-tailed frequency distributions or domains with dynamic vocabularies (e.g. social media).\nIn this work, we propose a language model that leverages subword information through a character-level convolutional neural network (CNN), whose output is used as an input to a recurrent neural network language model (RNNLM). Unlike previous works that utilize subword information via morphemes (Botha and Blunsom 2014; Luong et al. 2013), our model does not require morphological tagging as a pre-processing step. And, unlike the recent line of work which combines input word embeddings with features from a character-level model (dos Santos and Zadrozny 2014; dos Santos and Guimaraes 2015), our model does not utilize word embeddings at all in the input layer. Given that most of the parameters in NLMs are from the word embeddings, the proposed model therefore has significantly fewer parameters than a conventional NLM, making it attractive for applications where model size may be an issue (e.g. cell phones). To summarize, our contributions are as follows:\n• on (morphologically simple) English, we achieve results on par with the existing state-of-the-art on the Penn Treebank (PTB), despite having approximately 60% fewer parameters, and\n• on morphologically rich languages (Czech, German, French, Spanish, and Russian), our model outperforms various baselines (Kneser-Ney, word-level/morphemelevel LSTM) by a significant margin, again with fewer parameters.\nWe have released all the code for the models described in this paper.2\n1By structurally related we mean that they are close in the vector space and/or that they exhibit certain regularities by, for example, linear offsets (eventful - eventfully≈ uneventful - uneventfully) (Mikolov et al. 2013).\n2https://github.com/yoonkim/lstm-char-cnn\nar X\niv :1\n50 8.\n06 61\n5v 1\n[ cs\n.C L\n] 2\n6 A\nug 2\n01 5"
    }, {
      "heading" : "Model",
      "text" : "The architecture of our model, shown in Figure 1, is straightforward. Whereas a conventional NLM takes word embeddings as inputs, our model instead takes the output from a single-layer character-level CNN with max-over-time pooling.\nFor notation, we denote vectors with bold lower-case (e.g. xt,b), matrices with bold upper-case (e.g. W,Uo), scalars with italic lower-case (e.g. x, b), and sets with cursive uppercase (e.g. V, C) letters. For notational convenience we assume that words and characters have already been converted into indices."
    }, {
      "heading" : "Recurrent Neural Network",
      "text" : "A recurrent neural network (RNN) is a type of neural network architecture particularly suited for modeling sequential phenomena. At each time step t, an RNN takes the input vector xt ∈ Rn and the hidden state vector ht−1 ∈ Rm and produces the next hidden state ht by applying the following recursive operation:\nht = f(Wxt +Uht−1 + b) (1)\nHere f is an element-wise nonlinearity and W ∈ Rm×n,U ∈ Rm×m,b ∈ Rm are the parameters of an affine transformation. In theory an RNN can summarize all historical information up to time t with the hidden state ht. In practice however, learning long-range dependencies with a vanilla RNN is difficult due to vanishing/exploding gradients (Bengio et al. 1994), which occur as a result of the Jacobian’s multiplicativity with respect to time.\nLong short-term memory (LSTM) (Hochreiter and Schmidhuber 1997) addresses the problem of learning long range dependencies by augmenting the RNN with a memory cell vector ct ∈ Rn at each time step. Concretely, one step of an LSTM takes as input xt,ht−1, ct−1 and produces ht, ct via the following intermediate calculations:\nit = σ(W ixt +U iht−1 + b i)\nft = σ(W fxt +U fht−1 + b f )\not = σ(W oxt +U oht−1 + b o) gt = tanh(Wgxt +Ught−1 + bg) ct = ft ct−1 + it gt ht = ot tanh(ct)\n(2)\nHere σ(·) and tanh(·) are the element-wise sigmoid and hyperbolic tangent functions, is the element-wise multiplication operator, and it, ft, ot are referred to as input, forget, and output gates. At t = 1, h0 and c0 are initialized to zero vectors. Parameters of the LSTM are Wj ,Uj ,bj for j ∈ {i, f, o, g}.\nMemory cells in the LSTM are additive with respect to time, alleviating the gradient vanishing problem. Gradient exploding is still an issue, though in practice simple optimization strategies (such as gradient clipping) work well. LSTMs have been shown to outperform vanilla RNNs on many tasks.\nIt is easy to extend RNN/LSTM to two (or more) layers by having another network whose input at t is ht (from the\nfirst network). Indeed, having multiple layers is often crucial for obtaining competitive performance on various tasks (Pascanu et al. 2013)."
    }, {
      "heading" : "Recurrent Neural Network Language Model",
      "text" : "Let V be the fixed size vocabulary of words. A language model specifies a distribution over wt+1 (whose support is V) given the historical sequence w1:t = [w1, . . . , wt]. A recurrent neural network language model (RNN-LM) does this by applying an affine transformation to the hidden layer followed by a softmax:\nPr(wt+1 = k|w1:t) = exp(ht · pk + qk)∑\nk′∈V exp(ht · pk ′ + qk′)\n(3)\nwhere pk is the k-th column of P ∈ Rm×|V| (also referred to as the output embedding),3 and qk is the k-th element of q ∈ R|V|, which reflects the overall frequency of k. Similarly, for a conventional RNN-LM which usually takes words as inputs, if wt = j, then the input to the RNN-LM at t is the input embedding xj , the j-th column of the embedding matrix X ∈ Rn×|V|. Thus P,q,X are parameters of the model to be learned during training. Our model simply replaces the input embeddings X with the output from a character-level CNN, to be described below.\nIf we denote w1:T = [w1, · · · , wT ] to be the sequence of words in the whole corpus, training involves minimizing the negative log-likelihood (NLL) of the entire sequence\nNLL = − 1 T T∑ t=1 log Pr(wt|w1:t−1) (4)\nwhich is typically done by truncated backpropagation through time (BPTT) (Werbos 1990; Graves 2013)."
    }, {
      "heading" : "Character-level Convolutional Neural Network",
      "text" : "In our model, the input at time t is an output from a character-level convolutional neural network (CharCNN), which we describe in this section. CNNs (LeCun et al. 1989) have achieved state-of-the-art results on computer vision (Krizhevsky et al. 2012) and have also been shown to be effective for various NLP tasks (Collobert et al. 2011). Architectures employed for NLP applications differ in that they involve temporal (one-dimensional), rather than spatial, convolutions.\nLet C be the vocabulary of characters (e.g. |C| ≈ 50 for English), d be the dimensionality of character embeddings,4 and Q ∈ Rd×|C| be the (dense) matrix of character embeddings, which are to be learned. Suppose that word k ∈ V is made up of a sequence of characters [c1, . . . , cl], where l is the length of word k. Then the character-level representation of k is given by the matrix Ck ∈ Rd×l, where the j-th column corresponds to the character embedding for cj (i.e. the cj-th column of Q).5\nWe apply a narrow convolution between Ck and a filter (or kernel) Hi ∈ Rd×w of width w (we will use w ∈ [1, . . . , 7]) to obtain a feature map fki ∈ Rl−w+1, after adding a bias and applying a nonlinearity. Concretely, the j-th element of fki is given by:\nfki [j] = tanh(C k[∗, j : j + w − 1] Hi + bi) (5)\n3In our work, predictions are at the word-level, and hence we still utilize word embeddings in the output layer.\n4Given that |C| is usually small, some authors work with onehot representations of characters. However we found that using lower dimensional representations of characters (i.e. d < |C|) performed slightly better.\n5Two technical details warrant mention here: (1) we append ‘start-of-word’ and ‘end-of-word’ characters to each word to better represent prefixes and suffixes and hence Ck actually has l + 2 columns; (2) for more efficient batch-processing, we zero-pad Ck so that the number of columns is constant (equal to the max word length) for all words in V .\nwhere Ck[∗, j : j+w−1] is the j-to-(j+w−1)-th column of Ck. Finally, we take the max-over-time\nyki = max j fki [j] (6)\nas the feature corresponding to the filter Hi (when applied to word k).\nWe have described the process by which one feature is obtained from one filter matrix. Our CharCNN uses multiple filters of varying widths to obtain the feature vector for k. So if we have a total of h filters H1, . . . ,Hh, then yk = [yk1 , . . . , y k h] is the input representation of k. For many NLP applications h is typically chosen to be in [100, 1000]."
    }, {
      "heading" : "Highway Network",
      "text" : "We could simply replace xk (the word embedding) with yk at each t in the RNN-LM, and as we will show later, this simple model performs well on its own (Table 6). Alternatively one could have a multilayer perceptron (MLP) over yk to model interactions between features, but we found that this resulted in much worse performance.\nInstead we obtained improvements by running yk through a highway network (HW-Net), recently proposed by Srivastava et al. (2015). Whereas one layer of an ordinary MLP applies an affine transformation followed by a nonlinearity to obtain a new set of features,\nz = g(Wy + b) (7)\none layer of a HW-Net does the following:\nz = t g(WHy + bH) + (1− t) y (8)\nwhere g is a nonlinearity, t = σ(WTy + bT ) is called the transform gate, and (1− t) is called the carry gate. Similar to the adaptive memory cells in LSTM networks, HW-Net allows for training of deep networks by adaptively carrying some dimensions of the input directly to the output.6 Note that by construction, the dimensions of y and z have to match, and hence WT and WH are square matrices.\nApplying HW-Net to the CharCNN has the following interpretation: since each output yki is essentially detecting a character n-gram (where n equals the width of the filter), HW-Net allows some character n-grams to be combined to build new features (dimensions where transform≈ 1), while allowing other character n-grams to remain ‘as-is’ (dimensions where carry ≈ 1)."
    }, {
      "heading" : "Experimental Setup",
      "text" : "As is standard in language modeling, we use perplexity (PPL) to evaluate the performance of our models. Perplexity of a model over a sequence w1:T = [w1, . . . , wT ] is\nPPL = T∏ t=1 Pr(wt|w1:t−1)− 1 T (9)\n6Srivastava et al. (2015) recommend initializing bT to a negative value, in order to militate the initial behavior towards carry. We indeed found that this was crucial to obtaining good performance and initialized bT to −2.\nwhich is simply equal to exp[NLL]. We test the model on various corpora (statistics available in Table 1).\nWe conduct hyperparameter search, model introspection, and ablation studies on the English Penn Treebank (PTB) (Marcus et al. 1993), utilizing the standard training (0-20), validation (21-22), and test (23-24) splits along with preprocessing by Mikolov et al. (2010). With approximately 1M tokens and |V| = 10K, this version has been extensively used by the language modeling community and is publicly available.7 Note that this version already has outof-vocabulary (OOV) tokens replaced with <unk>, and thus disadvantages the character model as the character-level model can utilize surface forms of OOV tokens. Nonetheless we stick to the pre-processed version for exact comparison against prior work.\nWith the optimal hyperparameters tuned on PTB, we apply the model to various morphologically rich languages: Czech, German, French, Spanish, and Russian. Data for these languages comes from the 2013 ACL Workshop on Machine Translation.8 For training we use the Yandex corpus for Russian and Europarl-v7 corpus for others. We additionally use the news-commentary corpus for validation (2011) and test (2012) sets. While the raw data are publicly available, we obtained the pre-processed versions (which involved lower-casing, tokenizing, and filtering) from Botha and Blunsom (2014),9 whose morphological NLM serves as a baseline for our work. In these datasets only singleton words were pruned and hence we effectively use the full vocabulary. Note that we use the smaller datasets from Botha and Blunsom (2014), with 1M tokens per language—they also run their models on larger datasets."
    }, {
      "heading" : "Optimization",
      "text" : "Training is by truncated backpropagation through time (Werbos 1990; Graves 2013). We backpropagate for 35 time steps with a batch size of 20, using stochastic gradient descent where the learning rate is initially set to 1.0 and halved if the perplexity does not decrease by more than 1.0 on the validation set after an epoch. We train for 25 epochs and pick the best performing model on the validation set (which typically occurred on the last epoch). Parameters of the model\n7http://www.fit.vutbr.cz/∼imikolov/rnnlm/ 8http://www.statmt.org/wmt13/translation-task.html 9http://bothameister.github.io/\nare randomly initialized over a uniform distribution with support [−0.05, 0.05].\nFor regularization we employ dropout (Hinton et al. 2012) with probability 0.5 on the LSTM input-to-hidden layers (except on the initial input layer which is from the CharCNN) and the hidden-to-output softmax layer. We further constrain the norm of the gradients to be below 5.10 These choices were largely guided by previous work of Zaremba et al. (2014) on word-level language modeling with LSTMs."
    }, {
      "heading" : "Results",
      "text" : ""
    }, {
      "heading" : "English Penn Treebank",
      "text" : "We train two versions of our model to assess the tradeoff between performance and size. Hyperparameters of our small (LSTM-CharCNN-Small) and large (LSTMCharCNN-Large) models are summarized in Table 2. As another baseline, we also train two comparable LSTM models that use word embeddings only (LSTM-Word-Small, LSTM-Word-Large).11 We found that performance was most sensitive to the number of hidden units in the LSTM and the number of filters in the CharCNN, and largely insensitive to the other hyperparameters. Dropout proved to be crucial to obtaining good performance with a larger model.\nAs can be seen from Table 3, our large model is on par with the existing state-of-the-art (Zaremba et al. 2014), despite having approximately 60% fewer parameters. Our small model significantly outperforms other NLMs of similar size, even though it is penalized by the fact that the dataset already has OOV words replaced with <unk> (other models are purely word-level models). While lower perplexities have been reported with model ensembles (Mikolov and Zweig 2012; Zaremba et al. 2014), we do not include them here as they are not comparable to the current work.\n10That is, if the L2 norm exceeds 5 then we renormalize the gradients to have || · || = 5 before updating.\n11LSTM-Word-Small uses 200 hidden units and LSTM-WordLarge uses 650 hidden units. Word embedding sizes are also 200 and 650 respectively. These were chosen to keep the number of parameters similar to the corresponding character-level model."
    }, {
      "heading" : "Other Languages",
      "text" : "The model’s performance on the English PTB is informative to the extent that it facilitates honest comparison against prior work. However, English is relatively simple from a morphological standpoint, and thus our next set of results (and arguably the main contribution of this paper) is focused on languages with richer morphology (Table 4).\nWe initially compare our results against the morphological log-bilinear (M-LBL) model from Botha and Blunsom (2014),12 whose model also takes into account subword information through morpheme embeddings that are summed at the input and output layers. As comparison against the M-LBL models is confounded by our use of LSTMs— widely known to outperform their feed-forward/log-bilinear cousins—we also train an LSTM version of the morphological NLM, where the input representation of a word given to the LSTM is a summation of the word’s morpheme embeddings. Concretely, suppose thatM is the set of morphemes in a language, M ∈ Rn×|M| is a matrix of morpheme embeddings (to be learned), and mj is the j-th column of M. Given the input word k, we feed the following representation to the LSTM:\nxk + ∑\nj∈Mk mj (10)\nwhere xk is the word embedding (as before) andMk ⊂M is the set of morphemes for word k. The morphemes are obtained by (say) running an unsupervised morphological tagger as a pre-processing step.13 We emphasize that the\n12Their results in Table 4 were omitted in the original paper due to space limitations, so we obtained them directly from the authors.\n13We use Morfessor Cat-MAP (Creutz and Lagus 2007), as in Botha and Blunsom (2014).\nword embedding itself (i.e. xk) is added on top of the morpheme embeddings, as was done in Botha and Blunsom (2014). The morpheme embeddings are of size 200/650 for the small/large models respectively. We further train wordlevel LSTM models as another baseline.\nIt is clear from Table 4 that the character-level models outperform their word-level counterparts by a large margin despite, again, being smaller.14 Relative perplexity reductions (15–25%) are much more pronounced than is the case on English, showing the effectiveness of character inputs for modeling morphologically complex languages. The character models also outperform their morphological counterparts (both LBL and LSTM architectures). Although improvements over the morphological LSTMs are more measured, we still see marked perplexity reductions on German, Spanish, and French by going from morpheme-level to characterlevel inputs.\nFinally, while not listed in Table 3, we note that directionally same patterns were observed on the PTB."
    }, {
      "heading" : "Discussion",
      "text" : ""
    }, {
      "heading" : "Learned Word Representations",
      "text" : "We explore the word representations learned by the CharCNN/HW-Net on the PTB. Table 5 has the nearest neighbors of word representations learned from both the word-level and character-level models. For the character models we compare the representations obtained before and after highway layers.\nBefore the highway layers the representations seem to solely rely on surface forms—for example the nearest neighbors of you are your, young, four, youth, which are close to you in terms of edit distance. The highway layers however, seem to enable encoding of semantic features that are not discernable from orthography alone. After highway layers the nearest neighbor of you is we, which is orthographically distinct from you. Another example is while and though— these words are far apart edit distance-wise yet the composition model is able to place them near each other. The model\n14The difference in parameters is greater for non-PTB corpora as the size of the word-level model is more sensitive to |V|.\nalso makes some clear mistakes (e.g. his and hhs), highlighting the limits of our approach.\nThe learned representations of OOV words (computeraided, misinformed) are positioned near words with the same parts-of-speech, which could explain the model’s success on language modeling (for which a word’s syntactic role is important). The model is also able to correct for incorrect/nonstandard spelling (looooook), indicating potential applications for text normalization in noisy domains.\nLearned Character n-gram Representations Our initial expectation was that each filter would learn to activate on different morphemes and then build up semantic representations of words from the identified morphemes. However, upon reviewing the character n-grams picked up by each filter (i.e. those that maximized the value of the filter), we found that they did not (in general) correspond to valid morphemes.\nTo get a better intuition for what the character composition model is learning, we plot the learned representation of character n-grams (that occurred as part of a word in V) via principal components analysis (Figure 2). We use the CharCNN’s output as the representation for a character n-gram. As is apparent from Figure 2, the model learns to differentiate between prefixes (red), suffixes (blue), and others (grey). We also find that the representations are particularly sensitive to character n-grams containing hyphens (orange), presumably because this is a strong signal of a word’s part-ofspeech."
    }, {
      "heading" : "Highway Layers",
      "text" : "We quantitatively investigate the effect of highway network layers via ablation studies (Table 6). We train a model without any highway layers, and find that performance decreases significantly. As the difference in performance could be due to the decrease in model size, we also train a model that feeds yk (i.e. word representation from the CharCNN) through a one-layer multilayer perceptron (MLP) to use as\ninput into the LSTM. We find that the MLP model does poorly.\nWe hypothesize that highway networks are well-suited to work with CNNs, by adaptively combining local features detected by the individual filters. CNNs have already proven to be been successful for sentence/document composition (Kalchbrenner et al. 2014; Kim 2014; Zhang and LeCun 2015; Lei et al. 2015), and we posit that further gains could be achieved by employing highway layers on top of existing CNN architectures.\nWe also anecdotally note that (1) having one to two highway layers was important, but more highway layers generally resulted in similar performance (though this may be depend on the size of the datasets), (2) having more convolutional layers before max-pooling did not help, and (3) highway layers did not have any impact on a model that only uses word embeddings as inputs. The last point is unsurprising, as dimensions of word embeddings do not (a priori) encode features that benefit from nonlinear, hierarchical composition availed by highway layers (unlike character n-\ngrams detected by the CharCNN)."
    }, {
      "heading" : "Further Observations",
      "text" : "We report on some further experiments and observations:\n• The current model does not utilize subword information in the output layer as the hidden-to-output is a regular softmax. We tried to imbue subword information to the output by having another CharCNN (which is different from the input CharCNN) whose output is softmaxed with ht to obtain Pr(wt+1|w1:t). We found that: (1) training was very slow (despite caching strategies) as one needs to run the CharCNN over all V at each batch, and (2) the model performed poorly (PPL ≈ 1000 on PTB).\n• Combining word embeddings with the CharCNN’s output to form a combined representation of a word (to be used as an input to the LSTM) resulted in slightly worse performance. This was surprising, as improvements have been reported on part-of-speech tagging (dos Santos and Zadrozny 2014) and named entity recognition (dos Santos and Guimaraes 2015) by concatenating word embeddings with the output from a character-level CNN. While this could be due to insufficient experimentation on our part,15 it suggests that for some tasks, word embeddings are superfluous—character inputs are good enough.\n• A potential drawback of our work is training speed— our model requires additional convolution operations over characters, and is thus slower than a comparable wordlevel model which can perform a simple lookup at the input layer. We however found that optimized GPU implementation made the difference manageable—for example on PTB the large character-level model took 0.50 secs/batch whereas the word-level model took 0.25 secs/batch. For scoring, our model can have the same running time as a pure word-level model, as the CharCNN’s outputs can be pre-computed for all words in V . This would, however, be at the expense of increased model size, and thus a trade-off needs to be made between runtime speed and memory (e.g. one could restrict the precomputation to the most frequent words)."
    }, {
      "heading" : "Related Work",
      "text" : "Neural Language Models (NLM) encompass a rich family of neural network architectures for language modeling. Some\n15We experimented with (1) concatenation, (2) tensor products, (3) averaging, and (4) adaptive weighting schemes whereby the model learns to trade-off the weighting between word embeddings and CharCNN outputs.\nexample architectures include feed-forward (Bengio et al. 2003), recurrent (Mikolov et al. 2010), sum-product (Cheng et al. 2014), log-bilinear (Mnih and Hinton 2007), and convolutional (Wang et al. 2015) networks.\nIn order to address the rare word problem, Alexandrescu and Krichhoff (2006)—building on analogous work on count-based n-gram language models by Bilmes and Kirchhoff (2003)—represent a word as a set of shared factor embeddings. Their Factored Neural Language Model (FNLM) can incorporate morphemes, word shape information (e.g. capitalization) or any other annotation (e.g. part-of-speech tags) to represent words.\nA specific class of FNLMs leverages morphemic information by viewing a word as a function of its (learned) morpheme embeddings (Luong et al. 2013; Botha and Blunsom 2014; Qui et al. 2014). Luong et al. (2013) apply a recursive neural network over morpheme embeddings to obtain the embedding for a single word. Botha and Blunsom (2014) report large perplexity reductions on many languages by summing over a word’s morpheme embeddings to use as inputs into a feed-forward NLM. While such models have proved useful, they require morphological tagging as a preprocessing step.\nAnother direction of work has involved character-level NLMs, wherein both input and output are characters (Sutskever et al. 2011; Graves 2013). Character-level models obviate the need for morphological tagging or manual feature engineering, and have the attractive property of being able to produce novel words. However they are generally outperformed by word-level models (Mikolov et al. 2012).\nOutside of language modeling, improvements have been reported on part-of-speech tagging (dos Santos and Zadrozny 2014) and named entity recognition (dos Santos and Guimaraes 2015) by representing a word as a concatenation of its word embedding and an output from a characterlevel CNN, and using the combined representation as features in a Conditional Random Field (CRF). Zhang and LeCun (2015) do away with word embeddings completely and show that for text classification, a deep CNN over characters performs well. Ballesteros et al. (2015) use an RNN over characters only to train a transition-based parser, obtaining improvements on many morphologically rich languages.\nFinally, as we were preparing our manuscript for submission we became aware of parallel work by Ling et al. (2015), which applies a bi-directional LSTM over characters to use as inputs into a language model. They show improvements on various fusional (English, Portuguese, Catalan) and agglutinative (German, Turkish) languages. It remains open as to which character composition model (i.e. CNN or LSTM) performs better."
    }, {
      "heading" : "Conclusion",
      "text" : "We have introduced a neural language model that utilizes only character-level inputs. Predictions are still at the wordlevel. Despite being smaller, our model soundly outperforms similar models that take words/morphemes as inputs. The improvements are more pronounced on languages with complex morphology.\nAnalysis of word representations obtained from the character composition part of the model further indicates that the model is able to encode semantically meaningful features that are not immediately apparent from orthography alone. Our work questions the necessity of word embeddings as inputs for neural language modeling.\nInsofar as language modeling mostly relies on capturing a word’s syntactic role, it would be interesting to see if the architecture introduced in this paper is viable for more semantic tasks—for example, as an encoder/decoder in neural machine translation (Cho et al. 2014; Sutskever et al. 2014)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are especially grateful to Jan Botha for providing the non-English pre-processed datasets and the model results in Table 4."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline (by 30–35%) and a word-level LSTM baseline (by 15–25%), again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.",
    "creator" : "TeX"
  }
}