{
  "name" : "1205.2958.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning and Using GPUs for Fast Preprocessing with Simple Hash Functions",
    "authors" : [ "Ping Li", "Arnd Christian König" ],
    "emails" : [ "pingli@cornell.edu", "anshu@cs.cornell.edu", "chrisko@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 5.\n29 58\nv1 [\ncs .I\nR ]\n1 4\nM ay\n2 01\n(b-bit) Minwise hashing requires an expensive preprocessing step that computes k (e.g., 500) minimal values after applying the corresponding permutations for each data vector. Note that the required k is often substantially larger for classification tasks than for duplicate detections (which mainly concern highly similar pairs) . We developed a parallelization scheme using GPUs and observed that the preprocessing time can be reduced by a factor of 20 ∼ 80 and becomes substantially smaller than the data loading time. Reducing the preprocessing time is highly beneficial in practice, e.g., for duplicate Web page detection (where minwise hashing is a major step in the crawling pipeline) or for increasing the testing speed of online classifiers.\nOne major advantage of b-bit minwise hashing is that it can substantially reduce the amount of memory required for batch learning. However, as online algorithms become increasingly popular for large-scale learning in the context of search, it is not clear if bbit minwise yields significant improvements for them. This paper demonstrates that b-bit minwise hashing provides an effective data size/dimension reduction scheme and hence it can dramatically reduce the data loading time for each epoch of the online training process. This is significant because online learning often requires many (e.g., 10 to 100) epochs to reach a sufficient accuracy.\nAnother critical issue is that for very large data sets it becomes impossible to store a (fully) random permutation matrix, due to its space requirements. Our paper is the first study to demonstrate that b-bit minwise hashing implemented using simple hash functions, e.g., the 2-universal (2U) and 4-universal (4U) hash families, can produce very similar learning results as using fully random permutations. Experiments on datasets of up to 200GB are presented."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Minwise hashing [6–8] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc. The recent development of b-\nbit minwise hashing [26] provided a substantial improvement in the estimation accuracy and speed by proposing a new estimator that stores only the lowest b bits of each hashed value. More recently, [27] proposed the use of b-bit minwise hashing in the context of learning algorithms such as SVM or logistic regression on large binary data (which is typical in Web classification tasks). bbit minwise hashing can enable scalable learning where otherwise massive (and expensive) parallel architectures would have been required, at negligible reduction in learning quality. In [27], experiments showed this for the webspam dataset which has 16 million features with a total disk size of 24GB in standard LibSVM format.\nHowever, several crucial issues must be tackled before one can apply b-bit minwise hashing to industrial applications. To understand these issues, we begin with a review of the method.\n1.1 A Review of b-Bit Minwise Hashing Minwise hashing mainly focuses on binary (0/1) data, which can be viewed as sets. Consider sets S1, S2 ⊆ Ω = {0, 1, 2, ..., D−1}, minwise hashing applies a random permutation π : Ω → Ω on S1 and S2 and uses the following collision probability\nPr (min(π(S1)) = min(π(S2))) = |S1 ∩ S2|\n|S1 ∪ S2| = R (1)\nto estimate R, which is the resemblance between S1 and S2. With k permutations: π1, ..., πk, one can estimate R without bias:\nR̂M = 1\nk\nk ∑\nj=1\n1{z1 = z2} (2)\nz1 = min(πj(S1)), z2 = min(πj(S2)).\nA common practice is to store each hashed value, e.g., min(π(S1)), using 64 bits [16]. The storage (and computational) cost is prohibitive in industrial applications [28]. The recent work of b-bit minwise hashing [26] provides a simple solution by storing only the lowest b bits of each hashed value. For convenience, we define\nz (b) 1 = the lowest b bits of z1, z (b) 2 = the lowest b-bits of z2.\nTHEOREM 1. [26] Assume D is large.\nPb = Pr ( z (b) 1 = z (b) 2 ) = C1,b + (1− C2,b)R, (3)\nwhere C1,b and C2,b are functions of (D, |S1|, |S2|, |S1 ∩ S2|). ✷\nBased on Theorem 1, we can estimate Pb (and R) from k independent permutations π1, π2, ..., πk:\nR̂b = P̂b − C1,b 1− C2,b , P̂b = 1 k\nk ∑\nj=1\n1 {\nz (b) 1,πj = z (b) 2,πj\n}\n, (4)\nThe estimator P̂b is an inner product between two vectors in 2b×k dimensions with exactly k 1’s, because\n1 {\nz (b) 1 = z (b) 2\n} = 2b−1 ∑\nt=0\n1{z (b) 1 = t} × 1{z (b) 2 = t} (5)\nThis provides a practical strategy for using b-bit minwise hashing for large-scale learning. That is, each original data vector is transformed into a new data point consisting of k b-bit integers, which is expanded into a 2b × k-length binary vector at the run-time.\nThese days, many machine learning applications, especially in the context of search, are faced with large and inherently highdimensional datasets. For example, [35] discusses training datasets with (on average) n = 1011 items and D = 109 distinct features. [37] experimented with a dataset of potentially D = 16 trillion (1.6 × 1013) unique features. Effective algorithms for data/feature reductions will be highly beneficial for these industry applications."
    }, {
      "heading" : "1.2 Linear Learning Algorithms",
      "text" : "Clearly, b-bit minwise hashing can approximate both linear and nonlinear kernels (if they are functions of the inner products). We focus on linear learning because many high-dimensional datasets used in the context of search are naturally suitable for linear algorithms. Realistically, for industrial applications, “almost all the big impact algorithms operate in pseudo-linear or better time” [1].\nLinear algorithms such as linear SVM and logistic regression have become very powerful and extremely popular. Representative software packages include SVMperf [21], Pegasos [32], Bottou’s SGD SVM [4], and LIBLINEAR [15].\nGiven a dataset {(xi, yi)}ni=1, xi ∈ R D, yi ∈ {−1, 1}, the L2regularized linear SVM solves the following optimization problem:\nmin w\n1 2 w T w + C\nn ∑\ni=1\nmax { 1− yiw T xi, 0 } , (6)\nand the L2-regularized logistic regression solves a similar problem:\nmin w\n1 2 w T w +C\nn ∑\ni=1\nlog ( 1 + e−yiw T xi ) . (7)\nHere C > 0 is an important penalty parameter.\nNext, we elaborate on 3 major issues one must tackle in order to apply b-bit minwise hashing to large-scale industrial applications."
    }, {
      "heading" : "1.3 Issue 1: Expensive Preprocessing",
      "text" : "(b-bit) Minwise hashing requires a very expensive preprocessing step (substantially more costly than loading the data) in order to compute k (e.g., k = 500) minimal values (after permutation) for each data vector. Note that in prior studies for duplicate detection [6], k was usually not too large (i.e., 200), mainly because duplicate detection concerns highly similar pairs (e.g., R > 0.5). With b-bit minwise hashing, we have to use larger k values according to the analysis in [26] even in the context of duplicate detections. Note that classification tasks are quite different from duplicate detections. For example, in our most recent experiments on image classifications [24], even k = 2000 did not appear to be sufficient.\nConsider, for example, the task of computing b-bit minwise hash signatures for the task of Web page duplicate detection. While parallelizing this task is conceptually simple (as the signatures of different pages can be computed independently) it still comes at the cost of using additional hardware and electricity. Thus, any improvements in the speed of signature computation may be directly reflected in the cost of the required infrastructure.\nFor machine learning research and applications, this expensive preprocessing step can be a significant issue in scenarios where (either due to changing data distributions or features) models are frequently re-trained. In user-facing applications, the testing time performance can be severely affected by the preprocessing step if the (new) incoming data have not been previously processed.\nThis paper studies how to speed up the execution of the signature computation through the use of graphical processing units (GPUs). GPUs offer, compared to current CPUs, higher instruction parallelism and very low latency access to the internal GPU memory, but comparatively slow latencies when accessing the main memory [22]. As a result, many data processing algorithms (especially such with random memory access patterns) do not benefit significantly when implemented using a GPU. However, the characteristics of the minwise hashing algorithm make it very well suited for execution using a GPU. The algorithm accesses each set in its entirety, which allows for the use of main memory pre-fetching to reduce access latencies. Moreover, since we compute k different hash minima for each item in a set, the algorithm can make good use of the high degree of parallelism in GPUs. This is especially true for b-bit minwise hashing, which, compared to the original algorithm, typically increases the number of hash functions (and minima) to be computed by a factor of 3 or more. Also, note that any improvements to the speed of (b-bit) minwise hashing are directly applicable to large-scale instances of the other applications of minwise hashing mentioned previously."
    }, {
      "heading" : "1.4 Issue 2: Online Learning",
      "text" : "Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures. In this paper, we demonstrate that b-bit minwise hashing can also be highly beneficial for online learning because reducing the data size substantially decreases the loading time, which usually dominates the training cost, especially when many training epochs are needed. At the same time, the resulting reduction in the overall learning accuracy that we see in the experiments is negligible. Moreover, b-bit minwise hashing can also serve as an effective dimensionality reduction scheme here, which can be important in the context of web search, as (i) machine learning models based on text n-gram features generally have very large numbers of features, and thus require significant storage and (ii) there are typically a number of different models deployed in user-facing search servers, all of which compete for the available main memory space.\nThere is another benefit. Machine learning researchers have been actively developing good (and sometimes complex) online algorithms to minimize the need for loading the data many times (e.g., [38]). If the data loading time is no longer a major bottleneck, then perhaps very simple online algorithms may be sufficient in practice."
    }, {
      "heading" : "1.5 Issue 3: Massive Permutation Matrix",
      "text" : "When the data dimension (D) is not too large, e.g., millions, the implementation of b-bit minwise hashing for learning is straightforward. Basically, we can assume a “fully random permutation matrix” of size D×k, which defines k permutation mappings. This is actually how researchers use (e.g., Matlab) simulations to verify the theoretical results assuming perfectly random permutations.\nUnfortunately, when the dimension is on the order of billions (let alone 264), it becomes impractical (or too expensive) to store such a permutation matrix. Thus, we have to resort to simple hash functions such as various forms of 2-universal (2U) hashing (e.g., [13]).\nNow the question is how reliable those hash functions are in the context of learning with b-bit minwise hashing.\nThere were prior studies on the impact of limited randomness on the estimation accuracy of (64-bit) minwise hashing, e.g., [20, 30]. However, no prior studies reported how the learning accuracies were affected by the use of simple hash functions for b-bit minwise hashing. This study provides the empirical support that, as long as the data are reasonably sparse (as virtually always the case in the context of search), using 2U/4U hash functions results in negligible reduction of learning accuracies (unless b = 1 and k is very small).\nOne limitation of GPUs is that they have fairly limited memory [2]. Thus, it becomes even more beneficial if we can reliably replace a massive permutation matrix with simple hash functions."
    }, {
      "heading" : "2. SIMPLE HASH FUNCTIONS",
      "text" : "As previously discussed, in large-scale industry practice, it is often infeasible to assume perfect random permutations. For example, when D = 230 (about 1 billion) and k = 500, a matrix of D × k integers (4-byte each) would require > 2000GB of storage.\nTo overcome the difficulty in achieving perfect permutations, the common practice is to use the so-called universal hashing [9]. One standard 2-universal (2U) hash function is, for j = 1 to k,\nh (2U) j (t) = {a1,j + a2,j t mod p} mod D, (8)\nwhere p > D is a prime number and a1,j , a2,j are chosen uniformly from {0, 1, ..., p − 1}. To increase randomness, one can also use the following 4-universal (4U) hash function:\nh (4U) j (t) =\n{\n4 ∑\ni=1\nai,jt i−1 mod p\n}\nmod D, (9)\nwhere the ai,j (i = 1, 2, 3, 4) are chosen uniformly from {0, 1, ..., p− 1}. The storage cost for retaining the ai,j’s is minimal, compared to storing a permutation matrix. In theory, the 4U hash function is (in the worst-case) more random than the 2U hash function.\nNow, to compute the minwise hashes for a given feature vector (e.g., a parsed document represented as a list of 1-grams, 2-grams, and 3-grams, where each n-gram can be viewed as a binary feature), we iterate over all non-zero features; any non-zero location t in the original feature vector is mapped to its new location hj(t); we then iterate over all mapped locations to find their minimum, which will be the jth hashed value for that feature vector."
    }, {
      "heading" : "3. GPU FOR FAST PREPROCESSING",
      "text" : "In this section we will describe and analyze the use of graphics processors (GPUs) for fast computation of minwise hashes. We will first sketch the relevant properties of GPUs in general and then describe in how far minwise hashing computation is suited for execution on this architecture. Subsequently, we will describe our implementation and analyze the resulting performance improvements over a CPU-based implementation."
    }, {
      "heading" : "3.1 Introduction",
      "text" : "The use of GPUs as general-purpose coprocessors is relatively recent and primarily due to their high computational power at comparatively low cost. In comparison with commodity CPUs, GPUs offer significantly increased computation speed and memory bandwidth. However, since GPUs have been designed for graphics processing, the programming model (which includes massively parallel Single-Instruction-Multiple-Data (SIMD) processing and limited bus speeds for data transfers to/from main memory) is not suitable for arbitrary data processing applications [19]. GPUs consist\nof a number of SIMD multiprocessors. At each clock cycle, all processors in a multiprocessor execute identical instructions, but on different parts of the data. Thus, GPUs can leverage spatial locality in data access and group accesses to consecutive memory addresses into a single access; this is referred to as coalesced access."
    }, {
      "heading" : "3.2 Our Approach",
      "text" : "In light of the properties of GPU processing, our GPU algorithm to compute b-bit minwise hashes proceeds in 3 distinct phases: First, we read in chunks of 10K sets from disk into main memory and write these to the GPU memory. Then, we compute the hash values and the corresponding minima by applying all k hash functions to the data currently in the GPU and retaining, for each hash function and set, the corresponding minima. Finally, we write out the resulting minima back to main memory and repeat the process.\nThis batch-style computation has a number of advantages. Because we transfer larger blocks of data, the main memory latency is reduced through the use of main memory pre-fetching. Moreover, because the computation within the GPU itself scans through consecutive blocks of data in the GPU-internal memory (as opposed to random memory access patterns), performing the same computation (with a different hash function) for each set entry k times, we can take full advantage of coalesced access and the massive parallelism inherent in the GPU architecture.\nBecause GPUs are known to have fairly limited memory capacity, it becomes even more impractical to store a fully random permutation matrix; and hence it is crucial to utilize simple hash functions. We implemented both 2U and 4U hash functions introduced in Section 2. However, because the modulo operations in the definitions of the 2U/4U hash functions are expensive especially for GPUs [2], we have used the following tricks to avoid them and make our approach (more) suitable for GPU-based execution."
    }, {
      "heading" : "3.3 Avoid Modulo Operations in 2U Hashing",
      "text" : "To avoid the modulo operations in 2U hashing, we adopt a common trick [14]. Here, for simplicity, we assume D = 2s < 232 (note that D = 230 corresponds to about a billion features). It is known that the following hash function is essentially 2U [14]:\nh (s) j (t) = { a1,j + a2,j t mod 2 32} mod 2s, (10)\nwhere a1,j is chosen uniformly from {0, 1, ..., 232 − 1} and a2,j uniformly from {1, 3, ..., 232 − 1} (i.e., a2,j is odd). This scheme is much faster because we can effectively leverage the integer overflow mechanism and the efficient bit-shift operation. In this paper, we always implement 2U hash using h(s)j ."
    }, {
      "heading" : "3.4 Avoid Modulo Operations in 4U Hashing",
      "text" : "It is slightly tricky to avoid the modulo operations in evaluating 4U hash functions. Assuming D < p = 231− 1 (a prime number), we provide the C# code to compute v mod p with p = 231 − 1:\nprivate static ulong BitMod(ulong v) {\nulong p = 2147483647; // p = 2^31-1 v = (v >> 31) + (v & p); if (v >= 2 * p)\nv = (v >> 31) + (v & p); if (v >= p)\nreturn v - p; else\nreturn v; }\nTo better understand the code, consider\nv mod p = x, and v mod 231 = y\n=⇒v = p× Z + x = 231 × S + y\n=⇒x = 231(S − Z) + Z + y\nfor two integers S and Z. S and y can be efficiently evaluated using bit operations: S = v >> 31 and y = v & p.\nA recent paper [34] implemented a similar trick for p = 261−1, which was simpler than ours because with p = 261 − 1 there is no need to check the condition “if (v >= 2 * p)”. We find the case of p = 231 − 1 useful in machine learning practice because it suffices for datasets with less than a billion features. Note that a large value of p potentially increases the dimensionality of the hashed data."
    }, {
      "heading" : "3.5 Experiments: Datasets",
      "text" : "Table 1 summarizes the two datasets used in this evaluation: webspam and rcv1. The webspam dataset was used in the recent paper [27]. Since the webspam dataset (24 GB in LibSVM format) may be too small compared to datasets used in industrial practice, in this paper we also present an empirical study on the expanded rcv1 dataset [4], which we generated by using the original features + all pairwise combinations (products) of features + 1/30 of 3-way combinations (products) of features. Note that, for rcv1, we did not include the original test set in [4], which has only 20242 examples. To ensure reliable test results, we randomly split our expanded rcv1 dataset into two halves, for training and testing."
    }, {
      "heading" : "3.6 Experiments: Platform",
      "text" : "The GPU platform we use in our experiments is the NVIDIA Tesla C2050, which has 15 Simultaneous Multiprocessors (SMs), each with 2 groups of 16 scalar processors (hence 2 sets of 16- element wide SIMD units). The peak (single precision) GFlops of this GPU are 1030, with a peak memory bandwidth of 144 GB/s. In comparison, the numbers for a Intel Xeon processor X5670 (Westmere) processor are 278 GFlops and 60 GB/s."
    }, {
      "heading" : "3.7 Experiments: CPU Results",
      "text" : "We use the setting of k = 500 for these experiments. Table 2 shows the overhead of the CPU-based implementation, broken down into the time required to load the data into memory and the time for the minwise hashing computation. For 2U, we always use the 2U hash function (10). For 4U (Mod), we use the 4U hash function (9) which requires the modulo operation. For 4U (Bit), we use the implementation in Section 3.4, which converted the modulo operation into bit operations. Note that for rcv1 dataset, we only report the experimental results for 2U hashing.\nTable 2 shows that the preprocessing using CPUs (even for 2U) can be very expensive, substantially more than data-loading. 4U hashing with modulo operations can take an order of magnitude more time than 2U hashing. As expected, the cost for 4U hashing can be substantially reduced if modulo operations are avoided.\nNote that for webspam dataset (with only 16 million features), using permutations is actually slightly faster than the algebra required for 2U hash functions. The main constraint here is the storage space. The permutations are generated once and then stored\nin main memory. This makes them impractical for use with larger feature sets such as the rcv1 data (with about a billion features)"
    }, {
      "heading" : "3.8 Experiments: GPU results",
      "text" : "The total overhead for the GPU-based processing for batch size = 10K is summarized in Table 3, demonstrating the substantial time reduction compared to the CPU-based processing in Table 2. For example, the cost of 2U processing on the webspam dataset is reduced from 4100 seconds to 51 seconds, a 80-fold reduction. We also observe improvements of similar magnitude for 4U processing (both modulo and bit versions) on webspam. For the rcv1 dataset, the time reduction of the GPU-based implementation is about 20- fold, compared to the CPU-based implementation.\nFor both datasets, the costs for the GPU-based preprocessing become substantially smaller than the data loading time. Thus, while achieving further reduction of the preprocessing cost is still interesting, it becomes less practically significant because we have to load the data once in the learning process.\nFigures 1 to 3 provide the breakdowns of the overhead for the GPU-based implementations, using 2U hashing, 4U hashing with modulo operations, and 4U hashing without modulo operations, respectively. As shown in the figures, we separate the overhead into three components: (i) time spent transferring the data from main memory to the GPU (“CPU→ GPU”), (ii) the actual computation (“GPU Kernel”) and (iii) transferring the k minima back to main memory (“GPU→ CPU”).\nRecall we adopt a batch-based GPU implementation by reading chunks of sets from disk into main memory and write them to the GPU memory. If the chunk size is not chosen appropriately, it may affect the GPU performance. Thus, we vary this parameter and report the performance for chuck sizes ranging from 1 to 50000.\nWe can see from Figures 1 to 3 that the overall cost is not significantly affected by the batch size, as long as it is reasonably large (e.g., in this case > 100). This nice property may simplify the design because practitioners will not have to try out many batch sizes. Note that the time spent in transferring the data to the GPU is not affected significantly by the batch size, but the speed at which data is transferred back does vary significantly with this parameter. However, for any setting of the batch size does it hold that the time spent transferring data is about two orders of magnitude smaller than the time spent on the actual processing within the GPU. This is the key to the large speed-up over CPU implementations we see."
    }, {
      "heading" : "4. VALIDATION OF THE USE OF 2U/4U HASH FUNCTIONS FOR LEARNING",
      "text" : "For large-scale industrial applications, because storing a fully random permutation matrix is not practical, we have to resort to simple hash functions such as 2U or 4U hash families. However, before we can recommend them to practitioners, we must first validate on a smaller dataset that using such hash functions will not hurt the learning performance. To the best of our knowledge, this section is the first empirical study of the impact of hashing functions on machine learning with b-bit minwise hashing.\nIn addition, Appendix A provides another set of experiments for estimating resemblances using b-bit minwise hashing with simple hash functions. Those experiments demonstrate that, as long as the data are not too dense, using 2U hash will produce very similar estimates as using fully random permutations. That set of experiments may help understand the experimental results in this section. Note that both datasets listed in Table 1 are extremely sparse.\nThe webspam dataset is small enough (24GB and 16 million fea-\ntures) that we can conduct experiments using a permutation matrix. We chose LIBLINEAR as the underlying learning procedure. All experiments were conducted on workstations with Xeon(R) CPU (W5590@3.33GHz) and 48GB RAM, on a Windows 7 System."
    }, {
      "heading" : "4.1 Experimental Results",
      "text" : "We experimented with both 2U and 4U hash schemes for training linear SVM and logistic regression. We tried out 30 values for the regularization parameter C in the interval [10−3, 100]. We experimented with 11 k values from k = 10 to k = 500, and for 7 b values: b = 1, 2, 4, 6, 8, 10, 16. Each experiment was repeated 50 times. The total number of training experiments turns out to be\n2× 2× 30× 11× 7× 50 = 462000.\nTo maximize the repeatability, whenever page space allows, we always would like to present the detailed experimental results for all the parameters instead of, for example, only reporting the best results or the cross-validated results. In this subsection, we only present the results for webspam dataset using linear SVM because the results for logistic regression lead to the same conclusion.\nFigure 4 presents the SVM test accuracies (averaged over 50 runs). For the test cases that correspond to the most likely parameter settings used in practice (e.g., k ≥ 200 and b ≥ 4), we can see that the results from the three hashing schemes (full permutations, 2U, and 4U) are essentially identical. Overall, it appears that 4U is slightly better than 2U when b = 1 or k is very small.\nThis set of experiments can provide us with a strong experimental evidence that the simple and highly efficient 2U hashing scheme may be sufficient in practice, when used in the context of largescale machine learning using b-bit minwise hashing."
    }, {
      "heading" : "4.2 Experimental Results on VW Algorithm",
      "text" : "The application domains of b-bit minwise hashing are limited to binary data, whereas other hashing algorithms such as random projections and Vowpal Wabbit (VW) hashing algorithm [33, 37] are not restricted to binary data. It is shown in [27] that VW and random projections have essentially the same variances as reported in [25]. In this study, we also provide some experiments on VW hashing algorithm because we expect that practitioners will find applications which are more suitable for VW than b-bit minwise hashing. Since there won’t be space to explain the details of VW algorithm, interested readers please refer to [27, 33, 37].\nIn particular, in this subsection, we present a comparison between the accuracy of VW when using a fully random implementation and when using 2U hash functions. Figure 5 present the experimental results for both linear SVM and logistic regression. As we can see, the two graphs are nearly identical, meaning that the performance 2U hashing seen earlier is not limited to the context of learning with b-bit minwise hashing."
    }, {
      "heading" : "5. LEARNING ON RCV1 DATA (200GB)",
      "text" : "Compared to webspam, the size of the expanded rcv1 dataset may be more close to the training data sizes used in industrial applications. We report the experiments on linear SVM and logistic regression, as well as the comparisons with the VW hash algorithm."
    }, {
      "heading" : "5.1 Experiments on Linear SVM",
      "text" : "Figure 6 and Figure 7 respectively provide the test accuracies and train times, for training linear SVM. We can not report the baseline because the original dataset exceeds the memory capacity. Using merely k = 30 and b = 12, our method can achieve > 90% test accuracies. With k ≥ 300, we can achieve > 95% test accuracies."
    }, {
      "heading" : "5.2 Experiments on Logistic Regression",
      "text" : "Figure 8 and Figure 9 respectively present the test accuracies and training times for training logistic regression. Again, using merely k = 30 and b = 12, our method can achieve > 90% test accuracies. With k ≥ 300, we can achieve > 95% test accuracies.\nTo help understand the significance of these results, next we provide a comparison study with the VW hashing algorithm [33, 37]."
    }, {
      "heading" : "5.3 Comparisons with VW Algorithm",
      "text" : "The Vowpal Wabbit (VW) algorithm [33, 37] is an influential hashing method for data/dimension reduction. Since [27] only compared b-bit minwise hashing with VW on a small dataset, it is more informative to conduct a comparison of the two algorithms on this\nmuch larger dataset (200GB). We experimented with VW using k = 25 to 214 hash bins (sample size). Note that 214 = 16384. It is difficult to train LIBLINEAR with k = 215 because the training size of the hashed data by VW is close to 48 GB when k = 215.\nFigure 10 and Figure 11 plot the test accuracies for SVM and logistic regression, respectively. In each figure, every panel has the same set of solid curves for VW but a different set of dashed curves for different values of b in b-bit minwise hashing. Since the range\nof k is very large, here we choose to present the test accuracies against k. Representative C values (0.01, 0.1, 1, 10) are selected for the presentations.\nFrom Figures 10 and 11, we can see clearly that b-bit minwise hashing is substantially more accurate than VW at the same storage. In other words, in order to achieve the same accuracy, VW will require substantially more storage than b-bit minwise hashing.\nFigure 12 presents the training times for comparing VW with 8- bit minwise hashing. In this case, we can see that even at the same\nk, 8-bit hashing may have some computational advantages compared to VW. Of course, as it is clear that VW will require a much larger k in order to achieve the same accuracies as 8-bit minwise hashing, we know that the advantage of b-bit minwise hashing in terms of training time reduction is also enormous.\nOur comparison focuses on the VW hashing algorithm, not the VW online learning platform. The prior work [10] experimented with the VW online learning platform on the webspam dataset and reported an accuracy of 98.42% (compared to > 99.5% in our experiments with b-bit hashing) after 597 seconds of training."
    }, {
      "heading" : "6. ONLINE LEARNING",
      "text" : "Batch learning algorithms (e.g., the LIBLINEAR package) face a challenge when the data do not fit in memory. In the context of search, the training datasets often far exceed the memory capacity of a single machine. One common solution (e.g., [10, 39]) is to partition the data into blocks, which are repeatedly loaded into memory, to update the model coefficients. However, this does not solve the computational bottleneck because loading the data blocks for many iterations consumes a large number of disk I/Os. b-bit minwise hashing provides a simple solution for high-dimensional data by substantially reducing the data size.\nAnother increasing popular solution is online learning, which requires only loading one feature vector at a time. Here, we follow the notational convention used in the online learning literature (and the SGD code [4]). Given a training set {xi, yi}ni=1, we consider the following linear SVM optimization problem:\nmin w\nλ 2 w T w + 1 n\nn ∑\ni=1\nmax { 1− yiw T xi, 0 } . (11)\nThat is, we replace the parameter C in (6) by λ = 1 nC . The stochastic gradient descent (SGD) [4, 5] is a very popular algorithm for online learning. We modified Bottou’s SGD code [4] to load one data point at a time. Basically, when the new data point {xt, yt} arrives, the weights w are updated according to\nw ← w − ηt ×\n{\nλw if ytwxt > 1 λw − ytxt otherwise\n(12)\nwhere ηt is the learning rate. In Bottou’s SGD code, ηt is initially set by a careful calibration step using a (small) subset of the examples and is updated at every new data point.\nIt is often observed that the test accuracy improves with increasing number of epoches (one epoch means one pass of the data). For example, Figure 13 shows that we need about 60 epochs for both webspam and rcv1 datasets to reach (fairly) stable predictions."
    }, {
      "heading" : "6.1 SGD SVM Results on Webspam",
      "text" : "Figure 14 presents the test accuracies versus the regularization parameter λ, at the last (100th) epoch. When b ≥ 8 and k ≥\n200, using b-bit hashing can achieve similar test accuracies as using the original data. Figure 15 illustrates the test accuracies versus epochs, for two selected λ values. Perhaps 20 epochs are sufficient for reaching a sufficient accuracy using b-bit hashing.\nFigure 16 plots the training time and loading time for both using the original data and using 8-bit minwise hashing. On average, using the original data takes about 10 times more time than using 8-bit hashed data, as reflected in Table 4. Also, clearly the data loading time dominates the cost.\nBecause the data loading time dominates the cost of online learning, in our SGD experiments, we always first converted the data into binary format as opposed to the LIBSVM format used in our batch learning experiments. All the reported data loading times in this section were based on binary data. We would like to thank Leon Bottou for the highly helpful communications in this matter."
    }, {
      "heading" : "6.2 SGD SVM Results on Rcv1",
      "text" : "Figure 17 presents the test accuracies of SGD SVM on rcv1 at the 100th epoch, for both the original data and the b-bit (b = 8 and b = 12) hashed data. When k ≥ 500 and b = 12, b-bit minwise hashing achieves similar accuracies as using the original\ndata. Figure 18 presents the training time and data loading time. As explicitly calculated in Table 4, using the original data costs 30 times more time than using 12-bit minwise hashing. Again, the data loading time dominates the cost."
    }, {
      "heading" : "6.3 Averaged SGD (ASGD)",
      "text" : "In the course of writing this paper in 2011, Leon Bottou kindly informed us that there was a recent major upgrade of the SGD code, which implemented ASGD (Averaged SGD) [38]. Thus, we also provide experiments of ASGD on the webspam dataset as shown in Figure 19.\nCompared with the SGD results, it appears that ASGD does have some noticeable improvements over SGD. Nevertheless, ASGD\nstill needs more than 1 epoch (perhaps 10 to 20) to approach the best accuracy. Also, b-bit hashing continues to perform very well in terms of accuracy and training time reduction."
    }, {
      "heading" : "7. CONCLUSION",
      "text" : "(b-bit) Minwise Hashing is a standard technique for similarity computation which has also recently been shown [27] to be a valuable data reduction technique in (batch) machine learning, where it can reduce both the computational overhead as well as the required infrastructure and energy consumption by orders of magnitude, at often negligible reduction in learning accuracy.\nHowever, the use of b-bit minwise hashing on truly large learning datasets, which frequently occur in the context of search, requires study of a number of related challenges. First, datasets with very large numbers of features make it impossible to use pre-computed permutation matrices for the permutation step, due to prohibitive storage requirements. Second, for very large data, the initial preprocessing phase during which minhash signatures are computed, consumes significant resources. And finally, while the technique has been successfully applied in the context of batch learning (on a fairly small dataset), its efficacy in the context of online learning algorithms (which are becoming increasingly popular in the con-\ntext of web search and advertising) has not been shown.\nIn the context of duplicate detection (which normally concerns only highly similar pairs of documents) using minwise hashing with 64 bits per hashed value, the prior studies (e.g., [6]) demonstrated that it would be sufficient to use about k ≈ 200 permutations. However, b-bit minwise hashing (for small values of b) does require more permutations than the original minwise hashing, as explained in [26], for example, by increasing k by a factor of 3 when using b = 1 and the resemblance threshold is R = 0.5. In the context of machine learning and b-bit minwise hashing, we have also found that in some datasets k has to be fairly large, e.g., k = 500 or even more. This is because machine learning algorithms use all similarities, not just highly similar pairs.\nIn this paper, we addressed all of these challenges to adoption of b-bit minwise hashing in the context of Web-scale learning tasks. Regarding the first challenge, we were able to show that the use of 2- and 4-universal hash functions matches the accuracy of fully random permutations, while enabling the use of (b-bit) minwise hashing even for data with extremely large numbers of features.\nRegarding the 2nd challenge, we were able to formulate an implementation of the minwise hashing algorithm that effectively leverages the properties of current GPU architectures, in particular their massive parallelism and SIMD instruction processing, while minimizing the impact of their constraints (most notably slow modulo operations and the limited bandwidth available for main memory data transfer). We observed that the new GPU-based implementation resulted in speed-ups of between 20-80× for the minhash computation, thereby making the data loading time itself (and not the preprocessing) the new bottleneck.\nFinally, we were able to show that, similarly to batch learning, bbit minwise hashing can dramatically reduce the resource requirements for online learning as well, with little reduction in accuracy. However, for online learning, the reduction was mainly due to the reduction in data loading time, which becomes a major factor when online learning requires a large number of epochs to converge. A side-effect of the use of models leveraging b-bit minwise hashing is that the space requirements of the resulting model itself are also dramatically reduced, which is important in the context of web search, where an incoming search query may trigger various models for query classification, vertical selection, etc. and all of these models compete for memory on the user-facing servers."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank Leon Bottou for very helpful communications."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1]\nhttp://cacm.acm.org/blogs/blog-cacm/108385-research-directions-for[2]\nhttp://developer.download.nvidia.com/compute/cuda/2_3/toolkit/docs/NVIDI [3] Mikhail Bilenko, Sugato Basu, and Mehran Sahami. Adaptive product\nnormalization: Using online learning for record linkage in comparison shopping. In ICDM, 2005.\n[4] Leon Bottou. http://leon.bottou.org/projects/sgd. [5] Leon Bottou. Large-scale machine learning with stochastic gradient descent. In\nCOMPSTAT, 2010. [6] Andrei Z. Broder. On the resemblance and containment of documents. In the\nCompression and Complexity of Sequences, pages 21–29, Positano, Italy, 1997. [7] Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael Mitzenmacher.\nMin-wise independent permutations (extended abstract). In STOC, pages 327–336, Dallas, TX, 1998.\n[8] Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. Syntactic clustering of the web. In WWW, pages 1157 – 1166, Santa Clara, CA, 1997.\n[9] J. Lawrence Carter and Mark N. Wegman. Universal classes of hash functions (extended abstract). In STOC, pages 106–112, 1977.\n[10] Kai-Wei Chang and Dan Roth. Selective block minimization for faster convergence of limited memory large-scale linear models. In KDD, pages 699–707, 2011. [11] Ludmila Cherkasova, Kave Eshghi, Charles B. Morrey III, Joseph Tucek, and Alistair C. Veitch. Applying syntactic similarity algorithms for enterprise information management. In KDD, pages 1087–1096, Paris, France, 2009. [12] Massimiliano Ciaramita, Vanessa Murdock, and Vassilis Plachouras. Online learning from click data for sponsored search. In WWW Conference, 2008. [13] Martin Dietzfelbinger. Universal hashing and k-wise independent random variables via integer arithmetic without primes. In Proceedings of the 13th Annual Symposium on Theoretical Aspects of Computer Science, pages 569–580, 1996. [14] Martin Dietzfelbinger, Torben Hagerup, Jyrki Katajainen, and Martti Penttonen. A reliable randomized algorithm for the closest-pair problem. Journal of Algorithms, 25(1):19–51, 1997. [15] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874, 2008. [16] Dennis Fetterly, Mark Manasse, Marc Najork, and Janet L. Wiener. A large-scale study of the evolution of web pages. In WWW, pages 669–678, Budapest, Hungary, 2003. [17] George Forman, Kave Eshghi, and Jaap Suermondt. Efficient detection of large-scale redundancy in enterprise file systems. SIGOPS Oper. Syst. Rev., 43(1):84–91, 2009. [18] Venkatesh Ganti, Arnd Christian König, and Xiao Li. Precomputing search features for fast and accurate query classification. In WSDM, pages 61–70, 2010. [19] Bingsheng He, Mian Lu, Ke Yang, Rui Fang, Naga K. Govindaraju, Qiong Luo, and Pedro V. Sander. Relational Query Coprocessing on Graphics Processors. ACM Trans. Database Syst., 34:21:1–21:39, December 2009. [20] Piotr Indyk. A Small Approximately Min-wise Independent Family of Hash Functions. J. Algorithms, 38, 2001. [21] Thorsten Joachims. Training linear svms in linear time. In KDD, pages 217–226, Pittsburgh, PA, 2006. [22] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. Fast: fast architecture sensitive tree search on modern cpus and gpus. In SIGMOD, pages 339–350, 2010. [23] Gaston L’Huillier, Richard Weber, and Nicolas Figueroa. Online Phishing Classification using Adversarial Data Mining and Signaling Games. SIGKDD Explor. Newsl., 11:92–99, May 2010. [24] Ping Li. Image classification with hashing on locally and gloablly expanded features. Technical report. [25] Ping Li, Trevor J. Hastie, and Kenneth W. Church. Very sparse random projections. In KDD, pages 287–296, Philadelphia, PA, 2006. [26] Ping Li and Arnd Christian König. Theory and applications b-bit minwise hashing. Commun. ACM, 2011. [27] Ping Li, Anshumali Shrivastava, Joshua Moore, and Arnd Christian König. Hashing algorithms for large-scale learning. In NIPS, Vancouver, BC, 2011. [28] Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma. Detecting Near-Duplicates for Web-Crawling. In WWW, Banff, Alberta, Canada, 2007. [29] Sandeep Pandey, Andrei Broder, Flavio Chierichetti, Vanja Josifovski, Ravi Kumar, and Sergei Vassilvitskii. Nearest-neighbor caching for content-match applications. In WWW, pages 441–450, Madrid, Spain, 2009. [30] Mihai Pătraşcu and Mikkel Thorup. On the k-independence required by linear probing and minwise independence. In ICALP, pages 715–726, 2010. [31] D. Sculley and Gabriel M. Wachman. Relaxed online SVMs for Spam Filtering. In SIGIR, 2007. [32] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In ICML, pages 807–814, Corvalis, Oregon, 2007. [33] Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and S.V.N. Vishwanathan. Hash kernels for structured data. Journal of Machine Learning Research, 10:2615–2637, 2009. [34] Mikkel Thorup and Yin Zhang. Tabulation based 5-universal hashing and linear probing. In ALENEX, pages 62–76, 2010. [35] Simon Tong. Lessons learned developing a practical large scale machine learning system. http://googleresearch.blogspot.com/2010/04/lessons-learned-developing-practical.html, 2008. [36] Tanguy Urvoy, Emmanuel Chauveau, Pascal Filoche, and Thomas Lavergne. Tracking web spam with html style similarities. ACM Trans. Web, 2(1):1–28, 2008. [37] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature hashing for large scale multitask learning. In ICML, pages 1113–1120, 2009. [38] Wei Wu. Towards optimal one pass large scale learning with averaged\nstochastic gradient descent. Technical report, 2011. [39] Hsiang-Fu Yu, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen Lin. Large linear\nclassification when data cannot fit in memory. In KDD, pages 833–842, 2010.\nAPPENDIX"
    }, {
      "heading" : "A. RESEMBLANCE ESTIMATION USING SIMPLE HASH FUNCTIONS",
      "text" : "In this section we study the effect of using 2U/4U hashing function in place of (fully) random permutation matrices on the accuracy of resemblance estimation via b-bit minwise hashing. This will provide us a better understanding why the learning results (for SVM and logistic regression) using b-bit minwise hashing are not noticeably affected much by replacing the fully random permutation matrix with 2U/4U hash functions. As we shall see, as long as the original data are not too dense, using 2U/4U hash functions will not result in loss of estimation accuracy. As we observed that results from 2U and 4U are essentially indistinguishable, we only report the 2U experiments.\nThe task we study here is the estimation of word associations. The dataset, extracted from commercial Web crawls, consists of 9 pairs of sets (18 English words). Each set consists of the document IDs which contain the word at least once. Table 5 summarizes the data.\nWe implemented both 2U hash (10) and 4U hash schemes, for D = 216, 218, 220, 222, 224, 226, 228, 230, 232. Note that D ≥ 216 is necessary for this dataset. After sufficient number of repetitions, we computed the simulated mean square error (MSE = Var + Bias2) for each case, to compare with the theoretical variance (Eq. (11) in [26]), which was derived by assuming perfect random permutations. Ideally, the empirical MSEs and the theoretical variances should overlap. Indeed, we observe this is always the case when D ≥ 220. This is the reason why we only plot the results for D ≤ 220 in Figures 20 to 22. In fact, as shown in Figure 20, when the data are not dense (e.g., KONG-HONG, GABMIA-KIRIBATI, SAN-FRANCISCO), using 2U can achieve very similar results as using perfect random permutations, even at the smallest D = 216.\nPractical Implication: In practice, we expect the data vectors to be very sparse for a large number of applications, especially the many search-related tasks where features correspond to the presence/absence of text n-grams. For these tasks, the large number of distinct words (e.g., [18] reports 38M distinct 1-grams in an early Wikipedia corpus) and the much smaller number of terms in individual documents combine to cause this property. Therefore, we expect that 2U/4U hash functions will perform well when used for b-bit minwise hashing, as verified in the main body of the paper."
    } ],
    "references" : [ {
      "title" : "Adaptive product normalization: Using online learning for record linkage in comparison shopping",
      "author" : [ "Mikhail Bilenko", "Sugato Basu", "Mehran Sahami" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "Leon Bottou" ],
      "venue" : "In COMPSTAT,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "On the resemblance and containment of documents",
      "author" : [ "Andrei Z. Broder" ],
      "venue" : "In the Compression and Complexity of Sequences,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Min-wise independent permutations (extended abstract)",
      "author" : [ "Andrei Z. Broder", "Moses Charikar", "Alan M. Frieze", "Michael Mitzenmacher" ],
      "venue" : "In STOC,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "Syntactic clustering of the web",
      "author" : [ "Andrei Z. Broder", "Steven C. Glassman", "Mark S. Manasse", "Geoffrey Zweig" ],
      "venue" : "In WWW,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Universal classes of hash functions (extended abstract)",
      "author" : [ "J. Lawrence Carter", "Mark N. Wegman" ],
      "venue" : "In STOC,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1977
    }, {
      "title" : "Selective block minimization for faster convergence of limited memory large-scale linear models",
      "author" : [ "Kai-Wei Chang", "Dan Roth" ],
      "venue" : "In KDD,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Applying syntactic similarity algorithms for enterprise information management",
      "author" : [ "Ludmila Cherkasova", "Kave Eshghi", "Charles B. Morrey III", "Joseph Tucek", "Alistair C. Veitch" ],
      "venue" : "In KDD,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Online learning from click data for sponsored search",
      "author" : [ "Massimiliano Ciaramita", "Vanessa Murdock", "Vassilis Plachouras" ],
      "venue" : "In WWW Conference,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Universal hashing and k-wise independent random variables via integer arithmetic without primes",
      "author" : [ "Martin Dietzfelbinger" ],
      "venue" : "In Proceedings of the 13th Annual Symposium on Theoretical Aspects of Computer Science,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1996
    }, {
      "title" : "A reliable randomized algorithm for the closest-pair problem",
      "author" : [ "Martin Dietzfelbinger", "Torben Hagerup", "Jyrki Katajainen", "Martti Penttonen" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "A large-scale study of the evolution of web pages",
      "author" : [ "Dennis Fetterly", "Mark Manasse", "Marc Najork", "Janet L. Wiener" ],
      "venue" : "In WWW,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Efficient detection of large-scale redundancy in enterprise file systems",
      "author" : [ "George Forman", "Kave Eshghi", "Jaap Suermondt" ],
      "venue" : "SIGOPS Oper. Syst. Rev.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Precomputing search features for fast and accurate query classification",
      "author" : [ "Venkatesh Ganti", "Arnd Christian König", "Xiao Li" ],
      "venue" : "In WSDM,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "A Small Approximately Min-wise Independent Family of Hash Functions",
      "author" : [ "Piotr Indyk" ],
      "venue" : "J. Algorithms,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "Training linear svms in linear time",
      "author" : [ "Thorsten Joachims" ],
      "venue" : "In KDD,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Fast: fast architecture sensitive tree search on modern cpus and gpus",
      "author" : [ "Changkyu Kim", "Jatin Chhugani", "Nadathur Satish", "Eric Sedlar", "Anthony D. Nguyen", "Tim Kaldewey", "Victor W. Lee", "Scott A. Brandt", "Pradeep Dubey" ],
      "venue" : "In SIGMOD,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Online Phishing Classification using Adversarial Data Mining and Signaling",
      "author" : [ "Gaston L’Huillier", "Richard Weber", "Nicolas Figueroa" ],
      "venue" : "Games. SIGKDD Explor. Newsl.,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Very sparse random projections",
      "author" : [ "Ping Li", "Trevor J. Hastie", "Kenneth W. Church" ],
      "venue" : "In KDD,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Theory and applications b-bit minwise hashing",
      "author" : [ "Ping Li", "Arnd Christian König" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Hashing algorithms for large-scale learning",
      "author" : [ "Ping Li", "Anshumali Shrivastava", "Joshua Moore", "Arnd Christian König" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Detecting Near-Duplicates for Web-Crawling",
      "author" : [ "Gurmeet Singh Manku", "Arvind Jain", "Anish Das Sarma" ],
      "venue" : "In WWW,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    }, {
      "title" : "Nearest-neighbor caching for content-match applications",
      "author" : [ "Sandeep Pandey", "Andrei Broder", "Flavio Chierichetti", "Vanja Josifovski", "Ravi Kumar", "Sergei Vassilvitskii" ],
      "venue" : "In WWW,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "On the k-independence required by linear probing and minwise independence",
      "author" : [ "Mihai Pătraşcu", "Mikkel Thorup" ],
      "venue" : "In ICALP,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2010
    }, {
      "title" : "Relaxed online SVMs for Spam Filtering",
      "author" : [ "D. Sculley", "Gabriel M. Wachman" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2007
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro" ],
      "venue" : "In ICML,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2007
    }, {
      "title" : "Hash kernels for structured data",
      "author" : [ "Qinfeng Shi", "James Petterson", "Gideon Dror", "John Langford", "Alex Smola", "S.V.N. Vishwanathan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2009
    }, {
      "title" : "Tabulation based 5-universal hashing and linear probing",
      "author" : [ "Mikkel Thorup", "Yin Zhang" ],
      "venue" : "In ALENEX,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2010
    }, {
      "title" : "Lessons learned developing a practical large scale machine learning system",
      "author" : [ "Simon Tong" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "Tracking web spam with html style similarities",
      "author" : [ "Tanguy Urvoy", "Emmanuel Chauveau", "Pascal Filoche", "Thomas Lavergne" ],
      "venue" : "ACM Trans. Web,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2008
    }, {
      "title" : "Feature hashing for large scale multitask learning",
      "author" : [ "Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg" ],
      "venue" : "In ICML,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    }, {
      "title" : "Towards optimal one pass large scale learning with averaged  stochastic gradient descent",
      "author" : [ "Wei Wu" ],
      "venue" : "Technical report,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2011
    }, {
      "title" : "Large linear classification when data cannot fit in memory",
      "author" : [ "Hsiang-Fu Yu", "Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin" ],
      "venue" : "In KDD,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "The recent work [27] demonstrated a potential use of b-bit minwise hashing [26] for batch learning on large data.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 20,
      "context" : "The recent work [27] demonstrated a potential use of b-bit minwise hashing [26] for batch learning on large data.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "INTRODUCTION Minwise hashing [6–8] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "INTRODUCTION Minwise hashing [6–8] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "INTRODUCTION Minwise hashing [6–8] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : "INTRODUCTION Minwise hashing [6–8] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "INTRODUCTION Minwise hashing [6–8] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 7,
      "context" : "INTRODUCTION Minwise hashing [6–8] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.",
      "startOffset" : 345,
      "endOffset" : 349
    }, {
      "referenceID" : 30,
      "context" : "INTRODUCTION Minwise hashing [6–8] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.",
      "startOffset" : 360,
      "endOffset" : 364
    }, {
      "referenceID" : 20,
      "context" : "The recent development of bbit minwise hashing [26] provided a substantial improvement in the estimation accuracy and speed by proposing a new estimator that stores only the lowest b bits of each hashed value.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : "More recently, [27] proposed the use of b-bit minwise hashing in the context of learning algorithms such as SVM or logistic regression on large binary data (which is typical in Web classification tasks).",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "In [27], experiments showed this for the webspam dataset which has 16 million features with a total disk size of 24GB in standard LibSVM format.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : ", min(π(S1)), using 64 bits [16].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "The storage (and computational) cost is prohibitive in industrial applications [28].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "The recent work of b-bit minwise hashing [26] provides a simple solution by storing only the lowest b bits of each hashed value.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "[26] Assume D is large.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "For example, [35] discusses training datasets with (on average) n = 10 items and D = 10 distinct features.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 31,
      "context" : "[37] experimented with a dataset of potentially D = 16 trillion (1.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Representative software packages include SVM [21], Pegasos [32], Bottou’s SGD SVM [4], and LIBLINEAR [15].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 26,
      "context" : "Representative software packages include SVM [21], Pegasos [32], Bottou’s SGD SVM [4], and LIBLINEAR [15].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "Representative software packages include SVM [21], Pegasos [32], Bottou’s SGD SVM [4], and LIBLINEAR [15].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "Note that in prior studies for duplicate detection [6], k was usually not too large (i.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : "With b-bit minwise hashing, we have to use larger k values according to the analysis in [26] even in the context of duplicate detections.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "GPUs offer, compared to current CPUs, higher instruction parallelism and very low latency access to the internal GPU memory, but comparatively slow latencies when accessing the main memory [22].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "4 Issue 2: Online Learning Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures.",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "4 Issue 2: Online Learning Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures.",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : "4 Issue 2: Online Learning Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures.",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 25,
      "context" : "4 Issue 2: Online Learning Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures.",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 32,
      "context" : ", [38]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : ", [13]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : ", [20, 30].",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 24,
      "context" : ", [20, 30].",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "To overcome the difficulty in achieving perfect permutations, the common practice is to use the so-called universal hashing [9].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "3 Avoid Modulo Operations in 2U Hashing To avoid the modulo operations in 2U hashing, we adopt a common trick [14].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "It is known that the following hash function is essentially 2U [14]:",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "A recent paper [34] implemented a similar trick for p = 2−1, which was simpler than ours because with p = 2 − 1 there is no need to check the condition “if (v >= 2 * p)”.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "The webspam dataset was used in the recent paper [27].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "We tried out 30 values for the regularization parameter C in the interval [10, 100].",
      "startOffset" : 74,
      "endOffset" : 83
    }, {
      "referenceID" : 27,
      "context" : "2 Experimental Results on VW Algorithm The application domains of b-bit minwise hashing are limited to binary data, whereas other hashing algorithms such as random projections and Vowpal Wabbit (VW) hashing algorithm [33, 37] are not restricted to binary data.",
      "startOffset" : 217,
      "endOffset" : 225
    }, {
      "referenceID" : 31,
      "context" : "2 Experimental Results on VW Algorithm The application domains of b-bit minwise hashing are limited to binary data, whereas other hashing algorithms such as random projections and Vowpal Wabbit (VW) hashing algorithm [33, 37] are not restricted to binary data.",
      "startOffset" : 217,
      "endOffset" : 225
    }, {
      "referenceID" : 21,
      "context" : "It is shown in [27] that VW and random projections have essentially the same variances as reported in [25].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 19,
      "context" : "It is shown in [27] that VW and random projections have essentially the same variances as reported in [25].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : "Since there won’t be space to explain the details of VW algorithm, interested readers please refer to [27, 33, 37].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 27,
      "context" : "Since there won’t be space to explain the details of VW algorithm, interested readers please refer to [27, 33, 37].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 31,
      "context" : "Since there won’t be space to explain the details of VW algorithm, interested readers please refer to [27, 33, 37].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 27,
      "context" : "To help understand the significance of these results, next we provide a comparison study with the VW hashing algorithm [33, 37].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 31,
      "context" : "To help understand the significance of these results, next we provide a comparison study with the VW hashing algorithm [33, 37].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 27,
      "context" : "3 Comparisons with VW Algorithm The Vowpal Wabbit (VW) algorithm [33, 37] is an influential hashing method for data/dimension reduction.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 31,
      "context" : "3 Comparisons with VW Algorithm The Vowpal Wabbit (VW) algorithm [33, 37] is an influential hashing method for data/dimension reduction.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "Since [27] only compared b-bit minwise hashing with VW on a small dataset, it is more informative to conduct a comparison of the two algorithms on this 10 −3 10 −2 10 −1 10 0 10 1 10 2 50 60 70 80 90 100",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 6,
      "context" : "The prior work [10] experimented with the VW online learning platform on the webspam dataset and reported an accuracy of 98.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : ", [10, 39]) is to partition the data into blocks, which are repeatedly loaded into memory, to update the model coefficients.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 33,
      "context" : ", [10, 39]) is to partition the data into blocks, which are repeatedly loaded into memory, to update the model coefficients.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "The stochastic gradient descent (SGD) [4, 5] is a very popular algorithm for online learning.",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : "3 Averaged SGD (ASGD) In the course of writing this paper in 2011, Leon Bottou kindly informed us that there was a recent major upgrade of the SGD code, which implemented ASGD (Averaged SGD) [38].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 21,
      "context" : "CONCLUSION (b-bit) Minwise Hashing is a standard technique for similarity computation which has also recently been shown [27] to be a valuable data reduction technique in (batch) machine learning, where it can reduce both the computational overhead as well as the required infrastructure and energy consumption by orders of magnitude, at often negligible reduction in learning accuracy.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : ", [6]) demonstrated that it would be sufficient to use about k ≈ 200 permutations.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 20,
      "context" : "However, b-bit minwise hashing (for small values of b) does require more permutations than the original minwise hashing, as explained in [26], for example, by increasing k by a factor of 3 when using b = 1 and the resemblance threshold is R = 0.",
      "startOffset" : 137,
      "endOffset" : 141
    } ],
    "year" : 2012,
    "abstractText" : "ABSTRACT Minwise hashing is a standard technique in the context of search for approximating set similarities. The recent work [27] demonstrated a potential use of b-bit minwise hashing [26] for batch learning on large data. However, several critical issues must be tackled before one can apply b-bit minwise hashing to the volumes of data often used industrial applications, especially in the context of search.",
    "creator" : "LaTeX with hyperref package"
  }
}