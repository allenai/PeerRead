{
  "name" : "1506.07359.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sequential Extensions of Causal and Evidential Decision Theory",
    "authors" : [ "Tom Everitt Jan Leike", "Marcus Hutter" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n07 35\n9v 1\n[ cs\n.A I]\n2 4\nJu n\nMoving beyond the dualistic view in AI where agent and environment are separated incurs new challenges for decision making, as calculation of expected utility is no longer straightforward. The non-dualistic decision theory literature is split between causal decision theory and evidential decision theory. We extend these decision algorithms to the sequential setting where the agent alternates between taking actions and observing their consequences. We find that evidential decision theory has two natural extensions while causal decision theory only has one.\nKeywords. Evidential decision theory, causal decision theory, causal graphical models, planning, dualism, physicalism."
    }, {
      "heading" : "1 Introduction",
      "text" : "In artificial-intelligence problems an agent interacts sequentially with an environment by taking actions and receiving percepts [RN10]. This model is dualistic: the agent is distinct from the environment. It influences the environment only through its actions, and the environment has no other information about the agent. The dualism assumption is accurate for an algorithm that is playing chess, go, or other (video) games, which explains why it is ubiquitous in AI research. But often it is not true: real-world agents are embedded in (and computed by) the environment [OR12], and then a physicalistic model1 is more appropriate.\nThis distinction becomes relevant in multi-agent settings with similar agents, where each agent encounters ‘echoes’ of its own decision making. If the other agents are running the same source code, then the agents’ decisions are logically connected. This link can be used for uncoordinated cooperation [LFY+14]. Moreover, a physicalistic model is indispensable for self-reflection. If the agent is required to autonomously verify its integrity, and perform maintenance, repair, or upgrades, then the agent needs to be aware of its own functioning. For this, a reliable and accurate self-modeling is essential. Today, applications of this level of autonomy are mostly restricted to space probes distant from earth or robots navigating lethal situations, but in the future this might also become crucial for\n∗The final publication is available at http://link.springer.com/. 1Some authors also call this type of model materialistic or naturalistic.\nsustained self-improvement in generally intelligent agents [Yud08, Bos14, SF14a, RDT+15].\nIn the physicalistic model the agent is embedded inside the environment, as depicted in Figure 1. The environment has a hidden state that contains information about the agent that is inaccessible to the agent itself. The agent has an environment model that describes the behavior of the environment given the hidden state and includes beliefs about the agent’s own future actions (thus modeling itself).\nPhysicalistic agents may view their actions in two ways: as their selected output, and as consequences of properties of the environment. This leads to significantly more complex problems of inference and decision making, with actions simultaneously being both means to influence the environment and evidence about it. For example, looking at cat pictures online may simultaneously be a means of procrastination, and evidence of bad air quality in the room.\nDualistic decision making in a known environment is straightforward calculation of expected utilities. This is known as Savage decision theory [Sav72]. For non-dualistic decision making two main approaches are offered by the decision theory literature: causal decision theory (CDT) [GH78, Lew81, Sky82, Joy99, Wei12] and evidential decision theory (EDT) [Jef83, Bri14, Ahm14]. EDT and CDT both take actions that maximize expected utility, but differ in the way this expectation is computed: EDT uses the action under consideration as evidence about the environment while CDT does not. Section 2 formally introduces these decision algorithms.\nOur contribution is to formalize and explore a decision-theoretic setting with a physicalistic reinforcement learning agent interacting sequentially with an environment that it is embedded in (Section 3). Previous work on non-dualistic decision theories has focused on one-shot situations. We find that there are two natural extensions of EDT to the sequential case, depending on whether the agent updates beliefs based on its next action or its entire policy. CDT has only one natural extension. We extend two famous Newcomblike problems to the sequential setting to illustrate the differences between our (generalized) decision theories.\nSection 4 summarizes our results and outlines future directions. A list of\nnotation can be found on page 16 and Appendix A contains formal details to our examples."
    }, {
      "heading" : "2 One-Shot Decision Making",
      "text" : "In a one-shot decision problem, we take one action a ∈ A, receive a percept e ∈ E (typically called outcome in the decision theory literature) and get a payoff u(e) according to the utility function u : E → [0, 1]. We assume that the set of actions A and the set of percepts E are finite. Additionally, the environment contains a hidden state s ∈ S. The hidden state holds information that is inaccessible to the agent at the time of the decision, but may influence the decision and the percept. Formally, the environment is given by a probability distribution P over the hidden state, the action, and the percept that factors according to a causal graph [Pea09].\nA causal graph over the random variables x1, . . . , xn is a directed acyclic graph with nodes x1, . . . , xn. To each node xi belongs a probability distribution P (xi | pai), where pai is the set of parents of xi in the graph. It is natural to identify the causal graph with the factored distribution P (x1, . . . , xn) = ∏n\ni=1 P (xi | pai). Given such a causal graph/factored distribution, we define the do-operator as\nP (x1, . . . , xj−1, xj+1, . . . , xn | do(xj := b)) =\nn ∏\ni=1 i6=j\nP (xi | pai) (1)\nwhere xj is set to b wherever it occurs in pai, 1 ≤ i ≤ n. The result is a new probability distribution that can be marginalized and conditioned in the standard way. Intuitively, intervening on node xj means ignoring all incoming arrows to xj , as the effects they represent are no longer relevant when we intervene; the factor P (xj | paj) representing the ingoing influences to xj is therefore removed in the right-hand side of (1). Note that the do-operator is only defined for distributions for which a causal graph has been specified. See [Pea09, Ch. 3.4] for details."
    }, {
      "heading" : "2.1 Savage Decision Theory",
      "text" : "In the dualistic formulation of decision theory, we have a function P that takes an action a and returns a probability distribution Pa over percepts. Savage decision theory (SDT) [Sav72, Bri14] takes actions according to\nargmax a∈A\n∑\ne∈E\nPa(e)u(e). (SDT)\nIn the dualistic model it is usually conceptually clear what Pa should be. In the physicalistic model the environment model takes the form of a causal graph over a hidden state s, action a, and percept e, as illustrated in Figure 2. According to this causal graph, the probability distribution P factors causally into P (s, a, e) = P (s)P (a | s)P (e | s, a). The hidden state is not independent of the decision maker’s action and Savage’s model is not directly applicable since we do not have a specification of Pa. How should decisions be made in this context? The literature focuses on two answers to this question: CDT and EDT."
    }, {
      "heading" : "2.2 Causal and Evidential Decision Theory",
      "text" : "The literature on causal and evidential decision theory is vast, and we give only a very superficial overview that is intended to bring the reader up to speed on the basics. See [Bri14, Wei12] and references therein for more detailed introductions.\nEvidential decision theory (endorsed in [Jef83, Ahm14]) considers the probability of the percept e conditional on taking the action a:\nargmax a∈A\n∑\ne∈E\nP (e | a)u(e) with P (e | a) = ∑\ns∈S\nP (e | s, a)P (s | a) (EDT)\nCausal decision theory has several formulations [GH78, Lew81, Sky82, Joy99]; we use the one given in [Sky82], with Pearl’s calculus of causality [Pea09]. According to CDT, the probability of a percept e is given by the causal intervention of performing action a on the causal graph from Figure 2:\nargmax a∈A\n∑\ne∈E\nP (e | do(a))u(e) with P (e | do(a)) = ∑\ns∈S\nP (e | s, a)P (s)\n(CDT) where P (e | do(a)) follows from (1) and marginalization over s.\nThe difference between CDT and EDT is how the action affects the belief about the hidden state. EDT assigns credence P (s | a) to the hidden state s if action a is taken, while CDT assigns credence P (s). A common argument for CDT is that an action under my direct control should not influence my belief about things that are not causally affected by the action. Hence P (s) should be my belief in s, and not P (s | a). (By assumption, the action does not causally affect the hidden state.) EDT might reply that if action a does not have the same likelihood under all hidden states s, then action a should indeed inform me about the hidden state, regardless of causal connection. The following two classical examples from the decision theory literature describe situations where CDT and EDT disagree. A formal definition of these examples can be found in Appendix A.\nExample 1 (Newcomb’s Problem [Noz69]). In Newcomb’s Problem there are two boxes: an opaque box that is either empty or contains one million dollars and a transparent box that contains one thousand dollars. The agent can choose between taking only the opaque box (‘one-boxing’) and taking both boxes (‘twoboxing’). The content of the opaque box is determined by a prediction about\nthe agent’s action by a very reliable predictor: if the agent is predicted to onebox, the box contains the million, and if the agent is predicted to two-box, the box is empty. In Newcomb’s problem EDT prescribes one-boxing because oneboxing is evidence that the box contains a million dollars. In contrast, CDT prescribes two-boxing because two-boxing dominates one-boxing: in either case we are a thousand dollars richer, and our decision cannot causally affect the prediction. Newcomb’s problem has been raised as a critique to CDT, but many philosophers insist that two-boxing is in fact the rational choice,2 even if it means you end up poor.\nNote how the decision depends on whether the action influences the belief about the hidden state (the contents of the opaque box) or not.\nNewcomb’s problem may appear as an unrealistic thought experiment. However, we argue that problems with similar structure are fairly common. The main structural requirement is that P (s | a) 6= P (s) for some state or event s that is not causally affected by a. In Newcomb’s problem the predictor’s ability to guess the action induces an ‘information link’ between actions and hidden states. If the stakes are high enough, the predictor does not have to be much better than random in order to generate a Newcomblike decision problem. Consider for example spouses predicting the faithfulness of their partners, employers predicting the trustworthiness of their employees, or parents predicting their children’s intentions. For AIs, the potential for accurate predictions is even greater, as the predictor may have access to the AI’s source code. Although rarely perfect, all of these predictions are often substantially better than random.\nTo counteract the impression that EDT is generally superior to CDT, we also discuss the toxoplasmosis problem.\nExample 2 (Toxoplasmosis Problem [Alt13]). 3 This problem takes place in a world in which there is a certain parasite that causes its hosts to be attracted to cats, in addition to uncomfortable side effects. The agent is handed an adorable little kitten and is faced with the decision of whether or not to pet it. Petting the kitten feels nice and therefore yields more utility than not petting it. However, people suffering from the parasite are more likely to pet the kitten. Petting the kitten is evidence of having the parasite, so EDT recommends against it. CDT correctly observes that petting the kitten does not cause the parasite, and is therefore in favor of petting.\nNewcomb’s problem and the toxoplasmosis problem cannot be properly formalized in SDT, because SDT requires the percept-probabilities Pa to be specified, but it is not clear what the right choice of Pa would be. However, both CDT and EDT can be recast in the context of SDT by setting Pa to be P ( · | do(a)) and P ( · | a) respectively. Thus we could say that the formulation given by Savage needs a specification of the environment that tells us whether to act evidentially, causally, or otherwise.\n2In a 2009 survey, 31.4% of philosophers favored two-boxing, and 21.3% favored one-boxing (931 responses); see http://philpapers.org/surveys/results.pl . Is that the reason there are so few wealthy philosophers?\n3Historically, this problem has been known as the smoking lesion problem [Ega07]. We consider the smoking lesion formulation confusing, because today it is universally known that smoking does cause lung cancer."
    }, {
      "heading" : "3 Sequential Decision Making",
      "text" : "In this section we extend CDT and EDT to the sequential case. We start by formally specifying the physicalistic model depicted in Figure 1 in the first subsection, and discuss problems with time consistency in Section 3.2, before defining the extensions proper in Section 3.3 and 3.4. The final subsection dissects the role of the hidden state."
    }, {
      "heading" : "3.1 The Physicalistic Model",
      "text" : "For the remainder of this paper, we assume that the agent interacts sequentially with an environment. At time step t the agent chooses an action at ∈ A and receives a percept et ∈ E which yields a utility of u(et) ∈ R; the cycle then repeats for t + 1. A history is an element of (A × E)∗. We use æ ∈ A × E to denote one interaction cycle, and æ<t to denote a history of length t− 1. The percepts between time t and time m are denoted et:m. A policy is a function that maps a history æ<t to the next action at. We only consider deterministic policies.\nWe assume that the agent is given an environment model µ, but knows neither the hidden state s nor its own future actions. The unknown hidden state may influence both percepts and actions. Actions and percepts in turn influence the entire future. The environment model µ is given by a probability distribution over hidden states and histories that factors as\nµ(s,æ<t) = µ(s)\nt−1 ∏\ni=1\nµ(ai | s,æ<i)µ(ei | s,æ<iai) (2)\nfor any t ∈ N. While such a factorization is possible for any distribution, we additionally demand that this factorization is causal according to the causal graph in Figure 3. The distribution µ(at | s,æ<t) gives the likelihood of the agent’s own actions provided a hidden state s ∈ S (for example, the prior probability of an infected agent petting the kitten in the toxoplasmosis problem above). For technical reasons, this distribution must always leave some uncertainty about the actions: if the environment model assigned probability zero for an action a′, the agent could not deliberate taking action a′ since a′ could not be conditioned\non. Formally, we require µ( · | s) to be action-positive for all s ∈ S:\n∀æ<tat ∈ (A× E) ∗ ×A.\n( µ(æ<t | s) > 0 =⇒ µ(at | s,æ<t) > 0 )\n(3)\nThe distribution µ is a model of the environment, a belief held by the agent, but not the distribution from which the actual history is drawn. The actual history is distributed according to the true environment distribution. Because the environment contains the agent, the agent’s algorithm might get modified by it and the actions that the agent actually ends up taking might not be the actions that were planned. In the end, model and reality will disagree: for example, we simultaneously assume the agent’s policy π to be deterministic and the environment model to be action positive. Nevertheless, we assume the given environment model is accurate in the sense that it faithfully represents the environment in the ways relevant to the agent. In other words, we are interested in problems that arise during planning, not problems that arise due to poor modeling."
    }, {
      "heading" : "3.2 Time Consistency",
      "text" : "When planning for the infinite future we need to make sure that utilities do not sum to infinity; typically this is achieved with discounting. Here, we simplify by fixing a finite m ∈ N to be the agent’s lifetime: the agent cares about the sum of the utilities of all percepts e1 . . . em until and including time step m, but does not care what happens after that (presumably the agent is then retired).\nIn sequential decision theory we need to plan the next m − t actions in time step t. We plan what we would do for all possible future percepts et:m by choosing a policy π : (A×E)∗ → A that specifies which action we take depending on how the history plays out. For example, we take action at, and when we subsequently receive the percept et, we plan to take action at+1. Problems arise once we get to the next step and even tough we did take action at and the percept did turn out to be et, we change our mind and take a different action ât+1. This is called time inconsistency. Time inconsistency is an artifact of bad planning since the agent incorrectly anticipates her own actions. The choice of discounting can lead to time inconsistency: a sliding fixed-size horizon is time inconsistent, but a fixed finite lifetime is time consistent [LH14].\nWe achieve time consistency by using a fixed finite lifetime, and by calculating decisions recursively using value functions. A value function V πµ,m is a function of type ((A×E)∗∪((A×E)∗×A)) → R. It gives an estimate of future reward: V πµ,m(æ<t) and V π µ,m(æ<tat) are estimates of how much reward the policy π will obtain in environment µ within lifetime m subsequent to history æ<t and æ<tat respectively. For any history æ<t, we define V π µ,m(æ<t) := V π µ,m(æ<tπ(æ<t)). We say that a policy π is optimal and time consistent for the value function Vµ,m iff π(æ<t) = argmaxa V π µ,m(æ<ta) for all histories æ<t ∈ (A × E)\nt−1 and all t ≤ m."
    }, {
      "heading" : "3.3 Sequential Evidential Decision Theory",
      "text" : "Evidential decision theory assigns probability P (e | a) to action a resulting in percept e (Section 2.2). There are two ways to generalize this to the sequential setting, depending on whether we use only the next action or the whole future policy as evidence for the next percept.\nDefinition 3 (Action-Evidential Decision Theory). The action-evidential value of a policy π with lifetime m in environment µ given history æ<tat is\nV aev,πµ,m (æ<tat) := ∑\net\nµ(et | æ<tat) ( u(et) + V aev,π µ,m (æ<tatet) )\n(SAEDT)\nand V aev,πµ,m (æ<tat) := 0 for t > m. Sequential Action-Evidential Decision Theory (SAEDT) prescribes adopting an optimal and time consistent policy π for V aevµ,m.\nIt may be argued that SAEDT does not take all available (deliberative) information into account. When considering the consequences of an action, future developments of the environment-policy interactions could also be used as evidence. That is, we could condition not only on the next action, but on the future policy as a whole (within the lifetime). In order to define conditional probabilities with respect to (deterministic) policies, we define the following events. For a given policy π, let Πt:m be the set of all strings consistent with π between time step t and m:\nΠt:m := {æ1:∞ | ∀t ≤ i ≤ m. π(æ<i) = ai}\nThe likelihood of a next percept et provided a history æ<t and a (future) policy π followed from time step t until lifetime m (denoted πt:m) is then defined as\nµ(et | æ<t, πt:m) := µ(et | æ<t ∩ Πt:m). (4)\nThis is an atemporal conditional because we are conditioning on future actions up until the end of the agent’s lifetime. The conditional (4) is well-defined because we only take the actions from time step t tom into account; conditioning on policies with infinite lifetime leads to technical problems because such policies typically have µ-measure zero.\nDefinition 4 (Policy-Evidential Decision Theory). The policy-evidential value of a policy π with lifetime m in environment µ given history æ<tat is\nV pev,πµ,m (æ<tat) := ∑\net\nµ(et | æ<tat, πt+1:m) · ( u(et) + V pev,π µ,m (æ<tatet) )\n(SPEDT) and V pev,πµ,m (æ<t) := 0 for t > m. Sequential Policy-Evidential Decision Theory (SPEDT) prescribes adopting an optimal and time consistent policy π for V pevµ,m .\nFor one-step decisions (m = t+ 1), SAEDT and SPEDT coincide. To all our embedded agents, past actions constitute evidence about the hidden state. For evidential agents, this principle is extended to future actions. SAEDT and SPEDT differ in how far they extend it. The action-evidential agent only updates his belief on the action about to take place. In that sense, he only updates his belief about the next percept on events taking place before this percept. The policy-evidential agent takes the principle much further, using “thought-experiments” of what action he would take in hypothetical situations, most of which will never be realized. This is illustrated in the next example.\nExample 5 (Sequential Toxoplasmosis). In our sequential variation of the toxoplasmosis problem the agent has some probability of encountering a kitten. Additionally, the agent has the option of seeing a doctor (for a fee) and\ngetting tested for the parasite, which can then be safely removed. In the very beginning, an SPEDT agent updates his belief on the fact that if he encountered a kitten, he would not pet it, which lowers the probability that he has the parasite and makes seeing the doctor unattractive. An SAEDT agent only updates his belief about the parasite when he actually encounters a kitten, and thus prefers seeing the doctor. See Figure 4 for more details and a graphical illustration.\nThe observant reader may ask whether SPEDT could be enticed to make some percepts unlikely by choosing improbable actions subsequent to them. For example, could an SPEDT agent decide on a policy of selecting highly improbable actions in case it rained to make histories with rain less likely? The answer is no, as most such policies would not be time consistent. If it does rain, the highly improbable action would usually not the best one, and so the policy would not be prescribed by Definition 4."
    }, {
      "heading" : "3.4 Sequential Causal Decision Theory",
      "text" : "In sequential causal decision theory we ask what would happen if we causally intervened on the node at of the next action and fix it to π(æ<t) according to the policy π. This is expressed by the notation do(at := π(æ<t)), or do(π(æ<t)) for short.\nDefinition 6 (Sequential Causal Decision Theory). The causal value of a policy\nπ with lifetime m in environment µ given history æ<tat is\nV cau,πµ,m (æ<tat) := ∑\net∈E\nµ(et | æ<t, do(at)) ( u(et) + V cau,π µ,m (æ<tatet) ) (SCDT)\nand V cau,πµ,m (æ<tat) := 0 for t > m. Sequential Causal Decision Theory (SCDT) prescribes adopting an optimal and time consistent policy π for V cauµ,m.\nFor sequential evidential decision theory we discussed two versions (SAEDT) and (SPEDT), based on next action and future policy respectively. In SCDT we perform the causal intervention do(at := π(æ<t)). We could also consider a policy-causal decision theory by replacing µ(et | æ<t, do(at)) with µ(et | æ<t, do(πt:m)) in Definition 6. The causal intervention do(πt:m)) of a policy π between time step t and time step m is defined as as\nµ(et | æ<t, do(πt:m)) := ∑\net+1:m\nµ(et:m | æ<t, do(at := π(æ<t), . . . , am := π(æ<m))).\n(5) However, since the interventions are causal, we do not get any extra evidence from the future interventions. Therefore policy-causal decision theory is the same as action-causal decision theory:\nProposition 7 (Policy-Causal = Action-Causal). For all histories æ<t ∈ (A× E)∗ and all et ∈ E, we have µ(et | æ<t, do(πt:m)) = µ(et | æ<t, do(π(æ<t))).\nWe defer the proof to the end of this section. The following two examples illustrate the difference between SCDT and SAEDT/SPEDT in sequential settings.\nExample 8 (Newcomb with Precommitment). In this variation to Newcomb’s problem the agent first has the option to pay $300,000 to sign a contract that binds the agent to pay $2000 in case of two-boxing. An SAEDT or SPEDT agent knows that he will one-box anyways and hence has no need for the contract. An SCDT agent knows that she favors two-boxing, but signs the contract only if this occurs before the prediction is made (so it has a chance of causally affecting the prediction). With the contract in place, one-boxing is the dominant action, and thus the SCDT agent is predicted to one-box.\nExample 9 (Newcomb with Looking). In this variation to Newcomb’s problem the agent may look into the opaque box before making the decision which box to take. An SCDT agent is indifferent towards looking because she will take both boxes anyways. However, an SAEDT or SPEDT agent will avoid looking into the box, because once the content is revealed he two-boxes."
    }, {
      "heading" : "3.5 Expansion over the Hidden State",
      "text" : "The difference between sequential versions of EDT and CDT is how they update their prediction of a next percept et (Definitions 3, 4 and 6). The following proposition expands the different beliefs in terms of the hidden state.\nProposition 10. For all histories æ<tatet ∈ (A × E) ∗ the following holds for the next-percept beliefs of SAEDT, SPEDT and SCDT respectively:\nµ(et | æ<tat) = ∑\ns∈S\nµ(s | æ<tat)µ(et | s,æ<tat) (6)\nµ(et | æ<t, πt:m) = ∑\ns∈S\nµ(s | æ<t, πt:m)µ(et | s,æ<t, πt:m) (7)\nµ(et | æ<t, do(at)) = ∑\ns∈S\nµ(s | æ<t)µ(et | s,æ<tat) (8)\nProof. For the action-evidential conditional we take the joint distribution with s, and then split off et:\nµ(et | æ<tat) =\n∑\ns∈S µ(s,æ<tatet)\nµ(æ<tat) =\n∑\ns∈S µ(s,æ<tat)µ(et | s,æ<tat)\nµ(æ<tat)\n= ∑\ns∈S\nµ(s | æ<tat)µ(et | s,æ<tat)\nSimilarly for the policy-evidential conditional:\nµ(et | æ<t, πt:m) =\n∑\ns∈S µ(s,æ<tπ(æ<t)et, πt+1:m)\nµ(æ<t, πt:m)\n=\n∑\ns∈S µ(s,æ<tπ(æ<t), πt+1:m)µ(et | s,æ<tπ(æ<t), πt+1:m)\nµ(æ<t, πt:m)\n=\n∑\ns∈S µ(s,æ<t, πt:m)µ(et | s,æ<tπ(æ<t), πt+1:m)\nµ(æ<t, πt:m)\n= ∑\ns∈S\nµ(s | æ<t, πt:m)µ(et | s,æ<tπ(æ<t), πt+1:m)\n= ∑\ns∈S\nµ(s | æ<t, πt:m)µ(et | s,æ<t, πt:m)\nFor the causal conditional we turn to the rules of the do-operator [Pea09, Thm. 3.4.1]. The first equality below holds by definition. In the denominator of the second equality we can use Rule 3 (deletion of actions) to remove do(at) because the do-operator removes all incoming edges to at and makes at independent of the history æ<t. In the numerator of the second equality we use the definition of do (1):\nµ(et | æ<t, do(at)) = µ(æ<t, et | do(at))\nµ(æ<t | do(at))\n=\n∑\ns∈S µ(s,æ<t)µ(et | s,æ<tat)\nµ(æ<t)\n= ∑\ns∈S\nµ(s | æ<t)µ(et | s,æ<tat)\nProposition 10 shows that between SCDT and SAEDT, the difference in opinion about et only depends on differences in their (acausal) posterior belief µ(s | . . .) about the hidden state. SCDT and SAEDT thus become equivalent in scenarios where there is only one hidden state s∗ with µ(s∗) = 1, as this renders\nµ(s∗ | æ<t) = µ(s ∗ | æ<tat) = µ(s ∗) = 1. SPEDT, on the other hand, may disagree with the other two also after a hidden state has been fixed.\nFrom a problem modeler’s perspective, it is also instructive to consider the effect of moving uncertainty between the hidden state and environmental stochasticity. For two different environment models µ and µ′, the action and percept probabilities may be identical (i.e., µ(at | æ<t) = µ\n′(at | æ<t) and µ(et | æ<tat) = µ ′(et | æ<tat)) even though µ and µ ′ have non-isomorphic sets of hidden states S and S ′. For example, given any µ, an environment model µ′ with a single hidden state s0, µ\n′(s0) = 1, may be constructed from µ by µ′(s0,æ<t) := ∑\ns∈S µ(s,æ<t). The transformation will not affect SAEDT and SPEDT, as the definitions of their value functions only depends on the ‘observable’ action- and percept-probabilities µ(at | æ<t) and µ(et | æ<tat) which are preserved between µ and µ′. But the transformation will change SCDT’s behavior in any µ where SCDT disagrees with SAEDT, as SCDT and SAEDT are equivalent in µ′ that only has a single hidden state. That SCDT depends on what uncertainty is captured by the hidden state is unsurprising given that the hidden state has a special place in the causal structure of the problem. Ultimately, the modeler must decide what uncertainty to put in the hidden state, and what to attribute to environmental stochasticity. A general principle for how to do this is still an open question [SF14b].\nThe value functions of SAEDT, SPEDT and SCDT can be rewritten in the following iterative forms, where the latter form uses Proposition 10. Numbers above equality signs reference a justifying equation. Let ai := π(æ<i) for i ≥ t:\nV aev,πµ,m (æ<t) =\nm ∑\nk=t\n∑\net:k\nu(ek)\nk ∏\ni=t\nµ(ei | æ<iai) (9)\n(6) =\nm ∑\nk=t\n∑\net:k\nu(ek)\nk ∏\ni=t\n∑\ns∈S\nµ(s | æ<iai)µ(ei | s,æ<iai) (10)\nV pev,πµ,m (æ<t) =\nm ∑\nk=t\n∑\net:k\nu(ek)\nk ∏\ni=t\nµ(ei | æ<i, πi:m) (11)\n(7) =\nm ∑\nk=t\n∑\net:k\nu(ek) k ∏\ni=t\n∑\ns∈S\nµ(s | æ<iπi:m)µ(ei | s,æ<i, πi:m) (12)\nV cau,πµ,m (æ<t) =\nm ∑\nk=t\n∑\net:k\nu(ek)\nk ∏\ni=t\nµ(ei | æ<i, do(ai)) (13)\n(8) =\nm ∑\nk=t\n∑\net:k\nu(ei)\nk ∏\ni=t\n∑\ns∈S\nµ(s | æ<i)µ(ei | s,æ<iai) (14)\nProof of Proposition 7. By the definition (5) of do(πt:m),\nµ(et | æ<t, do(πt:m)) = ∑\net+1:m\nµ(et:m | æ<t, do(at := π(æ<t), . . . , am := π(æ<m)))\n= ∑\ns,et+1:m\nµ(s | æ<t)µ(et:m | s,æ<t, do(π(æ<t), . . . , π(æ<m)))\n(1) = ∑\ns,et+1:m\nµ(s | æ<t)\nm ∏\ni=t\nµ(ei | s,æ<iπ(æ<i))\n= ∑\ns\nµ(s | æ<t)µ(et | s,æ<tπ(æ<t))\n(8) = µ(et | æ<t, do(π(æ<t)))\nThe second equality follows from the equivalence P ( · ) = ∑\ns P (s)P ( · | s) applied to the distribution µ( · | æ<t, do(at := π(æ<t), . . . , am := π(æ<m))), and the third equality by (repeated) application of (1) to µ(æt:m | s,æ<t) = ∏m\ni=t µ(ai | s,æ<i)µ(ei | s,æ<iai)."
    }, {
      "heading" : "4 Discussion",
      "text" : "Our paper is a first stab at the problem of how physicalistic agents should make sequential decisions. CDT and EDT provide an existing basis for non-dualistic decision making, which we extended to the sequential setting. There are two natural ways for making sequential evidential decisions: do I update my beliefs about the hidden state based on my next action (‘what I do next’, SAEDT) or my whole policy (‘the kind of agent I am’, SPEDT)? By Proposition 7, this distinction does not exist for causal decision theory, because with that theory the agent does not consider its own actions evidence at all. Therefore we have only one version of sequential causal decision theory, SCDT.\nTo illustrate the differences between the decision theories, we discussed three variants of Newcomb’s problem (Example 1, Example 8, and Example 9) and two variants of the toxoplasmosis problem (Example 2 and Example 5). The formal specification of these examples can be found in Appendix A. We implemented SCDT, SAEDT, and SPEDT; Table 1 shows their behavior on those\nexamples.4\nSo which decision theory is better? The answer to this question depends on which decision you consider to be correct (or even rational) in each of the problems. We posit that ultimately, what counts is not whether your decision algorithm is theoretically pleasing, but whether you win. Winning means getting the most utility. If maximizing utility involves making crazy decisions, then this is what you should do!\nIn Newcomb’s problem, winning means one-boxing, because you end up richer. In the toxoplasmosis problem, winning means petting the kitten, because that yields more utility. (S)CDT performs suboptimally in the Newcomb variations, while the evidential decision theories perform suboptimally in the toxoplasmosis variations. This entails that neither CDT nor EDT are the final answer to the problem of non-dualistic decision making.\nFurthermore, neither CDT nor EDT agents are fully physicalistic: they do not model the environment to contain themselves [SF14b]. For example, when playing a prisoner’s dilemma against your own source code [SF15], your opponent defects if and only if you defect. This logical connection between your action and your opponent’s is disregarded in the formalization based on causal graphical models that we discuss here because it is not causal.\nTimeless decision theory [Yud10] and updateless decision theory [SF14b] are recent attempts of more physicalistic decision theories. However, so far both have eluded explicit formalization [SF15]. We conclude that finding a physicalistic decision theory remains an important open problem in artificial intelligence research.\nAcknowledgements. This work was in part supported by ARC grant DP120100950. It started at a MIRIxCanberra workshop sponsored by the Machine Intelligence Research Institute. Mayank Daswani and Daniel Filan contributed in the early stages of this paper and we thank them for interesting discussions and helpful suggestions. We also thank Nate Soares for useful feedback."
    }, {
      "heading" : "A Examples",
      "text" : "This section contains the formal calculations for Example 1, Example 2, Example 5, Example 8, and Example 9. These calculations are also available as Python code at http://jan.leike.name/.\nExample 11 (Newcomb’s Problem). This is a formalization of Example 1.\n• S := {E,F} where E means the opaque box is empty and F means the opaque box is full\n• A := {B1, B2} where B1 means one-boxing and B2 means two-boxing\n• E := {O0, OT , OM , OMT }\n• u(O0) := 0, u(OT ) := 1,000, u(OM ) := 1,000,000, u(OMT ) := 1,001,000\nLet ε > 0 be a small constant denoting the accuracy of the predictor. Because the environment has to assign non-zero probability to all actions, ε must be strictly positive. The environment’s distribution µ is defined as follows.\nµ(E) = µ(F ) = 0.5 µ(OT | E,B2) = 1\nµ(B1 | F ) = µ(B2 | E) = 1− ε µ(O0 | E,B1) = 1\nµ(B1 | E) = µ(B2 | F ) = ε µ(OMT | F,B2) = 1\nµ(OM | F,B1) = 1\nBy Bayes’ rule,\nµ(F | B1) = µ(B1 | F )µ(F ) ∑\ns∈S µ(B1 | s)µ(s) =\n1 2 (1− ε) 1 2 (1− ε) + 1 2ε = (1− ε)\nwhich also gives µ(E | B1) = ε. Similarly, µ(F | B2) = ε and µ(E | B2) = 1− ε. For EDT we use equation (EDT) to compute the value of an action. Since the percept e1 is generated deterministically, µ(e | s, a) only attains values 0 or 1. We therefore omit it in the calculation below. For action B1 we get\nV evi,B1 µ,1 :=\n∑\ne∈E\nµ(e | B1)u(e) = ∑\ne∈E\n∑\ns∈S\nµ(e | s,B1)µ(s | B1)u(e)\n= µ(E | B1)u(O0) + µ(F | B1)u(OM )\n= ε · 0 + (1− ε) · 1, 000, 000\nFor action B2 we get\nV evi,B2 µ,1 :=\n∑\ne∈E\nµ(e | B2)u(e) = ∑\ne∈E\n∑\ns∈S\nµ(e | s,B2)µ(s | B2)u(e)\n= µ(E | B2)u(OT ) + µ(F | B2)u(OMT )\n= (1 − ε) · 1, 000 + ε · 1, 001, 000\n= 1, 000 + ε · 1, 000, 000\nFor ε < 49.95 (just slightly better than random guessing), we get that EDT favors B1 over B2:\nV evi,B1 µ,1 = (1 − ε) · 1, 000, 000 > 500, 500 > 1, 000 + ε · 1, 000, 000 = V evi,B2 µ,1\nFor CDT we use equation (CDT) to compute the value of an action. For action B1 we get\nV cau,B1 µ,1 :=\n∑\ne∈E\nµ(e | do(B1))u(e) = ∑\ne∈E\n∑\ns∈S\nµ(e | s,B1)µ(s)u(e)\n= µ(E)u(O0) + µ(F )u(OM )\n= 0.5 · 0 + 0.5 · 1, 000, 000 = 500, 000\nFor action B2 we get\nV cau,B2 µ,1 :=\n∑\ne∈E\nµ(e | do(B2))u(e) = ∑\ne∈E\n∑\ns∈S\nµ(e | s,B2)µ(s)u(e)\n= µ(E)u(OT ) + µ(F )u(OMT )\n= 0.5 · 1, 000 + 0.5 · 1, 001, 000 = 500, 500\nWe get that CDT favors B2 over B1 regardless of the prediction accuracy ε:\nV evi,B1 µ,1 = 500, 000 < 500, 500 = V evi,B2 µ,1\nMoreover, CDT prefers B2 regardless of the prior over µ(E). Two-boxing is the dominant action because it yields $1,000 more regardless of the hidden state.\nExample 12 (Newcomb with Looking). This is a formalization of Example 9; it extends Example 11.\nIn the first time step, the agent gets to choose between looking into the box (L) and not looking (N). If the agent looks, the subsequent percept will be E or F , depending on whether the box is empty (E) or full (F ). If the agent does not look, the subsequent percept will be 0. All three of these percepts E, F , and 0 have zero utility.\nIn the second time step the agent chooses to one-box (B1) or to two-box (B2). The payoffs are then based on the boxes’ contents as in Example 11.\n• S := {E,F} where E means the opaque box is empty and F means the opaque box is full\n• A := {B1, B2} where B1 means one-boxing and B2 means two-boxing, L := B1 means looking into the box and N := B2 means not looking (the set of actions has to be the same for all time steps)\n• E := {E,F, 0, O0, OT , OM , OMT }\n• u(O0) := 0, u(OT ) := 1,000, u(OM ) := 1,000,000, u(OMT ) := 1,001,000, u(E) := u(F ) := u(0) := 0\nLet ε > 0 be a small constant denoting the prediction accuracy. Because the environment has to assign non-zero probability to all actions, ε must be strictly positive. The environment’s distribution µ is defined as follows. Question marks\nstand for single actions or percepts whose value is irrelevant.\nµ(E) = µ(F ) = 0.5 µ(E | E,L) = 1\nµ(L | F ) = µ(L | E) = 0.5 µ(0 | E,N) = 1\nµ(N | F ) = µ(N | E) = 0.5 µ(F | F,L) = 1\nµ(B1 | E, ??) = ε µ(0 | F,N) = 1\nµ(B1 | F, ??) = 1− ε µ(O0 | E, ??B1) = 1\nµ(B2 | E, ??) = 1− ε µ(OT | E, ??B2) = 1\nµ(B2 | F, ??) = ε µ(OM | F, ??B1) = 1\nµ(OMT | F, ??B2) = 1\nThe environment’s game tree is given as follows, where dashed lines connect states indistinguishable by the agent (also known as information sets):\nE\nL E B1 0\nB2 1,000\nN 0 B1 0\nB2 1,000\nF\nL F B1 1,000,000\nB2 1,001,000\nN 0 B1 1,000,000\nB2 1,001,000\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n1\n1\n1\n1\nε\n1− ε\nε\n1− ε\n1− ε\nε\n1− ε\nε\nUsing Bayes’ rule, we calculate the following conditional probabilities of the hidden state given a history a1 or a1e1a2:\n0.5 = µ(E | L) = µ(F | L) = µ(E | N) = µ(F | N)\n1 = µ(E | LEB1) = µ(E | LEB2) = µ(F | LFB1) = µ(F | LFB1)\nε = µ(E | N0B1) = µ(F | N0B2)\n1− ε = µ(E | N0B2) = µ(F | N0B1)\nNext, we write out the formula for SAEDT for a horizon of 2 based on (10). The first percept has no utility, which simplifies the equation.\nV aev,π µ,2 =\n∑\ne1:2\nu(e2)\n(\n∑\ns∈S\nµ(s | a1)µ(e1 | s, a1)\n)(\n∑\ns∈S\nµ(s | æ1a2)µ(e2 | s,æ1a2)\n)\nwhere a1 = π(ǫ) and a2 = π(æ1). The formula for SPEDT for a horizon of 2 based on (12) is as follows.\nV pev,π µ,2 =\n∑\ne1:2\nu(e2)\n∑\ns∈S µ(sa1e1π(a1e1)) ∑\ns∈S\n∑\ne∈E µ(sa1eπ(a1e))\n∑\ns∈S\nµ(s | æ1π2)µ(e2 | s,æ1a2)\nwith π1:2 and π2 defined according to (4). The formula for SCDT for a horizon of 2 based on (14) is as follows.\nV cau,π µ,2 =\n∑\ne1:2\nu(e2)\n(\n∑\ns∈S\nµ(s)µ(e1 | s, a1)\n)(\n∑\ns∈S\nµ(s | æ1)µ(e2 | s,æ1a2)\n)\nwhere a1 = π(ǫ) and a2 = π(æ1). There are six different possible policies:\n• Look and always one-box (curious one-boxer)\n• Look and always two-box (curious two-boxer)\n• Don’t look and one-box (incurious one-boxer)\n• Don’t look and two-box (incurious two-boxer)\n• Look and one-box iff the box is empty (paradox-lover)\n• Look and one-box iff the box full (fatalistic)\nUsing the formulas above we can calculate their value. We use ε := 0.01.\nV aev,π µ,2 V pev,π µ,2 V cau,π µ,2\nCurious one-boxer 500,000 990,000 500,000 Curious two-boxer 501,000 11,000 501,000 Incurious one-boxer 990,000 990,000 500,000 Incurious two-boxer 11,000 11,000 501,000 Paradox-lover 500,500 500,500 500,500 Fatalistic 500,500 500,500 500,500\nThe highest values are displayed in italics. The incurious one-boxer has the highest action-evidential value. The curious one-boxer and the incurious oneboxer have the highest policy-evidential value. However, of these two policies only the incurious one-boxer is a time-consistent policy for SPEDT, because the agent wants to two-box after looking into the box:\nV aev,B1 µ,1 (LF ) = V pev,B1 µ,1 (LF ) = 1, 000, 000 V aev,B2 µ,1 (LF ) = V pev,B2 µ,1 (LF ) = 1, 001, 000 V aev,B1 µ,1 (LE) = V pev,B1 µ,1 (LE) = 0 V aev,B2 µ,1 (LE) = V pev,B2 µ,1 (LE) = 1, 000\nThe curious two-boxer and the incurious two-boxer have the highest causal value, and they are both time-consistent for SCDT.\nExample 13 (Newcomb with Precommitment). This is a formalization of Example 8, it extends Example 11.\nIn the first time step, the agent gets to choose between signing the contract (S) and not signing (N). If the agent signs, the subsequent percept will be C, which costs $300,000, and the prediction will be updated to one-boxing. If the agent does not sign, the subsequent percept will be 0 with zero utility.\nIn the second time step the agent chooses to one-box (B1) or to two-box (B2). The payoffs are then based on the boxes’ contents as in Example 11. If the agent signed the contract and choses two boxes, this incurs an additional cost of $2,000.\n• S := {E,F} where E means the opaque box is empty and F means the opaque box is full\n• A := {B1, B2} where B1 means one-boxing and B2 means two-boxing, S := B1 means signing the contract and N := B2 means not signing (the set of actions has to be the same for all time steps)\n• E := {C, 0, O0, OT , O−T , OM , OMT , OM−T }\n• u(O0) := 0, u(OT ) := 1, 000, u(O−T ) := −1, 000 u(OM ) := 1, 000, 000, u(OMT ) := 1, 001, 000, u(OM−T ) := 999, 000, u(C) := −300, 000, u(0) := 0\nLet ε > 0 be a small constant denoting the prediction accuracy. Because the environment has to assign non-zero probability to all actions, ε must be strictly positive. The environment’s distribution µ is defined as follows. Question marks stand for single actions or percepts whose value is irrelevant.\nµ(E) = µ(F ) = 0.5 µ(C | E, S) = 1\nµ(S | F ) = µ(S | E) = 0.5 µ(0 | E,N) = 1\nµ(N | F ) = µ(N | E) = 0.5 µ(C | F, S) = 1\nµ(B1 | E,N0) = ε µ(0 | F,N) = 1\nµ(B1 | F,N0) = 1− ε µ(O0 | E,N0B1) = 1\nµ(B2 | E,N0) = 1− ε µ(OT | E,N0B2) = 1\nµ(B2 | F,N0) = ε µ(OM | F,N0B1) = 1\nµ(B2 | ?, SC) = ε µ(OMT | F,N0B2) = 1\nµ(B1 | ?, SC) = 1− ε µ(OM | E, SCB1) = 1\nµ(OM−T | E, SCB2) = 1\nThe environment’s game tree is given as follows:\nE S C B1 700,000 B2 699,000\nN 0 B1 0\nB2 1,000\nF\nS C B1 700,000\nB2 699,000\nN 0 B1 1,000,000\nB2 1,001,000\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n1\n1\n1\n1\nε\n1− ε\n1− ε\nε\n1− ε\nε\n1− ε\nε\nThere are four different possible policies:\n• Sign the contract and one-box (signing one-boxer)\n• Sign the contract and two-box (signing two-boxer)\n• Don’t sign the contract and one-box (refusing one-boxer)\n• Don’t sign the contract and two-box (refusing two-boxer)\nUsing the formulas from Example 12 we can calculate their value. We use ε := 0.01.\nV aev,π µ,2 V pev,π µ,2 V cau,π µ,2\nSigning one-boxer 700,000 700,00 700,000 Signing two-boxer 699,000 699,000 699,000 Refusing one-boxer 990,000 990,000 500,000 Refusing two-boxer 11,000 11,000 501,000\nThe highest values are displayed in italics. Both SAEDT and SPEDT refuse the contract: the refusing one-boxer has the highest action-evidential and the highest policy-evidential value. SCDT signs the contract and then one-boxes: the signing one-boxer has the highest causal value.\nExample 14 (Toxoplasmosis). This is a formalization of Example 2.\n• S := {T,H} where T means having the toxoplasmosis parasite and H means being healthy\n• A := {P,N} where P means petting and N means not petting\n• E := {P&T,N&T, P&H,N&H}where the percepts just reflect the action and hidden state\n• u(P&T ) := −9, u(N&T ) := −10, u(P&H) := 1, u(N&H) := 0 where petting gives a utility of 1 and suffering from the parasite gives a utility of −10\nThe environment’s distribution µ is defined as follows.\nµ(T ) = µ(H) = 0.5 µ(P&T | P, T ) = 1\nµ(P | T ) = 0.8 µ(N&T | N, T ) = 1\nµ(N | T ) = 0.2 µ(P&H | P,H) = 1\nµ(P | H) = 0.2 µ(N&H | N,H) = 1\nµ(N | H) = 0.8\nUsing Bayes’ rule, we calculate the following conditional probabilities.\nµ(T | P ) = 0.8 µ(H | P ) = 0.2 µ(T | N) = 0.2 µ(H | N) = 0.8\nWe consider EDT first. Since the percept e1 is generated deterministically, µ(e | s, a) only attains values 0 or 1. We therefore omit it in the calculation below. For action P (petting) we get\nV evi,P µ,1 :=\n∑\ne∈E\nµ(e | P )u(e) = ∑\ne∈E\n∑\ns∈S\nµ(e | s, P )µ(s | P )u(e)\n= µ(T | P )u(T&P ) + µ(H | P )u(P&H)\n= 0.8 · (−9) + 0.2 · 1 = −7\nFor action N (not petting) we get\nV evi,N µ,1 :=\n∑\ne∈E\nµ(e | N)u(e) = ∑\ne∈E\n∑\ns∈S\nµ(e | s,N)µ(s | N)u(e)\n= µ(T | N)u(T&N) + µ(H | N)u(H&N)\n= 0.2 · (−10) + 0.8 · 0 = −2\nTherefore we get that EDT favors N over P :\nV evi,P µ,1 = −7 < −2 = V evi,N µ,1\nFor CDT we get for action P (petting)\nV cau,P µ,1 :=\n∑\ne∈E\nµ(e | do(P ))u(e) = ∑\ne∈E\n∑\ns∈S\nµ(e | s, P )µ(s)u(e)\n= µ(T )u(T&P ) + µ(N)u(N&P )\n= 0.5 · (−9) + 0.5 · 1 = −4\nFor action N (not petting) we get\nV cau,N µ,1 :=\n∑\ne∈E\nµ(e | do(N))u(e) = ∑\ne∈E\n∑\ns∈S\nµ(e | s,N)µ(s)u(e)\n= µ(T )u(T&N) + µ(H)u(H&N)\n= 0.5 · (−10) + 0.5 · 0 = −5\nWe get that CDT favors P over N :\nV evi,P µ,1 = −4 > −5 = V evi,N µ,1\nExample 15 (Sequential Toxoplasmosis). We here formalize a version of Example 5. First the agent chooses whether to go to the doctor. Going to the doctor incurs a fee, but removes the risk of getting sick. Agents that do not go to the doctor have a chance of meeting a kitten. If they meet it, they can choose to pet it or not; infected agents are more likely to pet the kitten. The example is intended to elucidate the difference between SAEDT and SPEDT, whose decisions we will calculate in detail. We will not calculate the action of SCDT.\n• S := {T (oxoplasmosis), H(ealthy)}.\n• A := {Y (es), N(o)}. In this example, an action is taken twice. We use Y1 and Y2, and N1 and N2, to distinguish between the first and the second action.\n• E := {C(ured), K(itten), S(ick, not pet kitten), s(ick, pet kitten), P (et, not sick), 0(neutral)}\n• u(C) = −4, u(K) := 0, u(S) := −10, u(s) := −9, u(P ) := 1, and u(0) = 0.\nThe environment’s game tree is given as follows, where dashed lines connect states indistinguishable by the agent.\nH\nN1 K (0)\nN2 0 (0)\nY2 P (1)\nY1 C (−4)\nT\nN1\nK (0)\nN2 S (−10)\nY2 s (−9)\nS (−10)\nY1 C (−4)\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n0.2\n0.8 0.8\n0.2\n0.2\n0.8\nFirst, the environment chooses whether to infect the agent or not with the parasite with probability 0.5. The agent then decides whether to see the doctor. If the agent sees the doctor, this incurs a (utility) fee of −4, but the agent will not be sick. If the agent does not see the doctor, there will be a kitten with probability 0.2 (or 1) and the agent will pet it with probability 0.8 (or 0.2) if the parasite is present (or not). If there is no kitten, the next percept is S or 0 depending on whether the agent is infected or not. The agent gets −10 utility if infected and did not see the doctor, and gets +1 utility for petting the kitten.\nWe want to compare the choices of SAEDT and SPEDT. Their two-step value functions are\nV aev,π µ,2 =\n∑\ne1\nµ(e1 | a1) ( u(e1) + V aev,π µ,2 (a1e1) )\nV pev,π µ,2 =\n∑\ne1\nµ(e1 | π1:2) ( u(e1) + V pev,π µ,2 (a1e1) )\nwhere the second step value functions\nV aev,π µ,2 (a1e1) = V pev,π µ,2 (a1e1) =\n∑\ne2\nµ(e2 | a1e1a2) · u(e2)\nare the same for both decision theories. They only differ by assigning probability µ(e1 | a1) and µ(e1 | π1:2) to the first percept, respectively.\nSince not petting is always better than petting for evidential agents (the evidence towards not having the disease weighs stronger than the extra utility), the only policies that are potentially optimal and time consistent are π1 := N1N2 and π2 := Y1.\nFirst percept. For π1 the occurring action-evidential quantities µ(e1 | a1) are\nµ(N1) = ∑\ns∈S\nµ(s,N1) = µ(T,N1) + µ(H,N1) = 1\n4 +\n1 4 = 1 2\nµ(e1 = S | N1) =\n∑\ns∈S µ(s,N1S)\nµ(N1) =\nµ(T,N1S)\nµ(N1) =\n1 2 · 1 2 · 4 5\n1 2\n= 2\n5\nµ(e1 = K | N1) = 1− µ(S | N1) = 3\n5\nand the occurring policy-evidential quantities µ(e1 | π1:2) are\nµ(N1N2) = ∑\ns,e1,e2\nµ(s,N1e1N2e2)\n= µ(T,N1KN2S) + µ(T,N1SN20) + µ(H,N1KN20)\n= 1\n100 +\n1\n10 +\n1 5 = 31 100\nµ(e1 = K | N1N2) =\n∑\ns,e2 µ(s,N1KN2e2)\nµ(N1, N2)\n= µ(T,N1KN2S) + µ(H,N1KN20)\nµ(N1N2) =\n1 100 + 1 5\n31 100\n= 21\n31\nµ(e1 = S | N1N2) = 1− µ(K | N1N2) = 20\n31\nThe policy π2 = {Y1} always goes to the doctor for the treatment, and so\nµ(e1 = C | Y1) = 1\nfor both AESDT and PESDT.\nSecond percept. With the policy π2, the second percept is always empty. Under π1, the only action sequence that can reach the second percept is N1KN2\nµ(N1KN2) = ∑\ns\nµ(s,N1KN2) = µ(T,N1KN2) + µ(H,N1KN2)\n= 1\n100 +\n1 5 = 21 100\nµ(e2 = S | N1KN2) =\n∑\ns µ(s,N1KN2S)\nµ(N1KN2) =\nµ(T,N1KN2S)\nµ(N1KN2) = 1 100 21 100 = 1 21 .\nValue Functions. We start by evaluating the recursive definition from the second time step. The second step value functions are 0 for π1 and for the history N1S for π2. For the history N1K, both SAEDT and PAEDT assign the following identical value to π2:\nV aev,π1 µ,2 (N1K) = V pev,π µ,2 (N1K) =\n∑\ne2\nµ(e2 | N1KN2) · u(e2)\n= µ(e2 = S | N1KN2) · u(S) + µ(e2 = 0 | N1KN2) · u(0)\n= 1\n21 · (−10) +\n20 21 · 0 = − 10 21\nThe first step value functions now evaluates to:\nV aev,π1 µ,2 =\n∑\ne1\nµ(e1 | N1) · ( u(e1) + V aev,π1 µ,2 (N1e1) )\n= µ(S | N1) · (u(S) + V aev,π1 µ,2 (N1S))\n+ µ(K | N1) · (u(K) + V aev,π1 µ,2 (N1K))\n= 2\n5 · (−10 + 0) +\n3 5 · (0 − 10 21 ) = − 30 7 ≈ −4.3\nV pev,π1 µ,2 =\n∑\ne1\nµ(e1 | N1) · ( u(e1) + V pev,π1 µ,2 (N1e1) )\n= µ(S | N1N2) · (u(S) + V pev,π1 µ,2 (N1S))\n+ µ(K | N1N2) · (u(K) + V pev,π1 µ,2 (N1K))\n= 10\n31 · (−10 + 0) +\n21 31 · (0− 10 21 ) = − 110 31 ≈ −3.5\nMeanwhile, the value of π2 is\nV aev,π2 µ,2 = V aev,π2 µ,2 =\n∑\ne1\nµ(e1 | N1) ( u(e1) + V aev,π2 µ,2 (N1e1) )\n= µ(C | Y1)(u(C) + V aev,π2 µ,2 (Y1C)) = 1 · (−4 + 0) = −4\nThat is, V aev,π1µ,2 < V aev,π2 µ,2 = V pev,π2 µ,2 < V pev,π1 µ,2 . So SPEDT but not SAEDT prefers π1 to π2. In other words, an SAEDT agent considers himself sufficiently likely to have the parasite to adopt policy π2 of seeing the doctor. The SPEDT agent relies on the fact that he would pet the cat in case he saw it, and takes that as evidence of not being sick. Hence he will instead adopt policy π1 of not seeing the doctor."
    } ],
    "references" : [ {
      "title" : "Decision and Causality",
      "author" : [ "Arif Ahmed. Evidence" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "Ahm14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Technical report",
      "author" : [ "Alex Altair. A comparison of decision algorithms on Newcomblike problems" ],
      "venue" : "Machine Intelligence Research Institute,",
      "citeRegEx" : "Alt13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Superintelligence: Paths",
      "author" : [ "Nick Bostrom" ],
      "venue" : "Dangers, Strategies. Oxford University Press,",
      "citeRegEx" : "Bos14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Normative theories of rational choice: Expected utility",
      "author" : [ "Rachael Briggs" ],
      "venue" : "Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Fall 2014 edition,",
      "citeRegEx" : "Bri14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The Philosophical Review",
      "author" : [ "Andy Egan. Some counterexamples to causal decision theory" ],
      "venue" : "pages 93–114,",
      "citeRegEx" : "Ega07",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "pages 125–162",
      "author" : [ "Allan Gibbard andWilliam L Harper. Counterfactuals", "two kinds of expected utility. In Foundations", "Applications of Decision Theory" ],
      "venue" : "Springer,",
      "citeRegEx" : "GH78",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "University of Chicago Press",
      "author" : [ "Richard C Jeffrey. The Logic of Decision" ],
      "venue" : "2nd edition,",
      "citeRegEx" : "Jef83",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "The Foundations of Causal Decision Theory",
      "author" : [ "James M Joyce" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "Joy99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Australasian Journal of Philosophy",
      "author" : [ "David Lewis. Causal decision theory" ],
      "venue" : "59(1):5–30,",
      "citeRegEx" : "Lew81",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Program equilibrium in the prisoner’s dilemma via Löb’s theorem",
      "author" : [ "Patrick LaVictoire", "Benja Fallenstein", "Eliezer Yudkowsky", "Mihaly Barasz", "Paul Christiano", "Marcello Herreshoff" ],
      "venue" : "AAAI Workshop on Multiagent Interaction without Prior Coordination,",
      "citeRegEx" : "LFY14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Theoretical Computer Science",
      "author" : [ "Tor Lattimore", "Marcus Hutter. General time consistent discounting" ],
      "venue" : "519:140–154,",
      "citeRegEx" : "LH14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Newcomb’s problem and two principles of choice",
      "author" : [ "Robert Nozick" ],
      "venue" : "Essays in honor of Carl G. Hempel, pages 114–146. Springer,",
      "citeRegEx" : "Noz69",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "pages 209–218",
      "author" : [ "Laurent Orseau", "Mark Ring. Space-time embedded intelligence. In Artificial General Intelligence" ],
      "venue" : "Springer,",
      "citeRegEx" : "OR12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Cambridge University Press",
      "author" : [ "Judea Pearl. Causality" ],
      "venue" : "2nd edition,",
      "citeRegEx" : "Pea09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Technical report",
      "author" : [ "Stuart Russell", "Daniel Dewey", "Max Tegmark", "Janos Kramar", "Richard Mallah. Research priorities for robust", "beneficial artificial intelligence" ],
      "venue" : "Future of Life Institute,",
      "citeRegEx" : "RDT15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Prentice Hall",
      "author" : [ "Stuart J Russell", "Peter Norvig. Artificial Intelligence. A Modern Approach" ],
      "venue" : "3rd edition,",
      "citeRegEx" : "RN10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The Foundations of Statistics",
      "author" : [ "Leonard J Savage" ],
      "venue" : "Dover Publications,",
      "citeRegEx" : "Sav72",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Aligning superintelligence with human interests: A technical research agenda",
      "author" : [ "Nate Soares", "Benja Fallenstein" ],
      "venue" : "Technical report, Machine Intelligence Research Institute,",
      "citeRegEx" : "SF14a",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Technical report",
      "author" : [ "Nate Soares", "Benja Fallenstein. Toward idealized decision theory" ],
      "venue" : "Machine Intelligence Research Institute,",
      "citeRegEx" : "SF14b",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "In Artificial General Intelligence",
      "author" : [ "Nate Soares", "Benja Fallenstein. Counterpossibles as necessary for decision theory" ],
      "venue" : "Springer,",
      "citeRegEx" : "SF15",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "1 Introduction In artificial-intelligence problems an agent interacts sequentially with an environment by taking actions and receiving percepts [RN10].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 12,
      "context" : "But often it is not true: real-world agents are embedded in (and computed by) the environment [OR12], and then a physicalistic model is more appropriate.",
      "startOffset" : 94,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "This link can be used for uncoordinated cooperation [LFY14].",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "This is known as Savage decision theory [Sav72].",
      "startOffset" : 40,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "Formally, the environment is given by a probability distribution P over the hidden state, the action, and the percept that factors according to a causal graph [Pea09].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : "Causal decision theory has several formulations [GH78, Lew81, Sky82, Joy99]; we use the one given in [Sky82], with Pearl’s calculus of causality [Pea09].",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 11,
      "context" : "Example 1 (Newcomb’s Problem [Noz69]).",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Example 2 (Toxoplasmosis Problem [Alt13]).",
      "startOffset" : 33,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "Is that the reason there are so few wealthy philosophers? Historically, this problem has been known as the smoking lesion problem [Ega07].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "The choice of discounting can lead to time inconsistency: a sliding fixed-size horizon is time inconsistent, but a fixed finite lifetime is time consistent [LH14].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : "A general principle for how to do this is still an open question [SF14b].",
      "startOffset" : 65,
      "endOffset" : 72
    } ],
    "year" : 2015,
    "abstractText" : "Moving beyond the dualistic view in AI where agent and environment are separated incurs new challenges for decision making, as calculation of expected utility is no longer straightforward. The non-dualistic decision theory literature is split between causal decision theory and evidential decision theory. We extend these decision algorithms to the sequential setting where the agent alternates between taking actions and observing their consequences. We find that evidential decision theory has two natural extensions while causal decision theory only has one.",
    "creator" : "LaTeX with hyperref package"
  }
}