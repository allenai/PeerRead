{
  "name" : "1105.5516.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Ontology Alignment at the Instance and Schema Level",
    "authors" : [ "Fabian M. Suchanek", "Serge Abiteboul", "Pierre Senellart" ],
    "emails" : [ "fabian@xsuchanek.name", "serge.abiteboul@xinria.fr", "pierre@xsenellart.com", "fabian@xsuchanek.name", "serge.abiteboul@xinria.fr", "pierre@xsenellart.com", "fabian@xsuchanek.name", "serge.abiteboul@xinria.fr", "pierre@xsenellart.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Alignment, Ontology, Disambiguation, RDFS, Instances, Relations\nAcknowledgement: This work has been partially funded by the ERC grant Webdam.\nAlignement d’ontologies\nau niveau\ndes instances et des schémas\nFabian M. Suchanek INRIA Saclay, Projet Webdam, fabian@xsuchanek.name\nSerge Abiteboul INRIA Saclay & ENS Cachan, Projet Webdam, serge.abiteboul@xinria.fr\nPierre Senellart Institut Télécom ; Télécom ParisTech ; CNRS LTCI, pierre@xsenellart.com\nRésumé. Nous présentons PARIS, un système automatique d’alignement d’ontologies. PARIS réalise non seulement l’alignement d’instances de deux ontologies, mais aussi l’alignement des relations et des classes. L’alignement d’instances et de relations s’enrichissent mutuellement. Notre approche, qui fournit ainsi une solution holistique au problème d’alignement d’ontologies, repose sur un modèle probabiliste. De ce fait, notre algorithme ne nécessite ni paramètre arbitraire ni réglage manuel. Nous démontrons l’efficacité de PARIS à travers des expériences sur des ontologies diverses ; nous obtenons des niveaux de précisions d’approximativement 80% pour l’alignement de deux des plus grandes ontologies publiquement disponibles.\nMots-clefs : Alignement, Ontologie, Désambiguation, RDFS, Instances, Relations\nRemerciements : Ce travail a été financé par le project ERC Webdam."
    }, {
      "heading" : "1. Introduction",
      "text" : "Motivation. An ontology is a formal collection of world knowledge. In this paper, we use the word ontology in a very general sense, to mean both the schema (classes and relations), and the instances with their assertions. In recent years, the success of Wikipedia and algorithmic advances in information extraction have facilitated the automated construction of large general-purpose ontologies. Notable endeavors of this kind include DBpedia [1], KnowItAll [8], WikiTaxonomy [24], and yago [28], as well as commercial services such as freebase.com, trueknowledge.com, and wolframalpha.com. These ontologies are accompanied by a growing number of knowledge bases1 in a wide variety of domains including: music2, movies3, geographical data4, publications5, medical and biological data6, or government data7. Many of these ontologies contain complementing data. For instance, a general ontology may know who discovered a certain enzyme, whereas a biological database may know its function and properties. However, since the ontologies generally use different terms (identifiers) for an entity, their information cannot be easily brought together. In this respect, the ontologies by themselves can be seen as isolated islands of knowledge. The goal of the Semantic Web vision is to interlink them, thereby creating one large body of universal ontological knowledge [3, 4]. This goal may be seen as a much scaled-up version of record linking, with challenges coming from different dimensions: (i) unlike in record linkage, both instances and schemas should be reconciled; (ii) the semantics of the ontologies have to be respected; (iii) the ontologies are typically quite large and complex. Moreover, we are interested in performing the alignment in a fully automatic manner, and avoid tedious tuning or parameter settings. A number of recent research have investigated this problem. There have been many works on entity resolution (i.e., considering the A-Box only) [10, 23, 25, 26, 29, 15, 16]. In another direction, much research has focused on schema alignment (i.e., considering the T-Box only) [12, 19, 2, 18, 32]. However, in recent years, the landscape of ontologies has changed dramatically. Today’s ontologies often contain both a rich schema and, at the same time, a huge number of instances, with dozens of millions of assertions about them. To fully harvest the mine of knowledge they provide, their alignment has to be built on cross-fertilizing the alignments of both instances and schemas. In this paper, we propose a new, holistic algorithm for aligning ontologies. Our approach links not just related entity or relationship instances, but also related classes and relations, thereby capturing the fruitful interplay between schema and instance matching. Our final aim is to discover and link identical entities automatically across ontologies on a large scale, thus allowing ontologies to truly complement each other.\n1http://www.w3.org/wiki/DataSetRDFDumps 2http://musicbrainz.org/ 3http://www.imdb.com/ 4http://www.geonames.org/ 5http://www.informatik.uni-trier.de/~ley/db 6http://www.uniprot.org/ 7http://www.govtrack.us/, http://source.data.gov.uk/data/\nContribution. The contribution of the present paper is three-fold: 1. We present paris8, a probabilistic algorithm for aligning instances, classes, and\nrelations simultaneously across ontologies. 2. We show how this algorithm can be implemented efficiently and that it does not\nrequire any tuning 3. We prove the validity of our approach through experiments on real-world ontolo-\ngies. The paper is organized as follows. Section 2 provides an overview of related work.\nWe then introduce some preliminaries in Section 3. Section 4 describes our probabilistic algorithm and Section 5 its implementation. Section 6 discusses experiments. To ease the reading, some technical discussions are postponed to the appendix."
    }, {
      "heading" : "2. Related Work",
      "text" : "Overview. The problem of ontology matching has its roots in the problem of identifying duplicate entities, which is also known as record linkage, duplicate detection, or coreference resolution. This problem has been extensively studied in both database and natural language processing areas [5, 7]. These approaches are less applicable in the context of ontologies for two reasons. First, they do not consider the formal semantics that ontologies have (such as the subclassOf -taxonomy). Second, they focus on the alignment of instances and do not deal with the alignment of relations and classes. There are a number of surveys and analyses that shed light on the problem of record linking in ontologies. Halpin et al. [13] provide a good overview of the problem in general. They also study difficulties of existing sameAs-links. These links are further analyzed by Ding et al. [6]. Glaser, Jaffri, and Millard [11] propose a framework for the management of co-reference in the Semantic Web. Hu et al. [17] provide a study on how matches look in general.\nSchema Alignment. Traditional approaches to ontology matching have focused mostly either on aligning the classes (the “T-Box”) or on matching instances (the “A-Box”). The approaches that align the classes are manifold, using techniques such as sense clustering [12], lexical and structural characteristics [19], or composite approaches [2]. Unlike paris, these approaches can only align classes and do not consider the alignment of relations and instances. Most similar to our approach in this field are [18] and [32], which derive class similarity from the similarities of the instances. Both approaches consider only the equivalence of classes and do not compute subclasses, as does paris. Furthermore, neither can align relations or instances.\nInstance Matching. There are numerous approaches to match instances of one ontology to instances of another ontology. Ferrara, Lorusso, and Montanelli [10] introduce\n8Probabilistic Alignment of Relations, Instances, and Schema\nthis problem from a philosophical point of view. Different techniques are being used, such as exploiting the terminological structure [23], logical deduction [25], or a combination of logical and numerical methods [26]. The Sig.ma engine [29] uses heuristics to match instances. Perhaps closest to our approach is [15], which introduces the concept of functionality. Different from their approach, paris does not require an additional smoothening factor. The silk framework [31] allows specifying manual mapping rules. The ObjectCoref approach by Hu, Chen, and Qu [16] allows learning a mapping between the instances from training data. With paris, we aim at an approach that uses neither manual input nor training data. We compare some of the results of ObjectCoref to that of paris on the datasets of the ontology alignment evaluation initiative [9] in Section 6. Hogan [14] matches instances and propose to use these instances to compute the similarity between classes, but provides no experiments. Thus, none of these approaches can align classes and relations like paris.\nHolistic Approaches. Only very few approaches address the cause of aligning both schema and instances: the RiMOM [20] and iliads [30] systems. Both of these have only been tested on small ontologies. The RiMOM system can align classes, but it cannot find subclassOf relationships. Furthermore, the approach provides a bundle of heuristics and strategies to choose from, while paris is monolithic. None of the ontologies the iliads system has been tested on contained full-fledges instances with properties. In contrast, paris is shown to perform well even on large-scale real-world ontologies with millions of instances."
    }, {
      "heading" : "3. Preliminaries",
      "text" : "In this section, we recall the notions of ontology and of equivalence. Finally, we introduce the notion of functionality as one of the key concepts for ontology alignment.\nOntologies. We are concerned with ontologies available in the Resource Description Framework Schema (RDFS [33]), the W3C standard for knowledge representation. An RDFS ontology builds on resources. A resource is an identifier for a real-world object, such as a city, a person, or a university, but also the concept of mathematics. For example, London is a resource that represents the city of London. A literal is a string, date or number. A property (or relation) is a binary predicate that holds between two resources or between a resource and a literal. For example, the property isLocatedIn holds between the resources London and UK. In the RDFS model, it is assumed that there exists a fixed global set R of resources, a fixed global set L of literals, and a fixed global set P of properties. Each resource is described by a URI. An RDFS ontology can be seen as a set of triples O ⊂ R×P × (R∪L), called statements. In the following, we assume given an ontology O. To say that 〈x, r, y〉 ∈ O, we will write r(x, y) and we call x and y the arguments of r. Intuitively, such a statement means that the relation r holds between the entities x and y. We say that x, y is a pair of r. A relation r−1 is called\nthe inverse of a relation r if ∀x, y : r(x, y) ⇔ r−1(y, x). We assume that the ontology contains all inverse relations and their corresponding statements. Note that this results in allowing the first argument of a statement to be a literal, a minor digression from the standard. An RDFS ontology distinguishes between classes and instances. A class is a resource that represents a set of objects, such as, e.g., the class of all singers, the class of all cities or the class of all books. A resource that is a member of a class is called an instance of that class. We assume that the ontology partitions the resources into classes and instances.9 The rdf:type relation connects an instance to a class. For example, we can say that the resource Elvis is a member of the class of singers: rdf:type(Elvis, singer). A more specific class c can be specified as a subclass of a more general class d using the statement rdfs:subclassOf(c,d). This means that, by inference, all instances of c are also instances of d. Likewise, a relation r can be made a sub-relation of a relation s by the statement rdfs:subpropertyOf(r,s). This means that, by inference again, ∀x, y : r(x, y) ⇒ s(x, y). We assume that all such inferences have been established and that the ontologies are available in their deductive closure, i.e., all statements implied by the subclass and sub-property statements have been added to the ontology.\nEquivalence. In RDFS, the sets P, R, and L are global. That means that some resources, literals, and relations may be identical across different ontologies. For example, two ontologies may contain the resource London, therefore share that resource. (In practice, London is a URI, which makes it easy for two ontologies to use exactly the same identifier.) The semantics of RDFS enforces that these two occurrences of the identifier refer to the same real-world object (the city of London). The same applies to relations or literals that are shared across ontologies. Conversely, two different resources can refer to the same real-world object. For example, London and Londres can both refer to the city of London. Such resources are called equivalent. We write Londres ≡ London. The same observation applies not just to instances, but also to classes and relations. Two ontologies can talk about an identical class or relation. They can also use different resources, but refer to the very same real-world concepts. For example, one ontology can use the relation wasBornIn whereas another ontology can use the relation birthPlace. An important goal of our approach is to find out that wasBornIn ≡ birthPlace.\nIn this paper, we make the following assumption: an ontology does not contain equivalent resources. That is, if an ontology contains two instances x and x′, then we assume x 6≡ x′. We assume the same for relations and classes. This is a reasonable assumption, because most ontologies are either manually designed [21, 22], or generated from a database (such as the datasets mentioned in the introduction), or designed with avoiding equivalent resources in mind [28]. If the ontology does contain equivalent resources, then our approach will still work. It will just not discover the equivalent resources within one ontology.\n9RDFS allows classes to be instances of other classes, but in practice, this case is rare.\nFunctions. A relation r is a function if, for a given first argument, there is only one second argument. For example, the relation wasBornIn is a function, because one person is born in exactly one place. A relation is an inverse function if its inverse is a function. If r is a function and if r(x, y) in one ontology and r(x, y′) in another ontology, then y and y′ must be equivalent. In the example: If a person is born in both Londres and London, then Londres ≡ London. The same observation holds for two first arguments of inverse functions. As will see, functions play an essential role in deriving alignments between ontologies. Nevertheless, it turns out that the precise notion of function is too strict for our setting. This is due to two reasons:\n• First, a relation r ceases to be a function as soon as there is one x with y and y′\nsuch that r(x, y) and r(x, y′). This means that just one erroneous fact can make a relation r a non-function. Since real-world ontologies usually contain erroneous facts, the strict notion of function is not well-suited.\n• Second, even if a relation is not a function, it may contribute evidence that two entities are the same. For example, the relation livesIn is not a function, because some people may live in several places. However, a wide majority of people live in one place, or in very few places. So, if most people who live in London also live in Londres, this provides a strong evidence for the unification of London and Londres.\nThus, to derive alignments, we want to deal with “quasi-functions”. This motivates introducing the concept of functionality, first introduced in [15]. The local functionality of a relation r for a first argument x is defined as:\nfun(r, x) = 1\n#y r(x, y) (1)\nwhere we write “#y ϕ(y)” to mean “|{y | ϕ(y)}|”. Consider for example the relationship isCitizenOf . For most first arguments, the functionality will be 1, because most people are citizens of exactly one country. However, for people who have multiple nationalities, the functionality may be ½ or even smaller. The local inverse functionality is defined analogously as fun−1(r, x) = fun(r−1, x). Deviating from [15], we define the global functionality of a relation r as the harmonic mean of the local functionalities, which boils down to\nfun(r) = #x ∃y : r(x, y)\n#x, y r(x, y) . (2)\nWe discuss design alternatives for this definition in Appendix A. The global inverse functionality is defined analogously as fun−1(r) = fun(r−1)."
    }, {
      "heading" : "4. Probabilistic Model",
      "text" : ""
    }, {
      "heading" : "4.1. Equivalence",
      "text" : "We want to model the probability Pr(x ≡ x′) that one instance x in one ontology is equivalent to another instance x′ in another ontology. Let us assume that both ontologies share a relation r. Following our argument in Section 3, we want the probability Pr(x ≡ x′) to be large if r is highly inverse functional, and if there are y ≡ y′ with r(x, y), r(x′, y′) (if, say x and x′ share an e-mail address). This can be written pseudo-formally as:\n∃r, y, y′ : r(x, y) ∧ r(x′, y′) ∧ y ≡ y′ ∧ fun−1(r) is high =⇒ x ≡ x′. (3)\nWith the formalization that we describe in Appendix B, this can be modeled as:\nPr1(x ≡ x ′) := 1−\n∏\nr(x,y) r(x′,y′)\n( 1− fun−1(r)× Pr(y ≡ y′) )\n(4)\nAs soon as there is one relation r with fun−1(r) = 1 and with r(x, y), r(x′, y′), and Pr(y ≡ y′) = 1, it follows that Pr1(x ≡ x\n′) = 1. We discuss a design alternative in Appendix C.\nNote that the probability of x ≡ x′ depends recursively on the probabilities of other equivalences. These other equivalences may hold either between instances or between literals. We discuss the probability of equivalence between two literals in Section 5. Obviously, we set Pr(x ≡ x) := 1 for all literals and instances x. Equation (4) considers only positive evidence for an equality. To consider also evidence against an equality, we can use the following modification. We want the probability Pr(x ≡ x′) to be small, if there is a highly functional relation r with r(x, y) and if y 6≡ y′ for all y′ with r(x′, y′). Pseudo-formally, this can be written as\n∃r, y : r(x, y) ∧ (∀y′ : r(x′, y′) ⇒ y 6≡ y′) ∧ fun(r) is high =⇒ x 6≡ x. (5)\nThis can be modeled as\nPr2(x ≡ x ′) :=\n∏\nr(x,y)\n\n1− fun(r) ∏\nr(x′,y′)\n(1− Pr(y ≡ y′))\n\n (6)\nAs soon as there is one relation r with fun(r) = 1 and with r(x, y), r(x′, y′), and Pr(y ≡ y′) = 0, it follows that Pr2(x ≡ x\n′) = 0. We combine these two desiderata by multiplying the two probability estimates:\nPr3(x ≡ x ′) := Pr1(x ≡ x ′)× Pr2(x ≡ x ′) (7)\nIn the experiments, we found that Equation (4) suffices in practice. However, we discuss scenarios where Equation (7) can be useful in Section 6."
    }, {
      "heading" : "4.2. Subrelations",
      "text" : "The formulas we have just established estimate the equivalence between two entities that reside in two different ontologies, if there is a relation r that is common to the ontologies. It is also a goal to discover whether a relation r of one ontology is equivalent to a relation r′ of another ontology. More generally, we would like to find out whether r is a sub-relation of r′, written r ⊆ r′. Intuitively, the probability Pr(r ⊆ r′) is proportional to the number of pairs in r that are also pairs in r′:\nPr(r ⊆ r′) := #x, y r(x, y) ∧ r′(x, y)\n#x, y r(x, y) (8)\nThe numerator should take into account the resources that have already been matched across the ontologies. Therefore, the numerator is more appropriately phrased as:\n#x, y r(x, y) ∧ (∃x′, y′ : x ≡ x′ ∧ y ≡ y′ ∧ r′(x′, y′)) (9)\nUsing again our formalization from Appendix B, this can be modeled as:\n∑\nr(x,y)\n\n1− ∏\nr′(x′,y′)\n(1− (Pr(x ≡ x′)× Pr(y ≡ y′)))\n\n (10)\nIn the denominator, we want to normalize by the number of pairs in r that have a counterpart in the other ontology. This is\n∑\nr(x,y)\n\n1− ∏\nx′,y′\n(1− (Pr(x ≡ x′)× Pr(y ≡ y′)))\n\n (11)\nThus, we estimate the final probability as:\nPr(r ⊆ r′) :=\n∑\nr(x,y)\n(\n1− ∏ r′(x′,y′) (1− (Pr(x ≡ x ′)× Pr(y ≡ y′)))\n)\n∑\nr(x,y)\n(\n1− ∏ x′,y′ (1− P (x ≡ x ′)× Pr(y ≡ y′))\n) . (12)\nThis probability depends on the probability that two instances (or literals) are equivalent. One might be tempted to set Pr(r ⊆ r) := 1 for all relations r. However, in practice, we observe cases where the first ontology uses r where the second ontology omits it. Therefore, we compute Pr(r ⊆ r) as a contingent quantity.\nWe are now in a position to generalize Equation (4) to the case where the two ontologies do not share a common relation. For this, we need to replace every occurrence of r(x′, y′) by r′(x′, y′) and factor in the probability Pr(r′ ⊆ r). This yields:\nPr(x ≡ x′) := 1− ∏\nr(x,y) r′(x′,y′)\n( 1− Pr(r′ ⊆ r)× fun−1(r)× Pr(y ≡ y′) )\n(13)\nIf we want to consider also negative evidence as in Equation (7), we get\nPr(x ≡ x′) := ( 1− ∏\nr(x,y) r′(x′,y′)\n( 1− P (r′ ⊆ r)× fun−1(r)× Pr(y ≡ y′) )\n)\n× ∏\nr(x,y) r′\n(\n1− fun(r)× Pr(r′ ⊆ r)× ∏\nr′(x′,y′)\n(1− Pr(x ≡ x′)) )\n(14)\nThis formula looks asymmetric, because it considers only Pr(r′ ⊆ r) and fun(r) (and not fun(r′)). Yet, it is not asymmetric, because each instantiation of r′ will at some time also appear as an instantiation of r. It is justified to consider Pr(r′ ⊆ r), because a large Pr(r′ ⊆ r) implies that r′(x, y) ⇒ r(x, y). This means that a large Pr(r′ ⊆ r) implies that fun(r) < fun(r′) and fun−1(r) < fun−1(r′). If there is no x′, y′ with r′(x′, y′), we set the last factor of the formula to one, ∏\nr′(x′,y′)(1− Pr(x ≡ x′))) := 1. This decreases Pr(x ≡ x′) in case one instance has relations that the other one does not have. To each instance from the first ontology, our algorithm will assign multiple equivalent instances from the second ontology, each with a probability score. For each instance from the first ontology, we call the instance from the second ontology with the maximum score the maximal assignment."
    }, {
      "heading" : "4.3. Classes",
      "text" : "A class corresponds to a set of entities. One could be tempted to treat classes just like instances and compute their equivalence. However, the class structure of one ontology may be more fine-grained than the class structure of the other ontology. Therefore, we aim to find out not whether one class c of one ontology is equivalent to another class c′ of another ontology, but whether c is a subclass of c′, c ⊆ c′. Intuitively, the probability Pr(c ⊆ c′) shall be proportional to the number of instances of c that are also instances of c′:\nPr(c ⊆ c′) = # c ∩ c′\n#c . (15)\nAgain, we estimate the expected number of instances that are in both classes as\nE(# c ∩ c′) = ∑\nx:type(x,c)\n\n1− ∏\ny:type(y,d)\n(1− P (x ≡ y))\n\n . (16)\nWe divide this expected number by the total number of instances of c about which an equivalence is known:\nPr(c ⊆ c′) =\n∑\nx:type(x,c)\n(\n1− ∏ y:type(y,d) (1− P (x ≡ y)) )\n#x type(x, c) ∧ ∃y : Pr(x ≡ y) > 0 (17)\nThe fact that two resources are instances of the same class can reinforce our belief that the two resources are equivalent. Hence, it seems tempting to feed the subclassrelationship back into Equation (13). However, in practice, we found that the class\ninformation is of less use for the equivalence of instances. This may be because of different granularities in the class hierarchies. It might also be because some ontologies use classes to express certain properties (MaleSingers), whereas others use relations for the same purpose (gender = male). Therefore, we compute the class equivalences only after the instance equivalences have been computed."
    }, {
      "heading" : "5. Implementation",
      "text" : ""
    }, {
      "heading" : "5.1. Iteration",
      "text" : "Our algorithm takes as input two ontologies. We assume that a single ontology does not contain duplicate (equivalent) entities. This corresponds to some form of a domainrestricted unique name assumption. Therefore, our algorithm considers only equivalence between entities from different ontologies. Strictly speaking, the functionality of a relation (Equation (2)) depends recursively on the equivalence of instances. If, e.g., every citizen lives in two countries, then the functionality of livesIn is 1\n2 . If our algorithm unifies the two countries, then the func-\ntionality of livesIn jumps to 1. However, since we assume that there are no equivalent entities within one ontology, we compute the functionalities of the relations within each ontology upfront. We implemented a fixpoint computation for Equations (12) and (13). First, we compute the probabilities of equivalences of instances. Then, we compute the probabilities for sub-relationships. These two steps are iterated until convergence. In a last step, the equivalences between classes are computed by Equation (17) from the final assignment. To bootstrap the algorithm in the very first step, we set Pr(r ⊆ r′) = θ for all pairs of relations r, r′ of different ontologies. We chose θ = 0.10. The second round uses the computed values for Pr(r ⊆ r′) and no longer θ. We have not yet succeeded in proving a theoretical condition under which the iteration of Equations (12) and (13) reaches a fixpoint. In practice, we iterate until the entity pairs under the maximal assignments change no more (which is what we call convergence). In our experiments, this state was always reached after a few iterations. We note that one can always enforce convergence of such iterations by introducing a progressively increasing dampening factor. Our model changes the probabilities of two resources being equal – but never the probability that a certain statement holds. All statements in both ontologies remain valid. This is possible because an RDFS ontology cannot be made inconsistent by equating resources."
    }, {
      "heading" : "5.2. Optimization",
      "text" : "The equivalence of instances (Equation (13)) can be computed in different ways. In the most naïve setting, the equivalence is computed for each pair of instances. This would result in a runtime of O(n2m), where n is the number of instances and m is the average\nnumber of statements in which the instance occurs (a typical value for m is 20). This implementation took weeks to run one iteration. We overcame this difficulty as follows. First, we optimize the computation of Equation (13). For each instance x in the first ontology, we traverse all statements r(x, y) in which this instance appears as first argument. (Remember that we assume that the ontology contains all inverse statements as well.) For each statement r(x, y), we consider the second argument y, and all instances y′ that the second argument is known to be equal to ({y′ : Pr(y ≡ y′) > 0}). For each of these equivalent instances y′, we consider again all statements r(x′, y′) and update the equality of x and x′. This results in a runtime of O(nm2e), where e is the average number of equivalent instances per instance (typically around 10). Equations (12) and (17) are optimized in a similar fashion.\nIn general, our model distinguishes true equivalences (Pr(x ≡ x′) > 0) from false equivalences (Pr(x ≡ x′) = 0) and unknown equivalences (Pr(x ≡ x′) not yet computed). Unknown quantities are simply omitted in the sums and products of the equations. Interestingly, most equations contain a probability Pr(x ≡ x′) only in the form ∏\n(1 − P (x ≡ x′)). This means that the formula will evaluate to the same value if Pr(x ≡ x′) is unknown or if Pr(x ≡ x′) = 0. Therefore, our algorithm does not need to store equivalences of value 0 at all. Our implementation thresholds the probabilities and assumes every value below θ to be zero. This greatly reduces the number of equivalences that the algorithm needs to store. Furthermore, we limit the number of pairs that are evaluated in Equations (12) and (17) to 10, 000. For each computation, our algorithm considers only the equalities of the previous maximal assignment and ignores all other equalities. This reduces the runtime by an order of magnitude without affecting much the relation inclusion assessment. Our implementation is in Java, using the Java Tools developed for [27] and Berkeley DB. We used the Jena framework to load and convert the ontologies. The algorithm turns out to be heavily IO-bound. Therefore, we used a solid-state drive (SSD) with high read bandwidth to store the ontologies. This brought the computation time down from the order of days to the order of hours on very large ontologies. We considered parallelizing the algorithm and running it on a cluster, but it turned out to be not necessary."
    }, {
      "heading" : "5.3. Literal Equivalence",
      "text" : "The probability that two literals are equal is known a priori and will not change. Therefore, such probabilities can be set upfront (clamped), for example as follows:\n• The probability that two numeric values of the same dimension are equal can be a function of their proportional difference. • The probability that two strings are equal can be inverse proportional to their edit distance. • For other identifiers (Social Security Numbers etc.), the probability of equivalence can be a function that is robust to common misspellings. The check sum computations that are often defined for such identifiers can give a hint as to which misspellings are common.\n• The probability of literals of different types to be equal should be 0. These functions can also be designed depending on the application or on the specific ontologies. The probabilities can then be plugged into Equation (13). For our implementation, we chose a particularly simple equality function. We normalize numeric values by removing all data type or dimension information. Then we set Pr(x ≡ y) = 1 iff x and y are identical literals, to 0 else."
    }, {
      "heading" : "5.4. Parameters",
      "text" : "Our implementation uses the following parameters: 1. The initial value θ for the equivalence of relations in the very first step of the\nalgorithm. We show in the experiments that the choice of θ does not affect the results. 2. Similarity functions for literals. These are application-dependent. However, we show that even with the simple identity function, the algorithm performs well. Therefore, we believe that we can claim that our model has no dataset dependent tuning parameters. Our algorithm can be (and in fact, was) run on all datasets without any dataset specific settings. This contrasts paris with other algorithms, which are often heavily dependent on parameters that have to be tuned for each particular application or dataset. Traditional schema alignment algorithms, for example, usually use heuristics on the names of classes and relations, whose tuning requires expertise (e.g., [20]). A major goal of the present work was to base the algorithm on probabilities and make it as independent as possible from the tuning of parameters. We are happy to report that this works beautifully."
    }, {
      "heading" : "6. Experiments",
      "text" : "Setup. All experiments were run on a quad-core PC with 12 GB of RAM, running a 64bit version of Linux; all data was stored on a fast SSD drive, with a peak random access bandwidth of approximately 50 MB/s (to be compared with a typical random access bandwidth of 1 MB/s for a magnetic hard drive). Our experiments always compute relation, class, and instance equivalences between two given ontologies. Our algorithm was run until convergence. We evaluate the instance equalities by comparing the computed final maximal assignment to a gold standard, using the standard metrics of precision, recall, and the F-Measure. For the relation assignment, we performed a manual evaluation. As for the instances, we considered only the assignment with the maximal score. Since paris computes sub-relations, we evaluated the assignments in each direction. Class alignments were also evaluated manually. For all evaluations, we ignored the probability score that paris assigned, except when noted.\nBenchmark Test. To be comparable to [16, 20, 23, 25], we report results on the benchmark provided by the OAEI 2010 [9]. We ran experiments on two datasets, each of which\nconsists of two ontologies.10 For each dataset, the OAEI provides a gold standard list of instances of the first ontology that are equivalent to instances of the second ontology. The relations and classes are identical in the first and second ontology. To make the task more challenging for paris, we artificially renamed the relations and classes in the first ontology, so that the sets of instances, classes, and relations used in the first ontology are disjoint from the ones used in the second ontology. For the person dataset, paris converged after just 2 iterations and 2 minutes. For the restaurants, paris took 3 iterations and 6 seconds. Table 6 shows our results. We achieve near-perfect precision and recall, with the exception of recall in the second dataset. As reported in [16], all other approaches [20, 23, 25] remain below 80% of F-measure for the second dataset, while only ObjectCoref [16] achieves an F-measure of 90%. We are very satisfied with our result of 83%, because unlike ObjectCoref, paris does not require any training data. It should be further noted that, unlike all other approaches, paris did not even know that the relations and classes were identical, but discovered the class and relation equivalences by herself in addition to the instance equivalences.\nDesign Alternatives. To measure the influence of θ on our algorithm, we ran paris with θ = 0.001, 0.01, 0.05, 0.1, 0.2 on the restaurant dataset. A larger θ causes larger probability scores in the first iteration. However, the sub-relationship scores turn out to be the same, no matter what value θ had. Therefore, the final probability scores are the same, independently of θ. In a second experiment, we allowed the algorithm to take into account all probabilities from the previous iteration (and not just those of the maximal assignment). This did not change the results, because the first iteration already has a very good precision. In a third experiment, we allowed the algorithm to take into account negative evidence (i.e., we used Equation (14) instead of Equation (13)). Predictably, this maintained precision at 100%. However, only one entity was matched (instead of 112), making the algorithm converge after only 2 iterations with a recall of 0.8% and an F-score of 2%. The reason for this behavior turned out to be that most entities have slightly different attribute values (e.g., a phone number “213/467-1108” instead of “213-467-1108”). Therefore, we plugged in a different string equality measure. Our new measure normalizes two strings by removing all non-alphanumeric characters and lowercasing them. Then, the measure returns 1 if the strings are equal, 0 else. This\n10We could not run on the third dataset, because it violates our assumption of non-equivalence within\none ontology.\nbrought the probability scores back to the previous values, and precision and recall back to 98% and 73%, respectively. Our experience with yago and DBpedia (see next experiment) indicates that the negative evidence can be helpful to distinguish entities of different types (movies and songs) that share one value (the title). However, in our settings, the positive evidence proved sufficient.\nReal-world Ontologies. We wanted to test paris on real-world ontologies with a rich class and relation structure. At the same time, we wanted to restrict ourselves to cases where an error-free ground truth is available. Therefore, we chose to align the yago [28] and DBpedia [1] ontologies. With several million instances, these are some of the largest ontologies available. Each of them has thousands of classes and at least dozens of relations. We took only the non-meta facts from yago, and only the manually established ontology from DBpedia, which yields the datasets described in Table 2. Both ontologies use the identifiers from Wikipedia for their instances, so that the ground truth for the instance matching can be computed trivially.11 However, the statements about the instances differ in both ontologies, so that the matching is not trivial. The class structure and the relationships of yago and DBpedia were designed completely independently, making their alignment a challenging endeavor.\nOntology # Instances # Classes # Relations\nWe ran paris for 4 iterations, until we detected less than 1% change of the maximal assignment. Table 3 shows the results per iteration. For estimating recall, we assume that every instance of DBpedia has a counterpart in yago. This is not true, because both ontologies are based on different versions of Wikipedia and because yago does not contain all entities in Wikipedia. Therefore, our values for recall are lower bounds of the actual values. In the first iteration, paris establishes equalities with high precision. Manual inspection indicates that paris almost never renounces established equalities. Further iterations add more equalities, but with lower precision.\nparis assigns one class of one ontology to multiple classes in the taxonomy of the other ontology, taking into account the class inclusions. Some classes are assigned to multiple leaf-classes as well. For our evaluation, we excluded 10 trivial high-level classes (such as entity, physicalThing, etc.). Then, we randomly sampled from the remaining assignments and evaluated the precision manually. It turns out that the precision increases substantially with the probability score (see Figure 1). We report the numbers for a threshold of 0.6 in Table 3 (the number of evaluated sample assignments is 100 in both cases). The errors come from 3 sources: First, paris misclassifies 25% of the instances, which worsens the precision of the class assignment. Second, there are small\n11We hid this knowledge from paris.\ninconsistencies in the ontologies themselves (yago, e.g., has several people classified as lumber, because they work in the wood industry). Last, there may be biases in the instances that the ontologies talk about. For example, paris estimates that 12% of the people convicted of murder in Utah were soccer players. As the score increases, these assignments get sorted out. Evaluating whether a class is always assigned to its most specific counterpart would require exhaustive annotation of candidate inclusions. Therefore we only report the number of aligned classes and observe that even with high probability scores (see Figure 2 and Table 3) we find matches for most of the classes of each ontology into the other.\n0 0.2 0.4 0.6 0.8\n5\n10\n15\n20\n0.9\nThreshold\nN u m b er\nof cl as se s (×\n10 ,0 00 )\nFigure 2: Number of yago classes that have at least one assignment in DBpedia with a score greater than the threshold.\nThe relations are also evaluated manually in both directions. We consider only the maximally assigned relation, because the relations do not form a hierarchy in yago and DBpedia. In most cases one assignment dominates clearly. Table 4 shows some of the alignments. paris finds non-trivial alignments of more fine-grained relations to more coarse-grained ones, of inverses, of symmetric relations, and of relations with completely different names. There are a few plainly wrong alignments, but most errors come from semantic differences that do not show in practice (e.g., burialPlace is semantically dif-\nferent from deathPlace, so we count it as an error, even though in most cases the two will coincide). Recall is hard to estimate, because not all relations have a counterpart in the other ontology and some relations are poorly populated. We only note that we find alignments for roughly half of yago’s relations in DBpedia.\nOverall, we are pleased with the results, as this constitutes, to the best of our knowledge, the first holistic alignment of instances, relations and classes on two of the world’s largest ontologies."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We have presented paris, an algorithm for the automated alignment of RDFS ontologies. Unlike most other approaches, paris computes alignments not only for instances, but also for classes and relations. It does not need training data and it does not require any parameter tuning. paris is based on a probabilistic framework that captures the interplay between schema alignment and instance matching in a natural way, thus providing a holistic solution to the ontology alignment problem. Experiments show that our approach works extremely well in practice.\nparis does not use any kind of heuristics on relation names, which allows aligning relations with completely different names. We conjecture that the name heuristics of more traditional schema-alignment techniques could be factored into the model. For future work, we plan to analyze under which conditions our equations are guaranteed to converge. We also plan to investigate how classes of one ontology can be mapped to complex expressions on classes, properties and property values of the other ontology. For example, it should be possible to map the class Woman of one ontology to the expression Person(x) ∧ gender(x) = female in the other ontology. This would further increase the usefulness of paris for the dream of the Semantic Web."
    }, {
      "heading" : "A. Global Functionality",
      "text" : "There are several design alternatives to define the global functionality:\n1. We can count the number of statements and divide it by the number of pairs of statements with the same source:\nfun(r) = #x, y r(x, y)\n#x, y, y′ r(x, y) ∧ r(x, y′) .\nThis measure is very volatile to single sources that have a large number of targets.\n2. We can define the functionality as the ratio of the number of first arguments to the number of second arguments:\nfun(r) = #x ∃y : r(x, y)\n#y ∃x : r(x, y) .\nThis definition is treacherous: Assume that we have n people and n dishes, and the relationship likesDish(x, y). Now, assume that all people like all dishes. Then likesDish should have a low functionality, because everybody likes n dishes. But the above definition assigns a functionality of fun(likesDish) = n\nn = 1.\n3. We can average the local functionalities, as proposed in [15]:\nfun(r) = avg x fun(r, x) = avg x\n(\n1\n#y r(x, y)\n)\n= 1\n#x ∃y : r(x, y)\n∑\nx\n1\n#y r(x, y) .\nHowever, the local functionalities are ratios, so that the arithmetic mean is less appropriate.\n4. We can average the local functionalities not by the arithmetic mean, but by the harmonic mean instead\nfun(r) = HM x fun(r, x) = HM x\n(\n1\n#y r(x, y)\n)\n= #x ∃y : r(x, y) ∑\nx #y r(x, y) =\n#x ∃y : r(x, y)\n#x, y r(x, y) .\n5. We may say that the global functionality is the number of first arguments per relationship instance:\nfun(r) = #x ∃y : r(x, y)\n#x, y r(x, y) .\nThis notion is equivalent to the harmonic mean.\nWith these considerations in mind, we chose the harmonic mean for the definition of the global functionality."
    }, {
      "heading" : "B. Probabilistic Modeling of First-Order Formulas",
      "text" : "In Section 4, we presented our probabilistic model of ontology alignment based on firstorder sentences description of our alignment rules, such as Equation (5), reproduced here:\n∃r, y : r(x, y) ∧ (∀y′ : r(x′, y′) ⇒ y 6≡ y′) ∧ fun(r) is high =⇒ x 6≡ x. (5)\nWe derive from these equations probability assessments, such as Equation (6), by assuming mutual independence of all distinct elements of our models (instance equivalence, functionality, relationship inclusion, etc.). This assumption is of course not true in practice but it allows us to approximate efficiently the probability of the consequence of\nour alignment rules in a canonical manner. Independence allows us to use the following standard identities:\nPr(A ∧ B) = Pr(A)× Pr(B);\nPr(A ∨ B) = 1− (1− Pr(A))(1− Pr(B));\nPr(∀x : ϕ(x)) = ∏\nx\nPr(ϕ(x));\nPr(∃x : ϕ(x)) = 1− ∏\nx\n(1− Pr(ϕ(x))) ;\nE(#x ϕ(x)) = ∑\nx\nPr(ϕ(x)).\nThen, a rule ϕ =⇒ ψ is translated as a probability assignment Pr(ψ) := Pr(ϕ) and ϕ is recursively decomposed using these identities. Following the example of Equation 5, we derive Equation 6 as follows:\nPr2(x ≡ x ′) := 1− Pr(∃r, y : r(x, y) ∧ (∀y′ : r(x′, y′) ⇒ y 6≡ y′) ∧ fun(r) is high )\n= ∏\nr,y\n\n1− Pr(r(x, y)) ∧ ∏\ny′\n(1− Pr(r(x′, y′) ∧ y ≡ y′)× fun(r)\n\n\n= ∏\nr(x,y)\n\n1− fun(r) ∏\nr(x′,y′)\n(1− Pr(y ≡ y′))\n\n .\nsince r(x, y) and r(x′, y′) are crisp, non-probabilistic facts. Similarly, when we need to estimate a number such as “#x ϕ(x)”, we compute E(#x ϕ(x)) using the aforementioned identities."
    }, {
      "heading" : "C. Equivalence of Sets",
      "text" : "We compare two instances for equivalence by comparing every statement about the first instance with every statement about the second instance (if they have the same relation). This entails a quadratic number of comparisons. For example, if an actor x acted in the movies y1, y2, y3, and an actor x ′ acted in the movies y′1, y ′ 2, y ′ 3, then we will compare every statement actedIn(x, yi) with every statement actedIn(x ′, y′j). Alternatively, one could think of the target values as a set and of the relation as a function, as in actedIn(x, {y1, y2, y3}) and actedIn(x ′, {y′1, y ′ 2, y ′ 3}). Then, one would have to compare only two sets instead of a quadratic number of statements. However, all elements of one set are potentially equivalent to all elements of the other set. Thus, one would still need a quadratic number of comparisons. One could generalize a set equivalence measure (such as the Jaccard index) to sets with probabilistic equivalences. However, one would still need to take into account the functionality of the relations: If two people share an e-mail address (high inverse functionality), they are almost certainly equivalent. By contrast, if two people share the city they live in, they are not necessarily equivalent. To unify two instances, it is\nsufficient that they share the value of one highly inverse functional relation. Conversely, if two people have a different birth date, they are certainly different. By contrast, if they like two different books, they could still be equivalent (and like both books). Our model takes this into account. Thus, our formulas can be seen as a comparison measure for sets with probabilistic equivalences, which takes into account the functionalities."
    } ],
    "references" : [ {
      "title" : "DBpedia: A Nucleus for a Web of Open Data",
      "author" : [ "S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z.G. Ives" ],
      "venue" : "Proc. ISWC,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Schema and ontology matching with COMA++",
      "author" : [ "D. Aumueller", "H.-H. Do", "S. Massmann", "E. Rahm" ],
      "venue" : "Proc. SIGMOD,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Web of linked data",
      "author" : [ "C. Bizer" ],
      "venue" : "A global public data space on the Web. In Proc. WebDB,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Linked data on the Web",
      "author" : [ "C. Bizer", "T. Heath", "K. Idehen", "T. Berners-Lee" ],
      "venue" : "Proc. WWW,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Data fusion",
      "author" : [ "J. Bleiholder", "F. Naumann" ],
      "venue" : "ACM Computing Surveys, 41(1),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "SameAs networks and beyond: Analyzing deployment status and implications of owl:sameAs in linked data",
      "author" : [ "L. Ding", "J. Shinavier", "Z. Shangguan", "D.L. McGuinness" ],
      "venue" : "Proc. ISWC,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Duplicate record detection: A survey",
      "author" : [ "A. Elmagarmid", "P. Ipeirotis", "V. Verykios" ],
      "venue" : "IEEE TKDE, 19(1):1–16,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Web-scale information extraction in KnowItAll (preliminary results)",
      "author" : [ "O. Etzioni", "M. Cafarella", "D. Downey", "S. Kok", "A.-M. Popescu", "T. Shaked", "S. Soderland", "D.S. Weld", "A. Yates" ],
      "venue" : "Proc. WWW,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Results of the ontology alignment evaluation initiative",
      "author" : [ "J. Euzenat", "M. Mochol", "P. Shvaiko", "H. Stuckenschmidt", "O. Svab", "V. Svatek", "W. van Hage", "M. Yatskevich" ],
      "venue" : "In Proc. OM,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Automatic identity recognition in the semantic web",
      "author" : [ "A. Ferrara", "D. Lorusso", "S. Montanelli" ],
      "venue" : "Proc. IRSW,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Managing co-reference on the semantic Web",
      "author" : [ "H. Glaser", "A. Jaffri", "I. Millard" ],
      "venue" : "Proc. LDOW,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Large scale integration of senses for the semantic Web",
      "author" : [ "J. Gracia", "M. d’Aquin", "E. Mena" ],
      "venue" : "In Proc. WWW,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "When owl:sameAs isn’t the same: An analysis of identity in linked data",
      "author" : [ "H. Halpin", "P. Hayes", "J.P. McCusker", "D. McGuinness", "H.S. Thompson" ],
      "venue" : "Proc. ISWC,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Performing object consolidation on the semantic Web data graph",
      "author" : [ "A. Hogan" ],
      "venue" : "Proc. I3,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Some entities are more equal than others: statistical methods to consolidate linked data",
      "author" : [ "A. Hogan", "A. Polleres", "J. Umbrich", "A. Zimmermann" ],
      "venue" : "Proc. NeFoRS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A self-training approach for resolving object coreference on the semantic Web",
      "author" : [ "W. Hu", "J. Chen", "Y. Qu" ],
      "venue" : "Proc. WWW,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "How matchable are four thousand ontologies on the semantic Web",
      "author" : [ "W. Hu", "J. Chen", "H. Zhang", "Y. Qu" ],
      "venue" : "Proc. ESWC,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An empirical study of instance-based ontology matching",
      "author" : [ "A. Isaac", "L. Van Der Meij", "S. Schlobach", "S. Wang" ],
      "venue" : "In Proc. ISWC,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Ontology matching with semantic verification",
      "author" : [ "Y.R. Jean-Mary", "E.P. Shironoshita", "M.R. Kabuka" ],
      "venue" : "J. Web Semantics, 7(3):235–251,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Rimom: A dynamic multistrategy ontology alignment framework",
      "author" : [ "J. Li", "J. Tang", "Y. Li", "Q. Luo" ],
      "venue" : "IEEE TKDE, 21(8):1218–1232,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An introduction to the syntax and content of Cyc",
      "author" : [ "C. Matuszek", "J. Cabral", "M. Witbrock", "J. Deoliveira" ],
      "venue" : "Proc. AAAI Spring Symposium,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Towards a standard upper ontology",
      "author" : [ "I. Niles", "A. Pease" ],
      "venue" : "Proc. FOIS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Leveraging terminological structure for object reconciliation",
      "author" : [ "J. Noessner", "M. Niepert", "C. Meilicke", "H. Stuckenschmidt" ],
      "venue" : "Proc. ESWC,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deriving a large-scale taxonomy from Wikipedia",
      "author" : [ "S.P. Ponzetto", "M. Strube" ],
      "venue" : "Proc. AAAI,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "L2R: A logical method for reference reconciliation",
      "author" : [ "F. Saïs", "N. Pernelle", "M.-C. Rousset" ],
      "venue" : "Proc. AAAI,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Combining a logical and a numerical method for data reconciliation",
      "author" : [ "F. Saïs", "N. Pernelle", "M.-C. Rousset" ],
      "venue" : "J. Data Semantics, 12:66–94,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Combining linguistic and statistical analysis to extract relations from Web documents",
      "author" : [ "F.M. Suchanek", "G. Ifrim", "G. Weikum" ],
      "venue" : "KDD,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "YAGO: A core of semantic knowledge",
      "author" : [ "F.M. Suchanek", "G. Kasneci", "G. Weikum" ],
      "venue" : "Unifying WordNet and Wikipedia. In Proc. WWW,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Sig.ma: live views on the web of data",
      "author" : [ "G. Tummarello", "R. Cyganiak", "M. Catasta", "S. Danielczyk", "R. Delbru", "S. Decker" ],
      "venue" : "In Proc. WWW,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "Leveraging data and structure in ontology integration",
      "author" : [ "O. Udrea", "L. Getoor", "R.J. Miller" ],
      "venue" : "Proc. SIGMOD,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Discovering and maintaining links on the Web of data",
      "author" : [ "J. Volz", "C. Bizer", "M. Gaedke", "G. Kobilarov" ],
      "venue" : "Proc. ISWC,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning concept mappings from instance similarity",
      "author" : [ "S. Wang", "G. Englebienne", "S. Schlobach" ],
      "venue" : "Proc. ISWC,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Notable endeavors of this kind include DBpedia [1], KnowItAll [8], WikiTaxonomy [24], and yago [28], as well as commercial services such as freebase.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "Notable endeavors of this kind include DBpedia [1], KnowItAll [8], WikiTaxonomy [24], and yago [28], as well as commercial services such as freebase.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "Notable endeavors of this kind include DBpedia [1], KnowItAll [8], WikiTaxonomy [24], and yago [28], as well as commercial services such as freebase.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 27,
      "context" : "Notable endeavors of this kind include DBpedia [1], KnowItAll [8], WikiTaxonomy [24], and yago [28], as well as commercial services such as freebase.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "The goal of the Semantic Web vision is to interlink them, thereby creating one large body of universal ontological knowledge [3, 4].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "The goal of the Semantic Web vision is to interlink them, thereby creating one large body of universal ontological knowledge [3, 4].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : ", considering the A-Box only) [10, 23, 25, 26, 29, 15, 16].",
      "startOffset" : 30,
      "endOffset" : 58
    }, {
      "referenceID" : 22,
      "context" : ", considering the A-Box only) [10, 23, 25, 26, 29, 15, 16].",
      "startOffset" : 30,
      "endOffset" : 58
    }, {
      "referenceID" : 24,
      "context" : ", considering the A-Box only) [10, 23, 25, 26, 29, 15, 16].",
      "startOffset" : 30,
      "endOffset" : 58
    }, {
      "referenceID" : 25,
      "context" : ", considering the A-Box only) [10, 23, 25, 26, 29, 15, 16].",
      "startOffset" : 30,
      "endOffset" : 58
    }, {
      "referenceID" : 28,
      "context" : ", considering the A-Box only) [10, 23, 25, 26, 29, 15, 16].",
      "startOffset" : 30,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : ", considering the A-Box only) [10, 23, 25, 26, 29, 15, 16].",
      "startOffset" : 30,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : ", considering the A-Box only) [10, 23, 25, 26, 29, 15, 16].",
      "startOffset" : 30,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : ", considering the T-Box only) [12, 19, 2, 18, 32].",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : ", considering the T-Box only) [12, 19, 2, 18, 32].",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : ", considering the T-Box only) [12, 19, 2, 18, 32].",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : ", considering the T-Box only) [12, 19, 2, 18, 32].",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 31,
      "context" : ", considering the T-Box only) [12, 19, 2, 18, 32].",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "This problem has been extensively studied in both database and natural language processing areas [5, 7].",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "This problem has been extensively studied in both database and natural language processing areas [5, 7].",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "[13] provide a good overview of the problem in general.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "Glaser, Jaffri, and Millard [11] propose a framework for the management of co-reference in the Semantic Web.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : "[17] provide a study on how matches look in general.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "The approaches that align the classes are manifold, using techniques such as sense clustering [12], lexical and structural characteristics [19], or composite approaches [2].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "The approaches that align the classes are manifold, using techniques such as sense clustering [12], lexical and structural characteristics [19], or composite approaches [2].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "The approaches that align the classes are manifold, using techniques such as sense clustering [12], lexical and structural characteristics [19], or composite approaches [2].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 17,
      "context" : "Most similar to our approach in this field are [18] and [32], which derive class similarity from the similarities of the instances.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 31,
      "context" : "Most similar to our approach in this field are [18] and [32], which derive class similarity from the similarities of the instances.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "Ferrara, Lorusso, and Montanelli [10] introduce",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : "Different techniques are being used, such as exploiting the terminological structure [23], logical deduction [25], or a combination of logical and numerical methods [26].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 24,
      "context" : "Different techniques are being used, such as exploiting the terminological structure [23], logical deduction [25], or a combination of logical and numerical methods [26].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 25,
      "context" : "Different techniques are being used, such as exploiting the terminological structure [23], logical deduction [25], or a combination of logical and numerical methods [26].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 28,
      "context" : "ma engine [29] uses heuristics to match instances.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 14,
      "context" : "Perhaps closest to our approach is [15], which introduces the concept of functionality.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "The silk framework [31] allows specifying manual mapping rules.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "The ObjectCoref approach by Hu, Chen, and Qu [16] allows learning a mapping between the instances from training data.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "We compare some of the results of ObjectCoref to that of paris on the datasets of the ontology alignment evaluation initiative [9] in Section 6.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "Hogan [14] matches instances and propose to use these instances to compute the similarity between classes, but provides no experiments.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 19,
      "context" : "Only very few approaches address the cause of aligning both schema and instances: the RiMOM [20] and iliads [30] systems.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 29,
      "context" : "Only very few approaches address the cause of aligning both schema and instances: the RiMOM [20] and iliads [30] systems.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 20,
      "context" : "This is a reasonable assumption, because most ontologies are either manually designed [21, 22], or generated from a database (such as the datasets mentioned in the introduction), or designed with avoiding equivalent resources in mind [28].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "This is a reasonable assumption, because most ontologies are either manually designed [21, 22], or generated from a database (such as the datasets mentioned in the introduction), or designed with avoiding equivalent resources in mind [28].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : "This is a reasonable assumption, because most ontologies are either manually designed [21, 22], or generated from a database (such as the datasets mentioned in the introduction), or designed with avoiding equivalent resources in mind [28].",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 14,
      "context" : "This motivates introducing the concept of functionality, first introduced in [15].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "Deviating from [15], we define the global functionality of a relation r as the harmonic mean of the local functionalities, which boils down to fun(r) = #x ∃y : r(x, y) #x, y r(x, y) .",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "Our implementation is in Java, using the Java Tools developed for [27] and Berkeley DB.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : ", [20]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "To be comparable to [16, 20, 23, 25], we report results on the benchmark provided by the OAEI 2010 [9].",
      "startOffset" : 20,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "To be comparable to [16, 20, 23, 25], we report results on the benchmark provided by the OAEI 2010 [9].",
      "startOffset" : 20,
      "endOffset" : 36
    }, {
      "referenceID" : 22,
      "context" : "To be comparable to [16, 20, 23, 25], we report results on the benchmark provided by the OAEI 2010 [9].",
      "startOffset" : 20,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "To be comparable to [16, 20, 23, 25], we report results on the benchmark provided by the OAEI 2010 [9].",
      "startOffset" : 20,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "To be comparable to [16, 20, 23, 25], we report results on the benchmark provided by the OAEI 2010 [9].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : "As reported in [16], all other approaches [20, 23, 25] remain below 80% of F-measure for the second dataset, while only ObjectCoref [16] achieves an F-measure of 90%.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 19,
      "context" : "As reported in [16], all other approaches [20, 23, 25] remain below 80% of F-measure for the second dataset, while only ObjectCoref [16] achieves an F-measure of 90%.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "As reported in [16], all other approaches [20, 23, 25] remain below 80% of F-measure for the second dataset, while only ObjectCoref [16] achieves an F-measure of 90%.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : "As reported in [16], all other approaches [20, 23, 25] remain below 80% of F-measure for the second dataset, while only ObjectCoref [16] achieves an F-measure of 90%.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "As reported in [16], all other approaches [20, 23, 25] remain below 80% of F-measure for the second dataset, while only ObjectCoref [16] achieves an F-measure of 90%.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "Table 1: Results on OAEI datasets, compared with ObjectCoref [16].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 27,
      "context" : "Therefore, we chose to align the yago [28] and DBpedia [1] ontologies.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "Therefore, we chose to align the yago [28] and DBpedia [1] ontologies.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 27,
      "context" : "Table 2: yago [28] and DBpedia [1]",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Table 2: yago [28] and DBpedia [1]",
      "startOffset" : 31,
      "endOffset" : 34
    } ],
    "year" : 2017,
    "abstractText" : "We present PARIS, an approach for the automatic alignment of ontologies. PARIS aligns not only instances, but also relations and classes. Alignments at the instance-level cross-fertilize with alignments at the schema-level. Thereby, our system provides a truly holistic solution to the problem of ontology alignment. The heart of the approach is probabilistic. This allows PARIS to run without any parameter tuning. We demonstrate the efficiency of the algorithm and its precision through extensive experiments. In particular, we obtain a precision of around 80% in experiments with two of the world’s largest ontologies.",
    "creator" : "LaTeX with hyperref package"
  }
}