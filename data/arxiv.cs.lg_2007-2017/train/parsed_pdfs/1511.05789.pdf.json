{
  "name" : "1511.05789.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "pauline.wauquier@inria.fr", "mikaela.keller@univ-lille3.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n05 78\n9v 6\n[ cs\n.L G\n] 1"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Transductive or semi-supervised graph-based learning algorithms, such as Zhu et al. (2003); Zhou et al. (2004); Zhu (2005); Liu & Chang (2009), take advantage of both annotated and unannotate data to solve automated labeling tasks. It relies on the homophily property: entities that are close in the graph are supposed to have similar behaviors.\nHowever, those graph-based learning algorithms are dependent on the graph on which they are applied. Indeed the effectiveness of graph-based learning algorithms relies on the relevance of the data representation for the targeted task (Maier et al. (2008); Jebara et al. (2009); de Sousa et al. (2013)).\nIn general the graph representation of the data is built in two steps Liu & Chang (2009); Maier, Markus et al. (2013); de Sousa et al. (2013). In this representation, every data point is seen as a vertex of the graph and the first building step is to compute the edge weight between every pair of vertices. This weight, refleting the similarity degree between two data points is usually computed as a function of the euclidean scalar product between the data points. In the second step of the graph construction edge with low similarity degree are discarded. This is done by applying a non-linear transformation on the edges, like selecting a threshold ǫ on the weight value (a method referred as ǫ-graph), a fixed number k of neighbors for each node (k-nn graph) or by applying a kernel on the weights (making high weights higher and low weights lower).\nWe claim that there might be cases where the euclidean space in which the data points lie will not produce an optimal graph for solving the targeted task.\nTo answer this concern, we propose an approach to build a graph from our data but also adapted to a specific labeling task. We learn a new representation of our data based on constraints related to the\ntask: Data points should be close in the graph if they share similar labels. As the data may be too complex to be projected with a linear approach, we will use a deep neural network in order to learn the most appropriate representation of our data for the task.\nPrevious work related to ours fall into two main categories: representation learning algorithms and metric learning approaches. Among representation learning algorithms, some attempt at learning the best representation for a supervised task in a semi-supervised setting. In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison).\nAmong the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.). Although linear metrics are convenient to optimize, they are not able to capture the non-linear structure of the data; some non-linear metric learning algorithms have been developed and compose a second group of metric learning approaches. Most non-linear metric learning are kernelized version of linear metrics learning approaches (Kedem et al. (2012); He et al. (2013)) and present the drawback of the choice of the kernel. In Vikas Sindhwani & Belkin (2005), a deformed kernel is learn depending on the data geometry which will be used to the classification task resolve.\nOur approach stands in between metric learning and representation learning. It focuses on a specific existing metric w but learns to project the data in a space in which w is meaningful for the targeted task."
    }, {
      "heading" : "2 ALGORITHM’S DESCRIPTION",
      "text" : "Let us define D = {(xi, yi)}i=1..n a dataset of examples such that ∀i ∈ {1, ..., n}, xi ∈ Rd and yi ∈ {1, ..., c}. Let X be the projection of D on Rd and C1, ..., Cc be a partition of X , such that ∀k ∈ {1, ..., c}, Ck = {xi ∈ X |(xi, yi) ∈ D ∧ yi = k} and ∀h 6= k, Ck ∩ Ch = ∅.\nLet us assume that D is partioned into a training set Dtrain and a test set Dtest; we can define Xl to be the projection of Dtrain on Rd and Xu the projection of Dtest on Rd, X = Xl ∪Xu . Labels of elements x ∈ Xu are hidden. If T is the set of triplet constraints defined as:\nT = {(x, x+, x−)|(x, y), (x+, y+), (x−, y−) ∈ D ∧ y = y+ 6= y−},\nthen let us define T = Ttrain ∪ Ttest such that Ttrain = {(x, x+, x−) ∈ T |x, x+, x− ∈ Xl} and Ttest = T \\Ttrain.\nLet φ : Rd ← Rq be a non-linear function. Let us consider a distance measure d1 on Rq . We want the projection φ to respect the following constraint with respect to distance d:\n∀(x, x+, x−) ∈ T, d(φ(x), φ(x+)) < d(φ(x), φ(x−)).\nAt training time, and using a hinge loss we can reformulate our set of constraints as the following cost function to minimize:\nC(φ|Ttrain) = ∑\n(x,x+,x−)∈Ttrain\nmax(0, 1− [d(φ(x), φ(x−)− d(φ(x), φ(x+))])\nThe objective function of our problem is:\nφ = argmin φ̃ C(φ̃|Ttrain)\nIn order to learn φ, we train a Siamese neural network containing three replicated non-linear neural network φ (Fig. 1), by stochastic gradient descent optimization.\n1Concepts introduced in the following can be made true for a similarity measure instead, with a simple inversion in the constraints inequality and in the cost function.\nLet us define W ∈ Rn×n as the matrix whose components Wij = d(φ(xi), φ(xj)). W can be seen as the adjacency matrix of a complete graph. We can use the computed graph in order to predict the hidden label, through the well known label propagation algorithm (Zhu & Ghahramani (2002); Zhou et al. (2004)); In order to remove the non relevant edges of the graph, a pruning phase is usually performed on the graph. Different pruning methods can be applied. The creation of a k−nn graph, where only the edges for the k nearest neighbors of each instances are kept, is a popular one. Another pruning technique is the extraction of the ǫ-graph, where edges are kept depending on a threshold ǫ on their weights values. The obtained matrix W , pruned or not, is then row-normalized.\nAt each step of the label propagation algorithm, the label of an instance is decided according to the labels of its neighbors. Let us define Fm ∈ Rn×c such that Fmik is the probability at iteration\nm for xi to belong to the class Ck; let’s define F 0ik =\n{\n1 if xi ∈ Xl ∩ Ck 0 otherwise . Let’s consider the\nclamped label propagation algorithm, i.e. the label of training instances are not modified during the\ndifferent epochs: Fm+1ic =\n{\n(WFm)ic if xi ∈ Xu F 0im if xi ∈ Xl . Let LP (xi) be the label predicted by the\nlabel propagation algorithm for xi ∈ Xu. The label is computed as LP (xi) = arg max k F∞ik ."
    }, {
      "heading" : "3 THEORETICAL GUARANTEES",
      "text" : "We claim that under some initial assumptions, the algorithm described in section 2 provides a graph representation of the data that is optimal for classification through the label propagation algorithm. In the following we substantiate our claim by showing that we can find an ǫ value for pruning the complete graph obtained from our data in the projected space in such a way that the resulting ǫgraph is made of c connected components homogeneous in class. We show that we can ensure the existence of such an ǫ if we suppose that each testing triplet is in a close neighbourhood of at least one similarly labeled training triplet.\nWe prove our claim through three main steps. We first state that we can bound the distance between the projection of points depending on their distance in the initial space. In a second step we show that if a triplet is properly projected in the new space, then triplets that are projected nearby are also properly projected. By bringing together the first two steps we are able to show that triplets that are close in the initial space to a triplet that is properly projected is also projected properly. Knowing that, we can finally exhibit an ǫ that allows us to compute an optimal ǫ-graph for the label propagation algorithm.\nLet the distance be the euclidean distance and our transformation be a multi-layered perceptrons φ̃, φ : Rd 7→ Rq, with one hidden layer, as\nφ̃(x) = B̃ tanh(Ãx+ α̃) + β̃ and φ(x) = B tanh(Ax+ α) + β,\nwhere B̃, B ∈ Rq×p, Ã, A ∈ Rp×d, α̃, α ∈ Rp and β̃, β ∈ Rq . Let’s suppose that B, A, α and β are learned, with no error, such that\nφ = arg min φ̃\n∑\n(x,x+,x−)∈Ttrain\nmax [0, 1− d(φ̃(x), φ̃(x−)) + d(φ̃(x), φ̃(x+))]\nBased on φ, our first lemma claims that we can easily bound the euclidean distance between the projection of two points by a factor of their initial euclidean distance, where the factor is dependent on A and B. This can easily be done by bounding the application of each transformation of φ.\nOur second lemma defines the maximal distance, in the new representation space, between similarly labeled training and testing instances of triplets allowing us to ensure the respect of the relative constraints on the testing triplets. By considering that the distance of the projected testing instances to the projected training instances is lower than a fixed value, we can define the minimal margin needed between the training triplet elements such that the testing triplet respects the relative constraint.\nJust by bringing those two elements together, we can then show that there exists a maximum distance in the initial space between similarly labeled training and testing instances such that the projection\nof testing triplet will respect the relative constraints; this maximum distance is related to the margin of the projection of training triplets.\nLet’s now consider all our testing instances to be close enough in the initial space from similarly labeled instances. From previous lemmas, we know that all training and testing triplets respect the relative constraints, we can thus define a threshold distinguishing similarly labeled pair of instances from dissimilarly labeled ones. Thus, the ǫ-graph obtained based on this threshold now contains only edges between similarly labeled instances. As our graph is only composed of connected components of same classes instances, the label propagation will be optimal.\nWe also prove that those theoretical element are generalizable for deeper multi-layered perceptrons."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "After proving theoretically the interest of our approach for an ideal learned metric, we experimentally evaluate the performance of our algorithm.\nWe evaluate our framework and compare it to different algorithms on several artificial data sets, composed of 500 instances, chosen for their increasing degree of complexity. The first artificial data set we will use is the classical circle data set. It is obtained by sampling points on two concentric circles in 2-D, the circle defines the class membership. The three other artificial data sets are developed variants of the circle data set; thus their two first features are generated through the circle data set generative model. perturbedCircle1and perturbedCircle2 data sets both get two additional features, respectively random and from a 2-D gaussian. perturbedCircle3 is complemented twice by 2-D gaussian coordinates, depending on the sign of each of the two first features. Labels of perturbedCircle2 data sets are defined depending on the circle on which instances lie and on the gaussian they are associated to. For the other artificial data sets, the label is only dependent on the first two features, as for the initial circle data set. We also evaluate our framework on the real data sets cancer and vehicule, which are real data sets of same order of complexity than the previous data sets; yet vehicule is composed of 4 classes.\nFor each data set, we can compute a graph where the weights are computed by the the euclidean distance in, respectively, the initial space and the learned space for LMNN algorithm (Weinberger & Saul (2009)), the ExtraTrees feature selection algorithm (Geurts et al. (2006)) and our approach2. We apply an ǫ-simplification on each graph. Our network was trained for the euclidean distance with up to 30% of the possible triplets from the train set.\nIn figure 2, we compare, for each dataset, the classification accuracy of the label propagation algorithm performed on the ǫ-graph computed on the different representation spaces, with 20% of training instances. As expected, the euclidean distance and LMNN perform ideally on the circle data set, as the initial space is locally representative of the labelling similarity; however their performance decrease on artifical data sets as the initial description space is getting blurred and noisy,\n2Implemented with torch7 (Collobert et al. (2011))\nlike for perturbedCircle1, perturbedCircle2 and perturbedCircle3 data sets. Feature selection algorithm performs well enough on perturbedCircle1 and perturbedCircle3 data set, where the labels are non-perturbed by the added features; however it does not surpass euclidean distance and LMNN algorithm for other data sets. On those artificial data sets, our algorithm performs either ideally or better than the other algorithms, and was able to learn a more representative projection space.\nConcerning the real dataset cancer, the different algorithms perform quit well, broadly similar. Finally, on the vehicule dataset, we can see the interest of learning a projection space to obtain a better label propagation; yet the non-linear projection seems more adapted to the dataset complexity."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper, we introduced an algorithm to learn a representation space for dataset that is adapted to a specific task. We defined and proved a first theoretical requirement for our algorithm to be optimal; what have been proved can easily be generalized to multi-layers neural network. Experiments on artificial and real data sets confirmed the relevance of our approach for solving classification task."
    }, {
      "heading" : "6 ACKNOWLEDGEMENT",
      "text" : "This work was partially supported by a grant from CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020 and ANRT under the grant number CIFRE No 2013/0961"
    } ],
    "references" : [ {
      "title" : "Learning a similarity metric discriminatively, with application to face verification",
      "author" : [ "Sumit Chopra", "Raia Hadsell", "Yann LeCun" ],
      "venue" : "In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "Chopra et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Chopra et al\\.",
      "year" : 2005
    }, {
      "title" : "Torch7: A Matlab-like Environment for Machine Learning",
      "author" : [ "Ronan Collobert", "Koray Kavukcuoglu", "Clement Farabet" ],
      "venue" : "In BigLearn NIPS Workshop,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Influence of graph construction on semi-supervised learning",
      "author" : [ "Celso Andr R. de Sousa", "Solange O. Rezende", "Gustavo E.A.P.A. Batista" ],
      "venue" : null,
      "citeRegEx" : "Sousa et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sousa et al\\.",
      "year" : 2013
    }, {
      "title" : "Extremely randomized trees",
      "author" : [ "Pierre Geurts", "Damien Ernst", "Louis Wehenkel" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Geurts et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Geurts et al\\.",
      "year" : 2006
    }, {
      "title" : "Kernel density metric learning",
      "author" : [ "Yujie He", "Wenlin Chen", "Yixin Chen", "Yi Mao" ],
      "venue" : "In Data Mining (ICDM),",
      "citeRegEx" : "He et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep metric learning using triplet network",
      "author" : [ "Elad Hoffer", "Nir Ailon" ],
      "venue" : "CoRR, abs/1412.6622,",
      "citeRegEx" : "Hoffer and Ailon.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hoffer and Ailon.",
      "year" : 2014
    }, {
      "title" : "Deep learning via semi-supervised embedding",
      "author" : [ "R. Collobert. J. Weston", "F. Ratle" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Weston and Ratle.,? \\Q2008\\E",
      "shortCiteRegEx" : "Weston and Ratle.",
      "year" : 2008
    }, {
      "title" : "Graph construction and b-matching for semi-supervised learning",
      "author" : [ "T. Jebara", "J. Wang", "S.F. Chang" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Jebara et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jebara et al\\.",
      "year" : 2009
    }, {
      "title" : "Non-linear metric learning",
      "author" : [ "Dor Kedem", "Stephen Tyree", "Kilian Weinberger", "Fei Sha", "Gert Lanckriet" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kedem et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kedem et al\\.",
      "year" : 2012
    }, {
      "title" : "Robust multi-class transductive learning with graphs",
      "author" : [ "Wei Liu", "Shih-Fu Chang" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Liu and Chang.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu and Chang.",
      "year" : 2009
    }, {
      "title" : "Influence of graph construction on graphbased clustering measures",
      "author" : [ "Markus Maier", "Ulrike von Luxburg", "Matthias Hein" ],
      "venue" : "In Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Maier et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maier et al\\.",
      "year" : 2008
    }, {
      "title" : "How the result of graph clustering methods depends on the construction of the graph",
      "author" : [ "Maier", "Markus", "von Luxburg", "Ulrike", "Hein", "Matthias" ],
      "venue" : "ESAIM: PS,",
      "citeRegEx" : "Maier et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maier et al\\.",
      "year" : 2013
    }, {
      "title" : "The manifold tangent classifier. In Advances in Neural Information Processing Systems",
      "author" : [ "Salah Rifai", "Yann Dauphin", "Pascal Vincent", "Yoshua Bengio", "Xavier Muller" ],
      "venue" : "Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Rifai et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rifai et al\\.",
      "year" : 2011
    }, {
      "title" : "Beyond the point cloud: from transductive to semisupervised learning",
      "author" : [ "Partha Niyogi Vikas Sindhwani", "M. Belkin" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Sindhwani and Belkin.,? \\Q2005\\E",
      "shortCiteRegEx" : "Sindhwani and Belkin.",
      "year" : 2005
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "Kilian Q. Weinberger", "Lawrence K. Saul" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Weinberger and Saul.,? \\Q2009\\E",
      "shortCiteRegEx" : "Weinberger and Saul.",
      "year" : 2009
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "Dengyong Zhou", "Olivier Bousquet", "Thomas N. Lal", "Jason Weston", "Bernhard Schölkopf" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zhou et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2004
    }, {
      "title" : "Semi-supervised learning literature survey",
      "author" : [ "Xiaojin Zhu" ],
      "venue" : "Technical Report 1530,",
      "citeRegEx" : "Zhu.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhu.",
      "year" : 2005
    }, {
      "title" : "Learning from labeled and unlabeled data with label propagation",
      "author" : [ "Xiaojin Zhu", "Zoubin Ghahramani" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Zhu and Ghahramani.,? \\Q2002\\E",
      "shortCiteRegEx" : "Zhu and Ghahramani.",
      "year" : 2002
    }, {
      "title" : "Semi-supervised learning using gaussian fields and harmonic functions",
      "author" : [ "Xiaojin Zhu", "Zoubin Ghahramani", "John D. Lafferty" ],
      "venue" : "In Machine Learning, Proceedings of the Twentieth International Conference (ICML",
      "citeRegEx" : "Zhu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Transductive or semi-supervised graph-based learning algorithms, such as Zhu et al. (2003); Zhou et al.",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "(2003); Zhou et al. (2004); Zhu (2005); Liu & Chang (2009), take advantage of both annotated and unannotate data to solve automated labeling tasks.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "(2003); Zhou et al. (2004); Zhu (2005); Liu & Chang (2009), take advantage of both annotated and unannotate data to solve automated labeling tasks.",
      "startOffset" : 8,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "(2003); Zhou et al. (2004); Zhu (2005); Liu & Chang (2009), take advantage of both annotated and unannotate data to solve automated labeling tasks.",
      "startOffset" : 8,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Indeed the effectiveness of graph-based learning algorithms relies on the relevance of the data representation for the targeted task (Maier et al. (2008); Jebara et al.",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "(2008); Jebara et al. (2009); de Sousa et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "(2009); de Sousa et al. (2013)).",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "(2009); de Sousa et al. (2013)). In general the graph representation of the data is built in two steps Liu & Chang (2009); Maier, Markus et al.",
      "startOffset" : 11,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "(2009); de Sousa et al. (2013)). In general the graph representation of the data is built in two steps Liu & Chang (2009); Maier, Markus et al. (2013); de Sousa et al.",
      "startOffset" : 11,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "(2009); de Sousa et al. (2013)). In general the graph representation of the data is built in two steps Liu & Chang (2009); Maier, Markus et al. (2013); de Sousa et al. (2013). In this representation, every data point is seen as a vertex of the graph and the first building step is to compute the edge weight between every pair of vertices.",
      "startOffset" : 11,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J.",
      "startOffset" : 37,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work.",
      "startOffset" : 37,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work.",
      "startOffset" : 37,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J.",
      "startOffset" : 37,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees.",
      "startOffset" : 37,
      "endOffset" : 212
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees.",
      "startOffset" : 37,
      "endOffset" : 235
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison).",
      "startOffset" : 37,
      "endOffset" : 421
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison). Among the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.",
      "startOffset" : 37,
      "endOffset" : 814
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison). Among the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.). Although linear metrics are convenient to optimize, they are not able to capture the non-linear structure of the data; some non-linear metric learning algorithms have been developed and compose a second group of metric learning approaches. Most non-linear metric learning are kernelized version of linear metrics learning approaches (Kedem et al. (2012); He et al.",
      "startOffset" : 37,
      "endOffset" : 1186
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison). Among the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.). Although linear metrics are convenient to optimize, they are not able to capture the non-linear structure of the data; some non-linear metric learning algorithms have been developed and compose a second group of metric learning approaches. Most non-linear metric learning are kernelized version of linear metrics learning approaches (Kedem et al. (2012); He et al. (2013)) and present the drawback of the choice of the kernel.",
      "startOffset" : 37,
      "endOffset" : 1204
    }, {
      "referenceID" : 0,
      "context" : "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison). Among the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.). Although linear metrics are convenient to optimize, they are not able to capture the non-linear structure of the data; some non-linear metric learning algorithms have been developed and compose a second group of metric learning approaches. Most non-linear metric learning are kernelized version of linear metrics learning approaches (Kedem et al. (2012); He et al. (2013)) and present the drawback of the choice of the kernel. In Vikas Sindhwani & Belkin (2005), a deformed kernel is learn depending on the data geometry which will be used to the classification task resolve.",
      "startOffset" : 37,
      "endOffset" : 1294
    }, {
      "referenceID" : 15,
      "context" : "We can use the computed graph in order to predict the hidden label, through the well known label propagation algorithm (Zhu & Ghahramani (2002); Zhou et al.",
      "startOffset" : 120,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "We can use the computed graph in order to predict the hidden label, through the well known label propagation algorithm (Zhu & Ghahramani (2002); Zhou et al. (2004)); In order to remove the non relevant edges of the graph, a pruning phase is usually performed on the graph.",
      "startOffset" : 145,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "For each data set, we can compute a graph where the weights are computed by the the euclidean distance in, respectively, the initial space and the learned space for LMNN algorithm (Weinberger & Saul (2009)), the ExtraTrees feature selection algorithm (Geurts et al. (2006)) and our approach2.",
      "startOffset" : 252,
      "endOffset" : 273
    }, {
      "referenceID" : 1,
      "context" : "Implemented with torch7 (Collobert et al. (2011))",
      "startOffset" : 25,
      "endOffset" : 49
    } ],
    "year" : 2016,
    "abstractText" : "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. We claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
    "creator" : "LaTeX with hyperref package"
  }
}