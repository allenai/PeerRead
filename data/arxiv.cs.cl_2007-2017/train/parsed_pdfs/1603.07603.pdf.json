{
  "name" : "1603.07603.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semantic Regularities in Document Representations",
    "authors" : [ "Fei Sun", "Jiafeng Guo", "Yanyan Lan" ],
    "emails" : [ "ofey.sunfei@gmail.com", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n07 60\n3v 1\n[ cs\n.C L\n] 2\n4 M\nar 2\n01 6"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, Mikolov et al. (2013c) discovered that word representations learned by a recursive neural net (RNN) as well as by related log-linear models (Mikolov et al., 2013b) can capture the linguistic regularities in language, which allows easy solutions to analogy questions of the form “Beijing:China as Paris: ” using simple linear algebra. With this word analogy task, a flurry of subsequent work exhibited that similar linear structure can also be revealed from representations learned from other methods (Mnih and Kavukcuoglu, 2013; Pennington et al., 2014; Levy and Goldberg, 2014b).\nBesides word representation, document representation is also a fundamental and critical\nproblem in natural language processing. Over the past decades, various methods have been proposed to represent the document as a vector, including Bag of Words (BOW) (Harris, 1954), Latent Semantic Indexing (LSI) (Deerwester et al., 1990), Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999)) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Recently, there is a rising enthusiasm for applying the neural embedding methods to representing the documents (Srivastava et al., 2013; Le and Mikolov, 2014).\nIt is, therefore, natural to ask whether there is also linear structure in these learned document representations to allow similar reasoning at document level. For example, given three articles talking about naive bayes, logistic regression, and hidden markov model, is it possible to find the article about conditional random fields as the solution to the document analogy question “naive bayes: logistic regression as hidden Markov model: ” (i.e., document pairs explaining generative-discriminative model relations)? Obviously, such reasoning is much more complex in semantics and cannot be achieved by simple retrieval or classification based on lexical information. Representation with such linear structure would be useful for many semantic processing applications, e.g., it may help controversial search (Dori-Hacohen et al., 2015) by discovering document pairs talking about opposite facts on controversial topics with some seed pairs, or help non-local corpus navigation and paper recommendation together with word vectors (Dai et al., 2014).\nFor this purpose, we introduce a new document analogy task for evaluating the semantic regularities in document representations. Since it is non-trivial to directly label the analogy questions over documents, we leverage the existing word/phrase semantic analogy test set and map\nthe words/phrases in these questions to Wikipedia articles through title matching. In this way, we obtain a large labeled analogy test set over documents. The task is then to test whether different document representations over the Wikipedia articles can find the right answers to these semantic analogy questions.\nBased on this test set, we evaluate several existing state-of-the-art document representations and show that neural embedding based models can achieve better performance than conventional models. The major contributions of this paper include: 1) the introduction of a new document analogy task with benchmark dataset for evaluating document representations; 2) empirical comparison among state-of-the-art models and preliminary explanations over the results."
    }, {
      "heading" : "2 Measuring Semantic Regularities",
      "text" : ""
    }, {
      "heading" : "2.1 A Document Analogy Test Set",
      "text" : "We propose to create a document analogy test set so that we can quantitatively evaluate how well different document representations capture semantic regularities. Following the idea of word analogy task, we try to build a test set of analogy questions of the form “a is to b as c is to ”, where a, b, c are the identities of the documents. However, it is not trivial to directly label the relations between two arbitrary documents due to the diversity in topics. Fortunately, we found that each Wikipedia page is a concise document describing one specific concept, and thus the relations between the documents can be explained by their corresponding concepts. Therefore, we can convert the task of labeling between documents into that between concepts (which are of words or phrases), where we already have a large labeled data set from Mikolov et al. (2013b).\nBased on the idea above, we build a document analogy test set using Wikipedia and existing word and phrase analogy test set. Specifically, we adopt the publicly available April 2010 dump of Wikipedia1 (Shaoul and Westbury, 2010), which has been widely used in (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014). The corpus contains 3,035,070 articles and about 1 billion tokens. We then collect all the existing word and phrase analogy test sets and match the words/phrases in questions to Wikipedia page titles. Note here we do not take syntactic anal-\n1http://nlp.stanford.edu/data/WestburyLab.wikicorp.201004.txt.bz2\nogy questions of words into consideration because the relations between documents are usually semantic. By resolving the ambiguity in matching, we finally obtain 6112 analogy questions over Wikipedia documents. Table 1 shows the details of the test set."
    }, {
      "heading" : "2.2 Analogy Reasoning",
      "text" : "In this work, we adopt the same vector offset method (Mikolov et al., 2013c) for analogy reasoning. To answer the questions like “a is to b as c is to ”, we try to find a document with vector ~x, which is the closest to ~b − ~a + ~c according to the cosine similarity:\narg max x∈D,x 6=a x 6=b, x 6=c\n(~b+ ~c− ~a) · ~x (1)\nwhere ~b, ~c, ~a and ~x are the normalized document vectors. The question is judged as correctly answered only if x is exactly the answer document in the evaluation set. The evaluation metric for this task is the percentage of questions answered correctly."
    }, {
      "heading" : "3 Models",
      "text" : "In this section, we briefly summarize the models used in this paper. Before that, we first list the notations.\nLet D={d1, . . . , dN} denote a corpus of N documents over the word vocabulary V={w1, . . . , w|V |}. Let X ∈ R\nN×|V | be a document-word matrix, where entry xij in X denotes the weight of the j-th word wj in the i-th document dj .\nBag of Words (BOW) model treats a document as a bag (multiset) of its words. It represents a document di as a vector ~xi = (xi1, · · · , xi|V |), where xij denotes the weight of the j-th word wj in the i-th document dj . The most popular weighting scheme for xij is TF-IDF (Jones, 1972). However, The BOW model suffers from the sparsity and curse of dimensionality due to treating individual word as distinct feature.\nMatrix Factorization methods attempt to tackle the limitation of BOW model through learning a low-dimensional vector for document by factorizing the document-word matrix X.\nDeerwester et al. (1990) applied truncated Singular Value Decomposition (SVD) to document-\nord matrix, namely Latent Semantic Indexing\n(LSI). LSI approximates X by setting all but the largest k singular values in Σ to 0 (Σk), as\nX ≃ DΣkW T\nHence one might think of the rows of DΣk as representations for documents in the latent space.\nAn alternative way is factorizing X into two non-negative matrices (Lee and Seung, 1999),\nX ≃ DWT\nwhere the rows of D can be seen as the representations of documents.\nUnlike LSI which may have negative entries, NMF has better interpretability with the nonnegative constraint.\nTopic Models are also very popular in document representation fields because of their good interpretability, generalization ability and extensibility. The most representative work is the Latent Dirichlet Allocation (LDA) model introduced by Blei et al. (2003). It represents the documents as distributions over latent topics, where each topic is characterized by a distribution over words.\nNeural Embedding models have attracted much attention in text representations due to its breakthrough in statistical language model (Bengio et al., 2003).\nThe Paragraph Vector models are first introduced in (Le and Mikolov, 2014) for document representation. The Distributed Memory Model of Paragraph Vectors (PV-DM) captures the representation of a document via inserting a document vector to the continuous bag-of-words (CBOW) model (Mikolov et al., 2013a). A simpler model can be obtained by replacing the input word vector with document vector in Skip Gram (SG) model, which is called “Distributed Bag of Words version of Paragraph Vector” (PV-DBOW).\nBag of Word Embeddings (BOWE) model tries to represent the document as a linear combination of word vectors, where the word vectors W can be obtained by tools like Word2Vec or GloVe. The low-dimensional representations of documents in BOWE can be written as\nD = XW\nwhere X denotes the BOW representation of documents."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we first describe our experimental settings including the corpus, hyper-parameter selections, and specifications for different document representation methods. Then we compare these methods on document analogy task and discuss the results."
    }, {
      "heading" : "4.1 Corpus and Preprocessing",
      "text" : "The corpus used to learn document representations in this experiment is the same Wikipedia April 2010 dump as described in section 2.1. In preprocessing, we lowercase the corpus, remove pure digit words, non-English characters and the words occur less than 20 times."
    }, {
      "heading" : "4.2 Experimental Settings",
      "text" : "The baseline methods used in this paper including BOW with TF-IDF weight, LSI, NMF, LDA, PVDM, PV-DBOW, and BOWE. For BOW, LSI and LDA, we use the popular python topic model library gensim2. For NMF, we choose the python machine learning library scikit learn3. We implement PV-DM and PV-DBOW models in C++ due to Le and Mikolov (2014) have not released\n2http://radimrehurek.com/gensim 3http://scikit-learn.org\nsource codes of PV models. For word embeddings in BOWE, we use CBOW in the Word2Vec tool4. The negative sampling method is adopted to take the place of the hierarchal softmax since we found the former always achieves better performance. The learning rate is linearly decayed to 0 as described in (Mikolov et al., 2013a), where the initial learning rate of PV-DM and CBOW model is 0.05, and PV-DBOW is 0.025. We set context window size as 10 and use 10 negative samples."
    }, {
      "heading" : "4.3 Results",
      "text" : "In Table 2, we compare the results of 100- dimensional document vectors from all the methods on different subtasks of document analogy. As we can see, among all the methods, BOW is almost the worst. This demonstrates the weakness of simple vector space model on capturing semantic regularities.\nNeural embedding models such as PV-DM and PV-DBOW perform much better than conventional latent models such as LSI, NMF, and LDA. This is quite amazing since PV models can also be viewed as implicit matrix factorization according to the explanations on Word2Vec (Levy and Goldberg, 2014a). A major difference is that conventional latent models usually work on matrix with each entry standing for the frequency or TF-IDF of a word in a document, while PV models factorize a shifted pointwise mutual information (shifted-PMI) matrix. As discussed in (Arora et al., 2015), PMI is a key factor why Word2Vec can work well for word anal-\n4https://code.google.com/p/Word2Vec/\nogy task. We guess this might also be a major factor to explain the gap between PV models and other latent models. Therefore, we conducted further experiments on LSI. As a result, we find that the total accuracy of LSI can achieve 15.53% with PMI matrix (about 57% performance gain over LSI with TF-IDF matrix). The results indicate that PMI plays an important role in revealing linear structure in document representations.\nA surprising result is that the simple BOWE model performs significantly better than any other methods on almost all the subtasks (pvalue < 0.01). There might be two possible reasons for the result. Firstly, when learning word vectors alone with Word2Vec, one can achieve very high scores on word analogy tasks (Mikolov et al., 2013c). BOWE thus benefits from the strong linear structures in word vectors by directly using word vectors as the representation of a document. Secondly, the calculation of Euclidean distance5 between documents under BOWE is equivalent to using a relaxed Word Mover’s Distance (Kusner et al., 2015), which has been shown strong performance in measuring document distance.\nWe also conduct the experiments on different dimensions as shown in Table 3. Similar trending can be found as that in Table 2."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we introduce a new document analogy task for quantitatively evaluating how well\n5Note that dot product between normalized vectors in analogy reasoning is equivalent to Euclidean distance.\nTable 3: Results on the document analogy task under different dimensions.\nDIMENSION MODEL\n50 100 150 200\nBOW 1.34 1.34 1.34 1.34\nLSI 4.19 9.88 17.0 21.81\nNMF 1.59 4.81 8.75 11.85\nLDA 3.52 8.43 10.39 10.45\nPV-DM 25.62 37.47 37.71 36.03\nPV-DBOW 25.33 37.76 40.61 39.09\nBOWE 42.05 60.42 66.74 69.49\ndifferent document representations capture semantic regularities. Based on the introduced benchmark dataset, we conduct empirical comparisons among several state-of-the-art document representation methods. The results reveal that neural embedding based document representations work better on this analogy task. We provide some preliminary explanations on these observations, leaving the inherent differences of these models to be further investigated in the future. With this benchmark dataset, it would also be easier for us to develop new document representation models and to compare with existing methods."
    } ],
    "references" : [ {
      "title" : "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings. CoRR, abs/1502.03520",
      "author" : [ "Arora et al.2015] Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski" ],
      "venue" : null,
      "citeRegEx" : "Arora et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Bengio et al.2003] Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Document embedding with paragraph vectors",
      "author" : [ "Dai et al.2014] Andrew M Dai", "Christopher Olah", "Quoc V Le", "Greg S Corrado" ],
      "venue" : "NIPS Deep Learning Workshop",
      "citeRegEx" : "Dai et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2014
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman" ],
      "venue" : "Journal of the American Society for Information",
      "citeRegEx" : "Deerwester et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Navigating controversy as a complex search task",
      "author" : [ "Elad Yom-Tov", "James Allan" ],
      "venue" : "In Proceedings of the First International Workshop on Supporting Complex Search Tasks",
      "citeRegEx" : "Dori.Hacohen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dori.Hacohen et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Huang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "A statistical interpretation of term specificity and its application in retrieval",
      "author" : [ "Karen Sprck Jones" ],
      "venue" : "Journal of Documentation,",
      "citeRegEx" : "Jones.,? \\Q1972\\E",
      "shortCiteRegEx" : "Jones.",
      "year" : 1972
    }, {
      "title" : "From word embeddings to document distances",
      "author" : [ "Yu Sun", "Nicholas I. Kolkin", "Kilian Q. Weinberger" ],
      "venue" : "In Proceedings of the 32th International Conference on Machine Learning",
      "citeRegEx" : "Kusner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kusner et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Le", "Mikolov2014] Quoc Le", "Tomas Mikolov" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Le et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization",
      "author" : [ "Lee", "Seung1999] Daniel D. Lee", "H. Sebastian Seung" ],
      "venue" : "october",
      "citeRegEx" : "Lee et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 1999
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Levy", "Goldberg2014a] Omer Levy", "Yoav Goldberg" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Levy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2014
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Richard Socher", "Christopher D. Manning" ],
      "venue" : "In Proceedings of the Seventeenth Conference on Computational Natural",
      "citeRegEx" : "Luong et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "In Proceedings of Workshop of ICLR",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Wen tau Yih", "Geoffrey Zweig" ],
      "venue" : "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning word embeddings efficiently with noise-contrastive estimation",
      "author" : [ "Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient non-parametric estimation of multiple embeddings per word in vector space",
      "author" : [ "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum" ],
      "venue" : "In Proceedings of the 2014 Conference on Em-",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2014
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Richard Socher", "Christopher D. Manning" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "The westbury lab wikipedia corpus. Edmonton, AB: University of Alberta",
      "author" : [ "Shaoul", "Westbury2010] Cyrus Shaoul", "Chris Westbury" ],
      "venue" : null,
      "citeRegEx" : "Shaoul et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shaoul et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling documents with deep boltzmann machines",
      "author" : [ "Ruslan Salakhutdinov", "Geoffrey E. Hinton" ],
      "venue" : "In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Recently, Mikolov et al. (2013c) discovered that word representations learned by a recur-",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 18,
      "context" : "other methods (Mnih and Kavukcuoglu, 2013; Pennington et al., 2014; Levy and Goldberg, 2014b).",
      "startOffset" : 14,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "Over the past decades, various methods have been proposed to represent the document as a vector, including Bag of Words (BOW) (Harris, 1954), Latent Semantic Indexing (LSI) (Deerwester et al., 1990), Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999)) and Latent Dirichlet Allocation (LDA) (Blei et al.",
      "startOffset" : 173,
      "endOffset" : 198
    }, {
      "referenceID" : 2,
      "context" : ", 1990), Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999)) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : "Recently, there is a rising enthusiasm for applying the neural embedding methods to representing the documents (Srivastava et al., 2013; Le and Mikolov, 2014).",
      "startOffset" : 111,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : ", it may help controversial search (Dori-Hacohen et al., 2015) by discovering document pairs talking about oppo-",
      "startOffset" : 35,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "site facts on controversial topics with some seed pairs, or help non-local corpus navigation and paper recommendation together with word vectors (Dai et al., 2014).",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "data set from Mikolov et al. (2013b). Based on the idea above, we build a document analogy test set using Wikipedia and existing word and phrase analogy test set.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "adopt the publicly available April 2010 dump of Wikipedia1 (Shaoul and Westbury, 2010), which has been widely used in (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014).",
      "startOffset" : 118,
      "endOffset" : 184
    }, {
      "referenceID" : 12,
      "context" : "adopt the publicly available April 2010 dump of Wikipedia1 (Shaoul and Westbury, 2010), which has been widely used in (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014).",
      "startOffset" : 118,
      "endOffset" : 184
    }, {
      "referenceID" : 17,
      "context" : "adopt the publicly available April 2010 dump of Wikipedia1 (Shaoul and Westbury, 2010), which has been widely used in (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014).",
      "startOffset" : 118,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "The most popular weighting scheme for xij is TF-IDF (Jones, 1972).",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "The most representative work is the Latent Dirichlet Allocation (LDA) model introduced by Blei et al. (2003). It represents the documents as distributions over latent topics, where each topic is characterized by a distribution over words.",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "model (Bengio et al., 2003).",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "As discussed in (Arora et al., 2015), PMI is a key factor why Word2Vec can work well for word anal-",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "Secondly, the calculation of Euclidean distance5 between documents under BOWE is equivalent to using a relaxed Word Mover’s Distance (Kusner et al., 2015), which has",
      "startOffset" : 133,
      "endOffset" : 154
    } ],
    "year" : 2016,
    "abstractText" : "Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-theart document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations.",
    "creator" : "LaTeX with hyperref package"
  }
}