{
  "name" : "1703.07432.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient PAC Learning from the Crowd",
    "authors" : [ "Pranjal Awasthi", "Avrim Blum", "Nika Haghtalab", "Yishay Mansour" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n07 43\n2v 2\n[ cs\n.L G\n] 1\n3 A\npr 2\n01 7\nIn recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.\nIn this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in F and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any F that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good."
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the last decade, research in machine learning and AI has seen tremendous growth, partly due to the ease\nwith which we can collect and annotate massive amounts of data across various domains. This rate of data annotation has been made possible due to crowdsourcing tools, such as Amazon Mechanical TurkTM, that\nfacilitate individuals’ participation in a labeling task. In the context of classification, a crowdsourced model\nuses a large pool of workers to gather labels for a given training data set that will be used for the purpose\nof learning a good classifier. Such learning environments that involve the crowd give rise to a multitude of\ndesign choices that do not appear in traditional learning environments. These include: How does the goal of learning from the crowd differs from the goal of annotating data by the crowd? What challenges does\nthe high amount of noise typically found in curated data sets [Wais et al., 2010, Kittur et al., 2008, Ipeirotis\net al., 2010] pose to the learning algorithms? How do learning and labeling processes interplay? How many\nlabels are we willing to take per example? And, how much load can a labeler handle?\n∗Rutgers University, pranjal.awasthi@rutgers.edu †Carnegie Mellon University, avrim@cs.cmu.edu. Supported in part by NSF grants CCF-1525971 and CCF-1535967. This work was done in part while the author was visiting the Simons Institute for the Theory of Computing. ‡Carnegie Mellon University, nhaghtal@cs.cmu.edu. Supported in part by NSF grants CCF-1525971 and CCF-1535967 and a Microsoft Research Ph.D. Fellowship. This work was done in part while the author was visiting the Simons Institute for the Theory of Computing. §Blavatnik School of Computer Science, Tel-Aviv University, mansour@tau.ac.il. This work was done while the author was at Microsoft Research, Herzliya. Supported in part by a grant from the Science Foundation (ISF), by a grant from United States-Israel Binational Science Foundation (BSF), and by The Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11).\nIn recent years, there have been many exciting works addressing various theoretical aspects of these\nand other questions [Slivkins and Vaughan, 2014], such as reducing noise in crowdsourced data [Dekel and\nShamir, 2009], task assignment [Badanidiyuru et al., 2013, Tran-Thanh et al., 2014] in online or offline\nsettings [Karger et al., 2014], and the role of incentives [Ho et al., 2013]. In this paper we focus on one\nsuch aspect, namely, how to efficiently learn and generalize from the crowd with minimal cost? The standard\napproach is to view the process of acquiring labeled data through crowdsourcing and the process of learning\na classifier in isolation. In other words, a typical learning process involves collecting data labeled by many\nlabelers via a crowdsourcing platform followed by running a passive learning algorithm to extract a good\nhypothesis from the labeled data. As a result, approaches to crowdsourcing focus on getting high quality\nlabels per example and not so much on the task further down in the pipeline. Naive techniques such as taking majority votes to obtain almost perfect labels have a cost per labeled example that scales with the data size, namely log(mδ ) queries per label where m is the training data size and δ is the desired failure probability. This is undesirable in many scenarios when data size is large. Furthermore, if only a small fraction of the\nlabelers in the crowd are perfect, such approaches will inevitably fail. An alternative is to feed the noisy\nlabeled data to existing passive learning algorithms. However, we currently lack computationally efficient\nPAC learning algorithms that are provably robust to high amounts of noise that exists in crowdsourced data.\nHence separating the learning process from the data annotation process results in high labeling costs or\nsuboptimal learning algorithms.\nIn light of the above, we initiate the study of designing efficient PAC learning algorithms in a crowd-\nsourced setting where learning and acquiring labels are done in tandem. We consider a natural model of\ncrowdsourcing and ask the fundamental question of whether efficient learning with little overhead in labeling cost is possible in this scenario. We focus on the classical PAC setting of Valiant [1984] where there exists a true target classifier f∗ ∈ F and the goal is to learn F from a finite training set generated from the underlying distribution. We assume that one has access to a large pool of labelers that can provide (noisy)\nlabels for the training set. We seek algorithms that run in polynomial time and produce a hypothesis with\nsmall error. We are especially interested in settings where there are computationally efficient algorithms for learning F in the consistency model, i.e. the realizable PAC setting. Additionally, we also want our algorithms to make as few label queries as possible, ideally requesting a total number of labels that is within a constant factor of the amount of labeled data needed in the realizable PAC setting. We call this O(1) overhead or cost per labeled example. Furthermore, in a realistic scenario each labeler can only provide labels\nfor a constant number of examples, hence we cannot ask too many queries to a single labeler. We call the\nnumber of queries asked to a particular labeler the load of that labeler.\nPerhaps surprisingly, we show that when a noticeable fraction of the labelers in our pool are perfect all of the above objectives can be achieved simultaneously. That is, if F can be efficiently PAC learned in the realizable PAC model, then it can be efficiently PAC learned in the noisy crowdsourcing model with a\nconstant cost per labeled example. In other words, the ratio of the number of label requests in the noisy\ncrowdsourcing model to the number of labeled examples needed in the traditional PAC model with a perfect\nlabeler is a constant and does not increase with the size of the data set. Additionally, each labeler is asked to label only a constant number of examples, i.e., O(1) load per labeler. Our results also answer an open question of Dekel and Shamir [2009] regarding the possibility of efficient noise robust PAC learning by\nperforming labeling and learning simultaneously. When no perfect labelers exist, a related task is to find a\nset of the labelers which are good but not perfect. We show that we can identify the set of all good labelers,\nwhen at least the majority of labelers are good."
    }, {
      "heading" : "1.1 Overview of Results",
      "text" : "We study various versions of the model described above. In the most basic setting we assume that a large percentage, say 70% of the labelers are perfect, i.e., they always label according to the target function f∗.\nThe remaining 30% of the labelers could behave arbitrarily and we make no assumptions on them. Since the perfect labelers are in strong majority, a straightforward approach is to label each example with the\nmajority vote over a few randomly chosen labelers, to produce the correct label on every instance with high probability. However, such an approach leads to a query bound of O(log mδ ) per labeled example, where m is the size of the training set and δ is the acceptable probability of failure. In other words, the cost per labeled example is O(log mδ ) and scales with the size of the data set. Another easy approach is to pick a few labelers at random and ask them to label all the examples. Here, the cost per labeled example is a constant but the\napproach is infeasible in a crowdsourcing environment since it requires a single or a constant number of\nlabelers to label the entire data set. Yet another approach is to label each example with the majority vote of O(log 1ǫ ) labelers. While the labeled sample set created in this way only has error of ǫ, it is still unsuitable for being used with PAC learning algorithms as they are not robust to even small amounts of noise, if the noise is\nheterogeneous. So, the computational challenges still persist. Nevertheless, we introduce an algorithm that performs efficient learning with O(1) cost per labeled example and O(1) load per labeler.\nTheorem 4.3 (Informal) Let F be a hypothesis class that can be PAC learned in polynomial time to ǫ error with probability 1 − δ using mǫ,δ samples. Then F can be learned in polynomial time using O(mǫ,δ) samples in a crowdsourced setting with O(1) cost per labeled example, provided a 12 +Θ(1) fraction of the labelers are perfect. Furthermore, every labeler is asked to label only 1 example.\nNotice that the above theorem immediately implies that each example is queried only O(1) times on average as opposed to the data size dependent O(log(mδ )) cost incurred by the naive majority vote style procedures. We next extend our result to the setting where the fraction of perfect labelers is significant but might be less than 12 , say 0.4. Here we again show that F can be efficiently PAC learned using O(mǫ,δ) queries provided we have access to an “expert” that can correctly label a constant number of examples. We call such queries that are made to an expert golden queries. When the fraction of perfect labelers is close to 1 2 , say 0.4, we show that just one golden query is enough to learn. More generally, when the fraction of the perfect labelers is some α, we show that O(1/α) golden queries is sufficient to learn a classifier efficiently. We describe our results in terms of α, but we are particularly interested in regimes where α = Θ(1).\nTheorem 4.13 (Informal) Let F be a hypothesis class that can be PAC learned in polynomial time to ǫ error with probability 1 − δ using mǫ,δ samples. Then F can be learned in polynomial time using O(mǫ,δ) samples in a crowdsourced setting with O( 1α) cost per labeled example, provided more than an α fraction of the labelers are perfect for some constant α > 0. Furthermore, every labeler is asked to label only O( 1α ) examples and the algorithm uses at most 2α golden queries.\nThe above two theorems highlight the importance of incorporating the structure of the crowd in al-\ngorithm design. Being oblivious to the labelers will result in noise models that are notoriously hard. For\ninstance, if one were to assume that each example is labeled by a single random labeler drawn from the\ncrowd, one would recover the Malicious Misclassification Noise of Rivest and Sloan [1994]. Getting computationally efficient learning algorithms even for very simple hypothesis classes has been a long standing\nopen problem in this space. Our results highlight that by incorporating the structure of the crowd, one can\nefficiently learn any hypothesis class with a small overhead.\nFinally, we study the scenario when none of the labelers are perfect. Here we assume that the majority of the labelers are “good”, that is they provide labels according to functions that are all ǫ-close to the target function. In this scenario generating a hypothesis of low error is as hard as agnostic learning1. Nonetheless, we show that one can detect all of the good labelers using expected O(1ǫ log(n)) queries per labeler, where n is the target number of labelers desired in the pool.\nTheorem 5.1 (Informal) Assume we have a target set of n labelers that are partitioned into two sets, good and bad. Furthermore, assume that there are at least n2 good labelers who always provide labels according to\n1This can happen for instance when all the labelers label according to a single function f that is ǫ-far from f∗.\nfunctions that are ǫ-close to a target function f∗. The set of bad labelers always provide labels according to functions that are at least 4ǫ away from the target. Then there is a polynomial time algorithm that identifies, with probability at least 1−δ, all the good labelers and none of the bad labelers using expected O(1ǫ log(nδ )) queries per labeler."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "Crowdsourcing has received significant attention in the machine learning community. As mentioned in the\nintroduction, crowdsourcing platforms require one to address several questions that are not present in tradi-\ntional modes of learning.\nThe work of Dekel and Shamir [2009] shows how to use crowdsourcing to reduce the noise in a training\nset before feeding it to a learning algorithm. Our results answer an open question in their work by showing\nthat performing data labeling and learning in tandem can lead to significant benefits.\nA large body of work in crowdsourcing has focused on the problem of task assignment. Here, workers\narrive in an online fashion and a requester has to choose to assign specific tasks to specific workers. Ad-\nditionally, workers might have different abilities and might charge differently for the same task. The goal\nfrom the requester’s point of view is to finish multiple tasks within a given budget while maintaining a certain minimum quality [Ho et al., 2013, Tran-Thanh et al., 2014]. There is also significant work on dynamic\nprocurement where the focus is on assigning prices to the given tasks so as to provide incentive to the crowd\nto perform as many of them as possible within a given budget [Badanidiyuru et al., 2012, 2013, Singla and\nKrause, 2013]. Unlike our setting, the goal in these works is not to obtain a generalization guarantee or learn\na function, but rather to complete as many tasks as possible within the budget.\nThe work of Karger et al. [2011, 2014] also studies the problem of task assignment in offline and on-\nline settings. In the offline setting, the authors provide an algorithm based on belief propagation that infers\nthe correct answers for each task by pooling together the answers from each worker. They show that their\napproach performs better than simply taking majority votes. Unlike our setting, their goal is to get an approx-\nimately correct set of answers for the given data set and not to generalize from the answers. Furthermore,\ntheir model assumes that each labeler makes an error at random independently with a certain probability. We, on the other hand, make no assumptions on the nature of the bad labelers.\nAnother related model is the recent work of Steinhardt et al. [2016]. Here the authors look at the problem\nof extracting top rated items by a group of labelers among whom a constant fraction are consistent with the\ntrue ratings of the items. The authors use ideas from matrix completion to design an algorithm that can recover the top rated items with an ǫ fraction of the noise provided every labeler rates ∼ 1 ǫ4 items and one has access to ∼ 1 ǫ2 ratings from a trusted expert. Their model is incomparable to ours since their goal is to recover the top rated items and not to learn a hypothesis that generalizes to a test set.\nOur results also shed insights into the notorious problem of PAC learning with noise. Despite decades\nof research into PAC learning, noise tolerant polynomial time learning algorithms remain elusive. There\nhas been substantial work on PAC learning under realistic noise models such as the Massart noise or the\nTsybakov noise models [Bousquet et al., 2005]. However, computationally efficient algorithms for such models are known in very restricted cases [Awasthi et al., 2015, 2016]. In contrast, we show that by using\nthe structure of the crowd, one can indeed design polynomial time PAC learning algorithms even when the\nnoise is of the type mentioned above.\nMore generally, interactive models of learning have been studied in the machine learning commu-\nnity [Cohn et al., 1994, Dasgupta, 2005, Balcan et al., 2006, Koltchinskii, 2010, Hanneke, 2011, Zhang\nand Chaudhuri, 2015, Yan et al., 2016]. We describe some of these works in Appendix A."
    }, {
      "heading" : "2 Model and Notations",
      "text" : "Let X be an instance space and Y = {+1,−1} be the set of possible labels. A hypothesis is a function f : X → Y that maps an instance x ∈ X to its classification y. We consider the realizable setting where there is a distribution over X × Y and a true target function in hypothesis class F . More formally, we consider a distribution D over X ×Y and an unknown hypothesis f∗ ∈ F , where errD(f∗) = 0. We denote the marginal of D over X by D|X . The error of a hypothesis f with respect to distribution D is defined as errD(f) = Pr(x,f∗(x))∼D[f(x) 6= f∗(x)].\nIn order to achieve our goal of learning f∗ well with respect to distribution D, we consider having access to a large pool of labelers, some of whom label according to f∗ and some who do not. Formally, labeler i is defined by its corresponding classification function gi : X → Y . We say that gi is perfect if errD(gi) = 0. We consider a distribution P that is uniform over all labelers and let α = Pri∼P [errD(gi) = 0] be the fraction of perfect labelers. We allow an algorithm to query labelers on instances drawn from D|X . Our goal is to design learning algorithms that efficiently learn a low error classifier while maintaining a small\noverhead in the number of labels. We compare the computational and statistical aspects of our algorithms to\ntheir PAC counterparts in the realizable setting.\nIn the traditional PAC setting with a realizable distribution, mǫ,δ denotes the number of samples needed for learning F . That is, mǫ,δ is the total number of labeled samples drawn from the realizable distribution D needed to output a classifier f that has errD(f) ≤ ǫ, with probability 1 − δ. We know from the VC theory [Anthony and Bartlett, 1999], that for a hypothesis class F with VC-dimension d and no additional assumptions on F , mǫ,δ ∈ O ( ǫ−1 ( d ln ( 1 ǫ ) + ln ( 1 δ )))\n. Furthermore, we assume that efficient algorithms for the realizable setting exist. That is, we consider an oracleOF that for a set of labeled instances S, returns a function f ∈ F that is consistent with the labels in S, if one such function exists, and outputs “None” otherwise.\nGiven an algorithm in the noisy crowd-sourcing setting, we define the average cost per labeled example of the algorithm, denoted by Λ, to be the ratio of the number of label queries made by the algorithm to the number of labeled examples needed in the traditional realizable PAC model,mǫ,δ. The load of an algorithm, denoted by λ, is the maximum number of label queries that have to be answered by an individual labeler. In other words, λ is the maximum number of labels queried from one labeler, when P has an infinitely large support. 2 When the number of labelers is fixed, such as in Section 5, we define the load to simply be the\nnumber of queries answered by a single labeler. Moreover, we allow an algorithm to directly query the target hypothesis f∗ on a few, e.g., O(1), instances drawn from D|X . We call these “golden queries” and denote their total number by Γ.\nGiven a set of labelers L and an instance x ∈ X , we define MajL(x) to be the label assigned to x by the majority of labelers in L. Moreover, we denote by Maj-sizeL(x) the fraction of the labelers in L that agree with the label MajL(x). Given a set of classifiers H , we denote by MAJ (H) the classifier that for each x returns prediction MajH(x). Given a distribution P over labelers and a set of labeled examples S, we denote by P|S the distribution P conditioned on labelers that agree with labeled samples (x, y) ∈ S. We consider S to be small, typically of size O(1). Note that we can draw a labeler from P|S by first drawing a labeler according to P and querying it on all the labeled instances in S. Therefore, when P has infinitely large support, the load of an algorithm is the maximum size of S that P is ever conditioned on.\n2The concepts of total number of queries and load may be seen as analogous to work and depth in parallel algorithms, where work is the total number of operations performed by an algorithm and depth is the maximum number of operations that one processor has to perform in a system with infinitely many processors."
    }, {
      "heading" : "3 A Baseline Algorithm and a Road-map for Improvement",
      "text" : "In this section, we briefly describe a simple algorithm and the approach we use to improve over it. Consider a very simple baseline algorithm for the case of α > 12 :\nBASELINE: Draw a sample of size m = mǫ,δ from D|X and label each x ∈ S by MajL(x), where L ∼ P k for k = O ( (α− 0.5)−2 ln (\nm δ\n))\nis a set of randomly drawn labelers. Return\nclassifier OF (S). That is, the baseline algorithm queries enough labelers on each sample such that with probability 1 − δ all the labels are correct. Then, it learns a classifier using this labeled set. It is clear that the performance of BASELINE is far from being desirable. First, this approach takes log(m/δ) more labels than it requires samples, leading to an average cost per labeled example that increases with the size of the sample set. Moreover, when perfect labelers form a small majority of the labelers, i.e., α = 12 + o(1), the number of labels needed to correctly label an instance increases drastically. Perhaps even more troubling is that if the perfect labelers are in minority, i.e., α < 12 , S may be mislabeled andOF (S) may return a classifier that has large error, or no classifier at all. In this work, we improve over BASELINE in both aspects.\nIn Section 4, we improve the log(m/δ) average cost per labeled example by interleaving the two processes responsible for learning a classifier and querying labels. In particular, BASELINE first finds high\nquality labels, i.e., labels that are correct with high probability, and then learns a classifier that is consistent\nwith those labeled samples. However, interleaving the process of learning and acquiring high quality labels can make both processes more efficient. At a high level, for a given classifier h that has a larger than desirable error, one may be able to find regions where h performs particularly poorly. That is, the classifications provided by h may differ from the correct label of the instances. In turn, by focusing our effort for getting high quality labels on these regions we can output a correctly labeled sample set using less label queries overall. These additional correctly labeled instances from regions where h performs poorly can help us improve the error rate of h in return. In Section 4, we introduce an algorithm that draws on ideas from boosting and a probabilistic filtering approach that we develop in this work to facilitate interactions between learning\nand querying.\nIn Section 4.1, we remove the dependence of label complexity on (α − 0.5)−2 using O(1/α) golden queries. At a high level, instances where only a small majority of labelers agree are difficult to label using\nqueries asked from labelers. But, these instances are great test cases that help us identify a large fraction of\nimperfect labelers. That is, we can first ask a golden query on one such instance to get its correct label and\nfrom then on only consider labelers that got this label correctly. In other words, we first test the labelers on\none or very few tests questions, if they pass the tests, then we ask them real label queries for the remainder\nof the algorithm, if not, we never consider them again."
    }, {
      "heading" : "4 An Interleaving Algorithm",
      "text" : "In this section, we improve over the average cost per labeled example of the BASELINE algorithm, by inter-\nleaving the process of learning and acquiring high quality labels. Our Algorithm 2 facilitates the interactions\nbetween the learning process and the querying process using ideas from classical PAC learning and adaptive techniques we develop in this work. For ease of presentation, we first consider the case where α = 12+Θ(1), say α ≥ 0.7, and introduce an algorithm and techniques that work in this regime. In Section 4.1, we show how our algorithm can be modified to work with any value of α. For convenience, we assume in the analysis below that distribution D is over a discrete space. This is in fact without loss of generality, since using uniform convergence one can instead work with the uniform distribution over an unlabeled sample multiset of size O( d ǫ2 ) drawn from D|X .\nHere, we provide an overview of the techniques and ideas used in this algorithm.\nBoosting: In general, boosting algorithms [Schapire, 1990, Freund, 1990, Freund and Schapire, 1995] provide a mechanism for producing a classifier of error ǫ using learning algorithms that are only capable of producing classifiers with considerably larger error rates, typically of error p = 12 − γ for small γ. In particular, early work of Schapire [1990] in this space shows how one can combine 3 classifiers of error p to get a classifier of error O(p2), for any p > 0.\nTheorem 4.1 (Schapire [1990]). For any p > 0 and distributionD, consider three classifiers: 1) classifier h1 such that errD(h1) ≤ p; 2) classifier h2 such that errD2(h2) ≤ p, whereD2 = 12DC+ 12DI for distributions DC and DI that denote distribution D conditioned on {x | h1(x) = f∗(x)} and {x | h1(x) 6= f∗(x)}, respectively; 3) classifier h3 such that errD3(h3) ≤ p, where D3 is D conditioned on {x | h1(x) 6= h2(x)}. Then, errD(MAJ (h1, h2, h3)) ≤ 3p2 − 2p3.\nAs opposed to the main motivation for boosting where the learner only has access to a learning algorithm of error p = 12 − γ, in our setting we can learn a classifier to any desired error rate p as long as we have a sample set of mp,δ correctly labeled instances. The larger the error rate p, the smaller the total number of label queries needed for producing a correctly labeled set of the appropriate size. We use this idea in Algorithm 2. In particular, we learn classifiers of error O( √ ǫ) using sample sets of size O(m√ǫ,δ) that are labeled by majority vote of O(log(m√ǫ,δ)) labelers, using fewer label queries overall than BASELINE.\nProbabilistic Filtering: Given classifier h1, the second step of the classical boosting algorithm requires distribution D to be reweighed based on the correctness of h1. This step can be done by a filtering process as follows: Take a large set of labeled samples fromD and divide them to two sets depending on whether or not the instances are mislabeled by h1. Distribution D2, in which instances mislabeled by h1 make up half of the weight, can be simulated by picking each set with probability 12 and taking an instance from that set uniformly at random. To implement filtering in our setting, however, we would need to first get high quality labels for the set of instances used for simulating D2. Furthermore, this sample set is typically large, since at least 1pmp,δ random samples from D are needed to simulate D2 that has half of its weight on the points that\nh1 mislabels (which is a p fraction of the total points). In our case where p = O( √ ǫ), getting high quality labels for such a large sample set requires O ( mǫ,δ ln (mǫ,δ\nδ\n))\nlabel queries, which is as large as the total\nnumber of labels queried by BASELINE.\nAlgorithm 1 FILTER(S, h) Let SI = ∅ and N = log ( 1 ǫ ) . for x ∈ S do for t = 1, . . . , N do\nDraw a random labeler i ∼ P and let yt = gi(x) If t is odd and Maj(y1:t) = h(x), then break.\nend\nLet SI = SI ∪ {x}. // Reaches this step when for all t, Maj(y1:t) 6= h(x) end return SI\nIn this work, we introduce a probabilistic filtering approach, called FILTER, that only requires O (mǫ,δ) label queries, i.e., O(1) cost per labeled example. Given classifier h1 and an unlabeled sample set S, FILTER(S, h1) returns a set SI ⊆ S such that for any x ∈ S that is mislabeled by h1, x ∈ SI with probability at least Θ(1). Moreover, any x that is correctly labeled by h1 is most likely not included in SI . This procedure is described in detail in Algorithm 1. Here, we provide a brief description of its working: For any x ∈ S, FILTER queries one labeler at a time, drawn at random, until the majority of the labels it has\nacquired so far agree with h1(x), at which point FILTER removes x from consideration. On the other hand, if the majority of the labels never agree with h1(x), FILTER adds x to the output set SI . Consider x ∈ S that is correctly labeled by h. Since each additional label agrees with h1(x) = f\n∗(x) with probability ≥ 0.7, with high probability the majority of the labels on x will agree with f∗(x) at some point, in which case FILTER stops asking for more queries and removes x. As we show in Lemma 4.9 this happens within O(1) queries most of the time. On the other hand, for x that is mislabeled by h, a labeler agrees with h1(x) with probability ≤ 0.3. Clearly, for one set of random labelers —one snapshot of the labels queried by FILTER— the majority label agrees with h1(x) with a very small probability. As we show in Lemma 4.6, even when considering the progression of all labels queried by FILTER throughout the process, with probability Θ(1) the majority label never agrees with h1(x). Therefore, x is added to SI with probability Θ(1).\nSuper-sampling: Another key technique we use in this work is super-sampling. In short, this means that\nas long as we have the correct label of the sampled points and we are in the realizable setting, more samples\nnever hurt the algorithm. Although this seems trivial at first, it does play an important role in our approach. In particular, our probabilistic filtering procedure does not necessarily simulate D2 but a distribution D ′, such that Θ(1)d2(x) ≤ d′(x) for all x, where d2 and d′ are the densities of D2 and D′, respectively. At a high level, sampling Θ(m) instances fromD′ simulates a super-sampling process that samples m instances from D2 and then adds in some arbitrary instances. This is formally stated below and is proved in Appendix B.\nLemma 4.2. Given a hypothesis class F consider any two discrete distributions D and D′ such that for all x, d′(x) ≥ c · d(x) for an absolute constant c > 0, and both distributions are labeled according to f∗ ∈ F . There exists a constant c′ > 1 such that for any ǫ and δ, with probability 1− δ over a labeled sample set S of size c′mǫ,δ drawn from D′, OF (S) has error of at most ǫ with respect to distribution D.\nWith these techniques at hand, we present Algorithm 2. At a high level, the algorithm proceeds in three phases, one for each classifier used by Theorem 4.1. In Phase 1, the algorithm learns h1 such that errD(h1) ≤ 12 √ ǫ. In Phase 2, the algorithm first filters a set of size O(mǫ,δ) into the set SI and takes an additional set SC of Θ(m√ǫ,δ) samples. Then, it queries O(log( mǫ,δ δ )) labelers on each instance in SI and SC to get their correct labels with high probability. Next, it partitions these instances to two different sets based on whether or not h1 made a mistake on them. It then learns h2 on a sample set W that is drawn by weighting these two sets equally. As we show in Lemma 4.8, errD2(h2) ≤ 12 √ ǫ. In phase 3, the algorithm learns h3 on a sample set S3 drawn from D|X conditioned on h1 and h2 disagreeing. Finally, the algorithm returns MAJ (h1, h2, h3).\nTheorem 4.3 (α = 1 2 + Θ(1) case). Algorithm 2 uses oracle OF , runs in time poly(d, 1ǫ , ln(1δ )) and with probability 1 − δ returns f ∈ F with errD(f) ≤ ǫ, using Λ = O (√ ǫ log ( m√ǫ,δ δ ) + 1 ) cost per labeled example, Γ = 0 golden queries, and λ = 1 load. Note that when 1√ ǫ ≥ log ( m√ǫ,δ δ ) , the above cost per labeled sample is O(1).\nWe start our analysis of Algorithm 2 by stating that CORRECT-LABEL(S, δ) labels S correctly, with probability 1− δ. This is direct application of the Hoeffding bound and its proof is omitted.\nLemma 4.4. For any unlabeled sample set S, δ > 0, and S = CORRECT-LABEL(S, δ), with probability 1− δ, for all (x, y) ∈ S, y = f∗(x).\nNote that as a direct consequence of the above lemma, Phase 1 of Algorithm 2 achieves error of O( √ ǫ).\nLemma 4.5. In Algorithm 2, with probability 1− δ3 , errD(h1) ≤ 12 √ ǫ.\nAlgorithm 2 INTERLEAVING: BOOSTING BY PROBABILISTIC FILTERING FOR α = 12 +Θ(1) Input: Given a distribution D|X , a class of hypotheses F , parameters ǫ and δ. Phase 1:\nLet S1 = CORRECT-LABEL(S1, δ/6), for a set of sample S1 of size 2m√ǫ,δ/6 from D|X .\nLet h1 = OF (S1). Phase 2:\nLet SI = FILTER(S2, h1), for a set of samples S2 of size Θ(mǫ,δ) drawn from D|X . Let SC be a sample set of size Θ(m√ǫ,δ) drawn from D|X . Let SAll = CORRECT-LABEL(SI ∪ SC , δ/6). LetWI = {(x, y) ∈ SAll | y 6= h1(x)} and LetWC = SAll \\WI . Draw a sample set W of size Θ(m√ǫ,δ) from a distribution that equally weights WI and WC .\nLet h2 = OF (W ). Phase 3:\nLet S3 = CORRECT-LABEL(S3, δ/6), for a sample set S3 of size 2m√ǫ,δ/6 drawn fromD|X conditioned on h1(x) 6= h2(x). Let h3 = OF (S3).\nreturn Maj(h1, h2, h3).\nCORRECT-LABEL(S, δ): for x ∈ S do Let L ∼ P k for a set of k = O(log( |S|δ )) labelers drawn from P and S ← S ∪ {(x,MajL(x))}. end\nreturn S.\nNext, we prove that FILTER removes instances that are correctly labeled by h1 with good probability and retains instances that are mislabeled by h1 with at least a constant probability.\nLemma 4.6. Given any sample set S and classifier h, for every x ∈ S 1. If h(x) = f∗(x), then x ∈ FILTER(S, h) with probability < √ǫ. 2. If h(x) 6= f∗(x), then x ∈ FILTER(S, h) with probability ≥ 0.5.\nProof. For the first claim, note that x ∈ SI only if Maj(y1:t) 6= h(x) for all t ≤ N . Consider t = N time step. Since each random query agrees with f∗(x) = h(x) with probability ≥ 0.7 independently, majority of N = O(log(1/ √ ǫ)) labels are correct with probability at least 1 − √ǫ. Therefore, the probability that the majority label disagrees with h(x) = f∗(x) at every time step is at most √ ǫ.\nIn the second claim, we are interested in the probability that there exists some t ≤ N , for which Maj(y1:t) = h(x) 6= f∗(x). This is the same as the probability of return in biased random walks, also called the probability of ruin in gambling [Feller, 2008], where we are given a random walk that takes a step to the right with probability ≥ 0.7 and takes a step to the left with the remaining probability and we are interested in the probability that this walk ever crosses the origin to the left while taking N or even infinitely many steps. Using the probability of return for biased random walks (see Theorem E.1), the probability that Maj(y1:t) 6= f∗(x) ever is at most ( 1− ( 0.7 1−0.7 )N ) / ( 1− ( 0.7 1−0.7 )N+1 ) < 37 . Therefore, for each x such that h(x) 6= f∗(x), x ∈ SI with probability at least 4/7.\nIn the remainder of the proof, for ease of exposition we assume that not only errD(h1) ≤ 12 √ ǫ as per\nLemma 4.5, but in fact errD(h1) = 1 2\n√ ǫ. This assumption is not needed for the correctness of the results\nbut it helps simplify the notation and analysis. As a direct consequence of Lemma 4.6 and application of the\nChernoff bound, we deduce that with high probability W I , WC , and SI all have size Θ(m√ǫ,δ). The next lemma, whose proof appears in Appendix C, formalizes this claim.\nLemma 4.7. With probability 1− exp(−Ω(m√ǫ,δ)), W I , WC , and SI all have size Θ(m√ǫ,δ).\nThe next lemma combines the probabilistic filtering and super-sampling techniques to show that h2 has the desired error O( √ ǫ) on D2.\nLemma 4.8. Let DC and DI denote distribution D when it is conditioned on {x | h1(x) = f∗(x)} and {x | h1(x) 6= f∗(x)}, respectively, and letD2 = 12DI+ 12DC . With probability 1−2δ/3, errD2(h2) ≤ 12 √ ǫ.\nProof. Consider distribution D′ that has equal probability on the distributions induced by WI and WC and let d′(x) denote the density of point x in this distribution. Relying on our super-sampling technique (see Lemma 4.2), it is sufficient to show that for any x, d′(x) = Θ(d2(x)).\nFor ease of presentation, we assume that Lemma 4.5 holds with equality, i.e., errD(h1) is exactly 1 2\n√ ǫ\nwith probability 1− δ/3. Let d(x), d2(x), dC(x), and dI(x) be the density of instance x in distributions D, D2, DC , and DI , respectively. Note that, for any x such that h1(x) = f\n∗(x), we have d(x) = dC(x)(1 − 1 2 √ ǫ). Similarly, for any x such that h1(x) 6= f∗(x), we have d(x) = dI(x)12 √ ǫ. LetNC(x),NI(x),MC(x) andMI(x) be the number of occurrences of x in the sets SC , SI ,WC andWI , respectively. For any x, there are two cases:\nIf h1(x) = f ∗(x): Then, there exist absolute constants c1 and c2 according to Lemma 4.7, such that\nd′(x) = 1\n2 E\n[\nMC(x)\n|WC |\n]\n≥ E[MC(x)] c1 ·m√ǫ,δ ≥ E[NC(x)] c1 ·m√ǫ,δ = |SC | · d(x) c1 ·m√ǫ,δ\n= |SC | · dC(x) · (1− 12\n√ ǫ)\nc1 ·m√ǫ,δ ≥ c2dC(x) =\nc2d2(x)\n2 ,\nwhere the second and sixth transitions are by the sizes ofWC and |SC | and the third transition is by the fact that if h(x) = f∗(x), MC(x) > NC(x). If h1(x) 6= f∗(x): Then, there exist absolute constants c′1 and c′2 according to Lemma 4.7, such that\nd′(x) = 1\n2 E\n[\nMI(x)\n|WI |\n]\n≥ E[MI(x)] c′1 ·m√ǫ,δ ≥ E[NI(x)] c′1 ·m√ǫ,δ ≥ 4 7 d(x)|S2| c′1 ·m√ǫ,δ\n= 4 7 dI(x) 1 2\n√ ǫ · |S2|\nc′1 ·m√ǫ,δ ≥ c′2dI(x) = c′2d2(x) 2 ,\nwhere the second and sixth transitions are by the sizes of WI and |S2|, the third transition is by the fact that if h(x) 6= f∗(x), MI(x) > NI(x), and the fourth transition holds by part 2 of Lemma 4.6.\nUsing the super-sampling guarantees of Lemma 4.2, with probability 1−2δ/3, errD2(h2) ≤ √ ǫ/2.\nThe next claim shows that the probabilistic filtering step queries a few labels only. At a high level, this is achieved by showing that any instance x for which h1(x) = f ∗(x) contributes only O(1) queries, with high probability. On the other hand, instances that h1 mislabeled may each get log( 1 ǫ ) queries. But, because there are only few such points, the total number of queries these instances require is a lower order term. Lemma 4.9. Let S be a sample set drawn from distribution D and let h be such that errD(h) ≤ √ ǫ. With\nprobability 1− exp(−Ω(|S|√ǫ)), FILTER(S, h) makes O(|S|) label queries.\nProof. Using Chernoff bound, with probability 1 − exp (−|S|√ǫ) the total number of points in S where h disagrees with f∗ isO(|S|√ǫ). The number of queries spent on these points is at mostO (|S|√ǫ log(1/ǫ)) ≤ O(|S|).\nNext, we show that for each x such that h(x) = f∗(x), the number of queries taken until a majority of them agree with h(x) is a constant. Let us first show that this is the case in expectation. Let Ni be the expected number of labels queried until we have i more correct labels than incorrect ones. Then N1 ≤ 0.7(1) + 0.3(N2 + 1), since with probability at least α ≥ 0.7, we receive one more correct label and stop, and with probability ≤ 0.3 we get a wrong label in which case we have to get two more correct labels in future. Moreover, N2 = 2N1, since we have to get one more correct label to move from N2 to N1 and then one more. Solving these, we have that N1 ≤ 2.5. Therefore, the expected total number of queries is at most O(|S|). Next, we show that this random variable is also well-concentrated. Let Lx be a random variable that indicates the total number of queries on x before we have one more correct label than incorrect labels. Note that Lx is an unbounded random variable, therefore concentration bounds such as Hoeffding or Chernoff do not work here. Instead, to show that Lx is well-concentrated, we prove that the Bernstein inequality (see Theorem E.2) holds. That is, as we show in Appendix D, for any x, the Bernstein inequality is statisfied by the fact that for any i > 1, E[(Lx − E[Lx])i] ≤ 50(i + 1)! e4i. Therefore, over all instances in S, ∑\nx∈S Lx ∈ O(|S|) with probability 1− exp(−|S|).\nFinally, we have all of the ingredients needed for proving our main theorem.\nProof of Theorem 4.3. We first discuss the number of label queries Algorithm 2 makes. The total number of labels queried by Phases 1 and 3 is attributed to the labels queried by CORRECT-LABEL(S1, δ) and CORRECT-LABEL(S3, δ/6), which is O ( m√ǫ,δ log(m√ǫ,δ/δ) )\n. By Lemma 4.7, |SI ∪ SC | ≤ O(m√ǫ,δ) almost surely. Therefore, CORRECT-LABEL(SI ∪ SC , δ/6) contributes O ( m√ǫ,δ log(m√ǫ,δ/δ) ) labels. Moreover, as we showed in Lemma 4.9, FILTER(S2, h1) queries O(mǫ,δ) labels, almost surely. So, the total number of labels queried by Algorithm 2 is at most O ( m√ǫ,δ log ( m√ǫ,δ δ ) +mǫ,δ ) . This leads to Λ = O (√ ǫ log (\nm√ǫ,δ δ ) + 1 ) cost per labeled example.\nIt remains to show that MAJ (h1, h2, h3) has error ≤ ǫ on D. Since CORRECT-LABEL(S1, δ/6) and CORRECT-LABEL(S3, δ/6) return correctly labeled sets , errD(h1) ≤ 12 √ ǫ and errD3(h3) ≤ 12 √ ǫ, where\nD3 is distribution D conditioned on {x | h1(x) 6= h2(x)}. As we showed in Lemma 4.8, errD2(h2) ≤ 12 √ ǫ with probability 1 − 2δ/3. Using the boosting technique of Schapire [1990] described in Theorem 4.1, we conclude that MAJ (h1, h2, h3) has error ≤ ǫ on D."
    }, {
      "heading" : "4.1 The General Case of Any α",
      "text" : "In this section, we extend Algorithm 2 to handle any value of α, that does not necessarily satisfy α > 1 2 + Θ(1). We show that by using O( 1 α) golden queries, it is possible to efficiently learn any function class with a small overhead.\nThere are two key challenges that one needs to overcome when α < 12 + o(1). First, we can no longer assume that by taking the majority vote over a few random labelers we get the correct label of an instance. Therefore, CORRECT-LABEL(S, δ) may return a highly noisy labeled sample set. This is problematic, since efficiently learning h1, h2, and h3 using oracle OF crucially depends on the correctness of the input labeled set. Second, FILTER(S, h1) no longer “filters” the instances correctly based on the classification error of h1. In particular, FILTER may retain a constant fraction of instances where h1 is in fact correct, and it may throw out instances where h1 was incorrect with high probability. Therefore, the per-instance guarantees of Lemma 4.6 fall apart, immediately.\nWe overcome both of these challenges by using two key ideas outlined below.\nPruning: As we alluded to in Section 3, instances where only a small majority of labelers are in agreement\nare great for identifying and pruning away a noticeable fraction of the bad labelers. We call these instances good test cases. In particular, if we ever encounter a good test case x, we can ask a golden query y = f∗(x) and from then on only consider the labelers who got this test correctly, i.e., P ← P|{(x,y)}. Note that if we make our golden queries when Maj-sizeP (x) ≤ 1 − α2 , at least an α2 fraction of the labelers would be pruned. This can be repeated at mostO( 1α ) times before the number of good labelers form a strong majority, in which case Algorithm 2 succeeds. The natural question is how would we measure Maj-sizeP (x) using few label queries? Interestingly, CORRECT-LABEL(S, δ) can be modified to detect such good test cases by measuring the empirical agreement rate on a set L of O( 1 α2\nlog( |S|δ )) labelers. This is shown in procedure PRUNE-AND-LABEL as part Algorithm 3. That is, if Maj-sizeL(x) > 1− α/4, we take MajL(x) to be the label, otherwise we test and prune the labelers, and then restart the procedure. This ensures that whenever\nwe use a sample set that is labeled by PRUNE-AND-LABEL, we can be certain of the correctness of the\nlabels. This is stated in the following lemma, and proved in Appendix F.1.\nLemma 4.10. For any unlabeled sample set S, δ > 0, with probability 1−δ, either PRUNE-AND-LABEL(S, δ) prunes the set of labelers or S = PRUNE-AND-LABEL(S, δ) is such that for all (x, y) ∈ S, y = f∗(x).\nAs an immediate result, the first phase of Algorithm 3 succeeds in computing h1, such that errD(h1) ≤ 1 2 √ ǫ. Moreover, every time PRUNE-AND-LABEL prunes the set of labelers, the total fraction of good labeler among all remaining labelers increase. As we show, after O(1/α) prunings, the set of good labelers is guaranteed to form a large majority, in which case Algorithm 2 for the case of α = 12 + Θ(1) can be used. This is stated in the next lemma and proved in Appendix F.2.\nLemma 4.11. For any δ, with probability 1− δ, the total number of times that Algorithm 3 is restarted as a result of pruning is O( 1α).\nRobust Super-sampling: The filtering step faces a completely different challenge: Any point that is a good\ntest case can be filtered the wrong way. However, instances where still a strong majority of the labelers\nagree are not affected by this problem and will be filtered correctly. Therefore, as a first step we ensure that\nthe total number of good test cases that were not caught before FILTER starts is small. For this purpose, we start the algorithm by calling CORRECT-LABEL on a sample of size O(1ǫ log( 1 δ )), and if no test points were found in this set, then with high probability the total fraction of good test cases in the underlying distribution is at most ǫ2 . Since the fraction of good test cases is very small, one can show that except for\nan √ ǫ fraction, the noisy distribution constructed by the filtering process will, for the purposes of boosting,\nsatisfy the conditions needed for the super-sampling technique. Here, we introduce a robust version of the super-sampling technique to argue that the filtering step will indeed produce h2 of error O( √ ǫ).\nLemma 4.12 (Robust Super-Sampling Lemma). Given a hypothesis class F consider any two discrete distributions D and D′ such that except for an ǫ fraction of the mass under D, we have that for all x, d′(x) ≥ c · d(x) for an absolute constant c > 0 and both distributions are labeled according to f∗ ∈ F . There exists a constant c′ > 1 such that for any ǫ and δ, with probability 1− δ over a labeled sample set S of size c′mǫ,δ drawn from D′, OF (S) has error of at most 2ǫ with respect toD.\nBy combining these techniques at every execution of our algorithm we ensure that if a good test case\nis ever detected we prune a small fraction of the bad labelers and restart the algorithm, and if it is never detected, our algorithm returns a classifier of error ǫ.\nTheorem 4.13 (Any α). Suppose the fraction of the perfect labelers is α and let δ′ = cαδ for small enough constant c > 0. Algorithm 3 uses oracle OF , runs in time poly(d, 1α , 1ǫ , ln(1δ )), uses a training set of size\nAlgorithm 3 BOOSTING BY PROBABILISTIC FILTERING FOR ANY α Input: Given a distribution D|X and P , a class of hypothesis F , parameters ǫ, δ, and α. Phase 0:\nIf α > 34 , run Algorithm 2 and quit. Let δ′ = cαδ for small enough c > 0 and draw S0 of O(1ǫ log( 1 δ′ )) examples from the distribution D.\nPRUNE-AND-LABEL(S0, δ ′).\nPhase 1:\nLet S1 = PRUNE-AND-LABEL(S1, δ ′), for a set of sample S1 of size 2m√ǫ,δ′ from D.\nLet h1 = OF (S1). Phase 2:\nLet SI = FILTER(S2, h1), for a set of samples S2 of size Θ(mǫ,δ′) drawn from D. Let SC be a sample set of size Θ(m√ǫ,δ′) drawn from D. Let SAll = PRUNE-AND-LABEL(SI ∪ SC , δ′). Let WI = {(x, y) ∈ SAll | y 6= h1(x)} and LetWC = SAll \\WI . Draw a sample set W of size Θ(m√ǫ,δ′) from a distribution that equally weights WI and WC .\nLet h2 = OF (W ). Phase 3:\nLet S3 = PRUNE-AND-LABEL(S3, δ ′), for a sample set S3 of size 2m√ǫ,δ′ drawn from D conditioned\non h1(x) 6= h2(x). Let h3 = OF (S3). return Maj(h1, h2, h3).\nPRUNE-AND-LABEL(S, δ): for x ∈ S do Let L ∼ P k for a set of k = O( 1α2 log( |S| δ )) labelers drawn from P .\nif Maj-sizeL(x) ≤ 1− α4 then Get a golden query y∗ = f∗(x), Restart Algorithm 3 with distribution P ← P|{(x,y∗)} and α ← α1−α\n8\n.\nelse\nS ← S ∪ {(x,MajL(x))}. end\nend\nreturn S.\nO( 1αmǫ,δ′) size and with probability 1 − δ returns f ∈ F with errD(f) ≤ ǫ using O( 1α) golden queries, load of 1α per labeler, and a total number of queries\nO\n(\n1 α mǫ,δ′ + 1 αǫ log( 1 δ′ ) log( 1 ǫδ′ ) + 1 α3 m√ǫ,δ′ log(\nm√ǫ,δ′\nδ′ )\n)\n.\nNote that when 1 α2 √ ǫ ≥ log\n( m√ǫ,δ αδ ) and log( 1αδ ) < d, the cost per labeled query is O( 1 α ).\nProof Sketch. Let B = {x | Maj-sizeP (x) ≤ 1 − α/2} be the set of good test cases and let β = D[B] be the total density on such points. Note that if β > ǫ4 , with high probability S0 includes one such point, in which case PRUNE-AND-LABEL identifies it and prunes the set of labelers. Therefore, we can assume that β ≤ ǫ4 . By Lemma 4.10, it is easy to see that Phase 1 and Phase 3 of Algorithm 3 succeed in producing h1 and\nh3 such that errD(h1) ≤ 12 √ ǫ and errD3(h3) ≤ 12 √ ǫ. It remains to show that Phase 2 of Algorithm 3 also\nproduces h2 such that errD2(h2) ≤ 12 √ ǫ.\nConsider the filtering step of Phase 2. First note that for any x /∈ B, the per-point guarantees of FILTER expressed in Lemma 4.6 still hold. Let D′ be the distribution that has equal probability on the distributions induced by WI and WC , and is used for simulating D2. Similarly as in Lemma 4.8 one can show that for any x 6∈ B, d′(x) = Θ(d2(x)). Since D[B] ≤ ǫ4 , we have that D2[B] ≤ 14 √ ǫ. Therefore, D′ andD2 satisfy the conditions of the robust super-sampling lemma (Lemma 4.12) where the fraction of bad points is at most√ ǫ 4 . Hence, we can argue that errD2(h2) ≤ √ ǫ 2 .\nThe remainder of the proof follows by using the boosting technique of Schapire [1990] described in\nTheorem 4.1."
    }, {
      "heading" : "5 No Perfect Labelers",
      "text" : "In this section, we consider a scenario where our pool of labelers does not include any perfect labelers. Unfortunately, learning f∗ in this setting reduces to the notoriously difficult agnostic learning problem. A related task is to find a set of the labelers which are good but not perfect. In this section, we show how to\nidentify the set of all good labelers, when at least the majority of the labelers are good.\nWe consider a setting where the fraction of the perfect labelers, α, is arbitrarily small or 0. We further assume that at least half of the labelers are good, while others have considerably worst performance. More formally, we are given a set of labelers g1, . . . , gn and a distribution D with an unknown target classifier f∗ ∈ F . We assume that more than half of these labelers are “good”, that is they have error of ≤ ǫ on distribution D. On the other hand, the remaining labelers, which we call “bad”, have error rates ≥ 4ǫ on distribution D. We are interested in identifying all of the good labelers with high probability by querying the labelers on an unlabeled sample set drawn from D|X .\nThis model presents an interesting community structure: Two good labelers agree on at least 1 − 2ǫ fraction of the data, while a bad and a good labeler agree on at most 1 − 3ǫ of the data. Note that the rate of agreement between two bad labelers can be arbitrary. This is due to the fact that there can be multiple\nbad labelers with the same classification function, in which case they completely agree with each other, or two bad labelers who disagree on the classification of every instance. This structure serves as the basis of\nAlgorithm 4 and its analysis. Here we provide an overview of its working and analysis.\nAlgorithm 4 GOOD LABELER DETECTION\nInput: Given n labelers, parameters ǫ and δ Let G = ([n], ∅) be a graph on n vertices with no edges. Take set Q of 16 ln(2)n random pairs of nodes from G.\n1 for (i, j) ∈ Q do if DISAGREE(i, j) < 2.5ǫ then add edge (i, j) to G;\nend\n2 Let C be the set of connected components of G each with ≥ n/4 nodes. 3 for i ∈ [n] \\ ( ⋃\nC∈C C ) and C ∈ C do Take one node j ∈ C , if DISAGREE(i, j) < 2.5ǫ add edge (i, j) to G.\nend return The largest connected component of G\nDISAGREE(i, j): Take set S of Θ(1ǫ ln( n δ )) samples from D.\nreturn 1|S| ∑ x∈S 1(gi(x)6=gj(x)).\nTheorem 5.1 (Informal). Suppose that any good labeler i is such that errD(gi) ≤ ǫ. Furthermore, assume that errD(gj) 6∈ (ǫ, 4ǫ) for any j ∈ [n]. And let the number of good labelers be at least ⌊n2 ⌋ + 1. Then, Algorithm 4, returns the set of all good labeler with probability 1 − δ, using an expected load of λ = O (\n1 ǫ ln\n(\nn δ\n))\nper labeler.\nWe view the labelers as nodes in a graph that has no edges at the start of the algorithm. In step 1, the algorithm takesO(n) random pairs of labelers and estimates their level of disagreement by querying them on an unlabeled sample set of size O (\n1 ǫ ln\n(\nn δ\n))\nand measuring their empirical disagreement. By an application\nof Chernoff bound, we know that with probability 1− δ, for any i, j ∈ [n], ∣\n∣ ∣ ∣ DISAGREE(i, j) − Pr x∼D|X\n[gi(x) 6= gj(x)] ∣ ∣ ∣\n∣\n< ǫ\n2 .\nTherefore, for any pair of good labelers i and j tested by the algorithm, DISAGREE(i, j) < 2.5ǫ, and for any pair of labelers i and j that one is good and the other is bad, DISAGREE(i, j) ≥ 2.5ǫ. Therefore, the connected components of such a graph only include labelers from a single community.\nNext, we show that at step 2 of Algorithm 4 with probability 1 − δ there exists at least one connected component of size n/4 of good labelers.\nTo see this we first prove that for any two good labelers i and j, the probability of (i, j) existing is at least Θ(1/n). Let Vg be the set of nodes corresponding to good labelers. For i, j ∈ Vg, we have\nPr[(i, j) ∈ G] = 1− ( 1− 1 n2\n)4 ln(2)n\n≈ 4 ln(2) n ≥ 2 ln(2)|Vg| .\nBy the properties of random graphs, with very high probability there is a component of size β|Vg| in a random graph whose edges exists with probability c/|Vg|, for β+e−βc = 1 [Janson et al., 2011]. Therefore, with probability 1− δ, there is a component of size |Vg|/2 > n/4 over the vertices in Vg.\nFinally, at step 3 the algorithm considers smaller connected components and tests whether they join\nany of the bigger components, by measuring the disagreement of two arbitrary labelers from these components.,At this point, all good labelers form one single connected component of size > n2 . So, the algorithm succeeds in identifying all good labelers.\nNext, we briefly discuss the expected load per labeler in Algorithm 4. Each labeler participates in O(1) pairs of disagreement tests in expectation, each requiring O(1ǫ ln(n/δ)) queries. So, in expectation each labeler labels O(1ǫ ln(n/δ)) instances."
    }, {
      "heading" : "A Additional Related Work",
      "text" : "More generally, interactive models of learning have been studied in the machine learning community. The\nmost popular among them is the area of active learning [Cohn et al., 1994, Dasgupta, 2005, Balcan et al.,\n2006, Koltchinskii, 2010, Hanneke, 2011]. In this model, the learning algorithm can adaptively query for\nthe labels of a few examples in the training set and use them to produce an accurate hypothesis. The goal\nis to use as few label queries as possible. The number of labeled queries used is called the label complexity\nof the algorithm. It is known that certain hypothesis classes can be learned in this model using much fewer\nlabeled queries than predicted by the VC theory. In particular, in many instances the label complexity scales only logarithmically in 1ǫ as opposed to linearly in 1 ǫ . However, to achieve computational efficiency, the algorithms in this model rely on the fact that one can get perfect labels for every example queried. This would be hard to achieve in our model since in the worst case it would lead to each labeler answering log(dǫ ) many queries. In contrast, we want to keep the query load of a labeler to a constant and hence the techniques\ndeveloped for active learning are insufficient for our purposes. Furthermore, in noisy settings most work\non efficient active learning algorithms assumes the existence of an empirical risk minimizer (ERM) oracle\nthat can minimize training error even when the instances aren’t labeled according to the target classifier.\nHowever, in most cases such an ERM oracle is hard to implement and the improvements obtained in the\nlabel complexity are less drastic in such noisy scenarios.\nAnother line of work initiated by Zhang and Chaudhuri [2015] models related notions of weak and\nstrong labelers in the context of active learning. The authors study scenarios where the label queries to the\nstrong labeler can be reduced by querying the weak and potentially noisy labelers more often. However, as\ndiscussed above, the model does not yield relevant algorithms for our setting as in the worst case one might end up querying for dǫ high quality labels leading to a prohibitively large load per labeler in our setting. The work of Yan et al. [2016] studies a model of active learning where the labeler abstains from providing a label\nprediction more often on instances that are closer to the decision boundary. The authors then show how to\nuse the abstentions in order to approximate the decision boundary. Our setting is inherently different, since\nwe make no assumptions on the bad labelers."
    }, {
      "heading" : "B Proof of Lemma 4.2",
      "text" : "First, notice that because D and D′ are both labeled according to f∗ ∈ F , for any f ∈ F we have,\nerrD′(f) = ∑\nx\nd′(x)1f(x)6=f∗(x) ≥ ∑\nx\nc · d(x)1f(x)6=f∗(x) = c · errD(f).\nTherefore, if errD′(f) ≤ cǫ, then errD(f) ≤ ǫ. Let m′ = mcǫ,δ, we have\nδ > Pr S′∼D′m′\n[∃f ∈ F , s.t. errS′(f) = 0 ∧ errD′(f) ≥ cǫ]\n≥ Pr S′∼D′m′ [∃f ∈ F , s.t. errS′(f) = 0 ∧ errD(f) ≥ ǫ].\nThe claim follows by the fact that mcǫ,δ = O ( 1 cmǫ,δ ) ."
    }, {
      "heading" : "C Proof of Lemma 4.7",
      "text" : "Let us first consider the expected size of sets SI ,W I , and WC . Using Lemma 4.6, we have\nO(m√ǫ,δ) ≥ 1\n2\n√ ǫ|S2|+ √ ǫ|S2| ≥ E[|SI |] ≥ 1\n2\n(\n1\n2\n√ ǫ )\n|S2| ≥ Ω(m√ǫ,δ).\nSimilarly,\nO(m√ǫ,δ) ≥ E[SI ] + |SC | ≥ E[W I ] ≥ 1\n2\n(\n1\n2\n√ ǫ )\n|S2| ≥ Ω(m√ǫ,δ).\nSimilarly,\nO(m√ǫ,δ) ≥ E[SI ] + |SC | ≥ E[WC ] ≥ ( 1− 1 2 √ ǫ ) |SC | ≥ Ω(m√ǫ,δ).\nThe claim follows by the Chernoff bound."
    }, {
      "heading" : "D Remainder of the Proof of Lemma 4.9",
      "text" : "We prove that the Bernstein inequality holds for the total number of queries y1, y2, . . . , made before their majority agrees with f∗(x). Let Lx be the random variable denoting the number of queries the algorithm makes on instance x for which h(x) = f∗(x). Consider the probability that Lx = 2k+1 for some k. That is, Maj(y1:t) = f\n∗(x) for the first time when t = 2k + 1. This is at most the probability that Maj(y1:2k−1) 6= f∗(x). By Chernoff bound, we have that\nPr[Lx = 2k + 1] ≤ Pr[Maj(y1:2k−1) 6= f∗(x)] ≤ exp ( −0.7(2k − 1)(2 7 )2/2 )\n≤ exp (−0.02(2k − 1)) .\nFor each i > 1, we have\nE[(Lx − E[Lx])i] ≤ ∞ ∑\nk=0\nPr[Lx = 2k + 1](2k + 1− E[Lx])i\n≤ ∞ ∑\nk=0\ne−0.02(2k−1)(2k + 1)i\n≤ e0.04 ∞ ∑\nk=0\ne−0.02(2k+1)(2k + 1)i\n≤ e0.04 ∞ ∑\nk=0\ne−0.02kki\n≤ 50(i + 1)! e4i+0.04,\nwhere the last inequality is done by integration. This satisfies the Bernstein condition stated in Theorem E.2.\nTherefore,\nPr\n[\n∑ x∈S Lx − |S|E[Lx] ≥ O(|S|)]\n]\n≤ exp (−|S|) .\nTherefore, the total number of queries over all points in x ∈ S where h(x) = f∗(x) is at most O(|S|) with very high probability."
    }, {
      "heading" : "E Probability Lemmas",
      "text" : "Theorem E.1 (Probability of Ruin [Feller, 2008]). Consider a player who starts with i dollars against an adversary that has N dollars. The player bets one dollar in each gamble, which he wins with probability p.\nThe probability that the player ends up with no money at any point in the game is\n1− (\np 1−p\n)N\n1− (\np 1−p\n)N+i .\nTheorem E.2 (Bernstein Inequality). LetX1, . . . ,Xn be independent random variables with expectation µ. Supposed that for some positive real number L and every k > 1,\nE[(Xi − µ)k] ≤ 1 2 E[(Xi − µ)2]Lk−2k!.\nThen,\nPr\n\n\nn ∑\ni=1\nXi − nµ ≥ 2t\n√ √ √ √ n ∑\ni=1\nE[(Xi − µ)2]\n\n < exp(−t2), for 0 < t ≤ 1 2L √ E[(Xi − µ)2]."
    }, {
      "heading" : "F Omitted Proofs from Section 4.1",
      "text" : "In this section, we prove Theorem 4.13 and present the proofs that were omitted from Section 4.1. Theorem 4.13 (restated) Suppose the fraction of the perfect labelers is α and let δ′ = Θ(αδ). Algorithm 3 uses oracle OF , runs in time poly(d, 1α , 1ǫ , ln(1δ )), uses a training set of size O( 1αmǫ,δ′) size and with probability 1− δ returns f ∈ F with errD(f) ≤ ǫ using O( 1α ) golden queries, load of 1α per labeler, and a total number of queries\nO\n(\n1 α mǫ,δ′ + 1 αǫ log( 1 δ′ ) log( 1 ǫδ′ ) + 1 α3 m√ǫ,δ′ log(\nm√ǫ,δ′\nδ′ )\n)\n.\nNote that when 1 α2 √ ǫ ≥ log\n( m√ǫ,δ αδ ) and log( 1αδ ) < d, the cost per labeled query is O( 1 α ).\nF.1 Proof of Lemma 4.10\nBy Chernoff bound, with probability ≥ 1− δ, for every x ∈ S we have that |Maj-sizeP (x)−Maj-sizeL(x)| ≤ α\n8 ,\nwhereL is the set of labelers PRUNE-AND-LABEL(S, δ) queries on x. Hence, if x is such thatMaj-sizeP (x) ≤ 1− α2 , then it will be identified and the set of labelers is pruned. Otherwise, MajL(x) agrees with the good labelers and x gets labeled correctly according to the target function.\nF.2 Proof of Lemma 4.11\nRecall that δ′ = c · αδ for some small enough constant c > 0. Each time PRUNE-AND-LABEL(S, δ′) is called, by Hoeffding bound, it is guaranteed that with probability ≥ 1− δ′, for each x ∈ S,\n|Maj-sizeP (x)−Maj-sizeL(x)| ≤ α\n8 ,\nwhere L is the set of labelers PRUNE-AND-LABEL(S, δ′) queries on x. Hence, when we issue a golden query for x such that Maj-sizeL(x) ≤ 1 − α4 and prune away bad labelers, we are guaranteed to remove at least an α8 fraction of the labelers. Furthermore, no good labeler is ever removed. Hence, the fraction of good labelers increases from α to α/(1 − α8 ). So, in O( 1α) calls, the fraction of the good labelers surpasses 3 4 and we switch to using Algorithm 2. Therefore, with probability 1− δ overall, the total number of golden queries is O(1/α).\nF.3 Proof of Lemma 4.12\nLet B be the set of points that do not satisfy the condition that d′(x) ≥ c · d(x). Notice that because D and D′ are both labeled according to f∗ ∈ F , for any f ∈ F we have,\nerrD′(f) = ∑ x∈B d′(x)1f(x)6=f∗(x) + ∑ x/∈B d′(x)1f(x)6=f∗(x) ≥ ∑ x/∈B c · d(x)1f(x)6=f∗(x) ≥ c · (errD(f)− ǫ).\nTherefore, if errD′(f) ≤ cǫ, then errD(f) ≤ 2ǫ. Let m′ = mcǫ,δ, we have\nδ > Pr S′∼D′m′\n[∃f ∈ F , s.t. errS′(f) = 0 ∧ errD′(f) ≥ cǫ]\n≥ Pr S′∼D′m′ [∃f ∈ F , s.t. errS′(f) = 0 ∧ errD(f) ≥ 2ǫ].\nThe claim follows by the fact that mcǫ,δ = O ( 1 cmǫ,δ ) .\nF.4 Proof of Theorem 4.13\nRecall that δ′ = c · αδ for a small enough constant c > 0. Let B = {x | Maj-sizeP (x) ≤ 1 − α/2} be the set of good test cases and and let β = D[B] be the total density on such points. Note that if β > ǫ4 , with high probability S0 includes one such point, in which case PRUNE-AND-LABEL identifies it and prunes the set of labelers. Therefore, we can assume that β ≤ ǫ4 . By Lemma 4.10, it is easy to see that errD(h1) ≤ 12 √ ǫ.\nWe now analyze the filtering step of Phase 2. As in Section 4, our goal is to argue that errD2(h2) ≤ 12 √ ǫ. Consider distribution D′ that has equal probability on the distributions induced by WI and WC and let d′(x) denote the density of point x in this distribution. We will show that for any x /∈ B we have that d′(x) = Θ(d2(x)). Since D[B] ≤ ǫ4 , we have that D2[B] ≤ 14 √ ǫ. Therefore, D′ and D2 satisfy the conditions of the robust super-sampling lemma (Lemma 4.12) where the fraction of bad points is at most√ ǫ 4 . Hence, errD2(h2) ≤ 12 √ ǫ.\nWe now show that for any x ∈ B, d′(x) = Θ(d2(x)). The proof is identical to the one in Lemma 4.8. For ease of representation, we assume that errD(h1) is exactly 1 2 √ ǫ. Let d(x), d2(x), dC(x), and dI(x) be the density of instance x in distributions D, D2, DC , and DI , respectively. Note that, for any x such that h1(x) = f ∗(x), we have d(x) = dC(x)(1 − 12 √ ǫ). Similarly, for any x such that h1(x) 6= f∗(x), we have d(x) = dI(x) 1 2 √ ǫ. Let NC(x), NI(x), MC(x) and MI(x) be the number of occurrences of x in the sets SC , SI ,WC and WI , respectively. For any x, there are two cases: If h1(x) = f ∗(x): Then, there exist absolute constants c1 and c2 according to Lemma 4.7, such that\nd′(x) = 1\n2 E\n[\nMC(x)\n|WC |\n]\n≥ E[MC(x)] c1 ·m√ǫ,δ ≥ E[NC(x)] c1 ·m√ǫ,δ = |SC | · d(x) c1 ·m√ǫ,δ\n= |SC | · dC(x) · (1− 12\n√ ǫ)\nc1 ·m√ǫ,δ ≥ c2dC(x) =\nc2d2(x)\n2 ,\nwhere the second and sixth transitions are by the sizes ofWC and |SC | and the third transition is by the fact that if h(x) = f∗(x), MC(x) > NC(x).\nIf h1(x) 6= f∗(x): Then, there exist absolute constants c′1 and c′2 according to Lemma 4.7, such that\nd′(x) = 1\n2 E\n[\nMI(x)\n|WI |\n]\n≥ E[MI(x)] c′1 ·m√ǫ,δ ≥ E[NI(x)] c′1 ·m√ǫ,δ ≥ 0.5 d(x)|S2| c′1 ·m√ǫ,δ\n= 0.5 dI(x)\n1 2 √ ǫ · |S2|\nc′1 ·m√ǫ,δ = c′2dI(x) = c′2d2(x) 2 ,\nwhere the second and sixth transitions are by the sizes of WI and |S2|, the third transition is by the fact that if h(x) 6= f∗(x), MI(x) > NI(x), and the fourth transition holds by part 2 of Lemma 4.6.\nFinally, we have that errD3(h3) ≤ 12 √ ǫ, where D3 is distribution D conditioned on {x | h1(x) 6=\nh2(x)}. Using the boosting technique of Schapire [1990] describe in Theorem 4.1, we conclude thatMAJ (h1, h2, h3) has error ≤ ǫ onD.\nThe label complexity claim follows by the fact that we restart Algorithm 3 at mostO(1/α) times, take an additional O(1ǫ log( 1 δ′ )) high quality labeled set, and each run of Algorithm 3 uses the same label complexity as in Theorem 4.3 before getting restarted."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example. In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in F and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any F that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good.",
    "creator" : "LaTeX with hyperref package"
  }
}