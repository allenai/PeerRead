{
  "name" : "1504.03655.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients",
    "authors" : [ "Bo Xie", "Yingyu Liang", "Le Song" ],
    "emails" : [ "bo.xie@gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a simple, computationally efficient, and memory friendly algorithm based on the “doubly stochastic gradients” to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the non-convex nature of these problems, our method enjoys theoretical guarantees that it converges at the rate Õ(1/t) to the global optimum, even for the top k eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets."
    }, {
      "heading" : "1 Introduction",
      "text" : "Scaling up nonlinear component analysis has been challenging due to prohibitive computation and memory requirements. Recently, methods such as Randomized Component Analysis [16] are able to scale to larger datasets by leveraging random feature approximation. Such methods approximate the kernel function by using explicit random feature mappings, then perform subsequent steps in the primal form, resulting in linear computational complexity. Nonetheless, theoretical analysis [23, 16] shows that in order to get high quality results, the number of random features should grow linearly with the number of data points. Experimentally, one often sees that the statistical performance of the algorithm improves as one increases the number of random features.\nAnother approach to scale up the kernel component analysis is to use stochastic gradient descent and online updates [19, 20]. These stochastic methods have also been extended to the kernel case [13, 6, 11]. They require much less computation than their batch counterpart, converge in O(1/t) rate, and are naturally applicable to streaming data setting. Despite that, they share a severe drawback: all data points used in the updates need to be saved, rendering them impractical for large datasets.\nIn this paper, we propose to use the “doubly stochastic gradients” for nonlinear component analysis. This technique is a general framework for scaling up kernel methods [8] for convex problems and has been successfully applied to many popular kernel machines such as kernel SVM, kernel ridge regressions, and Gaussian process. It uses two types of stochastic approximation simultaneously: random data points instead of the whole dataset (as in stochastic update rules), and random features instead of the true kernel functions (as in randomized component analysis). These two approximations lead to the following benefits:\n∗College of Computing, Georgia Institute of Technology. Email:bo.xie@gatech.edu †Department of Computer Science, Princeton University. Email: yingyul@cs.princeton.edu ‡College of Computing, Georgia Institute of Technology. Email:lsong@cc.gatech.edu\nar X\niv :1\n50 4.\n03 65\n5v 4\n[ cs\n.L G\n] 1\n0 Ja\nn 20\n16\n• Computation efficiency The key computation is the generation of a mini-batch of random features and the evaluation of them on a mini-batch of data points, which is very efficient.\n• Memory efficiency Instead of storing training data points, we just keep a small program for regenerating the random features, and sample previously used random features according to pre-specified random seeds. This leads to huge savings: the memory requirement up to step t is O(t), independent of the dimension of the data.\n• Adaptibility Unlike other approaches that can only work with a fixed number of random features beforehand, doubly stochastic approach is able to increase the model complexity by using more features when new data points arrive, and thus enjoys the advantage of nonparametric methods.\nAlthough on first look our method appears similar to the approach in [8], the two methods are fundamentally different. In [8], they address convex problems, whereas our problem is highly non-convex. The convergence result in [8] crucially relies on the properties of convex functions, which do not translate to our problem. Instead, our analysis centers around the stochastic update of power iterations, which uses a different set of proof techniques.\nIn this paper, we make the following contributions.\n• General framework We show that the general framework of doubly stochastic updates can be applied in various kernel component analysis tasks, including KPCA, KSVD, KCCA, etc..\n• Strong theoretical guarantee We prove that the finite time convergence rate of doubly stochastic approach is Õ(1/t). This is a significant result since 1) the global convergence result is w.r.t. a nonconvex problem; 2) the guarantee is for update rules without explicit orthogonalization. Previous works require explicit orthogonalization, which is impractical for kernel methods on large datasets.\n• Strong empirical performance Our algorithm can scale to datasets with millions of data points. Moreover, the algorithm can often find much better solutions thanks to the ability to use many more random features. We demonstrate such benefits on both synthetic and real world datasets.\nSince kernel PCA is a typical task, we focus on it in the paper and provide a description of other tasks in Section 6. Although we only state the guarantee for kernel PCA, the analysis naturally carries over to the other tasks."
    }, {
      "heading" : "2 Related work",
      "text" : "Many efforts have been devoted to scale up kernel methods. The random feature approach [22, 23] approximates the kernel function with explicit random feature mappings and solves the problem in primal form, thus circumventing the quadratic computational complexity. It has been applied to various kernel methods [15, 8, 16], among which most related to our work is Randomized Component Analysis [16]. One drawback of Randomized Component Analysis is that their theoretical guarantees are only for kernel matrix approximation: it does not say anything about how close the solution obtained from randomized PCA is to the true solution. In contrast, we provide a finite time convergence rate of how our solution approaches the true solution. In addition, even though a moderate size of random features can work well for tens of thousands of data points, datasets with tens of millions of data points require many more random features. Our online approach allows the number of random features, hence the flexibility of the function class, to grow with the number of data points. This makes our method suitable for data streaming setting, which is not possible for previous approaches.\nOnline algorithms for PCA have a long history. Oja proposed two stochastic update rules for approximating the first eigenvector and provided convergence proof in [19, 20], respectively. These rules have been extended to the generalized Hebbian update rules [25, 27, 4] that compute the top k eigenvectors (the subspace case). Similar ones have also been derived from the perspective of optimization and stochastic gradient descent [27, 2]. They are further generalized to the kernel case [13, 6, 11]. However, online kernel PCA needs\nto store all the training data, which is impractical for large datasets. Our doubly stochastic method avoids this problem by using random features and keeping only a small program for regenerating previously used random features according to pre-specified seeds. As a result, it can scale up to tens of millions of data points.\nFor finite time convergence rate, [4] proved the O(1/t) rate for the top eigenvector in linear PCA using Oja’s rule. For the same task, [28] proposed a noise reduced PCA with linear convergence rate, where the rate is in terms of epochs, i.e., number of passes over the whole dataset. The noisy power method presented in [10] provided linear convergence for a subspace, although it only converges linearly to a constant error level. In addition, the updates require explicit orthogonalization, which is impractical for kernel methods. In comparison, our method converges in O(1/t) for a subspace, without the need for orthogonalization."
    }, {
      "heading" : "3 Preliminaries",
      "text" : ""
    }, {
      "heading" : "3.1 Kernels and Covariance Operators",
      "text" : "A kernel k(x, y) : X × X 7→ R is a function that is positive-definite (PD), i.e., for all n > 1, c1, . . . , cn ∈ R, and x1, . . . , xn ∈ X , we have\nn∑ i,j=1 cicjk(xi, xj) ≥ 0.\nA reproducing kernel Hilbert space (RKHS) F on X is a Hilbert space of functions from X to R. F is an RKHS if and only if there exists a k(x, x′) : X × X 7→ R such that ∀x ∈ X , k(x, ·) ∈ F , and ∀f ∈ F , 〈f(·), k(x, ·)〉F = f(x). If such a k(x, x′) exist, it is unique and it is a PD kernel. A function f ∈ F if and only if ‖f‖2F := 〈f, f〉F <∞.\nGiven a distribution P(x), a kernel function k(x, x′) with RKHS F , the covariance operator A : F 7→ F is a linear self-adjoint operator defined as\nAf(·) := Ex[f(x) k(x, ·)], ∀f ∈ F , (1)\nand furthermore 〈g,Af〉F = Ex[f(x) g(x)], ∀g ∈ F . Let F = (f1(·), f2(·), . . . , fk(·)) be a list of k functions in the RKHS, and we define matrix-like notation\nAF (·) := (Af1(·), . . . , Afk(·)) , (2)\nand F>AF is a k × k matrix, whose (i, j)-th element is 〈fi, Afj〉F . The outer-product of a function v ∈ F defines a linear operator vv> : F 7→ F such that\n(vv>)f(·) := 〈v, f〉F v(·), ∀f ∈ F (3)\nLet V = (v1(·), . . . , vk(·)) be a list of k functions, then the weighted sum of a set of linear operators,{ viv > i }k i=1 , can be denoted using matrix-like notation as\nV ΣkV > := k∑ i=1 λiviv > i (4)\nwhere Σk is a diagonal matrix with λi on the i-th entry of the diagonal."
    }, {
      "heading" : "3.2 Kernel PCA",
      "text" : "Kernel PCA aims to identify the top k eigenfunctions V = (v1(·), . . . , vk(·)) for the covariance operator A, where V is also called the top k subspace for A.\nGiven a set of eigenfunctions {vi} and associated eigenvalues {λi}, where 〈vi, vj〉F = δij . We can denote the eigenvalue of A as\nA = V ΣkV > + V⊥Σ⊥V > ⊥ (6)\nwhere V = (v1(·), . . . , vk(·)) is the top k eigenfunctions of A, and Σk is a diagonal matrix with the corresponding eigenvalues, V⊥ is the collection of the rest of the eigenfunctions, and Σ⊥ is a diagonal matrix with the rest of the eigenvalues.\nIn the finite data case, the empirical covariance operator is A = 1n ∑ i k(xi, ·)k(xi, ·)> or denoted as\n1 n ∑ i k(xi, ·)⊗ k(xi, ·). According to the representer theorem, the solutions of the top k eigenfunctions of A can be expressed as linear combinations of the training points with the set of coefficients {αi}ki=1 ∈ Rn,\nvi = n∑ j=1 αjik(xj , ·)\nUsing Av(·) = λv(·) and the kernel trick, we have\nKαi = λiαi,\nwhere K is the n× n Gram matrix. The infinite dimensional problem is thus reduced to a finite dimensional eigenvalue problem. However, this dual approach is clearly impractical on large scale datasets due quadratic memory and computational costs."
    }, {
      "heading" : "3.3 Random feature approximation",
      "text" : "The usage of random features to approximate a kernel function is motivated by the following theorem.\nTheorem 1 (Bochner). A continuous, real-valued, symmetric and shift-invariant function k(x − x′) on Rd is a PD kernel if and only if there is a finite non-negative measure P(ω) on Rd, such that k(x − x′) =∫ Rd e iω>(x−x′) dP(ω) = ∫ Rd×[0,2π] φω(x)φω(y) d (P(ω)× P(b)) , where P(b) is a uniform distribution on [0, 2π], and φω(x) = √ 2 cos(ω>x+ b).\nThe theorem says that any shift-invariant kernel function k(x, y) = k(x− y), e.g., Gaussian RBF kernel, can be considered as an expectation of two feature functions φω(x) and φω(y), where the expectation is taked over a distribution on the random frequency ω and phase b.\nWe can therefore approximate the kernel function as an empirical average of samples from the distribution. In other words,\nk(x, y) ≈ 1 B ∑ i φωi(x)φωi(y),\nwhere {(ωi, bi)}Bi are i.i.d. samples drawn from from P(ω) and P(b), respectively. The specific random feature functions and distributions have been worked out for many popular kernels. For Gaussian RBF kernel, k(x − x′) = exp(−‖x − x′‖2/2σ2), this yields a Gaussian distribution P(ω) with density proportional to exp(−σ2‖ω‖2/2); for the Laplace kernel, this yields a Cauchy distribution; and for the Martern kernel, this yields the convolutions of the unit ball [26]. Similar representation where the explicit form of φω(x) and P(ω) are known can also be derived for rotation invariant kernel, k(x, x′) = k(〈x, x′〉), using Fourier transformation on sphere [26]. For polynomial kernels, k(x, x′) = (〈x, x′〉 + c)p, a random tensor sketching approach can also be used [21]. See Table 1 for explicit representations of different kernels."
    }, {
      "heading" : "4 Algorithm",
      "text" : "In this section, we describe an efficient algorithm based on the “doubly stochastic gradients” to scale up kernel PCA. KPCA is essentially an eigenvalue problem in a functional space. Traditional approaches convert it to the dual form, leading to another eigenvalue problem whose size equals the number of training points, which is not scalable. Other approaches solve it in the primal form with stochastic functional gradient descent. However, these algorithms need to store all the training points seen so far. They quickly run into memory issues when working with hundreds of thousands of data points.\nWe propose to tackle the problem with “doubly stochastic gradients”, in which we make two unbiased stochastic approximations. One stochasticity comes from sampling data points as in stochastic gradient descent. Another source of stochasticity is from random features to approximate the kernel.\nOne technical difficulty in designing doubly stochastic KPCA is an explicit orthogonalization step required in the update rules, which ensures the top k eigenfunctions are orthogonal. This is infeasible for kernel methods on a large dataset since it requires solving an increasingly larger KPCA problem in every iteration. To solve this problem, we formulate the orthogonality constraints into Lagrange multipliers which leads to an Oja-style update rule. The new update enjoys small per iteration complexity and converges to the ground-truth subspace.\nWe present the algorithm by first deriving the stochastic functional gradient update without random feature approximations, then introducing the doubly stochastic updates."
    }, {
      "heading" : "4.1 Stochastic functional gradient update",
      "text" : "Kernel PCA can be formulated as the following non-convex optimization problem\nmax G\ntr ( G>AG ) s.t.G>G = I, (7)\nwhere G := ( g1, . . . , gk ) and gi is the i-th function.\nThe Lagrangian that incorporates the constraint is L(G,Λ) = tr ( G>AG ) + tr (( G>G− I ) Λ )\nwhere Λ is the Lagrangian multiplier. The gradient of the Lagrangian w.r.t G is ∇GL = 2AG+G ( Λ + Λ> ) .\nFurthermore, from the optimality conditions 2AG+G ( Λ + Λ> ) = 0,\nG>G− I = 0,\nwe can find Λ + Λ> = −2G>AG. Plugging this into the gradient, it suggests the following update rule\nGt+1 = Gt + ηt ( I −GtG>t ) AGt. (8)\nUsing a stochastic approximation forA: Atf(·) = f(xt) k(xt, ·), we haveAtGt = k(xt, ·)g>t andG>t AtGt = gtg > t , where gt = [ g1t (xt), . . . , g k t (xt) ]> . Therefore, the update rule is\nGt+1 = Gt ( I − ηtgtg>t ) + ηtk(xt, ·)g>t . (9)\nThis rule can also be derived using stochastic gradient and Oja’s rule [19, 20]."
    }, {
      "heading" : "4.2 Doubly stochastic update",
      "text" : "The update rule (9) has a fundamental computational drawback. At each time step t, a new basis k(xt, ·) is added to Gt, and it is therefore a linear combination of the feature mappings of all the data points up to t. This requires the algorithm to store all the data points it has seen so far, which is impractical for large scale datasets.\nTo address this issue, we use the random feature approximation k(x, ·) ≈ φωi(x)φωi(·). Denote Ht the function we get at iteration t, the update rule becomes\nHt+1 = Ht ( I − ηththt> ) + ηtφωt(xt)φωt(·)ht >, (10)\nwhere ht is the evaluation of Ht at the current data point: ht = [ h1t (xt), . . . , h k t (xt) ]> .\nGiven H0 = V0, we can explicitly represent Ht as a linear combination of all the random feature functions φωi(·):\nHt = ∑ i φωi(·)α>i + V0β,\nwhere αi ∈ Rk are the coefficients, and β = ∏ i≤t ( I − ηihihi> ) .\nThe update rule on the functions corresponds to the following update for the coefficients\nαt+1 = ηtφωt(xt)ht\nαi = αi − ηtα>i htht, ∀i ≤ t\nThe specific updates in terms of the coefficients are summarized in Algorithms 1 and 2. Note that in theory new random features are drawn in each iteration, but in practice one can revisit these random features."
    }, {
      "heading" : "5 Analysis",
      "text" : "In this section, we provide finite time convergence guarantees for our algorithm. As discussed in the previous section, explicit orthogonalization is not scalable for the kernel case, therefore we need to provide guarantees for the updates without orthogonalization. This challenge is even more prominent when using random features, since it introduces additional variance.\nFurthermore, our guarantees are w.r.t. the top k-dimension subspace. Although the convergence without normalization for a top eigenvector has been established before [19, 20], the subspace case is complicated by\nAlgorithm 1: {αi}t1 = DSGD-KPCA(P(x), k) Require: P(ω), φω(x). 1: for i = 1, . . . , t do 2: Sample xi ∼ P(x). 3: Sample ωi ∼ P(ω) with seed i. 4: hi = Evaluate(xi, {αj}i−1j=1) ∈ R k.\n5: αi = ηiφωi(xi)hi. 6: αj = αj − ηiα>j hihi, for j = 1, . . . , i− 1. 7: end for\nAlgorithm 2: h = Evaluate(x, {αi}ti=1) Require: P(ω), φω(x). 1: Set h = 0 ∈ Rk. 2: for i = 1, . . . , t do 3: Sample ωi ∼ P(ω) with seed i. 4: h = h+ φωi(x)αi. 5: end for\nthe fact that there are k angles between k-dimension subspaces, and we need to bound the largest angle. To the best of our knowledge, our result is the first finite time convergence result for a subspace without explicit orthogonalization.\nNote that even though it appears our algorithm is similar to [8] on the surface, the underlying analysis is fundamentally different. In [8], the result only applies to convex problems where every local optimum is a global optimum while the problems we consider are highly non-convex. As a result, many techniques that [8] builds upon are not applicable."
    }, {
      "heading" : "5.1 Notations",
      "text" : "In order to analyze the convergence of our doubly stochastic kernel PCA algorithm, we will need to define a few intermediate subspaces. For simplicity of notation, we will assume the mini-batch size for the data points is one.\n1. Let Ft := ( f1t , . . . , f k t ) be the subspace estimated using stochastic gradient and explicit orthogonaliza-\ntion:\nF̃t+1 ← Ft + ηtAtFt Ft+1 ← F̃t+1 ( F̃>t+1F̃t+1 )−1/2 2. Let Gt := ( g1t , . . . , g k t ) be the subspace estimated using stochastic update rule without orthogonaliza-\ntion:\nGt+1 ← Gt + ηt ( I −GtG>t ) AtGt.\nwhere AtGt and GtG > t AtGt can be equivalently written using the evaluation of the function { git }\non the current data point, leading to the equivalent rule :\nGt+1 ← Gt ( I − ηtgtg>t ) + ηtk(xt, ·)g>t . (11)\n3. Let G̃t := ( g̃1t , . . . , g̃ k t ) be the subspace estimated using stochastic update rule without orthogonaliza-\ntion, but the evaluation of the function { g̃it } on the current data point is replaced by the evaluation\nht = [ hit(xt) ]> :\nG̃t+1 ← G̃t + ηtk(xt, ·)h>t − ηtG̃thth>t\n4. Let Ht := ( h1t , . . . , h k t ) be the subspace estimated using doubly stochastic update rule without orthog-\nonalization, i.e., the update rule:\nHt+1 ← Ht + ηtφωt(xt)φωt(·)h>t − ηtHthth>t . (12)\nThe relation of these subspaces are summarized in Table 2. Using these notations, we describe a sketch of our analysis in the rest of the section, while the complete proofs are provided in the appendix.\nWe first consider the subspace Gt estimated using the stochastic update rule, since it is simpler and its proof can provide the bases for analyzing the subspace Ht estimated by the doubly stochastic update rule."
    }, {
      "heading" : "5.2 Conditions and Assumptions",
      "text" : "We will focus on the case when a good initialization V0 is given:\nV >0 V0 = I, cos 2 θ(V, V0) ≥ 1/2. (13)\nIn other words, we analyze the later stage of the convergence, which is typical in the literature (e.g., [28]). The early stage can be analyzed using established techniques (e.g., [4]).\nThroughout the paper we suppose |k(x, x′)| ≤ κ, |φω(x)| ≤ φ and regard κ and φ as constants. Note that this is true for all the kernels and corresponding random features considered. We further regard the eigengap λk − λk+1 as a constant, which is also true for typical applications and datasets."
    }, {
      "heading" : "5.3 Update without random features",
      "text" : "Our guarantee is on the cosine of the principal angle between the computed subspace and the ground truth eigen subspace (also called potential function): cos2 θ(V,Gt) = minw ‖V >Gtw‖2 ‖Gtw‖2 .\nConsider the two different update rules, one with explicit orthogonalization and another without\nFt+1 ← orth(Ft + ηtAtFt) Gt+1 ← Gt + ηt ( I −GtG>t ) AtGt\nwhere At is the empirical covariance of a mini-batch. Our final guarantee for Gt is the following.\nTheorem 2. Assume (13) and suppose the mini-batch sizes satisfy that for any 1 ≤ i ≤ t, ‖A−Ai‖ < (λk − λk+1)/8. There exist step sizes ηi = O(1/i) such that\n1− cos2 θ(V,Gt) = O(1/t).\nThe convergence rate O(1/t) is in the same order as that of computing only the top eigenvector in linear PCA [4]. The bound requires the mini-batch size is large enough so that the spectral norm of A is approximated up to the order of the eigengap. This is because the increase of the potential is in the order of the eigengap. Similar terms appear in the analysis of the noisy power method [10] which, however, requires orthogonalization and is not suitable for the kernel case. We do not specify the mini-batch size, but by assuming suitable data distributions, it is possible to obtain explicit bounds; see for example [30, 5]. Proof sketch We first prove the guarantee for the orthogonalized subspace Ft which is more convenient to analyze, and then show that the updates for Ft and Gt are first order equivalent so Gt enjoys the same guarantee. To do so, we will require lemma 3 and 4 below\nLemma 3. 1− cos2 θ(V, Ft) = O(1/t).\nLet c2t denote cos 2 θ(V, Ft), then a key step in proving the lemma is to show the following recurrence\nc2t+1 ≥ c2t (1 + 2ηt(λk − λk+1 − 2 ‖A−At‖)(1− c2t ))−O(η2t ). (14)\nWe will need the mini-batch size large enough so that 2 ‖A−At‖ is smaller than the eigen-gap. Another key element in the proof of the theorem is the first order equivalence of the two update rules. To show this, we introduce F (Gt)← orth(Gt+ ηtAtGt) to denote the subspace by applying the update rule of Ft on Gt. We show that the potentials of Gt+1 and F (Gt) are close:\nLemma 4. cos2 θ(V,Gt+1) = cos 2 θ(V, F (Gt))±O(η2t ).\nThe lemma means that applying the two update rules to the same input will result in two subspaces with similar potentials. Then by (14), we have 1− cos2 θ(V,Gt) = O(1/t) which leads to our theorem. The proof of Lemma 4 is based on the observation that cos2 θ(V,X) = λmin(V\n>X(X>X)−1X>V ). Comparing the Taylor expansions w.r.t. ηt for X = Gt+1 and X = F (Gt) leads to the lemma."
    }, {
      "heading" : "5.4 Doubly stochastic update",
      "text" : "The Ht computed in the doubly stochastic update is no longer in the RKHS so the principal angle is not well defined. Instead, we will compare the evaluation of functions from Ht and the true principal subspace V respectively on a point x. Formally, we show that for any function v ∈ V with unit norm ‖v‖F = 1, there exists a function h in Ht such that for any x, err := |v(x)− h(x)|2 is small with high probability.\nTo do so, we need to introduce a companion update rule: G̃t+1 ← G̃t + ηtk(xt, ·)h>t − ηtG̃thth>t resulting in function in the RKHS, but the update makes use of function values from ht ∈ Ht which outside the RKHS. Let w = G̃>v be the coefficients of v projected onto G̃, h = Htw, and z = G̃tw. Then the error can be decomposed as\n|v(x)− h(x)|2 = |v(x)− z(x) + z(x)− h(x)|2 ≤ 2 |v(x)− z(x)|2 + 2 |z(x)− h(x)|2\n≤ 2κ2 ‖v − z‖2F︸ ︷︷ ︸ (I: Lemma 6) + 2 |z(x)− h(x)|2︸ ︷︷ ︸ (II: Lemma 7) . (15)\nBy definition, ‖v − z‖2F = ‖v‖ 2 F − ‖z‖ 2 F ≤ 1 − cos2 θ(V, G̃t), so the first error term can be bounded by the guarantee on G̃t, which can be obtained by similar arguments in Theorem 2. For the second term, note that G̃t is defined in such a way that the difference between z(x) and h(x) is a martingale, which can be bounded by careful analysis.\nTheorem 5. Assume (13) and suppose the mini-batch sizes satisfy that for any 1 ≤ i ≤ t, ‖A−Ai‖ < (λk − λk+1)/8 and are of order Ω(ln tδ ). There exist step sizes ηi = O(1/i), such that the following holds. If Ω(1) = λk(G̃ > i G̃i) ≤ λ1(G̃>i G̃i) = O(1) for all 1 ≤ i ≤ t, then for any x and any function v in the span of V with unit norm ‖v‖F = 1, we have that with probability at least 1 − δ, there exists h in the span of Ht satisfying |v(x)− h(x)|2 = O ( 1 t ln t δ ) .\nThe point-wise error scales as Õ(1/t) with the step t. Besides the condition that ‖A−Ai‖ is up to the order of the eigengap, we additionally need that the random features approximate the kernel function up to constant accuracy on all the data points up to time t, which eventually leads to Ω(ln tδ ) mini-batch sizes. Finally, we need G̃>i G̃i to be roughly isotropic, i.e., G̃i is roughly orthonormal. Intuitively, this should be true for the following reasons: G̃0 is orthonormal; the update for G̃t is close to that for Gt, which in turn is close to Ft that are orthonormal. Proof sketch In order to bound term I in (15), we show that\nLemma 6. 1− cos2 θ(V, G̃t) = O ( 1 t ln t δ ) .\nThis is proved by following similar arguments to get the recurrence (14), except with an additional error term, which is caused by the fact that the update rule for G̃t+1 is using the evaluation ht(xt) rather than g̃t(xt). Bounding this additional term thus relies on bounding the difference between ht(x)− g̃t(x), which is also what we need for bounding term II in (15). For this, we show: Lemma 7. For any x and unit vector w, with probability ≥ 1 − δ over (Dt, ωt), |g̃t(x)w − ht(x)w|2 = O (\n1 t ln ( t δ )) .\nThe key to prove this lemma is that our construction of G̃t makes sure that the difference between g̃t(x)w and ht(x)w consists of their difference in each time step. Furthermore, the difference forms a martingale and thus can be bounded by Azuma’s inequality. See the supplementary for the details."
    }, {
      "heading" : "6 Extensions",
      "text" : "The proposed algorithm is a general technique for solving eigenvalue problems in the functional space. Numerous machine learning algorithms boil down to this fundamental operation. Therefore, our method can be easily extended to solve many related tasks, including latent variable estimation, kernel CCA, spectral clustering, etc..\nWe briefly illustrate how to extend to different machine learning algorithms in the following subsections."
    }, {
      "heading" : "6.1 Locating individual eigenfunctions",
      "text" : "The proposed algorithm finds the subspace spanned by the top k eigenfunctions, but it does not isolate the individual eigenfunctions. When we need to locate these individual eigenfunctions, we can use a modified version, called Generalized Hebbian Algorithm (GHA) [25]. Its update rule is\nGt+1 = Gt + ηtAtGt − ηtGt UT [ G>t AtGt ] , (16)\nwhere UT [·] is an operator that sets the lower triangular parts to zero. To understand the effect of the upper triangular operator, we can see that UT [·] forces the update rule for the first function of Gt to be exactly the same as that of one-dimensional subspace; all the contributions from the other functions are zeroed out.\ng1t+1 = g 1 t + ηtAtg 1 t − ηtg1t g1t > Atg 1 t , (17)\nTherefore, the first function will converge to the eigenfunction corresponding to the top eigenvalue. For all the other functions, UT [·] implements a Gram-Schmidt-like orthogonalization that subtracts the\ncontributions from other eigenfunctions."
    }, {
      "heading" : "6.2 Latent variable models and kernel SVD",
      "text" : "Latent variable models are probabilistic models that assume unobserved or latent structures in the data. It appears in specific forms such as Gaussian Mixture Models (GMM), Hidden Markov Models (HMM) and Latent Dirichlet Allocations (LDA), etc..\nAlgorithm 3: {αi, βi}t1 = DSGD-KSVD(P(x),P(y), k) Require: P(ω), φω(x). 1: for i = 1, . . . , t do 2: Sample xi ∼ P(x). Sample yi ∼ P(y). 3: Sample ωi ∼ P(ω) with seed i. 4: ui = Evaluate(xi, {αj}i−1j=1) ∈ R k.\n5: vi = Evaluate(yi, {βj}i−1j=1) ∈ R k. 6: W = uiv > i + viu > i 7: αi = ηiφωi(xi)vi. 8: βi = ηiφωi(yi)ui. 9: αj = αj − ηiWαj , for j = 1, . . . , i− 1.\n10: βj = βj − ηiWβj , for j = 1, . . . , i− 1. 11: end for\nThe EM algorithm [9] is considered the standard approach to solve such models. Recently, spectral methods have been proposed to estimate latent variable models with provable guarantees [1, 29]. Compared with the EM algorithm , spectral methods are faster to compute and do not suffer from local optima.\nThe key algorithm behind spectral methods is the SVD. However, kernel SVD scales quadratically with the number of data points. Our algorithm can be straightforwardly extended to solve kernel SVD. The extension hinges on the following relation[\n0 A> A 0 ] [ V U ] = [ A>U AV ] = [ V U ] Σ,\nwhere UΣV > is the SVD of A. It is therefore reduced to the eigenvalue problem. Plugging it into the update rule and treating the two blocks separately, we thus get two simultaneous update rules\nWt = U > t AVt + V > t A >Ut (18) Ut+1 = Ut + ηt (AVt − UtWt) , (19) Vt+1 = Vt + ηt ( A>Ut − VtWt ) . (20)\nThe algorithm for updating the coefficients is summarized in Algorithm 3."
    }, {
      "heading" : "6.3 Kernel CCA and generalized eigenvalue problem",
      "text" : "Kernel CCA and ICA [3] can also be solved under the proposed framework because they can be viewed as generalized eigenvalue problem.\nGiven two variables X and Y , CCA finds two projections such that the correlations between the two projected variables are maximized. Given the covariance matrices CXX , CY Y , and CXY , CCA is equivalent to the following problem[\nCXX CXY CY X CY Y ] [ gX gY ] = ( 1 + σ2 ) [ CXX CY Y ] [ gX gY ] ,\nwhere gX and gY are the top canonical correlation functions for variables X and Y , respectively, and σ is the corresponding canonical correlation.\nThis is a generalized eigenvalue problem. It can reformulated as the following non-convex optimization problem\nmax G\ntr ( G>AG ) , (21)\ns.t. G>BG = I. (22)\nAlgorithm 4: {αi, βi}t1 = DSGD-KCCA(P(x),P(y), k) Require: P(ω), φω(x). 1: for i = 1, . . . , t do 2: Sample xi ∼ P(x). Sample yi ∼ P(y). 3: Sample ωi ∼ P(ω) with seed i. 4: ui = Evaluate(xi, {αj}i−1j=1) ∈ R k.\nFollowing the derivation for the standard eigenvalue problem, we get the foliowing update rules Gt+1 = Gt + ηt ( I −BGtG>t ) AGt. (23)\nDenote GXt and G Y t the canonical correlation functions for X and Y , respectively. We can rewrite the\nabove update rule as two simultaneous rules\nWt = G Y t > CY XG X t +G X t > CXYG Y t (24)\nGXt+1 = G X t + ηt [ CXYG Y t − CXXGXt W ] (25) GYt+1 = G Y t + ηt [ CY XG X t − CY YGYt W ] . (26)\nWe present the detailed updates for coefficients in Algorithm 4."
    }, {
      "heading" : "6.4 Kernel sliced inverse regression",
      "text" : "Kernel sliced inverse regression [14] aims to do sufficient dimension reduction in which the found low dimension representation preserves the statistical correlation with the targets. It also reduces to a generalized eigenvalue problem, and has been shown to find the same subspace as KCCA [14]."
    }, {
      "heading" : "7 Experiments",
      "text" : "We demonstrate the effectiveness and scalability of our algorithm on both synthetic and real world datasets."
    }, {
      "heading" : "7.1 Synthetic dataset with analytical solution",
      "text" : "We first verify the convergence rate of DSGD-KPCA on a synthetic dataset with analytical solution of eigenfunctions [31]. If the data follow a Gaussian distribution, and we use a Gaussian kernel, then the eigenfunctions are given by the Hermite polynomials.\nWe generated 1 million data points, and ran DSGD-KPCA with a total of 262,144 random features. In each iteration, we use a data mini-batch of size 512, and a random feature mini-batch of size 128. After all random features are generated, we revisit and adjust the coefficients of existing random features. The kernel bandwidth is set as the true bandwidth of the data.\nThe step size is scheduled as\nηt = θ0\n1 + θ1t , (27)\nwhere θ0 and θ1 are two parameters. We use a small θ1 ≈ 0.01 such that in early stages the step size is large enough to arrive at a good initial solution.\nConvergence Figure 1 shows the convergence rate of the proposed algorithm seeking top k = 3 subspace. The potential function is calculated as the squared sine function of the subspace angle between the current solution and the ground-truth. We can see the algorithm indeed converges at the rate O(1/t).\nEigenfunction Recovery Figure 2 demonstrate the recovered top k = 3 eigenfunctions compared with the ground-truth. We can see the found solution coincides with one eigenfunction, and only disagree slightly on two others."
    }, {
      "heading" : "7.2 Nonparametric Latent Variable Model",
      "text" : "In [29], the authors proposed a multiview nonparametric latent variable model that is solved by kernel SVD followed by tensor power iterations. The algorithm can separate latent variables without imposing specific parametric assumptions of the conditional probabilities. However, the scalability of the algorithm was limited by kernel SVD.\nHere, we demonstrate that with DSGD-KSVD, we can learn latent variable models with one million data, achieving higher quality of learned components compared with two other approaches.\nDSGD-KSVD uses a total of 8192 random features, and in each iteration, it uses a feature mini-batch of size 256 and a data mini-batch of size 512.\nWe compare with 1) random Fourier features with fixed 2048 functions, and 2) random Nystrom features with fixed 2048 functions. The Nystrom features are calculated by first uniformly sampling 2048 data points, and then evaluate kernel function values on these data points [16].\nThe dataset consists of two latent components, one is a Gaussian distribution and the other follows a Gamma distribution with shape parameter α = 1.2. One million data point are generated from this mixture distribution.\nFigures 3 shows the learned conditional distributions for each component. We can see DSGD-KSVD achieves almost perfect recovery, while Fourier and Nystrom random feature methods either confuse high density areas or incorrectly estimate the spread of conditional distributions."
    }, {
      "heading" : "7.3 KCCA MNIST8M",
      "text" : "We then demonstrate the scalability and effectiveness of our algorithm on a large-scale real world dataset. MNIST8M consists of 8.1 million hand-written digits and their transformations. Each digit is of size 28×28. We divide each image into the left and right parts, and learn their correlations using KCCA. Thus the input feature dimension is 392.\nThe evaluation criteria is the total correlations on the top k = 50 canonical correlation directions calculated on a separate test set of size 10000. Out of the 8.1 million training data, we randomly choose 10000 as an evaluation set.\nWe compare with 1) random Fourier and 2) random Nystrom features on both total correlation and running time. We vary the number of random features used for both methods. Our algorithm uses a total of 20480 features. In each iteration, we use feature mini-batches of size 2048 and data mini-batches of size 1024, and we run 3000 iterations. The kernel bandwidth is set using the “median” trick and is the same for all methods. Due to randomness, all algorithms are run 5 times, and the mean is reported.\nThe results are presented in Table 3. We can see Nystrom features generally achieve better results than Fourier features. Note that for Fourier features, we are using the version with sin and cos pairs, so the real number of parameters is twice the number in the table, as a result the computational time is almost twice of that for Nystrom features.\nOur algorithm achieves the best test-set correlations in comparable run time with random Fourier features. This is especially significant for random Fourier features, since the run time would increase by almost four times if double the number of features were used. We can also see that for large datasets, it is important to use more random features for better performance. Actually, the number of random features required should grow linearly with the number of data points. Therefore, our algorithm provides a good balance between the number of random features used and the number of data points processed."
    }, {
      "heading" : "7.4 Kernel PCA visualization on molecular space dataset",
      "text" : "MolecularSpace dataset contains 2.3 million molecular motifs [8]. We are interested in visualizing the dataset with KPCA. The data are represented by sorted Coulomb matrices of size 75× 75 [18]. Each molecule also has an attribute called power conversion efficiency (PCE). We use a Gaussian kernel with bandwidth chosen\nby the “median trick”. We ran kernel PCA with a total of 16384 random features, with a feature mini-batch size of 512, and data mini-batch size of 1024. We ran 4000 iterations with step size ηt = 1/(1 + 0.001 ∗ t).\nFigure 4 presents visualization by projecting the data onto the top two principle components. Compared with linear PCA, KPCA shrinks the distances between the clusters and brings out the important structures in the dataset. We can also see although the PCE values do not necessarily correspond to the clusters, higher PCE values tend to lie towards the center of the ring structure."
    }, {
      "heading" : "7.5 Kernel sliced inverse regression on KUKA dataset",
      "text" : "We evaluate our algorithm under the setting of kernel sliced inverse regression [14], a way to perform sufficient dimension reduction (SDR) for high dimension regression. After performing SDR, we fit a linear regression model using the projected input data, and evaluate mean squared error (MSE). The dataset records rhythmic motions of a KUKA arm at various speeds, representing realistic settings for robots [17]. We use a variant that contains 2 million data points generated by the SL simulator. The KUKA robot has 7 joints, and the high dimension regression problem is to predict the torques from positions, velocities and accelerations of\nthe joints. The input has 21 dimensions while the output is 7 dimensions. Since there are seven independent joints, we set the reduced dimension to be seven. We randomly select 20% as test set and out of the remaining training set, we randomly choose 5000 as validation set to select step sizes. The total number of random features is 10240, with mini-feature batch and mini-data batch both equal to 1024. We run a total of 2000 iterations using step size ηt = 15/(1 + 0.001 ∗ t).\nFigure 5 shows the regression errors for different methods. The error decreases with more random features, and our algorithm achieves lowest MSE by using 10240 random features. Nystrom features do not perform as well in this setting probably because the spectrum decreases slowly (there are seven independent joints) as Nystrom features are known to work well for fast decreasing spectrum."
    }, {
      "heading" : "8 Conclusions",
      "text" : "We have proposed a general and scalable approach to solve nonlinear component analysis based on doubly stochastic gradients. It is simple, efficient and scalable. In addition, we have theoretical guarantees that the whole subspace converges at the rate Õ(1/t) to the true subspace. Moreover, since its core is an algorithm for eigenvalue problems in the functional space, it can be applied to various other tasks and models. Finally, we demonstrate the scalability and effectiveness of our algorithm on both synthetic and real world datasets."
    }, {
      "heading" : "A Setting",
      "text" : "Notations Given a distribution P(x), a kernel function k(x, x′) with RKHS F , the covariance operator A : F 7→ F is a linear self-adjoint operator defined as\nAf(·) := Ex[f(x) k(x, ·)], ∀f ∈ F , (28)\nand furthermore\n〈g,Af〉F = Ex[f(x) g(x)], ∀g ∈ F .\nLet F = (f1(·), f2(·), . . . , fk(·)) be a list of k functions in the RKHS, and we define matrix-like notation\nAF (·) := (Af1(·), . . . , Afk(·)) , (29)\nand F>AF is a k × k matrix, whose (i, j)-th element is 〈fi, Afj〉F . The outer-product of a function v ∈ F defines a linear operator vv> : F 7→ F such that\n(vv>)f(·) := 〈v, f〉F v(·), ∀f ∈ F (30)\nLet V = (v1(·), . . . , vk(·)) be a list of k functions, then the weighted sum of a set of linear operators,{ viv > i }k i=1 , can be denoted using matrix-like notation as\nV ΣkV > := k∑ i=1 λiviv > i (31)\nwhere Σk is a diagonal matrix with λi on the i-th entry of the diagonal.\nKernel PCA Kernel PCA aims to identify the top k eigenfunctions V = (v1(·), . . . , vk(·)) for the covariance operator A, where V is also called the top k subspace for A.\nA function v is an eigenfunction of covariance operator A with the corresponding eigenvalue λ if\nAv(·) = λv(·). (32)\nGiven a set of eigenfunctions {vi} and associated eigenvalues {λi}, where 〈vi, vj〉F = δij . We can denote the eigenvalue of A as\nA = V ΣkV > + V⊥Σ⊥V > ⊥ (33)\nwhere V = (v1(·), . . . , vk(·)) is the top k eigenfunctions of A, and Σk is a diagonal matrix with the corresponding eigenvalues, V⊥ is the collection of the rest of the eigenfunctions, and Σ⊥ is a diagonal matrix with the rest of the eigenvalues.\nUpdate rules The stochastic update rule is Gt+1 = Gt + ηt ( I −GtG>t ) AtGt (34)\nwhere Gt := ( g1t , . . . , g k t ) and git is the i-th function. Denote the evaluation of Gt at the current data point as\ngt = [ g1t (xt), . . . , g k t (xt) ]> ∈ Rk. (35) Then the update rule can be re-written as\nGt+1 = Gt ( I − ηtgtg>t ) + ηtk(xt, ·)g>t . (36)\nThe doubly stochastic update rule is\nHt+1 = Ht ( I − ηththt> ) + ηtφωt(xt)φωt(·)ht >, (37)\nwhere ht is the evaluation of Ht at the current data point:\nht = [ h1t (xt), . . . , h k t (xt) ]> ∈ Rk. (38) When larger mini-batch sizes are used, the update rule is adjusted accordingly. For example, when using\nBx,t points { xbt } and Bω,t features { ωb ′ t } , the update rule for Ht is\nHt+1 ← Ht + ηt ∑ b,b′ ( φωb′t (xbt)φωb′t (·) [ h1t (x b t), . . . , h k t (x b t) ])\nBx,tBω,t\n− ηtHt\n( 1\nBx,t ∑ b [ hit(x b t)h j t (x b t) ] )k\ni,j=1\n."
    }, {
      "heading" : "B Analysis Roadmap",
      "text" : "In order to analyze the convergence of our doubly stochastic kernel PCA algorithm, we will need to define a few intermediate subspaces. For simplicity of notation, we will assume the mini-batch size for the data points is one.\n1. Let Ft := ( f1t , . . . , f k t ) be the subspace estimated using stochastic gradient and explicit orthogonaliza-\ntion:\nF̃t+1 ← Ft + ηtAtFt (39) Ft+1 ← F̃t+1 ( F̃>t+1F̃t+1 )−1/2 2. Let Gt := ( g1t , . . . , g k t ) be the subspace estimated using stochastic update rule without orthogonaliza-\ntion:\nGt+1 ← Gt + ηt ( I −GtG>t ) AtGt. (40)\nwhere AtGt and GtG > t AtGt can be equivalently written using the evaluation of the function { git }\non the current data point, leading to the equivalent rule:\nGt+1 ← Gt ( I − ηtgtg>t ) + ηtk(xt, ·)g>t . (41)\n3. Let G̃t := ( g̃1t , . . . , g̃ k t ) be the subspace estimated using stochastic update rule without orthogonaliza-\ntion, but the evaluation of the function { g̃it } on the current data point is replaced by the evaluation\nht = [ hit(xt) ]> :\nG̃t+1 ← G̃t + ηtk(xt, ·)h>t − ηtG̃thth>t (42)\n4. Let Ht := ( h1t , . . . , h k t ) be the subspace estimated using doubly stochastic update rule without orthog-\nonalization, i.e., the update rule:\nHt+1 ← Ht + ηtφωt(xt)φωt(·)h>t − ηtHthth>t . (43)\nThe relation of these subspaces are summarized in Table 4. Using these notations, we describe a sketch of our analysis in the rest of the section, while the complete proofs are provided in the following sections.\nWe first consider the subspace Gt estimated using the stochastic update rule, since it is simpler and its proof can provide the bases for analyzing the subspace Ht estimated by the doubly stochastic update rule.\nB.1 Stochastic update\nOur guarantee is on the cosine of the principal angle between the computed subspace and the ground truth eigen subspace V (also called the potential function), which is a standard criterion for measuring the quality of the subspace:\ncos2 θ(V,Gt) = min w ∥∥V >Gtw∥∥2 ‖Gtw‖2 .\nWe will focus on the case when a good initialization V0 is given:\nV >0 V0 = I, cos 2 θ(V, V0) ≥ 1/2. (44)\nIn other words, we analyze the later stage of the convergence, which is typical in the literature (e.g., [28]). The early stage can be analyzed using established techniques (e.g., [4]).\nWe will also focus on the dependence of the potential function on the step t. For this reason, throughout the paper we suppose |k(x, x′)| ≤ κ, |φω(x)| ≤ φ and regard κ and φ as constants. Note that this is true for all the kernels and corresponding random features considered. We further regard the eigengap λk − λk+1 as a constant, which is also true for typical applications and datasets. Details can be found in the following sections.\nOur final guarantee for Gt is stated in the following.\nTheorem 2. Assume (44) and suppose the mini-batch sizes satisfy that for any 1 ≤ i ≤ t, ‖A−Ai‖ < (λk − λk+1)/8. There exist step sizes ηi = O(1/i) such that\n1− cos2 θ(V,Gt) = O(1/t).\nThe convergence rate O(1/t) is in the same order as that when computing only the top eigenvector in linear PCA [4], though we are not aware of any other convergence rate for computing the top k eigenfunctions in Kernel PCA. The bound requires the mini-batch sizes are large enough so that the spectral norm of A is approximated up to the order of the eigengap. This is due to the fact that approximating A with At will result in an error term in the order of ‖A−At‖, while the increase of the potential is in the order of the eigengap. Similar terms appear in the analysis of the noisy power method [10] which, however, requires normalization and is not suitable for the kernel case. We do not specify the mini-batch sizes, but by assuming suitable data distributions, it is possible to obtain explicit bounds; see for example [30, 5].\nProof sketch To prove the theorem, we first prove the guarantee for the normalized subspace Ft which is more convenient to analyze, and then show that the update rules for Ft and Gt are first order equivalent so that Gt enjoys the same guarantee.\nLemma 3. 1− cos2 θ(V, Ft) = O(1/t).\nLet c2t denote cos 2 θ(V, Ft), then a key step in proving the lemma is to show that\nc2t+1 ≥ c2t (1 + 2ηt(λk − λk+1 − 2 ‖A−At‖)(1− c2t ))−O(η2t ). (45)\nTherefore, we will need the mini-batch sizes large enough so that 2 ‖A−At‖ is smaller than the eigen-gap. Another key element in the proof of the theorem is the first order equivalence of the two update rules. To show this, we need to compare the subspaces obtained by applying the them on the same subspace Gt. So we introduce F (Gt) to denote the subspace by applying the update rule of Ft on Gt:\nF̃ (Gt)← Gt + ηtAtGt F (Gt)← F̃ (Gt) [ F̃ (Gt) >F̃ (Gt) ]−1/2\nWe show that the potentials of Gt+1 and F (Gt) are close:\nLemma 4. cos2 θ(V,Gt+1) = cos 2 θ(V, F (Gt))±O(η2t ).\nThe lemma means that applying the two update rules to the same input will result in two subspaces with similar potentials. Since cos2 θ(V, F (Gt)) enjoys the recurrence in (45), we know that cos\n2 θ(V,Gt+1) also enjoys such a recurrence, which then results in 1− cos2 θ(V,Gt) = O(1/t).\nThe proof of the lemma is based on the observation that\ncos2 θ(V,X) = λmin(V >X(X>X)−1X>V ).\nThe lemma follows by plugging in X = Gt+1 or X = F (Gt) and comparing their Taylor expansions w.r.t. ηt.\nB.2 Doubly stochastic update\nFor doubly stochastic update rule, the computed Ht is no longer in the RKHS so the principal angle is not well defined. Since the eigenfunction v is usually used for evaluating on points x, we will use the following point-wise convergence in our analysis. For any function v in the subspace of V with unit norm ‖v‖F = 1, we will find a specially chosen function h in the subspace of Ht such that for any x,\nerr := |v(x)− h(x)|2\nis small with high probability. More specifically, the w is chosen to be G̃>v, and let z = G̃tw and h = Htw. Then the error measure can be decomposed as\n|v(x)− h(x)|2\n= |v(x)− z(x) + z(x)− h(x)|2\n≤ 2 |v(x)− z(x)|2 + 2 |z(x)− h(x)|2 ≤ 2κ2 ‖v − z‖2F︸ ︷︷ ︸ (I: Lemma 6) + 2 |z(x)− h(x)|2︸ ︷︷ ︸ (II: Lemma 7) . (46)\nThe distance ‖v − z‖F is closely related to the squared sine of the subspace angle between V and G̃t. In fact, by definition, ‖v − z‖2F = ‖v‖ 2 F − ‖z‖ 2 F ≤ 1 − cos2 θ(V, G̃t). Therefore, the first error term can be bounded by the guarantee on G̃t, which can be obtained by similar arguments as for the stochastic update case. For the second term, note that G̃t is defined in such a way that the difference between z(x) = G̃t(x)w and h(x) = Ht(x)w is a martingale, which can be bounded by careful analysis.\nOverall, we have the following results. Suppose we use random Fourier features; see [22]. Similar bounds hold for other random features, where the batch sizes will depend on the concentration bound of the random features used.\nTheorem 5. Assume (44) and suppose the mini-batch sizes satisfy that for any 1 ≤ i ≤ t, ‖A−Ai‖ < (λk − λk+1)/8 and are of order Ω(ln tδ ). There exist step sizes ηi = O(1/i), such that the following holds. If Ω(1) = λk(G̃ > i G̃i) ≤ λ1(G̃>i G̃i) = O(1) for all 1 ≤ i ≤ t, then for any x and any function v in the span of V with unit norm ‖v‖F = 1, we have that with probability ≥ 1− δ, there exists h in the span of Ht satisfying\n|v(x)− h(x)|2 = O ( 1\nt ln t δ\n) .\nThe point-wise error scales as Õ(1/t) with the step t, which is in similar order as that for the stochastic update rule. Again, we require the spectral norm of A to be estimated up to the order of the eigengap, for the same reason as before. We additionally need that the random features approximate the kernel function up to constant accuracy on all the data points up to time t, since the evaluation of the kernel function on these points are used in the update. This eventually leads to Ω(ln tδ ) mini-batch sizes. Finally, we need G̃ > i G̃i to be roughly isotropic, i.e., G̃i is roughly orthonormal. Intuitively, this should be true for the following reasons: G̃0 is orthonormal; the update for G̃t is close to that for Gt, which in turn is close to Ft that are orthonormal.\nProof sketch The analysis is carried out by bounding each term in (46) separately. As discussed above, in order to bound term I, we need a bound on the squared cosine of the subspace angle between V and G̃t.\nLemma 6 . 1− cos2 θ(V, G̃t) = O ( 1 t ln t δ ) .\nTo prove this lemma, we follow the argument for Theorem 2 and get the recurrence as shown in (45), except with an additional error term, which is caused by the fact that the update rule for G̃t+1 is using the evaluation ht(xt) rather than g̃t(xt). Bounding this additional term thus relies on bounding the difference between ht(x)− g̃t(x), which is also what we need for bounding term II in (46). For this purpose, we show the following bound: Lemma 7 . For any x and unit vector w, with probability ≥ 1 − δ over (Dt, ωt), |g̃t(x)w − ht(x)w|2 = O (\n1 t ln ( t δ )) .\nThe key to prove this lemma is that our construction of G̃t makes sure that the difference between g̃t(x)w and ht(x)w consists of their difference in each time step. Furthermore, the difference in each time step conditioned on previous history has mean 0. In other words, the difference forms a martingale and\nthus can be bounded by Azuma’s inequality. The resulting bound depends on the mini-batch sizes, the step sizes ηi, and the evaluations hi(xi) used in the update rules. We then judiciously choose the parameters and simplify it to the bound in the lemma. The complication of the proof is mostly due to the interweaving of the parameter values; see the following sections for the details."
    }, {
      "heading" : "C Stochastic Update",
      "text" : "To prove the convergence of the stochastic update rule, we first prove the convergence of the normalized version Ft, and then we establish the first-order equivalence of the potential functions of the two update rules for Ft and Gt. Since the final recurrence result does not depend on higher order terms, this first-order equivalence establishes the convergence of the stochastic update rule without normalization.\nC.1 Stochastic update with normalization\nWe consider the potential function 1− cos2 θ (V, Ft) and prove a recurrence for it. We first show this for the simpler case where at each step we use the expected operator A in the update rule (Lemma 8), and then show this for the general case where At can be different from A (Lemma 9). Then the bound in Lemma 3 follows from solving the recurrence in Lemma 9.\nC.1.1 Update rule with expected operator\nThe following lemma states the recurrence for the update rule which replace At in the stochastic update rule with the expected operator A = EAt:\nF̃t+1 ← Ft + ηtAFt (47) Ft+1 ← F̃t+1 ( F̃>t+1F̃t+1 )−1/2 Lemma 8. Let the sequence {Fi}i be obtained from the update rule (47), then\n1− cos2 θ (V, Ft+1) ≤ [ 1− cos2 θ (V, Ft) ] [ 1− 2ηt (λk − λk+1) cos2 θ (V, Ft) ] + βt,\nwhere βt = 5η 2 tB 2 + 3η3tB 3 and λk and λk+1 are the top k and k + 1-th eigenvalues of A.\nProof. First note that the cosine of subspace angle does not change under linear combination of the basis\ncos2 θ (V, Ft+1) = min w′ ∥∥V >Ft+1w′∥∥2 ‖Ft+1w′‖2 = min w′ ∥∥∥∥V >F̃t+1 (F̃>t+1F̃t+1)−1/2 w′∥∥∥∥2∥∥∥∥F̃t+1 (F̃>t+1F̃t+1)−1/2 w′∥∥∥∥2 = min w ∥∥∥V >F̃t+1w∥∥∥2∥∥∥F̃t+1w∥∥∥2 (48) The update rule gives us∥∥∥V >F̃t+1w∥∥∥2 ≥ ∥∥V >Ftw∥∥2 + 2ηt 〈V >Ftw, V >AFtw〉 (49)\n∥∥∥F̃t+1w∥∥∥2 ≤ ‖Ftw‖2 + 2ηt 〈Ftw,AFtw〉+B ‖Ftw‖2 η2t (50)\nLet ŵ = w/ ‖Ftw‖, u = Ftŵ, so ‖u‖ = 1. Denote c = ∥∥V >u∥∥ and s = ∥∥V >⊥ u∥∥. According to the\ndefinition, we have c ≥ cos θk (V, Ft). Keep expanding the update rule leads to∥∥∥V >F̃t+1w∥∥∥2∥∥∥F̃t+1w∥∥∥2 ≥ ∥∥V >Ftw∥∥2 + 2ηt 〈V >Ftw, V >AFtw〉 ‖Ftw‖2 + 2ηt 〈Ftw,AFtw〉+B ‖Ftw‖2 η2t\n(51)\n= ∥∥V >u∥∥2 + 2ηt 〈V >u, V >Au〉 1 + 2ηt 〈u,Au〉+Bη2t\n≥ {∥∥V >u∥∥2 + 2ηt 〈V >u, V >Au〉}{1− 2ηt 〈u,Au〉 −Bη2t }\n≥ ∥∥V >u∥∥2 + 2ηt 〈V >u, V >Au〉− 2ηt ∥∥V >u∥∥2 〈u,Au〉 − 5η2tB2 − 2η3tB3\n= c2 + 2ηt { u>V V >Au− c2u>Au } − βt\n= c2 + 2ηtu > (V V > − c2I)Au− βt\n= c2 + 2ηtu > (s2V V > − c2V⊥V >⊥ )Au− βt.\nRecall that A = V ΛkV > + V⊥Λk+1V > ⊥ . Then\nu> ( s2V V > − c2V⊥V >⊥ ) Au = s2u>V ΛkV\n>u− c2u>V⊥Λk+1V >⊥ u (52) ≥ λks2c2 − λk+1c2s2 = s2c2 (λk − λk+1)\nThe recurrence is therefore\ncos2 θ (V, Ft+1) ≥ c2 + 2ηts2c2 (λk − λk+1)− βt (53) = c2 ( 1 + 2ηt (λk − λk+1) ( 1− c2 )) − βt.\nThe first term is a quadratic function of c2:\nx (1 + a (1− x)) (54)\nwhere x := c2 and a = 2ηt (λk − λk+1). It has two roots at 0 and 1 + 1a . Therefore, if 1 2 + 1 2a ≥ 1, it is a monotonic increasing function in the interval of [0, 1]. Thus, if ηt ≤ 14(λk−λk+1) , which holds for all t large enough, we have\ncos2 θ (V, Ft+1) ≥ cos2 θ (V, Ft) ( 1 + 2ηt (λk − λk+1) ( 1− cos2 θ (V, Ft) )) − βt (55)\nwhich leads to the lemma.\nC.1.2 Using different operators in different iterations\nNow consider the case of stochastic update rule (39) where we use a mini-batch to approximate the expectation in each iteration.\nLemma 9. Let the sequence {Fi}i be obtained from the update rule (39), then\n1− cos2 θ (V, Ft+1) ≤ [ 1− cos2 θ (V, Ft) ] [ 1− 2ηt (λk − λk+1 − ‖At −A‖) cos2 θ (V, Ft+1) ] + βt,\nwhere βt = 5η 2 tB 2 + 3η3tB 3 and λk and λk+1 are the top k and k + 1-th eigenvalues of A.\nProof. The effect of the stochastic update is an additional term in the recurrence cos2 θ (V, Ft+1) ≥ c2 + 2ηtu> ( s2V V > − c2V⊥V >⊥ ) Au+ Zt − βt (56)\nwhere\nZt = 2ηtu > (s2V V > − c2V⊥V >⊥ ) (At −A)u. (57)\nThe effect of the noise can be bounded, i.e.\nZt = 2ηts 2u>V V > (At −A)u− 2ηtc2u>V⊥V >⊥ (At −A)u (58)\n= 2ηts 2u> ( V V > + l1I ) (At −A)u− 2ηtc2u> ( V⊥V > ⊥ + l2I ) (At −A)u,\nwhere s2l1 = c 2l2 are positive numbers such that V V > + l1I and V⊥V > ⊥ + l2I are positive-definite.\nThe generalized Rayleigh quotient leads to the inequality∣∣u> (V V > + l1I) (At −A)u∣∣ ≤ λu> (V V > + l1I)u (59) ≤ λ ( c2 + l1\n) where λ is the largest generalized eigen-value that satisfies(\nV V > + l1I ) (At −A)x = λ ( V V > + l1I ) x. (60)\nSince V V > + l1I is positive definite, we have λ = ‖At −A‖. Similarly, we have ∣∣u> (V⊥V >⊥ + l2I) (At −A)u∣∣ ≤ ‖At −A‖ (s2 + l2) . (61) The noise term is thus bounded by\nZt ≥ −2ηts2 ‖At −A‖ ( c2 + l1 ) − 2ηtc2 ‖At −A‖ ( s2 + l2 ) . (62)\nNote that l1 and l2 can be infinitely small positive so we can ignore them. Therefore, the recurrence is\ncos2 θ (V, Ft+1) ≥ c2 + 2ηts2c2 (λk − λk+1)− 4ηt ‖At −A‖ s2c2 − βt (63) = c2 ( 1 + 2ηt (λk − λk+1 − 2 ‖At −A‖) ( 1− c2 )) − βt\nwhich then leads to the lemma.\nIn order to get fast convergence, we need to take sufficiently large mini-batches such that the variance of the noise is small enough compared with the eigen-gap.\nC.2 Stochastic update without normalization\nWe show that the cosine angles of the two updates are first-order equivalent. Then, since the recurrence is not affected by higher order terms, when the step size is small enough, we can show it also converges in O(1/t).\nTo show the first order equivalence, we need to compare the subspaces obtained by applying the them on the same subspace Gt. So we introduce F (Gt) to denote the subspace by applying the update rule of Ft on Gt:\nF̃ (Gt)← Gt + ηtAtGt (64) F (Gt)← F̃ (Gt) [ F̃ (Gt) >F̃ (Gt) ]−1/2\nThen the first order equivalence as stated in Lemma 4 follows from the following two lemmas for the normalized update rule (39) and the unnormalized update rule (64), respectively.\nLemma 10. cos2 θ (V, F (Gt)) = λmin ( M +O(η2) ) where\nM = V >PP>V + ηV >PP>AV + ηV >APP>V − 2ηV >PP>APP>V,\nwhere PP> = Gt ( G>t Gt )−1 G>t , and P is an orthonormal basis for the subspace Gt.\nProof. For simplicity, let G denote Gt, and let A denote At in the following. We first have cos2 θ (V, F (G)) = λmin ( V >F (G)F (G)>V ) (65)\n= λmin ( F (G)>V V >F (G) ) (66)\n= λmin { V > (G+ ηtAG) [ (G+ ηtAG) > (G+ ηtAG) ]−1 (G+ ηtAG) > V } . (67)\nNote that (66) is due to the fact that\nλmin ( F (G)>V V >F (G) ) = min\nw\nw>F (G)>V V >F (G)w\nw>w\n= min w\nw>R−1 (G+ ηtAG) > V V > (G+ ηtAG)R −1w\nw>w\n= min z\nz> (G+ ηtAG) > V V > (G+ ηtAG) z\nz>R2z\n= min z\nz> (G+ ηtAG) > V V > (G+ ηtAG) z\nz> (G+ ηtAG) > (G+ ηtAG) z\n= min z ∥∥V > (G+ ηtAG) z∥∥2 ‖(G+ ηtAG) z‖2\nwhere R = [ (G+ ηtAG) > (G+ ηtAG) ]1/2 .\nNow turn back to (67). Expand the matrix-valued function φ(η) = [ (G+ ηAG) > (G+ ηAG) ]−1 (68)\n= φ(0) + φ′(0)η +O(η2).\nφ′(0) = −2 ( G>G )−1 G>AG ( G>G )−1 . (69)\nSo,\nφ(η) = ( G>G )−1 − 2η (G>G)−1G>AG (G>G)−1 +O(η2). (70) Therefore,\nV > (G+ ηtAG) [ (G+ ηtAG) > (G+ ηtAG) ]−1 (G+ ηtAG) > V (71)\n= ( V >G+ ηtV >AG ) [( G>G )−1 − 2η (G>G)−1G>AG (G>G)−1 +O(η2)] (G>V + ηtG>AV )\n= V >G ( G>G )−1 G>V + ηV >G ( G>G )−1 G>AV + ηV >AG ( G>G )−1 G>V\n− 2ηV >G ( G>G )−1 G>AG ( G>G )−1 G>V +O(η2)\n= V >PP>V + ηV >PP>AV + ηV >APP>V − 2ηV >PP>APP>V +O(η2),\nwhere PP> = G ( G>G )−1 G>, and P is an orthonormal basis for the subspace G.\nLemma 11. cos2 θ (V,Gt+1) = λmin (M) where M is as defined in Lemma 10.\nProof. For simplicity, let G denote Gt and let A denote At. Then cos 2 θ (V,Gt+1) = λmin (N), where\nN = V >Gt+1 [ G>t+1Gt+1 ]−1 G>t+1V with Gt+1 = G+ η ( I −GG> ) AG.\nNow it suffices to show N = M . Consider φ(η) = [( G+ η ( I −GG> ) AG )> ( G+ η ( I −GG> ) AG )]−1 .\nThen\nφ′(0) = − ( G>G )−1 [ G> ( I −GG> ) AG+G>A ( I −GG> ) G ] ( G>G )−1 Therefore, N is\nV > ( G+ η ( I −GG> ) AG ) [( G+ η ( I −GG> ) AG )> ( G+ η ( I −GG> ) AG )]−1\n× ( G+ η ( I −GG> ) AG )> V\n= ( V >G+ ηV > ( I −GG> ) AG ) [( G+ η ( I −GG> ) AG )> ( G+ η ( I −GG> ) AG )]−1\n× ( G>V + ηG>A ( I −GG> ) V )\n= ( V >G+ ηV > ( I −GG> ) AG )\n× [( G>G )−1 − η (G>G)−1 [G> (I −GG>)AG+G>A (I −GG>)G] (G>G)−1] × ( G>V + ηG>A ( I −GG> ) V )\n= V >G ( G>G )−1 G>V + ηV >G ( G>G )−1 G>A ( I −GG> ) V + ηV > ( I −GG> ) AG ( G>G )−1 G>V\n−ηV >G ( G>G )−1 [ G> ( I −GG> ) AG+G>A ( I −GG> ) G ] ( G>G )−1 G>V\n= V >PP>V + ηV >PP>A ( I −GG> ) V + ηV > ( I −GG> ) APP>V\n−ηV >PP> ( I −GG> ) APP>V − ηV >PP>A ( I −GG> ) PP>V\n= V >PP>V + ηV >PP>AV + ηV >APP>V − 2ηV >PP>APP>V −ηV >PP>AGG>V − ηV >GG>APP>V + ηV >PP>GG>APP>V + ηV >PP>AGG>PP>V = V >PP>V + ηV >PP>AV + ηV >APP>V − 2ηV >PP>APP>V\nwhich completes the proof."
    }, {
      "heading" : "D Doubly Stochastic Update",
      "text" : "In this section, we consider the doubly stochastic update rule. Suppose in step t, we use a mini-batch consisting of Bx,t random data points x r t (1 ≤ r ≤ Bx,t) and Bω,t random features ωst (1 ≤ s ≤ Bω,t). Then the update rule is\nHt+1 = Ht + ηtEt [φωt(xt)φωt(·)ht(xt)]− ηtHtEt [ ht(xt) >ht(xt) ]\n(72) = Ht(I − ηtEt [ ht(xt) >ht(xt) ] ) + ηtEt [φωt(xt)φωt(·)ht(xt)] (73)\nwhere for any function f(x, ω), Etf(xt, ω) denotes ∑Bx,t r=1 ∑Bω,t s=1 f(x r t , ω s t )/(Bx,tBω,t). As before, we assume H0 = F0 is a good initialization, i.e., F > 0 F0 = I and cos\n2 θ(F0, V ) ≥ 1/2. Note that Ht = [h1t (·), . . . , hkt (·)], while ht(xt) is its evaluation at xt, i.e., ht(xt) is a row vector [h 1 t (xt), . . . , h k t (xt)].\nWe introduce the following intermediate function for analysis: G̃t+1 = G̃t + ηtEt [k(xt, ·)ht(xt)]− ηtG̃tEt [ ht(xt) >ht(xt) ]\n(74) = G̃t(I − ηtEt [ ht(xt) >ht(xt) ] ) + ηtEt [k(xt, ·)ht(xt)] . (75)\nAgain, G̃0 = F0. The analysis follows our intuition: we first bound the difference between Ht and G̃t by a martingale argument, and then bound the difference between G̃t and V . For the second step we can apply the previous argument. Note that G̃t is different from Ft since AtFt = k(xt, ·)Ft(xt) is now replaced by k(xt, ·)ht(xt), so we need to adjust our previous analysis.\nSuppose we use random Fourier features for points in Rd; see [22]. Then we have\nTheorem 5. Suppose the mini-batch sizes satisfy that for any 1 ≤ i ≤ t, ‖A−Ai‖ < (λk − λk+1)/8 and Bx,i = Ω(ln t δ ). There exist step sizes ηi = O(1/i), such that the following holds. If Ω(1) = λk(G̃ > i G̃i) ≤ λ1(G̃ > i G̃i) = O(1) for all 1 ≤ i ≤ t, then for any x and any function v in the span of V with unit norm ‖v‖F = 1, we have that with probability ≥ 1− δ, there exists h in the span of Ht satisfying\n|v(x)− h(x)|2 = O ( 1\nt ln t δ\n) .\nProof. Let w = G̃>t v, z = G̃tw, and h = Htw.\n|v(x)− h(x)|2 = |v(x)− z(x) + z(x)− h(x)|2\n≤ 2 |v(x)− z(x)|2 + 2 |z(x)− h(x)|2\n≤ 2 ‖v − z‖2F ‖k(x, ·)‖ 2 F + 2 |z(x)− h(x)| 2 ≤ 2κ2 ‖v − z‖2F + |z(x)− h(x)| 2 .\nRoughly speaking, the difference between v and z is the error due to random data points and can be bounded by Lemma 15, while the difference between z(x) and h(x) is the error due to random features and can be bounded by Lemma 13. More precisely, since z is the projection of v on the span of G̃t,\n‖v − z‖2F = ‖v‖ 2 F − ‖z‖ 2 F ≤ 1− cos 2 θ(G̃t, V ) = O\n( 1\nt ln t δ ) where the last step is by Lemma 15. Also, since ‖w‖ ≤ 1, we have |z(x)− h(x)|2 = O ( 1 t ln t δ ) by Lemma 13.\nWhat is left is to check the mini-batch sizes; see the assumptions in Lemma 12 and Lemma 15. We need λk(Ei [ hi(xi) >hi(xi) ] ) = λk(Ex [ hi(x) >hi(x) ] )±O(1), so we only need to estimate Ex [ hji (x) >h`i(x) ] up to constant accuracy for all 1 ≤ j, ` ≤ k, for which Bx,i = O(ln tδ ) suffices. We also need ∆ω = O(λk −λk+1) = O(1), so we only need ∆ω = O(1). This is a bound for (tBx,i) 2 pairs of points, for which Bω,i = O(ln t δ ) suffices.\nSimilar bounds hold for other random features, where the batch sizes will depend on the concentration bound of the random features used.\nThe rest of this section is the proof of the theorem. For simplicity, ‖·‖F is shorten as ‖·‖. First, we bound the difference between Ht and G̃t.\nLemma 12. Suppose |k(x, x′)| ≤ κ, |φω(x)| ≤ φ. Suppose the mini-batch sizes are large enough so that∣∣∣k(xi, xj)−∑Bω,is=1 φωs(xi)φωs(xj)/Bω,i∣∣∣ ≤ ∆ω for all sampled data points xi and xj. For any w and x, with probability ≥ 1− δ over (Dt, ωt),\n|g̃t+1(x)w − ht+1(x)w|2 ≤ B2t+1 := 1\n2 ∆2ω ln\n( 2\nδ ) t∑ i=1 ∣∣Ei |hi(xi)| at,iw∣∣2\nwhere at,i = ηi ∏t j=i+1 ( I − ηjEj [ hj(xj) >hj(xj) ]) for 1 ≤ i ≤ t, and |hi(xi)| := [∣∣∣hji (xi)∣∣∣]k\nj=1 .\nProof. Note that\nHt+1 = t∑ i=1 Ei [φωi(xi)φωi(·)hi(xi)] at,i + F0at,0, (76)\nG̃t+1 = t∑ i=1 Ei [k(xi, ·)hi(xi)] at,i + F0at,0, (77)\nwhere at,0 = ∏t j=1 ( I − ηjEj [ hj(xj) >hj(xj) ]) .\nWe have g̃t+1(x)w − ht+1(x)w = ∑t i=1 Vt,i(x) where\nVt,i(x) = Ei [k(xi, x)hi(xi)− φωi(xi)φωi(x)hi(xi)] at,iw.\nVt,i(x) is a function of (Di, ωi) and\nEDi,ωi [ Vt,i(x)|ωi−1 ] = EDi,ωi−1Eωi [ Vt,i(x)|ωi−1 ] = 0,\nso {Vt,i(x)} is a martingale difference sequence. Since |Vt,i(x)| < ∆ω|Ei |hi(xi)| at,iw|, the lemma follows from Azuma’s Inequality.\nSo to bound |g̃t(x)w − ht(x)w|, we need to bound |Ei |hi(xi)| at,iw|, which requires some additional assumptions.\nLemma 13 (Complete version of Lemma 7). Suppose the conditions in Lemma 12 are true. Further suppose for all i ≤ t, ηi = θ/i where θ is sufficiently large so that θλk(Ei [ hi(xi) >hi(xi) ] ) ≥ 1; also suppose\nλ1 ( G̃>i G̃i ) = O(1).\n(1) With probability ≥ 1− δ over (Dt, ωt), for all 1 ≤ i ≤ t and ` ∈ [k], we have\n|g̃`i (xi)− h`i(xi)|2 = O ( ∆2ωθ 4\nt ln\n( t\nδ\n)) .\n(2) For any x and unit vector w, with probability ≥ 1− δ over (Dt, ωt),\n|g̃t(x)w − ht(x)w|2 = O ( ∆2ωθ 4\nt ln\n( t\nδ\n)) .\nProof. We first do induction on statement (1), which is true initially. Assume it is true for t, we prove it for t+ 1.\nWe have that for any unit vector w,\n|Ei |hi(xi)| at,iw| = ∣∣∣∣∣∣ηiEi |hi(xi)| t∏\nj=i+1\n[ I − ηjEj [ hj(xj) >hj(xj) ]] w ∣∣∣∣∣∣ ≤ ηi ‖Ei |hi(xi)|‖ ‖w‖\nt∏ j=i+1 ∥∥I − ηjEj [hj(xj)>hj(xj)]∥∥ ≤ O(1)θ 2\ni t∏ j=i+1 ( 1− 1 j ) = O ( θ2 t ) .\nWe use in the second line\n‖hi(xi)‖ ≤ O\n(√ θ2\nt ln t δ\n) + ‖g̃i(xi)‖ ≤ O (√ θ2\nt ln t δ\n) + √∥∥∥G̃>i G̃i∥∥∥ ‖φ(xi)‖ = O(θ) that holds with probability 1−tδ/(t+1) by induction, and we use in the last line θλk(Ei [ hi(xi) >hi(xi) ] ) ≥ 1.\nThen by Lemma 12, with probability ≥ 1− δ/(k(t+ 1)),\n|g̃t+1(xt+1)w − ht+1(xt+1)w|2 ≤ 1\n2 ∆2ω ln\n( 2(t+ 1)\nδ\n) t∑ i=1 ∣∣Ei |hi(xi)| at,iw∣∣2 ≤ O(∆2ω) ln ( t+ 1\nδ ) t∑ i=1 θ4 t2 = O ( ∆2ωθ 4 t+ 1 ln ( t+ 1 δ )) .\nRepeating the argument for k basis vectors w = ei(1 ≤ i ≤ k) completes the proof. The other statement follows from similar arguments.\nNext, we bound the difference between G̃t and V .\nLemma 14. Suppose the conditions in Lemma 13 are true and furthermore, λk(G̃ > i G̃i) = Ω(1) for all i ∈ [t]. Let c2t denote cos 2 θ(G̃t, V ). Then with probability ≥ 1− δ,\nc2t+1 ≥ c2t { 1 + 2ηt [ λk − λk+1 − 2 ‖At −A‖ −O ( ∆ωθ 2 √ 1\nt ln t δ\n)]( 1− c2t ) −O ( ηt∆ωθ 2 √ 1− c2t t ln t δ )} −βt\nwhere βt is as defined in Lemma 8.\nProof. The potential of G̃t can be computed by a similar argument as in the previous section; the only difference is replacing Atu with k(xt, ·)ht(xt)ŵ. This leads to\ncos2 θ(G̃t+1, V ) ≥ c2 + 2ηtu> ( s2V V > − c2V⊥V >⊥ ) k(xt, ·)ht(xt)ŵ − βt\n= c2 + 2ηtu > (s2V V > − c2V⊥V >⊥ ) [(k(xt, ·)ht(xt)ŵ −Atu) + (Atu−Au) +Au]− βt (78)\nwhere u = G̃tŵ with unit norm ‖u‖ = 1. The terms involving (Atu−Au) and Au can be dealt with as before, so we only need to bound the extra term\nu> ( s2V V > − c2V⊥V >⊥ ) [k(xt, ·)ht(xt)ŵ −Atu]\n= u> ( s2V V > − c2V⊥V >⊥ ) [k(xt, ·)ht(xt)ŵ − k(xt, ·)g̃t(xt)ŵ]\n= u> ( s2V V > − c2V⊥V >⊥ ) k(xt, ·)[ht(xt)− g̃t(xt)]ŵ.\nSo we need to bound [ht(xt) − g̃t(xt)]ŵ, which in turn relies on Lemma 13. More precisely, we have ‖ht(xt)− g̃t(xt)‖∞ ≤ Õ ( ∆ωθ 2 √ 1/t )\nwith probability ≥ 1 − δ. Also, we have u = G̃tŵ has unit norm, so ‖ŵ‖ = O(1) when λk(G̃>i G̃i) = Ω(1). Then∣∣u>V V >k(xt, ·)[ht(xt)− g̃t(xt)]ŵ∣∣ ≤∥∥u>V ∥∥ ‖k(xt, ·)‖ Õ (∆ωθ2√1/t) ≤ c2Õ (∆ωθ2√1/t) where the last step follows from c ≥ 1/2 by assumption. Similarly,\n∣∣u>V⊥V >⊥ k(xt, ·)[ht(xt)− g̃t(xt)]ŵ∣∣ ≤∥∥u>V⊥∥∥ ‖k(xt, ·)‖ Õ (∆ωθ2√1/t) ≤ sÕ (∆ωθ2√1/t) = Õ ( ∆ωθ 2 √ 1− c2 t ) .\nPlugging into (78) and apply a similar argument as in Lemma 8 and 9 we have the lemma.\nLemma 15 (Complete version of Lemma 6). If the mini-batch sizes are large enough so that ‖A−Ai‖ < (λk − λk+1)/8, λk(Ei [ hi(xi) >hi(xi) ] ) = λk(Ex [ hi(x) >hi(x) ] )±O(1), and ∆ω = O(λk − λk+1), then\n(1) θ = O(1); (2) 1− c2t = O ( 1 t ln t δ ) .\nProof. If the mini-batch size is large enough so that λk(Ei [ hi(xi) >hi(xi) ] ) = λk(Ex [ hi(x) >hi(x) ] )±O(1),\nwe only need to show λk(Ex [ hi(x) >hi(x) ] ) = Ω(1), which will lead to θ = O(1) and then solving the recurrence in Lemma 14 leads to 1− cos2 θ(G̃t+1, V ) = Õ(1/t). Let ei(x) = hi(x)− g̃i(x). Then\nEx [ hi(x) >hi(x) ] = Ex [ g̃i(x) >g̃i(x) ] + 2Ex [ ei(x) >hi(x) ] − Ex [ ei(x) >ei(x) ] .\nBy Lemma 13, Ex ∣∣∣eji (x)∣∣∣ = Õ(θ4/t), which is o(1) if θ = O(1). Then the norm of 2Ex [ei(x)>hi(x)] −\nEx [ ei(x) >ei(x) ] is o(1), so we only need to consider Ex [ g̃i(x) >g̃i(x) ] .\nFormally, we prove our statements (1)(2) by induction. They are true initially. Suppose they are true for t− 1, we prove them for t.\nFirst, by solving the recurrence for ct, we have that statement (2) is true up to step t. Next, since Ex [ g̃t(x) >g̃t(x) ] = G̃>t AG̃t, we have\nw>Ex [ g̃t(x) >g̃t(x) ] w =w>G̃>t AG̃tw\n=w>G̃>t (V ΛkV > + V⊥Λ⊥V > ⊥ )G̃tw ≥w>G̃>t V ΛkV >G̃tw ≥λkc2t ‖w‖ 2\nwhich means λk(Ex [ g̃t(x) >g̃t(x) ] ) = Ω(1) by induction on ct and by the assumption that λk(G̃ > t G̃t) = Ω(1).\nThis then leads to λk(Ei [ hi(xi) >hi(xi) ] )) = Ω(1), which means θ = O(1) up to step t."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "<lb>Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel<lb>Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis,<lb>but they can not scale up to big datasets. Recent attempts have employed random feature approximations<lb>to convert the problem to the primal form for linear computational complexity. However, to obtain high<lb>quality solutions, the number of random features should be the same order of magnitude as the number<lb>of data points, making such approach not directly applicable to the regime with millions of data points.<lb>We propose a simple, computationally efficient, and memory friendly algorithm based on the “doubly<lb>stochastic gradients” to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA<lb>and SVD. Despite the non-convex nature of these problems, our method enjoys theoretical guarantees<lb>that it converges at the rate Õ(1/t) to the global optimum, even for the top k eigen subspace. Unlike<lb>many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big<lb>datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and<lb>real world datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}