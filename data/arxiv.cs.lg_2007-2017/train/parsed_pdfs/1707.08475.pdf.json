{
  "name" : "1707.08475.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning",
    "authors" : [ "Irina Higgins", "Arka Pal", "Andrei Rusu", "Loic Matthey", "Christopher Burgess", "Alexander Pritzel", "Matthew Botvinick", "Charles Blundell", "Alexander Lerchner" ],
    "emails" : [ "<irinah@google.com>,", "<arkap@google.com>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Autonomous agents can learn how to maximise future expected rewards by choosing how to act based on incoming sensory observations via reinforcement learning (RL). Early RL approaches did not scale well to environments with large state spaces and high-dimensional raw observations (Sutton & Barto, 1998). A commonly used workaround was to embed the observations in a lower-dimensional space, typically via hand-crafted and/or privileged-information features. Recently, the advent of deep learning and its successful combination with RL has enabled end-to-end learning of such embeddings directly from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016; Jaderberg et al., 2017). Despite the seemingly universal\n*Equal contribution 1DeepMind, 6 Pancras Square, Kings Cross, London, N1C 4AG, UK. Correspondence to: Irina Higgins <irinah@google.com>, Arka Pal <arkap@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nefficacy of deep RL, however, fundamental issues remain. These include data inefficiency, the reactive nature and general brittleness of learnt policies to changes in input data distribution, and lack of model interpretability (Garnelo et al., 2016; Lake et al., 2016). This paper focuses on one of these outstanding issues: the ability of RL agents to deal with changes to the input distribution, a form of transfer learning known as domain adaptation (Bengio et al., 2013). In domain adaptation scenarios, an agent trained on a particular input distribution with a specified reward structure (termed the source domain) is placed in a setting where the input distribution is modified but the reward structure remains largely intact (the target domain). We aim to develop an agent that can learn a robust policy using observations and rewards obtained exclusively within the source domain. Here, a policy is considered as robust if it generalises with minimal drop in performance to the target domain without extra fine-tuning.\nPast attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001). Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017). In many scenarios, such as robotics, this reliance on target domain information can be problematic, as the data may be expensive or difficult to obtain (Finn et al., 2017; Rusu et al., 2016). Furthermore, the target domain may simply not be known in advance. On the other hand, policies learnt exclusively on the source domain using existing deep RL approaches that have few constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor domain adaptation performance (Lake et al., 2016; Rusu et al., 2016).\nWe propose tackling both of these issues by focusing instead on learning representations which capture an underlying low-dimensional factorised representation of the world and are therefore not task or domain specific. Many nat-\nar X\niv :1\n70 7.\n08 47\n5v 1\n[ st\nat .M\nL ]\n2 6\nJu l 2\n01 7\nuralistic domains such as video game environments, simulations and our own world are well described in terms of such a structure. Examples of such factors of variation are object properties like colour, scale, or position; other examples correspond to general environmental factors, such as geometry and lighting. We think of these factors as a set of high-level parameters that can be used by a world graphics engine to generate a particular natural visual scene (Kulkarni et al., 2015). Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017). Disentangled representations are defined as interpretable, factorised latent representations where either a single latent or a group of latent units are sensitive to changes in single ground truth factors of variation used to generate the visual world, while being invariant to changes in other factors (Bengio et al., 2013). The theoretical utility of disentangled representations for supervised and reinforcement learning has been described before (Bengio et al., 2013; Higgins et al., 2017; Ridgeway, 2016); however, to our knowledge, it has not been empirically validated to date.\nWe demonstrate how disentangled representations can improve the robustness of RL algorithms in domain adaptation scenarios by introducing DARLA (DisentAngled Representation Learning Agent), a new RL agent capable of learning a robust policy on the source domain that achieves significantly better out-of-the-box performance in domain adaptation scenarios compared to various baselines. DARLA relies on learning a latent state representation that is shared between the source and target domains, by learning a disentangled representation of the environment’s generative factors. Crucially, DARLA does not require target domain data to form its representations. Our approach utilises a three stage pipeline: 1) learning to see, 2) learning to act, 3) transfer. During the first stage,\nDARLA develops its vision, learning to parse the world in terms of basic visual concepts, such as objects, positions, colours, etc. by utilising a stream of raw unlabelled observations – not unlike human babies in their first few months of life (Leat et al., 2009; Candy et al., 2009). In the second stage, the agent utilises this disentangled visual representation to learn a robust source policy. In stage three, we demonstrate that the DARLA source policy is more robust to domain shifts, leading to a significantly smaller drop in performance in the target domain even when no further policy finetuning is allowed (median 270.3% improvement). These effects hold consistently across a number of different RL environments (DeepMind Lab and Jaco/MuJoCo: Beattie et al., 2016; Todorov et al., 2012) and algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016)."
    }, {
      "heading" : "2. Framework",
      "text" : ""
    }, {
      "heading" : "2.1. Domain adaptation in Reinforcement Learning",
      "text" : "We now formalise domain adaptation scenarios in a reinforcement learning (RL) setting. We denote the source and target domains as DS and DT , respectively. Each domain corresponds to an MDP defined as a tuple DS ≡ (SS ,AS , TS , RS) or DT ≡ (ST ,AT , TT , RT ) (we assume a shared fixed discount factor γ), each with its own state space S, action space A, transition function T and reward function R.1 In domain adaptation scenarios the states S of the source and the target domains can be quite different, while the action spaces A are shared and the transitions T and reward functions R have structural similarity. For example, consider a domain adaptation scenario for the Jaco robotic arm, where the MuJoCo (Todorov et al., 2012) simulation of the arm is the source domain, and the real world setting is the target domain. The state spaces (raw pixels) of the source and the target domains differ significantly due to the perceptual-reality gap (Rusu et al., 2016); that is to say, SS 6= ST . Both domains, however, share action spaces (AS = AT ), since the policy learns to control the same set of actuators within the arm. Finally, the source and target domain transition and reward functions share structural similarity (TS ≈ TT and RS ≈ RT ), since in both domains transitions between states are governed by the physics of the world and the performance on the task depends on the relative position of the arm’s end effectors (i.e. fingertips) with respect to an object of interest."
    }, {
      "heading" : "2.2. DARLA",
      "text" : "In order to describe our proposed DARLA framework, we assume that there exists a set M of MDPs that is the set\n1For further background on the notation relating to the RL paradigm, see Section A.1 in the Supplementary Materials.\nof all natural world MDPs, and each MDP Di is sampled fromM. We defineM in terms of the state space Ŝ that contains all possible conjunctions of high-level factors of variation necessary to generate any naturalistic observation in any Di ∈ M. A natural world MDP Di is then one whose state space S corresponds to some subset of Ŝ. In simple terms, we assume that there exists some shared underlying structure between the MDPsDi sampled fromM. We contend that this is a reasonable assumption that permits inclusion of many interesting problems, including being able to characterise our own reality (Lake et al., 2016).\nWe now introduce notation for two state space variables that may in principle be used interchangeably within the source and target domain MDPs DS and DT – the agent observation state space So, and the agent’s internal latent state space Sz .2 Soi in Di consists of raw (pixel) observations soi generated by the true world simulator from a sampled set of data generative factors ŝi, i.e. soi ∼ Sim(̂si). ŝi is sampled by some distribution or process Gi on Ŝ, ŝi ∼ Gi(Ŝ).\nUsing the newly introduced notation, domain adaptation scenarios can be described as having different sampling processes GS and GT such that ŝS ∼ GS(Ŝ) and ŝT ∼ GT (Ŝ) for the source and target domains respectively, and then using these to generate different agent observation states soS ∼ Sim(̂sS) and soT ∼ Sim(̂sT). Intuitively, consider a source domain where oranges appear in blue rooms and apples appear in red rooms, and a target domain where the object/room conjunctions are reversed and oranges appear in red rooms and apples appear in blue rooms. While the true data generative factors of variation Ŝ remain the same - room colour (blue or red) and object type (apples and oranges) - the particular source and target distributions GS and GT differ.\nTypically deep RL agents (e.g. Mnih et al., 2015; 2016) operating in an MDP Di ∈ M learn an end-to-end mapping from raw (pixel) observations soi ∈ Soi to actions ai ∈ Ai (either directly or via a value function Qi(soi , ai) from which actions can be derived). In the process of doing so, the agent implicitly learns a function F : Soi → Szi that maps the typically high-dimensional raw observations soi to typically low-dimensional latent states s z i ; followed by a policy function πi : Szi → Ai that maps the latent states szi to actions ai ∈ Ai. In the context of domain adaptation, if the agent learns a naive latent state mapping function FS : SoS → SzS on the source domain using reward signals to shape the representation learning, it is likely that FS will overfit to the source domain and will not generalise well to the target domain. Returning to our\n2Note that we do not assume these to be Markovian i.e. it is not necessarily the case that p(sot+1|sot ) = p(sot+1|sot , sot−1, . . . , so1), and similarly for sz . Note the index t here corresponds to time.\nintuitive example, imagine an agent that has learnt a policy to pick up oranges and avoid apples on the source domain. Such a source policy πS is likely to be based on an entangled latent state space SzS of object/room conjunctions: oranges/blue → good, apples/red → bad, since this is arguably the most efficient representation for maximising expected rewards on the source task in the absence of extra supervision signals suggesting otherwise. A source policy πS(a|szS ; θ) based on such an entangled latent representation szS will not generalise well to the target domain without further fine-tuning, since FS(soS) 6= FS(soT ) and therefore crucially SzS 6= SzT .\nOn the other hand, since both ŝS ∼ GS(Ŝ) and ŝT ∼ GT (Ŝ) are sampled from the same natural world state space Ŝ for the source and target domains respectively, it should be possible to learn a latent state mapping function F̂ : So → SzŜ , which projects the agent observation state space So to a latent state space SzŜ expressed in terms of factorised data generative factors that are representative of the natural world i.e. Sz\nŜ ≈ Ŝ. Consider again our intuitive\nexample, where F̂ maps agent observations (soS : orange in a blue room) to a factorised or disentangled representation expressed in terms of the data generative factors (szŜ : room type = blue; object type = orange). Such a disentangled latent state mapping function should then directly generalise to both the source and the target domains, so that F̂(soS) = F̂(soT ) = szŜ . Since S z Ŝ is a disentangled representation of object and room attributes, the source policy πS can learn a decision boundary that ignores the irrelevant room attributes: oranges→ good, apples→ bad. Such a policy would then generalise well to the target domain out of the box, since πS(a|F̂(soS); θ) = πT (a|F̂(soT ); θ) = πT (a|szŜ ; θ). Hence, DARLA is based on the idea that a good quality F̂ learnt exclusively on the source domain DS ∈ M will zero-shot-generalise to all target domains Di ∈ M, and therefore the source policy π(a|SzŜ ; θ) will also generalise to all target domains Di ∈ M out of the box.\nNext we describe each of the stages of the DARLA pipeline that allow it to learn source policies πS that are robust to domain adaptation scenarios, despite being trained with no knowledge of the target domains (see Fig. 1 for a graphical representation of these steps):\n1) Learn to see (unsupervised learning of FU ) – the task of inferring a factorised set of generative factors SzŜ = Ŝ from observations So is the goal of the extensive disentangled factor learning literature (e.g. Chen et al., 2016; Higgins et al., 2017). Hence, in stage one we learn a mapping FU : SoU → SzU , where SzU ≈ SzŜ (U stands for ‘unsupervised’) using an unsupervised model for learning disentangled factors that utilises observations collected by an agent with a random policy πU from a visual pre-training\nMDP DU ∈ M. Note that we require sufficient variability of factors and their conjunctions in DU in order to have SzU ≈ SzŜ ;\n2) Learn to act (reinforcement learning of πS in the source domain DS utilising previously learned FU ) – an agent that has learnt to see the world in stage one in terms of the natural data generative factors is now exposed to a source domain DS ∈ M. The agent is tasked with learning the source policy πS(a|szS ; θ), where szS = FU (soS) ≈ szŜ , via a standard reinforcement learning algorithm. Crucially, we do not allow FU to be modified (e.g. by gradient updates) during this phase;\n3) Transfer (to a target domain DT ) – in the final step, we test how well the policy πS learnt on the source domain generalises to the target domain DT ∈ M in a zero-shot domain adaptation setting, i.e. the agent is evaluated on the target domain without retraining. We compare the performance of policies learnt with a disentangled latent state SzŜ to various baselines where the latent state mapping function FU projects agent observations so to entangled latent state representations sz ."
    }, {
      "heading" : "2.3. Learning disentangled representations",
      "text" : "In order to learn FU , DARLA utilises β-VAE (Higgins et al., 2017), a state-of-the-art unsupervised model for automated discovery of factorised latent representations from raw image data. β-VAE is a modification of the variational autoencoder framework (Kingma & Welling, 2014; Rezende et al., 2014) that controls the nature of the learnt latent representations by introducing an adjustable hyperparameter β to balance reconstruction accuracy with latent channel capacity and independence constraints. It maximises the objective:\nL(θ, φ;x, z, β) =Eqφ(z|x)[log pθ(x|z)] − β DKL(qφ(z|x)||p(z)) (1)\nwhere φ, θ parametrise the distributions of the encoder and the decoder respectively. Well-chosen values of β - usually larger than one (β > 1) - typically result in more disentangled latent representations z by limiting the capacity of the latent information channel, and hence encouraging a more efficient factorised encoding through the increased pressure to match the isotropic unit Gaussian prior p(z) (Higgins et al., 2017)."
    }, {
      "heading" : "2.3.1. PERCEPTUAL SIMILARITY LOSS",
      "text" : "The cost of increasing β is that crucial information about the scene may be discarded in the latent representation z, particularly if that information takes up a small proportion of the observations x in pixel space. We encountered this issue in some of our tasks, as discussed in Section 3.1. The shortcomings of calculating the log-likelihood term\nEqφ(z|x)[log pθ(x|z)] on a per-pixel basis are known and have been addressed in the past by calculating the reconstruction cost in an abstract, high-level feature space given by another neural network model, such as a GAN (Goodfellow et al., 2014) or a pre-trained AlexNet (Krizhevsky et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox, 2016; Warde-Farley & Bengio, 2017). In practice we found that pre-training a denoising autoencoder (Vincent et al., 2010) on data from the visual pre-training MDP DU ∈ M worked best as the reconstruction targets for β-VAE to match (see Fig. 1 for model architecture and Sec. A.3.1 in Supplementary Materials for implementation details). The new β-VAEDAE model was trained according to Eq. 2:\nL(θ, φ;x, z, β) =Eqφ(z|x) ‖J(x̂)− J(x)‖ 2 2\n− β DKL(qφ(z|x)||p(z)) (2)\nwhere x̂ ∼ pθ(x|z) and J : RW×H×C → RN is the function that maps images from pixel space with dimensionality W ×H × C to a high-level feature space with dimensionality N given by a stack of pre-trained DAE layers up to a certain layer depth. Note that by replacing the pixel based reconstruction loss in Eq. 1 with high-level feature reconstruction loss in Eq. 2 we are no longer optimising the variational lower bound, and β-VAEDAE with β = 1 loses its equivalence to the Variational Autoencoder (VAE) framework as proposed by (Kingma & Welling, 2014; Rezende et al., 2014). In this setting, the only way to interpret β is as a mixing coefficient that balances the capacity of the latent channel z of β-VAEDAE against the pressure to match the high-level features within the pre-trained DAE."
    }, {
      "heading" : "2.4. Reinforcement Learning Algorithms",
      "text" : "We used various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn the source policy πS during stage two of the pipeline using the latent states sz acquired by β-VAE based models during stage one of the DARLA pipeline.\nDeep Q Network (DQN) (Mnih et al., 2015) is a variant of the Q-learning algorithm (Watkins, 1989) that utilises deep learning. It uses a neural network to parametrise an approximation for the action-value function Q(s, a; θ) using parameters θ.\nAsynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) is an asynchronous implementation of the advantage actor-critic paradigm (Sutton & Barto, 1998; Degris & Sutton, 2012), where separate threads run in parallel and perform updates to shared parameters. The different threads each hold their own instance of the environment and have different exploration policies, thereby decorrelating parameter updates without the need for experience replay. Therefore, A3C is an online algorithm, whereas DQN learns its policy offline, resulting in different learning dynamics be-\ntween the two algorithms.\nModel-Free Episodic Control (EC) (Blundell et al., 2016) was proposed as a complementary learning system to the other RL algorithms described above. The EC algorithm relies on near-determinism of state transitions and rewards in RL environments; in settings where this holds, it can exploit these properties to memorise which action led to high returns in similar situations in the past. Since in its simplest form EC relies on a lookup table, it learns good policies much faster than value-function-approximation based deep RL algorithms like DQN trained via gradient descent - at the cost of generality (i.e. potentially poor performance in non-deterministic environments).\nWe also compared our approach to that of UNREAL (Jaderberg et al., 2017), a recently proposed RL algorithm which also attempts to utilise unsupervised data in the environment. The UNREAL agent takes as a base an LSTM A3C agent (Mnih et al., 2016) and augments it with a number of unsupervised auxiliary tasks that make use of the rich perceptual data available to the agent besides the (sometimes very sparse) extrinsic reward signals. This auxiliary learning tends to improve the representation learnt by the agent. See Sec. A.6 in Supplementary Materials for further details of the algorithms above."
    }, {
      "heading" : "3. Tasks",
      "text" : "We evaluate the performance of DARLA on different task and environment setups that probe subtly different aspects of domain adaptation. As a reminder, in Sec. 2.2 we defined Ŝ as a state space that contains all possible conjunctions of high-level factors of variation necessary to generate any naturalistic observation in any Di ∈ M. During domain adaptation scenarios agent observation states are generated according to soS ∼ SimS(̂sS) and soT ∼ SimT(̂sT) for the source and target domains respectively, where ŝS and ŝT are sampled by some distributions or processes GS and GT according to ŝS ∼ GS(Ŝ) and ŝT ∼ GT (Ŝ).\nWe use DeepMind Lab (Beattie et al., 2016) to test a version of domain adaptation setup where the source and target domain observation simulators are equal (SimS = SimT), but the processes used to sample ŝS and ŝT are different (GS 6= GT ). We use the Jaco arm with a matching MuJoCo simulation environment (Todorov et al., 2012) in two domain adaptation scenarios: simulation to simulation (sim2sim) and simulation to reality (sim2real). The sim2sim domain adaptation setup is relatively similar to DeepMind Lab i.e. the source and target domains differ in terms of processes GS and GT . However, there is a significant point of difference. In DeepMind Lab, all values of factors in the target domain, ŝT , are previously seen in the source domain; however, ŝS 6= ŝT as the conjunctions of\nthese factor values are different. In sim2sim, by contrast, novel factor values are experienced in the target domain (this accordingly also leads to novel factor conjunctions). Hence, DeepMind Lab may be considered to be assessing domain interpolation performance, whereas sim2sim tests domain extrapolation.\nThe sim2real setup, on the other hand, is based on identical processes GS = GT , but different observation simulators SimS 6= SimT corresponding to the MuJoCo simulation and the real world, which results in the so-called ‘perceptual reality gap’ (Rusu et al., 2016). More details of the tasks are given below."
    }, {
      "heading" : "3.1. DeepMind Lab",
      "text" : "DeepMind Lab is a first person 3D game environment with rich visuals and realistic physics. We used a standard seekavoid object gathering setup, where a room is initialised with an equal number of randomly placed objects of two different types. One of the object varieties is ‘good’ (its collection is rewarded +1), while the other is ‘bad’ (its collection is punished -1). The full state space Ŝ consisted of all conjunctions of two room types (pink and green based on the colour of the walls) and four object types (hat, can, cake and balloon) (see Fig. 2A). The source domain DS con-\ntained environments with hats/cans presented in the green room, and balloons/cakes presented in either the green or the pink room. The target domain DT contained hats/cans presented in the pink room. In both domains cans and balloons were the rewarded objects.\n1) Learn to see: we used β-VAEDAE to learn the disentangled latent state representation sz that includes both the room and the object generative factors of variation within DeepMind Lab. We had to use the high-level feature space of a pre-trained DAE within the β-VAEDAE framework (see Section 2.3.1), instead of the pixel space of vanilla βVAE , because we found that objects failed to reconstruct when using the values of β necessary to disentangle the generative factors of variation within DeepMind Lab (see Fig. 2B).\nβ-VAEDAE was trained on observations soU collected by an RL agent with a simple wall-avoiding policy πU (otherwise the training data was dominated by close up images of walls). In order to enable the model to learn F(soU ) ≈ Ŝ, it is important to expose the agent to at least a minimal set of environments that span the range of values for each factor, and where no extraneous correlations are added between different factors3(see Fig. 2A, yellow). See Section A.3.1 in Supplementary Materials for details of β-VAEDAE training.\n2) Learn to act: the agent was trained with the algorithms detailed in Section 2.4 on a seek-avoid task using the source domain (DS) conjunctions of object/room shown in Fig. 2A (green). Pre-trained β-VAEDAE from stage one was used as the ‘vision’ part of various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn a source policy πS that picks up balloons and avoids cakes in both the green and the pink rooms, and picks up cans and avoids hats in the green rooms. See Section A.3.1 in Supplementary Materials for more details of the various versions of DARLA we have tried, each based on a different base RL algorithm.\n3) Transfer: we tested the ability of DARLA to transfer the seek-avoid policy πS it had learnt on the source domain in stage two using the domain adaptation condition DT illustrated in Figure 2A (red). The agent had to continue picking up cans and avoid hats in the pink room, even though these objects had only been seen in the green room during source policy training. The optimal policy πT is one that maintains the reward polarity from the source domain (cans are good and hats are bad). For further details, see Appendix A.2.1.\n3In our setup of DeepMind Lab domain adaptation task, the object and environment factors are supposed to be independent. In order to ensure that β-VAEDAE learns a factorised representation that reflects this ground truth independence, we present observations of all possible conjunctions of room and individual object types."
    }, {
      "heading" : "3.2. Jaco Arm and MuJoCo",
      "text" : "We used frames from an RGB camera facing a robotic Jaco arm, or a matching rendered camera view from a MuJoCo physics simulation environment (Todorov et al., 2012) to investigate the performance of DARLA in two domain adaptation scenarios: 1) simulation to simulation (sim2sim), and 2) simulation to reality (sim2real). The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016). Solving control problems in reality is hard due to sparse reward signals, expensive data acquisition and the attendant danger of breaking the robot (or its human minders) during exploration.\nIn both sim2sim and sim2real, we trained the agent to perform an object reaching policy where the goal is to place the end effector as close to the object as possible. While conceptually the reaching task is simple, it is a hard control problem since it requires correct inference of the arm and object positions and velocities from raw visual inputs.\n1) Learn to see: β-VAE was trained on observations collected in MuJoCo simulations with the same factors of variation as in DS . In order to enable the model to learn F(soU ) ≈ ŝ, a reaching policy was applied to phantom objects placed in random positions - therefore ensuring that the agent learnt the independent nature of the arm position and object position (see Fig. 2C, left);\n2) Learn to act: a feedforward-A3C based agent with the vision module pre-trained in stage one was taught a source reaching policy πS towards the real object in simulation (see Fig. 2C (left) for an example frame, and Sec. A.4 in Supplementary Materials for a fuller description of the agent). In the source domain DS the agent was trained on a distribution of camera angles and positions. The colour of the tabletop on which the arm rests and the object colour were both sampled anew every episode.\n3) Transfer: sim2sim: in the target domain, DT , the agent was faced with a new distribution of camera angles and positions with little overlap with the source domain distributions, as well as a completely held out set of object colours (see Fig. 2C, middle). sim2real: in the target domain DT the camera position and angle as well as the tabletop colour and object colour were sampled from the same distributions as seen in the source domain DS , but the target domain DT was now the real world. Many details present in the real world such as shadows, specularity, multiple light sources and so on are not modelled in the simulation;"
    }, {
      "heading" : "Disentangled Entangled",
      "text" : "the physics engine is also not a perfect model of reality. Thus sim2real tests the ability of the agent to cross the perceptual-reality gap and generalise its source policy πS to the real world (see Fig. 2C, right). For further details, see Appendix A.2.2."
    }, {
      "heading" : "4. Results",
      "text" : "We evaluated the robustness of DARLA’s policy πS learnt on the source domain to various shifts in the input data distribution. In particular, we used domain adaptation scenarios based on the DeepMind Lab seek-avoid task and the Jaco arm reaching task described in Sec. 3. On each task we compared DARLA’s performance to that of various baselines. We evaluated the importance of learning ‘good’ vision during stage one of the pipeline, i.e one that maps the input observations so to disentangled representations sz ≈ ŝ. In order to do this, we ran the DARLA pipeline with different vision models: the encoders of a\ndisentangled β-VAE 4 (the original DARLA), an entangled β-VAE (DARLAENT), and a denoising autoencoder (DARLADAE). Apart from the nature of the learnt representations sz , DARLA and all versions of its baselines were equivalent throughout the three stages of our proposed pipeline in terms of architecture and the observed data distribution (see Sec. A.3 in Supplementary Materials for more details).\nFigs. 3-4 display the degree of disentanglement learnt by the vision modules of DARLA and DARLAENT on DeepMind Lab and MuJoCo. DARLA’s vision learnt to independently represent environment variables (such as room colour-scheme and geometry) and object-related variables (change of object type, size, rotation) on DeepMind Lab (Fig. 3, left). Disentangling was also evident in MuJoCo. Fig. 4, left, shows that DARLA’s single latent units zi learnt to represent different aspects of the Jaco arm, the object, and the camera. By contrast, in the representations learnt by DARLAENT, each latent is responsible for changes to both the environment and objects (Fig. 3, right) in DeepMind Lab, or a mixture of camera, object and/or arm movements (Fig. 4, right) in MuJoCo.\nThe table in Fig. 5 shows the average performance (across different seeds) in terms of rewards per episode of the various agents on the target domain with no fine-tuning of the source policy πS . It can be seen that DARLA is able to zero-shot-generalise significantly better than DARLAENT or DARLADAE, highlighting the importance of learning a disentangled representation sz = szŜ during the unsupervised stage one of the DARLA pipeline. In particular, this also demonstrates that the improved domain transfer performance is not simply a function of increased exposure to training observations, as both DARLAENT and DARLADAE were exposed to the same data. The results are mostly consistent across target domains and in most cases DARLA is significantly better than the second-best-performing agent. This holds in the sim2real task 5, where being able to perform zero-shot policy transfer is highly valuable due to the particular difficulties of gathering data in the real world.\nDARLA’s performance is particularly surprising as it actually preserves less information about the raw observations so than DARLAENT and DARLADAE. This is due to the nature of the β-VAE and how it achieves disentangling; the disentangled model utilised a significantly higher value of the hyperparameter β than the entangled model (see Appendix A.3 for further details), which constrains the ca-\n4In this section of the paper, we use the term β-VAE to refer to a standard β-VAE for the MuJoCo experiments, and a β-VAEDAE for the DeepMind Lab experiments (as described in stage 1 of Sec. 3.1).\n5See https://youtu.be/sZqrWFl0wQ4 for example sim2sim and sim2real zero-shot transfer policies of DARLA and baseline A3C agent.\n110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164\n165 166 167 168 169 170 171 172 173 174 175 176 177 178\n179\n180\n181\n182\n183\n184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\nDARLA: Improving Zero-Shot Transfer in Reinforcement Learning\nTable 1. Transfer performance\nDEEPMIND LAB JACO (A3C) VISION TYPE DQN A3C EC SIM2SIM SIM2REAL\nBASELINE AGENT 1.86 ± 3.91 5.32 ± 3.36 -0.41 ± 4.21 97.64 ± 9.02 94.56 ± 3.55 UNREAL - 4.13 ± 3.95 - - - DARLAFT 13.36 ± 5.8 1.4 ± 2.16 - 86.59 ± 5.53 99.25 ± 2.3 DARLAENT 3.45 ± 4.47 15.66 ± 5.19 5.69 ± 3.73 84.77 ± 4.42 59.99 ± 15.05 DARLADAE 7.83 ± 4.47 6.74 ± 2.81 5.59 ± 3.37 85.15 ± 7.43 100.72 ± 4.7\nDARLA 10.25 ± 5.46 19.7 ± 5.43 11.41 ± 3.52 100.85 ± 2.92 108.2 ± 5.97 DARLA’S PERFORMANCE IS SIGNIFICANTLY DIFFERENT FROM ALL BASELINES UNDER WELCH’S UNEQUAL VARIANCES T-TEST WITH p < 0.01 (N ∈ [60, 150]).\nFigure 5. Table: Zero-shot performance (avg. reward per episode) of the source policy πS in target domains within DeepMind Lab and Jaco/MuJoCo environments. Baseline agent refers to vanilla DQN/A3C/EC (DeepMind Lab) or A3C (Jaco) agents. See main text for more detailed model descriptions. Figure: Correlation between zero-shot performance transfer performance on the DeepMind Lab task obtained by EC based DARLA and the level of disentanglement as measured by the transfer/disentanglement score (r = 0.6, p < 0.001)\npacity of the latent channel. Indeed, DARLA’s β-VAE only utilises 8 of its possible 32 Gaussian latents to store observation-specific information for MuJoCo/Jaco (and 20 in DeepMind Lab), whereas DARLAENT utilises all 32 for both environments (as does DARLADAE).\nFurthermore, we examined what happens if DARLA’s vision (i.e. the encoder of the disentangled β-VAE ) is allowed to be fine-tuned via gradient updates while learning the source policy during stage two of the pipeline. This is denoted by DARLAFT in the table in Fig. 5. We see that it exhibits significantly worse performance than that of DARLA in zero-shot domain adaptation using an A3Cbased agent in all tasks. This suggests that a favourable initialisation does not make up for subsequent overfitting to the source domain for the on-policy A3C. However, the off-policy DQN-based fine-tuned agent performs very well. We leave further investigation of this curious effect for future work.\nFinally, we compared the performance of DARLA to an UNREAL (Jaderberg et al., 2017) agent with the same architecture. Despite also exploiting the unsupervised data available in the source domain, UNREAL performed worse than baseline A3C on the DeepMind Lab domain adaptation task. This further demonstrates that use of unsupervised data in itself is not a panacea for transfer performance; it must be utilised in a careful and structured manner conducive to learning disentangled latent states sz = szŜ .\nIn order to quantitatively evaluate our hypothesis that disentangled representations are essential for DARLA’s performance in domain adaptation scenarios, we trained various DARLAs with different degrees of learnt disentanglement in sz by varying β (of β-VAE) during stage one of the pipeline. We then calculated the correlation between the performance of the EC-based DARLA on the DeepMind Lab domain adaptation task and the transfer metric, which approximately measures the quality of disentanglement of DARLA’s latent representations sz (see Sec. A.5.2\nin Supplementary Materials). This is shown in the chart in Fig. 5; as can be seen, there is a strong positive correlation between the level of disentanglement and DARLA’s zeroshot domain transfer performance (r = 0.6, p < 0.001).\nHaving shown the robust utility of disentangled representations in agents for domain adaptation, we note that there is evidence that they can provide an important additional benefit. We found significantly improved speed of learning of πS on the source domain itself, as a function of how disentangled the model was. The gain in data efficiency from disentangled representations for source policy learning is not the main focus of this paper so we leave it out of the main text; however, we provide results and discussion in Section A.7 in Supplementary Materials."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We have demonstrated the benefits of using disentangled representations in a deep RL setting for domain adaptation. In particular, we have proposed DARLA, a multi-stage RL agent. DARLA first learns a visual system that encodes the observations it receives from the environment as disentangled representations, in a completely unsupervised manner. It then uses these representations to learn a robust source policy that is capable of zero-shot domain adaptation.\nWe have demonstrated the efficacy of this approach in a range of domains and task setups: a 3D naturalistic firstperson environment (DeepMind Lab), a simulated graphics and physics engine (MuJoCo), and crossing the simulation to reality gap (MuJoCo to Jaco sim2real). We have also shown that the effect of disentangling is consistent across very different RL algorithms (DQN, A3C, EC), achieving significant improvements over the baseline algorithms (median 2.7 times improvement in zero-shot transfer across tasks and algorithms). To the best of our knowledge, this is the first comprehensive empirical demonstration of the strength of disentangled representations for domain adaptation in a deep RL setting."
    }, {
      "heading" : "A. Supplementary Materials",
      "text" : ""
    }, {
      "heading" : "A.1. The Reinforcement Learning Paradigm",
      "text" : "The reinforcement learning (RL) paradigm consists of an agent receiving a sequence of observations sot which are some function of environment states st ∈ S and may be accompanied by rewards rt+1 ∈ R conditional on the actions at ∈ A, chosen at each time step t (Sutton & Barto, 1998). We assume that these interactions can be modelled as a Markov Decision Process (MDP) (Puterman, 1994) defined as a tuple D ≡ (S,A, T , R, γ). T = p(s|st, at) is a transition function that models the distribution of all possible next states given action at is taken in state st for all st ∈ S and at ∈ A. Each transition st at→ st+1 may be accompanied by a reward signal rt+1(st, at, st+1). The goal of the agent is to learn a policy π(at|st), a probability distribution over actions at ∈ A, that maximises the expected return i.e. the discounted sum of future rewards Rt = E[ ∑T−t τ=1 γ\nτ−1rt+τ ]. T is the time step at which each episode ends, and γ ∈ [0, 1) is the discount factor that progressively down-weights future rewards. Given a policy π(a|s), one can define the value function Vπ(s) = E[Rt|st = s, π], which is the expected return from state s following policy π. The action-value function Qπ(s, a) = E[Rt|st = s, at = a, π] is the expected return for taking action a in state s at time t, and then following policy π from time t+ 1 onward.\nA.2. Further task details"
    }, {
      "heading" : "A.2.1. DEEPMIND LAB",
      "text" : "As described in Sec 3.1, in each source episode of DeepMind Lab the agent was presented with one of three possible room/object type conjunctions, chosen at random. These are marked DS in Fig 2. The setup was a seek-avoid style task, where one of the two object types in the room gave a reward of +1 and the other gave a reward of -1. The agent was allowed to pick up objects for 60 seconds after which the episode would terminate and a new one would begin; if the agent was able to pick up all the ‘good’ objects in less than 60 seconds, a new episode was begun immediately. The agent was spawned in a random location in the room at the start of each new episode.\nDuring transfer, the agent was placed into the held out conjunction of object types and room background; see DT in Fig 2.\nVisual pre-training was performed in other conjunctions of object type and room background denoted DU in Fig 2.\nThe observation size of frames in the DeepMind Lab task was 84x84x3 (HxWxC)."
    }, {
      "heading" : "A.2.2. MUJOCO/JACO ARM EXPERIMENTS",
      "text" : "As described in Sec 3.2, the source task consisted of an agent learning to control a simulated arm in order to reach toward an object. A shaping reward was used, with a maximum value of 1 when the centre of the object fell between the pinch and grip sites of the end effector, or within a 10cm distance of the two. Distances on the x and y dimensions counted double compared to distances on the z dimension.\nDuring each episode the object was placed at a random drop point within a 40x40cm area, and the arm was set to a random initial start position high above the work-space, independent of the object’s position. Each episode lasted for 150 steps, or 7.5 seconds, with a control step of 50ms. Observations soU were sampled\nrandomly across episodes. Overall, 4 million frames of dimensions 64x64x3 (HxWxC) were used for this stage of the curriculum. For each episode the camera position and orientation were randomly sampled from an isotropic normal distribution centred around the approximate position and orientation of the real camera, with standard deviation 0.01. No precise measurements were used to match the two. Work-space table colour was sampled uniformly between −5% and +5% around the midpoint, independently for each RGB channel; object colours were sampled uniformly at random in RGB space, rejecting colours which fell within a ball around 10 held-out intensities (radius 10% of range); the latter were only used for simulated transfer experiments, i.e. in DT in the sim2sim experiments. Additionally, Gaussian noise with standard deviation 0.01 was added to the observations soT in the sim2sim task.\nFor the real Jaco arm and its MuJoCo simulation counterpart, each of the nine joints could independently take 11 different actions (a linear discretisation of the continuous velocity action space). In simulation Gaussian noise with standard deviation 0.1 was added to each discrete velocity output; delays in the real setup between observations and action execution were simulated by randomly mixing velocity outputs from two previous steps instead of emitting the last output directly. Speed ranges were between −50% and 50% of the Jaco arm’s top speed on joints 1 through 6 starting at the base, while the fingers could use a full range. For safety reasons the speed ranges have been reduced by a factor of 0.3 while evaluating agents on the Jaco arm, without significant performance degradation.\nA.3. Vision model details\nA.3.1. DENOISING AUTOENCODER FOR β-VAE\nA denoising autoencoder (DAE) was used as a model to provide the feature space for the β-VAE reconstruction loss to be computed over (for motivation, see Sec. 2.3.1). The DAE was trained with occlusion-style masking noise in the vein of (Pathak et al., 2016), with the aim for the DAE to learn a semantic representation of the input frames. Concretely, two values were independently sampled from U [0,W ] and two from U [0, H] where W and H were the width and height of the input frames. These four values determined the corners of the rectangular mask applied; all pixels that fell within the mask were set to zero.\nThe DAE architecture consisted of four convolutional layers, each with kernel size 4 and stride 2 in both the height and width dimensions. The number of filters learnt for each layer was {32, 32, 64, 64} respectively. The bottleneck layer consisted of a fully connected layer of size 100 neurons. This was followed by four deconvolutional layers, again with kernel sizes 4, strides 2, and {64, 64, 32, 32} filters. The padding algorithm used was ‘SAME’ in TensorFlow (Abadi et al., 2015). ReLU non-linearities were used throughout.\nThe model was trained with loss given by the L2 distance of the outputs from the original, un-noised inputs. The optimiser used was Adam (Kingma & Ba, 2014) with a learning rate of 1e-3.\nA.3.2. β-VAE WITH PERCEPTUAL SIMILARITY LOSS\nAfter training a DAE, as detailed in the previous section6, a β-VAEDAE was trained with perceptual similarity loss given by\n6In principle, the β-VAEDAE could also have been trained end-to-end in one pass, but we did not experiment with this.\nEq. 2, repeated here:\nL(θ, φ;x, z, β) =Eqφ(z|x) ‖J(x̂)− J(x)‖ 2 2\n− β DKL(qφ(z|x)||p(z)) (3)\nSpecifically, the input was passed through the β-VAE and a sampled7 reconstruction was passed through the pre-trained DAE up to a designated layer. The L2 distance of this representation from the representation of the original input passed through the same layers of the DAE was then computed, and this formed the training loss for the β-VAE part of the β-VAEDAE 8. The DAE weights remained frozen throughout.\nThe β-VAE architecture consisted of an encoder of four convolutional layers, each with kernel size 4, and stride 2 in the height and width dimensions. The number of filters learnt for each layer was {32, 32, 64, 64} respectively. This was followed by a fully connected layer of size 256 neurons. The latent layer comprised 64 neurons parametrising 32 (marginally) independent Gaussian distributions. The decoder architecture was simply the reverse of the encoder, utilising deconvolutional layers. The decoder used was Gaussian, so that the number of output channels was 2C, where C was the number of channels that the input frames had. The padding algorithm used was ‘SAME’ in TensorFlow. ReLU non-linearities were used throughout.\nThe model was trained with the loss given by Eq. 3. Specifically, the disentangled model used for DARLA was trained with a β hyperparameter value of 1 and the layer of the DAE used to compute the perceptual similarity loss was the last deconvolutional layer. The entangled model used for DARLAENT was trained with a β hyperparameter value of 0.1 with the last deconvolutional layer of the DAE was used to compute the perceptual similarity loss.\nThe optimiser used was Adam with a learning rate of 1e-4.\nA.3.3. β-VAE\nFor the MuJoCo/Jaco tasks, a standard β-VAE was used rather than the β-VAEDAE used for DeepMind Lab. The architecture of the VAE encoder, decoder and the latent size were exactly as described in the previous section A.3.2. β for the the disentangled βVAE in DARLA was 175. β for the entangled model DARLAENT was 1, corresponding to the standard VAE of (Kingma & Welling, 2014).\nThe optimizer used was Adam with a learning rate of 1e-4."
    }, {
      "heading" : "A.3.4. DENOISING AUTOENCODER FOR BASELINE",
      "text" : "For the baseline model DARLADAE, we trained a denoising autoencoder with occlusion-style masking noise as described in Appendix Section A.3.1. The architecture used matched that exactly of the β-VAE described in Appendix Section A.3.2 - however, all stochastic nodes were replaced with deterministic neurons.\nThe optimizer used was Adam with a learning rate of 1e-4.\n7It is more typical to use the mean of the reconstruction distribution, but this does not induce any pressure on the Gaussians parametrising the decoder to reduce their variances. Hence full samples were used instead.\n8The representations were taken after passing through the layer but before passing through the following non-linearity. We also briefly experimented with taking the L2 loss post-activation but did not find a significant difference."
    }, {
      "heading" : "A.4. Reinforcement Learning Algorithm Details",
      "text" : ""
    }, {
      "heading" : "A.4.1. DEEPMIND LAB",
      "text" : "The action space in the DeepMind Lab task consisted of 8 discrete actions.\nDQN: in DQN, the convolutional (or ‘vision’) part of the Q-net was replaced with the encoder of the β-VAEDAE from stage 1 and frozen. DQN takes four consecutive frames as input in order to capture some aspect of environment dynamics in the agent’s state. In order to match this in our setup with a pre-trained vision stack FU , we passed each observation frame so{1..4} through the pre-trained model sz{1..4} = FU (so{1..4}) and then concatenated the outputs together to form the k-dimensional (where k = 4|sz|) input to the policy network. In this case the size of sz was 64 for DARLA as well as DARLAENT, DARLADAE and DARLAFT.\nOn top of the frozen convolutional stack, two ‘policy’ layers of 512 neurons each were used, with a final linear layer of 8 neurons corresponding to the size of the action space in the DeepMind Lab task. ReLU non-linearities were used throughout. All other hyperparameters were as reported in (Mnih et al., 2015).\nA3C: in A3C, as with DQN, the convolutional part of the network that is shared between the policy net and the value net was replaced with the encoder of the β-VAEDAE in DeepMind Lab tasks. All other hyperparameters were as reported in (Mnih et al., 2016).\nEpisodic Control: for the Episodic Controller-based DARLA we used mostly the same hyperparameters as in the original paper by (Blundell et al., 2016). We explored the following hyperparameter settings: number of nearest neighbours ∈ {10, 50}, return horizon ∈ {100, 400, 800, 1800, 500000}, kernel type ∈ {inverse, gaussian}, kernel width ∈ {1e− 6, 1e− 5, 1e− 4, 1e− 3, 1e− 2, 1e − 1, 0.5, 0.99} and we tried training EC with and without Peng’s Q(λ) (Peng, 1993). In practice we found that none of the explored hyperparameter choices significantly influenced the results of our experiments. The final hyperparameters used for all experiments reported in the paper were the following: number of nearest neighbours: 10, return horizon: 400, kernel type: inverse, kernel width: 1e-6 and no Peng’s Q(λ) (Peng, 1993).\nUNREAL: We used a vanilla version of UNREAL, with parameters as reported in (Jaderberg et al., 2017)."
    }, {
      "heading" : "A.4.2. MUJOCO/JACO ARM EXPERIMENTS",
      "text" : "For the real Jaco arm and its MuJoCo simulation, each of the nine joints could independently take 11 different actions (a linear discretisation of the continuous velocity action space). Therefore the action space size was 99.\nDARLA for MuJoCo/Jaco was based on feedforward A3C (Mnih et al., 2016). We closely followed the simulation training setup of (Rusu et al., 2016) for feed-forward networks using raw visualinput only. In place of the usual conv-stack, however, we used the encoder of the β-VAE as described in Appendix A.3.3. This was followed by a linear layer with 512 units, a ReLU non-linearity and a collection of 9 linear and softmax layers for the 9 independent policy outputs, as well as a single value output layer that outputted the value function."
    }, {
      "heading" : "A.5. Disentanglement Evaluation",
      "text" : "A.5.1. VISUAL HEURISTIC DETAILS\nIn order to choose the optimal value of β for the β-VAE -DAE models and evaluate the fitness of the representations szU learnt in stage 1 of our pipeline (in terms of disentanglement achieved), we used the visual inspection heuristic described in (Higgins et al., 2017). The heuristic involved clustering trained β-VAE based models based on the number of informative latents (estimated as the number of latents zi with average inferred standard deviation below 0.75). For each cluster we examined the degree of learnt disentanglement by running inference on a number of seed images, then traversing each latent unit z{i} one at a time over three standard deviations away from its average inferred mean while keeping all other latents z{\\i} fixed to their inferred values. This allowed us to visually examine whether each individual latent unit zi learnt to control a single interpretable factor of variation in the data. A similar heuristic has been the de rigueur method for exhibiting disentanglement in the disentanglement literature (Chen et al., 2016; Kulkarni et al., 2015)."
    }, {
      "heading" : "A.5.2. TRANSFER METRIC DETAILS",
      "text" : "In the case of DeepMind Lab, we were able to use the ground truth labels corresponding to the two factors of variation of the object type and the background to design a proxy to the disentanglement metric proposed in (Higgins et al., 2017). The procedure used\nconsisted of the following steps:\n1) Train the model under consideration on observations soU to learn FU , as described in stage 1 of the DARLA pipeline. 2) Learn a linear model L : SzV → M × N from the representations szV = FV (soV ), where M ∈ {0, 1} corresponds to the set of possible rooms and N ∈ {0, 1, 2, 3} corresponds to the set of possible objects9. Therefore we are learning a low-VC dimension classifier to predict the room and the object class from the latent representation of the model. Crucially, the linear model L is trained on only a subset of the Cartesian productM×N e.g. on {{0, 0}, {0, 3}, {1, 1}, {1, 2}}. In practice, we utilised a softmax classifier each for M and N and trained this using backpropagation with a cross-entropy loss, keeping the unsupervised model (and therefore FU ) fixed. 3) The trained linear model L’s accuracy is evaluated on the held out subset of the Cartesian product M ×N . Although the above procedure only measures disentangling up to linearity, and only does so for the latents of object type and room background, we nevertheless found that the metric was highly correlated with disentanglement as determined via visual inspection (see Fig. 6)."
    }, {
      "heading" : "A.6. Background on RL Algorithms",
      "text" : "In this Appendix, we provide background on the different RL algorithms that the DARLA framework was tested on in this paper.\nA.6.1. DQN\n(DQN) (Mnih et al., 2015) is a variant of the Q-learning algorithm (Watkins, 1989) that utilises deep learning. It uses a neural network to parametrise an approximation for the action-value function Q(s, a; θ) using parameters θ. These parameters are updated by minimising the mean-squared error of a 1-step lookahead loss LQ = E [ (rt + γmaxa′Q(s ′, a′; θ−)−Q(s, a; θ))2 ] , where θ− are parameters corresponding to a frozen network and optimisation is performed with respect to θ, with θ− being synced to θ at regular intervals.\nA.6.2. A3C\nAsynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) is an asynchronous implementation of the advantage actor-critic paradigm (Sutton & Barto, 1998; Degris & Sutton, 2012), where separate threads run in parallel and perform updates to shared parameters. The different threads each hold their own instance of the environment and have different exploration policies, thereby decorrelating parameter updates without the need for experience replay.\nA3C uses neural networks to approximate both policy π(a|s; θ) and value Vπ(s; θ) functions using parameters θ using nstep look-ahead loss (Peng & Williams, 1996). The algorithm is trained using an advantage actor-critic loss function with an entropy regularisation penalty: LA3C ≈ LV R + Lπ − Es∼π [αH(π(a|s; θ))], where H is entropy. The parameter updates are performed after every tmax actions or when a terminal state is reached. LV R =\n9For the purposes of this metric, we utilised rooms with only single objects, which we denote by the subscript V e.g. the observation set SoV .\nEs∼π [ (Rt:t+n + γ nV (st+n+1; θ)− V (st; θ))2 ]\nand Lπ = Es∼π [log π(a|s; θ)(Qπ(s, a; θ)− V π(s; θ))]. Unlike DQN, A3C uses an LSTM core to encode its history and therefore has a longer term memory permitting it to perform better in partially observed environments. In the version of A3C used in this paper for the DeepMind Lab task, the policy net additionally takes the last action at−1 and last reward rt−1 as inputs along with the observation sot , as introduced in (Jaderberg et al., 2017).\nA.6.3. UNREAL\nThe UNREAL agent (Jaderberg et al., 2017) takes as a base an LSTM A3C agent (Mnih et al., 2016) and augments it with a number of unsupervised auxiliary tasks that make use of the rich perceptual data available to the agent besides the (sometimes very sparse) extrinsic reward signals. This auxiliary learning tends to improve the representation learnt by the agent. While training the base agent, its observations, rewards, and actions are stored in a replay buffer, which is used by the auxiliary learning tasks. The tasks include: 1) pixel control the agent learns how to control the environment by training auxiliary policies to maximally change pixel intensities in different parts of the input; 2) reward prediction - given a replay buffer of observations within a short time period of an extrinsic reward, the agent has to predict the reward obtained during the next unobserved timestep using a sequence of three preceding steps; 3) value function replay - extra training of the value function to promote faster value iteration."
    }, {
      "heading" : "A.6.4. EPISODIC CONTROL",
      "text" : "In its simplest form EC is a lookup table of states and actions denoted as QEC(s, a). In each state EC picks the action with the highest QEC value. At the end of each episode QEC(s, a) is set to Rt if (st, at) /∈ QEC , where Rt is the discounted return. Otherwise QEC(s, a) = max { QEC(s, a), Rt } . In order to generalise its policy to novel states that are not in QEC , EC uses a non-parametric nearest neighbours search Q̂EC(s, a) = 1 k ∑k i=1Q\nEC(si, a), where si, i = 1, ..., k are k states with the smallest distance to the novel state s. Like DQN, EC takes a concatenation of four frames as input.\nThe EC algorithm is proposed as a model of fast hippocampal instance-based learning in the brain (Marr, 1971; Sutherland & Rudy, 1989), while the deep RL algorithms described above are more analogous to slow cortical learning that relies on generalised statistical summaries of the input distribution (McClelland et al., 1995; Norman & O’Reilly, 2003; Tulving et al., 1991)."
    }, {
      "heading" : "A.7. Source Task Performance Results",
      "text" : "The focus of this paper is primarily on zero-shot domain adaptation performance. However, it is also interesting to analyse the effect of the DARLA approach on source domain policy performance. In order to compare the models’ behaviour on the source task, we examined the training curves (see Figures 7-10) and noted in particular their:\n1. Asymptotic task performance, i.e. the rewards per episode at the point where πS has converged for the agent under consideration.\n2. Data efficiency, i.e. how quickly the training curve was able to achieve convergence.\nWe note the following consistent trends across the results:\n1. Using DARLA provided an initial boost in learning performance, which depended on the degree of disentanglement of the representation. This was particularly observable in A3C,\nsee Fig. 8.\n2. Baseline algorithms where F could be fine-tuned to the source task were able to achieve higher asymptotic performance. This was particularly notable on DQN and A3C (see Figs. 7 and 8) in DeepMind Lab. However, in both those cases, DARLA was able to learn very reasonable policies on the source task which were on the order of 20% lower than the fine-tuned models – arguably a worthwhile sacrifice for a subsequent median 270% improvement in target domain performance noted in the main text.\n3. Allowing DARLA to fine-tune its vision module (DARLAFT) boosted its source task learning speed, and allowed the agent to asymptote at the same level as the baseline algorithms. As discussed in the main text, this comes at the cost of significantly reduced domain transfer performance on A3C. For DQN, however, finetuning appears to offer the best of both worlds.\n4. Perhaps most relevantly for this paper, even if solely examining source task performance, DARLA outperforms both DARLAENT and DARLADAE on both asymptotic performance and data efficiency – suggesting that disentangled representations have wider applicability in RL beyond the zero-shot domain adaptation that is the focus of this paper."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "author" : [ "Abadi", "Martin", "Agarwal", "Ashish", "Paul Barham" ],
      "venue" : "Preliminary White Paper,",
      "citeRegEx" : "Abadi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Abadi et al\\.",
      "year" : 2015
    }, {
      "title" : "Successor features for transfer in reinforcement learning",
      "author" : [ "Barreto", "André", "Munos", "Rémi", "Schaul", "Tom", "Silver", "David" ],
      "venue" : "CoRR, abs/1606.05312,",
      "citeRegEx" : "Barreto et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Barreto et al\\.",
      "year" : 2016
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : "In IEEE Transactions on Pattern Analysis & Machine Intelligence,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Model-free episodic control",
      "author" : [ "Blundell", "Charles", "Uria", "Benigno", "Pritzel", "Alexander", "Li", "Yazhe", "Ruderman", "Avraham", "Leibo", "Joel Z", "Rae", "Jack", "Wierstra", "Daan", "Hassabis", "Demis" ],
      "venue" : null,
      "citeRegEx" : "Blundell et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blundell et al\\.",
      "year" : 2016
    }, {
      "title" : "Retinal image quality and postnatal visual experience during infancy",
      "author" : [ "Candy", "T. Rowan", "Wang", "Jingyun", "Ravikumar", "Sowmya" ],
      "venue" : "Optom Vis Sci,",
      "citeRegEx" : "Candy et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candy et al\\.",
      "year" : 2009
    }, {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Discovering hidden factors of variation in deep networks",
      "author" : [ "Cheung", "Brian", "Levezey", "Jesse A", "Bansal", "Arjun K", "Olshausen", "Bruno A" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations, Workshop Track,",
      "citeRegEx" : "Cheung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cheung et al\\.",
      "year" : 2015
    }, {
      "title" : "Transformation properties of learned visual representations",
      "author" : [ "T. Cohen", "M. Welling" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Cohen and Welling,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen and Welling",
      "year" : 2015
    }, {
      "title" : "Learning the irreducible representations of commutative lie groups",
      "author" : [ "Cohen", "Taco", "Welling", "Max" ],
      "venue" : null,
      "citeRegEx" : "Cohen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning transferable policies for monocular reactive mav control",
      "author" : [ "Daftry", "Shreyansh", "Bagnell", "J. Andrew", "Hebert", "Martial" ],
      "venue" : "International Symposium on Experimental Robotics,",
      "citeRegEx" : "Daftry et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Daftry et al\\.",
      "year" : 2016
    }, {
      "title" : "Model-free reinforcement learning with continuous action in practice",
      "author" : [ "Degris", "Thomas", "Pilarski Patrick M", "Sutton", "Richard S" ],
      "venue" : "American Control Conference (ACC),",
      "citeRegEx" : "Degris et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Degris et al\\.",
      "year" : 2012
    }, {
      "title" : "Disentangling factors of variation via generative entangling",
      "author" : [ "G. Desjardins", "A. Courville", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Desjardins et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Desjardins et al\\.",
      "year" : 2012
    }, {
      "title" : "Generating images with perceptual similarity metrics based on deep networks",
      "author" : [ "Dosovitskiy", "Alexey", "Brox", "Thomas" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep spatial autoencoders for visuomotor learning",
      "author" : [ "Finn", "Chelsea", "Tan", "Xin Yu", "Duan", "Yan", "Darrell", "Trevor", "Levine", "Sergey", "Abbeel", "Pieter" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Finn et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2015
    }, {
      "title" : "Generalizing skills with semi-supervised reinforcement learning",
      "author" : [ "Finn", "Chelsea", "Yu", "Tianhe", "Fu", "Justin", "Abbeel", "Pieter", "Levine", "Sergey" ],
      "venue" : null,
      "citeRegEx" : "Finn et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards deep symbolic reinforcement learning",
      "author" : [ "Garnelo", "Marta", "Arulkumaran", "Kai", "Shanahan", "Murray" ],
      "venue" : null,
      "citeRegEx" : "Garnelo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Garnelo et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. WardeFarley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to linearize under uncertainty",
      "author" : [ "Goroshin", "Ross", "Mathieu", "Michael", "LeCun", "Yann" ],
      "venue" : null,
      "citeRegEx" : "Goroshin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goroshin et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient bayesadaptive reinforcement learning using sample-based search",
      "author" : [ "Guez", "Arthur", "Silver", "David", "Dayan", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Guez et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Guez et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning invariant feature spaces to transfer skills with reinforcement learning",
      "author" : [ "Gupta", "Abhishek", "Devin", "Coline", "Liu", "YuXuan", "Abbeel", "Pieter", "Levine", "Sergey" ],
      "venue" : null,
      "citeRegEx" : "Gupta et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning continuous control policies by stochastic value gradients",
      "author" : [ "Heess", "Nicolas", "Wayne", "Gregory", "Silver", "David", "Lillicrap", "Timothy P", "Erez", "Tom", "Tassa", "Yuval" ],
      "venue" : null,
      "citeRegEx" : "Heess et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Heess et al\\.",
      "year" : 2015
    }, {
      "title" : "Beta-vae: Learning basic visual concepts with a constrained variational framework",
      "author" : [ "Higgins", "Irina", "Matthey", "Loic", "Pal", "Arka", "Burgess", "Christopher", "Glorot", "Xavier", "Botvinick", "Matthew", "Mohamed", "Shakir", "Lerchner", "Alexander" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Higgins et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Higgins et al\\.",
      "year" : 2017
    }, {
      "title" : "Reinforcement learning with unsupervised auxiliary tasks",
      "author" : [ "Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray" ],
      "venue" : null,
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2017
    }, {
      "title" : "Bayesian representation learning with oracle constraints",
      "author" : [ "Karaletsos", "Theofanis", "Belongie", "Serge", "Rtsch", "Gunnar" ],
      "venue" : null,
      "citeRegEx" : "Karaletsos et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Karaletsos et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "Ba", "Jimmy" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "ICLR,",
      "citeRegEx" : "Kingma and Welling,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Deep convolutional inverse graphics",
      "author" : [ "Kulkarni", "Tejas", "Whitney", "William", "Kohli", "Pushmeet", "Tenenbaum", "Joshua" ],
      "venue" : null,
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Building machines that learn and think like people",
      "author" : [ "Lake", "Brenden M", "Ullman", "Tomer D", "Tenenbaum", "Joshua B", "Gershman", "Samuel J" ],
      "venue" : null,
      "citeRegEx" : "Lake et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2016
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity metric",
      "author" : [ "Larsen", "Anders Boesen Lindbo", "Snderby", "Sren Kaae", "Larochelle", "Hugo", "Winther", "Ole" ],
      "venue" : null,
      "citeRegEx" : "Larsen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 2016
    }, {
      "title" : "Development of visual acuity and contrast sensitivity in children",
      "author" : [ "Leat", "Susan J", "Yadav", "Naveen K", "Irving", "Elizabeth L" ],
      "venue" : "Journal of Optometry,",
      "citeRegEx" : "Leat et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Leat et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning neural network policies with guided policy search under unknown dynamics",
      "author" : [ "Levine", "Sergey", "Abbeel", "Pieter" ],
      "venue" : null,
      "citeRegEx" : "Levine et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2014
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan" ],
      "venue" : null,
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2015
    }, {
      "title" : "Predictive representations of state",
      "author" : [ "Littman", "Michael L", "Sutton", "Richard S", "Singh", "Satinder" ],
      "venue" : null,
      "citeRegEx" : "Littman et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Littman et al\\.",
      "year" : 2001
    }, {
      "title" : "Simple memory: A theory for archicortex",
      "author" : [ "D. Marr" ],
      "venue" : "Philosophical Transactions of the Royal Society of London, pp",
      "citeRegEx" : "Marr,? \\Q1971\\E",
      "shortCiteRegEx" : "Marr",
      "year" : 1971
    }, {
      "title" : "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
      "author" : [ "McClelland", "James L", "McNaughton", "Bruce L", "OReilly", "Randall C" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "McClelland et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "McClelland et al\\.",
      "year" : 1995
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David S", "Rusu", "Andrei A" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr", "Badia", "Adri Puigdomnech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray" ],
      "venue" : "URL https://arxiv. org/pdf/1602.01783.pdf",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Incremental semantically grounded learning from demonstration",
      "author" : [ "Niekum", "Scott", "Chitta", "Sachin", "Barto", "Andrew G", "Marthi", "Bhaskara", "Osentoski", "Sarah" ],
      "venue" : "Robotics: Science and Systems,",
      "citeRegEx" : "Niekum et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Niekum et al\\.",
      "year" : 2013
    }, {
      "title" : "Modeling hippocampal and neocortical contributions to recognition memory: a complementary-learning-systems approach",
      "author" : [ "Norman", "Kenneth A", "O’Reilly", "Randall C" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "Norman et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Norman et al\\.",
      "year" : 2003
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Pan", "Sinno Jialin", "Yang", "Quiang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "Pan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2009
    }, {
      "title" : "Actormimic: Deep multitask and transfer reinforcement learning",
      "author" : [ "Parisotto", "Emilio", "Ba", "Jimmy", "Salakhutdinov", "Ruslan" ],
      "venue" : null,
      "citeRegEx" : "Parisotto et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Parisotto et al\\.",
      "year" : 2015
    }, {
      "title" : "Context encoders: Feature learning by inpainting",
      "author" : [ "Pathak", "Deepak", "Krähenbühl", "Philipp", "Donahue", "Jeff", "Darrell", "Trevor", "Efros", "Alexei A" ],
      "venue" : "CoRR, abs/1604.07379,",
      "citeRegEx" : "Pathak et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pathak et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient dynamic programming-based learning for control",
      "author" : [ "J. Peng" ],
      "venue" : "PhD thesis, Northeastern University,",
      "citeRegEx" : "Peng,? \\Q1993\\E",
      "shortCiteRegEx" : "Peng",
      "year" : 1993
    }, {
      "title" : "Incremental multi-step qlearning",
      "author" : [ "Peng", "Jing", "Williams", "Ronald J" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Peng et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 1996
    }, {
      "title" : "Unsupervised learning of state representations for multiple tasks",
      "author" : [ "Raffin", "Antonin", "Hfer", "Sebastian", "Jonschkowski", "Rico", "Brock", "Oliver", "Stulp", "Freek" ],
      "venue" : null,
      "citeRegEx" : "Raffin et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Raffin et al\\.",
      "year" : 2017
    }, {
      "title" : "Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain",
      "author" : [ "Rajendran", "Janarthanan", "Lakshminarayanan", "Aravind", "Khapra", "P Mitesh M", "Prasanna", "Ravindran", "Balaraman" ],
      "venue" : null,
      "citeRegEx" : "Rajendran et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Rajendran et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to disentangle factors of variation with manifold interaction",
      "author" : [ "Reed", "Scott", "Sohn", "Kihyuk", "Zhang", "Yuting", "Lee", "Honglak" ],
      "venue" : null,
      "citeRegEx" : "Reed et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : null,
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "A survey of inductive biases for factorial Representation-Learning. arXiv, 2016",
      "author" : [ "Ridgeway", "Karl" ],
      "venue" : "URL http:// arxiv.org/abs/1612.05299",
      "citeRegEx" : "Ridgeway and Karl.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ridgeway and Karl.",
      "year" : 2016
    }, {
      "title" : "High-dimensional probability estimation with deep density models",
      "author" : [ "Rippel", "Oren", "Adams", "Ryan Prescott" ],
      "venue" : null,
      "citeRegEx" : "Rippel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rippel et al\\.",
      "year" : 2013
    }, {
      "title" : "Sim-to-real robot learning from pixels with progressive nets",
      "author" : [ "Rusu", "Andrei A", "Vecerik", "Matej", "Rothrl", "Thomas", "Heess", "Nicolas", "Pascanu", "Razvan", "Hadsell", "Raia" ],
      "venue" : null,
      "citeRegEx" : "Rusu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rusu et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning factorial codes by predictability minimization",
      "author" : [ "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schmidhuber and Jürgen.,? \\Q1992\\E",
      "shortCiteRegEx" : "Schmidhuber and Jürgen.",
      "year" : 1992
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter" ],
      "venue" : null,
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "High-dimensional continuous control using generalized advantage estimation",
      "author" : [ "Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter" ],
      "venue" : null,
      "citeRegEx" : "Schulman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2016
    }, {
      "title" : "Configural association theory: The role of the hippocampal formation in learning, memory, and amnesia",
      "author" : [ "Sutherland", "Robert J", "Rudy", "Jerry W" ],
      "venue" : "Psychobiology, 17:129144,",
      "citeRegEx" : "Sutherland et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Sutherland et al\\.",
      "year" : 1989
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "An experts algorithm for transfer learning",
      "author" : [ "Talvitie", "Erik", "Singh", "Satinder" ],
      "venue" : "In Proceedings of the 20th international joint conference on Artifical intelligence,",
      "citeRegEx" : "Talvitie et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Talvitie et al\\.",
      "year" : 2007
    }, {
      "title" : "Domain randomization for transferring deep neural networks from simulation to the real world",
      "author" : [ "Tobin", "Josh", "Fong", "Rachel", "Ray", "Alex", "Schneider", "Jonas", "Zaremba", "Wojciech", "Abbeel", "Pieter" ],
      "venue" : "arxiv,",
      "citeRegEx" : "Tobin et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tobin et al\\.",
      "year" : 2017
    }, {
      "title" : "Mujoco: A physics engine for model-based control",
      "author" : [ "E. Todorov", "T. Erez", "Y. Tassa" ],
      "venue" : null,
      "citeRegEx" : "Todorov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Todorov et al\\.",
      "year" : 2012
    }, {
      "title" : "Longlasting perceptual priming and semantic learning in amnesia: a case experiment",
      "author" : [ "Tulving", "Endel", "CA Hayman", "Macdonald", "Carol A" ],
      "venue" : "Journal of Experimental Psychology: Learning, Memory, and Cognition,",
      "citeRegEx" : "Tulving et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Tulving et al\\.",
      "year" : 1991
    }, {
      "title" : "Adapting deep visuomotor representations with weak pairwise constraints",
      "author" : [ "Tzeng", "Eric", "Devin", "Coline", "Hoffman", "Judy", "Finn", "Chelsea", "Abbeel", "Pieter", "Levine", "Sergey", "Saenko", "Kate", "Darrell", "Trevor" ],
      "venue" : null,
      "citeRegEx" : "Tzeng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tzeng et al\\.",
      "year" : 2016
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine" ],
      "venue" : null,
      "citeRegEx" : "Vincent et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    }, {
      "title" : "Improving generative adversarial networks with denoising feature matching",
      "author" : [ "Warde-Farley", "David", "Bengio", "Yoshua" ],
      "venue" : null,
      "citeRegEx" : "Warde.Farley et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Warde.Farley et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ "Watkins", "Christopher John Cornish Hellaby" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Watkins and Hellaby.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins and Hellaby.",
      "year" : 1989
    }, {
      "title" : "Understanding visual concepts with continuation learning. arXiv, 2016",
      "author" : [ "Whitney", "William F", "Chang", "Michael", "Kulkarni", "Tejas", "Tenenbaum", "Joshua B" ],
      "venue" : "URL http://arxiv. org/pdf/1602.06822.pdf",
      "citeRegEx" : "Whitney et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Whitney et al\\.",
      "year" : 2016
    }, {
      "title" : "Weakly-supervised disentangling with recurrent transformations for 3d view synthesis",
      "author" : [ "Yang", "Jimei", "Reed", "Scott", "Ming-Hsuan", "Lee", "Honglak" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "deep learning and its successful combination with RL has enabled end-to-end learning of such embeddings directly from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016; Jaderberg et al., 2017).",
      "startOffset" : 202,
      "endOffset" : 251
    }, {
      "referenceID" : 22,
      "context" : "deep learning and its successful combination with RL has enabled end-to-end learning of such embeddings directly from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016; Jaderberg et al., 2017).",
      "startOffset" : 202,
      "endOffset" : 251
    }, {
      "referenceID" : 15,
      "context" : "These include data inefficiency, the reactive nature and general brittleness of learnt policies to changes in input data distribution, and lack of model interpretability (Garnelo et al., 2016; Lake et al., 2016).",
      "startOffset" : 170,
      "endOffset" : 211
    }, {
      "referenceID" : 28,
      "context" : "These include data inefficiency, the reactive nature and general brittleness of learnt policies to changes in input data distribution, and lack of model interpretability (Garnelo et al., 2016; Lake et al., 2016).",
      "startOffset" : 170,
      "endOffset" : 211
    }, {
      "referenceID" : 2,
      "context" : "This paper focuses on one of these outstanding issues: the ability of RL agents to deal with changes to the input distribution, a form of transfer learning known as domain adaptation (Bengio et al., 2013).",
      "startOffset" : 183,
      "endOffset" : 204
    }, {
      "referenceID" : 13,
      "context" : "Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).",
      "startOffset" : 164,
      "endOffset" : 266
    }, {
      "referenceID" : 45,
      "context" : "Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).",
      "startOffset" : 164,
      "endOffset" : 266
    }, {
      "referenceID" : 1,
      "context" : "Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).",
      "startOffset" : 164,
      "endOffset" : 266
    }, {
      "referenceID" : 33,
      "context" : "Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001).",
      "startOffset" : 164,
      "endOffset" : 266
    }, {
      "referenceID" : 61,
      "context" : "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 342
    }, {
      "referenceID" : 9,
      "context" : "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 342
    }, {
      "referenceID" : 41,
      "context" : "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 342
    }, {
      "referenceID" : 18,
      "context" : "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 342
    }, {
      "referenceID" : 38,
      "context" : "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 342
    }, {
      "referenceID" : 19,
      "context" : "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 342
    }, {
      "referenceID" : 14,
      "context" : "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 342
    }, {
      "referenceID" : 46,
      "context" : "Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals from both domains (Tzeng et al., 2016; Daftry et al., 2016; Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh, 2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al., 2017; Rajendran et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 342
    }, {
      "referenceID" : 14,
      "context" : "In many scenarios, such as robotics, this reliance on target domain information can be problematic, as the data may be expensive or difficult to obtain (Finn et al., 2017; Rusu et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 190
    }, {
      "referenceID" : 51,
      "context" : "In many scenarios, such as robotics, this reliance on target domain information can be problematic, as the data may be expensive or difficult to obtain (Finn et al., 2017; Rusu et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 190
    }, {
      "referenceID" : 28,
      "context" : "On the other hand, policies learnt exclusively on the source domain using existing deep RL approaches that have few constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor domain adaptation performance (Lake et al., 2016; Rusu et al., 2016).",
      "startOffset" : 268,
      "endOffset" : 306
    }, {
      "referenceID" : 51,
      "context" : "On the other hand, policies learnt exclusively on the source domain using existing deep RL approaches that have few constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor domain adaptation performance (Lake et al., 2016; Rusu et al., 2016).",
      "startOffset" : 268,
      "endOffset" : 306
    }, {
      "referenceID" : 27,
      "context" : "We think of these factors as a set of high-level parameters that can be used by a world graphics engine to generate a particular natural visual scene (Kulkarni et al., 2015).",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 11,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 27,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 47,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 66,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 17,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 27,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 6,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 65,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 23,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 5,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 21,
      "context" : "Learning how to project raw observations into such a factorised description of the world is addressed by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012; Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 174,
      "endOffset" : 506
    }, {
      "referenceID" : 2,
      "context" : "Disentangled representations are defined as interpretable, factorised latent representations where either a single latent or a group of latent units are sensitive to changes in single ground truth factors of variation used to generate the visual world, while being invariant to changes in other factors (Bengio et al., 2013).",
      "startOffset" : 303,
      "endOffset" : 324
    }, {
      "referenceID" : 2,
      "context" : "The theoretical utility of disentangled representations for supervised and reinforcement learning has been described before (Bengio et al., 2013; Higgins et al., 2017; Ridgeway, 2016); however, to our knowledge, it has not been empirically validated to date.",
      "startOffset" : 124,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "The theoretical utility of disentangled representations for supervised and reinforcement learning has been described before (Bengio et al., 2013; Higgins et al., 2017; Ridgeway, 2016); however, to our knowledge, it has not been empirically validated to date.",
      "startOffset" : 124,
      "endOffset" : 183
    }, {
      "referenceID" : 30,
      "context" : "by utilising a stream of raw unlabelled observations – not unlike human babies in their first few months of life (Leat et al., 2009; Candy et al., 2009).",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "by utilising a stream of raw unlabelled observations – not unlike human babies in their first few months of life (Leat et al., 2009; Candy et al., 2009).",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 59,
      "context" : "These effects hold consistently across a number of different RL environments (DeepMind Lab and Jaco/MuJoCo: Beattie et al., 2016; Todorov et al., 2012) and algorithms (DQN, A3C and Episodic Control: Mnih et al.",
      "startOffset" : 77,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : ", 2012) and algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016).",
      "startOffset" : 23,
      "endOffset" : 102
    }, {
      "referenceID" : 59,
      "context" : "For example, consider a domain adaptation scenario for the Jaco robotic arm, where the MuJoCo (Todorov et al., 2012) simulation of the arm is the source domain, and the real world setting is the target domain.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 51,
      "context" : "The state spaces (raw pixels) of the source and the target domains differ significantly due to the perceptual-reality gap (Rusu et al., 2016); that is to say, SS 6= ST .",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "We contend that this is a reasonable assumption that permits inclusion of many interesting problems, including being able to characterise our own reality (Lake et al., 2016).",
      "startOffset" : 154,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : "1) Learn to see (unsupervised learning of FU ) – the task of inferring a factorised set of generative factors S Ŝ = Ŝ from observations S is the goal of the extensive disentangled factor learning literature (e.g. Chen et al., 2016; Higgins et al., 2017).",
      "startOffset" : 207,
      "endOffset" : 253
    }, {
      "referenceID" : 21,
      "context" : "In order to learn FU , DARLA utilises β-VAE (Higgins et al., 2017), a state-of-the-art unsupervised model for automated discovery of factorised latent representations from raw image data.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 48,
      "context" : "β-VAE is a modification of the variational autoencoder framework (Kingma & Welling, 2014; Rezende et al., 2014) that controls the nature of the learnt latent representations by introducing an adjustable hyperparameter β to balance reconstruction accuracy with latent channel capacity and independence constraints.",
      "startOffset" : 65,
      "endOffset" : 111
    }, {
      "referenceID" : 21,
      "context" : "Well-chosen values of β - usually larger than one (β > 1) - typically result in more disentangled latent representations z by limiting the capacity of the latent information channel, and hence encouraging a more efficient factorised encoding through the increased pressure to match the isotropic unit Gaussian prior p(z) (Higgins et al., 2017).",
      "startOffset" : 321,
      "endOffset" : 343
    }, {
      "referenceID" : 16,
      "context" : "The shortcomings of calculating the log-likelihood term Eqφ(z|x)[log pθ(x|z)] on a per-pixel basis are known and have been addressed in the past by calculating the reconstruction cost in an abstract, high-level feature space given by another neural network model, such as a GAN (Goodfellow et al., 2014) or a pre-trained AlexNet (Krizhevsky et al.",
      "startOffset" : 278,
      "endOffset" : 303
    }, {
      "referenceID" : 26,
      "context" : ", 2014) or a pre-trained AlexNet (Krizhevsky et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox, 2016; Warde-Farley & Bengio, 2017).",
      "startOffset" : 33,
      "endOffset" : 134
    }, {
      "referenceID" : 29,
      "context" : ", 2014) or a pre-trained AlexNet (Krizhevsky et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox, 2016; Warde-Farley & Bengio, 2017).",
      "startOffset" : 33,
      "endOffset" : 134
    }, {
      "referenceID" : 62,
      "context" : "In practice we found that pre-training a denoising autoencoder (Vincent et al., 2010) on data from the visual pre-training MDP DU ∈ M worked best as the reconstruction targets for β-VAE to match (see Fig.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 48,
      "context" : "2 we are no longer optimising the variational lower bound, and β-VAEDAE with β = 1 loses its equivalence to the Variational Autoencoder (VAE) framework as proposed by (Kingma & Welling, 2014; Rezende et al., 2014).",
      "startOffset" : 167,
      "endOffset" : 213
    }, {
      "referenceID" : 3,
      "context" : "We used various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn the source policy π during stage two of the pipeline using the latent states s acquired by β-VAE based models during stage one of the DARLA pipeline.",
      "startOffset" : 30,
      "endOffset" : 109
    }, {
      "referenceID" : 36,
      "context" : "Deep Q Network (DQN) (Mnih et al., 2015) is a variant of the Q-learning algorithm (Watkins, 1989) that utilises deep learning.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 37,
      "context" : "Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) is an asynchronous implementation of the advantage actor-critic paradigm (Sutton & Barto, 1998; Degris & Sutton, 2012), where separate threads run in parallel and perform updates to shared parameters.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "Model-Free Episodic Control (EC) (Blundell et al., 2016) was proposed as a complementary learning system to the other RL algorithms described above.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "We also compared our approach to that of UNREAL (Jaderberg et al., 2017), a recently proposed RL algorithm which also attempts to utilise unsupervised data in the environment.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 37,
      "context" : "The UNREAL agent takes as a base an LSTM A3C agent (Mnih et al., 2016) and augments it with a number of unsupervised auxiliary tasks that make use of the rich perceptual data available to the agent besides the (sometimes very sparse) extrinsic reward signals.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 59,
      "context" : "We use the Jaco arm with a matching MuJoCo simulation environment (Todorov et al., 2012) in two domain adaptation scenarios: simulation to simulation (sim2sim) and simulation to reality (sim2real).",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 51,
      "context" : "The sim2real setup, on the other hand, is based on identical processes GS = GT , but different observation simulators SimS 6= SimT corresponding to the MuJoCo simulation and the real world, which results in the so-called ‘perceptual reality gap’ (Rusu et al., 2016).",
      "startOffset" : 246,
      "endOffset" : 265
    }, {
      "referenceID" : 3,
      "context" : "Pre-trained β-VAEDAE from stage one was used as the ‘vision’ part of various RL algorithms (DQN, A3C and Episodic Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to learn a source policy πS that picks up balloons and avoids cakes in both the green and the pink rooms, and picks up cans and avoids hats in the green rooms.",
      "startOffset" : 91,
      "endOffset" : 170
    }, {
      "referenceID" : 59,
      "context" : "We used frames from an RGB camera facing a robotic Jaco arm, or a matching rendered camera view from a MuJoCo physics simulation environment (Todorov et al., 2012) to investigate the performance of DARLA in two domain adaptation scenarios: 1) simulation to simulation (sim2sim), and 2) simulation to reality (sim2real).",
      "startOffset" : 141,
      "endOffset" : 163
    }, {
      "referenceID" : 53,
      "context" : "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.",
      "startOffset" : 123,
      "endOffset" : 255
    }, {
      "referenceID" : 37,
      "context" : "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.",
      "startOffset" : 123,
      "endOffset" : 255
    }, {
      "referenceID" : 20,
      "context" : "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.",
      "startOffset" : 123,
      "endOffset" : 255
    }, {
      "referenceID" : 32,
      "context" : "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.",
      "startOffset" : 123,
      "endOffset" : 255
    }, {
      "referenceID" : 54,
      "context" : "The sim2real setup is of particular importance, since the progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine & Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015; Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al.",
      "startOffset" : 123,
      "endOffset" : 255
    }, {
      "referenceID" : 58,
      "context" : ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 175
    }, {
      "referenceID" : 61,
      "context" : ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 175
    }, {
      "referenceID" : 51,
      "context" : ", 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 175
    }, {
      "referenceID" : 22,
      "context" : "Finally, we compared the performance of DARLA to an UNREAL (Jaderberg et al., 2017) agent with the same architecture.",
      "startOffset" : 59,
      "endOffset" : 83
    } ],
    "year" : 2017,
    "abstractText" : "Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA’s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",
    "creator" : "LaTeX with hyperref package"
  }
}