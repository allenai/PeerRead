{
  "name" : "1511.06314.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks",
    "authors" : [ "Stefan Lee", "Senthil Purushwalkam", "Michael Cogswell", "David Crandall", "Dhruv Batra" ],
    "emails" : [ "djcran}@indiana.edu", "spurushw@andrew.cmu.edu", "dbatra}@vt.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Convolutional Neural Networks have achieved state-ofthe-art performance on a wide range of tasks. Most benchmarks are led by ensembles of these powerful learners, but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a firstclass problem to explicitly address the question: what are the best strategies to create an ensemble? We first compare a large number of ensembling strategies, and then propose and evaluate novel strategies, such as parameter sharing (through a new family of models we call TreeNets) as well as training under ensemble-aware and diversity-encouraging losses. We demonstrate that TreeNets can improve ensemble performance and that diverse ensembles can be trained endto-end under a unified loss, achieving significantly higher “oracle” accuracies than classical ensembles."
    }, {
      "heading" : "1. Introduction",
      "text" : "Convolutional Neural Networks (CNNs) have shown impressive performance on a wide range of computer vision tasks. An important (and perhaps under-acknowledged) fact is that the state-of-the-art models are generally ensembles of CNNs, including nearly all of the top performers on the ImageNet Large Scale Visual Recognition Challenge [31]. For example, GoogLeNet [36], one of the best-performing models submitted to the ILSVRC challenge, is an ensemble achieving a five percentage point increase in accuracy over a single base model of the same architecture.\nIn these ensembles, multiple classifiers are trained to perform the same task and their predictions are averaged to generate a new, typically more accurate, prediction. A number of related justifications have been given for the success of ensembles, including:\ni) Bayesian Model Averaging, that ensembles are a finite sample approximation to integration over the model\nclass [9, 26, 27]; ii) Model Combination, that ensembles enrich the space\nof hypotheses considered by the base model class and are representationally richer [8]; and iii) Reducing Estimation and Optimization Errors, that ensemble averaging reduces the variance of base models, averaging out variations due to objective function nonconvexity, initialization, and stochastic learning [7,28].\nAt the heart of these arguments is the idea of diversity: if we train multiple learners with decorrelated errors, their predictions can be averaged to improve performance [5]. In this work, we rigorously treat ensembling as a problem in its own right, examining multiple ensembling strategies ranging from standard bagging to parameter sharing and ensemble-aware losses. We compare these methods across multiple datasets and architectures, demonstrating that some standard techniques may not be suitable for deep ensembles and novel approaches improve performance.\nEnsemble-Aware Losses. Typically, ensemble members are trained independently with no unifying loss, despite the fact that outputs are combined at test time. It is common in classical literature to view ensemble members as “experts” [18] or “specialists” [17], but in typical practice no effort is made to encourage diversity or specialization. It seems natural then to question whether a ensemble-aware loss might result in better performance. Here we study two ensemble-aware losses: (1) directly training an ensemble to minimize the loss of the ensemble mean, and (2) generalization of Multiple Choice Learning [13] to explicitly encourage diversity.\nParameter Sharing. As a number of papers have demonstrated, initial layers of CNNs tend to learn simple, generic features which vary little between models, while deeper layers learn features specific to a particular task and input distribution [12,24,37]. We propose a family of tree-structured deep networks (which we call TreeNets) that exploit the generality in lower layers by sharing them across ensemble members to reduce parameters. We investigate the depth at which sharing should happen, along a spectrum from single models (full sharing) to independent ensembles (no sharing). This coupling of lower layers naturally forces any di-\n1\nar X\niv :1\n51 1.\n06 31\n4v 1\n[ cs\n.C V\n] 1\n9 N\nov 2\n01 5\nversity between ensemble members to be concentrated in the deeper, unshared layers. Perhaps somewhat surprisingly, we find that the optimal setting is not a classical ensemble, but instead a TreeNet that shares a few (typically 1- 2) initial layers. Thus tree-structured networks are a simple way to improve performance while reducing parameters.\nModel-Distributed Training of Coupled Ensembles. Unfortunately, both of the above approaches to coupling ensemble members, either at the “top” of the architecture with ensemble-aware losses that operate on outputs from all ensemble members, or at the “bottom” with parameter sharing in TreeNets, create significant computational difficulties. Since networks are not independent, it is no longer possible to train them separately in parallel, and sequential training may require months of GPU time even for relatively small ensembles. Moreover, even if training time is not a concern, larger models often take up most of the available RAM on a GPU, so it is not possible to fit an ensemble on one GPU. To overcome these hurdles, we present and will release a novel MPI-based model-parallel distributed modification to the popular Caffe deep learning framework [19] that implements cross-process communication as layers of the CNN.\nWe thoroughly evaluate each methodology across multiple datasets and network architectures. These experiments cast new light on ensembles in deep networks, demonstrating the effects of randomization in parameter and data space, parameter sharing, and unified losses on modern scale vision problems. More concretely, we:\ni) rigorously treat CNN ensembling as its own problem, ii) introduce a family of models called TreeNets that per-\nmit a spectrum of degrees of layer-sharing, iii) present ensemble-aware and diversity-encouraging\nloses, and iv) present a distributed model-parallel framework to train\ndeep ensembles."
    }, {
      "heading" : "2. Related Work",
      "text" : "Neural networks, ensembles, and techniques to improve robustness and diversity of grouped learners have decades of work in machine learning research, but only recently have ensembles of CNNs been studied. Related work can be broadly divided into two categories: ensemble learning for general networks, and its more recent application to CNNs.\nEnsemble Learning Theory. Neural networks have been applied in a wide variety settings with many diverse modifications. Much of the theoretical foundation for ensemble learning with neural networks was laid in the 1990s. Krogh et al. [22] and Hansen and Salamon [16] provided theoretical and empirical evidence that diversity in error distributions across member models can boost ensemble performance. This led to ensemble methods that averaged predic-\ntions from models trained with different initializations [16] and from models trained on different bootstrapped training sets [22, 38]. These methods take an indirect approach to introducing diversity in ensembles. Other work has explicitly trained decorrelated ensembles of neural networks by penalizing positive correlation between error distributions [2,23,29]. While effective on shallow networks, these methods have not been applied to deeper architectures.\nAlthough initially proposed for Structured SVMs, the work of Guzman-Rivera et al. [13–15] on Multiple Choice Learning (MCL) provides an attractive alternative that does not require computing correlation between error. Related ideas were studied by Dey et al. [6] in the context of submodular list prediction. We generalize MCL and apply it to CNNs – incorporate it with stochastic gradient descentbased training.\nCNN Ensembles. While ensembles of CNNs have been used extensively, little work has focused on improving the ensembling process. Most CNN ensembles use multiple random initializations or training data subsets to inject diversity. For example, popular ensembles of VGG [33] and AlexNet [21] simply retrain with different initializations and average the predictions. GoogLeNet [36] induces diversity with straightforward bagging, training each model with a sampled dataset. Other networks, like Sequence to Sequence RNNs [35], use both approaches simultaneously.\nParameter sharing is not a novel development in CNNs, but its effect on ensembles has not been studied. Recent related work by Bachman et al. [3] proposed a general framework called pseudo-ensembles for training robust models. They define a pseudo-ensemble as a group of child models which are instances of a parent model perturbed by some noise process. They explicitly encourage correlation in model parameters through the parent by a Pseudo-Ensemble Agreement (PEA) regularizer. Although outwardly related to parameter sharing, pseudo-ensembles are fundamentally different than the techniques presented here, as they use parameter sharing to train a single robust CNN model rather than to produce an ensemble with fewer parameters. Other recent work by Sercu et al. [32] uses parameter sharing in the context of multi-task learning to build a common representation for multilingual translation. Finally, Dropout [17] can be interpreted as a procedure that trains an exponential number of highly related networks and cheaply combines them into one network, similar to PEA\nOne relevant recent work is [17], which briefly focuses on ensembles. Members of this types of ensemble are specialists which are trained on subsets of all possible labels with each subset manually designed to include easily confused labels. These models are fine-tuned from one shared generalist and then combined to make a final prediction. In contrast, our diversity-encouraging loss require no human hand-designing of class specialization – our loss naturally\nallows members to specialize according to subset of classes or pockets of feature space, providing an end-to-end way of learning diverse ensembles."
    }, {
      "heading" : "3. Experimental Design",
      "text" : "We first describe the datasets, architectures, and evaluation metrics that we use in our experiments to better understand ensembling in deep networks."
    }, {
      "heading" : "3.1. Datasets and Architectures",
      "text" : "We evaluate on three popular image classification benchmarks: CIFAR10 [20], CIFAR100 [20], and the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [30]. Since our goal is not to present new designs and architectures for these tasks but rather to study the effect of different ensembling techniques, unless otherwise noted we use standard models and training routines. All models are trained via stochastic gradient descent with momentum and weight decay.\nCIFAR10. For this dataset, we use Caffe “CIFAR10 Quick” [19] network as our base model. The reference model is trained using a batch size of 350 for 5,000 iterations with a momentum of 0.9, weight decay of 0.004, and an initial learning rate of 0.001 which drops to 0.0001 after 4000 iterations. We refer to this network and training procedure as CIFAR10-Quick.\nCIFAR100. We use the Network in Network model by Lin et al. [25] as well as their reference training procedure, which runs for 300,000 iterations with a batch size of 128, momentum of 0.9, weight decay of 0.0001, and an initial learning rate of 0.1. The learning rate decays by a factor of 10 whenever the training loss fails to drop by at least 1% over 20,000 iterations; this occurs twice over the course of a typical training run. Our reference model’s accuracy is about 4% lower than reported in [25], because we do not perform their dataset normalization procedure. We refer to this network and training procedure as CIFAR100-NiN.\nILSVRC2012. For this dataset we use both the Network in Network model [25] and CaffeNet (similar to AlexNet [21]). Both networks are trained for 450,000 iterations with an initial learning rate of 0.1, momentum of 0.9, and weight decay of 0.0005. For NiN the batch size is 128 and the learning rate is reduced by a factor of 10 every 200,000 iterations. For CaffeNet the batch size is 256 and the learning rate schedule is accelerated, reducing every 100,000 iterations. We refer to these models as ILSVRC-NiN and ILSVRC-Alex, respectively."
    }, {
      "heading" : "3.2. Evaluation Metrics",
      "text" : "We evaluate our ensemble performance with respect to two different metrics. Ensemble-Mean Accuracy is the\naccuracy of the “standard” test-time procedure for ensembles – averaging the beliefs of all members and predicting the most confident class. Strong performance on this metric indicates that the ensemble members generally agree on the correct response, with errors reduced by smoothing across members. In contrast, Oracle Accuracy is the accuracy of the ensemble if an “oracle” selects the prediction of the most accurate ensemble member for each example. Oracle Accuracy demonstrates what the ensemble knows as a collection of specialists, and has been used in prior work to measure ensemble performance [4, 6, 13–15]."
    }, {
      "heading" : "4. Random Initialization and Bagging",
      "text" : "We now present our analysis of different approaches to training CNN ensembles. This section focuses on standard approaches, while Sections 5 and 6 present novel ideas on parameter sharing and ensemble-aware losses.\nRandomly initializing network weights and randomly resampling dataset subsets (bagging) are perhaps the most commonly-used methods to create model variation in members of CNN ensembles. Table 1 presents results using three different ensembling techniques: (1) Random Initialization, in which all member models see the same training data but are initialized using different random seeds, (2)Bagging, in which each member uses the same initial weights but trains on a subset of data sampled (with replacement) from the original, and (3) Combined, which uses both techniques. Numbers in the table are accuracies and standard deviations across three trials. The CIFAR ensembles were built with four members while the ILSVRC ensembles had five.\nAs expected, all ensembles improve performance over their single base model. Somewhat surprisingly, we find that bagging reduces Ensemble-Mean Accuracy compared to random initialization alone, while Oracle Accuracy remains nearly constant. This result suggests that the bagged networks are poorly calibrated, such that confident incorrect responses are negatively impacting results. The individual member networks (not shown in table) also perform worse than those trained on the original dataset. We attribute these\nresults to the reduction in unique training exemplars that bagging introduces. Given an initial dataset of M examples from which we draw M points with replacement to make a bagged set B, the probability of an example Xi being sam-\npled at least once is P (Xi ∈ B) = 1 − ( (M−1) M )M . The expected fraction of examples drawn at least once is thus 1 − ( 1− 1M )M , which is approximately 1 − 1/e ≈ 0.63 for large M ; i.e. bagging costs over a third of our unique data points! Not only are we losing 37% of our data, we are also introducing that many duplicated data points. To examine whether these duplicates affect performance, we reran the CIFAR10 experiments with a dataset of 31,500 unique examples (approximately 63% of the original dataset) and found similar reductions in accuracy, indicating that the loss of unique data is the primary negative effect of bagging.\nNote that for convex or shallow models, the loss of unique exemplars in bagging is typically acceptable as random parameter initialization is simply insufficient to produce diversity. To the best of our knowledge, this is the first finding to establish that random initialization may not only be sufficient but preferred over bagging for deep networks given their large parameter space and the necessity of large training data."
    }, {
      "heading" : "5. Parameter Sharing with TreeNets",
      "text" : "Ensembles and single models can be seen as two endpoints on a spectrum of approaches: single models require a careful allocation of parameters to perform well, while ensembles extract as much performance as possible from multiple instances of a base model. Ensemble approaches likely introduce wasteful duplication of parameters in generic lower layers, increasing training time and model size. The hierarchical nature of CNNs makes them well-suited to alternative ensembling approaches where member models benefit from shared information at the lower layers while retaining the advantages of classical ensembling methods.\nMotivated by this observation, in this section we present and evaluate a family of tree-structured CNN ensembles called TreeNets, as shown in Figure 1. A TreeNet is an ensemble consisting of zero or more shared initial layers, followed by a branching point and zero or more independent layers. During training, the shared layers above a branch receive gradient information from each child network, which are accumulated according to back-propagation. At test time, each path from root to leaf can be considered an independent network, except that redundant computations at the shared layers need not be performed.\nWe evaluated our novel TreeNet models on the two larger architectures trained on ImageNet, ILSVRC-Alex and ILSVRC-NiN, and Table 2 presents the results. The table shows the Ensemble-Mean Accuracy (again in terms of means and standard deviations across three trials) achieved\nby TreeNets with splits at different depths. For example, splitting at conv2 means that all layers up to and including conv2 are shared, and all branches are independent afterwards. Since layers that do not contain any parameters (e.g. pooling, nonlinearity) are unaffected by parameter sharing, we only show results for splitting on parameterized layers.\nWe see that shared parameter networks not only retain the performance of full ensembles, but can outperform them. For our best ILSVRC-NiN TreeNet, we improve accuracy over standard ensembles while reducing the parameter count by 7%. It may be that lower layer representations, though simple and generic, still had room for improvement. By sharing low level weights, each weight is updated by multiple sources of supervision, one per branch. This indicates TreeNets could provide regularization which favors slightly better low level representations.\nWe find further evidence for this claim by looking at individual branches of the TreeNet compared to the independently trained networks of the ensemble. Regardless of split point, each TreeNet branch in our shared ensemble achieved around 2 to 3 percentage points higher accuracy than independent ensemble members. Unlike in classical ensembles where each member model performs about as well as the base architecture, TreeNets seem to boost performance of not only the ensemble but the individual networks as well. We also experimented with multiple splits leading to more complicated “balanced binary” tree structures on ILSVRCNiN and found similar improvements.\nWe also tested ILSVRC-Alex TreeNet models trained for\nobject detection on PASCAL VOC 2007 [10] dataset. We used the Fast R-CNN [12] architecture fine-tuned from our TreeNet models. For the test-time bounding-box regression, we average the results from each member model for an ensemble. We found a statistically significant increase in mean average precision of about 0.7% across multiple runs compared to starting from a standard ensemble. We take these initial experiments to imply TreeNet models are at least as generalizable to other tasks as standard ensembles. More details are provided in the supplementary materials.\nTo summarize the key results in this section, we found that TreeNets with a few (typically 1-2) initial layers outperform classical ensembles, while also having fewer parameters which may reduce test-time computation time and memory requirements."
    }, {
      "heading" : "6. Training Under Ensemble-Aware Losses",
      "text" : "In the two previous sections, each ensemble member was trained with the same objective – independent cross-entropy of each ensemble member. What happens if the objective is aware of the ensemble? We begin by showing a surprising result: the first “natural” idea of simply optimizing the performance of the average-beliefs of the ensemble does not work, and we provide intuitions why this is the case (lack of diversity). This negative result shows that a more careful design for ensemble-aware loss functions is crucial. We then propose a diversity-encouraging loss function that shows significantly improved oracle performance."
    }, {
      "heading" : "6.1. Directly Optimizing for Model Averaging",
      "text" : "For a standard ensemble, test-time classification is typically performed by averaging the output of the member networks, so it is natural to explicitly optimize the performance of the corresponding Ensemble-Mean loss during training. We ran all four ensemble architectures under two settings: (1) Score-Averaged, in which we average the last layer outputs (i.e. the scores that are inputs to the softmax function), and (2) Probability-Averaged, in which we average the softmax probabilities of ensemble members. Intuitively, the difference between the two settings is that the former assumes the ensemble members are “calibrated” to produce scores of similar relative magnitudes while the latter does not.\nTable 3 shows the results of these experiments, again averaged over three trials. In all cases, network averaging\nreduced performance, with Probability-Averaged causing greater degradation. This is counter-intuitive: explicitly optimizing for the performance of Ensemble-Mean does worse than averaging independently trained models. We attribute this to two problems, which we now discuss: lack of diversity and numerical instability.\nAveraging Outputs Reduces Diversity. Unfortunately, averaging scores or probabilities during training has the unintended consequence of eliminating diversity in gradients back-propagated through the ensemble. Consider a generic averaging layer,\nµ(x1, ...,xN ) = 1\nN N∑ i=1 xi,\nthat ultimately contributes to some loss `, and consider the derivative of ` with respect to some xi,\n∂` ∂xi = ∂` ∂µ ∂µ ∂xi = ∂` ∂µ 1 N .\nThis expression does not depend on i — gradients backpropagated into all ensemble members are identical! Due to the averaging layer, responsibility for mistakes is shared, which eliminates gradient diversity. This is different from the behavior of an ensemble of independently trained networks, where each member receives a different gradient depending on individual performance. (The averaging also scales the gradients, so in our experiments we compensate by increasing the learning rate by a factor of N ; otherwise, we found learning tended to arrive at even worse solutions.)\nAveraging Probabilities Is Unstable. We attribute the further loss of accuracy when averaging probabilities (versus scores) to increased numerical instability. The softmax function’s derivative with respect to its input is unstable for outputs near 0 or 1. However, when paired with a crossentropy loss, the derivative of the loss with respect to softmax input reduces to a simple subtraction. Unfortunately, there is no similar simplification for cross-entropy over an average of softmax outputs (see supplemental materials for details). Optimization under these conditions is difficult, causing loss at convergence for Probability-Averaged networks to be nearly twice that of Score-Averaged networks, and about the same as a single network.\nMotivated by the finding that decreased diversity from optimizing Ensemble-Mean leads to reduced performance, we next present an explicit diversity-encouraging loss."
    }, {
      "heading" : "6.2. Adding Diversity via Multiple Choice Learning",
      "text" : "We have so far discussed the role of ensemble diversity in the context of model averaging; however, in many settings, generating multiple plausible hypotheses may be preferred to producing a single answer. Ensembles fit naturally into this space as they produce multiple answers by design.\nHowever, independently trained models typically converge to similar solutions, prompting the need to optimize for diversity directly. In this section, we develop and experiment with diversity encouraging losses and demonstrate their effectiveness at specializing ensembles.\nWe build on Multiple Choice Learning (MCL) [13], which we briefly recap here. Consider a set of predictors {θ1, ..., θM} such that θm : x → P where P is a probability distribution over some set of labels, and a dataset D={(x1, y1), ..., (xN , yN )}, where each feature vector xi has a ground truth label yi. From the point of view of an oracle that only listens to the most correct θm, the loss for an example (x, y) is\nLset(x, y) = min m∈[1,M ] ` (θm(x), y) ,\nwhich we will call the oracle set-loss. Intuitively, given that the oracle will select the most correct predictor, the loss on any example is the minimum loss over predictors. Alternatively, the oracle loss can be interpreted as allowing a system to guess M times, scoring an example as correct if any guess is correct. Thus an ensemble of M predictors is directly comparable to the commonly used top-M metric used in many benchmarks (e.g. top-5 in ILSVRC [30]).\nWe adapt this framework to the cross-entropy loss used for training deep classification networks. Given a single predictor θm, the cross-entropy loss for example (x, y) is\n`(x, y) = − log ( pθmy ) ,\nwhere pθmy is the predicted probability of class y. Let αmi be a binary variable indicating whether predictor θm has the lowest loss on example (xi, yi). We can then define a crossentropy oracle set-loss over a dataset D,\nLset(D) = 1 |D| ∑\n(xi,yi)∈D\nM∑ m=1 −αmi log ( pθmyi ) .\nNotice that just like cross-entropy is an upper-bound on training error, this expression is an upper-bound on the oracle training error [13]. Guzman-Rivera et al. [13] presented a coordinate descent algorithm for optimizing such an objective. Their approach alternates between two stages: first, each data point is assigned to its most accurate predictors, and then models are trained until convergence using only the assigned examples.\nEven if done in parallel, training multiple CNNs to convergence for each iteration is intractable. We thus interleave the assignment step with batch updates in stochastic gradient descent. For each batch, we pass the examples through the network, producing probability distributions over the label space from each ensemble member. During the backward pass, the gradient of the loss for each example is computed with respect only to the predictor with the lowest error\non that example (with ties broken randomly). Pseudo-code is available in the supplement.\nSo far we have assumed that the oracle can select only one answer, i.e. ∑ m αmi = 1, however this can easily be generalized to select the k predictors with lowest loss such that ∑ m αmi = k. Varying k from one to the number of predictors trades off between diversity and the number of training examples each predictor sees, which affects both generalization and convergence.\nExperimental results. We begin our experiments with MCL on the CIFAR10-Quick network. Table 4 shows the individual network accuracies and the oracle accuracy for MCL trained ensembles of different values of k. As k is increased, each member network is exposed to more of the data and we see decreased oracle accuracy in exchange for increased individual member performance. At k=4, the oracle-set loss reduces to independent cross-entropy for each member, producing a standard ensemble. The k=1 case showcases the degree of model specialization. Each individual network performs very poorly (accuracy of 19-27%); however, taken as an ensemble the oracle accuracy is over 93%! This clearly shows that the networks have specialized and diversified with each taking responsibility for a subset of examples. To the best of our knowledge, this is the first work to demonstrate such behavior.\nTo further characterize what the MCL member networks are learning, we tracked which test examples are assigned to each ensemble member by the oracle accuracy metric (i.e. which ensemble member has the lowest error on each example). Figure 2(a)-(d) show the distribution of classes assigned to each ensemble member, and the results are striking: at k=1 we see almost complete division of the label space! As k increases we see increased uniformity in these distributions. Note that these divisions emerge from the loss and are not hand-designed or pre-initialized in any way.\nIn Figure 2(e) we visualize how the ensemble members respond to input images using guided backprop [34], which is similar to the deconv visualizations of Zeiler and Fergus [37]. These images can be interpreted as the gradient of the indicated class score with respect to the input image. Features that are clear in these images have the largest influence on the network’s output for a given input image. Each row shows these visualizations for a single input image for a standard network and for members of an MCL ensemble.\nNetworks that have not specialized in the given class are agnostic to the image content. See supplementary material for more examples.\nMCL As Label-Space Clustering. We have shown that k = 1 MCL trained ensembles tend to converge to a labelspace clustering where each member focuses on a subset of the labels. The set of possible label-space clusterings is vast, so to put the MCL results into perspective we train hand-designed specialist ensembles with randomized label assignments. For CIFAR10 we randomly split the labels evenly to the four ensemble members and train each with respect to those labels. Over the course of 100 trials, we found oracle-accuracy ranged from 87.62 to 94.65 with a mean of 91.83. This shows that generally the MCL optimization selects high quality label space clusterings with respect to oracle accuracy.\nAn alternative strategy presented by [17] is to diversify members by dividing labels into clusters of hard to distinguish classes; very briefly described, assignments are generated by clustering the covariance matrix of label scores computed across an input set for a generalist CNN. We trained an ensemble using this clustering method and it led to significantly decreased oracle performance versus MCL on CIFAR10-Quick and ILSVRC-Alex. This is not surprising since they do not optimize for oracle accuracy.\nOvercoming Data Fragmentation. Despite not training member networks to convergence in each iteration of coordinate descent, our method results in improved oracle accuracy over standard ensembles. However, interleaving the assignment step with stochastic gradient descent results in data fragmentation, with each network seeing only a fraction of each batch (as illustrated by the class-specialization). We find this reduced effective batch size results in noisy gra-\ndients that inhibit learning, especially on larger networks. Deep networks are especially sensitive to the effects of data fragmentation early in training when errors (and therefore gradients) are typically larger. In Guzman-Rivera et al. [13], initial assignments for the first iteration of training were decided by clustering the data into k clusters. In contrast, assignments in our approach are based on network performance which is initially the result of random initialization. To investigate the effect of this initial phase of learning, we applied our MCL loss to fine-tune a previously trained CIFAR10-Quick ensemble. As shown in Table 5, the benefits of pretraining are most pronounced for lower values of k where data fragmentation is most severe.\nWhile pretraining did stabilize learning, data fragmentation on CIFAR10 is a relatively minor problem whereas training with MCL from scratch on larger networks using standard batch sizes consistently failed to outperform standard ensembles. We attribute this to a combination of data fragmentation and the difficulty of initial learning. To test this hypothesis, we experimented with fine-tuning and gradient accumulation across batches on the ILSVRC-Alex architecture. We accumulated gradients from 5 batches before updating parameters and fine-tuned from a fully-trained ensemble. Table 6 shows the result of 3000 iterations of this fine-tuning experiment for different values of k. This setup overcame the data fragmentation problem and we see the\nsame trends as in CIFAR10. These experiments demonstrate MCL’s ability to quickly diversify an ensemble. To push this further, we reran the fine-tuning experiment for k=1, this time initializing all ensemble members with the same network. Despite starting from an ensemble of identical networks with an oracle accuracy of 56.90%, the ensemble reached an oracle accuracy of 72.67% after only 3000 iterations!\nWe have demonstrated that the MCL loss is effective at inducing diversity, however the member networks specialize so much that Ensemble-Mean Accuracy suffers. We tried linearly combining the MCL loss with the standard cross-entropy to balance diversity with general performance. We find training under this loss improves CIFAR10 Ensemble-Mean accuracy by 1% over a standard ensemble.\nIn this section we have developed a novel MCL framework and shown it produces ensembles with substantially improved oracle accuracies when training from scratch and even when fine-tuning from a single network."
    }, {
      "heading" : "7. Distributed Ensemble Training",
      "text" : "Training an ensemble on a single GPU is prohibitively expensive, so standard practice for large ensembles is to train the multiple networks either sequentially or in parallel. However, any form of model coupling requires communication between learners. To make enable our experiments at scale, we have developed and will release a modification to Caffe, which we call MPI-Caffe, that uses the Message Passing Interface (MPI) [1] standard to enable crossGPU/machine communication. These communication operations are provided as Caffe model layers, allowing network designers to quickly experiment with distributed networks, where different parts of the model reside on different GPUs and machines. Figure 3 shows how an ensemble of CIFAR10-Quick networks with parameter sharing and model averaging is defined as a single specification and distributed across multiple process. In MPI-Caffe, each process is assigned a identifier (called a rank); by setting the ranks each network layer belongs to, we can easily design distributed ensembles.\nThe MPIBroadcast and MPIGather layers provide the core communication functionality. MPIBroadcast forwards its input to the other processes during a forward\npass and accumulates gradients from each during backpropagation. The forward pass for MPIGather collects all of its inputs from multiple processes and outputs them to a single network, and the backward pass simply routes the gradients back to the corresponding input.\nWe tested our MPI-Caffe framework on a large-scale cluster with one Telsa K20 GPU per node and a maximum MPI node interconnect bandwidth of 5.8 GB/sec. To characterize the communication overhead for an ensemble, we measure the time spent sharing various layers of the ILSVRC-Alex×5 architecture. The largest layer we shared was pool2 which amounts to broadcasting nearly 36 million floats per batch. Despite the layer’s size we find only 0.49% of the forward-backward pass time is used by communication. More details are available in the supplement."
    }, {
      "heading" : "8. Discussion and Conclusion",
      "text" : "There is a running theme behind all of the ideas presented in this paper: diversity. Our experiments on bagging demonstrate that the diversity induced in ensemble members by random parameter initializations is more useful than that introduced by bags with duplicated examples. Our experiment on explicitly training for Ensemble-Mean performance show that averaging beliefs of ensemble members before computing losses has the unintended effect of removing diversity in gradients. Our novel diversity-inducing MCL loss shows that encouraging diversity in ensemble members can significantly improve performance. Finally, our novel TreeNet architecture shows that diversity is important in high-level representations while low-level filters might be better off without it. Training these large-scale architectures is made practical by our MPI-Caffe framework.\nIn future work, we would like to adapt the MCL framework to structured predictions problems. In a structured context where the space of “good” solutions is quite large, we feel diverse models can have an even greater benefit."
    }, {
      "heading" : "Appendix A. TreeNet Object Detection Results on PASCAL VOC 2007",
      "text" : "As briefly described in Section 5 of the main paper, the ILSVRC-Alex TreeNet architecture was also evaluated for object detection using the PASCAL VOC 2007 dataset, which includes labeled ground-truth bounding-box annotations for 20 object classes. For this task, we used Fast R-CNNs [11]. During training, Fast RCNNs finetune a model pretrained on ImageNet for classification under two losses, one over the predicted class of an object proposal, and one with bounding box regression. For our ensembles, we average both the class prediction as well as the bounding box coordinates from ensemble member models.\nTo evaluate TreeNets and standard ensembles, we fine-tune four different instances for each under the Fast R-CNN framework and compute the mean and standard deviation of the classwise average precisions (APs) as well as the mean APs over all classes. Table 7 presents these results for various models with the averaged bounding boxes – a standard ensemble, a TreeNets split after conv1, conv2, and conv3, as well as a single model. We remind the reader that non-parameterized layers are irrelevant with respect to splits so we do not report results for those layers. We also evaluate without the bounding-box regression, instead using the initial selective search proposals directly. Table 8 shows these results.\nIn both tasks we see that TreeNets outperform the standard ensembles and single models by significant margins. We note that we see similar gains in accuracy when using the regressed bounding boxes for both single models and ensembles, implying that the bounding box averaging procedure for ensembles is reasonable."
    }, {
      "heading" : "Appendix B. Instability of Averaged Softmax Outputs",
      "text" : "As discussed in Section 6.1 of the main paper, training under a cross-entropy loss over averaged softmax outputs results in reduced performance compared to both standard ensembles and score-averaged ensembles. We find that this is because averaging softmax outputs prior to the cross-entropy loss has less stable gradients compared to standard cross-entropy over softmax outputs. Let us consider the standard case first and formulate the derivative of the cross-entropy loss with respect to softmax inputs. The cross-entropy loss and softmax function are defined as:\n`(x, y) = − log(py) and py = esy∑ i e si\nThe derivative of the softmax probability py with respect to some score sj is\n∂py ∂sj = py(Ijy − pj)\nwhere Ijy is 1 if y = j and 0 otherwise. This derivative requires multiplying probabilities which can be quite small, leading to underflow errors. Taking the derivative of the cross-entropy loss with respect to some sj results in a more stable solution:\n∂`\n∂sj =\n∂`\n∂py ∂py ∂sj = − 1 py py(Ijy − pj) = pj − Ijy\nLet us now consider the case where py is averaged over M predictors such that\npy = 1\nM ∑ m pmy and p m y = es m y∑ i e smi\nThe derivative of this new py with respect to the score of one predictor smj is then\n∂py ∂smj = ∂py ∂pmy ∂pmy ∂smj = 1 M ∂pmy ∂smj = 1 M pmy (Ijy − pmj )\nAgain computing the derivative of the loss with respect to a score smj we see\n∂`\n∂smj =\n∂`\n∂py ∂py ∂pmy ∂pmy ∂smj = 1 py 1 M pmy (p m j − Ijy) = pmy∑ i p i y (pmj − Ijy)\nThe rightmost term in this result is identical to the standard case presented above; however, the first term acts to weight the gradient for each predictor and can be shown to range from 0 to M. The product of this term and the probability pmj can be prone to underflow errors when pmy is less than py . On the other hand, when p m y is greater than py the gradients are increased in magnitude which can result in overshooting good minima. This scaling of the gradients has an interesting similarity with MCL. If a predictor puts little mass into the correct class compared to the other predictors, the weighting factor and thus the gradient go to zero – meaning worse performing members are less encouraged to improve than strong performers. This is similar behavior to what a soft-assignment variant of MCL might induce. However, we do not notice improved oracle accuracy relative to base ensembles for models trained with probability-averaged losses, implying the predictors are making relatively similar predictions."
    }, {
      "heading" : "Appendix C. Pseudo-code for Stochastic Gradient Descent with MCL",
      "text" : "We describe the classical MCL algorithm and our approach to integrate MCL coordinate descent with stochastic gradient descent in Section 6.2 of the main paper. Here we provide psuedocode for both algorithms to highlight the differences and provide additional clarity.\nData: Dataset (xi, yi) ∈ D and loss L Result: Predictor parameters θ1 · · · θM Initialization: D∗0 = {D1 · · ·DM} ← k-means(D, k = M) t← 0 while D∗t 6= D∗t−1 do Step 1: Train each predictor to completion using only its corresponding subset of the data θm ← Train(Dm)\nStep 2: Reassign each example to its least-loss predictor Dm = {d ∈ D|∀θj ,L(d; θm) ≤ L(d; θj)}\nt← t+ 1 end\nAlgorithm 1: Classical MCL\nData: Dataset (xi, yi) ∈ D, SGD parameters η, λ, and loss L = ∑ i ` Result: Network parameters θ1 · · · θM Initialization:\nRandomly initialize θ1 · · · θM t← 0\nwhile Ltset < Lt−1set do t← t+ 1 Sample batch B ⊂ D Step 1: Forward pass\nFor b ∈ B, compute forward-pass and losses `(b; θ1) · · · `(b; θM ) Partition B by updating indicator variables α as: αmi = 1[[m = argminm′in1:M `(b; theta ′ m)]]\nLm = ∑ i αmi`(bi; θm)\nLtset = ∑ Lm\nStep 2: Backward pass For each θm apply gradient descent update using only the subset of examples on which it achieves the lowest loss θm ← θm − η∇Lm − λ∆θm\nend Algorithm 2: Integrating MCL coordinate descent with SGD steps"
    }, {
      "heading" : "Appendix D. Visualizations for MCL Trained Ensembles",
      "text" : "In this section we present additional insight into how MCL ensemble training differs from the behavior of standard ensembles. To show how the distribution of class examples changes over training for MCL we have produced a video showing the proportion of each CIFAR10 class assigned to each predictor at test time and how it changes over training iterations. The intensity of each class icon is proportional to the fraction of class examples assigned to a predictor. Figure 4 shows a sample early and later frame from the video.\nWe also present additional guided-backprop [34] visualizations described in Section 6.2 of the main paper for different layers in members of traditional and MCL ensembles. These images visualize how the ensemble members respond to input images. These images can be interpreted as the gradient of a neuron output with respect to the input image. Features that are clear in these images have the largest influence on the network’s output for a given input image. Figure 5 shows these visualizations taken for an input image with respect to its true class label. Notice that ensemble members are agnostic to classes that they are not specialized in. The input images are those that produce the highest correct response on the ensemble model. Visualisations of the same neurons in Figure 6 are generated independently for each model using the image that gives the highest activation. We note that while there is a greater response for non-specialized ensemble members, they remain largely indifferent to image content. We see similar patterns of indifference in lower convolutional layer visualizations as well shown in Figures 7, 8, and 9."
    }, {
      "heading" : "Appendix E. MPI-Caffe",
      "text" : "MPI-Caffe is a modification of the popular Caffe deep learning framework that enables cross-GPU/cross-machine communication on MPI enabled systems as model layers. Providing these MPI operations as layers allows network designers the flexibility to quickly experiment with distributed networks while abstracting away much of the communication logic. This enables experimentation with extremely large (i.e. larger than can be held in a single GPU) networks as well as ensembleaware model parallelism schemes. This document explains the function of these layers as well as providing example usage. The core functionality in MPI-Caffe is provided by\n• the MPIBroadcast layer discussed in Section Appendix E.1.1\n• and the MPIGather layer discussed in Section Appendix E.1.2.\nThe primary file defining the interface of the MPI layers is MPILayers.hpp. There are also many supporting modifications in the source that should be noted in case anyone tries to modify or update the base Caffe version. The network initialization code in net.cpp has been substantially altered to accommodate the distributed framework. Some other changes occur in layer.hpp, solver.cpp, and caffe.cpp among others."
    }, {
      "heading" : "Appendix E.1. A Toy Example",
      "text" : "Let’s start with a toy example to build context for the MPI layer descriptions. Suppose we want to train an TreeNet ensemble of CIFAR10-Quick and we want train it under a score-averaged loss. Figure 10 shows how we might modify the LeNet structure using MPI-Caffe to implement this model across three processes/GPUs in an MPI enabled cluster. We will go through this example to explain the function and parametrization of the MPIBroadcast and MPIGather layers."
    }, {
      "heading" : "Appendix E.1.1 MPIBroadcast",
      "text" : "The first layer we discuss is MPIBroadcast (highlighted in red in Figure 10). The MPIBroadcast layer broadcasts a copy of its input blob to each process in its communication group during its forward pass. During the backward pass, the gradients from each copy are summed and passed back to the input blob. The communication group consists of all processes that carry a copy of a particular broadcast layer. By default a communication group contains all processes; however, adding mpi_rank:n rules in either the include or exclude layer parameters can alter this group.\n1 layer{ 2 name: broad 3 type: MPIBroadcast 4 bottom: pool2 5 top: pool2_b 6 mpi_param{ 7 root: 0 8 } 9 include{ 10 mpi_rank: 0 11 mpi_rank: 1 12 mpi_rank: 2 13 } 14 } Process  1   pool2_b  \nProcess  0  \nMPIBroadcast  \npool2  \npool2_b  \nProcess  2  \npool2_b  \nDuring a forward pass, the MPIBroadcast layer on process 0 will send a copy of pool2 to processes 1 and 2 as well as retain a copy for itself. For the example, we would also need to modify the ip1 layer to take pool2_b as input rather than pool2.\nIt is important to note the effect the choice of mpi_param{ root } has on network structure. As shown in the example in Figure 10, each process parses the entire network structure and retains only the layers that include its MPI rank. For process 0, this includes the entire network, but for processes 1 and 2 the network starts with the MPIBroadcast layer. In order to allow this behavior, non-root processes have the input blob (pool2 in our example) stripped out during network parsing. Additionally for this example we need to average these top blobs before sending the result into the softmax loss."
    }, {
      "heading" : "Appendix E.1.2 MPIGather",
      "text" : "If the purpose of a broadcast layer is to take some data and push copies into multiple process spaces, the MPIGather layer can be thought of as the opposite. In a forward pass, it takes multiple copies of a blob from multiple process spaces and collects them in the root process. During a backward pass, the gradients for each top blob are routed back to the corresponding input blob and process. Similar to the previous section, Figure 12 shows the layer definition from out example and a diagram of the forward pass behavior.\nThe mpi_param{root} parameter in the gather layer defines which process will be receiving the gathered blobs and producing top blobs. In analogy to the broadcast layer parsing, gather layers in non-root processes are pruned of the top blobs during network parsing (see Figure 10).\nThere are some restrictions to the gather layer’s use. First, the bottom blob (ip2 in our example) must be defined in all communication group processes. Second, the number of top blobs must equal the number of processes in the communication group. Both of these conditions are checked by the source and will report an error if not satisfied."
    }, {
      "heading" : "Appendix E.2. Notes and Other Examples",
      "text" : "It is worth noting a few other use points about MPI-Caffe:\n• the MPIBroadcast layer can be used to construct a very large single-path network spanned across multiple GPU’s\n• the MPIGather layer can be used to allow more sophisticated ensemble losses\n• there is no limit on the number or order of MPI layers such that complex distributed networks are possible\n• in situations where network latency is lower than reading from disk, the MPIBroadcast layer can be used to train multiple independent networks more quickly"
    }, {
      "heading" : "Appendix E.3. Communication Cost Analysis",
      "text" : "We tested our MPI-Caffe framework on a large-scale cluster with one Tesla K20 GPU per node and a maximum MPI node interconnect bandwidth of 5.8 GB/sec. To characterize the communication overhead for an ensemble, we measure the time spent sharing various layers of the ILSVRC-Alex×5 architecture. Each network was run on a separate node (with one node also holding the shared layers). Figure 13 shows the communication time to share a given layer as a fraction of the forwardbackward pass. The x-axis indicates the number of floats broadcast per batch for each layer. We note that for these layers overhead appears approximately linear and even the largest layer incurs very little overhead for communication."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Convolutional Neural Networks have achieved state-of-<lb>the-art performance on a wide range of tasks. Most bench-<lb>marks are led by ensembles of these powerful learners, but<lb>ensembling is typically treated as a post-hoc procedure im-<lb>plemented by averaging independently trained models with<lb>model variation induced by bagging or random initializa-<lb>tion. In this paper, we rigorously treat ensembling as a first-<lb>class problem to explicitly address the question: what are<lb>the best strategies to create an ensemble? We first compare<lb>a large number of ensembling strategies, and then propose<lb>and evaluate novel strategies, such as parameter sharing<lb>(through a new family of models we call TreeNets) as well as<lb>training under ensemble-aware and diversity-encouraging<lb>losses. We demonstrate that TreeNets can improve ensemble<lb>performance and that diverse ensembles can be trained end-<lb>to-end under a unified loss, achieving significantly higher<lb>“oracle” accuracies than classical ensembles.",
    "creator" : "LaTeX with hyperref package"
  }
}