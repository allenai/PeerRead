{
  "name" : "1302.4967.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Detection of Conflicts in Diagnostic Bayesian Networks Using Abstraction",
    "authors" : [ "Young-Gyun Kim", "Marco Valtorta" ],
    "emails" : [ "ykim@usceast.cs.sc.edu", "mgv@usceast.cs.sc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nDeveloping a diagnostic expert system that uses a Bayesian network as a model of the domain of interest requires several activities, such as knowledge acquisi tion, learning, sensitivity analysis, and conflict check ing [Andreassen et al., 1987; Andersen et al., 1989; Spiegelhalter et al., 1993]. By conflict we mean that the model is no longer valid for the given data [Jensen et. al., 1990; Laskey, 1991]1. In other words, for a given piece of data, we cannot have any confidence in the results coming from the system. Note that this is possible even though Bayesian networks cannot be inherently inconsistent, in the sense that, if their (lo cal) conditional probability tables are consistent, they\n1 The definition of conflict in this paper is not that used in the model-based diagnosis literature, where a conflict is defined as a set of components at least one of which must be faulty.\nalways encode a probability distribution [Pearl, 1988, Chapter 3; Neapolitan, 1990, Theorem 5.2].\nIn practice, what can happen is that, while using a Bayesian network in a diagnostic application, we ob serve that the evidence on which the diagnosis should be based is very unlikely given the network. It may be that the evidence at hand describes a rare case that the network is fully qualified to handle, but it may also be that the network is not equipped to handle the case, and that the low probability of the case is an indication of that. We maintain that it is important for Bayesian network to distinguish between the two situations just described.\nConsider, as a simple example, the use of a system designed to classify geological resources. This system, when presented with data from, say, the moon, would show the data to be very unlikely. A (naive) use of the system would wonder whether this low probability derives from the presence of an unusual sample that the system is built to handle, or whether the system is simply not built to handle the sample. In the example, the latter situation holds. By definition, a geological resource is a resource of the earth, but our hypothetical user's command of the English (or Greek) language is weak enough. It may be disastrous or very expensive to treat the sample as correctly classified. It would be very desirable for the system to indicate to the user the existence of a conflict.\nThe world we live in is filled with countless factors. If we restrict our interest to the medical world, some factors might be age, gender, stress, and so on. If we have a disease, we try to find out why it happened and how to cure it. In other words, the main concern in this world is to find out relationships among factors. Hence, we need a world that has a probability distri bution that represents the relationships among factors. We call this world a model.\nBecause the world is big and complex, models of the world are difficult to design and represent. We use instead a small world in which we ignore some fac tors and some relationship, assumed to be small and unimportant. Every world we can express and use is to be considered a small world. Moreover, the models\nOn the Detection of Conflicts in Diagnostic Bayesian Networks Using Abstraction 363\nwe can manage are always approximated models for a small world, obtained by making some background as sumptions. In certain conditions, called conflict data, that violate the background assumptions, the model may be weak. If the model is presented with conflict data, the results obtained from it are unreliable.\nWhen we observe that the probability of some data is very low in a model, it is very difficult to decide whether the low probability comes from a rare situa tion covered in the model or from data conflict. The best way to solve this problem is to compare the result with the full world model [Laskey and Lehner, 1992; 1994]. Since it is impossible to construct this kind of model, we fall back on the next best option, which is to construct a better model and compare results. But the reason for checking conflict is not only to diagnose the model itself, but also to trigger revision for a better model. In other words, we cannot assume the availabil ity of a better model; if we had it, we would use it in stead of the model for which a conflict is suspected. To overcome this quandary, Laskey [1991] suggests straw models that \"capture some of the expert's intuitions about how the model could go wrong, but are com putationally much simpler than the fully specified al ternate model.\" In particular, a straw model may be an approximated model of a given model. The straw model is structurally poor and less probable than the given model because of the absence of details (factors and relationships) that may be very relevant. How ever, the absence of these factors and relationships can make the straw model more probable when some con flict is caused by them.\nBy comparing a given model with a straw model, a conflict can be detected easily and automatically with out construction of a better model that requires more effort. More concretely, suppose that we obtain a low probability of some findings (evidence) that we suspect to be too low to be happening in a real-life situation. Further suppose that the probability of the findings in the straw model is higher than the probability of the findings in the given model. Then, we can con clude very reliably that the evidence is a conflict for the given model, and we should alert the user of this.\nLaskey [1991] quantifies these issues as follows. (Also see Laskey [1994] and Laskey and Lehner [1992;1994] for related material.) A world is a vector of propo sitional variables and a model can be represented by assigning a probability distribution over the variables in the world. Assume that a small world X is embed ded within a larger world W = (X', lJ. \\Ve use the following notation.\nX: A vector of propositional variables that represents a small world: Each variable X; can take on values in a set x;1, . . . , Xik·\nX': A vector with the same variables as X, but which has additional outcomes in the larger world that are not represented in the small world.\nY: A vector of variables that are not explicitly rep resented in the small world.\n!.: Variables to which a value is assigned (data or evidence variables).\npa(-): Probability distribution encoded in the model (usually, assessed by the expert).\nP0(-): Probability distribution of alternate (straw) model.\nP(·): Probability distribution on the larger world that pa intends to approximate.\nUsing the notation defined above, the global probabil ity distribution P( ·) is what we want to approximate with the distribution pa(-). The following statements and theorem explain the relation between these two probability distributions and how a straw model (with its distribution P0(-)) can be used to detect conflict. Let the proposition q represent the background as sumptions for pa(-):\nq = ((\\ x; E {x;1, . . . , x;k}) !\\ CV Y = JLi) i !L.j\nFor any !. in the small world X, we have:\npa(-) = P(!.l q) = L P(!., JL I q) !!:.\nWhen the model pa(-) is not appropriate, we have:\nP0(-) = P(!.l•q) = L P(!., JL I •q) !!:.\nLet P(q) = 1 - c. The model P(-), restricted to the variables X', can be written:\nTheorem 1 (Surprise Index) [Laskey, 1991]. Let pa(-) and P'(·) be probability distributions over X. Define the index of surprise at evidence Xe under pa(-) relative to ps (-) as: -\nLet 'Irk be the probability under pa that c8 is greater than I\\. Then 'Irk = 2-K.\nWhen pa ( ·) applies, P8 should be much more probable than pa to produce high values of the surprise index. Equivalently, \"high values of conflict are a priori un likely when the assessed model is considered probable. In other words, any alternate model specified a priori is unlikely to fit the data much better than the as sessed model\" [Laskey, 1991, p. 201]. It is therefore reasonable to inform the user that a conflict is possible when the surprise index has a high value.\n364 Kim and Valtorta\n2 THE BIP ARTITE STRAW MODEL\nA bipartite straw model can be obtained by the elim ination of some factors (variables) from the given model2. Elimination of variables is a classic kind of abstraction. (See, e.g., [Mozetic, 1991].) Clearly, the quality of the straw model depends critically on which variables are eliminated, a choice that depends on the domain and on the task addressed by the expert sys tem. Since we are interested in diagnosis, we consider the task of heuristic classification and divide variables into three groups. One is called Target and consists of the variables for which we want to know the prob abilities. The next group is Evidence, and the find ings belong to this group. The last group is called Other, and it includes variables that do not belong to the above two groups. In medical diagnosis the tar get variables might be diseases, the evidence variables may by symptoms, and the variables in Other may be factors such as age or weight3. (See Figure 1 for an illustration.)\nSince we construct the bipartite straw model by elimi nating the Other variables, the straw model consists of the target and evidence variables only. More precisely and formally, letting pa(A) be the parents of node A, we have:\nU = Target U Evidence U Other\n2We could have called this straw model naive Bayes or idiot Bayes model, but we chose a more neutral name for it.\n3While our implementation of the algorithm for con struction of the bipartite straw model does not assume any relative ordering of the three groups of variables, in the di agnostic applications we have in mind the target variables precede the evidence variables, and the Other variables ei ther are setting factors that precede the target variables, or are physiological states that are placed between the target variables and the evidence variables.\nu•traw = Target U Evidence pa(A) =Target 'VA E Evidence\nThen, for each A E Evidence and for each Target, the vector of Target variables, we have:\np•traw(A I Target) = L pgiven(A I finding= Target) \\A\nWe can therefore obtain the entries in the table of conditional probabilities for each evidence variable by belief propagation. Similarly, for all A E Target, the prior probabilities for the straw model are related to probabilities in the given model as follows:\np•traw(A) = L pgiven(A I finding={}) \\A\nThe details of the algorithm used to compute p•traw are beyond the scope of this paper and can be found in [Kim, 1994].\nIn our bipartite straw model, all target variables are parents of each evidence variable, and the target vari ables are conditionally independent of each other ( cf. Figure 1). The variable elimination method makes the straw model weak and structurally poor when the Other variables give a high value of probabilistic cau sation, such as, for example, in the case of old age and neuralgia. But when conflict data is entered, and the conflict is caused by variables in Other, the bipartite straw model is more probable than the given model.\n3 EX AMPLE OF CONSTRUCTION AND USE OF THE BIP ARTITE STR AW MODEL\nAssume a (fictitious) medical diagnosis system for liver and breast cancer represented by the Bayesian network\nOn the Detection of Conflicts in Diagnostic Bayesian Networks Using Abstraction 365\nof Figure 2 with conditional probability tables given in Figure 3.\nThe given model is such that palpation is associated with females and diabetes is associated with males. The findings (Palpation = yes, Diabetes= no) conflict with the assumptions used when building he model. This is well detected by a positive conflict index. Our program gives the following results:\npgiven(Palpation =yes, Diabetes =yes) = 0.0452\npstraw(Palpation =yes, Diabetes =yes) = 0.0619\nTherefore the conflict index c, = log(P•traw j pgiven) is positive and the user is alerted of a conflict.\n4 COMPARISON OF THE INDEP END ENT AND BIP ARTIT E STR AW MODELS\nFinn Jensen et al. [1990] propose a straw model in which all variables are independent of each other and have implemented this straw model to compute the conflict index in HUG IN. Figure 4 illustrates the in dependent straw model. Jensen and his collabora tors assume that, in the absence of conflict, the joint probability of all evidence variables is greater that the product of probabilities of each evidence variable when the given model is applied, i.e., P(x, y) > P(x)P(y). They argue that this is normally the case, because P(xly) > P(x) and P(x, y) = P(xly)P(y). For the independent straw model, the conflict measure is de fined as:\nconf(x, ... , y) = log[(P(x) x . . . x P(y))j P(x, ... , y)], where x, ... , y are the findings. The independent straw model is computationally sim pler than the bipartite straw model, but it may fail to detect conflict when the above assumption is violated, as shown in the following example, which continues the example of the previous section.\nIf we obtain findings (Palpation = yes, X-ray = yes, Diabetes = yes) in the model of Figure 2, the relevant probabilities are:\npgiven(Palpation =yes)= 0.252 pgiven(X- ray= yes) = 0.365 pgiven(Diabetes =yes)= 0.247\npstraw (Palpation = yes, X - ray = yes, Diabetes = yes) = 0.252 x 0.365 x 0.247 = 0.0227\npgive\"(Palpation = yes, X- ray= yes, Diabetes = yes) = 0.0388\nHence, the conflict index c, is c, = log(0.0227/0.0388)\nwhich is negative and indicates absence of conflict.\nOn the other hand, since using the bipartite straw model pstraw(Palpation = yes, X - ray = yes, Diabetes = yes) = 0.0551, the conflict index us ing the bipartite straw model is\nc, = log(0.0551/0.0388) which is positive and indicates presence of conflict.\nWe emphasize that a positive conflict index does not necessarily indicate that the given model is bad; it only alerts the user that assumptions taken in constructing the given model may not be appropriate for the data at hand. In the example, the given model was built with the background assumption that diabetes is associated with males and palpation is associated with females. This assumption is not satisfied in the case at hand.\n366 Kim and Valtorta\n5 CONCLUSIONS\nWe have shown that the proposed bipartite straw model can be constructed automatically by eliminating the variables in Other, as described in the Section enti tled \"The Bipartite Straw Model.\" This construction requires several procedures, which we implemented in the C language under the ULTRIX operating system. The two main algorithms used for the computation of the probabilities needed to obtain the conflict index are Lauritzen and Spiegelhalter's algorithm [1988], as described in [Neapolitan, 1990] and the algorithm by F.V. Jensen, described in [Jensen, 1990] The details of the system we have implemented are given in [Kim, 1994].\nThe main contributions of our research may be sum marized as follows:\n1. We have demonstrated that it is possible to con struct straw models different and sometimes bet ter than the ones that have been presented so far in the literature (independent-type straw models)\n2. These new models (which we call bipartite straw models) can be constructed by abJtraction from diagnostic Bayesian networks (heuristic classifi cation models) with minimal intervention from knowledge engineers or experts, who only need to identify the target and evidence nodes in their models.\n3. To substantiate the claims, we wrote a program and tested it over several examples.\nHere is a list of suggestions for further research.\n1. The bipartite straw model is only one of vari ous possible ones. There may be other methods for designing straw models through various other elimination methods or the use of other (auto matic or semi-automatic) approximations and ab stractions, such as, e.g.:\n• Eliminate variables for which one value is much more probable than the others.\n• Eliminate certain states in variables [Well man, 1994].\n• Combine certain variables into one.\n2. This approach should be tested on more realistic models.\n3. Conditions under which bipartite straw models are better than independent straw models should be investigated. We expect he bipartite straw model to be more accurate than the independent straw model, since it is structurally more complex. This, however, is not necessarily true in all situ ations, because the bipartite straw model drops the Other variables, such as Age in the example of Figure 2.\n4. The conflict (surprise) index should be character ized as a measure, if possible.\n5. The complexity of computing the conditional probability tables for the straw models needs to be analyzed. In particular, conditions under which the computation of the new tables can be done in polynomial time need to be investigated.\nAcknowledgments\nWe are thankful to Professors Finn V. Jensen, John Rose, Juan E. Vargas, and the anonymous referees for their helpful comments. After the paper was reviewed, the authors became aware of Michael Wellman's work on abstraction in Bayesian networks; we thank Pro fessor \\Vellman for general discussions on abstraction. M.V. gratefully acknowledges support from the U.S. Department of Agriculture for the project \"Expert Systems for Agricultural Loans: Collaboration with S.C. State University.\"\nReferences\n[Andreassen et al., 1987] Andreassen, S., M. Wold bye, B. Falck, and S.K. Andersen. \"MUNIN A Causal Probabilistic Network for Interpretation of Electromyographic Findings.\" Proceedings of the Tenth International Joint Conference on Artificial In telligence, Milan, 366-372, 1987.\n[Andersen et al., 1989] Andersen, S.K., K.G. Olesen, F.V. Jensen, and F. Jensen. \"HUGIN-A Shell for Building Bayesian Belief Universes for Expert Sys tems.\" Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, Detroit, 1080- 1085, 1989.\n[Jensen et al., 1990] Jensen, F.V., F. Jensen, and T. Nordhal. \"Analysis in HUGIN of Data Conflict.\" Un certainty in Artificial Intelligence: Proceedings of the Sixth Conference. Mountain View, CA, 1990.\n[Jensen, 1990] Jensen, F.V. \"Calculation in HUGIN of Probabilities for Specific Configurations: A Trick\nOn the Detection of Conflicts in Diagnostic Bayesian Networks Using Abstraction 367\nwith Many Applications.\" Research Report R90-6, Department of Mathematics and Computer Science, University of Aalborg, Denmark, 1990.\n[Kim, 1994] Kim, Young-Gyun. \"Design and Con struction of a New Straw Model in Bayesian Net works.\" M.S. Thesis, Department of Computer Sci ence, University of South Carolina, Columbia, SC 29208, U.S.A.\n[Laskey 1991] Laskey, K.B. \"Conflict and Surprise: Heuristics for Model Revision.\" Uncertainty in Arti ficial Intelligence: Proceedings of the Seventh Confer ence. San Mateo, CA: Morgan-Kaufmann, 197-204, 1991.\n[Laskey, 1994] Laskey, K.B. \"Bounded Rationality and Search over Small-World Models.\" International Jour nal of Approximate Reasoning, 11, 361-384, 1994.\n[Laskey and Lehner, 1992] Laskey, K.B. and P.E. Lehner. \"Some Maxims for Small Worlds.\" Working Notes of the AAAI-92 Workshop on Knowledge-Based Construction of Probabilistic and Decision Models, 89- 94, 1992.\n[Laskey and Lehner, 1994] Laskey, K.B. and P.E. Lehner. \"Metereasoning and the Problem of Small Worlds.\" IEEE Transactions on Systems, Man, and Cybernetics, 24, 11, 1643-1652, November 1994.\n[Mozetic, 1991) Mozetic, I. \"Hierarchical Model-Based Diagnosis.\" International Journal of Man-Machine Studies, 35, 3, 329-362, 1991.\n[Neapolitan, 1990] Neapolitan, R.E. Probabilistic Rea soning in Expert Systems. New York: John Wiley and Sons, 1990.\n[Pearl, 88] Pearl, Judea. Probabilistic Reasoning in In telligent Systems, San Mateo, CA: Morgan Kaufmann, 1988.\n[Spiegelhalter et al., 1992] Spiegelhalter, D.J., A.P. Dawid, S.L. Lauritzen, and R.G. Cowell. \"Bayesian Analysis in Expert Systems.\" Statistical Science, 8 , 3, 219-283, 1993.\n[Wellman, 1994] Wellman, M.P. and C.-L. Liu. \"State Space Abstraction for Any-Time Evaluation of Prob abilistic Networks.\" Uncertainty in Artificial Intelli gence: Proceedings of the Tenth Conference. San Ma teo, CA: Morgan-Kaufmann, 567-574."
    } ],
    "references" : [ {
      "title" : "MUNIN­ A Causal Probabilistic Network for Interpretation of Electromyographic Findings.",
      "author" : [ "Andreassen et al", "S. 1987] Andreassen", "M. Wold­ bye", "B. Falck", "S.K. Andersen" ],
      "venue" : "Proceedings of the Tenth International Joint Conference on Artificial In­",
      "citeRegEx" : "al. et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1987
    }, {
      "title" : "HUGIN-A Shell for Building Bayesian Belief Universes for Expert Sys­ tems.",
      "author" : [ "Andersen et al", "S.K. 1989] Andersen", "K.G. Olesen", "F.V. Jensen", "F. Jensen" ],
      "venue" : "Proceedings of the Eleventh International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1989
    }, {
      "title" : "Nordhal. \"Analysis in HUGIN of Data Conflict.\" Un­ certainty in Artificial Intelligence",
      "author" : [ "Jensen et al", "F.V. 1990] Jensen", "F. Jensen" ],
      "venue" : "Proceedings of the Sixth Conference. Mountain View,",
      "citeRegEx" : "al. et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1990
    }, {
      "title" : "Calculation in HUGIN of Probabilities for Specific Configurations: A Trick",
      "author" : [ "F.V. Jensen" ],
      "venue" : null,
      "citeRegEx" : "Jensen,? \\Q1990\\E",
      "shortCiteRegEx" : "Jensen",
      "year" : 1990
    }, {
      "title" : "Conflict and Surprise: Heuristics for Model Revision.\" Uncertainty in Arti­",
      "author" : [ "K.B. Laskey" ],
      "venue" : "ficial Intelligence: Proceedings of the Seventh Confer­ ence",
      "citeRegEx" : "Laskey,? \\Q1991\\E",
      "shortCiteRegEx" : "Laskey",
      "year" : 1991
    }, {
      "title" : "Some Maxims for Small Worlds.\" Working Notes of the AAAI-92 Workshop on Knowledge-Based Construction of Probabilistic and Decision Models",
      "author" : [ "K.B. Laskey", "P.E. Lehner" ],
      "venue" : null,
      "citeRegEx" : "Laskey and Lehner.,? \\Q1992\\E",
      "shortCiteRegEx" : "Laskey and Lehner.",
      "year" : 1992
    }, {
      "title" : "Metereasoning and the Problem of Small Worlds.",
      "author" : [ "K.B. 1994] Laskey", "P.E. Lehner" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics,",
      "citeRegEx" : "Laskey and Lehner.,? \\Q1994\\E",
      "shortCiteRegEx" : "Laskey and Lehner.",
      "year" : 1994
    }, {
      "title" : "Bayesian Analysis in Expert Systems.",
      "author" : [ "D.J. 1992] Spiegelhalter", "A.P. Dawid", "S.L. Lauritzen", "R.G. Cowell" ],
      "venue" : null,
      "citeRegEx" : "Spiegelhalter et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Spiegelhalter et al\\.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Such a situation may be detected by the method of straw models, first presented by Jensen et al. [1990] and later generalized and justified by Laskey [1991].",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Such a situation may be detected by the method of straw models, first presented by Jensen et al. [1990] and later generalized and justified by Laskey [1991]. We describe an algorithm, which we have implemented, that takes as input an annotated diagnostic Bayesian net­ work (the base model) and constructs, with­ out assistance, a bipartite network to be used as a straw model.",
      "startOffset" : 83,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "By conflict we mean that the model is no longer valid for the given data [Jensen et. al., 1990; Laskey, 1991]1.",
      "startOffset" : 73,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "The best way to solve this problem is to compare the result with the full world model [Laskey and Lehner, 1992; 1994].",
      "startOffset" : 86,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "The best way to solve this problem is to compare the result with the full world model [Laskey and Lehner, 1992; 1994]. Since it is impossible to construct this kind of model, we fall back on the next best option, which is to construct a better model and compare results. But the reason for checking conflict is not only to diagnose the model itself, but also to trigger revision for a better model. In other words, we cannot assume the availabil­ ity of a better model; if we had it, we would use it in­ stead of the model for which a conflict is suspected. To overcome this quandary, Laskey [1991] suggests straw models that \"capture some of the expert's intuitions about how the model could go wrong, but are com­ putationally much simpler than the fully specified al­ ternate model.",
      "startOffset" : 87,
      "endOffset" : 599
    }, {
      "referenceID" : 4,
      "context" : "Theorem 1 (Surprise Index) [Laskey, 1991].",
      "startOffset" : 27,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "Finn Jensen et al. [1990] propose a straw model in which all variables are independent of each other and have implemented this straw model to compute the conflict index in HUG IN.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "Jensen, described in [Jensen, 1990] The details of the system we have implemented are given in [Kim, 1994].",
      "startOffset" : 21,
      "endOffset" : 35
    } ],
    "year" : 2011,
    "abstractText" : "An important issue in the use of expert sys­ tems is the so-called brittleness problem. Ex­ pert systems model only a limited part of the world. While the explicit management of uncertainty in expert systems mitigates the brittleness problem, it is still possible for a system to be used, unwittingly, in ways that the system is not prepared to address. Such a situation may be detected by the method of straw models, first presented by Jensen et al. [1990] and later generalized and justified by Laskey [1991]. We describe an algorithm, which we have implemented, that takes as input an annotated diagnostic Bayesian net­ work (the base model) and constructs, with­ out assistance, a bipartite network to be used as a straw model. We show that in some cases this straw model is better that the in­ dependent straw model of Jensen et al., the only other straw model for which a construc­ tion algorithm has been designed and imple­ mented.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}