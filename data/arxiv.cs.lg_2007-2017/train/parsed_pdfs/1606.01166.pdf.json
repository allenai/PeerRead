{
  "name" : "1606.01166.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalizing the Convolution Operator to Extend CNNs to Irregular Domains",
    "authors" : [ "Jean-Charles Vialatte", "Vincent Gripon", "Grégoire Mercier" ],
    "emails" : [ "1jean-charles.vialatte@cityzendata.com", "gregoire.mercier}@telecom-bretagne.eu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "CNNs[1] are state-of-the-art for performing supervised learning on data defined on lattices. Contrary to classical multilayer perceptrons (MLPs) [2], CNNs take advantage of the underlying structure of the inputs. When facing data defined on irregular domains, CNNs cannot always be directly applied even if the data may present an exploitable underlying structure.\nAn example of such data are spatio-temporal time series generated by a set of Internet of Things devices. They typically consist of datapoints irregularly spaced out on a Euclidean space. As a result, a graph G can be defined where the vertices are the devices and the edges connect neighbouring ones. Other examples include signals on graphs including brain imaging, transport networks, bag of words graphs. . . In all these examples, a signal can generally be seen as a vector in Rd where d is the order of the graph. As such, each vertex is associated with a coordinate, and the edges weights represent some association between the corresponding vertices.\nDisregarding the graph G, MLPs can be applied on these datasets. We are interested in defining convolutional operators that are able to exploit G as well as a possible embedding Euclidean space. Our motivation is to imitate the gain of performance allowed by CNNs over MLPs on irregular domains.\nIn graph signal processing, extended convolutional operators exploiting G have been proposed [3]. These operators have been applied to deep learning [4] and obtain performance similar to CNNs on regular domains, despite the fact they differ from classical convolutions. However, for slightly distorted domains, the obtained operators become non-localized, thus failing taking account of the underlying structure.\nar X\niv :1\n60 6.\n01 16\n6v 1\n[ cs\n.L G\n] 3\nJ un\nWe propose another approach that generalizes convolutional operators by taking into account G. Namely, we make sure that the proposed solution has properties inherent to convolutions: linearity, locality and kernel weight sharing. We then apply it directly to the graph structure. We use it as a substitution of a convolutional layer in a CNN and stress our resulting technique on comparative benchmarks, showing a significant improvement compared with a MLP. The obtained operator happens to exactly be the classical convolutional one when applied on regular domains.\nThe outline of the paper is as follows. In Section 2, we discuss about related work. In Section 3 we introduce our proposed operator. In Section 4 we explain how to apply it to CNN-like structures. Section 5 contains the experiments. Section 6 is a conclusion."
    }, {
      "heading" : "2 Related Works",
      "text" : "For graph-structured data, Bruna et al. [4] have proposed an extension of the CNN using the graph signal processing theory [3]. Their convolution is defined in the spectral domain related to the Laplacian matrix of the graph. As such, in the case where the graph is a lattice, the construction is analogous to the regular convolution defined in the Fourier domain. The operation is defined as spectral multipliers obtained by smooth interpolation of a weight kernel, and they explain how it ensures the localization property. In this paper, they also define a construction that creates a multi-resolution structure of a graph, for allowing it to support a deep learning architecture. Henaff et al. [5] have extended Bruna’s spectral network to large scale classification tasks and have proposed both supervised and unsupervided methods to find an underlying graph structure when it isn’t already given.\nHowever, when the graph is irregular, they partly losse the localization property of their convolution is partially lost. As the spectral domain is undirected with respect to the graph domain, some sort of rotation invariances are also introduced. Hence, the results in the graph domain aren’t ressembling to a convolution. In our case, we want to define a convolution supported locally. Moreover, we want that every input can be defined on a different underlying graph structure, so that the learnt filters can be applied on data embedded in the same space regardless of what structure have supported the convolution during the training.\nThese properties are also retained by the convolution defined in the ShapeNet paper[6], which defines a convolution for data living on non-euclidean manifolds. I.e their construction does maintain the locality and allow for reusing the learnt filters on other manifolds. However, it requires at least a manifold embedding of the data and a geodesic polar coordinates system. Although being less specific, our proposed method will be a strict generalization of CNN in the sense that CNNs are a special case of it.\nIn the case where the data is sparse, Graham [7] has proposed a framework to implement a spatially sparse CNN efficiently. If the underlying graph structure is embedded into an Euclidean space, the convolution we propose in this paper can be seen as spatially sparse too, in the sense that the data has only non-zero coordinates on the vertices of the graph it is defined on. In our case we want to define a convolution for which the inputs can have values on any point of the embedding space, whereas in the regular case inputs can only have values on vertices of an underlying grid."
    }, {
      "heading" : "3 Proposed convolution operator",
      "text" : ""
    }, {
      "heading" : "3.1 Definitions",
      "text" : "Let S be a set of points, such that each one defines a set of neighbourhoods.\nAn entry e of a dataset D is a column vector that can be of any size, for which each dimension represents a value taken by e at a certain point u ∈ S. u is said to be activated by e. A point u can be associated to at max one dimension of e. If it is the ith dimension of e, then we denote the value taken by e at u by either eu or ei. D is said to be embedded in S. We say that two entries e and e′ are homogeneous if they have the same size and if their dimensions are always associated to the same points.\n3.2 Formal description of the generalized convolution\nLet’s denote by C a generalized convolution operator.We want it to observe the following conditions:\n• Linearity • Locality • Kernel weight sharing\nAs C must be linear, then for any entry e, there is a matrix W e such that C(e) = W ee. Note that unless the entries were all homogeneous, W e depends on e.\nIn order to meet the locality condition, we first want that the coordinates of C(e) have a local meaning. To this end, we impose that C(e) lives in the same space than e, and that C(e) and e are homogeneous. Secondly, for each u, we want C(e)u to be only function of values taken by e at points contained in a certain neighbourhood of u. It results that lines of W e are generally sparse.\nLet’s attribute to C a kernel of n weights in the form of a row vector w = (w1, w2, .., wn), and let’s define the set of allocation matrices A as the set of binary matrices that have at most one non-zero coordinate per column. As C must share its weights across the activated points, then for each row W ei of W\ne, there is an allocation matrix Aei ∈ A such that W ei = wAei . To maintain locality, the jth column of Aei must have a non-zero coordinate if and only if the ith and jth activated points are in a same neighbourhood.\nLet’s Ae denotes the block column vector that has the matrices Aei for attributes, and let’s ⊗ denotes the tensor product. Hence, C is defined by a weight kernel w and an allocation map e 7→ Ae that maintains locality, such that\nC(e) = (w ⊗Ae) . e (1)"
    }, {
      "heading" : "3.3 Generalized convolution supported by an underlying graph",
      "text" : "As vertices, the set of activated points of an entry e defines a complete oriented graph that we call Ge. If all the entries are homogeneous, then Ge is said to be static. In this case, we note G.\nSuppose we are given u 7→ Vu which maps each u ∈ S to a neighbourhood Vu. We then define GeV as the subgraph of Ge such that it contains the edge (u′, u) if and only if u′ ∈ Vu. Let aV : e 7→ Ae be an allocation map such that the jth column of Aei have a non-zero coordinate if and only if (ei, ej) is an edge of GeV .\nThen the generalized convolution of e by the couple (w, aV) is supported by the underlying graph GeV in the sense that W e V = w ⊗ aV(e) is its adjacency matrix. Note that the underlying graph of a regular convolution is a lattice and its adjacency matrix is a Toëplitz matrix.\nAlso note that the family (Vu)u∈S can be seen as local receptive fields for the generalized convolution, and that the map aV can be seen as if it were distributing the kernel weights into each Vu.\nRemarks\nThe generalized convolution has been defined here in view of being applied to the CNN paradigm, i.e. as an operation between a kernel weight and a vector. In the context of graph signal processing, if G = (E, V ) denotes a graph, then such operator ∗ with respect to the 3D tensor A between two graph signals f, g ∈ RE would have been defined as:\nf ∗ g = (f> ⊗A) . g (2) ∀v ∈ V, (f ∗ g)(v) = f>Av g, with Av ∈ A (3)\nNote that in our context, the underlying graph depends on the entry. So that if the generalized convolution is placed inside a deep neural network architecture (see section 4), then the learnt filter w will be reusable regardless of the entry’s underlying structure."
    }, {
      "heading" : "3.4 Example of a generalized convolution shaped by a rectangular window",
      "text" : "Let E be a two-dimensional Euclidean space and let’s suppose here that S = E. Let’s denote by CR, a generalized convolution shaped by a rectangular windowR. We suppose that its weight kernel w is of size Nw = (2p+ 1)(2q + 1) and thatR is of width (2p+ 1)µ and height (2q + 1)µ, where µ is a given unit scale.\nLetGep,q be the subgraph ofG e such that it contains the edge (u′, u) if and only if |u′x−ux| ≤ (p+ 12 )µ and |u′y − uy| ≤ (q + 12 )µ. In other terms, Gep,q connects a vertex u to every vertex contained inR when centered on u.\nThen, we define CR as being supported by Gep,q . As such, its adjacency matrix acts as the convolution operator. At this point, we still need to affect the kernel weights to its non-zero coordinates, via the edges of Gep,q. This amounts for defining explicitely the map e 7→ Ae for CR. To this end, let’s consider the grid of same size as that rectangle, which breaks it down into (2p+ 1)(2q + 1) squares of side length µ, and let’s associate a different weight to each square. Then, for each edge (u′, u), we affect to it the weight associated with the square within which u′ falls when the grid is centered on u. This procedure allows for the weights to be shared across the edges. It is illustrated on figure 1.\nNote that if the entries are homogeneous and the activated points are vertices of a regular grid, then the matrix W , independant of e, is a Toëpliz matrix which acts as a regular convolution operator on the entries e. In this case, CR is just a regular convolution. For example, this is the case in section 3.5."
    }, {
      "heading" : "3.5 Link with the standard convolution on image datasets",
      "text" : "Let D be an image dataset. Entries of D are homogeneous and their dimensions represent the value at each pixel. In this case, we can set S = E, of dimension 2, such that each pixel is located at entire coordinates. More precisely, if the images are of width n and height m, then the pixels are located at coordinates (i, j) ∈ {0, 1, 2, ..., n}x{0, 1, 2, ...,m}. Hence, the pixels lie on a regular grid and thus are spaced out by a constant distance µ = 1.\nLet’s consider the static underlying graph Gp,q and the generalized convolution by a rectangular window CR, as defined in the former section. Then, applying the same weight allocation strategy will lead to affect every weight of the kernel into the moving windowR. Except on the border, one and only one pixel will fall into each square of the moving grid at each position, as depicted in figure 2. Indeed,R behaves exactly like a moving window of the standard convolution, except that it considers that the images are padded with zeroes on the borders."
    }, {
      "heading" : "4 Application to CNNs",
      "text" : ""
    }, {
      "heading" : "4.1 Neural network interpretation",
      "text" : "Let Ld and Ld+1 be two layers of neurons, such that forward-propagation is defined from Ld to Ld+1. Let’s define such layers as a set of neurons being located in S. These layers must contain as many neurons as points that can be activated. In other terms, S ∼= Ld ∼= Ld+1. As such, we will abusively use the term of neuron instead of point.\nThe generalized convolution between these two layers can be interpreted as follow. An entry e activates the same N neurons in each layer. Then, a convolution shape takes N positions onto Ld, each position being associated to one of the activated neurons of Ld+1. At each position, connections are drawn from the activated neurons located inside the convolution shape in destination to the associated neuron. And a subset of weights from w are affected to these connections, according to a weight sharing strategy defined by an allocation map. Figure 3 illustrates a convolution shaped by a rectangular window.\nThe forward and backward propagation between Ld and Ld+1 are applied using the described neurons and connections. After a generalized convolution operation, an activation function is applied on the output neurons. Then a pooling operation is done spatially: the input layer is divided into patches of same size, and all activated neurons included in this patch are pooled together. Unlike a standard pooling operation, the number of activated neurons in a patch may vary.\nIndeed, generalized convolution layers can be vectorized. They can have multiple input channels and multiple feature maps. They shall naturally be placed into the same kind of deep neural network structure than in a CNN. Thus, they are for the irregular input spaces what the standard convolution layers are for regular input spaces."
    }, {
      "heading" : "4.2 Implementation",
      "text" : "There are two main strategies to implement the propagations. The first one is to start from (1), derive it and vectorize it. It implies handling semi-sparse representations to minimize memory consumption and to use adequate semi-sparse tensor products.\nInstead, we decide to use the neural network interpretation and the underlying graph structure whose edges amount for neurons connections. By this mean, the sparse part of the computations is included via the use of this graph. Also, computations on each edge can be parallelized."
    }, {
      "heading" : "4.3 Forward and back propagation formulae",
      "text" : "Let’s first recall the propagation formulae from a neural network point of view.\nLet’s denote by eu the value of a neuron of Ld located at u ∈ S, by fv for a neuron of Ld+1, and by gv if this neuron is activated by the activation function σ.\nWe denote by prev(v) the set neurons from the previous layer connected to v, and by next(u) those of the next layer connected to u. wuv is the weight affected to the connection between neurons u and v. b is the bias term associated to Ld+1. After the forward propagation, values of neurons of Ld+1 are determined by those of Ld:\nfv = ∑\nu∈prev(v)\neuwuv (4)\ngv = σ(fv + b) (5)\nThanks to the chain rule, we can express derivatives of a layer with those of the next layer:\nδv = ∂E ∂fv = ∂E ∂gv ∂gv ∂fv = ∂E ∂gv σ ′ (fv + b) (6)\n∂E ∂eu = ∑ v∈next(u) ∂E ∂fv ∂fv ∂eu = ∑ v∈next(u) δvwuv (7)\nWe call edges(w) the set of edges to which the weight w is affected. If ω ∈ edges(w), fω+ denotes the value of the destination neuron, and eω− denotes the value of the origin neuron.\nThe back propagation allows to express the derivative of any weight w:\n∂E ∂w = ∑ ω∈edges(w) ∂E ∂fω+ ∂fω+ ∂w = ∑ ω∈edges(w) δω+eω− (8)\n∂E ∂b = ∑ v ∂E ∂gv ∂gv ∂b = ∑ v δv (9)\nThe sets prev(v), next(u) and edges(w) are determined by the graph structure, which in turn is determined beforehand by a procedure like the one described in section 3.4."
    }, {
      "heading" : "4.4 Vectorization",
      "text" : "Computations are done per batch of entries B. Hence, the graph structure used for the computations must contain the weighted edges of all entries e ∈ B. If necessary, entries of B are made homogeneous: if a neuron u is not activated by an entry e but is activated by another entry of B, then eu is defined and is set to zero.\nThe 3D tensor counterparts of e, f , and g are thus denoted by E, F and G. Their third dimension indexes the channels (input channels or feature maps). Their submatrix along the neuron located at x ∈ S are denoted Ex, Fx and Gx, rows are indexing entries and columns are indexing channels. The counterparts of w and b are W and β. The first being a 3D tensor and the second being a vector with one value per feature map. β̃ denotes the 3D tensor obtained by broadcasting β along the two other dimensions of F. Ww denotes the submatrix of W along the kernel weight w. Its rows index the feature maps and its columns index the input channels.\nWith these notations, the vectorized counterparts of the formulae from section 4.3 can be obtained in the same way:\nFv = ∑\nu∈prev(v)\nEuW > wuv (10)\nG = σ(F + β̃) (11)\n∆ =\n( ∂E\n∂F\n) = ( ∂E\n∂G ) ◦ σ′(F + β̃) (12)(\n∂E\n∂E ) u = ∑\nv∈next(u) ∆vWuv (13)(\n∂E\n∂W ) w = ∑\nω∈edges(w)\n∆>ω+Eω− (14)\n∂E ∂β = ∑ j (∑ v ∆v ) j-th column\n(15)\nWhere ◦ and > respectively denotes Hadamard product and transpose."
    }, {
      "heading" : "5 Experiments",
      "text" : "In order to measure the gain of performance allowed by the generalized CNN over MLP on irregular domains, we made a series of benchmarks on distorded versions of the MNIST dataset [8], consisting of images of 28x28 pixels. To distort the input domain, we plunged the images into a 2-d euclidean space by giving entire coordinates to pixels. Then, we applied a gaussian displacement on each pixel, thus making the data irregular and unsuitable for regular convolutions. For multiple values of standard deviation of the displacement, we trained a generalized CNN and compared it with a MLP that has the same number of parameters. We choosed a simple yet standard architecture in order to better see the impact of generalized layers.\nThe architecture [1] used is the following: a generalized convolution layer with relu [9] and max pooling, made of 20 feature maps, followed by a dense layer and a softmax output layer. The generalized convolution is shaped by a rectangular window of width and height 5µ where the unit scale µ is chosen to be equal to original distance between two pixels. The max-pooling is done with square patches of side length 2µ. The dense layer is composed of 500 hidden units and is terminated by relu activation as well. In order to have the same number of parameters, the compared MLP have 2 dense layers of 500 hidden units each and is followed by the same output layer. For training, we used stochastic gradient descent [10] with Nesterov momentum [11] and a bit of L2 regularization [12].\nThe plot drawn on figure 4 illustrates the gain of performance of a generalized convolutional layer over a dense layer with equal number of parameters. After 3 epochs for both, it shows that the generalized CNN on a distorded domain performs better than the MLP. Even with a domain distortion of 200µ, the former still performs significantly better than the latter."
    }, {
      "heading" : "6 Conclusion and future work",
      "text" : "In this paper, we have defined the generalized convolution operator. This operator makes possible to transport the CNN paradigm to irregular domains. It retains the proprieties of a regular convolutional operator. Namely, it is linear, supported locally and uses the same kernel of weights for each local operation. The generalized convolution operator can then naturally be used instead of convolutional layers in a deep learning framework. Typically, the created model is well suited for input data that has an underlying graph structure.\nThe definition of this operator is flexible enough for it allows to adapt it weight-allocation map to any input domain, so that depending on the case, the distribution of the kernel weight can be done in a way that is natural for this domain. However, in some cases, there is no natural way but multiple acceptable methods to define the weight allocation. In further works, we plan to study these methods. We also plan to apply the generalized operator on unsupervised learning tasks."
    }, {
      "heading" : "Acknowledgements",
      "text" : "I would like to thank my academic mentors, Vincent Gripon and Grégoire Mercier who helped me in this work, as well as my industrial mentor, Mathias Herberts who gave me insights in view of applying the designed model to industrial datasets. This work was partly funded by Cityzen Data, the company behind the Warp10 platform, and by the ANRT (Agence Nationale de la Recherche et de la Technologie) through a CIFRE (Convention Industrielle de Formation par la REcherche), and also by the European Research Council under the European Union’s Seventh Framework Program (FP7/2007-2013) / ERC grant agreement number 290901."
    } ],
    "references" : [ {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "K. Hornik", "M. Stinchcombe", "H. White" ],
      "venue" : "Neural networks, vol. 2, no. 5, pp. 359–366, 1989.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
      "author" : [ "D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 30, no. EPFL-ARTICLE-189192, pp. 83–98, 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1891
    }, {
      "title" : "Spectral networks and locally connected networks on graphs",
      "author" : [ "J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1312.6203, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep convolutional networks on graph-structured data",
      "author" : [ "M. Henaff", "J. Bruna", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1506.05163, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Shapenet: Convolutional neural networks on non-euclidean manifolds",
      "author" : [ "J. Masci", "D. Boscaini", "M. Bronstein", "P. Vandergheynst" ],
      "venue" : "tech. rep., 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Spatially-sparse convolutional neural networks",
      "author" : [ "B. Graham" ],
      "venue" : "arXiv preprint arXiv:1409.6070, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Y. LeCun", "C. Cortes", "C.J. Burges" ],
      "venue" : "1998.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, pp. 315–323, 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "Proceedings of COMPSTAT’2010, pp. 177–186, Springer, 2010. 8",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton" ],
      "venue" : "Proceedings of the 30th international conference on machine learning (ICML-13), pp. 1139–1147, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Feature selection, l 1 vs. l 2 regularization, and rotational invariance",
      "author" : [ "A.Y. Ng" ],
      "venue" : "Proceedings of the twenty-first international conference on Machine learning, p. 78, ACM, 2004. 9",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "CNNs[1] are state-of-the-art for performing supervised learning on data defined on lattices.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "Contrary to classical multilayer perceptrons (MLPs) [2], CNNs take advantage of the underlying structure of the inputs.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "In graph signal processing, extended convolutional operators exploiting G have been proposed [3].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "These operators have been applied to deep learning [4] and obtain performance similar to CNNs on regular domains, despite the fact they differ from classical convolutions.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "[4] have proposed an extension of the CNN using the graph signal processing theory [3].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[4] have proposed an extension of the CNN using the graph signal processing theory [3].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "[5] have extended Bruna’s spectral network to large scale classification tasks and have proposed both supervised and unsupervided methods to find an underlying graph structure when it isn’t already given.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "These properties are also retained by the convolution defined in the ShapeNet paper[6], which defines a convolution for data living on non-euclidean manifolds.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "In the case where the data is sparse, Graham [7] has proposed a framework to implement a spatially sparse CNN efficiently.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "In order to measure the gain of performance allowed by the generalized CNN over MLP on irregular domains, we made a series of benchmarks on distorded versions of the MNIST dataset [8], consisting of images of 28x28 pixels.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "The architecture [1] used is the following: a generalized convolution layer with relu [9] and max pooling, made of 20 feature maps, followed by a dense layer and a softmax output layer.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "The architecture [1] used is the following: a generalized convolution layer with relu [9] and max pooling, made of 20 feature maps, followed by a dense layer and a softmax output layer.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "For training, we used stochastic gradient descent [10] with Nesterov momentum [11] and a bit of L2 regularization [12].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "For training, we used stochastic gradient descent [10] with Nesterov momentum [11] and a bit of L2 regularization [12].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "For training, we used stochastic gradient descent [10] with Nesterov momentum [11] and a bit of L2 regularization [12].",
      "startOffset" : 114,
      "endOffset" : 118
    } ],
    "year" : 2017,
    "abstractText" : "Convolutional Neural Networks (CNNs) have become the state-of-the-art in supervised learning vision tasks. Their convolutional filters are of paramount importance for they allow to learn patterns while disregarding their locations in input images. When facing highly irregular domains, generalized convolutional operators based on an underlying graph structure have been proposed. However, these operators do not exactly match standard ones on grid graphs, and introduce unwanted additional invariance (e.g. with regards to rotations). We propose a novel approach to generalize CNNs to irregular domains using weight sharing and graph-based operators. Using experiments, we show that these models resemble CNNs on regular domains and offer better performance than multilayer perceptrons on distorded ones.",
    "creator" : "LaTeX with hyperref package"
  }
}