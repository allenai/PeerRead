{
  "name" : "1511.06333.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "fessler}@umich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n06 33\n3v 1\n[ cs\n.L G\n] 1\n9 N\nov 2"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "The sparsity of natural signals and images in a transform domain or dictionary has been extensively exploited in several applications such as compression, denoising, compressed sensing and other inverse problems. More recently, the data-driven adaptation of synthesis dictionaries called dictionary learning, has shown promise in applications (Elad & Aharon, 2006; Mairal et al., 2009; Ravishankar & Bresler, 2011) compared to analytical dictionaries such as wavelets or DCT.\nGiven a collection of training signals {yi} N\ni=1 that are represented as columns of the matrix Y ∈ R n×N , the dictionary learning problem is often formulated as follows (Aharon et al., 2006)\n(P0) min D,X\n‖Y −DX‖ 2\nF s.t. ‖xi‖0 ≤ s ∀ i, ‖dk‖2 = 1 ∀ k.\nHere, dk and xi denote the columns of the dictionary D ∈ Rn×K and sparse code matrix X ∈ RK×N , respectively, and s denotes the maximum sparsity level (non-zeros in representations xi) allowed for each training signal. The ℓ0 “norm” counts the number of non-zero entries in a vector. The columns of the dictionary are constrained to have unit norm in (P0) to avoid the scaling ambiguity (Gribonval & Schnass, 2010). Variants of Problem (P0) include replacing the ℓ0 “norm” for sparsity with an ℓ1 norm or an alternative sparsity criterion, or enforcing additional properties (e.g., incoherence) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem (Mairal et al., 2010).\nAlgorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D). Some of these algorithms also partially update X in the dictionary update step. A few recent methods attempt to solve for D and X jointly in an iterative fashion (Rakotomamonjy,\n2013). The K-SVD method (Aharon et al., 2006) has been particularly popular and demonstrated to be useful in numerous applications (Elad & Aharon, 2006; Ravishankar & Bresler, 2011). However, Problem (P0) is highly non-convex and NP-hard, and most dictionary learning approaches lack proven convergence guarantees. Moreover, the algorithms adopted to solve (P0) tend to be computationally expensive (particularly alternating-type algorithms), with the computations usually dominated by the synthesis sparse coding step.\nSome recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms. However, these dictionary learning methods have not been demonstrated to be useful in applications such as image denoising. Bao et al. (2014) in fact show that their method, although a fast proximal scheme, denoises less effectively (typically 0.1 − 0.3 dB worse) than the K-SVD method. Many prior works use restrictive assumptions (e.g., noiseless data, etc.) for their convergence results.\nIn this work, we focus on synthesis dictionary learning and investigate a learning problem with an ℓ0 sparsity penalty instead of constraints. Our approach first models the training data set as an approximate sum of sparse rank-one matrices (or outer products). Then, we use a simple and exact block coordinate descent approach to estimate the factors of the various rank-one matrices. We provide a convergence analysis of the proposed block coordinate descent method for a highly nonconvex problem. Our numerical experiments demonstrate the promising performance and significant speed-ups provided by our method over the classicial K-SVD dictionary learning scheme in sparse signal representation and image denoising.\nOur work shares similarities with a recent dictionary learning approach (Sadeghi et al., 2014) that exploits a sum of outer products model for the training data. However, the specific problem formulation and algorithm studied here differ from the prior work. Importantly, unlike the previous approach, we provide a convergence analysis for our algorithm and demonstrate its usefulness in sparse signal representation and in image denoising.\n2 ℓ0 PENALIZED PROBLEM FORMULATION AND ALGORITHM"
    }, {
      "heading" : "2.1 FORMULATION",
      "text" : "We consider a sparsity penalized variant of Problem (P0) (Bao et al., 2014) in this work. Specifically, we replace the sparsity constraints in (P0) with an ℓ0 penalty ∑N i=1 ‖xi‖0. Next, with C , X T , we decompose the matrix DX = DCT as a sum of (sparse) rank-one matrices or outer products ∑K\nk=1 dkc T k , where ck is the kth column of C. This is a natural decomposition of the training data Y because it separates out the contributions of the various atoms in representing the data. It may also provide a natural way to set the number of atoms (degrees of freedom) in the dictionary. In particular, atoms of a dictionary whose contributions to the data (Y ) representation error or modeling error are small could be dropped. Such a Sum of OUter Products (SOUP) decomposition has been exploited before (Aharon et al., 2006; Smith & Elad, 2013). With the above modifications, we have the following dictionary learning problem, where λ2 > 0 is a weight:\n(P1) min {dk,ck}\n∥ ∥Y − ∑K\nk=1 dkc T k\n∥ ∥ 2\nF + λ2\nK ∑\nk=1\n‖ck‖0 s.t. ‖dk‖2 = 1, ‖ck‖∞ ≤ L ∀ k.\nAs in Problem (P0), the matrix dkcTk in (P1) is invariant to joint scaling of dk and ck as αdk and (1/α)ck, for α 6= 0. The constraint ‖dk‖2 = 1 helps remove this scaling ambiguity. We also enforce the constraint ‖ck‖∞ ≤ L, with L > 0, in (P1). Consider a dictionary D that has a column dj that repeats. Then, in this case, the outer product expansion of Y in (P1) could have both the terms djcTj and −djcTj with cj that is highly sparse, and the objective would be invariant to (arbitrarily) large scalings of cj . The ℓ∞ constraints on the columns of C eliminate such representations from the set of feasible representations in (P1). Problem (P1) is designed to learn the factors {dk} K\nk=1 and {ck} K k=1 that enable the best SOUP sparse representation of Y ."
    }, {
      "heading" : "2.2 ALGORITHM",
      "text" : "We propose a block coordinate descent method to solve for the variables in Problem (P1). For each 1 ≤ j ≤ K , we first solve (P1) with respect to cj keeping all the other variables fixed (called sparse coding step). Once cj is updated, we solve (P1) with respect to dj keeping all other variables fixed (called dictionary atom update step or dictionary update step)."
    }, {
      "heading" : "2.2.1 SPARSE CODING STEP",
      "text" : "Minimizing (P1) with respect to cj leads to the following non-convex problem, where Ej , Y − ∑\nk 6=j dkc T k is a fixed matrix based on the most recent values of all other atoms and coefficients:\nmin cj\n∥ ∥Ej − djc T j ∥ ∥ 2 F + λ2 ‖cj‖0 s.t. ‖cj‖∞ ≤ L. (1)\nThe following proposition provides the solution to Problem (1), where the hard-thresholding operator Hλ(·) is defined as\n(Hλ(b))i =\n{\n0, |bi| < λ bi, |bi| ≥ λ (2)\nwith b ∈ Rn, and the subscript i indexes vector entries. We assume that the bound L > λ and let 1N denote a vector of ones of length N . The operation “⊙” denotes element-wise multiplication, sign(·) computes the signs of the elements of a vector, and z = min(a, u) for vectors a, u ∈ Rq denotes the element-wise minimum operation, i.e., zi = min(ai, bi), 1 ≤ i ≤ q.\nProposition 1 Given Ej ∈ Rn×N and dj ∈ Rn, and assuming L > λ, a global minimizer of the sparse coding problem (1) is\nĉj = min ( ∣ ∣Hλ ( ETj dj )∣ ∣ , L1N ) ⊙ sign ( Hλ ( ETj dj ))\n(3)\nThe solution is unique if and only if the vector ETj dj has no entry with a magnitude of λ."
    }, {
      "heading" : "2.2.2 DICTIONARY ATOM UPDATE STEP",
      "text" : "Minimizing (P1) with respect to dj leads to the following non-convex problem:\nmin dj\n∥ ∥Ej − djc T j ∥ ∥ 2 F s.t. ‖dj‖2 = 1 (4)\nProposition 2 provides the closed-form solution for (4).\nProposition 2 Given Ej ∈ Rn×N and cj ∈ RN , a global minimizer of the dictionary atom update problem (4) is\nd̂j =\n{\nEjcj ‖Ejcj‖2 , if cj 6= 0\nv1 if cj = 0 (5)\nwhere v1 is the first column of the n×n identity matrix. The solution is unique if and only if cj 6= 0.\nFig. 1 shows the Sum of OUter Products DIctionary Learning (SOUP-DIL) Algorithm 1 for Problem (P1). The algorithm assumes that an initial estimate {\nd0k, c 0 k\n}K\nk=1 for the variables is provided. For\nexample, the initial sparse coefficients can be set to zero, and the initial dictionary can be a known analytical dictionary such as the overcomplete DCT (Elad & Aharon, 2006). When ctj = 0, the setting dtj = v1 in (9) in Algorithm 1 could also be replaced with other (equivalent) settings such as dtj = d t−1 j or setting d t j to a random unit norm vector. All of these settings have been observed to work well in practice. A random ordering of the atom/sparse coefficient updates in Fig. 1 (i.e., random j sequence) also helps in practice (in accelerating convergence) compared to cycling in the same order 1 through K every iteration. One could also alternate several times between the sparse coding and dictionary atom update steps for each j in Fig. 1. However, this would increase computation.\nWe now discuss the computational advantages of our Algorithm 1. Define the sparsity factor α of the sparse matrix C ∈ RN×K as the non-negative number satisfying ∑K\nk=1 ‖ck‖0 = αNn. It is\neasy to show that the computational cost of the K ≥ n inner iterations (sparse coding and atom update) in iteration t in Fig. 1 is dominated (for N ≫ K,n) by NKn + 2αmNKn + βNn2, where αm is the maximum sparsity factor of the estimated C’s during the inner iterations, and β is the sparsity factor of the estimated C at the end of iteration t. Thus, the cost per iteration of the block coordinate descent SOUP-DIL Algorithm 1 is about (1 + α′)NKn, with α′ ≪ 1 typically. On the other hand, the proximal alternating algorithm (that involves more parameters than our scheme) proposed very recently by Bao et al. (2015; 2014) has a per-iteration computational cost of at least 2NKn + 6αNKn + 4αNn2. This is clearly more computation than SOUP-DIL. Assuming K ∝ n, the cost per iteration of Algorithm 1 scales as O(Nn2). This is lower than the per-iteration cost of learning an n×K synthesis dictionary D using K-SVD (Aharon et al., 2006), which scales (assuming that the synthesis sparsity level s ∝ n and K ∝ n in K-SVD)1 as O(Nn3).\nAs illustrated in Section 4.1, our algorithms converge in few iterations in practice. Therefore, the per-iteration computational advantages also translate to net computational advantages in practice. The low computational cost of our approach could be particularly useful for big data applications, or higher dimensional (3D or 4D) applications."
    }, {
      "heading" : "3 CONVERGENCE ANALYSIS",
      "text" : "This section presents a convergence analysis for the Algorithm 1 that solves Problem (P1). The proposed algorithm is an exact block coordinate descent procedure to solve for the unknowns in (P1). However, due to the high degree of non-convexity involved, standard results on convergence of block coordinate descent methods (e.g., (Tseng, 2001)) do not apply here. We present some definitions and notations below, before stating our convergence results for Algorithm 1.\nFirst, a necessary condition for x ∈ Rp to be a minimizer of a (proper) function g : Rp 7→ (−∞,+∞] is that x is a critical point of g, i.e., 0 ∈ ∂g(x), where ∂g(x) is the sub-differential of g at x (Rockafellar & Wets, 1997; Mordukhovich, 2006). Critical points are considered to be “generalized stationary points” (Rockafellar & Wets, 1997).\n1When s ∝ n and K ∝ n, the per-iteration computational cost of the efficient implementation of K-SVD (Rubinstein et al., 2008) also scales similarly as O(Nn3).\nThe constraints ‖dk‖2 = 1, 1 ≤ k ≤ K , in (P1) can instead be added as penalties in the cost by using barrier functions χ(dk) (taking the value +∞ when the norm constraint is violated, and zero otherwise). The constraints ‖ck‖∞ ≤ L, 1 ≤ k ≤ K , can also be similarly replaced with barrier penalties ψ(ck). Then, (P1) can be written in an unconstrained form with objective\nf(C,D) = f (c1, ..., cK , d1, ..., dK) = ∥ ∥Y − ∑K\nk=1 dkc T k\n∥ ∥ 2\nF +\nK ∑\nk=1\n{ λ2 ‖ck‖0 + χ(dk) + ψ(ck) }\nFor Algorithm 1, the iterates in the tth outer iteration are denoted as {\nctj , d t j\n}K\nj=1 or as (Ct, Dt)."
    }, {
      "heading" : "3.1 MAIN RESULTS",
      "text" : "Assume that the initial (C0, D0) satisfies the constraints in (P1). We then have the following result. (The proofs of results are not included due to space constraints, and will be presented elsewhere.)\nTheorem 1 Let {Ct, Dt} denote the iterate sequence generated by Algorithm 1 with training data Y ∈ Rn×N and initial (C0, D0). Then, the objective sequence {f t} with f t , f (Ct, Dt) is monotone decreasing, and converges to a finite value, say f∗. Moreover, the iterate sequence {Ct, Dt} is bounded, and all its accumulation points are equivalent in the sense that they achieve the exact same value f∗ of the objective.\nTheorem 1 establishes that for each initial point (C0, D0), the bounded iterate sequence in Algorithm 1 is such that all its accumulation points achieve the same value f∗ of the objective. They are equivalent in that sense. The value of f∗ could vary with different initializations (C0, D0). We thus have the following corollary of Theorem 1 that holds because the distance between a bounded sequence and its (non-empty and compact) set of accumulation points converges to zero.\nCorollary 1 For each (C0, D0), the iterate sequence in Algorithm 1 converges to an equivalence class of accumulation points.\nThe following Theorem 2 considers a very special case of SOUP dictionary learning, where the dictionary has a single atom. In this case, the SOUP learning Problem (P1) is the problem of obtaining a sparse rank-one approximation of the training matrix Y . In this case, Theorem 2 establishes that the iterates in Algorithm 1 converge to the set of critical points (i.e., the distance between the iterates and the set converges to zero) of the objective f .\nTheorem 2 Consider Algorithm 1 with K = 1. Let {ct, dt} denote the bounded iterate sequence generated by Algorithm 1 in this case with data Y ∈ Rn×N and initial (c0, d0). Then, every accumulation point of the iterate sequence is a critical point of the objective f , i.e., the iterates converge to the set of critical points of f .\nFor the general case (K 6= 1), we have the following results.\nTheorem 3 Let {Ct, Dt} denote the bounded iterate sequence generated by Algorithm 1 with training data Y ∈ Rn×N and initial (C0, D0). Suppose each accumulation point (C,D) of the iterate sequence is such that the matrix B with columns bj = ETj dj and Ej = Y −DC T + djc T j , has no entry with magnitude λ. Then every accumulation point of the iterate sequence is a critical point of the objective f(C,D). Moreover, the sequence {at} with at , ∥ ∥ ∥Dt (Ct) T −Dt−1 ( Ct−1 )T ∥ ∥ ∥\nF ,\nconverges to zero.\nTheorem 3 establishes that the iterates in SOUP-DIL converge to the set of critical points of f(C,D). For each initial (C0, D0), the iterate sequence in Algorithm 1 converges (using Corollary 1) to an equivalence class of critical points of f . Theorem 3 also establishes that the sparse approximation to the training data Zt = Dt (Ct)T is such that ∥ ∥Zt − Zt−1 ∥ ∥\nF → 0. This is a necessary but not\nsufficient condition for the convergence of the entire sequence {Zt}. The assumption on the entries of the matrix B in Theorem 3 is equivalent to assuming that for every 1 ≤ j ≤ K , there is a unique minimizer of f with respect to cj with all other variables fixed to their values in the accumulation point (C,D).\nAlthough Theorem 3 uses a uniqueness condition, the following conjecture postulates that provided the following Assumption 1 (that uses a probabilistic model for the data) holds, the uniqueness condition holds with probability 1, i.e., the probability of a tie in assigning sparse codes is zero.\nAssumption 1. The training signals yi ∈ Rn for 1 ≤ i ≤ N , are drawn independently from an absolutely continuous probability measure over the n-dimensional ball S , {y ∈ Rn : ‖y‖\n2 ≤ c0}\nfor some c0 > 0.\nConjecture 1 Let Assumption 1 hold. Then, with probability 1, every accumulation point (C,D) of the iterate sequence in Algorithm 1 is such that for each 1 ≤ j ≤ K , the minimizer of f(c1, ..., cj−1, c̃j , cj+1, ..., cK , d1, ..., dK) with respect to c̃j is unique.\nIf Conjecture 1 holds, then every accumulation point of the iterate sequence in Algorithm 1 is immediately a critical point of f(C,D) with probability 1."
    }, {
      "heading" : "4 NUMERICAL EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 CONVERGENCE EXPERIMENT",
      "text" : "To study the convergence behavior of the proposed SOUP-DIL Algorithm 1, we extracted 3 × 104 patches of size 8× 8 from randomly chosen locations in the images Barbara, Boat, and Hill, shown in Fig. 2. Problem (P1) was solved to learn a 64 × 256 overcomplete dictionary for this data, with λ = 69. Algorithm 1 is initialized as mentioned in Section 2.2.\nFig. 3 illustrates the convergence behavior of SOUP-DIL. The objective in our method (Fig. 3(a)) converged monotonically and quickly over the iterations. We define the normalized sparse representation error (NSRE) as ∥ ∥Y −DCT ∥ ∥\nF / ‖Y ‖F , which is used to measure the sparse representation\nperformance of the learned dictonaries. Fig. 3(b) shows the normalized sparse representation error and sparsity factor (for C), both expressed as percentages, for Algorithm 1. Both these components of the objective converged quickly, and the NSRE improved by 1 dB (decibel) beyond the first iteration, indicating the success of the SOUP-DIL Algorithm 1 in representing data using a small number of non-zero coefficients (sparsity factor of 3.14% at convergence).\nImportantly, both the quantities ∥ ∥Dt −Dt−1 ∥ ∥\nF (Fig. 3(c)) and\n∥ ∥Ct − Ct−1 ∥ ∥\nF (Fig. 3(d)) converge\ntowards 0. This implies that ∥ ∥Zt − Zt−1 ∥ ∥ F with Zt = Dt (Ct)T converges towards zero too, as predicted by Theorem 3. The above results are indicative (are necessary but not sufficient conditions) of the convergence of the entire sequences {Dt}, {Ct}, and {Zt} for our algorithm in practice. In contrast, Bao et al. (2014) showed that the distance between successive iterates may not converge to 0 for popular algorithms such as K-SVD."
    }, {
      "heading" : "4.2 SPARSE REPRESENTATION OF DATA",
      "text" : "The second experiment worked with the same data as in Section 4.1 and learned dictionaries of size 64 × 256 for various choices of the parameter λ in (P1). We compare the performance of our algorithm to the K-SVD dictionary learning scheme (Aharon et al., 2006). The K-SVD method was executed for several choices of sparsity (number of non-zeros) of the columns of CT . The λ values for (P1) were chosen so as to achieve similar average column sparsity levels in CT as K-SVD. Both algorithms were initialized with the same overcomplete DCT and ran for 10 iterations each.\nFig. 4 shows the behavior of the SOUP-DIL and K-SVD algorithms for average column sparsity levels of CT ranging from about 2% to 20%. As expected, the NSRE values for the SOUP-DIL algorithm decreased monotonically (Fig. 4(a)) when the average column sparsity levels in CT increased (i.e., as λ decreased). Importantly, the SOUP-DIL algorithm provided better data representations (Fig. 4(b)) than K-SVD at the various tested sparsity levels. Large improvements of about 14 dB and 1 dB are observed at low and mid sparsity levels, respectively.\nFig. 4(c) compares the runtimes of the unoptimized Matlab implementation of our method to those of the unoptimized Matlab implementation of K-SVD (Elad, 2009) as well as the efficient (partial) MEX/C implementation (Rubinstein, 2010; Rubinstein et al., 2008) of K-SVD demonstrating large speedups of 40-50 times for SOUP-DIL over the first K-SVD implementation (at most sparsities), while the runtimes for the SOUP-DIL method are about the same as those of the second K-SVD implementation. Since these results were obtained using only an unoptimized Matlab implementation of SOUP-DIL, we expect significant speed-ups for our scheme with code optimization or C/C++ implementations. All computations above were performed with an Intel Xeon CPU X3230 at 2.66 GHz and 8 GB memory, employing a 64-bit Windows 7 operating system."
    }, {
      "heading" : "4.3 IMAGE DENOISING",
      "text" : "In image denoising, the goal is to recover an estimate of an image x ∈ RM (2D image represented as a vector) from its corrupted measurements y = x + h, where h is the noise. To perform image denoising using (P1), we first extract all the overlapping patches (with maximum overlap) of the noisy image y, and construct the training set Y ∈ Rn×N as a matrix whose columns are those noisy patches. We then use Problem (P1) to learn a dictionary and sparse codes for Y , with the weight λ ∝ σ. To obtain the denoised image estimate, we then solve the following least squares problem, where D̂ and α̂j denote the learned dictionary and patch sparse codes obtained from the noisy patches, and Pj is an operator that extracts a patch as a vector:\nmin x\nN ∑\nj=1\n∥ ∥Pjx− D̂α̂j ∥ ∥ 2\n2 + ν ‖x− y‖\n2 2 (10)\nThe optimal x̂ in (10) is easily obtained by summing together the denoised patch estimates D̂α̂j at their 2D locations, and computing a weighted average between this result and the noisy image.\nK-SVD based denoising2 (Elad & Aharon, 2006) involves a similar methodology as described above for (P1), but differs in the dictionary learning procedure, where the ℓ0 “norms” of the sparse codes are minimized so that a fitting constraint or error constraint of ∥ ∥Pjy − D̂α̂j ∥ ∥ 2\n2 ≤ nC2σ2 is met\nfor representing the noisy patches. This constraint serves as a strong prior, and is a key reason for the denoising capability of K-SVD. Hence, in our method based on (P1), once the dictionary D̂ is learned from noisy patches, we estimate the patch sparse codes α̂j using a single pass (over the noisy patches) of orthogonal matching pursuit (OMP) by employing an error constraint criterion like in K-SVD. Although we only use information on the noise statistics in a sub-optimal way for (P1), we still show good denoising performance (vis-a-vis K-SVD) in the following with this approach.\nFor the denoising experiments, we work with the images in Fig. 2, and simulate i.i.d. Gaussian noise at six different noise levels (σ = 5, 10, 20, 25, 30, 100) for each of the images. We then denoise these images using both the SOUP-DIL denoising method outlined above and K-SVD. The dictionary learning on the noisy patches (of size 8 × 8) for SOUP-DIL is executed with a 64× 256 dictionary, λ = 5σ, ν = 20/σ (in (10)), an overcomplete DCT initial dictionary, and using 10 iterations of Algorithm 1. These settings were found to work well for our method.\nTable 1 lists the denoising PSNRs obtained by the SOUP-DIL denoising method, along with the PSNRs obtained by K-SVD. For comparison, we also list the denoising PSNRs obtained by employing the overcomplete DCT dictionary for denoising. In the latter case, the same strategy (and parameter settings) as used by K-SVD based denoising is adopted but while skipping the learning process.\nThe various methods denoise about the same in Table 1 for a low noise level of σ = 5. For σ > 5, the SOUP-DIL method denoises about 0.4 dB better on the average than the overcomplete DCT. While the SOUP-DIL method and K-SVD denoise about the same at low and mid noise levels, the SOUP-DIL scheme performs about 0.1 dB better on average at higher noise levels such as σ = 30 or 100. Importantly, SOUP-DIL based denoising is highly efficient and the learning procedure has good convergence properties. We have observed similar effects as in Section 4.2 for the runtimes of SOUP-DIL denoising vis-a-vis the Matlab (Elad, 2009) or the efficient (partial) MEX/C (Rubinstein, 2010) implementations of the K-SVD approach.\nFig. 5 shows the dictionaries learned from noisy patches using the SOUP-DIL method and K-SVD for the image Barbara at σ = 20. Both dictionaries show frequency and textural features that are specific to the image Barbara. By learning such image-specific features, the SOUP-DIL method (and K-SVD) easily outperforms fixed dictionaries such as the overcomplete DCT in denoising.\n2The K-SVD method is a highly popular dictionary learning scheme that has been applied to a wide variety of image processing applications including denoising (Elad & Aharon, 2006). Mairal et al. (2009) proposed a non-local method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D (Dabov et al., 2007) denoising method. Similar extensions to our proposed method to achieve state-of-the-art performance in denoising and other applications is beyond the scope of this work and is left for future work. For K-SVD denoising, we used the built-in parameter settings of the author’s implementation (Elad, 2009), unless otherwise stated."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "This paper proposed a fast block coordinate descent method for synthesis dictionary learning with an ℓ0 penalty. The key idea is to decompose the training data as a sum of sparse rank-one matrices, and then efficiently estimate the factors of the rank-one matrices. A convergence analysis was presented for the proposed block coordinate descent algorithm for a highly non-convex problem. The proposed approach had comparable or superior performance and significant speed-ups over the classical KSVD method in sparse signal representation and denoising. Extensions of the method for online learning (Mairal et al., 2010) merit further study."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work was supported in part by the following grants: ONR grant N00014-15-1-2141, DARPA Young Faculty Award D14AP00086, ARO MURI grants W911NF-11-1-0391 and 2015-05174-05, NIH grant U01 EB01875301, and a UM-SJTU seed grant."
    } ],
    "references" : [ {
      "title" : "Learning sparsely used overcomplete dictionaries",
      "author" : [ "A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation",
      "author" : [ "M. Aharon", "M. Elad", "A. Bruckstein" ],
      "venue" : "IEEE Transactions on signal processing,",
      "citeRegEx" : "Aharon et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Aharon et al\\.",
      "year" : 2006
    }, {
      "title" : "New algorithms for learning incoherent and overcomplete dictionaries",
      "author" : [ "S. Arora", "R. Ge", "A. Moitra" ],
      "venue" : null,
      "citeRegEx" : "Arora et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2013
    }, {
      "title" : "L0 norm based dictionary learning by proximal methods with global convergence",
      "author" : [ "C. Bao", "H. Ji", "Y. Quan", "Z. Shen" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Bao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2014
    }, {
      "title" : "Dictionary learning for sparse coding: Algorithms and analysis",
      "author" : [ "C. Bao", "H. Ji", "Y. Quan", "Z. Shen" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Bao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2015
    }, {
      "title" : "Image denoising by sparse 3D transformdomain collaborative filtering",
      "author" : [ "K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian" ],
      "venue" : "IEEE Trans. on Image Processing,",
      "citeRegEx" : "Dabov et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dabov et al\\.",
      "year" : 2007
    }, {
      "title" : "Image denoising via sparse and redundant representations over learned dictionaries",
      "author" : [ "M. Elad", "M. Aharon" ],
      "venue" : "IEEE Trans. Image Process.,",
      "citeRegEx" : "Elad and Aharon,? \\Q2006\\E",
      "shortCiteRegEx" : "Elad and Aharon",
      "year" : 2006
    }, {
      "title" : "Method of optimal directions for frame design",
      "author" : [ "K. Engan", "S.O. Aase", "J.H. Hakon-Husoy" ],
      "venue" : "In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Engan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Engan et al\\.",
      "year" : 1999
    }, {
      "title" : "Dictionary identification–sparse matrix-factorization via l1 minimization",
      "author" : [ "R. Gribonval", "K. Schnass" ],
      "venue" : "IEEE Trans. Inform. Theory,",
      "citeRegEx" : "Gribonval and Schnass,? \\Q2010\\E",
      "shortCiteRegEx" : "Gribonval and Schnass",
      "year" : 2010
    }, {
      "title" : "Non-local sparse models for image restoration",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman" ],
      "venue" : "In IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Mairal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mairal et al\\.",
      "year" : 2009
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Mairal et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mairal et al\\.",
      "year" : 2010
    }, {
      "title" : "Variational Analysis and Generalized Differentiation",
      "author" : [ "B.S. Mordukhovich" ],
      "venue" : "Vol. I: Basic theory. Springer-Verlag,",
      "citeRegEx" : "Mordukhovich,? \\Q2006\\E",
      "shortCiteRegEx" : "Mordukhovich",
      "year" : 2006
    }, {
      "title" : "Direct optimization of the dictionary learning problem",
      "author" : [ "A. Rakotomamonjy" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Rakotomamonjy,? \\Q2013\\E",
      "shortCiteRegEx" : "Rakotomamonjy",
      "year" : 2013
    }, {
      "title" : "MR image reconstruction from highly undersampled k-space data by dictionary learning",
      "author" : [ "S. Ravishankar", "Y. Bresler" ],
      "venue" : "IEEE Trans. Med. Imag.,",
      "citeRegEx" : "Ravishankar and Bresler,? \\Q2011\\E",
      "shortCiteRegEx" : "Ravishankar and Bresler",
      "year" : 2011
    }, {
      "title" : "Dictionary learning for sparse representation: A novel approach",
      "author" : [ "M. Sadeghi", "M. Babaie-Zadeh", "C. Jutten" ],
      "venue" : "IEEE Signal Processing Letters,",
      "citeRegEx" : "Sadeghi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sadeghi et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning overcomplete dictionaries based on atomby-atom updating",
      "author" : [ "M. Sadeghi", "M. Babaie-Zadeh", "C. Jutten" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Sadeghi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sadeghi et al\\.",
      "year" : 2014
    }, {
      "title" : "Recursive least squares dictionary learning algorithm",
      "author" : [ "K. Skretting", "K. Engan" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Skretting and Engan,? \\Q2010\\E",
      "shortCiteRegEx" : "Skretting and Engan",
      "year" : 2010
    }, {
      "title" : "Improving dictionary learning: Multiple dictionary updates and coefficient reuse",
      "author" : [ "L.N. Smith", "M. Elad" ],
      "venue" : "IEEE Signal Processing Letters,",
      "citeRegEx" : "Smith and Elad,? \\Q2013\\E",
      "shortCiteRegEx" : "Smith and Elad",
      "year" : 2013
    }, {
      "title" : "Exact recovery of sparsely-used dictionaries",
      "author" : [ "D.A. Spielman", "H. Wang", "J. Wright" ],
      "venue" : "In Proceedings of the 25th Annual Conference on Learning Theory, pp",
      "citeRegEx" : "Spielman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Spielman et al\\.",
      "year" : 2012
    }, {
      "title" : "Convergence of a block coordinate descent method for nondifferentiable minimization",
      "author" : [ "P. Tseng" ],
      "venue" : "J. Optim. Theory Appl.,",
      "citeRegEx" : "Tseng,? \\Q2001\\E",
      "shortCiteRegEx" : "Tseng",
      "year" : 2001
    }, {
      "title" : "A fast patch-dictionary method for whole-image recovery",
      "author" : [ "Y. Xu", "W. Yin" ],
      "venue" : "URL ftp://ftp.math.ucla.edu/pub/camreport/cam13-38.pdf. UCLA CAM report",
      "citeRegEx" : "Xu and Yin,? \\Q2013\\E",
      "shortCiteRegEx" : "Xu and Yin",
      "year" : 2013
    }, {
      "title" : "Dictionary learning for sparse approximations with the majorization method",
      "author" : [ "M. Yaghoobi", "T. Blumensath", "M. Davies" ],
      "venue" : "IEEE Transaction on Signal Processing,",
      "citeRegEx" : "Yaghoobi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yaghoobi et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "More recently, the data-driven adaptation of synthesis dictionaries called dictionary learning, has shown promise in applications (Elad & Aharon, 2006; Mairal et al., 2009; Ravishankar & Bresler, 2011) compared to analytical dictionaries such as wavelets or DCT.",
      "startOffset" : 130,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "Given a collection of training signals {yi} N i=1 that are represented as columns of the matrix Y ∈ R n×N , the dictionary learning problem is often formulated as follows (Aharon et al., 2006) (P0) min D,X ‖Y −DX‖ 2 F s.",
      "startOffset" : 171,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : ", incoherence) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem (Mairal et al., 2010).",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).",
      "startOffset" : 36,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).",
      "startOffset" : 36,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).",
      "startOffset" : 36,
      "endOffset" : 188
    }, {
      "referenceID" : 10,
      "context" : "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).",
      "startOffset" : 36,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).",
      "startOffset" : 36,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "The K-SVD method (Aharon et al., 2006) has been particularly popular and demonstrated to be useful in numerous applications (Elad & Aharon, 2006; Ravishankar & Bresler, 2011).",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Some recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms.",
      "startOffset" : 18,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "Some recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms.",
      "startOffset" : 18,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "Some recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms.",
      "startOffset" : 18,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "Some recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms.",
      "startOffset" : 18,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "Our work shares similarities with a recent dictionary learning approach (Sadeghi et al., 2014) that exploits a sum of outer products model for the training data.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms. However, these dictionary learning methods have not been demonstrated to be useful in applications such as image denoising. Bao et al. (2014) in fact show that their method, although a fast proximal scheme, denoises less effectively (typically 0.",
      "startOffset" : 8,
      "endOffset" : 257
    }, {
      "referenceID" : 3,
      "context" : "1 FORMULATION We consider a sparsity penalized variant of Problem (P0) (Bao et al., 2014) in this work.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "Such a Sum of OUter Products (SOUP) decomposition has been exploited before (Aharon et al., 2006; Smith & Elad, 2013).",
      "startOffset" : 76,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "This is lower than the per-iteration cost of learning an n×K synthesis dictionary D using K-SVD (Aharon et al., 2006), which scales (assuming that the synthesis sparsity level s ∝ n and K ∝ n in K-SVD)1 as O(Nn).",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : ", (Tseng, 2001)) do not apply here.",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 11,
      "context" : ", 0 ∈ ∂g(x), where ∂g(x) is the sub-differential of g at x (Rockafellar & Wets, 1997; Mordukhovich, 2006).",
      "startOffset" : 59,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "In contrast, Bao et al. (2014) showed that the distance between successive iterates may not converge to 0 for popular algorithms such as K-SVD.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "We compare the performance of our algorithm to the K-SVD dictionary learning scheme (Aharon et al., 2006).",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "(2009) proposed a non-local method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D (Dabov et al., 2007) denoising method.",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 8,
      "context" : "Mairal et al. (2009) proposed a non-local method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D (Dabov et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "Extensions of the method for online learning (Mairal et al., 2010) merit further study.",
      "startOffset" : 45,
      "endOffset" : 66
    } ],
    "year" : 2017,
    "abstractText" : "The sparsity of natural signals in a transform domain or dictionary has been extensively exploited in several applications. More recently, the data-driven adaptation of synthesis dictionaries has shown promise in many applications compared to fixed or analytical dictionaries. However, dictionary learning problems are typically non-convex and NP-hard, and the alternating minimization approaches usually adopted to solve these problems are often computationally expensive, with the computations dominated by the NP-hard synthesis sparse coding step. In this work, we investigate an efficient method for dictionary learning by first decomposing the training data set into a sum of sparse rank-one matrices and then using a block coordinate descent approach to estimate the rank-one terms. The proposed algorithm involves efficient closed-form solutions. In particular, the sparse coding step involves a simple form of thresholding. We provide a convergence analysis for the proposed block coordinate descent method that solves a highly non-convex problem. Our experiments show the promising performance and significant speed-ups provided by our method over the classical K-SVD scheme in sparse signal representation and image denoising.",
    "creator" : "LaTeX with hyperref package"
  }
}