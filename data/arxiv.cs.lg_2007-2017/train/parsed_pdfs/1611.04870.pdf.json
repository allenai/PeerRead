{
  "name" : "1611.04870.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Constrained Low-rank Learning Using Least Squares Based Regularization",
    "authors" : [ "Ping Li" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n04 87\n0v 1\n[ cs\n.C V\n] 1\n5 N\nov 2\n01 6\nConstrained Low-rank Learning Using Least Squares Based Regularization\nPing Li, Member, IEEE, Jun Yu, Member, IEEE, Meng Wang, Member, IEEE, Luming Zhang, Member, IEEE, Deng Cai, Member, IEEE, and Xuelong Li, Fellow, IEEE,\nAbstract—Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace segmentation, image categorization. Most low-rank methods are incapable of capturing low-dimensional subspace for supervised learning tasks, e.g., classification and regression. This work aims to learn both the discriminant low-rank representation and the robust projecting subspace in a supervised manner. To achieve this goal, we cast the problem into a constrained rank minimization framework by adopting the least squares regularization. Naturally, the data label structure tends to resemble that of the corresponding lowdimensional representation, which is derived from the robust subspace projection of clean data by low-rank learning. Moreover, the low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint, e.g., Laplacian regularizer. Therefore, we propose a novel Constrained Low-Rank Representation (CLRR) method. The objective function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact Augmented Lagrange Multiplier algorithm. Extensive experiments on image classification, human pose estimation and robust face recovery have confirmed the superiority of our method.\nIndex Terms—Low-rank learning, robust recovery, image classification, regularization, data representation.\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Low-rank learning has exhibited its advantages in a broad range of real-world applications, such as subspace segmentation, image classification, outlier detection. Besides, it theoretically supports dealing with contaminated observations by outliers and noise, e.g., disguise, occlusion, specular reflections or pixel corruptions. Hence, it has been receiving much attention from both academia and industry, recently. As suggested in low-rank matrix recovery and matrix completion, we assume that data points from the same pattern tend to linearly correlate in the subspace. Thus, data points from different categories can be treated as samples nearly drawn from a union of multiple low-rank subspaces. However, the collected data points might be vulnerable to noise and corruptions in unfavorable situations, such as varying lighting, serious fading and partial occlusion, potentially damaging the subspace structures as well as deteriorating the learning process. Therefore, it is desirable to develop one technique to recover clean data from noisy observations while maintaining the intrinsic subspace structure of the data. To this end, there have emerged a number of low-rank learning methods [1]–[3]. Among them, Robust Principal Component Analysis (RPCA) [1] and Low-Rank\n• This work was supported in part by the National Natural Science Foundation of China under Grant 61502131, Zhejiang Provincial Natural Science Foundation of China under Grant LQ15F020012, the National Basic Research Program of China (973 Program) under Grant 2013CB336500, the National Natural Science Foundation of China under Grants 61572169, 61472266, 61472110, and China Scholarship Council. • Corresponding author: P. Li (patriclouis.lee@gmail.com)\nManuscript received July 31, 2016.\nRepresentation (LRR) [2] are two typical approaches, which decouple the noisy data into the clean component (i.e., the recovered data) and the sparse error component. The difference is that RPCA implicitly assumes the underlying data structure is a single low-rank subspace. This neglects the specific property of the individual subspace. By contrast, LRR explicitly considers the multiple low-rank subspaces by adopting a dictionary linearly spanning the data space, thus better respecting the underlying data structure. In theory, LRR can be considered as a generalization of RPCA for which the dictionary degenerates to identity matrix. Due to their success in a vast number of applications, they are often refined to satisfy different requirements. For example, some researchers [4], [5] explored the idea of enforcing the sparsity constraint on the lowest rank representation while others [6] adopted the fixed-rank strategy to accelerate the computation. Nevertheless, these methods are unsupervised and cannot encode the prior knowledge (e.g., pairwise constraints, label structures) which could help capture the discriminant information. In this paper, we seek a robust projecting subspace, where the label structure of training data is inherently incorporated. It is expected that the label structure of the data could be well embedded in low-rank representation, which integrates the recovered data with discriminating power. Thus, we can not only use the recovered data to enjoy more promising classification performances, but also employ the projecting subspace to obtain better regression results. Furthermore, to equip the learned subspace with some appealing properties, we impose an adaptive constraint on the low-dimensional representation of original data as\na regularizer. Several popular constraints can be used, e.g., the locality constraint [7] and the Fisher constraint [8]. Coupling these factors, we develop a novel method called Constrained Low-Rank Representation (CLRR) to achieve the aforementioned goals. We formulate it as a constrained rank minimization problem, which can be solved by the inexact Augmented Lagrange Multiplier (ALM) algorithm [9]. The primary advantage of our method is that it can yield a robust projecting subspace where data points can be mapped into the low-dimensional data space, as well as generating the discriminant lowest rank representation of the data. To investigate the performance of our method, we have applied CLRR to several practical tasks, including image classification, human pose estimation, and robust face recovery. Empirical studies have shown its superiority over several alternatives, e.g., CLRR can achieve the lowest classification error rates on three publicly available image databases, and yields the lowest angle errors on 3D human body poses recovery. The remainder of the paper is structured as follows. Firstly, Section 2 briefly reviews the previous works related to ours. Then, we describe the problem setting of this work in Section 3 and introduce the proposed method covering the formulations, followed by its optimization framework in Section 4. Moreover, we have some discussions in Section 5. Furthermore, a number of comprehensive experiments were conducted on several real-world databases and the results are reported in Section 6. Finally, we conclude the paper in Section 7."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "In this section, we mainly describe some works related to our proposed method, including low-rank learning, constrained matrix factorization and subspace learning. Low-rank learning has attracted much attention due to its overwhelming advantages in a wide range of real-world applications [10], [11], such as subspace segmentation, face recognition, outlier detection and video surveillance. Given a collection of data points, to consider the situation where a fraction of data matrix entries are missing or arbitrarily corrupted, Candès’ et al. [1] proposed Robust Principal Component Analysis (RPCA), which treats this problem as decomposing the input matrix into two components involving a low-rank matrix and a sparse matrix. However, it suffers from the deficit that data points are assumed to be populated in one subspace, which violates popular situations where data points reside on multiple subspaces. To overcome this shortcoming, Liu et al. [2] put forward Low-Rank Representation (LRR) which was proved to be very effective for robust subspace segmentation. LRR seeks the low-rank component based on a dictionary, which could be learned from the data or simply be the data space. Hence, it is expected that LRR could better cater for the requirements of many real-world tasks, e.g., image restoration [12]. Actually, it shares with Sparse Subspace\nClustering (SSC) [13] the common assumption that the underlying subspaces of the data are low-dimensional. The different philosophy lies in that SSC inherently seeks a sparse representation while LRR attempts to make the representation low-rank, leading to different objective functions. Besides, someone have tried to consider inducing both the sparsity and the low-rankness of the representation [5], so that the intrinsic structure of the underlying subspace could be well respected. Thereafter, Liu et al. [14] proposed Latent LRR that exploits both the observed and the hidden data to construct the dictionary, in order to complement the insufficient observations. Moreover, to consider the intrinsically geometric structure of the data, Yin et al. [15] proposed a Laplacian regularized LRR model to learn nonnegative sparse and low-rank representation, which takes into account the high-order relations among data points by hypergraphs. Furthermore, to avoid the expensive singular value decomposition, Liu et al. [6] adopted a fixed-rank strategy to obtain the low-rank representation, which largely improves the computational speed. Aside from that, one can extend the work to semi-supervised scenario by utilizing the prior knowledge in the form of constraints [16], and can employ LRR to substitute sparse code or vector quantization in spatial pyramid matching to encode local descriptors [17]. In addition, some researchers use lowrank learning for dictionary learning, e.g., Jiang et al. [18] proposed supervised low-rank dictionary decomposition to facilitate a sparse and dense hybrid representation framework as well as alleviating the problem of the corrupted training data for face recognition; Li et al. [19] attempted to refine the sparse representation by learning a discriminative dictionary with one low-rank regularizer. Generally speaking, low-rank representation can be cast into the paradigm of matrix factorization, which plays an important role in machine learning and pattern recognition. As is known to us, several constraints can be imposed onto matrix factorization to make the learned representation more informative. Therefore, constrained matrix factorization as a principled fashion has been widely applied in a large quantity of methods. Recently, one of the most popular constraints is graph-based regularization by virtue of manifold learning, which is capable of preserving the geometrical structure of the data space, such as graph-based non-negative matrix factorization methods [7] and concept factorization [20]. Moreover, to better encode the manifold structure of the data, some researchers attempt to use multiple graphs to boost the learning performances, such as classification [21], [22] and coclustering [23]. And some others consider the problem from the topographic perspective, e.g., topographic non-negative matrix factorization for data representation [24]. Motivated by previous works, we have designed a regularizer that incorporates the informative constraints, such as local consistency and discriminative property, to enforce the effective structure onto the learned low-rank representation.\nActually, our proposed method can be also treated as a subspace learning method, since it aims to learn both the low-rank subspace and the robust projecting subspace. To this end, subspace learning has established itself as an important tool to obtain effective data representation in the last decade. Typical subspace learning methods have been proved to be very effective in practical applications, such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) [25]. Recently, Jiang et al. [26] developed a subspace method for facial eigenfeature regularization and extraction (ERE), and the eigenspace of the within-class scatter matrix is decomposed into three subspaces involving a reliable subspace spanned mainly by variation, an unstable one due to noise as well as limited training data, and a null subspace. This could alleviate the problem of instability, overfitting and poor generalization. As we know, PCA is an unsupervised method and does not require label information. To take into account the label information, an Asymmetric Principal Component Analysis (APCA) [27] approach was proposed, which utilizes class covariance matrices and enables removing the unreliable dimensions of principal components. While APCA is designed for handling the two-class problem, Supervised Principal Component Analysis (SPCA) [28] deals with the multiple-class problem. Unlike APCA, SPCA imposes different weights on the covariance matrices so as to consider class-specific information of the data set. To summarize, the above methods are expected to generate considerable subspaces, but they cannot explicitly yield low-rank subspaces and separate the error matrix as our approach can, e.g., they cannot recover the clean component of the corrupted image. In some sense, this would hinder potentially more widespread applications of these methods. It is worth noting that there exist two works related to ours. One is the Supervised Regularization based Robust Subspace (SRRS) method [29], which smoothly integrates subspace learning and data recovery in a unified framework to jointly learn discriminative subspace and low-rank representation from the data. It differs from our method in several aspects: (a) it adopts the Fisher criterion to capture the discriminant structure while ours utilizes both the Laplacian regularizer and the least squares regularizer under the guidance of the supervised information; (b) it includes a generalized eigen-decomposition problem to obtain the projecting subspace while ours gives a closed-form solution accordingly and avoids solving the expensive Sylvester equation; (c) Our method can be used for regression tasks directly while SRRS cannot. The other is Robust Regression (RR) [30], which leverages the rank regularizer and the sparse error term, but it regards the underlying data structure as a single low-rank subspace that might cause the inaccurate recovery. By contrast, we assume the data populates on a mixture of multiple subspaces to guarantee the correct recovery. Moreover, the subspace obtained from RR does not have the desired informative properties, e.g., the locality-preserving ability, while our method can easily achieve this based on the adaptive regularizer. Details of our method are elaborated in the following sections."
    }, {
      "heading" : "3 PROBLEM SETTING",
      "text" : "In this paper, we define the constrained low-rank learning problem as follows. Given a collection of data points {x1,x2, . . . ,xn} and their labels {y1, y2, . . . , yn} distributed in k classes, we assume they are samples approximately drawn from a mixture of several subspaces [2]. The principal goal is to seek the discriminant lowest rank representation Z as well as the robust projecting subspace P. More specifically, we denote the training data by X ∈ Rd×n with each data point stacked in a column, and the data matrix can be decomposed into a clean component X̃ = AZ and an error component E ∈ Rd×n, where A ∈ Rd×m is treated as the dictionary linearly spanning the data space while Z ∈ Rm×n reveals the underlying subspace structure of the data. More importantly, we argue that the recovered data can be mapped onto a low-dimensional data space by the robust projecting subspace P ∈ Rd×k (the reduced dimension k is set to the number of classes), i.e., V = P T AZ ∈ Rk×n. On one hand, the low-dimensional data representation V is expected to be closely correlated to the label indicator matrix Y ∈ Rk×n while it acts as the estimated output given the input data. The matrix Y takes discrete values for classification and continuous values for regression, respectively, e.g., the entries in each column of Y are set to 1 if the sample belongs to the corresponding class. On the other hand, it is easy to endow the low-dimensional representation PTX, derived from the original data space, with several appealing properties like the locality-preserving ability, by the constraint matrix L ∈ Rn×n. Usually, this matrix should be semi-positive definite to make the imposed regularizer convex. By tradition, the lowest rank representation is employed to construct an affinity matrix for subspace segmentation in unsupervised learning. Here, we mainly use it for recovering the clean data by AZ, where Z plays a dominant role. Under such circumstance, both the recovered training data and testing data could show the robustness to noise or corruptions, and it also allows to discriminate the samples from different categories."
    }, {
      "heading" : "4 OUR METHOD",
      "text" : "This section concentrates on elaborating the proposed method, including the formulation, and the optimization framework as well as the algorithmic procedures."
    }, {
      "heading" : "4.1 Formulation",
      "text" : "As mentioned earlier, our goal is to jointly seek the discriminant lowest-rank representation Z ∈ Rm×n and the robust projecting subspace P ∈ Rd×k in a supervised\nmanner. Essentially, we have to minimize rank(Z), which is yet difficult to solve due to its discrete nature. As a common practice in low-rank methods [2], [5], we use the nuclear norm as its convex surrogate. In this work, the dictionary A is set to X. Hence, our objective function can be formulated as:\nmin Z,E,P\n‖Z‖∗ + λ‖E‖2,1 + αTr(PTXLXTP) + β‖V −Y‖2F ,\ns. t. X = XZ+E,V = PTXZ,1TnZ = 1 T n .\n(1)\nwhere the nuclear norm ‖ · ‖∗ is the sum of singular values of a matrix, the group sparse norm ‖ · ‖2,1 computes the sum of absolute values of l2-norm on each column vector of a matrix, e.g., ∑\nj ‖Ej‖2 for E, ‖·‖F denotes the Frobenius norm of a matrix, 1 is a column vector with all ones. The parameter α > 0 balances the contribution of the constraint to the objective, β > 0 controls the fitting of the least squares term, and λ > 0 governs the noise level. Note that the regularized error component can also be replaced by l1-norm often used for sparse coding [31] if necessary.\nIn the objective function, the first term aims to minimize the rank of Z while the second term encourages the sparseness of the error matrix E for different groups. The former two terms are generally used by most lowrank learning methods, while the latter constraints are coherently taken into account for the first time. The third term is the enforced Laplacian regularizer, which can make the derived low-dimensional representation P T X be characteristic with the intrinsic property of the constraint matrix L. When L is simply set to the identity matrix I, this term reduces to its Frobenius norm. The fourth term is the least squares regularizer, which would make the recovered low-dimensional representation V have the similar structure as that of the label matrix Y. Besides, we explicitly impose the normalization constraint on the columns of Z to ease the non-unique solution problem [14]. It should be noted that we explicitly use the supervised information in the least squares regularizer to guide the learning process of the projecting subspace P and the lowest-rank representation Z, and this strongly encourages the discriminative power for them. Moreover, while the error matrix E is used to encode the noise or corruptions, it is still impossible to eliminate all noise or corruptions of data points in practice. As a result, it is sensible to impose constraints on the low-dimensional representation of the original data space X. Thus, the supervised information plays an overwhelming role on the low-dimensional representation for both the original data space and the recovered data space by the third and the fourth constraints, respectively. In addition, the two constraints are said to be inherently correlated in between, since during the optimization process the matrix P and the matrix Z are simultaneously updated by iteration. Details are shown below."
    }, {
      "heading" : "4.2 Optimization",
      "text" : "In this part, we show how to optimize the objective function in (1) by the Augmented Lagrange Multipler (ALM) algorithm [9], which has a variant named the Alternating Direction Method (ADMM) [32] widely used in solving low-rank based problems [14], [29], [33]. To make the objective function separable and solvable, we introduce the relaxation variable J to represent Z and substitute the constraint V = PTXZ into (1), leading to the equivalent problem:\nmin Z,E,P,J\n‖J‖∗ + λ‖E‖2,1 + F(Z,P),\ns. t. X = XZ+E,1TnZ = 1 T n ,Z = J.\n(2)\nwhere the constraint term is:\nF(Z,P) = αTr[PTXLXTP] + β‖PTXZ−Y‖2F . (3)\nThe above problem in (2) can be solved by minimizing its augmented Lagrangian function:\nL = ‖J‖∗ + λ‖E‖2,1 + F(Z,P) + Tr[ΨTa (X−XZ−E)] + Tr[ΨTb (Z− J)] + Tr[ΨTc (1TnZ− 1Tn )] + µ\n2 (‖X−XZ−E‖2F + ‖Z− J‖2F\n+ ‖1TnZ− 1Tn‖2F ),\n(4)\nwhere the matrices Ψa ∈ Rd×n, Ψb ∈ Rn×n, Ψc ∈ R1×n are the Lagrange multipliers, and µ > 0 is a penalty parameter. Now, this problem becomes an unconstrained one, which can be solved using an alternating strategy. In other words, we can respectively minimize the variables Z, E, P, J by holding the rest, followed by updating the Lagrange multipliers Ψa, Ψb, Ψc. The convergence of the inexact ALM algorithm has been provably guaranteed with mild conditions [9] when optimizing multiple variables.\nCalculation of P: By fixing Z, E, J and dropping the constant terms, (4) can be rewritten as:\nL(P) = F(P). (5)\nTaking its derivative and let ∇L(P) = 0, we can easily obtain the closed-form solution P:\nP = [X(αL + βZZT )XT ]−1XZYT . (6)\nThis step is more efficient than that in [29], which needs to solve the expensive generalized eigen-decomposition problem.\nCalculation of J: By holding the variables P, E, Z and eliminating the irrelevant terms, (4) degenerates to:\nL(J) = ‖J‖∗ + Tr[ΨTb (Z− J)] + µ\n2 ‖Z− J‖2F\n= 1\nµ ‖J‖∗ +\n1 2 ‖J− (Z+ 1 µ Ψb)‖2F .\n(7)\nWe employ the Singular Value Thresholding (SVT) operator [34] to compute the optimal J efficiently. Specifically, we first conduct Singular Value Decomposition (SVD)\non the matrix Z + 1 µ Ψb = UJΣJVJ , where ΣJ is a diagonal matrix with its entries being a group of singular values {σi}ri=1 (r is the rank). Thereafter, we can obtain the optimal solution J∗ = UJΩ 1\nµ ΣJVJ , where\nΩ 1 µ ΣJ = diag({σi − 1µ}+), and the marker “+” denotes the positive part.\nCalculation of Z: While keeping the variables P, E, J fixed, (4) reduces to:\nL(Z) = F(Z,P) + Tr[ΨTa (X−XZ) +ΨTb Z+ΨTc 1TmZ] + µ\n2 (‖X−XZ−E‖2F + ‖Z− J‖2F\n+ ‖1TmZ− 1Tn‖2F ).\n(8)\nNow let the derivative of (8) w.r.t. Z be zero, we have\nUaZ = Ub +Uc, (9)\nwhere\nUa = 2βX T PP T X+ µ(XTX+ 1n1 T n + In), (10)\nUb = 2βX T PY + µ(XTX−XTE+ J+ 1n1Tn ), (11)\nUc = X T Ψa −Ψb − 1nΨc. (12)\nThen, we can obtain the solution to Z, i.e.,\nZ = U−1a (Ub +Uc). (13)\nwhere the variable Ua is positive definite, which would make its inversion more stable to some extent during the solving process.\nCalculation of E: Similarly to the above routines, we fix the variables P, J, Z and drop the constant matrices, then (4) can be reformulated as\nmin E\nλ µ ‖E‖2,1 + 1 2 ‖E− (X−XZ+ 1 µ Ψa)‖2F . (14)\nThis problem has been solved by existing work [2] and its optimal solution is given by\nE(:, i) =\n{\n‖Ψi‖−λ ‖Ψi‖ Ψi, λ µ < ‖Ψi‖, 0, otherwise. (15)\nwhere Ψi is the i-th column vector of the matrix Ψa. Up to now, the solutions to all four variables have been obtained in the optimization framework, and the complete procedures are summarized in Algorithm 1. This framework can be used for both classification and regression tasks."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "In this section, we concentrate on discussing the advantages, the constraints, and the computational complexity of the proposed CLRR method.\nAlgorithm 1 Solving Problem (4) by Inexact ALM\nInput: The data matrix X, the label matrix Y, the parameters α, β, λ. Initialization: Set all entries of Z, J, E, Ψa, Ψb, Ψc to zero, µ = 0.01, µmax = 10\n6, ρ = 1.3, ǫ = 10−7. Procedure: 1: while not converged do 2: Fix the others and update P, J, Z and E, respec-\ntively, by solving (6), (7), (8) and (15). 3: Update the Lagrange multipliers:\nΨa ← Ψa + µ(X−XZ−E), Ψb ← Ψb + µ(Z− J), Ψc ← Ψc + µ(1TmZ− 1Tn ).\n4: Update the parameter µ: µ ← min(ρµ, µmax). 5: Check the convergence conditions:\n‖X−XZ−E‖∞ < ǫ, ‖Z− J‖∞ < ǫ, ‖1TmZ− 1Tn‖∞ < ǫ.\n6: end while Output: Z,E,P.\nAlgorithm 2 CLRR for Data Classification\nInput: The training data X and their labels Y, the testing data Xt.\n1: Utilize Algorithm 1 to derive the low-rank matrix Z for training data. 2: Learn the low-rank matrix Zt for testing data using degenerated Algorithm 1 while setting α = 0 and β = 0. 3: Recover the clean data by XZ and XtZt from corrupted training and testing data in respective. 4: Train a model using a classifier and predict the classes of the testing data points."
    }, {
      "heading" : "5.1 Advantages",
      "text" : "As we know, CLRR explicitly takes advantage of supervised information, i.e., data labels, to guide the lowrank learning and the robust subspace projection. This property naturally differentiates it from previous works [1], [2], [29], [35]. In this paper, we apply our method into several important real-world applications, i.e., data classification, pose estimation, and robust data recovery. For classification in Algorithm 2, we can either use the recovered data XZ or the reduced data representation P T XZ. However, in practice we found the classification performance of using PTXZ is less satisfying than that of using XZ, especially when the number of classes is small. This could be attributed to the fact that the dimension of the former is fixed by the number of classes in the training data, which may result in some information loss to low-rank based representation, while the latter does not. For regression tasks, e.g., pose estimation in Algorithm 3, we use the robust projecting subspace\nAlgorithm 3 CLRR for Human Pose Estimation\nInput: The training motion captured data X, the test data Xt, the matrix Y spanned by pose vectors (e.g., body joint angles).\n1: Learn the projecting subspace P using Algorithm 1.\n2: Estimate the pose of the test data by calculating P T Xt, where each test sample is stacked in the\ncolumn of Xt.\nAlgorithm 4 CLRR for Robust Data Recovery\nInput: The contaminated data X, the label matrix Y. 1: Learn the matrices Z and E through Algorithm 1. 2: Recover the clean data by computing X̃ = XZ. 3: Eliminate the noise existing in the contaminated data\nby the matrix E.\nP to estimate the outputs of testing data points, as shown in Algorithm 4. For robust data recovery, we directly employ XZ to recover the clean component from corrupted data. Since we enforce the low-rank representation and the robust projecting subspace to be intrinsically correlated with the data ground-truth by constraints, it will help yield discriminant representations, encouraging improved classification and regression performances. Meanwhile, the robust recovery is guided by the least squares constraint, allowing to better identify the noise in different classes. However, there exits a drawback in our method, i.e., it cannot directly recover the clean component of the testing data due to the lack of labels. Yet, to handle this, we can solve the degenerated objective function by simply dropping the third and the fourth regularization terms in (1)."
    }, {
      "heading" : "5.2 Constraints",
      "text" : "In all, there are two constraints in the objective function, i.e., the least squares regularizer and the Laplacian regularizer. Recall that the least squares constraint in our method allows to build the inter-connections among the discriminant lowest rank representation, the robust projecting subspace and the label structure of the training data. This actually encourages the supervised guidance when recovering the clean samples and mapping the data points onto the robust low-dimensional subspace. Moreover, the adaptive constraint on the lowdimensional representation can be equipped with the locality-preserving ability by manifold learning or the discriminating ability using the Fisher rule [36]. On the whole, these properties can be unified into a regularizer from a graph viewpoint. For instance, we consider a general regularizer in terms of the trace ratio criterion [8]. Given n data points with labels, each point is treated as a vertex and the relation between two vertices are encoded by an edge weight. Define two similarity matrices including the\nwithin-class matrix Ww ∈ Rn×n and the between-class matrix Wb ∈ Rn×n, in addition to the adjacency matrix Wn ∈ Rn×n, where Ww(i, j) = 1nc (nc is the number of samples in the c-th class) and Wb(i, j) = 1 n − 1 nc if xi and xj belong to class c, otherwise Ww(i, j) = 0 and Wb(i, j) = 1 n ; Wn(i, j) = 1 if xi and xj are nearest neighbours, otherwise zero. Thus, their corresponding Laplacian matrices are Lw = Dw −Ww, Lb = Db −Wb and Ln = Dn − Wn, where the entries of the diagonal matrices D are the column or row sums of the weight matrices W. As shown in [37], the sum of the withinclass scatter matrix Sw = P T XLwX T P and the betweenclass scatter matrix Sb = P T XLbX T P is the total-scatter matrix St = Sw + Sb [25], leading to Lt = Lw + Lb = In− 1n11T , which is a centering matrix. These Laplacian matrices are all positive semi-definite (PSD), and thus the sum of them is also PSD. They can be easily incorporated into our adaptive regularizer as an alternative of L, which could be also a graph-based matrix [7]. In empirical studies, we adopted the between-class scatter matrix Sb as the Laplacian regularizer. Additionally, CLRR can be extended to the semi-supervised scenario by modifying the adaptive regularizer as in [8], [38] where the prior knowledge is used as the supervised information. Apart from that, to better encode the lowrank representation, we can resort to a more informative dictionary learned from the data points iteratively as in [39]."
    }, {
      "heading" : "5.3 Computational Complexity Analysis",
      "text" : "For computational cost, we use the big O notation to express the computational complexity of the proposed method. In total, the objective function has four variables, whose solutions require iterative computing in the whole process. To update P, it needs O(n3) to compute ZZ\nT , which makes it be the most costing component in solving P. To update J, it needs O(n2r+r2n), where r is the rank of (Z+ 1\nµ Ψb). To update Z, it needs O(n2d+ndk)\nto compute Ua, Ub and O(n2d) to compute Uc. To update E, it needs O(d2) to compute E. Suppose the updates stop after t iterations, the overall cost for CLRR is\nO[t(n3 + n2(r + d) + r2n+ d2 + ndk)]. (16)\nIn practical tests, our objective function usually converges fast, and thus t is actually small. If the given data matrix is highly low-rank, e.g., many samples are linearly correlated, then we have r ≪ min(n, d). Besides, the number of classes k can be omitted compared to n. Therefore, (16) can be compacted into O(n3 + n2d + d2 + nd). Hence, the computational cost of CLRR is at the same level of SRRS [29] which also requires to compute ZZT . Moreover, during the iterations, SRRS requires to solve the Sylvester equation [40], which is very expensive and the solution would be unstable occasionally. The cost could be further reduced when the number of data points is much less than that of\nfeatures, i.e., n ≪ d. While both of them are more computationally intensive than several alternatives, e.g., LRR and SPCA, they are equipped with more inspiring advantages, such as least squares guidance, Fisher information and locality-preserving property, which could strengthen the informative structures of the recovered data space and the robust projecting subspace."
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : "In this section, we have conducted a broad range of experiments to examine the performance of the proposed CLRR method in three real-world applications, i.e., image classification, human pose estimation and robust face recovery. All experiments were carried out in MATLAB R2014a on Windows 7 with Intel i7-5820K CPU at 3.30GHz."
    }, {
      "heading" : "6.1 Image Classification",
      "text" : "In this part, we investigate classification performances of several state-of-the-art methods on three publicly available image databases, i.e., COIL100, VOC2012, Caltech101.\nDatabase descriptions: COIL1001 contains 7,200 samples and 100 categories, each of which has 72 samples; VOC20122 comes from the Visual Object Classes challenge 2012 [41], which has 17,125 samples covering 20 different categories; Caltech1013 consists of 8,677 pictures from 101 categories. Here, the background and clutter classes are abandoned. To represent the images, we adopt the toolbox used in [42] to extract their feature descriptors, i.e., GIST, LBP, dense HOG, dense SIFT. GIST [43] describes the spatial envelope of the image and has 512D; LBP [44] extracts the non-uniform local binary pattern and concatenates 3 levels of spatial pyramid to obtain final 1239D feature vector; dense HOG extracts HOG [45] in a dense manner and concatenates 2×2 cells to obtain a descriptor at each grid; dense SIFT extracts SIFT [46] in a dense manner at multiple patch sizes. For HOG and SIFT, we apply the bag-of-words and spatial pyramid pipeline to obtain the final feature vector, i.e., the LLC [47] framework using three layers with max pooling. To train the dictionary, we randomly select ten percent of the images or at least 20 images in each category to detect interest points, from which we used one million descriptors for clustering. The dictionary sizes of HOG and SIFT are 256 and 1,024, while their feature dimensions are 5,376 and 21,504, respectively.\nPerformance comparisons: We have investigated classification performances of the proposed CLRR method, ERE [26], SPCA [28], RPCA [1], LRR [2], FRR [6], Latent LRR (LLRR) [14], and SRRS [29]. Among them, ERE and SPCA are not low-rank subspace methods, but they can be both used to enhance classification performance by\n1. http://www.cs.columbia.edu/CAVE/software/softlib/coil100.php 2. http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/index.html 3. http://www.vision.caltech.edu/Image Datasets/Caltech101/\ndimensionality reduction as CLRR does. Since APCA [27] is a degenerated version of SPCA while ERE has been shown to perform better than Fisher LDA [26], we thus compare CLRR with ERE and SPCA. The rest are all low-rank learning methods, being ideal alternatives to the proposed approach.\nExperimental setups: Each database was randomly divided into three disjoint parts: training data (50%), validation data (20%), and testing data (30%). For all methods, best parameters were empirically searched by two-fold cross validation on the validation data. For ERE, the start point of the noise region was estimated by varying µ from 0 to 4 with an interval of 0.5. For SPCA, there are two weights αi and η in the covariance mixture, and αi was inversely proportional to the sample size of class i [27] while the optimal η was chosen from 2[−5:1:5] to differentiate the covariance matrix Σα from the total scatter matrix with η = 1 as in [28]. Suppose d was the feature dimension, we selected the optimal λ from [0.2 : 0.2 : 2]/ √ d for RPCA, [1 : 3 : 30]/ √ d for LRR and FRR, respectively. There are two parameters in LLRR and SRRS. For LLRR, we chose the optimal α, λ from [0.6 : 0.2 : 1.4]. For SRRS, we chose the optimal α, λ from 2[−4:2:4]. There are three parameters in CLRR. We chose the best α from 2[−8:1:3], β from 2[−3:1:5], and λ from 2[−5:1:3]. When there are more than one parameters, we adopted the one-by-one strategy, e.g., choose the best λ for LLRR while fixing α to 1, and then choose the best α using the optimal λ. Parameter selections were done on the validation data. We did the best to choose optimal parameters for each method, and the parameter intervals vary for different methods. Actually, they could be further tuned to refine the results. In some sense, the compared methods can be treated as subspace approaches for image classification, and the optimal dimensionality of the learned data representation is difficult to know. CLRR is also considered as a subspace learning method in this scenario. In particular, the learned data representation XZ consists of one basis matrix X (dictionary) and one coefficient matrix Z (lowrank subspace). Though it can directly use the full lowrank subspace Z for training the classification model, we empirically found the performance usually reaches optimal using less dimensions rather than the whole. Hence, we have varied the dimension from 50 to the full with a grid of 50 for all methods when conducting classification. Note that it would be possible that multiple dimensions all yield the optimal results, and we report those with less dimensions. We adopt the classification error rate as evaluation criterion. Though SRRS simply used Nearest-Neighbor (NN) as the classifier, we find it ineffective on large image databases. Instead, Support Vector Machine (SVM) [25] has been proved to be powerful in many applications, and here we utilized LibSVM [48] as primary classifier. In particular, we used the C-SVC model [48] with linear kernel and chose the best C from 2[−3:1:6] by two-fold cross validation on the validation data. During\nparameter selection for all compared methods, C was kept to 10. To better convince merits of the proposed method, we have also reported the results using MinimumMahalonobis Distance (MMD) classifier [49], which leverages within class scatter matrix of the data.\nTo sufficiently explore the performances of the above methods, we did some tests on the four different types of features, i.e., GIST (512D), LBP (1239D), HOG (2000D), SIFT (2000D), separately. The former two use the original dimension while the latter ones use the reduced dimen-\nsion 2000 by PCA [25]. The results of them are reported in Tables 1, 2, 3 and 4, respectively, where each record is paired with the dimensionality which produces the result. The best record in each column of the table is highlighted in boldface. From these results, a number of interesting observations can be found in the following.\n• CLRR systematically and consistently outperforms the compared methods on image classification. We attribute this to the fact that our method fully takes\nTABLE 5 Classification error rates on testing data using combined features with 400, 800 and 1600 dimensions by LibSVM.\nMethod Combined Features (400D) Combined Features (800D) Combined Features (1600D)\nCOIL100 VOC2012 Caltech101 COIL100 VOC2012 Caltech101 VOC2012 Caltech101\nERE 0.0057 (350) 0.4559 (350) 0.3167 (200) 0.0081 (650) 0.4419 (750) 0.2845 (400) 0.4413 (1600) 0.2609 ( 750) SPCA 0.0062 (250) 0.4561 (350) 0.3130 (400) 0.0081 (650) 0.4421 (650) 0.2912 (650) 0.4398 (1250) 0.2786 (1450) RPCA 0.0048 (200) 0.4390 (400) 0.2874 (200) 0.0052 (350) 0.4402 (750) 0.2732 (400) 0.4386 (1600) 0.2548 ( 700) LRR 0.0057 (350) 0.4482 (400) 0.2929 (200) 0.0048 (400) 0.4397 (750) 0.2650 (400) 0.4477 (1400) 0.2485 ( 750) FRR 0.0057 (350) 0.4352 (400) 0.2929 (200) 0.0043 (350) 0.4340 (750) 0.2657 (350) 0.4391 (1500) 0.2532 ( 800) LLRR 0.0052 (350) 0.4263 (350) 0.2707 (200) 0.0052 (400) 0.4223 (750) 0.2492 (400) 0.4329 (1400) 0.2351 ( 800) SRRS 0.0057 (400) 0.4311 (350) 0.2765 (200) 0.0048 (400) 0.4309 (750) 0.2593 (350) 0.4381 (1400) 0.2524 ( 750) CLRR 0.0052 (400) 0.4297 (250) 0.2472 (200) 0.0041 (750) 0.4221 (750) 0.2346 (400) 0.4186 (1300) 0.2274 ( 650)\nTABLE 6 Classification error rates on testing data for examining individual components of the proposed CLRR method.\nMethod GIST (512D) Combined Features (1600D)\nLibSVM MMD LibSVM MMD VOC2012 Caltech101 VOC2012 Caltech101 VOC2012 Caltech101 VOC2012 Caltech101\nCLRR 0.4912 (350) 0.2816 (500) 0.5936 (200) 0.3235 (100) 0.4186 (1300) 0.2374 ( 650) 0.5061 (1050) 0.2170 ( 550) CLRRa 0.5163 (400) 0.3038 (300) 0.6132 (150) 0.3425 (100) 0.4301 (1150) 0.2525 (1000) 0.5239 (1100) 0.2305 (1200) CLRRb 0.4971 (300) 0.2990 (500) 0.6010 (300) 0.3390 ( 50) 0.4260 ( 750) 0.2408 ( 900) 0.5152 ( 900) 0.2261 ( 600) CLRRc 0.5206 (350) 0.2911 (400) 0.6367 (200) 0.3418 (200) 0.4457 (1000) 0.2450 ( 300) 0.5411 (1100) 0.2376 ( 550)\nadvantage of the discriminant information encoded by the Laplacian regularizer and the supervised information by the adaptive least squares regularizer. Besides, enforcing each column sum of the lowrank matrix Z to be one has positive effects on classification. • The classification error rates of SRRS and CLRR are lower than those of other low-rank methods on GIST and HOG features of VOC2012, which demonstrates that supervised constraints are indeed beneficial for classification. Futhermore, CLRR achieves more performance improvements than that of SRRS, which empirically validates the efficacy of the enforced least squares regularizer. • LLRR can produce better classification results than most other methods on VOC2012 and Caltech101, which indicates the advantages of constructing the dictionary using both observed data and hidden data to complement the insufficient observations. Thus, it might be helpful for CLRR by considering this in a possible way. • ERE and SPCA, though simple, performs satisfactorily on COIL100, even better than some low-rank based methods. In particular, SPCA enjoys more promising performances than ERE overall, which suggests that considering class-specific information by assigning different weights indeed leads to a better model. However, it seems difficult for them to perform well on VOC2012 and Caltech101, which are more challenging than COIL100. • For features using original dimension, GIST exhibits better performances than LBP using LibSVM while LBP produces better results than GIST on COIL100 and VOC2012 using MMD, which indicates more dimensions do not necessarily enhance the per-\nformance. For features using reduced dimension, SIFT performs better than HOG on VOC2012 and Caltech101, which suggests SIFT would be a better choice for challenging databases. Overall, HOG and SIFT yield much better results than GIST and LBP in most cases, which demonstrates that reducing dimensions by PCA enables boosting classification performances. • Generally, the results derived from LibSVM are superior to those of MMD, which is more obvious on the latter two databases. This reflects that SVM often performs better than MMD in real-world applications. However, MMD could obtain comparable or even better results on COIL100, which means both of them can be used to well handle simple databases, where it is easy to distinguish different categories covered in the data.\nIn order to test the cumulative effect of combined features with different dimensions, we integrated the four kinds of features for each database. In particular, the dimension of each feature was first reduced to 100, 200, 400 by PCA individually, and then different features were concatenated to generate one vector for each sample. Consequently, we can obtain the final 400, 800, 1600- dimensional data vectors for testing. The results produced by LibSVM are shown in Table 5, where the best results are highlighted in boldface. From these results, it can be observed that higher dimension only shows slight improvements but requiring more computations. Actually, the cumulative effects are insignificant compared to the results produced by separated SIFT feature in Table 4, which indicates that sometimes one informative feature is good enough for classification task.\nTo examine the individual components of CLRR, we compare it with several alternatives of its objective func-\ntion in Eq. (1), i.e., we use CLRRa to denote the method removing the least squares term by setting β = 0, CLRRb removing the Laplacian term by setting α = 0, and CLRRc removing the low-rank term ‖Z‖∗ as well as setting β = 1. Previous parameter selections regarding CLRR also apply to these alternatives. For the variant CLRRc, we directly used the projection matrix P to map both the training data and the testing data into the new subspace, since the low-rank term was removed in this situation, failing to capture the coefficient matrix Z for the testing data due to the lack of label information. Unlike previous variants degenerating the objective function to LRR by abandoning the constraints, we could not optimize Z on the testing data since it is not included in the objective function. The tests were carried out on one separated feature, i.e., GIST (512D), and one combined feature with 1600 dimensions, respectively, the results of which on VOC2012 and Caltech101 are recorded in Table 6. From these results and those in Table 1 and 5, we see that CLRRa and CLRRb are comparable and even better than other methods, which indicates that using Laplacian regularizer or least squares regularizer individually is still able to capture informative representation. Moreover, CLRRb can reduce the error rate in a slightly larger magnitude than CLRRa, which suggests explicit label structure may have stronger guidance on learning low-rank subspaces than between-class scatter based Laplacian regularizer. More importantly, coupling both of the two supervised regularizers gives satisfying improvements on classification performance. In addition, CLRRc performs well on Caltech101 while it reports worse records on VOC2012, which would be due to the reason that the learned data representation of VOC2012 only has 20 dimensions, and thus some reliable information would miss during the projection.\nTo have an overview of the parameter sensitivity on compared methods, we plot several parameter selection results in Figure 1. This intuitively shows the influences of parameters on classification performance. From these figures, we observe that most parameters enjoy\npromising performances in a wide range on different databases. In practice, it is advised to choose the optimal parameters considering both the searching cost and the generalization ability of the method.\nTo show the convergence of iteration-based methods, we draw the convergence curves in Figure 2. As vividly depicted in the figures, CLRR converges fast in a few iterations on different databases as others do. While CLRR and SRRS are at the same level of computational complexity higher than other low-rank approaches, they enjoy more inspiring performances on image classification, which could compensate for the larger computing overheads. In particular, CLRR not only performs satisfactorily on classification, but also can be used for regression while the rest would fail. It should be acknowledged that ERE and SPCA indeed run fast compared with others, however, there exists a shortcoming that they can neither be applied to regression nor to robust recovery on noisy data, which can limit their potential applications.\nFurthermore, we illustrate several examples of choosing the optimal parameter C of the C-SVC model for LibSVM in Figure 3. As shown in the charts, LibSVM exhibits robustness in a wider range of parameter space on VOC2012 and Caltech101 than COIL100, and larger values are preferred as the proper C. Apart from that, we have provided several charts to show the classification performance using LibSVM against different dimensions of the data in Figure 4. From these curves, it can be seen that the performance of the reduced feature (SIFT) would reach the optimal with less dimensions, while that of the original feature (GIST) needs more dimensions. In some sense, this suggests that the leading dimensions of the reduced feature would dominate the data representation, and thus more dimensions are unable to further boost the performance.\nIn addition, we have displayed the confusion matrices generated by randomly choosing eight categories from the classification results on VOC2012 and Caltech101 for CLRR in Figure 5. From the figures, it is easy to calculate\n5 10 15 20 25 0\n0.2\n0.4\n0.6\n0.8\nIterations (RPCA + SIFT)\nC on\nve rg\nen ce\nV al\nue (\nT ra\nin in\ng)\nCOIL100 VOC2012 Caltech101\n20 40 60 80 0\n10\n20\n30\n40\n50\nIterations (LRR + SIFT)\nC on\nve rg\nen ce\nV al\nue (\nT ra\nin in\ng)\nCOIL100 VOC2012 Caltech101\n50 100 150 0\n5\n10\n15\n20\n25\nIterations (FRR + SIFT)\nC on\nve rg\nen ce\nV al\nue (\nT ra\nin in\ng)\n×10\n2\nCOIL100 VOC2012 Caltech101\n50 100 150 200 250 0\n2\n4\n6\n8\n10\nIterations (LLRR + GIST)\nC on\nve rg\nen ce\nV al\nue (\nT ra\nin in\ng)\n×10\nCOIL100 VOC2012 Caltech101\n10 20 30 40 0\n5\n10\n15\n20\n25\n30\nIterations (CLRR + GIST)\nC on\nve rg\nen ce\nV al\nue (\nT ra\nin in\ng)\n×10\nCOIL100 VOC2012 Caltech101\nFig. 3. Model selection of parameter C in the C-SVC model of LibSVM on validation set of three image databases. Others follow the same fashion.\n500 1000 1500 2000 0\n0.2\n0.4\n0.6\n0.8\nDimension (ERE + SIFT)\nC la\nss ifi\nca tio\nn E\nrr or\nR at\ne\nCOIL100 VOC2012 Caltech101\n500 1000 1500 2000 0\n0.1\n0.2\n0.3\n0.4\n0.5\nDimension (RPCA + SIFT)\nC la\nss ifi\nca tio\nn E\nrr or\nR at\ne\nCOIL100 VOC2012 Caltech101\n500 1000 1500 2000 0\n0.1\n0.2\n0.3\n0.4\nDimension (CLRR + SIFT)\nC la\nss ifi\nca tio\nn E\nrr or\nR at\ne\nCOIL100 VOC2012 Caltech101\n100 200 300 400 500 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDimension (FRR + GIST)\nC la\nss ifi\nca tio\nn E\nrr or\nR at\ne\nCOIL100 VOC2012 Caltech101\n100 200 300 400 500 0\n0.2\n0.4\n0.6\n0.8\nDimension (LLRR + GIST)\nC la\nss ifi\nca tio\nn E\nrr or\nR at\ne\nCOIL100 VOC2012 Caltech101\nFig. 4. Classification performance against different dimensions of three image databases. Others follow the same fashion.\nthe True Positive Rate and the False Positive Rate for each category. This also gives us a possible direction to improve the integrated classification performance. Other methods have similar behaviors."
    }, {
      "heading" : "6.2 Human Pose Estimation",
      "text" : "To investigate the regression performance of the proposed CLRR method, we consider the problem of estimating 3D configurations of complex articulated objects from monocular images for applications requiring 3D human body pose analysis. The database is detailed in [50]. We choose the image silhouettes, as they are more reliably extractable. And the shape context distributions is used to give rise to 100D features for each sample. The 3D body pose is recovered as a 54D vector, including three joint angles for each of the 18 major body joints. Just like [50], we simply regress the original motion capture-based training format in the form of Euler angles. In total, we have used 1,691 training points coming from seven sequences and 418 testing points from one sequence, while the silhouette descriptors have 100 dimensions. We compare CLRR with three popular regression methods including Least Squares Regression (LSR), Relevance Vector Machines (RVMs) [51] and Robust Regression (RR) [30]. The best parameters were obtained by five-fold cross-validation on training data. The results in terms of angle error are reported in Table 7. It can be observed that CLRR performs best, which is due to two reasons. On one hand, the robust projecting subspace derived from low-rank learning is more discriminative than LSR and RVM; on the other hand, CLRR respects\nthe underlying data structure better than RR does, as RR implicitly assumes only a single subspace exists. In addition, we have drawn the recovered 3D body poses in Figure 6, which shows a person walking from left to right and then in the inverse direction. We can see the recovered body poses are very close to the extracted silhouettes, demonstrating the favorable performance of our method."
    }, {
      "heading" : "6.3 Robust Face Recovery",
      "text" : "To examine the robustness of CLRR, we conducted experiments on the Cohn-Kanade AU-Coded Facial Expression Database4 [52], [53] In line with the user agreement, we randomly choose six available subjects including 848 images. They were cropped and resized to 64×64. Here, we show the recovery effects of our method and LRR on several examples contaminated by artificial noise. To capture favorable performance, we have tuned the parameters to the best, and the resulting images are illustrated in Figure 7. These images indicate the superiority of our method to handle the noisy scenario. Specifically, the recovered faces by CLRR have much less noises compared with LRR. This is because the low-rank subspace Z is learned by simultaneously considering the Laplacian regularizer and the least squares regularizer, which encodes the supervised information in appropriate structures. Thus, the recovered faces from XZ are clearer compared with LRR. From another perspective, the noise of the faces is better encoded by the error matrix E when supervised information is employed in decomposing the noisy matrix into two components, i.e., the recovered component and the error component. Hence, these images have shown the effectiveness of the proposed method in the adverse situation where data points involve contaminations. In addition, the mouth of the fifth individual is closed by CLRR, which might be due to the reason that when learning the low-rank subspace, the samples would utilize the information of other samples within the same category. Furthermore, we observed that both LRR and CLRR share similar block-diagonal structures w.r.t. six categories, which can be attributed to the fact that they are essentially low-rank learning approaches with only different constraints."
    }, {
      "heading" : "7 CONCLUSIONS",
      "text" : "This paper has developed a novel Constrained LowRank Representation method, which provides a sensible way to jointly learn both the discriminant lowest rank\n4. http://www.pitt.edu/ emotion/ck-spread.htm\nrepresentation and the robust projecting subspace. Unlike most low-rank learning methods neglecting supervised information, we explicitly utilize label information via the adaptive least squares regularizer, such that the learned lowest rank representation and low-dimensional subspace have more discriminating power. Hence, it can naturally improve the performances of several realworld applications. The objective function is formulated as a constrained rank minimization problem solved by the inexact Augmented Lagrange Multiplier algorithm. Moreover, we have some discussions regarding the advantages, constraints and computational complexity analysis of CLRR. To demonstrate the effectiveness of our method, comprehensive experiments were conducted on image classification, human pose estimation, and robust face recovery. Results have clearly justified\nthe promising efficacy of the proposed approach."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors would like to thank the anonymous reviewers for their helpful and constructive comments that have greatly contributed to improving this manuscript."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace<lb>segmentation, image categorization. Most low-rank methods are incapable of capturing low-dimensional subspace for supervised<lb>learning tasks, e.g., classification and regression. This work aims to learn both the discriminant low-rank representation and the robust<lb>projecting subspace in a supervised manner. To achieve this goal, we cast the problem into a constrained rank minimization framework<lb>by adopting the least squares regularization. Naturally, the data label structure tends to resemble that of the corresponding low-<lb>dimensional representation, which is derived from the robust subspace projection of clean data by low-rank learning. Moreover, the<lb>low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint,<lb>e.g., Laplacian regularizer. Therefore, we propose a novel Constrained Low-Rank Representation (CLRR) method. The objective<lb>function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact Augmented Lagrange<lb>Multiplier algorithm. Extensive experiments on image classification, human pose estimation and robust face recovery have confirmed<lb>the superiority of our method.",
    "creator" : "LaTeX with hyperref package"
  }
}