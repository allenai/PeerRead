{
  "name" : "1606.02012.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Can neural machine translation do simultaneous translation?",
    "authors" : [ "Kyunghyun Cho", "Masha Esipova" ],
    "emails" : [ "kyunghyun.cho@nyu.edu", "masha.esipova@nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation."
    }, {
      "heading" : "1 Introduction",
      "text" : "Simultaneous translation differs from a more usual consecutive translation. In simultaneous translation, the objective of a translator, or a translation system, is defined as a combination of quality and delay, as opposed to consecutive translation in which translation quality alone matters. In order to minimize delay while maximizing quality, a simultaneous translator must start generating symbols in a target languages before a full source sentence is received.\nConventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013). A segmentation algorithm, or model, first divides a source sentence into phrases. Each phrase is then\ntranslated by an underlying, often black box, translation system, largely independent of the preceding phrases. These two stages are separate from each other, in that the exchange of information between these two modules is limited.\nIn this paper, we study the problem of simultaneous translation in the context of neural machine translation (Forcada and Ñeco, 1997; Sutskever et al., 2014; Bahdanau et al., 2015). Rather than attempting to build a completely novel model along with a new training algorithm, we design a novel decoding algorithm, called simultaneous greedy decoding, that is capable of performing simultaneous translation with a neural machine translation model trained to maximize the quality of consecutive translation. Unlike previous approaches, our proposal performs segmentation and translation jointly based solely on the translation quality (indirectly measured by the conditional probabilities.) Furthermore, translation of each and every segment is fully conditioned on all the preceding segments through the hidden states of a recurrent network.\nWe extensively evaluate the proposed simultaneous greedy decoding together with two waiting criteria on three language pairs–En-Cs, En-De and EnRu. Our analysis reveals that it is indeed possible to use an existing neural machine translation system for simultaneous translation, and the proposed algorithm provides a way to control the trade-off between quality and delay. Our qualitative analysis on En-Ru simultaneous translation reveals interesting behaviours such as phrase repetition as well as premature commitment.\nWe consider this work as a first step toward build-\nar X\niv :1\n60 6.\n02 01\n2v 1\n[ cs\n.C L\n] 7\nJ un\n2 01\ning a full simultaneous translation system based on neural machine translation. In the conclusion, we identify the following directions for the future research. First, a trainable waiting criterion will allow a deeper integration between simultaneous decoding and neural machine translation, resulting in a better simultaneous translation system. Second, we need to eventually develop a learning algorithm specifically for simultaneous translation."
    }, {
      "heading" : "2 Attention-based Neural Translation",
      "text" : "Neural machine translation (Forcada and Ñeco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al., 2003). For instance, in the translation task of WMT’16, the top rankers for En↔Cs, En↔De, En→Fi and En→Ru all used attention-based neural machine translation (Bahdanau et al., 2015; Luong et al., 2015).1\nThe attention-based neural machine translation is built as a composite of three modules–encoder, decoder and attention mechanism. The encoder is usually implemented as a recurrent network that reads a source sentence X = (x1, . . . ,xTx) and returns a set of context vectors C = {h1, . . . ,hTX}, where\nht = φenc(ht−1,xt). (1)\nInstead of a vanilla, unidirectional recurrent network (Luong et al., 2015), it is possible to use a more sophisticated network such as a bidirectional recurrent network (Bahdanau et al., 2015) or a tree-based recursive network (Eriguchi et al., 2016).\nThe decoder is a conditional language model based on a recurrent network (Mikolov et al., 2010). At each time step t ′, it first uses the attention mechanism to compute a single time-dependent vector out of all the context vectors: ct ′ = ∑Txt=1 αtht , where\nαt ∝ exp( fatt(zt ′−1, ỹt ′−1,ht)) . (2)\nzt−1 and ỹt−1 are the decoder’s hidden state and the previous target word. This content-based attention mechanism can be extended to incorporate also the location of each context vector (Luong et al., 2015).\n1newstest2016 at http://matrix.statmt.org/\nAlgorithm 1 Simultaneous Greedy Decoding Require: δ , s0, Input Pipe X , Output Pipe Y\n1: Initialize s← s0, C← READ(X ,s), C′←{} 2: Initialize the decoder’s state z0 based on C 3: while true do 4: ŷt = argmaxyt log p(yt |y<t ,C) 5: if s≥ TX then 6: WRITE(Y, ŷt), t← t +1 7: else 8: C′← READ(X ,δ ) if |C′|= 0. 9: if Λ(C,C∪C′) then\n10: C←C∪C′, s← s+δ , C′←{} 11: continue 12: else 13: WRITE(Y, ŷt), t← t +1 14: end if 15: end if 16: if ŷt = 〈eos〉 then 17: break 18: end if 19: end while\nThe decoder updates its own state with this time-dependent context vector by zt ′ = φdec(zt ′−1, ỹt ′−1,ct ′). The distribution over the next target word is then computed by\np(yt ′ = j|ỹ<t ′ ,X) ∝ exp(φoout(zt ′)) .\nThe initial state of the decoder is often initialized as\nz0 = φinit(C). (3)\nThe whole model, consisting of the encoder, decoder and attention mechanism, is jointly trained to maximize the log-likelihood given a set of N training pairs: maxθ 1N ∑ N n=1 ∑ T ny t ′=1 log p(y n t ′ |yn<t ′ ,Xn), where θ denotes all the parameters."
    }, {
      "heading" : "3 Simultaneous Greedy Decoding",
      "text" : "We investigate the potential of using a trained neural machine translation as a simultaneous translator by introducing a novel decoding algorithm. In this section, we propose and describe such an algorithm, called simultaneous greedy decoding, in detail."
    }, {
      "heading" : "3.1 Algorithm",
      "text" : "An overall algorithm is presented in Alg. 1.\nInput Arguments The proposed simultaneous greedy decoding has two hyperparameters that are used to control the trade-off between delay and quality. They are the step size δ and the number of initially-read source symbols s0. We will describe how the delay is defined later in Sec. 3.3.\nAs the goal is to do simultaneous translation, the algorithm receives two input X and output Y pipes instead of a full source sentence at the beginning. Each pipe comes with two API’s which are WRITE and READ. WRITE(P,x) commits the symbol x to the pipe P, and READ(P,n) receives n symbols from the pipe P and computes the additional context vectors using the encoder (line 11 of Alg. 1).\nState The proposed algorithm maintains the following state variables:\n1. s: # of received source words 2. t: # of committed target words + 1 3. C = {h1, . . . ,hs} 4. C′: additional context vectors 5. zt−1: the decoder’s latest hidden state\nInitialization Initially, the algorithm reads s0 source symbols from X and initialize its context set C to contain the context vectors h1, . . . ,hs0 . Based on this initial context set, the decoder’s hidden state z0 is initialized according to Eq. (3). See lines 2–3.\nTranslation Core At each iteration, the algorithm checks whether the full source sentence has been read already (line 6). If so, it simply performs greedy decoding by committing the most likely target symbol to Y , as would have been done in consecutive translation, while incrementing t by one for each committed target word (line 7).\nOtherwise, the algorithm reads δ more source symbols from X and forms an additional context set C′, if C′ is currently empty. It then compares the conditional distributions over the next target symbols yt given either the current context set C or the new context set C∪C′ based on a predefined criterion Λ. The criterion should be designed so as to determine whether the amount of information in C is enough to make a decision in that the additional information C′ would not make a difference. Two such criteria will be described later in Sec. 3.2\nIf this waiting criterion is met (i.e., it is determined better to consider C′,) the context set is updated to include the additional context vectors (line\n13,) and C′ is flushed. If not, the algorithm commits the most likely target symbols ŷt given the current context set C and increments t by one (line 15).\nThe iteration terminates when the last committed symbol ŷt is the end-of-sentence symbol.\nComputational Complexity The worse-case complexity of this simultaneous greedy decoding is twice as expensive as that of a consecutive greedy decoding is, with some trivial cacheing of C′, log p(yt |y<t ,C) and log p(yt |y<t ,C∪C′). The worse case happens when the waiting criterion is met for all the source symbols, in which case the proposed algorithm reduces to the consecutive greedy translation. The complexity however drops significantly in practice as the waiting criterion is satisfied more, because this reduces the number of context vectors over which the alignment weights are computed (see Eq. (2).) The empirical evaluation, which will be presented later in this paper, reveals that we can control this behaviour by adjusting δ and s0.\nWhy “Greedy” Decoding? It is possible to extend the proposed algorithm to be less greedy. For instance, instead of considering a single target word at a time (ŷt), we can let it consider a multi-symbol sequence at a time. This however increase the computational complexity at each time step greatly, and we leave it as a future research."
    }, {
      "heading" : "3.2 Waiting Criteria Λ",
      "text" : "At each point in time, the simultaneous greedy decoding makes a decision on whether to wait for a next batch of source symbols or generate a target symbol given the current context (C∪C′ in line 10 of Alg. 1.) This decision is made based on a predefined criterion Λ(C,C∪C′). In this paper, we investigate two different, but related criteria.\nCriterion 1: Wait-If-Worse The first alternative is Wait-If-Worse. It considers whether the confidence in the prediction based on the current source context (up to s source words) decreases with more source context (up to s+ δ source words). This is detected by comparing the log-probabilities of the target word selected to be most likely when only the\nfirst s source words were considered, i.e.,\nΛ(C,C∪C′) : ( log p(ŷ|ŷ<t ,C) (4) > log p(ŷ|ŷ<t ,C∪C′)),\nwhere ŷ = argmaxy p(y|ŷ<t ,C).\nCriterion 2: Wait-If-Diff The next alternative, Wait-If-Diff, compares the most likely target words given up to s and up to s+δ source words. If they are same, we commit to this target word. Otherwise, we wait. This is different from the Wait-If-Worse criterion, because the decrease in the log-probability of the most likely word given more source words does not necessarily imply that the most likely word has changed. We define this criterion as\nΛ(C,C∪C′) : (ŷ 6= ŷ′), (5)\nwhere ŷ′ = argmaxy log p(y|ŷ<t ,C∪C′).\nOther Criteria Although we consider the two criteria described above, it is certainly possible to design another criterion. For instance, we have tested the following entropy-based criterion during our preliminary experiments:\nΛ(C,C∪C′) : (H (y|ŷ<t ,C)> H (y|ŷ<t ,C∪C′)).\nThat is, the algorithm should wait for next source symbols, if the entropy is expected to decrease (i.e., if the confidence in prediction is expected to increase.) This criterion however did not work as well."
    }, {
      "heading" : "3.3 Delay in Translation",
      "text" : "For each decoded target symbol ŷt , we can track how many source symbols were required, i.e., |C ∪C′| in line 10 of Alg. 1. We will use s(t) to denote this quantity. Then, we define the (normalized) total amount of time spent for translating a give source sentence, or equivalent delay in translation, as\n0 < τ(X ,Ŷ ) = 1 |X ||Ŷ |\n|Ŷ | ∑ t=1 s(t)≤ 1. (6)\nIn the case of full translation, as opposed to simultaneous translation, τ(X ,Y ) = 1 for all Y . This follows from the fact that s(t) = |X | for all t. In the case of word-by-word translation in which case |X |= |Y |, τ(X ,Y ) = 0.5, because s(t) = t.\nAlignment vs. Delay s(t) however does not accurately reflect on which source prefix the t-th target symbol is conditioned. It is rather s′(t)= |C| instead, which reflects the alignment between the source and target symbols. It is thus more useful to inspect s′(t) in order to understand the behaviour of the proposed simultaneous greedy decoding."
    }, {
      "heading" : "4 Related Work",
      "text" : "Perhaps not surprisingly, there are only a small number of prior work in simultaneous machine translation. Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013). Often, incoming speech is transcribed by an automatic speech recognition (ASR) system, and the transcribed text is segmented into a translation unit largely based on acoustic and linguistic cues (e.g., silence or punctuation marks.) Each of these segments is then translated largely independent from each other by a separate machine translation system. The simultaneous greedy decoding with neural machine translation, proposed in this paper, is clearly distinguished from these approaches in that (1) segmentation and translation happen jointly to maximize the translation quality and (2) each segmentation is strongly dependent on all the previous segment in both the source and target sentences.\nMore recently, Grissom II et al. (2014) and Oda et al. (2014) proposed to extend those earlier approaches by introducing a trainable segmentation policy which is trained to maximizes the translation quality. For instance, the trainable policy introduced in (Grissom II et al., 2014) works by keeping an intermediate source/translation prefix, querying an underlying black-box machine translation system and deciding to commit these intermediate translations once a while. The policy is trained in the framework of imitation learning (Daumé Iii et al., 2009). In both of these cases translation is still done largely per segment, unlike the simultaneous translation with neural machine translation. We however find this idea of learning a policy for simultaneous decoding be directly applicable to the proposed simultaneous greedy decoding and leave it as a future work.\nAnother major contribution by Grissom II et al.\n(2014) was to let the policy predict the final verb of a source sentence, which is especially useful in the case of translating from a verb-final language (e.g., German) to another type of language. They do so by having a separate verb-conditioned n-gram language model of all possible source prefixes. This prediction, explicit in (Grissom II et al., 2014) is however done implicitly in neural machine translation, where the decoder acts as strong language model."
    }, {
      "heading" : "5 Experimental Settings",
      "text" : "Tasks and Corpora We extensively study the proposed simultaneous greedy decoding algorithm on three language pairs–En-Cs (12.12m sentence pairs), En-De (4.15m) and En-Ru (2.32m)– and in both directions per pair. We use all the parallel\ncorpora available from WMT’15.2 All the sentences were first tokenized3 and segmented into subword units using byte pair encoding (BPE) following (Sennrich et al., 2015b). During training, we only use sentence pairs, where both sides are less than or equal to 50 BPE subword symbols long.\nnewstest-2013 is used as a validation set both to early-stop training and to extensively evaluate the proposed simultaneous greedy decoding algorithm. We use newstest-2015 as a test set to confirm that the neural translation models used in the experiments are reasonably well trained.\nTranslation Models In total, we train six separate neural translation models, one for each pair–\n2http://www.statmt.org/wmt15/ 3tokenizer.perl from Moses (Koehn et al., 2007).\nnet as an encoder, the translation qualities are comparable. direction. We use a unidirectional recurrent network with 1028 gated recurrent units (GRU, (Cho et al., 2014)) as an encoder. A decoder is similarly a recurrent neural net with 1028 GRUs. The soft-alignment function is a feedforward network with one hidden layer consisting of 1028 tanh units. Each model is trained with Adadelta (Zeiler, 2012) until the average log-probability on the validation set does not improve, which takes about a week per model.\nThese trained models do not achieve the state-ofthe-art translation qualities, as they do not exploit the ensemble technique (Sutskever et al., 2014) nor monolingual corpus (Sennrich et al., 2015a), both of which have been found to be crucial in improving neural machine translation. Under these constraints, however, the trained models translates as well as those reported earlier by, for instance, Firat et al. (2016), as can be seen in Table 1.\nDecoding Parameters With the proposed simultaneous greedy decoding, we vary δ ∈ {1,2,3} and s0 ∈ {2,3,4,5,6,7}. For each combination, we re-\nport both BLEU and the delay measure τ from Eq. (6). In order to put the translation quality of the simultaneous translation in perspective, we report BLEU scores from consecutive translation with both greedy and beamsearch decoding. The beam width is set to 5, as used in (Chung et al., 2016)."
    }, {
      "heading" : "6 Quantitative Analysis",
      "text" : "Trade-off between Quality and Delay As shown in Fig. 1, there is a clear trade-off between the translation quality and delay. This trade-off is observed regardless of the waiting criterion, although it is more apparent with the Wait-If-Diff. Out of two waiting criteria, the Wait-If-Worse tends to achieve a better translation quality, while it has substantially higher delay in translation.\nDespite this general trend, we notice significant variations among the three languages we considered. First, in the case of German, we see the trade-off between the delay and quality is maintained in both translation directions (En→De and De→En), while the delay slightly decreases when translating to English. A similar trend is observed with Czech, except that the Wait-If-Diff criterion tends to improve the translation quality while maintaining the delay. On the other hand, the experiments with Russian show that this trend is not universal across languages.\nIn the case of Russian, translation to English does not enjoy any improvement in translation quality, as consecutive translation to English did (compare F\nvs. P in the right panel of Fig. 1.) Instead, there is a general trend of lowered delay when translating Russian to English compared to the other way around.\nWe observe in all the cases that the delay decreases when the model translates to English, compared to translating from English, with the exception of Czech and the Wait-If-Worse criterion. We conjecture that the richness of morphology in all the three languages compared to English is behind this phenomenon. Because each word in these languages has more information than a usual English word, it becomes easier for the simultaneous greedy decoder to generate more target symbols per source symbol. The same explanation applies well to the increase in delay when translating from English, as the languages with richer morphology often require complex patterns of agreement across many words.\nWait-If-Worse vs. Wait-If-Diff The Wait-If-Diff criterion tends to cover wider spectra of the delay and the translation quality. On the other hand, with the same set of decoding parameters–δ and s0–, the Wait-If-Worse results in more delayed translation with less variance in translation quality. In order to further examine the difference between these two criteria, we plot the effect of δ and s0 on both the delay τ and translation quality in Fig. 2.\nIn Fig. 2, we see a stark difference between these two criteria. This difference reveals itself when we inspect the translation quality w.r.t. the decoding parameters (right panel of each sub-figure). The WaitIf-Worse criterion is clearly more sensitive to s0, while the Wait-If-Diff one is more sensitive to δ . The only exception is the case of translating Czech to English, in which case the Wait-If-Diff behaves similar to the Wait-If-Worse when s0 ≥ 5. On the other hand, the delay patterns w.r.t. the decoding parameters are invariant to the choice of criterion."
    }, {
      "heading" : "7 Qualitative Analysis",
      "text" : "We define a metric of quality-to-delay ratio as Q2D = BLEUτ̃ , where τ̃ is an average delay over a test corpus. We use this ratio, which prefers a translation system with a high score and low delay, to choose the best model for each language pair–direction.\nIn Fig. 3, we present the simultaneous translation examples by the selected models when translating from English to a target language. We alternate between red and blue colors to indicate the correspondence between the target symbols and their source context. The source sentence is divided into chunks based on s′(t) defined in Sec. 3.3. We see that the Wait-If-Worse is relatively conservative than the Wait-If-Diff, which was observed earlier in Fig. 1."
    }, {
      "heading" : "7.1 Ru-En Simultaneous Translation",
      "text" : "Here, we take more in-depth look at the simultaneous greedy decoding with En-Ru as an example.\nWord/Phrase Repetition with Wait-If-Diff In Fig. 4, we show the Russian translations of “All the effect of vitamin D on cancer is not clear.” with three different settings. The most obvious observation we can make is again that the Wait-If-Worse is much more conservative than the other criterion (compare (a) vs. (b–c).)\nAnother noticeable phenomenon happens with the Wait-If-Diff, which is that some words or phrases are repeated as being translated. For instance in Fig. 4 (c), we see that “также эффект” (“also, effect”) was repeated twice. This has been observed with other sentences. As another example, see Fig. 6\nwhere “не означает” (“not means”) was repeated three times. We conjecture that this is an inherent weakness in the Wait-If-Diff criterion which does not take into account the decrease in confidence (i.e., the output conditional distribution becoming flatter) unlike the other criterion Wait-If-Worse.\nPremature Commitment In Russian, a preceding adjective needs to agree with the following noun. This is unlike in English, and when translating from English to Russian, premature commitment of adjectives may result in an inaccurate translation. For example in Fig. 7, we see that the simultaneous greedy decoding committed too early to plural adjectives (“нормальных” and “инфракрасных”, “normal” and “infrared”) before it saw the noun “photography”. The decoder translated “photography”, which is singular, into “фотографий” (“photos”).\nAcombination of normal and infra– red phot– ography produced the spectac– ular col– ouring .<eos>\nСо– отношение нормаль– ных иинфра– крас– ных фотографий ,произвед– енных впечатля– ющих кол– ор– ит– ной цвет– овой .\nFigure 7: An example of premature commitment when simultaneously translating using the Wait-If-Diff criterion.\nRussian-to-English Translation and Other Language Pairs In the other direction (Ru→En), we observed a similar trend. For instance, the Wait-IfWorse tends to be more conservative than the WaitIf-Diff (see Fig. 5), while the latter more often results in repeated phrases. Our manual inspection of other language pairs revealed similar behaviours."
    }, {
      "heading" : "8 Discussion and Future Research",
      "text" : "The quantitative analyses have revealed two major findings. First, we found that it is indeed possible to use the neural machine translation model trained without simultaneous translation as a goal for the\npurpose of simultaneous machine translation. This was done simply by a novel simultaneous greedy decoding algorithm described in Alg. 1. The algorithm, which has two adjustable parameters–s0 and δ , allows a user to smoothly trade off translation delay and quality, as shown in Fig. 1.\nThe second finding is that this trade-off property depends heavily on the choice of waiting criterion. Two criteria introduced in this paper showed markedly opposite behaviours, where the Wait-IfWorse was sensitive to s0 while the other, WaitIf-Diff, was to δ . We suspect that this difference in sensitivity has led to stark qualitative differences such as the ones discussed in Sec. 7.1.\nThis paper presents the first work investigating the potential for simultaneous machine translation in a recently introduced framework of neural machine translation. Based on our findings, we anticipate further research in the following directions.\nFirst, the waiting criteria proposed in this paper are both manually designed and does not exploit rich information embedded in the hidden representation learned by the recurrent neural networks. Information captured by the hidden states is however difficult to extract, and we expect a trainable criterion that takes as input the hidden state to outperform the manually designed waiting criteria in terms of both delay and quality\nSecond, all the translation models tested in this paper were not trained to do simultaneous translation. More specifically, the decoder was exposed to a full set of context vectors returned by the encoder only after the full source sentence was read. A training procedure that addresses this mismatch between the context sets seen by the decoder during training and test phases should be designed and evaluated."
    }, {
      "heading" : "Acknowledgments",
      "text" : "KC thanks Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015–2016)."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR 2015.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Real-time incremental speech-to-speech translation of dialogs",
      "author" : [ "Srinivas Bangalore", "Vivek Kumar Rangarajan Sridhar", "Prakash Kolan", "Ladan Golipour", "Aura Jimenez." ],
      "venue" : "NAACL, pages 437–445.",
      "citeRegEx" : "Bangalore et al\\.,? 2012",
      "shortCiteRegEx" : "Bangalore et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv:1406.1078.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "A character-level decoder without explicit segmentation for neural machine translation",
      "author" : [ "Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ACL.",
      "citeRegEx" : "Chung et al\\.,? 2016",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Search-based structured prediction",
      "author" : [ "Hal Daumé Iii", "John Langford", "Daniel Marcu." ],
      "venue" : "Machine learning, 75(3):297–325.",
      "citeRegEx" : "Iii et al\\.,? 2009",
      "shortCiteRegEx" : "Iii et al\\.",
      "year" : 2009
    }, {
      "title" : "Tree-to-sequence attentional neural machine translation",
      "author" : [ "Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka." ],
      "venue" : "ACL.",
      "citeRegEx" : "Eriguchi et al\\.,? 2016",
      "shortCiteRegEx" : "Eriguchi et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-way, multilingual neural machine translation with a shared attention mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Firat et al\\.,? 2016",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "Recursive hetero-associative memories for translation",
      "author" : [ "Mikel L. Forcada", "Ramón P. Ñeco." ],
      "venue" : "IWANN’97, pages 453–462.",
      "citeRegEx" : "Forcada and Ñeco.,? 1997",
      "shortCiteRegEx" : "Forcada and Ñeco.",
      "year" : 1997
    }, {
      "title" : "Simple, lexicalized choice of translation timing for simultaneous speech translation",
      "author" : [ "Tomoki Fujita", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura." ],
      "venue" : "INTERSPEECH, pages 3487– 3491.",
      "citeRegEx" : "Fujita et al\\.,? 2013",
      "shortCiteRegEx" : "Fujita et al\\.",
      "year" : 2013
    }, {
      "title" : "Don’t until the final verb wait: Reinforcement learning for simultaneous machine translation",
      "author" : [ "Alvin Grissom II", "He He", "Jordan L Boyd-Graber", "John Morgan", "Hal Daumé III." ],
      "venue" : "EMNLP, pages 1342– 1352.",
      "citeRegEx" : "II et al\\.,? 2014",
      "shortCiteRegEx" : "II et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "EMNLP, pages 1700–1709.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Statistical phrase-based translation",
      "author" : [ "Philipp Koehn", "Franz Josef Och", "Daniel Marcu." ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages",
      "citeRegEx" : "Koehn et al\\.,? 2003",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "Moses: Open source toolkit for statistical machine translation",
      "author" : [ "Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens" ],
      "venue" : null,
      "citeRegEx" : "Koehn et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2007
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1508.04025.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "INTERSPEECH, 2:3.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Optimizing segmentation strategies for simultaneous speech translation",
      "author" : [ "Yusuke Oda", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura." ],
      "venue" : "ACL, pages 551–556.",
      "citeRegEx" : "Oda et al\\.,? 2014",
      "shortCiteRegEx" : "Oda et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1511.06709.",
      "citeRegEx" : "Sennrich et al\\.,? 2015a",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1508.07909.",
      "citeRegEx" : "Sennrich et al\\.,? 2015b",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Segmentation strategies for streaming speech translation",
      "author" : [ "Vivek Kumar Rangarajan Sridhar", "John Chen", "Srinivas Bangalore", "Andrej Ljolje", "Rathinavelu Chengalvarayan." ],
      "venue" : "HLT-NAACL, pages 230–238.",
      "citeRegEx" : "Sridhar et al\\.,? 2013",
      "shortCiteRegEx" : "Sridhar et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le." ],
      "venue" : "NIPS, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Incremental segmentation and decoding strategies for simultaneous translation",
      "author" : [ "Mahsa Yarmohammadi", "Vivek Kumar Rangarajan Sridhar", "Srinivas Bangalore", "Baskaran Sankaran." ],
      "venue" : "IJCNLP, pages 1032–1036.",
      "citeRegEx" : "Yarmohammadi et al\\.,? 2013",
      "shortCiteRegEx" : "Yarmohammadi et al\\.",
      "year" : 2013
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 193
    }, {
      "referenceID" : 8,
      "context" : "Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : "Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 193
    }, {
      "referenceID" : 20,
      "context" : "Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 193
    }, {
      "referenceID" : 7,
      "context" : "In this paper, we study the problem of simultaneous translation in the context of neural machine translation (Forcada and Ñeco, 1997; Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 180
    }, {
      "referenceID" : 19,
      "context" : "In this paper, we study the problem of simultaneous translation in the context of neural machine translation (Forcada and Ñeco, 1997; Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 180
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we study the problem of simultaneous translation in the context of neural machine translation (Forcada and Ñeco, 1997; Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "Neural machine translation (Forcada and Ñeco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al.",
      "startOffset" : 27,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "Neural machine translation (Forcada and Ñeco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al.",
      "startOffset" : 27,
      "endOffset" : 130
    }, {
      "referenceID" : 19,
      "context" : "Neural machine translation (Forcada and Ñeco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al.",
      "startOffset" : 27,
      "endOffset" : 130
    }, {
      "referenceID" : 0,
      "context" : "Neural machine translation (Forcada and Ñeco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al.",
      "startOffset" : 27,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : ", 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al., 2003).",
      "startOffset" : 117,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "For instance, in the translation task of WMT’16, the top rankers for En↔Cs, En↔De, En→Fi and En→Ru all used attention-based neural machine translation (Bahdanau et al., 2015; Luong et al., 2015).",
      "startOffset" : 151,
      "endOffset" : 194
    }, {
      "referenceID" : 13,
      "context" : "For instance, in the translation task of WMT’16, the top rankers for En↔Cs, En↔De, En→Fi and En→Ru all used attention-based neural machine translation (Bahdanau et al., 2015; Luong et al., 2015).",
      "startOffset" : 151,
      "endOffset" : 194
    }, {
      "referenceID" : 13,
      "context" : "Instead of a vanilla, unidirectional recurrent network (Luong et al., 2015), it is possible to use a more sophisticated network such as a bidirectional recurrent network (Bahdanau et al.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : ", 2015), it is possible to use a more sophisticated network such as a bidirectional recurrent network (Bahdanau et al., 2015) or a tree-based recursive network (Eriguchi et al.",
      "startOffset" : 102,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : ", 2015) or a tree-based recursive network (Eriguchi et al., 2016).",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "The decoder is a conditional language model based on a recurrent network (Mikolov et al., 2010).",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "This content-based attention mechanism can be extended to incorporate also the location of each context vector (Luong et al., 2015).",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).",
      "startOffset" : 66,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).",
      "startOffset" : 66,
      "endOffset" : 160
    }, {
      "referenceID" : 18,
      "context" : "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).",
      "startOffset" : 66,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).",
      "startOffset" : 66,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013). Often, incoming speech is transcribed by an automatic speech recognition (ASR) system, and the transcribed text is segmented into a translation unit largely based on acoustic and linguistic cues (e.g., silence or punctuation marks.) Each of these segments is then translated largely independent from each other by a separate machine translation system. The simultaneous greedy decoding with neural machine translation, proposed in this paper, is clearly distinguished from these approaches in that (1) segmentation and translation happen jointly to maximize the translation quality and (2) each segmentation is strongly dependent on all the previous segment in both the source and target sentences. More recently, Grissom II et al. (2014) and Oda et al.",
      "startOffset" : 67,
      "endOffset" : 901
    }, {
      "referenceID" : 1,
      "context" : "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013). Often, incoming speech is transcribed by an automatic speech recognition (ASR) system, and the transcribed text is segmented into a translation unit largely based on acoustic and linguistic cues (e.g., silence or punctuation marks.) Each of these segments is then translated largely independent from each other by a separate machine translation system. The simultaneous greedy decoding with neural machine translation, proposed in this paper, is clearly distinguished from these approaches in that (1) segmentation and translation happen jointly to maximize the translation quality and (2) each segmentation is strongly dependent on all the previous segment in both the source and target sentences. More recently, Grissom II et al. (2014) and Oda et al. (2014) proposed to extend those earlier approaches by introducing a trainable segmentation policy which is trained to maximizes the translation quality.",
      "startOffset" : 67,
      "endOffset" : 923
    }, {
      "referenceID" : 17,
      "context" : "2 All the sentences were first tokenized3 and segmented into subword units using byte pair encoding (BPE) following (Sennrich et al., 2015b).",
      "startOffset" : 116,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "perl from Moses (Koehn et al., 2007).",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "We use a unidirectional recurrent network with 1028 gated recurrent units (GRU, (Cho et al., 2014)) as an encoder.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "Each model is trained with Adadelta (Zeiler, 2012) until the average log-probability on the validation set does not improve, which takes about a week per model.",
      "startOffset" : 36,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "These trained models do not achieve the state-ofthe-art translation qualities, as they do not exploit the ensemble technique (Sutskever et al., 2014) nor monolingual corpus (Sennrich et al.",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 16,
      "context" : ", 2014) nor monolingual corpus (Sennrich et al., 2015a), both of which have been found to be crucial in improving neural machine translation.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "We use a unidirectional recurrent network with 1028 gated recurrent units (GRU, (Cho et al., 2014)) as an encoder. A decoder is similarly a recurrent neural net with 1028 GRUs. The soft-alignment function is a feedforward network with one hidden layer consisting of 1028 tanh units. Each model is trained with Adadelta (Zeiler, 2012) until the average log-probability on the validation set does not improve, which takes about a week per model. These trained models do not achieve the state-ofthe-art translation qualities, as they do not exploit the ensemble technique (Sutskever et al., 2014) nor monolingual corpus (Sennrich et al., 2015a), both of which have been found to be crucial in improving neural machine translation. Under these constraints, however, the trained models translates as well as those reported earlier by, for instance, Firat et al. (2016), as can be seen in Table 1.",
      "startOffset" : 81,
      "endOffset" : 864
    }, {
      "referenceID" : 3,
      "context" : "The beam width is set to 5, as used in (Chung et al., 2016).",
      "startOffset" : 39,
      "endOffset" : 59
    } ],
    "year" : 2016,
    "abstractText" : "We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.",
    "creator" : "TeX"
  }
}